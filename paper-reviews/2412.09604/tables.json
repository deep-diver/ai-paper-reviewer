[{"content": "|   | Task | #Sam. | Datasets |\n|---|---|---|---| \n|   | Gen. | 667M | LAION-Aesthetics [67], Megalith [52], SAM [33], Objects365 [69], ImageNet-1k [18], |\n| S.1 | Und. | 667M | Laion-En [67], COYO [6], SAM [33] |\n|   | Gen. | 170M | LAION-Aesthetics [67], Megalith [52], Objects365 [69], Unsplash [85], Dalle-3-HQ [3], JourneyDB [74], Internal Dataset |\n|   |   | 170M | **Captioning:** Laion-En [67], Laion-Zh [67], COYO [6], GRIT [60], COCO [40], TextCaps [71] |\n|   |   |   | **Detection:** Objects365 [69], GRIT [60], All-Seeing [90] |\n|   |   |   | **OCR (large):** Wukong-OCR [26], LaionCOCO-OCR [68], Common Crawl PDF |\n|   |   |   | **OCR (small):** MMC-Inst [41], LSVT [79], ST-VQA [5], RCTW-17 [70], ReCTs [106], ArT [13], SynthDoG [32], ChartQA [53], CTW [104], DocVQA [15], TextOCR [73], |\n| S.2 | Und. | 170M | COCO-Text [87], PlotQA [55], InfoVQA [54] |", "caption": "Table 1: Summary of datasets used in Visual Alignment Pretraining. \u201cS.1\u201d and \u201cS.2\u201d denote the first and second stage. \u201cGen.\u201d and \u201cUnd.\u201d denote the image generation and understanding task. \u201c#Sam.\u201d denotes the number of total samples seen during training of each task at each stage. Note that all data used for image understanding in the second stage is also used in InternVL-1.5\u00a0[11].", "description": "This table summarizes the datasets used for visual alignment pre-training of SynerGen-VL, a multimodal large language model. The table is organized into stages (S.1 and S.2), tasks (image generation and understanding), and datasets. For each task and stage, the number of samples used is specified. Notably, all image understanding data from S.2 is also used in InternVL-1.5.", "section": "3.2. Training"}, {"content": "| Model | #A-Param | POPE | MMB | MMVet | MMMU | MME | MME-P | MathVista | SEED-I | OCRBench |\n|---|---|---|---|---|---|---|---|---|---|---| \n| **Understanding Only** | | | | | | | | | | |\n| *Encoder-based* | | | | | | | | | | |\n| LLaVA-1.5 [43] | 7B | 85.9 | 64.3 | 31.1 | 35.4 | - | 1512 | - | 58.6 | - |\n| Mini-Gemini-2B [38] | 3.5B | - | 59.8 | 31.1 | 31.7 | 1653 | - | 29.4 | - | - |\n| DeepSeek-VL-1.3B [48] | 2B | 87.6 | 64.6 | 34.8 | 32.2 | 1532 | - | 31.1 | 66.7 | 409 |\n| PaliGemma-3B [4] | 2.9B | 87.0 | 71.0 | 33.1 | 34.9 | 1686 | - | 28.7 | 69.6 | 614 |\n| MiniCPM-V2 [100] | 2.8B | - | 69.1 | 41.0 | 38.2 | 1809 | - | 38.7 | 67.1 | 605 |\n| InternVL-1.5 [11] | 2B | - | 70.9 | 39.3 | 34.6 | 1902 | - | 41.1 | 69.8 | 654 |\n| Qwen2-VL [88] | 2B | - | 74.9 | 49.5 | 41.1 | 1872 | - | 43.0 | - | 809 |\n| *Encoder-free* | | | | | | | | | | |\n| Fuyu-8B (HD) [2] | 8B | - | 10.7 | 21.4 | - | - | - | - | - | - |\n| EVE-7B [19] | 7B | 83.6 | 49.5 | 25.6 | 32.3 | 1483 | - | 25.2 | 61.3 | 327 |\n| Mono-InternVL [51] | 1.8B | - | 65.5 | 40.1 | 33.7 | 1875 | - | 45.7 | 67.4 | 767 |\n| **Understanding & Generation** | | | | | | | | | | |\n| *Encoder-based* | | | | | | | | | | |\n| Emu [78] | 14B | - | - | 36.3 | - | - | - | - | - | - |\n| Emu2 [76] | 37B | - | 63.6 | 48.5 | 34.1 | - | 1345 | - | 62.8 | - |\n| SEED-X [24] | 17B | 84.2 | 75.4 | - | 35.6 | - | 1436 | - | - | - |\n| LWM [45] | 7B | 75.2 | - | 9.6 | - | - | - | - | - | - |\n| DreamLLM [20] | 7B | - | 58.2 | 36.6 | - | - | - | - | - | - |\n| Janus [92] | 1.3B | 87.0 | 69.4 | 34.3 | 30.5 | - | 1338 | - | 63.7 | - |\n| *Encoder-free* | | | | | | | | | | |\n| Chameleon [8] | 7B | - | - | 8.3 | 22.4 | - | - | - | - | - |\n| Show-o [96] | 1.3B | 84.5 | - | - | 27.4 | - | 1233 | - | - | - |\n| VILA-U [94] | 7B | 85.8 | - | 33.5 | - | - | 1402 | - | 59.0 | - |\n| Emu3-Chat [91] | 8B | 85.2 | 58.5 | 37.2 | 31.6 | - | - | - | 68.2 | 687 |\n| SynerGen-VL (Ours) | 2.4B | 85.3 | 53.7 | 34.5 | 34.2 | 1837 | 1381 | 42.7 | 62.0 | 721 |", "caption": "Table 2: Results on general MLLM benchmarks. Our model with 2.4B parameters achieves competitive image understanding performance compared with significantly larger encoder-free unified MLLMs such as Emu3-Chat-8B\u00a0[91].", "description": "This table presents the results of various Multimodal Large Language Models (MLLMs) on a set of general benchmarks, evaluating their performance in image understanding tasks. SynerGen-VL, a novel encoder-free model with 2.4B parameters, is compared against other state-of-the-art MLLMs, both encoder-based and encoder-free, across various metrics such as POPE, MMB, MMVet, MMMU, MME, MME-P, MathVista, SEED-I, and OCRBench. The results demonstrate that SynerGen-VL achieves competitive performance compared to larger encoder-free models and even approaches the performance of some encoder-based methods, highlighting its efficiency and strong visual understanding capabilities.", "section": "4. Experiments"}, {"content": "| Method | #A-Param | TextVQA | SQA-I | GQA | DocVQA | AI2D | ChartQA | InfoVQA |\n|---|---|---|---|---|---|---|---|---| \n| **Understanding Only** | | | | | | | | |\n| *Encoder-based* | | | | | | | | |\n| MobileVLM-V2 [14] | 1.7B | 52.1 | 66.7 | 59.3 | - | - | - | - |\n| Mini-Gemini-2B [39] | 3.5B | 56.2 | - | - | 34.2 | - | - | - |\n| PaliGemma-3B [4] | 2.9B |  | 68.1 | - | - | - | 68.3 | - |\n| MiniCPM-V2 [100] | 2.8B | 74.1 | - | - | 71.9 | - | - | - |\n| InternVL-1.5 [11] | 2B | 70.5 | 84.9 | 61.6 | 85.0 | 69.8 | 74.8 | 55.4 |\n| *Encoder-free* | | | | | | | | |\n| EVE-7B [19] | 7B | 51.9 | 63.0 | 60.8 | - | - | - | - |\n| Mono-InternVL [51] | 1.8B | 72.6 | 93.6 | 59.5 | 80.0 | 68.6 | 73.7 | 43.0 |\n| **Understanding & Generation** | | | | | | | | |\n| *Encoder-based* | | | | | | | | |\n| Emu2 [76] | 37B | 66.6 | - | 65.1 | - | - | - | - |\n| LWM [45] | 7B | 18.8 | 47.7 | 44.8 | - | - | - | - |\n| DreamLLM [20] | 7B | 41.8 | - | - | - | - | - | - |\n| MM-Interleaved [82] | 13B | 61.0 | - | 60.5 | - | - | - | - |\n| Janus [92] | 1.3B | - | - | 59.1 | - | - | - | - |\n| *Encoder-free* | | | | | | | | |\n| Chameleon<sup>\u22c4</sup> [8] | 7B | 4.8 | 47.2 | - | 1.5 | 46.0 | 2.9 | 5.0 |\n| Show-o [96] | 1.3B | - | - | 61.0 | - | - | - | - |\n| VILA-U [94] | 7B | 60.8 | - | 60.8 | - | - | - | - |\n| Emu3-Chat [91] | 8B | 64.7 | 89.2 | 60.3 | 76.3 | 70.0 | 68.6 | 43.8 |\n| SynerGen-VL (Ours) | 2.4B | 67.5 | 92.6 | 59.7 | 76.6 | 60.8 | 73.4 | 37.5 |", "caption": "Table 3: Comparison with existing MLLMs on visual question answering benchmarks. #A-Params denotes the number of activated parameters during inference. \u22c4Some results of Chameleon are sourced from [51].", "description": "This table presents a comparison of SynerGen-VL with other Multimodal Large Language Models (MLLMs) on various visual question answering (VQA) benchmarks.  The models are compared based on their performance on TextVQA, SQA-I, GQA, DocVQA, AI2D, ChartQA, and InfoVQA datasets. The table also lists the number of activated parameters during inference (#A-Params) to showcase the efficiency of SynerGen-VL in achieving strong performance with fewer activated parameters. Some of the results for the \"Chameleon\" model were obtained from the Mono-Intern VL paper [51].", "section": "4.2. Image Understanding"}, {"content": "| Method | # A-Param | Single Obj. | Two Obj. | Counting | Colors | Position | Color Attri. | Overall\u2191 | \n |---|---|---|---|---|---|---|---|---| \n | **Generation Only** | | | | | | | | | \n | LlamaGen [75] | 0.8B | 0.71 | 0.34 | 0.21 | 0.58 | 0.07 | 0.04 | 0.32 | \n | LDM [65] | 1.4B | 0.92 | 0.29 | 0.23 | 0.70 | 0.02 | 0.05 | 0.37 | \n | SDv1.5 [65] | 0.9B | 0.97 | 0.38 | 0.35 | 0.76 | 0.04 | 0.06 | 0.43 | \n | SDXL [61] | 2.6B | 0.98 | 0.74 | 0.39 | 0.85 | 0.15 | 0.23 | 0.55 | \n | PixArt-\u03b1 [9] | 0.6B | 0.98 | 0.50 | 0.44 | 0.80 | 0.08 | 0.07 | 0.48 | \n | DALL-E 2 [64] | 6.5B | 0.94 | 0.66 | 0.49 | 0.77 | 0.10 | 0.19 | 0.52 | \n | **Understanding & Generation** | | | | | | | | | \n | SEED-X\u2020 [24] | 17B | 0.97 | 0.58 | 0.26 | 0.80 | 0.19 | 0.14 | 0.49 | \n | Show-o [96] | 1.3B | 0.95 | 0.52 | 0.49 | 0.82 | 0.11 | 0.28 | 0.53 | \n | LWM [45] | 7B | 0.93 | 0.41 | 0.46 | 0.79 | 0.09 | 0.15 | 0.47 | \n | Chameleon [8] | 34B | - | - | - | - | - | - | 0.39 | \n | Emu3-Gen [91] | 8B | 0.98 | 0.71 | 0.34 | 0.81 | 0.17 | 0.21 | 0.54 | \n | Janus [92] | 1.3B | 0.97 | 0.68 | 0.30 | 0.84 | 0.46 | 0.42 | 0.61 | \n | **SynerGen-VL (Ours)** | 2.4B | 0.99 | 0.71 | 0.34 | 0.87 | 0.37 | 0.37 | 0.61 |", "caption": "Table 4: Evaluation of text-to-image generation on GenEval\u00a0[25] benchmark. #A-Params denotes the number of activated parameters during inference. \u2020\u2020{\\dagger}\u2020 indicates models with external pretrained diffusion model. Obj.: Object. Attri.: Attribution.", "description": "This table presents the evaluation results of various text-to-image generation models on the GenEval benchmark.  It includes both single modality generation models and multi-modal models with image understanding and generation capabilities. The metrics used for evaluation cover aspects like single object, two objects, counting, color accuracy, positional accuracy, color attribute accuracy, and the overall score. The number of activated parameters during inference (#A-Params) is also provided for each model. Some models (marked with \u2020) utilize external pre-trained diffusion models.", "section": "4.3. Image Generation"}, {"content": "| Model | #A-Param | MS-COCO\u2193 | MJHQ\u2193 |\n|---|---|---|---|\n| **Generation Only** |  |  |  |\n| DALL-E [63] | 12B | 27.50 | - |\n| LDM [65] | 1.4B | 12.64 | - |\n| GLIDE [58] | 5B | 12.24 | - |\n| DALL-E 2 [64] | 6.5B | 10.39 | - |\n| RAPHAEL [97] | 3B | 6.61 | - |\n| Imagen [66] | 34B | 7.27 | - |\n| SDv1.5 [65] | 0.9B | 9.62 | - |\n| SDXL [61] | 0.9B | 7.38 | 8.76 |\n| PixArt-\u03b1 [9] | 0.6B | 7.32 | 6.14 |\n| **Understanding & Generation** |  |  |  |\n| NExT-GPT [93] | 13B | 11.18 | - |\n| SEED-X [24] | 17B | 14.99 | - |\n| Show-o [96] | 1.3B | 9.24 | 15.18 |\n| LWM [45] | 7B | 12.68 | 17.77 |\n| VILA-U [94] | 7B | - | 7.69 |\n| Emu3-Gen [91] | 8B | 19.3 | - |\n| Janus [92] | 1.3B | 8.53 | 10.10 |\n| **SynerGen-VL (Ours)** | 2.4B | 7.65 | 6.10 |", "caption": "Table 5: Image generation results on MSCOCO-30K\u00a0[40] and MJHQ-30K\u00a0[35] datasets. FID\u00a0[27] is reported. #A-Param denotes the number of activated parameters during inference.", "description": "This table presents a comparison of different models on image generation tasks, using the MSCOCO-30K and MJHQ-30K datasets.  The Fr\u00e9chet Inception Distance (FID) score is used to evaluate the quality of generated images, with lower FID scores indicating better quality. The table includes both models designed solely for image generation and unified models capable of both image understanding and generation.  The number of activated parameters during inference (#A-Param) is also provided for each model, giving insight into the computational resources required.", "section": "4.3. Image Generation"}, {"content": "| Model | TextVQA | GQA | DocVQA | AI2D | ChartQA | InfoVQA |\n|---|---|---|---|---|---|---| \n| *w/o* token folding | 18.7 | 45.3 | 14.7 | 42.0 | 20.9 | 18.7 |\n| *w/* token folding | 35.0 | 45.1 | 36.7 | 42.1 | 49.7 | 21.1 |", "caption": "Table 6: Comparison between models with and without token-folding on VQA benchmarks. The model with token folding demonstrates significant performance improvements with the same image token sequence length.", "description": "This table compares the performance of two models on six Visual Question Answering (VQA) benchmarks: TextVQA, GQA, DocVQA, AI2D, ChartQA, and InfoVQA. One model uses token folding, while the other does not.  The purpose of this comparison is to demonstrate the effectiveness of token folding, specifically in scenarios that require understanding of high-resolution images or detailed image comprehension, such as in OCR-related tasks.  Both models were trained using a subset of stage 2 understanding data, as outlined in Section 3.2 of the paper. The results show that the model with token folding achieves significantly better performance across all six VQA benchmarks, supporting the hypothesis that this technique improves the model's ability to understand high-resolution image details.  The metric used for evaluation is presumably accuracy, although the specific metric is not explicitly stated in the caption or surrounding text. The table also includes the scores of a baseline model 'Qwen2-0.5B' for comparison.", "section": "5. Ablation Study"}, {"content": "| Stage | Strategy | TextVQA | GQA | DocVQA | AI2D | ChartQA | InfoVQA | MMLU | CMMLU | AGIEVAL | MATH | MSCOCO | \n|---|---|---|---|---|---|---|---|---|---|---|---|---| \n| Baseline (Qwen2-0.5B) | | - | - | - | - | - | - | 42.3 | 51.4 | 29.3 | 12.1 | - |\n| S.1 + S.2 | Full | 14.3 | 42.9 | 11.3 | 24.7 | 12.4 | 12.6 | 23.1 | 23.0 | 8.1 | 0.9 | 30.7 |\n| S.1 only | Progressive | 0.1 | 13.0 | 0.2 | 0.3 | 0.0 | 0.0 | 42.3 | 51.4 | 29.3 | 12.1 | 28.3 |\n| S.2 only | Progressive | 8.7 | 36.9 | 8.6 | 40.9 | 11.7 | 16.2 | 37.6 | 45.3 | 28.9 | 7.2 | 34.9 |\n| S.1 + S.2 | Progressive | 13.2 | 41.2 | 11.4 | 41.9 | 12.8 | 17.0 | 39.3 | 48.2 | 26.2 | 8.9 | 20.2 |", "caption": "Table 7: Zero-shot performance of different pre-training strategies. \u201cS.1\u201d and \u201cS.2\u201d denote the first and second pre-training stage. \u201cFull\u201d and \u201cProgressive\u201d denote the full parameter tuning and our progressive tuning strategy with MMoEs, respectively. FID\u00a0[27] is reported for text-to-image generation (T2I) on MSCOCO\u00a0[40].", "description": "This table (Table 7) presents the zero-shot performance results of SynerGen-VL under different pre-training strategies, compared to a baseline model (Qwen2-0.5B-Instruct). The pre-training strategies involve different combinations of two stages (S.1 and S.2) and two tuning approaches ('Full' and 'Progressive'). 'Full' refers to training all model parameters, while 'Progressive' denotes training with Multimodal Mixture-of-Experts (MMoEs), where only specific visual components are trained initially, followed by training the entire model. The evaluation metrics include various VQA benchmarks, NLP benchmarks (MMLU, CMMLU, AGIEVAL, MATH), and FID score for text-to-image generation on MSCOCO. The table aims to demonstrate the effectiveness of the proposed progressive alignment pre-training with MMoEs in preserving the pre-trained LLM's knowledge while improving performance on visual tasks. ", "section": "5. Ablation Study"}, {"content": "| Configuration | Alignment Pre-training | Instruction |\n|---|---|---| \n| | S.1 | S.2 | Tuning |\n| Maximum number of image tiles | 1 | 6 | 12 |\n| LLM sequence length | 4,096 | 8,192 | 16,384 |\n| Use thumbnail | \u2717 | \u2713 | \u2713 |\n| Global batch size (per-task) | 6,988 | 5,090 | 1,760 |\n| Peak learning rate | 1e^{-4} | 5e^{-5} | 5e^{-5} |\n| Learning rate schedule | constant with warm-up | cosine decay | cosine decay |\n| Weight decay | 0.05 | 0.05 | 0.01 |\n| Training steps | 95k | 35k | 12k |\n| Warm-up steps | 200 | 200 | 200 |\n| Optimizer | AdamW | AdamW | AdamW |\n| Optimizer hyperparameters | \\beta_{1}=0.9,\\beta_{2}=0.95,eps=1e^{-8} | \\beta_{1}=0.9,\\beta_{2}=0.95,eps=1e^{-8} | \\beta_{1}=0.9,\\beta_{2}=0.95,eps=1e^{-8} |\n| Gradient accumulation | 1 | 1 | 1 |\n| Numerical precision | bfloat16 | bfloat16 | bfloat16 |", "caption": "Table 8: Hyper-parameters used in the alignment pre-training and instruction tuning stages.", "description": "This table provides a detailed breakdown of the hyper-parameters employed during the two stages of alignment pre-training and the subsequent instruction tuning stage. The alignment pre-training aims to integrate visual capabilities into the pre-trained language model while minimizing disruption to its existing knowledge. Stage 1 focuses on establishing basic visual concept understanding and generating images that align with the language model's representation space. Stage 2 refines this alignment using higher-quality data to enhance image-text alignment and improve image aesthetics.  Instruction tuning then adapts the model to a wide range of downstream tasks by fine-tuning all parameters on a diverse set of instructions. The table details settings like maximum image tiles, sequence length, batch size, learning rate, and optimization strategies for each stage.", "section": "A. Detailed Training Configurations"}]