{"references": [{" publication_date": "2024", "fullname_first_author": "Jian Xie", "paper_title": "Revealing the Barriers of Language Agents in Planning", "reason": "This is the main paper of the study, introducing the problem, methodology, findings, and discussion.  It is foundational to understanding the research and its contributions to the field.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Yanan Chen", "paper_title": "Can we rely on llm agents to draft long-horizon plans?", "reason": "This paper directly addresses the limitation of LLMs in long-horizon planning, which is a crucial aspect of the research question.  Its focus on TravelPlanner, a real-world benchmark, complements the main study's analysis.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Gautier Dagan", "paper_title": "Dynamic planning with a llm", "reason": "This paper explores an approach to dynamic planning using LLMs, a key topic related to the main study's investigation into different planning strategies and their limitations.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Abhimanyu Dubey", "paper_title": "The llama 3 herd of models", "reason": "This work introduces a set of LLMs that are used in the study, providing context for the model selection and performance evaluation.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Yao Fu", "paper_title": "Autoguide: Automated generation and selection of state-aware guidelines for large language model agents", "reason": "This paper tackles the challenge of improving LLM planning by automating the generation of guidelines, a relevant aspect of the main study\u2019s examination of strategies to improve language agent planning performance.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Gemini G Team", "paper_title": "Gemini: a family of highly capable multimodal models", "reason": "This paper introduces a significant LLM model, Gemini, used in the comparative analysis of the study, contributing to the understanding of the models' performance and limitations.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Carlos G\u00f3mez-Rodr\u00edguez", "paper_title": "A confederacy of models: a comprehensive evaluation of llms on creative writing", "reason": "While not directly related to planning, this paper provides context on the overall capabilities and limitations of LLMs, relevant to the discussion on the general limitations of LLMs in the field of planning.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Yu Gu", "paper_title": "Middleware for llms: Tools are instrumental for language agents in complex environments", "reason": "This paper discusses the use of tools by language agents, a related area to planning that sheds light on the broader capabilities and challenges of language agents in real-world applications.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Atharva Gundawar", "paper_title": "Robust planning with llm-modulo framework: Case study in travel planning", "reason": "This research directly relates to real-world planning tasks, specifically travel planning, which is one of the benchmarks used in the main study. It offers a comparative perspective on the challenges of planning in complex real-world scenarios.", "section_number": 2}, {" publication_date": "2011", "fullname_first_author": "Daniel Kahneman", "paper_title": "Thinking, fast and slow", "reason": "This book provides a theoretical framework for understanding human cognitive processes, relevant to the discussion on the difference between human-level planning and the current capabilities of language agents.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Subbarao Kambhampati", "paper_title": "Position: Llms can't plan, but can help planning in Ilm-modulo frameworks", "reason": "This position paper offers a direct comparison of LLMs' planning capabilities with human-level performance, adding context to the research question of the main study.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Bo Liu", "paper_title": "Llm+p: Empowering large language models with optimal planning proficiency", "reason": "This study explores a method to enhance planning capabilities using LLMs, which is directly relevant to the main research question and contributes to the understanding of existing strategies.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Nelson F. Liu", "paper_title": "Lost in the middle: How language models use long contexts", "reason": "This paper investigates the challenges LLMs face in handling long contexts, a crucial aspect of planning tasks that the main study also considers.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Haoyan Luo", "paper_title": "From understanding to utilization: A survey on explainability for large language models", "reason": "This survey is relevant as the main study uses interpretability techniques to analyze the reasoning process of LLMs in planning. The survey provides a broader context for the methods used in the study.", "section_number": 2}, {" publication_date": "2001", "fullname_first_author": "Leo Breiman", "paper_title": "Random forests", "reason": "This paper introduces the Random Forest algorithm, the basis for the Permutation Feature Importance method, which is a core methodology in the main study for assessing the relative importance of various factors in influencing the language agents' planning outcomes.", "section_number": 3}, {" publication_date": "2019", "fullname_first_author": "Aaron Fisher", "paper_title": "All models are wrong, but many are useful: Learning a variable's importance by studying an entire class of prediction models simultaneously", "reason": "This work is highly relevant as it provides the theoretical foundation for feature importance analysis, a key method used in the main study to understand the role of constraints and questions in influencing the planning process of the language agents.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Shunyu Yao", "paper_title": "Reflexion: Language agents with verbal reinforcement learning", "reason": "This paper directly addresses the issue of improving language agent performance through feedback mechanisms.  This is highly relevant to the central theme of the main study which explores various memory updating strategies for improving LLM planning.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Da Yin", "paper_title": "Agent lumos: Unified and modular training for open-source language agents", "reason": "This paper explores a modular training approach for improving language agents, directly addressing the limitations of current methods for improving language agent planning. This relates directly to the comparative study of different memory updating methods undertaken in the main paper.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Aohan Zeng", "paper_title": "Agenttuning: Enabling generalized agent abilities for llms", "reason": "This paper directly addresses the limitations of generalizing language agents\u2019 abilities, a key challenge in the domain of autonomous planning. The method proposed in this paper is one of the main methods that were used in the current study to evaluate.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Andrew Zhao", "paper_title": "Expel: Llm agents are experiential learners", "reason": "This paper presents a novel method for improving language agent planning, based on experiential learning. This is directly comparable to the memory updating methods explored in the main study, providing additional context and a comparison point for the main findings.", "section_number": 5}]}