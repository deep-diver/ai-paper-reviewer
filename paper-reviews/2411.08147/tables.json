[{"content": "| Prompt | Llama-3.1-8B-Instruct |  |  | Llama-3.1-70B-Instruct |  |  |\n|---|---|---|---|---|---|---|\n|  | HotpotQA | MuSiQue | 2WikiMQA | HotpotQA | MuSiQue | 2WikiMQA |\n|---|---|---|---|---|---|---|\n| Default | 55.5 | 33.0 | 66.0 | 60.0 | 54.0 | 77.0 |\n| Direct answer | 49.0 | 28.5 | 55.0 | 61.5 | 51.5 | 74.0 |\n| Think step-by-step [Kojima et al., 2022] | 62.5 | **50.5** | 77.5 | 75.5 | 62.5 | 85.0 |\n| Fact-and-reflection [Zhao et al., 2024b] | **67.0** | 49.0 | 76.5 | **78.0** | 62.0 | 84.0 |\n| Plan-and-solve [Wang et al., 2023a] | 64.0 | 49.5 | **82.0** | 74.0 | **68.5** | **85.5** |", "caption": "Table 1: \nComparison of various prompting methods. The best result is highlighted in bold.", "description": "This table compares the performance of several prompting methods on two LLMs, Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct, across three long-context reasoning tasks: HotpotQA, MuSiQue, and 2WikiMQA.  The performance is measured using the substring exact match (SubEM) metric. The table helps illustrate how different prompting techniques can significantly impact the effectiveness of LLMs in long-context reasoning. The best performing prompting method for each task and LLM is highlighted in bold.", "section": "2 Understanding the Potential of LLMs in Long-context Reasoning"}, {"content": "| Model | Qasper | MultiFieldQA-En | HotpotQA | MuSiQue | 2WikiMQA | Avg. |\n|---|---|---|---|---|---|---|\n| Qwen-2.5-7B-Instruct (Yang et al., 2024a) | 21.0 | 28.0 | 70.5 | 48.0 | 77.5 | 49.0 |\n| + SeaLong | **26.0** | **29.3** | **72.5** | **51.5** | **79.5** | **51.8** |\n| Qwen-2.5-14B-Instruct (Yang et al., 2024a) | 21.0 | **32.0** | 73.0 | 52.0 | 83.0 | 52.2 |\n| + SeaLong | **24.0** | 30.0 | **75.0** | **57.0** | **87.5** | **54.7** |\n| Llama-3.1-8B-Instruct (Dubey et al., 2024) | 29.0 | 29.3 | 64.0 | 49.5 | 82.0 | 50.8 |\n| + SeaLong | **32.5** | **31.3** | **68.0** | **58.5** | **84.5** | **55.0** |\n| Qwen-2.5-32B-Instruct (Yang et al., 2024a) | 24.5 | 26.0 | 72.0 | 55.0 | 88.0 | 53.1 |\n| Qwen-2.5-72B-Instruct (Yang et al., 2024a) | 27.0 | 28.7 | **74.5** | 58.5 | **89.0** | 55.5 |\n| Llama-3.1-70B-Instruct (Dubey et al., 2024) | **30.0** | **33.3** | 74.0 | **68.5** | 85.5 | **58.3** |\n| GPT-4o (Hurst et al., 2024) | 21.5 | 28.0 | **74.5** | 64.0 | 84.0 | 54.4 |", "caption": "Table 2: \nMain evaluation results. Substring exact match (SubEM) serves as the evaluation metric, with the top-performing results emphasized in bold. SeaLong utilizes the training set of MuSiQue with self-supervision (\u00a73.1), and its performance on other tasks demonstrates the generalization ability of SeaLong.", "description": "Table 2 presents the main experimental results of the SEALONG model, compared against various baselines. The evaluation metric used is Substring Exact Match (SubEM), which measures if the correct answer is a substring of the model's output.  The table highlights the best-performing model for each task in bold.  Importantly, SEALONG only used the MuSiQue training set with a self-supervision approach for training, showcasing its ability to generalize well to other datasets.", "section": "4.2 Evaluation Setup"}, {"content": "| Task | # Example | Max Tokens | Avg. Tokens |\n|---|---|---|---| \n| Qasper | 200 | 21,110 | 4,921 |\n| MultiFieldQA-en | 150 | 14,947 | 6,888 |\n| HotpotQA | 200 | 16,322 | 12,779 |\n| MuSiQue | 200 | 16,335 | 15,542 |\n| 2WikiMultihopQA | 200 | 16,319 | 7,096 |", "caption": "Table 3: \nStatistics of evaluation tasks, with token counts calculated using the tokenizer of Llama-3.1-8B-Instruct.", "description": "This table presents a statistical overview of the datasets used for evaluating long-context reasoning models. It shows the number of examples, the maximum number of tokens, and the average number of tokens per example for five different tasks: Qasper, MultiFieldQA-en, HotpotQA, MuSiQue, and 2WikiMultihopQA.  Token counts are calculated using the Llama-3.1-8B-Instruct tokenizer, ensuring consistency in the tokenization process across different datasets.", "section": "2 Understanding the Potential of LLMs in Long-context Reasoning"}, {"content": "| Model | Avg. Long-context | Avg. Output Tokens |\n|---|---|---|\n| Qwen-2.5-Instruct 7B | 49.0 | 375 |\n| Qwen-2.5-Instruct 7B + SeaLong | 51.8 | 371 |\n| Llama-3.1-Instruct 8B | 50.8 | 289 |\n| Llama-3.1-Instruct 8B + SeaLong | 55.0 | 295 |", "caption": "Table 4: \nAverage performance on long-context tasks (Tab. 2) and average token count in model predictions for these tasks, measured with the model\u2019s tokenizer.", "description": "This table presents the average performance of different large language models (LLMs) on various long-context reasoning tasks, as reported in Table 2 of the paper.  It also shows the average number of tokens generated by each model in its responses.  The token count is a measure of the length of the model's answer and is calculated using the model's internal tokenizer.", "section": "4 Experiments"}, {"content": "| Model | Qasper | MultiFieldQA-En | HotpotQA | MuSiQue | 2WikiMQA | Avg. |\n|---|---|---|---|---|---|---|\n| Llama-3.1-8B-Instruct | 29.0 | 29.3 | 64.0 | 49.5 | 82.0 | 50.8 |\n| *Supervised Fine-tuning* |  |  |  |  |  |  |\n| + TULU-V2-mix | 26.5 | 27.3 | 49.5 | 27.5 | 54.0 | 37.0 |\n| + WildChat | 20.5 | 29.3 | 46.5 | 28.0 | 58.0 | 36.5 |\n| + LongAlpaca | 22.5 | 31.3 | 48.0 | 31.0 | 45.0 | 35.6 |\n| + LongAlign | 25.0 | **36.7** | 58.5 | 47.5 | 76.0 | 48.7 |\n| + LongMIT | 20.0 | 30.0 | 56.0 | 36.0 | 66.5 | 41.7 |\n| + LongReward-SFT | 22.0 | 28.7 | 58.0 | 52.0 | 76.5 | 47.4 |\n| + GPT-4o-MuSiQue | 21.5 | 31.3 | 64.0 | **54.0** | 83.5 | 50.9 |\n| + SEAlong-SFT | **28.5** | 30.7 | **68.5** | 50.5 | **84.0** | **52.4** |\n| *Preference Optimization* |  |  |  |  |  |  |\n| + UltraFeedback | 26.0 | 27.3 | 47.5 | 28.5 | 46.0 | 35.1 |\n| + LongReward-Preference | 26.5 | **32.0** | 63.5 | 52.0 | 80.5 | 50.9 |\n| + SEAlong | **32.5** | 31.3 | **68.0** | **58.5** | **84.5** | **55.0** |", "caption": "Table 5: \nA comparison between SeaLong and previous datasets. The results are based on Llama-3.1-8B-Instruct finetuned on the corresponding dataset. To ensure fairness, 2\u2062K2\ud835\udc3e2K2 italic_K examples are randomly sampled from each dataset, with the exception of TULU-V2-mix, WildChat, and UltraFeedback, where the longest 2\u2062K2\ud835\udc3e2K2 italic_K examples are selected. The preference optimization strategy is ORPO (Hong et\u00a0al., 2024).", "description": "This table compares the performance of the SEALONG method with several other methods for long-context reasoning, all fine-tuned on Llama-3.1-8B-Instruct.  The results for each method are presented as average SubEM scores across five different long-context reasoning tasks.  To ensure a fair comparison, 2000 examples were sampled from each dataset (except for three datasets where the longest 2000 were used).  The comparison highlights SEALONG's effectiveness relative to other approaches that utilize different training data sources. ORPO (a preference optimization strategy) was used for all preference optimization methods.", "section": "4.2 Evaluation Setup"}, {"content": "| Dataset | Supervision | Avg. Tokens |\n|---|---|---|\n| TULU-V2-mix (2023) | [1], [2], [3] | 3,788 |\n| WildChat (2024a) | [2], [3] | 32,230 |\n| LongAlpaca (2024b) | [1], [4] | 9,160 |\n| LongAlign (2024) | [4] | 16,881 |\n| LongMIT (2024c) | [5] | 78,412 |\n| LongReward-SFT (2024b) | [6] | 22,206 |\n| LongReward-Preference (2024b) | [6] | 22,689 |\n| UltraFeedback (2023) | [3] | 1,356 |\n| GPT-4o-MuSiQue | [7] | 18,476 |\n| SeaLong | [8] | 18,532 |", "caption": "Table 6: \nDataset statistics, including supervision source and average token count, measured with the Llama3.1-8B-Instruct tokenizer. Sources: [1] Human, [2] GPT-3.5-Turbo (OpenAI, 2022), [3] GPT-4 (Achiam et\u00a0al., 2023), [4] Claude (Anthropic, 2023), [5] Qwen2-72B-Instruct (Yang et\u00a0al., 2024a), [6] GLM-4 (GLM et\u00a0al., 2024), [7] GPT-4o (Hurst et\u00a0al., 2024), and [8] Self.", "description": "Table 6 presents a detailed breakdown of various datasets used in the paper's experiments, focusing on their characteristics relevant to long-context reasoning.  It lists each dataset's name, the type of supervision used to create it (e.g., human annotation, GPT-3.5-Turbo, GPT-4, etc.), and the average number of tokens per data point, all calculated using Llama-3.1-8B-Instruct tokenizer. This information is crucial for understanding the different resources and data characteristics that the models were trained on and how this might have impacted the results.", "section": "4 Experiments"}, {"content": "| Method | HotpotQA | MuSiQue | 2WikiMQA |\n|---|---|---|---| \n| Greedy Search | 64.0 | 49.5 | 82.0 |\n| Random | 61.0 | 50.5 | 79.5 |\n| Reference-free Self-evaluation | 64.0 | 51.5 | 83.0 |\n| *Minimum Bayes Risk* |  |  |  |\n| ROUGE | 66.5 | 53.5 | 85.0 |\n| BERTScore | **67.5** | 50.0 | 86.5 |\n| Reference-based Self-evaluation | 63.5 | 51.5 | 84.5 |\n| Sentence Embedding | **67.5** | **56.0** | **88.0** |", "caption": "Table 7: \nComparison of various scoring methods and greedy search. Each scoring method evaluates 16161616 outputs sampled from Llama-3.1-8B-Instruct. The results indicate the performance of the highest-scoring output for each method.", "description": "This table compares different methods for scoring multiple outputs generated by Llama-3.1-8B-Instruct, a large language model.  The goal is to determine which scoring approach best identifies the highest-quality output among multiple options.  Each scoring method is applied to 16 different outputs, and the table reports the performance of only the highest-scoring output from each method.  The performance is presumably measured on a downstream task, and comparing performance across various methods helps determine the best strategy for selecting high-quality responses from an LLM.", "section": "4.4 Analysis"}, {"content": "| Model | Long-Context | MMLU | GSM8K | ARC-Challenge | HellaSwag | Winogrande | TruthfulQA | Avg. |\n|---|---|---|---|---|---|---|---|---|\n| Qwen-2.5-7B-Instruct | 49.0 | 74.2 | 82.4 | 67.1 | 81.5 | 74.7 | 64.7 | 74.1 |\n| Qwen-2.5-7B-Instruct + SeaLong | **51.8** | 74.1 | 83.2 | 66.5 | 81.3 | 74.4 | 64.8 | 74.1 |\n| Llama-3.1-8B-Instruct | 50.8 | 68.3 | 77.7 | 60.2 | 80.1 | 77.4 | 54.1 | 69.6 |\n| Llama-3.1-8B-Instruct + SeaLong | **55.0** | 68.4 | 77.8 | 60.3 | 79.9 | 77.3 | 53.8 | 69.6 |", "caption": "Table 8: \nEvaluation results on short-context tasks from the Open LLM Leaderboard (Beeching et\u00a0al., 2023), with the long-context average performance referenced from Tab.2. SeaLong demonstrates a marked improvement in long-context performance, with minimal impact on short-context performance.", "description": "Table 8 presents the evaluation results of SeaLong and baseline models on several short-context tasks from the Open LLM Leaderboard.  It compares the average performance on these short-context tasks with the average performance on long-context tasks (reported in Table 2). This comparison demonstrates SeaLong's significant improvement in long-context reasoning abilities while maintaining comparable performance on short-context tasks.", "section": "4.3 Main Results"}, {"content": "| Strategy | Prompt |\n|---|---| \n| Default | {context}<br>{input} |\n| Direct Answer | {context}<br>{input}<br>Let\u2019s answer the question directly. |\n| Think step-by-step (Kojima et al., 2022) | {context}<br>{input}<br>Let\u2019s think step by step. |\n| Fact-and-reflection (Zhao et al., 2024b) | {context}<br>{input}<br>Let\u2019s first identify the relevant information from the long context and list it. Then, carry out step-by-step reasoning based on that information, and finally, provide the answer. |\n| Plan-and-solve (Wang et al., 2023a) | {context}<br>{input}<br>Let\u2019s first understand the problem and devise a plan to solve it. Then, let\u2019s carry out the plan and solve the problem step-by-step. |", "caption": "Table 9: The prompts for various prompting strategies (\u00a72.1), where {context} and {input} serve as placeholders for the long context and input query, respectively.", "description": "This table lists various prompting strategies used in the paper's experiments and shows the prompts used for each strategy.  The prompts are templates;  '{context}' is replaced with the actual long text provided to the LLM, and '{input}' is replaced with the question. Different strategies include a simple default prompt, a direct answer prompt, step-by-step reasoning, fact-and-reflection prompting, and plan-and-solve prompting. Each prompt is designed to elicit different reasoning behaviors from the language model.", "section": "2.1 Prompting Strategies Matter"}, {"content": "| Strategy | Prompt |\n|---|---| \n| Reference-free Self-Evaluation | [Context]<br>{context}<br>[Question]<br>{question}<br>[Predicted Response]<br>{prediction}<br>Please evaluate the correctness of the predicted response based on the context and the question. Begin your evaluation by providing a brief explanation. Be as objective as possible. After giving your explanation, you must rate the response on a scale from 1 to 5, following this format exactly: \u201c[[rating]]\u201d. For example, \u201cRating: [[3]]\u201d. |\n| Reference-based Self-Evaluation | Here is a question along with two responses: one is the reference response, and the other is the predicted response. Please determine whether the two responses provide the same answer to the question. Respond with \u201cTrue\u201d or \u201cFalse\u201d directly.<br>[Question]<br>{question}<br>[Reference Response]<br>{reference}<br>[Predicted Response]<br>{prediction} |", "caption": "Table 10: The prompts for the reference-free and reference-based self-evaluation strategies (\u00a74.4), where {question}, {reference}, {prediction}, and {context} serve as placeholders for their respective elements.", "description": "Table 10 shows the different prompts used in the reference-free and reference-based self-evaluation strategies.  The reference-free strategy asks the LLM to evaluate the correctness of a given response based on the context and question, providing a rating from 1-5. The reference-based strategy presents the LLM with a question, a reference response and a predicted response, asking it to determine if both responses provide the same answer.  The table uses placeholders {context}, {question}, {reference}, and {prediction} to indicate where the actual context, question, reference response, and prediction would be inserted.", "section": "4.4 Analysis"}]