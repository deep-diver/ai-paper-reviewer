[{"figure_path": "https://arxiv.org/html/2411.08147/extracted/5995567/figures/impact_of_exploration.png", "caption": "Figure 1: \nScaling up the number of sampled outputs improves the performance of both the oracle sample and MBR decoding (\u00a73.1). The results are based on Llama-3.1-8B-Instruct.", "description": "This figure shows how increasing the number of sampled model outputs affects the accuracy of both an oracle (the best possible output) and the MBR decoding method.  The x-axis represents the number of samples, and the y-axis shows the accuracy (SubEM score) on three different long-context reasoning tasks: HotpotQA, MuSiQue, and 2WikiMQA.  The results demonstrate that as the number of samples increases, the accuracy of both the oracle and MBR decoding improve significantly.  This improvement suggests that selecting the best model response from a set of candidates is more effective than relying on a single prediction. The model used for this experiment is Llama-3.1-8B-Instruct.", "section": "2 Understanding the Potential of LLMs in Long-context Reasoning"}, {"figure_path": "https://arxiv.org/html/2411.08147/x1.png", "caption": "Figure 2: \nSeaLong consists of two stages: self-supervision creation and fine-tuning. Given a long context and a corresponding query, multiple outputs are sampled, each assigned a score based on Minimum Bayes Risk. Fine-tuning is then conducted using either the highest-scoring output for supervised fine-tuning or both high-scoring and low-scoring outputs for preference optimization.", "description": "SEALONG is a two-stage process. First, it generates multiple responses to a given long context and question using a plan-and-solve prompting strategy.  These responses are scored using Minimum Bayes Risk (MBR), which favors responses with higher consistency.  Second, these scores inform the fine-tuning method.  The highest-scoring response can be used for supervised fine-tuning, or high and low scoring responses can be used for preference optimization.", "section": "3 SEALONG"}, {"figure_path": "https://arxiv.org/html/2411.08147/extracted/5995567/figures/impact_of_the_number_of_training_examples.png", "caption": "Figure 3: \nLong-context performance of SeaLong with varying numbers of synthetic training examples, evaluated based on Llama-3.1-8B-Instruct fine-tuned on the corresponding dataset.", "description": "This figure displays the relationship between the number of synthetic training examples used in SeaLong and the resulting performance on long-context tasks.  The performance is measured using Llama-3.1-8B-Instruct, which was fine-tuned on datasets created with different numbers of synthetic examples. The graph shows that SeaLong's performance improves with increasing numbers of synthetic training examples, but the improvement plateaus after a certain point, demonstrating the efficiency of the method.", "section": "4.4 Analysis"}, {"figure_path": "https://arxiv.org/html/2411.08147/extracted/5995567/figures/impact_of_sampling_quantity.png", "caption": "Figure 4: \nLong-context performance of SeaLong with varying numbers of samples per example during data synthesis, evaluated based on Llama-3.1-8B-Instruct fine-tuned on the corresponding dataset.", "description": "This figure shows how the performance of the SeaLong model changes depending on the number of samples used per example during the data synthesis phase.  The evaluation was done using the Llama-3.1-8B-Instruct model, fine-tuned on the data created with varying numbers of samples.  The performance is measured across several long-context reasoning tasks (as shown in the different colored lines), illustrating how increasing the number of samples improves performance up to a certain point, after which improvements become marginal.  This demonstrates SeaLong's efficiency and effectiveness in leveraging multiple LLM outputs for improved performance.", "section": "4.4 Analysis"}]