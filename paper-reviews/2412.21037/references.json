{"references": [{"fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-03-02", "reason": "This paper is foundational to the field of aligning LLMs through reinforcement learning from human feedback (RLHF), which is a key technique the current paper builds upon."}, {"fullname_first_author": "Rafael Rafailov", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "publication_date": "2024-05-12", "reason": "This paper introduces Direct Preference Optimization (DPO), a method the current paper adapts for training audio generation models, offering a more efficient approach to alignment."}, {"fullname_first_author": "Michael S. Albergo", "paper_title": "Building normalizing flows with stochastic interpolants", "publication_date": "2023-09-15", "reason": "This paper presents advancements in normalizing flows, a crucial component of the TANGOFLUX architecture that enhances efficiency in audio generation."}, {"fullname_first_author": "Patrick Esser", "paper_title": "Scaling rectified flow transformers for high-resolution image synthesis", "publication_date": "2024-03-03", "reason": "This paper details the architecture of FluxTransformer, a key building block of TANGOFLUX, highlighting its superior performance and scalability in generating high-quality audio."}, {"fullname_first_author": "Yaron Lipman", "paper_title": "Flow matching for generative modeling", "publication_date": "2023-10-27", "reason": "This paper introduces the flow matching framework, the foundation upon which TANGOFLUX's training methodology is based, offering improvements in robustness and efficiency."}]}