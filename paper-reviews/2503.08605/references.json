{"references": [{"fullname_first_author": "Yogesh Balaji", "paper_title": "ediffi: Text-to-image diffusion models with an ensemble of expert denoisers", "publication_date": "2022-11-01", "reason": "This paper introduces a text-to-image diffusion model, a fundamental technique upon which text-to-video models are built."}, {"fullname_first_author": "Jiaming Song", "paper_title": "Denoising diffusion implicit models", "publication_date": "2020-01-01", "reason": "This work presents DDIM, which is a non-Markovian approach to accelerate sampling while preserving the marginal distribution, critical for efficient diffusion modeling."}, {"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-01-01", "reason": "This reference introduces CLIP, a model for learning visual features from natural language supervision, and features from this model were used to study and demonstrate the effectiveness of the current work."}, {"fullname_first_author": "Robin Rombach", "paper_title": "High-resolution image synthesis with latent diffusion models", "publication_date": "2022-01-01", "reason": "This paper introduces latent diffusion models, which enable high-resolution image synthesis and improve the efficiency of diffusion models."}, {"fullname_first_author": "Fu-Yun Wang", "paper_title": "Gen-l-video: Multi-text to long video generation via temporal co-denoising", "publication_date": "2023-05-18", "reason": "This work is a tuning-free baseline, and therefore is used as a comparison in the current work."}]}