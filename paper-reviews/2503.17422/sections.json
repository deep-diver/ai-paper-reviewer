[{"heading_title": "RISC-V for LLMs", "details": {"summary": "**RISC-V** is emerging as a compelling alternative to GPUs and x86 CPUs for **LLM inference**, particularly due to its open-source nature and vendor neutrality. This allows for greater flexibility and customization. The paper addresses the gap in mature **RISC-V** hardware and software ecosystems for LLMs, optimizing inference on the Sophon SG2042, a many-core **RISC-V** CPU with vector processing. They achieved significant speedups on DeepSeek models. Key optimizations include developing quantized kernels, selecting a suitable compiler toolchain (**Clang outperforms GCC**), and optimizing model mapping with NUMA-aware thread allocation, reducing memory page migration. It is shown that a **carefully optimized RISC-V** platform can be competitive with incumbent architectures for specific LLM workloads. The research highlights the potential of **RISC-V** in democratizing access to LLM inference capabilities and paving the way for tailored hardware solutions."}}, {"heading_title": "SG2042 Optimizing", "details": {"summary": "Based on the paper, optimizing for the Sophon SG2042 involves a multi-faceted approach. **Exploiting the RISC-V vector extensions (RVV)** through custom kernel development is crucial for performance. The paper highlights the importance of choosing the **right compiler toolchain**, with Clang demonstrating superior performance over GCC due to better ISA extension support and advanced optimization passes. **Careful model mapping and thread allocation** are essential to mitigate NUMA effects and memory bottlenecks. Specifically, disabling NUMA balancing and enabling memory interleaving are key strategies to reduce memory page migrations and improve throughput. Ultimately, achieving optimal performance on the SG2042 requires a holistic approach encompassing kernel optimization, compiler selection, and memory management."}}, {"heading_title": "Compiler Impact", "details": {"summary": "The research indicates a significant impact of the compiler choice on LLM inference performance. Specifically, **Clang 19 demonstrates superior performance compared to GCC 13.2**, with average performance gains of 34% for token generation and 25% for prefill in the DeepSeek 8B model when using the proposed kernel. The primary reason is attributed to the **combination of ISA extension support and more advanced compilation passes**, such as more aggressive in-lining and loop unrolling. The study also reveals that using more than 32 threads leads to performance degradation, potentially due to a suboptimal NUMA balancing policy that causes excessive thread and memory page migrations. This highlight the importance of compiler selection and optimization techniques in maximizing the performance of LLM inference on RISC-V platforms and to avoid performance bottleneck."}}, {"heading_title": "NUMA Scaling", "details": {"summary": "NUMA (Non-Uniform Memory Access) scaling is a crucial aspect when optimizing LLM inference on multi-core systems, especially those with complex memory hierarchies. **The key challenge is to minimize the latency associated with accessing memory that is not local to the processor core**, as this can significantly impact performance. The paper explores different `numactl` options, which are essentially policies governing how memory is allocated and accessed across NUMA nodes. These policies include balancing memory access, disabling balancing altogether, binding cores to specific NUMA nodes, and interleaving memory across nodes. **The effectiveness of each policy depends on the workload characteristics and the system architecture**. Disabling NUMA balancing and enabling memory interleaving are found to be particularly effective for LLM inference, likely due to the predictable nature of the workload and the reduction in memory page migrations, resulting in improved token generation and prefill performance. **The optimal NUMA configuration can significantly reduce memory access latency, resulting in considerable speedups for LLM tasks.**"}}, {"heading_title": "Open Source LLMs", "details": {"summary": "The paper highlights the importance of **open-source LLMs** as key drivers for innovation and accessibility in the field. It emphasizes the need for optimization on **RISC-V platforms**, a rapidly emerging architecture with its **open and vendor-neutral ISA**. By focusing on the **Sophon SG2042**, the first commercial many-core RISC-V CPU with vector processing capabilities, the research addresses the limitations of current **software ecosystems** optimized for x86 or ARM architectures. It emphasizes bridging the gap for LLM workloads, indicating the potential for **lower hardware costs** and **enhanced flexibility**. Performance results with DeepSeek R1 Distill Llama 8B and DeepSeek R1 Distill QWEN 14B highlight performance improvements in token generation and prompt processing, making them competitive with incumbent architectures. This work also shows improvements in **efficiency** by leveraging the available ISA extensions."}}]