[{"Alex": "Hey everyone, welcome to the podcast! Today, we're diving into the wild world of Large Language Models, or LLMs, but with a twist. Forget those power-hungry GPUs \u2013 we're talking about running these behemoths on something way more accessible: RISC-V CPUs! We'll be decoding a fascinating research paper that's shaking things up. Joining me is Jamie, who's ready to unravel this tech mystery with me.", "Jamie": "Hey Alex, super excited to be here! LLMs on RISC-V? That sounds like trying to run a marathon on a unicycle. I mean, is it even possible?"}, {"Alex": "That\u2019s the million-dollar question, Jamie! And this paper basically says, 'Yes, but it needs some serious finesse.' The research focuses on optimizing LLM performance on the Sophon SG2042, which is a commercially available many-core RISC-V CPU. Basically, it\u2019s like giving this unicycle rocket boosters and a super-smart AI co-pilot.", "Jamie": "Okay, rocket boosters on a unicycle... I\u2019m picturing it! So, what exactly did they optimize? Like, where did they put those rocket boosters?"}, {"Alex": "They attacked the problem from three main angles: optimizing the core kernels, tweaking the compilation toolchain, and fine-tuning the model mapping. The researchers essentially rewrote some of the key mathematical operations at the heart of LLMs to specifically exploit the architecture of the RISC-V processor, which is why I called it rocket boosters, it is a brand new kernel.", "Jamie": "So, they're not just running existing LLM code, they had to get their hands dirty and recode it? That sounds intense."}, {"Alex": "Exactly! And it\u2019s crucial because the standard software isn't really optimized for RISC-V yet. They looked at things like how data is stored and accessed in memory, and how to best use the processor's vector units \u2013 those are special circuits designed to speed up certain calculations. All tailored to RISC-V.", "Jamie": "Hmm, okay, so custom-built engines for this RISC-V unicycle. What about the compilation toolchain? That sounds like programmer-speak."}, {"Alex": "Think of the compilation toolchain as the translator between the LLM code and the processor's language. They experimented with different compilers, specifically GCC and Clang. And it turned out that Clang gave them a significant performance boost.", "Jamie": "Interesting. Why Clang over GCC? Is it just a better translator in this case?"}, {"Alex": "It boils down to Clang having better support for ISA extensions and more aggressive optimization passes. ISA extensions are like adding new words to the processor's vocabulary, allowing it to perform certain tasks more efficiently. And the more aggressive optimization passes are all about making that code as compact and streamlined as possible. In this case, Clang knew some RISC-V 'slang' that GCC didn't.", "Jamie": "Got it. So, it's like Clang speaks RISC-V fluently while GCC is still learning the ropes. What was the third thing they optimized? Something about model mapping?"}, {"Alex": "Ah, yes. Model mapping is all about how the LLM's data and computations are distributed across the processor's cores and memory. The SG2042 has a Non-Uniform Memory Access (NUMA) architecture. Imagine the processor like a city and its memory like various districts. NUMA means accessing memory in one district might be faster than another, and we needed to know where the data goes to prevent traffic jams.", "Jamie": "Okay, I see. So, if you don't map the model correctly, it's like everyone trying to get to the same district at once. How did they tackle that?"}, {"Alex": "They played around with different NUMA policies using a tool called `numactl`, looking at things like core binding and memory interleaving. Basically, they experimented with different traffic patterns to see what minimized those memory access delays.", "Jamie": "And did they find a winning traffic pattern? What were the results of all this optimization?"}, {"Alex": "They did! They tested their optimizations on three different open-source LLMs: Llama 7B, DeepSeek R1 Distill Llama 8B, and DeepSeek R1 Distill QWEN 14B. The results were pretty impressive. They saw speedups of up to 3x in token generation and 2.8x in prompt processing compared to a baseline implementation.", "Jamie": "Wow, a 3x speedup? That's like turning that unicycle into a decent bicycle! But what do those numbers actually mean in the real world? What's 'token generation' and 'prompt processing'?"}, {"Alex": "Great question, Jamie. Imagine you're asking the LLM a question. Prompt processing, or prefill, is how quickly it understands your question. Token generation is how fast it spits out the answer, one word or fragment\u2014'token'\u2014at a time. So, a 3x speedup means the LLM can understand and answer questions much faster on this RISC-V platform.", "Jamie": "Okay, that makes sense. So, it's not just possible to run LLMs on RISC-V, but with the right optimizations, they can actually perform reasonably well. What kind of throughput are we talking about?"}, {"Alex": "They hit a maximum throughput of 13.07 tokens per second with the Llama 7B model. The two deepseek models performed at 6.54 and 3.68 tok/s. It might not sound like much compared to a high-end GPU, but remember, this is a RISC-V CPU. It's about making LLMs accessible on more affordable and energy-efficient hardware.", "Jamie": "So, it's not about beating GPUs, it's about offering a viable alternative for specific use cases. What are those use cases? Who would benefit from this?"}, {"Alex": "Think on-premise servers, edge computing, and situations where energy efficiency is paramount. Imagine running a smart factory where you need real-time analysis of sensor data using an LLM. Or a local server offering AI-powered customer service in an area with limited bandwidth. RISC-V opens up possibilities where the cost and power consumption of GPUs are prohibitive.", "Jamie": "Okay, I'm seeing the potential. But are there any downsides? What are the limitations of this approach?"}, {"Alex": "The biggest limitation is still raw performance. GPUs are optimized for the kind of massive parallel computations that LLMs require. RISC-V CPUs are catching up, but they're not quite there yet. Plus, the software ecosystem for RISC-V is still developing.", "Jamie": "So, there's still work to be done. What are the next steps for this research? What are they hoping to achieve in the future?"}, {"Alex": "The researchers point to several avenues for future work. One is further optimizing the kernels and exploring even more advanced compilation techniques. Another is to investigate different quantization methods to reduce the memory footprint of the models. There are even more new things to explore now that the platform is proven.", "Jamie": "Quantization? That sounds like something out of Star Trek."}, {"Alex": "Haha, it kind of is! Quantization is about reducing the precision of the numbers used to represent the model's parameters. It's like using rough sketches instead of detailed paintings. This can significantly reduce the memory requirements and speed up computations, but you have to be careful not to lose too much accuracy.", "Jamie": "So, it's a balancing act between performance and accuracy. What about the energy efficiency you mentioned earlier? How does this RISC-V approach compare to GPUs in terms of power consumption?"}, {"Alex": "The paper doesn't go into extensive energy consumption analysis, but they do mention that their RISC-V setup is competitive with CPU-based inference on x86 architectures in terms of energy efficiency. They even saw an improvement of 1.2x compared to one specific x86 setup in terms of tokens per second per watt.", "Jamie": "That's a significant improvement! So, it's not just about cost, it's about being greener too. One thing I am curious about is the vector extensions the paper mentions."}, {"Alex": "Ah yes, those are the key to the kernel. Vector extensions are special instructions added to the RISC-V instruction set that allow the processor to perform the same operation on multiple data points simultaneously. It's like having multiple tiny CPUs working in parallel within a single core.", "Jamie": "So, it's like giving the RISC-V processor a mini-GPU inside. I would imagine this type of work could be directly applied to other RISC-V chips in the future?"}, {"Alex": "That's absolutely right. The insights and techniques developed in this research can be generalized to other RISC-V platforms, especially those with vector extensions. This is about building a software ecosystem for RISC-V that can support a wide range of AI workloads.", "Jamie": "This makes me wonder, could you combine the two? Could the kernels they developed be somehow ran in the GPUs?"}, {"Alex": "That's a great point. The core optimization techniques, like exploiting data locality and optimizing memory access patterns, could be applicable to GPUs as well. However, the specific vectorization strategies would need to be adapted to the GPU's architecture.", "Jamie": "Well, Alex, this has been incredibly insightful. It's amazing to see how much progress is being made in running LLMs on alternative hardware platforms. Thanks for breaking it all down for me!"}, {"Alex": "My pleasure, Jamie! So, the big takeaway is this: running LLMs on RISC-V is not just a pipe dream. With clever optimization, it's a viable and potentially more sustainable alternative for many applications. The paper opens the door for broader innovation, driving AI to places where it wasn't previously feasible. More research will be required to test newer RISC-V chips. Thanks for tuning in, everyone, and we'll catch you next time!", "Jamie": "string"}]