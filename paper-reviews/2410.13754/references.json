{"references": [{" publication_date": "2023", "fullname_first_author": "Josh Achiam", "paper_title": "Gpt-4 technical report", "reason": "This paper is foundational for understanding the capabilities of large language models, like GPT-4, which are crucial for evaluating multi-modal AI models.  The technical report provides detailed information on the model's architecture, training data, and performance across various tasks, providing a benchmark against which other models can be compared.  It is referenced in Section 1 and indirectly relevant to many other sections of the paper.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Rohan Anil", "paper_title": "Gemini: A family of highly capable multimodal models", "reason": "This paper introduces Gemini, a family of multimodal models that can handle various input and output modalities, making it highly relevant to the MixEval-X benchmark which aims to evaluate models across multiple modalities. Understanding Gemini's capabilities and its evaluation methods is crucial to assessing MixEval-X's effectiveness. It is mentioned in Section 1 and has relevance for model comparison in Section 3.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Jinze Bai", "paper_title": "Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond", "reason": "This paper introduces Qwen-VL, a vision-language model that is directly compared against in the MixEval-X benchmark (Section 3).  Its architecture, performance on various tasks, and overall capabilities are relevant to understanding the relative strengths and weaknesses of different multimodal models being evaluated by MixEval-X.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Yatong Bai", "paper_title": "Consistencytta: Accelerating diffusion-based text-to-audio generation with consistency distillation", "reason": "This paper details a method for enhancing diffusion-based text-to-audio generation and is directly related to MixEval-X's evaluation of MMG tasks, especially Text2Audio (Section 3).  Understanding the effectiveness and limitations of this method is crucial to interpreting the results and understanding the current state-of-the-art in audio generation.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "James Betker", "paper_title": "Improving image generation with better captions", "reason": "This paper presents DALL-E 3, a prominent image generation model directly relevant to MixEval-X's evaluation, particularly Text2Image (Section 3).  The model's architecture, performance, and training data are crucial to understanding the state-of-the-art in image generation. The evaluation methods used in this paper inform the selection of appropriate evaluation methods for similar tasks in MixEval-X.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Josh Achiam", "paper_title": "Gpt-4 technical report", "reason": "This paper is foundational for understanding the capabilities of large language models, like GPT-4, which are crucial for evaluating multi-modal AI models. The technical report provides detailed information on the model's architecture, training data, and performance across various tasks, providing a benchmark against which other models can be compared.  It is referenced in Section 1 and indirectly relevant to many other sections of the paper.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Haoxin Chen", "paper_title": "Videocrafter2: Overcoming data limitations for high-quality video diffusion models", "reason": "This paper introduces VideoCrafter2, a video generation model directly relevant to MixEval-X's evaluation of MMG tasks, particularly Text2Video (Section 3). The model's capabilities and the challenges in generating high-quality videos are important for interpreting the results obtained from MixEval-X.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Wei-Lin Chiang", "paper_title": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality", "reason": "This paper introduces Vicuna, a large language model used in the MixEval-X pipeline (Section 2.1) for web query detection.  Its performance and characteristics are critical for understanding the quality and distribution of the queries used to build the MixEval-X benchmark.  The accuracy and efficiency of the query detection pipeline significantly impacts the overall quality of MixEval-X.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Wei-Lin Chiang", "paper_title": "Chatbot arena: An open platform for evaluating llms by human preference", "reason": "This paper introduces Chatbot Arena, a platform used in Section 4.2 for comparison with MixEval-X.  The strong correlation (0.96) that MixEval achieved with Chatbot Arena rankings in prior work emphasizes the importance of human preference in evaluating large language models. This is directly relevant to MixEval-X's meta-evaluation, highlighting the importance of human judgment in validating the benchmark's effectiveness.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Yunfei Chu", "paper_title": "Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models", "reason": "This paper introduces Qwen-Audio, an audio-language model directly compared in MixEval-X's Audio2Text tasks (Section 3).  Understanding Qwen-Audio's capabilities and performance helps assess MixEval-X's effectiveness in evaluating audio understanding and generation tasks.  The model's strengths and weaknesses are essential contextual information when assessing MixEval-X's overall performance.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Yunfei Chu", "paper_title": "Qwen2-audio technical report", "reason": "This paper details Qwen2-Audio, another prominent audio-language model, crucial for comparing against in MixEval-X's evaluation of Audio2Text tasks (Section 3).  Its detailed description of architecture, training data, and performance metrics is invaluable for understanding the current state-of-the-art and for interpreting MixEval-X's results.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Together Computer", "paper_title": "Redpajama: an open dataset for training large language models", "reason": "This paper introduces RedPajama, a large language model dataset used in MixEval-X's pipeline (Section 2.1) for web query detection. This dataset's size, quality, and characteristics are vital for understanding the scale and scope of the data used to build the MixEval-X benchmark. The dataset's coverage and limitations influence the overall quality and representativeness of MixEval-X's evaluation.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Wenliang Dai", "paper_title": "Instructblip: Towards general-purpose vision-language models with instruction tuning", "reason": "This paper introduces InstructBLIP, a vision-language model relevant to MixEval-X's evaluation of Image2Text and Image2Action tasks (Section 3).  Understanding its performance in comparison to other models helps assess MixEval-X's effectiveness and the relative strengths and weaknesses of various models in vision-language tasks.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Soham Deshmukh", "paper_title": "Pengi: An audio language model for audio tasks", "reason": "This paper presents Pengi, an audio language model directly compared in MixEval-X's Audio2Text tasks (Section 3).  Pengi's architecture, performance, and training data are crucial for analyzing the effectiveness of MixEval-X in evaluating audio-related tasks. Its performance helps gauge the capabilities and limitations of current audio-language models, providing essential context for interpreting MixEval-X's results.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Xiaoyi Dong", "paper_title": "Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model", "reason": "This paper introduces InternLM-XComposer2, a vision-language model directly compared against in MixEval-X's evaluation of Image2Text and Image2Action tasks (Section 3). The model's architecture, performance, and training data are important for understanding the relative strengths and weaknesses of different multimodal models being evaluated by MixEval-X.", "section_number": 3}, {" publication_date": "2022", "fullname_first_author": "Haytham M Fayek", "paper_title": "Temporal reasoning via audio question answering", "reason": "This paper focuses on temporal reasoning in audio question answering, a task relevant to MixEval-X's Audio2Text tasks (Section 3). Understanding the challenges and current approaches in this area is essential for evaluating MixEval-X's effectiveness in handling temporal aspects within audio data. The paper highlights the need for advanced techniques to address the complexities of temporal reasoning in audio, which informs the design and evaluation of similar tasks in MixEval-X.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Peng Gao", "paper_title": "Llama-adapter v2: Parameter-efficient visual instruction model", "reason": "This paper introduces Llama-Adapter V2, a parameter-efficient visual instruction model relevant to MixEval-X's evaluation of Image2Text and Image2Action tasks (Section 3). This model's ability to adapt to visual instructions is significant in evaluating multi-modal models.  Understanding the performance of this model is crucial to evaluate the effectiveness of MixEval-X in assessing the adaptation abilities of multimodal models.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Tianrui Guan", "paper_title": "Hallusionbench: an advanced diagnostic suite for entangled language hallucination and visual illusion in large vision-language models", "reason": "This paper introduces HallucinationBench, a benchmark for evaluating vision-language models' propensity for hallucinations. This is relevant to MixEval-X as it highlights the importance of evaluating robustness and reliability in multimodal models, which are key concerns addressed by MixEval-X's design (Section 2) and evaluation (Section 3). By understanding the limitations of existing models in avoiding hallucinations, MixEval-X can better design its tasks to assess the robustness of models across various modalities.", "section_number": 3}, {" publication_date": "2018", "fullname_first_author": "Danna Gurari", "paper_title": "Vizwiz grand challenge: Answering visual questions from blind people", "reason": "This paper introduces VizWiz, a benchmark dataset for visual question answering tailored to visually impaired users, which is relevant to MixEval-X's Image2Text and Image2Action tasks (Section 3). Its focus on accessibility and real-world applicability aligns with MixEval-X's goal of providing a comprehensive and practical benchmark for multi-modal AI.  Understanding the challenges and best practices in designing accessible benchmarks for visual tasks informs MixEval-X's design and evaluation methodology.", "section_number": 3}]}