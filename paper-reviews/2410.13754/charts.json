[{"figure_path": "2410.13754/charts/charts_7_0.png", "caption": "Figure 6: The overall Elo scores of MMG models on the MixEval-X MMG subsets, with error bars representing the 95% confidence intervals for the ratings. These scores are derived using the Bradley-Terry model, based on crowd-sourced user preferences. Additionally, the number of human evaluators per subset is provided for reference. The turn-level scores are shown in Figure 46.", "description": "The chart displays the overall Elo scores of various multi-modal generation (MMG) models across three different tasks (Text2Image, Text2Video, Text2Audio) based on human evaluations, showing the model rankings and confidence intervals.", "section": "3.3 MMG Tasks"}, {"figure_path": "2410.13754/charts/charts_7_1.png", "caption": "Figure 6: The overall Elo scores of MMG models on the MixEval-X MMG subsets, with error bars representing the 95% confidence intervals for the ratings. These scores are derived using the Bradley-Terry model, based on crowd-sourced user preferences. Additionally, the number of human evaluators per subset is provided for reference. The turn-level scores are shown in Figure 46.", "description": "The chart displays the overall Elo scores for multi-modal generation (MMG) models across three different tasks (Text2Image, Text2Video, Text2Audio), indicating model performance based on crowd-sourced evaluations.", "section": "3.3 MMG Tasks"}, {"figure_path": "2410.13754/charts/charts_8_0.png", "caption": "Figure 6: The overall Elo scores of MMG models on the MixEval-X MMG subsets, with error bars representing the 95% confidence intervals for the ratings. These scores are derived using the Bradley-Terry model, based on crowd-sourced user preferences. Additionally, the number of human evaluators per subset is provided for reference. The turn-level scores are shown in Figure 46.", "description": "The chart displays the Elo scores of multi-modal generation (MMG) models across three different MixEval-X subsets (Text2Image, Text2Video, Text2Audio), showing model rankings based on human evaluations.", "section": "3.3 MMG Tasks"}, {"figure_path": "2410.13754/charts/charts_8_1.png", "caption": "Figure 6: The overall Elo scores of MMG models on the MixEval-X MMG subsets, with error bars representing the 95% confidence intervals for the ratings. These scores are derived using the Bradley-Terry model, based on crowd-sourced user preferences. Additionally, the number of human evaluators per subset is provided for reference. The turn-level scores are shown in Figure 46.", "description": "The chart displays the overall Elo scores of various multi-modal generation (MMG) models across three MixEval-X subsets (Text2Image, Text2Video, Text2Audio), showing the model rankings based on crowd-sourced user preferences and the confidence intervals.", "section": "3.3 MMG Tasks"}, {"figure_path": "2410.13754/charts/charts_10_0.png", "caption": "Figure 10: Model judge scores and crowd-sourcing Elo scores of the Text2Image subset. The upper and lower error bars represent the 1st and 2nd turn scores, respectively. Each data point is an average of five different runs.", "description": "The chart displays a comparison of model judge scores and crowd-sourced Elo scores for Text2Image tasks, showing the correlation between automated evaluation and human preference.", "section": "4.2 Correlation Analysis"}, {"figure_path": "2410.13754/charts/charts_56_0.png", "caption": "Figure 40: Image2Text benchmark pool distribution on benchmark level.", "description": "The bar chart displays the distribution of benchmark tasks within the Image2Text benchmark pool, categorized by benchmark and task type (free-form or multiple-choice).", "section": "F BENCHMARK POOL DETAILS"}, {"figure_path": "2410.13754/charts/charts_56_1.png", "caption": "Figure 9: Task distribution of various modality benchmarks, with each modality uniquely color-coded. Benchmark data points (orange dots) are plotted against the detected web queries (blue dots) for the corresponding modality. The sentence embeddings of the queries were dimensionally reduced into a unified 2D space, enabling direct comparison of topic distributions across benchmarks. Benchmarks are sorted by their cluster distance (C-Dist) from the corresponding web queries.", "description": "This chart visualizes the similarity between the task distributions of various multi-modal benchmarks and real-world web queries, showing how closely each benchmark aligns with real-world tasks.", "section": "4 Meta Evaluation"}, {"figure_path": "2410.13754/charts/charts_57_0.png", "caption": "Figure 9: Task distribution of various modality benchmarks, with each modality uniquely color-coded. Benchmark data points (orange dots) are plotted against the detected web queries (blue dots) for the corresponding modality. The sentence embeddings of the queries were dimensionally reduced into a unified 2D space, enabling direct comparison of topic distributions across benchmarks. Benchmarks are sorted by their cluster distance (C-Dist) from the corresponding web queries.", "description": "The chart visualizes the distribution similarity between various benchmark datasets and real-world web queries across eight different multi-modal categories, revealing MixEval-X's close alignment with real-world task distributions.", "section": "4 Meta Evaluation"}, {"figure_path": "2410.13754/charts/charts_57_1.png", "caption": "Figure 9: Task distribution of various modality benchmarks, with each modality uniquely color-coded. Benchmark data points (orange dots) are plotted against the detected web queries (blue dots) for the corresponding modality. The sentence embeddings of the queries were dimensionally reduced into a unified 2D space, enabling direct comparison of topic distributions across benchmarks. Benchmarks are sorted by their cluster distance (C-Dist) from the corresponding web queries.", "description": "This chart visualizes the distribution of various benchmark tasks and their proximity to real-world web queries across different modalities (Image2Text, Video2Text, Audio2Text, etc.).", "section": "4 Meta Evaluation"}, {"figure_path": "2410.13754/charts/charts_58_0.png", "caption": "Figure 44: Audio2Text benchmark pool distribution on benchmark level.", "description": "The bar chart displays the distribution of the Audio2Text benchmark pool across different benchmark datasets, showing the proportion of tasks from each dataset.", "section": "3.2 MMU Tasks"}, {"figure_path": "2410.13754/charts/charts_58_1.png", "caption": "Figure 44: Audio2Text benchmark pool distribution on benchmark level.", "description": "The chart displays the distribution of the Audio2Text benchmark pool at the benchmark level, showing the proportion of tasks from each benchmark.", "section": "F BENCHMARK POOL DETAILS"}, {"figure_path": "2410.13754/charts/charts_59_0.png", "caption": "Figure 6: The overall Elo scores of MMG models on the MixEval-X MMG subsets, with error bars representing the 95% confidence intervals for the ratings. These scores are derived using the Bradley-Terry model, based on crowd-sourced user preferences. Additionally, the number of human evaluators per subset is provided for reference. The turn-level scores are shown in Figure 46.", "description": "The chart displays the Elo scores of multi-modal generation (MMG) models across three MixEval-X subsets (Text2Image, Text2Video, Text2Audio), showing model performance based on crowd-sourced evaluations.", "section": "3.3 MMG Tasks"}]