[{"figure_path": "2410.13754/figures/figures_3_0.png", "caption": "Figure 2: The overall pipeline for creating MixEval-X.", "description": "The figure illustrates the pipeline used to create the MixEval-X benchmark, showing the steps involved in gathering web queries, creating benchmark mixtures, and performing adaptation and rectification to construct the final benchmark.", "section": "2 MIXEVAL-X"}, {"figure_path": "2410.13754/figures/figures_20_0.png", "caption": "Figure 1: MixEval-Xencompasses eight input-output modality combinations and can be further extended. Its data points reflect real-world task distributions. The last grid presents the scores of frontier organizations' flagship models on MixEval-X, normalized to a 0-100 scale, with MMG tasks using win rates instead of Elo. Section C presents example data samples and model responses.", "description": "MixEval-X is a benchmark that encompasses eight input-output modality combinations, shows real-world task distributions, and presents scores of frontier organizations' flagship models.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.13754/figures/figures_21_0.png", "caption": "Figure 1: MixEval-Xencompasses eight input-output modality combinations and can be further extended. Its data points reflect real-world task distributions. The last grid presents the scores of frontier organizations' flagship models on MixEval-X, normalized to a 0-100 scale, with MMG tasks using win rates instead of Elo. Section C presents example data samples and model responses.", "description": "Figure 1 shows MixEval-X, a multi-modal benchmark encompassing eight input-output modality combinations, along with example tasks and the performance of leading AI models.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.13754/figures/figures_21_1.png", "caption": "Figure 1: MixEval-Xencompasses eight input-output modality combinations and can be further extended. Its data points reflect real-world task distributions. The last grid presents the scores of frontier organizations' flagship models on MixEval-X, normalized to a 0-100 scale, with MMG tasks using win rates instead of Elo. Section C presents example data samples and model responses.", "description": "MixEval-X is a benchmark that encompasses eight input-output modality combinations, showing frontier organization model scores, normalized to 0-100, reflecting real-world task distributions.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.13754/figures/figures_21_2.png", "caption": "Figure 1: MixEval-Xencompasses eight input-output modality combinations and can be further extended. Its data points reflect real-world task distributions. The last grid presents the scores of frontier organizations' flagship models on MixEval-X, normalized to a 0-100 scale, with MMG tasks using win rates instead of Elo. Section C presents example data samples and model responses.", "description": "MixEval-X is a benchmark that evaluates eight different input-output modality combinations, showing scores from various organizations' models on real-world data.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.13754/figures/figures_22_0.png", "caption": "Figure 1: MixEval-Xencompasses eight input-output modality combinations and can be further extended. Its data points reflect real-world task distributions. The last grid presents the scores of frontier organizations' flagship models on MixEval-X, normalized to a 0-100 scale, with MMG tasks using win rates instead of Elo. Section C presents example data samples and model responses.", "description": "MixEval-X is a benchmark that encompasses eight input-output modality combinations, showing the scores of frontier organizations' flagship models across various tasks.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.13754/figures/figures_22_1.png", "caption": "Figure 1: MixEval-Xencompasses eight input-output modality combinations and can be further extended. Its data points reflect real-world task distributions. The last grid presents the scores of frontier organizations' flagship models on MixEval-X, normalized to a 0-100 scale, with MMG tasks using win rates instead of Elo. Section C presents example data samples and model responses.", "description": "MixEval-X is a benchmark that encompasses eight input-output modality combinations, showing real-world task distributions and scores from frontier organizations.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.13754/figures/figures_23_0.png", "caption": "Figure 1: MixEval-Xencompasses eight input-output modality combinations and can be further extended. Its data points reflect real-world task distributions. The last grid presents the scores of frontier organizations' flagship models on MixEval-X, normalized to a 0-100 scale, with MMG tasks using win rates instead of Elo. Section C presents example data samples and model responses.", "description": "MixEval-X is a benchmark that encompasses eight input-output modality combinations, showing real-world task distributions and scores from frontier organizations.", "section": "INTRODUCTION"}, {"figure_path": "2410.13754/figures/figures_23_1.png", "caption": "Figure 1: MixEval-Xencompasses eight input-output modality combinations and can be further extended. Its data points reflect real-world task distributions. The last grid presents the scores of frontier organizations' flagship models on MixEval-X, normalized to a 0-100 scale, with MMG tasks using win rates instead of Elo. Section C presents example data samples and model responses.", "description": "MixEval-X is a benchmark encompassing eight input-output modality combinations, showing real-world task distributions and the scores of several frontier organizations' models.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.13754/figures/figures_23_2.png", "caption": "Figure 1: MixEval-Xencompasses eight input-output modality combinations and can be further extended. Its data points reflect real-world task distributions. The last grid presents the scores of frontier organizations' flagship models on MixEval-X, normalized to a 0-100 scale, with MMG tasks using win rates instead of Elo. Section C presents example data samples and model responses.", "description": "MixEval-X is a benchmark that encompasses eight input-output modality combinations, showing real-world task distributions and model scores from frontier organizations.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.13754/figures/figures_24_0.png", "caption": "Figure 1: MixEval-Xencompasses eight input-output modality combinations and can be further extended. Its data points reflect real-world task distributions. The last grid presents the scores of frontier organizations' flagship models on MixEval-X, normalized to a 0-100 scale, with MMG tasks using win rates instead of Elo. Section C presents example data samples and model responses.", "description": "MixEval-X is a benchmark encompassing eight input-output modality combinations, showing real-world task distributions and model performance scores from several frontier organizations.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.13754/figures/figures_24_1.png", "caption": "Figure 9: Task distribution of various modality benchmarks, with each modality uniquely color-coded. Benchmark data points (orange dots) are plotted against the detected web queries (blue dots) for the corresponding modality. The sentence embeddings of the queries were dimensionally reduced into a unified 2D space, enabling direct comparison of topic distributions across benchmarks. Benchmarks are sorted by their cluster distance (C-Dist) from the corresponding web queries.", "description": "The figure visualizes the distributions of various benchmark datasets and real-world web queries across different modalities using t-SNE for dimensionality reduction, showing MixEval-X's alignment with real-world task distributions.", "section": "4 Meta Evaluation"}, {"figure_path": "2410.13754/figures/figures_24_2.png", "caption": "Figure 9: Task distribution of various modality benchmarks, with each modality uniquely color-coded. Benchmark data points (orange dots) are plotted against the detected web queries (blue dots) for the corresponding modality. The sentence embeddings of the queries were dimensionally reduced into a unified 2D space, enabling direct comparison of topic distributions across benchmarks. Benchmarks are sorted by their cluster distance (C-Dist) from the corresponding web queries.", "description": "This figure shows the comparison of the task distributions between various existing benchmarks and the distribution of real-world user queries, using t-SNE to visualize 2D embeddings of sentence embeddings of the queries.", "section": "4 META EVALUATION"}, {"figure_path": "2410.13754/figures/figures_25_0.png", "caption": "Figure 9: Task distribution of various modality benchmarks, with each modality uniquely color-coded. Benchmark data points (orange dots) are plotted against the detected web queries (blue dots) for the corresponding modality. The sentence embeddings of the queries were dimensionally reduced into a unified 2D space, enabling direct comparison of topic distributions across benchmarks. Benchmarks are sorted by their cluster distance (C-Dist) from the corresponding web queries.", "description": "This figure visualizes the distribution of various benchmark tasks and compares them to real-world web queries across eight different input/output modalities, showing the alignment of MixEval-X with real-world task distributions.", "section": "Meta Evaluation"}, {"figure_path": "2410.13754/figures/figures_25_1.png", "caption": "Figure 9: Task distribution of various modality benchmarks, with each modality uniquely color-coded. Benchmark data points (orange dots) are plotted against the detected web queries (blue dots) for the corresponding modality. The sentence embeddings of the queries were dimensionally reduced into a unified 2D space, enabling direct comparison of topic distributions across benchmarks. Benchmarks are sorted by their cluster distance (C-Dist) from the corresponding web queries.", "description": "This figure visualizes the distributions of various modality benchmarks and compares them with real-world task distributions using t-SNE.", "section": "4 META EVALUATION"}, {"figure_path": "2410.13754/figures/figures_25_2.png", "caption": "Figure 9: Task distribution of various modality benchmarks, with each modality uniquely color-coded. Benchmark data points (orange dots) are plotted against the detected web queries (blue dots) for the corresponding modality. The sentence embeddings of the queries were dimensionally reduced into a unified 2D space, enabling direct comparison of topic distributions across benchmarks. Benchmarks are sorted by their cluster distance (C-Dist) from the corresponding web queries.", "description": "The figure visualizes the distribution of various benchmark datasets and compares them to the distribution of real-world web queries for multiple modalities.", "section": "4 META EVALUATION"}, {"figure_path": "2410.13754/figures/figures_25_3.png", "caption": "Figure 1: MixEval-Xencompasses eight input-output modality combinations and can be further extended. Its data points reflect real-world task distributions. The last grid presents the scores of frontier organizations' flagship models on MixEval-X, normalized to a 0-100 scale, with MMG tasks using win rates instead of Elo. Section C presents example data samples and model responses.", "description": "MixEval-X is a multimodal benchmark that evaluates eight input-output modality combinations, showing the scores of different models from frontier organizations.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.13754/figures/figures_26_0.png", "caption": "Figure 6: The overall Elo scores of MMG models on the MixEval-X MMG subsets, with error bars representing the 95% confidence intervals for the ratings. These scores are derived using the Bradley-Terry model, based on crowd-sourced user preferences. Additionally, the number of human evaluators per subset is provided for reference. The turn-level scores are shown in Figure 46.", "description": "The figure shows the Elo scores of different MMG models across three subsets (Text2Image, Text2Video, Text2Audio) of MixEval-X, based on human evaluations.", "section": "3.3 MMG Tasks"}, {"figure_path": "2410.13754/figures/figures_26_1.png", "caption": "Figure 6: The overall Elo scores of MMG models on the MixEval-X MMG subsets, with error bars representing the 95% confidence intervals for the ratings. These scores are derived using the Bradley-Terry model, based on crowd-sourced user preferences. Additionally, the number of human evaluators per subset is provided for reference. The turn-level scores are shown in Figure 46.", "description": "The figure shows the overall Elo scores of different MMG models across three subsets (Text2Image, Text2Video, Text2Audio) based on crowd-sourced human evaluations.", "section": "3.3 MMG Tasks"}, {"figure_path": "2410.13754/figures/figures_26_2.png", "caption": "Figure 1: MixEval-Xencompasses eight input-output modality combinations and can be further extended. Its data points reflect real-world task distributions. The last grid presents the scores of frontier organizations' flagship models on MixEval-X, normalized to a 0-100 scale, with MMG tasks using win rates instead of Elo. Section C presents example data samples and model responses.", "description": "The figure shows an overview of MixEval-X, illustrating eight input-output modality combinations, their real-world task distributions, and the performance of various frontier organizations' models.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.13754/figures/figures_26_3.png", "caption": "Figure 6: The overall Elo scores of MMG models on the MixEval-X MMG subsets, with error bars representing the 95% confidence intervals for the ratings. These scores are derived using the Bradley-Terry model, based on crowd-sourced user preferences. Additionally, the number of human evaluators per subset is provided for reference. The turn-level scores are shown in Figure 46.", "description": "The figure shows the overall Elo scores of different MMG models across three subsets (Text2Image, Text2Video, and Text2Audio) based on crowd-sourced human evaluations.", "section": "3.3 MMG Tasks"}, {"figure_path": "2410.13754/figures/figures_26_4.png", "caption": "Figure 1: MixEval-Xencompasses eight input-output modality combinations and can be further extended. Its data points reflect real-world task distributions. The last grid presents the scores of frontier organizations' flagship models on MixEval-X, normalized to a 0-100 scale, with MMG tasks using win rates instead of Elo. Section C presents example data samples and model responses.", "description": "MixEval-X is a benchmark that evaluates eight input-output modality combinations and shows the performance of flagship models from frontier organizations.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.13754/figures/figures_26_5.png", "caption": "Figure 1: MixEval-Xencompasses eight input-output modality combinations and can be further extended. Its data points reflect real-world task distributions. The last grid presents the scores of frontier organizations' flagship models on MixEval-X, normalized to a 0-100 scale, with MMG tasks using win rates instead of Elo. Section C presents example data samples and model responses.", "description": "MixEval-X is a benchmark encompassing eight input-output modality combinations, showing real-world task distributions and model performance scores from several organizations.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.13754/figures/figures_27_0.png", "caption": "Figure 1: MixEval-Xencompasses eight input-output modality combinations and can be further extended. Its data points reflect real-world task distributions. The last grid presents the scores of frontier organizations' flagship models on MixEval-X, normalized to a 0-100 scale, with MMG tasks using win rates instead of Elo. Section C presents example data samples and model responses.", "description": "MixEval-X is a benchmark encompassing eight input-output modality combinations, showing real-world task distributions and the performance of frontier organization models.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.13754/figures/figures_27_1.png", "caption": "Figure 1: MixEval-Xencompasses eight input-output modality combinations and can be further extended. Its data points reflect real-world task distributions. The last grid presents the scores of frontier organizations' flagship models on MixEval-X, normalized to a 0-100 scale, with MMG tasks using win rates instead of Elo. Section C presents example data samples and model responses.", "description": "MixEval-X is a benchmark that encompasses eight input-output modality combinations, showing frontier organization model scores, and example data samples.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.13754/figures/figures_27_2.png", "caption": "Figure 1: MixEval-Xencompasses eight input-output modality combinations and can be further extended. Its data points reflect real-world task distributions. The last grid presents the scores of frontier organizations' flagship models on MixEval-X, normalized to a 0-100 scale, with MMG tasks using win rates instead of Elo. Section C presents example data samples and model responses.", "description": "MixEval-X is a benchmark encompassing eight input-output modality combinations, showing real-world task distributions and model performance from leading organizations.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.13754/figures/figures_27_3.png", "caption": "Figure 1: MixEval-X encompasses eight input-output modality combinations and can be further extended. Its data points reflect real-world task distributions. The last grid presents the scores of frontier organizations' flagship models on MixEval-X, normalized to a 0-100 scale, with MMG tasks using win rates instead of Elo. Section C presents example data samples and model responses.", "description": "Figure 1 shows an overview of MixEval-X, its eight input-output modality combinations, real-world task distributions, and the performance of various frontier organizations' flagship models.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.13754/figures/figures_27_4.png", "caption": "Figure 1: MixEval-Xencompasses eight input-output modality combinations and can be further extended. Its data points reflect real-world task distributions. The last grid presents the scores of frontier organizations' flagship models on MixEval-X, normalized to a 0-100 scale, with MMG tasks using win rates instead of Elo. Section C presents example data samples and model responses.", "description": "Figure 1 shows an overview of MixEval-X, including its eight input-output modality combinations, real-world task distributions, and the performance of various organizations' models.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.13754/figures/figures_27_5.png", "caption": "Figure 1: MixEval-Xencompasses eight input-output modality combinations and can be further extended. Its data points reflect real-world task distributions. The last grid presents the scores of frontier organizations' flagship models on MixEval-X, normalized to a 0-100 scale, with MMG tasks using win rates instead of Elo. Section C presents example data samples and model responses.", "description": "MixEval-X is a multi-modal benchmark with eight input-output modality combinations, showing real-world task distributions and the performance of flagship models from leading AI organizations.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.13754/figures/figures_28_0.png", "caption": "Figure 8: The evaluation results of prominent models on Image2Action. See Section G for details.", "description": "The figure shows the performance of various models on the Image2Action task of MixEval-X, highlighting the relative strengths and weaknesses of different models.", "section": "3.2 MMU Tasks"}]