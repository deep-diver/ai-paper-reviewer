[{"figure_path": "https://arxiv.org/html/2411.17176/x1.png", "caption": "Figure 1: Illustration of tedious steps in T2I. Our method can select an appropriate model with suitable prompts and arguments. Note: Arle is a character from the game Genshin Impact.", "description": "The figure illustrates the multiple steps involved in text-to-image (T2I) generation using existing methods.  It highlights three main stages: crafting effective prompts, selecting suitable models, and configuring optimal generation arguments. The complexity of each step presents a significant challenge for users.  In contrast, the authors' proposed method automates these steps, allowing for straightforward image generation through simple freestyle chatting.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2411.17176/x2.png", "caption": "Figure 2: Illustration of the data collection pipeline.", "description": "This figure illustrates the multi-step process of collecting data for the ChatGenBench benchmark.  It begins with collecting high-quality image generation demonstrations from the Civitai community, filtering them for quality and relevance, and removing duplicates or NSFW content.  Then, an LLM-driven role-play method is used to simulate diverse freestyle chatting inputs based on the demonstrations.  The role-play involves prompting LLMs with different personas (e.g., doctor, student, etc.) to rephrase the high-quality prompts into more casual and conversational chatting inputs.  The resulting data includes multiple input types, such as single-turn, multi-turn, multimodal (image + text), and historical. Finally, these data are paired with the original prompt, model, and arguments to form the final step-by-step dataset for evaluation. ", "section": "3.1 ChatGenBench: Benchmarking Automatic T2I"}, {"figure_path": "https://arxiv.org/html/2411.17176/x3.png", "caption": "Figure 3: Illustration of the framework for ChatGen-Base and ChatGen-Evo.", "description": "This figure illustrates the architectures of two models: ChatGen-Base and ChatGen-Evo.  ChatGen-Base uses a single-stage supervised fine-tuning (SFT) approach, directly mapping freestyle inputs to the final outputs (prompt, model, and argument). In contrast, ChatGen-Evo employs a three-stage evolution strategy.  Stage 1 uses SFT to train the model for prompt generation from freestyle input. Stage 2 introduces ModelTokens for efficient model selection, and Stage 3 leverages in-context learning for argument configuration. The figure visually depicts the distinct steps and information flow in each model, highlighting the multi-stage nature of ChatGen-Evo.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2411.17176/x4.png", "caption": "Figure 4: User study results of ChatGen-Base and ChatGen-Evo.", "description": "This figure presents the results of a user study comparing the performance of ChatGen-Base and ChatGen-Evo.  The study involved pairwise comparisons, where users were shown two images generated from the same input: one by ChatGen-Base and one by ChatGen-Evo.  Users were asked to choose the image that better matched the image quality and relevance to the given input. The graph displays the win rates of each model in both supervised and few-shot settings, indicating which model was selected more frequently as superior in terms of quality and relevance to the input.", "section": "4.2 Human Evaluation"}, {"figure_path": "https://arxiv.org/html/2411.17176/x5.png", "caption": "Figure 5: Examples of images generated by different methods. Three rows represent single, multi-modal and historical inputs, respectively.", "description": "Figure 5 showcases example images generated using three different methods: ChatGen-Evo, ChatGen-Base, and a default Stable Diffusion model.  The figure is divided into three rows, each illustrating image generation results from different input modalities.  The first row demonstrates single-input generation (text-only prompt), the second row shows multi-modal input generation (text and image prompt), and the third row presents results from historical input (multiple turns in a chat-like conversation), highlighting the models' capabilities in handling various input types and complexities.", "section": "4.4 Visualizations"}, {"figure_path": "https://arxiv.org/html/2411.17176/x6.png", "caption": "Figure 6: Examples of single inputs with step-wise outputs.", "description": "This figure showcases five examples of single freestyle text inputs processed by the ChatGen-Evo model. For each input, the figure displays the automatically generated prompt, selected model, and configured arguments, along with the resulting image. This step-by-step breakdown visually demonstrates the model's ability to automate the entire text-to-image generation process, from understanding freestyle user requests to producing the final output image.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2411.17176/x7.png", "caption": "Figure 7: Examples of multimodal inputs with step-wise outputs. The image in the top-right corner represents the input reference image.", "description": "This figure showcases examples of multimodal inputs processed by the ChatGen-Evo model.  Each example demonstrates the model's ability to generate images based on a combination of text and a reference image (shown in the top right corner of each example).  The step-wise outputs (prompt, model, and arguments) generated by the model for each example are also displayed, illustrating the model's multi-stage reasoning process in generating the final image.  The multimodal inputs include various scenarios:  matching image details, replicating styles from provided images, and combining different styles.  These examples highlight the model's capability of handling diverse and complex inputs to produce high-quality, contextually relevant outputs.", "section": "4.4 Visualizations"}, {"figure_path": "https://arxiv.org/html/2411.17176/x8.png", "caption": "Figure 8: Examples of images generated by ChatGen-Evo and DALL-E 3. Three rows represent single, multi-modal and historical inputs, respectively.", "description": "This figure displays a comparison of images generated by ChatGen-Evo and DALL-E 3, showcasing the performance of both models across various input types.  The first row shows results from single-text inputs where users describe their desired image using natural language. The second row demonstrates results from multi-modal inputs, where users provide both a text description and a reference image to guide the generation process. Finally, the third row illustrates the capabilities of both models when provided with historical dialogue inputs, where the user and the model engage in a conversational exchange to refine the image generation request over multiple rounds. This figure highlights the ability of ChatGen-Evo to handle diverse input formats and generate high-quality images that match user intent.", "section": "4.4 Visualizations"}]