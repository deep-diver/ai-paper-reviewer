[{"content": "| Model | TrainSet (# Numbers) | TestSet (# Numbers) | Based Models? | Step-wise Evaluation? | Freestyle User-input? | Multi-modal Support? | History Support? |\n|---|---|---|---|---|---|---|---| \n| BeautifulPrompt [4] | 143K | 2K | Single | \u2717 | \u2713 | \u2717 | \u2717 |\n| DiffChat [38] | 234K | 5K | Single | \u2717 | \u2713 | \u2717 | \u2713 |\n| DiffusionGPT [28] | - | - | Multi(\u224820) | \u2717 | \u2713 | \u2717 | \u2717 |\n| DABench [49] | 40K | 5K | Multi(5K) | \u2717 | \u2717 | \u2717 | \u2717 |\n| **ChatGenBench** | 256K | 14K | Multi(6K) | \u2713 | \u2713 | \u2713 | \u2713 |", "caption": "Table 1: \nComparison of different methods for benchmarking automatic text-to-image generation.", "description": "This table compares various methods used for benchmarking automatic text-to-image generation.  It contrasts approaches on several key aspects: the size of their training and testing datasets, whether they utilize single or multiple models, whether they perform step-wise evaluation to analyze the individual stages of prompt generation, model selection, and argument configuration, whether they support freestyle user input (as opposed to structured prompts), their capability to handle multi-modal inputs (like text and images), and finally whether they consider historical data in the input sequences.  This allows for a comprehensive comparison of the strengths and weaknesses of different benchmarking strategies.", "section": "3.1 ChatGenBench: Benchmarking Automatic T2I"}, {"content": "| MLLM | Numbers |\n|---|---| \n| gpt-4o-2024-08-06 | \u2248100,000 |\n| gpt-4o-mini-2024-07-18 | \u224870,000 |\n| gpt-4-turbo-2024-04-09 | \u224870,000 |\n| claude-3-5-sonnet-20240620 | \u2248100,000 |", "caption": "Table 2: Overview of MLLMs utilized in chatting generation. The table lists different MLLMs on the left, with corresponding approximate numbers of generated chatting queries on the right.", "description": "This table details the large language models (LLMs) used in the ChatGen system for generating chatting queries.  The left column lists the specific LLMs employed, while the right column provides an estimate of the number of chatting queries generated using each respective LLM. This information is crucial for understanding the scale and diversity of the training data used in the study.", "section": "3.1 ChatGenBench: Benchmarking Automatic T2I"}, {"content": "| Dataset | Total | Single | M-Modal | History |\n|---|---|---|---|---|\n| TrainSet | 256,606 | 147,888 | 69,548 | 39,170 |\n| TestSet Init | 74,364 | 42,838 | 20,214 | 11,312 |\n| Benchmark | 14,564 | 11,011 | 1,668 | 1,132 |\n| Supervised | 10,240 | 8,009 | 1,099 | 1,132 |\n| Few-Shot | 4,324 | 3,002 | 569 | 753 |", "caption": "Table 3: Summary of dataset statistics.", "description": "This table presents a summary of the statistics for the ChatGenBench dataset, broken down by input type (single, multi-modal, and history).  It shows the total number of samples in the training and test sets, as well as the distribution across the different input types.  This allows for a clear understanding of the dataset's size and composition, highlighting the balance (or imbalance) of different data types included.", "section": "3.1 ChatGenBench: Benchmarking Automatic T2I"}, {"content": "| Setting | Model | Prompt (BertScore \u2191) | Selection (Acc \u2191) | Config (Acc \u2191) | FID (Score \u2193) | CLIP (Score \u2191) | HPS (v2 \u2191) | Image (Reward \u2191) | Unified (Metric \u2191) |\n|---|---|---|---|---|---|---|---|---|---| \n| Supervised | Baseline | 0.026 | - | - | 32.7 | 64.6 | 20.2 | -34.6 | 37.3 |\n|  | ChatGen-Base(2B) | 0.184 | 0.206 | 0.384 | 21.3 | 69.9 | 23.5 | 2.4 | 59.0 |\n|  | ChatGen-Base(4B) | 0.197 | 0.230 | 0.490 | 20.7 | 70.0 | 23.4 | 1.5 | 58.7 |\n|  | ChatGen-Base(8B) | 0.208 | 0.264 | 0.509 | 20.8 | 70.7 | 23.9 | 4.0 | 60.7 |\n|  | ChatGen-Evo (2B) | 0.247 | 0.328 | 0.537 | 19.1 | 72.9 | 25.1 | 8.9 | 65.9 |\n| Few-shot | Baseline | 0.055 | - | - | 54.4 | 63.4 | 20.0 | -40.2 | 29.7 |\n|  | ChatGen-Base(2B) | 0.221 | 0.153 | 0.349 | 42.8 | 69.1 | 23.3 | -4.8 | 51.1 |\n|  | ChatGen-Base(4B) | 0.236 | 0.171 | 0.448 | 41.2 | 69.4 | 23.4 | -4.3 | 51.9 |\n|  | ChatGen-Base(8B) | 0.252 | 0.201 | 0.462 | 41.4 | 70.6 | 23.4 | -3.1 | 52.5 |\n|  | ChatGen-Evo (2B) | 0.283 | 0.231 | 0.493 | 40.7 | 72.5 | 25.0 | 5.1 | 59.2 |", "caption": "Table 4: The Step-wise and Final evaluation results of different methods on ChatGenBench.", "description": "This table presents a comprehensive evaluation of different methods for automatic text-to-image generation, using the ChatGenBench benchmark.  It shows the step-wise performance across three key stages: prompt generation (using BERTScore and accuracy), model selection (accuracy), and argument configuration (accuracy).  Additionally, it provides an overall evaluation of the final image quality, combining results from FID, CLIP score, HPS v2, and ImageReward into a single Unified Metric. The results are compared across various baselines and model sizes (2B, 4B, and 8B parameters), offering insight into each method's strengths and weaknesses in each phase of the process.", "section": "4. Experiment"}, {"content": "| Prompt | BertScore \u2191 |\n|---|---|", "caption": "Table 5: Efficiency comparison. Training efficiency is measured in GPU hours, while inference is expressed as the seconds required to process each data. The training was conducted on 8 A100 GPUs, and inference was performed on a single A100 80GB GPU.", "description": "This table compares the training and inference efficiency of different models. Training efficiency is measured in GPU hours, reflecting the computational resources required for training. Inference efficiency is measured in seconds, representing the time taken to process a single data point.  The training was performed using 8 A100 GPUs, while inference utilized a single A100 80GB GPU.  The table highlights the trade-off between model size (parameters), training time, and inference speed, demonstrating the practical implications of model complexity.", "section": "3.1 ChatGenBench: Benchmarking Automatic T2I"}, {"content": "| Selection | Acc \u2191 |\n|---|---|", "caption": "Table 6: Ablation experiments on the supervised setting.", "description": "This table presents the results of ablation experiments conducted on the supervised subset of the ChatGenBench dataset.  It analyzes the individual contributions of each stage in the ChatGen-Evo model\u2014prompt writing, model selection, and argument configuration\u2014to the overall performance. By systematically removing or replacing components of the model, the table quantifies the impact of each stage on key metrics such as FID score, ImageReward, and the Unified Metric. This allows for a precise understanding of the effectiveness and importance of the multi-stage evolution strategy implemented in ChatGen-Evo.", "section": "4.3 Capability Analysis"}, {"content": "| Config | Acc \u2191 |\n|---|---|", "caption": "Table 7: The evaluation results of different input types.", "description": "This table presents a quantitative analysis of the ChatGen-Evo model's performance across various input types: single, multimodal, and historical.  The results show the model's ability to generate images given different kinds of input data, highlighting any strengths and weaknesses in handling varied input complexities. The metrics evaluated are step-wise accuracy for prompt generation, model selection, and argument configuration, and the final image quality as assessed by a Unified Metric that combines multiple quality assessment scores.", "section": "4.3 Input Type Analysis"}]