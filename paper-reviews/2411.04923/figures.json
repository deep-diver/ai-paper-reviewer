[{"figure_path": "https://arxiv.org/html/2411.04923/extracted/5975936/figures/cvpr25-teaser.png", "caption": "Figure 1: Grounded Conversation with VideoGLaMM. \nOur proposed multimodal video conversational model provides text responses grounded at the pixel level in the input video. The generated masks are spatio-temporally consistent across frames. The fine-grained grounded outputs from VideoGLaMM describe different levels of granularity, e.g., person, objects (bike), stuff (road), and explain object and scene attributes. Existing Video-LMMs do not\noffer pixel-level grounded conversational capability.", "description": "VideoGLaMM, a new multimodal video conversational model, generates pixel-level grounded text descriptions.  Unlike previous models, VideoGLaMM provides fine-grained descriptions detailing various levels of granularity (person, objects, scene attributes) and spatio-temporally consistent masks across video frames. This allows for a more nuanced understanding of the video content than previously possible with existing Video-LMMs.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2411.04923/x1.png", "caption": "Figure 2: Working of VideoGLaMM. VideoGLaMM consists of a dual spatio-temporal encoder for encoding image and video level features. The spatial features represent the local information and the temporal features represent global information. The spatial and temporal tokens are passed through V-L adapters and concatenated with the text tokens, before feeding to LLM. A L-V projector is employed to align LLM\u2019s response with the visual space of pixel decoder. Finally, the aligned LLM features along with the frame features from a frame encoder are passed to a grounded pixel decoder, to obtain the fine-grained object masks corresponding to the LLM response.", "description": "VideoGLaMM processes user queries by first encoding video content into spatial (local details) and temporal (global context) features using a dual spatio-temporal encoder. These features are then aligned with textual information via Vision-to-Language (V\u2192L) adapters.  The combined spatial, temporal, and textual data is fed into a Large Language Model (LLM), which generates a response and corresponding segmentation masks. A Language-to-Vision (L\u2192V) projector aligns the LLM's response with the visual space of the pixel decoder.  Finally, the aligned LLM features, along with frame features from a separate frame encoder, are input to a grounded pixel decoder which outputs fine-grained object masks that precisely match the objects mentioned in the LLM's response.", "section": "3. VideoGLAMM"}, {"figure_path": "https://arxiv.org/html/2411.04923/extracted/5975936/figures/videoglamm_annotation_pipeline.png", "caption": "Figure 3: Proposed Semi-automatic Annotation Pipeline. Our dataset for grounded conversation generation (GCG) is built from three video dataset types: i) Videos having masks only: Object patches are extracted from video frames using masks and processed by the Gemini model for initial object descriptions, which are then refined to produce detailed object captions. These refined captions and masks are used again with the Gemini model to create dense, grounded captions. ii) Videos having bbox annotations and captions: Frames are first processed with a Video-LMM to generate a comprehensive caption which is combined with the original caption and fed to GPT-4o to obtain dense grounded captions. Masks are generated using frames and ground-truth bounding boxes with the SAM model. iii) Videos having object bboxes and referring expressions: Frames, bounding boxes, and referring expressions are input to GPT-4o for dense grounded captions, while masks are generated by feeding frames and bounding boxes to the SAM model.", "description": "This figure illustrates the semi-automatic annotation pipeline used to create the Grounded Conversation Generation (GCG) dataset.  The pipeline handles three types of video data: 1) Videos with only masks, where object patches are extracted, processed by the Gemini model for initial descriptions, refined for detailed captions, and then fed back into Gemini with the masks to create dense, grounded captions. 2) Videos with bounding box annotations and captions, where frames are processed by a Video-LMM for a comprehensive caption, combined with the original caption and fed to GPT-4 to generate dense captions, and masks are created using the SAM model with frames and bounding boxes. 3) Videos with bounding boxes and referring expressions, where frames, bounding boxes, and referring expressions are input to GPT-4 for dense captions, and masks are generated using the SAM model with frames and bounding boxes.", "section": "4. Our Benchmark & Annotation Pipeline"}, {"figure_path": "https://arxiv.org/html/2411.04923/extracted/5975936/figures/cvpr25_qualitative_new.jpg", "caption": "Figure 4: Qualitative results of VideoGLaMM on grounded conversation generation (GCG). Given user queries, the VideoGLaMM generates textual\nresponses and grounds objects and phrases using pixel-level masks, showing its detailed understanding of the video.", "description": "Figure 4 showcases VideoGLaMM's performance on Grounded Conversation Generation (GCG).  The model receives user queries about videos. In response, it produces detailed textual descriptions. Importantly, these descriptions are not just general summaries but pinpoint specific objects and phrases within the video using pixel-level segmentation masks. The masks visually highlight the precise parts of the video the model is referring to in its text.  The figure provides several examples demonstrating VideoGLaMM's capability to accurately identify and label objects, illustrating its in-depth understanding of the video content.", "section": "5.1 Grounded Conversation Generation"}]