[{"content": "| Model | mIoU | Recall | METEOR | CIDEr | CLAIR |\n|---|---|---|---|---|---| \n| PG-Video-LLaVA [31] | 24.03 | 0.093 | 0.10 | 0.01 | 15.0 |\n| GLaMM [38] + SAM2 [40] | 28.60 | 0.117 | 0.097 | 0.15 | 22.9 |\n| VideoGLaMM | **62.34** | **0.375** | **0.103** | **0.59** | **28.2** |", "caption": "Table 1: Evaluation on grounded conversation generation (GCG): VideoGLaMM shows superior performance in generating accurate video-level captions which are tied to corresponding segmentation masks in the video frames.", "description": "This table presents a quantitative comparison of VideoGLaMM against two baseline models (PG-Video-LLaVA and GLaMM+SAM2) on the Grounded Conversation Generation (GCG) task.  The metrics used are mIOU (mean Intersection over Union) and Recall, evaluating the accuracy of generated segmentation masks; and METEOR, CIDEr, and CLAIR, assessing the quality of the generated video-level captions.  Higher scores indicate better performance in both mask generation and caption description quality, demonstrating the model's ability to accurately generate descriptive video captions aligned with precise pixel-level segmentations.  VideoGLaMM shows significant improvement over the baseline models across all metrics.", "section": "5.1 Grounded Conversation Generation"}, {"content": "| Model | \\mathcal{J} | \\mathcal{F} | \\mathcal{J&F} |\n|---|---|---|---| \n| PG-Video-LLaVA [31] | 18.35 | 19.39 | 18.87 |\n| GLaMM [38] + SAM2 [40] | 35.80 | 41.50 | 38.66 |\n| VideoLISA [5] | 41.30 | 47.60 | 44.40 |\n| VideoGLaMM | 42.07 | 48.23 | 45.15 |", "caption": "Table 4: Effect of Spatio-Temporal Dual Encoder: We obtain low performance using only spatial (image) encoder. Using only a video encoder gives the highest mIOU but lower scores on CLAIR, METEOR and CIDEr. For a better trade-off, we employ dual (image and video) encoders to have accurate, grounded conversations.", "description": "This table presents an ablation study evaluating the impact of using different encoder configurations in the VideoGLaMM model on the task of grounded conversation generation.  The study compares three setups: using only the spatial (image) encoder, using only the temporal (video) encoder, and using both spatial and temporal encoders (the full VideoGLaMM model). The results show that relying solely on the spatial encoder leads to significantly worse performance across all metrics (mIOU, Recall, METEOR, CIDEr, and CLAIR).  While using only the temporal encoder achieves the highest mIOU (indicating better object localization), it performs poorly in terms of the conversational quality metrics (METEOR, CIDEr, and CLAIR).  The table concludes that using both encoders provides the best balance, achieving high accuracy in object grounding while maintaining strong conversational quality.", "section": "5.4 Ablation studies"}, {"content": "| Model | VidSTG (interrogative mIoU) |\n|---|---| \n| PG-Video-LLaVA-7B [31] | 34.20 |\n| PG-Video-LLaVA-13B [31] | 35.10 |\n| GLaMM [38] + SAM2 [40] | 38.63 |\n| VideoGLaMM | 39.66 |", "caption": "Table 5: Spatial vs Spatio-temporal Pixel decoder: We observe that using Pixel decoder without the temporal branch gives limited performance as the model faces difficulties in temporal grounding. When using temporal branch, the performance on both the temporal grounding and grounded LLM response improves indicating the importance of temporal processing in VideoGLaMM.", "description": "This ablation study investigates the impact of incorporating temporal information into the pixel decoder of the VideoGLaMM model.  The table compares the model's performance on grounded conversation generation when using only a spatial pixel decoder versus a spatio-temporal pixel decoder. The results demonstrate that including the temporal branch significantly improves both the accuracy of temporal grounding (as measured by mIOU) and the quality of the generated language responses (as measured by METEOR, CIDEr, and CLAIR). This highlights the crucial role of temporal context for effective visual grounding in video.", "section": "5.4 Ablation studies"}, {"content": "| Encoder Configuration | mIoU | Recall | METEOR | CIDEr | CLAIR |\n|---|---|---|---|---|---| \n| Image encoder | 60.06 | 0.395 | 0.081 | 0.371 | 18.9 |\n| Video encoder | 64.62 | 0.375 | 0.097 | 0.568 | 26.5 |\n| Dual encoder | 62.34 | 0.375 | 0.103 | 0.590 | 28.2 |", "caption": "Table 6: Effect of number of frames for Pixel Decoder: We observe that using 4 supervision frames for pixel decoder gives better mIOU but relatively modest conversation quality measured by METEOR and CLAIR. With 8 supervision frames, mIOU slightly decreases while the conversational quality increases.", "description": "This table presents an ablation study analyzing the impact of the number of input frames to the pixel decoder on the performance of the VideoGLaMM model. The study reveals a trade-off between mask accuracy (mIOU) and conversational quality (METEOR, CIDEr, CLAIR). Using 4 frames yields a slightly better mIOU, but lower conversational quality scores, compared to using 8 frames.  Using 8 frames achieves a somewhat lower mIOU score, but significantly improves the conversational quality metrics.", "section": "5.4 Ablation studies"}]