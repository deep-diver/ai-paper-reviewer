[{"figure_path": "https://arxiv.org/html/2503.06626/x1.png", "caption": "Figure 1: CC3M Pretraining: CLIP vs.\u00a0DiffCLIP Across Six Tasks.\nWe compare standard CLIP (blue) and our DiffCLIP variant (pink) on linear probing, few-shot classification, image/text retrieval, zero-shot ImageNet, and zero-shot OOD.\nIn each case, DiffCLIP consistently outperforms CLIP, highlighting the effectiveness of differential attention with only 0.003% extra parameters.", "description": "Figure 1 compares the performance of the original CLIP model and the proposed DiffCLIP model across six different vision-language tasks.  The tasks include linear probing, a few-shot classification setting, image retrieval, text retrieval, zero-shot classification on the ImageNet dataset, and zero-shot classification on out-of-distribution (OOD) data.  The results are presented using bar charts showing accuracy percentages.  The original CLIP model is represented in blue, and the DiffCLIP model is represented in pink. In every task, DiffCLIP outperforms the baseline CLIP model, demonstrating that integrating the differential attention mechanism significantly improves performance without adding any considerable computational cost (only 0.003% extra parameters).", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.06626/extracted/6264640/figure/images.png", "caption": "Figure 2: Comparing CLIP vs.\u00a0DiffCLIP Attention Maps.\nFor two images (rows), we visualize where CLIP and DiffCLIP attend when matching each image against two different textual queries.\nWhile CLIP allocates attention to irrelevant background regions, DiffCLIP more effectively centers on query-relevant objects, highlighting how differential attention can reduce noise and improve focus. \nQueries: First Row: \u2018Mug\u201d, Lamp\u201d; Second Row: Flower\u201d, Dog\u201d.", "description": "This figure compares the attention mechanisms of CLIP and DiffCLIP models on two example images. Each image is processed with two different text queries. The heatmaps visualize where each model focuses its attention.  CLIP's attention is spread out, including irrelevant background details. In contrast, DiffCLIP's attention is more concentrated on the query-relevant objects, showcasing its ability to filter out noise and improve focus.  The text queries for the top row are \"Mug\" and \"Lamp,\" while the bottom row uses \"Flower\" and \"Dog.\"", "section": "3. Preliminaries"}, {"figure_path": "https://arxiv.org/html/2503.06626/x2.png", "caption": "Figure 3: OOD Zero-Shot ImageNet Performance.\nComparison of zero-shot accuracy (%) on ImageNet, ImageNet-V2, ImageNet-A, ImageNet-R, and ImageNet-Sketch, plus the average. Bars show performance of CLIP (blue) versus DiffCLIP (pink), trained on CC3M (left) or CC12M (right). Numerical deltas above the bars indicate the absolute improvement or drop for DiffCLIP relative to CLIP. DiffCLIP improves on average the zero-shot performance on OOD ImageNet datasets as compared to CLIP.", "description": "This figure compares the zero-shot performance of CLIP and DiffCLIP on various out-of-distribution (OOD) ImageNet datasets.  The datasets used are ImageNet, ImageNet-V2, ImageNet-A, ImageNet-R, and ImageNet-Sketch.  The performance is measured as zero-shot accuracy (%).  Two different pretraining datasets were used for the models: CC3M and CC12M.  Blue bars represent CLIP, and pink bars represent DiffCLIP. The numerical values above each bar represent the absolute difference in accuracy between DiffCLIP and CLIP for that specific dataset. The average accuracy across all datasets is also shown.  The results demonstrate that DiffCLIP generally improves the zero-shot accuracy compared to the standard CLIP model on these challenging OOD datasets.", "section": "4.3. Does Differential Attention Improve Out-of-Domain Robustness?"}, {"figure_path": "https://arxiv.org/html/2503.06626/x3.png", "caption": "Figure 4: MMVP-VLM Benchmarking.\nRadar plot illustrating performance on different fine-grained visual categories. Both models (CLIP in blue, DiffCLIP in pink) are evaluated on properties like orientation, positional context, and color appearance. DiffCLIP (average 27.6%) consistently outperforms CLIP (average 21.9%), demonstrating more focused attention on subtle visual details.", "description": "The radar plot in Figure 4 compares the performance of CLIP and DiffCLIP on the MMVP-VLM benchmark, which assesses fine-grained visual understanding.  Each axis represents a different visual property: orientation, positional context, color and appearance, structural characteristics, presence of specific features, viewpoint and perspective, and distribution and quantity.  The plot shows that DiffCLIP (pink) consistently outperforms CLIP (blue), achieving an average score of 27.6% compared to CLIP's 21.9%. This improvement highlights DiffCLIP's superior ability to focus on subtle visual details relevant to the task, improving performance across multiple visual properties.", "section": "4.4. Does DiffCLIP Improve Fine-Grained Visual Understanding?"}, {"figure_path": "https://arxiv.org/html/2503.06626/x4.png", "caption": "Figure 5: Comparing Different DiffCLIP Variants.\nWe evaluate four models on six tasks (linear probing, few-shot, image retrieval, text retrieval, ImageNet zero-shot, and zero-shot OOD), all pretrained on CC12M.\nCLIP (blue) is the baseline, DiffCLIP (pink) uses a fixed differential attention parameter,\nDiffCLIP\u2217 (purple) employs a dynamic schedule for differential attention,\nand DiffCLIP\u2020 (yellow) applies differential attention only to the vision encoder.", "description": "Figure 5 compares the performance of four different CLIP variants across six tasks: linear probing, few-shot classification, image retrieval, text retrieval, ImageNet zero-shot classification, and out-of-distribution (OOD) zero-shot classification.  All models were pretrained on the Conceptual Captions 12M dataset. The variants are: 1) The baseline CLIP model (blue), 2) DiffCLIP with a fixed differential attention parameter (pink), 3) DiffCLIP with a dynamic schedule for the differential attention parameter (purple), and 4) DiffCLIP applying differential attention only to the vision encoder (yellow).  The figure visually represents the performance differences between these variants on each of the six tasks.", "section": "4. Experiments"}]