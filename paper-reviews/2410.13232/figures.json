[{"figure_path": "2410.13232/figures/figures_4_0.png", "caption": "Figure 3: Framework overview. We first collect training data for world models (top). After training, we perform policy optimization by selecting the action leading to an optimal next state (bottom).", "description": "The figure illustrates the framework of World-Model-Augmented web agents, which includes world model training and inference-time policy optimization via the world model.", "section": "4 WORLD-MODEL-AUGMENTED WEB AGENTS"}, {"figure_path": "2410.13232/figures/figures_4_1.png", "caption": "Figure 3: Framework overview. We first collect training data for world models (top). After training, we perform policy optimization by selecting the action leading to an optimal next state (bottom).", "description": "The figure illustrates the framework of World-Model-Augmented (WMA) web agents, showing the process of world model training and inference-time policy optimization.", "section": "4 WORLD-MODEL-AUGMENTED WEB AGENTS"}, {"figure_path": "2410.13232/figures/figures_6_0.png", "caption": "Figure 5: The overview of transition-focused observation abstraction.", "description": "The figure illustrates the process of transition-focused observation abstraction, highlighting the use of the Hungarian algorithm to identify key differences between consecutive observations and their transformation into free-form natural language descriptions.", "section": "4.1 World Model Training"}, {"figure_path": "2410.13232/figures/figures_6_1.png", "caption": "Figure 5: The overview of transition-focused observation abstraction.", "description": "The figure illustrates the transition-focused observation abstraction process, showing how the Hungarian algorithm matches elements between consecutive observations to highlight important state differences, which are then converted into a free-form natural language description by an LLM.", "section": "4.1 World Model Training"}, {"figure_path": "2410.13232/figures/figures_6_2.png", "caption": "Figure 5: The overview of transition-focused observation abstraction.", "description": "The figure illustrates the process of transition-focused observation abstraction, showing how the Hungarian algorithm matches elements between consecutive observations and an LLM generates a free-form description highlighting state differences.", "section": "4.1 World Model Training"}, {"figure_path": "2410.13232/figures/figures_20_0.png", "caption": "Figure 8: Human annotation interface for preliminary analysis I in \u00a73.1.", "description": "The figure shows the user interface used for human annotation in the preliminary analysis I, where annotators were asked to identify the next state given the current state and an action.", "section": "3.1 PRELIMINARY ANALYSIS I: LLMS STRUGGLE WITH PREDICTING THE NEXT STATES CAUSED BY THEIR ACTIONS"}, {"figure_path": "2410.13232/figures/figures_20_1.png", "caption": "Figure 10: Erroneous example (Counterfactual imagination). The model predicts that specific products (96 TY CITY86 Bmw 740i Limited Collector Hoodie Men's Close; Toyota 86 Bad Institute Monkey Champagne Cup, Volkswagen A9 Bug Pick Dead Red) will appear in the next observation, while this specific page does not list them as the products for sell.", "description": "The figure shows an example of a counterfactual imagination error in the world model's prediction, where non-existent products are predicted to appear on the next observation.", "section": "6.2 Types of Errors in World Models' Predictions"}, {"figure_path": "2410.13232/figures/figures_21_0.png", "caption": "Figure 11: Erroneous example (Correct yet overly generic statements). \u201cComprehensive layout", "description": "The figure shows an example of an erroneous prediction where the model provides overly generic statements instead of specific details about the next state observation.", "section": "3.2 PRELIMINARY ANALYSIS II - LLMS MAKE BETTER ACTION SELECTION WHEN ACCESSING THE OUTCOME OF EACH ACTION CANDIDATE"}, {"figure_path": "2410.13232/figures/figures_21_1.png", "caption": "Figure 12: Erroneous example (Others). The predicted next state (i.e., contributions and activities) is actually several steps further away from the current time step.", "description": "The figure shows an example of an erroneous prediction made by the world model, where the predicted next state is several steps ahead of the actual next state.", "section": "6.2 TYPES OF ERRORS IN WORLD MODELS' PREDICTIONS"}, {"figure_path": "2410.13232/figures/figures_22_0.png", "caption": "Figure 13: Successful example (Mind2Web). WMA web agent successfully inferences on the Mind2Web benchmark (menards task #0). Using the policy model (i.e., GPT-40), WMA web agent selects the most proper action click [208] by leveraging its learned environment dynamics.", "description": "The figure shows a successful example of the WMA web agent using a policy model (GPT-40) to select the optimal action (click [208]) on the Mind2Web benchmark (menards task #0) by leveraging its learned environment dynamics.", "section": "5.2 MAIN RESULTS"}, {"figure_path": "2410.13232/figures/figures_23_0.png", "caption": "Figure 5: The overview of transition-focused observation abstraction.", "description": "The figure illustrates the process of transition-focused observation abstraction, highlighting the steps involved in transforming raw observations into free-form natural language descriptions that emphasize state differences.", "section": "4.1 World Model Training"}]