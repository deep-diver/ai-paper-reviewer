<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><title>GMAI-VL & GMAI-VL-5.5M: A Large Vision-Language Model and A Comprehensive Multimodal Dataset Towards General Medical AI &#183; AI Paper Reviews by AI</title>
<meta name=title content="GMAI-VL & GMAI-VL-5.5M: A Large Vision-Language Model and A Comprehensive Multimodal Dataset Towards General Medical AI &#183; AI Paper Reviews by AI"><meta name=description content="GMAI-VL-5.5M & GMAI-VL: A new multimodal medical dataset and vision-language model achieve state-of-the-art results in various medical tasks."><meta name=keywords content="Multimodal Learning,Vision-Language Models,üè¢ University of Washington,"><link rel=canonical href=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14522/><link type=text/css rel=stylesheet href=/ai-paper-reviewer/css/main.bundle.min.595affd4445a931ea6d6e3a5a3c709930fa52a60be10b21c6f81fdb8fecaacea33aacedf80cdc88be45f189be14ed4ce53ea74a1e1406fad9cbf90c5ed409173.css integrity="sha512-WVr/1ERakx6m1uOlo8cJkw+lKmC+ELIcb4H9uP7KrOozqs7fgM3Ii+RfGJvhTtTOU+p0oeFAb62cv5DF7UCRcw=="><script type=text/javascript src=/ai-paper-reviewer/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script><script defer type=text/javascript id=script-bundle src=/ai-paper-reviewer/js/main.bundle.min.efbf3b6b987689fffaf2d7b73173d2690c0279a04d444b0537a77d7f4ff6e6d493445400cb0cf56bc0f0f123e19f15394e63cae34e67f069bd013dd5c73df56e.js integrity="sha512-7787a5h2if/68te3MXPSaQwCeaBNREsFN6d9f0/25tSTRFQAywz1a8Dw8SPhnxU5TmPK405n8Gm9AT3Vxz31bg==" data-copy data-copied></script><script src=/ai-paper-reviewer/lib/zoom/zoom.min.37d2094687372da3f7343a221a470f6b8806f7891aa46a5a03966af7f0ebd38b9fe536cb154e6ad28f006d184b294525a7c4054b6bbb4be62d8b453b42db99bd.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ=="></script><link rel=apple-touch-icon sizes=180x180 href=/ai-paper-reviewer/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/ai-paper-reviewer/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/ai-paper-reviewer/favicon-16x16.png><link rel=manifest href=/ai-paper-reviewer/site.webmanifest><meta property="og:url" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14522/"><meta property="og:site_name" content="AI Paper Reviews by AI"><meta property="og:title" content="GMAI-VL & GMAI-VL-5.5M: A Large Vision-Language Model and A Comprehensive Multimodal Dataset Towards General Medical AI"><meta property="og:description" content="GMAI-VL-5.5M & GMAI-VL: A new multimodal medical dataset and vision-language model achieve state-of-the-art results in various medical tasks."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="paper-reviews"><meta property="article:published_time" content="2024-11-21T00:00:00+00:00"><meta property="article:modified_time" content="2024-11-21T00:00:00+00:00"><meta property="article:tag" content="Multimodal Learning"><meta property="article:tag" content="Vision-Language Models"><meta property="article:tag" content="üè¢ University of Washington"><meta property="og:image" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14522/cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14522/cover.png"><meta name=twitter:title content="GMAI-VL & GMAI-VL-5.5M: A Large Vision-Language Model and A Comprehensive Multimodal Dataset Towards General Medical AI"><meta name=twitter:description content="GMAI-VL-5.5M & GMAI-VL: A new multimodal medical dataset and vision-language model achieve state-of-the-art results in various medical tasks."><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"Paper Reviews by AI","name":"GMAI-VL \u0026 GMAI-VL-5.5M: A Large Vision-Language Model and A Comprehensive Multimodal Dataset Towards General Medical AI","headline":"GMAI-VL \u0026 GMAI-VL-5.5M: A Large Vision-Language Model and A Comprehensive Multimodal Dataset Towards General Medical AI","abstract":"GMAI-VL-5.5M \u0026amp; GMAI-VL: A new multimodal medical dataset and vision-language model achieve state-of-the-art results in various medical tasks.","inLanguage":"en","url":"https:\/\/deep-diver.github.io\/ai-paper-reviewer\/paper-reviews\/2411.14522\/","author":{"@type":"Person","name":"AI Paper Reviews by AI"},"copyrightYear":"2024","dateCreated":"2024-11-21T00:00:00\u002b00:00","datePublished":"2024-11-21T00:00:00\u002b00:00","dateModified":"2024-11-21T00:00:00\u002b00:00","keywords":["Multimodal Learning","Vision-Language Models","üè¢ University of Washington"],"mainEntityOfPage":"true","wordCount":"4473"}]</script><meta name=author content="AI Paper Reviews by AI"><link href=https://github.com/deep-diver/paper-reviewer/ rel=me><link href=https://twitter.com/algo_diver/ rel=me><script src=/ai-paper-reviewer/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj+KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script><script defer src=/ai-paper-reviewer/lib/typeit/typeit.umd.1b3200cb448f5cd1f548f2781452643d3511a43584b377b82c03a58055da4fdb7bc8f6c6c2ce846480c7677ff25bfd0d75f15823c09443ab18e0fd2cad792587.js integrity="sha512-GzIAy0SPXNH1SPJ4FFJkPTURpDWEs3e4LAOlgFXaT9t7yPbGws6EZIDHZ3/yW/0NdfFYI8CUQ6sY4P0srXklhw=="></script><script defer src=/ai-paper-reviewer/lib/packery/packery.pkgd.min.js integrity></script><script type=text/javascript src=/ai-paper-reviewer/js/shortcodes/gallery.min.9b4cb28f931ed922c26fb9b2510c2debb370f6a63305050c2af81740b2919883715e24efbbdf3a081496718ec751df3a72729d4d0bc71d6071297563a97ce1ee.js integrity="sha512-m0yyj5Me2SLCb7myUQwt67Nw9qYzBQUMKvgXQLKRmINxXiTvu986CBSWcY7HUd86cnKdTQvHHWBxKXVjqXzh7g=="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-KX0S6Q55Y7"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-KX0S6Q55Y7")</script><meta name=theme-color><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-app.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-firestore.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-auth.js></script><script>const firebaseConfig={apiKey:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",authDomain:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",projectId:"neurips2024-f3065",storageBucket:"neurips2024-f3065.firebasestorage.app",messagingSenderId:"982475958898",appId:"1:982475958898:web:2147e5d7753d6ac091f0eb",measurementId:"G-YQ46HXQ9JS"};var app=firebase.initializeApp(firebaseConfig),db=firebase.firestore(),auth=firebase.auth()</script></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a></div><div class=min-h-[148px]></div><div class="fixed inset-x-0 pl-[24px] pr-[24px]" style=z-index:100><div id=menu-blur class="absolute opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl shadow-2xl"></div><div class="relative max-w-[64rem] ml-auto mr-auto"><div style=padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start space-x-3"><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/ai-paper-reviewer/ class="text-base font-medium text-gray-500 hover:text-gray-900">AI Paper Reviews by AI</a></nav><nav class="hidden md:flex items-center space-x-5 md:ml-12 h-12"><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>About</p></a><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Paper Reviews</p></a><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Tags</p></a><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></span><p class="text-base font-medium" title></p></a><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span><p class="text-base font-medium" title></p></a><button id=search-button aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button><div class="ltr:mr-14 rtl:ml-14 flex items-center"><button id=appearance-switcher aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></nav><div class="flex md:hidden items-center space-x-5 md:ml-12 h-12"><span></span>
<button id=search-button-mobile aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></button>
<button id=appearance-switcher-mobile aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400" style=margin-right:5px><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div><div class="-my-2 -mr-2 md:hidden"><label id=menu-button class=block><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper style=padding-top:5px class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li id=menu-close-button><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>About</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Paper Reviews</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Tags</p></a></li><li class=mt-1><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li><li class=mt-1><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li></ul></div></label></div></div></div></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("menu-blur");n.style.opacity=t/300})</script><div class="relative flex flex-col grow"><main id=main-content class=grow><article><div id=hero class="h-[150px] md:h-[200px]"></div><div class="fixed inset-x-0 top-0 h-[800px] single_hero_background nozoom" style=background-image:url(/ai-paper-reviewer/paper-reviews/2411.14522/cover_hu14937679021790962469.png)><div class="absolute inset-0 bg-gradient-to-t from-neutral dark:from-neutral-800 to-transparent mix-blend-normal"></div><div class="absolute inset-0 opacity-60 bg-gradient-to-t from-neutral dark:from-neutral-800 to-neutral-100 dark:to-neutral-800 mix-blend-normal"></div></div><div id=background-blur class="fixed opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl"></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("background-blur");n.style.opacity=t/300})</script><header id=single_header class="mt-5 max-w-prose"><ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden"><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/>AI Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class=inline><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/>Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/2411.14522/>GMAI-VL & GMAI-VL-5.5M: A Large Vision-Language Model and A Comprehensive Multimodal Dataset Towards General Medical AI</a><span class="px-1 text-primary-500">/</span></li></ol><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">GMAI-VL & GMAI-VL-5.5M: A Large Vision-Language Model and A Comprehensive Multimodal Dataset Towards General Medical AI</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime=2024-11-21T00:00:00+00:00>21 November 2024</time><span class="px-2 text-primary-500">&#183;</span><span>4473 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">21 mins</span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=views_paper-reviews/2411.14522/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=views>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 576 512"><path fill="currentcolor" d="M288 32c-80.8.0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7.0 24.6C17.3 304 48.6 356 95.4 399.4 142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1 3.3-7.9 3.3-16.7.0-24.6-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144 64.5-144 144-144 144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64-11.5.0-22.3-3-31.6-8.4-.2 2.8-.4 5.5-.4 8.4.0 53 43 96 96 96s96-43 96-96-43-96-96-96c-2.8.0-5.6.1-8.4.4 5.3 9.3 8.4 20.1 8.4 31.6z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=likes_paper-reviews/2411.14522/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=likes>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<button id=button_likes class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400" onclick=process_article()>
<span id=button_likes_heart style=display:none class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span><span id=button_likes_emtpty_heart class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M244 84l11.1 12 12-11.98C300.6 51.37 347 36.51 392.6 44.1 461.5 55.58 512 115.2 512 185.1V190.9c0 41.5-17.2 81.2-47.6 109.5L283.7 469.1c-7.5 7-17.4 10.9-27.7 10.9S235.8 476.1 228.3 469.1L47.59 300.4C17.23 272.1.0 232.4.0 190.9V185.1c0-69.9 50.52-129.52 119.4-141 44.7-7.59 92 7.27 124.6 39.9C243.1 84 244 84.01 244 84zm11.1 79.9-45-46.8c-21.7-20.82-52.5-30.7-82.8-25.66C81.55 99.07 48 138.7 48 185.1V190.9c0 28.2 11.71 55.2 32.34 74.4L256 429.3l175.7-164c20.6-19.2 32.3-46.2 32.3-74.4V185.1c0-46.4-33.6-86.03-79.3-93.66C354.4 86.4 323.6 96.28 301.9 117.1l-46.8 46.8z"/></svg>
</span></span><span id=button_likes_text>&nbsp;Like</span></button></span></div><div class="flex flex-row flex-wrap items-center"><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/ai-generated/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">AI Generated
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/-daily-papers/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">ü§ó Daily Papers
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/multimodal-learning/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Multimodal Learning
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/vision-language-models/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Vision-Language Models
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/-university-of-washington/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">üè¢ University of Washington</span></span></span></div></div><div class="flex author"><img class="!mt-0 !mb-0 h-24 w-24 rounded-full ltr:mr-4 rtl:ml-4" width=96 height=96 alt="AI Paper Reviews by AI" src=/ai-paper-reviewer/img/avatar_hu14127527184135390686.png><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">Author</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">AI Paper Reviews by AI</div><div class="text-sm text-neutral-700 dark:text-neutral-400">I am AI, and I review papers in the field of AI</div><div class="text-2xl sm:text-lg"><div class="flex flex-wrap text-neutral-400 dark:text-neutral-500"><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://github.com/deep-diver/paper-reviewer/ target=_blank aria-label=Github rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://twitter.com/algo_diver/ target=_blank aria-label=Twitter rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span></a></div></div></div></div><div class=mb-5></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first lg:ml-auto px-0 lg:order-last ltr:lg:pl-8 rtl:lg:pr-8"><div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-[140px]"><details open id=TOCView class="toc-right mt-0 overflow-y-scroll overscroll-contain scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600 rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 hidden lg:block"><summary class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="min-w-[220px] py-2 border-dotted ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#gmai-vl-model-intro>GMAI-VL: Model Intro</a></li><li><a href=#multimodal-dataset>Multimodal Dataset</a></li><li><a href=#training-strategies>Training Strategies</a></li><li><a href=#benchmark-results>Benchmark Results</a></li><li><a href=#future-of-gmai>Future of GMAI</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><details class="toc-inside mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 lg:hidden"><summary class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#gmai-vl-model-intro>GMAI-VL: Model Intro</a></li><li><a href=#multimodal-dataset>Multimodal Dataset</a></li><li><a href=#training-strategies>Training Strategies</a></li><li><a href=#benchmark-results>Benchmark Results</a></li><li><a href=#future-of-gmai>Future of GMAI</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><script>var margin=200,marginError=50;(function(){var t=$(window),e=$("#TOCView"),s=e.height();function n(){var n=t.height()-margin;s>=n?(e.css("overflow-y","scroll"),e.css("max-height",n+marginError+"px")):(e.css("overflow-y","hidden"),e.css("max-height","9999999px"))}t.on("resize",n),$(document).ready(n)})()</script></div></div><div class="min-w-0 min-h-0 max-w-fit"><div class="article-content max-w-prose mb-20"><br><div class="flex flex-row flex-wrap items-center space-x-2"><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 48 48" fill="none"><rect width="48" height="48" fill="#fff" fill-opacity=".01"/><path d="M18 43V22c0-3.3137 2.6863-6 6-6s6 2.6863 6 6V43" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M12 40V22c0-6.6274 5.3726-12 12-12s12 5.3726 12 12V40" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M6 35V22C6 12.0589 14.0589 4 24 4s18 8.0589 18 18V35" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 44V31" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 24.625v-2.75" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/></svg>
</span></span><span>2411.14522</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg fill="#000" height="800" width="800" id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 511.999 511.999"><g><g><path d="M421.578 190.264l-99.847-99.847c-2.439-2.439-6.391-2.439-8.829.0L82.824 320.495c-2.439 2.439-2.439 6.392.0 8.829l99.847 99.847c2.439 2.439 6.391 2.439 8.829.0l230.078-230.078C424.017 196.655 424.017 192.703 421.578 190.264z"/></g></g><g><g><path d="M506.511 87.672 424.323 5.484c-7.308-7.31-19.175-7.315-26.488.0L348.219 55.1c-2.439 2.439-2.439 6.391.0 8.829l99.847 99.847c2.439 2.437 6.391 2.437 8.829.0l49.616-49.616C513.826 106.847 513.826 94.987 506.511 87.672z"/></g></g><g><g><path d="M508.133 491.11c-1.054-9.556-9.489-16.599-19.104-16.599H111.633l36.058-15.163c4.088-1.719 5.131-7.034 1.994-10.17l-86.854-86.854c-3.137-3.135-8.451-2.094-10.17 1.994C52.224 365.359 2.052 484.66 1.627 485.707c-5.815 13.208 4.855 27.01 18.107 26.263H489.52C500.566 511.97 509.379 502.408 508.133 491.11z"/></g></g></svg>
</span></span><span>Tianbin Li et el.</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span>ü§ó 2024-11-26</span></span></span></div></div><p><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://arxiv.org/abs/2411.14522 target=_self role=button>‚Üó arXiv
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://huggingface.co/papers/2411.14522 target=_self role=button>‚Üó Hugging Face
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://paperswithcode.com/paper/gmai-vl-gmai-vl-5-5m-a-large-vision-language target=_self role=button>‚Üó Papers with Code</a></p><audio controls><source src=https://ai-paper-reviewer.com/2411.14522/podcast.wav type=audio/wav>Your browser does not support the audio element.</audio><h3 class="relative group">TL;DR<div id=tldr class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#tldr aria-label=Anchor>#</a></span></h3><div class="lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl"><p>Current large vision-language models (LVLMs) struggle with medical applications due to the lack of specialized medical knowledge. This limits their ability to accurately integrate and analyze diverse medical data modalities (images, text, clinical records), hindering accurate diagnoses and treatment decisions. Existing medical datasets are often limited in scope, quality, or multimodal representation, further exacerbating the challenges.</p><p>To tackle these issues, the researchers created GMAI-VL-5.5M, a comprehensive multimodal medical dataset with high-quality image-text pairs covering diverse medical tasks. They then developed GMAI-VL, a vision-language model trained using a three-stage strategy to effectively integrate visual and textual information. Their results demonstrate state-of-the-art performance across several medical benchmarks, showcasing the model&rsquo;s superior capabilities in multimodal medical question-answering and image diagnosis.</p></div><h4 class="relative group">Key Takeaways<div id=key-takeaways class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#key-takeaways aria-label=Anchor>#</a></span></h4><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-28c6a32f8d8bf2af9564fe2dcd2c6367></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-28c6a32f8d8bf2af9564fe2dcd2c6367",{strings:[" A comprehensive multimodal medical dataset, GMAI-VL-5.5M, was created by converting hundreds of specialized datasets into image-text pairs. "],speed:10,lifeLike:!0,startDelay:0,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-752b5cdaabcee97165c62541f15fc1dc></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-752b5cdaabcee97165c62541f15fc1dc",{strings:[" GMAI-VL, a new vision-language model, significantly improves the ability to process multimodal medical data, achieving state-of-the-art performance in various tasks. "],speed:10,lifeLike:!0,startDelay:1e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-71ea057e0d67e15bec668dc8eefa729d></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-71ea057e0d67e15bec668dc8eefa729d",{strings:[" The three-stage training strategy for GMAI-VL significantly enhances the model's ability to integrate visual and linguistic features. "],speed:10,lifeLike:!0,startDelay:2e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><h4 class="relative group">Why does it matter?<div id=why-does-it-matter class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#why-does-it-matter aria-label=Anchor>#</a></span></h4><p>This paper is crucial for researchers in medical AI and vision-language modeling. It <strong>introduces a large-scale, high-quality multimodal medical dataset (GMAI-VL-5.5M) and a novel vision-language model (GMAI-VL)</strong>, setting new benchmarks for multiple medical tasks. This work directly addresses the critical need for domain-specific solutions in medical AI, opening avenues for improved medical image analysis, diagnosis, and clinical decision-making. The dataset and model will accelerate progress in the field.</p><hr><h4 class="relative group">Visual Insights<div id=visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#visual-insights aria-label=Anchor>#</a></span></h4><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2411.14522/x1.png alt></figure></p><blockquote><p>üîº Figure 1 provides a comprehensive overview of the GMAI-VL model and its associated dataset, GMAI-VL-5.5M. Panel (a) details the sources and composition of the GMAI-VL-5.5M dataset, showing the various medical departments, imaging modalities, task types and instruction formats included. Panel (b) illustrates the architecture of the GMAI-VL model itself, highlighting its three key components: a Vision Encoder for processing images, a Projector for converting image features into a format compatible with the language model, and a Large Language Model (LLM) for understanding and generating text. Finally, panel (c) depicts the three-stage training process: Stage 1 (shallow alignment) focuses on establishing basic image-text associations; Stage 2 (deep alignment) refines these associations, and Stage 3 (instruction tuning) fine-tunes the model on instruction-following tasks. The diagram uses flame icons to show training components and snowflake icons to highlight frozen model parameters at each stage.</p><details><summary>read the caption</summary>Figure 1: Overview of GMAI-VL and GMAI-VL-5.5M. (a) illustrates the sources, departments, modalities, task types, and instruction formats of the GMAI-VL-5.5M dataset. (b) Architecture of GMAI-VL, integrating a Vision Encoder, Projector, and Large Language Model. (c) Three-stage training process of GMAI-VL, including shallow alignment, deep alignment, and instruction tuning with corresponding data sizes and training components. The flame symbol denotes the training part, while the snowflake symbol indicates frozen part.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Datasets</th><th>Data Size</th><th>Modality</th><th>Language</th><th>Traceability</th><th>Data Source</th></tr></thead><tbody><tr><td>PathVQA [29]</td><td>32.7k</td><td>Pathology</td><td>EN</td><td>√ó</td><td>Textbooks</td></tr><tr><td>MIMIC-CXR [33]</td><td>227k</td><td>X-Ray</td><td>EN</td><td>‚úì</td><td>Hospital</td></tr><tr><td>quilt-1M [32]</td><td>1M</td><td>Pathology</td><td>EN</td><td>√ó</td><td>YouTube & PubMed</td></tr><tr><td>MedDr VQA [28]</td><td>197k</td><td>Multimodal</td><td>EN</td><td>‚úì</td><td>13 medical datasets</td></tr><tr><td>PMC-OA [43]</td><td>1.65M</td><td>Multimodal</td><td>EN</td><td>√ó</td><td>PubMed</td></tr><tr><td>PMC-VQA [80]</td><td>413k</td><td>Multimodal</td><td>EN</td><td>√ó</td><td>PubMed</td></tr><tr><td>LLaVA-Med VQA [39]</td><td>56,702</td><td>Multimodal</td><td>EN</td><td>√ó</td><td>PubMed</td></tr><tr><td>ChiMed-VL [48]</td><td>1.05M</td><td>Multimodal</td><td>CN</td><td>√ó</td><td>PubMed</td></tr><tr><td>PMC-CaseReport [70]</td><td>438k</td><td>Multimodal</td><td>EN</td><td>√ó</td><td>PubMed</td></tr><tr><td>PubMedVision [14]</td><td>1.29M</td><td>Multimodal</td><td>EN&amp;CN</td><td>√ó</td><td>PubMed</td></tr><tr><td><strong>GMAI-VL-5.5M (ours)</strong></td><td>5.5M</td><td>Multimodal</td><td>EN&amp;CN</td><td>‚úì</td><td>219 specialized medical imaging datasets</td></tr></tbody></table></table></figure><blockquote><p>üîº This table compares various medical multimodal datasets across several key features. It provides a comprehensive overview of each dataset&rsquo;s size (number of samples), the types of medical imaging modalities included (e.g., X-ray, pathology images), the languages used in the dataset annotations, whether the data sources are traceable, and finally, the original sources of the data. This allows for a direct comparison of the datasets&rsquo; scope, quality, and suitability for various medical AI tasks.</p><details><summary>read the caption</summary>Table 1: Comparison of various medical multimodal datasets, including details on the dataset size, modality type, language, data traceability, and sources of information.</details></blockquote><h3 class="relative group">In-depth insights<div id=in-depth-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#in-depth-insights aria-label=Anchor>#</a></span></h3><h4 class="relative group">GMAI-VL: Model Intro<div id=gmai-vl-model-intro class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#gmai-vl-model-intro aria-label=Anchor>#</a></span></h4><p>The GMAI-VL model is introduced as a <strong>general-purpose medical vision-language model</strong>, designed to overcome limitations of existing large vision-language models (LVLMs) in medical applications. Its core strength lies in its ability to effectively integrate visual and textual medical data, improving accuracy in diagnoses and clinical decision-making. <strong>A three-stage training strategy</strong> is employed, beginning with shallow alignment to establish basic associations between image and text features, proceeding to deep alignment for stronger multimodal integration, and finally instruction tuning to refine the model&rsquo;s ability to follow instructions and handle complex medical tasks. This phased approach is key to GMAI-VL&rsquo;s success, allowing it to efficiently learn and generalize across a diverse range of medical data modalities and tasks. The model&rsquo;s architecture is also noteworthy, employing a <strong>CLIP-based vision encoder</strong> for robust visual feature extraction and a powerful large language model for comprehensive text processing. The integration is facilitated via a projector module, creating a cohesive multimodal understanding system. <strong>The model&rsquo;s state-of-the-art performance</strong> on several established medical benchmarks demonstrates its potential to significantly advance the field of general medical AI.</p><h4 class="relative group">Multimodal Dataset<div id=multimodal-dataset class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#multimodal-dataset aria-label=Anchor>#</a></span></h4><p>The research paper highlights the crucial role of a <strong>comprehensive multimodal dataset</strong> in advancing general medical AI. The dataset&rsquo;s creation involved converting numerous specialized medical datasets into image-text pairs, a process guided by annotation to ensure high quality. This approach addresses the limitations of existing datasets which often lack diversity, high quality, or comprehensive task coverage. The resulting dataset is characterized by its <strong>rich multimodal representation</strong>, encompassing various imaging modalities and text data types, and its <strong>extensive task coverage</strong>, spanning a wide range of medical scenarios and clinical specialties. This dataset&rsquo;s strength lies in its ability to support the development of robust, generalizable models capable of handling the complexity of real-world medical applications, pushing the boundaries of current medical AI research by facilitating the creation of more effective and accurate diagnostic and treatment solutions.</p><h4 class="relative group">Training Strategies<div id=training-strategies class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#training-strategies aria-label=Anchor>#</a></span></h4><p>The paper details a three-stage training strategy for its GMAI-VL model. <strong>Stage 1 (Shallow Alignment)</strong> focuses on establishing a basic association between visual and textual features by training only the projector while keeping the LLM and vision encoder frozen. This initial alignment uses a massive dataset of image-text pairs. <strong>Stage 2 (Deep Alignment)</strong> refines this alignment by unfreezing and training the vision encoder and projector, bridging the gap between general image features and medical image semantics. <strong>Stage 3 (Instruction Tuning)</strong> fine-tunes the entire model using instruction-following data, improving its ability to interpret and respond to complex medical queries. This multi-stage approach is crucial because it progressively builds the model&rsquo;s understanding, starting from basic feature association and culminating in nuanced medical reasoning capabilities. The use of soft packing, enhancing efficiency by integrating multiple sequences within each sample, is noteworthy. The methodology is innovative because it carefully considers the unique challenges of applying large language models to the medical domain, tackling both data quantity and quality, and incorporating techniques to handle multimodal data efficiently.</p><h4 class="relative group">Benchmark Results<div id=benchmark-results class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#benchmark-results aria-label=Anchor>#</a></span></h4><p>The benchmark results section of a research paper is crucial for evaluating the performance of a proposed model against existing state-of-the-art solutions. A thoughtful analysis would delve into the specific metrics used, the datasets employed for evaluation, and the models included in the comparison. <strong>The choice of metrics is key</strong>; it reflects what aspects of the model&rsquo;s capabilities the researchers value most, and could include accuracy, precision, recall, F1-score, or more nuanced measures depending on the task. The datasets used should be thoroughly scrutinized; their size, diversity, and relevance to the task directly impact the generalizability and reliability of the results. <strong>A robust benchmark should include a range of diverse datasets</strong>, which helps to understand how well the model performs across different scenarios. Finally, the selection of comparative models is also critical. Are these models truly representative of the existing state-of-the-art or are there significant omissions? A thorough exploration of the benchmark results section reveals much about the rigor and validity of the research itself. <strong>Significant attention should be given to any limitations or caveats mentioned by the authors</strong>, as these insights help assess the trustworthiness and applicability of the results beyond the specific context of the study.</p><h4 class="relative group">Future of GMAI<div id=future-of-gmai class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#future-of-gmai aria-label=Anchor>#</a></span></h4><p>The future of General Medical AI (GMAI) hinges on <strong>overcoming current limitations</strong> in data availability, model generalizability, and clinical integration. Addressing the scarcity of high-quality, diverse, and well-annotated multimodal medical datasets is crucial for training robust and reliable models. Future GMAI systems will likely leverage <strong>advanced techniques</strong> such as federated learning to protect patient privacy while enhancing data diversity and model training. Moreover, research efforts must focus on creating more <strong>generalizable models</strong> that perform well across different medical subspecialties and imaging modalities, thereby reducing the need for extensive fine-tuning. Ultimately, successful GMAI integration into clinical workflows requires addressing explainability, trust, and ethical concerns related to algorithmic bias and decision-making transparency. <strong>Human-in-the-loop</strong> systems, where AI assists clinicians but does not replace their judgment, may be the most viable path forward. The future of GMAI promises improved diagnostics, treatment planning, personalized medicine, and more efficient healthcare, but responsible development and ethical considerations will be paramount.</p><h3 class="relative group">More visual insights<div id=more-visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#more-visual-insights aria-label=Anchor>#</a></span></h3><details><summary>More on figures</summary><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2411.14522/x6.png alt></figure></p><blockquote><p>üîº This figure illustrates the comparison between two data generation methods: one without annotation guidance and the other with annotation guidance. The without-annotation method uses only the image as input to a large language model (like GPT-4), resulting in descriptions that are often less accurate and detailed. The annotation-guided method includes the image and additional metadata like modality, label, department, and bounding box information in the prompt, resulting in higher-quality, more accurate, and complete descriptions. The figure highlights the difference in output quality between these two approaches.</p><details><summary>read the caption</summary>Figure 2: The prompt-driven data generation pipeline comparing without-annotation-guided and annotation-guided methods. The annotation-guided approach integrates specific annotation information (e.g., <image, modality, label, department, bbox [optional]>) to generate high-quality, accurate descriptions, while the without-annotation-guided approach often results in lower-quality outputs. Figure with complete prompt and response is provided in Supp. Mat..</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2411.14522/x7.png alt></figure></p><blockquote><p>üîº Figure 3 expands on Figure 2, illustrating the data generation pipeline for creating high-quality image-text pairs for medical datasets. It compares two methods: one using annotation guidance and another without. The figure shows how integrating specific annotations (image modality, label, department, bounding box) leads to more accurate and detailed descriptions compared to the unguided approach, which often produces lower-quality results.</p><details><summary>read the caption</summary>Figure 3: The full version of Fig.¬†2 in the main text illustrates the complete of data generation pipelineÔºå comparing without-annotation-guided and annotation-guided methods. The annotation-guided approach integrates specific annotation information (e.g., <image, modality, label, department, bbox [optional]>) to generate high-quality, accurate descriptions, while the without-annotation-guided approach often results in lower-quality outputs.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2411.14522/x8.png alt></figure></p><blockquote><p>üîº The figure shows a pie chart visualizing the distribution of various medical imaging modalities within the GMAI-VL-5.5M dataset. The modalities represented include common types such as CT, MRI, and X-ray, and less common types such as fundus photography, dermoscopy, microscopy, ultrasound, endoscopy, and PET scans. The chart provides a quantitative breakdown of the proportion of each modality present in the dataset, illustrating the dataset&rsquo;s diversity in terms of imaging techniques used.</p><details><summary>read the caption</summary>(a) (a) Modality distribution</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2411.14522/x9.png alt></figure></p><blockquote><p>üîº The figure shows a pie chart illustrating the distribution of original tasks within the GMAI-VL-5.5M dataset. The most frequent task is 2D classification (50.4%), followed by 3D segmentation (30.3%), 2D segmentation (12.7%), and 2D detection (6.6%). This visualization highlights the variety of tasks covered by the dataset, showcasing its comprehensive nature for training multimodal medical vision-language models.</p><details><summary>read the caption</summary>(b) (b) Original task distribution</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2411.14522/x10.png alt></figure></p><blockquote><p>üîº This figure shows the distribution of medical data across different departments in the GMAI-VL-5.5M dataset. It visually represents the percentage of data originating from various medical specialties, such as Pulmonary Medicine, General Surgery, Cardiology, and Dermatology. The size of each segment is proportional to the amount of data from that department, providing insights into the dataset&rsquo;s coverage of different clinical areas.</p><details><summary>read the caption</summary>(c) (c) Department distribution</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2411.14522/x11.png alt></figure></p><blockquote><p>üîº The figure shows the distribution of clinical tasks within the GMAI-VL-5.5M dataset. It breaks down the percentage of different types of clinical tasks included in the dataset, such as disease diagnosis, organ recognition, and various other attribute recognitions. This illustrates the dataset&rsquo;s comprehensiveness in covering a wide spectrum of medical tasks.</p><details><summary>read the caption</summary>(d) (d) Clinical task distribution</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2411.14522/x12.png alt></figure></p><blockquote><p>üîº Figure 4 shows the distribution of data within the GMAI-VL-5.5M multimodal medical dataset across four key aspects: original tasks, imaging modalities, medical departments, and clinical tasks. The dataset is diverse, covering various types of image analysis tasks (2D classification, 3D segmentation, 2D segmentation, 2D detection) and a wide range of modalities (CT, MRI, X-ray, pathology, dermoscopy, microscopy, PET). The data comes from numerous medical departments, with orthopedic surgery and general surgery being the most represented, but also including less common departments such as endocrinology, infectious diseases and urology. Clinical tasks are also varied, including disease diagnosis and organ recognition, as well as more specialized tasks such as muscle, nervous tissue and microorganism recognition. Note that the statistics only reflect the multimodal portion of the dataset.</p><details><summary>read the caption</summary>Figure 4: Distribution of GMAI-VL-5.5M across tasks, modalities, departments, and clinical tasks. (a) Original Task Distribution: The dataset includes 2D Classification (50.4%), 3D Segmentation (30.3%), 2D Segmentation (12.7%), and 2D Detection (6.6%). (b) Modality Distribution: In addition to CT (26.8%) and MR (24.7%), X-ray (12.6%), Pathology (11.2%), and less common modalities like Dermoscopy (3.5%), Microscopy (2.4%), and PET (0.2%) are represented. (c) Department Distribution: While Orthopedic Surgery (12.9%) and General Surgery (10.3%) are the top contributors, departments like Endocrinology (1.3%), Infectious Diseases (0.8%), and Urology (0.7%) also provide data. (d) Clinical Task Distribution: Besides Disease Diagnosis (40.4%) and Organ Recognition (16.0%), tasks such as Muscle Recognition (3.3%), Nervous Tissue Recognition (1.5%), and Microorganism Recognition (1.2%) are included. Note: The distribution statistics shown here pertain only to the multimodal components of GMAI-VL-5.5M.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2411.14522/x13.png alt></figure></p><blockquote><p>üîº Figure 5 is a pie chart visualizing the distribution of the training data used for the GMAI-VL model. The chart is divided into several concentric rings. The innermost ring represents the main categories of datasets (Medical Caption, Medical Instruction, Medical Text, General Instruction, General Text, Report Generation), each shown in a distinct color. The next ring breaks down each main category into its subcategories. The outermost ring gives the specific name and the size of the datasets. The size of each segment is proportional to the amount of data in that category or subcategory, as detailed in the legend.</p><details><summary>read the caption</summary>Figure 5: Distribution of all our training data. The inner ring represents major categories, each depicted in a different color. The outer ring corresponds to the subcategories within each major category. The size of each segment is proportional to the amount of data, as indicated in the legend, where the data volume for each subcategory is also provided.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2411.14522/x14.png alt></figure></p><blockquote><p>üîº The figure illustrates the three-stage training process for the GMAI-VL model. Stage 1 (Shallow Alignment) involves freezing the language model and vision encoder while training only the projector to establish an initial alignment between images and text. Stage 2 (Deep Alignment) unfreezes the vision encoder and continues training the projector to enhance the alignment. Finally, Stage 3 (Instruction Tuning) fine-tunes the entire model using instruction-following data to improve its ability to understand and respond to various instructions.</p><details><summary>read the caption</summary>Figure 6: Diagram of the three-stage training process.</details></blockquote></details><details><summary>More on tables</summary><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Model</th><th>VQA-RAD</th><th>SLAKE</th><th>PMC-VQA</th><th>Avg.</th></tr></thead><tbody><tr><td>Med-Flamingo [54]</td><td>45.4</td><td>43.5</td><td>23.3</td><td>37.4</td></tr><tr><td>RadFM [70]</td><td>50.6</td><td>34.6</td><td>25.9</td><td>37.0</td></tr><tr><td>LLAVA-Med-7B [39]</td><td>51.4</td><td>48.6</td><td>24.7</td><td>41.6</td></tr><tr><td>Qwen-VL-Chat [6]</td><td>47.0</td><td>56.0</td><td>36.6</td><td>46.5</td></tr><tr><td>Yi-VL-34B [77]</td><td>53.0</td><td>58.9</td><td>39.5</td><td>50.5</td></tr><tr><td>LLAVA-v1.6-7B [46]</td><td>52.6</td><td>57.9</td><td>35.5</td><td>48.7</td></tr><tr><td>LLAVA-v1.6-13B [46]</td><td>55.8</td><td>58.9</td><td>36.6</td><td>50.8</td></tr><tr><td>LLAVA-v1.6-34B [46]</td><td>58.6</td><td>67.3</td><td>44.4</td><td>56.8</td></tr><tr><td>HuatuoGPT-Vision-7B [14]</td><td>63.8</td><td>74.5</td><td>52.7</td><td><strong>63.7</strong></td></tr><tr><td>GMAI-VL(w/o our data)</td><td>62.3</td><td>66.3</td><td>39.0</td><td>55.9</td></tr><tr><td><strong>GMAI-VL(ours)</strong></td><td><strong>66.3</strong></td><td><strong>72.9</strong></td><td><strong>54.3</strong></td><td><strong>64.5</strong></td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents a comparison of various Vision-Language Models (VLMs) on three established medical Visual Question Answering (VQA) benchmark datasets: VQA-RAD, SLAKE, and PMC-VQA. Each dataset focuses on different aspects of medical image understanding and question answering. The table displays the performance (accuracy) of each VLM on each benchmark dataset. The highest accuracy score in each column (dataset) is highlighted in red, and the second-highest score is highlighted in blue, making it easy to identify top-performing models for each specific VQA task.</p><details><summary>read the caption</summary>Table 2: Results on Traditional Medical VQA Benchmarks. The highest performance in each column is highlighted in red, and the second-highest performance is highlighted in blue.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Model</th><th>MR</th><th>AI</th><th>DD</th><th>LG</th><th>OBA</th><th>Overall</th></tr></thead><tbody><tr><td>Random Guess</td><td>25.00</td><td>25.84</td><td>28.41</td><td>25.40</td><td>37.49</td><td>28.28</td></tr><tr><td>Open-Source LLMs</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>MiniGPT-4 [81]</td><td>36.98</td><td>32.68</td><td>24.19</td><td>20.45</td><td>26.14</td><td>27.59</td></tr><tr><td>LLaVA [45]</td><td>52.30</td><td>35.27</td><td>11.80</td><td>9.77</td><td>24.70</td><td>22.86</td></tr><tr><td>LLaMA_Adapter_v2 [27]</td><td>58.45</td><td>38.18</td><td>29.12</td><td>23.73</td><td>30.97</td><td>35.08</td></tr><tr><td>InstructBLIP [20]</td><td>72.35</td><td>39.90</td><td>32.01</td><td>43.80</td><td>47.91</td><td>41.14</td></tr><tr><td>BLIP-2 [40]</td><td>57.48</td><td>49.83</td><td>46.21</td><td>30.52</td><td>73.52</td><td>50.77</td></tr><tr><td>Qwen-VL-Chat [6]</td><td>33.69</td><td>10.95</td><td>16.27</td><td>6.71</td><td>41.68</td><td>20.29</td></tr><tr><td>mPLUG-Owl2 [76]</td><td>78.01</td><td>48.52</td><td>39.68</td><td>20.56</td><td>59.36</td><td>48.44</td></tr><tr><td>LLaVa-NeXT [46]</td><td>68.23</td><td>46.74</td><td>41.21</td><td>18.43</td><td>39.57</td><td>45.57</td></tr><tr><td>DeepSeek-VL [49]</td><td>74.01</td><td>51.94</td><td>45.46</td><td>21.06</td><td>29.04</td><td>48.76</td></tr><tr><td>Yi-VL [77]</td><td>59.56</td><td>44.81</td><td>48.97</td><td>32.93</td><td>24.63</td><td>47.28</td></tr><tr><td>InternVL2-40B [18]</td><td>96.76</td><td>64.25</td><td>76.28</td><td>76.50</td><td>76.27</td><td>78.70</td></tr><tr><td>Medical Special Model</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>MedVInT-TE [80]</td><td>62.62</td><td>41.03</td><td>40.57</td><td>12.17</td><td>45.17</td><td>43.83</td></tr><tr><td>LLaVA-Med [39]</td><td>48.41</td><td>27.96</td><td>23.72</td><td>16.10</td><td>21.94</td><td>27.82</td></tr><tr><td>Med-Flamingo [54]</td><td>26.74</td><td>25.10</td><td>23.80</td><td>28.04</td><td>16.26</td><td>23.82</td></tr><tr><td>RadFM [70]</td><td>27.45</td><td>21.65</td><td>23.75</td><td>16.94</td><td>20.05</td><td>23.48</td></tr><tr><td>MedDr [28]</td><td>91.37</td><td>51.62</td><td>65.56</td><td>73.18</td><td>74.52</td><td>68.27</td></tr><tr><td>HuatuoGPT-Vision-34B [14]</td><td>95.06</td><td>75.67</td><td>66.51</td><td>72.83</td><td>74.92</td><td>73.23</td></tr><tr><td>Our Model</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>GMAI-VL(w/o our data)</td><td>96.40</td><td>80.97</td><td>79.14</td><td>70.29</td><td>75.66</td><td>79.96</td></tr><tr><td>GMAI-VL(ours)</td><td>98.64</td><td>92.95</td><td>88.7</td><td>87.21</td><td>82.95</td><td>88.48</td></tr></tbody></table></table></figure><blockquote><p>üîº This table compares the performance of various large vision-language models (LVLMs), including the GMAI-VL model, on the OmniMedVQA benchmark dataset. The comparison is broken down by five different question types within the benchmark: Modality Recognition (MR), Anatomy Identification (AI), Disease Diagnosis (DD), Lesion Grading (LG), and Other Biological Attributes (OBA). The table shows the accuracy of each model for each question type, highlighting the top-performing model in red and the second-best in blue. This allows for a detailed assessment of each model&rsquo;s strengths and weaknesses across different medical image analysis tasks.</p><details><summary>read the caption</summary>Table 3: Comparison of performance between representative LVLMs and GMAI-VL on OmniMedVQA across five different question type. The best performance in each column is highlighted in red, and the second-best performance is highlighted in blue. Abbreviations: MR = Modality Recognition, AI = Anatomy Identification, DD = Disease Diagnosis, LG = Lesion Grading, OBA = Other Biological Attributes.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Model Name</th><th>Overall (val)</th><th>Overall (test)</th><th>AR</th><th>BVR</th><th>B</th><th>CR</th><th>C</th><th>DD</th><th>IQG</th><th>MR</th><th>M</th><th>NT</th><th>OR-A</th><th>OR-HN</th><th>OR-P</th><th>OR-T</th><th>SG</th><th>SAR</th><th>SIR</th><th>SWR</th></tr></thead><tbody><tr><td>Random Guess</td><td></td><td></td><td></td><td></td><td></td><td></td><td>25.70</td><td>25.94</td><td>38.20</td><td>22.73</td><td>22.92</td><td>22.72</td><td>24.06</td><td>26.66</td><td>27.13</td><td>27.00</td><td>20.00</td><td>24.75</td><td>21.37</td><td>22.93</td></tr><tr><td>Open-Source LVLMs</td><td></td><td></td><td></td><td></td><td></td><td></td><td>25.58</td><td>26.34</td><td>37.74</td><td>21.50</td><td>20.62</td><td>22.00</td><td>22.41</td><td>27.29</td><td>25.91</td><td>27.45</td><td>18.00</td><td>28.79</td><td>25.16</td><td>22.13</td></tr><tr><td>Flamingo v2 [4]</td><td></td><td></td><td></td><td></td><td></td><td></td><td>29.58</td><td>30.45</td><td>40.16</td><td>33.92</td><td>24.92</td><td>25.22</td><td>24.21</td><td>32.99</td><td>29.96</td><td>29.53</td><td>21.20</td><td>37.88</td><td>30.32</td><td>24.80</td></tr><tr><td>VisualGLM-6B [22]</td><td></td><td></td><td></td><td></td><td></td><td></td><td>31.80</td><td>30.95</td><td>42.12</td><td>26.92</td><td>24.92</td><td>28.09</td><td>21.65</td><td>34.58</td><td>31.58</td><td>29.23</td><td>22.40</td><td>30.30</td><td>28.95</td><td>27.47</td></tr><tr><td>InstructBLIP-7B [20]</td><td></td><td></td><td></td><td></td><td></td><td></td><td>34.80</td><td>36.05</td><td>37.05</td><td>37.24</td><td>35.85</td><td>28.98</td><td>24.81</td><td>43.60</td><td>24.70</td><td>30.12</td><td>19.20</td><td>44.44</td><td>29.68</td><td>31.87</td></tr><tr><td>Qwen-VL [6]</td><td></td><td></td><td></td><td></td><td></td><td></td><td>34.82</td><td>34.31</td><td>41.66</td><td>39.16</td><td>26.62</td><td>30.23</td><td>31.88</td><td>38.01</td><td>26.72</td><td>24.93</td><td>25.20</td><td>37.37</td><td>29.58</td><td>31.20</td></tr><tr><td>Yi-VL-6B [77]</td><td></td><td></td><td></td><td></td><td></td><td></td><td>36.71</td><td>36.70</td><td>43.96</td><td>37.59</td><td>21.54</td><td>37.57</td><td>18.80</td><td>43.26</td><td>32.39</td><td>27.30</td><td>22.80</td><td>43.43</td><td>29.47</td><td>37.33</td></tr><tr><td>ShareGPT4V-7B [15]</td><td></td><td></td><td></td><td></td><td></td><td></td><td>38.23</td><td>37.96</td><td>45.45</td><td>34.27</td><td>30.92</td><td>41.32</td><td>21.65</td><td>44.68</td><td>34.01</td><td>27.74</td><td>23.60</td><td>43.43</td><td>28.00</td><td>42.13</td></tr><tr><td>LLAVA-V1.5-7B [45]</td><td></td><td></td><td></td><td></td><td></td><td></td><td>38.68</td><td>39.20</td><td>41.89</td><td>37.59</td><td>33.69</td><td>40.79</td><td>22.26</td><td>45.87</td><td>36.44</td><td>32.94</td><td>27.20</td><td>58.59</td><td>26.11</td><td>36.40</td></tr><tr><td>XComposer2 [24]</td><td></td><td></td><td></td><td></td><td></td><td></td><td>38.71</td><td>39.11</td><td>36.36</td><td>36.54</td><td>32.62</td><td>38.10</td><td>30.68</td><td>46.53</td><td>34.82</td><td>28.19</td><td>25.20</td><td>48.99</td><td>28.11</td><td>40.53</td></tr><tr><td>LLAVA-InternLM-7b [19]</td><td></td><td></td><td></td><td></td><td></td><td></td><td>38.86</td><td>39.73</td><td>43.84</td><td>44.58</td><td>34.00</td><td>33.99</td><td>31.28</td><td>45.59</td><td>33.20</td><td>38.28</td><td>32.40</td><td>42.42</td><td>31.89</td><td>42.80</td></tr><tr><td>InternVL-Chat-V1.5 [18]</td><td></td><td></td><td></td><td></td><td></td><td></td><td>39.52</td><td>40.01</td><td>41.66</td><td>44.06</td><td>27.38</td><td>38.46</td><td>34.29</td><td>46.99</td><td>33.60</td><td>34.42</td><td>21.20</td><td>47.98</td><td>30.63</td><td>42.80</td></tr><tr><td>InternVL-Chat-V1.2 [17]</td><td></td><td></td><td></td><td></td><td></td><td></td><td>40.07</td><td>40.45</td><td>39.82</td><td>37.94</td><td>30.62</td><td>35.24</td><td>29.77</td><td>48.97</td><td>34.01</td><td>25.96</td><td>20.80</td><td>53.03</td><td>30.95</td><td>42.67</td></tr><tr><td>LLAVA-InternLM2-7b [19]</td><td></td><td></td><td></td><td></td><td></td><td></td><td>41.73</td><td>43.43</td><td>38.43</td><td>47.03</td><td>42.31</td><td>37.03</td><td>26.47</td><td>51.11</td><td>33.20</td><td>31.16</td><td>26.00</td><td>44.95</td><td>36.00</td><td>58.13</td></tr><tr><td>DeepSeek-VL-7B [49]</td><td></td><td></td><td></td><td></td><td></td><td></td><td>41.79</td><td>42.54</td><td>40.74</td><td>43.01</td><td>36.46</td><td>37.57</td><td>27.82</td><td>51.08</td><td>28.74</td><td>29.08</td><td>26.80</td><td>47.47</td><td>37.05</td><td>46.40</td></tr><tr><td>MiniCPM-V2 [73]</td><td></td><td></td><td></td><td></td><td></td><td></td><td>Proprietary LVLMs</td><td></td><td></td><td></td><td></td><td></td><td></td><td>32.37</td><td>32.44</td><td>1.61</td><td>39.51</td><td>34.31</td><td>31.66</td><td>12.63</td></tr><tr><td>Claude3-Opus [2]</td><td></td><td></td><td></td><td></td><td></td><td></td><td>41.34</td><td>42.16</td><td>32.68</td><td>44.58</td><td>31.38</td><td>40.79</td><td>10.68</td><td>50.53</td><td>32.79</td><td>44.36</td><td>29.20</td><td>51.52</td><td>41.37</td><td>58.00</td></tr><tr><td>Qwen-VL-Max [6]</td><td></td><td></td><td></td><td></td><td></td><td></td><td>42.50</td><td>44.08</td><td>29.92</td><td>48.95</td><td>44.00</td><td>37.39</td><td>12.93</td><td>52.88</td><td>32.79</td><td>44.21</td><td>32.80</td><td>63.64</td><td>39.89</td><td>54.13</td></tr><tr><td>GPT-4V [1]</td><td></td><td></td><td></td><td></td><td></td><td></td><td>44.38</td><td>44.93</td><td>42.12</td><td>45.10</td><td>46.46</td><td>37.57</td><td>20.45</td><td>53.29</td><td>35.22</td><td>36.94</td><td>25.20</td><td>51.01</td><td>34.74</td><td>59.60</td></tr><tr><td>Gemini 1.0 [62]</td><td></td><td></td><td></td><td></td><td></td><td></td><td>47.42</td><td>48.36</td><td>43.50</td><td>56.12</td><td>51.23</td><td>47.58</td><td>2.26</td><td>55.33</td><td>38.87</td><td>48.07</td><td>30.00</td><td>76.26</td><td>51.05</td><td>75.87</td></tr><tr><td>Gemini 1.5 [56]</td><td></td><td></td><td></td><td></td><td></td><td></td><td>53.53</td><td>53.96</td><td>38.32</td><td>61.01</td><td>57.08</td><td>49.02</td><td>46.62</td><td>61.45</td><td>46.56</td><td>56.38</td><td>34.00</td><td>75.25</td><td>53.79</td><td>69.47</td></tr><tr><td>GPT-4o [1]</td><td></td><td></td><td></td><td></td><td></td><td></td><td>Medical Special Model</td><td></td><td></td><td></td><td></td><td></td><td></td><td>12.74</td><td>11.64</td><td>6.67</td><td>10.14</td><td>9.23</td><td>11.27</td><td>6.62</td></tr><tr><td>Med-Flamingo [54]</td><td></td><td></td><td></td><td></td><td></td><td></td><td>20.54</td><td>19.60</td><td>24.51</td><td>17.83</td><td>17.08</td><td>19.86</td><td>15.04</td><td>19.81</td><td>20.24</td><td>21.51</td><td>13.20</td><td>15.15</td><td>20.42</td><td>23.73</td></tr><tr><td>LLaVA-Med [39]</td><td></td><td></td><td></td><td></td><td></td><td></td><td>22.34</td><td>22.06</td><td>29.57</td><td>19.41</td><td>16.46</td><td>23.79</td><td>15.79</td><td>24.19</td><td>21.86</td><td>16.62</td><td>7.20</td><td>13.64</td><td>24.00</td><td>14.67</td></tr><tr><td>Qilin-Med-VL-Chat [48]</td><td></td><td></td><td></td><td></td><td></td><td></td><td>22.95</td><td>22.93</td><td>27.16</td><td>20.63</td><td>13.23</td><td>19.14</td><td>20.45</td><td>24.51</td><td>23.48</td><td>22.85</td><td>15.60</td><td>16.16</td><td>14.32</td><td>24.93</td></tr><tr><td>RadFM [70]</td><td></td><td></td><td></td><td></td><td></td><td></td><td>41.95</td><td>43.69</td><td>41.20</td><td>50.70</td><td>37.85</td><td>29.87</td><td>28.27</td><td>52.53</td><td>36.03</td><td>31.45</td><td>29.60</td><td>47.47</td><td>33.37</td><td>51.33</td></tr><tr><td>MedDr [28]</td><td></td><td></td><td></td><td></td><td></td><td></td><td>Our Model</td><td></td><td></td><td></td><td></td><td></td><td></td><td>54.99</td><td>56.23</td><td>51.26</td><td>61.05</td><td>53.79</td><td>44.39</td><td>44.51</td></tr><tr><td>GMAI-VL(w/o our data)</td><td></td><td></td><td></td><td></td><td></td><td></td><td>61.74</td><td>62.43</td><td>75.26</td><td>59.66</td><td>67.24</td><td>56.86</td><td>54.29</td><td>67.14</td><td>42.80</td><td>79.97</td><td>41.60</td><td>75.00</td><td>60.45</td><td>75.48</td></tr><tr><td>GMAI-VL(ours)</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></tbody></table></table></figure><blockquote><p>üîº Table 4 presents a comprehensive evaluation of various vision-language models on the GMAI-MMBench benchmark, specifically focusing on clinical Visual Question Answering (VQA) tasks. The table details the performance of each model across multiple subtasks within the benchmark. The best-performing model for each subtask is highlighted in red, while the second-best is highlighted in blue. To understand the specific tasks evaluated, refer to Table 5 in the referenced literature [16].</p><details><summary>read the caption</summary>Table 4: Results on the val and test sets of GMAI-MMBench for clinical VQA tasks. The full names of the evaluated tasks can be found in Table 5 in literature¬†[16]. The best model in each category is highlighted in red, while the second-best model is indicated in blue.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Model</th><th>BMS</th><th>CM</th><th>DLM</th><th>P</th><th>PH</th><th>MMMU (Health & Medicine)</th></tr></thead><tbody><tr><td>Med-Flamingo [54]</td><td>33.6</td><td>30.2</td><td>23.3</td><td>29.3</td><td>25.8</td><td>28.4</td></tr><tr><td>RadFM [70]</td><td>31.6</td><td>28.6</td><td>26.7</td><td>26.2</td><td>26.8</td><td>27.9</td></tr><tr><td>LLaVA-Med-7B [39]</td><td>33.8</td><td>32.3</td><td>26.7</td><td>40.7</td><td>43.3</td><td>38.6</td></tr><tr><td>Qwen-VL-Chat [6]</td><td>32.7</td><td>20.6</td><td>19.3</td><td>29.6</td><td>33.3</td><td>31.7</td></tr><tr><td>Yi-VL-34B [77]</td><td>48.1</td><td>55.6</td><td>36.7</td><td>35.4</td><td>31.3</td><td>48.2</td></tr><tr><td>LLaVA-v1.6-7B [45]</td><td>46.4</td><td>43.4</td><td>30.0</td><td>29.6</td><td>26.7</td><td>33.1</td></tr><tr><td>LLaVA-v1.6-13B [45]</td><td>53.6</td><td>46.7</td><td>33.3</td><td>22.2</td><td>40.0</td><td>39.3</td></tr><tr><td>HuatouGPT-Vision-7B [14]</td><td>50.0</td><td>63.3</td><td>36.7</td><td>48.1</td><td>53.3</td><td>50.3</td></tr><tr><td><strong>GMAI-VL(w/o our data)</strong></td><td>43.3</td><td>56.7</td><td>43.3</td><td>46.7</td><td>40.0</td><td>46.0</td></tr><tr><td><strong>GMAI-VL(ours)</strong></td><td>50.0</td><td>60.0</td><td>43.3</td><td>50.0</td><td>53.3</td><td>51.3</td></tr></tbody></table></table></figure><blockquote><p>üîº Table 5 presents the performance of various vision-language models on the MMMU Health & Medicine benchmark&rsquo;s validation set. The benchmark is broken down into five categories: Basic Medical Science (BMS), Clinical Medicine (CM), Diagnostics and Laboratory Medicine (DLM), Pharmacy (P), and Public Health (PH). The table shows each model&rsquo;s score for each category, with the top-performing model in each category highlighted in red and the second-best in blue. This allows comparison of the models&rsquo; performance across different medical domains.</p><details><summary>read the caption</summary>Table 5: Performance on the val set for the MMMU Health & Medicine track. This track is divided into five categories: BMS (Basic Medical Science), CM (Clinical Medicine), DLM (Diagnostics and Laboratory Medicine), P (Pharmacy), and PH (Public Health). The best performance in each column is highlighted in red, and the second-best performance is highlighted in blue.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Dataset</th><th>Sub-Dataset Name</th><th>Description</th><th>Size</th></tr></thead><tbody><tr><td>GMAI-VL-5.5M</td><td>GMAI-MM-Caption-1.7M</td><td>A curated set of detailed medical image captions.</td><td>1.7M</td></tr><tr><td></td><td>GMAI-MM-Instrunct-0.9M</td><td>A diverse set of instructions for medical image analysis.</td><td>0.9M</td></tr><tr><td></td><td>GMAI-MM-Percept-1.3M</td><td>A dataset of labels for medical image classification and segmentation.</td><td>1.3M</td></tr><tr><td></td><td>GMAI-Text-Single-1M</td><td>A set of single-round medical dialogues on patient queries</td><td>1.0M</td></tr><tr><td></td><td>GMAI-Text-Multi-0.6M</td><td>A dataset of multi-turn medical conversations on various topics.</td><td>0.6M</td></tr></tbody></table></table></figure><blockquote><p>üîº Table 6 provides detailed information about the sub-datasets that comprise the GMAI-VL-5.5M multimodal medical dataset. It lists each sub-dataset&rsquo;s name, a brief description of its content (e.g., image captions, instructions for image analysis, medical dialogues), and the total size of the dataset. This breakdown is essential for understanding the composition and scope of the GMAI-VL-5.5M dataset, clarifying the various types of data included and their relative proportions, which are crucial for assessing the dataset&rsquo;s suitability and effectiveness for training vision-language models in the medical domain.</p><details><summary>read the caption</summary>Table 6: Sub-Dataset Details for GMAI-VL-5.5M</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Dataset Category</th><th>Dataset Name</th><th>Size</th><th>ratio in stage 1&amp;2</th><th>ratio in stage 3</th></tr></thead><tbody><tr><td>General</td><td>Captioning</td><td>ALLaVA<sup>[13]</sup></td><td>468k</td><td>100.0%</td></tr><tr><td></td><td></td><td>ShareGPT4V<sup>[15]</sup></td><td>102k</td><td>100.0%</td></tr><tr><td>Medical</td><td>Captioning</td><td>GMAI-MM-Caption-1.7M</td><td>1.7M</td><td>100.0%</td></tr><tr><td></td><td></td><td>PubMedVision<sup>[14]</sup></td><td>1.3M</td><td>100.0%</td></tr><tr><td></td><td></td><td>MedICaT<sup>[60]</sup></td><td>173k</td><td>100.0%</td></tr><tr><td></td><td></td><td>MPx-Single<sup>[70]</sup></td><td>31k</td><td>100.0%</td></tr><tr><td></td><td></td><td>PMC-OA<sup>[43]</sup></td><td>1.3M</td><td>100.0%</td></tr><tr><td></td><td></td><td>QUILT-1M<sup>[32]</sup></td><td>643k</td><td>100.0%</td></tr><tr><td></td><td></td><td>Retina Image Bank<sup>[3]</sup></td><td>22k</td><td>100.0%</td></tr><tr><td>Report</td><td>Generation</td><td>CheXpertPlus<sup>[12]</sup></td><td>223k</td><td>100.0%</td></tr><tr><td></td><td></td><td>MIMIC-CXR<sup>[33]</sup></td><td>486k</td><td>100.0%</td></tr><tr><td></td><td></td><td>OpenI<sup>[21]</sup></td><td>7k</td><td>100.0%</td></tr><tr><td>General</td><td>Instruction</td><td>GeoQA+<sup>[11]</sup></td><td>72k</td><td>100.0%</td></tr><tr><td></td><td></td><td>AI2D<sup>[36]</sup></td><td>12k</td><td>100.0%</td></tr><tr><td></td><td></td><td>SynthDoG<sup>[37]</sup></td><td>29k</td><td>100.0%</td></tr><tr><td></td><td></td><td>ChartQA<sup>[51]</sup></td><td>18k</td><td>100.0%</td></tr><tr><td></td><td></td><td>MMChemExam<sup>[42]</sup></td><td>219k</td><td>100.0%</td></tr><tr><td></td><td></td><td>LLaVA-Instruct-150K<sup>[45]</sup></td><td>157k</td><td>100.0%</td></tr><tr><td></td><td></td><td>DVQA<sup>[34]</sup></td><td>200k</td><td>100.0%</td></tr><tr><td></td><td></td><td>DocVQA<sup>[52]</sup></td><td>10k</td><td>100.0%</td></tr><tr><td>Medical</td><td>Instruction</td><td>GMAI-MM-Percept-1.3M</td><td>1.3M</td><td>100.0%</td></tr><tr><td></td><td></td><td>GMAI-MM-Instruct-0.9M</td><td>0.9M</td><td>100.0%</td></tr><tr><td></td><td></td><td>PubMedVision<sup>[14]</sup></td><td>1.28M</td><td>100.0%</td></tr><tr><td></td><td></td><td>LLaVA-Med-60k<sup>[39]</sup></td><td>56k</td><td>100.0%</td></tr><tr><td></td><td></td><td>PMC-Inline<sup>[70]</sup></td><td>288k</td><td>100.0%</td></tr><tr><td></td><td></td><td>VQA-Med-2019<sup>[8]</sup></td><td>3.2k</td><td>100.0%</td></tr><tr><td></td><td></td><td>Medical-Diff-VQA<sup>[30]</sup></td><td>260k</td><td>100.0%</td></tr><tr><td></td><td></td><td>PathVQA<sup>[29]</sup></td><td>2.6k</td><td>100.0%</td></tr><tr><td></td><td></td><td>PMC-CaseReport<sup>[70]</sup></td><td>109k</td><td>100.0%</td></tr><tr><td></td><td></td><td>PMC-VQA<sup>[80]</sup></td><td>251k</td><td>100.0%</td></tr><tr><td></td><td></td><td>ROCOV2<sup>[57]</sup></td><td>60k</td><td>100.0%</td></tr><tr><td></td><td></td><td>SLAKE<sup>[44]</sup></td><td>0.6k</td><td>100.0%</td></tr><tr><td></td><td></td><td>VQA-RAD<sup>[38]</sup></td><td>0.3k</td><td>100.0%</td></tr><tr><td>General Text</td><td></td><td>blossom_orca<sup>[5]</sup></td><td>20k</td><td>0.0%</td></tr><tr><td></td><td></td><td>COIG-CQIA<sup>[7]</sup></td><td>14.8k</td><td>0.0%</td></tr><tr><td></td><td></td><td>Cosmopedia-100k<sup>[9]</sup></td><td>33k</td><td>0.0%</td></tr><tr><td></td><td></td><td>ShareGPT4V<sup>[15]</sup></td><td>26k</td><td>0.0%</td></tr><tr><td></td><td></td><td>Orca-Math<sup>[53]</sup></td><td>379k</td><td>0.0%</td></tr><tr><td></td><td></td><td>Leetcode<sup>[10]</sup></td><td>1.7k</td><td>0.0%</td></tr><tr><td></td><td></td><td>LogiQA<sup>[47]</sup></td><td>12.7k</td><td>0.0%</td></tr><tr><td></td><td></td><td>Lima<sup>[26]</sup></td><td>83k</td><td>0.0%</td></tr><tr><td></td><td></td><td>Open Hermes 2.5<sup>[64]</sup></td><td>200k</td><td>0.0%</td></tr><tr><td></td><td></td><td>Firefly<sup>[74]</sup></td><td>189k</td><td>0.0%</td></tr><tr><td></td><td></td><td>UltraChat<sup>[23]</sup></td><td>189k</td><td>0.0%</td></tr><tr><td></td><td></td><td>Alpaca-Instruct-52K<sup>[61]</sup></td><td>49k</td><td>0.0%</td></tr><tr><td>Medical Text</td><td></td><td>GMAI-Text-Single-1M</td><td>1.0M</td><td>0.0%</td></tr><tr><td></td><td></td><td>GMAI-Text-Multi-0.6M</td><td>649k</td><td>0.0%</td></tr><tr><td>Overall</td><td></td><td>-</td><td>15.7M</td><td>-</td></tr></tbody></table></table></figure><blockquote><p>üîº Table 7 details the composition of the datasets used to train the GMAI-VL model. It breaks down the datasets by category (Captioning, Report Generation, Instruction, Text), listing each dataset&rsquo;s name, size, and the percentage of the dataset used in training stages 1&amp;2 and stage 3. This provides insight into the model&rsquo;s training methodology and the relative importance of different data sources.</p><details><summary>read the caption</summary>Table 7: List of datasets used in our model. We employ a large collection of image-text data and instruction data for training stage.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Settings</th><th>Stage I</th><th>Stage II</th><th>Stage III</th></tr></thead><tbody><tr><td>freeze LLM</td><td>True</td><td>True</td><td>False</td></tr><tr><td>freeze MLP</td><td>False</td><td>False</td><td>False</td></tr><tr><td>freeze Vision Encoder</td><td>True</td><td>False</td><td>False</td></tr><tr><td>packing type</td><td>soft packing</td><td>soft packing</td><td>soft packing</td></tr><tr><td>learning rate</td><td>1e-3</td><td>1e-4</td><td>1e-5</td></tr><tr><td>learning rate schedule</td><td>cosine decay</td><td>cosine decay</td><td>cosine decay</td></tr><tr><td>optimizer</td><td>AdamW</td><td>AdamW</td><td>AdamW</td></tr><tr><td>optimizer hyper-parameters</td><td>(\beta_{1}=0.9,\beta_{2}=0.999)</td><td>(\beta_{1}=0.9,\beta_{2}=0.999)</td><td>(\beta_{1}=0.9,\beta_{2}=0.999)</td></tr><tr><td>input size</td><td>336x336</td><td>336x336</td><td>336x336</td></tr><tr><td>total batch size</td><td>32x8x2</td><td>32x4x4</td><td>32x4x4</td></tr><tr><td>drop rate</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>numerical precision</td><td>DeepSpeed bf16</td><td>DeepSpeed bf16</td><td>DeepSpeed bf16</td></tr><tr><td>GPUs for training</td><td>32xA100 (80G)</td><td>32xA100 (80G)</td><td>32xA100 (80G)</td></tr></tbody></table></table></figure><blockquote><p>üîº This table details the hyperparameters and training settings used for each of the three stages in the training of the GMAI-VL model. It specifies whether components like the Language Model (LLM), Multilayer Perceptron (MLP), and Vision Encoder were frozen or trained, the type of data packing used, the learning rate and its decay schedule, the optimizer employed, and other relevant parameters like batch size and input image dimensions. It also indicates the number of GPUs and the precision used for training.</p><details><summary>read the caption</summary>Table 8: Training settings of GMAI-VL‚Äôs stage I, stage II, and stage III.</details></blockquote></details><h3 class="relative group">Full paper<div id=full-paper class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#full-paper aria-label=Anchor>#</a></span></h3><div id=gallery-a64127ca2bee47daef9d99bbaa476c3b class=gallery><img src=https://ai-paper-reviewer.com/2411.14522/1.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.14522/2.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.14522/3.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.14522/4.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.14522/5.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.14522/6.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.14522/7.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.14522/8.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.14522/9.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.14522/10.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.14522/11.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.14522/12.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.14522/13.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.14522/14.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.14522/15.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.14522/16.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.14522/17.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.14522/18.png class="grid-w50 md:grid-w33 xl:grid-w25"></div></div><section class="flex flex-row flex-wrap justify-center pt-4 text-xl"><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14522/&amp;title=GMAI-VL%20&%20GMAI-VL-5.5M:%20A%20Large%20Vision-Language%20Model%20and%20A%20Comprehensive%20Multimodal%20Dataset%20Towards%20General%20Medical%20AI" title="Share on LinkedIn" aria-label="Share on LinkedIn"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://twitter.com/intent/tweet/?url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14522/&amp;text=GMAI-VL%20&%20GMAI-VL-5.5M:%20A%20Large%20Vision-Language%20Model%20and%20A%20Comprehensive%20Multimodal%20Dataset%20Towards%20General%20Medical%20AI" title="Tweet on Twitter" aria-label="Tweet on Twitter"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="mailto:?body=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14522/&amp;subject=GMAI-VL%20&%20GMAI-VL-5.5M:%20A%20Large%20Vision-Language%20Model%20and%20A%20Comprehensive%20Multimodal%20Dataset%20Towards%20General%20Medical%20AI" title="Send via email" aria-label="Send via email"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1-27.64 140.9 68.65 266.2 199.1 285.1 19.01 2.888 36.17-12.26 36.17-31.49l1e-4-.6631c0-15.74-11.44-28.88-26.84-31.24-84.35-12.98-149.2-86.13-149.2-174.2.0-102.9 88.61-185.5 193.4-175.4 91.54 8.869 158.6 91.25 158.6 183.2v16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98.0036c-7.299.0-13.2 4.992-15.12 11.68-24.85-12.15-54.24-16.38-86.06-5.106-38.75 13.73-68.12 48.91-73.72 89.64-9.483 69.01 43.81 128 110.9 128 26.44.0 50.43-9.544 69.59-24.88 24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3c0-149.2-133.9-265.632-287.3-235.57zM239.1 304.3c-26.47.0-48-21.56-48-48.05s21.53-48.05 48-48.05 48 21.56 48 48.05-20.6 48.05-48 48.05z"/></svg></span></a></section></div><script>var oid="views_paper-reviews/2411.14522/index.md",oid_likes="likes_paper-reviews/2411.14522/index.md"</script><script type=text/javascript src=/ai-paper-reviewer/js/page.min.0860cf4e04fa2d72cc33ddba263083464d48f67de06114529043cb4623319efed4f484fd7f1730df5abea0e2da6f3538855634081d02f2d6e920b956f063e823.js integrity="sha512-CGDPTgT6LXLMM926JjCDRk1I9n3gYRRSkEPLRiMxnv7U9IT9fxcw31q+oOLabzU4hVY0CB0C8tbpILlW8GPoIw=="></script></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="flex group mr-3" href=/ai-paper-reviewer/paper-reviews/2411.14432/><span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&larr;</span>
<span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large Language Models</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2024-11-21T00:00:00+00:00>21 November 2024</time>
</span></span></a></span><span><a class="flex text-right group ml-3" href=/ai-paper-reviewer/paper-reviews/2411.14257/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Do I Know This Entity? Knowledge Awareness and Hallucinations in Language Models</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2024-11-21T00:00:00+00:00>21 November 2024</time>
</span></span><span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&larr;</span></a></span></div></div><div class=pt-3><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class=pt-3><script src=https://utteranc.es/client.js repo=pmnxis/pmnxis.github.io issue-term=pathname label=Comment theme=dark-blue crossorigin=anonymous async></script></div></div></footer></article><div id=top-scroller class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer id=site-footer class="py-10 print:hidden"><nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400"><ul class="flex flex-col list-none sm:flex-row"><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=/ai-paper-reviewer/tags/ title>Tags</a></li></ul></nav><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2024
AI Paper Reviews by AI</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a></p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/ai-paper-reviewer/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh+sCQ0E53ghYrxgYqw+0GCRyIEpA=="></script></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://deep-diver.github.io/ai-paper-reviewer/ style=z-index:500><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body><script data-name=BMC-Widget data-cfasync=false src=https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js data-id=chansung data-description="Support me on Buy me a coffee!" data-message data-color=#FFDD00 data-position=Left data-x_margin=18 data-y_margin=18></script></html>