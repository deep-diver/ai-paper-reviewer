[{"figure_path": "https://arxiv.org/html/2411.14522/x1.png", "caption": "Figure 1: Overview of GMAI-VL and GMAI-VL-5.5M. (a) illustrates the sources, departments, modalities, task types, and instruction formats of the GMAI-VL-5.5M dataset. (b) Architecture of GMAI-VL, integrating a Vision Encoder, Projector, and Large Language Model. (c) Three-stage training process of GMAI-VL, including shallow alignment, deep alignment, and instruction tuning with corresponding data sizes and training components. The flame symbol\n denotes the training part, while the snowflake symbol\n\nindicates frozen part.", "description": "Figure 1 provides a comprehensive overview of the GMAI-VL model and its associated dataset, GMAI-VL-5.5M.  Panel (a) details the sources and composition of the GMAI-VL-5.5M dataset, showing the various medical departments, imaging modalities, task types and instruction formats included.  Panel (b) illustrates the architecture of the GMAI-VL model itself, highlighting its three key components: a Vision Encoder for processing images, a Projector for converting image features into a format compatible with the language model, and a Large Language Model (LLM) for understanding and generating text.  Finally, panel (c) depicts the three-stage training process:  Stage 1 (shallow alignment) focuses on establishing basic image-text associations; Stage 2 (deep alignment) refines these associations, and Stage 3 (instruction tuning) fine-tunes the model on instruction-following tasks.  The diagram uses flame icons to show training components and snowflake icons to highlight frozen model parameters at each stage.", "section": "3. GMAI-VL-5.5M: A Comprehensive Multimodal Dataset"}, {"figure_path": "https://arxiv.org/html/2411.14522/x6.png", "caption": "Figure 2: \nThe prompt-driven data generation pipeline comparing without-annotation-guided and annotation-guided methods. The annotation-guided approach integrates specific annotation information (e.g., <image, modality, label, department, bbox [optional]>) to generate high-quality, accurate descriptions, while the without-annotation-guided approach often results in lower-quality outputs.\nFigure with complete prompt and response is provided in Supp. Mat..", "description": "This figure illustrates the comparison between two data generation methods: one without annotation guidance and the other with annotation guidance.  The without-annotation method uses only the image as input to a large language model (like GPT-4), resulting in descriptions that are often less accurate and detailed. The annotation-guided method includes the image and additional metadata like modality, label, department, and bounding box information in the prompt, resulting in higher-quality, more accurate, and complete descriptions.  The figure highlights the difference in output quality between these two approaches.", "section": "3. GMAI-VL-5.5M: A Comprehensive Multimodal Dataset"}, {"figure_path": "https://arxiv.org/html/2411.14522/x7.png", "caption": "Figure 3: The full version of Fig.\u00a02 in the main text illustrates the complete of data generation pipeline\uff0c comparing without-annotation-guided and annotation-guided methods. The annotation-guided approach integrates specific annotation information (e.g., <image, modality, label, department, bbox [optional]>) to generate high-quality, accurate descriptions, while the without-annotation-guided approach often results in lower-quality outputs.", "description": "Figure 3 expands on Figure 2, illustrating the data generation pipeline for creating high-quality image-text pairs for medical datasets.  It compares two methods: one using annotation guidance and another without. The figure shows how integrating specific annotations (image modality, label, department, bounding box) leads to more accurate and detailed descriptions compared to the unguided approach, which often produces lower-quality results.", "section": "3. GMAI-VL-5.5M: A Comprehensive Multimodal Dataset"}, {"figure_path": "https://arxiv.org/html/2411.14522/x8.png", "caption": "(a) (a) Modality distribution", "description": "The figure shows a pie chart visualizing the distribution of various medical imaging modalities within the GMAI-VL-5.5M dataset.  The modalities represented include common types such as CT, MRI, and X-ray, and less common types such as fundus photography, dermoscopy, microscopy, ultrasound, endoscopy, and PET scans.  The chart provides a quantitative breakdown of the proportion of each modality present in the dataset, illustrating the dataset's diversity in terms of imaging techniques used.", "section": "3. GMAI-VL-5.5M: A Comprehensive Multimodal Dataset"}, {"figure_path": "https://arxiv.org/html/2411.14522/x9.png", "caption": "(b) (b) Original task distribution", "description": "The figure shows a pie chart illustrating the distribution of original tasks within the GMAI-VL-5.5M dataset. The most frequent task is 2D classification (50.4%), followed by 3D segmentation (30.3%), 2D segmentation (12.7%), and 2D detection (6.6%).  This visualization highlights the variety of tasks covered by the dataset, showcasing its comprehensive nature for training multimodal medical vision-language models.", "section": "3. GMAI-VL-5.5M: A Comprehensive Multimodal Dataset"}, {"figure_path": "https://arxiv.org/html/2411.14522/x10.png", "caption": "(c) (c) Department distribution", "description": "This figure shows the distribution of medical data across different departments in the GMAI-VL-5.5M dataset.  It visually represents the percentage of data originating from various medical specialties, such as Pulmonary Medicine, General Surgery, Cardiology, and Dermatology. The size of each segment is proportional to the amount of data from that department, providing insights into the dataset's coverage of different clinical areas.", "section": "3. GMAI-VL-5.5M: A Comprehensive Multimodal Dataset"}, {"figure_path": "https://arxiv.org/html/2411.14522/x11.png", "caption": "(d) (d) Clinical task distribution", "description": "The figure shows the distribution of clinical tasks within the GMAI-VL-5.5M dataset.  It breaks down the percentage of different types of clinical tasks included in the dataset, such as disease diagnosis, organ recognition, and various other attribute recognitions.  This illustrates the dataset's comprehensiveness in covering a wide spectrum of medical tasks.", "section": "3. GMAI-VL-5.5M: A Comprehensive Multimodal Dataset"}, {"figure_path": "https://arxiv.org/html/2411.14522/x12.png", "caption": "Figure 4: \nDistribution of GMAI-VL-5.5M across tasks, modalities, departments, and clinical tasks.\n(a) Original Task Distribution: The dataset includes 2D Classification (50.4%), 3D Segmentation (30.3%), 2D Segmentation (12.7%), and 2D Detection (6.6%).\n(b) Modality Distribution: In addition to CT (26.8%) and MR (24.7%), X-ray (12.6%), Pathology (11.2%), and less common modalities like Dermoscopy (3.5%), Microscopy (2.4%), and PET (0.2%) are represented.\n(c) Department Distribution: While Orthopedic Surgery (12.9%) and General Surgery (10.3%) are the top contributors, departments like Endocrinology (1.3%), Infectious Diseases (0.8%), and Urology (0.7%) also provide data.\n(d) Clinical Task Distribution: Besides Disease Diagnosis (40.4%) and Organ Recognition (16.0%), tasks such as Muscle Recognition (3.3%), Nervous Tissue Recognition (1.5%), and Microorganism Recognition (1.2%) are included.\nNote: The distribution statistics shown here pertain only to the multimodal components of GMAI-VL-5.5M.", "description": "Figure 4 shows the distribution of data within the GMAI-VL-5.5M multimodal medical dataset across four key aspects: original tasks, imaging modalities, medical departments, and clinical tasks.  The dataset is diverse, covering various types of image analysis tasks (2D classification, 3D segmentation, 2D segmentation, 2D detection) and a wide range of modalities (CT, MRI, X-ray, pathology, dermoscopy, microscopy, PET).  The data comes from numerous medical departments, with orthopedic surgery and general surgery being the most represented, but also including less common departments such as endocrinology, infectious diseases and urology.  Clinical tasks are also varied,  including disease diagnosis and organ recognition, as well as more specialized tasks such as muscle, nervous tissue and microorganism recognition. Note that the statistics only reflect the multimodal portion of the dataset.", "section": "3. GMAI-VL-5.5M: A Comprehensive Multimodal Dataset"}, {"figure_path": "https://arxiv.org/html/2411.14522/x13.png", "caption": "Figure 5: Distribution of all our training data. The inner ring represents major categories, each depicted in a different color. The outer ring corresponds to the subcategories within each major category. The size of each segment is proportional to the amount of data, as indicated in the legend, where the data volume for each subcategory is also provided.", "description": "Figure 5 is a pie chart visualizing the distribution of the training data used for the GMAI-VL model. The chart is divided into several concentric rings. The innermost ring represents the main categories of datasets (Medical Caption, Medical Instruction, Medical Text, General Instruction, General Text, Report Generation), each shown in a distinct color. The next ring breaks down each main category into its subcategories. The outermost ring gives the specific name and the size of the datasets. The size of each segment is proportional to the amount of data in that category or subcategory, as detailed in the legend.", "section": "9. Training Data Overview"}, {"figure_path": "https://arxiv.org/html/2411.14522/x14.png", "caption": "Figure 6: Diagram of the three-stage training process.", "description": "The figure illustrates the three-stage training process for the GMAI-VL model. Stage 1 (Shallow Alignment) involves freezing the language model and vision encoder while training only the projector to establish an initial alignment between images and text. Stage 2 (Deep Alignment) unfreezes the vision encoder and continues training the projector to enhance the alignment. Finally, Stage 3 (Instruction Tuning) fine-tunes the entire model using instruction-following data to improve its ability to understand and respond to various instructions.", "section": "4. GMAI-VL: A General Medical Vision-Language Model"}]