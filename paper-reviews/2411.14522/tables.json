[{"content": "| Datasets | Data Size | Modality | Language | Traceability | Data Source |\n|---|---|---|---|---|---| \n| PathVQA [29] | 32.7k | Pathology | EN | \u00d7 | Textbooks |\n| MIMIC-CXR [33] | 227k | X-Ray | EN | \u2713 | Hospital |\n| quilt-1M [32] | 1M | Pathology | EN | \u00d7 | YouTube & PubMed |\n| MedDr VQA [28] | 197k | Multimodal | EN | \u2713 | 13 medical datasets |\n| PMC-OA [43] | 1.65M | Multimodal | EN | \u00d7 | PubMed |\n| PMC-VQA [80] | 413k | Multimodal | EN | \u00d7 | PubMed |\n| LLaVA-Med VQA [39] | 56,702 | Multimodal | EN | \u00d7 | PubMed |\n| ChiMed-VL [48] | 1.05M | Multimodal | CN | \u00d7 | PubMed |\n| PMC-CaseReport [70] | 438k | Multimodal | EN | \u00d7 | PubMed |\n| PubMedVision [14] | 1.29M | Multimodal | EN&CN | \u00d7 | PubMed |\n| **GMAI-VL-5.5M (ours)** | 5.5M | Multimodal | EN&CN | \u2713 | 219 specialized medical imaging datasets |", "caption": "Table 1: Comparison of various medical multimodal datasets, including details on the dataset size, modality type, language, data traceability, and sources of information.", "description": "This table compares various medical multimodal datasets across several key features.  It provides a comprehensive overview of each dataset's size (number of samples), the types of medical imaging modalities included (e.g., X-ray, pathology images), the languages used in the dataset annotations, whether the data sources are traceable, and finally, the original sources of the data. This allows for a direct comparison of the datasets' scope, quality, and suitability for various medical AI tasks.", "section": "3. GMAI-VL-5.5M: A Comprehensive Multimodal Dataset"}, {"content": "| Model | VQA-RAD | SLAKE | PMC-VQA | Avg. |\n|---|---|---|---|---|\n| Med-Flamingo [54] | 45.4 | 43.5 | 23.3 | 37.4 |\n| RadFM [70] | 50.6 | 34.6 | 25.9 | 37.0 |\n| LLAVA-Med-7B [39] | 51.4 | 48.6 | 24.7 | 41.6 |\n| Qwen-VL-Chat [6] | 47.0 | 56.0 | 36.6 | 46.5 |\n| Yi-VL-34B [77] | 53.0 | 58.9 | 39.5 | 50.5 |\n| LLAVA-v1.6-7B [46] | 52.6 | 57.9 | 35.5 | 48.7 |\n| LLAVA-v1.6-13B [46] | 55.8 | 58.9 | 36.6 | 50.8 |\n| LLAVA-v1.6-34B [46] | 58.6 | 67.3 | 44.4 | 56.8 |\n| HuatuoGPT-Vision-7B [14] | 63.8 | 74.5 | 52.7 | **63.7** |\n| GMAI-VL(w/o our data) | 62.3 | 66.3 | 39.0 | 55.9 |\n| **GMAI-VL(ours)** | **66.3** | **72.9** | **54.3** | **64.5** |", "caption": "Table 2: Results on Traditional Medical VQA Benchmarks. The highest performance in each column is highlighted in red, and the second-highest performance is highlighted in blue.", "description": "This table presents a comparison of various Vision-Language Models (VLMs) on three established medical Visual Question Answering (VQA) benchmark datasets: VQA-RAD, SLAKE, and PMC-VQA.  Each dataset focuses on different aspects of medical image understanding and question answering. The table displays the performance (accuracy) of each VLM on each benchmark dataset. The highest accuracy score in each column (dataset) is highlighted in red, and the second-highest score is highlighted in blue, making it easy to identify top-performing models for each specific VQA task.", "section": "5.1. Medical VQA Benchmarks"}, {"content": "| Model | MR | AI | DD | LG | OBA | Overall |\n|---|---|---|---|---|---|---|\n| Random Guess | 25.00 | 25.84 | 28.41 | 25.40 | 37.49 | 28.28 |\n| Open-Source LLMs |  |  |  |  |  |  |\n| MiniGPT-4 [81] | 36.98 | 32.68 | 24.19 | 20.45 | 26.14 | 27.59 |\n| LLaVA [45] | 52.30 | 35.27 | 11.80 | 9.77 | 24.70 | 22.86 |\n| LLaMA_Adapter_v2 [27] | 58.45 | 38.18 | 29.12 | 23.73 | 30.97 | 35.08 |\n| InstructBLIP [20] | 72.35 | 39.90 | 32.01 | 43.80 | 47.91 | 41.14 |\n| BLIP-2 [40] | 57.48 | 49.83 | 46.21 | 30.52 | 73.52 | 50.77 |\n| Qwen-VL-Chat [6] | 33.69 | 10.95 | 16.27 | 6.71 | 41.68 | 20.29 |\n| mPLUG-Owl2 [76] | 78.01 | 48.52 | 39.68 | 20.56 | 59.36 | 48.44 |\n| LLaVa-NeXT [46] | 68.23 | 46.74 | 41.21 | 18.43 | 39.57 | 45.57 |\n| DeepSeek-VL [49] | 74.01 | 51.94 | 45.46 | 21.06 | 29.04 | 48.76 |\n| Yi-VL [77] | 59.56 | 44.81 | 48.97 | 32.93 | 24.63 | 47.28 |\n| InternVL2-40B [18] | 96.76 | 64.25 | 76.28 | 76.50 | 76.27 | 78.70 |\n| Medical Special Model |  |  |  |  |  |  |\n| MedVInT-TE [80] | 62.62 | 41.03 | 40.57 | 12.17 | 45.17 | 43.83 |\n| LLaVA-Med [39] | 48.41 | 27.96 | 23.72 | 16.10 | 21.94 | 27.82 |\n| Med-Flamingo [54] | 26.74 | 25.10 | 23.80 | 28.04 | 16.26 | 23.82 |\n| RadFM [70] | 27.45 | 21.65 | 23.75 | 16.94 | 20.05 | 23.48 |\n| MedDr [28] | 91.37 | 51.62 | 65.56 | 73.18 | 74.52 | 68.27 |\n| HuatuoGPT-Vision-34B [14] | 95.06 | 75.67 | 66.51 | 72.83 | 74.92 | 73.23 |\n| Our Model |  |  |  |  |  |  |\n| GMAI-VL(w/o our data) | 96.40 | 80.97 | 79.14 | 70.29 | 75.66 | 79.96 |\n| GMAI-VL(ours) | 98.64 | 92.95 | 88.7 | 87.21 | 82.95 | 88.48 |", "caption": "Table 3: Comparison of performance between representative LVLMs and GMAI-VL on OmniMedVQA across five different question type. The best performance in each column is highlighted in red, and the second-best performance is highlighted in blue. Abbreviations: MR = Modality Recognition, AI = Anatomy Identification, DD = Disease Diagnosis, LG = Lesion Grading, OBA = Other Biological Attributes.", "description": "This table compares the performance of various large vision-language models (LVLMs), including the GMAI-VL model, on the OmniMedVQA benchmark dataset.  The comparison is broken down by five different question types within the benchmark: Modality Recognition (MR), Anatomy Identification (AI), Disease Diagnosis (DD), Lesion Grading (LG), and Other Biological Attributes (OBA). The table shows the accuracy of each model for each question type, highlighting the top-performing model in red and the second-best in blue. This allows for a detailed assessment of each model's strengths and weaknesses across different medical image analysis tasks.", "section": "5. Experimental Results"}, {"content": "Model Name|Overall (val)|Overall (test)|AR|BVR|B|CR|C|DD|IQG|MR|M|NT|OR-A|OR-HN|OR-P|OR-T|SG|SAR|SIR|SWR\n---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---\nRandom Guess|||||||25.70|25.94|38.20|22.73|22.92|22.72|24.06|26.66|27.13|27.00|20.00|24.75|21.37|22.93|22.33|21.18|32.43|24.23|21.39|23.71\nOpen-Source LVLMs|||||||25.58|26.34|37.74|21.50|20.62|22.00|22.41|27.29|25.91|27.45|18.00|28.79|25.16|22.13|22.00|22.00|34.61|22.88|20.44|27.43\nFlamingo v2 [4]|||||||29.58|30.45|40.16|33.92|24.92|25.22|24.21|32.99|29.96|29.53|21.20|37.88|30.32|24.80|13.33|29.88|33.11|19.62|19.16|37.43\nVisualGLM-6B [22]|||||||31.80|30.95|42.12|26.92|24.92|28.09|21.65|34.58|31.58|29.23|22.40|30.30|28.95|27.47|23.00|24.82|32.88|19.81|21.64|26.57\nInstructBLIP-7B [20]|||||||34.80|36.05|37.05|37.24|35.85|28.98|24.81|43.60|24.70|30.12|19.20|44.44|29.68|31.87|25.00|31.18|30.26|21.54|20.10|26.86\nQwen-VL [6]|||||||34.82|34.31|41.66|39.16|26.62|30.23|31.88|38.01|26.72|24.93|25.20|37.37|29.58|31.20|32.33|30.59|36.71|24.81|23.18|31.43\nYi-VL-6B [77]|||||||36.71|36.70|43.96|37.59|21.54|37.57|18.80|43.26|32.39|27.30|22.80|43.43|29.47|37.33|22.00|31.76|34.98|24.42|25.06|30.00\nShareGPT4V-7B [15]|||||||38.23|37.96|45.45|34.27|30.92|41.32|21.65|44.68|34.01|27.74|23.60|43.43|28.00|42.13|29.00|35.06|33.41|22.12|23.61|29.14\nLLAVA-V1.5-7B [45]|||||||38.68|39.20|41.89|37.59|33.69|40.79|22.26|45.87|36.44|32.94|27.20|58.59|26.11|36.40|43.67|37.29|32.06|23.46|27.80|32.86\nXComposer2 [24]|||||||38.71|39.11|36.36|36.54|32.62|38.10|30.68|46.53|34.82|28.19|25.20|48.99|28.11|40.53|33.33|36.00|34.08|26.73|24.12|29.71\nLLAVA-InternLM-7b [19]|||||||38.86|39.73|43.84|44.58|34.00|33.99|31.28|45.59|33.20|38.28|32.40|42.42|31.89|42.80|27.00|36.82|34.76|23.27|24.72|32.57\nInternVL-Chat-V1.5 [18]|||||||39.52|40.01|41.66|44.06|27.38|38.46|34.29|46.99|33.60|34.42|21.20|47.98|30.63|42.80|27.67|35.88|35.59|23.85|24.98|28.00\nInternVL-Chat-V1.2 [17]|||||||40.07|40.45|39.82|37.94|30.62|35.24|29.77|48.97|34.01|25.96|20.80|53.03|30.95|42.67|32.00|39.88|32.43|21.73|24.38|38.00\nLLAVA-InternLM2-7b [19]|||||||41.73|43.43|38.43|47.03|42.31|37.03|26.47|51.11|33.20|31.16|26.00|44.95|36.00|58.13|36.33|47.29|34.91|18.08|25.49|39.43\nDeepSeek-VL-7B [49]|||||||41.79|42.54|40.74|43.01|36.46|37.57|27.82|51.08|28.74|29.08|26.80|47.47|37.05|46.40|25.33|46.59|35.89|22.31|23.44|31.71\nMiniCPM-V2 [73]|||||||Proprietary LVLMs|||||||32.37|32.44|1.61|39.51|34.31|31.66|12.63|39.26|28.74|30.86|22.40|37.37|25.79|41.07|29.33|33.18|31.31|21.35|23.87|4.00\nClaude3-Opus [2]|||||||41.34|42.16|32.68|44.58|31.38|40.79|10.68|50.53|32.79|44.36|29.20|51.52|41.37|58.00|30.67|41.65|26.95|25.00|24.64|39.14\nQwen-VL-Max [6]|||||||42.50|44.08|29.92|48.95|44.00|37.39|12.93|52.88|32.79|44.21|32.80|63.64|39.89|54.13|37.00|50.59|27.55|23.08|25.75|37.43\nGPT-4V [1]|||||||44.38|44.93|42.12|45.10|46.46|37.57|20.45|53.29|35.22|36.94|25.20|51.01|34.74|59.60|34.00|50.00|36.64|23.65|23.87|35.43\nGemini 1.0 [62]|||||||47.42|48.36|43.50|56.12|51.23|47.58|2.26|55.33|38.87|48.07|30.00|76.26|51.05|75.87|46.33|62.24|20.57|27.69|30.54|40.57\nGemini 1.5 [56]|||||||53.53|53.96|38.32|61.01|57.08|49.02|46.62|61.45|46.56|56.38|34.00|75.25|53.79|69.47|48.67|65.88|33.93|22.88|29.51|39.43\nGPT-4o [1]|||||||Medical Special Model|||||||12.74|11.64|6.67|10.14|9.23|11.27|6.62|13.43|12.15|6.38|8.00|18.18|9.26|18.27|11.00|11.53|12.16|5.19|8.47|11.43\nMed-Flamingo [54]|||||||20.54|19.60|24.51|17.83|17.08|19.86|15.04|19.81|20.24|21.51|13.20|15.15|20.42|23.73|17.67|19.65|21.70|19.81|14.11|20.86\nLLaVA-Med [39]|||||||22.34|22.06|29.57|19.41|16.46|23.79|15.79|24.19|21.86|16.62|7.20|13.64|24.00|14.67|12.67|15.53|26.13|24.42|17.37|25.71\nQilin-Med-VL-Chat [48]|||||||22.95|22.93|27.16|20.63|13.23|19.14|20.45|24.51|23.48|22.85|15.60|16.16|14.32|24.93|17.33|21.53|29.73|17.12|19.59|31.14\nRadFM [70]|||||||41.95|43.69|41.20|50.70|37.85|29.87|28.27|52.53|36.03|31.45|29.60|47.47|33.37|51.33|32.67|44.47|35.14|25.19|25.58|32.29\nMedDr [28]|||||||Our Model|||||||54.99|56.23|51.26|61.05|53.79|44.39|44.51|62.60|40.80|57.42|35.20|79.50|61.31|77.81|53.60|69.29|35.39|35.77|29.71|44.86\nGMAI-VL(w/o our data)|||||||61.74|62.43|75.26|59.66|67.24|56.86|54.29|67.14|42.80|79.97|41.60|75.00|60.45|75.48|53.33|58.12|42.09|72.31|37.40|59.14\nGMAI-VL(ours)|||||||", "caption": "Table 4: Results on the val and test sets of GMAI-MMBench for clinical VQA tasks. The full names of the evaluated tasks can be found in Table 5 in literature\u00a0[16]. The best model in each category is highlighted in red, while the second-best model is indicated in blue.", "description": "Table 4 presents a comprehensive evaluation of various vision-language models on the GMAI-MMBench benchmark, specifically focusing on clinical Visual Question Answering (VQA) tasks.  The table details the performance of each model across multiple subtasks within the benchmark. The best-performing model for each subtask is highlighted in red, while the second-best is highlighted in blue.  To understand the specific tasks evaluated, refer to Table 5 in the referenced literature [16].", "section": "5.3. GMAI-MMBench"}, {"content": "| Model | BMS | CM | DLM | P | PH | MMMU (Health & Medicine) |\n|---|---|---|---|---|---|---|\n| Med-Flamingo [54] | 33.6 | 30.2 | 23.3 | 29.3 | 25.8 | 28.4 |\n| RadFM [70] | 31.6 | 28.6 | 26.7 | 26.2 | 26.8 | 27.9 |\n| LLaVA-Med-7B [39] | 33.8 | 32.3 | 26.7 | 40.7 | 43.3 | 38.6 |\n| Qwen-VL-Chat [6] | 32.7 | 20.6 | 19.3 | 29.6 | 33.3 | 31.7 |\n| Yi-VL-34B [77] | 48.1 | 55.6 | 36.7 | 35.4 | 31.3 | 48.2 |\n| LLaVA-v1.6-7B [45] | 46.4 | 43.4 | 30.0 | 29.6 | 26.7 | 33.1 |\n| LLaVA-v1.6-13B [45] | 53.6 | 46.7 | 33.3 | 22.2 | 40.0 | 39.3 |\n| HuatouGPT-Vision-7B [14] | 50.0 | 63.3 | 36.7 | 48.1 | 53.3 | 50.3 |\n| **GMAI-VL(w/o our data)** | 43.3 | 56.7 | 43.3 | 46.7 | 40.0 | 46.0 |\n| **GMAI-VL(ours)** | 50.0 | 60.0 | 43.3 | 50.0 | 53.3 | 51.3 |", "caption": "Table 5: Performance on the val set for the MMMU Health & Medicine track. This track is divided into five categories: BMS (Basic Medical Science), CM (Clinical Medicine), DLM (Diagnostics and Laboratory Medicine), P (Pharmacy), and PH (Public Health). The best performance in each column is highlighted in red, and the second-best performance is highlighted in blue.", "description": "Table 5 presents the performance of various vision-language models on the MMMU Health & Medicine benchmark's validation set.  The benchmark is broken down into five categories: Basic Medical Science (BMS), Clinical Medicine (CM), Diagnostics and Laboratory Medicine (DLM), Pharmacy (P), and Public Health (PH). The table shows each model's score for each category, with the top-performing model in each category highlighted in red and the second-best in blue. This allows comparison of the models' performance across different medical domains.", "section": "5.4 MMMU Health & Medicine track"}, {"content": "| Dataset | Sub-Dataset Name | Description | Size |\n|---|---|---|---|\n| GMAI-VL-5.5M | GMAI-MM-Caption-1.7M | A curated set of detailed medical image captions. | 1.7M |\n|  | GMAI-MM-Instrunct-0.9M | A diverse set of instructions for medical image analysis. | 0.9M |\n|  | GMAI-MM-Percept-1.3M | A dataset of labels for medical image classification and segmentation. | 1.3M |\n|  | GMAI-Text-Single-1M | A set of single-round medical dialogues on patient queries | 1.0M |\n|  | GMAI-Text-Multi-0.6M | A dataset of multi-turn medical conversations on various topics. | 0.6M |", "caption": "Table 6: Sub-Dataset Details for GMAI-VL-5.5M", "description": "Table 6 provides detailed information about the sub-datasets that comprise the GMAI-VL-5.5M multimodal medical dataset.  It lists each sub-dataset's name, a brief description of its content (e.g., image captions, instructions for image analysis, medical dialogues), and the total size of the dataset.  This breakdown is essential for understanding the composition and scope of the GMAI-VL-5.5M dataset, clarifying the various types of data included and their relative proportions, which are crucial for assessing the dataset's suitability and effectiveness for training vision-language models in the medical domain.", "section": "3. GMAI-VL-5.5M: A Comprehensive Multimodal Dataset"}, {"content": "| Dataset Category | Dataset Name | Size | ratio in stage 1&2 | ratio in stage 3 |\n|---|---|---|---|---|\n| General | Captioning | ALLaVA<sup>[13]</sup> | 468k | 100.0% | 50.0% |\n|  |  | ShareGPT4V<sup>[15]</sup> | 102k | 100.0% | 50.0% |\n| Medical | Captioning | GMAI-MM-Caption-1.7M | 1.7M | 100.0% | 100.0% |\n|  |  | PubMedVision<sup>[14]</sup> | 1.3M | 100.0% | 100.0% |\n|  |  | MedICaT<sup>[60]</sup> | 173k | 100.0% | 5.0% |\n|  |  | MPx-Single<sup>[70]</sup> | 31k | 100.0% | 5.0% |\n|  |  | PMC-OA<sup>[43]</sup> | 1.3M | 100.0% | 5.0% |\n|  |  | QUILT-1M<sup>[32]</sup> | 643k | 100.0% | 5.0% |\n|  |  | Retina Image Bank<sup>[3]</sup> | 22k | 100.0% | 5.0% |\n| Report | Generation | CheXpertPlus<sup>[12]</sup> | 223k | 100.0% | 30.0% |\n|  |  | MIMIC-CXR<sup>[33]</sup> | 486k | 100.0% | 30.0% |\n|  |  | OpenI<sup>[21]</sup> | 7k | 100.0% | 30.0% |\n| General | Instruction | GeoQA+<sup>[11]</sup> | 72k | 100.0% | 75.0% |\n|  |  | AI2D<sup>[36]</sup> | 12k | 100.0% | 75.0% |\n|  |  | SynthDoG<sup>[37]</sup> | 29k | 100.0% | 75.0% |\n|  |  | ChartQA<sup>[51]</sup> | 18k | 100.0% | 75.0% |\n|  |  | MMChemExam<sup>[42]</sup> | 219k | 100.0% | 75.0% |\n|  |  | LLaVA-Instruct-150K<sup>[45]</sup> | 157k | 100.0% | 75.0% |\n|  |  | DVQA<sup>[34]</sup> | 200k | 100.0% | 75.0% |\n|  |  | DocVQA<sup>[52]</sup> | 10k | 100.0% | 75.0% |\n| Medical | Instruction | GMAI-MM-Percept-1.3M | 1.3M | 100.0% | 100.0% |\n|  |  | GMAI-MM-Instruct-0.9M | 0.9M | 100.0% | 100.0% |\n|  |  | PubMedVision<sup>[14]</sup> | 1.28M | 100.0% | 100.0% |\n|  |  | LLaVA-Med-60k<sup>[39]</sup> | 56k | 100.0% | 100.0% |\n|  |  | PMC-Inline<sup>[70]</sup> | 288k | 100.0% | 10.0% |\n|  |  | VQA-Med-2019<sup>[8]</sup> | 3.2k | 100.0% | 10.0% |\n|  |  | Medical-Diff-VQA<sup>[30]</sup> | 260k | 100.0% | 10.0% |\n|  |  | PathVQA<sup>[29]</sup> | 2.6k | 100.0% | 10.0% |\n|  |  | PMC-CaseReport<sup>[70]</sup> | 109k | 100.0% | 10.0% |\n|  |  | PMC-VQA<sup>[80]</sup> | 251k | 100.0% | 10.0% |\n|  |  | ROCOV2<sup>[57]</sup> | 60k | 100.0% | 10.0% |\n|  |  | SLAKE<sup>[44]</sup> | 0.6k | 100.0% | 10.0% |\n|  |  | VQA-RAD<sup>[38]</sup> | 0.3k | 100.0% | 10.0% |\n| General Text |  | blossom_orca<sup>[5]</sup> | 20k | 0.0% | 100.0% |\n|  |  | COIG-CQIA<sup>[7]</sup> | 14.8k | 0.0% | 100.0% |\n|  |  | Cosmopedia-100k<sup>[9]</sup> | 33k | 0.0% | 100.0% |\n|  |  | ShareGPT4V<sup>[15]</sup> | 26k | 0.0% | 100.0% |\n|  |  | Orca-Math<sup>[53]</sup> | 379k | 0.0% | 100.0% |\n|  |  | Leetcode<sup>[10]</sup> | 1.7k | 0.0% | 100.0% |\n|  |  | LogiQA<sup>[47]</sup> | 12.7k | 0.0% | 100.0% |\n|  |  | Lima<sup>[26]</sup> | 83k | 0.0% | 100.0% |\n|  |  | Open Hermes 2.5<sup>[64]</sup> | 200k | 0.0% | 100.0% |\n|  |  | Firefly<sup>[74]</sup> | 189k | 0.0% | 100.0% |\n|  |  | UltraChat<sup>[23]</sup> | 189k | 0.0% | 100.0% |\n|  |  | Alpaca-Instruct-52K<sup>[61]</sup> | 49k | 0.0% | 100.0% |\n| Medical Text |  | GMAI-Text-Single-1M | 1.0M | 0.0% | 100.0% |\n|  |  | GMAI-Text-Multi-0.6M | 649k | 0.0% | 100.0% |\n| Overall |  | - | 15.7M | - | - |", "caption": "Table 7: List of datasets used in our model. We employ a large collection of image-text data and instruction data for training stage.", "description": "Table 7 details the composition of the datasets used to train the GMAI-VL model.  It breaks down the datasets by category (Captioning, Report Generation, Instruction, Text), listing each dataset's name, size, and the percentage of the dataset used in training stages 1&2 and stage 3. This provides insight into the model's training methodology and the relative importance of different data sources.", "section": "4. GMAI-VL: A General Medical Vision-Language Model"}, {"content": "| Settings | Stage I | Stage II | Stage III |\n|---|---|---|---|\n| freeze LLM | True | True | False |\n| freeze MLP | False | False | False |\n| freeze Vision Encoder | True | False | False |\n| packing type | soft packing | soft packing | soft packing |\n| learning rate | 1e-3 | 1e-4 | 1e-5 |\n| learning rate schedule | cosine decay | cosine decay | cosine decay |\n| optimizer | AdamW | AdamW | AdamW |\n| optimizer hyper-parameters | \\(\\beta_{1}=0.9,\\beta_{2}=0.999\\) | \\(\\beta_{1}=0.9,\\beta_{2}=0.999\\) | \\(\\beta_{1}=0.9,\\beta_{2}=0.999\\) |\n| input size | 336x336 | 336x336 | 336x336 |\n| total batch size | 32x8x2 | 32x4x4 | 32x4x4 |\n| drop rate | 0.0 | 0.0 | 0.0 |\n| numerical precision | DeepSpeed bf16 | DeepSpeed bf16 | DeepSpeed bf16 |\n| GPUs for training | 32xA100 (80G) | 32xA100 (80G) | 32xA100 (80G) |", "caption": "Table 8: Training settings of GMAI-VL\u2019s stage I, stage II, and stage III.", "description": "This table details the hyperparameters and training settings used for each of the three stages in the training of the GMAI-VL model.  It specifies whether components like the Language Model (LLM), Multilayer Perceptron (MLP), and Vision Encoder were frozen or trained, the type of data packing used, the learning rate and its decay schedule, the optimizer employed, and other relevant parameters like batch size and input image dimensions. It also indicates the number of GPUs and the precision used for training.", "section": "4. GMAI-VL: A General Medical Vision-Language Model"}]