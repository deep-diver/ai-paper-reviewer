[{"figure_path": "https://arxiv.org/html/2412.19326/x1.png", "caption": "Figure 1: TPO uses differentiable task preferences from dense visual supervisions via task-specific heads to enhance MLLMs in fine-grained understanding.", "description": "The figure illustrates the Task Preference Optimization (TPO) method.  TPO enhances the fine-grained visual understanding capabilities of Multimodal Large Language Models (MLLMs) by incorporating differentiable task preferences. These preferences are derived from dense visual supervision via task-specific heads, which establish connections between multiple task-specific heads and the MLLM.  The visual example shows how the model tracks the movement of a cup with a candy underneath, demonstrating the ability to handle fine-grained visual details.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2412.19326/x2.png", "caption": "Figure 2: Comparison of Learning Method. A solid line indicates data flow, and a dotted line represents feedback.\n and  denote modules that are frozen and unfrozen.", "description": "This figure compares three different learning methods: Preference Optimization (PO), Direct Preference Optimization (DPO), and Task Preference Optimization (TPO).  PO uses a reward model to maximize the likelihood of the model's output given the reward. DPO uses a reference model's output as a preference signal to guide the main model's training. TPO uses visual task annotations as preferences, enabling it to incorporate this information through task-specific heads and tokens.  The diagram illustrates the data flow in each method, indicating which components are frozen or unfrozen during training.  The use of solid and dotted lines visualizes data flow and feedback, respectively. This highlights how TPO incorporates visual task knowledge into the MLLM through jointly maximizing the likelihood of visual task estimations and multimodal dialogue.  In essence, the figure shows the evolution of model training techniques from purely likelihood-based methods to methods incorporating user preferences or visual task-specific information, culminating in TPO.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2412.19326/x6.png", "caption": "Figure 3: Overall Pipeline of TPO. The architecture of Task Preference Optimization (TPO) consists of four main components: (1) a vision encoder, (2) a connector, (3) a large language model, and (4) a series of visual task heads. Differently colored flame symbols indicate which components are unfrozen at various stages of the training process.", "description": "This figure illustrates the overall architecture of the Task Preference Optimization (TPO) method and its training process.  TPO enhances multimodal large language models (MLLMs) by incorporating visual task knowledge. The architecture comprises four key parts: a vision encoder which processes visual input; a connector that integrates visual and textual information; a large language model (LLM) that generates responses; and a series of visual task heads which perform specific visual tasks (like object detection, tracking, etc.). The different colors of the flame symbols indicate which components of the model are updated during each of the three training stages.  In essence, the diagram visually depicts how TPO fine-tunes these individual parts jointly to improve MLLM's multimodal performance and visual task-specific accuracy.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2412.19326/x7.png", "caption": "Table 3: Performance on Grounded QA.", "description": "This table presents the results of the Grounded Question Answering (Grounded QA) task.  The table compares the performance of several models, including VideoChat-TPO (the proposed method), on several metrics relevant to the task.  These metrics likely include accuracy (Acc), intersection over prediction (IoP), and Intersection over Union (IoU) at various thresholds, which measure the model's ability to both correctly answer questions and accurately locate the relevant visual information within the video.", "section": "4.1 General Understanding Evaluation"}, {"figure_path": "https://arxiv.org/html/2412.19326/x8.png", "caption": "Table 4: Performance on Image Understanding.", "description": "This table presents a quantitative comparison of the model's performance on several image understanding benchmarks.  It compares VideoChat-TPO against other state-of-the-art models on metrics such as accuracy and intersection over union (IoU).  The benchmarks likely assess the model's ability to understand and reason about images, possibly including tasks like image classification, object detection, or visual question answering. The results showcase the improvement achieved by the proposed method, highlighting its effectiveness in improving image understanding capabilities.", "section": "4. Experiment"}, {"figure_path": "https://arxiv.org/html/2412.19326/x9.png", "caption": "Table 5: Zero-Shot Performance on Moment Retrieval.Gray means no LLM.", "description": "Table 5 presents the zero-shot performance of moment retrieval models.  Zero-shot refers to evaluating the model's ability to perform the task without any fine-tuning or specific training on the moment retrieval dataset.  The table compares the performance of VideoChat-TPO against various other models, some of which use LLMs (large language models) and some which don't.  The models are evaluated using metrics such as recall at various Intersection over Union (IoU) thresholds (e.g., R@0.3, R@0.5, R@0.7).  The use of IoU indicates that the model's success in identifying moments is assessed based on the degree of overlap between the predicted moment and the ground truth moment.", "section": "4.2 Vision Task Evaluation"}, {"figure_path": "https://arxiv.org/html/2412.19326/x10.png", "caption": "Table 6: Fine-tuning Performance on Moment Retrieval and Highlight Detection.\u00a0Gray means no LLM.", "description": "This table presents the results of fine-tuning various models on moment retrieval and highlight detection tasks.  The performance is measured and compared across different models, including those with and without a large language model (LLM).  The 'Gray' annotation indicates models that do not utilize an LLM. The table likely shows metrics such as recall (R@k) at different thresholds (e.g., R@0.3, R@0.5, R@0.7), mean Intersection over Union (mIoU), Mean Average Precision (MAP), and HIT@1. This allows for a comprehensive comparison of performance across different models and task types.", "section": "4.2 Vision Task Evaluation"}, {"figure_path": "https://arxiv.org/html/2412.19326/x11.png", "caption": "Table 7: Spatial Grounding Task.\n\u2605\u2605\\bigstar\u2605 with a refined decoder.", "description": "Table 7 displays the results of the spatial grounding task. Spatial grounding involves localizing objects within an image based on textual descriptions.  The table compares the performance of the VideoChat-TPO model against several other methods, including pixel-to-sequence models (VisionLLM-H), pixel-to-embedding approaches (NExT-Chat), and a state-of-the-art expert model (G-DINO).  The VideoChat-TPO model, despite employing a simpler task head, achieves comparable or superior performance to the more complex methods, indicating its effectiveness in handling spatial grounding.", "section": "4.2 Vision Task Evaluation"}]