{"references": [{"fullname_first_author": "Ouyang, L.", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-12-01", "reason": "This paper is foundational to the field of aligning language models with human values, introducing Reinforcement Learning from Human Feedback (RLHF), which many direct alignment algorithms aim to simplify."}, {"fullname_first_author": "Rafailov, R.", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "publication_date": "2023-05-01", "reason": "This paper introduces Direct Preference Optimization (DPO), a key direct alignment algorithm that the current paper analyzes and compares against."}, {"fullname_first_author": "Hong, J.", "paper_title": "Orpo: Monolithic preference optimization without reference model", "publication_date": "2024-03-01", "reason": "This paper introduces Odds Ratio Preference Optimization (ORPO), another significant direct alignment algorithm directly compared and improved upon in this study."}, {"fullname_first_author": "Wang, R.", "paper_title": "Asft: Aligned supervised fine-tuning through absolute likelihood", "publication_date": "2024-09-01", "reason": "This paper introduces Aligned Supervised Fine-Tuning (ASFT), a direct alignment algorithm improved upon in the current paper through the addition of an explicit SFT phase and a beta parameter."}, {"fullname_first_author": "Stiennon, N.", "paper_title": "Learning to summarize from human feedback", "publication_date": "2020-12-01", "reason": "This paper is among the earliest works on using reinforcement learning from human feedback for language model alignment, providing a crucial early foundation for many later techniques, including direct alignment algorithms."}]}