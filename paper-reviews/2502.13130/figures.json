[{"figure_path": "https://arxiv.org/html/2502.13130/extracted/6213812/figures/intro_fig.png", "caption": "Figure 1: We introduce Magma, the first foundation model that is capable of interpreting and grounding multimodal inputs within its environment. Given a described goal, Magma is able to formulate plans and execute actions to achieve it. By effectively transferring knowledge from freely available visual and language data, Magma bridges verbal and spatial intelligence to navigate complex tasks.", "description": "This figure showcases Magma, a novel foundation model designed for multimodal AI agents.  Magma uniquely integrates verbal and spatial intelligence, enabling it to understand and respond to various forms of input (visual and textual), plan actions, and execute those plans in real-world and digital environments. The diagrams illustrate Magma's ability to accomplish diverse tasks, from interacting with user interfaces and manipulating objects to answering complex questions. This is achieved by leveraging knowledge effectively from freely available data encompassing visuals and language.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2502.13130/extracted/6213812/figures/som_fig.png", "caption": "Figure 2: A multimodal AI agent should be capable of mutimodal understanding and action-prediction towards a given goal.", "description": "This figure illustrates the core functionalities of a multimodal AI agent.  The agent receives both a multimodal understanding input (combining visual and textual information) and a goal specification. Its core task is to predict the appropriate actions required to achieve the given goal, demonstrating both perception and action capabilities.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2502.13130/extracted/6213812/figures/tom_fig.png", "caption": "Figure 3: Set-of-Mark supervisions for action grounding on UI screenshot\u00a0(left), robot manipulation\u00a0(middle) and human video\u00a0(right). All coordinates are normalized by image size\u00a0(height, width) and then quantized into 256 bins. Images better viewed by zooming in.", "description": "Figure 3 illustrates the concept of Set-of-Mark (SoM), a method used for action grounding in various contexts.  The image on the left shows SoM applied to a User Interface (UI) screenshot; actionable elements (like clickable buttons) are labeled with numerical marks overlaid onto the image. The middle image displays SoM applied to robot manipulation data; specific robot arm positions and actions are marked numerically.  The image on the right shows SoM for human action video data; points corresponding to human hand motions during actions are labeled with numerical marks overlaid onto the video frames.  All coordinates in the marks are normalized to the image height and width and then quantized into 256 bins for easier representation.", "section": "3. Multimodal Agentic Modeling"}, {"figure_path": "https://arxiv.org/html/2502.13130/extracted/6213812/figures/tom_fig5.png", "caption": "Figure 4: Trace-of-Mark supervisions for robot manipulation\u00a0(left) and human action\u00a0(right). Same coordinate normalization and quantization is used as SoM. Images show the future traces to predict.", "description": "This figure illustrates the concept of Trace-of-Mark, a method used to enhance spatial-temporal intelligence in the Magma model.  The left panel showcases Trace-of-Mark applied to robot manipulation, demonstrating how the model predicts the future trajectory of the robot's end-effector by overlaying marks onto consecutive video frames. This shows the planned path of the robot arm as it moves a white object. The right panel shows Trace-of-Mark applied to human actions, illustrating how the model predicts the future hand movements involved in a task, here, gathering potato peels.  In both cases, the marks show the trajectory over several frames. The same coordinate normalization and quantization scheme used for Set-of-Mark is consistently applied for the Trace-of-Mark.", "section": "3.2. Method"}, {"figure_path": "https://arxiv.org/html/2502.13130/extracted/6213812/figures/magma_pt_v3.png", "caption": "Figure 5: An illustration of Alg.\u00a02 to handle videos with camera motions for SoM/ToM generation.", "description": "This figure illustrates Algorithm 2, which addresses the challenge of camera motion in videos when generating Set-of-Mark (SoM) and Trace-of-Mark (ToM) annotations for action grounding and planning.  Algorithm 2 first uses CoTracker to generate point traces, then filters for global motion using homography transformation, classifying traces into foreground and background. Finally, it clusters foreground and background traces separately and applies SoM to the first frame.", "section": "3.2. Method"}, {"figure_path": "https://arxiv.org/html/2502.13130/x4.png", "caption": "Figure 6: Overview of Pretraining Data Sources. A diverse collection of datasets including instructional videos (orange), robotics manipulation (green), UI navigation (pink), and multimodal understanding (blue). Note that we count the size of each dataset by the number of image samples. For video and robotics data, we extract the images from the short clips and trajectories, respectively.", "description": "Figure 6 provides a detailed breakdown of the datasets used for pretraining the Magma multimodal AI model.  The figure visually represents the composition of the datasets using a pie chart, color-coding each data type.  Instructional videos are shown in orange, robotics manipulation data in green, UI navigation data in pink, and multimodal understanding data in blue. The size of each slice in the pie chart corresponds to the number of image samples within each dataset. It's important to note that for datasets containing videos (instructional videos and robotics manipulation data), only the images extracted from short video clips and robot trajectories are counted, not the complete videos themselves.", "section": "4. Multimodal Agentic Pretraining"}, {"figure_path": "https://arxiv.org/html/2502.13130/x5.png", "caption": "Figure 7: Magma pretraining pipeline. For all training data, texts are tokenized into tokens, while images and videos from different domains are encoded by a shared vision encoder. The resulted discrete and continuous tokens are then fed into a LLM to generate the outputs in verbal, spatial and action types. Our proposed method reconcile the multimodal understanding and action prediction tasks.", "description": "The figure illustrates the architecture of the Magma multimodal AI agent's pretraining pipeline.  Text data from various sources is tokenized.  Images and videos, regardless of source domain (UI, robotics, instructional videos), are processed by a shared vision encoder, producing a common representation. These visual features, along with the text tokens, are fed into a large language model (LLM). The LLM then generates outputs representing verbal, spatial, and action information. This unified approach enables the model to bridge multimodal understanding and action prediction tasks, a key feature of the Magma model.", "section": "3. Multimodal Agentic Modeling"}, {"figure_path": "https://arxiv.org/html/2502.13130/extracted/6213812/figures/images/magma_libero.png", "caption": "Figure 8: SimplerEnv performance comparison on Google Robots and Bridge. Magma(OXE) represents our model trained solely on Open-X-Embodiment (OXE)\u00a0[22], while Magma is our pretrained model. Results for each task are averaged across visual matching and variant aggregation scenarios.", "description": "This figure compares the performance of two models, Magma and Magma(OXE), on the SimplerEnv benchmark for robot manipulation tasks using Google Robots and Bridge simulators. Magma(OXE) was trained solely on the Open-X-Embodiment dataset, while Magma is a pretrained model trained on a more diverse set of data. The results shown are the average success rates across various scenarios, including visual matching and variant aggregation tasks, showcasing the generalizability and robustness of the pretrained Magma model.", "section": "5. Experiment"}, {"figure_path": "https://arxiv.org/html/2502.13130/extracted/6213812/figures/magma_spatial_visualizations_v2.png", "caption": "Figure 9: Few-shot finetuning and generalization performance on real robot. On a WidowX robot, we evaluate Magma on 4 tasks including diverse everyday object manipulation.", "description": "Figure 9 presents a comparative analysis of few-shot finetuning and generalization capabilities of the Magma model and other models on a WidowX real robot.  The experiment involved four diverse everyday object manipulation tasks. The results highlight Magma's superior performance in these tasks, particularly in challenging scenarios requiring precise spatial understanding and planning, where other models often fail.", "section": "5.1 Evaluating Agentic Capability"}, {"figure_path": "https://arxiv.org/html/2502.13130/x6.png", "caption": "Figure 10: Few-shot finetuning results on the LIBERO simulation benchmark, using 10 trajectories per task for fine-tuning.", "description": "The figure visualizes the outcomes of a few-shot fine-tuning experiment conducted on the LIBERO simulation benchmark.  The experiment involved using 10 trajectories per task for the fine-tuning process.  The graph likely presents a comparison of the model's performance (e.g., success rate) across different tasks within the LIBERO benchmark, possibly showcasing how well the model generalizes after a small amount of fine-tuning. Each bar might represent a specific task, and the height of the bar corresponds to the model's performance on that task.", "section": "5. Experiment"}, {"figure_path": "https://arxiv.org/html/2502.13130/x7.png", "caption": "Figure 11: Spatial evaluation predictions. Spatial reasoning questions are challenging even for GPT-4o but Magma can answer relatively well despite relying on much fewer pretraining data.", "description": "Figure 11 presents a comparison of GPT-4 and Magma's performance on spatial reasoning tasks.  The figure shows example questions requiring complex spatial understanding, along with the answers produced by each model.  It highlights Magma's ability to perform relatively well on these challenging tasks, despite having been trained on significantly less data than GPT-4.  This demonstrates Magma's efficiency and effectiveness in spatial reasoning.", "section": "5.2. Evaluating Spatial Reasoning"}, {"figure_path": "https://arxiv.org/html/2502.13130/x8.png", "caption": "Figure 12: Training samples in our Magma-PT-UI. It covers a wide range of action grounding and UI understanding tasks including: (a) Given the bounding box or point coordinates as the query, assistant should return the natural language description or the content. (b) Given the natural language or the exact content as the query, assistant should return the value of the bounding box coordinates.. (c) Given the natural language as the query, assistant should return the value of the point coordinate. (d) Widget captioning. (e) UI summarization.", "description": "Figure 12 showcases examples of the Magma-PT-UI training dataset, demonstrating its versatility in handling various UI interaction tasks.  (a) illustrates action grounding: given bounding box coordinates, the model generates a natural language description of the box's content. (b) shows the reverse process: given a natural language description or the exact content, the model returns the corresponding bounding box coordinates. (c) demonstrates another aspect of action grounding, where the model identifies the point coordinates for a given text description. (d) and (e) present examples of widget captioning and UI summarization, respectively, showcasing the model's ability to understand and process UI elements holistically.", "section": "3.2. Method"}, {"figure_path": "https://arxiv.org/html/2502.13130/x9.png", "caption": "Figure 13: Action distributions in three types of action-oriented pretraining datasets. (a) UI Navigation; (b) Robotic Manipulation; (c) Instructional Videos.", "description": "This figure presents bar charts visualizing the frequency distribution of action verbs used in the three types of datasets used for pretraining the Magma model: UI Navigation, Robotic Manipulation, and Instructional Videos.  Each chart shows the top most frequent action verbs in that particular dataset, offering a glimpse into the semantic distinctions between the types of actions present in each dataset. The visualization provides insight into how the action vocabularies of each dataset differ, reflecting the varied nature of tasks and actions involved in each domain.", "section": "3.2 Method"}, {"figure_path": "https://arxiv.org/html/2502.13130/extracted/6213812/figures/openvla_hotdog.png", "caption": "Figure 14: Real robot setup. Magma is deployed on a WidowX 250 robot arm to perform a sequence of kitchen manipulation tasks including object pick-place and soft manipulation.", "description": "The image shows a WidowX 250 robot arm, equipped with a gripper, performing a series of kitchen-related manipulation tasks. This setup is used to evaluate the performance of the Magma model in real-world scenarios.  The tasks involve precise movements and interactions, such as picking up and placing objects and performing soft manipulations (like adjusting the position of items). The figure highlights Magma's capabilities in real-world object manipulation.", "section": "3. Multimodal Agentic Modeling"}, {"figure_path": "https://arxiv.org/html/2502.13130/extracted/6213812/figures/openvla_mushroom.png", "caption": "Figure 15: Examples for mobile UI navigation sample. We prompt the model with two tasks: \u201cWhat\u2019s the weather like in Tokyo\u201d and \u201cInstall app \u2018Instagram\u2019\u201d. The model take actions sequentially given the new observation and history action information.", "description": "This figure showcases two examples of mobile UI navigation tasks performed by the Magma model.  The first task is to find the weather in Tokyo, which involves a sequence of actions such as opening a weather app, entering a location, and reading the results. The second task is to install the Instagram app, involving actions like opening the app store, searching for the app, and initiating the installation process. The model's actions are presented in a step-by-step fashion, illustrating how it processes new visual information and builds upon its history of actions to achieve the final goal.  The images and text clearly demonstrate Magma's ability to interpret user requests, generate a plan, and execute the plan by interacting with a mobile UI.", "section": "5.1. Evaluating Agentic Capability"}, {"figure_path": "https://arxiv.org/html/2502.13130/extracted/6213812/figures/magma_hotdog.png", "caption": "a Robot policy rollout for task \u201cPut the sausage to hotdog\u201d for OpenVLA model. (Failure)", "description": "The figure shows a sequence of images depicting a robot's failed attempt at placing a sausage into a hotdog bun. This is a result of using the OpenVLA model, which demonstrates limitations in executing complex manipulation tasks requiring precise spatial understanding and planning.", "section": "5.1. Evaluating Agentic Capability"}, {"figure_path": "https://arxiv.org/html/2502.13130/extracted/6213812/figures/magma_mushroom.png", "caption": "b Robot policy rollout for task \u201cPick up the mushroom to the pot\u201d for OpenVLA model. (Failure)", "description": "The figure shows a sequence of images depicting a robot's failed attempt to perform the task of picking up a mushroom and placing it in a pot.  The robot's actions are clumsy and inefficient, ultimately failing to complete the task successfully. This illustrates the limitations of the OpenVLA model in handling this specific robotic manipulation task, which requires precise spatial reasoning and planning.", "section": "5. Experiment"}]