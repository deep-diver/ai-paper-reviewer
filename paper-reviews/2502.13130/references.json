{"references": [{"fullname_first_author": "Jianwei Yang", "paper_title": "Magma: A Foundation Model for Multimodal AI Agents", "publication_date": "2025-02-18", "reason": "This is the main subject of the paper and is therefore intrinsically the most important reference."}, {"fullname_first_author": "Anthony Brohan", "paper_title": "RT-1: Robotics Transformer for Real-World Control at Scale", "publication_date": "2022-12-06", "reason": "This paper introduces RT-1, a foundational model for robotics which is directly related to the core subject of this paper."}, {"fullname_first_author": "Anthony Brohan", "paper_title": "RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control", "publication_date": "2023-07-15", "reason": "This paper builds upon RT-1 and explores the transfer of knowledge from web data to robotic control, a key aspect of the current paper."}, {"fullname_first_author": "Haotian Liu", "paper_title": "LLaVA: A Foundation Model for Vision-Language-Action", "publication_date": "2023-04-08", "reason": "LLaVA is a significant multimodal model which is directly compared against in this work."}, {"fullname_first_author": "Kanzhi Cheng", "paper_title": "SeeClick: Harnessing GUI Grounding for Advanced Visual GUI Agents", "publication_date": "2024-01-01", "reason": "This paper is another key comparison model that provides a benchmark for UI-based tasks, a key component of this paper."}]}