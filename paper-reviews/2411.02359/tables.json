[{"content": "| # LLM layers | 24 | 12 | 6 |\n|---|---|---|---| \n| GFLOPs/action (LLM) | 31.2 | 15.6 | 7.8 |\n| Task success rate % | 78.9 | 78.0 | 75.7 |", "caption": "Table 1: Computation cost v.s. task successful rate222Average successful rate over all subtasks in the long-horizon chains.(RoboFlamingo++) on CALVIN LH-MTLC chanllenge D\u2192\u2192\\rightarrow\u2192D.\nNotably, we mainly focus on the core component, LLM, of the MLLM, which comprises the majority of parameters. We vary the size of the LLM to examine its impact. For a focused comparison, we report the FLOPs (and GPU memory usage) of the LLM in our paper, unless otherwise specified.", "description": "This table shows the trade-off between computational cost and task success rate when using different sizes of the language model (LLM) within the RoboFlamingo model on the CALVIN LH-MTLC D\u2192D benchmark.  It demonstrates that while larger LLMs (more layers, higher FLOPs) achieve slightly better performance, the increase in computation is not proportional to the gain in accuracy. The focus is on the LLM component within the larger multimodal language model (MLLM) because it consumes most of the resources. FLOPs (floating point operations) and GPU memory usage are reported for the LLM, illustrating the efficiency implications of choosing a model size.", "section": "1 Introduction"}, {"content": "| Method | Input | Data | Foundation model | D\u2192D | ABCD\u2192D | ABC\u2192D |\n|---|---|---|---|---|---|---|\n| GR-1 [69] (ICLR\u201924) | RGB+<br>Proprio | LANG | Video-pretrained<br>Transformer | - | 4.21 | 3.06 |\n| HULC [13] (RA-L\u201922) | RGB | ALL | \u2717 | 2.64 | 3.06 | 0.67 |\n| RT-1 [15] (RSS\u201923) | RGB | LANG | \u2717 | - | 2.45 | 0.9 |\n| SPIL [70] (ICML\u201924) | RGB | ALL | \u2717 | 2.67 | - | 1.71 |\n| SuSIE [71] (ICLR\u201924) | RGB | ALL | InstructPix2Pix [72] | - | - | 2.69 |\n| RoboFlamingo (ICLR\u201924) | RGB | LANG | OpenFlamingo 3B | 2.46 (31.2) | 4.08 (31.2) | 2.47 (31.2) |\n| RoboFlamingo++ | RGB | LANG | OpenFlamingo 3B | 2.71 (31.2) | 4.07 (31.2) | 2.59 (31.2) |\n| DeeR (ours) | RGB | LANG | OpenFlamingo 3B | **2.83** (8.6) | **4.13** (10.0) | **2.82** (12.5) |\n| DeeR w. online (ours) | RGB | LANG | OpenFlamingo 3B | **2.92** (8.5) | **4.13** (9.7) | **2.90** (9.5) |", "caption": "Table 2: Comparison with baselines.\nGR-1 uses extra proprioceptive information as input.\nNote that some baselines mainly focus on one or two settings, and we present results following their original papers.\nWe report the performance of our method at the last epoch.\nThe value in parentheses indicates the LLM FLOPs required to achieve the reported score.\nThe success rates for the 1st to 5th subtasks are in\u00a0Section\u00a0B.1.", "description": "Table 2 compares the performance of DeeR with several state-of-the-art baselines on the CALVIN benchmark.  It highlights DeeR's efficiency gains by showing average successful lengths achieved across various settings (D\u2192D, ABCD\u2192D, ABC\u2192D) while comparing computational costs (LLM GFLOPs) .  The table notes that GR-1 uses additional proprioceptive information and that some baselines reported results for only a subset of the settings; DeeR's results presented are from its final training epoch.  Detailed success rates for individual subtasks are available in the supplementary materials (Section B.1).", "section": "4 Experiments"}, {"content": "| RGB+ | Proprio |\n|---|---|", "caption": "Table 3: Ablation study of auxiliary losses on ABCD\u2192\u2192\\rightarrow\u2192D.", "description": "This ablation study investigates the impact of auxiliary losses on the ABCD\u2192D experimental setting within the DeeR model.  It compares the performance of the model with and without auxiliary losses, showing their contribution to the overall accuracy and the effect on the successful length of task completion.", "section": "4.2 Ablation Study"}, {"content": "| Video-pretrained | Transformer |\n|---|---|", "caption": "Table 4: Ablation study of exit criteria. Comparing feature similarity, time, and action consistency.", "description": "This ablation study investigates the impact of different early-exit criteria on the performance of the DeeR model.  Three criteria are compared: feature similarity (measuring the similarity between action predictions from adjacent intermediate features), time (progressively increasing the model size as a task progresses), and action consistency (using the consistency of action predictions from differently sized MLLMs as a criterion). The table shows the average successful length and average GFLOPs per action for each criterion across different experimental settings (D\u2192D, ABC\u2192D, ABCD\u2192D) to analyze their effectiveness and efficiency.", "section": "3.2 Adaptive Inference"}, {"content": "| GFLOPs | DeeR | w.o. aux |\n|---|---|---|\n| 4.9 | **3.94** | 2.64 |\n| 10.0 | **4.13** | 2.71 |", "caption": "Table 5: Comparison of real inference efficiency on the ABCD\u2192\u2192\\rightarrow\u2192D dataset. The average LLM inference time is reported.", "description": "This table presents a comparison of the real-world inference efficiency between DeeR and RoboFlamingo++, focusing on the ABCD\u2192D setting of the CALVIN benchmark.  The comparison specifically highlights the average time taken for Large Language Model (LLM) inference.  This demonstrates the computational speedup achieved by DeeR in real-world robotic applications compared to the baseline model.", "section": "4 Experiments"}, {"content": "| Settings | GFLOPs | avg. succss len |  |  |  \n|---|---|---|---|---|---|\n| D\u2192D | 4.9 | 2.52 | 2.35 | **2.65** | \n|  | 9.1 | 2.62 | 2.82 | **2.83** | \n| ABCD\u2192D | 4.9 | 3.66 | 3.92 | **3.94** | \n|  | 9.1 | 3.92 | 4.08 | **4.10** | \n| ABC\u2192D | 4.9 | 2.29 | 2.46 | **2.62** | \n|  | 9.1 | 2.45 | 2.71 | **2.75** | ", "caption": "Table 6: DeeR with quantization on the ABCD\u2192\u2192\\rightarrow\u2192D setting.", "description": "This table presents the results of applying quantization techniques to the DeeR model. It shows how different levels of quantization (float32, float16, int4) affect both the model size (memory) and the average successful length of tasks completed. This demonstrates the trade-off between model compression and performance.", "section": "4 Experiments"}, {"content": "| Model | Len | GFLOPs | Time |\n|---|---|---|---|\n| Robo++ | 4.07 | 31.2 | 55ms |\n| DeeR | 4.08 | 6.0 | 17.5ms |", "caption": "Table 7: Architecture details of the OpenFlamingo models. \u2018xattn interval\u2019 means cross-attention interval.", "description": "This table presents the architecture details for the OpenFlamingo models used in the paper. It shows the language model, vision encoder, number of layers in the Large Language Model (LLM), and the cross-attention interval used in the model architecture.  The cross-attention interval indicates how frequently cross-attention layers are interspersed within the self-attention layers of the LLM, facilitating effective multimodal fusion.", "section": "A.1 Network Architecture"}, {"content": "| DeeR | Memory | Avg Len |\n|---|---|---|\n| float32 | 6G | 4.13 |\n| float16 | 3G | 4.12 |\n| int4 | 1.7G | 3.91 |", "caption": "Table 8: Training hyper-parameters for setting D\u2192\u2192\\rightarrow\u2192D/ABC\u2192\u2192\\rightarrow\u2192D/ABCD\u2192\u2192\\rightarrow\u2192D.", "description": "This table lists the hyperparameters used during the training process for the DeeR model on three different settings: D\u2192D, ABC\u2192D, and ABCD\u2192D.  The settings represent different experimental conditions to evaluate the model's performance and generalization ability. The hyperparameters include details about the batch size, optimizer, learning rates for the MLLM and the action head, learning rate schedule, warm-up steps, dropout rates for LSTM and MLP layers, the number of training epochs (both joint training and post-training for the action head), and the coefficient \u03bb, and LSTM window size.", "section": "A.3 Training Details"}, {"content": "| Model | Lanugage Model | VIsion Encoder | # LLM Layers | xattn interval |\n|---|---|---|---|---|\n| OpenFlamingo 3B | MPT-1B (Instruct) [76] | CLIP ViT-L/14 428M | 24 | 1 |\n| OpenFlamingo 9B | MPT-7B [76] | CLIP ViT-L/14 428M | 32 | 4 |", "caption": "Table 9: Detailed results in the setting D\u2192\u2192\\rightarrow\u2192D.", "description": "This table presents a detailed breakdown of the experimental results obtained for the D\u2192D setting in the CALVIN benchmark.  It shows the average task completion length and the percentage of successful task completions for each of the five subtasks in the task chains.  Different methods are compared and contrasted, demonstrating the performance of each approach for various input modalities and data sources.", "section": "4.1 Main Results"}, {"content": "| Hyper-parameters | Values |\n|---|---| \n| batch size | 4*8 |\n| optimizer | AdamW |\n| MLLM learning rate | 1e-4 |\n| action head learning rate | 2.5e-5 |\n| learninrg rate schedule | constant |\n| warmup steps | 2500 |\n| LSTM dropout | 0.3 |\n| MLP dropout | 0.4 |\n| jointly-train epochs | 4 / 4 / 3 |\n| post-train epochs | 4 / 1 / 1 |\n| \u03bb | 0.01 |\n| LSTM window size | 12 |", "caption": "Table 10: Detailed results in the setting ABCD\u2192\u2192\\rightarrow\u2192D.", "description": "This table presents a detailed breakdown of the experimental results obtained for the ABCD\u2192D setting in the CALVIN benchmark.  It shows the average successful task length achieved by various methods, including the proposed DeeR model and several baselines, and the associated LLM GFLOPs. Each method's performance is evaluated across five consecutive subtasks, and the results are reported as percentages representing the success rate for each subtask.", "section": "4.1 Main Results"}, {"content": "| Method | Only RGB Input | Data | 1 | 2 | 3 | 4 | 5 | Avg. len (LLM GFLOPs) |\n|---|---|---|---|---|---|---|---|---|---|\n| HULC | \u2713 | ALL | 82.7% | 64.9% | 50.4% | 38.5% | 28.3% | 2.64 |\n| SPIL | \u2713 | ALL | 84.6% | 65.1% | 50.8% | 38.0% | 28.6% | 2.67 |\n| RoboFlamingo | \u2713 | LANG | 83.9% | 64.3% | 42.9% | 35.7% | 19.6% | 2.46 (31.2) |\n| RoboFlamingo++ | \u2713 | LANG | **87.1%** | **69.6%** | 49.6% | 37.1% | 27.2% | 2.71 (31.2) |\n| DeeR (ours) | \u2713 | LANG | 85.3% | **69.6%** | **54.9%** | **42.0%** | **31.2%** | **2.83** (8.6) |\n| DeeR w. online (ours) | \u2713 | LANG | **89.7%** | **70.5%** | 51.8% | **44.2%** | **35.3%** | **2.92** (8.5) |", "caption": "Table 11: Detailed results in the setting ABC\u2192\u2192\\rightarrow\u2192D.", "description": "This table presents a detailed breakdown of the experimental results obtained using the ABC\u2192D setting in the CALVIN benchmark.  It compares different methods (HULC, SPIL, SuSIE, RoboFlamingo, RoboFlamingo++, DeeR, and DeeR w. online) across various metrics, including the average successful length of task chains and the number of consecutive successful instructions in each task chain (1 to 5).  The input data used (RGB, LANG, ALL) are also specified for each method.", "section": "4.1 Main Results"}]