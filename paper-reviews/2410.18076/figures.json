[{"figure_path": "2410.18076/figures/figures_2_0.png", "caption": "Figure 1: SUPE utilizes unlabeled trajectory data twice, both for offline unsupervised skill pretraining and for online high-level policy learning using RL. Left: in the offline pretraining phase (Stage 1), we unsupervisedly learn both a trajectory segment encoder (a) and a low-level latent conditioned skill policy (b) via a behavior cloning objective where the policy is optimized to reconstruct the action in the trajectory segment. Right: in the online exploration phase (Stage 2), the pretrained trajectory segment encoder (a) and an optimistic reward module (d) are used to pseudo-label the prior data and transform it into high-level trajectories (f) that can be readily consumed by a high-level off-policy RL agent. Leveraging these offline trajectories and the online replay buffer (e), we learn a high-level policy (c) that picks the pretrained low-level skills online to explore in the environment. Finally, the observed transitions and reward values are used to update the optimistic reward module and the online replay buffer.", "description": "The figure illustrates the SUPE method, showing how unlabeled trajectory data is used for both offline skill pretraining and online high-level policy learning via off-policy RL.", "section": "Skills from Unlabeled Prior Data for Exploration (SUPE)"}, {"figure_path": "2410.18076/figures/figures_6_0.png", "caption": "Figure 2: We experiment on three challenging, sparse-reward domains: AntMaze, Kitchen, and Visual AntMaze. a): AntMaze (Fu et al., 2020) (state-based) with three different maze layouts (antmaze-medium, antmaze-large, and antmaze-ultra) and the corresponding four goal locations (denoted as the red dots) that we experiment with for each of the layouts; b): Kitchen (Fu et al., 2020) (state-based); c): Visual AntMaze (Park et al., 2023a) with colors added to the floor with local 64 \u00d7 64 image observations (e.g., see examples right of the maze). The color of the floor uniquely identifies the ant's position within the maze. For both state-based and visual AntMaze, the ant starts at the bottom-left corner in the beginning of every episode.", "description": "Figure 2 shows the three challenging sparse-reward environments used in the paper: AntMaze, Kitchen, and Visual AntMaze.", "section": "5.1 EXPERIMENTAL SETUP"}, {"figure_path": "2410.18076/figures/figures_9_0.png", "caption": "Figure 2: We experiment on three challenging, sparse-reward domains: AntMaze, Kitchen, and Visual AntMaze. a): AntMaze (Fu et al., 2020) (state-based) with three different maze layouts (antmaze-medium, antmaze-large, and antmaze-ultra) and the corresponding four goal locations (denoted as the red dots) that we experiment with for each of the layouts; b): Kitchen (Fu et al., 2020) (state-based); c): Visual AntMaze (Park et al., 2023a) with colors added to the floor with local 64 \u00d7 64 image observations (e.g., see examples right of the maze). The color of the floor uniquely identifies the ant's position within the maze. For both state-based and visual AntMaze, the ant starts at the bottom-left corner in the beginning of every episode.", "description": "Figure 2 shows the three challenging sparse-reward environments used in the paper: AntMaze, Kitchen, and Visual AntMaze, illustrating their layouts and observation modalities.", "section": "5.1 EXPERIMENTAL SETUP"}, {"figure_path": "2410.18076/figures/figures_23_0.png", "caption": "Figure 2: We experiment on three challenging, sparse-reward domains: AntMaze, Kitchen, and Visual AntMaze. a): AntMaze (Fu et al., 2020) (state-based) with three different maze layouts (antmaze-medium, antmaze-large, and antmaze-ultra) and the corresponding four goal locations (denoted as the red dots) that we experiment with for each of the layouts; b): Kitchen (Fu et al., 2020) (state-based); c): Visual AntMaze (Park et al., 2023a) with colors added to the floor with local 64 \u00d7 64 image observations (e.g., see examples right of the maze). The color of the floor uniquely identifies the ant's position within the maze. For both state-based and visual AntMaze, the ant starts at the bottom-left corner in the beginning of every episode.", "description": "The figure shows the three challenging sparse-reward environments used in the paper: AntMaze, Kitchen, and Visual AntMaze, illustrating their layouts and observation modalities.", "section": "5.1 EXPERIMENTAL SETUP"}]