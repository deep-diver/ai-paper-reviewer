{"references": [{" publication_date": "2018", "fullname_first_author": "Joshua Achiam", "paper_title": "Surprise-based intrinsic motivation for deep reinforcement learning", "reason": "This paper is highly relevant as it explores intrinsic motivation methods for deep reinforcement learning, a crucial aspect of efficient exploration.  The concept of surprise-based intrinsic motivation is closely related to the goal of encouraging exploration in environments where the reward signal is sparse or delayed, aligning directly with the core problem that SUPE addresses.", "section_number": 2}, {" publication_date": "2018", "fullname_first_author": "Joshua Achiam", "paper_title": "Variational option discovery algorithms", "reason": "This paper is crucial as it presents a method for discovering options (skills) in reinforcement learning.  The concept of options forms the basis for SUPE's hierarchical approach, where low-level skills are discovered and composed into a higher-level policy for effective exploration. The variational approach aligns well with the VAE used in SUPE for skill extraction.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Rishabh Agarwal", "paper_title": "Reincarnating reinforcement learning: Reusing prior computation to accelerate progress", "reason": "This work directly addresses the offline-to-online paradigm that SUPE is also focused on.  It explores using prior computations to accelerate progress in online reinforcement learning.  This is especially relevant to SUPE which proposes a two-stage method using both offline (for pretraining skills) and online (for learning a high-level policy) learning phases. The ideas in this paper are directly used for the online learning part of the SUPE.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Anurag Ajay", "paper_title": "OPAL: Offline primitive discovery for accelerating offline reinforcement learning", "reason": "This paper is highly relevant due to its focus on offline primitive discovery.  SUPE similarly focuses on offline discovery of skills from unlabeled data.  The offline skill extraction phase in SUPE builds upon the ideas developed in this paper. The offline skill extraction method is directly compared and used in the experiments of the SUPE paper.", "section_number": 2}, {" publication_date": "2017", "fullname_first_author": "Pierre-Luc Bacon", "paper_title": "The option-critic architecture", "reason": "This paper is important because it introduces the option-critic architecture, which is a hierarchical reinforcement learning framework.  SUPE uses a hierarchical approach by composing low-level skills to perform exploration, aligning closely with the hierarchical nature of the option-critic architecture. The framework in this paper is directly related to the approach of SUPE.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Philip J Ball", "paper_title": "Efficient online reinforcement learning with offline data", "reason": "This paper is highly relevant as it directly addresses the offline-to-online reinforcement learning setting. The method in this paper is directly compared and used in the experiments of the SUPE paper.  This work explores utilizing offline data for online learning, providing a direct comparison point for SUPE's approach.", "section_number": 2}, {" publication_date": "2016", "fullname_first_author": "Marc Bellemare", "paper_title": "Unifying count-based exploration and intrinsic motivation", "reason": "This paper is highly relevant to the problem of exploration in reinforcement learning.  SUPE employs an optimistic reward model to incentivize exploration, and count-based methods are a common approach to exploration that SUPE builds upon.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "Tom B. Brown", "paper_title": "Language models are few-shot learners", "reason": "This paper is included because it shows the transformative effect of large language models pretrained on massive datasets.  This highlights the benefits of unsupervised pretraining, which is a core motivation behind the development of SUPE. The paper is used for background information only, as it is not directly related to the reinforcement learning.", "section_number": 1}, {" publication_date": "2018", "fullname_first_author": "Yuri Burda", "paper_title": "Exploration by random network distillation", "reason": "This paper introduces the random network distillation (RND) method for exploration, which is directly used in SUPE to provide an optimistic reward estimate for pseudo-relabeling offline data. This is a key component of the online exploration phase in SUPE, and the effectiveness of RND is evaluated in the experimental results section. The RND is directly used in the experiments and ablation studies of the SUPE paper.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Boyuan Chen", "paper_title": "Self-supervised reinforcement learning that transfers using random features", "reason": "This paper is relevant as it explores self-supervised reinforcement learning techniques, which are closely related to unsupervised skill discovery methods employed in SUPE.  This paper also introduces a method for transferring knowledge learned through self-supervised learning, which is an important area for future research.", "section_number": 2}, {" publication_date": "2014", "fullname_first_author": "Kyunghyun Cho", "paper_title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "reason": "This paper is included because it introduces GRUs (gated recurrent units), which are used in the VAE architecture implemented in SUPE for offline skill extraction.  The use of GRUs in the VAE is a critical implementation detail of the SUPE approach, and this paper provides the foundational work for understanding this component.", "section_number": 4}, {" publication_date": "2018", "fullname_first_author": "Benjamin Eysenbach", "paper_title": "Diversity is all you need: Learning skills without a reward function", "reason": "This paper is highly relevant as it explores the problem of learning skills without explicit reward signals, a common challenge in reinforcement learning.  SUPE also focuses on learning skills from unlabeled data, sharing a common goal.  The method of unsupervised learning of skill without reward function in this paper is directly related to the approach of SUPE.", "section_number": 2}, {" publication_date": "2017", "fullname_first_author": "Carlos Florensa", "paper_title": "Stochastic neural networks for hierarchical reinforcement learning", "reason": "This paper is relevant as it addresses hierarchical reinforcement learning, which aligns with the hierarchical approach used in SUPE.   SUPE also employs a hierarchical structure where a high-level policy composes low-level skills; this paper helps build the background for hierarchical approaches in RL.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "Justin Fu", "paper_title": "D4RL: Datasets for deep data-driven reinforcement learning", "reason": "This paper is highly relevant since it introduces the D4RL benchmark datasets used in the experimental evaluation of SUPE.  The AntMaze and Kitchen environments are directly taken from this benchmark.  The quality and relevance of the experimental results depend heavily on the datasets used for evaluation.", "section_number": 5}, {" publication_date": "2016", "fullname_first_author": "Karol Gregor", "paper_title": "Variational intrinsic control", "reason": "This paper is relevant to the exploration aspect in reinforcement learning.  Intrinsic motivation methods, such as those used in this paper, are often crucial for effective exploration in sparse-reward environments. This is relevant as SUPE uses an optimistic reward module to guide exploration.", "section_number": 2}, {" publication_date": "2018", "fullname_first_author": "Tuomas Haarnoja", "paper_title": "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor", "reason": "This paper introduces the Soft Actor-Critic (SAC) algorithm, which is directly used as the underlying RL algorithm in SUPE for training the high-level policy in the online phase.  SAC's ability to handle off-policy data and its focus on maximum entropy are crucial to SUPE's efficiency in using both online and offline data.", "section_number": 4}, {" publication_date": "2022", "fullname_first_author": "Joshua Achiam", "paper_title": "Surprise-based intrinsic motivation for deep reinforcement learning", "reason": "This paper is relevant because it introduces intrinsic motivation, a key aspect of exploration in reinforcement learning. SUPE's approach relies on encouraging exploration through optimistic reward estimates, which is closely connected to the concepts of intrinsic motivation discussed in this paper.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Anurag Ajay", "paper_title": "OPAL: Offline primitive discovery for accelerating offline reinforcement learning", "reason": "This paper is highly relevant because it provides an effective offline method for discovering primitives, which aligns with SUPE's offline skill discovery approach.  The authors use a similar trajectory VAE architecture for learning skills that's directly used for comparisons.", "section_number": 2}, {" publication_date": "1999", "fullname_first_author": "Richard S Sutton", "paper_title": "Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning", "reason": "This seminal paper lays the foundation for the options framework in reinforcement learning, which is relevant to SUPE's hierarchical approach.  Although SUPE uses a simpler skill representation (fixed-horizon skills), the fundamental idea of hierarchical decomposition using options is a significant contribution to the field of RL, and this paper provides the basis for such an approach.", "section_number": 2}]}