[{"figure_path": "2410.18076/charts/charts_8_0.png", "caption": "Figure 3: Aggregated normalized return across three different domains. Ours achieves the best performance through training on all three domains. ExPLORe achieves strong later stage performance on AntMaze, but struggles in high-dimensional Visual AntMaze and Kitchen tasks. Online w/ HILP Skills and HILP w/ Offline Data achieve decent initial return on Kitchen, but struggle to learn in all three domains. Online w/ Trajectory Skills consistently underperforms Ours across all three environments. Diffusion BC + JSRL learns reasonably well in Kitchen, but performs much worse in AntMaze and Visual AntMaze. Online does not perform competitively at any stage of exploration. Section 5.2 contains details on the baselines we compare with. Each curve is an average over 8 seeds. For AntMaze, we aggregate over 3 maze layouts and 4 goals. For Kitchen, we aggregate over 3 tasks. For Visual AntMaze, we aggregate over 4 goals on one maze layout.", "description": "The chart shows the aggregated normalized return across three different domains (AntMaze, Visual AntMaze, and Kitchen) over various environment steps for different exploration strategies.", "section": "5 EXPERIMENTAL RESULTS"}, {"figure_path": "2410.18076/charts/charts_10_0.png", "caption": "Figure 5: Coverage on three different AntMaze mazes, averaged over runs on four goals. Ours has the best coverage performance on the challenging antmaze-ultra, and is only passed by HILP w/ Offline Data on antmaze-large. Online w/ Traj. Skills and Online with HILP Skills struggle to explore after initial learning, and Online and Diffusion BC + JSRL generally perform poorly at all time steps.", "description": "The chart displays the coverage performance of different exploration methods across three AntMaze environments over training time.", "section": "5 EXPERIMENTAL RESULTS"}, {"figure_path": "2410.18076/charts/charts_19_0.png", "caption": "Figure 6: Success rate on Visual AntMaze environment with and without ICVF. Ours works well without ICVF, almost matching the original performance. However, the other baselines Online w/ Trajectory Skills and EXPLORe achieve far worse performance without ICVF, which shows that using offline data both for extracting skills and online learning leads to better utilization of noisy exploration bonuses. Initializing ExPLORe critic with ICVF helps, but does not substantially change performance.", "description": "The chart shows the success rate of different methods on the Visual AntMaze environment with and without using ICVF (a method for learning image/state representations from passive data).", "section": "5.3 CAN WE LEVERAGE UNSUPERVISED TRAJECTORY SKILLS TO ACCELERATE ONLINE LEARNING?"}, {"figure_path": "2410.18076/charts/charts_20_0.png", "caption": "Figure 7: Normalized return on three AntMaze mazes, comparing Ours with a KL regularized alternative (Ours (KL)). We that Ours consistently outperforms Ours (KL) on all three mazes, with initial learning that is at least as fast and significantly improved asymptotic performance. Only Ours is able to meet or surpass the asymptotic performance of ExPLORe on all mazes.", "description": "The chart compares the performance of the proposed method (Ours) with a KL-regularized version and ExPLORE across three AntMaze environments of varying complexity.", "section": "5.5 DOES OFFLINE DATA HELP OUR METHOD TO COMPOSE SKILLS BETTER FOR FASTER EXPLORATION?"}, {"figure_path": "2410.18076/charts/charts_21_0.png", "caption": "Figure 8: Success rate by goal location. The addition of online RND in ExPLORe leads to better performance on goals with less offline data coverage, and slightly worse performance on goals well-represented in the dataset. Ours consistently matches are outperforms all other methods on all goals throughout training.", "description": "The chart displays the success rate of different methods across various AntMaze goal locations, illustrating the impact of online RND and the effectiveness of the proposed method.", "section": "5.3 CAN WE LEVERAGE UNSUPERVISED TRAJECTORY SKILLS TO ACCELERATE ONLINE LEARNING?"}, {"figure_path": "2410.18076/charts/charts_22_0.png", "caption": "Figure 9: Coverage for every goal location on three antmaze environments. There is significant variation between goals, and Ours consistently has the best initial coverage performance on 11 of 12 goals. Flattening coverage compared to other methods can be at least partially attributed to having already found the goal, and sucessfully optimizing reaching that goal, rather than continuing to explore after already finding the goal.", "description": "The chart displays the percentage of the maze explored by different methods over time across various goal locations and maze sizes.", "section": "5.5 DOES OFFLINE DATA HELP OUR METHOD TO COMPOSE SKILLS BETTER FOR FASTER EXPLORATION?"}, {"figure_path": "2410.18076/charts/charts_23_0.png", "caption": "Figure 10: Data corruption ablation on state-based antmaze-large. Top: The success rate of different methods on these data corruption settings. Bottom: Visualization of the data distribution for each corruption setting. We experiment with two data corruption settings. Our method performs worse than the full data setting but still consistently outperforms all baselines.", "description": "The chart displays the success rate of different reinforcement learning methods on the AntMaze task under two data corruption scenarios (5% data and insufficient coverage), showing the robustness of the proposed method.", "section": "5 EXPERIMENTAL RESULTS"}]