[{"figure_path": "2410.11190/figures/figures_1_0.png", "caption": "Figure 1: The Mini-Omni2 model architecture.", "description": "The figure illustrates the architecture of Mini-Omni2, showing how visual, audio, and text modalities are processed and integrated to produce real-time speech and text responses.", "section": "1 Introduction"}, {"figure_path": "2410.11190/figures/figures_3_0.png", "caption": "Figure 2: Mini-Omni2 now supports streaming speech responses for image, audio and text inputs.", "description": "The figure shows a screenshot of the Mini-Omni2 model interacting with a user, providing streaming speech responses to both image and audio inputs.", "section": "3 Mini-Omni2"}, {"figure_path": "2410.11190/figures/figures_7_0.png", "caption": "Figure 5: Mini-Omni2's three-stage training phases", "description": "The figure illustrates the three-stage training process of the Mini-Omni2 model, showing how the model is progressively trained to handle multimodal inputs and outputs.", "section": "3.3 Training Strategies"}, {"figure_path": "2410.11190/figures/figures_10_0.png", "caption": "Figure 1: The Mini-Omni2 model architecture.", "description": "The figure shows the architecture of Mini-Omni2, illustrating how visual, audio, and text inputs are processed by their respective encoders and adapters before being integrated into a language model to generate text and audio outputs.", "section": "Introduction"}]