[{"figure_path": "2410.11190/figures/figures_1_0.png", "caption": "Figure 1: The Mini-Omni2 model architecture.", "description": "The figure illustrates the architecture of Mini-Omni2, showing how visual, audio, and text inputs are processed by separate encoders before being integrated into a language model to generate text and speech outputs.", "section": "Abstract"}, {"figure_path": "2410.11190/figures/figures_3_0.png", "caption": "Figure 2: Mini-Omni2 now supports streaming speech responses for image, audio and text inputs.", "description": "Mini-Omni2 is shown to provide streaming speech responses for image, audio, and text inputs.", "section": "1 Introduction"}, {"figure_path": "2410.11190/figures/figures_7_0.png", "caption": "Figure 1: The Mini-Omni2 model architecture.", "description": "The figure illustrates the architecture of Mini-Omni2, showing how visual, audio, and text modalities are integrated into a single language model for end-to-end voice responses.", "section": "3 Mini-Omni2"}, {"figure_path": "2410.11190/figures/figures_10_0.png", "caption": "Figure 1: The Mini-Omni2 model architecture.", "description": "The figure shows the architecture of Mini-Omni2, illustrating how visual, audio, and text modalities are integrated through pretrained encoders and a language model.", "section": "1 Introduction"}]