{"references": [{" publication_date": "2024", "fullname_first_author": "OpenAI", "paper_title": "GPT-40", "reason": "GPT-40 serves as the benchmark model throughout the paper, establishing the high bar for multi-modal capabilities that Mini-Omni2 aims to achieve.  The paper frequently references its functionalities and limitations to contextualize its own contributions and demonstrate advancements in the field of multi-modal language models.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Jean-Baptiste Alayrac", "paper_title": "Flamingo: a visual language model for few-shot learning", "reason": "This paper is highly relevant because it is a prominent example of a visual language model that Mini-Omni2 builds upon and aims to surpass in terms of comprehensive multi-modal abilities, including audio and more sophisticated interaction. It represents a key milestone in the development of vision-language models and is frequently cited in related work.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "reason": "CLIP, introduced in this paper, forms the basis for the vision encoder used in Mini-Omni2, establishing its relevance in the architectural design.  The use of CLIP is a critical aspect of the paper's methodology, showing the significant influence of this foundational work in the development of vision-language modeling and Mini-Omni2's overall capabilities.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Alec Radford", "paper_title": "Robust speech recognition via large-scale weak supervision", "reason": "Whisper, introduced in this paper, provides the foundation for the audio encoder in Mini-Omni2.  This makes the paper directly relevant to the architecture and functionality of the proposed model, highlighting a crucial component underpinning Mini-Omni2's audio processing capabilities.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Lightning AI", "paper_title": "LitGPT", "reason": "This paper describes the LitGPT framework, which is directly used in Mini-Omni2.  This means it's a foundational piece of the model's development.  The significance of the LitGPT framework lies in its role as the training framework and its contribution to the overall architecture of Mini-Omni2.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Yunfei Chu", "paper_title": "Qwen2-audio technical report", "reason": "This paper presents Qwen2, the base language model used in Mini-Omni2.  This choice fundamentally impacts the model's performance and abilities. The paper is essential for understanding Mini-Omni2's linguistic foundation and its capabilities.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Zhifei Xie", "paper_title": "Mini-Omni: Language models can hear, talk while thinking in streaming", "reason": "This paper introduces the predecessor to Mini-Omni2, which is built upon and significantly improved upon in this paper.  The previous model's limitations and successes inform and contextualize the development and improvements found in Mini-Omni2.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Hubert Siuzdak", "paper_title": "SNAC", "reason": "The SNAC tokenizer is the core technology for high-quality speech output in Mini-Omni2. The direct dependence of Mini-Omni2 on SNAC makes this paper a critical component of the work and understanding the tokenizer is essential in comprehending the model's capabilities.", "section_number": 3}, {" publication_date": "2019", "fullname_first_author": "Heiga Zen", "paper_title": "Libritts: A corpus derived from librispeech for text-to-speech", "reason": "LibriTTS, a dataset presented in this paper, is a key component of the training data used in Mini-Omni2 for speech processing. The dataset significantly impacts the model's training and performance; thus, the paper holds considerable importance in describing the training process and understanding the model's capabilities.", "section_number": 4}, {" publication_date": "2020", "fullname_first_author": "Vineel Pratap", "paper_title": "Mls: A large-scale multilingual dataset for speech research", "reason": "The Multilingual LibriSpeech dataset, described in this paper, forms a significant part of the training data for the Mini-Omni2 model. It contributes substantially to the model's ability to understand and generate speech across multiple languages, making this citation important in understanding the scope and methodology of the work.", "section_number": 4}, {" publication_date": "2021", "fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "reason": "This citation, already mentioned earlier, is extremely important to understanding the architecture of Mini-Omni2, but its importance here lies in how it establishes the model's use of pre-trained vision and audio encoders and is relevant in discussing the data used in training.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Tianxiang Sun", "paper_title": "Moss: An open conversational large language model", "reason": "The Moss-002-sft dataset, from this paper, is used in the Mini-Omni2 training process. The dataset is vital in shaping the model's capacity for understanding and generating spoken language, thus making this reference crucial for understanding the training data and the model's resultant capabilities.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Haotian Liu", "paper_title": "Improved baselines with visual instruction tuning", "reason": "Although not directly used in the model architecture, this paper reflects the advancements in the field and suggests methods of improvement that are directly relevant to the overall work and could improve Mini-Omni2 in future iterations. This makes this citation vital in discussing the broader context and research that would influence Mini-Omni2.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Mihaiii", "paper_title": "QAassistant", "reason": "This dataset is used for training Mini-Omni2 and reflects the diversity of data sources used for training. It directly impacts the model's performance in question-answering tasks. This citation is important to understanding the model's training data and resultant capabilities.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "OpenAI", "paper_title": "GPT-4V", "reason": "GPT-4V, while not directly used in the architecture, serves as a further comparison point and represents the latest advancement in the field that Mini-Omni2 aims to replicate or surpass in open-source format. This citation helps the reader to understand the competitive landscape and goals of Mini-Omni2.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Eliya Nachmani", "paper_title": "Spoken question answering and speech continuation using spectrogram-powered llm", "reason": "This paper's approach of using spectrograms is closely related to the methodology used in Mini-Omni2, indicating a similar line of thought in developing end-to-end speech interaction models.  It offers a valuable point of comparison and reflects the ongoing progress in this area of multimodal interaction.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Ziyang Ma", "paper_title": "Language model can listen while speaking", "reason": "This paper discusses the concurrent speech-in and speech-out capability which is a feature sought after in the Mini-Omni2 model.  The methods used to achieve this feature are relevant to the goals of Mini-Omni2 and demonstrates the continued research in this advanced capability in multimodal models.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Alexandre D\u00e9fossez", "paper_title": "Moshi: a speech-text foundation model for real-time dialogue", "reason": "Moshi, presented in this paper, is a close competitor to Mini-Omni2. This provides a useful comparison and analysis of the approach taken by the authors in relation to another model pursuing similar multi-modal interaction capabilities and real-time generation.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Qingkai Fang", "paper_title": "Llama-Omni: Seamless speech interaction with large language models", "reason": "Llama-Omni, presented in this paper, is another model that Mini-Omni2 is benchmarked against, offering a comparative perspective on end-to-end multi-modal models and providing insight into the landscape of similar research approaches aiming for similar capabilities.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Junnan Li", "paper_title": "BLIP: Bootstrapping language-image pre-training for unified vision-language understanding and generation", "reason": "BLIP is a prominent example of a vision-language model that Mini-Omni2 builds upon, drawing parallels and potentially offering alternative approaches in integrating vision into language processing.  The model\u2019s success and methodology form a relevant benchmark.", "section_number": 2}]}