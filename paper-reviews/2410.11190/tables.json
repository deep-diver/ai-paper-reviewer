[{"figure_path": "2410.11190/tables/table_6_0.html", "caption": "Table 1: The datasets and their usage for training Mini-Omni2.", "description": "The table lists the datasets used for training the Mini-Omni2 model, categorized by task, stage, modality, and number of items.", "section": "4 Data and Evaluation"}, {"figure_path": "2410.11190/tables/table_9_0.html", "caption": "Table 1: The datasets and their usage for training Mini-Omni2.", "description": "The table lists the datasets used for training Mini-Omni2, categorized by task (ASR, Text QA, Audio QA, Visual QA, voice QA), training stage, dataset name, modality, and number of items.", "section": "4 Data and Evaluation"}, {"figure_path": "2410.11190/tables/table_9_1.html", "caption": "Table 2: Comparison of the model's ASR with the base model used. (* our reproduced evaluation result.)", "description": "Table 2 compares the accuracy of Mini-Omni2's Automatic Speech Recognition (ASR) with Wav2vec2-base, VITA, and Whisper-small, showing a slight performance improvement over Whisper-small.", "section": "4.4 Experimental Results"}]