[{"figure_path": "https://arxiv.org/html/2502.12143/x1.png", "caption": "Figure 1: Small student models (\u2264\\leq\u22643B parameters) do not consistently benefit from long CoT reasoning or distillation from large teacher models. Instead, they perform better when fine-tuned on shorter CoT reasoning or distilled from smaller teachers, which better matches their intrinsic learning capacity. We term this phenomenon the Small Model Learnability Gap.", "description": "This figure illustrates the Small Model Learnability Gap.  It shows that smaller language models (with 3 billion parameters or less) don't consistently improve their performance when trained using long chain-of-thought (CoT) reasoning examples or by distilling knowledge from much larger models.  Instead, these smaller models perform better when trained on shorter, simpler CoT examples or when knowledge is distilled from similarly sized models. This is because their learning capacity is better suited to shorter, less complex reasoning steps. The gap highlights the difficulty of transferring complex reasoning abilities from large to small models directly.", "section": "Small Model Learnability Gap"}, {"figure_path": "https://arxiv.org/html/2502.12143/x2.png", "caption": "Figure 2: Long CoT Gap (\u0394L\u2062o\u2062n\u2062g=PL\u2062o\u2062n\u2062g\u2212PS\u2062h\u2062o\u2062r\u2062tsubscript\u0394\ud835\udc3f\ud835\udc5c\ud835\udc5b\ud835\udc54subscript\ud835\udc43\ud835\udc3f\ud835\udc5c\ud835\udc5b\ud835\udc54subscript\ud835\udc43\ud835\udc46\u210e\ud835\udc5c\ud835\udc5f\ud835\udc61\\Delta_{Long}=P_{Long}-P_{Short}roman_\u0394 start_POSTSUBSCRIPT italic_L italic_o italic_n italic_g end_POSTSUBSCRIPT = italic_P start_POSTSUBSCRIPT italic_L italic_o italic_n italic_g end_POSTSUBSCRIPT - italic_P start_POSTSUBSCRIPT italic_S italic_h italic_o italic_r italic_t end_POSTSUBSCRIPT) of student models with different models sizes for (a) Qwen family (b) Llama family. For teacher models, QwQ-preview-32B is chosen to generate long CoT responses, while Qwen2.5-32B-Instruct is chosen to generate short CoT responses. Negative (Positive) \u0394L\u2062o\u2062n\u2062gsubscript\u0394\ud835\udc3f\ud835\udc5c\ud835\udc5b\ud835\udc54\\Delta_{Long}roman_\u0394 start_POSTSUBSCRIPT italic_L italic_o italic_n italic_g end_POSTSUBSCRIPT indicates that long CoT is worse (better) than short CoT. Our results demonstrate that short CoT is better for smaller student models (indicated by \u0394L\u2062o\u2062n\u2062gsubscript\u0394\ud835\udc3f\ud835\udc5c\ud835\udc5b\ud835\udc54\\Delta_{Long}roman_\u0394 start_POSTSUBSCRIPT italic_L italic_o italic_n italic_g end_POSTSUBSCRIPT < 0), while long CoT is better for larger student models (indicated by \u0394L\u2062o\u2062n\u2062gsubscript\u0394\ud835\udc3f\ud835\udc5c\ud835\udc5b\ud835\udc54\\Delta_{Long}roman_\u0394 start_POSTSUBSCRIPT italic_L italic_o italic_n italic_g end_POSTSUBSCRIPT > 0).", "description": "This figure illustrates the performance difference between using long chain-of-thought (CoT) reasoning and short CoT reasoning for training small language models.  The x-axis represents the size of the student model (in billions of parameters), and the y-axis represents the difference in performance (Long CoT Gap) calculated as the performance with long CoT minus the performance with short CoT.  A positive Long CoT Gap indicates that long CoT improves performance, while a negative gap means short CoT is better. The results show that smaller models (less than 3B parameters) perform better with short CoT, whereas larger models benefit more from long CoT training.  The figure includes separate graphs for the Qwen family and Llama family of language models, highlighting that this trend holds consistently across different model architectures.", "section": "Small Model Learnability Gap"}, {"figure_path": "https://arxiv.org/html/2502.12143/x3.png", "caption": "Figure 3: Large model CoT Gap (\u0394L\u2062a\u2062r\u2062g\u2062e=PL\u2062a\u2062r\u2062g\u2062e\u2212PS\u2062m\u2062a\u2062l\u2062lsubscript\u0394\ud835\udc3f\ud835\udc4e\ud835\udc5f\ud835\udc54\ud835\udc52subscript\ud835\udc43\ud835\udc3f\ud835\udc4e\ud835\udc5f\ud835\udc54\ud835\udc52subscript\ud835\udc43\ud835\udc46\ud835\udc5a\ud835\udc4e\ud835\udc59\ud835\udc59\\Delta_{Large}=P_{Large}-P_{Small}roman_\u0394 start_POSTSUBSCRIPT italic_L italic_a italic_r italic_g italic_e end_POSTSUBSCRIPT = italic_P start_POSTSUBSCRIPT italic_L italic_a italic_r italic_g italic_e end_POSTSUBSCRIPT - italic_P start_POSTSUBSCRIPT italic_S italic_m italic_a italic_l italic_l end_POSTSUBSCRIPT) of student models with different models sizes for (a) Qwen family (b) Llama family. For teacher models, Qwen2.5-72B-Instruct is chosen as the large teacher to generate responses, while Qwen2.5-3B-Instruct is chosen as the small teacher to generate responses. Negative (positive) \u0394L\u2062a\u2062r\u2062g\u2062esubscript\u0394\ud835\udc3f\ud835\udc4e\ud835\udc5f\ud835\udc54\ud835\udc52\\Delta_{Large}roman_\u0394 start_POSTSUBSCRIPT italic_L italic_a italic_r italic_g italic_e end_POSTSUBSCRIPT indicates that large teacher CoT is worse (better) than small teacher CoT. Our results demonstrate that small teacher CoT is better for smaller student models (indicated by \u0394L\u2062a\u2062r\u2062g\u2062esubscript\u0394\ud835\udc3f\ud835\udc4e\ud835\udc5f\ud835\udc54\ud835\udc52\\Delta_{Large}roman_\u0394 start_POSTSUBSCRIPT italic_L italic_a italic_r italic_g italic_e end_POSTSUBSCRIPT < 0), while large model CoT is better for larger student models (indicated by \u0394L\u2062a\u2062r\u2062g\u2062esubscript\u0394\ud835\udc3f\ud835\udc4e\ud835\udc5f\ud835\udc54\ud835\udc52\\Delta_{Large}roman_\u0394 start_POSTSUBSCRIPT italic_L italic_a italic_r italic_g italic_e end_POSTSUBSCRIPT > 0).", "description": "This figure displays the performance difference between using large teacher-generated chain-of-thought (CoT) reasoning and small teacher-generated CoT reasoning to fine-tune student models of varying sizes.  The x-axis represents the size of the student model (in billions of parameters), and the y-axis shows the difference in performance (Large Teacher CoT performance - Small Teacher CoT performance).  Positive values indicate that the large teacher CoT is better, while negative values mean the small teacher CoT performs better. The results are shown separately for Qwen and Llama families of language models. The key finding is that smaller student models benefit from using CoT from smaller teachers, while larger models benefit more from using CoT from larger teachers.", "section": "3 Small Model Learnability Gap"}, {"figure_path": "https://arxiv.org/html/2502.12143/x4.png", "caption": "Figure 4: Math expert models usually have a less significant Learnability Gap than the general models.\nA positive Gap means long CoT or large teacher CoT is better while negative means worse. This indicates that the math expert model could more easily learn from long CoT data or large teacher CoT.", "description": "This figure compares the performance of general-purpose and math-specialized language models (LLMs) when trained using different methods: long chain-of-thought (CoT) reasoning, short CoT reasoning, large teacher CoT, and small teacher CoT.  A positive gap indicates that the longer/larger methods led to better performance than shorter/smaller methods. The results show that math-specialized models exhibit a smaller learnability gap; they are less sensitive to the complexity of the training data, demonstrating a stronger ability to learn from complex, large teacher CoT data.", "section": "3.4 Analysis of Small Model Learnability Gap"}, {"figure_path": "https://arxiv.org/html/2502.12143/x5.png", "caption": "Figure 5: Base models generally exhibit a more significant learnability gap than Instruct models. A positive gap indicates that long CoT data or large teacher CoT enhance performance, whereas a negative gap suggests they have the opposite effect. This implies that it is more challenging for small base models to effectively learn from long CoT data or large teacher CoT.", "description": "This figure compares the performance of base and instruct models of varying sizes when trained using different types of chain-of-thought (CoT) reasoning data: long CoT, short CoT, large teacher CoT, and small teacher CoT.  The x-axis represents the model size, while the y-axis shows the difference in performance between using complex (long CoT or large teacher CoT) vs simpler (short CoT or small teacher CoT) reasoning data.  A positive value on the y-axis indicates that the model performs better with complex reasoning data, while a negative value shows the opposite. The results reveal that smaller base models struggle to effectively utilize the complex reasoning data provided by longer chains of thought or larger teacher models, exhibiting a greater performance difference compared to instruct models. This highlights a significant 'learnability gap' between base and instruct models.", "section": "3 Small Model Learnability Gap"}, {"figure_path": "https://arxiv.org/html/2502.12143/x6.png", "caption": "Figure 6: The average performance varies with the mix weight of long CoT or large teacher CoT data. Qwen2.5-3B-Instruct is chosen as the student model. At a weight of 0.2, mix distillation achieves the highest average performance.", "description": "This figure illustrates the impact of varying the ratio of long Chain-of-Thought (CoT) examples or large teacher model responses when training a smaller language model using Mix Distillation. The x-axis represents the weighting of long CoT data (or large teacher data) in the training data, ranging from 0 to 1. The y-axis shows the average performance of the Qwen2.5-3B-Instruct model across several benchmarks. The graph reveals an optimal mix ratio where a weighting of 0.2 for long CoT or large teacher data results in peak performance. This suggests a balance needs to be struck between the complexity of long CoT examples and the capacity of smaller models.", "section": "4 Mix Distillation: Bridge Small Model Learnability Gap"}, {"figure_path": "https://arxiv.org/html/2502.12143/extracted/6211364/figs/mix_long.png", "caption": "Figure 7: Case Study of Mix-Long. Models fine-tuned on long CoT tended to overthink, while those trained on short CoT produced incorrect answers. In contrast, Mix-Long, incorporating branching elements (e.g., \u201cAlternatively\u201d), achieved a balanced reasoning process and arrived at the correct answer.", "description": "Figure 7 presents a comparative analysis of model performance across three different training methodologies: long chain-of-thought (CoT), short CoT, and Mix-Long (a blend of both).  The figure showcases how models trained solely on long CoT reasoning may produce overly complex and ultimately incorrect responses due to an inability to identify a concise solution path.  In contrast, models trained exclusively on short CoT sequences provide short, yet often inaccurate results due to a lack of detailed reasoning. The Mix-Long approach effectively combines the strengths of long and short CoT training paradigms, allowing the model to benefit from both the comprehensive reasoning steps provided by long CoT examples and the efficiency and conciseness of short CoT. The result is a balanced approach that produces accurate and efficient answers, as demonstrated by the example shown in Figure 7.", "section": "4 Mix Distillation: Bridge Small Model Learnability Gap"}, {"figure_path": "https://arxiv.org/html/2502.12143/extracted/6211364/figs/speaking_way_shift.png", "caption": "Figure 8: The process of calculating most shifted tokens. We decode each token generated by the fine-tuned LLM in the student model before fine-tuning. Then we calculate the rank shift in the student model for each token generated by the fine-tuned model. We annotate the tokens that exhibit the largest rank shifts as the most shifted tokens. We found that these tokens are predominantly associated with expressive and stylistic elements, such as \u201cBut\u201d and \u201cLet\u201d.", "description": "This figure illustrates the methodology for identifying the tokens that have undergone the most significant rank shifts during the fine-tuning process of a language model.  The process begins by decoding tokens generated by the fine-tuned model, prior to fine-tuning. Then, the rank shift for each of these tokens is computed within the student model.  Tokens with the largest rank shifts are highlighted as the most shifted tokens. The analysis reveals that these tokens are predominantly associated with expressive and stylistic elements in the generated text, specifically words like \"But\" and \"Let\".", "section": "3.4 Analysis of Small Model Learnability Gap"}]