[{"content": "| Method | \u2191Prof. | \u2193Gen. | \u2191Worst |\n|---|---|---|---|\n| Original | 61.9 | 87.4 | 24.4 |\n| CBP | 83.3 | 60.1 | 67.7 |\n| Neuron skyline | 75.5 | 73.2 | 41.5 |\n| GSAE SHIFT | 88.5 | 54.0 | 76.0 |\n| SSAE SHIFT | 90.2 | 53.4 | 88.5 |\n| GSAE SHIFT+retrain | 93.1 | 52.0 | 89.0 |\n| SSAE SHIFT+retrain | 93.4 | 51.9 | 89.5 |\n| Comp. GSAE SHIFT | 80.5 | 68.2 | 48.6 |\n| Comp. SSAE SHIFT | 89.6 | 52.2 | 78.8 |\n| Comp. GSAE SHIFT+retrain | 80.0 | 68.8 | 57.1 |\n| Comp. SSAE SHIFT+retrain | 93.2 | 52.1 | 88.5 |\n| Oracle | 93.0 | 49.4 | 91.9 |", "caption": "Table 1: Balanced set accuracies for intended (profession) and unintended (gender) labels. Worst refers to lowest profession accuracy among male professors, male nurses, female professors, and female nurses. Comp.: Compressed SAE (sliced to 1/8th width). Best results per method category are bolded.", "description": "This table presents the classification accuracy results on the Bias in Bios dataset for predicting professional roles while controlling for gender bias. It compares different methods for mitigating spurious correlations: original classifier, concept bottleneck probing (CBP), neuron skyline, and sparse autoencoder (SAE)-based SHIFT methods.  The metrics include overall profession accuracy, gender accuracy, and worst-group accuracy (the lowest accuracy among the four subgroups: male professors, male nurses, female professors, female nurses).  The table also shows results for compressed SAEs and the impact of retraining after feature removal. The best-performing method within each category is highlighted in bold.", "section": "3.1.3 Case study: Removing Spurious Features in Bias in Bios Classifier"}, {"content": "| Feature | Explanation |\n|---|---| \n| h.7_feature3 | Unified explanation: This neuron recognizes narrative structures in simple, moralistic children\u2019s stories. It activates on new story segments, character introductions, settings, conflicts, and dialogue. Frequent themes include lessons on kindness, honesty, and sharing.  Examples: 1. \"Lily woke up early on Saturday morning. \u2018Mom, can I go play with my friend Jenny?\u2019 she asked.\" 2. \"Once upon a time, there was a little boy named Tommy who loved to play with his toys but never wanted to share.\" 3. \"After school, Timmy came home feeling sad. \u2018What\u2019s wrong?\u2019 his mom asked. \u2018I got in trouble for not telling the truth,\u2019 Timmy replied.\" Diversity Score: 71 Justification: Activates on diverse narrative elements in children\u2019s stories, including dialogue, character introductions, settings, events, emotions, and moral lessons. High diversity within the genre of educational stories for young audiences.  |\n| h.7_feature5 | Unified explanation: This neuron activates on language patterns associated with conveying moral lessons, advice, and guidance on appropriate behavior in children\u2019s stories or parental scenarios. It frequently fires on modal verbs like \"should\" and \"can\" when characters are learning about right and wrong actions, facing consequences, or being instructed on proper conduct. Examples: 1. \"You should not take things that don\u2019t belong to you,\" said Mom, after catching Timmy taking a candy bar from the store. 2. \"The little boy learned that he can be kind to others by sharing his toys.\" 3. \"If you can\u2019t say something nice, you should not say anything at all,\" advised the teacher to the rowdy class. Diversity Score: 68 Justification: While specializing in moral lessons and guidance, the range of potential lessons, advice, and behavioral instructions is quite broad. It activates across various story elements and moral themes, encompassing a diverse array of instructional language in children\u2019s literature.  |\n| h.7_feature6 | Unified Explanation: This neuron activates when \"&lt;|endoftext|&gt;\" is followed by the beginning of a short, simple story or narrative, often with a moral lesson, cautionary tale, or tragic ending. These stories frequently feature children or animals as main characters, written in a style suitable for young readers. Examples: 1. \"&lt;|endoftext|&gt; Once upon a time, there was a little girl who loved to play in the forest. One day, she wandered too far from home and got lost...\" 2. \"&lt;|endoftext|&gt; A group of young animals decided to explore the old abandoned barn, despite their parents\u2019 warnings. But it was too late when they realized the danger inside...\" 3. \"&lt;|endoftext|&gt; Tommy was a curious boy who couldn\u2019t resist the temptation of the old well in his backyard. He leaned over too far and...\" Diversity Score: 61 Justification: While specific to children\u2019s stories, the diversity is high, involving various characters, settings, actions, and themes. It captures a range of narrative elements, including plot structure, character archetypes, and common literary devices.  |\n| h.7_feature12 | Unified explanation: This neuron activates at the beginning of short stories or narratives aimed at children. The consistent trigger is the token \"&lt;|endoftext|&gt;\", indicating the start of a new text sample. It recognizes the opening of simple narrative structures, often involving young protagonists, animal characters, moral lessons, or elements of danger or misfortune. Examples: 1. \"&lt;|endoftext|&gt; Once upon a time, there was a little girl named Lily who loved to explore the enchanted forest near her home.\" 2. \"&lt;|endoftext|&gt; In a cozy burrow, a family of rabbits lived happily until a hungry fox threatened their safety.\" 3. \"&lt;|endoftext|&gt; Tommy the turtle was always in a hurry, but his impatience nearly cost him his life when he wandered too far from home.\" Diversity Score: 71 Justification: While focused on children\u2019s stories, the range of possible stories and themes is quite diverse, involving different characters, settings, plots, and outcomes. |", "caption": "Table 2: ERM-trained GSAE Features", "description": "This table presents a detailed analysis of the features learned by a Sparse Autoencoder (SAE) trained using Empirical Risk Minimization (ERM). It focuses on features extracted from the 7th layer of a language model, showing the features' explanations and diversity scores.  The explanations offer insights into what kinds of linguistic patterns each feature captures, offering an understanding of how the model processes information. The diversity score provides a quantitative measure of how broadly each feature is applied within the dataset. This information helps in understanding the model's behavior and disentangling its internal representations.", "section": "3.2 Tilted ERM for Enhanced Detection"}, {"content": "| Feature | Explanation |\n|---|---| \n| h.7_feature8 | Unified explanation: This feature detects the indefinite article \"an\" when introducing new or significant elements in children\u2019s stories or simple narratives. It activates when \"an\" precedes a noun at the beginning of a sentence or clause, signaling a novel element important to the plot.  Examples: 1. \"An old man lived in a tiny house by the forest.\" 2. \"One day, an unexpected visitor arrived at the village.\" 3. \"Deep in the ocean, an ancient treasure awaited discovery.\" Diversity Score: 65 Justification: High diversity in types of elements introduced (characters, objects, concepts) within children\u2019s stories, but limited to narrative contexts.  | \n| h.7_feature13 | Unified explanation: This feature captures interjections or exclamations in children\u2019s stories or dialogues expressing surprise, excitement, or drawing attention to something noteworthy. Tokens like \"Wow\" or \"Look\" often appear at the beginning of quoted speech or exclamations. Examples: 1. \"Wow! Look at that giant castle!\" a child might exclaim upon seeing an impressive structure. 2. \"Look, the caterpillar turned into a butterfly!\" a character might say, pointing out a transformation. 3. \"Wow, that was a close one!\" someone might remark after narrowly avoiding danger. Diversity Score: 71 Justification: While specific to interjections, these can be used across a wide range of contexts and story elements, reflecting a high degree of diversity within children\u2019s stories and dialogues.  | \n| h.7_feature14 | Unified explanation: This neuron predicts words related to pleasant or appetizing food experiences in children\u2019s stories or simple narratives. It activates on the first few letters of words like \"yummy\", \"candy\", \"crumbs\", and \"celery\", generating vocabulary associated with tasty treats, cooking, or domestic activities. Examples: 1. \"The little girl licked her lips as she stared at the yummy chocolate cake.\" 2. \"After playing outside, the kids ran to the kitchen for a snack of celery and peanut butter.\" 3. \"Mom swept up the crumbs from the cookies the children had enjoyed earlier.\" Diversity Score: 53 Justification: While primarily focused on food-related words, it recognizes a range of vocabulary including adjectives, nouns, and verbs related to food experiences in children\u2019s stories.  | \n| h.7_feature17 | Unified explanation: This neuron processes text related to children\u2019s stories, simple narratives, and basic concepts in children\u2019s literature. It responds to character names, diminutives, dialogue markers, sensory experiences, emotions, onomatopoeias, common objects, food items, childhood experiences, simple actions, and basic vocabulary. Examples: 1. \"Ducky waddled over to the lollipop on the ground. \u2019Yum!\u2019 he exclaimed, gobbling it up.\" 2. \"Ow, ow, ow! Timmy had scraped his knee on the rough sand. Mom kissed it better and gave him a sausage to cheer him up.\" 3. \"Bark, bark! Spidey\u2019s new puppy was digging in the garden, scattering the soil everywhere. \u2019No, no, pup!\u2019 scolded Spidey.\" Diversity Score: 85 Justification: Displays very high diversity within children\u2019s literature, responding to a wide range of elements including characters, emotions, actions, objects, sensory experiences, and dialogue patterns. |", "caption": "Table 3: TERM-trained GSAE Features", "description": "This table presents a detailed analysis of features extracted by a Generative Sparse Autoencoder (GSAE) trained using Tilted Empirical Risk Minimization (TERM).  Each row represents a distinct feature, providing its numerical identifier (Feature), a concise explanation of the patterns the feature recognizes within the TinyStories dataset, and a diversity score that quantifies how broadly the feature is applied within the data. The explanations describe the kinds of textual elements captured by each feature (e.g., dialogue, character names, actions, descriptions of settings), illustrating its function within the dataset.  The diversity score offers a metric to judge how many different contexts or elements within the dataset are represented by each feature, offering a way to measure the feature's specificity or generality.", "section": "3.2 Tilted ERM for Enhanced Detection"}]