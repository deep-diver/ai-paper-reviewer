[{"figure_path": "https://arxiv.org/html/2502.03544/x1.png", "caption": "Figure 1: Handling \u201cdouble\" points in AG2. It is hard to prove that the intersection of a\ud835\udc4eaitalic_a, b\ud835\udc4fbitalic_b is on \u03c9\ud835\udf14\\omegaitalic_\u03c9. But if a language model suggests a construction X\u2032\u2208a\u2229\u03c9superscript\ud835\udc4b\u2032\ud835\udc4e\ud835\udf14X^{\\prime}\\in a\\cap\\omegaitalic_X start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT \u2208 italic_a \u2229 italic_\u03c9, then DDAR can prove the goal by proving X\u2032\u2208bsuperscript\ud835\udc4b\u2032\ud835\udc4fX^{\\prime}\\in bitalic_X start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT \u2208 italic_b, and hence X=X\u2032\ud835\udc4bsuperscript\ud835\udc4b\u2032X=X^{\\prime}italic_X = italic_X start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT.", "description": "The figure illustrates how AlphaGeometry2 handles situations where two points share the same coordinates.  A common geometry problem involves proving that a point X is on a circle \u03c9\u03c9\n\u03c9. However, X is defined as the intersection of two lines.  Directly proving X lies on \u03c9\u03c9\n\u03c9 is difficult for a symbolic engine.  Instead, a language model suggests constructing an auxiliary point X\u2032X\u2032 that's also at the intersection of line a and circle \u03c9\u03c9\n\u03c9.  Then, AlphaGeometry2 proves that X\u2032X\u2032 is on line b. Because X and X\u2032X\u2032 share coordinates, and both are on line b and a,  this indirectly proves that the original point X lies on \u03c9\u03c9\n\u03c9.", "section": "4.1 Handling double points"}, {"figure_path": "https://arxiv.org/html/2502.03544/x2.png", "caption": "(a) AG2 includes more complicated/longer problems compared to AG1.", "description": "This figure shows a comparison of problem size distributions in the training data for AlphaGeometry1 (AG1) and AlphaGeometry2 (AG2).  The x-axis represents the number of points in a geometry problem, indicating its complexity. The y-axis shows the number of problems of each size in the training data.  The bars show that AG2's training data contains a significantly larger number of problems with a higher number of points, indicating that AG2 was trained on more complex geometry problems than AG1. This difference is visually apparent, showing AG2 trained on more complex problems and thus providing a rationale for its improved performance.", "section": "5. Better synthetic training data"}, {"figure_path": "https://arxiv.org/html/2502.03544/x3.png", "caption": "(b) AG2 has a more balanced distribution of examples per question type.", "description": "This figure shows a bar chart comparing the distribution of question types in the training data of AlphaGeometry1 (AG1) and AlphaGeometry2 (AG2).  AlphaGeometry2 demonstrates a more even distribution across various geometry problem types, unlike AG1 which shows a skewed distribution. This balance in AG2's training data ensures the model is exposed to a more diverse range of geometry problem characteristics, thus leading to improved performance and generalization. ", "section": "Better synthetic training data"}, {"figure_path": "https://arxiv.org/html/2502.03544/x4.png", "caption": "(c) AG2 has a much more balanced mix between proofs with auxiliary points and proofs without (50:50 in AG2 vs 9:91 in AG1).", "description": "The figure shows a comparison of the distribution of auxiliary points in the training data between AlphaGeometry1 (AG1) and AlphaGeometry2 (AG2). AG1 has a highly skewed distribution, with only 9% of the problems using auxiliary points. In contrast, AG2 has a significantly more balanced distribution, with approximately 50% of the problems utilizing auxiliary points.", "section": "5 Better synthetic training data"}, {"figure_path": "https://arxiv.org/html/2502.03544/x5.png", "caption": "Figure 2: Training data distributions for AG2 compared to AG1.", "description": "This figure compares the training data distributions of AlphaGeometry 1 (AG1) and AlphaGeometry 2 (AG2).  Panel (a) shows a bar chart illustrating the distribution of problem sizes (number of points) in the training data for both AG1 and AG2. AG2 includes more problems with a higher number of points, indicating a larger and more complex dataset. Panel (b) displays a bar chart comparing the distribution of question types in AG1 and AG2 training data.  AG2 shows a more balanced distribution across different question types, unlike AG1 which was skewed. Panel (c) shows a bar chart comparing the distribution of problems with and without auxiliary points in the AG1 and AG2 training sets. AG2 exhibits a much more balanced distribution (approximately 50/50) compared to AG1 (9/91). Overall, the figure demonstrates that AG2's training data is both larger and more diverse than that of AG1, improving the model's ability to handle a wider range of problem complexities and types.", "section": "5 Better synthetic training data"}, {"figure_path": "https://arxiv.org/html/2502.03544/x6.png", "caption": "Figure 3: Basic greedy algorithm to find a minimal set of points satisfying a monotonic predicate check.", "description": "This figure presents a Python function named `prune_points`.  The function takes a set of points and a function `check_provable` as input.  `check_provable` is a function that determines if a given subset of points satisfies a specific condition (a \"monotonic predicate\"). The algorithm iterates through the points in reverse topological order (meaning it starts with points that don't depend on others for their definition). In each iteration, it checks if removing the current point still allows `check_provable` to return true. If it does, the point is removed from the set of points (`pruned`). The function returns the minimal set of points that still satisfy the condition. This greedy approach efficiently finds a minimal set of points, ensuring that the condition remains true without unnecessary points.", "section": "Faster data generation algorithm"}, {"figure_path": "https://arxiv.org/html/2502.03544/x7.png", "caption": "Figure 4: Overview of our search algorithm. We employ several different search trees which can share facts they proved via a special knowledge sharing mechanism.", "description": "The figure illustrates the AlphaGeometry2 search algorithm, called Shared Knowledge Ensemble of Search Trees (SKEST).  Instead of using a single search tree, the algorithm employs multiple search trees, each with a different configuration (e.g., classic beam search, multi-auxiliary point search, operator search).  These trees operate in parallel and share proven facts through a central knowledge base. This shared knowledge accelerates the search process and enhances the robustness of the system by allowing different search strategies to complement each other.", "section": "6 Novel search algorithm"}, {"figure_path": "https://arxiv.org/html/2502.03544/x8.png", "caption": "Figure 5: Learning curves for AlphaGeometry2 language models of different sizes in terms of parameter count (\u201cm\" - million, \u201cB\" - billion). Increasing the model size results in decreasing loss for train, eval and IMO evaluation sets.", "description": "This figure shows the learning curves for three AlphaGeometry2 language models with different numbers of parameters (51 million, 176 million, and 3.3 billion).  The x-axis represents the number of tokens processed during training, and the y-axis represents the loss.  Three curves are shown for each model: one for the training data, one for a held-out evaluation dataset, and one specifically for the International Mathematical Olympiad (IMO) evaluation dataset. The results indicate that as the model size increases, the loss consistently decreases across all three datasets (train, eval, and IMO).  This demonstrates improved model performance with a larger number of parameters.", "section": "7 Better language model"}, {"figure_path": "https://arxiv.org/html/2502.03544/x9.png", "caption": "Figure 6: Ratio of unique samples for various temperatures for top-k sampling.", "description": "This figure shows the impact of temperature on the diversity of samples generated using top-k sampling.  The x-axis represents the temperature values, and the y-axis shows the ratio of unique samples obtained for each temperature.  As temperature increases, the ratio of unique samples also increases, indicating that higher temperatures lead to more diverse samples. This demonstrates the effect of temperature as a hyperparameter in controlling the randomness of the sampling process.", "section": "7.2 Inference setup"}, {"figure_path": "https://arxiv.org/html/2502.03544/x10.png", "caption": "Figure 7: Number of 2000-2024 IMO problems solved by one language model as a function of seen tokens during training.", "description": "This figure shows the relationship between the number of 2000-2024 International Mathematical Olympiad (IMO) geometry problems solved by a single language model and the amount of data (measured in tokens) it has been trained on.  It illustrates how the model's problem-solving ability improves as it is exposed to more training data.", "section": "7. Better language model"}, {"figure_path": "https://arxiv.org/html/2502.03544/x11.png", "caption": "Figure 8: AlphaGeometry2 results on all 2000-2024 IMO geometry problems. Problems are grouped together based on their status, and ordered chronologically within the groups.", "description": "This figure presents a visual representation of AlphaGeometry2's performance on International Mathematical Olympiad (IMO) geometry problems from 2000 to 2024. Each problem is categorized into one of three groups: 'Not Attempted', 'Not Solved', and 'Solved'.  Within each group, problems are chronologically ordered, providing a clear visualization of the system's progress over time. The use of a visual matrix aids in understanding AlphaGeometry2's success rate on various problems and highlights which problems were successfully solved using only a deductive database arithmetic reasoning (DDAR) engine, without the need for a language model.", "section": "Results"}, {"figure_path": "https://arxiv.org/html/2502.03544/x12.png", "caption": "Figure 9: Number of 2000-2024 IMO geometry problems solved for different inference settings with one search tree. We start with beam size 512, beam depth 4, 32 samples and vary one of the parameters while keeping others fixed.", "description": "This figure displays the impact of different inference settings on the number of solved 2000-2024 IMO geometry problems.  The experiment uses a single search tree and systematically varies one parameter (beam size, beam depth, or number of samples) while holding the others constant at their initial values (beam size=512, beam depth=4, number of samples=32).  The results are presented as a graph showing the count of problems solved for each parameter setting variation, allowing for a clear visual comparison of the effectiveness of each setting.", "section": "7.2. Inference setup"}, {"figure_path": "https://arxiv.org/html/2502.03544/x13.png", "caption": "Figure 10: Learning curves for two 3B models: one is trained from scratch and another one pre-trained on math data and then fine-tuned on the AG data. The model pre-trained on math has initially lower loss but both converge to the same point after training for 200B tokens.", "description": "This figure displays the learning curves for two 3-billion parameter language models.  One model was trained from scratch, while the other was initially pre-trained on mathematical datasets before undergoing fine-tuning on AlphaGeometry (AG) data.  The plot shows the loss values over the training process, measured in terms of the number of tokens processed. Notably, the model that started with pre-training on math data exhibits a lower initial loss. However, both models eventually converge to a similar loss level after training with 200 billion tokens.", "section": "A Fine-tuning of math specialized language models on AG data"}, {"figure_path": "https://arxiv.org/html/2502.03544/x14.png", "caption": "Figure 11: IMO 2024 P4 diagram with AlphaGeometry auxiliary construction, point E\ud835\udc38Eitalic_E.", "description": "The figure displays the diagram for problem IMO 2024 P4, enhanced with AlphaGeometry's auxiliary construction point E. This problem involves a triangle ABC with incenter I, where AB < AC < BC. Point X lies on line BC (excluding C), such that the line through X parallel to AC is tangent to the incircle. Similarly, point Y lies on BC (excluding B) such that the line through Y parallel to AB is tangent to the incircle. Line AI intersects the circumcircle of triangle ABC at point P.  K and L are midpoints of AC and AB, respectively. The problem's objective is to prove that \u2220KIL + \u2220YPX = 180\u00b0.  Point E, added by AlphaGeometry, is crucial to solving this complex geometry problem. The diagram illustrates the geometric relationships between the various points and lines involved, highlighting the auxiliary construction.", "section": "C. Featured AlphaGeometry2 solutions"}, {"figure_path": "https://arxiv.org/html/2502.03544/x15.png", "caption": "Figure 12: IMO 2013 P3 diagram with AlphaGeometry auxiliary construction, point D\ud835\udc37Ditalic_D. It allows proving B\u2062A1\u2062D\u2062Ia\ud835\udc35subscript\ud835\udc341\ud835\udc37subscript\ud835\udc3c\ud835\udc4eBA_{1}DI_{a}italic_B italic_A start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT italic_D italic_I start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT is cyclic, which is the key to solve this problem.", "description": "Figure 12 shows the diagram for IMO 2013 Problem 3, enhanced with the auxiliary point D added by AlphaGeometry2.  The key insight provided by AlphaGeometry2 is identifying that points B, A\u2081 (A sub 1), D, and I\u2090 (I sub a) are concyclic (lie on the same circle). This is a crucial step in solving the problem. The figure visually demonstrates this concyclicity and other geometric elements used in the proof.", "section": "C. Featured AlphaGeometry2 solutions"}, {"figure_path": "https://arxiv.org/html/2502.03544/x16.png", "caption": "Figure 13: IMO 2014 P3 diagram with AlphaGeometry auxiliary constructions.", "description": "This figure shows the diagram for problem 3 from the 2014 International Mathematical Olympiad (IMO) as interpreted by AlphaGeometry2.  The diagram includes the original problem's points and lines, as well as additional points and lines constructed by AlphaGeometry2's algorithm to aid in solving the problem. These auxiliary constructions, added by the AI system, illustrate the steps of its reasoning process and are crucial to the solution path it found.", "section": "C. Featured AlphaGeometry2 solutions"}, {"figure_path": "https://arxiv.org/html/2502.03544/x17.png", "caption": "Figure 14: IMOSL 2009 G7 diagram with AlphaGeometry auxiliary constructions (colored red), key cyclic properties (colored polygons) and key similar triangle pairs (colored triangle pairs).", "description": "This figure shows the solution to problem G7 from the 2009 International Mathematical Olympiad shortlist, as solved by AlphaGeometry2.  The diagram displays triangle ABC with its incenter I, and the incenters X, Y, and Z of triangles BIC, CIA, and AIB respectively.  AlphaGeometry2's auxiliary constructions (shown in red) are used to create several key cyclic quadrilaterals (colored polygons) and similar triangle pairs (colored triangle pairs). These geometric relationships are crucial for proving that if triangle XYZ is equilateral, then triangle ABC must also be equilateral. The colored elements highlight the key geometric properties and relationships exploited by AlphaGeometry2 in its solution.", "section": "C. Featured AlphaGeometry2 solutions"}]