{"references": [{" publication_date": "2017", "fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "reason": "This paper introduced the Transformer architecture, a fundamental building block of many modern large language models and has significantly impacted the field of natural language processing and beyond.  The attention mechanism is crucial for MarDini's design, enabling efficient processing of long sequences and complex relationships between frames in video data.", "section_number": 1}, {" publication_date": "2020", "fullname_first_author": "Jonathan Ho", "paper_title": "Denoising diffusion probabilistic models", "reason": "This seminal work introduced denoising diffusion probabilistic models (DDPMs), a powerful framework for generating high-quality images and videos.  DDPMs are the foundation of MarDini's generation model, providing a stable and scalable method for generating continuous visual data.", "section_number": 1}, {" publication_date": "2021", "fullname_first_author": "Kaiming He", "paper_title": "Masked autoencoders are scalable vision learners", "reason": "This paper introduced masked autoencoders (MAE), a self-supervised learning approach for image recognition.  The concept of masked auto-regression (MAR), a key component of MarDini, is inspired by MAE's success in efficiently handling high-dimensional visual data.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Robin Rombach", "paper_title": "High-resolution image synthesis with latent diffusion models", "reason": "This work demonstrated the efficacy of diffusion models for high-resolution image generation. MarDini builds upon this success, extending the approach to video generation with an efficient asymmetric architecture.", "section_number": 1}, {" publication_date": "2021", "fullname_first_author": "Imanol Schlag", "paper_title": "Taming transformers for high-resolution image synthesis", "reason": "This paper proposed the use of transformers in high-resolution image generation, which is closely related to MarDini's architecture and the use of transformers in both planning and generation models.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Tianhong Li", "paper_title": "Masked generative encoder to unify representation learning and image synthesis", "reason": "This work proposes a masked autoregressive model for image synthesis, influencing MarDini's design philosophy and the use of masked autoregression for efficient planning.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Abhimanyu Dubey", "paper_title": "The LLAMA 3 herd of models", "reason": "This paper describes the training strategy and architecture of the Llama models, a family of large language models that influence MarDini's planning model design and training techniques.  MarDini adopts and adapts several key design choices from Llama models.", "section_number": 2}, {" publication_date": "2019", "fullname_first_author": "Biao Zhang", "paper_title": "Root mean square layer normalization", "reason": "This paper introduced RMS-Norm, a layer normalization technique used in MarDini's planning model to enhance training stability and improve the model's performance.  The choice of RMS-Norm is crucial for handling the high-dimensional input data in the MAR model.", "section_number": 22}, {" publication_date": "2023", "fullname_first_author": "Andreas Blattmann", "paper_title": "Stable video diffusion: Scaling latent video diffusion models to large datasets", "reason": "This paper presented a scalable method for training video diffusion models, which is crucial for MarDini's training recipe and ability to scale to larger video datasets.  It also addresses training instability, a challenge MarDini directly tackles.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Andreas Blattmann", "paper_title": "Align your latents: High-resolution video synthesis with latent diffusion models", "reason": "This work is important because it shows that high-resolution video synthesis with latent diffusion models is possible, and it provides a baseline for comparing MarDini's performance. It relates to MarDini's use of diffusion models in video generation.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Huiwen Chang", "paper_title": "Muse: Text-to-image generation via masked generative transformers", "reason": "This paper demonstrates the use of masked autoregressive models with transformers for text-to-image generation, a task relevant to MarDini's image-to-video generation capabilities. It also shows the potential of applying MAR to image generation tasks that MarDini extends to video generation.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Siddhant Jain", "paper_title": "Video interpolation with diffusion models", "reason": "This paper introduces VIDIM-Bench, a benchmark used in evaluating MarDini's performance on video interpolation, making it crucial for establishing MarDini's competitive position in the field.  Understanding the benchmark is essential for interpreting the results.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Ziqi Huang", "paper_title": "VBench: Comprehensive benchmark suite for video generative models", "reason": "This paper introduced VBench, another important benchmark for video generation models.  MarDini's performance evaluation on VBench demonstrates its competitive capabilities in image-to-video generation, showing its efficacy compared to other methods.", "section_number": 3}, {" publication_date": "2022", "fullname_first_author": "Fitsum Reda", "paper_title": "FILM: Frame interpolation for large motion", "reason": "FILM is one of the state-of-the-art video interpolation methods, serving as a strong baseline for comparing MarDini's performance.  The comparison highlights MarDini's improvement in handling large motions during video interpolation.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Haozhe Liu", "paper_title": "Faster diffusion via temporal attention decomposition", "reason": "This is a related work using temporal attention for video generation, which MarDini partially adopts for the generation part of its asymmetric model.  It directly relates to the efficiency considerations made in MarDini's design.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Tianhong Li", "paper_title": "Amt: All-pairs multi-field transforms for efficient frame interpolation", "reason": "This paper introduces AMT, another leading video interpolation method, providing a strong baseline for comparing MarDini's performance.  The comparison demonstrates MarDini's competitive results and improved scalability.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Jiahui Yu", "paper_title": "Language model beats diffusion-tokenizer is key to visual generation", "reason": "This paper relates to the use of large language models in image and video generation.  While MarDini does not directly use a language model, it explores an alternative approach to achieve scalable video generation without the need for pre-training on images, offering another perspective on the field.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Huiwen Chang", "paper_title": "Pixart-\u03c3: Weak-to-strong training of diffusion transformer for 4k text-to-image generation", "reason": "This work utilizes diffusion transformers for high-quality text-to-image generation, which is relevant to MarDini's image-to-video generation capabilities.  Comparing MarDini's performance to this method highlights the differences in efficiency and the potential benefits of MarDini's asymmetric architecture.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Tom B Brown", "paper_title": "Language models are few-shot learners", "reason": "This paper showed the remarkable ability of large language models to learn from a small number of examples. MarDini leverages the masked autoregressive principle, which enables it to learn from limited training data while effectively generalizing to various tasks.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Aditya Ramesh", "paper_title": "Zero-shot text-to-image generation", "reason": "This paper is significant because it presented a method for generating high-quality images from text descriptions without explicit training on specific image categories. MarDini takes inspiration from this technique and focuses on generating videos with no text input, demonstrating that its model is suitable for diverse generation tasks.", "section_number": 4}]}