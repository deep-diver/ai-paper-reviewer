[{"figure_path": "2410.20280/figures/figures_3_0.png", "caption": "Figure 1 MarDini Training Pipeline Overview. A latent representation is computed for unmasked frames that serve as a conditional signal to a generative process. On the first hand, we have a planning model that autoregressively encodes global conditioning signals from a low-resolution version of the unmasked latent inputs. On the other hand, the planning signals are fed to the diffusion-based generation model through cross-attention layers. A high-resolution version of the input conditions is also ingested by the diffusion model, enabling generation with a coherent temporal structure and a direct mechanism to attend to fine-grained details of the unmasked frames. MarDini is trained end-to-end via masked frame-level diffusion loss.", "description": "The figure illustrates the training pipeline of MarDini, showing how a latent representation is generated and used to condition the planning and generation models in an asymmetric network design.", "section": "2 MarDini: An Efficient and Asymmetric Video Diffusion Model"}, {"figure_path": "2410.20280/figures/figures_4_0.png", "caption": "Figure 2 MarDini Design Details. MarDini employs a transformer architecture for both the planning and generation models, incorporating a DiT-style block for the generation model and a Llama-style block for the planning model. We set L1 \u226b L2, where L\u2081 and L2 refer to the number of layers in the planning and generation model respectively.", "description": "The figure shows the detailed architecture of the MarDini model, illustrating the asymmetric design of the planning and generation networks.", "section": "2.3 Architecture Design"}, {"figure_path": "2410.20280/figures/figures_5_0.png", "caption": "Figure 3 Identity Attention Design Details in DM. In this setup, [REF] tokens only attend to themselves, while [NOISE] tokens attend to all other tokens across different frames.", "description": "The figure illustrates the design of the Identity Attention mechanism in the DM model, showing how [REF] and [NOISE] tokens attend to each other.", "section": "2.3 Architecture Design"}, {"figure_path": "2410.20280/figures/figures_6_0.png", "caption": "Figure 4 MarDini Training Manual. We list the mask ratios, frame rate (FPS), number of frames, and the size of training data for each training stage. This training manual applies to both small (MarDini-S) and large (MarDini-L) models. Note that the total training data refers to the amount of data observed by the model for gradient updates, rather than the vanilla size of the training dataset. Our final model checkpoints are highlighted in gray.", "description": "This figure shows the multi-stage training pipeline of MarDini, illustrating how the model is trained progressively to increase task difficulty from simple video interpolation to full video generation.", "section": "2.4 MarDini Training Recipes"}, {"figure_path": "2410.20280/figures/figures_6_1.png", "caption": "Figure 4 MarDini Training Manual. We list the mask ratios, frame rate (FPS), number of frames, and the size of training data for each training stage. This training manual applies to both small (MarDini-S) and large (MarDini-L) models. Note that the total training data refers to the amount of data observed by the model for gradient updates, rather than the vanilla size of the training dataset. Our final model checkpoints are highlighted in gray.", "description": "This figure shows the multi-stage progressive training pipeline of MarDini, detailing the mask ratios, frame rates, number of frames, and training data for each stage.", "section": "2.4 MarDini Training Recipes"}, {"figure_path": "2410.20280/figures/figures_9_0.png", "caption": "Figure 5 MarDini's generations with and without the planning model. Here we show video frames generated when conditioning on the middle frame. Without MAR's planning signal, DM generates degraded motion, such as pixel distortions (highlighted in red, left) or incorrect motions (highlighted in blue, right).", "description": "The figure compares video frames generated with and without the MAR planning model, highlighting improved motion and pixel details when using the MAR model.", "section": "3.1 Ablation Studies and Analysis"}, {"figure_path": "2410.20280/figures/figures_12_0.png", "caption": "Figure 1 MarDini Training Pipeline Overview. A latent representation is computed for unmasked frames that serve as a conditional signal to a generative process. On the first hand, we have a planning model that autoregressively encodes global conditioning signals from a low-resolution version of the unmasked latent inputs. On the other hand, the planning signals are fed to the diffusion-based generation model through cross-attention layers. A high-resolution version of the input conditions is also ingested by the diffusion model, enabling generation with a coherent temporal structure and a direct mechanism to attend to fine-grained details of the unmasked frames. MarDini is trained end-to-end via masked frame-level diffusion loss.", "description": "The figure illustrates the training pipeline of MarDini, showing the asymmetric design with a heavy-weight MAR planning model and a light-weight diffusion generation model.", "section": "2 MarDini: An Efficient and Asymmetric Video Diffusion Model"}, {"figure_path": "2410.20280/figures/figures_12_1.png", "caption": "Figure 8 Visualization of Video Expansion. The model is conditioned on a sequence of 16 consecutive frames to predict the subsequent 12 frames. The video data used for visualization is sourced from publicly available research dataset (Nan et al., 2024).", "description": "Figure 8 shows the video expansion results where the model is given 16 frames as input to predict the next 12 frames.", "section": "3.4 Additional Applications"}, {"figure_path": "2410.20280/figures/figures_20_0.png", "caption": "Figure 9 Failure case of reconstruction metrics (SSIM, LPIPS) in video interpolation. We visualize two generated frames together with their corresponding ground-truth frames. While the frames generated by MarDini are sharper than competitors, their corresponding reconstruction scores are worse.", "description": "Figure 9 shows that while MarDini generates sharper frames than other methods, its reconstruction scores (SSIM, LPIPS) are sometimes lower due to blurrier images sometimes receiving higher reconstruction error scores.", "section": "A Reconstruction metrics in Video Interpolation"}, {"figure_path": "2410.20280/figures/figures_20_1.png", "caption": "Figure 10 Visualization of video interpolation methods conditioned on the first and last frames. We present the generated frames from FILM (Reda et al., 2022), LDMVFI (Danier et al., 2024), VIDIM (Jain et al., 2024), and MarDini. The comparison results for these methods are sourced from Jain et al. (2024). We have included additional samples in the supplementary materials.", "description": "Figure 10 compares the video interpolation results of MarDini against FILM, LDMVFI, and VIDIM, highlighting MarDini's superior performance in handling complex motions.", "section": "C Visualization of Video Interpolation"}, {"figure_path": "2410.20280/figures/figures_22_0.png", "caption": "Figure 1 MarDini Training Pipeline Overview. A latent representation is computed for unmasked frames that serve as a conditional signal to a generative process. On the first hand, we have a planning model that autoregressively encodes global conditioning signals from a low-resolution version of the unmasked latent inputs. On the other hand, the planning signals are fed to the diffusion-based generation model through cross-attention layers. A high-resolution version of the input conditions is also ingested by the diffusion model, enabling generation with a coherent temporal structure and a direct mechanism to attend to fine-grained details of the unmasked frames. MarDini is trained end-to-end via masked frame-level diffusion loss.", "description": "The figure illustrates the training pipeline of MarDini, which uses an asymmetric network design consisting of a heavy-weight planning model and a lightweight generation model to efficiently generate high-resolution videos.", "section": "2 MarDini: An Efficient and Asymmetric Video Diffusion Model"}]