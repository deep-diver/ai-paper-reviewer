[{"figure_path": "https://arxiv.org/html/2411.14430/x2.png", "caption": "Figure 1: Stable Flow. Our training-free editing method is able to perform various types of image editing operations, including non-rigid editing, object addition, object removal, and global scene editing. These different edits are done using the same mechanism.", "description": "This figure showcases the versatility of the Stable Flow method in performing various image editing tasks without any additional training.  It demonstrates the method's ability to seamlessly handle different editing styles such as non-rigid transformations (e.g., changing the pose of a dog), adding objects (e.g., placing a dog next to an avocado), removing objects (e.g., removing a plastic bag), and performing global scene edits (e.g., changing the scene from daytime to nighttime).  The consistent results across diverse edit types highlight the unified mechanism at the core of the Stable Flow approach.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2411.14430/x3.png", "caption": "Figure 2: Leveraging Reduced Diversity. Using the same initial seed with different editing prompts, diffusion models such as (1) SDXL generate diverse results (different identities of the dog and the cat), while (2) FLUX generates a more stable (less diverse) set of results out-of-the-box. However, there are still some unintended differences (the dog is standing in the leftmost column and sitting in the others, the color of the cat is changing, and the road is different on the right). Using our approach, (3) Stable Flow, the edits are stable, maintaining consistency of the unrelated content.", "description": "This figure compares the results of three different diffusion models when performing text-driven image editing using the same initial seed.  The first model, SDXL, shows significant diversity in the generated images, with variations in the identities of the dog and cat and changes in the environment. The second model, FLUX, provides more stable results but still exhibits some unintended inconsistencies (e.g., posture changes in the dog, color variations in the cat, and background changes).  Stable Flow, the authors' proposed method, demonstrates consistent and stable image edits, preserving unrelated image content across different editing prompts.", "section": "Leveraging Reduced Diversity"}, {"figure_path": "https://arxiv.org/html/2411.14430/extracted/6015496/figures/user_study_details/assets/user_study_instructions.jpg", "caption": "Figure 3: Layer Removal. (Left) Text-to-image DiT models consist of consecutive layers connected through residual connections\u00a0[33]. Each layer implements a multimodal diffusion transformer block\u00a0[25] that processes a combined sequence of text and image embeddings. (Right) For each DiT layer, we performe an ablation by bypassing the layer using its residual connection. Then, we compare the generated result on the ablated model with the complete model using a perceptual similarity metric.", "description": "This figure illustrates the concept of layer removal in Diffusion Transformer (DiT) models for image editing. The left panel shows the architecture of a text-to-image DiT model, highlighting its consecutive layers connected by residual connections. Each layer is a multimodal diffusion transformer block processing combined text and image embeddings.  The right panel details the ablation process: for each layer, the layer is bypassed using its residual connection, generating an image. The generated image from the ablated model is then compared to that of the complete model using a perceptual similarity metric to determine the importance of the removed layer.  This method helps identify crucial layers for image generation.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2411.14430/extracted/6015496/figures/user_study_details/assets/user_study_trial.jpg", "caption": "Figure 4: Layer Removal Quantitative Comparison. As explained in Section\u00a03.1, we measured the effect of removing each layer of the model by calculating the perceptual similarity between the generated images with and without this layer. Lower perceptual similarity indicates significant changes in the generated images (Figure\u00a05). As can be seen, removing certain layers significantly affects the generated images, while others have minimal impact. Importantly, influential layers are distributed across the transformer rather than concentrated in specific regions. Note that the first vital layers were omitted for clarity (as their perceptual similarity approached zero).", "description": "This figure shows a graph plotting the perceptual similarity of images generated with and without individual layers of a diffusion model.  The x-axis represents the layer index and the y-axis represents the perceptual similarity, measured using a metric (likely LPIPS or similar).  Each point on the graph corresponds to a single layer, indicating the effect of removing that layer on the image generation process.  A lower perceptual similarity means that removing the layer caused more significant changes in the output image.  The results demonstrate that some layers ('vital layers') have a substantially larger impact than others, which supports the idea of a non-uniform distribution of importance across different layers of the diffusion model. The vital layers that strongly influence image generation are not clustered together but are spread out across the model.", "section": "3.1. Measuring the Importance of DiT Layers"}]