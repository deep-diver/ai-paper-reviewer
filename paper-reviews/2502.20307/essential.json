{"importance": "This paper introduces a novel method that allows the pre-trained text-to-video diffusion model without additional training, opening new research avenues for generating open-domain looping videos with dynamic motions. The generated results show better visual quality.", "summary": "Mobius generates seamless looping videos from text using latent shift, repurposing pre-trained models without training.", "takeaways": ["Mobius generates seamless looping videos from text descriptions without requiring training.", "The latent shift strategy enables the generation of looping videos of arbitrary lengths.", "Frame-invariance latent decoding and Rotary Position Encoding interpolation enhance video quality and temporal consistency."], "tldr": "Current cinemagraph methods need manual effort, such as camera stabilization and object annotation, focusing on limited movement. To solve the issue, this paper defines a new research problem beyond cinemagraph synthesis: generating seamless looping videos from text descriptions using pre-trained text-to-video models. This allows more dynamic motions and natural visual effects, generating videos that are unusual in real life.\n\nTo address the challenge, **Mobius** is presented to iteratively transform the latent position, ensuring temporal consistency. It constructs a cycle by connecting the starting and ending noise of the videos and shifts the first-frame latent to the end in each step. The experiments also show that it can generate videos with longer context, utilizing NTK-aware interpolation, extending the Rotary Position Embedding.", "affiliation": "Chongqing University of Post and Telecommunications, China", "categories": {"main_category": "Computer Vision", "sub_category": "Video Understanding"}, "podcast_path": "2502.20307/podcast.wav"}