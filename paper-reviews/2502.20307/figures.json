[{"figure_path": "https://arxiv.org/html/2502.20307/x1.png", "caption": "Figure 1. \nWithout any training, the proposed Mobius can generate seamless looping videos using the pre-trained Text-to-Video latent diffusion model directly. Can you identify the end in the above video?\nBest viewed with Acrobat Reader. Click the video to play the animation clips. We also give these examples in the supplementary video. Project page: http://mobius-diffusion.github.io.", "description": "This figure showcases the capability of the Mobius model to generate seamless looping videos from text prompts alone, without any further training.  Two example videos are shown.  The first depicts a young female activist holding a flag, while the second shows a koala eating eucalyptus leaves.  The seamless looping nature of the generated videos is highlighted, demonstrating the model's ability to create engaging, high-quality visual content from text descriptions.", "section": "1 INTRODUCTION"}, {"figure_path": "https://arxiv.org/html/2502.20307/x2.png", "caption": "Figure 2. Latent Shift for looping video generation. Taking 4 latent toys pre-trained Video Diffusion Models\u00a0(VDM) as an example, we build a latent cycle and shift the start point in each denoising step in inference for text-guided looping video generation. Notice that, the shifting is conducted in the latent space, we emit the latent encoder and decoder for easy understanding.", "description": "Figure 2 illustrates the Mobius method for generating looping videos.  It uses a pre-trained Video Diffusion Model (VDM). The process begins by creating a cycle of latent representations from a text prompt.  This cycle represents the video's entire duration, compressed into a latent space.  During inference, the method shifts the starting point of the latent cycle one step forward in each denoising iteration. By gradually shifting this starting point, the denoising process uses different parts of the latent cycle as context, resulting in a seamlessly looping video without abrupt transitions. The figure highlights that this shifting occurs in the latent space for efficiency and clarity. The actual encoder and decoder components of the VDM are omitted in the diagram for simplicity.", "section": "3 METHOD"}, {"figure_path": "https://arxiv.org/html/2502.20307/x3.png", "caption": "Figure 3. Frame-invariance latent decoding reduces the artifacts caused by the 3D VAE decoding.", "description": "The figure illustrates how the 3D Variational Autoencoder (VAE) used in video generation can introduce artifacts.  Specifically, it shows how the inconsistent treatment of the first frame's latent during encoding and decoding can lead to visual imperfections in generated videos, particularly for seamless looping videos.  The method proposed in the paper, 'frame-invariant latent decoding', aims to mitigate these artifacts by ensuring that all frames are processed equally in the latent space, leading to improved video quality.", "section": "3.3 Frame-Invariance Latent Decoding"}, {"figure_path": "https://arxiv.org/html/2502.20307/x4.png", "caption": "Figure 4. We illustrate this with the example of the toy latent video diffusion model with a context window equal to 4. The utilized RoPE-Interp. enables longer video context without training by interpolation.", "description": "Figure 4 illustrates the concept of Rotary Position Embedding Interpolation (RoPE-Interp).  It uses a simplified 'toy' latent video diffusion model with a context window of only 4 frames to show how RoPE-Interp. works.  In standard diffusion models, the model only considers a limited number of preceding frames (the context window) when generating the next frame. RoPE-Interp. overcomes this limitation by cleverly interpolating the positional information, effectively extending the context window and allowing the model to 'see' and utilize more of the preceding video frames. This is achieved without the need for further training of the model, demonstrating the power of RoPE-Interp. in enabling longer video context.", "section": "3.4 Rotary Position Embedding Interpolation"}, {"figure_path": "https://arxiv.org/html/2502.20307/x5.png", "caption": "Figure 5. Compare with other methods. We give the first frame, the intermediate frame, and the last frame for comparison. Notice that, both Svd-Interp. and Cog-Interp. are frame-interpolation methods, we manually give the same start frame and end frame as key-frames.", "description": "Figure 5 presents a comparison of video generation results from different methods.  Three methods are shown: the proposed method ('Ours'), a latent mixing approach ('Latent Mix'), and two frame interpolation techniques ('Svd-Interp.' and 'Cog-Interp.'). For each method, the figure displays three key frames: the first frame, an intermediate frame, and the last frame.  The comparison highlights the differences in the visual consistency and motion quality achieved by each method. Notably, the frame interpolation techniques (Svd-Interp. and Cog-Interp.) use the same first and last frames as input, which are manually specified, to ensure a fair comparison with the other approaches that generate these frames.", "section": "4.2 Comparison with Other Methods"}, {"figure_path": "https://arxiv.org/html/2502.20307/x6.png", "caption": "Figure 6. Ablation study on different latent skip. The shift step in each denoising iteration will also influence the generated content.", "description": "This ablation study explores how altering the number of latent shifts during each denoising iteration impacts the generated video's content.  Different shift steps represent varying degrees of change applied to the latent variables throughout the video generation process.  The figure visually demonstrates the effects of different skip sizes on the resulting video frames, showcasing how the generated content changes based on the frequency of latent shifting.", "section": "3.2 Latent Shifting"}, {"figure_path": "https://arxiv.org/html/2502.20307/x7.png", "caption": "Figure 7. Ablation study on RoPE-Interp. Under the implementation of latent shifting, different RoPE strategies can have a significant impact on the content of video generation.", "description": "This ablation study investigates the effect of different Rotary Position Embedding (RoPE) interpolation strategies on video generation when using the latent shifting method.  The latent shifting method, described earlier in the paper, introduces temporal consistency by shifting the starting frame's latent representation throughout the denoising process.  This figure displays the results obtained using three different RoPE approaches: one without RoPE interpolation, one with a shifted RoPE interpolation, and one with a fixed RoPE interpolation.  By comparing the generated video frames from each method, the impact of each RoPE strategy on the generated video content is analyzed. The results reveal how each RoPE strategy affects the generated video's coherence and overall quality.", "section": "3.2 Latent Shifting"}, {"figure_path": "https://arxiv.org/html/2502.20307/x8.png", "caption": "Figure 8. Limitation. The generated results might not show a very smooth video in the customized domain, e.g., the illustration, restricted by the pre-trained text-to-video diffusion model.", "description": "Figure 8 demonstrates limitations of the proposed method when generating longer videos in niche domains.  Specifically, it shows that while the model generally produces high-quality looping videos, its performance can degrade in scenarios requiring highly specific or uncommon actions or detailed objects that may not be well represented in the pre-trained model's dataset.  The example shows a sequence of frames from a generated video of an illustration-style scene. The video suffers from inconsistencies in the appearance of objects,  lacking smooth and coherent motion, highlighting a limitation where the model struggles to generate realistic details in less frequently represented scenarios.", "section": "4.5 Limitations"}]