[{"figure_path": "https://arxiv.org/html/2503.16057/extracted/6296282/figures/teaser_v7.png", "caption": "Figure 1: (Left) Our proposed Expert Race routing employs Top-k\ud835\udc58kitalic_k selection over full token-expert affinity logits, achieving the highest flexibility compared to prior methods like Token Choice and Expert Choice. \n(Right) Training curve comparisons between DiT-XL\u00a0[25] and ours. Our model, with equal number of activated parameters, achieves a 7.2\u00d7\\mathbf{7.2\\times}bold_7.2 \u00d7 speedup in iterations when reaching the same training loss.", "description": "This figure shows a comparison of different routing strategies in Mixture of Experts (MoE) models for diffusion transformers. The left panel illustrates the proposed 'Expert Race' routing strategy, which uses a Top-k selection to choose the best combination of tokens and experts, providing greater flexibility compared to previous methods such as 'Token Choice' and 'Expert Choice'. The right panel presents a training curve comparison between the proposed model (Race-DiT) and the DiT-XL model from the literature.  Despite having the same number of activated parameters, the proposed model achieves a 7.2x speedup in reaching the same training loss, demonstrating its efficiency and scalability.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2503.16057/extracted/6296282/figures/pipeline_v6.png", "caption": "Figure 2: The Race-DiT Architecture. We replace the Multi-Layer Perceptron (MLP) with the MoE block, which consists of a Router and multiple Experts. In Race-DiT, the token assignment is done once for all. Each token can be assigned to any number of experts, and each expert can process any number of tokens (including zero).", "description": "This figure illustrates the architecture of Race-DiT, a novel Mixture of Experts (MoE) model for diffusion transformers.  The key innovation is the replacement of the traditional Multi-Layer Perceptron (MLP) with a MoE block. This block comprises a router and multiple experts.  The model's routing mechanism is designed for flexibility: token assignment happens only once, and each token can be routed to zero or more experts, with each expert potentially processing a varying number of tokens. This differs from previous methods, where each token was typically assigned to a fixed number of experts. The flexibility allows for dynamic and efficient resource allocation in processing image data.", "section": "4 Taming Diffusion Models with Expert Race"}, {"figure_path": "https://arxiv.org/html/2503.16057/extracted/6296282/figures/arxiv_logits_assign_v2.png", "caption": "Figure 3: Top-\ud835\udca6\ud835\udca6\\mathcal{K}caligraphic_K Selection Flexibility and Specifications. B\ud835\udc35Bitalic_B: batch size; L\ud835\udc3fLitalic_L: sequence length; E\ud835\udc38Eitalic_E: the number of experts.\n(a) Token Choice selects top-\ud835\udca6\ud835\udca6\\mathcal{K}caligraphic_K experts along the expert dimension for each token. (b) Expert Choice selects top-\ud835\udca6\ud835\udca6\\mathcal{K}caligraphic_K tokens along the sequence dimension for each expert. (c) Expert Race selects top-\ud835\udca6\ud835\udca6{\\mathcal{K}}caligraphic_K across the entire set.", "description": "This figure compares three different routing strategies in Mixture of Experts (MoE) models for diffusion transformers: Token Choice, Expert Choice, and Expert Race.  Token Choice selects the top \ud835\udc3e experts for each token independently. Expert Choice selects the top \ud835\udc3e tokens for each expert independently.  Expert Race, in contrast, performs a global selection, choosing the top \ud835\udc3e token-expert pairs across the entire set, offering the greatest flexibility. The variables \ud835\udc35, \ud835\udc3f, and \ud835\udc38 represent batch size, sequence length, and the number of experts, respectively.", "section": "4 Taming Diffusion Models with Expert Race"}, {"figure_path": "https://arxiv.org/html/2503.16057/extracted/6296282/figures/output_norm.png", "caption": "Figure 4: Toy examples of token assignment. Both of the two cases show perfect load balance that each expert process two tokens. But in the case above, experts 1 and 2 are assigned the same token, as are experts 3 and 4, where the 2-in-4 MoE collapse into 1-in-2. The example below shows a more diverse assignment, making full use of the expert specialization.", "description": "This figure illustrates the concept of load balancing in Mixture of Experts (MoE) models.  The top panel shows a scenario where, despite each expert processing two tokens (achieving perfect load balance), experts 1 and 2 are assigned identical tokens, as are experts 3 and 4. This results in a collapse where the effective number of experts is reduced from four to two, negating the benefits of having multiple experts. The bottom panel demonstrates a more desirable allocation, where each expert is assigned unique tokens, leading to better utilization of expert specialization and improved model performance.", "section": "Load Balancing via Router Similarity Loss"}, {"figure_path": "https://arxiv.org/html/2503.16057/extracted/6296282/figures/allocation.png", "caption": "Figure 5: The norm of each block\u2019s output before added to the shortcuts. The output norm increases rapidly in deep layers, resulting in the weakening of shallow-layer components. This issue is alleviated with our proposed per-layer regularization.", "description": "This figure displays the norm of each block's output in a diffusion model before it's added to the skip connections (shortcuts).  It shows that the norm grows significantly larger in deeper layers compared to shallower layers.  This disparity weakens the shallower layers in terms of their influence on the overall network output. The researchers address this issue with their proposed method: per-layer regularization.", "section": "Per-Layer Regularization for Efficient Training"}, {"figure_path": "https://arxiv.org/html/2503.16057/extracted/6296282/figures/arxiv_dense_vs_moe.png", "caption": "Figure 6: Average token allocation at different time steps. Expert-Race assigns more experts to the more complex denoising time steps, which occur at lower timestep indices that handle finer-grain image details.", "description": "This figure visualizes the dynamic allocation of experts across different time steps during the image denoising process in the Expert Race model.  It shows that the Expert Race method intelligently assigns a higher number of experts to the more challenging time steps, which are those earlier in the process (represented by lower timestep indices) that are focused on generating finer image details. In contrast to other methods, Expert Race adapts the number of experts based on the complexity of the task at each step, allowing for more efficient resource utilization.", "section": "7.2 Routing Strategy"}, {"figure_path": "https://arxiv.org/html/2503.16057/extracted/6296282/figures/arxiv_scaling_law.png", "caption": "Figure 7: Combination usage comparison between different number of experts.", "description": "This figure shows the effectiveness of the proposed router similarity loss in promoting diversity among expert activations.  It compares the usage of expert combinations (pairs of experts activated together) across different model sizes, varying the number of experts from 16 to 128. The models incorporating the router similarity loss show significantly higher combination usage than those without, indicating a more diverse and efficient use of experts. The models using the standard balance loss fall between those with no constraint and the router similarity loss.", "section": "7.4 Load Balance"}, {"figure_path": "https://arxiv.org/html/2503.16057/extracted/6296282/figures/compare_choices_methods_v2.png", "caption": "Figure 8: A comparison between Dense and our MoE models. Our MoE models consistently outperform their Dense counterparts across the FID and training curves. Notably, the MoE model with activation size M shows better performance compared to the Dense model scaled to size XL.", "description": "Figure 8 presents a comparison of the performance of dense and MoE models across different scales (Base, Medium, Large, and Extra-Large).  It shows training loss curves and FID (Fr\u00e9chet Inception Distance) scores for both types of models.  The key takeaway is that the MoE models consistently achieve lower training loss and better FID scores compared to their dense counterparts, demonstrating improved efficiency.  Furthermore, the MoE model with a medium activation size outperforms the dense model at the extra-large scale, highlighting the scalability benefits of the MoE approach.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2503.16057/extracted/6296282/figures/random_generation.jpg", "caption": "Figure 9: Scaling results of DiT-B in different MoE configurations. Our method demonstrates linear performance improvement when varying expert split ratios and increasing the number of candidate experts.", "description": "Figure 9 presents scaling experiments performed on the DiT-B model with various MoE configurations.  The x-axis represents the ratio of hidden dimension split to the number of experts (hidden split ratio) while the y-axis displays the FID score (Fr\u00e9chet Inception Distance), a measure of image generation quality, on the ImageNet 256x256 dataset.  The different colored lines represent different MoE configurations (e.g., 1-in-4, 2-in-8, etc.), where 'x-in-y' indicates that 'x' experts are activated out of 'y' candidates.  The results show a roughly linear decrease in FID scores (improvement in image quality) as the model size increases by adjusting either the number of candidate experts or the split ratio.  This demonstrates that the proposed Expert Race method scales effectively and achieves consistent improvement in performance.", "section": "7.6 Scaling Law"}, {"figure_path": "https://arxiv.org/html/2503.16057/extracted/6296282/figures/PLR_figure.png", "caption": "Figure 10: Top-\ud835\udca6\ud835\udca6\\mathcal{K}caligraphic_K selection flexibility in more extended routing strategies.", "description": "This figure illustrates different routing strategies used in Mixture of Experts (MoE) models.  It compares the flexibility of selecting the top-k tokens and experts in the model.  (a) shows Token Choice which selects top-k experts for each token independently. (b) shows Expert Choice which selects top-k tokens for each expert independently. (c) to (e) expand the selection flexibility by choosing the top-k experts in various combinations of batch, sequence length, and experts. (f) Expert Race which selects top-k across the entire set, offering the maximum flexibility.", "section": "4.2 Expert Race"}, {"figure_path": "https://arxiv.org/html/2503.16057/x2.png", "caption": "Figure 11: Random generated 256\u00d7256256256256\\times 256256 \u00d7 256 samples from RaceDiT-XL/2-4in32. Classifier-free guidance scale is 4.", "description": "This figure displays several 256x256 images generated by the RaceDiT-XL/2-4in32 model.  The model uses a Mixture-of-Experts (MoE) architecture, and the images demonstrate the model's ability to generate diverse and high-quality images. A classifier-free guidance scale of 4 was used during generation.", "section": "7.8 More Results on ImageNet 256 \u00d7 256"}, {"figure_path": "https://arxiv.org/html/2503.16057/extracted/6296282/figures/compare_to_ditmoe.png", "caption": "Figure 12: An illustration of the calculation for Per-Layer Regularization.", "description": "This figure illustrates the Per-Layer Regularization (PLR) method.  The input (hidden activation, h\u1d62) from layer i is fed into a two-layer Multi-Layer Perceptron (MLP) router. The first layer performs a linear transformation and GELU activation, maintaining the original hidden dimension. The second layer splits into two branches: a gating head that produces routing logits for the MoE, and a target head which makes a prediction of the final target (y).  The L2 loss between the prediction from the target head (H(h\u1d62)) and the ground truth target (y) is calculated for each layer. This process aligns intermediate layer outputs with the final target, improving the optimization of the shallow MoE layers. This addresses the issue of shallower layers lagging behind deeper layers in training due to adaLN's amplification of deeper layer outputs.", "section": "6 Per-Layer Regularization for Efficient Training"}, {"figure_path": "https://arxiv.org/html/2503.16057/extracted/6296282/figures/gen_samples/88.jpg", "caption": "Figure 13: Compute process of Combination Usage.", "description": "This figure illustrates the process of calculating the Combination Usage metric.  This metric quantifies the diversity of expert pairings used in the model. It starts by creating a histogram showing the frequency of each unique pair of experts activated together across all tokens.  These counts are then sorted in descending order and normalized to obtain a sorted normalized histogram. Finally, a cumulative sum is computed, and the percentage of combinations whose cumulative sum is less than 95% is calculated and reported as the Combination Usage. This value indicates the proportion of commonly activated expert pairs, thus reflecting the degree of diversity in how experts are used.", "section": "Load Balancing via Router Similarity Loss"}, {"figure_path": "https://arxiv.org/html/2503.16057/extracted/6296282/figures/gen_samples/33.jpg", "caption": "Figure 14: Training curve comparisons between DiT-MoE\u00a0[9] and our model.", "description": "This figure compares the training curves of the DiT-MoE model from a related work [9] and the proposed RaceDiT model.  It shows the training loss plotted against the number of training iterations. The comparison illustrates the relative training speed and efficiency of the two models.  The RaceDiT model appears to converge faster to a lower loss than DiT-MoE, suggesting improved training performance.", "section": "D Additional Comparisons with DiT-MoE"}, {"figure_path": "https://arxiv.org/html/2503.16057/extracted/6296282/figures/gen_samples/291.jpg", "caption": "Figure 15: Samples from RaceDiT-XL/2-4in32 (256\u00d7256256256256\\times 256256 \u00d7 256).\nClassifier-free guidance: 4.0\nLabel: Macaw (88)", "description": "This figure displays several images of macaws generated by the RaceDiT-XL/2-4in32 model. The images are 256x256 pixels, generated with a classifier-free guidance scale of 4.0.  The model successfully generates a variety of macaw images with different poses and backgrounds, demonstrating its ability to produce high-quality and diverse outputs. The label 'Macaw (88)' indicates the class ID of 88 used for conditional generation.", "section": "7.8 More Results on ImageNet 256 \u00d7 256"}, {"figure_path": "https://arxiv.org/html/2503.16057/extracted/6296282/figures/gen_samples/387.jpg", "caption": "Figure 16: Samples from RaceDiT-XL/2-4in32 (256\u00d7256256256256\\times 256256 \u00d7 256).\nClassifier-free guidance: 4.0\nLabel: loggerhead turtle (33)", "description": "This figure displays several images of loggerhead turtles generated by the RaceDiT-XL/2-4in32 model, a Mixture of Experts (MoE) model for diffusion transformers. The images are 256x256 pixels.  The model utilized a classifier-free guidance scale of 4.0 during generation.  The 'loggerhead turtle' label was used as a prompt to guide the generation process.", "section": "7.8 More Results on ImageNet 256 \u00d7 256"}, {"figure_path": "https://arxiv.org/html/2503.16057/extracted/6296282/figures/gen_samples/511.jpg", "caption": "Figure 17: Samples from RaceDiT-XL/2-4in32 (256\u00d7256256256256\\times 256256 \u00d7 256).\nClassifier-free guidance: 4.0\nLabel: lion (291)", "description": "This figure displays a sample of images generated by the RaceDiT-XL/2-4in32 model, which is a Mixture of Experts (MoE) diffusion model. The images are 256x256 pixels and the classifier-free guidance scale was set to 4.0.  All the images in the sample depict lions, showcasing the model's ability to generate high-quality images of a specific class.", "section": "7.8 More Results on ImageNet 256 \u00d7 256"}, {"figure_path": "https://arxiv.org/html/2503.16057/extracted/6296282/figures/gen_samples/417.jpg", "caption": "Figure 18: Samples from RaceDiT-XL/2-4in32 (256\u00d7256256256256\\times 256256 \u00d7 256).\nClassifier-free guidance: 4.0\nLabel: lesser panda (387)", "description": "This figure displays sample images generated by the RaceDiT-XL/2-4in32 model, a diffusion transformer model employing the Expert Race routing strategy. The images are 256x256 pixels and were generated using a classifier-free guidance scale of 4.0. The specific label used for generation is \"lesser panda\" (class ID 387).  The figure showcases the model's ability to generate high-quality, realistic images of lesser pandas.", "section": "7.8 More Results on ImageNet 256 \u00d7 256"}, {"figure_path": "https://arxiv.org/html/2503.16057/extracted/6296282/figures/gen_samples/975.jpg", "caption": "Figure 19: Samples from RaceDiT-XL/2-4in32 (256\u00d7256256256256\\times 256256 \u00d7 256).\nClassifier-free guidance: 4.0\nLabel: convertible (511)", "description": "This figure displays several images generated by the RaceDiT-XL/2-4in32 model, a diffusion model that utilizes a Mixture of Experts (MoE) architecture. The images are all 256x256 pixels in size and were generated with a classifier-free guidance scale of 4.0.  The specific label used for generating these examples was \"convertible\" (ImageNet class ID 511). This showcases the model's ability to generate high-quality and diverse images of convertibles.", "section": "7.8 More Results on ImageNet 256 \u00d7 256"}, {"figure_path": "https://arxiv.org/html/2503.16057/extracted/6296282/figures/gen_samples/980.jpg", "caption": "Figure 20: Samples from RaceDiT-XL/2-4in32(256\u00d7256256256256\\times 256256 \u00d7 256).\nClassifier-free guidance: 4.0\nLabel: balloon (417)", "description": "This figure displays a set of images generated by the RaceDiT-XL/2-4in32 model, a Mixture-of-Experts (MoE) diffusion model.  The images are 256x256 pixels in size and are conditioned on the label \"balloon\" (class ID 417).  The classifier-free guidance scale used was 4.0.  The figure visually demonstrates the model's ability to generate diverse and realistic images of hot air balloons.", "section": "7.8 More Results on ImageNet 256 \u00d7 256"}]