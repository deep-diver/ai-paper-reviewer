[{"figure_path": "https://arxiv.org/html/2503.07197/x1.png", "caption": "Figure 1: Generated samples from eMIGM trained on ImageNet 512\u00d7512512512512\\times 512512 \u00d7 512.", "description": "This figure displays various images generated by the eMIGM model. The model was trained using the ImageNet dataset, specifically at a resolution of 512x512 pixels.  The images showcase the model's ability to generate diverse and realistic images, representing a range of objects and scenes from the ImageNet dataset.  The quality and variety of the generated samples are used to demonstrate the effectiveness of the eMIGM model.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.07197/x2.png", "caption": "(a) Choices of mask schedule", "description": "The figure shows the impact of different mask schedules on the training process of the model.  The x-axis represents the training epochs, and the y-axis represents the FID (Fr\u00e9chet Inception Distance) score, a metric used to evaluate the quality of generated images. Lower FID scores indicate better image quality. Three different mask schedules are compared: Linear, Cosine, and Exp. The results show that the cosine schedule leads to lower FID scores than the linear schedule, and the exp schedule is unstable, indicating that the cosine schedule is the most effective for training.", "section": "4. Investigating the Design Space of Training"}, {"figure_path": "https://arxiv.org/html/2503.07197/x3.png", "caption": "(b) Choices of weighting function", "description": "This figure compares the performance of different weighting functions used in the loss function during the training process of the masked image generation model. The x-axis represents the training epochs, and the y-axis represents the FID (Fr\u00e9chet Inception Distance) score, a metric used to evaluate the quality of generated images.  The lower the FID score, the better the generated image quality. The figure shows that using a weighting function of w(t) = 1 yields better image quality than w(t) = Yt/sqrt(t), which is used in the original MDM model. This suggests that a simpler weighting function may be more effective for training the masked image generation model.", "section": "4. Investigating the Design Space of Training"}, {"figure_path": "https://arxiv.org/html/2503.07197/x4.png", "caption": "(c) Use the MAE trick", "description": "This figure shows the impact of using the Masked Autoencoder (MAE) architecture on model training.  The MAE architecture processes only unmasked tokens, which can improve performance compared to a single-encoder transformer architecture. The x-axis represents training epochs, and the y-axis represents the FID (Fr\u00e9chet Inception Distance) score.  Lower FID values indicate better image generation quality. The figure compares the performance of the model trained with and without the MAE architecture using the exponential masking schedule.", "section": "4. Investigating the Design Space of Training"}, {"figure_path": "https://arxiv.org/html/2503.07197/x5.png", "caption": "(d) Use the time truncation", "description": "This figure explores the impact of time truncation on the training process of the masked image generation model. Time truncation modifies the minimum value of the time variable 't' during training, effectively controlling the minimum masking ratio.  The results show the effect of different time truncation values (tmin = 0, 0.2, and 0.4) on the FID (Fr\u00e9chet Inception Distance) score over training epochs using the exponential masking schedule and the MAE (Masked Autoencoder) architecture, with and without classifier free guidance (CFG) with mask.  The optimal value of tmin balances accelerating training convergence with avoiding performance degradation due to excessive masking.", "section": "4. Investigating the Design Space of Training"}, {"figure_path": "https://arxiv.org/html/2503.07197/x6.png", "caption": "(e) Use CFG with mask", "description": "This figure shows the effect of using Classifier-Free Guidance (CFG) with a mask token instead of a fake class token on the training performance of the masked image generation model.  The graph plots FID (Fr\u00e9chet Inception Distance) score versus training epochs.  The orange line represents the model trained with CFG using a mask token, while the blue line represents the model trained with standard CFG. The results demonstrate that using a mask token with CFG leads to improved performance compared to the standard CFG approach, suggesting that replacing the fake class token with a mask token is beneficial for this type of image generation model.", "section": "4. Investigating the Design Space of Training"}, {"figure_path": "https://arxiv.org/html/2503.07197/x7.png", "caption": "Figure 2: Exploring the design space of training. Orange solid lines indicate the preferred choices in each subfigure.", "description": "Figure 2 systematically investigates the impact of various design choices during the training phase of a masked image generation model.  Each subfigure focuses on a specific hyperparameter or architectural decision, such as the mask schedule, weighting function, use of the MAE trick, time truncation, and incorporating CFG with a mask. The x-axis typically represents training epochs, and the y-axis usually shows the FID score as a measure of generated image quality. Orange lines highlight the design choices that yield the best performance according to the paper's experiments. This figure allows the reader to visualize how different choices affect the training process and ultimately, the quality of the generated images.", "section": "4. Investigating the Design Space of Training"}, {"figure_path": "https://arxiv.org/html/2503.07197/x8.png", "caption": "(a) Choices of sample mask schedule", "description": "This figure compares the performance of three different sample mask schedules: linear, cosine, and exponential.  The x-axis represents the number of sampling steps, and the y-axis shows the FID score (Fr\u00e9chet Inception Distance), a metric for evaluating the quality of generated images. Lower FID scores indicate better image quality. The plot shows how the choice of mask schedule affects the generated image quality as the number of sampling steps increases.  The results show the relative performance of each mask schedule in the context of the paper's overall image generation model.", "section": "5.1 Mask Schedule during Sampling"}, {"figure_path": "https://arxiv.org/html/2503.07197/x9.png", "caption": "(b) Use the DPM-Solver", "description": "This figure compares the performance of different sampling methods for masked image generation models. Specifically, it shows how the Fr\u00e9chet Inception Distance (FID) changes as the number of sampling steps increases, when using the DPM-Solver algorithm. The DPM-Solver is an efficient ODE sampler that accelerates the diffusion sampling process and converges faster with fewer steps than other methods like DDPM. The results indicate that DPM-Solver generally outperforms other methods, particularly when fewer sampling steps are used. It demonstrates that DPM-Solver is a suitable method for efficient and high-quality masked image generation.", "section": "5. Investigating the Design Space of Sampling"}, {"figure_path": "https://arxiv.org/html/2503.07197/x10.png", "caption": "(c) Use the time interval", "description": "This figure shows the impact of using a time interval strategy for classifier-free guidance (CFG) during sampling.  The FID (Fr\u00e9chet Inception Distance) is plotted against the number of training epochs for different CFG approaches: the standard CFG, and CFG with time intervals (tmin=0, tmin=0.2, and tmin=0.4). The time interval strategy applies CFG only to later stages of sampling, which improves efficiency by reducing function evaluations (NFEs) while maintaining performance. The results demonstrate that a time interval of tmin=0.2 provides the best balance of efficiency and performance.", "section": "5.3 Time Interval for Classifier Free Guidance"}, {"figure_path": "https://arxiv.org/html/2503.07197/x11.png", "caption": "Figure 3: Exploring the design space of sampling. For each plot, points from left to right correspond to an increasing number of mask prediction steps: 8, 16, 32, and up to 256. In each subfigure, DPM-Solver is donated as DPMS. (a) The exp schedule outperforms others by predicting fewer tokens early. (b) DPM-Solver performs better with fewer prediction steps. (c) The time interval maintains performance while reducing sampling cost for each mask prediction step, particularly for high mask prediction steps.", "description": "This figure explores the impact of different sampling strategies on the performance of masked image generation models.  It shows how the choice of mask schedule (linear, cosine, exponential), the sampling method (DPM-Solver vs. standard diffusion), and the use of a time interval for classifier-free guidance affect FID scores across varying numbers of mask prediction steps (8, 16, 32, ..., 256). The exponential mask schedule is highlighted for predicting fewer tokens in earlier steps, improving efficiency. DPM-Solver is demonstrated to be superior, especially with fewer sampling steps. Finally, the time-interval approach for classifier-free guidance shows that it can maintain FID performance while significantly reducing sampling costs.", "section": "5. Investigating the Design Space of Sampling"}, {"figure_path": "https://arxiv.org/html/2503.07197/x12.png", "caption": "(a) FLOPs vs. FID across model scales.", "description": "This figure shows the relationship between FLOPs (floating point operations) and FID (Fr\u00e9chet Inception Distance) for different scales of the eMIGM model.  The x-axis represents the number of FLOPs during training, while the y-axis shows the FID score, a measure of generated image quality (lower is better).  The plot reveals how the model's performance (FID) improves as the model size increases (more FLOPs are used during training).  This demonstrates the scaling properties of the eMIGM model, showing that larger models achieve better image generation quality with increased computational cost.", "section": "Scalability of eMIGM"}, {"figure_path": "https://arxiv.org/html/2503.07197/x13.png", "caption": "(b) FLOPs vs. FID under different budgets.", "description": "This figure shows the relationship between FLOPs (floating-point operations) and FID (Fr\u00e9chet Inception Distance) for different model sizes of eMIGM, under various computational budget constraints.  Each point represents a model with different FLOPs and the corresponding FID.  It illustrates the trade-off between model size and generation quality.  The trend shows that generally higher FLOPs lead to lower FID (better image quality), however the figure also highlights the relative efficiency of larger models, showing how well they perform given a certain FLOP budget.", "section": "4. Investigating the Design Space of Training"}, {"figure_path": "https://arxiv.org/html/2503.07197/x14.png", "caption": "(c) Inference speed vs. FID.", "description": "This figure shows the relationship between the inference speed (time taken to generate one image) and the Fr\u00e9chet Inception Distance (FID), a measure of image quality.  Faster inference speeds are desirable, but ideally without sacrificing image quality (a lower FID score is better).  The plot likely shows how inference time changes as the size of the eMIGM model increases, suggesting that larger models may be more efficient at generating high-quality images.  Different points on the graph likely represent different model sizes.", "section": "Scalability of eMIGM"}, {"figure_path": "https://arxiv.org/html/2503.07197/x15.png", "caption": "Figure 4: Scalability of eMIGM. (a) A negative correlation demonstrates that eMIGM benefits from scaling. (b) Larger models are more training-efficient (i.e., achieving better sample quality with the same training FLOPs). (c) Larger models are more sampling-efficient (i.e., achieving better sample quality with the same inference time).", "description": "Figure 4 demonstrates the scalability and efficiency of the eMIGM model.  Panel (a) shows a negative correlation between the model size (measured in FLOPs) and the Fr\u00e9chet Inception Distance (FID) score, indicating that larger models generally produce higher-quality images (lower FID).  Panel (b) highlights the training efficiency of eMIGM; larger models achieve better image quality with the same number of training FLOPs, demonstrating improved training efficiency as model size increases.  Finally, panel (c) showcases the sampling efficiency: larger models maintain high image quality while using less inference time, indicating that larger models are more efficient during the inference phase (image generation).", "section": "6. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.07197/x16.png", "caption": "Figure 5: Different choices of mask schedules. Left: \u03b3tsubscript\ud835\udefe\ud835\udc61\\gamma_{t}italic_\u03b3 start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT (i.e., the probability that each token is masked during the forward process). Right: Weight of the loss in MDM.", "description": "This figure compares three different mask schedules: linear, cosine, and exponential.  The left panel shows the probability of masking a token (\u03b3t) at different time steps (t) for each schedule. The right panel shows the weight (w(t)) assigned to the loss function at each time step, which is also determined by the mask schedule. The different functions are to illustrate the relationship between the probability of masking a token and the weight associated with the loss in the masked diffusion model (MDM). The choice of mask schedule affects both the training process and the quality of the generated images.", "section": "4. Investigating the Design Space of Training"}, {"figure_path": "https://arxiv.org/html/2503.07197/x17.png", "caption": "Figure 6: Comparison of mask removal for different sample mask schedule.", "description": "This figure shows the average number of tokens predicted at each step during the sampling process for three different mask schedules: linear, cosine, and exponential. The x-axis represents the sampling step, and the y-axis represents the average number of tokens removed.  The linear schedule shows a relatively constant number of tokens removed at each step. The cosine schedule removes fewer tokens in the early steps and progressively more in later steps. The exponential schedule removes the fewest tokens in the early steps and gradually increases the number of tokens removed as sampling progresses.", "section": "5.1. Mask Schedule during Sampling"}, {"figure_path": "https://arxiv.org/html/2503.07197/x18.png", "caption": "(a) CFG vs. FID", "description": "This figure shows the relationship between classifier-free guidance (CFG) and Fr\u00e9chet Inception Distance (FID) scores.  The x-axis represents different CFG values, and the y-axis represents the FID score.  Lower FID scores indicate better image quality. The plot helps to determine the optimal CFG value that balances image quality and generation speed.  This analysis is important because strong guidance can sometimes decrease the diversity and realism of generated images, while insufficient guidance can hurt the quality.", "section": "5.1 Mask Schedule during Sampling"}]