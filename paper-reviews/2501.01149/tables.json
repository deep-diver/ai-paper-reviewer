[{"content": "| Name | Eval Mode | # Tasks | # General Apps | Operation | Inf. Query |\n|---|---|---|---|---|---| \n| AitW | static | - | - | \u2713 | \u2717 |\n| AndroidControl | static | - | - | \u2713 | \u2717 |\n| AMEX | static | - | - | \u2713 | \u2717 |\n| AndroidArena | dynamic | 221 | 4 | \u2713 | \u2717 |\n| Mobile-Env | dynamic | 74 | 5 | \u2713 | \u2717 |\n| AndroidWorld | dynamic | 116 | 15 | \u2713 | \u2717 |\n| B-Moca | dynamic | 131 | 4 | \u2713 | \u2717 |\n| AndroidLab | dynamic | 138 | 5 | \u2713 | \u2713 |\n| A3 (Our) | dynamic | 201 | 21 | \u2713 | \u2713 |", "caption": "Table 1: GUI related datasets and benchmarks. 15 general apps of AndroidWorld are from F-Droid.", "description": "This table compares several existing datasets and benchmarks for evaluating GUI agents, highlighting key characteristics such as evaluation method (static or dynamic), the number of tasks and general-purpose apps included, and whether they support operational instructions and information queries.  It notes that 15 of AndroidWorld's apps come from F-Droid, an open-source app repository. This provides context on the scope and diversity of each benchmark, allowing researchers to choose the most appropriate tools for their needs.", "section": "2 Related Work"}, {"content": "| LLM | Correct Func. | Wrong Line |\n|---|---|---|\n| GPT-4o | 24% | 27% |", "caption": "Table 2: The capability of GPT-4o to directly generate evaluation function. \u201cCorrect Func.\u201d represents the percentage of correct files over all generated files. \u201cWrong Line\u201d represents the percentage of incorrect lines over all code generated. The evaluation is collected from coding experts.", "description": "This table presents the results of using GPT-40 to automatically generate evaluation functions for tasks in the Android Agent Arena (A3) dataset.  It shows the percentage of generated evaluation functions that were completely correct and the percentage of lines within those functions that contained errors.  These percentages are based on assessments by coding experts, providing a measure of the model's ability to generate accurate and functional code for automated task evaluation.", "section": "3.4.2 LLM Evaluation System"}, {"content": "| LLM | Eval Correct |\n|---|---| \n| GPT-4o | 84% |\n| Gemini 1.5 Pro | 80% |", "caption": "Table 3: The correctness of LLM evaluation by human validation from 50 tasks. \u201cEval Correct\u201d represents the correctness of LLM evaluation results determined by human.", "description": "This table presents the accuracy of using Large Language Models (LLMs) for automated evaluation of tasks in the Android Agent Arena.  Fifty tasks were evaluated using LLMs, and the results were then checked by human evaluators for correctness. The \"Eval Correct\" column indicates the percentage of times the LLM's evaluation matched the human evaluator's judgment.", "section": "3.4 Evaluation"}, {"content": "| Test Subset | Test Level | Succ. Rate |\n|---|---|---|\n| IDD | High | 69.6 |\n| IDD | Low | 92.1 |\n| Category | High | 51.8 |\n| Unseen | Low | 84.4 |\n| App | High | 56.8 |\n| Unseen | Low | 83.0 |\n| Task | High | 73.7 |\n| Unseen | Low | 88.5 |", "caption": "Table 4: Static frame evaluation results on AndroidControl test set.", "description": "This table presents the results of static frame evaluations performed on the AndroidControl test set.  The test set is divided into four subsets: IDD, Category Unseen, App Unseen, and Task Unseen. Each subset represents a different aspect of unseen data, allowing for a comprehensive evaluation of generalization capabilities.  Further, the evaluations are categorized into high-level and low-level, reflecting variations in task instruction detail (high-level provides only the overall task goal, while low-level adds additional, step-by-step instructions).  The success rates for each subset and level are reported to show the performance of the tested agent under various scenarios.", "section": "4 Experiments"}, {"content": "| Agent | Test Level | Succ. Rate |\n|---|---|---|\n| InternVL2 | Easy | 23.4% |\n|  | Medium | 5.6% |\n|  | Hard | 2.0% |\n| GPT-4o | Easy | 9.9% |\n|  | Medium | 1.4% |\n|  | Hard | 0.0% |\n| AppAgent | Easy | 30.8% |\n|  | Medium | 7.0% |\n|  | Hard | 2.0% |", "caption": "Table 5: Dynamic evaluation results on A3 by difficulty level.", "description": "This table presents the results of dynamic evaluations performed on the Android Agent Arena (A3) platform.  The evaluations assess the success rate of different agents (InternVL2, GPT-40, AppAgent) across various difficulty levels (Easy, Medium, Hard) for completing tasks. The success rate represents the percentage of tasks successfully completed by each agent at each difficulty level.  This provides a measure of how well each agent performs in dynamic, real-world settings.", "section": "4 Experiments"}, {"content": "| Agent | Test Category | Succ. Rate |\n|---|---|---|\n| InternVL2 | Operation | 17.1% |\n|  | Single-frame Query | 0.0% |\n|  | Multi-frame Query | 0.0% |\n| GPT-4o | Operation | 5.7% |\n|  | Single-frame Query | 2.0% |\n|  | Multi-frame Query | 0.0% |\n| AppAgent | Operation | 20.0% |\n|  | Single-frame Query | 6.0% |\n|  | Multi-frame Query | 0.0% |", "caption": "Table 6: Dynamic evaluation results on A3 by task category.", "description": "This table presents the success rates of three different agents (InternVL2, GPT-40, and AppAgent) on the Android Agent Arena (A3) platform.  The agents are evaluated on three distinct task categories: Operation, Single-frame Query, and Multi-frame Query, representing different levels of complexity.  The success rate indicates the percentage of tasks successfully completed by each agent within each category. This provides a comparative analysis of agent performance across various task types.", "section": "4 Experiments"}]