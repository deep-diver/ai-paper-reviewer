{"importance": "This paper introduces a resource-efficient framework and enhances the way towards achieving both understanding and generation capabilities for MLLMs. The architecture introduces an asymmetric design and a novel training algorithm which could open new research avenues for more efficient multimodal learning.", "summary": "ARMOR: Empowers MLLMs with interleaved multimodal generation via asymmetric synergy, using limited resources.", "takeaways": ["ARMOR introduces an asymmetric encoder-decoder architecture with a forward-switching mechanism for interleaved text-image generation.", "A meticulously curated, high-quality interleaved dataset and a novel \"what or how to generate\" algorithm are proposed for fine-tuning MLLMs.", "Experimental results demonstrate that ARMOR upgrades existing MLLMs to UniMs with promising image generation capabilities, using limited training resources."], "tldr": "**Unified models are promising for multimodal tasks but demand computational resources and struggle with interleaved generation**. Existing models simultaneously learn understanding and generation, requiring extensive resources. The goal is to develop a resource-efficient approach that achieves both understanding and generation effectively. \n\nThis paper introduces ARMOR framework by fine-tuning existing MLLMs. **ARMOR extends existing MLLMs using an asymmetric encoder-decoder architecture**, a high-quality interleaved dataset, and a training algorithm, and experimental results demonstrate that ARMOR empowers MLLMs with enhanced capabilities using limited resources.", "affiliation": "Nankai University", "categories": {"main_category": "Multimodal Learning", "sub_category": "Multimodal Understanding"}, "podcast_path": "2503.06542/podcast.wav"}