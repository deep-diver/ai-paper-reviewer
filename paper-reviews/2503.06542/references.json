{"references": [{"fullname_first_author": "Jean-Baptiste Alayrac", "paper_title": "Flamingo: a visual language model for few-shot learning", "publication_date": "2022-03-01", "reason": "It exemplifies explicit attention interaction, enabling deep vision-language fusion by projecting visual features into text-aligned tokens."}, {"fullname_first_author": "Zhe Chen", "paper_title": "Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks", "publication_date": "2024-01-01", "reason": "It exemplifies implicit space mapping, transforming visual features into token sequences using MLPs for modality alignment."}, {"fullname_first_author": "Junnan Li", "paper_title": "BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models", "publication_date": "2023-01-01", "reason": "It also exemplifies explicit attention interaction, enabling deep vision-language fusion by projecting visual features into text-aligned tokens."}, {"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-01-01", "reason": "It pioneers cross-modal alignment via contrastive learning, inspiring MLLMs that bridge vision encoders and large language models."}, {"fullname_first_author": "Aditya Ramesh", "paper_title": "Hierarchical text-conditional image generation with clip latents", "publication_date": "2022-01-01", "reason": "It addresses image generation by hierarchical text-conditional generation."}]}