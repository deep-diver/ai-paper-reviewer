[{"figure_path": "https://arxiv.org/html/2503.05639/x2.png", "caption": "Figure 1. \nVideoPainter\u00a0enables plug-and-play text-guided video inpainting and editing for any video length and pre-trained Diffusion Transformer with masked video and video caption\u00a0(user editing instruction).\nThe upper part demonstrates the effectiveness of VideoPainter\u00a0in various video inpainting scenarios, including object, landscape, human, animal, multi-region\u00a0(Multi), and random masks.\nThe lower section demonstrates the performance of VideoPainter\u00a0in video editing, including adding, removing, changing attributes, and swapping objects.\nIn both video inpainting and editing, we demonstrate strong ID consistency in generating long videos\u00a0(Any Len.).\nProject Page: https://yxbian23.github.io/project/video-painter", "description": "This figure showcases VideoPainter's capabilities in video inpainting and editing. The top half illustrates its ability to handle various inpainting tasks with different mask types (objects, landscapes, humans, animals, multiple regions, and random masks), consistently maintaining object identity even in long videos.  The bottom half demonstrates VideoPainter's video editing functionality, enabling users to add, remove, modify attributes of, and swap objects within videos, again showing consistent object identity over longer video sequences.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2503.05639/x3.png", "caption": "Figure 2. Framework Comparison.\nNon-generative approaches, limited to pixel propagation from backgrounds, fail to inpaint fully segmentation-masked objects. Generative methods adapt single-branch image inpainting models to video by adding temporal attention, struggling to maintain background fidelity and generate foreground contents in one model. In contrast, VideoPainter\u00a0implements a dual-branch architecture that leverages an efficient context encoder with any pre-trained DiT, decoupling video inpainting to background preservation and foreground generation, and enabling plug-and-play video inpainting control.", "description": "Figure 2 illustrates three different approaches to video inpainting: non-generative methods, generative methods, and VideoPainter. Non-generative methods use pixel propagation from the background to fill in missing parts of a video, but this approach struggles to inpaint fully masked objects. Generative methods adapt image inpainting models to videos by adding temporal attention, but they often struggle to maintain background fidelity and generate foreground content simultaneously. In contrast, VideoPainter uses a dual-branch architecture with a lightweight context encoder and a pre-trained Diffusion Transformer. This approach separates background preservation and foreground generation, resulting in better inpainting quality and allowing for plug-and-play control.", "section": "2 RELATED WORK"}, {"figure_path": "https://arxiv.org/html/2503.05639/x4.png", "caption": "Figure 3. Dataset Construction Pipeline. It consists of five pre-processing steps: collection, annotation, splitting, selection, and captioning.", "description": "This figure illustrates the pipeline used to create the VPData and VPBench datasets.  The pipeline consists of five main stages: 1) Data Collection from sources like Videvo and Pexels; 2) Annotation using a multi-stage process involving object recognition, bounding box detection, mask generation, and scene splitting; 3) Splitting videos into shorter clips (10 seconds), discarding clips shorter than 6 seconds; 4) Selection of high-quality clips based on criteria such as aesthetic quality, motion strength, and content safety; and 5) Captioning, which uses advanced vision language models (CogVLM and GPT-4) to generate detailed captions and descriptions of masked regions in the videos. This pipeline allows for the creation of a large-scale, high-quality video inpainting dataset with detailed annotations.", "section": "3 METHOD"}, {"figure_path": "https://arxiv.org/html/2503.05639/x5.png", "caption": "Figure 4. \nModel overview.\nThe upper figure shows the architecture of VideoPainter. The context encoder performs video inpainting based on concatenation of the noisy latent, downsampled masks, and masked video latent via VAE. Features extracted by the context encoder are integrated into the pre-trained DiT in a group-wise and token-selective manner, where two encoder layers modulate the first and second halves of the DiT, respectively, and only the background tokens will be integrated into the backbone to prevent information ambiguity.\nThe lower figure illustrates the inpainting ID region resampling with the ID Resample Adapter. During training, tokens of the current masked region are concatenated to the KV vectors, enhancing ID preservation of the inpainting region. During inference, the ID tokens of the last clip are concatenated to the current KV vectors, maintaining ID consistency with the last clip by resampling.", "description": "Figure 4 provides a detailed breakdown of the VideoPainter model's architecture and operation. The upper half illustrates the model's dual-branch structure.  A context encoder processes a combination of noisy latent data, downscaled masks, and masked video latent representations (all processed through a Variational Autoencoder or VAE). The resulting features are then selectively integrated into a pre-trained Diffusion Transformer (DiT), with two encoder layers affecting the first and second halves of the DiT. Critically, only background tokens are added to maintain clarity. The lower half shows the inpainting ID region resampling, utilizing an ID Resample Adapter. During training, current masked region tokens are appended to key-value (KV) vectors for improved ID consistency. During inference, this process ensures consistency with the preceding video clip. ", "section": "METHOD"}, {"figure_path": "https://arxiv.org/html/2503.05639/x6.png", "caption": "Figure 5. Comparison of previous inpainting methods and VideoPainter\u00a0on standard and long video inpainting. More visualizations are in the demo video.", "description": "Figure 5 presents a qualitative comparison of VideoPainter against several state-of-the-art video inpainting methods.  It showcases the performance differences on both standard-length and long videos. The figure highlights VideoPainter's superior ability to maintain video coherence, generate high-quality results, and achieve strong alignment with text captions.  The results demonstrate VideoPainter's effectiveness in preserving backgrounds, accurately filling in masked regions, and generating realistic content that is consistent with the provided text prompts. More detailed visualizations are available in the accompanying demo video.", "section": "4.2 Video Inpainting"}, {"figure_path": "https://arxiv.org/html/2503.05639/x7.png", "caption": "Figure 6. Comparison of previous editing methods and VideoPainter on standard and long video editing.\nMore visualizations are in the demo video.", "description": "Figure 6 presents a qualitative comparison of VideoPainter against other state-of-the-art video editing methods.  It showcases the results of standard and long-video editing tasks on various video clips. The figure visually demonstrates VideoPainter's superior performance in terms of background preservation, text alignment, and overall video quality, highlighting its ability to handle both short and extended video sequences effectively.  For a more comprehensive view of the results, the authors suggest referring to the accompanying demo video.", "section": "4.2 Video Editing"}, {"figure_path": "https://arxiv.org/html/2503.05639/x8.png", "caption": "Figure 7. Integrating VideoPainter\u00a0to Gromit-style LoRA\u00a0(Cseti, 2024).", "description": "Figure 7 demonstrates the versatility and plug-and-play nature of VideoPainter.  It shows how easily VideoPainter integrates with pre-trained, community-developed models, specifically a Gromit-style LoRA (Cseti, 2024). This highlights VideoPainter's ability to work with diverse styles and models, even those with a significant domain gap (in this case, an anime style LoRA applied to a dataset not trained on anime). The successful integration underscores VideoPainter\u2019s adaptability and ease of use, proving that users can leverage various pre-trained models and customize their video inpainting results.", "section": "4.6 Plug-and-Play Control Ability"}, {"figure_path": "https://arxiv.org/html/2503.05639/x9.png", "caption": "Figure 8. More video inpainting results.", "description": "Figure 8 presents supplementary examples of video inpainting results produced by VideoPainter.  Each row showcases a different scenario: object inpainting, landscape inpainting, human inpainting, animal inpainting, multi-object inpainting, and random inpainting.  The figure demonstrates the model's ability to effectively inpaint diverse content within videos, preserving temporal coherence and maintaining the overall quality of the video.", "section": "Experiments"}]