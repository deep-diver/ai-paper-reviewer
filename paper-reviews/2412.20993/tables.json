[{"content": "| Algorithm | Dataset | Thres. (<math alttext=\"\\tilde{\\mathcal{H}_{\\tau}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.1.1.1.1.m1.1\"><semantics id=\"S6.T1.1.1.1.1.m1.1a\"><mover accent=\"true\" id=\"S6.T1.1.1.1.1.m1.1.1\" xref=\"S6.T1.1.1.1.1.m1.1.1.cmml\"><msub id=\"S6.T1.1.1.1.1.m1.1.1.2\" xref=\"S6.T1.1.1.1.1.m1.1.1.2.cmml\"><mi class=\"ltx_font_mathcaligraphic\" id=\"S6.T1.1.1.1.1.m1.1.1.2.2\" xref=\"S6.T1.1.1.1.1.m1.1.1.2.2.cmml\">\u210b</mi><mi id=\"S6.T1.1.1.1.1.m1.1.1.2.3\" xref=\"S6.T1.1.1.1.1.m1.1.1.2.3.cmml\">\u03c4</mi></msub><mo id=\"S6.T1.1.1.1.1.m1.1.1.1\" xref=\"S6.T1.1.1.1.1.m1.1.1.1.cmml\">~</mo></mover><annotation-xml encoding=\"MathML-Content\" id=\"S6.T1.1.1.1.1.m1.1b\"><apply id=\"S6.T1.1.1.1.1.m1.1.1.cmml\" xref=\"S6.T1.1.1.1.1.m1.1.1\"><ci id=\"S6.T1.1.1.1.1.m1.1.1.1.cmml\" xref=\"S6.T1.1.1.1.1.m1.1.1.1\">~</ci><apply id=\"S6.T1.1.1.1.1.m1.1.1.2.cmml\" xref=\"S6.T1.1.1.1.1.m1.1.1.2\"><csymbol cd=\"ambiguous\" id=\"S6.T1.1.1.1.1.m1.1.1.2.1.cmml\" xref=\"S6.T1.1.1.1.1.m1.1.1.2.1\">subscript</csymbol><ci id=\"S6.T1.1.1.1.1.m1.1.1.2.2.cmml\" xref=\"S6.T1.1.1.1.1.m1.1.1.2.2\">\u210b</ci><ci id=\"S6.T1.1.1.1.1.m1.1.1.2.3.cmml\" xref=\"S6.T1.1.1.1.1.m1.1.1.2.3\">\ud835\udf0f</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T1.1.1.1.1.m1.1c\">\\tilde{\\mathcal{H}_{\\tau}}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S6.T1.1.1.1.1.m1.1d\">over~ start_ARG caligraphic_H start_POSTSUBSCRIPT italic_\u03c4 end_POSTSUBSCRIPT end_ARG</annotation></semantics></math>) | Thres. (<math alttext=\"\\mathcal{R}_{\\tau}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.2.2.2.1.m1.1\"><semantics id=\"S6.T1.2.2.2.1.m1.1a\"><msub id=\"S6.T1.2.2.2.1.m1.1.1\" xref=\"S6.T1.2.2.2.1.m1.1.1.cmml\"><mi class=\"ltx_font_mathcaligraphic\" id=\"S6.T1.2.2.2.1.m1.1.1.2\" xref=\"S6.T1.2.2.2.1.m1.1.1.2.cmml\">\u211b</mi><mi id=\"S6.T1.2.2.2.1.m1.1.1.3\" xref=\"S6.T1.2.2.2.1.m1.1.1.3.cmml\">\u03c4</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S6.T1.2.2.2.1.m1.1b\"><apply id=\"S6.T1.2.2.2.1.m1.1.1.cmml\" xref=\"S6.T1.2.2.2.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S6.T1.2.2.2.1.m1.1.1.1.cmml\" xref=\"S6.T1.2.2.2.1.m1.1.1.1\">subscript</csymbol><ci id=\"S6.T1.2.2.2.1.m1.1.1.2.cmml\" xref=\"S6.T1.2.2.2.1.m1.1.1.2\">\u211b</ci><ci id=\"S6.T1.2.2.2.1.m1.1.1.3.cmml\" xref=\"S6.T1.2.2.2.1.m1.1.1.3\">\ud835\udf0f</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T1.2.2.2.1.m1.1c\">\\mathcal{R}_{\\tau}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S6.T1.2.2.2.1.m1.1d\">caligraphic_R start_POSTSUBSCRIPT italic_\u03c4 end_POSTSUBSCRIPT</annotation></semantics></math>) | Detect @knob | 0.7 | 0.7 | 0.4 | | SC | MATH |  | / | 5 | | SC | GSM8K |  | / | 5 | | SC | LiveCodeBench |  | / | 5 | | MCTS | GSM8K | 0.99 | 0.4 | 3 | | MCTS | ASDiv | / | 0.4 | 3 | | Rebase | GSM8K | 0.85 | 0.99 | 16 | | Rebase | MATH | 0.75 | / | 16 |", "caption": "Table 1: Hyperparameter configurations for certaindex. Rebase is evaluated on the MATH-OAI\u00a0[28] subset of the MATH benchmark.", "description": "This table presents the hyperparameter settings used for the certaindex metric in Dynasor's experiments.  It shows the thresholds used for the entropy (H) and reward (R) metrics in different algorithms and datasets.  The 'Detect @knob' column specifies the reasoning step at which certaindex was measured, indicating the point in the reasoning process where the algorithm's confidence was assessed. Note that the Rebase algorithm was only evaluated on the MATH-OAI subset of the MATH benchmark.", "section": "3 Certaindex Based Resource Allocation"}, {"content": "| (Algo., Dataset, LLM) | Reward Model | # Samples | Resource Cap |\n|---|---|---|---| \n| (SC, LCB, Llama3.1 8B) | / | 400 | 5,10,15,20,25,30 |\n| (SC, GSM8K, Llama3.1 8B) | / | 1000 | 5,10,15,20,25,30 |\n| (MCTS, ASDiv, Llama2 7B) | Skywork 7B | 300 | 3,7,10,15,20 |\n| (MCTS, GSM8K, Llama2 7B) | Skywork 7B | 300 | 3,7,10,15,20 |\n| (Rebase, MATH, Llemma 7B) | Llemma 34b | 500 | 16,32,64,128 |\n| (Rebase, GSM8K, Llemma 34B) | Llemma 34b | 500 | 16,32,64,128 |", "caption": "Table 2: Offline workload Configurations. LiveCodeBench (LCB), Llama2 7B\u00a0[12], Skywork7B\u00a0[35], Llemma 7B and 34B\u00a0[51] are fine-tuned models used in different settings as LLM/reward model.", "description": "This table details the configurations used for offline experiments in the paper.  It lists the algorithm used (SC, MCTS, Rebase), the dataset employed (LiveCodeBench, GSM8K, ASDiv, MATH), the specific large language model (LLM) and reward model used (Llama, Skywork, Llemma), the reward model hyperparameters, the number of samples used, and the resource cap (maximum computation budget) for each experimental setting. The different resource caps correspond to various points on the token-to-accuracy curves shown in Figure 8. This allows for the comparison of different computational budgets and the impact on accuracy across various algorithms and datasets.", "section": "6.1 Experimental Setup"}, {"content": "| Algorithm | Dataset | LLM | Reward Model | Base Deadline (s) |\n|---|---|---|---|---|\n| SC | MATH | Llama3.1 8B | / | 240 |\n| MCTS | ASDiv | Llama2 7B | Skywork 7B | 60 |\n| Rebase | GSM8K | Llemma 34B | Llemma 34b | 300 |", "caption": "Table 3: Online workload Configurations", "description": "This table details the configurations used in the online evaluation of the Dynasor system.  It lists the specific algorithms (Self-Consistency, Monte Carlo Tree Search, and Rebase), datasets (GSM8K, ASDiv), and large language models (LLMs and reward models) used in each experimental setting.  Importantly, it also specifies the base deadline (in seconds) used for each configuration, providing context for the online serving performance evaluation.", "section": "6.1 Experimental Setup"}, {"content": "| Allocation Method | SC/MATH | MCTS/ASDiv |\n|---|---|---|\n| Baseline | 1.18M | 354K |\n| Static Thres. (Ours) | 1.05M (-11.0%) | 308K (-13.0%) |\n| + Initial Step Curve Fit.(Ours) | 1.04M (-11.9%) | 306K (-13.6%) |\n| + 5-Step Thres. (Ours) | 1.03M (-12.7%) | 307K (-13.3%) |\n| + Single-Step Thres. (Ours) | 1.03M (-12.7%) | 306K (-13.6%) |\n| + Dynamic Curve Fitting (Ours) | 1.01M (-14.4%) | 298K (-15.8%) |", "caption": "Table 4: Token consumption comparison of different scheduling strategies while maintaining accuracy", "description": "This table compares the token consumption of various scheduling strategies for two different reasoning tasks: self-consistency on the MATH dataset and Monte Carlo Tree Search on the ASDiv dataset.  It shows the token usage under different methods: the baseline (no optimization), a static threshold approach (Dynasor's core method), a refined approach combining initial step curve fitting, a 5-step threshold approach, a single-step threshold approach, and a dynamic curve fitting approach.  The goal is to demonstrate the effectiveness of Dynasor's fine-grained resource allocation compared to simpler scheduling approaches, while maintaining the same level of accuracy for each method.", "section": "6.4 Fine-grained Resource Allocation"}]