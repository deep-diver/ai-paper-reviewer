{"references": [{"fullname_first_author": "Patrick Esser", "paper_title": "Taming transformers for high-resolution image synthesis", "publication_date": "2021-01-01", "reason": "This paper introduces VQGAN, a foundational work using transformers for high-resolution image synthesis with vector quantization, which is highly cited within this paper."}, {"fullname_first_author": "Diederik P Kingma", "paper_title": "Auto-encoding variational bayes", "publication_date": "2013-01-01", "reason": "This paper introduces Variational Autoencoders (VAEs), a foundational framework for continuous tokenization used in the approach, and heavily referenced throughout the paper."}, {"fullname_first_author": "Robin Rombach", "paper_title": "High-resolution image synthesis with latent diffusion models", "publication_date": "2022-01-01", "reason": "This work on Latent Diffusion Models (LDM) is a key baseline and component used in the described models' tokenizer and is thus considered important."}, {"fullname_first_author": "Jiahui Yu", "paper_title": "Vector-quantized image modeling with improved vqgan", "publication_date": "2021-01-01", "reason": "This work introduces Vector-Quantized Generative Adversarial Networks (VQGAN), an approach used by some baselines in this paper."}, {"fullname_first_author": "Lijun Yu", "paper_title": "Language model beats diffusion: Tokenizer is key to visual generation", "publication_date": "2024-01-01", "reason": "This work is a key competitor and influences the tokenization choices for both discrete and continuous methods, so it is mentioned often."}]}