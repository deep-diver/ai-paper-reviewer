[{"figure_path": "https://arxiv.org/html/2411.12044/x1.png", "caption": "Figure 1: Qualitative comparison of training-free semantic segmentation methods. We compare ITACLIP with SCLIP [60] and NACLIP [24] using images from the COCO-Stuff [8] dataset. Additional visualizations are included in the Appendix.", "description": "Figure 1 presents a qualitative comparison of three different training-free semantic segmentation methods: ITACLIP (the authors' method), SCLIP [60], and NACLIP [24].  The figure showcases the segmentation results for several images from the COCO-Stuff dataset [8].  Each image is accompanied by its ground truth segmentation mask and the segmentation masks generated by the three methods. This visual comparison allows for a direct assessment of the relative performance of the different approaches in terms of accuracy and detail.", "section": "Abstract"}, {"figure_path": "https://arxiv.org/html/2411.12044/x2.png", "caption": "Figure 2: Overview of ITACLIP. Our method integrates image, text, and architectural enhancements to produce a more accurate segmentation map. We apply various data augmentation techniques, then process both the original and augmented images through a modified image encoder to obtain image embeddings. We also utilize an LLM to generate auxiliary texts (e.g., definitions or synonyms) for each original class name. The \u03bb\ud835\udf06\\lambdaitalic_\u03bb and \u03b1\ud835\udefc\\alphaitalic_\u03b1 symbols denote the image engineering and auxiliary text coefficients used in weighted summations, respectively.", "description": "Figure 2 illustrates the ITACLIP model architecture.  The model takes an original image as input and augments it using various techniques. Both the original and augmented images are fed into a modified image encoder, which incorporates architectural enhancements (self-attention modifications and removal of the feed-forward network).  The encoder outputs image embeddings. Simultaneously, an LLM generates auxiliary text (definitions or synonyms) for each class label, which is then processed by a text encoder to create text embeddings.  Image and text embeddings are combined using weighted summations controlled by parameters \u03bb (lambda) for image engineering and \u03b1 (alpha) for auxiliary text integration.  The final output is a refined segmentation map.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2411.12044/x3.png", "caption": "Figure 3: Visualization of attention maps from various layers for a selected patch. The red rectangle indicates the position of the randomly selected patch. Note that we use CLIP-ViT-B/16 as our visual backbone, with Layer 12 serving as the final layer.", "description": "This figure visualizes attention maps from different layers of a CLIP-ViT-B/16 model for a single randomly selected image patch.  The red rectangle highlights the location of the chosen patch within the image. The visualization shows how the attention mechanism focuses on different aspects of the image at different layers.  Shallow layers show localized attention around the patch, while deeper layers exhibit more global attention, encompassing semantically relevant regions beyond the immediate patch. Layer 12, the final layer of the model, provides the most informative attention map for object recognition. This figure demonstrates the concept of how the attention evolves across layers, emphasizing the spatial and contextual information captured at different depths. The inclusion of attention maps from multiple layers is critical to the model's proposed architecture.", "section": "3.3. Proposed Architectural Modifications"}, {"figure_path": "https://arxiv.org/html/2411.12044/x4.png", "caption": "(a) We employ the illustrated prompt to generate definitions.", "description": "This figure shows the prompt used to generate definitions using the LLaMa language model.  The prompt instructs the model to provide concise definitions of given words, similar to the example definitions provided.  The example definitions are of 'house' and 'car'.  The input word for the prompt in the example is 'bicycle'.  The model's generated response is also displayed, demonstrating how the model produces a short definition of the input word in the style of the provided examples. This process is a component of the ITACLIP model's auxiliary text generation.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2411.12044/x5.png", "caption": "(b) We employ the illustrated prompt to generate synonyms.", "description": "This figure shows the prompt used to generate synonyms for a given word using the LLaMa language model. The prompt instructs the model to provide a single-word synonym for a given word, and if a synonym does not exist, to provide the closest meaning.  The example illustrates the input word 'aeroplane' and the model's output, 'aircraft'. This process aids in enriching the text input to the CLIP model by providing additional textual information beyond the original class names, thereby enhancing the segmentation accuracy.", "section": "3.4. LLM-based Auxiliary Text Generation"}, {"figure_path": "https://arxiv.org/html/2411.12044/x6.png", "caption": "Figure 4: Procedure for generating auxiliary texts for a given class name.", "description": "This figure shows the process of generating auxiliary texts (definitions and synonyms) for a given class name using the LLaMa 3 language model.  Panel (a) illustrates generating definitions using a prompt that requests a brief definition and examples to guide the model in creating an appropriate definition. Panel (b) shows how synonyms are generated using a prompt requesting a single-word synonym or the closest meaning if a synonym does not exist.", "section": "3.4. LLM-based Auxiliary Text Generation"}]