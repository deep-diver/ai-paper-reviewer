[{"content": "| Method | Post-process | COCO-Stuff | COCO-Object | VOC | Context |\n|---|---|---|---|---|---| \n| Baseline | - | 7.1 | 8.6 | 20.3 | 9.0 |\n| ReCo [55] | - | 14.8 | 15.7 | 25.1 | 19.9 |\n| GroupViT [64] | - | 15.3 | 27.5 | 52.3 | 18.7 |\n| TCL [10] | PAMR | 19.6 | 30.4 | 55.0 | 30.4 |\n| MaskCLIP [69] | - | 14.6 | 20.6 | 38.8 | 23.2 |\n| CLIP-DIY [62] | - | - | 31.0 | 59.9 | - |\n| ClearCLIP [32] | - | 23.9 | 33.0 | 51.8 | 32.6 |\n| SCLIP [60] | PAMR | 23.9 | 32.1 | 61.7 | 31.5 |\n| NACLIP [24] | PAMR | 25.7 | 36.2 | 64.1 | 35.0 |\n| TagCLIP* [41] | - | 18.7 | 33.5 | 64.8 | - |\n| CaR [57] | Dense-CRF | - | 36.6 | 67.6 | 30.5 |\n| ITACLIP (Ours) | PAMR | 27.0 | 37.7 | 67.9 | 37.5 |", "caption": "Table 1: Comparison of ITACLIP with state-of-the-art methods (mIoU, %). We indicate which post-processing method has been applied to each model, if applicable. \u2217 denotes our reimplementation of this model on the COCO-Stuff and COCO-Object datasets. Note that the original paper of TagCLIP [41] evaluates the model on 27 mid-level categories of COCO-Stuff rather than on all 171 classes. Hence, we re-evaluate TagCLIP on COCO-Stuff using all class names for a fair comparison. For each dataset, bold values highlight the best scores, while underlined values signify the second-best scores.", "description": "Table 1 presents a comparison of the ITACLIP model's performance against other state-of-the-art methods for semantic segmentation. The comparison is based on four common datasets: COCO-Stuff, COCO-Object, VOC, and Context. The mIoU (mean Intersection over Union) metric is used to evaluate the performance of each model on each dataset. The table also indicates which post-processing methods (if any) were applied to each model for a fair comparison.  A note is included to explain that the TagCLIP results presented were re-implemented by the authors of this paper, using all class names instead of the original paper's 27 mid-level categories, to provide a consistent and fair comparison across all models. The best score for each dataset is highlighted in bold, with the second-best score underlined.", "section": "4. Main Results"}, {"content": "| Attention Combination | VOC |\n|---|---| \n| q-k | 19.0 |\n| q-q | 58.9 |\n| k-k | 52.2 |\n| v-v | 57.7 |\n| q-q + k-k | **67.9** |\n| q-q + v-v | 64.9 |\n| q-q + k-k + v-v | 66.4 |", "caption": "Table 2: Self-self attention combinations. We evaluate our method with different self-self attention combinations on Pascal VOC. v-v represents the value-value attention.", "description": "This table presents the results of an ablation study on the impact of different self-attention mechanisms within the ITACLIP model. Specifically, it investigates the performance of various combinations of self-attention types (query-query, key-key, and value-value) on the Pascal VOC dataset.  The goal is to determine which combination yields the best segmentation performance, providing insights into the effectiveness of different self-attention strategies.", "section": "4.3 Ablation Study"}, {"content": "| Method | FFN | Stuff | Object | VOC | Context |\n|---|---|---|---|---|---| \n| ITACLIP | \u2713 | 26.3 | 36.9 | 66.3 | 36.3 |\n| ITACLIP | \u2717 | **27.0** | **37.7** | **67.9** | **37.5** |", "caption": "Table 3: Removing the feed-forward block. \u201cStuff\u201d and \u201cObject\u201d refer to the COCO-Stuff and COCO-Object, respectively.", "description": "This table presents the ablation study results focusing on the impact of removing the feed-forward network (FFN) from the final layer of the Vision Transformer (ViT) in the ITACLIP model.  It shows the mean Intersection over Union (mIoU) scores for different semantic segmentation datasets: COCO-Stuff (evaluating 'stuff' classes), COCO-Object (evaluating 'object' classes which are simplified from COCO-Stuff), and Pascal VOC.  The results demonstrate how removing the FFN affects the performance of the model on various datasets.", "section": "3.3. Proposed Architectural Modifications"}, {"content": "| Intermediate Layers (<math alttext=\"l^{\" class=\"ltx_Math\" display=\"inline\">l^{\"}</math>) | VOC |\n|---|---| \n| \u2717 | 65.0 |\n| {7} | 65.4 |\n| {8} | 65.5 |\n| {7, 8} | 65.5 |\n| {7, 8, 10} | **65.6** |\n| {7, 8, 9, 10} | 65.5 |", "caption": "Table 4: Impact of selected intermediate layers. We assess ITACLIP with various intermediate layers on Pascal VOC. \u2717 indicates that the model does not use intermediate layers for evaluation.", "description": "This table investigates the effect of incorporating attention maps from intermediate layers of the CLIP model's visual encoder into the final layer's attention map for improved semantic segmentation.  The experiment focuses on the Pascal VOC dataset.  The rows represent different combinations of intermediate layers included, while the columns present the resulting mean Intersection over Union (mIoU) scores. The 'X' entry indicates that only the final layer's attention map was used, serving as a baseline for comparison.", "section": "4. Experiments"}, {"content": "| Method | PAMR | Stuff | Object | VOC | Context |\n|---|---|---|---|---|---| \n| ITACLIP | \u2717 | 26.3 | 36.4 | 65.6 | 36.0 |\n| ITACLIP | \u2713 | **27.0** | **37.7** | **67.9** | **37.5** |", "caption": "Table 5: Influence of post-processing operation. Comparing the performance of ITACLIP with and without PAMR on all datasets.", "description": "Table 5 presents a comparison of the performance of the ITACLIP model with and without the application of post-processing using Pixel-Adaptive Mask Refinement (PAMR).  The table shows the mean Intersection over Union (mIoU) scores achieved on four semantic segmentation benchmark datasets: COCO-Stuff, COCO-Object, Pascal VOC, and Pascal Context.  This allows for assessing the impact of PAMR on the model's overall accuracy and highlighting its contribution to enhancing segmentation quality.", "section": "4.3 Ablation Study"}, {"content": "| Method | LTG | IE | Context | Stuff |\n|---|---|---|---|---|\n| ITACLIP | \u2717 | \u2717 | 34.3 | 24.5 |\n| ITACLIP | \u2713 | \u2717 | 34.6 | 24.8 |\n| ITACLIP | \u2713 | \u2713 | **35.4** | **25.4** |", "caption": "Table 6: Effect of Image Engineering and LLM-based Text Generation modules. LTG and IE represent the LLM-based Text Generation and Image Engineering modules, respectively.", "description": "This table presents an ablation study evaluating the impact of two key modules in the ITACLIP model: Image Engineering (IE) and LLM-based Text Generation (LTG).  It shows the model's performance on the Pascal Context and COCO-Stuff datasets with different combinations of these modules enabled or disabled.  The results demonstrate the individual and combined contributions of each module to the overall segmentation accuracy.", "section": "4.3 Ablation Study"}, {"content": "| Method | Stride | Stuff | Object | VOC | Context |\n|---|---|---|---|---|---| \n| ITACLIP | 224 | 25.3 | 36.7 | 66.1 | 36.9 |\n| ITACLIP | 112 | 26.6 | 37.4 | 67.1 | 37.4 |\n| ITACLIP | 56 | 26.9 | 37.7 | 67.9 | 37.5 |\n| ITACLIP | 28 | 27.0 | 37.7 | 67.9 | 37.5 |", "caption": "Table 7: Role of stride value. We investigate the role of the stride value in our method across all four datasets.", "description": "This table investigates the impact of different stride values on the performance of the ITACLIP model across four semantic segmentation datasets: COCO-Stuff, COCO-Object, Pascal VOC, and Pascal Context.  The stride value affects the speed and resolution of the segmentation process, with smaller strides potentially offering better accuracy at the cost of increased computational time.  The results show how the mIoU score varies across different stride values for each dataset, enabling the researchers to determine the optimal balance between computational efficiency and segmentation quality.", "section": "4. Experiments"}, {"content": "| \u03bb | \u03b1 | Context |\n|---|---|---|\n| 0.75 | 0.15 | **36.0** |\n| 0.6 | 0.15 | 35.9 |\n| 0.5 | 0.15 | 35.8 |\n| 0.75 | 0.2 | 35.9 |\n| 0.5 | 0.2 | 35.7 |\n| 0.6 | 0.2 | 35.9 |\n| 0.6 | 0.1 | 35.9 |\n| 0.5 | 0.1 | 35.8 |", "caption": "Table 8: Effect of Hyperparameters. \u03bb\ud835\udf06\\lambdaitalic_\u03bb and \u03b1\ud835\udefc\\alphaitalic_\u03b1 denote the Image Engineering and Auxiliary Text Coefficients, respectively. The experiments are conducted without applying PAMR.", "description": "This table presents an ablation study analyzing the effect of the hyperparameters  \u03bb (lambda), representing the Image Engineering coefficient, and \u03b1 (alpha), representing the Auxiliary Text coefficient, on the model's performance.  The experiment is conducted without the post-processing technique PAMR to isolate the impact of \u03bb and \u03b1.  Different values for \u03bb and \u03b1 are tested to determine their influence on model performance, measured on several benchmark datasets.", "section": "4.3 Ablation Study"}, {"content": "| Hyperparameter | Stuff | Object | VOC | Context |\n|---|---|---|---|---|\n| \u03bb | 0.75 | 0.75 | 0.7 | 0.75 |\n| \u03b1 | 0.2 | 0.1 | 0.05 | 0.15 |", "caption": "Table 9: Hyperparameter values used in our experiments.", "description": "Table 9 shows the values used for the hyperparameters \u03bb (lambda) and \u03b1 (alpha) in the ITACLIP model experiments.  Lambda controls the weighting between first-category and second-category image features during image engineering, while alpha balances the contribution of original class names and LLM-generated auxiliary texts in the text embeddings.  These hyperparameters were tuned and set for all four datasets used in the ITACLIP model evaluation (COCO-Stuff, COCO-Object, Pascal Context, Pascal VOC).", "section": "4. Experiments"}, {"content": "| Backbone | VOC |\n|---|---| \n| ViT-L/14 | 53.3 |\n| ViT-B/32 | 56.7 |\n| ViT-B/16 | **67.9** |", "caption": "Table 10: Impact of different visual backbones. We compare the performance of ITACLIP with different visual backbones.", "description": "This table presents a comparison of the performance of the ITACLIP model when using different visual backbones.  It shows the mean Intersection over Union (mIoU) scores achieved on the Pascal VOC dataset for three different visual backbone architectures: ViT-L/14, ViT-B/32, and ViT-B/16.  The results highlight the impact of the visual backbone choice on the overall model performance, indicating which architecture is most effective for the ITACLIP semantic segmentation method.", "section": "4.2 Main Results"}, {"content": "| Method | Background Set | Object |\n|---|---|---|\n| ITACLIP | \u2717 | 34.5 |\n| ITACLIP | \u2713 | 37.7 |", "caption": "Table 11: Effect of background set. ITACLIP performs better when the background set is employed.", "description": "This table presents an ablation study evaluating the impact of using a defined background set on the performance of the ITACLIP model for semantic segmentation on the COCO-Object dataset.  The results demonstrate a significant improvement in performance when a specific background set is included in the model's input, highlighting its importance in distinguishing between foreground and background elements.  The table compares the mIoU scores obtained with and without this defined background set, clearly showing the benefit of using it.", "section": "4.2 Main Results"}]