{"references": [{"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-00-00", "reason": "This paper introduces CLIP, a foundational vision-language model that is the basis of the proposed ITACLIP method."}, {"fullname_first_author": "Alexander Kirillov", "paper_title": "Segment anything", "publication_date": "2023-00-00", "reason": "This paper introduces SAM, a large foundational segmentation model that is compared against in terms of performance and discussed in the context of training-free semantic segmentation."}, {"fullname_first_author": "Holger Caesar", "paper_title": "Coco-stuff: Thing and stuff classes in context", "publication_date": "2018-00-00", "reason": "This paper introduces the COCO-Stuff dataset, a benchmark dataset used for evaluating the performance of the ITACLIP model."}, {"fullname_first_author": "Mark Everingham", "paper_title": "The pascal visual object classes (voc) challenge", "publication_date": "2010-00-00", "reason": "This paper introduces the Pascal VOC dataset, another benchmark dataset used for evaluating the performance of the ITACLIP model."}, {"fullname_first_author": "Roozbeh Mottaghi", "paper_title": "The role of context for object detection and semantic segmentation in the wild", "publication_date": "2014-00-00", "reason": "This paper discusses the importance of context in object detection and semantic segmentation, which is relevant to the ITACLIP model's approach."}]}