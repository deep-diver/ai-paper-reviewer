<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><title>TokenVerse: Versatile Multi-concept Personalization in Token Modulation Space &#183; HF Daily Paper Reviews by AI</title>
<meta name=title content="TokenVerse: Versatile Multi-concept Personalization in Token Modulation Space &#183; HF Daily Paper Reviews by AI"><meta name=description content="TokenVerse: Extract & combine visual concepts from multiple images for creative image generation!"><meta name=keywords content="Computer Vision,Image Generation,üè¢ Google DeepMind,"><link rel=canonical href=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.12224/><link type=text/css rel=stylesheet href=/ai-paper-reviewer/css/main.bundle.min.595affd4445a931ea6d6e3a5a3c709930fa52a60be10b21c6f81fdb8fecaacea33aacedf80cdc88be45f189be14ed4ce53ea74a1e1406fad9cbf90c5ed409173.css integrity="sha512-WVr/1ERakx6m1uOlo8cJkw+lKmC+ELIcb4H9uP7KrOozqs7fgM3Ii+RfGJvhTtTOU+p0oeFAb62cv5DF7UCRcw=="><script type=text/javascript src=/ai-paper-reviewer/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script><script defer type=text/javascript id=script-bundle src=/ai-paper-reviewer/js/main.bundle.min.efbf3b6b987689fffaf2d7b73173d2690c0279a04d444b0537a77d7f4ff6e6d493445400cb0cf56bc0f0f123e19f15394e63cae34e67f069bd013dd5c73df56e.js integrity="sha512-7787a5h2if/68te3MXPSaQwCeaBNREsFN6d9f0/25tSTRFQAywz1a8Dw8SPhnxU5TmPK405n8Gm9AT3Vxz31bg==" data-copy data-copied></script><script src=/ai-paper-reviewer/lib/zoom/zoom.min.37d2094687372da3f7343a221a470f6b8806f7891aa46a5a03966af7f0ebd38b9fe536cb154e6ad28f006d184b294525a7c4054b6bbb4be62d8b453b42db99bd.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ=="></script><link rel=apple-touch-icon sizes=180x180 href=/ai-paper-reviewer/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/ai-paper-reviewer/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/ai-paper-reviewer/favicon-16x16.png><link rel=manifest href=/ai-paper-reviewer/site.webmanifest><meta property="og:url" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.12224/"><meta property="og:site_name" content="HF Daily Paper Reviews by AI"><meta property="og:title" content="TokenVerse: Versatile Multi-concept Personalization in Token Modulation Space"><meta property="og:description" content="TokenVerse: Extract & combine visual concepts from multiple images for creative image generation!"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="paper-reviews"><meta property="article:published_time" content="2025-01-21T00:00:00+00:00"><meta property="article:modified_time" content="2025-01-21T00:00:00+00:00"><meta property="article:tag" content="Computer Vision"><meta property="article:tag" content="Image Generation"><meta property="article:tag" content="üè¢ Google DeepMind"><meta property="og:image" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.12224/cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.12224/cover.png"><meta name=twitter:title content="TokenVerse: Versatile Multi-concept Personalization in Token Modulation Space"><meta name=twitter:description content="TokenVerse: Extract & combine visual concepts from multiple images for creative image generation!"><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"Paper Reviews by AI","name":"TokenVerse: Versatile Multi-concept Personalization in Token Modulation Space","headline":"TokenVerse: Versatile Multi-concept Personalization in Token Modulation Space","abstract":"TokenVerse: Extract \u0026amp; combine visual concepts from multiple images for creative image generation!","inLanguage":"en","url":"https:\/\/deep-diver.github.io\/ai-paper-reviewer\/paper-reviews\/2501.12224\/","author":{"@type":"Person","name":"Hugging Face Daily Papers"},"copyrightYear":"2025","dateCreated":"2025-01-21T00:00:00\u002b00:00","datePublished":"2025-01-21T00:00:00\u002b00:00","dateModified":"2025-01-21T00:00:00\u002b00:00","keywords":["Computer Vision","Image Generation","üè¢ Google DeepMind"],"mainEntityOfPage":"true","wordCount":"4649"}]</script><meta name=author content="Hugging Face Daily Papers"><link href=https://github.com/deep-diver/paper-reviewer/ rel=me><link href=https://twitter.com/algo_diver/ rel=me><script src=/ai-paper-reviewer/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj+KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script><script defer src=/ai-paper-reviewer/lib/typeit/typeit.umd.1b3200cb448f5cd1f548f2781452643d3511a43584b377b82c03a58055da4fdb7bc8f6c6c2ce846480c7677ff25bfd0d75f15823c09443ab18e0fd2cad792587.js integrity="sha512-GzIAy0SPXNH1SPJ4FFJkPTURpDWEs3e4LAOlgFXaT9t7yPbGws6EZIDHZ3/yW/0NdfFYI8CUQ6sY4P0srXklhw=="></script><script defer src=/ai-paper-reviewer/lib/packery/packery.pkgd.min.js integrity></script><script type=text/javascript src=/ai-paper-reviewer/js/shortcodes/gallery.min.9b4cb28f931ed922c26fb9b2510c2debb370f6a63305050c2af81740b2919883715e24efbbdf3a081496718ec751df3a72729d4d0bc71d6071297563a97ce1ee.js integrity="sha512-m0yyj5Me2SLCb7myUQwt67Nw9qYzBQUMKvgXQLKRmINxXiTvu986CBSWcY7HUd86cnKdTQvHHWBxKXVjqXzh7g=="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-KX0S6Q55Y7"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-KX0S6Q55Y7")</script><meta name=theme-color><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-app.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-firestore.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-auth.js></script><script>const firebaseConfig={apiKey:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",authDomain:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",projectId:"neurips2024-f3065",storageBucket:"neurips2024-f3065.firebasestorage.app",messagingSenderId:"982475958898",appId:"1:982475958898:web:2147e5d7753d6ac091f0eb",measurementId:"G-YQ46HXQ9JS"};var app=firebase.initializeApp(firebaseConfig),db=firebase.firestore(),auth=firebase.auth()</script></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a></div><div class=min-h-[148px]></div><div class="fixed inset-x-0 pl-[24px] pr-[24px]" style=z-index:100><div id=menu-blur class="absolute opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl shadow-2xl"></div><div class="relative max-w-[64rem] ml-auto mr-auto"><div style=padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start space-x-3"><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/ai-paper-reviewer/ class="text-base font-medium text-gray-500 hover:text-gray-900">HF Daily Paper Reviews by AI</a></nav><nav class="hidden md:flex items-center space-x-5 md:ml-12 h-12"><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title="About This Project">About</p></a><a href=/ai-paper-reviewer/2025-01-24/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title=2025-01-24s>2025-01-24</p></a><a href=/ai-paper-reviewer/2025-01-27/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title=2025-01-27s>2025-01-27</p></a><a href=/ai-paper-reviewer/2025-01-28/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title=2025-01-28s>2025-01-28</p></a><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title="Paper Reviews by AI">Archive</p></a><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title=Tags>Tags</p></a><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></span><p class="text-base font-medium" title></p></a><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span><p class="text-base font-medium" title></p></a><button id=search-button aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button><div class="ltr:mr-14 rtl:ml-14 flex items-center"><button id=appearance-switcher aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></nav><div class="flex md:hidden items-center space-x-5 md:ml-12 h-12"><span></span>
<button id=search-button-mobile aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></button>
<button id=appearance-switcher-mobile aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400" style=margin-right:5px><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div><div class="-my-2 -mr-2 md:hidden"><label id=menu-button class=block><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper style=padding-top:5px class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li id=menu-close-button><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title="About This Project">About</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-01-24/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title=2025-01-24s>2025-01-24</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-01-27/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title=2025-01-27s>2025-01-27</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-01-28/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title=2025-01-28s>2025-01-28</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title="Paper Reviews by AI">Archive</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title=Tags>Tags</p></a></li><li class=mt-1><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li><li class=mt-1><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li></ul></div></label></div></div></div></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("menu-blur");n.style.opacity=t/300})</script><div class="relative flex flex-col grow"><main id=main-content class=grow><article><div id=hero class="h-[150px] md:h-[200px]"></div><div class="fixed inset-x-0 top-0 h-[800px] single_hero_background nozoom" style=background-image:url(/ai-paper-reviewer/paper-reviews/2501.12224/cover_hu_6ba00ad953a388d2.png)><div class="absolute inset-0 bg-gradient-to-t from-neutral dark:from-neutral-800 to-transparent mix-blend-normal"></div><div class="absolute inset-0 opacity-60 bg-gradient-to-t from-neutral dark:from-neutral-800 to-neutral-100 dark:to-neutral-800 mix-blend-normal"></div></div><div id=background-blur class="fixed opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl"></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("background-blur");n.style.opacity=t/300})</script><header id=single_header class="mt-5 max-w-prose"><ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden"><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/>HF Daily Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class=inline><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/>Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/2501.12224/>TokenVerse: Versatile Multi-concept Personalization in Token Modulation Space</a><span class="px-1 text-primary-500">/</span></li></ol><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">TokenVerse: Versatile Multi-concept Personalization in Token Modulation Space</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime=2025-01-21T00:00:00+00:00>21 January 2025</time><span class="px-2 text-primary-500">&#183;</span><span>4649 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">22 mins</span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=views_paper-reviews/2501.12224/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=views>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 576 512"><path fill="currentcolor" d="M288 32c-80.8.0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7.0 24.6C17.3 304 48.6 356 95.4 399.4 142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1 3.3-7.9 3.3-16.7.0-24.6-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144 64.5-144 144-144 144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64-11.5.0-22.3-3-31.6-8.4-.2 2.8-.4 5.5-.4 8.4.0 53 43 96 96 96s96-43 96-96-43-96-96-96c-2.8.0-5.6.1-8.4.4 5.3 9.3 8.4 20.1 8.4 31.6z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=likes_paper-reviews/2501.12224/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=likes>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<button id=button_likes class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400" onclick=process_article()>
<span id=button_likes_heart style=display:none class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span><span id=button_likes_emtpty_heart class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M244 84l11.1 12 12-11.98C300.6 51.37 347 36.51 392.6 44.1 461.5 55.58 512 115.2 512 185.1V190.9c0 41.5-17.2 81.2-47.6 109.5L283.7 469.1c-7.5 7-17.4 10.9-27.7 10.9S235.8 476.1 228.3 469.1L47.59 300.4C17.23 272.1.0 232.4.0 190.9V185.1c0-69.9 50.52-129.52 119.4-141 44.7-7.59 92 7.27 124.6 39.9C243.1 84 244 84.01 244 84zm11.1 79.9-45-46.8c-21.7-20.82-52.5-30.7-82.8-25.66C81.55 99.07 48 138.7 48 185.1V190.9c0 28.2 11.71 55.2 32.34 74.4L256 429.3l175.7-164c20.6-19.2 32.3-46.2 32.3-74.4V185.1c0-46.4-33.6-86.03-79.3-93.66C354.4 86.4 323.6 96.28 301.9 117.1l-46.8 46.8z"/></svg>
</span></span><span id=button_likes_text>&nbsp;Like</span></button></span></div><div class="flex flex-row flex-wrap items-center"><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/ai-generated/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">AI Generated
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/-daily-papers/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">ü§ó Daily Papers
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/computer-vision/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Computer Vision
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/image-generation/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Image Generation
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/-google-deepmind/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">üè¢ Google DeepMind</span></span></span></div></div><div class="flex author"><img class="!mt-0 !mb-0 h-24 w-24 rounded-full ltr:mr-4 rtl:ml-4" width=96 height=96 alt="Hugging Face Daily Papers" src=/ai-paper-reviewer/img/avatar_hu_97e7d424fadd1c26.png><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">Author</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">Hugging Face Daily Papers</div><div class="text-sm text-neutral-700 dark:text-neutral-400">I am AI, and I review papers on HF Daily Papers</div><div class="text-2xl sm:text-lg"><div class="flex flex-wrap text-neutral-400 dark:text-neutral-500"><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://github.com/deep-diver/paper-reviewer/ target=_blank aria-label=Github rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://twitter.com/algo_diver/ target=_blank aria-label=Twitter rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span></a></div></div></div></div><div class=mb-5></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first lg:ml-auto px-0 lg:order-last ltr:lg:pl-8 rtl:lg:pr-8"><div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-[140px]"><details open id=TOCView class="toc-right mt-0 overflow-y-scroll overscroll-contain scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600 rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 hidden lg:block"><summary class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="min-w-[220px] py-2 border-dotted ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#tokenverse-a-novel-method>TokenVerse: A Novel Method</a></li><li><a href=#multi-concept-personalization>Multi-Concept Personalization</a></li><li><a href=#modulation-space-control>Modulation Space Control</a></li><li><a href=#disentangled-concept-learning>Disentangled Concept Learning</a></li><li><a href=#future-research-directions>Future Research Directions</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><details class="toc-inside mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 lg:hidden"><summary class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#tokenverse-a-novel-method>TokenVerse: A Novel Method</a></li><li><a href=#multi-concept-personalization>Multi-Concept Personalization</a></li><li><a href=#modulation-space-control>Modulation Space Control</a></li><li><a href=#disentangled-concept-learning>Disentangled Concept Learning</a></li><li><a href=#future-research-directions>Future Research Directions</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><script>var margin=200,marginError=50;(function(){var t=$(window),e=$("#TOCView"),s=e.height();function n(){var n=t.height()-margin;s>=n?(e.css("overflow-y","scroll"),e.css("max-height",n+marginError+"px")):(e.css("overflow-y","hidden"),e.css("max-height","9999999px"))}t.on("resize",n),$(document).ready(n)})()</script></div></div><div class="min-w-0 min-h-0 max-w-fit"><div class="article-content max-w-prose mb-20"><br><div class="flex flex-row flex-wrap items-center space-x-2"><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 48 48" fill="none"><rect width="48" height="48" fill="#fff" fill-opacity=".01"/><path d="M18 43V22c0-3.3137 2.6863-6 6-6s6 2.6863 6 6V43" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M12 40V22c0-6.6274 5.3726-12 12-12s12 5.3726 12 12V40" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M6 35V22C6 12.0589 14.0589 4 24 4s18 8.0589 18 18V35" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 44V31" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 24.625v-2.75" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/></svg>
</span></span><span>2501.12224</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg fill="#000" height="800" width="800" id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 511.999 511.999"><g><g><path d="M421.578 190.264l-99.847-99.847c-2.439-2.439-6.391-2.439-8.829.0L82.824 320.495c-2.439 2.439-2.439 6.392.0 8.829l99.847 99.847c2.439 2.439 6.391 2.439 8.829.0l230.078-230.078C424.017 196.655 424.017 192.703 421.578 190.264z"/></g></g><g><g><path d="M506.511 87.672 424.323 5.484c-7.308-7.31-19.175-7.315-26.488.0L348.219 55.1c-2.439 2.439-2.439 6.391.0 8.829l99.847 99.847c2.439 2.437 6.391 2.437 8.829.0l49.616-49.616C513.826 106.847 513.826 94.987 506.511 87.672z"/></g></g><g><g><path d="M508.133 491.11c-1.054-9.556-9.489-16.599-19.104-16.599H111.633l36.058-15.163c4.088-1.719 5.131-7.034 1.994-10.17l-86.854-86.854c-3.137-3.135-8.451-2.094-10.17 1.994C52.224 365.359 2.052 484.66 1.627 485.707c-5.815 13.208 4.855 27.01 18.107 26.263H489.52C500.566 511.97 509.379 502.408 508.133 491.11z"/></g></g></svg>
</span></span><span>Daniel Garibi et el.</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span>ü§ó 2025-01-22</span></span></span></div></div><p><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://arxiv.org/abs/2501.12224 target=_self role=button>‚Üó arXiv
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://huggingface.co/papers/2501.12224 target=_self role=button>‚Üó Hugging Face</a></p><audio controls><source src=https://ai-paper-reviewer.com/2501.12224/podcast.wav type=audio/wav>Your browser does not support the audio element.</audio><h3 class="relative group">TL;DR<div id=tldr class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#tldr aria-label=Anchor>#</a></span></h3><div class="lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl"><p>Current methods for generating personalized images struggle with handling multiple concepts and composing them flexibly from various sources. They often require segmentation masks or bounding boxes, and struggle to disentangle multiple concepts within a single image, limiting their versatility. Existing approaches either fine-tune the model, limiting seamless concept integration, or optimize the input text embedding, lacking the expressiveness to fully capture the nuances of concepts.</p><p>TokenVerse overcomes these issues using an optimization-based framework applied to a pre-trained text-to-image diffusion model. It disentangles visual concepts from images using only accompanying captions. The method utilizes the modulation space of the model to learn personalized representations for text tokens which can then be flexibly incorporated into new prompts to generate images with creative compositions of multiple concepts. Experiments show improved results over existing techniques in image personalization and composition tasks.</p></div><h4 class="relative group">Key Takeaways<div id=key-takeaways class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#key-takeaways aria-label=Anchor>#</a></span></h4><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-3d7b0a5cd3c8f9160b757810c1adf505></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-3d7b0a5cd3c8f9160b757810c1adf505",{strings:[" TokenVerse enables disentangled multi-concept personalization, extracting diverse visual elements from single images. "],speed:10,lifeLike:!0,startDelay:0,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-d3695a6c9ceba03b0a6686608b7db1e8></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-d3695a6c9ceba03b0a6686608b7db1e8",{strings:[" It supports plug-and-play composition of concepts from multiple images without using masks or fine-tuning. "],speed:10,lifeLike:!0,startDelay:1e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-2d97b9e9f06fa18c007448cbfa7510fe></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-2d97b9e9f06fa18c007448cbfa7510fe",{strings:[" TokenVerse shows superior performance on challenging personalization tasks compared to existing methods. "],speed:10,lifeLike:!0,startDelay:2e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><h4 class="relative group">Why does it matter?<div id=why-does-it-matter class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#why-does-it-matter aria-label=Anchor>#</a></span></h4><p>This paper is important because it presents <strong>TokenVerse</strong>, a novel framework for multi-concept personalization in image generation. It addresses limitations of existing methods by enabling <strong>flexible combination of concepts from multiple images without requiring masks or additional supervision</strong>. This opens avenues for creative content generation and diverse applications. The work is highly relevant to the current research trends in text-to-image generation, particularly in addressing controllability and disentanglement issues. The proposed method and insights into token modulation space will impact the work of researchers in computer vision and AI.</p><hr><h4 class="relative group">Visual Insights<div id=visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#visual-insights aria-label=Anchor>#</a></span></h4><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2501.12224/x2.png alt></figure></p><blockquote><p>üîº TokenVerse is a novel framework that enables multi-concept image generation with personalized attributes extracted from multiple source images. The top row shows example source images, each containing various visual concepts (e.g., objects, attributes, poses, lighting). TokenVerse independently processes each source image and its caption to learn a disentangled representation for each concept, without needing masks or other supervision. It achieves this by learning a unique embedding for each word in the caption. Then, these personalized text tokens (shown in color in the caption below the generated images) are combined flexibly to create new and diverse images (bottom row) that express combinations of learned concepts.</p><details><summary>read the caption</summary>Figure 1: TokenVerse extracts distinct complex visual concepts from a set of concept images (top), and allows users to generate images that depict these concepts in novel versatile compositions (bottom row). Our framework independently processes each concept image, and learns to disentangle its concepts based solely on an accompanying caption, without any additional supervision or masks. This is achieved by learning a personalized representation for each token in the source caption. Our personalized text tokens, extracted from multiple images, are then flexibly incorporated into new text prompts (colored words) to generate novel creative images.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id=S6.T1.15><thead class=ltx_thead><tr class=ltx_tr id=S6.T1.15.16.1><th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id=S6.T1.15.16.1.1></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id=S6.T1.15.16.1.2><span class=ltx_text id=S6.T1.15.16.1.2.1 style=font-size:90%>Concept</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id=S6.T1.15.16.1.3><span class=ltx_text id=S6.T1.15.16.1.3.1 style=font-size:90%>Concept</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id=S6.T1.15.16.1.4><span class=ltx_text id=S6.T1.15.16.1.4.1 style=font-size:90%>masks</span></th></tr><tr class=ltx_tr id=S6.T1.15.17.2><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r" id=S6.T1.15.17.2.1><span class="ltx_text ltx_font_bold" id=S6.T1.15.17.2.1.1 style=font-size:90%>Method</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id=S6.T1.15.17.2.2><span class=ltx_text id=S6.T1.15.17.2.2.1 style=font-size:90%>Decomposition</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id=S6.T1.15.17.2.3><span class=ltx_text id=S6.T1.15.17.2.3.1 style=font-size:90%>Composition</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id=S6.T1.15.17.2.4><span class=ltx_text id=S6.T1.15.17.2.4.1 style=font-size:90%>free</span></th></tr></thead><tbody class=ltx_tbody><tr class=ltx_tr id=S6.T1.3.3><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id=S6.T1.3.3.4><span class=ltx_text id=S6.T1.3.3.4.1 style=font-size:90%>DB LoRA</span></th><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S6.T1.1.1.1><math alttext="\times" class="ltx_Math" display="inline" id="S6.T1.1.1.1.m1.1"><semantics id="S6.T1.1.1.1.m1.1a"><mo id="S6.T1.1.1.1.m1.1.1" mathsize="90%" xref="S6.T1.1.1.1.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S6.T1.1.1.1.m1.1b"><times id="S6.T1.1.1.1.m1.1.1.cmml" xref="S6.T1.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.1.1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S6.T1.1.1.1.m1.1d">√ó</annotation></semantics></math></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S6.T1.2.2.2><math alttext="\times" class="ltx_Math" display="inline" id="S6.T1.2.2.2.m1.1"><semantics id="S6.T1.2.2.2.m1.1a"><mo id="S6.T1.2.2.2.m1.1.1" mathsize="90%" xref="S6.T1.2.2.2.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S6.T1.2.2.2.m1.1b"><times id="S6.T1.2.2.2.m1.1.1.cmml" xref="S6.T1.2.2.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.2.2.2.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S6.T1.2.2.2.m1.1d">√ó</annotation></semantics></math></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S6.T1.3.3.3><math alttext="\surd" class="ltx_Math" display="inline" id="S6.T1.3.3.3.m1.1"><semantics id="S6.T1.3.3.3.m1.1a"><mo id="S6.T1.3.3.3.m1.1.1" mathsize="90%" xref="S6.T1.3.3.3.m1.1.1.cmml">‚àö</mo><annotation-xml encoding="MathML-Content" id="S6.T1.3.3.3.m1.1b"><csymbol cd="latexml" id="S6.T1.3.3.3.m1.1.1.cmml" xref="S6.T1.3.3.3.m1.1.1">square-root</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.3.3.3.m1.1c">\surd</annotation><annotation encoding="application/x-llamapun" id="S6.T1.3.3.3.m1.1d">‚àö</annotation></semantics></math></td></tr><tr class=ltx_tr id=S6.T1.6.6><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r" id=S6.T1.6.6.4><span class=ltx_text id=S6.T1.6.6.4.1 style=font-size:90%>Break-A-Scene</span></th><td class="ltx_td ltx_align_center ltx_border_r" id=S6.T1.4.4.1><math alttext="\surd" class="ltx_Math" display="inline" id="S6.T1.4.4.1.m1.1"><semantics id="S6.T1.4.4.1.m1.1a"><mo id="S6.T1.4.4.1.m1.1.1" mathsize="90%" xref="S6.T1.4.4.1.m1.1.1.cmml">‚àö</mo><annotation-xml encoding="MathML-Content" id="S6.T1.4.4.1.m1.1b"><csymbol cd="latexml" id="S6.T1.4.4.1.m1.1.1.cmml" xref="S6.T1.4.4.1.m1.1.1">square-root</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.4.4.1.m1.1c">\surd</annotation><annotation encoding="application/x-llamapun" id="S6.T1.4.4.1.m1.1d">‚àö</annotation></semantics></math></td><td class="ltx_td ltx_align_center ltx_border_r" id=S6.T1.5.5.2><math alttext="\times" class="ltx_Math" display="inline" id="S6.T1.5.5.2.m1.1"><semantics id="S6.T1.5.5.2.m1.1a"><mo id="S6.T1.5.5.2.m1.1.1" mathsize="90%" xref="S6.T1.5.5.2.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S6.T1.5.5.2.m1.1b"><times id="S6.T1.5.5.2.m1.1.1.cmml" xref="S6.T1.5.5.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.5.5.2.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S6.T1.5.5.2.m1.1d">√ó</annotation></semantics></math></td><td class="ltx_td ltx_align_center ltx_border_r" id=S6.T1.6.6.3><math alttext="\times" class="ltx_Math" display="inline" id="S6.T1.6.6.3.m1.1"><semantics id="S6.T1.6.6.3.m1.1a"><mo id="S6.T1.6.6.3.m1.1.1" mathsize="90%" xref="S6.T1.6.6.3.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S6.T1.6.6.3.m1.1b"><times id="S6.T1.6.6.3.m1.1.1.cmml" xref="S6.T1.6.6.3.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.6.6.3.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S6.T1.6.6.3.m1.1d">√ó</annotation></semantics></math></td></tr><tr class=ltx_tr id=S6.T1.9.9><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r" id=S6.T1.9.9.4><span class=ltx_text id=S6.T1.9.9.4.1 style=font-size:90%>ConceptExpress</span></th><td class="ltx_td ltx_align_center ltx_border_r" id=S6.T1.7.7.1><math alttext="\surd" class="ltx_Math" display="inline" id="S6.T1.7.7.1.m1.1"><semantics id="S6.T1.7.7.1.m1.1a"><mo id="S6.T1.7.7.1.m1.1.1" mathsize="90%" xref="S6.T1.7.7.1.m1.1.1.cmml">‚àö</mo><annotation-xml encoding="MathML-Content" id="S6.T1.7.7.1.m1.1b"><csymbol cd="latexml" id="S6.T1.7.7.1.m1.1.1.cmml" xref="S6.T1.7.7.1.m1.1.1">square-root</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.7.7.1.m1.1c">\surd</annotation><annotation encoding="application/x-llamapun" id="S6.T1.7.7.1.m1.1d">‚àö</annotation></semantics></math></td><td class="ltx_td ltx_align_center ltx_border_r" id=S6.T1.8.8.2><math alttext="\times" class="ltx_Math" display="inline" id="S6.T1.8.8.2.m1.1"><semantics id="S6.T1.8.8.2.m1.1a"><mo id="S6.T1.8.8.2.m1.1.1" mathsize="90%" xref="S6.T1.8.8.2.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S6.T1.8.8.2.m1.1b"><times id="S6.T1.8.8.2.m1.1.1.cmml" xref="S6.T1.8.8.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.8.8.2.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S6.T1.8.8.2.m1.1d">√ó</annotation></semantics></math></td><td class="ltx_td ltx_align_center ltx_border_r" id=S6.T1.9.9.3><math alttext="\surd" class="ltx_Math" display="inline" id="S6.T1.9.9.3.m1.1"><semantics id="S6.T1.9.9.3.m1.1a"><mo id="S6.T1.9.9.3.m1.1.1" mathsize="90%" xref="S6.T1.9.9.3.m1.1.1.cmml">‚àö</mo><annotation-xml encoding="MathML-Content" id="S6.T1.9.9.3.m1.1b"><csymbol cd="latexml" id="S6.T1.9.9.3.m1.1.1.cmml" xref="S6.T1.9.9.3.m1.1.1">square-root</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.9.9.3.m1.1c">\surd</annotation><annotation encoding="application/x-llamapun" id="S6.T1.9.9.3.m1.1d">‚àö</annotation></semantics></math></td></tr><tr class=ltx_tr id=S6.T1.12.12><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r" id=S6.T1.12.12.4><span class=ltx_text id=S6.T1.12.12.4.1 style=font-size:90%>OMG</span></th><td class="ltx_td ltx_align_center ltx_border_r" id=S6.T1.10.10.1><math alttext="\times" class="ltx_Math" display="inline" id="S6.T1.10.10.1.m1.1"><semantics id="S6.T1.10.10.1.m1.1a"><mo id="S6.T1.10.10.1.m1.1.1" mathsize="90%" xref="S6.T1.10.10.1.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S6.T1.10.10.1.m1.1b"><times id="S6.T1.10.10.1.m1.1.1.cmml" xref="S6.T1.10.10.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.10.10.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S6.T1.10.10.1.m1.1d">√ó</annotation></semantics></math></td><td class="ltx_td ltx_align_center ltx_border_r" id=S6.T1.11.11.2><math alttext="\surd" class="ltx_Math" display="inline" id="S6.T1.11.11.2.m1.1"><semantics id="S6.T1.11.11.2.m1.1a"><mo id="S6.T1.11.11.2.m1.1.1" mathsize="90%" xref="S6.T1.11.11.2.m1.1.1.cmml">‚àö</mo><annotation-xml encoding="MathML-Content" id="S6.T1.11.11.2.m1.1b"><csymbol cd="latexml" id="S6.T1.11.11.2.m1.1.1.cmml" xref="S6.T1.11.11.2.m1.1.1">square-root</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.11.11.2.m1.1c">\surd</annotation><annotation encoding="application/x-llamapun" id="S6.T1.11.11.2.m1.1d">‚àö</annotation></semantics></math></td><td class="ltx_td ltx_align_center ltx_border_r" id=S6.T1.12.12.3><math alttext="\surd" class="ltx_Math" display="inline" id="S6.T1.12.12.3.m1.1"><semantics id="S6.T1.12.12.3.m1.1a"><mo id="S6.T1.12.12.3.m1.1.1" mathsize="90%" xref="S6.T1.12.12.3.m1.1.1.cmml">‚àö</mo><annotation-xml encoding="MathML-Content" id="S6.T1.12.12.3.m1.1b"><csymbol cd="latexml" id="S6.T1.12.12.3.m1.1.1.cmml" xref="S6.T1.12.12.3.m1.1.1">square-root</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.12.12.3.m1.1c">\surd</annotation><annotation encoding="application/x-llamapun" id="S6.T1.12.12.3.m1.1d">‚àö</annotation></semantics></math></td></tr><tr class=ltx_tr id=S6.T1.15.15><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r" id=S6.T1.15.15.4><span class=ltx_text id=S6.T1.15.15.4.1 style=font-size:90%>TokenVerse (Ours)</span></th><td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id=S6.T1.13.13.1><math alttext="\surd" class="ltx_Math" display="inline" id="S6.T1.13.13.1.m1.1"><semantics id="S6.T1.13.13.1.m1.1a"><mo id="S6.T1.13.13.1.m1.1.1" mathsize="90%" xref="S6.T1.13.13.1.m1.1.1.cmml">‚àö</mo><annotation-xml encoding="MathML-Content" id="S6.T1.13.13.1.m1.1b"><csymbol cd="latexml" id="S6.T1.13.13.1.m1.1.1.cmml" xref="S6.T1.13.13.1.m1.1.1">square-root</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.13.13.1.m1.1c">\surd</annotation><annotation encoding="application/x-llamapun" id="S6.T1.13.13.1.m1.1d">‚àö</annotation></semantics></math></td><td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id=S6.T1.14.14.2><math alttext="\surd" class="ltx_Math" display="inline" id="S6.T1.14.14.2.m1.1"><semantics id="S6.T1.14.14.2.m1.1a"><mo id="S6.T1.14.14.2.m1.1.1" mathsize="90%" xref="S6.T1.14.14.2.m1.1.1.cmml">‚àö</mo><annotation-xml encoding="MathML-Content" id="S6.T1.14.14.2.m1.1b"><csymbol cd="latexml" id="S6.T1.14.14.2.m1.1.1.cmml" xref="S6.T1.14.14.2.m1.1.1">square-root</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.14.14.2.m1.1c">\surd</annotation><annotation encoding="application/x-llamapun" id="S6.T1.14.14.2.m1.1d">‚àö</annotation></semantics></math></td><td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id=S6.T1.15.15.3><math alttext="\surd" class="ltx_Math" display="inline" id="S6.T1.15.15.3.m1.1"><semantics id="S6.T1.15.15.3.m1.1a"><mo id="S6.T1.15.15.3.m1.1.1" mathsize="90%" xref="S6.T1.15.15.3.m1.1.1.cmml">‚àö</mo><annotation-xml encoding="MathML-Content" id="S6.T1.15.15.3.m1.1b"><csymbol cd="latexml" id="S6.T1.15.15.3.m1.1.1.cmml" xref="S6.T1.15.15.3.m1.1.1">square-root</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.15.15.3.m1.1c">\surd</annotation><annotation encoding="application/x-llamapun" id="S6.T1.15.15.3.m1.1d">‚àö</annotation></semantics></math></td></tr></tbody></table></table></figure><blockquote><p>üîº Table 1 compares the capabilities of five different methods for generating images with personalized concepts: DreamBooth, Break-a-Scene, ConceptExpress, OMG, and the authors&rsquo; proposed method (TokenVerse). The table focuses on two key aspects: concept decomposition (extracting and personalizing multiple distinct concepts from a single image) and concept composition (combining concepts learned from multiple images to generate new images). The comparison highlights the unique advantage of TokenVerse in performing both tasks without requiring segmentation masks, unlike previous methods which often need masks or other visual cues for concept isolation.</p><details><summary>read the caption</summary>Table 1: Capabilities of competing baselines. The table lists the capabilities of DreamBooth¬†[32], BAS¬†[8], ConceptExpress¬†[14], OMG¬†[23] and our method. Concept decomposition is the task of disentangling and personalizing multiple objects from a single image. Composition is the task of combining separately learned concepts in a new generated image. Unlike existing approaches, our method enables mask-free multi-object disentangled personalization, as well as the composition of multiple objects from several reference images.</details></blockquote><h3 class="relative group">In-depth insights<div id=in-depth-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#in-depth-insights aria-label=Anchor>#</a></span></h3><h4 class="relative group">TokenVerse: A Novel Method<div id=tokenverse-a-novel-method class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#tokenverse-a-novel-method aria-label=Anchor>#</a></span></h4><p>TokenVerse presents a novel approach to multi-concept personalization in image generation. <strong>Its core innovation lies in leveraging the modulation space of diffusion transformers (DiTs) for disentangling and composing visual concepts.</strong> Unlike methods relying on fine-tuning or explicit masking, TokenVerse extracts concept representations directly from image-caption pairs through an optimization process. This allows for unsupervised disentanglement of multiple concepts within a single image, even complex ones involving objects, accessories, poses, and lighting. <strong>The method&rsquo;s strength is its modularity</strong>, enabling the seamless combination of concepts extracted from various images, creating novel and coherent compositions without user-provided segmentation. <strong>This plug-and-play approach significantly broadens the scope of personalized image generation.</strong> By focusing on the per-token modulation space (M+), TokenVerse achieves highly localized control, avoiding unwanted side effects often seen in global modulation. The framework&rsquo;s efficiency and versatility are demonstrated through both quantitative and qualitative evaluations, showing superior performance over existing techniques in multi-concept personalization and composition tasks. The method&rsquo;s ability to handle a wide range of concepts with minimal supervision positions it as a significant advancement in the field of AI-based image generation, opening avenues for creative applications such as storytelling and personalized content creation.</p><h4 class="relative group">Multi-Concept Personalization<div id=multi-concept-personalization class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#multi-concept-personalization aria-label=Anchor>#</a></span></h4><p>Multi-concept personalization, as a research area, presents a significant challenge and opportunity in AI. It tackles the problem of <strong>generating images that seamlessly integrate multiple distinct concepts</strong> extracted from various sources, such as different images or text descriptions. This surpasses the limitations of single-concept personalization which struggles with complex scene generation. The core difficulty lies in disentangling individual concepts and learning their personalized representations in a way that allows for flexible composition. <strong>Unsupervised methods</strong>, which learn solely from image-caption pairs without requiring additional supervision like segmentation masks, are particularly valuable because they are more scalable and adaptable to various concepts. Successfully addressing multi-concept personalization requires novel approaches to representation learning that capture intricate relationships between concepts, and generation methods capable of fine-grained control over these relationships to achieve coherent and creative output. The ability to combine concepts extracted from diverse sources offers exciting possibilities for content creation and various other applications.</p><h4 class="relative group">Modulation Space Control<div id=modulation-space-control class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#modulation-space-control aria-label=Anchor>#</a></span></h4><p>The concept of &lsquo;Modulation Space Control&rsquo; in the context of generative models, particularly diffusion models, is a powerful technique for achieving fine-grained control over image generation. It leverages the model&rsquo;s internal representation, specifically the modulation space, to directly manipulate the generated image&rsquo;s features. Unlike traditional methods that rely on modifying text prompts or fine-tuning model weights, <strong>modulation space control offers a more direct and localized approach</strong>. By identifying specific directions within this space, corresponding to semantic attributes like object characteristics, poses, or lighting, one can selectively enhance or suppress these features in the generated image. This allows for a level of personalization and customization not readily available through other methods. <strong>The key advantage lies in its disentanglement capabilities</strong>, enabling independent control over multiple concepts within a single image. This disentanglement facilitates the creation of novel image compositions by combining different attributes extracted from multiple source images. However, the success of this technique hinges on the quality of the learned directions, <strong>requiring careful optimization to prevent interference or unwanted modifications to other features</strong>. Further research into efficient and robust methods for learning these directions, as well as a deeper understanding of the underlying semantic structure of the modulation space, is crucial for unlocking the full potential of this promising approach.</p><h4 class="relative group">Disentangled Concept Learning<div id=disentangled-concept-learning class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#disentangled-concept-learning aria-label=Anchor>#</a></span></h4><p>The concept of &lsquo;Disentangled Concept Learning&rsquo; within the context of a text-to-image model centers on the ability to <strong>isolate and independently manipulate individual visual elements</strong> within a generated image. Instead of treating an image as a monolithic entity, this approach seeks to <strong>parse it into constituent components</strong> such as objects, textures, poses, and lighting. The goal is to learn separate, disentangled representations for each concept, enabling flexible recombination and personalization. <strong>This disentanglement allows for granular control</strong>; the user can selectively modify certain aspects of an image without affecting others, which is a significant improvement over previous methods that frequently struggled with holistic image manipulation. Successful disentangled learning facilitates <strong>plug-and-play composition</strong>, where concepts extracted from diverse source images can be seamlessly integrated into novel compositions. This modular approach opens possibilities for creative image generation and personalized content creation exceeding the capabilities of previous techniques that primarily focused on single-concept manipulation or required explicit image segmentation.</p><h4 class="relative group">Future Research Directions<div id=future-research-directions class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#future-research-directions aria-label=Anchor>#</a></span></h4><p>Future research could explore several promising avenues. <strong>Improving the disentanglement of concepts</strong> within the modulation space is crucial; the current method sometimes struggles with highly similar concepts or those sharing name identifiers, leading to blending or unpredictable results. <strong>Developing more robust methods for handling incompatible concepts</strong> would significantly enhance the versatility of the model. Currently, attempting complex combinations can produce unexpected outputs. Investigating the potential of incorporating external knowledge graphs or semantic relationships could further improve concept understanding and combination. This would allow the model to reason about concepts in a more sophisticated way, generating even more creative and coherent results. <strong>Exploring different model architectures</strong> beyond Diffusion Transformers could reveal novel approaches to achieve improved disentanglement and compositional flexibility. Furthermore, the method&rsquo;s effectiveness with diverse data types and image modalities should be investigated, <strong>expanding beyond natural images</strong> to encompass, for example, 3D models, videos, or other visual formats. Finally, a deeper understanding of the underlying mechanisms of the modulation space and its relationship to semantic meaning would open new possibilities for enhanced user control and concept manipulation.</p><h3 class="relative group">More visual insights<div id=more-visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#more-visual-insights aria-label=Anchor>#</a></span></h3><details><summary>More on figures</summary><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2501.12224/x3.png alt></figure></p><blockquote><p>üîº Figure 2 illustrates the effect of modifying the modulation vector in the global modulation space (‚Ñ≥) and the per-token modulation space (‚Ñ≥+). The top row shows a generated image. Part (a) demonstrates modifying the global modulation vector to change concepts in the image. This often causes unintended changes to other concepts. Part (b) shows how modifying the modulation vector for only a single token (e.g., &lsquo;dog&rsquo;) results in localized changes that primarily affect the selected concept. This localized control is key to the proposed approach for multi-concept personalization.</p><details><summary>read the caption</summary>Figure 2: Directions in the global modulation space (‚Ñ≥‚Ñ≥\mathcal{M}caligraphic_M) and our per-token modulation space (‚Ñ≥+superscript‚Ñ≥\mathcal{M}^{+}caligraphic_M start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT). Given a generated image (top row), we modify it using text-driven directions in both ‚Ñ≥‚Ñ≥\mathcal{M}caligraphic_M and ‚Ñ≥+superscript‚Ñ≥\mathcal{M}^{+}caligraphic_M start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT spaces. (a) Adding a direction to the vector that is used to modulate all the text and image tokens (i.e. a direction in the space ‚Ñ≥‚Ñ≥\mathcal{M}caligraphic_M) can be used to effectively modify desired concepts in the generated image. Yet, this often results in non-local changes that also affect other concepts in the generated image. (b)¬†Adding a direction only to the modulation vector of a specific text token, like ‚Äúdog‚Äù or ‚Äúball‚Äù (i.e. a direction in the space ‚Ñ≥+superscript‚Ñ≥\mathcal{M}^{+}caligraphic_M start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT) leads to a localized modification that mostly affects the concept of interest.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2501.12224/x4.png alt></figure></p><blockquote><p>üîº This figure provides a detailed overview of the TokenVerse framework. Panel (a) illustrates the architecture of a pre-trained text-to-image diffusion transformer (DiT) model, highlighting the modulation, attention, and feed-forward modules within each DiT block. The modulation block&rsquo;s function of modulating tokens using a vector derived from a pooled text embedding is emphasized. Panel (b) depicts the training process of TokenVerse. It shows how, given a concept image and its caption, personalized modulation vector offsets are learned for each text token. These offsets represent unique directions within the modulation space, learned through a reconstruction objective. Panel (c) demonstrates the inference stage. It explains how the pre-learned direction vectors are applied to modulate text tokens during image generation, enabling the incorporation of personalized concepts into the resulting images.</p><details><summary>read the caption</summary>Figure 3: TokenVerse overview. (a) A pre-trained text-to-image DiT model processes both image and text tokens via a series of DiT blocks. Each block consists of modulation, attention and feed-forward modules. We focus on the modulation block, in which the tokens are modulated via a vector yùë¶yitalic_y, which is derived from a pooled text embedding. (b) Given a concept image and its corresponding caption, TokenVerse learns a personalized modulation vector offset ŒîŒî\Deltaroman_Œî for each text token. These offsets represent personalized directions in the modulation space and are learned using a simple reconstruction objective. (c) At inference, the pre-learned direction vectors are used to modulate the text tokens, enabling the injection of personalized concepts into the generated images.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2501.12224/x5.png alt></figure></p><blockquote><p>üîº This figure illustrates the concept isolation loss used in training the Concept-Mod model. During training, the model is presented with a concept image and its caption in 50% of the training steps. In addition, the model is presented with a second image and its caption that is unrelated to the concept image. The goal is for the model to learn directions in the modulation space that only modify the portions of the generated image corresponding to the concept image, leaving the unrelated image mostly unaffected. This is done by minimizing a loss function that encourages similarity between the portions of the generated image not relevant to the target concept image and its counterpart in the unrelated image.</p><details><summary>read the caption</summary>Figure 4: Concept isolation loss. When training Concept-Mod we apply an additional concept isolation loss in 50% of the training steps. This loss encourages learning directions that do not interfere with other images by enforcing that the parts in the image that should not be affected by the directions remain similar.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2501.12224/x6.png alt></figure></p><blockquote><p>üîº This figure showcases qualitative results of the TokenVerse model. Each row presents four source images, each containing several distinct visual concepts. The model independently extracts these concepts from each source image. The right side of the figure then displays three newly generated images. These generated images demonstrate the model&rsquo;s ability to seamlessly combine the extracted concepts into novel, coherent compositions, showcasing the versatility and effectiveness of TokenVerse in multi-concept personalization and image generation.</p><details><summary>read the caption</summary>Figure 5: Qualitative results. Each row begins with a bank of four source images, from which our method independently extracts concepts. To the right, three generated images are shown, demonstrating the seamless combination of these concepts into new, coherent outputs.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2501.12224/x7.png alt></figure></p><blockquote><p>üîº Figure 6 showcases the capability of TokenVerse to handle extreme multi-concept personalization. Unlike other methods, TokenVerse doesn&rsquo;t have limitations on the number of concepts that can be combined within a single generated image. The figure demonstrates this by showing an example image that successfully integrates many different concepts, highlighting the model&rsquo;s flexibility and power.</p><details><summary>read the caption</summary>Figure 6: Extreme multi-concept personalization. Our method has no technical constraint on the number of concepts that can be combined in an image. As can be seen, TokenVerse can generate images composing a significant number of concepts.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2501.12224/x8.png alt></figure></p><blockquote><p>üîº Figure 7 showcases TokenVerse&rsquo;s ability to personalize and combine various visual concepts beyond just objects. Three concept types are demonstrated: object (a bear, though its source image isn&rsquo;t displayed), pose (shown in the left column), and lighting (top row). The results highlight that TokenVerse effectively learns distinct representations for pose and lighting without being overly influenced by the specific subject or lighting setup, a significant advancement in disentangled concept learning.</p><details><summary>read the caption</summary>Figure 7: Concepts beyond objects. We demonstrate the composition of three types of personalized concepts: object (the bear; concept image not shown), pose (left column) and lighting (top row). TokenVerse successfully learns the pose and lighting without overfitting to the identity of the poser or the specific lit scene.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2501.12224/x9.png alt></figure></p><blockquote><p>üîº Figure 8 presents a qualitative comparison of different methods for multi-concept image generation. Each row shows two source images (concept images) and several generated images created by different techniques: ConceptExpress, Break-a-Scene, DreamBooth, OMG, and the proposed method (TokenVerse). The generated images combine concepts extracted from the two source images. The caption highlights that TokenVerse most effectively integrates the concepts while preserving their individual fidelity.</p><details><summary>read the caption</summary>Figure 8: Qualitative comparisons. Each row depicts two concept images (left) and images containing a combination of those concepts, generated by ConceptExpress¬†[14], BAS¬†[8], DreamBooth¬†[32], OMG¬†[23] and our method. The concepts associated with the green and blue words are taken from the left and right concept images, respectively. As can be seen, our method best composes the two concepts while preserving concept fidelity.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2501.12224/x10.png alt></figure></p><blockquote><p>üîº Figure 9 presents a quantitative comparison of the proposed TokenVerse method against other baselines across three tasks: concept composition (combining concepts from different images), concept decomposition (extracting concepts from a single image), and a full task combining both. The comparison is based on two metrics: concept preservation and prompt fidelity, both evaluated using DreamBench++ and a user study. TokenVerse consistently outperforms other methods in concept preservation while achieving high prompt fidelity, as detailed in Appendix C.2.</p><details><summary>read the caption</summary>Figure 9: Quantitative comparison. We compare our method to other baselines on concept preservation and prompt fidelity (higher is better) using DreamBench++ and a user study. (a) We compare three different settings: (i)ùëñ(i)( italic_i ) composing two concepts from different images (concept composition), (i‚Å¢i)ùëñùëñ(ii)( italic_i italic_i ) decomposing two concepts from the same image (concept decomposition), and (i‚Å¢i‚Å¢i)ùëñùëñùëñ(iii)( italic_i italic_i italic_i ) the combination of the two (full task). (b) We conduct a user study, comparing our method to existing methods on our full task. Our method consistently scores best in terms of concept preservation while maintaining high prompt fidelity scores. See App.¬†C.2 for the exact metrics.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2501.12224/x11.png alt></figure></p><blockquote><p>üîº This ablation study demonstrates the effect of progressively adding components to the TokenVerse model. The leftmost part of the figure displays the source concepts used to generate the images. Column (a) shows results without using the M+ space, applying direction vectors directly to input text tokens before entering the transformer. Column (b) incorporates the M+ space, adding the directions to modulation vectors of individual text tokens. Column (c) further refines the results by adding per-block directions. Finally, column (d) presents the full TokenVerse model, including the isolation loss to mitigate interference between concepts from different images. The progressive integration of these components highlights their individual contributions to the model&rsquo;s overall performance in generating images with multiple concepts from multiple sources.</p><details><summary>read the caption</summary>Figure 10: Ablations. The left pane shows all the concepts used to generate the result images. Columns (a) to (d) shows the results of our method as additional components are progressively integrated.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2501.12224/x12.png alt></figure></p><blockquote><p>üîº Figure 11 demonstrates limitations of the TokenVerse model. It shows examples where the model struggles with certain combinations of concepts, primarily due to the independent training process for each concept. Panel (a) shows instances of rare blending between concepts, which can be addressed through joint training. Panel (b) illustrates difficulties when concepts share the same name, resolved by using distinct names. Lastly, panel (c) shows that incompatible combinations (like a doll with tiny limbs in a complex pose) produce undesirable outputs.</p><details><summary>read the caption</summary>Figure 11: Limitations. Concept images are shown in the top row, with the generated images using TokenVerse below in each case. While our method supports both disentangled learning and multi-concept composition, limitations remain. (a) Rare blending can occur in specific combinations due to independent training of concepts; We provide analysis and mitigations in App.¬†F. (b)¬†Challenges arise with concepts sharing the same name identifier, which can be mitigated by using distinct terms. (c) Certain incompatible combinations, such as a doll with tiny limbs in a complex pose, may result in undesired outputs.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2501.12224/x13.png alt></figure></p><blockquote><p>üîº This ablation study investigates the impact of data augmentations on the model&rsquo;s performance. The top row displays the source concepts used to generate images. Column (a) presents the results obtained using the full method, which incorporates both text and image augmentations during training. Column (b) shows the results when these augmentations are omitted, highlighting their contribution to improved image generation.</p><details><summary>read the caption</summary>Figure 12: Augmentations Ablation. The top row shows the concepts used to generate the result images. Column (a) displays the results of our full method, while column (b) shows the results without text and image augmentations.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2501.12224/x14.png alt></figure></p><blockquote><p>üîº This figure demonstrates the progressive composition of concepts using TokenVerse. Each row shows a sequence of images, starting with a base image and progressively adding new personalized concepts (object, pose, lighting, and hair) while keeping other aspects consistent through text prompts. The background changes with each row, showcasing the flexibility of concept control through text input. For example, the top row shows the same subject in various poses and lighting conditions, set against backgrounds specified in the captions such as a city, garden, and mars.</p><details><summary>read the caption</summary>Figure 13: Progressive composition of concepts. TokenVerse can be used to progressively add concepts into a generated image, while controlling all other aspects of the generated images via text. In each row, the object, pose, lighting, and hair are personalized, while the background is described by text (e.g. ‚ÄúNY city‚Äù, ‚Äúgarden‚Äù, and ‚ÄúMars‚Äù for the top row.)</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2501.12224/x15.png alt></figure></p><blockquote><p>üîº This figure illustrates a limitation of the TokenVerse model when highly similar modulation tokens are used. Panel (a) shows a successful combination of a doll and a dog, representing a typical scenario where the model functions as expected. Panel (b) demonstrates a failure case where independent training leads to the generation of a hybrid object (characteristics of both the doll and the dog are merged). Panel (c) proposes a solution: employing joint training on both concepts during the model&rsquo;s training phase, which addresses the issue of generating hybrid objects by mitigating the similarity of modulation tokens. This demonstrates that training methodology affects the resulting images.</p><details><summary>read the caption</summary>Figure 14: Limitation ‚Äì highly similar modulated tokens. (a)¬†The common scenario of combining two distinct objects, such as a doll and a dog, into a single image. (b)¬†A failure case where independent training of concepts leads to the creation of hybrid objects. (c)¬†A potential mitigation for this issue by employing joint training on both concepts.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2501.12224/x16.png alt></figure></p><blockquote><p>üîº Figure 15 demonstrates a limitation of the TokenVerse model: when two objects in different images share the same textual identifier (e.g., both are described as &lsquo;dolls&rsquo;), the model struggles to generate images with both objects distinct and correctly represented. This issue is easily fixed by using different textual identifiers for each object during the initial training phase. The image shows two examples: (a) colliding captions where the model produces an unsatisfactory result; and (b) distinct captions where the model generates a satisfactory result.</p><details><summary>read the caption</summary>Figure 15: Limitations ‚Äì colliding captions. Our method may fail when handling cases of colliding identifiers, such as two dolls (a). This issue can be easily resolved by assigning distinct identifiers to each object during the initial training (b).</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2501.12224/x17.png alt></figure></p><blockquote><p>üîº This figure showcases the qualitative results of the TokenVerse model. Each row presents a set of four source images that exemplify distinct visual concepts (such as a doll, a dog, a man in various poses, lighting conditions, etc.). These concepts are independently extracted by TokenVerse from their respective images and are then seamlessly combined to generate novel image compositions. For each concept image set, the figure displays two generated images demonstrating the flexible and versatile nature of TokenVerse in creating novel image compositions that incorporate multiple visual concepts.</p><details><summary>read the caption</summary>Figure 16: Qualitative results. Each row contains two result images and the source images of the concepts that they contain.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2501.12224/x18.png alt></figure></p><blockquote><p>üîº This figure showcases qualitative results of the TokenVerse model. Each row presents four source images depicting various concepts (e.g., a doll in different poses, a dog with different accessories, a person in various settings, etc.). To the right of each set of source images, are two images generated by the model. These generated images demonstrate the model&rsquo;s ability to combine and personalize the extracted concepts in novel ways. The generated images show diverse compositions and attributes of the concepts, highlighting the model&rsquo;s flexibility in handling multiple concepts and its capacity for seamless integration of personalized elements from various sources.</p><details><summary>read the caption</summary>Figure 17: Qualitative results. Each row contains two result images and the source images of the concepts that they contain.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2501.12224/x19.png alt></figure></p><blockquote><p>üîº Figure 18 presents qualitative results showcasing TokenVerse&rsquo;s ability to generate images by combining concepts extracted from multiple source images. Each row displays four concept images followed by two generated images. The generated images demonstrate how TokenVerse successfully integrates the concepts from the source images into novel compositions. This visually demonstrates the model&rsquo;s ability to disentangle and recombine visual attributes (objects, poses, lighting conditions, etc.) from different sources seamlessly.</p><details><summary>read the caption</summary>Figure 18: Qualitative results. Each row contains two result images and the source images of the concepts that they contain.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2501.12224/x20.png alt></figure></p><blockquote><p>üîº This figure demonstrates the application of the TokenVerse method to storytelling. The left side displays the characters, scenes, and poses used in the story. The story itself, created by a large language model (LLM), is shown on the right. The LLM further processed the story to generate prompts for image generation using TokenVerse, resulting in the images shown in the figure.</p><details><summary>read the caption</summary>Figure 19: Storytelling results. Demonstration of our method‚Äôs usability for storytelling applications. All the characters, scenes, and poses featured in the story are shown on the left. On the right is the story itself, generated by a language model (LLM). This story was then reprocessed by the LLM to generate prompts, which were used to create the accompanying images.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2501.12224/x21.png alt></figure></p><blockquote><p>üîº This figure shows example questions from a user study conducted to evaluate the quality of generated images. Participants were shown a generated image and asked to rate how well it matched a given text description (prompt alignment) and how well it incorporated specific concepts from input images (concept preservation). Concept preservation was assessed separately for different visual elements present in the image.</p><details><summary>read the caption</summary>Figure 20: An example of the questions asked in the user study. Given a generated image the users are asked about its alignment with both the text and the input concepts</details></blockquote></details><details><summary>More on tables</summary><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id=A3.T2.14><thead class=ltx_thead><tr class=ltx_tr id=A3.T2.14.15.1><th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id=A3.T2.14.15.1.1></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan=3 id=A3.T2.14.15.1.2><span class="ltx_text ltx_font_bold" id=A3.T2.14.15.1.2.1 style=font-size:90%>Composition</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan=3 id=A3.T2.14.15.1.3><span class="ltx_text ltx_font_bold" id=A3.T2.14.15.1.3.1 style=font-size:90%>Decomposition</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan=3 id=A3.T2.14.15.1.4><span class="ltx_text ltx_font_bold" id=A3.T2.14.15.1.4.1 style=font-size:90%>Full task</span></th></tr><tr class=ltx_tr id=A3.T2.12.12><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id=A3.T2.12.12.13><span class="ltx_text ltx_font_bold" id=A3.T2.12.12.13.1 style=font-size:90%>Metric</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id=A3.T2.1.1.1><span class=ltx_text id=A3.T2.1.1.1.1 style=font-size:90%>CP</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="A3.T2.1.1.1.m1.1"><semantics id="A3.T2.1.1.1.m1.1a"><mo id="A3.T2.1.1.1.m1.1.1" mathsize="90%" stretchy="false" xref="A3.T2.1.1.1.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="A3.T2.1.1.1.m1.1b"><ci id="A3.T2.1.1.1.m1.1.1.cmml" xref="A3.T2.1.1.1.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T2.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A3.T2.1.1.1.m1.1d">‚Üë</annotation></semantics></math></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id=A3.T2.2.2.2><span class=ltx_text id=A3.T2.2.2.2.1 style=font-size:90%>PF</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="A3.T2.2.2.2.m1.1"><semantics id="A3.T2.2.2.2.m1.1a"><mo id="A3.T2.2.2.2.m1.1.1" mathsize="90%" stretchy="false" xref="A3.T2.2.2.2.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="A3.T2.2.2.2.m1.1b"><ci id="A3.T2.2.2.2.m1.1.1.cmml" xref="A3.T2.2.2.2.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T2.2.2.2.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A3.T2.2.2.2.m1.1d">‚Üë</annotation></semantics></math></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id=A3.T2.4.4.4><span class=ltx_text id=A3.T2.4.4.4.1 style=font-size:90%>CP </span><math alttext="\cdot" class="ltx_Math" display="inline" id="A3.T2.3.3.3.m1.1"><semantics id="A3.T2.3.3.3.m1.1a"><mo id="A3.T2.3.3.3.m1.1.1" mathsize="90%" xref="A3.T2.3.3.3.m1.1.1.cmml">‚ãÖ</mo><annotation-xml encoding="MathML-Content" id="A3.T2.3.3.3.m1.1b"><ci id="A3.T2.3.3.3.m1.1.1.cmml" xref="A3.T2.3.3.3.m1.1.1">‚ãÖ</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T2.3.3.3.m1.1c">\cdot</annotation><annotation encoding="application/x-llamapun" id="A3.T2.3.3.3.m1.1d">‚ãÖ</annotation></semantics></math><span class=ltx_text id=A3.T2.4.4.4.2 style=font-size:90%> PF</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="A3.T2.4.4.4.m2.1"><semantics id="A3.T2.4.4.4.m2.1a"><mo id="A3.T2.4.4.4.m2.1.1" mathsize="90%" stretchy="false" xref="A3.T2.4.4.4.m2.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="A3.T2.4.4.4.m2.1b"><ci id="A3.T2.4.4.4.m2.1.1.cmml" xref="A3.T2.4.4.4.m2.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T2.4.4.4.m2.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A3.T2.4.4.4.m2.1d">‚Üë</annotation></semantics></math></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id=A3.T2.5.5.5><span class=ltx_text id=A3.T2.5.5.5.1 style=font-size:90%>CP</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="A3.T2.5.5.5.m1.1"><semantics id="A3.T2.5.5.5.m1.1a"><mo id="A3.T2.5.5.5.m1.1.1" mathsize="90%" stretchy="false" xref="A3.T2.5.5.5.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="A3.T2.5.5.5.m1.1b"><ci id="A3.T2.5.5.5.m1.1.1.cmml" xref="A3.T2.5.5.5.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T2.5.5.5.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A3.T2.5.5.5.m1.1d">‚Üë</annotation></semantics></math></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id=A3.T2.6.6.6><span class=ltx_text id=A3.T2.6.6.6.1 style=font-size:90%>PF</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="A3.T2.6.6.6.m1.1"><semantics id="A3.T2.6.6.6.m1.1a"><mo id="A3.T2.6.6.6.m1.1.1" mathsize="90%" stretchy="false" xref="A3.T2.6.6.6.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="A3.T2.6.6.6.m1.1b"><ci id="A3.T2.6.6.6.m1.1.1.cmml" xref="A3.T2.6.6.6.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T2.6.6.6.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A3.T2.6.6.6.m1.1d">‚Üë</annotation></semantics></math></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id=A3.T2.8.8.8><span class=ltx_text id=A3.T2.8.8.8.1 style=font-size:90%>CP </span><math alttext="\cdot" class="ltx_Math" display="inline" id="A3.T2.7.7.7.m1.1"><semantics id="A3.T2.7.7.7.m1.1a"><mo id="A3.T2.7.7.7.m1.1.1" mathsize="90%" xref="A3.T2.7.7.7.m1.1.1.cmml">‚ãÖ</mo><annotation-xml encoding="MathML-Content" id="A3.T2.7.7.7.m1.1b"><ci id="A3.T2.7.7.7.m1.1.1.cmml" xref="A3.T2.7.7.7.m1.1.1">‚ãÖ</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T2.7.7.7.m1.1c">\cdot</annotation><annotation encoding="application/x-llamapun" id="A3.T2.7.7.7.m1.1d">‚ãÖ</annotation></semantics></math><span class=ltx_text id=A3.T2.8.8.8.2 style=font-size:90%> PF</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="A3.T2.8.8.8.m2.1"><semantics id="A3.T2.8.8.8.m2.1a"><mo id="A3.T2.8.8.8.m2.1.1" mathsize="90%" stretchy="false" xref="A3.T2.8.8.8.m2.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="A3.T2.8.8.8.m2.1b"><ci id="A3.T2.8.8.8.m2.1.1.cmml" xref="A3.T2.8.8.8.m2.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T2.8.8.8.m2.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A3.T2.8.8.8.m2.1d">‚Üë</annotation></semantics></math></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id=A3.T2.9.9.9><span class=ltx_text id=A3.T2.9.9.9.1 style=font-size:90%>CP</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="A3.T2.9.9.9.m1.1"><semantics id="A3.T2.9.9.9.m1.1a"><mo id="A3.T2.9.9.9.m1.1.1" mathsize="90%" stretchy="false" xref="A3.T2.9.9.9.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="A3.T2.9.9.9.m1.1b"><ci id="A3.T2.9.9.9.m1.1.1.cmml" xref="A3.T2.9.9.9.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T2.9.9.9.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A3.T2.9.9.9.m1.1d">‚Üë</annotation></semantics></math></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id=A3.T2.10.10.10><span class=ltx_text id=A3.T2.10.10.10.1 style=font-size:90%>PF</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="A3.T2.10.10.10.m1.1"><semantics id="A3.T2.10.10.10.m1.1a"><mo id="A3.T2.10.10.10.m1.1.1" mathsize="90%" stretchy="false" xref="A3.T2.10.10.10.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="A3.T2.10.10.10.m1.1b"><ci id="A3.T2.10.10.10.m1.1.1.cmml" xref="A3.T2.10.10.10.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T2.10.10.10.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A3.T2.10.10.10.m1.1d">‚Üë</annotation></semantics></math></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id=A3.T2.12.12.12><span class=ltx_text id=A3.T2.12.12.12.1 style=font-size:90%>CP </span><math alttext="\cdot" class="ltx_Math" display="inline" id="A3.T2.11.11.11.m1.1"><semantics id="A3.T2.11.11.11.m1.1a"><mo id="A3.T2.11.11.11.m1.1.1" mathsize="90%" xref="A3.T2.11.11.11.m1.1.1.cmml">‚ãÖ</mo><annotation-xml encoding="MathML-Content" id="A3.T2.11.11.11.m1.1b"><ci id="A3.T2.11.11.11.m1.1.1.cmml" xref="A3.T2.11.11.11.m1.1.1">‚ãÖ</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T2.11.11.11.m1.1c">\cdot</annotation><annotation encoding="application/x-llamapun" id="A3.T2.11.11.11.m1.1d">‚ãÖ</annotation></semantics></math><span class=ltx_text id=A3.T2.12.12.12.2 style=font-size:90%> PF</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="A3.T2.12.12.12.m2.1"><semantics id="A3.T2.12.12.12.m2.1a"><mo id="A3.T2.12.12.12.m2.1.1" mathsize="90%" stretchy="false" xref="A3.T2.12.12.12.m2.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="A3.T2.12.12.12.m2.1b"><ci id="A3.T2.12.12.12.m2.1.1.cmml" xref="A3.T2.12.12.12.m2.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T2.12.12.12.m2.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A3.T2.12.12.12.m2.1d">‚Üë</annotation></semantics></math></th></tr></thead><tbody class=ltx_tbody><tr class=ltx_tr id=A3.T2.14.16.1><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id=A3.T2.14.16.1.1><span class=ltx_text id=A3.T2.14.16.1.1.1 style=font-size:90%>Dreambooth</span></th><td class="ltx_td ltx_align_center ltx_border_t" id=A3.T2.14.16.1.2><span class=ltx_text id=A3.T2.14.16.1.2.1 style=font-size:90%>0.280242</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=A3.T2.14.16.1.3><span class="ltx_text ltx_font_bold" id=A3.T2.14.16.1.3.1 style=font-size:90%>0.844422</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=A3.T2.14.16.1.4><span class="ltx_text ltx_framed ltx_framed_underline" id=A3.T2.14.16.1.4.1 style=font-size:90%>0.236642</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=A3.T2.14.16.1.5><span class="ltx_text ltx_framed ltx_framed_underline" id=A3.T2.14.16.1.5.1 style=font-size:90%>0.668524</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=A3.T2.14.16.1.6><span class=ltx_text id=A3.T2.14.16.1.6.1 style=font-size:90%>0.660167</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=A3.T2.14.16.1.7><span class="ltx_text ltx_framed ltx_framed_underline" id=A3.T2.14.16.1.7.1 style=font-size:90%>0.441337</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=A3.T2.14.16.1.8><span class=ltx_text id=A3.T2.14.16.1.8.1 style=font-size:90%>0.207116</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=A3.T2.14.16.1.9><span class="ltx_text ltx_framed ltx_framed_underline" id=A3.T2.14.16.1.9.1 style=font-size:90%>0.827521</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=A3.T2.14.16.1.10><span class=ltx_text id=A3.T2.14.16.1.10.1 style=font-size:90%>0.171393</span></td></tr><tr class=ltx_tr id=A3.T2.13.13><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r" id=A3.T2.13.13.1><span class=ltx_text id=A3.T2.13.13.1.1 style=font-size:90%>DreamBooth</span><math alttext="{}_{\text{joint}}" class="ltx_Math" display="inline" id="A3.T2.13.13.1.m1.1"><semantics id="A3.T2.13.13.1.m1.1a"><msub id="A3.T2.13.13.1.m1.1.1" xref="A3.T2.13.13.1.m1.1.1.cmml"><mi id="A3.T2.13.13.1.m1.1.1a" xref="A3.T2.13.13.1.m1.1.1.cmml"></mi><mtext id="A3.T2.13.13.1.m1.1.1.1" mathsize="90%" xref="A3.T2.13.13.1.m1.1.1.1a.cmml">joint</mtext></msub><annotation-xml encoding="MathML-Content" id="A3.T2.13.13.1.m1.1b"><apply id="A3.T2.13.13.1.m1.1.1.cmml" xref="A3.T2.13.13.1.m1.1.1"><ci id="A3.T2.13.13.1.m1.1.1.1a.cmml" xref="A3.T2.13.13.1.m1.1.1.1"><mtext id="A3.T2.13.13.1.m1.1.1.1.cmml" mathsize="63%" xref="A3.T2.13.13.1.m1.1.1.1">joint</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.T2.13.13.1.m1.1c">{}_{\text{joint}}</annotation><annotation encoding="application/x-llamapun" id="A3.T2.13.13.1.m1.1d">start_FLOATSUBSCRIPT joint end_FLOATSUBSCRIPT</annotation></semantics></math></th><td class="ltx_td ltx_align_center" id=A3.T2.13.13.2><span class=ltx_text id=A3.T2.13.13.2.1 style=font-size:90%>0.371304</span></td><td class="ltx_td ltx_align_center" id=A3.T2.13.13.3><span class=ltx_text id=A3.T2.13.13.3.1 style=font-size:90%>0.619288</span></td><td class="ltx_td ltx_align_center ltx_border_r" id=A3.T2.13.13.4><span class=ltx_text id=A3.T2.13.13.4.1 style=font-size:90%>0.229944</span></td><td class="ltx_td ltx_align_center" id=A3.T2.13.13.5><span class="ltx_text ltx_framed ltx_framed_underline" id=A3.T2.13.13.5.1 style=font-size:90%>0.668524</span></td><td class="ltx_td ltx_align_center" id=A3.T2.13.13.6><span class=ltx_text id=A3.T2.13.13.6.1 style=font-size:90%>0.660167</span></td><td class="ltx_td ltx_align_center ltx_border_r" id=A3.T2.13.13.7><span class="ltx_text ltx_framed ltx_framed_underline" id=A3.T2.13.13.7.1 style=font-size:90%>0.441337</span></td><td class="ltx_td ltx_align_center" id=A3.T2.13.13.8><span class=ltx_text id=A3.T2.13.13.8.1 style=font-size:90%>0.306262</span></td><td class="ltx_td ltx_align_center" id=A3.T2.13.13.9><span class=ltx_text id=A3.T2.13.13.9.1 style=font-size:90%>0.591337</span></td><td class="ltx_td ltx_align_center ltx_border_r" id=A3.T2.13.13.10><span class=ltx_text id=A3.T2.13.13.10.1 style=font-size:90%>0.181104</span></td></tr><tr class=ltx_tr id=A3.T2.14.17.2><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r" id=A3.T2.14.17.2.1><span class=ltx_text id=A3.T2.14.17.2.1.1 style=font-size:90%>OMG</span></th><td class="ltx_td ltx_align_center" id=A3.T2.14.17.2.2><span class=ltx_text id=A3.T2.14.17.2.2.1 style=font-size:90%>0.238606</span></td><td class="ltx_td ltx_align_center" id=A3.T2.14.17.2.3><span class="ltx_text ltx_framed ltx_framed_underline" id=A3.T2.14.17.2.3.1 style=font-size:90%>0.743968</span></td><td class="ltx_td ltx_align_center ltx_border_r" id=A3.T2.14.17.2.4><span class=ltx_text id=A3.T2.14.17.2.4.1 style=font-size:90%>0.177515</span></td><td class="ltx_td ltx_align_center" id=A3.T2.14.17.2.5><span class=ltx_text id=A3.T2.14.17.2.5.1 style=font-size:90%>0.267477</span></td><td class="ltx_td ltx_align_center" id=A3.T2.14.17.2.6><span class=ltx_text id=A3.T2.14.17.2.6.1 style=font-size:90%>0.</span><span class="ltx_text ltx_font_bold" id=A3.T2.14.17.2.6.2 style=font-size:90%>791793</span></td><td class="ltx_td ltx_align_center ltx_border_r" id=A3.T2.14.17.2.7><span class=ltx_text id=A3.T2.14.17.2.7.1 style=font-size:90%>0.211787</span></td><td class="ltx_td ltx_align_center" id=A3.T2.14.17.2.8><span class=ltx_text id=A3.T2.14.17.2.8.1 style=font-size:90%>0.207787</span></td><td class="ltx_td ltx_align_center" id=A3.T2.14.17.2.9><span class="ltx_text ltx_font_bold" id=A3.T2.14.17.2.9.1 style=font-size:90%>0.843395</span></td><td class="ltx_td ltx_align_center ltx_border_r" id=A3.T2.14.17.2.10><span class=ltx_text id=A3.T2.14.17.2.10.1 style=font-size:90%>0.175246</span></td></tr><tr class=ltx_tr id=A3.T2.14.14><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r" id=A3.T2.14.14.1><span class=ltx_text id=A3.T2.14.14.1.1 style=font-size:90%>Break-A-Scene</span><math alttext="{}_{\text{joint}}" class="ltx_Math" display="inline" id="A3.T2.14.14.1.m1.1"><semantics id="A3.T2.14.14.1.m1.1a"><msub id="A3.T2.14.14.1.m1.1.1" xref="A3.T2.14.14.1.m1.1.1.cmml"><mi id="A3.T2.14.14.1.m1.1.1a" xref="A3.T2.14.14.1.m1.1.1.cmml"></mi><mtext id="A3.T2.14.14.1.m1.1.1.1" mathsize="90%" xref="A3.T2.14.14.1.m1.1.1.1a.cmml">joint</mtext></msub><annotation-xml encoding="MathML-Content" id="A3.T2.14.14.1.m1.1b"><apply id="A3.T2.14.14.1.m1.1.1.cmml" xref="A3.T2.14.14.1.m1.1.1"><ci id="A3.T2.14.14.1.m1.1.1.1a.cmml" xref="A3.T2.14.14.1.m1.1.1.1"><mtext id="A3.T2.14.14.1.m1.1.1.1.cmml" mathsize="63%" xref="A3.T2.14.14.1.m1.1.1.1">joint</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.T2.14.14.1.m1.1c">{}_{\text{joint}}</annotation><annotation encoding="application/x-llamapun" id="A3.T2.14.14.1.m1.1d">start_FLOATSUBSCRIPT joint end_FLOATSUBSCRIPT</annotation></semantics></math></th><td class="ltx_td ltx_align_center" id=A3.T2.14.14.2><span class="ltx_text ltx_framed ltx_framed_underline" id=A3.T2.14.14.2.1 style=font-size:90%>0.392912</span></td><td class="ltx_td ltx_align_center" id=A3.T2.14.14.3><span class=ltx_text id=A3.T2.14.14.3.1 style=font-size:90%>0.372664</span></td><td class="ltx_td ltx_align_center ltx_border_r" id=A3.T2.14.14.4><span class=ltx_text id=A3.T2.14.14.4.1 style=font-size:90%>0.154380</span></td><td class="ltx_td ltx_align_center" id=A3.T2.14.14.5><span class=ltx_text id=A3.T2.14.14.5.1 style=font-size:90%>0.598387</span></td><td class="ltx_td ltx_align_center" id=A3.T2.14.14.6><span class=ltx_text id=A3.T2.14.14.6.1 style=font-size:90%>0.641935</span></td><td class="ltx_td ltx_align_center ltx_border_r" id=A3.T2.14.14.7><span class=ltx_text id=A3.T2.14.14.7.1 style=font-size:90%>0.384126</span></td><td class="ltx_td ltx_align_center" id=A3.T2.14.14.8><span class="ltx_text ltx_framed ltx_framed_underline" id=A3.T2.14.14.8.1 style=font-size:90%>0.499054</span></td><td class="ltx_td ltx_align_center" id=A3.T2.14.14.9><span class=ltx_text id=A3.T2.14.14.9.1 style=font-size:90%>0.641236</span></td><td class="ltx_td ltx_align_center ltx_border_r" id=A3.T2.14.14.10><span class="ltx_text ltx_framed ltx_framed_underline" id=A3.T2.14.14.10.1 style=font-size:90%>0.320011</span></td></tr><tr class=ltx_tr id=A3.T2.14.18.3><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r" id=A3.T2.14.18.3.1><span class=ltx_text id=A3.T2.14.18.3.1.1 style=font-size:90%>ConceptExpress</span></th><td class="ltx_td ltx_align_center" id=A3.T2.14.18.3.2><span class=ltx_text id=A3.T2.14.18.3.2.1 style=font-size:90%>0.214718</span></td><td class="ltx_td ltx_align_center" id=A3.T2.14.18.3.3><span class=ltx_text id=A3.T2.14.18.3.3.1 style=font-size:90%>0.548723</span></td><td class="ltx_td ltx_align_center ltx_border_r" id=A3.T2.14.18.3.4><span class=ltx_text id=A3.T2.14.18.3.4.1 style=font-size:90%>0.117821</span></td><td class="ltx_td ltx_align_center" id=A3.T2.14.18.3.5><span class=ltx_text id=A3.T2.14.18.3.5.1 style=font-size:90%>0.246387</span></td><td class="ltx_td ltx_align_center" id=A3.T2.14.18.3.6><span class=ltx_text id=A3.T2.14.18.3.6.1 style=font-size:90%>0.695087</span></td><td class="ltx_td ltx_align_center ltx_border_r" id=A3.T2.14.18.3.7><span class=ltx_text id=A3.T2.14.18.3.7.1 style=font-size:90%>0.171260</span></td><td class="ltx_td ltx_align_center" id=A3.T2.14.18.3.8><span class=ltx_text id=A3.T2.14.18.3.8.1 style=font-size:90%>0.187853</span></td><td class="ltx_td ltx_align_center" id=A3.T2.14.18.3.9><span class=ltx_text id=A3.T2.14.18.3.9.1 style=font-size:90%>0.733286</span></td><td class="ltx_td ltx_align_center ltx_border_r" id=A3.T2.14.18.3.10><span class=ltx_text id=A3.T2.14.18.3.10.1 style=font-size:90%>0.137750</span></td></tr><tr class=ltx_tr id=A3.T2.14.19.4><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id=A3.T2.14.19.4.1><span class=ltx_text id=A3.T2.14.19.4.1.1 style=font-size:90%>Ours</span></th><td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id=A3.T2.14.19.4.2><span class="ltx_text ltx_font_bold" id=A3.T2.14.19.4.2.1 style=font-size:90%>0.470108</span></td><td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id=A3.T2.14.19.4.3><span class=ltx_text id=A3.T2.14.19.4.3.1 style=font-size:90%>0.688061</span></td><td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id=A3.T2.14.19.4.4><span class="ltx_text ltx_font_bold" id=A3.T2.14.19.4.4.1 style=font-size:90%>0.323463</span></td><td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id=A3.T2.14.19.4.5><span class="ltx_text ltx_font_bold" id=A3.T2.14.19.4.5.1 style=font-size:90%>0.669940</span></td><td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id=A3.T2.14.19.4.6><span class="ltx_text ltx_framed ltx_framed_underline" id=A3.T2.14.19.4.6.1 style=font-size:90%>0.747698</span></td><td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id=A3.T2.14.19.4.7><span class="ltx_text ltx_font_bold" id=A3.T2.14.19.4.7.1 style=font-size:90%>0.500431</span></td><td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id=A3.T2.14.19.4.8><span class="ltx_text ltx_font_bold" id=A3.T2.14.19.4.8.1 style=font-size:90%>0.553125</span></td><td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id=A3.T2.14.19.4.9><span class=ltx_text id=A3.T2.14.19.4.9.1 style=font-size:90%>0.821875</span></td><td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id=A3.T2.14.19.4.10><span class="ltx_text ltx_font_bold" id=A3.T2.14.19.4.10.1 style=font-size:90%>0.454600</span></td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents a quantitative comparison of different methods for image generation, focusing on two key metrics: concept preservation (CP) and prompt fidelity (PF). Concept preservation measures how well the generated image retains the specific features of the target concept, while prompt fidelity assesses how accurately the image matches the text prompt. The table provides precise numerical values for CP and PF, allowing for a detailed comparison across multiple approaches and evaluation scenarios (composition, decomposition, and a combined full task). These results supplement the visual comparisons shown in the main paper.</p><details><summary>read the caption</summary>Table 2: Dreambench++ Evaluation We complement the quantitative comparison graphs of the main paper with the exact measurements of concept preservation (CP) and prompt fidelity (PF).</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id=A3.T3.4><thead class=ltx_thead><tr class=ltx_tr id=A3.T3.2.2><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id=A3.T3.2.2.3><span class="ltx_text ltx_font_bold" id=A3.T3.2.2.3.1>Metric</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id=A3.T3.1.1.1>CP<math alttext="\uparrow" class="ltx_Math" display="inline" id="A3.T3.1.1.1.m1.1"><semantics id="A3.T3.1.1.1.m1.1a"><mo id="A3.T3.1.1.1.m1.1.1" stretchy="false" xref="A3.T3.1.1.1.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="A3.T3.1.1.1.m1.1b"><ci id="A3.T3.1.1.1.m1.1.1.cmml" xref="A3.T3.1.1.1.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T3.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A3.T3.1.1.1.m1.1d">‚Üë</annotation></semantics></math></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id=A3.T3.2.2.2>PF<math alttext="\uparrow" class="ltx_Math" display="inline" id="A3.T3.2.2.2.m1.1"><semantics id="A3.T3.2.2.2.m1.1a"><mo id="A3.T3.2.2.2.m1.1.1" stretchy="false" xref="A3.T3.2.2.2.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="A3.T3.2.2.2.m1.1b"><ci id="A3.T3.2.2.2.m1.1.1.cmml" xref="A3.T3.2.2.2.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T3.2.2.2.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A3.T3.2.2.2.m1.1d">‚Üë</annotation></semantics></math></th></tr></thead><tbody class=ltx_tbody><tr class=ltx_tr id=A3.T3.4.5.1><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id=A3.T3.4.5.1.1>Dreambooth</th><td class="ltx_td ltx_align_center ltx_border_t" id=A3.T3.4.5.1.2>2.2505</td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=A3.T3.4.5.1.3><span class="ltx_text ltx_font_bold" id=A3.T3.4.5.1.3.1>4.465</span></td></tr><tr class=ltx_tr id=A3.T3.3.3><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r" id=A3.T3.3.3.1>DreamBooth<math alttext="{}_{\text{joint}}" class="ltx_Math" display="inline" id="A3.T3.3.3.1.m1.1"><semantics id="A3.T3.3.3.1.m1.1a"><msub id="A3.T3.3.3.1.m1.1.1" xref="A3.T3.3.3.1.m1.1.1.cmml"><mi id="A3.T3.3.3.1.m1.1.1a" xref="A3.T3.3.3.1.m1.1.1.cmml"></mi><mtext id="A3.T3.3.3.1.m1.1.1.1" xref="A3.T3.3.3.1.m1.1.1.1a.cmml">joint</mtext></msub><annotation-xml encoding="MathML-Content" id="A3.T3.3.3.1.m1.1b"><apply id="A3.T3.3.3.1.m1.1.1.cmml" xref="A3.T3.3.3.1.m1.1.1"><ci id="A3.T3.3.3.1.m1.1.1.1a.cmml" xref="A3.T3.3.3.1.m1.1.1.1"><mtext id="A3.T3.3.3.1.m1.1.1.1.cmml" mathsize="70%" xref="A3.T3.3.3.1.m1.1.1.1">joint</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.T3.3.3.1.m1.1c">{}_{\text{joint}}</annotation><annotation encoding="application/x-llamapun" id="A3.T3.3.3.1.m1.1d">start_FLOATSUBSCRIPT joint end_FLOATSUBSCRIPT</annotation></semantics></math></th><td class="ltx_td ltx_align_center" id=A3.T3.3.3.2>2.7582</td><td class="ltx_td ltx_align_center ltx_border_r" id=A3.T3.3.3.3>3.462</td></tr><tr class=ltx_tr id=A3.T3.4.6.2><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r" id=A3.T3.4.6.2.1>OMG</th><td class="ltx_td ltx_align_center" id=A3.T3.4.6.2.2>2.205</td><td class="ltx_td ltx_align_center ltx_border_r" id=A3.T3.4.6.2.3>3.611</td></tr><tr class=ltx_tr id=A3.T3.4.4><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r" id=A3.T3.4.4.1>Break-A-Scene<math alttext="{}_{\text{joint}}" class="ltx_Math" display="inline" id="A3.T3.4.4.1.m1.1"><semantics id="A3.T3.4.4.1.m1.1a"><msub id="A3.T3.4.4.1.m1.1.1" xref="A3.T3.4.4.1.m1.1.1.cmml"><mi id="A3.T3.4.4.1.m1.1.1a" xref="A3.T3.4.4.1.m1.1.1.cmml"></mi><mtext id="A3.T3.4.4.1.m1.1.1.1" xref="A3.T3.4.4.1.m1.1.1.1a.cmml">joint</mtext></msub><annotation-xml encoding="MathML-Content" id="A3.T3.4.4.1.m1.1b"><apply id="A3.T3.4.4.1.m1.1.1.cmml" xref="A3.T3.4.4.1.m1.1.1"><ci id="A3.T3.4.4.1.m1.1.1.1a.cmml" xref="A3.T3.4.4.1.m1.1.1.1"><mtext id="A3.T3.4.4.1.m1.1.1.1.cmml" mathsize="70%" xref="A3.T3.4.4.1.m1.1.1.1">joint</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.T3.4.4.1.m1.1c">{}_{\text{joint}}</annotation><annotation encoding="application/x-llamapun" id="A3.T3.4.4.1.m1.1d">start_FLOATSUBSCRIPT joint end_FLOATSUBSCRIPT</annotation></semantics></math></th><td class="ltx_td ltx_align_center" id=A3.T3.4.4.2><span class="ltx_text ltx_framed ltx_framed_underline" id=A3.T3.4.4.2.1>3.203</span></td><td class="ltx_td ltx_align_center ltx_border_r" id=A3.T3.4.4.3>3.151</td></tr><tr class=ltx_tr id=A3.T3.4.7.3><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r" id=A3.T3.4.7.3.1>ConceptExpress</th><td class="ltx_td ltx_align_center" id=A3.T3.4.7.3.2>2.211</td><td class="ltx_td ltx_align_center ltx_border_r" id=A3.T3.4.7.3.3>2.686</td></tr><tr class=ltx_tr id=A3.T3.4.8.4><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id=A3.T3.4.8.4.1>Ours</th><td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id=A3.T3.4.8.4.2><span class="ltx_text ltx_font_bold" id=A3.T3.4.8.4.2.1>4.078</span></td><td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id=A3.T3.4.8.4.3><span class="ltx_text ltx_framed ltx_framed_underline" id=A3.T3.4.8.4.3.1>4.292</span></td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents the quantitative results of a user study evaluating the performance of TokenVerse. It complements the graphical representations in the main paper by providing the precise numerical scores for concept preservation and prompt fidelity, metrics used to assess how well the generated images retain the original concepts and adhere to the text prompts, respectively. The results are presented for different methods, allowing for direct comparison of TokenVerse against other approaches.</p><details><summary>read the caption</summary>Table 3: User Study We complement the user study results graphs of the main paper with the exact measurements of concept preservation and prompt fidelity.</details></blockquote></details><h3 class="relative group">Full paper<div id=full-paper class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#full-paper aria-label=Anchor>#</a></span></h3><div id=gallery-948dc05439c3bc8709c4da2da96894a0 class=gallery><img src=https://ai-paper-reviewer.com/2501.12224/1.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.12224/2.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.12224/3.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.12224/4.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.12224/5.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.12224/6.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.12224/7.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.12224/8.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.12224/9.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.12224/10.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.12224/11.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.12224/12.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.12224/13.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.12224/14.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.12224/15.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.12224/16.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.12224/17.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.12224/18.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.12224/19.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.12224/20.png class="grid-w50 md:grid-w33 xl:grid-w25"></div></div><section class="flex flex-row flex-wrap justify-center pt-4 text-xl"><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.12224/&amp;title=TokenVerse:%20Versatile%20Multi-concept%20Personalization%20in%20Token%20Modulation%20Space" title="Share on LinkedIn" aria-label="Share on LinkedIn"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://twitter.com/intent/tweet/?url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.12224/&amp;text=TokenVerse:%20Versatile%20Multi-concept%20Personalization%20in%20Token%20Modulation%20Space" title="Tweet on Twitter" aria-label="Tweet on Twitter"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="mailto:?body=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.12224/&amp;subject=TokenVerse:%20Versatile%20Multi-concept%20Personalization%20in%20Token%20Modulation%20Space" title="Send via email" aria-label="Send via email"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1-27.64 140.9 68.65 266.2 199.1 285.1 19.01 2.888 36.17-12.26 36.17-31.49l1e-4-.6631c0-15.74-11.44-28.88-26.84-31.24-84.35-12.98-149.2-86.13-149.2-174.2.0-102.9 88.61-185.5 193.4-175.4 91.54 8.869 158.6 91.25 158.6 183.2v16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98.0036c-7.299.0-13.2 4.992-15.12 11.68-24.85-12.15-54.24-16.38-86.06-5.106-38.75 13.73-68.12 48.91-73.72 89.64-9.483 69.01 43.81 128 110.9 128 26.44.0 50.43-9.544 69.59-24.88 24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3c0-149.2-133.9-265.632-287.3-235.57zM239.1 304.3c-26.47.0-48-21.56-48-48.05s21.53-48.05 48-48.05 48 21.56 48 48.05-20.6 48.05-48 48.05z"/></svg></span></a></section></div><script>var oid="views_paper-reviews/2501.12224/index.md",oid_likes="likes_paper-reviews/2501.12224/index.md"</script><script type=text/javascript src=/ai-paper-reviewer/js/page.min.0860cf4e04fa2d72cc33ddba263083464d48f67de06114529043cb4623319efed4f484fd7f1730df5abea0e2da6f3538855634081d02f2d6e920b956f063e823.js integrity="sha512-CGDPTgT6LXLMM926JjCDRk1I9n3gYRRSkEPLRiMxnv7U9IT9fxcw31q+oOLabzU4hVY0CB0C8tbpILlW8GPoIw=="></script></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="flex group mr-3" href=/ai-paper-reviewer/paper-reviews/2501.12326/><span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&larr;</span>
<span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">UI-TARS: Pioneering Automated GUI Interaction with Native Agents</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2025-01-21T00:00:00+00:00>21 January 2025</time>
</span></span></a></span><span><a class="flex text-right group ml-3" href=/ai-paper-reviewer/paper-reviews/2501.12380/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">MMVU: Measuring Expert-Level Multi-Discipline Video Understanding</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2025-01-21T00:00:00+00:00>21 January 2025</time>
</span></span><span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&larr;</span></a></span></div></div><div class=pt-3><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class=pt-3><script src=https://utteranc.es/client.js repo=pmnxis/pmnxis.github.io issue-term=pathname label=Comment theme=dark-blue crossorigin=anonymous async></script></div></div></footer></article><div id=top-scroller class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer id=site-footer class="py-10 print:hidden"><nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400"><ul class="flex flex-col list-none sm:flex-row"><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=/ai-paper-reviewer/tags/ title=Tags>Tags</a></li><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=https://deep-diver.github.io/neurips2024/ title>NeurIPS2024</a></li></ul></nav><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2025
Hugging Face Daily Papers</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a></p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/ai-paper-reviewer/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh+sCQ0E53ghYrxgYqw+0GCRyIEpA=="></script></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://deep-diver.github.io/ai-paper-reviewer/ style=z-index:500><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body><script data-name=BMC-Widget data-cfasync=false src=https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js data-id=chansung data-description="Support me on Buy me a coffee!" data-message data-color=#FFDD00 data-position=Left data-x_margin=18 data-y_margin=18></script></html>