[{"content": "| Variant | Inference (Test) | Exploration | Learn | Remark |\n|---|---|---|---|---|\n| 1 | \u03c0<sub>\u03b8</sub> | passive | \u03c0<sub>\u03b8</sub> | Online DAP (Guo et al., 2024) |\n| 2 | \u03c0<sub>\u03b8</sub> | active | (\u03c0<sub>\u03b8</sub>,\u211b) | **SEA** without ERM sync (Section 4.2.3) |\n| 3 | \u03c0<sub>\u03b8</sub> | active | (\u03c0<sub>\u03b8</sub>\u2194\u211b) | **SEA** |\n| 4 | BoN(\u03c0<sub>\u03b8</sub>,\u211b) | passive | (\u03c0<sub>\u03b8</sub>,\u211b) | - |\n| 5 | BoN(\u03c0<sub>\u03b8</sub>,\u211b) | active | (\u03c0<sub>\u03b8</sub>,\u211b) | - |\n| 6 | BoN(\u03c0<sub>\u03b8</sub>,\u211b) | active | (\u03c0<sub>\u03b8</sub>\u2194\u211b) | **SEA** with Best-of-N sampling |\n| 7 | BoN(\u03c0<sub>ref</sub>,\u211b) | active | \u211b | Not learn policy (Dwaracherla et al., 2024) |", "caption": "Table 1: Decomposition of different driving factors of online active alignment algorithms.", "description": "This table breaks down the key components contributing to the effectiveness of different online active alignment algorithms.  It analyzes three main factors: the method used for inference (testing), the type of exploration strategy employed, and the learning mechanism used. By varying these factors, the table demonstrates the individual and combined impact of each component on the overall performance of the algorithm.  This allows for a more nuanced understanding of how different design choices affect the sample efficiency and alignment quality.", "section": "6.2 ABLATION ANALYSIS"}, {"content": "| RL Optimizer | Method | Exploration (Active) | Exploration (Passive) | Interaction (Online) | Interaction (Iterative) | Interaction (Offline) | Proposal Policy ($\\\\;\\pi_\\theta$) | Proposal Policy ($\\\\;\\pi_\\beta$) |\n|---|---|---|---|---|---|---|---|---|\n| RL Optimizer | Christiano et al. (2017) |  | \u2713 |  | \u2713 | \u2713 | \u2713 |  |\n|  | Stiennon et al. (2020) |  | \u2713 |  | \u2713 | \u2713 | \u2713 |  |\n|  | Bai et al. (2022) |  | \u2713 |  | \u2713 | \u2713 | \u2713 |  |\n|  | Ouyang et al. (2022) |  | \u2713 |  | \u2713 | \u2713 | \u2713 |  |\n| Direct Optimizer | Zhao et al. (2023) |  | \u2713 |  |  | \u2713 | \u2713 |  |\n|  | Rafailov et al. (2023) |  | \u2713 |  |  | \u2713 | \u2713 |  |\n|  | Azar et al. (2024) |  | \u2713 |  |  | \u2713 | \u2713 |  |\n|  | Meng et al. (2024) |  | \u2713 |  |  | \u2713 | \u2713 |  |\n|  | Xu et al. (2023) |  | \u2713 |  | \u2713 |  | \u2713 |  |\n|  | Guo et al. (2024) |  | \u2713 | \u2713 |  |  | \u2713 |  |\n|  | Mehta et al. (2023) | \u2713 |  | \u2713 |  |  |  | \u2713 |\n|  | Das et al. (2024) | \u2713 |  | \u2713 |  |  |  | \u2713 |\n|  | Melo et al. (2024) | \u2713 |  | \u2713 |  |  |  | \u2713 |\n|  | Dwaracherla et al. (2024) | \u2713 |  | \u2713 |  |  |  | \u2713 |\n|  | Zhang et al. (2024a) | \u2713 |  | \u2713 |  |  | \u2713 |  |\n|  | Xie et al. (2024) | \u2713 |  | \u2713 |  |  | \u2713 |  |\n|  | Muldrew et al. (2024) | \u2713 |  | \u2713 |  |  | \u2713 |  |", "caption": "Table 2: A summary of prior work. \u03c0\u03b8subscript\ud835\udf0b\ud835\udf03\\pi_{\\theta}italic_\u03c0 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT denotes the proposal policy that is continuously updated based on newly collected preference data, while \u03c0\u03b2subscript\ud835\udf0b\ud835\udefd\\pi_{\\beta}italic_\u03c0 start_POSTSUBSCRIPT italic_\u03b2 end_POSTSUBSCRIPT denotes a fixed proposal policy. Algorithms that encompass online interaction (Property\u00a01), active exploration (Property\u00a02), and learnable \u03c0\u03b8subscript\ud835\udf0b\ud835\udf03\\pi_{\\theta}italic_\u03c0 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT offer the best sample efficiency. Notably, only three methods (listed at the bottom of the table) satisfy these characteristics, and we include them for comparisons in our experiments.", "description": "This table compares various large language model (LLM) alignment methods based on two key properties: online interaction and active exploration.  Online interaction refers to methods that continuously learn and update their policies using real-time feedback, while active exploration refers to methods that strategically select data to gather the most informative feedback and improve their efficiency.  The table shows which methods satisfy each property, and whether they use a continuously updated policy (\u03c0\u03b8) or a fixed policy (\u03c0\u03b2).  It highlights that only three methods at the bottom satisfy both online interaction and active exploration criteria.  These three methods are chosen for detailed comparison in the paper's experiments.", "section": "Supplementary Materials"}]