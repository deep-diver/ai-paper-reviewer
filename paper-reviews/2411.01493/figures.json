[{"figure_path": "https://arxiv.org/html/2411.01493/x3.png", "caption": "Figure 1: Win rate comparison of model responses against reference responses on the TL;DR task, judged by the preference oracle. All compared methods use the same optimization method (DPO).\n(Left) Performance improvements at convergence over SFT models achieved by offline (Offline DPO), passively online (Online DPO), and our active exploration (SEA DPO) methods. (Right) The number of queries required by the passively online method (Passive) versus that by different active exploration methods to attain various levels of win rates. SEA\u00a0achieves the best sample efficiency for online alignment compared to XPO and APL.", "description": "Figure 1 presents a comparison of Large Language Model (LLM) response quality using different training methods.  The task is TL;DR summarization, and success is judged by comparing the model's output to a reference summary.  The left panel shows the performance improvement achieved by three methods compared to a baseline (Supervised Fine-Tuning, or SFT).  'Offline DPO' represents a method that trains entirely on a fixed dataset, while 'Online DPO' updates continuously but passively incorporates new data.  'SEA DPO' incorporates active exploration, strategically selecting data that improves performance the most efficiently. The results demonstrate that SEA DPO significantly outperforms both Offline and Online DPO. The right panel shows the sample efficiency of the different methods. Sample efficiency refers to the number of queries required to achieve a given level of performance. This panel demonstrates SEA's superior sample efficiency, requiring fewer queries to achieve the same performance as other active methods, such as XPO and APL.", "section": "6 EMPIRICAL RESULTS"}, {"figure_path": "https://arxiv.org/html/2411.01493/x4.png", "caption": "Figure 2: Illustrative comparison between CDB and LLM alignment.", "description": "The figure illustrates the analogous relationship between contextual dueling bandits (CDB) and LLM alignment.  The CDB framework involves an agent interacting with an environment, receiving feedback (in the form of pairwise comparisons), and learning to select optimal actions.  The LLM alignment interface mirrors this, with the LLM acting as the agent, humans providing preference feedback on generated text responses, and the LLM's policy being updated to better align with human preferences.  The diagram highlights the parallel structure of both problems, demonstrating how the theoretical framework of CDB can be applied to the practical problem of LLM alignment.", "section": "2 LLM ALIGNMENT AS CONTEXTUAL DUELING BANDITS"}, {"figure_path": "https://arxiv.org/html/2411.01493/x5.png", "caption": "Figure 3: Different paradigms for solving the LLM alignment problem in the CDB framework. Note that although all paradigms follow the LLM alignment interface (Figure\u00a02) with the interaction loop, some are actually offline or iteratively online (i.e., loop only once or a few times). Detailed comparisons will be made in Section\u00a03. We use colors to denote learnable components, RL optimizer, direct optimizer, and active exploration. r\u03d5subscript\ud835\udc5fitalic-\u03d5r_{\\phi}italic_r start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT denotes a point estimate of human\u2019s implicit reward, while \u211b\u03a6subscript\u211b\u03a6{\\mathcal{R}}_{\\Phi}caligraphic_R start_POSTSUBSCRIPT roman_\u03a6 end_POSTSUBSCRIPT refers to an uncertainty-aware reward model.", "description": "This figure illustrates four different approaches to aligning large language models (LLMs) with human preferences.  The approaches are categorized within the Contextual Dueling Bandit (CDB) framework.  Each approach is represented diagrammatically, showing the interaction between the LLM agent, the human, and the data flow.  The key differences lie in how they collect and utilize feedback for learning.  Some methods are purely offline or iterative (performing the interaction loop only a few times). Others operate fully online, learning continuously from new interactions. The figure highlights the different components of each approach: the learnable parameters (model weights), the optimization method (reinforcement learning or direct optimization), and whether active exploration is used to maximize learning efficiency.  The color-coding aids in distinguishing these components.  Specifically,  $r_\\phi$ represents a point estimate of the human's implicit reward, while $\\mathcal{R}_\\Phi$ is an uncertainty-aware reward model.", "section": "How Prior Works (Partially) Solve LLM Alignment as CDB"}, {"figure_path": "https://arxiv.org/html/2411.01493/x6.png", "caption": "Figure 4: The learning system for experimenting online LLM alignment algorithms.", "description": "This figure illustrates the distributed learning system designed for online LLM alignment experiments. The system consists of three main components: Actors, Learner, and Oracle.  Actors generate multiple LLM responses concurrently for a given prompt. The Learner updates the LLM parameters using feedback from the Oracle. The Oracle judges the quality of the LLM's generated responses by comparing them against references and provides feedback to the Learner. This system is designed to accelerate online LLM alignment research by enabling efficient experimentation with various online alignment algorithms.", "section": "Experimental Setup"}, {"figure_path": "https://arxiv.org/html/2411.01493/x7.png", "caption": "Figure 5: Win rate comparison of different algorithms against their initial SFT models across three scales and three direct optimizers.", "description": "This figure displays the results of a comparative study evaluating various LLM alignment algorithms across different model sizes (1B, 2.8B, and 6.9B parameters) and three optimization methods (DPO, IPO, and SLiC).  The win rate, representing the percentage of times the model's response was preferred over its initial SFT (Supervised Fine-Tuning) version by a human oracle, is plotted against the number of queries made to the oracle. This illustrates the sample efficiency of each algorithm in achieving alignment with human preferences. The figure allows for a comparison of different methods' performance, showing how quickly and effectively each achieves high win rates across varying model scales and optimization techniques.", "section": "6 EMPIRICAL RESULTS"}, {"figure_path": "https://arxiv.org/html/2411.01493/x8.png", "caption": "Figure 6: Win rate comparison of different agent variants when using (Left) policy and (Right) Best-of-N sampling for inference.", "description": "This figure displays the win rates of different agent variants across various query steps.  The left panel showcases results when the agent utilizes its learned policy for inference, directly using the policy output to select responses. The right panel demonstrates the results when using Best-of-N sampling for inference, where the algorithm samples N responses from the policy and selects the best one according to a given criteria. The different agent variants are created by changing components such as inference methods, exploration strategies, and learning components,  allowing for analysis of the impact of each on performance.", "section": "6.2 ABLATION ANALYSIS"}, {"figure_path": "https://arxiv.org/html/2411.01493/x9.png", "caption": "Figure 7: (Left and Middle) Win rate comparison of different exploration strategies measured in E&E and BAI settings. (Right) Win rate comparison of different agents when using GPT4o-mini to simulate human feedback via LLM-as-a-judge.", "description": "Figure 7 presents a comparison of different exploration strategies within the context of online LLM alignment. The left and middle panels display the win rates achieved by three exploration strategies (Uncertainty, E&E-TS, BAI-TS) in both explore-exploit (E&E) and best-arm identification (BAI) settings, respectively.  The right panel shows a comparison of win rates when a GPT4-mini model is used to simulate human feedback in the alignment process.  The results highlight how different exploration approaches perform under various learning objectives and feedback mechanisms.", "section": "6.3 Choice of Exploration Strategies"}, {"figure_path": "https://arxiv.org/html/2411.01493/x10.png", "caption": "Figure 8: Two example configurations of oat used in benchmarking experiments.", "description": "This figure illustrates two different configurations used in the experimental setup to benchmark the efficiency of the online DPO training. Config 1 shows a full collocation approach where all the workloads (actor, learner, oracle) are fully collocated on all available GPUs. This maximizes GPU utilization but demands high GPU memory.  Config 2 demonstrates a half collocation strategy where actors and oracles are collocated on half of the GPUs while the learner utilizes the other half.  This approach reduces memory pressure but introduces data dependency and potential idle time due to asynchronous updates.", "section": "Experimental Setup"}, {"figure_path": "https://arxiv.org/html/2411.01493/extracted/5969199/assets/chat-gpt-online-feedback.jpg", "caption": "Figure 9: Averaged training latency (over 10 batches, equivalent to 1280 samples) comparing sail-sg/oat against huggingface/trl.", "description": "Figure 9 presents a bar chart comparing the training latency of the online DPO algorithm using two different systems:  `sail-sg/oat` and `huggingface/trl`.  The latency is averaged over 10 batches (which equates to 1280 samples in total).  The chart breaks down the latency for three different parts of the training process: response generation, oracle calls (reward calculations), and the learner update step.  The comparison highlights that `sail-sg/oat` achieves significantly lower latency across different model scales (1B, 2.8B, and 6.9B parameters) and system configurations.", "section": "Experimental Setup"}]