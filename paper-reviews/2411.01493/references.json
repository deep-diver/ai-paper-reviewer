{"references": [{"fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-12-01", "reason": "This paper is foundational to RLHF, a central topic in the main paper, and introduces a crucial method for aligning LLMs with human preferences."}, {"fullname_first_author": "Paul F Christiano", "paper_title": "Deep reinforcement learning from human preferences", "publication_date": "2017-12-01", "reason": "This paper is a seminal work in reinforcement learning from human feedback, which is a core methodology explored and improved upon in the main paper."}, {"fullname_first_author": "Rafael Rafailov", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "publication_date": "2023-12-01", "reason": "This paper introduces a significant technique called Direct Preference Optimization (DPO), which is directly compared and contrasted with the proposed method in the main paper."}, {"fullname_first_author": "Miroslav Dud\u00edk", "paper_title": "Contextual dueling bandits", "publication_date": "2015-12-01", "reason": "This paper lays out the theoretical framework of contextual dueling bandits, which serves as the mathematical foundation for formulating the LLM alignment problem in the main paper."}, {"fullname_first_author": "William Thompson", "paper_title": "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples", "publication_date": "1933-12-01", "reason": "This paper introduces Thompson sampling, a core algorithm used in the proposed sample-efficient alignment method, providing the theoretical underpinning for the approach."}]}