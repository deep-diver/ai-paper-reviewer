{"importance": "This paper is important because it offers a simple yet effective solution to improve large language models' reasoning abilities.  **It challenges the common practice of prioritizing speed over accuracy in LLM inference, demonstrating that encouraging 'patient' reasoning through a novel training approach significantly enhances performance**. This work opens new avenues for enhancing LLMs without relying on expensive, large-scale datasets, making it highly relevant to current research trends focusing on improving LLM reasoning and efficiency.", "summary": "Boosting Large Language Model (LLM) reasoning without massive datasets: A novel training method encourages 'patient' reasoning, improving accuracy by up to 6.7% on benchmark tasks.", "takeaways": ["A simple training method significantly improves LLM reasoning by prioritizing detailed responses.", "The method enhances accuracy in mathematical problem-solving without needing extensive new data.", "The approach achieves a performance increase on benchmark tasks while remaining computationally feasible."], "tldr": "Large Language Models (LLMs) often prioritize speed over accuracy, especially in complex tasks like mathematical problem-solving.  Existing methods to improve reasoning usually involve extensive and costly training data. This limits their practical application. The current research identifies the importance of  **'patient reasoning'** - allowing more time for the model to carefully consider a problem before giving a response - as a key factor impacting LLM performance. \nThe study proposes a novel training method to address this. Instead of creating new complex datasets, it uses a simple preference optimization approach.  This involves training the model to favor more detailed reasoning steps by presenting examples of thorough solutions as 'positive' and concise solutions as 'negative'.  The result demonstrates a substantial increase in performance in math problem-solving benchmarks, showing that the **'patient' LLM strategy is effective**. This offers a more cost-effective way to improve LLM reasoning.", "affiliation": "Tsinghua University", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}, "podcast_path": "2411.13082/podcast.wav"}