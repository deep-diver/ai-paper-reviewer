{"references": [{"fullname_first_author": "Karl Cobbe", "paper_title": "Training Verifiers to Solve Math Word Problems", "publication_date": "2021-10-26", "reason": "This paper introduces the GSM8K dataset, a benchmark heavily used in the paper for evaluating the performance of different LLM reasoning methods."}, {"fullname_first_author": "Dan Hendrycks", "paper_title": "Measuring Mathematical Problem Solving With the MATH Dataset", "publication_date": "2021-03-11", "reason": "This paper introduces the MATH dataset, another benchmark frequently used to evaluate the LLM's mathematical reasoning capabilities."}, {"fullname_first_author": "Jason Wei", "paper_title": "Chain-of-thought prompting elicits reasoning in large language models", "publication_date": "2022-11-28", "reason": "This paper introduces the chain-of-thought (CoT) prompting method, which is the foundation for many LLM reasoning improvement techniques, including the method proposed in this paper."}, {"fullname_first_author": "Rafael Rafailov", "paper_title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model", "publication_date": "2023-11-28", "reason": "This paper introduces the Direct Preference Optimization (DPO) method, which is utilized by the authors to train their model and improve its reasoning performance."}, {"fullname_first_author": "Yiwei Qin", "paper_title": "Step-DPO: Step-wise preference optimization for long-chain reasoning of LLMs", "publication_date": "2024-06-17", "reason": "This paper is cited as one of the many works attempting to construct valuable CoT-like reasoning data to improve LLMs; its dataset is used in the experiments of this paper."}]}