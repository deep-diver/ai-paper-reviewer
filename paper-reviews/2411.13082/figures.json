[{"figure_path": "https://arxiv.org/html/2411.13082/extracted/6012215/patient_dpo.png", "caption": "Figure 1: The overall process of our methods.", "description": "This figure illustrates the workflow of the proposed method. It starts with collecting mathematical problems and generating initial solutions using an LLM.  These solutions are then refined by prompting the LLM to provide more detailed and patient reasoning steps, creating positive examples.  Concurrently, the concise original solutions serve as negative examples.  These positive and negative examples are used in a preference optimization process (DPO) to fine-tune a base language model. The ultimate goal is to train the model to favor more thorough and elaborate reasoning processes when solving problems.", "section": "2 Method"}]