[{"figure_path": "https://arxiv.org/html/2412.17295/x1.png", "caption": "Figure 1: An example of multi-modal multi-party conversation. The task of conversation speaker identification is to infer the dotted arrows pointing from characters to utterances, and the task of conversation response prediction is to infer the last utterance in the dotted rectangular. Only one frame of the video is shown as the visual context to avoid clutter.", "description": "Figure 1 shows an example of a multi-modal, multi-party conversation from the Friends-MMC dataset.  The image displays several characters from the TV show *Friends* along with corresponding dialogue. The task of conversation speaker identification involves determining which speaker produced each utterance (represented by dotted arrows). The task of conversation response prediction focuses on identifying the final utterance within the conversation snippet (enclosed in a dotted rectangle). Note that only a single video frame is shown as visual context for clarity.", "section": "Friends-MMC Dataset"}, {"figure_path": "https://arxiv.org/html/2412.17295/x2.png", "caption": "Figure 2: An overview of the construction process of Friends-MMC dataset.", "description": "This figure illustrates the data pipeline for creating the Friends-MMC dataset.  It begins with video preprocessing, where video clips are extracted based on timestamps from subtitles. Next, face detection and tracking identify and group faces across frames, forming face tracks.  These tracks are then labeled with character names using a face prototype matching approach, leveraging pre-trained models and a similarity threshold. A validation step ensures accuracy against manually labeled data. The process culminates in session selection using sliding windows, combining multiple consecutive turns into dialogue sessions, which finally produces the Friends-MMC dataset.", "section": "Friends-MMC Dataset"}, {"figure_path": "https://arxiv.org/html/2412.17295/x3.png", "caption": "Figure 3: Model overview of the three modules in different colors: the visual (M1subscript\ud835\udc401M_{1}italic_M start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT) is yellow, the textual (M2subscript\ud835\udc402M_{2}italic_M start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT) is green, and the optimization solver taking vision and text reward matrix as input is blue.", "description": "Figure 3 illustrates the architecture of the proposed baseline model for conversation speaker identification.  It's a modular design with three main components: a visual module (M1) represented in yellow, a textual module (M2) in green, and a quadratic binary optimization solver in blue. The visual module processes visual features (either from an image or a video) to estimate the probability of each detected face being the speaker. The textual module analyzes the conversational context to identify relations between utterances. These two modules' outputs are then combined using the optimization solver to determine the final speaker identification for each turn.", "section": "Conversation Speaker Identification"}]