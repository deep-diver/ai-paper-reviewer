{"importance": "This paper is crucial for researchers working on multilingual and multimodal large language models (MLLMs). It addresses the critical issue of Western-centric bias in existing models by introducing PANGEA, a fully open-source MLLM trained on a diverse, multilingual dataset. This provides a valuable resource for the community to build upon, and the findings on data scaling and the importance of balanced English/multilingual proportions are valuable insights for future model development.  The holistic evaluation suite, PANGEABENCH, also sets a new standard for assessing MLLM performance across diverse languages and cultures.", "summary": "PANGEA: A fully open multilingual, multimodal LLM for 39 languages, significantly outperforming existing models in diverse cultural contexts.", "takeaways": ["PANGEA, a new multilingual, multimodal LLM, significantly outperforms existing models on multilingual tasks.", "PANGEA's performance highlights the importance of balanced English/multilingual data in training.", "The open-sourced PANGEA, its dataset PANGEAINS, and benchmark PANGEABENCH, advance research on inclusive and robust multilingual MLLMs."], "tldr": "The paper introduces PANGEA, a groundbreaking multilingual and multimodal large language model (LLM) trained on a massive 6M instruction dataset spanning 39 languages.  Unlike most LLMs focusing on English and Western languages, PANGEA tackles the underrepresentation of global languages and diverse cultural contexts.  Its training data, PANGEAINS, includes high-quality English instructions, carefully machine-translated versions, and culturally relevant multimodal tasks.  Researchers rigorously evaluated PANGEA using PANGEABENCH, a holistic benchmark incorporating 14 datasets across 47 languages. Results demonstrate that PANGEA substantially outperforms existing open-source models in multilingual and multicultural scenarios. Ablation studies highlight the critical role of English data proportion, language popularity, and the amount of multimodal training data on model performance. Notably, the authors make PANGEA, PANGEAINS, and PANGEABENCH fully open-source to foster inclusive and robust multilingual MLLM development."}