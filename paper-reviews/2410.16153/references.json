{"references": [{" publication_date": "2023", "fullname_first_author": "Damian Blasi", "paper_title": "Systematic inequalities in language technology performance across the world's languages", "reason": "This paper is highly relevant to the core theme of the PANGEA paper, as it directly addresses the issue of systematic inequalities in language technology performance across different languages.  It provides strong empirical evidence supporting the need for multilingual and culturally inclusive models like PANGEA, by demonstrating the significant performance gaps between high-resource and low-resource languages. This directly justifies the motivation for building a more inclusive multilingual model.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Lucas Beyer", "paper_title": "PaliGemma: A versatile 3B VLM for transfer", "reason": "This paper introduces PaliGemma, a multilingual vision-language model, which is directly comparable to PANGEA.  Its performance is used as a baseline to demonstrate PANGEA's superior capabilities in multilingual settings. By including PaliGemma's results, the authors provide a benchmark against existing state-of-the-art models, thereby strengthening the claim of PANGEA's contributions to the field.", "section_number": 1}, {" publication_date": "2021", "fullname_first_author": "Damian Blasi", "paper_title": "Systematic inequalities in language technology performance across the world's languages", "reason": "This paper directly addresses the issue of systematic inequalities in language technology performance across different languages. It provides strong empirical evidence to support the paper's claim that current MLLMs suffer from significant bias towards English and Western-centric datasets, thus motivating the need for a more inclusive approach.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Badr AlKhamissi", "paper_title": "Investigating cultural alignment of large language models", "reason": "This paper directly addresses the issue of cultural bias in LLMs. It explores how cultural norms influence the outputs of LLMs and highlights the importance of addressing cultural factors in the development of inclusive models. The PANGEA model is similar because it aims to reduce the cultural biases in LLM training data and enhance its cultural inclusivity.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Vikram V. Ramaswamy", "paper_title": "Geode: a geographically diverse evaluation dataset for object recognition", "reason": "This paper is important because it directly relates to the challenge of Western-centric bias in MLLMs. The authors highlight the limited representation of objects from geographically diverse regions in existing datasets, which motivates the need for more culturally inclusive datasets like PANGEAINS. Its importance is in highlighting the limitations of existing datasets that lack the diverse cultural and geographical representation necessary for training fair and inclusive models.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Yueqi Song", "paper_title": "GlobalBench: A benchmark for global progress in natural language processing", "reason": "This paper is highly relevant to the evaluation methodology of PANGEA, as it focuses on evaluating models across a range of languages and tasks.  The creation of PANGEABENCH is motivated in part by a need to address the shortcomings of existing benchmarks, as highlighted by this paper.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Matt Deitke", "paper_title": "Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models", "reason": "This paper introduces Molmo, a large multilingual multimodal model. Its performance is directly compared to PANGEA's performance on various tasks and datasets, serving as a crucial benchmark in assessing the strengths and weaknesses of both models. The results highlight PANGEA's performance in terms of improvements over open-source models.", "section_number": 3}, {" publication_date": "2025", "fullname_first_author": "Hanoona Rasheed", "paper_title": "PALO: A large multilingual multimodal language model", "reason": "This paper introduces PALO, a multilingual multimodal language model, which is directly comparable to PANGEA.  Its performance is used as a baseline to demonstrate PANGEA's superior capabilities in multilingual settings. By including PALO's results, the authors provide a benchmark against existing state-of-the-art models, thereby strengthening the claim of PANGEA's contributions to the field.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Gregor Geigle", "paper_title": "mBLIP: Efficient bootstrapping of multilingual vision-LLMs", "reason": "This paper introduces mBLIP, a multilingual vision-language model, that is used as one of the baselines against which PANGEA's performance is measured. The performance comparison between PANGEA and mBLIP helps demonstrate the relative improvement of PANGEA in multilingual capabilities. This is crucial in showing the advancements of PANGEA over previous models.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Xiang Yue", "paper_title": "M3Exam: A multilingual, multimodal, multilevel benchmark for examining large language models", "reason": "This paper introduces a benchmark for evaluating large language models that assesses various aspects of language proficiency.  This is relevant to PANGEA's evaluation methodology in that it demonstrates the need for comprehensive evaluation strategies and highlights the strengths and weaknesses of various approaches to model development.  The dataset contributes to a more robust multilingual evaluation.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Xiang Yue", "paper_title": "Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi", "reason": "This paper is directly related to the evaluation of PANGEA because it introduces MMMU, a new benchmark used to assess PANGEA's capabilities in multimodal understanding and reasoning. The MMMU benchmark offers a more challenging and comprehensive evaluation of multilingual and multimodal abilities compared to existing benchmarks.  This further justifies the use of the more comprehensive benchmark PANGEABENCH.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Haotian Liu", "paper_title": "Llava-next: Improved reasoning, ocr, and world knowledge", "reason": "This paper introduces LLaVA-Next, a large multilingual model that is used as a key component in training PANGEA.   This is critical because it directly relates to the architectural design of PANGEA and its performance is used as a baseline in demonstrating PANGEA's improvements in multilingual and multimodal tasks.  The close relationship between the two models makes this paper one of the most important references.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "An Yang", "paper_title": "Qwen2 technical report", "reason": "This paper introduces Qwen-2, the language model backbone used in the PANGEA architecture. Its performance is directly compared to PANGEA's performance on various text-only tasks, providing evidence of PANGEA's improved capabilities in handling multilingual and multimodal contexts compared to its text-only backbone model. This is especially relevant to demonstrating how combining the language model with visual understanding components yields improved results.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Abhimanyu Dubey", "paper_title": "The llama 3 herd of models", "reason": "This paper is important for the translation part of PANGEAINS dataset creation. The authors used this strong open-source machine translation model, to translate English instructions to other languages. This is an essential part of building the PANGEAINS dataset, which is crucial for training the PANGEA model. Therefore, the paper's relevance is in providing insights into the choices made in the process of creating a multilingual dataset.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Christoph Schuhmann", "paper_title": "Laion-5b: An open large-scale dataset for training next generation image-text models", "reason": "This paper is significant because it introduces LAION-5B, a large-scale image-text dataset used as a source for images in PANGEAINS. The use of LAION-5B is crucial in addressing the issue of Western-centric bias in visual data. By leveraging this dataset, the authors ensure that PANGEAINS includes images from diverse cultural contexts, making the model more inclusive and equitable.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Matt Deitke", "paper_title": "Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models", "reason": "This paper provides a direct comparison of different multimodal language models. The paper presents results from various models including Molmo, which directly demonstrates PANGEA's outperformance in terms of its multilingual performance across various datasets and tasks, reinforcing the claim of PANGEA's advanced capabilities.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Seungone Kim", "paper_title": "Prometheus: Inducing fine-grained evaluation capability in language models", "reason": "This paper is highly relevant to the development of the xChatBench dataset, which employed a more precise assessment of MLLM performance using a fine-grained evaluation pipeline. The paper outlines an evaluation pipeline where human annotators are involved in crafting both reference answers and scoring rubrics. This advanced method helps to resolve ambiguities and provides a more nuanced evaluation compared to coarse LLM-as-Judge methods.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Lianmin Zheng", "paper_title": "Judging llm-as-a-judge with mt-bench and chatbot arena", "reason": "This paper focuses on the limitations of using LLMs as judges for evaluating other LLMs and thus relates directly to the evaluation methodology of the PANGEA model.  The authors highlight potential issues with using coarse-grained evaluation criteria in assessing model performance, which further justifies the use of a more robust, fine-grained evaluation pipeline in xChatBench.", "section_number": 3}, {" publication_date": "2022", "fullname_first_author": "Jonas Pfeiffer", "paper_title": "xGQA: Cross-lingual visual question answering", "reason": "This paper introduces the xGQA dataset, a benchmark used in PANGEABENCH for evaluating multilingual visual question-answering capabilities.  Including xGQA as part of PANGEABENCH helps to demonstrate PANGEA's ability to handle a wide range of cross-lingual and cross-cultural visual understanding tasks.  This is important because xGQA focuses on cultural diversity in visual QA.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Bo Li", "paper_title": "lmms-eval: Accelerating the development of large multimoal models", "reason": "This paper is relevant to the evaluation of PANGEA because it introduces lmms-eval, which is a multimodal evaluation package utilized in the paper to evaluate PANGEA's performance on various multimodal benchmarks.  The use of lmms-eval ensures that PANGEA's evaluation process is rigorous and comparable to other studies in the field.  This is important in ensuring the reliability of the paper's results.", "section_number": 4}]}