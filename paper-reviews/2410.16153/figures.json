[{"figure_path": "2410.16153/figures/figures_2_0.png", "caption": "Figure 1: Overview of the aggregate performance of various multimodal LLMs on PANGEABENCH. Our PANGEA-7B demonstrates comparable performance to SoTA open-source models in English settings, while significantly outperforming them in multilingual scenarios.", "description": "The figure shows a comparison of the aggregate performance of various multimodal LLMs on the PANGEABENCH benchmark, highlighting PANGEA-7B's competitive performance in English and superior performance in multilingual settings.", "section": "Multilingual Performance"}, {"figure_path": "2410.16153/figures/figures_23_0.png", "caption": "Figure 1: Overview of the aggregate performance of various multimodal LLMs on PANGEABENCH. Our PANGEA-7B demonstrates comparable performance to SoTA open-source models in English settings, while significantly outperforming them in multilingual scenarios.", "description": "The figure shows a comparison of the aggregate performance of various multilingual and multimodal large language models on the PANGEABENCH benchmark, highlighting PANGEA-7B's superior performance in multilingual scenarios compared to existing models.", "section": "ABSTRACT"}, {"figure_path": "2410.16153/figures/figures_27_0.png", "caption": "Figure 1: Overview of the aggregate performance of various multimodal LLMs on PANGEABENCH. Our PANGEA-7B demonstrates comparable performance to SoTA open-source models in English settings, while significantly outperforming them in multilingual scenarios.", "description": "The figure shows a bar chart comparing the aggregate performance of various multimodal large language models (MLLMs) on the PANGEABENCH benchmark, highlighting PANGEA-7B's superior performance in multilingual scenarios.", "section": "Multilingual Performance"}, {"figure_path": "2410.16153/figures/figures_28_0.png", "caption": "Figure 1: Overview of the aggregate performance of various multimodal LLMs on PANGEABENCH. Our PANGEA-7B demonstrates comparable performance to SoTA open-source models in English settings, while significantly outperforming them in multilingual scenarios.", "description": "The figure shows the aggregate performance of various multimodal LLMs on the PANGEABENCH benchmark, highlighting PANGEA-7B's comparable performance to state-of-the-art open-source models in English and significantly better performance in multilingual scenarios.", "section": "Multilingual Performance"}, {"figure_path": "2410.16153/figures/figures_29_0.png", "caption": "Figure 1: Overview of the aggregate performance of various multimodal LLMs on PANGEABENCH. Our PANGEA-7B demonstrates comparable performance to SoTA open-source models in English settings, while significantly outperforming them in multilingual scenarios.", "description": "The figure shows a bar chart comparing the performance of various multilingual and multimodal LLMs on the PANGEABENCH benchmark, highlighting the superior performance of PANGEA-7B in multilingual settings.", "section": "Multilingual Performance"}, {"figure_path": "2410.16153/figures/figures_31_0.png", "caption": "Figure 1: Overview of the aggregate performance of various multimodal LLMs on PANGEABENCH. Our PANGEA-7B demonstrates comparable performance to SoTA open-source models in English settings, while significantly outperforming them in multilingual scenarios.", "description": "The figure shows the aggregate performance of various multimodal LLMs on PANGEABENCH, demonstrating PANGEA-7B's comparable performance to state-of-the-art open-source models in English and superior performance in multilingual scenarios.", "section": "Multilingual Performance"}, {"figure_path": "2410.16153/figures/figures_32_0.png", "caption": "Figure 1: Overview of the aggregate performance of various multimodal LLMs on PANGEABENCH. Our PANGEA-7B demonstrates comparable performance to SoTA open-source models in English settings, while significantly outperforming them in multilingual scenarios.", "description": "The figure shows a comparison of the aggregate performance of various multimodal large language models (MLLMs) on the PANGEABENCH benchmark, highlighting PANGEA-7B's superior performance in multilingual settings.", "section": "ABSTRACT"}, {"figure_path": "2410.16153/figures/figures_36_0.png", "caption": "Figure 1: Overview of the aggregate performance of various multimodal LLMs on PANGEABENCH. Our PANGEA-7B demonstrates comparable performance to SoTA open-source models in English settings, while significantly outperforming them in multilingual scenarios.", "description": "The figure shows a comparison of the aggregate performance of various multimodal LLMs on the PANGEABENCH benchmark, highlighting PANGEA-7B's comparable performance to state-of-the-art open-source models in English and significantly superior performance in multilingual scenarios.", "section": "Multilingual Performance"}, {"figure_path": "2410.16153/figures/figures_37_0.png", "caption": "Figure 1: Overview of the aggregate performance of various multimodal LLMs on PANGEABENCH. Our PANGEA-7B demonstrates comparable performance to SoTA open-source models in English settings, while significantly outperforming them in multilingual scenarios.", "description": "The figure shows a bar chart comparing the aggregate performance of various multilingual and multimodal LLMs on the PANGEABENCH benchmark, highlighting that PANGEA-7B outperforms existing models in multilingual settings.", "section": "Multilingual Performance"}, {"figure_path": "2410.16153/figures/figures_40_0.png", "caption": "Figure 1: Overview of the aggregate performance of various multimodal LLMs on PANGEABENCH. Our PANGEA-7B demonstrates comparable performance to SoTA open-source models in English settings, while significantly outperforming them in multilingual scenarios.", "description": "The figure shows a comparison of the aggregate performance of various multimodal LLMs on the PANGEABENCH benchmark, highlighting PANGEA-7B's competitive performance in English and its superior performance in multilingual scenarios.", "section": "Multilingual Performance"}, {"figure_path": "2410.16153/figures/figures_42_0.png", "caption": "Figure 1: Overview of the aggregate performance of various multimodal LLMs on PANGEABENCH. Our PANGEA-7B demonstrates comparable performance to SoTA open-source models in English settings, while significantly outperforming them in multilingual scenarios.", "description": "The figure shows the aggregate performance of various multimodal LLMs on PANGEABENCH, demonstrating PANGEA-7B's competitive performance in English and superior performance in multilingual settings.", "section": "Multilingual Performance"}, {"figure_path": "2410.16153/figures/figures_43_0.png", "caption": "Figure 1: Overview of the aggregate performance of various multimodal LLMs on PANGEABENCH. Our PANGEA-7B demonstrates comparable performance to SoTA open-source models in English settings, while significantly outperforming them in multilingual scenarios.", "description": "The figure shows a bar chart comparing the performance of various multilingual and English-centric multimodal LLMs on the PANGEABENCH benchmark, highlighting PANGEA-7B's superior performance in multilingual scenarios.", "section": "Multilingual Performance"}]