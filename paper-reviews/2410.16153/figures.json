[{"figure_path": "2410.16153/figures/figures_2_0.png", "caption": "Figure 1: Overview of the aggregate performance of various multimodal LLMs on PANGEABENCH. Our PANGEA-7B demonstrates comparable performance to SoTA open-source models in English settings, while significantly outperforming them in multilingual scenarios.", "description": "The figure shows the aggregate performance of various multimodal LLMs on the PANGEABENCH benchmark, highlighting PANGEA-7B's competitive performance in English and superior performance in multilingual scenarios.", "section": "Multilingual Performance"}, {"figure_path": "2410.16153/figures/figures_23_0.png", "caption": "Figure 1: Overview of the aggregate performance of various multimodal LLMs on PANGEABENCH. Our PANGEA-7B demonstrates comparable performance to SoTA open-source models in English settings, while significantly outperforming them in multilingual scenarios.", "description": "The figure shows a bar chart comparing the aggregate performance of various multimodal LLMs on the PANGEABENCH benchmark, highlighting PANGEA-7B's superior multilingual performance.", "section": "Multilingual Performance"}, {"figure_path": "2410.16153/figures/figures_27_0.png", "caption": "Figure 1: Overview of the aggregate performance of various multimodal LLMs on PANGEABENCH. Our PANGEA-7B demonstrates comparable performance to SoTA open-source models in English settings, while significantly outperforming them in multilingual scenarios.", "description": "The figure shows a comparison of the aggregate performance of various multimodal LLMs on PANGEABENCH, highlighting PANGEA-7B's competitive performance in English and superior performance in multilingual scenarios.", "section": "Multilingual Performance"}, {"figure_path": "2410.16153/figures/figures_28_0.png", "caption": "Figure 1: Overview of the aggregate performance of various multimodal LLMs on PANGEABENCH. Our PANGEA-7B demonstrates comparable performance to SoTA open-source models in English settings, while significantly outperforming them in multilingual scenarios.", "description": "The figure shows a comparison of the aggregate performance of various multimodal LLMs on the PANGEABENCH benchmark, highlighting PANGEA-7B's competitive performance in English and superior performance in multilingual scenarios.", "section": "Multilingual Performance"}, {"figure_path": "2410.16153/figures/figures_29_0.png", "caption": "Figure 1: Overview of the aggregate performance of various multimodal LLMs on PANGEABENCH. Our PANGEA-7B demonstrates comparable performance to SoTA open-source models in English settings, while significantly outperforming them in multilingual scenarios.", "description": "The figure shows that PANGEA-7B achieves comparable performance to state-of-the-art open-source models on English benchmarks but significantly outperforms them on multilingual benchmarks.", "section": "Multilingual Performance"}, {"figure_path": "2410.16153/figures/figures_30_0.png", "caption": "Figure 1: Overview of the aggregate performance of various multimodal LLMs on PANGEABENCH. Our PANGEA-7B demonstrates comparable performance to SoTA open-source models in English settings, while significantly outperforming them in multilingual scenarios.", "description": "The figure shows a bar chart comparing the aggregate performance of various multimodal large language models (MLLMs) on a multilingual benchmark, highlighting the superior performance of the PANGEA-7B model in multilingual scenarios compared to English-centric models.", "section": "Multilingual Performance"}, {"figure_path": "2410.16153/figures/figures_31_0.png", "caption": "Figure 1: Overview of the aggregate performance of various multimodal LLMs on PANGEABENCH. Our PANGEA-7B demonstrates comparable performance to SoTA open-source models in English settings, while significantly outperforming them in multilingual scenarios.", "description": "The figure shows a bar chart comparing the aggregate performance of various multilingual and English-centric multimodal large language models (MLLMs) on the PANGEABENCH benchmark, highlighting PANGEA-7B's superior performance in multilingual scenarios.", "section": "ABSTRACT"}, {"figure_path": "2410.16153/figures/figures_34_0.png", "caption": "Figure 1: Overview of the aggregate performance of various multimodal LLMs on PANGEABENCH. Our PANGEA-7B demonstrates comparable performance to SoTA open-source models in English settings, while significantly outperforming them in multilingual scenarios.", "description": "The figure shows a comparison of the aggregate performance of various multilingual and multimodal LLMs on the PANGEABENCH benchmark, highlighting PANGEA-7B's superior performance in multilingual scenarios.", "section": "Multilingual Performance"}, {"figure_path": "2410.16153/figures/figures_36_0.png", "caption": "Figure 1: Overview of the aggregate performance of various multimodal LLMs on PANGEABENCH. Our PANGEA-7B demonstrates comparable performance to SoTA open-source models in English settings, while significantly outperforming them in multilingual scenarios.", "description": "The figure shows a comparison of the aggregate performance of various multimodal LLMs on the PANGEABENCH benchmark, highlighting PANGEA-7B's competitive performance in English and superior performance in multilingual settings.", "section": "Multilingual Performance"}, {"figure_path": "2410.16153/figures/figures_37_0.png", "caption": "Figure 1: Overview of the aggregate performance of various multimodal LLMs on PANGEABENCH. Our PANGEA-7B demonstrates comparable performance to SoTA open-source models in English settings, while significantly outperforming them in multilingual scenarios.", "description": "The figure shows the aggregate performance of various multimodal LLMs on the PANGEABENCH benchmark, highlighting PANGEA-7B's comparable English performance and superior multilingual performance.", "section": "Multilingual Performance"}, {"figure_path": "2410.16153/figures/figures_40_0.png", "caption": "Figure 1: Overview of the aggregate performance of various multimodal LLMs on PANGEABENCH. Our PANGEA-7B demonstrates comparable performance to SoTA open-source models in English settings, while significantly outperforming them in multilingual scenarios.", "description": "The figure shows a comparison of the aggregate performance of various multimodal LLMs on PANGEABENCH, highlighting PANGEA-7B's comparable performance to state-of-the-art open-source models in English and significantly superior performance in multilingual settings.", "section": "Multilingual Performance"}, {"figure_path": "2410.16153/figures/figures_42_0.png", "caption": "Figure 1: Overview of the aggregate performance of various multimodal LLMs on PANGEABENCH. Our PANGEA-7B demonstrates comparable performance to SoTA open-source models in English settings, while significantly outperforming them in multilingual scenarios.", "description": "The figure shows a bar chart comparing the aggregate performance of various multilingual and English-centric multimodal LLMs on the PANGEABENCH benchmark, highlighting PANGEA-7B's superior performance in multilingual settings.", "section": "Multilingual Performance"}, {"figure_path": "2410.16153/figures/figures_43_0.png", "caption": "Figure 1: Overview of the aggregate performance of various multimodal LLMs on PANGEABENCH. Our PANGEA-7B demonstrates comparable performance to SoTA open-source models in English settings, while significantly outperforming them in multilingual scenarios.", "description": "The figure shows a bar chart comparing the aggregate performance of various multimodal LLMs on the PANGEABENCH benchmark, highlighting PANGEA-7B's superior performance in multilingual settings.", "section": "Multilingual Performance"}]