[{"figure_path": "2410.16153/charts/charts_1_0.png", "caption": "Figure 1: Overview of the aggregate performance of various multimodal LLMs on PANGEABENCH. Our PANGEA-7B demonstrates comparable performance to SoTA open-source models in English settings, while significantly outperforming them in multilingual scenarios.", "description": "The chart shows the aggregate performance of various multimodal LLMs on the PANGEABENCH benchmark, illustrating PANGEA-7B's comparable performance to state-of-the-art open-source models in English and its superior performance in multilingual settings.", "section": "Multilingual Performance"}, {"figure_path": "2410.16153/charts/charts_2_0.png", "caption": "Figure 1: Overview of the aggregate performance of various multimodal LLMs on PANGEABENCH. Our PANGEA-7B demonstrates comparable performance to SoTA open-source models in English settings, while significantly outperforming them in multilingual scenarios.", "description": "The chart displays the aggregate performance of various multimodal LLMs on the PANGEABENCH benchmark, showing that PANGEA-7B achieves comparable performance to state-of-the-art open-source models in English but significantly outperforms them in multilingual scenarios.", "section": "Multilingual Performance"}, {"figure_path": "2410.16153/charts/charts_10_0.png", "caption": "Figure 1: Overview of the aggregate performance of various multimodal LLMs on PANGEABENCH. Our PANGEA-7B demonstrates comparable performance to SoTA open-source models in English settings, while significantly outperforming them in multilingual scenarios.", "description": "The chart compares the aggregate performance of various multilingual and English-centric multimodal large language models (MLLMs) on the PANGEABENCH evaluation suite, highlighting PANGEA-7B's superior performance in multilingual settings.", "section": "Multilingual Performance"}, {"figure_path": "2410.16153/charts/charts_10_1.png", "caption": "Figure 1: Overview of the aggregate performance of various multimodal LLMs on PANGEABENCH. Our PANGEA-7B demonstrates comparable performance to SoTA open-source models in English settings, while significantly outperforming them in multilingual scenarios.", "description": "The chart shows a comparison of the aggregate performance of various multilingual and multimodal LLMs on the PANGEABENCH evaluation suite, highlighting PANGEA-7B's competitive performance in English and superior performance in multilingual scenarios.", "section": "Multilingual Performance"}, {"figure_path": "2410.16153/charts/charts_10_2.png", "caption": "Figure 1: Overview of the aggregate performance of various multimodal LLMs on PANGEABENCH. Our PANGEA-7B demonstrates comparable performance to SoTA open-source models in English settings, while significantly outperforming them in multilingual scenarios.", "description": "The chart displays the aggregate performance of various multimodal LLMs on the PANGEABENCH benchmark, showing PANGEA-7B's comparable English performance to state-of-the-art open-source models and significantly better multilingual performance.", "section": "Multilingual Performance"}, {"figure_path": "2410.16153/charts/charts_11_0.png", "caption": "Figure 1: Overview of the aggregate performance of various multimodal LLMs on PANGEABENCH. Our PANGEA-7B demonstrates comparable performance to SoTA open-source models in English settings, while significantly outperforming them in multilingual scenarios.", "description": "The chart displays the aggregate performance of various multimodal LLMs on the PANGEABENCH benchmark, showing PANGEA-7B's comparable performance to state-of-the-art open-source models in English and its superior performance in multilingual scenarios.", "section": "Multilingual Performance"}]