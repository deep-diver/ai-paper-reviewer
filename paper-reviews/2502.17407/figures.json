[{"figure_path": "https://arxiv.org/html/2502.17407/x1.png", "caption": "Figure 1: Performance of Qwen2.5-1.5B-Math with different test-time scaling strategies.\u2014\u2014Once configured to use comparable inference FLOPs, all three methods (Outcome Reward Modeling, Process Reward Modeling, and Budget Forcing) achieve similar performance.", "description": "This figure displays the performance of the Qwen2.5-1.5B-Math model on a multilingual mathematics benchmark (MCLM) when employing three different test-time scaling strategies: Outcome Reward Modeling (ORM), Process Reward Modeling (PRM), and Budget Forcing (BF).  The x-axis represents the three test-time scaling methods. The y-axis indicates the accuracy achieved by each method. The bars show the accuracy for each method, with error bars illustrating the variability of results.  Importantly, the results demonstrate that once the three scaling methods are adjusted to have roughly the same computational cost (measured in FLOPs), they all achieve similar performance on the MCLM benchmark. This suggests that the choice of test-time scaling method may be less crucial than the total computational budget used during inference.", "section": "3.1 Baselines: Test-Time Scaling Strategies"}, {"figure_path": "https://arxiv.org/html/2502.17407/x2.png", "caption": "Figure 2: Comparison of different inference-time scaling strategies. Blue boxes represent selected outputs, while red boxes indicate rejected ones.", "description": "This figure illustrates three different test-time scaling strategies: Outcome Reward Modeling (ORM), Process Reward Modeling (PRM), and Budget Forcing (BF).  Each strategy is represented visually.  The blue boxes represent the model's outputs that were considered correct or accepted, while the red boxes show rejected or incorrect outputs.  The figure highlights the different approaches to scaling inference at test time and visually represents which outputs each method would accept or reject, emphasizing their differing processes.", "section": "Baselines: Test-Time Scaling Strategies"}, {"figure_path": "https://arxiv.org/html/2502.17407/x3.png", "caption": "Figure 3: # of generated tokens for 1.5B and 7B models in a greedy setting, divided by correctness. Languages are represented as scatter plots, overlaid on box plots.", "description": "This figure displays the number of tokens generated by 1.5B and 7B parameter models during greedy decoding, categorized by whether the generated answer was correct or not.  Each data point represents a single problem solved in one of the 55 languages included in the MCLM benchmark. The data is presented as a combination of box plots showing the overall distribution of token counts for each model size and correctness level, and overlaid scatter plots to show the individual data points for each language. This visualization helps to understand the relationship between model size, answer correctness, and the length of the model's reasoning process in different languages.", "section": "3 Experimental Settings"}, {"figure_path": "https://arxiv.org/html/2502.17407/x4.png", "caption": "Figure 4: Gains of ORM compared to a greedy-decoding baseline. The semi-transparent \u201ccloud\u201d indicates the 2D data distribution via a KDE density plot, and the overlaid lines are third-order polynomial regressions modeling how each ORM setting scales with the baseline score.", "description": "This figure displays the performance gains achieved by using Outcome Reward Modeling (ORM) compared to a standard greedy decoding approach.  The x-axis represents the baseline score (obtained through greedy decoding), while the y-axis shows the improvement gained by applying the ORM method. Different ORM settings (with varying numbers of generated responses: k = 2, 4, 8) are represented by separate lines and data clouds.  A KDE (Kernel Density Estimate) plot, visually depicted as a semi-transparent cloud, helps visualize the distribution of data points for each ORM setting.  Third-order polynomial regression curves provide a smooth fit to the data, illustrating the relationship between the baseline score and ORM performance improvements across various settings and across the two datasets (MT-MATH100 and MT-AIME2024).  This visualization helps to understand how the effectiveness of ORM varies depending on the baseline performance and which parameter settings (number of responses K) lead to the most gains in performance.", "section": "3.1 Baselines: Test-Time Scaling Strategies"}, {"figure_path": "https://arxiv.org/html/2502.17407/x5.png", "caption": "Figure 5: PRM inference FLOPs as a function of generation steps S\ud835\udc46Sitalic_S and candidates per step c\ud835\udc50citalic_c. The left panel uses a verifier size of 72B, while the right panel uses a 7B RM, displaying adjusted configurations to yield similar costs.", "description": "This figure illustrates the computational cost (in FLOPs) of the Process Reward Modeling (PRM) test-time scaling method.  PRM involves generating multiple candidate continuations at each step of the reasoning process and selecting the best one using a reward model. The figure shows how the FLOPs change as a function of two key parameters: (1) the number of generation steps (S) and (2) the number of candidate continuations generated at each step (c). The left panel shows the FLOPs when using a large 72B parameter reward model, while the right panel shows the FLOPs when using a smaller 7B parameter reward model.  Importantly, the configurations in both panels have been adjusted to ensure that the total computational cost (FLOPs) remains roughly equal for each configuration, allowing for a fair comparison of the different parameter settings.", "section": "3.2 Calculating Inference FLOPS"}, {"figure_path": "https://arxiv.org/html/2502.17407/x6.png", "caption": "Figure 6: Inference FLOPs versus PRM performance and consistency. (Left) Second-degree polynomial regressions for average performance on 14 languages, comparing the 7B (blue) and 72B (green) reward models. (Right) Fleiss\u2019 kappa (top) and standard deviation (bottom) plotted against the same FLOPs budget; the fitted curves reveal no clear monotonic trend.", "description": "This figure analyzes the performance and consistency of the Process Reward Modeling (PRM) method across different inference FLOPs budgets. The left panel shows the average performance of PRM on 14 languages using 7B and 72B reward models, fitted with second-degree polynomial regressions.  The right panel displays Fleiss\u2019 kappa (measuring inter-annotator agreement) and standard deviation for the same 14 languages.  The analysis demonstrates the relationship between the computational cost (FLOPs) and both the accuracy and consistency of PRM across languages, highlighting that increased FLOPs does not guarantee better multilingual performance or consistency.", "section": "3 Experimental Settings"}, {"figure_path": "https://arxiv.org/html/2502.17407/x7.png", "caption": "Figure 7: Comparison of PRM vs. ORM performance on MATH (solid lines) and AIME (dashed lines). 1.5B models are shown with plus markers, 7B models with stars. Blue lines represent PRM, green lines represent ORM. White box annotations indicate the performance difference (ORM \u2212 PRM) at the highest compute setting for each line.", "description": "Figure 7 is a graph comparing the performance of Process Reward Modeling (PRM) and Outcome Reward Modeling (ORM) on two mathematical reasoning benchmarks: MATH and AIME.  The x-axis represents the inference FLOPS (floating point operations) used, reflecting computational cost. The y-axis shows the accuracy, or percentage of correctly solved problems.  Separate lines are plotted for both the 1.5B parameter model (plus markers) and the 7B parameter model (stars).  Blue lines indicate PRM, while green lines represent ORM. The white boxes highlight the difference in accuracy between ORM and PRM at the highest FLOPS setting for each model/benchmark combination, illustrating how much better ORM performs than PRM at higher computational costs.", "section": "Result 1: ORM and PRM"}, {"figure_path": "https://arxiv.org/html/2502.17407/x8.png", "caption": "Figure 8: Performance of Qwen2.5-Math-1.5B +SFT and + MT-SFT at each training checkpoint. Average score and error bars for each checkpoint are displayed. The shaded region is the mean \u00b1plus-or-minus\\pm\u00b1 standard deviation for MT-SFT.", "description": "This figure shows the performance of two fine-tuned models, Qwen2.5-Math-1.5B + SFT and Qwen2.5-Math-1.5B + MT-SFT, across multiple training checkpoints.  The y-axis represents the average accuracy achieved by the models, and the x-axis shows the number of training checkpoints. Error bars are included to display the variability or uncertainty in the model's performance.  The shaded region visually represents the mean plus or minus one standard deviation of the MT-SFT model's performance, illustrating the range of its performance across different checkpoints.", "section": "5.2 Performance of trained models"}, {"figure_path": "https://arxiv.org/html/2502.17407/x9.png", "caption": "Figure 9: Performance of MR1 on MT-AIME2024 at B\u2062F={2048,4096,8192}\ud835\udc35\ud835\udc39204840968192BF=\\{2048,4096,8192\\}italic_B italic_F = { 2048 , 4096 , 8192 }. Grey dots represent individual languages. Solid lines indicate average performance, while dashed lines highlight reference performances for selected languages.", "description": "This figure shows the performance of the multilingual large language model (MR1) on the MT-AIME2024 dataset using the budget-forcing method with varying budget levels (BF = 2048, 4096, and 8192).  Each point represents the performance of MR1 in a specific language, illustrating the impact of the budget on model performance across various languages. The solid lines display the average performance for each budget level, while the dashed lines highlight the performance for selected languages, serving as a reference point for comparing performance across languages and budget levels.", "section": "Result 2: Budget Forcing"}, {"figure_path": "https://arxiv.org/html/2502.17407/x10.png", "caption": "Figure 10: Heatmap representation of IMO problems from 2006 to 2024. Each row corresponds to a competition year, and each column represents a problem (Q1\u2013Q6). Green cells indicate questions that have been included in the M-IMO subset, while gray cells represent problems that were not selected.", "description": "This heatmap visualizes the selection of IMO (International Mathematical Olympiad) problems for the M-IMO subset of the MCLM benchmark. Each row represents a year from 2006 to 2024, and each column corresponds to one of the six problems (Q1-Q6) presented in each year's competition.  Green cells indicate that a problem from that year was included in the M-IMO dataset, while gray cells show problems that were excluded. This provides a clear overview of which problems across the competition years were selected for this specific subset.", "section": "Multilingual Competition Level Math"}, {"figure_path": "https://arxiv.org/html/2502.17407/x11.png", "caption": "Figure 11: Solve rates (%) of different multilingual math datasets evaluated. For the OLMo2 series, we use the base models, while for the Qwen2.5 series, the instruct-tuned variants are used. Euler-Instruct presents a significantly lower solve rate, indicating its greater difficulty.", "description": "This figure shows the success rate of different large language models (LLMs) in solving math problems from various multilingual datasets.  The x-axis represents the different LLMs used, including OLMo2 models (using base versions without instruction tuning) and Qwen2.5 models (using instruction-tuned versions). The y-axis displays the percentage of problems successfully solved by each model. The Euler-Instruct dataset stands out, demonstrating a noticeably lower success rate than others, thus highlighting its increased difficulty compared to the other datasets.", "section": "3 Experimental Settings"}, {"figure_path": "https://arxiv.org/html/2502.17407/x12.png", "caption": "Figure 12: Model Results from Table\u00a09. Left shows accuracy on MT-MATH500 (entire translated subset for language group (B)), and right shows average performance of MT-AIME2024.", "description": "This figure presents the results of an ablation study on the training data for multilingual mathematical reasoning. The left panel displays the accuracy of different models on MT-MATH500, using various sizes of training datasets in different languages.  The right panel shows the average performance on MT-AIME2024 using the same training data configurations.  The plots illustrate how the size and composition of the training data influence model performance on these two distinct mathematical reasoning benchmarks.  The results reveal that more data, and the inclusion of more languages leads to better performance, especially on MT-MATH100.", "section": "3 Experimental Settings"}]