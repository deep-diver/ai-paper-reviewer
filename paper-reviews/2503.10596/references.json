{"references": [{"fullname_first_author": "Alexander Kirillov", "paper_title": "Segment anything", "publication_date": "2023-01-01", "reason": "This work introduces the Segment Anything Model (SAM), which enables promptable segmentation at scale and is a key component of the automated annotation framework used in GroundingSuite."}, {"fullname_first_author": "Tsung-Yi Lin", "paper_title": "Microsoft coco: Common objects in context", "publication_date": "2014-09-06", "reason": "This paper introduces the COCO dataset, a widely used benchmark in computer vision, and the GroundingSuite evaluation benchmark is built upon the unlabeled images from COCO, ensuring zero overlap with existing annotated sets while maintaining natural scene diversity."}, {"fullname_first_author": "Liunian Harold Li", "paper_title": "Grounded language-image pre-training", "publication_date": "2022-01-01", "reason": "This paper introduces GLIP, a method for grounded language-image pre-training, and the GroundingSuite uses Vision-Language Models (VLM) as efficient annotation agents and effective quality checkers."}, {"fullname_first_author": "Hanoona Rasheed", "paper_title": "Glamm: Pixel grounding large multimodal model", "publication_date": "2024-01-01", "reason": "This paper introduces the GLaMM, which is an automatic annotation pipeline and verification criteria, and it is mentioned in the text for its low-quality annotation and high-cost utilization problems compared to the proposed method, GSSculpt."}, {"fullname_first_author": "Haotian Liu", "paper_title": "Visual instruction tuning", "publication_date": "2023-01-01", "reason": "This paper introduces visual instruction tuning, the GroundingSuite annotation framework uses instruction-based segmentation models to filter the noisy references."}]}