{"references": [{"fullname_first_author": "Josh Achiam", "paper_title": "GPT-4 Technical Report", "publication_date": "2023-03-01", "reason": "This report details the technical specifications of GPT-4, a foundational large language model that significantly influenced the field and serves as a reference point for subsequent research."}, {"fullname_first_author": "Robin Rombach", "paper_title": "High-Resolution Image Synthesis with Latent Diffusion Models", "publication_date": "2022-01-01", "reason": "This paper introduces latent diffusion models, which have become a cornerstone technique in image and video generation, enabling high-resolution synthesis with reduced computational demands."}, {"fullname_first_author": "Jonathan Ho", "paper_title": "Denoising Diffusion Probabilistic Models", "publication_date": "2020-01-01", "reason": "This foundational work introduces denoising diffusion probabilistic models, a crucial technique enabling much of the recent progress in generative modeling."}, {"fullname_first_author": "William Peebles", "paper_title": "Scalable Diffusion Models with Transformers", "publication_date": "2023-01-01", "reason": "This paper explores the integration of transformers with diffusion models, which is critical for achieving scalability and high performance in video generation."}, {"fullname_first_author": "Jared Kaplan", "paper_title": "Scaling Laws for Neural Language Models", "publication_date": "2020-01-01", "reason": "This paper establishes scaling laws, which are empirical relationships guiding the training and performance of large language models; understanding these is crucial for generative model advancements."}]}