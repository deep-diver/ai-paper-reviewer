[{"content": "| Type | Method | Background Preservation PSNR \u2191 | Background Preservation MSE \u2193 \u00d7 10\u2074 | CLIP Semantics Whole \u2191 | CLIP Semantics Edited \u2191 | Runtime (seconds) \u2193 |\n|---|---|---|---|---|---|---|\n| Multi-step (50 steps) | DDIM + P2P | 17.87 | 219.88 | 25.01 | 22.44 | 25.98 |\n|  | NT-Inv + P2P | 27.03 | 35.86 | 24.75 | 21.86 | 134.06 |\n|  | DDIM + MasaCtrl | 22.17 | 86.97 | 23.96 | 21.16 | 23.21 |\n|  | Direct Inversion + MasaCtrl | 22.64 | 81.09 | 24.38 | 21.35 | 29.68 |\n|  | DDIM + P2P-Zero | 20.44 | 144.12 | 22.80 | 20.54 | 35.57 |\n|  | Direct Inversion + P2P-Zero | 21.53 | 127.32 | 23.31 | 21.05 | 35.34 |\n|  | DDIM + PnP | 22.28 | 83.64 | 25.41 | 22.55 | 12.62 |\n|  | Direct Inversion + PnP | 22.46 | 80.45 | 25.41 | 22.62 | 12.79 |\n| Few-steps (4 steps) | ReNoise (SDXL Turbo) | 20.28 | 54.08 | 24.29 | 21.07 | 5.11 |\n|  | TurboEdit | 22.43 | 9.48 | 25.49 | 21.82 | 1.32 |\n|  | ICD (SD 1.5) | 26.93 | 3.32 | 22.42 | 19.07 | 1.62 |\n| One-step | SwiftEdit (Ours) | 23.33 | 6.60 | 25.16 | 21.25 | 0.23 |\n|  | SwiftEdit (Ours with GT masks) | 23.31 | 6.18 | 25.56 | 21.91 | 0.23 |", "caption": "Table 1: Quantitative comparison of SwiftEdit against other editing methods with metrics employed from PieBench [11].", "description": "This table presents a quantitative comparison of the proposed SwiftEdit method against various other image editing techniques.  The comparison uses metrics from the PieBench benchmark [11], specifically focusing on background preservation (PSNR and MSE), and editing semantics (CLIP Whole and CLIP Edited).  Runtime is also included, highlighting the speed advantage of SwiftEdit.  The methods are categorized into multi-step (50 steps), few-step (4 steps), and one-step approaches, enabling a clear comparison of performance across different methodologies.", "section": "5. Experiments"}, {"content": "| Method | PSNR \u2191 | LPIPS \u2193 \u00d7 10\u00b3 | MSE \u2193 \u00d7 10\u2074 | SSIM \u2191 \u00d7 10\u00b2 |\n|---|---|---|---|---|\n| w/o stage 1 | 22.26 | 111.57 | 7.03 | 72.39 |\n| w/o stage 2 | 17.95 | 305.23 | 17.46 | 55.97 |\n| w/o IP-Adapter | 18.57 | 165.78 | 16.11 | 63.87 |\n| Full Setting (Ours) | **24.35** | **89.69** | **4.59** | **76.34** |", "caption": "Table 2: Impact of inversion framework design on real image reconstruction.", "description": "This table presents the ablation study results on the effect of different components in the proposed one-step inversion framework on real image reconstruction. It shows the performance of the model with different components removed, such as the first stage of training, the second stage of training, and the IP-Adapter. The metrics used for evaluation are PSNR, LPIPS, MSE, and SSIM. This allows for a quantitative assessment of the contribution of each component to the overall performance of the framework.", "section": "4.1 Inversion Network and Two-stage Training"}, {"content": "| Setting | <math>\\mathcal{L}_{regr}^{stage1}</math> | <math>\\mathcal{L}_{regu}^{stage2}</math> | Whole (<math>\\uparrow</math>) | Edited(<math>\\uparrow</math>) |\n|---|---|---|---|---|\n| Setting 1 | \u2717 | \u2717 | 22.91 | 19.07 |\n| Setting 2 | \u2717 | \u2713 | 22.98 | 19.01 |\n| Setting 3 | \u2713 | \u2717 | 24.19 | 20.55 |\n| Setting 4 (Full) | \u2713 | \u2713 | **25.16** | **21.25** |", "caption": "Table 3: Effect of loss on editing semantics score.", "description": "This table shows the impact of different loss functions on the editing semantics score in the SwiftEdit model.  Specifically, it compares the CLIP scores (Whole and Edited) achieved when including or excluding the reconstruction loss from Stage 1 (Cstage1) and the perceptual and regularization losses from Stage 2 (Cstage2). This demonstrates the contribution of each loss component to the model's performance in producing edits aligned with text prompts.", "section": "4. Proposed Method"}]