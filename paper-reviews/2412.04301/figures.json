[{"figure_path": "https://arxiv.org/html/2412.04301/x2.png", "caption": "Figure 1: \nSwiftEdit empowers instant, localized image editing using only text prompts, freeing users from the need to define masks. In just 0.23 seconds on a single A100 GPU, it unlocks a world of creative possibilities demonstrated across diverse editing scenarios.", "description": "This figure showcases SwiftEdit's ability to perform fast and localized image edits using only text prompts.  It eliminates the need for users to manually define masks.  Three example edits are shown, highlighting the diverse range of editing tasks SwiftEdit can handle. The speed of the process is emphasized: each edit takes only 0.23 seconds on a single A100 GPU. The examples demonstrate editing actions such as changing objects within an image (apples to puppies), changing the scene (forest to sea), and modifying object attributes (closed mouth to open mouth). This visually demonstrates the core functionality and efficiency of SwiftEdit.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2412.04301/x3.png", "caption": "Figure 2: \nComparing our one-step SwiftEdit with few-step and multi-step diffusion editing methods in terms of background preservation (PSNR), editing semantics (CLIP score), and runtime. Our method delivers lightning-fast text-guided editing while achieving competitive results.", "description": "This figure compares SwiftEdit's performance against other state-of-the-art image editing methods.  It showcases how SwiftEdit, a one-step diffusion-based method, achieves significantly faster editing speeds (runtime) than the multi-step and few-step alternatives while maintaining competitive performance in terms of image quality.  Specifically, it measures background preservation using the Peak Signal-to-Noise Ratio (PSNR) and assesses the accuracy of the edits by using the CLIP score. The chart visually demonstrates that SwiftEdit achieves superior results in terms of speed without significant compromise in the quality of the edits.", "section": "2. Related Work"}, {"figure_path": "https://arxiv.org/html/2412.04301/x4.png", "caption": "Figure 3: Proposed two-stage training for our one-step inversion framework. In stage 1, we warms up our inversion network on synthetic data generated by SwiftBrushv2. At stage 2, we shift our focus to real images, enabling our inversion framework to instantly invert any input images without additional fine-tuning or retraining.", "description": "This figure illustrates the two-stage training process for a one-step inversion network. Stage 1 uses synthetic data generated by the SwiftBrushv2 model to pre-train the network. This initial training helps the network learn fundamental image features and representations. In Stage 2, the training shifts to real images, allowing the network to adapt to the variations and complexities of real-world data. This two-stage approach enables the network to instantly invert any input image, without requiring further fine-tuning or retraining, achieving efficient and versatile image inversion for various applications.", "section": "4. Proposed Method"}, {"figure_path": "https://arxiv.org/html/2412.04301/x5.png", "caption": "Figure 4: Comparison of inverted noise predicted by our inversion network when trained without and with stage 2 regularization loss.", "description": "This figure compares the inverted noise predicted by the one-step inversion network trained with and without the stage 2 regularization loss.  The stage 2 training incorporates real images and a perceptual loss, aiming to improve the quality of reconstruction while maintaining the ability of the model to produce editable noise.  The figure visually demonstrates the impact of this additional training on the characteristics of the inverted noise, highlighting the differences that impact image editing capabilities.", "section": "4.1 Inversion Network and Two-stage Training"}, {"figure_path": "https://arxiv.org/html/2412.04301/x6.png", "caption": "(a) Self-guided editing mask extraction. Given source and editing prompts, our inversion network predicts two different noise maps, highlighting the editing regions M\ud835\udc40Mitalic_M.", "description": "The figure illustrates how the model generates an editing mask.  Two noise maps are produced by the inversion network, one using the source prompt and another using the editing prompt. By comparing these two maps, the model automatically identifies regions that need modification, creating a mask (represented as M) to highlight these areas. This mask is then used to guide the editing process, ensuring that edits are only applied to the intended regions while preserving the rest of the image.", "section": "4.2. Attention Rescaling for Mask-aware Editing (ARM)"}, {"figure_path": "https://arxiv.org/html/2412.04301/x7.png", "caption": "(b) Effect of global scale and our edit-aware scale. Comparison of edited results between varying global image condition scale s\ud835\udc31subscript\ud835\udc60\ud835\udc31s_{{\\bf x}}italic_s start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT with our ARaM.", "description": "This figure compares image editing results using two different methods: a global scaling approach (varying the global image condition scale s<sub>x</sub>) and the proposed edit-aware attention rescaling method (ARaM).  It demonstrates that ARaM offers better control over the editing process, leading to higher-quality edits with improved background preservation. The figure visually showcases several edited images to highlight the differences between these two approaches.", "section": "4.2. Attention Rescaling for Mask-aware Editing (ARM)"}, {"figure_path": "https://arxiv.org/html/2412.04301/x8.png", "caption": "(c) Effect of editing strength scale. Visualization of edited results when varying mask-based text-alignment scale sysubscript\ud835\udc60\ud835\udc66s_{y}italic_s start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT.", "description": "This figure demonstrates how the parameter s<sub>y</sub>, which controls the strength of the text-based edit, affects the editing outcome.  By varying s<sub>y</sub>,  the model can adjust the intensity of the change applied to the image, allowing for fine-grained control over the edit's impact.  Higher values of s<sub>y</sub> result in more pronounced edits, while lower values produce subtler modifications. The editing mask, which isolates the area to be modified, is assumed to be provided or generated beforehand.", "section": "4.2. Attention Rescaling for Mask-aware Editing (ARM)"}, {"figure_path": "https://arxiv.org/html/2412.04301/x9.png", "caption": "Figure 5: Illustration of Attention Rescaling for Mask-aware Editing (ARaM). We apply attention rescaling with our self-guided editing mask to achieve local image editing and enable editing strength control.", "description": "Figure 5 demonstrates the Attention Rescaling for Mask-aware Editing (ARaM) technique.  ARaM uses a mask (which can be user-provided or automatically generated by the model) to control the influence of the source image features during the editing process.  This allows for localized edits while preserving the background. Subfigure (a) shows how the mask is extracted using the difference in inverted noise maps generated with source and edit prompts. Subfigure (b) compares the effects of using a global scaling factor versus ARaM, highlighting the improved preservation of background elements and clearer editing semantics with ARaM. Subfigure (c) illustrates how the editing strength can be further controlled via the scaling factor applied to the mask within the editing region.", "section": "4. Proposed Method"}, {"figure_path": "https://arxiv.org/html/2412.04301/x10.png", "caption": "Figure 6: Comparative edited results. The first column shows the source image, while source and edit prompts are noted under each row.", "description": "Figure 6 presents a comparison of image editing results using various methods. Each row showcases a source image and its edits using different techniques: NT + P2P, DDIM + P2P, Pix2Pix-Zero, MasaCtrl, Plug-and-Play, ReNoise, TurboEdit, ICD (SD 1.5), and SwiftEdit (the proposed method). The source and edit prompts used for each method are specified below each image.", "section": "5.2. Comparison with Prior Methods"}, {"figure_path": "https://arxiv.org/html/2412.04301/x11.png", "caption": "Figure 7: User Study.", "description": "This figure presents the results of a user study comparing SwiftEdit with two other image editing methods: Null-text Inversion and TurboEdit.  140 participants evaluated the editing results based on background preservation and editing semantics using 20 random edit prompts from the PieBench dataset.  The chart shows the preference rate for each method, demonstrating SwiftEdit's superior performance.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.04301/x12.png", "caption": "Figure 8: Qualitative results when combining our inversion framework with other one-step text-to-image generation models.", "description": "This figure shows qualitative results of using different one-step text-to-image generation models combined with the proposed one-step inversion framework.  Each row demonstrates the source image and the edited image results using four different models (InstaFlow, DMDv2, SBv1, SBv2), highlighting how the framework adapts to different model architectures and achieves varied results in image editing across different models.", "section": "9. Additional Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2412.04301/x13.png", "caption": "(a) Varying seditsubscript\ud835\udc60edits_{\\text{edit}}italic_s start_POSTSUBSCRIPT edit end_POSTSUBSCRIPT scale at different levels of snon-editsubscript\ud835\udc60non-edits_{\\text{non-edit}}italic_s start_POSTSUBSCRIPT non-edit end_POSTSUBSCRIPT with default sy=2subscript\ud835\udc60\ud835\udc662s_{y}=2italic_s start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT = 2.", "description": "This figure shows the effect of varying the scale parameter, *s<sub>edit</sub>*, on background preservation and editing semantics.  The x-axis represents *s<sub>edit</sub>*, ranging from 0 to 1.  Multiple lines are plotted, each representing a different value of the *s<sub>non-edit</sub>* parameter (0.2, 0.4, 0.6, 0.8, and 1). The y-axis on the left shows the PSNR (Peak Signal-to-Noise Ratio), a measure of background preservation, and the y-axis on the right shows the CLIP-Edited score, representing editing semantics. The default value of *s<sub>y</sub>* is set to 2.  The plot demonstrates the trade-off between background preservation and editing quality as *s<sub>edit</sub>* changes for various levels of *s<sub>non-edit</sub>*.  Specifically, lower *s<sub>edit</sub>* values generally lead to better editing semantics but slightly compromise background preservation, whereas higher *s<sub>edit</sub>* values enhance background preservation but may not be optimal for edits.", "section": "4.2. Attention Rescaling for Mask-aware Editing (ARM)"}, {"figure_path": "https://arxiv.org/html/2412.04301/x14.png", "caption": "(b) Varying sysubscript\ud835\udc60\ud835\udc66s_{y}italic_s start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT scale at different levels of snon-editsubscript\ud835\udc60non-edits_{\\text{non-edit}}italic_s start_POSTSUBSCRIPT non-edit end_POSTSUBSCRIPT with default sedit=0subscript\ud835\udc60edit0s_{\\text{edit}}=0italic_s start_POSTSUBSCRIPT edit end_POSTSUBSCRIPT = 0.", "description": "This figure shows the effects of varying the scaling factor s<sub>y</sub> on background preservation and editing semantics at different levels of s<sub>non-edit</sub>, while keeping s<sub>edit</sub> constant at 0.  It visually demonstrates how changes in s<sub>y</sub>, which controls the strength of text-image alignment in the editing mask, impact the balance between preserving the original image's background and achieving the desired edits based on the text prompt. The graph plots PSNR (Peak Signal-to-Noise Ratio), reflecting background preservation quality, and CLIP-Edited score, measuring how well the edits match the prompt's semantics.  Different lines represent different values of s<sub>non-edit</sub>, illustrating how the interaction between s<sub>y</sub> and s<sub>non-edit</sub> affects the overall result.", "section": "4.2. Attention Rescaling for Mask-aware Editing (ARM)"}, {"figure_path": "https://arxiv.org/html/2412.04301/x15.png", "caption": "Figure 9: Effects on background preservation and editing semantics while varying seditsubscript\ud835\udc60edits_{\\text{edit}}italic_s start_POSTSUBSCRIPT edit end_POSTSUBSCRIPT and sysubscript\ud835\udc60\ud835\udc66s_{y}italic_s start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT at different levels of snon-editsubscript\ud835\udc60non-edits_{\\text{non-edit}}italic_s start_POSTSUBSCRIPT non-edit end_POSTSUBSCRIPT.", "description": "This figure analyzes the effects of varying the scaling factors  `sedit` and `sy` on background preservation and editing semantics, while holding `snon-edit` constant at different levels.  The plots show how changes in `sedit` (controlling edit strength) and `sy` (controlling text alignment) impact PSNR (background preservation) and CLIP-Edited scores (editing semantics) under various `snon-edit` settings. The goal is to illustrate the interplay between these factors in achieving a balance between preserving the original image and successfully implementing the edits.", "section": "Additional Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2412.04301/x16.png", "caption": "Figure 10: Visualization of our extracted mask along with edited results using guided text described under each image row.", "description": "This figure visualizes the self-guided editing masks generated by the model alongside the corresponding edited images.  Each row displays a source image, the generated mask highlighting the areas targeted for editing, and the final edited image. The text below each row provides the original and target prompts, showing how the model interprets and applies the text instructions for image manipulation. The masks illustrate the model's understanding of which regions need modification to achieve the desired edit based on the text prompts.", "section": "4.2. Attention Rescaling for Mask-aware Editing (ARM)"}, {"figure_path": "https://arxiv.org/html/2412.04301/x17.png", "caption": "Figure 11: Edit images with flexible prompting. SwiftEdit achieves satisfactory reconstructed and edited results with flexible source and edit prompt input (denoted under each image).", "description": "Figure 11 showcases SwiftEdit's capability to handle flexible prompting during image editing.  The figure presents several examples where both the source prompt (describing the original image) and the edit prompt (specifying the desired changes) are varied, demonstrating the model's ability to successfully reconstruct and edit images even with minimal or nuanced descriptions. The results highlight the robustness and flexibility of the SwiftEdit model in responding accurately to different and flexible prompt combinations.", "section": "10. More Qualitative Results"}, {"figure_path": "https://arxiv.org/html/2412.04301/x18.png", "caption": "Figure 12: Face identity and expression editing via simple prompts. Given a portrait input image, SwiftEdit can perform a variety of facial identities along with expression editing scenarios guided by simple text within just 0.23 seconds.", "description": "Figure 12 showcases SwiftEdit's capability to perform face identity and expression editing using simple text prompts.  Starting with a single portrait image, various facial identities (e.g., Beckham, Ronaldo) and expressions (e.g., smiling, angry) can be generated by simply providing the desired identity and expression in the text prompt. The entire process is completed in a mere 0.23 seconds, highlighting SwiftEdit's speed and efficiency.", "section": "5. Experiments"}]