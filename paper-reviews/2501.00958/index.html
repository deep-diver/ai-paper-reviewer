<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><title>2.5 Years in Class: A Multimodal Textbook for Vision-Language Pretraining &#183; HF Daily Paper Reviews by AI</title>
<meta name=title content="2.5 Years in Class: A Multimodal Textbook for Vision-Language Pretraining &#183; HF Daily Paper Reviews by AI"><meta name=description content="New multimodal textbook dataset boosts Vision-Language Model (VLM) performance!"><meta name=keywords content="Multimodal Learning,Vision-Language Models,üè¢ College of Computer Science and Technology,Zhejiang University,"><link rel=canonical href=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.00958/><link type=text/css rel=stylesheet href=/ai-paper-reviewer/css/main.bundle.min.595affd4445a931ea6d6e3a5a3c709930fa52a60be10b21c6f81fdb8fecaacea33aacedf80cdc88be45f189be14ed4ce53ea74a1e1406fad9cbf90c5ed409173.css integrity="sha512-WVr/1ERakx6m1uOlo8cJkw+lKmC+ELIcb4H9uP7KrOozqs7fgM3Ii+RfGJvhTtTOU+p0oeFAb62cv5DF7UCRcw=="><script type=text/javascript src=/ai-paper-reviewer/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script><script defer type=text/javascript id=script-bundle src=/ai-paper-reviewer/js/main.bundle.min.efbf3b6b987689fffaf2d7b73173d2690c0279a04d444b0537a77d7f4ff6e6d493445400cb0cf56bc0f0f123e19f15394e63cae34e67f069bd013dd5c73df56e.js integrity="sha512-7787a5h2if/68te3MXPSaQwCeaBNREsFN6d9f0/25tSTRFQAywz1a8Dw8SPhnxU5TmPK405n8Gm9AT3Vxz31bg==" data-copy data-copied></script><script src=/ai-paper-reviewer/lib/zoom/zoom.min.37d2094687372da3f7343a221a470f6b8806f7891aa46a5a03966af7f0ebd38b9fe536cb154e6ad28f006d184b294525a7c4054b6bbb4be62d8b453b42db99bd.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ=="></script><link rel=apple-touch-icon sizes=180x180 href=/ai-paper-reviewer/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/ai-paper-reviewer/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/ai-paper-reviewer/favicon-16x16.png><link rel=manifest href=/ai-paper-reviewer/site.webmanifest><meta property="og:url" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.00958/"><meta property="og:site_name" content="HF Daily Paper Reviews by AI"><meta property="og:title" content="2.5 Years in Class: A Multimodal Textbook for Vision-Language Pretraining"><meta property="og:description" content="New multimodal textbook dataset boosts Vision-Language Model (VLM) performance!"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="paper-reviews"><meta property="article:published_time" content="2025-01-01T00:00:00+00:00"><meta property="article:modified_time" content="2025-01-01T00:00:00+00:00"><meta property="article:tag" content="Multimodal Learning"><meta property="article:tag" content="Vision-Language Models"><meta property="article:tag" content="üè¢ College of Computer Science and Technology, Zhejiang University"><meta property="og:image" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.00958/cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.00958/cover.png"><meta name=twitter:title content="2.5 Years in Class: A Multimodal Textbook for Vision-Language Pretraining"><meta name=twitter:description content="New multimodal textbook dataset boosts Vision-Language Model (VLM) performance!"><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"Paper Reviews by AI","name":"2.5 Years in Class: A Multimodal Textbook for Vision-Language Pretraining","headline":"2.5 Years in Class: A Multimodal Textbook for Vision-Language Pretraining","abstract":"New multimodal textbook dataset boosts Vision-Language Model (VLM) performance!","inLanguage":"en","url":"https:\/\/deep-diver.github.io\/ai-paper-reviewer\/paper-reviews\/2501.00958\/","author":{"@type":"Person","name":"Hugging Face Daily Papers"},"copyrightYear":"2025","dateCreated":"2025-01-01T00:00:00\u002b00:00","datePublished":"2025-01-01T00:00:00\u002b00:00","dateModified":"2025-01-01T00:00:00\u002b00:00","keywords":["Multimodal Learning","Vision-Language Models","üè¢ College of Computer Science and Technology, Zhejiang University"],"mainEntityOfPage":"true","wordCount":"4036"}]</script><meta name=author content="Hugging Face Daily Papers"><link href=https://github.com/deep-diver/paper-reviewer/ rel=me><link href=https://twitter.com/algo_diver/ rel=me><script src=/ai-paper-reviewer/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj+KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script><script defer src=/ai-paper-reviewer/lib/typeit/typeit.umd.1b3200cb448f5cd1f548f2781452643d3511a43584b377b82c03a58055da4fdb7bc8f6c6c2ce846480c7677ff25bfd0d75f15823c09443ab18e0fd2cad792587.js integrity="sha512-GzIAy0SPXNH1SPJ4FFJkPTURpDWEs3e4LAOlgFXaT9t7yPbGws6EZIDHZ3/yW/0NdfFYI8CUQ6sY4P0srXklhw=="></script><script defer src=/ai-paper-reviewer/lib/packery/packery.pkgd.min.js integrity></script><script type=text/javascript src=/ai-paper-reviewer/js/shortcodes/gallery.min.9b4cb28f931ed922c26fb9b2510c2debb370f6a63305050c2af81740b2919883715e24efbbdf3a081496718ec751df3a72729d4d0bc71d6071297563a97ce1ee.js integrity="sha512-m0yyj5Me2SLCb7myUQwt67Nw9qYzBQUMKvgXQLKRmINxXiTvu986CBSWcY7HUd86cnKdTQvHHWBxKXVjqXzh7g=="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-KX0S6Q55Y7"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-KX0S6Q55Y7")</script><meta name=theme-color><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-app.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-firestore.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-auth.js></script><script>const firebaseConfig={apiKey:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",authDomain:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",projectId:"neurips2024-f3065",storageBucket:"neurips2024-f3065.firebasestorage.app",messagingSenderId:"982475958898",appId:"1:982475958898:web:2147e5d7753d6ac091f0eb",measurementId:"G-YQ46HXQ9JS"};var app=firebase.initializeApp(firebaseConfig),db=firebase.firestore(),auth=firebase.auth()</script></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a></div><div class=min-h-[148px]></div><div class="fixed inset-x-0 pl-[24px] pr-[24px]" style=z-index:100><div id=menu-blur class="absolute opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl shadow-2xl"></div><div class="relative max-w-[64rem] ml-auto mr-auto"><div style=padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start space-x-3"><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/ai-paper-reviewer/ class="text-base font-medium text-gray-500 hover:text-gray-900">HF Daily Paper Reviews by AI</a></nav><nav class="hidden md:flex items-center space-x-5 md:ml-12 h-12"><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title="About This Project">About</p></a><a href=/ai-paper-reviewer/2025-02-10/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title=2025-02-10s>2025-02-10</p></a><a href=/ai-paper-reviewer/2025-02-11/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title=2025-02-11s>2025-02-11</p></a><a href=/ai-paper-reviewer/2025-02-12/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title=2025-02-12s>2025-02-12</p></a><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title="Paper Reviews by AI">Archive</p></a><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title=Tags>Tags</p></a><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></span><p class="text-base font-medium" title></p></a><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span><p class="text-base font-medium" title></p></a><button id=search-button aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button><div class="ltr:mr-14 rtl:ml-14 flex items-center"><button id=appearance-switcher aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></nav><div class="flex md:hidden items-center space-x-5 md:ml-12 h-12"><span></span>
<button id=search-button-mobile aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></button>
<button id=appearance-switcher-mobile aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400" style=margin-right:5px><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div><div class="-my-2 -mr-2 md:hidden"><label id=menu-button class=block><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper style=padding-top:5px class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li id=menu-close-button><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title="About This Project">About</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-02-10/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title=2025-02-10s>2025-02-10</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-02-11/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title=2025-02-11s>2025-02-11</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-02-12/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title=2025-02-12s>2025-02-12</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title="Paper Reviews by AI">Archive</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title=Tags>Tags</p></a></li><li class=mt-1><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li><li class=mt-1><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li></ul></div></label></div></div></div></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("menu-blur");n.style.opacity=t/300})</script><div class="relative flex flex-col grow"><main id=main-content class=grow><article><div id=hero class="h-[150px] md:h-[200px]"></div><div class="fixed inset-x-0 top-0 h-[800px] single_hero_background nozoom" style=background-image:url(/ai-paper-reviewer/paper-reviews/2501.00958/cover_hu_90c75270fbe2dde.png)><div class="absolute inset-0 bg-gradient-to-t from-neutral dark:from-neutral-800 to-transparent mix-blend-normal"></div><div class="absolute inset-0 opacity-60 bg-gradient-to-t from-neutral dark:from-neutral-800 to-neutral-100 dark:to-neutral-800 mix-blend-normal"></div></div><div id=background-blur class="fixed opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl"></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("background-blur");n.style.opacity=t/300})</script><header id=single_header class="mt-5 max-w-prose"><ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden"><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/>HF Daily Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class=inline><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/>Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/2501.00958/>2.5 Years in Class: A Multimodal Textbook for Vision-Language Pretraining</a><span class="px-1 text-primary-500">/</span></li></ol><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">2.5 Years in Class: A Multimodal Textbook for Vision-Language Pretraining</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime=2025-01-01T00:00:00+00:00>1 January 2025</time><span class="px-2 text-primary-500">&#183;</span><span>4036 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">19 mins</span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=views_paper-reviews/2501.00958/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=views>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 576 512"><path fill="currentcolor" d="M288 32c-80.8.0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7.0 24.6C17.3 304 48.6 356 95.4 399.4 142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1 3.3-7.9 3.3-16.7.0-24.6-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144 64.5-144 144-144 144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64-11.5.0-22.3-3-31.6-8.4-.2 2.8-.4 5.5-.4 8.4.0 53 43 96 96 96s96-43 96-96-43-96-96-96c-2.8.0-5.6.1-8.4.4 5.3 9.3 8.4 20.1 8.4 31.6z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=likes_paper-reviews/2501.00958/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=likes>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<button id=button_likes class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400" onclick=process_article()>
<span id=button_likes_heart style=display:none class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span><span id=button_likes_emtpty_heart class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M244 84l11.1 12 12-11.98C300.6 51.37 347 36.51 392.6 44.1 461.5 55.58 512 115.2 512 185.1V190.9c0 41.5-17.2 81.2-47.6 109.5L283.7 469.1c-7.5 7-17.4 10.9-27.7 10.9S235.8 476.1 228.3 469.1L47.59 300.4C17.23 272.1.0 232.4.0 190.9V185.1c0-69.9 50.52-129.52 119.4-141 44.7-7.59 92 7.27 124.6 39.9C243.1 84 244 84.01 244 84zm11.1 79.9-45-46.8c-21.7-20.82-52.5-30.7-82.8-25.66C81.55 99.07 48 138.7 48 185.1V190.9c0 28.2 11.71 55.2 32.34 74.4L256 429.3l175.7-164c20.6-19.2 32.3-46.2 32.3-74.4V185.1c0-46.4-33.6-86.03-79.3-93.66C354.4 86.4 323.6 96.28 301.9 117.1l-46.8 46.8z"/></svg>
</span></span><span id=button_likes_text>&nbsp;Like</span></button></span></div><div class="flex flex-row flex-wrap items-center"><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/ai-generated/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">AI Generated
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/-daily-papers/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">ü§ó Daily Papers
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/multimodal-learning/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Multimodal Learning
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/vision-language-models/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Vision-Language Models
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/-college-of-computer-science-and-technology-zhejiang-university/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">üè¢ College of Computer Science and Technology, Zhejiang University</span></span></span></div></div><div class="flex author"><img class="!mt-0 !mb-0 h-24 w-24 rounded-full ltr:mr-4 rtl:ml-4" width=96 height=96 alt="Hugging Face Daily Papers" src=/ai-paper-reviewer/img/avatar_hu_97e7d424fadd1c26.png><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">Author</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">Hugging Face Daily Papers</div><div class="text-sm text-neutral-700 dark:text-neutral-400">I am AI, and I review papers on HF Daily Papers</div><div class="text-2xl sm:text-lg"><div class="flex flex-wrap text-neutral-400 dark:text-neutral-500"><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://github.com/deep-diver/paper-reviewer/ target=_blank aria-label=Github rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://twitter.com/algo_diver/ target=_blank aria-label=Twitter rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span></a></div></div></div></div><div class=mb-5></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first lg:ml-auto px-0 lg:order-last ltr:lg:pl-8 rtl:lg:pr-8"><div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-[140px]"><details open id=TOCView class="toc-right mt-0 overflow-y-scroll overscroll-contain scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600 rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 hidden lg:block"><summary class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="min-w-[220px] py-2 border-dotted ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#multimodal-textbook>Multimodal Textbook</a></li><li><a href=#video-to-text-pipeline>Video-to-Text Pipeline</a></li><li><a href=#benchmark-results>Benchmark Results</a></li><li><a href=#context-awareness>Context Awareness</a></li><li><a href=#future-work>Future Work</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><details class="toc-inside mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 lg:hidden"><summary class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#multimodal-textbook>Multimodal Textbook</a></li><li><a href=#video-to-text-pipeline>Video-to-Text Pipeline</a></li><li><a href=#benchmark-results>Benchmark Results</a></li><li><a href=#context-awareness>Context Awareness</a></li><li><a href=#future-work>Future Work</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><script>var margin=200,marginError=50;(function(){var t=$(window),e=$("#TOCView"),s=e.height();function n(){var n=t.height()-margin;s>=n?(e.css("overflow-y","scroll"),e.css("max-height",n+marginError+"px")):(e.css("overflow-y","hidden"),e.css("max-height","9999999px"))}t.on("resize",n),$(document).ready(n)})()</script></div></div><div class="min-w-0 min-h-0 max-w-fit"><div class="article-content max-w-prose mb-20"><br><div class="flex flex-row flex-wrap items-center space-x-2"><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 48 48" fill="none"><rect width="48" height="48" fill="#fff" fill-opacity=".01"/><path d="M18 43V22c0-3.3137 2.6863-6 6-6s6 2.6863 6 6V43" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M12 40V22c0-6.6274 5.3726-12 12-12s12 5.3726 12 12V40" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M6 35V22C6 12.0589 14.0589 4 24 4s18 8.0589 18 18V35" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 44V31" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 24.625v-2.75" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/></svg>
</span></span><span>2501.00958</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg fill="#000" height="800" width="800" id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 511.999 511.999"><g><g><path d="M421.578 190.264l-99.847-99.847c-2.439-2.439-6.391-2.439-8.829.0L82.824 320.495c-2.439 2.439-2.439 6.392.0 8.829l99.847 99.847c2.439 2.439 6.391 2.439 8.829.0l230.078-230.078C424.017 196.655 424.017 192.703 421.578 190.264z"/></g></g><g><g><path d="M506.511 87.672 424.323 5.484c-7.308-7.31-19.175-7.315-26.488.0L348.219 55.1c-2.439 2.439-2.439 6.391.0 8.829l99.847 99.847c2.439 2.437 6.391 2.437 8.829.0l49.616-49.616C513.826 106.847 513.826 94.987 506.511 87.672z"/></g></g><g><g><path d="M508.133 491.11c-1.054-9.556-9.489-16.599-19.104-16.599H111.633l36.058-15.163c4.088-1.719 5.131-7.034 1.994-10.17l-86.854-86.854c-3.137-3.135-8.451-2.094-10.17 1.994C52.224 365.359 2.052 484.66 1.627 485.707c-5.815 13.208 4.855 27.01 18.107 26.263H489.52C500.566 511.97 509.379 502.408 508.133 491.11z"/></g></g></svg>
</span></span><span>Wenqi Zhang et el.</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span>ü§ó 2025-01-03</span></span></span></div></div><p><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://arxiv.org/abs/2501.00958 target=_self role=button>‚Üó arXiv
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://huggingface.co/papers/2501.00958 target=_self role=button>‚Üó Hugging Face</a></p><audio controls><source src=https://ai-paper-reviewer.com/2501.00958/podcast.wav type=audio/wav>Your browser does not support the audio element.</audio><h3 class="relative group">TL;DR<div id=tldr class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#tldr aria-label=Anchor>#</a></span></h3><div class="lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl"><p>Current Vision-Language Models (VLMs) struggle with existing datasets due to issues like low knowledge density, weak image-text relations, and poor logical coherence. These limitations hinder VLMs&rsquo; ability to understand complex concepts and reason effectively. The lack of high-quality, textbook-level multimodal data further exacerbates this problem.</p><p>This paper introduces a novel multimodal textbook corpus created from 2.5 years of instructional videos, totaling 22,000 class hours. <strong>The corpus is meticulously designed to overcome the limitations of existing datasets</strong>, featuring high-quality image-text alignment, richer foundational knowledge, and improved logical coherence between images. Extensive experiments demonstrate superior performance of VLMs pre-trained on this corpus, especially on knowledge and reasoning-intensive tasks. The dataset is made publicly available to foster further research.</p></div><h4 class="relative group">Key Takeaways<div id=key-takeaways class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#key-takeaways aria-label=Anchor>#</a></span></h4><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-0cbde29f82408d4d81073cc67692f901></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-0cbde29f82408d4d81073cc67692f901",{strings:[" A novel high-quality multimodal textbook corpus is presented, addressing limitations in existing datasets by offering better image-text coherence and knowledge density. "],speed:10,lifeLike:!0,startDelay:0,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-65de2b4b6018029c5732ef30f33deaa8></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-65de2b4b6018029c5732ef30f33deaa8",{strings:[" VLMs pretrained on this dataset show significant improvements on knowledge-intensive tasks, highlighting its effectiveness in enhancing contextual awareness and reasoning capabilities. "],speed:10,lifeLike:!0,startDelay:1e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-1ec6ae6f7bde0730d33adb96e4e89d73></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-1ec6ae6f7bde0730d33adb96e4e89d73",{strings:[" The open-access nature of the dataset promotes collaborative research and development in the field of vision-language modeling. "],speed:10,lifeLike:!0,startDelay:2e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><h4 class="relative group">Why does it matter?<div id=why-does-it-matter class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#why-does-it-matter aria-label=Anchor>#</a></span></h4><p>This paper is crucial because <strong>it addresses the limitations of existing multimodal datasets</strong> by introducing a high-quality, textbook-level corpus for VLM pretraining. Its focus on coherent image-text relations and rich foundational knowledge directly tackles the challenges faced by current VLMs, paving the way for significant advancements in knowledge-intensive tasks and reasoning capabilities. The open-access nature of the dataset further facilitates broader adoption and collaborative research.</p><hr><h4 class="relative group">Visual Insights<div id=visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#visual-insights aria-label=Anchor>#</a></span></h4><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2501.00958/x1.png alt></figure></p><blockquote><p>üîº This figure compares previous interleaved datasets (MMC4 and OBELICS) with the newly introduced multimodal textbook dataset. It highlights the shortcomings of existing datasets, such as loose image-text relationships, low knowledge density, and a lack of logical coherence in image sequences. In contrast, the multimodal textbook dataset, derived from instructional videos, boasts high-quality data with tightly coupled text-image relationships and a logically structured organization of images and text. The dataset construction process employs a coarse-to-fine knowledge extraction and multi-level filtering approach to ensure high-quality data suitable for Vision-Language Model (VLM) training.</p><details><summary>read the caption</summary>Figure 1: Previous interleaved datasets, e.g., MMC4 and OBELICS, suffer from limitations like weak text-image relations, low knowledge density, and incoherent image sequences. Our multimodal textbook, sourced from massive tutorial videos, employs coarse-to-fine knowledge extraction and multi-level filtering to create a high-quality, textbook-level dataset. It interleaves video keyframes with tutorial texts (extracted from ASR and OCR), enabling VLMs to acquire rich knowledge through tightly coupled text-image and more coherent logic.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Dataset</th><th>#Image</th><th></th><th></th><th>#Text Token</th><th></th><th></th><th><em>L</em>=4</th><th><em>L</em>=5</th><th><em>L</em>=6</th><th><em>L</em>=7</th><th><em>L</em>=8</th><th>Avg.</th><th>Source</th><th></th></tr></thead><tbody><tr><td>Min.</td><td>Max.</td><td>Avg.</td><td>Min.</td><td>Max.</td><td>Avg.</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td></tr><tr><td><em>Image-text Paired Dataset</em></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>COYO-700M</td><td>1</td><td>1</td><td>1</td><td>1</td><td>811</td><td>16</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>Common Crawl</td><td></td></tr><tr><td>LAION-5B</td><td>1</td><td>1</td><td>1</td><td>6</td><td>683</td><td>27</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>Common Crawl</td><td></td></tr><tr><td><em>Image-text Interleaved Dataset</em></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>MMC4</td><td>0</td><td>117</td><td>5.7</td><td>4</td><td>16715</td><td>417</td><td>0.363</td><td>0.348</td><td>0.310</td><td>0.298</td><td>0.276</td><td>0.319</td><td>Common Crawl</td><td></td></tr><tr><td>MMC4-core-ff</td><td>0</td><td>15</td><td>4.1</td><td>15</td><td>16715</td><td>329</td><td>0.431</td><td>0.406</td><td>0.404</td><td>0.403</td><td>0.396</td><td>0.407</td><td>Common Crawl</td><td></td></tr><tr><td>OBELICS</td><td>1</td><td>30</td><td>2.5</td><td>12</td><td>10717</td><td>816</td><td>0.366</td><td>0.351</td><td>0.339</td><td>0.337</td><td>0.336</td><td>0.345</td><td>Common Crawl</td><td></td></tr><tr><td>OmniCorpus*</td><td>1</td><td>16</td><td>3.9</td><td>14</td><td>6893</td><td>574</td><td>0.358</td><td>0.329</td><td>0.310</td><td>0.305</td><td>0.301</td><td>0.321</td><td>Multi-sources</td><td></td></tr><tr><td><strong>Ours</strong></td><td><strong>2</strong></td><td><strong>45</strong></td><td><strong>10.7</strong></td><td><strong>11</strong></td><td><strong>34174</strong></td><td><strong>1297</strong></td><td><strong>0.687</strong></td><td><strong>0.697</strong></td><td><strong>0.698</strong></td><td><strong>0.688</strong></td><td><strong>0.662</strong></td><td><strong>0.686</strong></td><td>Video Website</td><td></td></tr></tbody></table></table></figure><blockquote><p>üîº This table compares the characteristics of various multimodal datasets, focusing on the distribution of images and text tokens. It contrasts the proposed &lsquo;Multimodal Textbook&rsquo; dataset with existing image-text paired datasets and interleaved datasets commonly used for vision-language model training. Key metrics include the number of images and text tokens per sample, as well as a newly introduced metric, &lsquo;In-sample Image SIML&rsquo;, which quantifies the semantic and structural correlation among multiple images within a single sample from an interleaved dataset. This helps assess the coherence and logical relationships between images within a sample. The table also notes that due to its size, the OmniCorpus dataset was analyzed using a randomly sampled subset.</p><details><summary>read the caption</summary>Table 1: We compare our multimodal textbook with image-text paired datasets and webpage-centric interleaved datasets in terms of image and text distributions. In-sample Image SIMLsuperscriptSIMùêø\text{SIM}^{L}SIM start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT measures the semantic and structural correlation between multiple images within an interleaved sample. OmniCorpus‚àósuperscriptOmniCorpus\text{OmniCorpus}^{*}OmniCorpus start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT: Due to the extensive size of the dataset, we perform statistical analysis on a randomly sampled subset.</details></blockquote><h3 class="relative group">In-depth insights<div id=in-depth-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#in-depth-insights aria-label=Anchor>#</a></span></h3><h4 class="relative group">Multimodal Textbook<div id=multimodal-textbook class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#multimodal-textbook aria-label=Anchor>#</a></span></h4><p>The concept of a &ldquo;Multimodal Textbook&rdquo; in the context of vision-language pre-training is innovative. It leverages the richness of instructional videos to create a <strong>high-quality, knowledge-dense dataset</strong>. Unlike typical image-text pairs or web-crawled interleaved corpora, which often suffer from weak image-text alignment and logical coherence, this approach offers <strong>tightly coupled text-image sequences</strong> extracted from videos. This results in a <strong>coherent learning experience</strong>, allowing models to grasp foundational knowledge and develop stronger reasoning capabilities. The <strong>systematic data collection process</strong>, employing LLMs for taxonomy creation and video filtering, ensures both efficiency and quality control. The resulting dataset showcases improved performance on knowledge-intensive and reasoning tasks. The <strong>multi-level extraction and filtering</strong> process, encompassing ASR, OCR, and keyframe selection, is noteworthy. This ensures both visual and textual information is comprehensively integrated, enhancing model training. <strong>Temporal alignment</strong> between images and text is a crucial aspect, enabled by video timestamps, fostering better context awareness.</p><h4 class="relative group">Video-to-Text Pipeline<div id=video-to-text-pipeline class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#video-to-text-pipeline aria-label=Anchor>#</a></span></h4><p>A robust video-to-text pipeline is crucial for processing large volumes of instructional videos. <strong>Effective extraction of both audio and visual information is paramount</strong>, requiring advanced techniques like Automatic Speech Recognition (ASR) for transcribing lectures and Optical Character Recognition (OCR) for extracting text from slides or on-screen content. The pipeline should also incorporate <strong>multi-level filtering</strong> to remove noise, irrelevant segments (e.g., advertisements), and low-quality data. <strong>Temporal alignment between extracted text and visual keyframes</strong> is essential to create a coherent multimodal dataset. <strong>Strategies for handling long videos</strong> (e.g., segmentation into shorter clips) must be implemented to ensure compatibility with various vision-language models. Finally, <strong>quality control and validation steps</strong> should be integrated to evaluate the final output&rsquo;s accuracy and coherence, ultimately leading to a high-quality multimodal textbook suitable for pre-training vision-language models.</p><h4 class="relative group">Benchmark Results<div id=benchmark-results class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#benchmark-results aria-label=Anchor>#</a></span></h4><p>A dedicated &lsquo;Benchmark Results&rsquo; section in a research paper is crucial for evaluating the proposed model&rsquo;s performance. It should present a clear comparison against existing state-of-the-art methods on relevant benchmark datasets. <strong>The choice of benchmarks is vital</strong>: they must be established and widely recognized within the field to ensure credibility. The results should be presented comprehensively, including metrics such as accuracy, precision, recall, F1-score, etc., depending on the task. <strong>Visualizations like tables and graphs are highly recommended</strong> to aid readers in quickly understanding the comparative performance. Beyond raw numbers, the discussion should analyze the results, highlighting <strong>strengths and weaknesses</strong> of the proposed model relative to the baselines. <strong>Statistical significance testing is important</strong> to ensure that observed differences are not due to random chance. A thorough analysis of the benchmark results contributes significantly to the paper&rsquo;s overall impact and persuasiveness, establishing the novelty and value of the proposed approach.</p><h4 class="relative group">Context Awareness<div id=context-awareness class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#context-awareness aria-label=Anchor>#</a></span></h4><p>Context awareness in vision-language models (VLMs) is crucial for bridging the semantic gap between visual and textual data. <strong>Effective VLMs should not only process individual image-text pairs in isolation but also understand the relationships and connections between them within a broader context.</strong> This is especially important for complex tasks requiring reasoning and inference, where understanding the temporal and sequential order of information is key. The paper highlights how existing interleaved datasets often suffer from weak text-image relations, poor logical coherence, and low knowledge density, limiting the ability of VLMs to achieve true context awareness. <strong>The proposed multimodal textbook addresses these shortcomings by providing a highly structured and coherent corpus, with tightly coupled image-text sequences derived from instructional videos.</strong> This ensures a more natural and intuitive learning process, enabling VLMs to better grasp foundational knowledge and perform exceptionally well in knowledge-intensive tasks. The ‚Äúcheat test‚Äù experiments further underscore the importance of context awareness, revealing that models trained on the proposed dataset exhibit a superior ability to leverage interleaved contextual cues for task solving. <strong>This superior performance strongly suggests that a well-structured, context-rich multimodal corpus is vital for fostering true context awareness in VLMs.</strong> Future research should explore how to further enhance context modeling techniques in VLMs and develop more sophisticated evaluation metrics for assessing context-aware capabilities.</p><h4 class="relative group">Future Work<div id=future-work class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#future-work aria-label=Anchor>#</a></span></h4><p>The authors mention several avenues for future work, primarily centered on <strong>improving the dataset</strong> and <strong>extending the model&rsquo;s capabilities</strong>. Improving the dataset could involve refining the knowledge taxonomy used for video selection, leading to more focused and relevant instructional content. <strong>Enhancing the data extraction pipeline</strong> is also crucial, aiming for higher quality ASR and OCR outputs, minimizing noise and errors, and potentially incorporating additional modalities beyond visual and textual data such as interactive elements or student feedback. Regarding model capabilities, <strong>exploring different model architectures</strong> beyond those tested could unveil performance benefits. <strong>Investigating different pretraining strategies</strong> or using alternative training objectives may enhance the VLM&rsquo;s performance. Finally, a significant area for future research involves <strong>more comprehensive evaluation</strong>. The authors could expand their evaluation beyond existing benchmarks, incorporating a wider range of tasks, including those focusing on open-world scenarios and assessing the model&rsquo;s robustness to noisy data. In addition, qualitative assessments could provide further insights into the model&rsquo;s understanding and reasoning abilities.</p><h3 class="relative group">More visual insights<div id=more-visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#more-visual-insights aria-label=Anchor>#</a></span></h3><details><summary>More on figures</summary><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2501.00958/x2.png alt></figure></p><blockquote><p>üîº This figure illustrates the process of creating a multimodal textbook from instructional videos. It begins with using LLMs to create a knowledge taxonomy, which is then used to collect and filter 159,000 instructional videos from the internet. A multi-stage pipeline then processes these videos. First, non-instructional videos are removed based on their Automatic Speech Recognition (ASR) transcripts, leaving 75,000 high-quality videos. These videos are then segmented into shorter clips using timestamps from the ASR, removing clips with misaligned visuals and audio. Finally, keyframes are extracted from each clip, and Optical Character Recognition (OCR) is used to extract text and symbols from the keyframes. The result is a multimodal textbook containing 6.5 million keyframes, 259 million ASR tokens, and 500 million OCR tokens, organized in an image-text interleaved format.</p><details><summary>read the caption</summary>Figure 2: An illustration of constructing a multimodal textbook from instructional videos. We first instruct LLMs to construct a knowledge taxonomy, then retrieve and filter videos at metadata level, collecting 159K instructional videos. Then a video-to-textbook pipeline is designed for multi-level knowledge extraction. ‚ë† We filter out non-instructional videos using ASR transcripts, retaining 75K high-quality videos. ‚ë° We use ASR‚Äôs timestamp to segment long videos into short clips, discarding those with misaligned visuals and ASR. ‚ë¢ We detect keyframes from each clip and extract text and symbols by OCR. Our pipeline produces 6.5M keyframes, 259M ASR, and 500M OCR tokens and organizes them into an image-text interleaved textbook.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2501.00958/extracted/6106116/sec/fig/fig3.png alt></figure></p><blockquote><p>üîº This figure displays the effect of shuffling image order within samples on the performance of vision-language models (VLMs). Three different datasets (MMC4-Core-ff, OBELICS, and the authors&rsquo; Textbook-6.5M) were used. For each dataset, 20%, 50%, and 100% of the samples were randomly selected, and the image order within those samples was shuffled. The resulting datasets with shuffled images were then used for pretraining VLMs. The y-axis shows the average accuracy across seven benchmark tasks. The x-axis indicates the percentage of samples that underwent image shuffling (0%, 20%, 50%, 100%). The graph visually demonstrates how sensitive each dataset is to changes in image order, offering insights into the influence of image coherence on VLM performance.</p><details><summary>read the caption</summary>Figure 3: We randomly select 20%, 50%, and 100% samples from datasets and shuffle the image order within each sample. These datasets with shuffled images are also used for pretraining. The Accuracy denotes the average of seven benchmarks.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2501.00958/x3.png alt></figure></p><blockquote><p>üîº This figure visualizes the structure of the knowledge taxonomy used in the paper. The top part shows a hierarchical tree diagram of six main subjects (Mathematics, Physics, Chemistry, Earth Science, Engineering, and Computer Science). Each subject branches out into several sub-courses, representing more specific areas of study within the subject. Due to space limitations, only the sub-courses with the highest proportions (most videos) are shown for each subject. The bottom part displays bar charts, one for each subject, showing the distribution of knowledge points across the sub-courses within that subject. This gives a visual representation of the breadth and depth of the educational content included in the dataset.</p><details><summary>read the caption</summary>Figure 4: Top: We plot six subjects along with their corresponding sub-courses. Due to space constraints, we selectively visualized only the courses with the highest proportions. Bottom: We count the knowledge points distribution belongs to each subject and its course</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2501.00958/x4.png alt></figure></p><blockquote><p>üîº This figure shows a visualization of the water cycle. The image depicts the various stages of the water cycle including evaporation from bodies of water, condensation forming clouds, precipitation in the form of rain, surface runoff, infiltration into the ground, and groundwater flow. The illustration highlights the interconnectedness of these processes and how water continuously circulates within the Earth&rsquo;s system. This example is presented as part of an earth science lesson within the multimodal textbook.</p><details><summary>read the caption</summary>Figure 5: A case presented in our textbook illustrates the water cycle within the domain of earth science.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2501.00958/x5.png alt></figure></p><blockquote><p>üîº Figure 6 showcases an example from the textbook illustrating the principles of mechanics in physics. It presents a step-by-step calculation of acceleration using the formula (v-u)/t, where &lsquo;v&rsquo; is the final velocity, &lsquo;u&rsquo; is the initial velocity, and &rsquo;t&rsquo; is the time taken. The example problem involves an object initially at rest accelerating to a certain velocity, visually depicted with keyframes and corresponding text explanations of the calculations. Further, it explains the concept of acceleration units and its derivation from velocity and time. The figure demonstrates the use of both visual and textual cues to clarify the physical principle within the context of an instructional video.</p><details><summary>read the caption</summary>Figure 6: A case presented in our textbook introducing the principles of mechanics within the domain of physics.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2501.00958/x6.png alt></figure></p><blockquote><p>üîº This figure shows an example from the textbook that uses two objects with different masses (10kg and 100kg) to illustrate the concept of inertia. A constant force of 50N is applied to both objects. The resulting accelerations are calculated using Newton&rsquo;s second law (F=ma), demonstrating how a larger mass leads to a smaller acceleration. It then expands on the concept by comparing the rotational inertia of a thin hoop and a solid disk with the same mass and radius, prompting reflection on how mass distribution affects rotational inertia. This section visually explains the relationship between mass and inertia, in both linear and rotational motion.</p><details><summary>read the caption</summary>Figure 7: A case presented in our textbook introducing the concepts of velocity and acceleration within the context of physics.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2501.00958/x7.png alt></figure></p><blockquote><p>üîº Figure 8 presents a step-by-step solution to a planar geometry problem. The problem involves finding the area of a rectangle inscribed within a semicircle. The solution methodically uses geometric principles, including properties of isosceles right triangles, inscribed angles, and central angles, to determine the dimensions of the rectangle and calculate its area. The figure is highly visual, using diagrams and annotations to guide the reader through each step of the reasoning process. This showcases how the textbook uses a combination of visual and textual explanations to teach mathematical concepts.</p><details><summary>read the caption</summary>Figure 8: A case presented in our textbook demonstrates how to solve a question about planar geometry in the domain of mathematics.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2501.00958/x8.png alt></figure></p><blockquote><p>üîº This figure from the section &lsquo;Analysis of Multimodal Textbook&rsquo; shows a chemistry lesson explaining the difference between atoms and molecules using examples of helium, hydrogen, and water. It helps to illustrate that atoms are the basic building blocks of matter, while molecules are formed when two or more atoms are chemically bonded. The figure visually depicts atoms (He, Ne) and molecules (H2, O2, CO2, H2O) clarifying pure substances (elements and compounds) using diagrams and text annotations.</p><details><summary>read the caption</summary>Figure 9: A case presented in our textbook illustrates the concepts of molecules, atoms, and compounds in the domain of chemistry.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2501.00958/x9.png alt></figure></p><blockquote><p>üîº This figure shows a step-by-step visual explanation of the Depth-First Search (DFS) algorithm, a graph traversal algorithm. The visualization uses a simple graph and highlights the process of visiting nodes, backtracking when reaching dead ends, and ensuring that each node is visited exactly once. The accompanying text explains the choices made during each step of the traversal, illustrating the algorithm&rsquo;s logic and how it systematically explores the graph. Additionally, pseudocode for the DFS algorithm is provided, further reinforcing the textual and visual explanation.</p><details><summary>read the caption</summary>Figure 10: A case presented in our textbook introduces a depth-first search algorithm.</details></blockquote></details><details><summary>More on tables</summary><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>#Shot</th><th>0</th><th>1</th><th>2</th><th>4</th><th>0</th><th>1</th><th>2</th><th>4</th><th>0</th><th>1</th><th>2</th><th>4</th><th>0</th><th>1</th><th>2</th><th>4</th></tr></thead><tbody><tr><td><strong>Dataset</strong></td><td>ScienceQA<sup>IMG</sup></td><td></td><td></td><td></td><td>OKVQA</td><td></td><td></td><td></td><td>TextVQA</td><td></td><td></td><td></td><td>TextVQA<sup>ocr</sup></td><td></td><td></td><td></td></tr><tr><td>MMC4</td><td>-</td><td>1.6</td><td>3.9</td><td>11.6</td><td>8.6</td><td>23.6</td><td>21.5</td><td>28.7</td><td>12.1</td><td>16.2</td><td>16.8</td><td>20.9</td><td>14.5</td><td>23.9</td><td>29.9</td><td>34.7</td></tr><tr><td>MMC4-Core-ff</td><td>-</td><td>2.1</td><td>10.1</td><td>10.2</td><td>11.8</td><td>21.2</td><td>25.3</td><td>30.4</td><td>13.6</td><td>18.7</td><td>18.8</td><td>22.1</td><td>16.1</td><td>26.6</td><td>28.7</td><td>33.1</td></tr><tr><td>OBELICS</td><td>-</td><td>2.8</td><td>3.0</td><td>16.4</td><td>13.0</td><td>31.7</td><td>35.7</td><td>37.5</td><td>9.2</td><td>26.5</td><td>30.2</td><td>32.2</td><td>11</td><td>30.7</td><td>36.3</td><td>41</td></tr><tr><td>Textbook-6.5M</td><td>26.3</td><td>29.4</td><td>25.1</td><td>37.3</td><td>10.2</td><td>31.2</td><td>36.8</td><td>39.9</td><td>11.8</td><td>26.7</td><td>32.1</td><td>33.5</td><td>14.1</td><td>33.1</td><td>36.4</td><td>42.8</td></tr><tr><td><strong>Dataset</strong></td><td>MathVista</td><td></td><td></td><td></td><td>MathVision</td><td></td><td></td><td></td><td>MathVerse</td><td></td><td></td><td></td><td>Avg.</td><td></td><td></td><td></td></tr><tr><td>MMC4</td><td>20.4</td><td>30</td><td>27.9</td><td>26</td><td>12.2</td><td>21.3</td><td>15.5</td><td>16.1</td><td>8.6</td><td>19.4</td><td>21.2</td><td>15.9</td><td>10.9</td><td>19.4</td><td>19.5</td><td>21.9</td></tr><tr><td>MMC4-Core-ff</td><td>22.5</td><td>33.0</td><td>29.2</td><td>27.8</td><td>13.7</td><td>23.4</td><td>16.3</td><td>17.7</td><td>8.6</td><td>19.9</td><td>21.8</td><td>15.2</td><td>12.3</td><td>20.7</td><td>21.4</td><td>22.3</td></tr><tr><td>OBELICS</td><td>21.6</td><td>28.5</td><td>31.1</td><td>27.6</td><td>13.4</td><td>20.1</td><td>16.8</td><td>14.9</td><td>6.9</td><td>19.4</td><td>20.7</td><td>14</td><td>10.7</td><td>22.8</td><td>24.8</td><td>26.2</td></tr><tr><td>Textbook-6.5M</td><td>24.3</td><td>43.4</td><td>33.2</td><td>29.2</td><td>14.5</td><td>25.6</td><td>18.2</td><td>18.1</td><td>7.7</td><td>28.5</td><td>19.8</td><td>14.6</td><td>15.5</td><td>31.1</td><td>28.8</td><td>30.8</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents the results of fine-tuning the LLaVA-1.5-7B base model using various interleaved datasets, including the multimodal textbook introduced in the paper. The performance of the model is evaluated across seven different benchmarks: four common Visual Question Answering (VQA) tasks and three math-related tasks. The evaluation is performed under few-shot settings (0-shot, 1-shot, 2-shot, and 4-shot), showing the model&rsquo;s performance with varying amounts of example data. The table allows for a comparison of the multimodal textbook&rsquo;s effectiveness in pre-training against existing interleaved datasets, highlighting the impact of dataset quality on model performance in different tasks.</p><details><summary>read the caption</summary>Table 2: We continued pre-training the base model of LLaVA-1.5-7B using different interleaved datasets. The results are evaluated on 4 common VQA and 3 math-related benchmarks under few-shot settings.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th></th><th>OKVQA</th><th>TextVQA</th><th>MathVista</th><th>MathVison</th><th>MathVerse</th><th>OKVQA</th><th>TextVQA</th><th>MathVista</th><th>MathVison</th><th>MathVerse</th></tr></thead><tbody><tr><td><strong>Continual Pre-training from Idefics2-8B-base</strong></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Dataset</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>MMC4-cf</td><td>54.1</td><td>57.7</td><td>27.8</td><td>14.0</td><td>17.3</td><td>9.4</td><td>25.1</td><td>24</td><td>13.3</td><td>18.3</td></tr><tr><td>OBELICS</td><td>54.6</td><td>57.5</td><td>27.6</td><td>14.3</td><td>17.5</td><td>10.5</td><td>25.7</td><td>24.2</td><td>13.6</td><td>17.7</td></tr><tr><td>Textbook-6.5M</td><td>55.1</td><td>58.2</td><td>29.7</td><td>16.2</td><td>19.4</td><td>10.1</td><td>26.8</td><td>26.1</td><td>14.4</td><td>19.8</td></tr><tr><td><strong>Pre-training Idefics2-8B from scratch</strong></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></tbody></table></table></figure><blockquote><p>üîº Table 3 presents the results of experiments using the Idefics-8B model, a Vision-Language Model (VLM) capable of handling multiple images. Unlike the previous Table 2 which used LLaVA-1.5-7B, this table shows results for Idefics-8B under two conditions: continual pre-training (starting from a pre-trained Idefics-8B-base model) and training from scratch (with randomly initialized weights). The evaluation is performed across multiple benchmarks (OKVQA, TextVQA, MathVista, MathVision, MathVerse) and extended to an 8-shot setting, using randomly selected examples for evaluation, a methodology consistent with prior work [16]. The table aims to compare the performance of the Idefics-8B model trained on different datasets, including the multimodal textbook dataset, illustrating the impact of different training methods and dataset characteristics on the model&rsquo;s capabilities.</p><details><summary>read the caption</summary>Table 3: Except for LLaVA, we also pre-train advanced VLMs with multi-image ability (Idefics): continual pretraining from Idefics-8B-base or pre-training from scratch. The evaluations are extended to an 8-shot using randomly selected examples as previous works¬†[16].</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Dataset</th><th>OKVQA</th><th>TextVQA</th><th>Mathvista</th><th>Mathvision</th><th>Mathverse</th></tr></thead><tbody><tr><td><em>1-shot Cheat: Example:{I<sub>t</sub>, q<sub>t</sub>, a<sub>t</sub>} + Test-case: I<sub>t</sub>, q<sub>t</sub></em></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>MMC4-cf</td><td>69.0</td><td>41.0</td><td>72.6</td><td>69.3</td><td>55.7</td></tr><tr><td>OBELICS</td><td>71.5</td><td>43.8</td><td>67.7</td><td>66.5</td><td>62.8</td></tr><tr><td>Ours</td><td><strong>79.2</strong></td><td><strong>51.9</strong></td><td><strong>94.1</strong></td><td><strong>98.4</strong></td><td><strong>76.8</strong></td></tr><tr><td><em>2-shot Cheat: Example:{I<sub>t</sub>, q<sub>t</sub>, a<sub>t</sub>}, {I<sub>e</sub>, q<sub>e</sub>, a<sub>e</sub>}+Test-case: I<sub>t</sub>, q<sub>t</sub></em></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>MMC4-Cf</td><td>53.5</td><td>39.2</td><td>55.7</td><td>51.9</td><td>40.8</td></tr><tr><td>OBELICS</td><td>71.3</td><td>42.8</td><td>56.7</td><td>39.9</td><td>39.5</td></tr><tr><td>Ours</td><td><strong>84.3</strong></td><td><strong>49.4</strong></td><td><strong>77.1</strong></td><td><strong>70.7</strong></td><td><strong>63.1</strong></td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents the results of a &lsquo;Cheat Test&rsquo; designed to evaluate the ability of Vision-Language Models (VLMs) to utilize interleaved context. The test replaces one or two few-shot examples within the prompt with the test sample itself. This assesses if the VLMs recognize the identical image, question, and answer combination and answer efficiently without further processing. The results compare the performance of various models (MMC4-cf, OBELICS, and the authors&rsquo; model) across multiple benchmarks (OKVQA, TextVQA, MathVista, MathVision, MathVerse) under 1-shot and 2-shot conditions. The goal is to understand how well the models leverage their interleaved context for accurate and efficient responses.</p><details><summary>read the caption</summary>Table 4: We design ‚ÄúCheat Test‚Äù to observe whether VLMs can attend to their interleaved context. We replace a few-shot example with the test sample itself and observe whether VLM notice this identical <<<image,question,answer>>> within their prompt. Itsubscriptùêºùë°I_{t}italic_I start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, qtsubscriptùëûùë°q_{t}italic_q start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, atsubscriptùëéùë°a_{t}italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT denote the test case, IesubscriptùêºùëíI_{e}italic_I start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT, qesubscriptùëûùëíq_{e}italic_q start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT, aesubscriptùëéùëía_{e}italic_a start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT denote a random selected example.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Pretraining</th><th>Continual Pretraining</th><th>SFT</th><th>OKVQA</th><th>MathVista</th></tr></thead><tbody><tr><td>‚úì</td><td>-</td><td>‚úì</td><td>61.1</td><td>23.2</td></tr><tr><td>‚úì</td><td>MMC4-Core-ff</td><td>‚úì</td><td>61.5 ‚Üë0.4</td><td>24.8 ‚Üë1.6</td></tr><tr><td>‚úì</td><td>OBELICS</td><td>‚úì</td><td>61.8 ‚Üë0.7</td><td>25.6 ‚Üë2.4</td></tr><tr><td>‚úì</td><td>Textbook-6.5M</td><td>‚úì</td><td><strong>62.2 ‚Üë1.1</strong></td><td><strong>28.7 ‚Üë5.5</strong></td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents the zero-shot performance results of three different vision-language models (VLMs) after instruction fine-tuning. The models were fine-tuned using 665K data from the LLaVA-1.5 dataset. The table compares the performance on two visual question answering (VQA) benchmarks (OKVQA and TextVQA) and one math-related benchmark (MathVista). The results show the improvement in performance after instruction fine-tuning for each model on the specified benchmarks. This demonstrates the impact of fine-tuning and allows for comparison of different models&rsquo; performance on the same benchmarks.</p><details><summary>read the caption</summary>Table 5: We also evaluated the zero-shot result after instruction fine-tuning using the 665K data from LLaVA-1.5.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Dataset</th><th>Perplexity ‚Üì</th><th>1-shot Acc.</th></tr></thead><tbody><tr><td>MMC4-Core-ff</td><td>12.56</td><td>20.7</td></tr><tr><td>OBELICS</td><td>11.27</td><td>22.8</td></tr><tr><td>Ours ( <em>ASR Refine, OCR, SSIM</em> )</td><td>13.92</td><td>31.1</td></tr><tr><td>- <em>w/o ASR Refine</em></td><td>16.86</td><td>26.2 (‚Üì4.9)</td></tr><tr><td>- <em>w/o OCR</em></td><td>12.7</td><td>28.8 (‚Üì2.3)</td></tr><tr><td>Keyframe Extraction algorithms</td><td>#Keyframe</td><td>1-shot Acc.</td></tr><tr><td>- <em>SSIM ‚Üí Pixel-level extractor</em></td><td>6.5M ‚Üí 18M</td><td>22.1 (‚Üì9)</td></tr><tr><td>- <em>SSIM ‚Üí CLIP-based extractor</em></td><td>6.5M ‚Üí 1.7M</td><td>24.6 (‚Üì6.5)</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents an ablation study on the process of creating the multimodal textbook dataset. It shows the impact of different components of the pipeline on the final dataset performance. Specifically, it analyzes the effects of refining the automatically generated speech-to-text (ASR) transcripts, the incorporation of optical character recognition (OCR) to extract text from images, and the choice of algorithm used for selecting keyframes from video clips. The results demonstrate the contribution of each step to overall performance metrics.</p><details><summary>read the caption</summary>Table 6: We perform an ablation study on video-to-textbook pipeline, including the impact of ASR refinement, the necessity of incorporating OCR, and the algorithms for extracting keyframes.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Subject</th><th>#Video</th><th>Duration (h)</th><th>#Topic</th><th>#Video Clip</th><th>#Keyframe</th><th>#ASR Token</th><th>#OCR Token</th><th>#Sample</th></tr></thead><tbody><tr><td>Mathematics</td><td>21.7k</td><td>4,423</td><td>725</td><td>809k</td><td>1.67M</td><td>72.5M</td><td>145M</td><td>123k</td></tr><tr><td>Physics</td><td>11k</td><td>3,511</td><td>530</td><td>822k</td><td>0.95M</td><td>36.7M</td><td>73.4M</td><td>119k</td></tr><tr><td>Chemistry</td><td>4.5k</td><td>2,643</td><td>410</td><td>234k</td><td>0.49M</td><td>15M</td><td>30M</td><td>32k</td></tr><tr><td>Earth Science</td><td>12k</td><td>3,670</td><td>520</td><td>640k</td><td>1.03M</td><td>40M</td><td>80M</td><td>88k</td></tr><tr><td>Engineering</td><td>13k</td><td>4,096</td><td>810</td><td>713k</td><td>1.15M</td><td>43.3M</td><td>86.6M</td><td>98k</td></tr><tr><td>Computer Science</td><td>12.8k</td><td>4,354</td><td>820</td><td>782k</td><td>1.21M</td><td>42.8M</td><td>85.5M</td><td>150k</td></tr><tr><td><strong>All</strong></td><td><strong>75k</strong></td><td><strong>22,697</strong></td><td><strong>3,915</strong></td><td><strong>4M</strong></td><td><strong>6.58M</strong></td><td><strong>258M</strong></td><td><strong>500M</strong></td><td><strong>610k</strong></td></tr></tbody></table></table></figure><blockquote><p>üîº Table 7 provides a detailed statistical overview of the multimodal textbook dataset used in the paper. It breaks down the dataset&rsquo;s composition across six subjects (Mathematics, Physics, Chemistry, Earth Science, Engineering, and Computer Science). For each subject, the table shows the number of videos, the total duration of those videos in hours, the number of topics covered, the number of video clips extracted, the number of keyframes extracted, and the counts of Automatic Speech Recognition (ASR) tokens and Optical Character Recognition (OCR) tokens. Finally, it indicates the total number of samples generated for model training. This information offers insights into the scale and characteristics of the dataset, highlighting its richness and diversity in terms of video content, extracted textual data, and the resulting training samples.</p><details><summary>read the caption</summary>Table 7: The statistics of our multimodal textbook. Topic denotes the knowledge points covered by each category of videos, which are sourced from our knowledge taxonomy.</details></blockquote></details><h3 class="relative group">Full paper<div id=full-paper class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#full-paper aria-label=Anchor>#</a></span></h3><div id=gallery-8463221d9f720def9c045d07eea2e347 class=gallery><img src=https://ai-paper-reviewer.com/2501.00958/1.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.00958/2.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.00958/3.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.00958/4.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.00958/5.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.00958/6.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.00958/7.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.00958/8.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.00958/9.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.00958/10.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.00958/11.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.00958/12.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.00958/13.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.00958/14.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.00958/15.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.00958/16.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.00958/17.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.00958/18.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.00958/19.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.00958/20.png class="grid-w50 md:grid-w33 xl:grid-w25"></div></div><section class="flex flex-row flex-wrap justify-center pt-4 text-xl"><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.00958/&amp;title=2.5%20Years%20in%20Class:%20A%20Multimodal%20Textbook%20for%20Vision-Language%20Pretraining" title="Share on LinkedIn" aria-label="Share on LinkedIn"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://twitter.com/intent/tweet/?url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.00958/&amp;text=2.5%20Years%20in%20Class:%20A%20Multimodal%20Textbook%20for%20Vision-Language%20Pretraining" title="Tweet on Twitter" aria-label="Tweet on Twitter"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="mailto:?body=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.00958/&amp;subject=2.5%20Years%20in%20Class:%20A%20Multimodal%20Textbook%20for%20Vision-Language%20Pretraining" title="Send via email" aria-label="Send via email"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1-27.64 140.9 68.65 266.2 199.1 285.1 19.01 2.888 36.17-12.26 36.17-31.49l1e-4-.6631c0-15.74-11.44-28.88-26.84-31.24-84.35-12.98-149.2-86.13-149.2-174.2.0-102.9 88.61-185.5 193.4-175.4 91.54 8.869 158.6 91.25 158.6 183.2v16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98.0036c-7.299.0-13.2 4.992-15.12 11.68-24.85-12.15-54.24-16.38-86.06-5.106-38.75 13.73-68.12 48.91-73.72 89.64-9.483 69.01 43.81 128 110.9 128 26.44.0 50.43-9.544 69.59-24.88 24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3c0-149.2-133.9-265.632-287.3-235.57zM239.1 304.3c-26.47.0-48-21.56-48-48.05s21.53-48.05 48-48.05 48 21.56 48 48.05-20.6 48.05-48 48.05z"/></svg></span></a></section></div><script>var oid="views_paper-reviews/2501.00958/index.md",oid_likes="likes_paper-reviews/2501.00958/index.md"</script><script type=text/javascript src=/ai-paper-reviewer/js/page.min.0860cf4e04fa2d72cc33ddba263083464d48f67de06114529043cb4623319efed4f484fd7f1730df5abea0e2da6f3538855634081d02f2d6e920b956f063e823.js integrity="sha512-CGDPTgT6LXLMM926JjCDRk1I9n3gYRRSkEPLRiMxnv7U9IT9fxcw31q+oOLabzU4hVY0CB0C8tbpILlW8GPoIw=="></script></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="flex group mr-3" href=/ai-paper-reviewer/paper-reviews/2501.00874/><span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&larr;</span>
<span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">LUSIFER: Language Universal Space Integration for Enhanced Multilingual Embeddings with Large Language Models</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2025-01-01T00:00:00+00:00>1 January 2025</time>
</span></span></a></span><span><a class="flex text-right group ml-3" href=/ai-paper-reviewer/paper-reviews/2501.01427/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">VideoAnydoor: High-fidelity Video Object Insertion with Precise Motion Control</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2025-01-02T00:00:00+00:00>2 January 2025</time>
</span></span><span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&larr;</span></a></span></div></div><div class=pt-3><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class=pt-3><script src=https://utteranc.es/client.js repo=pmnxis/pmnxis.github.io issue-term=pathname label=Comment theme=dark-blue crossorigin=anonymous async></script></div></div></footer></article><div id=top-scroller class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer id=site-footer class="py-10 print:hidden"><nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400"><ul class="flex flex-col list-none sm:flex-row"><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=/ai-paper-reviewer/tags/ title=Tags>Tags</a></li><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=https://deep-diver.github.io/neurips2024/ title>NeurIPS2024</a></li></ul></nav><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2025
Hugging Face Daily Papers</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a></p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/ai-paper-reviewer/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh+sCQ0E53ghYrxgYqw+0GCRyIEpA=="></script></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://deep-diver.github.io/ai-paper-reviewer/ style=z-index:500><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body><script data-name=BMC-Widget data-cfasync=false src=https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js data-id=chansung data-description="Support me on Buy me a coffee!" data-message data-color=#FFDD00 data-position=Left data-x_margin=18 data-y_margin=18></script></html>