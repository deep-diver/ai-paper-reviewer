[{"content": "| Dataset | #Image |  |  | #Text Token |  |  | _L_=4 | _L_=5 | _L_=6 | _L_=7 | _L_=8 | Avg. | Source |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| Min. | Max. | Avg. | Min. | Max. | Avg. |  |  |  |  |  |  |  |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| _Image-text Paired Dataset_ |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| COYO-700M | 1 | 1 | 1 | 1 | 811 | 16 | - | - | - | - | - | - | Common Crawl |\n| LAION-5B | 1 | 1 | 1 | 6 | 683 | 27 | - | - | - | - | - | - | Common Crawl |\n| _Image-text Interleaved Dataset_ |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| MMC4 | 0 | 117 | 5.7 | 4 | 16715 | 417 | 0.363 | 0.348 | 0.310 | 0.298 | 0.276 | 0.319 | Common Crawl |\n| MMC4-core-ff | 0 | 15 | 4.1 | 15 | 16715 | 329 | 0.431 | 0.406 | 0.404 | 0.403 | 0.396 | 0.407 | Common Crawl |\n| OBELICS | 1 | 30 | 2.5 | 12 | 10717 | 816 | 0.366 | 0.351 | 0.339 | 0.337 | 0.336 | 0.345 | Common Crawl |\n| OmniCorpus* | 1 | 16 | 3.9 | 14 | 6893 | 574 | 0.358 | 0.329 | 0.310 | 0.305 | 0.301 | 0.321 | Multi-sources |\n| **Ours** | **2** | **45** | **10.7** | **11** | **34174** | **1297** | **0.687** | **0.697** | **0.698** | **0.688** | **0.662** | **0.686** | Video Website |", "caption": "Table 1: We compare our multimodal textbook with image-text paired datasets and webpage-centric interleaved datasets in terms of image and text distributions. In-sample Image SIMLsuperscriptSIM\ud835\udc3f\\text{SIM}^{L}SIM start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT measures the semantic and structural correlation between multiple images within an interleaved sample. OmniCorpus\u2217superscriptOmniCorpus\\text{OmniCorpus}^{*}OmniCorpus start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT: Due to the extensive size of the dataset, we perform statistical analysis on a randomly sampled subset.", "description": "This table compares the characteristics of various multimodal datasets, focusing on the distribution of images and text tokens.  It contrasts the proposed \"Multimodal Textbook\" dataset with existing image-text paired datasets and interleaved datasets commonly used for vision-language model training.  Key metrics include the number of images and text tokens per sample, as well as a newly introduced metric, \"In-sample Image SIML\", which quantifies the semantic and structural correlation among multiple images within a single sample from an interleaved dataset. This helps assess the coherence and logical relationships between images within a sample.  The table also notes that due to its size, the OmniCorpus dataset was analyzed using a randomly sampled subset.", "section": "4. Analysis of Multimodal Textbook"}, {"content": "| #Shot | 0 | 1 | 2 | 4 | 0 | 1 | 2 | 4 | 0 | 1 | 2 | 4 | 0 | 1 | 2 | 4 |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| **Dataset** | ScienceQA<sup>IMG</sup> |  |  |  | OKVQA |  |  |  | TextVQA |  |  |  | TextVQA<sup>ocr</sup> |  |  |  |\n| MMC4 | - | 1.6 | 3.9 | 11.6 | 8.6 | 23.6 | 21.5 | 28.7 | 12.1 | 16.2 | 16.8 | 20.9 | 14.5 | 23.9 | 29.9 | 34.7 |\n| MMC4-Core-ff | - | 2.1 | 10.1 | 10.2 | 11.8 | 21.2 | 25.3 | 30.4 | 13.6 | 18.7 | 18.8 | 22.1 | 16.1 | 26.6 | 28.7 | 33.1 |\n| OBELICS | - | 2.8 | 3.0 | 16.4 | 13.0 | 31.7 | 35.7 | 37.5 | 9.2 | 26.5 | 30.2 | 32.2 | 11 | 30.7 | 36.3 | 41 |\n| Textbook-6.5M | 26.3 | 29.4 | 25.1 | 37.3 | 10.2 | 31.2 | 36.8 | 39.9 | 11.8 | 26.7 | 32.1 | 33.5 | 14.1 | 33.1 | 36.4 | 42.8 |\n| **Dataset** | MathVista |  |  |  | MathVision |  |  |  | MathVerse |  |  |  | Avg. |  |  |  |\n| MMC4 | 20.4 | 30 | 27.9 | 26 | 12.2 | 21.3 | 15.5 | 16.1 | 8.6 | 19.4 | 21.2 | 15.9 | 10.9 | 19.4 | 19.5 | 21.9 |\n| MMC4-Core-ff | 22.5 | 33.0 | 29.2 | 27.8 | 13.7 | 23.4 | 16.3 | 17.7 | 8.6 | 19.9 | 21.8 | 15.2 | 12.3 | 20.7 | 21.4 | 22.3 |\n| OBELICS | 21.6 | 28.5 | 31.1 | 27.6 | 13.4 | 20.1 | 16.8 | 14.9 | 6.9 | 19.4 | 20.7 | 14 | 10.7 | 22.8 | 24.8 | 26.2 |\n| Textbook-6.5M | 24.3 | 43.4 | 33.2 | 29.2 | 14.5 | 25.6 | 18.2 | 18.1 | 7.7 | 28.5 | 19.8 | 14.6 | 15.5 | 31.1 | 28.8 | 30.8 |", "caption": "Table 2: We continued pre-training the base model of LLaVA-1.5-7B using different interleaved datasets. The results are evaluated on 4 common VQA and 3 math-related benchmarks under few-shot settings.", "description": "This table presents the results of fine-tuning the LLaVA-1.5-7B base model using various interleaved datasets, including the multimodal textbook introduced in the paper.  The performance of the model is evaluated across seven different benchmarks: four common Visual Question Answering (VQA) tasks and three math-related tasks.  The evaluation is performed under few-shot settings (0-shot, 1-shot, 2-shot, and 4-shot), showing the model's performance with varying amounts of example data. The table allows for a comparison of the multimodal textbook's effectiveness in pre-training against existing interleaved datasets, highlighting the impact of dataset quality on model performance in different tasks.", "section": "5. Experiments"}, {"content": "|                       | OKVQA | TextVQA | MathVista | MathVison | MathVerse | OKVQA | TextVQA | MathVista | MathVison | MathVerse |\n|-----------------------|-------|---------|-----------|-----------|----------|-------|---------|-----------|-----------|----------|\n| **Continual Pre-training from Idefics2-8B-base** |       |         |           |           |          |       |         |           |           |          |\n| Dataset                 |       |         |           |           |          |       |         |           |           |          |\n| MMC4-cf                 | 54.1  | 57.7    | 27.8      | 14.0      | 17.3     | 9.4   | 25.1    | 24        | 13.3      | 18.3     |\n| OBELICS                 | 54.6  | 57.5    | 27.6      | 14.3      | 17.5     | 10.5  | 25.7    | 24.2      | 13.6      | 17.7     |\n| Textbook-6.5M          | 55.1  | 58.2    | 29.7      | 16.2      | 19.4     | 10.1  | 26.8    | 26.1      | 14.4      | 19.8     |\n| **Pre-training Idefics2-8B from scratch** |       |         |           |           |          |       |         |           |           |          |", "caption": "Table 3: Except for LLaVA, we also pre-train advanced VLMs with multi-image ability (Idefics): continual pretraining from Idefics-8B-base or pre-training from scratch. The evaluations are extended to an 8-shot using randomly selected examples as previous works\u00a0[16].", "description": "Table 3 presents the results of experiments using the Idefics-8B model, a Vision-Language Model (VLM) capable of handling multiple images.  Unlike the previous Table 2 which used LLaVA-1.5-7B, this table shows results for Idefics-8B under two conditions: continual pre-training (starting from a pre-trained Idefics-8B-base model) and training from scratch (with randomly initialized weights).  The evaluation is performed across multiple benchmarks (OKVQA, TextVQA, MathVista, MathVision, MathVerse) and extended to an 8-shot setting, using randomly selected examples for evaluation, a methodology consistent with prior work [16].  The table aims to compare the performance of the Idefics-8B model trained on different datasets, including the multimodal textbook dataset, illustrating the impact of different training methods and dataset characteristics on the model's capabilities.", "section": "5. Experiments"}, {"content": "| Dataset | OKVQA | TextVQA | Mathvista | Mathvision | Mathverse |\n|---|---|---|---|---|---| \n| _1-shot Cheat: Example:{I<sub>t</sub>, q<sub>t</sub>, a<sub>t</sub>} + Test-case: I<sub>t</sub>, q<sub>t</sub>_ |  |  |  |  |  |\n| MMC4-cf | 69.0 | 41.0 | 72.6 | 69.3 | 55.7 |\n| OBELICS | 71.5 | 43.8 | 67.7 | 66.5 | 62.8 |\n| Ours | **79.2** | **51.9** | **94.1** | **98.4** | **76.8** |\n| _2-shot Cheat: Example:{I<sub>t</sub>, q<sub>t</sub>, a<sub>t</sub>}, {I<sub>e</sub>, q<sub>e</sub>, a<sub>e</sub>}+Test-case: I<sub>t</sub>, q<sub>t</sub>_ |  |  |  |  |  |\n| MMC4-Cf | 53.5 | 39.2 | 55.7 | 51.9 | 40.8 |\n| OBELICS | 71.3 | 42.8 | 56.7 | 39.9 | 39.5 |\n| Ours | **84.3** | **49.4** | **77.1** | **70.7** | **63.1** |", "caption": "Table 4: We design \u201cCheat Test\u201d to observe whether VLMs can attend to their interleaved context. We replace a few-shot example with the test sample itself and observe whether VLM notice this identical <<<image,question,answer>>> within their prompt. Itsubscript\ud835\udc3c\ud835\udc61I_{t}italic_I start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, qtsubscript\ud835\udc5e\ud835\udc61q_{t}italic_q start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, atsubscript\ud835\udc4e\ud835\udc61a_{t}italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT denote the test case, Iesubscript\ud835\udc3c\ud835\udc52I_{e}italic_I start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT, qesubscript\ud835\udc5e\ud835\udc52q_{e}italic_q start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT, aesubscript\ud835\udc4e\ud835\udc52a_{e}italic_a start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT denote a random selected example.", "description": "This table presents the results of a \"Cheat Test\" designed to evaluate the ability of Vision-Language Models (VLMs) to utilize interleaved context.  The test replaces one or two few-shot examples within the prompt with the test sample itself. This assesses if the VLMs recognize the identical image, question, and answer combination and answer efficiently without further processing. The results compare the performance of various models (MMC4-cf, OBELICS, and the authors' model) across multiple benchmarks (OKVQA, TextVQA, MathVista, MathVision, MathVerse) under 1-shot and 2-shot conditions.  The goal is to understand how well the models leverage their interleaved context for accurate and efficient responses.", "section": "5.3. Analysis"}, {"content": "| Pretraining | Continual Pretraining | SFT | OKVQA | MathVista |\n|---|---|---|---|---|\n| \u2713 | - | \u2713 | 61.1 | 23.2 |\n| \u2713 | MMC4-Core-ff | \u2713 | 61.5 \u21910.4 | 24.8 \u21911.6 |\n| \u2713 | OBELICS | \u2713 | 61.8 \u21910.7 | 25.6 \u21912.4 |\n| \u2713 | Textbook-6.5M | \u2713 | **62.2 \u21911.1** | **28.7 \u21915.5** |", "caption": "Table 5: We also evaluated the zero-shot result after instruction fine-tuning using the 665K data from LLaVA-1.5.", "description": "This table presents the zero-shot performance results of three different vision-language models (VLMs) after instruction fine-tuning. The models were fine-tuned using 665K data from the LLaVA-1.5 dataset.  The table compares the performance on two visual question answering (VQA) benchmarks (OKVQA and TextVQA) and one math-related benchmark (MathVista).  The results show the improvement in performance after instruction fine-tuning for each model on the specified benchmarks. This demonstrates the impact of fine-tuning and allows for comparison of different models' performance on the same benchmarks.", "section": "5. Experiments"}, {"content": "| Dataset | Perplexity \u2193 | 1-shot Acc. |\n|---|---|---|\n| MMC4-Core-ff | 12.56 | 20.7 |\n| OBELICS | 11.27 | 22.8 |\n| Ours ( _ASR Refine, OCR, SSIM_ ) | 13.92 | 31.1 |\n| - _w/o ASR Refine_ | 16.86 | 26.2 (\u21934.9) |\n| - _w/o OCR_ | 12.7 | 28.8 (\u21932.3) |\n| Keyframe Extraction algorithms | #Keyframe | 1-shot Acc. |\n| - _SSIM \u2192 Pixel-level extractor_ | 6.5M \u2192 18M | 22.1 (\u21939) |\n| - _SSIM \u2192 CLIP-based extractor_ | 6.5M \u2192 1.7M | 24.6 (\u21936.5) |", "caption": "Table 6: We perform an ablation study on video-to-textbook pipeline, including the impact of ASR refinement, the necessity of incorporating OCR, and the algorithms for extracting keyframes.", "description": "This table presents an ablation study on the process of creating the multimodal textbook dataset.  It shows the impact of different components of the pipeline on the final dataset performance. Specifically, it analyzes the effects of refining the automatically generated speech-to-text (ASR) transcripts, the incorporation of optical character recognition (OCR) to extract text from images, and the choice of algorithm used for selecting keyframes from video clips. The results demonstrate the contribution of each step to overall performance metrics.", "section": "5.4. Ablation of Video-to-Textbook's Design"}, {"content": "| Subject | #Video | Duration (h) | #Topic | #Video Clip | #Keyframe | #ASR Token | #OCR Token | #Sample |\n|---|---|---|---|---|---|---|---|---|\n| Mathematics | 21.7k | 4,423 | 725 | 809k | 1.67M | 72.5M | 145M | 123k |\n| Physics | 11k | 3,511 | 530 | 822k | 0.95M | 36.7M | 73.4M | 119k |\n| Chemistry | 4.5k | 2,643 | 410 | 234k | 0.49M | 15M | 30M | 32k |\n| Earth Science | 12k | 3,670 | 520 | 640k | 1.03M | 40M | 80M | 88k |\n| Engineering | 13k | 4,096 | 810 | 713k | 1.15M | 43.3M | 86.6M | 98k |\n| Computer Science | 12.8k | 4,354 | 820 | 782k | 1.21M | 42.8M | 85.5M | 150k |\n| **All** | **75k** | **22,697** | **3,915** | **4M** | **6.58M** | **258M** | **500M** | **610k** |", "caption": "Table 7: The statistics of our multimodal textbook. Topic denotes the knowledge points covered by each category of videos, which are sourced from our knowledge taxonomy.", "description": "Table 7 provides a detailed statistical overview of the multimodal textbook dataset used in the paper.  It breaks down the dataset's composition across six subjects (Mathematics, Physics, Chemistry, Earth Science, Engineering, and Computer Science). For each subject, the table shows the number of videos, the total duration of those videos in hours, the number of topics covered, the number of video clips extracted, the number of keyframes extracted, and the counts of Automatic Speech Recognition (ASR) tokens and Optical Character Recognition (OCR) tokens.  Finally, it indicates the total number of samples generated for model training. This information offers insights into the scale and characteristics of the dataset, highlighting its richness and diversity in terms of video content, extracted textual data, and the resulting training samples.", "section": "4. Analysis of Multimodal Textbook"}]