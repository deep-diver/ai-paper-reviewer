[{"content": "| Model | Micro-Avg | DaruMMLU | DaruMERA | DaruSum | DaruCopy (EN) | DaruCopy (RU) |\n|---|---|---|---|---|---|---|\n| Openchat 3.5 (Mistral-7B) | 0,607 | 0,543 | 0,526 | 0,322 | 0,999 | 0,917 |\n| LLaMa-3-8B (Instruct) | 0,610 | 0,571 | 0,510 | 0,322 | 1,000 | 0,972 |\n| Saiga (LLaMa-3-8B) | 0,608 | 0,574 | 0,514 | 0,320 | 0,995 | 0,939 |\n| Vikhr-5.2 (Mistral-7B) | 0,587 | 0,494 | 0,573 | 0,308 | 0,959 | 0,693 |\n| Qwen-2 7B | 0,613 | 0,624 | 0,548 | 0,300 | 0,938 | 0,842 |\n| Mistral Nemo (12B) | 0,639 | 0,592 | 0,576 | 0,320 | 0,998 | 0,924 |\n| Ours |  |  |  |  |  |  |\n| Openchat 3.5 + LEP-Extended + calibration (best) | 0,632<sup>\u2191</sup> | 0,541 | 0,563<sup>\u2191</sup> | 0,321 | 1,000 | 0,989<sup>\u2191</sup> |\n| LLaMa-3-8B (Instruct) + LEP-Extended + calibration (best) | 0,618<sup>\u2191</sup> | 0,565<sup>\u2193</sup> | 0,521<sup>\u2191</sup> | 0,339<sup>\u2191</sup> | 1,000 | 0,984<sup>\u2191</sup> |", "caption": "Table 1: Darumeru zero-shot evaluation results for popular open-source instruct-tuned models.", "description": "This table presents the zero-shot performance of several popular open-source, instruction-tuned large language models (LLMs) on the Darumeru benchmark.  Darumeru is a newly developed benchmark designed to evaluate Russian language capabilities, encompassing a variety of tasks including multiple-choice questions, text comprehension, text summarization, and text generation. The table shows the average performance across all Darumeru sub-tasks (Micro-Avg) and individual task scores for each LLM.", "section": "3.1 Open-source LLM Benchmark"}, {"content": "| Model | Vocab | Symbols per token | Micro-Avg | DaruMMLU | DaruMERA | DaruSum | DaruCopy (EN) | DaruCopy (RU) |\n|---|---|---|---|---|---|---|---|---|\n| Mistral-7B | Original | 2,44 | 0,604 | **0,545** | 0,504 | 0,307 | **1,000** | **1,000** |\n|  | BPE | **3,76** | 0,616 | 0,528 | 0,537 | **0,316** | 0,995 | 0,984 |\n|  | Unigram | **3,78** | 0,614 | **0,544** | 0,311 | 0,995 | 0,960 |\n|  | Extended | **3,77** | **0,617** | 0,532 | **0,314** | **1,000** | 0,995 |\n| LLaMa-3-8B | Original | 2,89 | **0,629** | **0,582** | **0,547** | **0,326** | 0,980 | 0,982 |\n|  | BPE | **4,40** | 0,618 | 0,532 | 0,321 | **1,000** | 0,963 |\n|  | Unigram | **4,35** | 0,609 | 0,517 | 0,316 | **1,000** | 0,951 |\n|  | Extended | 3,78 | **0,627** | **0,550** | **0,325** | 0,980 | 0,983 |\n|  | Optimized | 3,40 | 0,620 | 0,552 | 0,536 | 0,323 | 0,981 | **0,989** |", "caption": "Table 2: Darumeru few-shot evaluation results for best language-adaptation checkpoints.", "description": "This table presents the results of a few-shot evaluation on the Darumeru benchmark, which assesses the performance of language models on various tasks.  Specifically, it shows the performance of different language adaptation methods for two large language models (LLMs), Mistral-7B and LLaMa-3-8B, after vocabulary optimization and continued pre-training.  The metrics include Micro-Avg (average across all tasks), DaruMMLU, DaruMERA, DaruSum, DaruCopy (EN), and DaruCopy (RU), representing different benchmark subtasks focused on language understanding and generation capabilities. The table compares the original LLM's performance with the performance after applying four distinct vocabulary adaptation techniques (BPE, Unigram, Extended, Optimized). Each method's performance is quantified by the average score across all benchmark tasks, offering a comprehensive comparison of the efficacy of each approach for Russian language adaptation.", "section": "2.1 Model Language Adaptation"}, {"content": "| Vocab | LEP method | Micro-Avg | DaruMMLU | DaruMERA | DaruSum | DaruCopy (En) | DaruCopy (Ru) |\n|---|---|---|---|---|---|---|---| \n| OpenChat-3.5 |  |  |  |  |  |  |  |\n| BPE | Swap | **0,587** | **0,528** | **0,526** | 0,277 | 0,988 | **0,829** |\n| BPE | Overlap | 0,584 | 0,525 | 0,523 | 0,281 | 0,986 | 0,818 |\n| BPE | Conversion | 0,583 | 0,526 | 0,524 | **0,284** | **0,993** | 0,791 |\n| Unigram | Swap | 0,556 | **0,517** | 0,517 | 0,282 | 0,985 | 0,614 |\n| Unigram | Overlap | **0,572** | 0,514 | **0,534** | 0,297 | 0,981 | **0,680** |\n| Unigram | Conversion | 0,565 | 0,515 | 0,519 | **0,301** | **0,999** | 0,651 |\n| Extended | Swap | **0,608** | **0,535** | **0,540** | 0,298 | **0,999** | **0,907** |\n| Extended | Overlap | **0,607** | **0,535** | **0,539** | **0,307** | **0,999** | 0,898 |\n| Extended | Conversion | **0,609** | **0,535** | **0,541** | **0,306** | **0,999** | **0,909** |\n| LLaMa-3-8B (instruct) |  |  |  |  |  |  |  |\n| BPE | Swap | 0,565 | **0,544** | 0,486 | **0,317** | **0,999** | 0,729 |\n| BPE | Overlap | **0,569** | **0,546** | **0,489** | 0,314 | **0,999** | **0,753** |\n| BPE | Conversion | **0,570** | **0,546** | **0,490** | **0,318** | **0,999** | **0,754** |\n| Unigram | Swap | **0,582** | **0,545** | **0,488** | **0,313** | **0,999** | 0,865 |\n| Unigram | Overlap | 0,580 | **0,545** | 0,482 | **0,314** | **0,999** | 0,876 |\n| Unigram | Conversion | **0,584** | **0,545** | **0,488** | **0,315** | 0,994 | **0,889** |\n| Extended | Swap | 0,592 | **0,557** | 0,498 | **0,319** | 0,969 | 0,921 |\n| Extended | Overlap | **0,597** | **0,556** | **0,504** | **0,321** | 0,964 | **0,936** |\n| Extended | Conversion | **0,597** | **0,556** | 0,501 | 0,318 | **0,994** | 0,921 |\n| Optimized | Swap | 0,594 | **0,554** | **0,499** | **0,327** | 0,970 | **0,928** |\n| Optimized | Overlap | 0,586 | **0,553** | 0,495 | 0,323 | 0,925 | 0,925 |\n| Optimized | Conversion | **0,598** | **0,555** | **0,500** | 0,324 | **0,995** | **0,928** |", "caption": "Table 3: Darumeru zero-shot evaluation results for Learned Embedding Propagation methods.", "description": "This table presents the results of a zero-shot evaluation using the Darumeru benchmark.  The evaluation focuses on assessing the performance of the Learned Embedding Propagation (LEP) method for adapting language models.  Different vocabulary adaptation techniques (BPE, Unigram, Extended, Optimized) are combined with three LEP methods (Swap, Overlap, Conversion).  The results show the micro-averaged scores across multiple sub-benchmarks of Darumeru (DaruMMLU, DaruMERA, DaruSum, DaruCopy(EN), DaruCopy(RU)), providing a comprehensive comparison of the effectiveness of various LEP strategies under different vocabulary conditions for both OpenChat 3.5 and LLaMa-3-8B models.", "section": "3.3 Learned Embedding Propagation"}, {"content": "| Model | Fine-tuning data | Micro-Avg | DaruMMLU | DaruMERA | DaruSum | DaruCopy (EN) | DaruCopy (RU) |\n|---|---|---|---|---|---|---|---| \n| OpenChat-3.5 |  |  |  |  |  |  |  |\n| Original model | - | 0,607 | **0,543** | 0,526 | 0,322 | **0,999** | 0,917 |\n|  | saiga d7 | 0,611 | 0,540 | **0,528** | **0,325** | **0,999** | 0,945 |\n|  | +copy task | **0,615** | 0,541 | 0,524 | 0,324 | **1,000** | **0,995** |\n| Unigram | - | 0,565 | 0,515 | 0,519 | 0,301 | **0,999** | 0,651 |\n|  | saiga d7 | 0,599 | 0,556 | 0,316 | **0,999** | 0,754 |\n|  | +copy task | **0,630** | **0,559** | **0,321** | **1,000** | **0,999** |\n| Extended | - | 0,609 | 0,535 | 0,541 | 0,306 | **0,999** | 0,909 |\n|  | saiga d7 | 0,616 | **0,566** | 0,319 | **0,999** | 0,845 |\n|  | +copy task | **0,632** | 0,563 | **0,321** | **1,000** | **0,989** |\n| LLaMa-3-8B (instruct) |  |  |  |  |  |  |  |\n| Original model | - | 0,610 | 0,571 | 0,510 | 0,322 | **1,000** | 0,972 |\n|  | saiga d7 | 0,615 | 0,512 | 0,329 | **1,000** | 0,983 |\n|  | +copy task | **0,616** | **0,513** | **0,332** | **1,000** | **0,995** |\n| Extended | - | 0,597 | 0,556 | 0,501 | 0,318 | 0,994 | 0,921 |\n|  | self-calibration | 0,606 | 0,552 | 0,512 | 0,321 | **1,000** | 0,958 |\n|  | saiga d7 | 0,614 | 0,519 | 0,338 | 0,995 | 0,961 |\n|  | +copy task | **0,618** | 0,565 | **0,521** | **0,339** | **1,000** | **0,984** |\n| Optimized | - | 0,598 | **0,555** | 0,500 | 0,324 | 0,995 | 0,928 |\n|  | self-calibration | 0,601 | 0,550 | 0,501 | 0,325 | **1,000** | 0,950 |\n|  | saiga d7 | 0,611 | 0,515 | 0,336 | **1,000** | 0,971 |\n|  | +copy task | **0,617** | **0,555** | **0,522** | **0,339** | **1,000** | **0,989** |", "caption": "Table 4: Benchmark results for model calibration schemes of Conversion LEP models", "description": "This table presents the benchmark results comparing different model calibration methods applied to the Conversion LEP (Learned Embedding Propagation) models.  It evaluates the performance of these models across several benchmarks (DaruMMLU, DaruMERA, DaruSum, DaruCopy (EN), and DaruCopy (RU)) to assess their effectiveness in language understanding and text generation tasks.  The table shows the performance of the original model and then compares it to the results obtained using the Saiga dataset with and without an additional copy task. The results help analyze the impact of calibration techniques on the models' overall performance and the influence of the additional copy task.", "section": "3.5 Case Study: Continued Instruction-Tuning Calibration"}]