[{"figure_path": "https://arxiv.org/html/2412.07776/x2.png", "caption": "Figure 1: Overview of DiTFlow. We propose a motion transfer method tailored for video Diffusion Transformers (DiT). We exploit a training-free strategy to transfer the motion of a reference video (top) to newly synthesized video content with arbitrary prompts (bottom). By optimizing DiT-specific positional embeddings, we can also synthesize new videos in a zero-shot manner.", "description": "This figure illustrates the DiTFlow method, which adapts video motion transfer for Diffusion Transformers (DiTs).  The top row shows a reference video whose motion is to be transferred.  The bottom row displays newly generated videos based on arbitrary text prompts.  DiTFlow uses a training-free approach, extracting motion information from the reference video and applying it to the generation process.  A key aspect is optimizing DiT-specific positional embeddings, enabling the generation of new videos without additional training (zero-shot synthesis).", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2412.07776/x3.png", "caption": "Figure 2: Core idea of DiTFlow. We extract the AMF from a reference video and we use that to guide the latent representation ztsubscript\ud835\udc67\ud835\udc61z_{t}italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT towards the motion of the reference video. In our experiments, we also tested optimizing positional embeddings for improved zero-shot performance.", "description": "DiTFlow uses a reference video to guide the generation of new videos.  First, it extracts a patch-wise motion signal (Attention Motion Flow or AMF) from the reference video using a pre-trained Diffusion Transformer (DiT). This AMF is then used to condition the latent representation (z<sub>t</sub>) during the video generation process, ensuring the synthesized video mimics the motion of the reference video.  The figure highlights the key steps: extracting the AMF from the reference video, using the DiT to generate a new video latent representation, and guiding that latent representation with the AMF to match the reference motion.  Additionally, the experiments explored optimizing positional embeddings within the DiT to enable zero-shot motion transfer, where motion is transferred to new videos without additional optimization.", "section": "Method"}, {"figure_path": "https://arxiv.org/html/2412.07776/x4.png", "caption": "Figure 3: Guidance. We compute the reference displacement by processing cross-frame attentions with an argmax operation and rearranging them into displacement maps, identifying patch-aware cross-frame relationships. For video synthesis, we do the same operation with a soft argmax to preserve gradients, and impose reconstruction with the reference displacement.", "description": "Figure 3 illustrates the process of generating Attention Motion Flow (AMF) from a reference video.  It begins by processing cross-frame attention maps obtained from a Diffusion Transformer (DiT) model. The argmax operation identifies the strongest attention relationships between patches in consecutive frames, creating a displacement map that shows how patches move over time.  For generating new videos, a soft argmax is used instead of argmax to retain gradient information during optimization, ensuring smoother motion transfer.  The reference displacement is then used as guidance to constrain the motion of the synthesized video.", "section": "4.2 Attention Motion Flows"}, {"figure_path": "https://arxiv.org/html/2412.07776/extracted/6056616/figures/supp/supp_squat1.png", "caption": "Figure 4: Baseline comparison. Baselines associate motion to wrong elements due to poor layout representation typical of UNet-based approaches that do spatial averaging or only consider deviations at each location. DiTFlow captures the spatio-temporal motion of each patch, resulting in correct spatial positioning and sizing of moving elements, e.g. the dog (left), the bear (middle), the parachute (right).", "description": "Figure 4 demonstrates a comparison of motion transfer results between DiTFlow and several baseline methods.  The baselines, relying on UNet architectures, struggle with accurate motion transfer because their spatial averaging or localized deviation analysis fails to capture the complete spatiotemporal context of motion. This leads to errors in associating motion to the correct elements within the video frame.  In contrast, DiTFlow leverages the attention mechanism within Diffusion Transformers to comprehensively understand the spatiotemporal dynamics of each patch within the video.  This results in more precise motion transfer and accurate spatial positioning and sizing of moving elements within the generated video. The figure provides three examples illustrating this difference: a dog running, a bear running, and a parachutist descending.  In each example, DiTFlow achieves significantly better motion transfer accuracy than the baselines.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.07776/extracted/6056616/figures/supp/supp_squat2.png", "caption": "Figure 5: Qualitative results of DiTFlow. We are able to perform motion transfer in various conditions. Note how varying the prompt completely changes the scene\u2019s appearance while maintaining consistent motion. We map motion to correct elements even in cases where the motion changes drastically in positioning and size (bottom right).", "description": "Figure 5 showcases the qualitative results of DiTFlow, demonstrating its ability to successfully transfer motion from a reference video to newly synthesized videos under diverse conditions.  The top row illustrates how altering the text prompt significantly changes the generated video's appearance (scene, objects, etc.) while preserving the original motion. This highlights DiTFlow's capacity for content-agnostic motion transfer.  The bottom right image further emphasizes this capability, showcasing accurate motion transfer despite substantial changes in the object's position and scale in the generated video compared to the reference video.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.07776/extracted/6056616/figures/supp/supp_latentnn.png", "caption": "Figure 6: Human evaluation. We asked humans to evaluate agreement on the quality of generated samples in terms of motion (left) and prompt (right) adherence. DiTFlow consistently outperforms baselines in both evaluations.", "description": "This figure displays the results of a human evaluation comparing DiTFlow's performance to other baselines in video generation.  Participants rated generated videos on two aspects using a Likert scale: motion adherence (how well the generated video's motion matched the reference video) and prompt adherence (how well the generated video matched the textual prompt). The bar chart visually represents the distribution of responses for each model across the levels of agreement (strongly agree, agree, neutral, disagree, strongly disagree).  DiTFlow consistently outperforms the baselines in both motion and prompt adherence, demonstrating superior performance in capturing motion from reference videos and adhering to prompts.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.07776/extracted/6056616/figures/supp/supp_argmax.png", "caption": "(a) Quantitative evaluation. SMM and Ours-ztsubscript\ud835\udc67\ud835\udc61z_{t}italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT follow Table\u00a01 (All)", "description": "Figure 7a presents a quantitative comparison of the zero-shot motion transfer performance between two versions of the proposed DiTFlow method (optimizing latent representations z<sub>t</sub> and positional embeddings p) and the SMM baseline.  The results are based on the 'All' prompts category (Caption, Subject, Scene) from Table 1 and showcase the effectiveness of the zero-shot approach for various prompts.", "section": "5.3. Zero-shot generation"}]