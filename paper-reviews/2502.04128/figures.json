[{"figure_path": "https://arxiv.org/html/2502.04128/extracted/6183923/comparison_plot.png", "caption": "Figure 1: Comparison of mean expert score for Chinese and English", "description": "This figure presents a comparison of average expert scores for text understanding ability in both Chinese and English.  The scores are based on evaluations performed using several categories of texts designed to assess different facets of text understanding, such as emotional expressiveness, handling of complex syntax, and nuanced prosody. The evaluations were conducted by expert linguistic annotators who rated the generated speech according to predefined criteria. The figure visually shows how the scores vary across different models (Llasa models with varying parameter counts and training data sizes) to demonstrate the relationship between model capacity and training data and the overall text understanding capabilities of the speech synthesis system.", "section": "3.2. Scaling Train-time Compute"}, {"figure_path": "https://arxiv.org/html/2502.04128/extracted/6183923/comparison_plot2.png", "caption": "Figure 2: Illustration of test-time scaling", "description": "This figure illustrates the effects of scaling inference-time compute on the model's performance.  It shows how different search strategies (PRM with beam search, ORM with best-of-N, and a hybrid approach combining PRM and ORM) impact speaker similarity (SIM) and word error rate (WER). The x-axis represents beam width, a parameter related to the computational cost of the search. The y-axis shows the SIM and WER metrics, where higher SIM indicates better speaker similarity and lower WER indicates higher accuracy.  The figure demonstrates that increasing the inference-time compute generally improves speaker similarity, particularly with the partial-PRM approach, which strikes a balance between maximizing similarity and minimizing WER.", "section": "3.2.3. Scaling Inference-time Compute"}]