{"references": [{"fullname_first_author": "Brown, T.", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper is foundational for the field of large language models (LLMs), establishing the effectiveness of scaling and demonstrating that LLMs are few-shot learners, a concept central to the current study's approach to speech synthesis."}, {"fullname_first_author": "Kaplan, J.", "paper_title": "Scaling laws for neural language models", "publication_date": "2020-01-01", "reason": "This paper explores the scaling laws of LLMs, establishing the relationships between compute, data, and model size, a concept that directly relates to the current work's exploration of training and inference compute scaling for Llasa."}, {"fullname_first_author": "Radford, A.", "paper_title": "Language models are unsupervised multitask learners", "publication_date": "2019-01-01", "reason": "This paper is highly influential in the LLM field, showcasing the ability of LLMs to perform various tasks with minimal supervision, influencing the choice of a single transformer architecture with a well-designed tokenizer in the Llasa framework."}, {"fullname_first_author": "Anastassiou, P.", "paper_title": "Seed-tts: A family of high-quality versatile speech generation models", "publication_date": "2024-06-01", "reason": "This work directly addresses scaling behavior in TTS systems, introducing a range of models that serve as an important baseline for comparison against the proposed Llasa model."}, {"fullname_first_author": "Wang, C.", "paper_title": "Neural codec language models are zero-shot text to speech synthesizers", "publication_date": "2023-01-01", "reason": "This paper pioneers the neural codec approach for TTS, converting waveforms into neural codec tokens which are relevant to the X-codec2 speech tokenizer and the Llasa model in general."}]}