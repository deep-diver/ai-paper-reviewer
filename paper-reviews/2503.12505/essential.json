{"importance": "This paper introduces **a new benchmark**, addressing the gap in multimodal PRM evaluation. It enhances research by providing tools to **comprehensively assess** and improve reasoning in MLLMs, enabling advancements in AI.", "summary": "MPBench: Multimodal benchmark to identify errors in reasoning processes.", "takeaways": ["MPBench is a new multimodal benchmark for evaluating process-level reward models (PRMs).", "MPBench includes three evaluation paradigms: step correctness, answer aggregation, and reasoning process search.", "Experiments on 12 MLLMs reveal distinct performance characteristics and highlight areas for improvement."], "tldr": "**Reasoning in large language models (LLMs)** is crucial for complex tasks, yet identifying process errors remains a challenge. Current process-level reward models (PRMs) benchmarks are text-based and focus on error detection, overlooking reasoning search scenarios. To address this gap, the paper introduces a comprehensive, multi-task, multimodal benchmark designed to systematically assess the effectiveness of PRMs in diverse scenarios. \n\nThe paper introduces **MPBench**, which employs three evaluation paradigms targeting specific roles of PRMs: Step Correctness, Answer Aggregation, and Reasoning Process Search. MPBench includes 9,745 fine-grained data instances. Experiments with 12 multimodal LLMs (MLLMs), including GPT-40 and Gemini-2.0, reveal performance characteristics and guide future multimodal PRM development.", "affiliation": "HIT", "categories": {"main_category": "Multimodal Learning", "sub_category": "Multimodal Reasoning"}, "podcast_path": "2503.12505/podcast.wav"}