[{"figure_path": "2410.20290/tables/table_6_0.html", "caption": "Table 1: Win-rate results across various settings for the Mistral-7B, Llama-3-8B, and Llama-3-8B-Instruct models, scored by the reward model ArmoRM-Llama-3-8B and evaluated using GPT-4-Turbo. \"WR\" refers to win-rate, and \"LC-WR\" refers to length-controlled win-rate.", "description": "The table presents win-rate and length-controlled win-rate results for different language models and varying numbers of generations, comparing Best-of-N with the proposed Speculative Rejection method.", "section": "5.2 Win-rate Evaluation"}, {"figure_path": "2410.20290/tables/table_9_0.html", "caption": "Table 1: Win-rate results across various settings for the Mistral-7B, Llama-3-8B, and Llama-3-8B-Instruct models, scored by the reward model ArmoRM-Llama-3-8B and evaluated using GPT-4-Turbo. \"WR\" refers to win-rate, and \"LC-WR\" refers to length-controlled win-rate.", "description": "Table 1 shows the win rates and length-controlled win rates of different models and methods, comparing Best-of-N and Speculative Rejection, evaluated by GPT-4-Turbo.", "section": "5.2 Win-rate Evaluation"}, {"figure_path": "2410.20290/tables/table_10_0.html", "caption": "Table 1: Win-rate results across various settings for the Mistral-7B, Llama-3-8B, and Llama-3-8B-Instruct models, scored by the reward model ArmoRM-Llama-3-8B and evaluated using GPT-4-Turbo. \"WR\" refers to win-rate, and \"LC-WR\" refers to length-controlled win-rate.", "description": "Table 1 presents win-rate results for different language models, comparing the performance of SPECULATIVE REJECTION against Best-of-N using different settings and metrics.", "section": "5.2 Win-rate Evaluation"}]