{"references": [{" publication_date": "2020", "fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "reason": "This paper is foundational in establishing the capabilities of large language models (LLMs) as few-shot learners, a crucial element underlying the premise of efficient inference-time alignment techniques discussed throughout this paper.  The impressive capabilities of LLMs highlighted here directly motivate the search for efficient decoding methods to improve their real-world applicability, as post-training methods are deemed to be too computationally expensive.", "section_number": 1}, {" publication_date": "2017", "fullname_first_author": "Paul F Christiano", "paper_title": "Deep reinforcement learning from human preferences", "reason": "This is a seminal paper in reinforcement learning from human feedback (RLHF), a core method used for aligning LLMs.  The paper's impact on alignment techniques, particularly in the context of reward modeling, is significant, directly influencing the development and discussion of the Best-of-N method and the motivation for efficient inference-time alignment techniques.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "reason": "This paper introduces a prominent method for aligning LLMs, providing the foundation for understanding the effectiveness of Best-of-N. The concepts and techniques detailed here are essential to the context of the paper, serving as both a comparative baseline and a source of motivation for more efficient alignment strategies. The importance of human feedback in guiding alignment and influencing reward model design is highlighted.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Leo Gao", "paper_title": "Scaling laws for reward model overoptimization", "reason": "This is a crucial piece of work that establishes limitations and scaling behavior associated with the current alignment methods, including reward model overoptimization. The findings provide a strong theoretical foundation and practical implications for the development of computationally efficient inference-time alignment algorithms like Speculative Rejection by identifying challenges in scaling traditional reward models and the need for alternative alignment approaches.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Yann Dubois", "paper_title": "Alpacafarm: A simulation framework for methods that learn from human feedback", "reason": "The AlpacaFarm dataset serves as a key experimental benchmark in this paper for evaluation of the alignment methods, including the proposed Speculative Rejection method.  The paper that introduced this dataset serves as an important reference to understand the experimental setup and context of the findings.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Albert Q Jiang", "paper_title": "Mistral 7b", "reason": "The Mistral 7B model is used as a crucial component in several experiments throughout the paper, most prominently as a reward model in comparative evaluations. The introduction and key properties of this model are directly related to the practical methodology and results interpretation, particularly in the evaluation of the win-rate and overall effectiveness.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Afra Amini", "paper_title": "Variational best-of-n alignment", "reason": "This paper is highly relevant as it directly addresses the Best-of-N alignment method, a prominent inference-time strategy, comparing it to the proposed Speculative Rejection algorithm in the context of efficiency gains. The variational approach taken in this work aligns with broader trends in efficient decoding and provides a complementary perspective on the optimization landscape.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Lin Gui", "paper_title": "Bonbon alignment for large language models and the sweetness of best-of-n sampling", "reason": "This paper addresses a similar problem to the one considered in the current work; that is, improving the efficiency of inference-time alignment methods. Specifically, it focuses on improving the Best-of-N strategy.  The comparison of efficiency gains achieved in this related work strengthens the argument presented in this paper about the relative computational efficiency of Speculative Rejection.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Kwangjun Ahn", "paper_title": "Spectr++: Improved transport plans for speculative decoding of large language models", "reason": "This paper shares the concept of speculative decoding and its application to LLMs, which directly relates to the core methodology of Speculative Rejection. The techniques and insights of Spectr++ are relevant to the discussions and design choices in this paper, allowing for a comparison and highlighting the unique aspects of Speculative Rejection.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Charlie Chen", "paper_title": "Accelerating large language model decoding with speculative sampling", "reason": "This paper proposes speculative sampling, a closely related technique for accelerating LLM decoding. Comparing and contrasting the approaches helps establish the novelty and improvements offered by Speculative Rejection, providing context to understanding the contributions in this work.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Ziteng Sun", "paper_title": "Spectr: Fast speculative decoding via optimal transport", "reason": "This paper introduces the speculative decoding approach using optimal transport, which is conceptually related to the core idea in Speculative Rejection. By comparing and contrasting the proposed method with related techniques, this paper contributes to a deeper understanding of Speculative Rejection's novelty, efficiency, and potential limitations.", "section_number": 2}, {" publication_date": "2017", "fullname_first_author": "Thomas Coste", "paper_title": "Reward model ensembles help mitigate overoptimization", "reason": "This work addresses the challenge of over-optimization in reward models, which is highly relevant to the problem of aligning LLMs effectively.  The proposed method helps to mitigate the potential limitations of a single reward model, directly informing the choice of robust reward model evaluation strategies in the paper.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Rafael Rafailov", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "reason": "This paper directly addresses the use of reward models in LLM alignment, providing a strong theoretical and practical background for understanding the crucial role of reward models in this paper.  The insights and methods introduced here directly influence the design and evaluation choices of Speculative Rejection.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Rafael Rafailov", "paper_title": "From r to q*: Your language model is secretly a q-function", "reason": "This paper extends the understanding of reward models by treating them as Q-functions, offering a more nuanced perspective on reward modeling in the context of LLM alignment. This is highly relevant because Speculative Rejection relies heavily on reward modeling for efficient decoding, making this paper essential to understanding the theoretical foundations of the algorithm.", "section_number": 4}, {" publication_date": "1978", "fullname_first_author": "G\u00e9rard M Baudet", "paper_title": "On the branching factor of the alpha-beta pruning algorithm", "reason": "This classic work on the Alpha-Beta pruning algorithm provides a foundational understanding of efficient search techniques in game playing, which draws parallels to the problem of efficient decoding in LLMs.  The ideas of pruning and efficient search are central to the approach taken in Speculative Rejection, making this paper relevant to the theoretical foundations.", "section_number": 4}, {" publication_date": "1986", "fullname_first_author": "T Anthony Marsland", "paper_title": "A review of game-tree pruning", "reason": "This paper offers a broad overview of game-tree pruning techniques, which is directly relevant to understanding the rationale behind early stopping employed in Speculative Rejection. The concepts and strategies discussed in this review inform the design of Speculative Rejection and help establish the context for using similar ideas in LLMs.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Hanze Dong", "paper_title": "RAFT: Reward ranked finetuning for generative foundation model alignment", "reason": "This paper introduces the RAFT method, focusing on reward-based fine-tuning for alignment.  It provides an alternative and complementary perspective to the inference-time approach taken in the current work.  A comparison of the RAFT and Speculative Rejection methods enhances the understanding of the comparative advantages and efficiency.", "section_number": 4}, {" publication_date": "2020", "fullname_first_author": "Nisan Stiennon", "paper_title": "Learning to summarize with human feedback", "reason": "This paper provides valuable context for reinforcement learning from human feedback (RLHF) which is a major technique for LLM alignment.  The insights into training LLMs with human feedback are crucial to understanding the design and implications of reward modeling in the current work.  The use of human feedback is closely connected to the reward function used in Speculative Rejection, which helps to measure and improve the quality of generated responses.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Yann Dubois", "paper_title": "Length-controlled alpacaeval: A simple way to debias automatic evaluators", "reason": "This paper proposes a new metric for evaluating LLM outputs, directly addressing the challenges of using existing metrics to assess the quality of automatically generated text. This paper is important for understanding the selection of evaluation metrics and their limitations used in the present work. In particular, the insights on debiased automatic evaluation are critical to the robustness of the results presented in this paper.", "section_number": 5}]}