{"references": [{" publication_date": "2022", "fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "reason": "This paper is foundational to the field of RLHF, introducing a key technique for aligning LLMs with human preferences. Its influence on the development and application of reward models is immense, making it a cornerstone reference for the current work.  The authors' method of using human feedback to train language models to follow instructions is directly relevant to the current paper's focus on improving reward model evaluation, which is crucial for effective RLHF.", "section_number": 1}, {" publication_date": "2022a", "fullname_first_author": "Yuntao Bai", "paper_title": "Training a helpful and harmless assistant with reinforcement learning from human feedback", "reason": "This paper significantly advanced the RLHF technique and contributed to a better understanding of reward models in LLM alignment. The detailed explanation of their method using human feedback to train a helpful and harmless LLM is directly related to the focus of the current paper on evaluating reward models, which are central to the success of RLHF.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Charlie Snell", "paper_title": "Scaling LLM test-time compute optimally can be more effective than scaling model parameters", "reason": "This paper explores the efficiency aspect of LLMs, particularly focusing on inference scaling laws. The authors' investigation of using reward models to select the best response from a set of candidates is directly relevant to the current work's goal of improving reward model benchmarks. Optimizing resource utilization during the inference phase is crucial for the adoption of large language models, and the role of reward models in this process is essential.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Nathan Lambert", "paper_title": "RewardBench: Evaluating reward models for language modeling", "reason": "This paper is highly relevant as it represents one of the most recent efforts to create a benchmark for evaluating reward models. Its detailed methodology, especially focusing on preference datasets, offers valuable insights and a point of comparison for the proposed RM-BENCH.  Analyzing both the strengths and weaknesses of RewardBench helps to contextualize the development and design choices behind RM-BENCH.", "section_number": 1}, {" publication_date": "2021", "fullname_first_author": "Jacob Austin", "paper_title": "Program synthesis with large language models", "reason": "This paper is important because it highlights the growing use of LLMs for tasks beyond text generation. The focus on using LLMs for program synthesis directly relates to the code domain in RM-BENCH, which uses code generation tasks to test the reward model\u2019s abilities.  Evaluating reward models within the context of program synthesis is an important aspect of broader LLM evaluation.", "section_number": 1}, {" publication_date": "2020", "fullname_first_author": "Dan Hendrycks", "paper_title": "Measuring massive multitask language understanding", "reason": "This paper is highly relevant because it's a seminal work in benchmarking LLMs across multiple tasks. The authors' emphasis on evaluating various aspects of LLM performance is directly relevant to the current paper's goal of creating a comprehensive benchmark for reward models.  Understanding how to evaluate LLMs across multiple tasks informs the design of RM-BENCH, including its focus on various domains.", "section_number": 1}, {" publication_date": "2021", "fullname_first_author": "Dan Hendrycks", "paper_title": "Measuring Mathematical Problem Solving With the MATH Dataset", "reason": "This paper is important because it introduces a benchmark dataset specifically for evaluating mathematical problem-solving capabilities of LLMs, which is directly relevant to the math domain of RM-BENCH.  The focus on mathematical reasoning tasks and the availability of a benchmark dataset influenced the development and design choices behind RM-BENCH.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Wei-Lin Chiang", "paper_title": "Chatbot arena: An open platform for evaluating llms by human preference", "reason": "This paper introduces an innovative approach to evaluating LLMs using human preferences, which is directly relevant to the chat domain in RM-BENCH.  The use of human preferences as a basis for evaluating LLM responses is a key feature in the current paper\u2019s benchmark development. Understanding the complexities of human preferences in LLM evaluation helps to inform the design choices behind RM-BENCH.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Banghua Zhu", "paper_title": "Starling-7B: Improving LLM helpfulness & harmlessness with RLAIF", "reason": "This work is notable for its focus on improving the helpfulness and harmlessness of LLMs, which is directly related to the safety domain within RM-BENCH.  The authors\u2019 approach of utilizing reinforcement learning from AI feedback to enhance these aspects of LLMs directly influenced the design and development of the safety-related components within RM-BENCH.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Hamish Ivison", "paper_title": "Camels in a Changing Climate: Enhancing LM Adaptation with T\u00fclu 2", "reason": "This paper is highly relevant due to its focus on improving LLM adaptation and its use of the Tulu models, which are used in the experiments of the current work. The detailed analysis of the Tulu models' performance and adaptation strategies is directly relevant to the current work's goal of providing a better benchmark for evaluating reward models in LLM adaptation tasks.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Bo Adler", "paper_title": "Nemotron-4 340b technical report", "reason": "This paper is significant due to its introduction of a large-scale reward model (Nemotron-340B-Reward), which is included in the experiments of the current work. The detailed technical report on Nemotron provides valuable insights and context for interpreting the benchmark results. Understanding the capabilities and limitations of state-of-the-art reward models helps to contextualize the findings of the current work.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Cem Anil", "paper_title": "Many-shot jailbreaking", "reason": "This paper is significant for its detailed study of jailbreaking techniques, which are directly relevant to the design of RM-BENCH's methodology. The authors' approach of injecting errors into responses is directly related to the techniques used to create the rejected responses in RM-BENCH. Understanding the implications of jailbreaking techniques is crucial for developing robust and reliable reward model benchmarks.", "section_number": 3}, {" publication_date": "2017", "fullname_first_author": "John Schulman", "paper_title": "Proximal policy optimization algorithms", "reason": "This paper is foundational to the field of reinforcement learning, introducing the Proximal Policy Optimization (PPO) algorithm.  The use of PPO in training the policy models in the current work's experiments makes this paper a key reference.  Understanding the fundamentals of PPO helps in interpreting the correlation between reward model performance and policy model training.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Ganqu Cui", "paper_title": "UltraFeedback: Boosting Language Models with High-quality Feedback", "reason": "This paper is highly relevant as it focuses on improving LLM training with high-quality feedback, which is directly related to the goals of using reward models in LLM alignment. The method of using UltraFeedback to enhance LLM performance highlights the critical role of reward models and reinforces the importance of having effective benchmarks for their evaluation.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Huggingface", "paper_title": "stack exchange preference dataset", "reason": "This dataset is crucial to the current work because it serves as a source of preference data for training reward models and policy models.  The utilization of this dataset in the current study highlights its importance in LLM alignment and shows the significant impact of high-quality preference datasets on both reward model and policy model performance.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Chris Yuhao Liu", "paper_title": "Skywork reward model series", "reason": "This work is highly relevant due to the inclusion of the Skywork reward model in the current study's evaluation of various models on RM-BENCH.  This model serves as a state-of-the-art example, and its performance within the context of RM-BENCH is directly relevant to assessing the overall effectiveness of the benchmark.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Xuechen Li", "paper_title": "AlpacaEval: An Automatic Evaluator of Instruction-following Models", "reason": "This paper is crucial as it provides a benchmark dataset specifically for evaluating instruction-following capabilities of LLMs, which is directly relevant to the chat domain within RM-BENCH. The focus on instruction-following tasks and the availability of a benchmark dataset influenced the design and development of RM-BENCH.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Niklas Muennighoff", "paper_title": "OctoPack: Instruction Tuning Code Large Language Models", "reason": "This paper is relevant due to its emphasis on fine-tuning LLMs for code generation tasks, aligning with the code domain of RM-BENCH.  The study of instruction tuning techniques and their application to code generation tasks directly influences the design of the code-related component of RM-BENCH.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Paul R\u00f6ttger", "paper_title": "XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models", "reason": "This work directly addresses safety concerns in LLMs, providing a benchmark dataset for evaluating safety-related behaviors, directly influencing the safety domain of RM-BENCH. The authors\u2019 approach to identifying and evaluating safety-related aspects of LLMs is highly relevant to the current study's design of the safety domain within RM-BENCH.", "section_number": 3}]}