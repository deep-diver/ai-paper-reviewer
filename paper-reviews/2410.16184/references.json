{"references": [{" publication_date": "2022", "fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "reason": "This paper is foundational to the field of RLHF, introducing a method for aligning language models with human preferences.  The paper's methodology and results are widely cited and have significantly influenced subsequent research on reward model development and evaluation.  Its contribution to the understanding and application of RLHF makes it a cornerstone of the current LLM alignment landscape.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Yuntao Bai", "paper_title": "Training a helpful and harmless assistant with reinforcement learning from human feedback", "reason": "This paper presents a significant advancement in RLHF, detailing a method for training helpful and harmless language models. The techniques and results presented are highly influential in current research and development, and the paper's emphasis on safety and helpfulness in language model design is widely recognized and cited.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Charlie Snell", "paper_title": "Scaling LLM test-time compute optimally can be more effective than scaling model parameters", "reason": "This paper challenges the conventional wisdom regarding LLM scaling, proposing that optimized test-time compute is more effective than simply scaling model size. This is highly relevant to the reward model evaluation context, as it raises questions about the most efficient ways to use LLMs in these scenarios and has impacted the efficiency discussion of inference scaling laws.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Nathan Lambert", "paper_title": "Rewardbench: Evaluating reward models for language modeling", "reason": "This paper is a direct precursor to the current work.  It introduces a benchmark for reward models but is criticized in the current paper for its shortcomings. Understanding the limitations of RewardBench makes the proposed RM-BENCH more impactful and better-justified.", "section_number": 1}, {" publication_date": "2020", "fullname_first_author": "Dan Hendrycks", "paper_title": "Measuring massive multitask language understanding", "reason": "This paper introduces a comprehensive benchmark for evaluating various aspects of language model capabilities.  While not directly focused on reward models, it provides a context for the broader evaluation of language models, including the evaluation of reward models' alignment with human preferences.", "section_number": 1}, {" publication_date": "2021", "fullname_first_author": "Jacob Austin", "paper_title": "Program synthesis with large language models", "reason": "This paper explores the application of large language models to program synthesis, a task that often involves evaluation of code correctness and quality.  The evaluation criteria used in this context relate directly to the design considerations of reward models, especially regarding code generation tasks.", "section_number": 1}, {" publication_date": "2021", "fullname_first_author": "Mark Chen", "paper_title": "Evaluating large language models trained on code", "reason": "This paper focuses on evaluating large language models trained specifically on code, which provides a relevant context for the code domain within the RM-BENCH. Its evaluation metrics and methodology provide insights into the challenges of assessing code quality, which are crucial considerations for evaluating reward models in the code domain.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "BIG bench authors", "paper_title": "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models", "reason": "This paper provides a comprehensive benchmark for language models, offering a broader context for evaluating the role of reward models within the larger ecosystem of language model evaluation.  Understanding the strengths and weaknesses of various large language models is essential for proper reward model benchmarking and calibration.", "section_number": 1}, {" publication_date": "2021", "fullname_first_author": "Dan Hendrycks", "paper_title": "Measuring Mathematical Problem Solving With the MATH Dataset", "reason": "This paper focuses on measuring mathematical problem-solving capabilities of language models, which is highly relevant to the RM-BENCH Math domain.  The dataset and evaluation methodology provide valuable insights into the challenges of assessing mathematical reasoning abilities, essential aspects when evaluating reward models within this domain.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Hamish Ivison", "paper_title": "Camels in a Changing Climate: Enhancing LM Adaptation with T\u00fclu 2", "reason": "This work is crucial as it provides the policy models and reward models used in the correlation experiments in Section 5.  The Tulu-2 models are state-of-the-art and their use demonstrates the practical application of RM-BENCH with high-performance models.", "section_number": 5}, {" publication_date": "2017", "fullname_first_author": "John Schulman", "paper_title": "Proximal policy optimization algorithms", "reason": "This paper introduces the Proximal Policy Optimization (PPO) algorithm, a key reinforcement learning technique used for training the policy models in the experimental section. Understanding PPO is critical for interpreting the results and the relationships between reward model performance and policy model improvements.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Ganqu Cui", "paper_title": "UltraFeedback: Boosting Language Models with High-quality Feedback", "reason": "This paper is relevant due to its focus on reward modeling and improving language model performance via feedback. It contributes to the broader context of reward model research, and its techniques and methodologies are comparable to those used in RM-BENCH, making it highly relevant to the study.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Bo Adler", "paper_title": "Nemotron-4 340b technical report", "reason": "This paper describes Nemotron-340B-Reward, a state-of-the-art reward model.  The model is evaluated in section 4 of this paper and shows that even state-of-the-art models require improvement, thus underlining the need for RM-BENCH.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Chris Yuhao Liu", "paper_title": "Skywork reward model series", "reason": "This paper introduces Skywork-Reward, another state-of-the-art reward model used in the evaluation in Section 4. It further emphasizes the importance of comparing current reward models and assessing their strengths and weaknesses using a comprehensive benchmark such as RM-BENCH.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Banghua Zhu", "paper_title": "Starling-7B: Improving LLM Helpfulness & Harmlessness with RLAIF", "reason": "This paper demonstrates another advanced reward model. The comparison with other models in section 4 helps to further showcase the limitations of existing reward models and reinforces the significance of a robust benchmark like RM-BENCH in assessing their capabilities.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Zhilin Wang", "paper_title": "Helpsteer2: Open-source dataset for training top-performing reward models", "reason": "This paper offers a dataset related to reward model training, highlighting the importance of using high-quality datasets in developing reliable reward models. It's relevant to the current work because the quality and characteristics of the training data directly impact the performance and reliability of reward models, thereby influencing the effectiveness of benchmarks like RM-BENCH.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Cem Anil", "paper_title": "Many-shot jailbreaking", "reason": "This paper describes the jailbreaking technique used in generating the rejected responses for RM-BENCH, especially in the chat domain. Understanding this technique is important for replicating the benchmark and appreciating the difficulty of creating subtle yet meaningful errors to evaluate reward models effectively.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Wei-Lin Chiang", "paper_title": "Chatbot Arena: An open platform for evaluating LLMs by human preference", "reason": "This paper introduces a large-scale evaluation platform, Chatbot Arena, which focuses on human preference comparisons for language models. This is important for understanding the broader context of reward model evaluation, particularly regarding how human preferences are used to guide the design and development of effective benchmarks.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Paul R\u00f6ttger", "paper_title": "XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models", "reason": "This paper presents XSTest, a benchmark specifically designed to evaluate the safety performance of large language models. This is highly relevant to RM-BENCH's Safety domain because safety is a critical aspect that needs to be carefully considered when assessing the performance of reward models and the overall alignment of language models with human values.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Yangzhen Wu", "paper_title": "An empirical analysis of compute-optimal inference for problem-solving with language models", "reason": "This paper discusses compute-optimal inference for problem-solving with language models.  Understanding compute optimization is important in the context of reward model evaluation because it directly impacts the cost and feasibility of using reward models for various applications. The efficiency aspect discussed is crucial for evaluating real-world applicability and scalability of any reward model benchmark.", "section_number": 4}]}