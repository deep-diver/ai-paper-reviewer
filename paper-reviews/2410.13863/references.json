{"references": [{" publication_date": "2020", "fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "reason": "This paper is foundational for understanding scaling laws in large language models. It demonstrates the impressive capabilities of large language models even with few examples, which is a direct comparison to what this work attempts to achieve in computer vision.  The findings from this paper are a critical baseline to the hypotheses made and work performed within this paper.", "section_number": 1}, {" publication_date": "2020", "fullname_first_author": "Jared Kaplan", "paper_title": "Scaling laws for neural language models", "reason": "This is a seminal work on scaling laws for language models, establishing a strong empirical relationship between model size, dataset size, and computational resources, and their impact on model performance.  This paper's findings provide the context for the investigation in this paper of whether similar scaling laws apply to image generation models.", "section_number": 1}, {" publication_date": "2018", "fullname_first_author": "Jacob Devlin", "paper_title": "Bert: Pre-training of deep bidirectional transformers for language understanding", "reason": "BERT is a highly influential model architecture used as a foundation for many text-to-image models.  This work's influence extends to how this paper uses BERT-like architectures to tackle the text-to-image generation problem, particularly in the context of random-order autoregressive model designs.", "section_number": 1}, {" publication_date": "2018", "fullname_first_author": "Alec Radford", "paper_title": "Improving language understanding by generative pre-training", "reason": "This paper introduced the concept of generative pre-training, a critical technique used in many large language models, providing a framework for effectively training models to capture contextual information. This paper's contributions are relevant to this work in that both language and image models are trained with similar autoregressive techniques.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Aditya Ramesh", "paper_title": "Hierarchical text-conditional image generation with clip latents", "reason": "DALL-E 2 is a highly impactful text-to-image model.  This paper is a benchmark for comparing the performance of autoregressive models in image generation and it is relevant to this paper's work which aims to improve upon existing text-to-image models.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Robin Rombach", "paper_title": "High-resolution image synthesis with latent diffusion models", "reason": "Stable Diffusion is a highly influential and widely used text-to-image model, known for its ability to generate high-quality, realistic images.  Understanding its approach and strengths compared to the autoregressive model is essential for this paper's work on scaling autoregressive models.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Jonathan Ho", "paper_title": "Imagen video: High definition video generation with diffusion models", "reason": "Imagen represents a top-performing text-to-image model that sets a high benchmark. This paper's model is compared to Imagen (and its successor Imagen 3) in terms of both qualitative image generation and quantitative metrics such as FID and GenEval scores.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Yutong Bai", "paper_title": "Sequential modeling enables scalable learning for large vision models", "reason": "This recent work directly tackles scaling in vision models, a key focus of this paper.  Its findings on sequential modeling and large vision models are highly relevant to this work's research goals on improving autoregressive image generation model scaling.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Tianhong Li", "paper_title": "Autoregressive image generation without vector quantization", "reason": "This paper introduces a key innovation which directly addresses a central limitation of many existing autoregressive models.  Using continuous tokens and a diffusion loss enables improvements in both training efficiency and generation quality; it also provides a direct baseline for this paper's exploration of continuous tokens.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "Tom Henighan", "paper_title": "Scaling laws for autoregressive generative modeling", "reason": "This paper establishes scaling laws for autoregressive models in a more general context.  Its findings regarding the relationship between model size, data size, and compute are critical for understanding the scaling behavior of the models being studied in this paper.", "section_number": 5}, {" publication_date": "2017", "fullname_first_author": "Martin Heusel", "paper_title": "GANs trained by a two time-scale update rule converge to a local nash equilibrium", "reason": "The Fr\u00e9chet Inception Distance (FID) score is a critical metric used to evaluate the quality of generated images.  This paper introduces FID, a commonly used evaluation metric, which provides the basis for the evaluation in this paper.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Dhruba Ghosh", "paper_title": "Geneval: An object-focused framework for evaluating text-to-image alignment", "reason": "GenEval is a comprehensive benchmark for evaluating the performance of text-to-image generation models.  This paper provides the basis for a key evaluation metric used in this paper, allowing for a more detailed assessment of model quality beyond FID.", "section_number": 5}, {" publication_date": "2022", "fullname_first_author": "Chitwan Saharia", "paper_title": "Photorealistic text-to-image diffusion models with deep language understanding", "reason": "Imagen is a leading text-to-image generation model and this paper describes its architecture and performance, establishing a strong baseline for this paper's efforts to scale autoregressive text-to-image models,  The performance of the authors' model is compared to Imagen throughout the paper.", "section_number": 5}, {" publication_date": "2017", "fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "reason": "The Transformer architecture is central to many modern language and image models, including the models used in this paper.  This seminal paper details the Transformer architecture, explaining its key components and contributions to both natural language processing and computer vision.  Understanding the transformer architecture is essential to understanding the architectures used in this paper.", "section_number": 4}, {" publication_date": "2020", "fullname_first_author": "Colin Raffel", "paper_title": "Exploring the limits of transfer learning with a unified text-to-text transformer", "reason": "This paper introduces T5, a powerful text encoder used as a component of the text-to-image generation models in this work.  This model provides an efficient and effective way to encode text, allowing the focus of the paper to be on the image generation aspects of the model.", "section_number": 4}, {" publication_date": "2021", "fullname_first_author": "Prafulla Dhariwal", "paper_title": "Diffusion models beat GANs on image synthesis", "reason": "This paper demonstrates the effectiveness of diffusion models in image generation, highlighting their superior performance compared to Generative Adversarial Networks (GANs). This is relevant because the work in this paper aims to improve upon other autoregressive models, but also to compare the efficacy and capabilities of the autoregressive model against the diffusion model.", "section_number": 4}, {" publication_date": "2021", "fullname_first_author": "Alexander Quinn Nichol", "paper_title": "Improved denoising diffusion probabilistic models", "reason": "This paper details improvements to denoising diffusion probabilistic models (DDPMs), which are closely related to diffusion models used for image generation.  Understanding these advancements is essential to this paper's work and is relevant to the use of continuous tokens and diffusion models in image generation.", "section_number": 4}, {" publication_date": "2021", "fullname_first_author": "Patrick Esser", "paper_title": "Taming Transformers for high-resolution image synthesis", "reason": "This paper is relevant to the implementation of this work because it discusses the use of transformers for image synthesis, a key aspect of the model architecture presented in this paper.  Furthermore, understanding the use of transformers in high-resolution image synthesis provides a direct baseline for this paper's work on scaling autoregressive image generation models.", "section_number": 4}, {" publication_date": "2022", "fullname_first_author": "Huiwen Chang", "paper_title": "Muse: Text-to-image generation via masked generative Transformers", "reason": "This paper is highly relevant as it explores autoregressive models for text-to-image generation using masking techniques, similar to the approach used in this paper. Understanding the masking strategy and the performance of similar models provides valuable context for this paper's work on scaling autoregressive models.", "section_number": 5}, {" publication_date": "2022", "fullname_first_author": "Jordan Hoffmann", "paper_title": "Training compute-optimal large language models", "reason": "This paper explores compute-optimal training strategies for large language models. Its findings and insights on optimal training strategies and resource allocation are highly relevant to this paper's investigation of scaling autoregressive image generation models, particularly in terms of resource utilization and training efficiency.", "section_number": 5}]}