[{"content": "| Benchmark | Strictly Contamination-Free | Automated | Multilingual | Data Source |\n|---|---|---|---|---| \n| Realtime QA | \u2717 | \u2717 | \u2717 | Real world |\n| LiveBench | \u2717 | \u2717 | \u2717 | Real world |\n| ADU | \u2717 | \u2713 | \u2717 | LLM generation |\n| **AntiLeak-Bench** | \u2713 | \u2713 | \u2713 | Real world |", "caption": "Table 1: \nComparisons between AntiLeak-Bench and other benchmarking frameworks.", "description": "This table compares AntiLeak-Bench with other benchmarks like RealtimeQA, LiveBench, and ADU, based on four criteria: strictly contamination-free, automated, multilingual, and data source.  It highlights that AntiLeak-Bench is the only benchmark that satisfies all four criteria.", "section": "2 Related Work"}, {"content": "| Attributes | Examples |\n|---|---| \n| question (generation) | What sports team is Lionel Andr\u00e9s Messi a member of? |\n| answer (generation) | Inter Miami CF\nInter Miami\nClub Internacional de F\u00fatbol Miami |\n| question (multi-choice) | What sports team is Lionel Andr\u00e9s Messi a member of?\nA. Inter Miami CF\nB. Paris Saint-Germain F.C.\nC. Prime Minister of Romania\nD. Unknown. |\n| answer (multi-choice) | A |\n| subject | Lionel Messi\nLionel Andres Messi\nLionel Andr\u00e9s Messi |\n| pid | P54 (member of sports team) |\n| object | Inter Miami CF\nInter Miami\nClub Internacional de F\u00fatbol Miami |\n| object_old | Paris Saint-Germain F.C.\nParis Saint-Germain Football Club\nParis Saint-Germain FC |\n| context | Lionel Andr\u00e9s Messi (; born 24 June 1987), also known as Leo Messi, is an Argentine professional footballer who plays as a forward for Major League Soccer club Inter Miami\u2026 |", "caption": "Table 2: \nAn example from AntiLeak-Bench.", "description": "This table presents an example from the AntiLeak-Bench, demonstrating how questions, answers, and contexts are structured within the benchmark.  It includes examples for both Generation and Multi-Choice question formats. The attributes provided are 'question' (in both formats), 'answer' (in both formats), 'subject', 'pid' (property ID), 'object', 'object_old', and 'context'. The table showcases the different components used to create a contamination-free example.", "section": "3 AntiLeak-Bench"}, {"content": "| Quality Metrics | Single-Hop Gold | Multi-Hop Gold |\n|---|---|---| \n| Context Accuracy | 97.3 | 98.7 |\n| Answer Accuracy | 96.7 | 97.3 |", "caption": "Table 3: \nData quality by human verification.", "description": "This table presents the human evaluation results of the generated samples' context and answer accuracy for single-hop and multi-hop question answering.  The results demonstrate high accuracy for both contexts and answers in the generated samples, indicating that the samples are of good quality.", "section": "3. AntiLeak-Bench"}, {"content": "| Language Models | Single-Hop | Single-Hop | Single-Hop | Single-Hop | Multi-Hop | Multi-Hop | Multi-Hop | Multi-Hop | Avg | Avg |\n|---|---|---|---|---|---|---|---|---|---|---| \n| | Gold | F1 | $N_d$=3 | F1 | $N_d$=5 | F1 | $N_d$=7 | Gold | F1 | $N_d$=3 | F1 | $N_d$=5 | F1 | $N_d$=7 | EM | F1 |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---| \n| Llama-2-7B | 40.6 | 63.5 | 16.8 | 41.2 | 11.6 | 30.9 | 9.4 | 24.5 | 33.6 | 50.2 | **19.4** | **32.2** | **15.8** | **28.1** | **12.2** | **22.7** |\n| Llama-2-13B | 42.7 | 65.3 | 14.0 | 40.6 | 9.4 | 30.6 | 7.0 | 24.0 | 13.3 | 34.6 | 4.1 | 21.5 | 2.7 | 17.8 | 2.3 | 15.2 |\n| Mistral-7B | 65.4 | 77.2 | 27.8 | 41.3 | 16.7 | 27.3 | 7.3 | 15.3 | 21.4 | 27.9 | 11.5 | 17.2 | 8.1 | 14.3 | 6.5 | 11.1 |\n| Vicuna-v1.5-7B | 66.8 | 79.9 | 39.1 | 60.4 | 25.8 | 48.3 | 15.3 | 39.1 | 26.0 | 43.5 | 11.1 | 22.9 | 8.1 | 19.5 | 5.4 | 15.7 |\n| Longchat-v1.5-7B | **75.5** | **84.5** | **58.2** | **72.8** | **47.6** | **65.5** | **37.0** | **56.3** | **38.8** | **51.4** | 17.6 | 30.6 | 12.0 | 25.8 | 4.7 | 3.9 |\n| Llama-3.1-8B | 19.2 | 66.2 | 21.4 | 59.4 | 18.1 | 53.5 | 14.2 | 45.7 | 24.4 | 50.2 | 11.7 | 33.0 | 9.4 | 27.5 | 6.8 | 21.9 |\n| Phi-3.5-mini | 69.0 | 78.7 | 34.0 | 40.5 | 26.5 | 33.7 | 15.2 | 22.2 | 45.4 | 59.7 | 20.8 | 29.5 | 14.9 | 21.1 | 9.8 | 14.4 |\n| Qwen-2-7B | 54.8 | 72.4 | 15.5 | 38.5 | 9.8 | 26.6 | 7.2 | 21.2 | 35.9 | 48.3 | 23.7 | 33.4 | 18.1 | 26.1 | 13.6 | 20.1 |\n| Mistral-Nemo-12B | 82.7 | 89.7 | 75.6 | 83.8 | 66.3 | 75.1 | 51.8 | 62.2 | 57.7 | 67.3 | 39.1 | 47.7 | 33.8 | 41.4 | 24.0 | 29.0 |\n| Gemma-2-9B | **85.0** | **91.6** | 80.2 | 86.2 | 68.8 | 75.2 | 55.4 | 61.2 | **82.7** | **86.4** | 63.0 | 68.3 | 55.8 | 61.2 | 49.0 | 53.5 |\n| GPT-4o-mini | 78.5 | 88.1 | 80.3 | 89.2 | 79.1 | 88.1 | 79.2 | 88.5 | 68.8 | 83.1 | 60.5 | 75.3 | 57.1 | 73.1 | 54.2 | 70.6 |\n| GPT-4o | 81.2 | 89.5 | **84.1** | **90.8** | **83.5** | **90.3** | **84.8** | **91.4** | 71.5 | 85.9 | **71.9** | **86.1** | **70.2** | **84.8** | **70.2** | **84.8** |", "caption": "Table 4: \nEM (Exact Match) and F1 results in the generation format on AntiLeak-Bench.\nGold means only gold documents; Ndsubscript\ud835\udc41\ud835\udc51N_{d}italic_N start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT is the number of distracting documents.\nThe best is in bold.", "description": "This table presents the Exact Match (EM) and F1 scores for several Large Language Models (LLMs) evaluated on the AntiLeak-Bench using the generation format.  The benchmark evaluates LLMs' ability to answer questions about updated real-world knowledge, while mitigating data contamination. Results are reported for different conditions: 'Gold' signifies evaluations with only relevant supporting documents provided, while 'Ndsubscript\ud835\udc41\ud835\udc51N_{d}italic_N start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT' denotes evaluations with an increasing number (3, 5, or 7) of distracting documents included in the context.  Higher EM and F1 scores signify better performance and the highest scores are highlighted in bold.  This allows for an analysis of LLM performance under varying difficulty levels within the AntiLeak-Bench.", "section": "4 Experiment"}, {"content": "| Language Models | Single-Hop | Single-Hop | Single-Hop | Single-Hop | Multi-Hop | Multi-Hop | Multi-Hop | Multi-Hop | Avg | Avg |\n|---|---|---|---|---|---|---|---|---|---|---| \n| | Gold | F1 | $N_d$=3 | F1 | $N_d$=5 | F1 | $N_d$=7 | F1 | Gold | F1 | $N_d$=3 | F1 | $N_d$=5 | F1 | $N_d$=7 | F1 | Acc | F1 |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---| \n| Llama-2-7B | 41.7 | 30.7 | 3.7 | 5.6 | 3.5 | 5.3 | 2.8 | 5.4 | 18.7 | 30.9 | 6.8 | 9.9 | 5.6 | 8.1 | 3.6 | 6.9 | 10.8 | 12.9 |\n| Llama-2-13B | **82.1** | **82.2** | 73.7 | 73.6 | 60.1 | 59.9 | 51.7 | 51.3 | **97.5** | **97.5** | **88.5** | **88.5** | **82.8** | **83.1** | 75.2 | 75.2 | 76.5 | 76.4 |\n| Mistral-7B | 81.8 | 81.8 | 65.9 | 65.8 | 58.3 | 58.2 | 52.3 | 52.3 | 88.7 | 88.6 | 77.2 | 77.2 | 72.7 | 72.8 | 67.7 | 67.2 | 70.6 | 70.5 |\n| Vicuna-v1.5-7B | 80.1 | 80.0 | **75.6** | **75.4** | **73.1** | **72.9** | **69.6** | **69.4** | 96.8 | 96.9 | 84.0 | 84.2 | 82.6 | 83.0 | **77.0** | **77.2** | **79.8** | **79.9** |\n| Longchat-v1.5-7B | 79.6 | 79.7 | 68.5 | 68.8 | 65.1 | 51.8 | 62.3 | 61.2 | 93.2 | 93.4 | 76.7 | 78.0 | 70.4 | 71.5 | 66.6 | 68.0 | 72.8 | 71.6 |\n| Llama-3.1-8B | 86.7 | 90.4 | 62.2 | 74.0 | 48.9 | 62.9 | 37.8 | 52.9 | 70.5 | 81.4 | 50.7 | 64.8 | 40.9 | 56.2 | 30.8 | 44.9 | 53.6 | 65.9 |\n| Phi-3.5-mini | 87.4 | 87.5 | 85.6 | 85.8 | 84.7 | 85.4 | 79.6 | 82.5 | 96.5 | 97.0 | 85.3 | 86.2 | 78.0 | 80.3 | 68.6 | 72.3 | 83.2 | 84.6 |\n| Qwen-2-7B | 89.1 | 39.7 | 83.0 | 27.9 | 78.2 | 24.6 | 77.0 | 78.5 | 97.6 | 98.3 | 94.5 | 54.2 | 92.4 | 46.4 | 91.5 | 91.7 | 87.9 | 57.7 |\n| Mistral-Nemo-12B | 88.5 | 71.1 | 88.8 | 71.8 | 84.7 | 70.2 | 77.8 | 83.8 | 91.1 | 94.6 | 77.1 | 68.4 | 69.9 | 64.0 | 43.1 | 58.7 | 77.6 | 72.8 |\n| Gemma-2-9B | 92.4 | 92.4 | 86.7 | 86.5 | 76.9 | 61.6 | 69.4 | 69.3 | 97.1 | 97.1 | 88.3 | 88.3 | 81.8 | 65.4 | 77.4 | 77.4 | 83.8 | 79.8 |\n| GPT-4o-mini | **93.2** | **93.2** | **93.8** | **93.8** | 93.3 | 93.3 | 93.5 | 93.5 | **98.5** | **98.5** | **96.4** | **96.4** | **95.4** | **95.4** | 93.5 | 93.5 | **94.7** | **94.7** |\n| GPT-4o | 92.8 | 92.8 | 93.5 | 93.5 | **94.0** | **94.0** | **94.0** | **94.0** | 97.9 | 97.9 | 95.8 | 95.8 | **95.4** | **95.4** | **93.9** | **93.9** | 94.7 | 94.7 |\n", "caption": "Table 5: \nAcc and F1 results in the multi-choice format on AntiLeak-Bench.\nGold means only gold documents; Ndsubscript\ud835\udc41\ud835\udc51N_{d}italic_N start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT is the number of distracting documents.\nThe best is in bold.", "description": "This table presents the accuracy (Acc) and F1 scores of several large language models (LLMs) on the AntiLeak-Bench using the multi-choice question format. The benchmark evaluates LLMs' ability to answer questions correctly given a context, where \"Gold\" refers to providing only the gold standard supporting document as context.  \"N_d\" represents the number of additional distracting documents added to the context, increasing the task's difficulty by requiring the models to filter out irrelevant information. The table compares LLM performance across different levels of distraction (N_d = 3, 5, 7) and identifies the best-performing model for each setting with bold formatting.", "section": "4 Experiment"}, {"content": "| Time period | Single-Hop | | | | Multi-Hop | | | |\n|---|---|---|---|---|---|---|---|---|\n| | Gold | _N_<sub>d</sub>=3 | _N_<sub>d</sub>=5 | _N_<sub>d</sub>=7 | Gold | _N_<sub>d</sub>=3 | _N_<sub>d</sub>=5 | _N_<sub>d</sub>=7 |\n| 2022-01-01 to 2023-01-01 | 1090 | 1089 | 1088 | 1088 | 443 | 443 | 443 | 443 |\n| 2023-05-01 to 2024-08-01 | 819 | 818 | 818 | 818 | 941 | 939 | 939 | 939 |", "caption": "Table 6: \nSample sizes in the constructed AntiLeak-Bench in the experiments.", "description": "This table presents the number of samples within each time period, task, and number of distracting documents in AntiLeak-Bench.  The table is split into two rows based on time period (2022-01-01 to 2023-01-01 and 2023-05-01 to 2024-08-01). The columns represent different tasks: single-hop and multi-hop question answering, with varying numbers of distracting documents (0, 3, 5, and 7).", "section": "4 Experiment"}, {"content": "| Time period | Single-Hop | | | Multi-Hop | | |\n|---|---|---|---|---|---|---| \n| | Gold | $N_d$=3 | $N_d$=5 | $N_d$=7 | Gold | $N_d$=3 | $N_d$=5 | $N_d$=7 |\n| 2022-01-01 to 2023-01-01 | 5998 | 23163 | 33867 | 46033 | 24646 | 40611 | 50846 | 61761 |\n| 2023-05-01 to 2024-08-01 | 7210 | 27501 | 40800 | 54451 | 25505 | 43926 | 53898 | 66957 |", "caption": "Table 7: \nAverage word counts of samples in the constructed AntiLeak-Bench in the experiments.", "description": "This table presents the average word counts of samples in the constructed AntiLeak-Bench across different time periods (2022-01-01 to 2023-01-01 and 2023-05-01 to 2024-08-01), tasks (single-hop and multi-hop), and the number of distracting documents (0, 3, 5, and 7).  The data is organized by time period, task type, and the number of distracting documents, allowing for an analysis of question complexity and context length across various experimental settings.", "section": "4 Experiment"}, {"content": "| Model | Release time | Knowledge cutoff time |\n|---|---|---| \n| Llama-2-7B | 2023-07 | 2022-09 |\n| Llama-2-13B | 2023-07 | 2022-09 |\n| Mistral-7B | 2023-09 | 2022* |\n| Vicuna-v1.5-7B | 2023-07 | 2022-09 |\n| Longchat-v1.5-7B | 2023-07 | 2022-09 |\n| Llama-3.1-8B | 2024-07 | 2023-12 |\n| Phi-3.5-mini | 2024-08 | 2023-10 |\n| Qwen-2-7B | 2024-06 | 2023* |\n| Mistral-Nemo-12B | 2024-07 | 2024-04 |\n| Gemma-2-9B | 2024-08 | 2024-06* |\n| GPT-4o-mini | 2024-07 | 2023-10 |\n| GPT-4o | 2024-07 | 2023-12 |", "caption": "Table 8: \nRelease dates and knowledge cutoff dates of LLMs.\n* means estimated time.", "description": "This table lists the release date and knowledge cutoff date for each of the large language models (LLMs) used in the study. The knowledge cutoff date refers to the point in time after which any newly generated knowledge is not included in the training dataset. An estimated cutoff date is marked with an asterisk.", "section": "4.1 Experiment Setup"}]