[{"figure_path": "https://arxiv.org/html/2412.13670/x1.png", "caption": "Figure 1: \nIllustration of AntiLeak-Bench.\nIt constructs contamination-free samples\nwith the knowledge updated after LLMs\u2019 cutoff time,\nwhich thus are not in LLMs\u2019 training sets.", "description": "AntiLeak-Bench constructs contamination-free benchmark samples by identifying knowledge updated after a given LLM's knowledge cutoff time.  It then uses this updated knowledge to create questions and gathers relevant supporting documents from sources like Wikipedia, ensuring the benchmark evaluates an LLM's ability to handle truly novel information not present in its training data.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2412.13670/x2.png", "caption": "Figure 2: \nIllustration of the automated benchmark building workflow without human labor.\nAfter data preparation, it includes three main steps:\n(1) Identify updated knowledge after the cutoff time;\n(2) Build supporting documents;\n(3) Construct contamination-free samples (Figure\u00a03 exemplifies how to construct multi-hop samples).", "description": "The figure illustrates the automated process of building the AntiLeak-Bench.  It starts with preparing data from Wikidata.  The workflow then identifies knowledge updated after an LLM's knowledge cutoff time by comparing claim histories. Next, supporting documents are retrieved from Wikipedia based on the updated knowledge.  Finally, contamination-free question-answering samples are generated using the updated knowledge and supporting documents.", "section": "3 AntiLeak-Bench"}, {"figure_path": "https://arxiv.org/html/2412.13670/x3.png", "caption": "Figure 3: \nIllustration of constructing multi-hop samples.\nFind the consequent relation of previous objects.", "description": "The figure illustrates the process of constructing multi-hop question-answering samples. It starts with an initial fact, such as Lionel Messi being a member of Inter Miami.  Subsequent \"hops\" are made by connecting the object of the previous fact to the subject of a new fact, forming a chain. For example, the second hop connects Inter Miami to its location (or head coach), and a third hop might link the head coach to their country of citizenship. This chain of relations forms the basis of a multi-hop question, where the answer requires traversing multiple linked facts.  The supporting context for the question would include text related to each entity involved in these \"hops\".", "section": "3 AntiLeak-Bench"}, {"figure_path": "https://arxiv.org/html/2412.13670/x4.png", "caption": "Figure 4: \nEM and F1 performance at each time interval.", "description": "This figure presents the Exact Match (EM) and F1 scores of different large language models (LLMs) over multiple 2-month or 3-month intervals between 2022 and 2024. The models evaluated include Llama-2-7B, Llama-2-13B, Mistral-7B, Vicuna-v1.5-7B, LongChat-v1.5-7B, Phi-3.5-mini, Qwen-2-7B, Mistral-Nemo-12B, and Gemma-2-9B.  The x-axis represents the time intervals, while the y-axis represents the EM and F1 scores. Different colors and line styles distinguish the performance of each model. The vertical dotted lines likely represent the knowledge cut-off times of the LLMs, indicating the point in time after which information used in evaluating the models was not included in their training data. The figure demonstrates the performance trends of different LLMs over time, highlighting potential data contamination issues and illustrating the effectiveness of the AntiLeak-Bench in evaluating LLMs in a contamination-free environment. This figure is important for understanding the reliability and validity of using benchmarks to assess LLMs' capabilities.", "section": "4.2 Data Contamination Analysis"}, {"figure_path": "https://arxiv.org/html/2412.13670/x5.png", "caption": "Figure 5: \nCorrect and outdated option proportions at each time interval.", "description": "This figure showcases the proportion of times Large Language Models (LLMs) select correct and outdated options in multi-choice question answering tasks related to data contamination in evaluations. The figure is separated into two parts based on different models and time intervals reflecting knowledge cut-off dates and updates. The analysis reveals that outdated options were selected more frequently by LLMs over time, and LLMs struggled to answer the questions correctly, with some LLMs performing poorly even before their knowledge cut-off date. The x-axis represents different time intervals, while the y-axis shows the percentage. Each line in the chart represents the selection frequency of an option over each time interval. ", "section": "4.2 Data Contamination Analysis"}]