[{"content": "| VLMs | QBench | AI2D | ChartQA | POPE | HallB | MME | MathVista | MMB | MMB<sup>CN</sup> | MM-Vet | MMMU |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| LLaVA-NeXT-7B [61] | - | - | - | 86.5 | - | 1851 | 34.6 | 69.6 | 63.3 | 43.9 | 35.1 |\n| LLaVA-NeXT-8B [61] | - | 71.6 | 69.5 | - | - | 1972 | 37.5 | 72.1 | - | - | 41.7 |\n| LLaVA-NeXT-13B [61] | - | 70.0 | 62.2 | 86.7 | - | 1892 | 35.1 | 70.0 | 68.5 | 47.3 | 35.9 |\n| MM1-7B [69] | - | - | - | 86.6 | - | 1858 | 35.9 | 72.3 | - | 42.1 | 37.0 |\n| MM1-MoE-7B<binary data, 1 bytes><binary data, 1 bytes>32 [69] | - | - | - | 87.8 | - | 1992 | 40.9 | 72.7 | - | 45.2 | 40.9 |\n| MiniGemini-HD-7B [56] | - | - | - | - | - | 1865 | 32.2 | 65.8 | - | 41.3 | 36.8 |\n| MiniGemini-HD-13B [56] | - | - | - | - | - | 1917 | 37.0 | 68.6 | - | 50.5 | 37.3 |\n| Cambrian-1-8B [84] |  | 73.0 | 73.3 | - | - | - | 49.0 | 75.9 | - | - | 42.7 |\n| Cambrian-1-13B [84] |  | 73.6 | 73.8 | - | - | - | 48.0 | 75.7 | - | - | 40.0 |\n| Eagle-8B [77] |  | 76.1 | 80.1 | - | - | - | 52.7 | 75.9 | - | - | 43.8 |\n| Eagle-13B [77] |  | 74.0 | 77.6 | - | - | - | 54.4 | 75.7 | - | - | 41.6 |\n| VILA1.5-8B [58] | - | - | - | 85.6 | - | - | - | 75.3 | 69.9 | 43.2 | 38.6 |\n| VILA1.5-13B [58] | - | - | - | 86.3 | - | - | - | 74.9 | 66.3 | 44.3 | 37.9 |\n| VILA<sup>2</sup>-8B [26] | - | - | - | 86.7 | - | - | - | 76.6 | 71.7 | 50.0 | 38.3 |\n| CogVLM2-8B [35] | - | 73.4 | 81.0 | - | - | 1870 | - | 80.5 | - | 60.4 | 44.3 |\n| LLaVA-OneVision-7B [52] | - | 81.4 | 80.0 | - | - | 1998 | 63.2 | 80.8 | - | 57.5 | 48.8 |\n| InternVL2-8B [10] | - | 83.8 | 83.3 | - | - | 2210 | 58.3 | 81.7 | 81.2 | 54.2 | 49.3 |\n| MiniCPM-V2.5-8B [92] | - | - | - | - | - | 2025 | 54.3 | 77.2 | 74.2 | - | 45.8 |\n| MiniCPM-V2.6-8B [92] | - | - | - | - | - | 2348 | 60.6 | - | - | 60.0 | 49.8 |\n| TroL-7B [46] | 73.6 | 78.5 | 71.2 | 87.8 | 65.3 | 2308 | 51.8 | 83.5 | 81.2 | 54.7 | 49.9 |\n| Phantom-7B [45] | 73.8 | 79.5 | 87.8 | 87.7 | 65.4 | 2126 | 70.9 | 84.8 | 84.7 | 70.8 | 51.2 |\n| Qwen2-VL-7B [87] | 77.5 | 77.5 | 83.0 | 88.9 | 65.7 | 2327 | 58.2 | 83.0 | 80.5 | 62.0 | 54.1 |\n| ![https://arxiv.org/html/2412.01822/x35.png](https://arxiv.org/html/2412.01822/x35.png) VLsI-7B | 77.5 | 87.3 | 86.1 | 88.6 | 74.2 | 2338 | 74.7 | 86.3 | 85.5 | 75.2 | 69.3 |", "caption": "Table 1: Evaluation of existing open-source VLMs and \u2006VLsI on various vision-language benchmarks: QBench\u00a0[88], AI2D\u00a0[41], ChartQA\u00a0[67], POPE\u00a0[54], HallB\u00a0[59], MME\u00a0[28], MathVista\u00a0[66], MMB\u00a0[62], MMBCNCN{}^{\\text{CN}}start_FLOATSUPERSCRIPT CN end_FLOATSUPERSCRIPT\u00a0[62], MM-Vet\u00a0[94], and MMMU\u00a0[96]. Bold and underline indicate the top and second-best results, respectively.", "description": "Table 1 presents a comparative analysis of various open-source Vision-Language Models (VLMs) and the newly proposed VLSI model.  The evaluation is conducted across ten prominent vision-language benchmarks, assessing performance across diverse tasks.  These benchmarks cover a wide range of capabilities, including low-level visual abilities (QBench), diagram interpretation (AI2D), chart-based reasoning (ChartQA), object hallucination detection (POPE), visual and logical reasoning in various settings (HallB, MME, MathVista, MMB, MMBCN, MM-Vet), and multimodal understanding and reasoning (MMMU).  The table highlights VLSI's competitive performance against existing models, particularly in achieving superior results on multiple benchmarks.", "section": "4. Experiments"}, {"content": "| VLMs | QBench | AI2D | ChartQA | POPE | HallB | MME | MathVista | MMB | MMB<sup>CN</sup> | MM-Vet | MMMU |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| MiniCPM-2.4B [38] | - | 56.3 | - | - | - | 1650 | 28.9 | 64.1 | 62.6 | 31.1 | - |\n| MiniCPM-V2-2.8B [38] | - | 62.9 | - | - | - | 1809 | 38.7 | 69.1 | 66.5 | 41.0 | - |\n| MM1-3B [69] | - | - | - | 87.4 | - | 1762 | 32.0 | 67.8 | - | 43.7 | 33.9 |\n| MM1-MoE-3B<math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo>&#215;</mo></math>64 [69] | - | - | - | 87.6 | - | 1773 | 32.6 | 70.8 | - | 42.2 | 38.6 |\n| ALLaVA-3B [6] | - | - | - | - | - | 1623 | - | 64.0 | - | 32.2 | 35.3 |\n| VILA1.5-3B [6] | - | - | - | 85.3 | - | - | - | 62.8 | 52.2 | 38.6 | 33.3 |\n| InternVL2-4B [10] | - | 78.9 | 81.5 | - | - | **2064** | 58.6 | 78.6 | 73.9 | 51.0 | 34.3 |\n| TroL-3.8B [46] | 70.0 | 73.6 | 73.8 | 86.5 | **62.2** | 1980 | 55.1 | 79.2 | **77.1** | 51.1 | 37.5 |\n| Phantom-3.8B [45] | 70.3 | 71.7 | **87.3** | 87.1 | 60.8 | **2046** | 60.6 | **80.4** | **77.1** | **54.4** | 39.2 |\n| DeepSeek-VL-1.3B [65] | - | - | - | 87.6 | - | - | 31.1 | 64.6 | 62.9 | 34.8 | 32.2 |\n| MobileVLM-1.7B [13] | - | - | - | 84.5 | - | - | - | 53.2 | - | - | - |\n| MobileVLM-V2-1.7B [14] | - | - | - | 84.3 | - | - | - | 57.7 | - | - | - |\n| MoE-LLaVA-1.8B<math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo>&#215;</mo></math>4 [57] | - | - | - | 87.0 | - | - | - | 59.7 | - | 25.3 | - |\n| Mini-Gemini-2B [56] | - | - | - | - | - | 1653 | 29.4 | 59.8 | - | - | 31.7 |\n| InternVL2-2B [10] | - | **74.1** | 76.2 | - | - | 1877 | 46.3 | 73.2 | 70.9 | 39.5 | 34.3 |\n| TroL-1.8B [46] | 68.2 | 68.9 | 64.0 | **88.6** | 60.1 | 2038 | 45.4 | 76.1 | 74.1 | 45.1 | 35.2 |\n| Phantom-1.8B [45] | 69.1 | 62.3 | **87.0** | **89.6** | **62.2** | 1885 | **60.9** | 76.6 | 75.1 | 54.1 | 40.6 |\n| Qwen2-VL-2B [87] | **70.8** | 60.2 | 73.5 | 87.8 | 61.2 | 1872 | 43.0 | 74.9 | 73.5 | 49.5 | **41.1** |\n| VLSI-2B | **72.3** | **89.0** | 85.8 | 87.9 | **70.0** | 2022 | **68.4** | **81.7** | **78.8** | **64.8** | **51.4** |", "caption": "Table 2: Comparison of smaller open-source VLMs and \u2006VLsI on the same evaluation benchmarks as in Table\u00a01.", "description": "This table presents a comparison of the performance of various smaller open-source vision-language models (VLMs) and the proposed VLSI model on a set of common vision-language benchmarks.  It allows for a direct comparison of VLSI's performance against existing open-source models of similar scale, highlighting its efficiency and accuracy in handling a variety of vision-language tasks.  The benchmarks used are the same as those in Table 1, enabling a consistent evaluation across model sizes and architectures.", "section": "4. Experiments"}, {"content": "| Benchmarks | OmniFusion-7B | DeepSeek-VL-7B | MoVA-7B | Eagle-8B | CoLLaVO-7B | MoAI-7B | Meteor-7B | VLsI-2B | VLsI-7B |\n|---|---|---|---|---|---|---|---|---|---| \n| MMB [62] | 69.0 | 73.2 | 81.3 | 75.9 | 83.0 | 79.3 | 82.9 | 81.7 | 86.3 |\n| MathVista [66] | - | - | 44.3 | 52.7 | 57.6 | 56.2 | 53.4 | 68.4 | 74.7 |\n| MM-Vet [94] | 39.4 | 41.5 | - | - | 40.3 | 43.7 | 57.3 | 64.8 | 70.8 |\n| MMMU [96] | 36.6 | 36.6 | - | 43.8 | 42.2 | 45.6 | 48.3 | 51.4 | 69.3 |", "caption": "(a) Validation of open-source VLMs with additional modules and projectors compared to \u2006VLsI: OmniFusion\u00a0[32], DeepSeek-VL\u00a0[65], MoVA\u00a0[40], Eagle\u00a0[77], CoLLaVO\u00a0[48], MoAI\u00a0[49], and Meteor\u00a0[47].", "description": "Table 2 presents a comparative analysis of various open-source Vision-Language Models (VLMs) on the MM-Vet and MMMU benchmarks.  It contrasts the performance of VLMs enhanced with additional modules and projectors against the performance of VLSI. The models compared against VLSI include OmniFusion, DeepSeek-VL, MoVA, Eagle, CoLLaVO, MoAI, and Meteor. This allows for an assessment of VLSI's performance relative to existing state-of-the-art methods that utilize specialized modules or architectures.", "section": "4. Experiments"}, {"content": "| VLMs | MM-Vet | MM-Vet-v2 | MMMU | MMStar | AI2D | SEED-2-Plus | MathVista | BLINK | CV-Bench | LLaVA-Wilder |\n|---|---|---|---|---|---|---|---|---|---|---|\n| LLaVA-NeXT-34B [61] | 50.7 | 50.9 | 48.8 | 51.6 | 78.9 | 65.9 | 40.4 | - | - | - |\n| VILA1.5-40B [58] | 51.2 | - | 55.1 | 55.2 | 77.8 | - | 49.5 | - | - | - |\n| Cambrian-34B [84] | 53.2 | - | 50.4 | 54.2 | 79.5 | 65.1 | 50.3 | - | 76.9 | - |\n| Molmo-72B [19] | 61.1 | - | 52.8 | 63.3 | 83.4 | - | 55.8 | - | - | - |\n| LLaVA-OV-72B [52] | 60.6 | - | 56.6 | 65.8 | 86.2 | - | 68.4 | - | - | 72.0 |\n| LLaMA-3.2-Vision | 64.1 | - | 60.3 | 55.3 | 69.5 | 68.2 | 58.3 | 48.0 | - | - |\n| Claude3.5-Sonnet [1] | 66.0 | 71.8 | 65.9 | 62.2 | 80.2 | 71.7 | 61.6 | 28.2 | - | 83.1 |\n| NVLM-D-72B [18] | 58.9 | - | 60.8 | 63.7 | 80.1 | 68.4 | 63.9 | 48.0 | - | - |\n| GPT-4V (0409) [8] | 67.5 | 66.3 | 61.7 | 56.0 | 78.6 | 69.3 | 54.7 | 58.3 | 69.1 | 71.5 |\n| Gemini-1.5-Pro | 64.0 | 66.9 | 60.6 | 59.1 | 79.1 | 70.8 | 57.7 | 59.1 | - | - |\n| InternVL2-76B [10] | 64.4 | 68.4 | 58.3 | 67.1 | 87.6 | 70.0 | 65.6 | 57.5 | - | - |\n| GPT-4o (0806) | 75.1 | 71.0 | 69.9 | 64.7 | 84.7 | 70.8 | 62.7 | 64.7 | - | 85.9 |\n| Qwen2-VL-72B [87] | 73.9 | 68.7 | 64.3 | 68.6 | 88.3 | 72.3 | 69.7 | 60.5 | 74.3 | 84.1 |\n| TroL-1.8B [46] | 45.1 | - | 35.2 | 45.5 | 68.9 | - | 45.4 | - | - | - |\n| TroL-7B [46] | 54.7 | - | 49.9 | 51.3 | 78.5 | - | 51.8 | - | - | - |\n| Phantom-1.8B [45] | 54.1 | 46.3 | 40.6 | 45.5 | 62.3 | 57.1 | 60.9 | 44.2 | 63.1 | 78.5 |\n| Phantom-7B [45] | 70.8 | 60.6 | 51.2 | 57.3 | 79.5 | 65.5 | 70.9 | 58.9 | 74.9 | 82.9 |\n| ![https://arxiv.org/html/2412.01822/x46.png](https://arxiv.org/html/2412.01822/x46.png) VLsI-2B | 64.8 | 60.8 | 51.4 | 76.6 | 89.0 | 81.1 | 68.4 | 52.4 | 90.1 | 90.1 |\n| ![https://arxiv.org/html/2412.01822/x47.png](https://arxiv.org/html/2412.01822/x47.png) VLsI-7B | 75.8 | 70.0 | 69.3 | 73.6 | 87.3 | 74.9 | 74.7 | 59.7 | 89.1 | 92.0 |", "caption": "(b) Comparison of \u2006VLsI with other open-source and closed-source VLMs on challenging benchmarks: MM-Vet\u00a0[94], MM-Vet-v2\u00a0[95], MMMU\u00a0[96], MMStar\u00a0[9], AI2D\u00a0[41], SEED-2-Plus\u00a0[51], MathVista\u00a0[66], BLINK\u00a0[29], CV-Bench\u00a0[84], and LLaVA-Wilder\u00a0[52]. This comparison includes models embedding additional knowledge\u00a0[46, 45] and larger open/closed-source VLMs.", "description": "Table 3 presents a comparative analysis of VLSI's performance against other prominent Vision-Language Models (VLMs), both open-source and closed-source, across a range of challenging benchmarks.  These benchmarks assess diverse capabilities, including visual and reasoning skills.  The models compared include those enhanced with additional knowledge modules and larger models with greater parameter counts, offering a comprehensive overview of VLSI's performance in relation to state-of-the-art systems.", "section": "4.3. Comparison on Evaluation Benchmarks"}, {"content": "| VLMs | MMB | MM-Vet | MMMU |\n|---|---|---|---|\n| LLaVA-OV-0.5B | 52.1 | 29.1 | 31.4 |\n| VLsI-0.5B (LLaVA-OV-72B) | **72.5** | **50.7** | **49.9** |\n| LLaVA-OV-7B | 80.8 | 57.5 | 48.8 |\n| VLsI-7B (Qwen2-VL-72B) | **86.3** | **75.8** | **69.3** |\n| VLsI-7B (LLaVA-OV-72B) | 86.1 | 61.6 | 59.1 |\n| LLaVA-OV-72B | 85.9 | 63.7 | 56.8 |", "caption": "Table 3: Detailed comparison of \u2006VLsI with various open and closed-source VLMs on challenging evaluation benchmarks. Appendix A provides detailed descriptions of the evaluation benchmarks listed in Tables\u00a01 and 2.", "description": "Table 3 presents a comprehensive comparison of the proposed VLSI model against a range of open-source and closed-source Vision-Language Models (VLMs) across a set of challenging evaluation benchmarks.  The table highlights VLSI's performance relative to these other models, showcasing its strengths and weaknesses on different tasks.  The specific benchmarks used are detailed in Appendix A of the paper for readers seeking further information on the nature of the individual evaluation tasks.", "section": "4.3 Comparison on Evaluation Benchmarks"}, {"content": "| IL-Ops | LL-Ops | MMB | BLINK | MM-Vet | MMMU |\n|---|---|---|---|---|---| \n| CE | \u2717 | 79.2 | 51.3 | 64.5 | 56.5 |\n| CE | CE | 77.8 | 50.2 | 63.2 | 55.2 |\n| CE | KLD | 81.0 | 53.5 | 67.2 | 59.0 |\n| L2 | KLD | 81.5 | 53.2 | 66.8 | 58.0 |\n| KLD | \u2717 | 83.0 | 55.0 | 69.5 | 61.0 |\n| KLD | CE | 81.5 | 54.3 | 68.5 | 59.8 |\n| KLD | KLD | **86.3** | **59.7** | **75.8** | **69.3** |\n| L2 | KLD | 81.7 | 53.5 | 67.0 | 58.3 |", "caption": "Table 4: Ablation studies examining the six main factors influencing the effectiveness of \u2006VLsI.", "description": "This table presents the results of ablation studies conducted to analyze the impact of six key factors on the performance of the VLSI model.  The factors investigated include the choice of backbone Vision-Language Model (VLM), the inclusion of a reinforcement step in the training process, the percentage of training iterations dedicated to the reinforcement step, the type of operations used for intermediate and last layers, the components of the layer matching strategy, and the architecture of the verbalizer.  Each row represents a different experimental setup modifying one or more of these factors, while the columns display the resulting performance metrics on the MM-Vet and MMMU benchmarks.", "section": "4. Experiments"}]