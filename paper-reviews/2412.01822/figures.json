[{"figure_path": "https://arxiv.org/html/2412.01822/x24.png", "caption": "Figure 1: Performance overview of \u2006VLsI on vision-language benchmarks. (a) Accuracy on MM-Vet\u00a0[94] for various model sizes, showing that \u2006VLsI (2B and 7B) achieves competitive performance compared to proprietary closed-source VLMs. (b) Comparative evaluation on multiple challenging benchmarks, where \u2006VLsI (green and blue) outperforms leading closed-source VLMs, including GPT-4V\u00a0[74], Claude-3.5-Sonnet\u00a0[1], and Gemini-1.5-Pro\u00a0[82], highlighting its efficiency and effectiveness across diverse tasks.", "description": "Figure 1 presents a performance comparison of the proposed VLSI model against other Vision-Language Models (VLMs) on various benchmark datasets.  Panel (a) focuses on the MM-Vet benchmark, demonstrating that VLSI, in its 2B and 7B parameter versions, achieves accuracy comparable to leading proprietary closed-source VLMs of similar size. Panel (b) broadens the comparison across several challenging benchmarks, showing that VLSI surpasses the performance of top closed-source models (GPT-4V, Claude-3.5-Sonnet, and Gemini-1.5-Pro) while maintaining significantly higher efficiency.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2412.01822/x52.png", "caption": "(a) Verbalization Step", "description": "This figure illustrates the verbalization step of the VLSI (Verbalized Layers-to-Interactions) training process.  In this step, intermediate layers in both the large and small backbone VLMs are equipped with a \"verbalizer.\" This verbalizer allows the layer's output to be projected into natural language space. Autoregressive loss is then used to align the verbalized outputs with the target responses. This process makes the intermediate layers' outputs interpretable as text-based responses, enhancing interpretability and alignment with the larger model. The verbalization step is crucial for enabling effective layer-wise knowledge transfer from the large to small VLM.", "section": "3. VLSI: Verbalized Layers-to-Interactions"}, {"figure_path": "https://arxiv.org/html/2412.01822/x55.png", "caption": "(b) Interaction Step", "description": "This figure illustrates the second step of the VLSI training process, the 'Interaction Step'.  The goal is to establish a layer-wise mapping between the large and small backbone Vision Language Models (VLMs).  The small VLM searches for corresponding layers in the large VLM based on intermediate verbal outputs.  This ensures that the small VLM mimics the reasoning process of the larger VLM layer by layer. The search for matching layers is shown as a range search within the larger VLM, suggesting an adaptive, rather than strictly one-to-one mapping strategy.", "section": "3. VLSI: Verbalized Layers-to-Interactions"}, {"figure_path": "https://arxiv.org/html/2412.01822/x71.png", "caption": "Figure 2: Illustration of the training process in \u2006VLsI, showing (a) the verbalization step and (b) the interaction step. (a) In the verbalization step, intermediate layers in both the large- and small-backbone VLMs are equipped with a \u201cverbalizer\u201d, allowing their outputs to be projected into natural language space. Autoregressive loss is applied to align these verbalized outputs with the target responses. (b) In the interaction step, each intermediate layer in the small-backbone VLM searches for a matching layer in the large backbone VLM within a specified range. For example, once the 2ndnd{}^{\\text{nd}}start_FLOATSUPERSCRIPT nd end_FLOATSUPERSCRIPT layer of the small VLM is matched with the 4thth{}^{\\text{th}}start_FLOATSUPERSCRIPT th end_FLOATSUPERSCRIPT layer in the large VLM, the next matching search for the 3rdrd{}^{\\text{rd}}start_FLOATSUPERSCRIPT rd end_FLOATSUPERSCRIPT layer in the small VLM will proceed from the 5thth{}^{\\text{th}}start_FLOATSUPERSCRIPT th end_FLOATSUPERSCRIPT to the 7thth{}^{\\text{th}}start_FLOATSUPERSCRIPT th end_FLOATSUPERSCRIPT layers of the large VLM, ensuring progressive alignment.", "description": "This figure illustrates the training process of the VLSI model, focusing on two key steps: verbalization and interaction.  The verbalization step (a) involves adding a 'verbalizer' module to intermediate layers of both large and small Vision Language Models (VLMs).  These verbalizers translate the layer's feature representations into natural language descriptions.  The model then uses autoregressive loss to align these verbalized descriptions with the ground truth responses, effectively teaching the smaller VLM to produce similar textual output at each layer. The interaction step (b) shows how the model finds corresponding layers between the large and small VLMs.  It doesn't directly map layers one-to-one, but rather searches within a defined range to find the best matching layers that align in terms of their reasoning progression.  This adaptive layer matching ensures that the small VLM learns to follow the reasoning process of the large VLM effectively.", "section": "3. VLSI: Verbalized Layers-to-Interactions"}, {"figure_path": "https://arxiv.org/html/2412.01822/x72.png", "caption": "Figure 3: Example of verbalized outputs from each intermediate target layer in an alternative small-backbone VLM (without \u2006VLsI enhancements) and the \u2006VLsI. The visual question prompts VLM to predict the missing image in a sequence pattern. The outputs illustrate how each layer progressively interprets the visual cues, with \u2006VLsI accurately identifying the answer as \u2018a star with a dot\u2019 in the final layer, while the alternative small-backbone VLM incorrectly predicts \u2018a diamond with a dot\u2019. This demonstrates the improved interpretative capability of \u2006VLsI through layer-wise, language-based distillation.", "description": "Figure 3 demonstrates the effectiveness of VLSI's layer-wise, language-based distillation method. It shows the verbalized outputs at each intermediate layer of a small-backbone VLM (without VLSI enhancements) and VLSI when asked to predict the missing image in a sequence.  The alternative small-backbone VLM incorrectly identifies the missing image as a 'diamond with a dot', while VLSI correctly predicts 'a star with a dot'. This difference illustrates how VLSI progressively interprets visual cues at each layer, leading to a more accurate final prediction. The figure highlights the improved interpretative capabilities of VLSI compared to a standard small-backbone VLM.", "section": "3. VLSI: Verbalized Layers-to-Interactions"}, {"figure_path": "https://arxiv.org/html/2412.01822/x73.png", "caption": "Figure 4: Comparison of performance on MM-Vet\u00a0[94] and MMMU\u00a0[96] across different model size combinations in large and small backbone VLMs. Each cell shows the evaluation results for various interaction configurations between 0.5B, 2B, and 7B small backbone VLMs trained with either Qwen2-VL\u00a0[87] or LLaVA-OV\u00a0[52] as the large-backbone VLM.", "description": "Figure 4 presents a detailed comparison of the performance achieved by various Vision-Language Models (VLMs) on two established benchmarks: MM-Vet and MMMU.  The experiment focuses on evaluating the impact of different model sizes and training strategies.  Specifically, the figure explores the performance differences when using small backbone VLMs (0.5B, 2B, and 7B parameters) trained using either Qwen2-VL or LLaVA-OV as the large backbone VLM. Each cell within the figure represents the performance achieved under a particular interaction configuration between a specific small backbone VLM and a large backbone VLM. This setup allows for a thorough analysis of how different model sizes and training approaches influence the final performance on these complex vision-language tasks.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.01822/x74.png", "caption": "(a) Backbone VLMs", "description": "This figure compares the performance of two different backbone Vision Language Models (VLMs), Qwen2-VL and LLaVA-OV, when used as the foundation for building the VLSI model.  It shows how the choice of backbone VLM impacts the final performance of the VLSI model across various metrics on different vision-language benchmarks.  The results illustrate that the performance of the VLSI model can be significantly influenced by the capabilities of its underlying backbone VLM.", "section": "3. VLSI: Verbalized Layers-to-Interactions"}, {"figure_path": "https://arxiv.org/html/2412.01822/x75.png", "caption": "(d) Operations for Intermediate/Last Layers", "description": "This ablation study investigates the impact of applying different operations to intermediate and last layers within the VLSI model.  It compares the effectiveness of using cross-entropy (CE), L2 loss, and Kullback-Leibler divergence (KLD) for both intermediate layer matching and the final layer interaction.  The results illustrate the relative performance of various loss functions, showing the optimal combination for transferring knowledge from large to small VLMs.", "section": "3.3 Reinforcement Step"}, {"figure_path": "https://arxiv.org/html/2412.01822/x76.png", "caption": "(b) Use of Reinforcement Step (RS)", "description": "This ablation study investigates the effect of the reinforcement step (RS) in the VLSI model training.  It compares the model's performance on multiple vision-language benchmarks (MMB, BLINK, MM-Vet, MMMU) when the reinforcement step is included versus when it's excluded.  The results demonstrate the impact of the reinforcement step on enhancing performance across different benchmarks, highlighting its effectiveness in fine-tuning the model for improved accuracy and responsiveness.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.01822/x77.png", "caption": "(e) Components in Matching Strategy", "description": "This figure details the ablation study on different layer matching strategies used in the VLSI model.  It compares the performance of different strategies: Random Index, Bottom-k Index (where k represents the number of bottom indices selected), and Multinomial Sampling.  The results demonstrate the impact of various components on matching layer effectiveness within the distillation process.  Order preservation and adaptive temperature further enhance performance. The best results are achieved when utilizing multinomial sampling, incorporating order preservation, and adjusting temperature dynamically.", "section": "3. VLSI: Verbalized Layers-to-Interactions"}, {"figure_path": "https://arxiv.org/html/2412.01822/x78.png", "caption": "(c) Percentage of Training Iterations in RS", "description": "This figure shows the ablation study results on the impact of the reinforcement step (RS) training iterations on the performance of VLSI.  It compares the model's performance across various settings: using only the interaction step without RS, and using RS with 50% and 100% of the total training iterations. The results demonstrate the impact of adding the RS, which further enhances the VLSI model's performance on various vision-language benchmarks.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.01822/x87.png", "caption": "(f) Verbalizer Architecture", "description": "This figure details the architecture of the verbalizer module used in the VLSI model.  The verbalizer is a crucial component of VLSI's layer-wise distillation process.  It projects intermediate layer features from both the large and small backbone VLMs into a natural language space. This allows for a flexible alignment between the reasoning processes of these models of differing sizes, improving efficiency and accuracy without the need for scaling or architectural changes. The verbalizer is comprised of a feed-forward network (verb-FFN) and a language head, facilitating a text-based output.", "section": "3. VLSI: Verbalized Layers-to-Interactions"}, {"figure_path": "https://arxiv.org/html/2412.01822/x88.png", "caption": "Figure 5: \nDistribution changes of the matched indices between small-backbone and large-backbone VLMs at the interaction step. The left figure shows the distribution at the beginning of training, while the right figure shows it at the end.", "description": "This figure visualizes how the alignment between layers of small and large Vision Language Models (VLMs) evolves during the interaction step of the VLSI training process.  The heatmaps represent the distribution of matched indices between layers. The left heatmap shows this distribution at the training's start, while the right one displays it at the end.  A change in distribution indicates that the smaller VLM is learning to align its layer-wise reasoning process with that of the larger VLM throughout training.", "section": "3. VLSI: Verbalized Layers-to-Interactions"}]