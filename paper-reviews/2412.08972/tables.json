[{"content": "| Airline | NBA | Tax |\n|---|---|---|\n| # Rules | 10 | 54 | 31 |\n| Average # Tokens | 376 | 398 | 359 |", "caption": "Table 1: Statistics of rules in each domain.", "description": "This table presents a statistical summary of the rules used in each of the three domains included in the RULEARENA benchmark: Airline, NBA, and Tax.  For each domain, it shows the number of rules, and the average number of tokens (words or sub-words) per rule. This information gives an indication of the complexity and length of the rules that the large language models (LLMs) need to process in the benchmark.", "section": "3 RuleArena"}, {"content": "Models|Settings|Level 1 P(t)|Level 1 AC(t)|Level 1 R(t)|Level 1 Acc(t)|Level 2 P(t)|Level 2 AC(t)|Level 2 R(t)|Level 2 Acc(t)|Level 3 P(t)|Level 3 AC(t)|Level 3 R(t)|Level 3 Acc(t)\n---|---|---|---|---|---|---|---|---|---|---|---|---\nAirline|---|---|---|---|---|---|---|---|---|---|---|---|---\nLlama-3.1 70B|0-shot|1.000|0.764|0.558|0.01|1.000|0.732|0.535|0.01|1.000|0.752|0.578|0.00\nLlama-3.1 70B|1-shot|1.000|0.809|0.787|0.17|1.000|0.827|0.801|0.07|1.000|0.769|0.815|0.01\nQwen-2.5 72B|0-shot|1.000|0.636|0.586|0.01|1.000|0.627|0.554|0.01|1.000|0.588|0.544|0.00\nQwen-2.5 72B|1-shot|1.000|0.836|0.908|0.19|1.000|0.818|0.901|0.10|1.000|0.801|0.904|0.01\nLlama-3.1 405B|0-shot|1.000|0.854|0.604|0.03|1.000|0.844|0.587|0.06|1.000|0.845|0.570|0.01\nLlama-3.1 405B|1-shot|1.000|0.919|0.921|0.32|1.000|0.897|0.905|0.16|1.000|0.870|0.946|0.04\nClaude-3.5 Sonnet|0-shot|1.000|0.930|0.702|0.04|1.000|0.876|0.669|0.00|1.000|0.888|0.646|0.01\nClaude-3.5 Sonnet|1-shot|1.000|0.960|0.871|0.29|1.000|0.966|0.822|0.30|1.000|0.972|0.718|0.11\nGPT-4o|0-shot|1.000|0.862|0.616|0.02|1.000|0.868|0.578|0.00|1.000|0.813|0.548|0.00\nGPT-4o|1-shot|1.000|0.922|0.885|0.32|1.000|0.875|0.853|0.16|1.000|0.835|0.798|0.05\nNBA Transaction|---|---|---|---|---|---|---|---|---|---|---|---|---\nLlama-3.1 70B|0-shot|0.579|\u2013|0.428|0.40|0.498|\u2013|0.246|0.36|0.540|\u2013|0.250|0.22\nLlama-3.1 70B|1-shot|0.560|\u2013|0.565|0.49|0.466|\u2013|0.386|0.25|0.578|\u2013|0.438|0.26\nQwen-2.5 72B|0-shot|0.556|\u2013|0.409|0.44|0.537|\u2013|0.339|0.43|0.592|\u2013|0.305|0.30\nQwen-2.5 72B|1-shot|0.595|\u2013|0.526|0.53|0.495|\u2013|0.378|0.35|0.574|\u2013|0.327|0.17\nLlama-3.1 405B|0-shot|0.581|\u2013|0.419|0.49|0.577|\u2013|0.323|0.30|0.561|\u2013|0.297|0.28\nLlama-3.1 405B|1-shot|0.608|\u2013|0.550|0.56|0.559|\u2013|0.439|0.29|0.575|\u2013|0.461|0.10\nClaude-3.5 Sonnet|0-shot|0.660|\u2013|0.457|0.38|0.630|\u2013|0.373|0.40|0.588|\u2013|0.292|0.28\nClaude-3.5 Sonnet|1-shot|0.676|\u2013|0.528|0.58|0.676|\u2013|0.410|0.47|0.650|\u2013|0.371|0.26\nGPT-4o|0-shot|0.650|\u2013|0.446|0.40|0.570|\u2013|0.327|0.26|0.603|\u2013|0.291|0.24\nGPT-4o|1-shot|0.616|\u2013|0.506|0.40|0.597|\u2013|0.392|0.28|0.569|\u2013|0.318|0.20\nTax|---|---|---|---|---|---|---|---|---|---|---|---|---\nLlama-3.1 70B|0-shot|1.000|0.834|0.989|0.01|1.000|0.767|0.918|0.00|1.000|0.745|0.852|0.00\nLlama-3.1 70B|1-shot|1.000|0.923|0.998|0.11|1.000|0.895|0.941|0.00|1.000|0.873|0.910|0.00\nQwen-2.5 72B|0-shot|1.000|0.888|0.998|0.10|1.000|0.835|0.944|0.01|1.000|0.785|0.903|0.00\nQwen-2.5 72B|1-shot|1.000|0.931|1.000|0.17|1.000|0.919|0.934|0.00|1.000|0.921|0.868|0.00\nLlama-3.1 405B|0-shot|1.000|0.923|0.999|0.16|1.000|0.876|0.964|0.02|1.000|0.797|0.926|0.00\nLlama-3.1 405B|1-shot|1.000|0.941|1.000|0.24|1.000|0.914|0.958|0.03|1.000|0.873|0.880|0.00\nClaude-3.5 Sonnet|0-shot|1.000|0.964|1.000|0.32|1.000|0.934|0.940|0.02|1.000|0.887|0.866|0.00\nClaude-3.5 Sonnet|1-shot|1.000|0.979|1.000|0.64|1.000|0.954|0.969|0.16|1.000|0.895|0.888|0.00\nGPT-4o|0-shot|1.000|0.965|1.000|0.42|1.000|0.951|0.957|0.07|1.000|0.945|0.908|0.00\nGPT-4o|1-shot|1.000|0.975|1.000|0.57|1.000|0.975|0.944|0.07|1.000|0.982|0.893|0.00", "caption": "Table 2: Main problem-wise evaluation results on airline, NBA, and tax domains. P\u2062(t)P\ud835\udc61{\\rm P}(t)roman_P ( italic_t ) denotes problem-wise precision, AC\u2062(t)AC\ud835\udc61{\\rm AC}(t)roman_AC ( italic_t ) denotes problem-wise rule application correctness, and R\u2062(t)R\ud835\udc61{\\rm R}(t)roman_R ( italic_t ) denotes problem-wise recall.", "description": "This table presents the performance of various large language models (LLMs) on three different real-world scenarios: airline baggage fees, NBA transactions, and tax calculations.  The performance is evaluated across three difficulty levels for each scenario.  The metrics used are problem-wise precision (P(t)), indicating the proportion of correctly identified relevant rules; problem-wise rule application correctness (AC(t)), showing the proportion of correctly applied rules; and problem-wise recall (R(t)), representing the proportion of relevant rules successfully applied.", "section": "Main Results"}, {"content": "|       | Airline | NBA | Tax |\n| :---- | :------: | :-: | :-: |\n| Mean(P(r)) | 1.000 | 0.504 | 1.000 |\n| Var(P(r)) | 0.000 | 0.110 | 0.000 |\n| Mean(Ac(r)) | 0.798 | \u2013 | 0.828 |\n| Var(Ac(r)) | 0.026 | \u2013 | 0.047 |\n| Mean(R(r)) | 0.721 | 0.308 | 0.900 |\n| Var(R(r)) | 0.109 | 0.082 | 0.050 |", "caption": "Table 3: Statistics of our three rule-wise metrics.", "description": "This table presents a statistical summary of three key rule-wise metrics (Recall, Application Correctness, and Precision) across all rules within the Airline, NBA, and Tax domains of the RULEARENA benchmark.  It shows the mean and variance for each metric in each domain, offering insights into the consistency and variability of LLM performance at the rule level.", "section": "3.4 Evaluation Metrics"}, {"content": "| Airline | essential | NBA | essential | Tax | essential |\n|---|---|---|---|---|---| \n| maximum violation fee |  | salary space consumption of bird right |  | education credits |  |\n| complementary overweight |  | salary space consumption of early bird right |  | american opportunity credit |  |\n| oversize fee |  | sign and trade maximum salary | \u2713 | net profit |  |\n| 3rd base check fee | \u2713 | Arenas provision |  | ctc or other dependent credit |  |\n| main plus extra free bag |  | over 38 rule |  | taxes with qualified dividends | \u2713|", "caption": "Table 4: Top-5555 rules of the lowest recall in ascent order of recall.", "description": "This table lists the top five rules from the Airline, NBA, and Tax domains that were least frequently applied correctly by LLMs.  It highlights the non-essential rules that are often overlooked, illustrating challenges with recall in the model's rule-guided reasoning.", "section": "4.2.2 Rule-wise Analysis"}, {"content": "| Airline | Tax |\n|---|---|---|---|\n| Rule | Composition | Rule | Composition |\n| main plus extra free bag | \u2713 | taxes with qualified dividends | \u2713 |\n| overall fee aggregation | \u2713 | standard taxes |  |\n| overweight fee matching |  | itemized deductions | \u2713 |\n| 3rd base check fee |  | standard deductions | \u2713 |\n| oversize fee matching | \u2713 | total income | \u2713 |", "caption": "Table 5: Top-5555 rules of the lowest correctness in ascent order of correctness.", "description": "This table presents the five rules with the lowest correctness scores in the RULEARENA benchmark's Airline domain, ordered by increasing correctness.  The \"correctness\" metric assesses whether the LLM correctly applied the rule in the problems where the rule was relevant.", "section": "4.2.2 Rule-wise Analysis"}, {"content": "| Rule | Substitutable |\n|---|---| \n| higher max criterion |  |\n| non bird right | \u2713 |\n| taxpayer mid level exception hard cap | \u2713 |\n| standard traded player exception | \u2713 |\n| salary increase ratio except bird right | \u2713 |", "caption": "Table 6: Top-5555 rules of the lowest precision in ascent order of precision.", "description": "This table presents the top five rules with the lowest precision in the NBA domain, ordered by ascending precision values.  It highlights rules where the model frequently misapplies them, often confusing them with similar rules due to subtle differences in application conditions.  The table helps to illustrate the challenges posed by similar-looking rules in a complex, real-world scenario.", "section": "4.2.2 Rule-wise Analysis"}, {"content": "| Rule | Setting | R(r) | P(r) |\n|---|---|---|---| \n| Over 38 rule | 0-shot | 0.00 | N/A |\n|  | 1-shot | 0.35 | 0.76 |\n| Salary consumption | 0-shot | 0.00 | N/A |\n|  | 1-shot | 0.23 | 0.20 |", "caption": "Table 7: 0-shot and 1-shot rule-wise comparison.", "description": "This table compares the performance of LLMs in applying rules when using zero-shot and one-shot prompting strategies.  It shows the recall (R(r)) and precision (P(r)) for two specific rules: 'Over 38 rule' and 'Salary consumption' in the NBA transaction domain.  These metrics reveal how well LLMs identify and apply these rules accurately under different prompting conditions.", "section": "5.2 Do In-Context Examples Help?"}, {"content": "| Models | Setting | Airline AC(t) | Airline R(t) | Airline Acc(t) | Tax AC(t) | Tax R(t) | Tax Acc(t) |\n|---|---|---|---|---|---|---|---| \n| Llama 70 | Table | 0.764 | 0.558 | 0.01 | 0.834 | 0.989 | 0.01 |\n|  | Text | 0.764 | 0.582 | 0.01 | 0.814 | 0.991 | 0.00 |\n| Qwen 72 | Table | 0.636 | 0.586 | 0.01 | 0.888 | 0.998 | 0.10 |\n|  | Text | 0.748 | 0.633 | 0.02 | 0.859 | 0.996 | 0.01 |\n| Llama 405 | Table | 0.854 | 0.604 | 0.03 | 0.923 | 0.999 | 0.16 |\n|  | Text | 0.835 | 0.587 | 0.07 | 0.919 | 0.998 | 0.05 |\n| Sonnet | Table | 0.930 | 0.702 | 0.04 | 0.964 | 1.000 | 0.32 |\n|  | Text | 0.937 | 0.705 | 0.06 | 0.971 | 1.000 | 0.33 |\n| GPT-4o | Table | 0.862 | 0.616 | 0.02 | 0.965 | 1.000 | 0.42 |\n|  | Text | 0.864 | 0.669 | 0.03 | 0.960 | 1.000 | 0.33 |", "caption": "Table 8: Results of different LLMs given different rule representations.", "description": "This table compares the performance of different large language models (LLMs) on a rule-following task.  Specifically, it shows how each LLM performs when given rules in two different formats:  a table format and a text format. The comparison allows for an analysis of whether the representation of the rules (tabular vs. textual) influences the LLMs' accuracy, recall, and correctness in applying the rules to solve a problem.", "section": "Experiments"}]