{"references": [{"fullname_first_author": "Jason Wei", "paper_title": "Chain-of-thought prompting elicits reasoning in large language models", "publication_date": "2022-12-01", "reason": "This paper introduces chain-of-thought prompting, a technique heavily used in the benchmark and directly influences the design of experimental setups."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Llama: Open and efficient foundation language models", "publication_date": "2023-02-13", "reason": "LLaMA is a foundational large language model used in the benchmark, its architecture and capabilities are fundamental to the results and conclusions of the paper."}, {"fullname_first_author": "Yihan Chen", "paper_title": "Benchmarking large language models on controllable generation under diversified instructions", "publication_date": "2024-01-01", "reason": "This work provides a relevant comparative study on instruction following abilities of LLMs, which is directly related to the rule-guided reasoning in the benchmark."}, {"fullname_first_author": "Abhimanyu Dubey", "paper_title": "The llama 3 herd of models", "publication_date": "2024-07-21", "reason": "The LLaMA 3 models are among the strongest models tested in the benchmark, understanding their capabilities is essential for interpreting the results."}, {"fullname_first_author": "Norman Mu", "paper_title": "Can LLMs follow simple rules?", "publication_date": "2023-11-04", "reason": "This paper directly addresses the topic of LLMs following rules, making it a key comparative work for the benchmark which explores the challenges of complex rules."}]}