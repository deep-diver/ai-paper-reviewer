[{"figure_path": "https://arxiv.org/html/2412.08972/x1.png", "caption": "Figure 1: Overview of RuleArena. RuleArena contains 95 commonly used and moderately complex rules and 816 test problems from three representative real-world scenarios - airline luggage fees, NBA transactions, and taxation policies. LLMs are given a the task instruction, the reference rules in this scenario, and a user instance, and required to conduct reasoning and computation for the user input under the guidance of reference rules.", "description": "RULEARENA is a benchmark dataset for evaluating LLMs' ability to perform rule-guided reasoning on real-world tasks.  It contains 95 rules and 816 test problems across three domains: airline baggage fees, NBA transactions, and tax policies.  Each problem presents an LLM with a user's request, the relevant rules, and asks the LLM to conduct the reasoning and calculations necessary to provide a correct answer based on those rules.", "section": "3 RuleArena"}, {"figure_path": "https://arxiv.org/html/2412.08972/x2.png", "caption": "(a) Recall", "description": "This bar chart visualizes the rule-wise recall metric for the airline domain.  The x-axis represents individual rules, ordered from lowest to highest recall,  while the y-axis indicates the recall score (proportion of problems the LLM correctly applied each rule to). Each bar's height corresponds to a specific rule's recall, demonstrating the relative effectiveness of different rules in this context. Rules with higher recall are more consistently applied by the LLM. Conversely, lower recall highlights rules where the model's performance was less successful.", "section": "4.2.2 Rule-wise Analysis"}, {"figure_path": "https://arxiv.org/html/2412.08972/x3.png", "caption": "(b) Correctness", "description": "This figure shows the rule-wise application correctness of rules in the airline domain.  The x-axis represents the rules, and the y-axis represents the correctness score.  Each bar shows the correctness of applying a particular rule in the airline domain, indicating the proportion of times the LLM applied a given rule correctly.", "section": "4.2.2 Rule-wise Analysis"}, {"figure_path": "https://arxiv.org/html/2412.08972/x4.png", "caption": "Figure 2: Rule-wise metrics of rules in airline domain.", "description": "This figure shows the performance of large language models (LLMs) on a rule-by-rule basis for the airline domain.  It presents two bar charts displaying the recall and precision for each of the rules used in the airline baggage fee calculation task.  Higher bars indicate better performance for a given rule. The x-axis represents the individual rules, and the y-axis represents the recall and precision scores.  This visualization allows for a granular analysis of the model's understanding and application of specific rules in the complex airline baggage fee calculation process.", "section": "4.2.2 Rule-wise Analysis"}, {"figure_path": "https://arxiv.org/html/2412.08972/x5.png", "caption": "(a) Airline", "description": "Figure 2(a) shows the recall of rules in the airline domain. The x-axis represents the rules, and the y-axis represents the recall. The height of each bar corresponds to the recall of the corresponding rule. It is observed that some rules have perfect recall, meaning the LLM always applies them correctly when needed, while others have lower recall, indicating the LLM sometimes fails to apply them correctly even when needed.", "section": "4.2.2 Rule-wise Analysis"}, {"figure_path": "https://arxiv.org/html/2412.08972/x6.png", "caption": "(b) NBA", "description": "This figure shows the results of problem-wise analysis for the NBA transaction domain.  It presents the precision (P(t)), rule application correctness (AC(t)), recall (R(t)), and accuracy (Acc(t)) for different LLMs (Llama-3.1 70B, Qwen-2.5 72B, Llama-3.1 405B, Claude-3.5 Sonnet, and GPT-40) under varying levels of difficulty (Level 1, Level 2, Level 3). Both 0-shot and 1-shot prompting strategies are compared.  The data illustrate the LLMs' performance in applying relevant rules correctly to solve NBA transaction problems according to the specified regulations.", "section": "4.2 Main Results"}, {"figure_path": "https://arxiv.org/html/2412.08972/x7.png", "caption": "(c) Tax", "description": "This figure shows the correlation between problem-wise metrics (recall, correctness, and precision) and accuracy for the tax domain in the RULEARENA benchmark.  It visually represents the relationship between the accuracy of LLM's answers and how well they recalled, correctly applied, and precisely selected the relevant rules for each problem. The x-axis represents the range of recall/correctness/precision values, while the y-axis represents accuracy.", "section": "4.2 Main Results"}, {"figure_path": "https://arxiv.org/html/2412.08972/x8.png", "caption": "Figure 3: Correlation between problem-wise metrics and accuracy. The correlation is the most obvious and almost linear between R\u2062(t)R\ud835\udc61{\\rm R}(t)roman_R ( italic_t ) and Acc\u2062(t)Acc\ud835\udc61{\\rm Acc}(t)roman_Acc ( italic_t ), while highly non-linear or unclear between other two metrics and Acc\u2062(t)Acc\ud835\udc61{\\rm Acc}(t)roman_Acc ( italic_t ).", "description": "This figure displays the correlation analysis between accuracy and other problem-wise metrics (precision, recall, rule application correctness).  The results show a strong, nearly linear positive correlation between accuracy and recall, indicating that higher recall strongly predicts higher accuracy. In contrast, the correlations between accuracy and precision, and accuracy and rule application correctness are considerably weaker and highly non-linear, suggesting that achieving high precision or rule application correctness alone is not sufficient to guarantee high overall accuracy. The figure helps visualize and understand how different factors contribute to successful rule-following performance in the three benchmark domains.", "section": "Main Results"}, {"figure_path": "https://arxiv.org/html/2412.08972/x9.png", "caption": "(a) Correctness", "description": "This figure shows the correlation between the problem-wise correctness metric and the accuracy metric across three different domains (Airline, NBA, and Tax).  It demonstrates how well the large language models (LLMs) apply the rules correctly in relation to the overall accuracy of their problem solutions.  The relationship is analyzed and visualized in three separate charts to better showcase the performance across all three domains.", "section": "Main Results"}, {"figure_path": "https://arxiv.org/html/2412.08972/x10.png", "caption": "(b) Recall", "description": "This bar chart visualizes the rule-wise recall of rules within the NBA domain. The x-axis represents individual rules, numbered 1 through 52. The y-axis denotes the recall rate, ranging from 0.0 to 1.0. Each bar's height corresponds to a specific rule's recall rate, indicating the proportion of times the rule was correctly applied across the dataset by LLMs. The chart offers a granular view of the rules' recall performance, helping to identify rules that are consistently or inconsistently applied by LLMs.", "section": "4.2.2 Rule-wise Analysis"}, {"figure_path": "https://arxiv.org/html/2412.08972/x11.png", "caption": "(c) Accuracy", "description": "This figure shows the correlation between the problem-wise accuracy (Acc(t)) and other metrics (Recall, Correctness, Precision) for the Tax domain in the RULEARENA benchmark.  It visually represents how well the LLMs accurately answer the problems compared to the ground truth, considering the influence of rule recall, correct rule application, and the precision of rule selection. The x-axis displays the ranges of Recall/Precision/Correctness, and the y-axis represents the corresponding accuracy.", "section": "Main Results"}, {"figure_path": "https://arxiv.org/html/2412.08972/x12.png", "caption": "Figure 4: The effect of distractive rules and context length. The \u201cStandard\u201d mode refers to the default setting of Level 1 tax problems, the \u201cDistractor\u201d mode appends nullified forms after the \u201cStandard\u201d input, and the \u201cPlaceholder\u201d mode adds meaningless tokens on space lines. Distractive rules lead to a significant drop on the performances of all LLMs, while meaningless tokens make little difference to the performance.", "description": "This figure demonstrates the impact of irrelevant information and increased context length on the performance of Large Language Models (LLMs) in tax calculations. Three experimental conditions are compared: a standard setting with only relevant information, a distractor setting where irrelevant (but seemingly valid) tax forms are added, and a placeholder setting where meaningless tokens are added to match the length of the distractor forms. The results show that while additional context length (placeholder setting) has a minimal impact on performance, the addition of irrelevant tax forms significantly reduces the accuracy of LLMs. This highlights LLMs' vulnerability to distraction from irrelevant information, even if the added information is seemingly valid and not inherently contradictory.", "section": "5 What Impacts Rule Following?"}, {"figure_path": "https://arxiv.org/html/2412.08972/x13.png", "caption": "Figure 5: Failure Case Studies. Existing LLMs commonly fail due to inadequate rule recall, inappropriate usage of similar rules, and computation errors.", "description": "Figure 5 presents three examples showcasing common failure modes of Large Language Models (LLMs) when performing rule-guided reasoning tasks.  The first example demonstrates inadequate rule recall, where the LLM overlooks a crucial rule leading to an incorrect calculation. The second example shows inappropriate usage of similar rules, where the LLM confuses similar but distinct regulations. The third example highlights computation errors, where the LLM makes a mathematical mistake, resulting in an incorrect final answer. These examples highlight the challenges LLMs face in accurately applying rules and performing complex computations, even when given comprehensive instructions.", "section": "6 Case Studies"}, {"figure_path": "https://arxiv.org/html/2412.08972/x14.png", "caption": "(a) Recall", "description": "This figure shows the rule-wise recall of rules in the airline domain. The x-axis represents the rules, and the y-axis represents the recall rate. Each bar corresponds to a rule, and its height indicates the recall rate for that rule. The rules are ordered from highest recall to lowest recall.", "section": "4.2.2 Rule-wise Analysis"}, {"figure_path": "https://arxiv.org/html/2412.08972/x15.png", "caption": "(b) Correctness", "description": "This figure shows the rule-wise application correctness of rules in the airline domain.  The x-axis represents the rules, ordered by correctness score. The y-axis represents the application correctness score for each rule, indicating the proportion of times the LLM correctly applied the rule.  A higher bar indicates higher correctness.", "section": "4.2.2 Rule-wise Analysis"}, {"figure_path": "https://arxiv.org/html/2412.08972/x16.png", "caption": "Figure 6: Rule-wise metrics of rules in airline domain.", "description": "This figure shows the performance of large language models (LLMs) on a rule-by-rule basis for the airline domain of the RULEARENA benchmark.  It presents two bar charts: one for recall (R(r)) and one for application correctness (AC(r)). Each bar represents a specific rule, and the height of the bar indicates the metric's value for that rule.  This visualization allows for a detailed analysis of which rules are easier or harder for LLMs to correctly identify and apply, highlighting potential weaknesses in the models' rule-guided reasoning capabilities in real-world scenarios.", "section": "4.2.2 Rule-wise Analysis"}]