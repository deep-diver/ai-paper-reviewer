[{"content": "| Models | #Frames | Param | TFLOPs | Narra. | Event | Ingre. | Causal | Theme | Conte. | Influ. | Role | Inter. | Behav. | Emoti. | Cook. | Traff. | Situa. | Avg. |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| **Closed-source LVLMs** |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| GPT-4o [31] | FPS=3 | - | - | 32.3 | 16.7 | 25.5 | 22.8 | 32.8 | 27.5 | 37.5 | 28.6 | 24.2 | 19.3 | 30.8 | 30.2 | 20.0 | 22.0 | 26.4 |\n| Qwen-VL-Max [3] | FPS=3 | - | - | 33.9 | 22.4 | 23.5 | 21.4 | 26.2 | 30.3 | 41.7 | 30.2 | 27.4 | 26.3 | 20.0 | 20.8 | 16.7 | 24.0 | 26.0 |\n| **Opened-source LVLMs** |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| LLaVA-1.5 [23] | 4 | 7B | 14.50 | 32.3 | 21.3 | 19.4 | 17.1 | 26.2 | 20.2 | 36.1 | 33.3 | 21.0 | 21.1 | 20.0 | 35.8 | 16.7 | 18.0 | 24.2 |\n| InternVL2 [7] | FPS=1 | 8B | 73.23 | 33.9 | 24.1 | 27.6 | 24.4 | 42.6 | 33.0 | 45.8 | 28.6 | 19.4 | 22.8 | 21.5 | 34.0 | 20.0 | 24.0 | 28.7 |\n| LLaVA-N-Inter [17] | FPS=1 | 7B | 62.78 | 24.2 | 23.6 | 26.5 | 19.2 | 31.1 | 32.1 | 31.9 | 17.5 | 24.2 | 21.1 | 26.2 | 30.2 | 13.3 | 20.0 | 24.4 |\n| Qwen2-VL [3] | FPS=1 | 7B | 64.60 | 27.4 | 23.0 | 24.5 | 23.5 | 29.5 | 31.2 | 47.2 | 31.7 | 22.6 | 28.1 | 40.0 | 22.6 | 30.0 | 18.0 | 28.5 |\n| LongVA-DPO [49] | 128 | 7B | 465.4 | 35.5 | 14.9 | 16.3 | 19.0 | 34.4 | 22.0 | 37.5 | 23.8 | 29.0 | 22.8 | 20.0 | 37.7 | 16.7 | 12.0 | 24.4 |\n| mPLUG-Owl3 [46] | FPS=1 | 7B | 89.78 | 30.6 | 23.6 | 20.4 | 22.3 | 37.7 | 29.4 | 48.6 | 34.9 | 30.6 | 24.6 | 27.7 | 24.5 | 13.3 | 24.0 | 28.0 |\n| LLaVA-N-Video [50] | FPS=1 | 7B | 60.42 | 31.2 | 20.2 | 16.2 | 17.6 | 36.5 | 32.7 | 30.6 | 24.5 | 26.4 | 24.5 | 34.7 | 20.8 | 20.3 | 17.0 | 25.2 |\n| Ours | 2.36 | 8.5B | 9.26 | 45.2 | 27.0 | 33.7 | 26.1 | 39.3 | 36.7 | 55.6 | 41.3 | 30.6 | 29.8 | 30.8 | 35.8 | 20.0 | 26.0 | 34.1 |", "caption": "Table 1: \nMain Result on Our Objective Benchmark. We report results of closed-source and opened-source LVLMs with ours. The process of constructing task evaluations is shown in the supplementary. TFLOPs refers to the total computational cost of inference, measured under the same 16-second video input.", "description": "Table 1 presents the performance comparison of various Large Vision Language Models (LVLMs) on the VideoEspresso benchmark.  It includes both closed-source models (like GPT-4) and open-source models (like LLaVA). The table shows the average accuracy across 14 different video reasoning tasks, categorized by the type of reasoning involved.  Each LVLMs' performance is shown in terms of accuracy for each task, alongside metadata including the number of frames processed, model parameters (in billions), and the total TeraFLOPs (TFLOPS) of computation required for a 16-second video. This allows a comprehensive comparison of accuracy, efficiency, and computational cost across various LVLMs.", "section": "4. Hybrid LVLMs Collaboration for VideoQA"}, {"content": "| Models | Log. | Fac. | Acc. | Con. | Overall |\n|---|---|---|---|---|---| \n| *Closed-source LVLMs* |  |  |  |  |  |\n| GPT-4o | 73.15 | 63.11 | 61.66 | 70.02 | 66.13 |\n| Qwen-VL-Max | 62.46 | 50.33 | 48.43 | 60.21 | 53.37 |\n| *Open-source LVLMs* |  |  |  |  |  |\n| LLaVA 1.5 | 60.53 | 49.56 | 49.93 | 62.1 | 52.12 |\n| InternVL2 | 70.64 | 56.32 | 54.53 | 66.76 | 60.05 |\n| LLaVA-N-inter | 63.27 | 52.34 | 48.45 | 66.78 | 55.16 |\n| Qwen2-VL-7B | 66.31 | 53.67 | 50.84 | 68.88 | 57.66 |\n| LongVA-7B-DPO | 67.98 | 54.72 | 52.78 | 58.38 | 57.19 |\n| mPLUG-Owl3 | 66.14 | 53.05 | 50.97 | 67.3 | 57.14 |\n| LLaVA-N-Video | 63.42 | 54.11 | 49.55 | 63.31 | 56.43 |\n| Ours | **72.25** | **61.28** | **59.68** | **75.73** | **65.84** |", "caption": "Table 2: Results on Subjective Benchmark. We report the metrics of Logic (Log.), Factuality (Fac.), Description Accuracy (Acc.), and Conciseness (Con.).", "description": "This table presents a subjective evaluation of various Large Vision Language Models (LVLMs) on video question answering tasks.  The models' responses are assessed across four key dimensions: logical reasoning (Log.), factuality (Fac.), description accuracy (Acc.), and conciseness (Con.). Higher scores in each category indicate better performance, providing a comprehensive understanding of the models' strengths and weaknesses in generating high-quality, coherent answers.", "section": "5. Experiments"}, {"content": "| Model | Sample | #Frame | Ratio<sub>tok</sub> | TFLOPs | Acc. |\n|---|---|---|---|---|---| \n| GPT-4o | Uniform | 16 | 1 | - | 26.86 |\n| GPT-4o | 1B/0.5B | 2.77 | 0.17 | - | 28.26 |\n| GPT-4o | 1B/1.5B | 2.36 | 0.15 | - | 29.45 |\n| InternVL2 | Uniform | 16 | 1 | 73.23 | 28.57 |\n| InternVL2 | 1B/0.5B | 2.77 | 0.17 | 12.68 | 29.23 |\n| InternVL2 | 1B/1.5B | 2.36 | 0.15 | 10.80 | 30.03 |\n| LongVA | Uniform | 128 | 1 | 465.44 | 24.41 |\n| LongVA | 1B/0.5B | 2.77 | 0.02 | 10.07 | 23.18 |\n| LongVA | 1B/1.5B | 2.36 | 0.02 | 8.58 | 23.85 |\n| LLaVA-N-i | Uniform | 16 | 1 | 62.78 | 24.37 |\n| LLaVA-N-i | 1B/0.5B | 2.77 | 0.17 | 10.86 | 24.20 |\n| LLaVA-N-i | 1B/1.5B | 2.36 | 0.15 | 9.26 | 24.26 |", "caption": "Table 5: \nEvaluations results with selector adoption.", "description": "This table presents the results of experiments evaluating the effectiveness of incorporating a frame selector module into various Large Vision Language Models (LVLMs).  The frame selector module aims to reduce computational cost by selecting only the most relevant frames for video understanding tasks.  The table shows the accuracy achieved by different models (GPT-40, InternVL2, LongVA, LLaVA-N-i) using both uniform frame sampling and the proposed frame selector.  Results are presented in terms of accuracy and computational cost (TFLOPS), broken down by model and frame selection strategy.  The data demonstrates the trade-off between computational efficiency and accuracy when using a frame selector. ", "section": "4. Hybrid LVLMs Collaboration for VideoQA"}, {"content": "| Benchmark | Core Frames | CoT | # Questions |\n|---|---|---|---| \n| How2QA [21] | \u2717 | \u2717 | 2,852 |\n| ActivityNet-QA [47] | \u2717 | \u2717 | 8,000 |\n| NExT-QA [41] | \u2717 | \u2717 | 8,564 |\n| MovieChat [35] | \u2717 | \u2717 | 13,000 |\n| TVQA [15] | \u2717 | \u2717 | 15,253 |\n| MSRVTT-QA [43] | \u2717 | \u2717 | 72,821 |\n| VideoCoT [38] | \u2717 | **T** | 11,182 |\n| **VideoEspreeso** | \u2713 | **T&V** | **203,546** |", "caption": "Table 6: Dataset comparison between videoQA datasets. T and V represent the textual and visual elements in the CoT, respectively.", "description": "This table compares several video question answering (VideoQA) datasets, highlighting their key characteristics.  It shows whether each dataset includes core frame annotations, chain-of-thought (CoT) annotations (textual and visual), and the total number of questions. The datasets compared are How2QA, ActivityNet-QA, NEXT-QA, MovieChat, TVQA, MSRVTT-QA, VideoCoT and VideoEspresso. The presence of textual (T) and visual (V) CoT annotations is indicated for each dataset.  This allows for a comparison of dataset size and the complexity of the reasoning tasks they support.", "section": "3. VideoEspresso"}, {"content": "| Task | # Train Set | # Test Set |\n|---|---|---|\n| Causal Inference | 87,009 | 426 |\n| Contextual Interpretation | 20,057 | 109 |\n| Event Process | 29,227 | 174 |\n| Interaction Dynamics | 7,322 | 62 |\n| Behavior Profiling | 660 | 57 |\n| Emotional Recognition | 3,505 | 65 |\n| Influence Tracing | 5,749 | 72 |\n| Role Identification | 9,134 | 63 |\n| Narrative Structuring | 3,940 | 62 |\n| Thematic Insight | 10,650 | 61 |\n| Situational Awareness | 1,018 | 50 |\n| Cooking Steps | 276 | 53 |\n| Ingredient Details | 22,552 | 98 |\n| Traffic Analysis | 1,065 | 30 |\n| **Total** | **202,164** | **1,382** |", "caption": "Table 7: Tasks distribution and dataset split in VideoEspresso.", "description": "This table details the distribution of tasks and the dataset split within the VideoEspresso dataset.  It shows how many instances (train and test) are included for each of the fourteen tasks defined in the dataset, providing a quantitative overview of the dataset's composition and balance across different reasoning challenges.", "section": "3. VideoEspresso"}, {"content": "| config | Stage1 | Stage2 |\n|---|---|---|\n| input resolution | 224 | 224 |\n| max token length | 6144 | 6144 |\n| LoRA | True | True |\n| weight ratio | 0.02 | 0.02 |\n| learning rate schedule | cosine decay | cosine decay |\n| learning rate | 2e-5 | 1e-5 |\n| batch size | 16 | 16 |\n| warmup epochs | 0.03 | 0.03 |\n| total epochs | 1 | 1 |", "caption": "Table 8: \nTraining Hyperparameters for different stages.", "description": "This table details the hyperparameters used during the two training stages of the VideoEspresso model. It lists the settings for various aspects of the training process, including image resolution, maximum token length, LORA (Low-Rank Adaptation) usage, weight ratio, learning rate schedule, learning rate, batch size, warmup epochs, and total epochs. The table shows how these hyperparameters differ between Stage 1 and Stage 2 of the training process.  Understanding these settings is crucial for replicating the model's training and understanding its performance.", "section": "B. Training Implementation"}, {"content": "| Category                     | Description                                                                                                                                                               |\n|------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| **Logical Reasoning**         |                                                                                                                                                                          |\n| Causal Inference             | How did the actions of the robot and display on the screen contribute to the successful resolution in the control room?                                                     |\n| Contextual Interpretation     | How does the presence of the small cat and George\u2019s exploration relate to the chef\u2019s activities?                                                                            |\n| Event Process                | What transition do the rabbits experience from the time the moon rose to when they drift off to sleep?                                                                    |\n| **Social Understanding**      |                                                                                                                                                                          |\n| Interaction Dynamics         | Considering the atmosphere and expressions depicted, what can be concluded about the progression of the interaction between the man and the woman?                              |\n| Behavior Profiling            | Discuss how the actions of the baby triceratops with different dinosaurs reveal aspects of its behavior and the responses of the other dinosaurs.                                 |\n| Emotional Recognition         | How does the emotional journey of the small purple dinosaur from feeling lost to excitement tie into the group\u2019s decision to explore the cave?                                |\n| Influence Tracing            | How did the presence of the dolphin and the sea monster influence the dinosaurs\u2019 experience at the waterbody?                                                                |\n| **Discourse Comprehension** |                                                                                                                                                                          |\n| Role Identification          | How does the woman\u2019s role in coordinating town safety relate to the device\u2019s activation with a green checkmark and an orange flame?                                           |\n| Narrative Structuring        | Considering the changes between the two frames, what can you infer about the narrative progression between the two depicted scenes?                                             |\n| Thematic Insight             | How do the changing production logos contribute to the thematic preparation for the viewer before the main storyline begins?                                                   |\n| Situational Awareness         | Based on the sequence of events, how does the situation described contribute to the visual effect observed in the third frame?                                                   |\n| **Reality Application**       |                                                                                                                                                                          |\n| Cooking Steps                | Considering the sequence of actions, what cooking technique is being employed, and how is it crucial for the fried chicken?                                                    |\n| Ingredient Details           | If the person is preparing chili con carne, what is the purpose of the liquid being poured into the pan?                                                                     |\n| Traffic Analysis             | Analyze the potential destinations of the visible vehicles based on their types and cargo as inferred from the images.                                                          |", "caption": "Table 9: \nOur proposed task categories with question prototypes.", "description": "This table presents fourteen distinct video reasoning tasks included in the VideoEspresso dataset.  For each task, a concise description and an example question prototype are provided to illustrate the type of reasoning involved. These tasks cover a wide range of reasoning abilities, including causal inference, contextual interpretation, social understanding, discourse comprehension, and real-world application scenarios.", "section": "3. VideoEspresso"}]