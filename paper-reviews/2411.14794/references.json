{"references": [{"fullname_first_author": "Jinze Bai", "paper_title": "Qwen-vl: A frontier large vision-language model with versatile abilities", "publication_date": "2023-00-00", "reason": "This paper introduces a large vision-language model that is a key component of the proposed Hybrid LVLMs Collaboration framework, which is central to achieving cost-effective and accurate video reasoning."}, {"fullname_first_author": "Zhe Chen", "paper_title": "Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks", "publication_date": "2023-12-14", "reason": "InternVL is used for frame-level captioning, a crucial step in the VideoEspresso pipeline for filtering out redundant frames and generating QA pairs."}, {"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-00-00", "reason": "CLIP is used for verifying consistency between labels and objects in bounding boxes during spatial annotation for the Chain-of-Thought process, improving the quality and accuracy of video reasoning annotations."}, {"fullname_first_author": "Haotian Liu", "paper_title": "Visual instruction tuning", "publication_date": "2023-00-00", "reason": "This paper provides the foundation for the two-stage supervised fine-tuning of the reasoning LVLM, enhancing its ability to effectively leverage multimodal spatiotemporal evidence for complex reasoning tasks."}, {"fullname_first_author": "OpenAI", "paper_title": "GPT-4 technical report", "publication_date": "2023-00-00", "reason": "GPT-4 is extensively used for several crucial stages in constructing the VideoEspresso dataset, including generating QA pairs and chain-of-thought annotations, playing a vital role in dataset creation."}]}