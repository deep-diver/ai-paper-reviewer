[{"figure_path": "https://arxiv.org/html/2411.14794/x3.png", "caption": "Figure 1: Overview of VideoEspresso. (a) Comparison of annotation pipelines: Unlike traditional videoQA datasets, VideoEspresso features an automatic pipeline for constructing complex reasoning QA tasks and multimodal Chain-of-Thought (CoT) annotations. This enhances the diversity of QA data and significantly improves scalability. (b) Examples from VideoEspresso: Illustrated are sample question-answer pairs, along with CoT bounding boxes and evidence annotations, demonstrating the dataset\u2019s richness. (c) Benchmark performance: Comparative results on our benchmark highlight the video reasoning capabilities of our model.", "description": "Figure 1 provides a comprehensive overview of the VideoEspresso dataset. Panel (a) contrasts the annotation process of VideoEspresso with traditional videoQA datasets, highlighting VideoEspresso's automated pipeline for generating complex reasoning questions and multimodal Chain-of-Thought (CoT) annotations.  This automation leads to a more diverse and scalable dataset. Panel (b) showcases example question-answer pairs from VideoEspresso, illustrating the inclusion of CoT bounding boxes and evidence annotations, which enrich the dataset's complexity and provide more detailed reasoning information. Panel (c) presents benchmark performance results, comparing various Large Vision Language Models (LVLMs) on the VideoEspresso benchmark and highlighting the superior video reasoning capabilities of the proposed model.", "section": "3. VideoEspresso"}, {"figure_path": "https://arxiv.org/html/2411.14794/x4.png", "caption": "Figure 2: The automatic generation pipeline of VideoEspresso. (i) Question-Answer Pair Construction: We use video frame-leveled captions to extract the key frames of the video and group descriptions of these frames. Then, we prompt GPT-4 to design questions for each group of video frames. (ii) Multimodal Chain-of-Thought Annotation: We extract key evidence text and generate captions with the highest relevance to the question with GPT-4o. Additionally, we annotate spatial and temporal information for key items, which results in multimodal Chain of Thought data pairs grounded in both temporal and spatial dimensions.", "description": "This figure illustrates the two-stage automatic pipeline used to create the VideoEspresso dataset. The first stage, Question-Answer Pair Construction, involves generating frame-level captions, grouping similar captions, and then using GPT-4 to create questions based on these groups.  The second stage, Multimodal Chain-of-Thought Annotation, refines this process by selecting key evidence and generating highly relevant captions with GPT-40.  Crucially, this stage adds spatial and temporal annotations to key items, resulting in multimodal Chain-of-Thought (CoT) data pairs which include both spatial and temporal context.", "section": "3. VideoEspresso"}, {"figure_path": "https://arxiv.org/html/2411.14794/x5.png", "caption": "Figure 3: The statistical analysis of our VideoEspresso dataset.", "description": "Figure 3 presents a statistical analysis of the VideoEspresso dataset, illustrating the distribution of distances between adjacent core frames (a), the number of key items (b), and the data sources (c).  The distribution of distances highlights the variability in the temporal spacing between key frames across different tasks, indicating that uniform sampling is not optimal.  The key item counts reveal the varying complexity of reasoning tasks, with some involving only a few key items while others involve numerous elements. The data sources breakdown shows the diverse origin of videos in VideoEspresso.", "section": "3. VideoEspresso"}, {"figure_path": "https://arxiv.org/html/2411.14794/x6.png", "caption": "Figure 4: The dataset attributes comparison between our VideoEspresso and MVbench.", "description": "Figure 4 presents a comparative analysis of the attributes of VideoEspresso and MVBench datasets.  It includes subfigures (a) and (b). Subfigure (a) compares the token length distributions of questions and answers in both datasets, illustrating the difference in length and complexity. Subfigure (b) presents word clouds for questions and answers in both datasets, visually highlighting the key terms and concepts prevalent in each. This comparison reveals the distinct characteristics of VideoEspresso, showing its focus on complex reasoning tasks as opposed to simpler fact-based queries typical of MVBench.", "section": "3. VideoEspresso"}, {"figure_path": "https://arxiv.org/html/2411.14794/x7.png", "caption": "Figure 5: Two-Stage Video Evidence of Thought Training Procedure. The Frame Selector comprises a tiny LVLM and a tiny LLM, tasked with generating captions for videos and selecting the most relevant frame to as core video token for large reasoning model. A two-stage supervised fine-tuning technique is employed. During stage-1, a set of cue prompts is introduced to guide the model in producing evidence, while in stage-2, the evidence generated from stage-1 is concatenated and used directly to guide the answer generation.", "description": "This figure illustrates the two-stage training process for the Video Evidence of Thought model.  The process begins with a Frame Selector, composed of a small Vision Language Model (LVLM) and a small Language Model (LLM). This selector first generates captions for the input video frames and then selects the most pertinent frame as the core video token. This core frame is then used for training a larger reasoning model. The training utilizes a two-stage supervised fine-tuning approach. In stage one, cue prompts guide the model to generate evidence relevant to a question. In stage two, this evidence is combined and used to train the model to directly produce an answer.", "section": "4. Hybrid LVLMs Collaboration for VideoQA"}, {"figure_path": "https://arxiv.org/html/2411.14794/x8.png", "caption": "Figure 6: Comparison between VideoEspresso and other VideoQA dataset.", "description": "Figure 6 demonstrates the differences in data annotation and question-answering approaches between VideoEspresso and other VideoQA datasets.  Traditional VideoQA datasets typically sample frames uniformly across the video and generate simple question-answer pairs based on overall video content.  In contrast, VideoEspresso selects and groups key frames relevant to the question, constructing complex, fine-grained reasoning tasks that require understanding of the temporal and spatial relationships between those frames.  The figure visually illustrates this by displaying examples of how questions and answers are formulated for each dataset and showcasing the richer context and detailed annotations (bounding boxes, key items and reasoning steps) included in VideoEspresso.", "section": "3. VideoEspresso"}, {"figure_path": "https://arxiv.org/html/2411.14794/x9.png", "caption": "Figure 7: QA-Construction Prompt.", "description": "This figure shows the prompt used for constructing question-answer pairs in the VideoEspresso dataset.  The prompt instructs GPT-4 to generate multiple QA pairs based on a given list of video frame captions. It emphasizes that the generated questions should necessitate multi-image reasoning, involve complex logic, and avoid subjective or overly open-ended queries.  The prompt also specifies constraints on question and answer formats, emphasizing consistency with the video's narrative and observable information.", "section": "3. VideoEspresso"}, {"figure_path": "https://arxiv.org/html/2411.14794/x10.png", "caption": "Figure 8: QA-Filter Prompt.", "description": "This prompt is used to filter low-quality question-answer pairs generated in the previous step.  It provides instructions to assess each QA pair based on several criteria: ensuring the questions and answers are consistent with the observed content in the video, confirming that the questions are not overly subjective or open-ended, and checking for continuity within the narrative flow. For any low-quality QA pairs, a brief explanation of the violated criteria is required.", "section": "3. Multimodal Chain-of-Thought Annotation"}, {"figure_path": "https://arxiv.org/html/2411.14794/x11.png", "caption": "Figure 9: CoT-Evidence Construction Prompt.", "description": "This figure shows the prompt used to generate the Chain-of-Thought (CoT) evidence annotations in the VideoEspresso dataset.  The prompt guides GPT-4 to select the most relevant captions from a list, extract key objects from those captions, and construct a sentence explaining the answer using these key objects as evidence. The prompt emphasizes the use of both textual and visual information for reasoning.", "section": "3.4. Multimodal Chain-of-Thought Annotation"}, {"figure_path": "https://arxiv.org/html/2411.14794/x12.png", "caption": "Figure 10: Subjective Evaluation Prompt.", "description": "This figure shows the prompt used for subjective evaluation of the generated answers.  The prompt instructs the evaluator to score the model's output based on several criteria, namely: logic, factuality, accuracy, and conciseness. Each criterion is defined and explained, with instructions for evaluating the answer on a scale of 1 to 10 for each. The evaluator is instructed to provide an integrated overall score, reflecting the holistic quality of the answer. The scoring guidelines are clearly laid out to ensure consistency and objectivity across different evaluations.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2411.14794/extracted/6017056/pic/Supp_opt_len.png", "caption": "Figure 11: Example of test set. R\ud835\udc45Ritalic_R represent the Reference Answer, while Disubscript\ud835\udc37\ud835\udc56D_{i}italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT stand for the i\ud835\udc56iitalic_i-th Distractor.", "description": "Figure 11 shows an example from the VideoEspresso test set, illustrating how the objective evaluation is conducted.  It presents a question related to a video clip and then provides a reference answer (R) along with three distractor answers (D1, D2, D3). The task is to determine which of these options is the correct answer to the question given the video content.  The distractor answers are designed to be plausible but incorrect, providing a challenge to the evaluation process.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2411.14794/x13.png", "caption": "Figure 12: The Distribution of token length disparities between reference answers and the longest distractor option.", "description": "This histogram illustrates the distribution of differences in the number of tokens between the reference answers and the longest distractor option in the objective evaluation of the VideoEspresso dataset.  The x-axis represents the token length difference (reference answer length minus longest distractor length), while the y-axis shows the frequency of such differences.  The distribution is roughly centered around zero, indicating that the length of reference answers and their corresponding longest distractor options are fairly balanced. A relatively small difference in the number of tokens suggests that the distractors were carefully designed to be comparable to the reference answers.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2411.14794/x14.png", "caption": "Figure 13: Example of over-analysis with GPT-4o.", "description": "This figure shows a comparison of how GPT-4 and VideoEspresso's model analyze a video clip showing elephants and monkeys foraging. GPT-4 provides a detailed but somewhat irrelevant answer, incorporating information not directly visible in the video.  VideoEspresso's model focuses on visual details and directly observable information within the video to produce a more concise and accurate description of the animals' foraging behaviors.", "section": "5. Experiments"}]