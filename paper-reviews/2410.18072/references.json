{"references": [{" publication_date": "2023", "fullname_first_author": "Josh Achiam", "paper_title": "GPT-4 technical report", "reason": "This paper is foundational as it presents GPT-4, a highly influential large language model. Its impact on various AI tasks is significant, thus making it essential to the study of predictive models and their capabilities, especially the text-based predictive modeling in the S0 stage, as discussed in Section 2.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Bowen Baker", "paper_title": "Video pretraining (VPT): Learning to act by watching unlabeled online videos", "reason": "VPT introduces a novel video pretraining method which significantly advances the understanding and utilization of video data in training agents, including those in the context of embodied AI.  This method is especially important to S2 and S3 Predictive Models based on their ability to generate and use videos for action, described in Sections 2 and 3.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Kevin Black", "paper_title": "Zero-shot robotic manipulation with pretrained image-editing diffusion models", "reason": "This paper demonstrates a significant advancement in zero-shot robotic manipulation using pretrained image-editing models. Its relevance to the study lies in the concept of predictive models driving actions. This work is related to S3 stages and discussed in Section 2.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Tim Brooks", "paper_title": "Instructpix2pix: Learning to follow image editing instructions", "reason": "This paper provides a significant contribution to image generation and manipulation, relevant to the S1 stage of predictive models. The ability to generate and edit images based on instructions is crucial for building more sophisticated predictive models that could potentially generate videos, discussed in Sections 2 and 3.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "Holger Caesar", "paper_title": "nuscenes: A multimodal dataset for autonomous driving", "reason": "The nuScenes dataset is a critical resource for autonomous driving research. Its multimodal nature, encompassing various sensor modalities, makes it vital for training and evaluating predictive models in autonomous driving scenarios, specifically relevant to the AD scenario in Section 4.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Yi Chen", "paper_title": "Egoplan-bench: Benchmarking egocentric embodied planning with multimodal large language models", "reason": "This paper introduces EgoPlan-Bench, a benchmark specifically designed for egocentric embodied planning. Its relevance to the study is apparent due to its focus on embodied AI and predictive models, particularly those in the S0 stage, as described in Section 2.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Zeren Chen", "paper_title": "RH20T-P: A primitive-level robotic dataset towards composable generalization agents", "reason": "This paper introduces the RH20T-P dataset for evaluating robotic manipulation tasks.  The dataset's focus on primitive-level actions and composable generalization is crucial for evaluating the capabilities of embodied predictive models at the S3 stage and the design of the evaluation framework (WorldSimBench) in section 4.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Wei-Lin Chiang", "paper_title": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality", "reason": "This paper introduces Vicuna, a large language model relevant to the S0 stage of Predictive Models. Its performance demonstrates the advancements in text-based predictive models and its impact on other stages, detailed in Section 2.", "section_number": 2}, {" publication_date": "2017", "fullname_first_author": "Alexey Dosovitskiy", "paper_title": "Carla: An open urban driving simulator", "reason": "CARLA is a widely used open-source simulator for autonomous driving research and is explicitly used in the AD portion of the experiments (section 5). The simulator is essential for evaluating and testing predictive models in driving scenarios, as detailed in Section 4.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Danny Driess", "paper_title": "Palm-e: An embodied multimodal language model", "reason": "This paper introduces PALM-E, an embodied multimodal language model which is relevant to the S3 stage models (World Simulators).  Its ability to integrate language and multimodal data into actions highlights the advancements in embodied AI. It is discussed in Section 2.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Yilun Du", "paper_title": "Video language planning", "reason": "This paper focuses on Video Language Planning, a crucial aspect of predictive models that generate videos for actions (S3 stage).  This is directly relevant to the core concept of World Simulators and is discussed in Section 2.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Yilun Du", "paper_title": "Learning universal policies via text-guided video generation", "reason": "This paper is related to the use of video generation models for embodied AI. It is directly relevant to S3 models (World Simulators) and the evaluation framework (WorldSimBench), detailed in Sections 2 and 4.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Frederik Ebert", "paper_title": "Bridge data: Boosting generalization of robotic skills with cross-domain datasets", "reason": "This paper is relevant because it highlights the importance of using diverse and extensive datasets for training predictive models, particularly for embodied AI tasks.  Data quality and quantity are crucial for training World Simulators, discussed in Section 2.", "section_number": 2}, {" publication_date": "2017", "fullname_first_author": "Raghav Goyal", "paper_title": "The\" something something\" video database for learning and evaluating visual common sense", "reason": "The Something-Something dataset is a valuable resource for video understanding and common sense reasoning. This is relevant because it's used for training predictive models that generate videos, specifically relevant to S2 and S3 stages discussed in Section 2.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "William Guss", "paper_title": "Minerl: A large-scale dataset of minecraft demonstrations", "reason": "MineRL is a large-scale dataset for Minecraft demonstrations, used in the Open-Ended Embodied Environment portion of the WorldSimBench evaluation (Section 4). The dataset's importance lies in its relevance for training and testing embodied agents in complex, open-ended environments.", "section_number": 4}, {" publication_date": "2018", "fullname_first_author": "David Ha", "paper_title": "World models", "reason": "This paper introduces the concept of \"World Models\", which is closely related to the concept of \"World Simulators\" presented in this paper. The connection between World Models and World Simulators makes this reference relevant to the core of the paper, discussed in Sections 1 and 3.", "section_number": 3}, {" publication_date": "2021", "fullname_first_author": "Michael Janner", "paper_title": "Planning with diffusion for flexible behavior synthesis", "reason": "This paper introduces a method for planning with diffusion models, which is a crucial element in generating actions from video predictions. This method is relevant to Section 4, especially for Implicit Manipulative Evaluation of World Simulators and is referenced in Section 2.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Bolin Lai", "paper_title": "LEGO: Learning egocentric action frame generation via visual instruction tuning", "reason": "LEGO is a benchmark for evaluating egocentric action generation models, relevant to the S1 and S2 stages of Predictive Models.  The use of visual instruction tuning highlights advancements in image-based predictive modeling and its relationship to video generation, discussed in Section 2.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Xiaoqi Li", "paper_title": "ManipLLM: Embodied multimodal large language model for object-centric robotic manipulation", "reason": "ManipLLM focuses on embodied multimodal large language models for object-centric robotic manipulation, relevant to S3 models and the overall concept of embodied AI.  Its significance lies in the advancements in using language models for controlling robots, which is central to the concept of World Simulators, detailed in Section 2.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Haotian Liu", "paper_title": "Visual instruction tuning", "reason": "Visual instruction tuning is a technique used for training models to perform tasks based on visual instructions.  It's relevant to the S1 and S2 stages of Predictive Models, as visual inputs are crucial for more advanced video generation models (S3), discussed in Section 2.", "section_number": 2}]}