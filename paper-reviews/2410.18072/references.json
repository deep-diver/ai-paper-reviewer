{"references": [{" publication_date": "2023", "fullname_first_author": "Josh Achiam", "paper_title": "Gpt-4 technical report", "reason": "This paper is foundational to understanding the capabilities of large language models and their applications in various fields. As GPT-4 is a leading model for various natural language tasks, this paper's insights into its capabilities are highly relevant for the context of evaluating World Simulators' ability to generate meaningful outputs based on complex input instructions.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Bowen Baker", "paper_title": "Video pretraining (vpt): Learning to act by watching unlabeled online videos", "reason": "This paper introduces a novel approach to learning agent behavior from unlabeled videos, which is a crucial aspect of embodied AI and has direct relevance to the evaluation of World Simulators. The concepts and techniques described in this paper are highly relevant to understanding and evaluating the video generation and action transformation aspects of World Simulators.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Kevin Black", "paper_title": "Zero-shot robotic manipulation with pretrained image-editing diffusion models", "reason": "This paper demonstrates the effectiveness of pretrained image-editing models in robotic manipulation, which is highly relevant to the evaluation of World Simulators in the context of Robot Manipulation scenarios. The success of zero-shot manipulation using pre-trained models highlights the potential of leveraging pre-trained models in embodied tasks, which is critical for efficiently evaluating World Simulators.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Tim Brooks", "paper_title": "Instructpix2pix: Learning to follow image editing instructions", "reason": "This paper is important because it directly addresses the challenge of aligning model outputs with human instructions in image editing tasks. The approach of using instruction following in image editing is highly relevant to the task of evaluating World Simulators' ability to generate videos that accurately reflect the input instructions.", "section_number": 4}, {" publication_date": "2020", "fullname_first_author": "Holger Caesar", "paper_title": "nuscenes: A multimodal dataset for autonomous driving", "reason": "This paper provides a high-quality multimodal dataset for autonomous driving, which is crucial for evaluating the performance of World Simulators in Autonomous Driving scenarios.  The dataset's rich annotation and diverse driving conditions allow for a rigorous assessment of the model's ability to generate realistic and contextually accurate videos.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Yi Chen", "paper_title": "Egoplan-bench: Benchmarking egocentric embodied planning with multimodal large language models", "reason": "This paper presents a benchmark for evaluating egocentric embodied planning capabilities of large language models, which is directly relevant to the evaluation of predictive models in Open-Ended Embodied Environments. The benchmark helps provide an understanding of the state-of-the-art in embodied planning, offering insights into evaluating the high-level reasoning aspects of World Simulators.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Zeren Chen", "paper_title": "Rh20t-p: A primitive-level robotic dataset towards composable generalization agents", "reason": "This paper introduces a detailed and comprehensive dataset for evaluating robotic manipulation abilities, which is essential for evaluating the capabilities of World Simulators in Robot Manipulation scenarios. The dataset\u2019s focus on primitive-level actions and its composable structure allow for a fine-grained assessment of the model's ability to generate actions that are both contextually appropriate and physically realistic.", "section_number": 4}, {" publication_date": "2017", "fullname_first_author": "Alexey Dosovitskiy", "paper_title": "Carla: An open urban driving simulator", "reason": "This paper introduces the CARLA simulator, a widely used platform for autonomous driving research. CARLA is central to the evaluation of World Simulators' capabilities in Autonomous Driving scenarios. The simulator\u2019s realistic environment and comprehensive API allow for detailed and objective evaluation of model performance.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Danny Driess", "paper_title": "Palm-e: An embodied multimodal language model", "reason": "This paper demonstrates the capabilities of a multimodal language model in embodied tasks, which is highly relevant to the evaluation of World Simulators.  The combination of language and multimodal capabilities is crucial for generating realistic videos that capture complex interactions and physical constraints.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Yilun Du", "paper_title": "Video language planning", "reason": "This paper introduces a novel approach to video generation that incorporates language planning, which is highly relevant to the evaluation of World Simulators. The ability to generate videos based on high-level instructions is critical for assessing World Simulators.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Yilun Du", "paper_title": "Learning universal policies via text-guided video generation", "reason": "This paper explores the use of text-guided video generation to learn universal policies, which is directly relevant to the evaluation of World Simulators. The ability to generate videos based on textual instructions is crucial for assessing World Simulators' ability to capture complex interactions and produce high-quality video outputs.", "section_number": 4}, {" publication_date": "2021", "fullname_first_author": "Frederik Ebert", "paper_title": "Bridge data: Boosting generalization of robotic skills with cross-domain datasets", "reason": "This paper addresses the challenge of improving the generalization ability of robotic skills across diverse scenarios and domains, which is highly relevant to evaluating the capabilities of World Simulators in embodied scenarios.  The techniques described in this paper are invaluable for understanding and improving the ability of World Simulators to generalize to new and unseen situations.", "section_number": 4}, {" publication_date": "2017", "fullname_first_author": "Raghav Goyal", "paper_title": "The\" something something\" video database for learning and evaluating visual common sense", "reason": "This paper introduces the Something-Something video database, a dataset that has direct relevance to evaluating visual common sense in videos, which is an important aspect of evaluating World Simulators. The dataset's focus on common sense reasoning and its large-scale nature are invaluable for developing and evaluating models capable of generating videos that capture complex interactions and adhere to the principles of visual common sense.", "section_number": 4}, {" publication_date": "2022", "fullname_first_author": "Kristen Grauman", "paper_title": "Ego4d: Around the world in 3,000 hours of egocentric video", "reason": "This paper introduces the Ego4D dataset, a large-scale egocentric video dataset that is essential for evaluating embodied AI systems and generating high-quality egocentric videos, which is directly relevant to the evaluation of World Simulators in Open-Ended Embodied Environment scenarios.  The large scale and rich annotation of the dataset are invaluable for developing and evaluating models capable of generating realistic egocentric videos.", "section_number": 4}, {" publication_date": "2019", "fullname_first_author": "William H Guss", "paper_title": "Minerl: A large-scale dataset of minecraft demonstrations", "reason": "This paper introduces MineRL, a large-scale dataset of Minecraft demonstrations which is essential for evaluating the performance of World Simulators in Open-Ended Embodied Environments. The dataset's size and complexity enable a comprehensive evaluation of models' capabilities in this challenging environment, providing valuable insights for future research.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Yuwei Guo", "paper_title": "Animatediff: Animate your personalized text-to-image diffusion models without specific tuning", "reason": "This paper demonstrates the potential of using text-to-image diffusion models for generating animations, which is relevant to evaluating the video generation capabilities of World Simulators. The ability to generate high-quality animations based on textual instructions is a significant aspect of evaluating World Simulators.", "section_number": 4}, {" publication_date": "2018", "fullname_first_author": "David Ha", "paper_title": "World models", "reason": "This paper introduced the concept of 'world models', a fundamental idea for embodied AI.  WorldSimBench builds upon the ideas of World Models, and the paper is therefore essential context for understanding the theoretical foundation upon which the benchmark is built.  It provides insights into the broader context of embodied AI and the challenges associated with building and evaluating such models, which is crucial for understanding the novelty and significance of WorldSimBench.", "section_number": 4}, {" publication_date": "2022", "fullname_first_author": "Michael Janner", "paper_title": "Planning with diffusion for flexible behavior synthesis", "reason": "This paper proposes a novel planning approach that uses diffusion models for generating flexible behaviors, which is highly relevant to the context of evaluating World Simulators. The ability to generate flexible and adaptive behaviors is a key characteristic of World Simulators, and this paper\u2019s insights are valuable for assessing the models\u2019 capacity to plan and execute dynamic actions.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Bolin Lai", "paper_title": "Lego: Learning egocentric action frame generation via visual instruction tuning", "reason": "This paper presents a method for generating egocentric action frames using visual instruction tuning, which is highly relevant to the evaluation of World Simulators, especially in the context of Open-Ended Embodied Environments. The use of visual instruction tuning allows for a more realistic and nuanced assessment of the models' ability to generate videos that accurately reflect the input instructions and the environment.", "section_number": 4}]}