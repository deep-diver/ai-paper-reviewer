{"references": [{" publication_date": "2023", "fullname_first_author": "Josh Achiam", "paper_title": "GPT-4 technical report", "reason": "This paper is foundational for the current research in Large Language Models (LLMs), providing the technical details and capabilities of GPT-4, a model that is referenced multiple times in the paper's methodology for both prompting and comparison. Its comprehensive nature, particularly on aspects of model capabilities and limitations, makes it a cornerstone for evaluating and contextualizing the current state of AI.", "section_number": 5}, {" publication_date": "2021", "fullname_first_author": "Max Bain", "paper_title": "Frozen in time: A joint video and image encoder for end-to-end retrieval", "reason": "This paper is cited within the context of video generation models and datasets. Its focus on video and image encoding is crucial to the research since it directly impacts the evaluation of video generation models, influencing data preparation, feature extraction, and the assessment of visual fidelity and content alignment, all of which are important parameters within the study.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Bowen Baker", "paper_title": "Video pretraining (vpt): Learning to act by watching unlabeled online videos", "reason": "The paper introduces Video PreTraining (VPT), a significant advancement in video-based learning and action generation.  It directly relates to the study's focus on embodied agents and video-to-action models; specifically, it provides a valuable base for understanding how unlabeled video data can be used to train models that can effectively translate visual information into actions, improving the evaluation and understanding of the World Simulators.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Kevin Black", "paper_title": "Zero-shot robotic manipulation with pretrained image-editing diffusion models", "reason": "This paper explores zero-shot robotic manipulation using pre-trained diffusion models, aligning directly with the research's investigation into embodied AI. Its focus on zero-shot learning is important as it relates to the generalization capabilities of World Simulators across different tasks and environments, a key aspect the evaluation framework seeks to assess.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Tim Brooks", "paper_title": "InstructPix2Pix: Learning to follow image editing instructions", "reason": "This paper is highly relevant because it focuses on image editing following instructions, which is a key component in evaluating the visual fidelity and alignment of generated videos to the given instructions.  The methods and metrics used for assessing the quality and compliance of image editing could be applied and extended in the proposed evaluation framework for assessing video generation models.", "section_number": 4}, {" publication_date": "2020", "fullname_first_author": "Holger Caesar", "paper_title": "nuScenes: A multimodal dataset for autonomous driving", "reason": "This paper introduces the nuScenes dataset, a widely used and valuable resource for autonomous driving research, specifically referenced within the paper's methodology for data collection and model evaluation.  The dataset's richness in multimodal data (including images, lidar, radar) provides a solid foundation for training and validating the autonomous driving component of World Simulators, enabling a more comprehensive and accurate assessment.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Yi Chen", "paper_title": "EgoPlan-Bench: Benchmarking egocentric embodied planning with multimodal large language models", "reason": "This paper focuses on benchmarking egocentric planning with multimodal LLMs and provides valuable insights into how to assess embodied AI systems.  This is highly relevant to the current study as it provides guidance and context for designing the evaluation of the planning and decision-making capabilities of the World Simulators, providing benchmarks and metrics which are directly applicable.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Zeren Chen", "paper_title": "RH20T-P: A primitive-level robotic dataset towards composable generalization agents", "reason": "This paper provides a robotic dataset with fine-grained annotations, useful for training and evaluating robotic manipulation capabilities in World Simulators. The detailed descriptions of actions and the focus on composable generalization align directly with the goals of the current study, offering valuable insights for data preparation, model training, and evaluation criteria in the Robot Manipulation scenario.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Wei-Lin Chiang", "paper_title": "Vicuna: An open-source chatbot impressing GPT-4 with 90%* chatGPT quality", "reason": "This paper is referenced for its discussion of large language models (LLMs) and their ability to generate human-quality text. This relates to the current study as it provides context for the use of LLMs in instruction generation and evaluation, a crucial component of the proposed evaluation framework. The focus on text quality and the comparison with GPT-4 enhances the relevance to the research context.", "section_number": 4}, {" publication_date": "2017", "fullname_first_author": "Alexey Dosovitskiy", "paper_title": "CARLA: An open urban driving simulator", "reason": "This paper introduces the CARLA simulator, a critical tool for the autonomous driving component of the evaluation framework.  CARLA's open-source nature, realistic environments, and diverse functionalities make it essential for creating a robust and comprehensive evaluation. The paper's detailed description of the simulator's capabilities provides a foundation for setting up the evaluation experiments.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Danny Driess", "paper_title": "Palm-e: An embodied multimodal language model", "reason": "This paper discusses embodied multimodal language models, which are highly relevant to the study's focus on World Simulators. The paper's findings on embodied agents and their capabilities provide a valuable context for evaluating the performance of World Simulators in terms of their ability to integrate multimodal information and generate meaningful actions within dynamic environments.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Yilun Du", "paper_title": "Video language planning", "reason": "This paper directly addresses video generation for planning, a core functionality of World Simulators.  Its exploration of how video and language can be integrated for planning is essential to this research, offering insights into the design and evaluation of World Simulators.  Understanding how video can be used for high-level planning directly informs the development of both the evaluation framework and the metrics used in the study.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Yilun Du", "paper_title": "Learning universal policies via text-guided video generation", "reason": "This paper is significant for its investigation into text-guided video generation for learning universal policies.  This is directly relevant to the research because it provides insights into the effectiveness of text as a modality for controlling and evaluating video generation models, which are central to the concept of World Simulators and their evaluation.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Frederik Ebert", "paper_title": "Bridge data: Boosting generalization of robotic skills with cross-domain datasets", "reason": "This paper is relevant because it focuses on improving the generalization abilities of robotic skills through cross-domain datasets.  This relates to the goal of evaluating World Simulators, which aim for broader applicability across various scenarios. The techniques for improving generalization and the insights gained from using cross-domain datasets are valuable for the current research.", "section_number": 2}, {" publication_date": "2017", "fullname_first_author": "Raghav Goyal", "paper_title": "The \u201csomething something\u201d video database for learning and evaluating visual common sense", "reason": "This paper introduces a large-scale video dataset, \"Something-Something,\" specifically designed for evaluating visual common sense. This is highly relevant to the current research because the understanding of visual common sense is essential for evaluating the accuracy and realism of videos generated by World Simulators. The dataset and its associated research offer a valuable benchmark and point of comparison for assessing the visual quality and understanding of physical rules inherent in the generated videos.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Kristen Grauman", "paper_title": "Ego4D: Around the world in 3,000 hours of egocentric video", "reason": "This paper introduces the Ego4D dataset, a vast collection of egocentric videos that can serve as a valuable resource for training and evaluating embodied AI models, specifically within the context of World Simulators. The large-scale and diverse nature of the dataset, coupled with its egocentric perspective, directly informs the development of the evaluation framework, providing a rich source of data for training and testing the models and ensuring a diverse range of evaluations.", "section_number": 4}, {" publication_date": "2019", "fullname_first_author": "William H Guss", "paper_title": "MineRL: A large-scale dataset of minecraft demonstrations", "reason": "This paper introduces MineRL, a large-scale dataset of Minecraft demonstrations that is directly used within the evaluation methodology for the Open-Ended Embodied Environment scenario.  MineRL's scale and the diversity of actions within the dataset are crucial for training and evaluating models that can successfully execute complex tasks within the Minecraft environment.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Yuwei Guo", "paper_title": "AnimateDiff: Animate your personalized text-to-image diffusion models without specific tuning", "reason": "This paper introduces AnimateDiff, a method for animating images via text prompts.  While not directly about video generation, its relevance lies in the potential for adapting its techniques to enhance the video generation capabilities of World Simulators.  The study of how text prompts can drive image animation directly relates to the task of using text instructions to guide video generation, a central element of this research.", "section_number": 4}, {" publication_date": "2022", "fullname_first_author": "Michael Janner", "paper_title": "Planning with diffusion for flexible behavior synthesis", "reason": "This paper is highly relevant as it explores the use of diffusion models for planning, a key aspect of World Simulators.  The focus on flexible behavior synthesis directly relates to the ability of World Simulators to generate diverse and adaptable actions, making this paper crucial for contextualizing the evaluation criteria and developing appropriate metrics for assessing the planning and control capabilities of these advanced models.", "section_number": 2}, {" publication_date": "2019", "fullname_first_author": "Alec Radford", "paper_title": "Language models are unsupervised multitask learners", "reason": "This foundational paper establishes the capabilities of Language Models (LMs) as unsupervised multitask learners, which is essential context for this study.  The research leverages LLMs for various tasks, including prompt generation and human-preference evaluation.  Understanding the foundational capabilities and limitations of LMs from this seminal paper is critical for understanding and interpreting results of using LMs in the current research.", "section_number": 2}]}