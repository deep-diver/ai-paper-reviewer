[{"figure_path": "2410.18072/figures/figures_2_0.png", "caption": "Figure 1: Overview of the hierarchical capabilities of the Predictive Models. Models at higher stages demonstrate more advanced capabilities. We take the initial step in evaluating Predictive Generative Models up to the S3 stage, known as World Simulators, by introducing a parallel evaluation framework, WorldSimBench. WorldSimBench assesses the models both Explicit Perceptual Evaluation and Implicit Manipulative Evaluation, focusing on video generation and action transformation across three critical embodied scenarios.", "description": "Figure 1 shows a hierarchical system categorizing predictive models by their capabilities and level of embodiment, and introduces WorldSimBench, a dual evaluation framework for assessing World Simulators.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.18072/figures/figures_5_0.png", "caption": "Figure 2: Overview of Explicit Perceptual Evaluation. (Top) Instruction Prompt Generation. We use a large collection of video captions from the internet and our predefined embodied evaluation dimensions. These are expanded using GPT and manually verified to create a corresponding Task Instruction Prompt List for data generation and evaluation. (Bottom) HF-Embodied Dataset Generation. Massive internet-sourced embodied videos with captions are used to train data generation models. Fine-grained Human Feedback Annotation is then applied to the embodied videos according to the corresponding Task Instruction Prompt List, covering multiple embodied dimensions.", "description": "The figure illustrates the process of Explicit Perceptual Evaluation, including instruction prompt generation and HF-Embodied dataset creation with fine-grained human feedback annotation.", "section": "4.1 EXPLICIT PERCEPTUAL EVALUATION"}, {"figure_path": "2410.18072/figures/figures_7_0.png", "caption": "Figure 3: Overview of Implicit Manipulative Evaluation. Embodied tasks in different scenarios are decomposed into executable sub-tasks. The video generation model generates corresponding predicted videos based on the current instructions and real-time observations. Using a pre-trained IDM or a goal-based policy, the agent executes the generated sequence of actions. After a fixed timestep, the predicted video is refreshed by sampling again from the video generation model, and this process repeats. Finally, the success rates of various embodied tasks are obtained through monitors in the simulation environment.", "description": "The figure illustrates the Implicit Manipulative Evaluation process, showing how embodied tasks are broken down into sub-tasks, video generation models predict videos, and video-to-action mappings allow agents to execute actions in simulation environments.", "section": "4.2 IMPLICIT MANIPULATIVE EVALUATION"}, {"figure_path": "2410.18072/figures/figures_22_0.png", "caption": "Figure 7: Rollout of Open-Ended Embodied Environment in Implicit Manipulative Evaluation.", "description": "The figure shows a sequence of images from the Minecraft environment illustrating the agent's actions in response to the instruction to collect wood.", "section": "4.2 Implicit Manipulative Evaluation-OE"}, {"figure_path": "2410.18072/figures/figures_24_0.png", "caption": "Figure 8: Rollout of Autonomous Driving in Implicit Manipulative Evaluation.", "description": "The figure shows a sequence of frames from the Autonomous Driving simulation environment, illustrating the execution of a driving task guided by a video generation model.", "section": "4.2 Implicit Manipulative Evaluation"}, {"figure_path": "2410.18072/figures/figures_26_0.png", "caption": "Figure 9: Rollout of Robot Manipulation in Implicit Manipulative Evaluation.", "description": "The figure shows a sequence of images depicting the steps involved in a robot manipulation task, illustrating the process of translating video predictions into executable actions.", "section": "F Implicit Manipulative Evaluation-RM"}]