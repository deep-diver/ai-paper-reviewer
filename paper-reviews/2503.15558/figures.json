[{"figure_path": "https://arxiv.org/html/2503.15558/x1.png", "caption": "Figure 1: An overview of Cosmos-Reason1. Cosmos-Reason1 contains two multimodal large language models of 8B and 56B, trained in four stages, including vision pre-training, general SFT, Physical AI SFT, and Physical AI RL. We also define two ontologies for physical common sense and embodied reasoning, and build three benchmarks to evaluate models\u2019 Physical AI reasoning capabilities.", "description": "Cosmos-Reason1 is composed of two large multimodal language models (8B and 56B parameters).  These models are trained in four stages: 1) vision pre-training, 2) general supervised fine-tuning (SFT), 3) Physical AI SFT, and 4) Physical AI reinforcement learning (RL).  The training process leverages two ontologies: one for physical common sense and one for embodied reasoning.  Three benchmarks are used to evaluate the model's Physical AI reasoning abilities.", "section": "3. Cosmos-Reason1"}, {"figure_path": "https://arxiv.org/html/2503.15558/x2.png", "caption": "Figure 2: A pie chart showing our physical common sense ontology. The ontology has three categories (Space, Time, and Fundamental Physics) and 16 fine-grained subcategories.", "description": "This pie chart illustrates the hierarchical ontology developed for representing physical common sense within the Cosmos-Reason1 model.  The ontology is structured into three main categories: Space, Time, and Fundamental Physics. Each of these high-level categories is further broken down into more specific subcategories (16 in total) which capture detailed aspects of physical understanding. For instance, the \"Space\" category includes subcategories such as \"Relationship\" (describing spatial relationships between objects), \"Plausibility\" (assessing the feasibility of spatial arrangements), \"Affordance\" (considering how objects can be used or interacted with), and \"Environment\" (understanding the context of the surrounding area). Similarly, \"Time\" is subdivided into concepts such as \"Actions\", \"Order\", \"Causality\", and \"Planning\", representing various temporal relationships and reasoning capabilities. The \"Fundamental Physics\" category covers essential physical principles, including aspects of mechanics, electromagnetism, and thermodynamics, along with the crucial concept of \"Object Permanence.\"", "section": "2. Physical AI Reasoning"}, {"figure_path": "https://arxiv.org/html/2503.15558/x3.png", "caption": "Figure 3: An illustration of our multimodal large language model architecture. Given an input video and an input text prompt, the video is projected into the LLM\u2019s token embedding space as video tokens by a vision encoder followed by a projector. The text tokens are concatenated with the video tokens and fed into the LLM backbone, a hybrid Mamba-MLP-Transformer architecture. Our model can output responses with long chain-of-thought reasoning processes.", "description": "The figure illustrates the architecture of the Cosmos-Reason1 multimodal large language model.  The process begins with an input video and a text prompt. The video is first processed by a vision encoder, which extracts relevant visual features. These features are then projected into the LLM's token embedding space using a projector, converting them into a format compatible with the text tokens from the prompt. The video tokens and text tokens are concatenated and fed into the core of the model: a hybrid Mamba-MLP-Transformer architecture. This architecture is designed for efficient and effective processing of long sequences of tokens, enabling the model to engage in long chain-of-thought reasoning processes to generate its output. The final output is a natural language response that incorporates the information from both the video and the text prompt.", "section": "3. Cosmos-Reason1"}, {"figure_path": "https://arxiv.org/html/2503.15558/x4.png", "caption": "Figure 4: An illustration of our hybrid Mamba-MLP-Transformer backbone architecture. The middle sub-figure represents the 8B LLM backbone, and the bottom sub-figure depicts the 56B LLM backbone. A Transformer block consists of a self-attention layer and an MLP layer. We also show an example of Alternating Mamba-MLP module on top of the figure.", "description": "This figure details the architecture of the Cosmos-Reason1 models' backbone, a hybrid Mamba-MLP-Transformer.  The diagram shows how Transformer blocks (composed of self-attention and MLP layers) are combined with alternating Mamba-MLP modules for efficiency.  The middle section illustrates the 8B parameter model, while the bottom shows the architecture of the larger 56B parameter model.  The Mamba modules are designed to improve the efficiency of processing long sequences.", "section": "3. Cosmos-Reason1"}, {"figure_path": "https://arxiv.org/html/2503.15558/extracted/6291482/images/4_data/common_sense.jpg", "caption": "(a)", "description": "This figure shows an overview of the Cosmos-Reason1 model architecture.  The input is a video, processed through a vision encoder and a projector to align with the text embeddings of a pre-trained large language model (LLM). The model is trained in four stages: vision pre-training, general supervised fine-tuning (SFT), Physical AI SFT, and Physical AI reinforcement learning (RL).  Two ontologies are used to represent physical common sense and embodied reasoning, and the model is evaluated on three benchmarks.", "section": "3. Cosmos-Reason1"}, {"figure_path": "https://arxiv.org/html/2503.15558/extracted/6291482/images/4_data/bridgev2.jpg", "caption": "(b)", "description": "This figure is a pie chart illustrating the hierarchical ontology for physical common sense used in the Cosmos-Reason1 model.  The ontology categorizes fundamental knowledge about the physical world into three main categories: Space, Time, and Fundamental Physics. Each of these categories is further subdivided into more specific subcategories, representing fine-grained aspects of physical understanding. For example, \"Space\" includes subcategories like \"Relationship,\" \"Plausibility,\" and \"Affordance,\" reflecting different aspects of spatial reasoning. Similarly, \"Time\" encompasses subcategories such as \"Actions,\" \"Order,\" and \"Causality,\" while \"Fundamental Physics\" incorporates concepts like \"Mechanics,\" \"Electromagnetism,\" and \"Thermodynamics.\"", "section": "2.1. Common Sense Reasoning"}, {"figure_path": "https://arxiv.org/html/2503.15558/extracted/6291482/images/4_data/robovqa.jpg", "caption": "(c)", "description": "This figure shows example video frames from the Physical AI Supervised Fine-Tuning datasets. The images depict various scenarios involving different agents (humans, robots) performing actions in real-world environments.  These examples illustrate the diversity and complexity of the data used to train the Cosmos-Reason1 models on physical common sense and embodied reasoning.", "section": "4.3 Physical AI Supervised Fine-Tuning"}, {"figure_path": "https://arxiv.org/html/2503.15558/extracted/6291482/images/4_data/agibot.jpg", "caption": "(d)", "description": "This figure shows an example of video frames from the Physical AI Supervised Fine-tuning datasets. The images depict various scenarios involving different embodied agents (such as robots and humans) performing different tasks in real-world environments. These videos are used in the Physical AI SFT stage of training the Cosmos-Reason1 model to help it learn physical common sense and develop embodied reasoning capabilities.", "section": "4.3 Physical AI Supervised Fine-Tuning"}, {"figure_path": "https://arxiv.org/html/2503.15558/extracted/6291482/images/4_data/holoassist.jpg", "caption": "(e)", "description": "This figure shows an example of video frames from the Physical AI Supervised Fine-tuning datasets.  It highlights the diversity of scenarios and tasks included in the dataset, encompassing various activities and agent types relevant to physical AI, such as robotic manipulation and autonomous driving.  The images represent a small subset of the larger dataset used to train the models.", "section": "4.3 Physical AI Supervised Fine-Tuning"}, {"figure_path": "https://arxiv.org/html/2503.15558/extracted/6291482/images/4_data/av.jpg", "caption": "(f)", "description": "This figure shows an example of video frames from the Physical AI Supervised Fine-Tuning datasets.  These datasets were used to enhance the model's ability to understand Physical AI-specific tasks, focusing on embodied reasoning capabilities such as understanding the spatial relationships between objects, the temporal order of events, and object permanence.  The images illustrate the visual diversity captured within the data, showcasing a variety of actions and contexts that the model must reason about to successfully complete Physical AI tasks.", "section": "4.3. Physical AI Supervised Fine-Tuning"}, {"figure_path": "https://arxiv.org/html/2503.15558/x5.png", "caption": "Figure 5: Example of video frames from our Physical AI supervised fine-tuning datasets.", "description": "Figure 5 presents example video frames from the Physical AI Supervised Fine-Tuning stage of the Cosmos-Reason1 model training.  These diverse video clips showcase various scenarios used to train the model in understanding and reasoning about physical phenomena. The visual examples highlight the real-world contexts and diverse physical actions the model is trained on, demonstrating the wide range of physical situations represented in the dataset.", "section": "4. Data"}, {"figure_path": "https://arxiv.org/html/2503.15558/x6.png", "caption": "Figure 6: Embodied reasoning SFT data curation pipeline. We demonstrate an illustrative example for AgiBot, where we (1) extract short horizon segments corresponding to the subtask, (2) caption the extracted clip to obtain state-action context, (3) curate QA pairs for \u201cnext plausible subtask prediction\u201d, (4) prompt R1 with the question and caption to elicit reasoning, (5) clean and rewrite the reasoning trace to obtain valid SFT samples.", "description": "Figure 6 illustrates the data curation pipeline for embodied reasoning within the context of Physical AI.  The example focuses on the AgiBot dataset. The process begins by extracting short video segments that represent individual subtasks from a longer video.  These clips are then annotated with captions that provide detailed context about the state of the scene and the actions taken by the agent.  Next, question-answer pairs are generated, focusing on predicting the most probable next subtask.  The question and caption are provided as input to the DeepSeek-R1 model, which then generates a reasoning trace (a chain of thought) to answer the question. Finally, this reasoning trace is cleaned and rewritten to create high-quality supervised fine-tuning (SFT) samples.", "section": "4.3.2. Embodied Reasoning SFT"}]