[{"figure_path": "https://arxiv.org/html/2412.15084/x1.png", "caption": "Figure 1: AceMath versus leading open-weights and proprietary LLMs on math reasoning benchmarks. Additionally, we report rm@8 accuracy (best of 8) with our reward model AceMath-72B-RM and use the official reported numbers from Qwen2.5-Math.", "description": "This figure compares the performance of AceMath models against other leading open-source and proprietary large language models (LLMs) on various math reasoning benchmarks.  AceMath models consistently outperform the others. The chart displays the accuracy of each model on several benchmark datasets, including those focused on different grade levels and types of math problems.  The results show AceMath's superiority and the benefit of incorporating AceMath's reward model (AceMath-72B-RM) for improved accuracy.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2412.15084/x2.png", "caption": "Figure 2: The proportion of total SFT tokens for math, coding, and other categories.", "description": "This figure shows the distribution of the total number of tokens used in the supervised fine-tuning (SFT) data for training the general SFT model. The total tokens are broken down into three categories: math, coding, and others.  The math category consists of tokens from various math-related datasets, including NuminaMath, OrcaMathWordProblems, MathInstruct, MetaMathQA, and synthetically generated data. The coding category contains tokens from datasets like Magicoder, WizardCoder, GlaiveCodeAssistant, and CodeSFT. The 'others' category encompasses tokens from various general-purpose instruction-following datasets such as ShareGPT, SlimOrca, EvolInstruct, GPTeacher, AlpacaGPT4, and UltraInteract.", "section": "3. Supervised Fine-tuning"}, {"figure_path": "https://arxiv.org/html/2412.15084/x3.png", "caption": "Figure 3: Studies on the impact of using either the base model or the math base model as the backbone on the performance of our AceMath-Instruct models. We compare our models against the corresponding math-instruct baselines across different model types and sizes. Results are the average scores of greedy decoding over the math benchmarks.", "description": "This figure compares the performance of AceMath-Instruct models, which are fine-tuned using both general and math-specific data, against their corresponding math-instruct baselines (models trained only on math data).  It investigates the impact of using either a general-purpose base model or a math-specialized base model as the starting point for fine-tuning. The comparison is done across various model sizes and types, and the results are shown as average greedy decoding scores across several math reasoning benchmarks.  This illustrates whether pre-training a model on a large math corpus before instruction-tuning improves its performance on math-related tasks.", "section": "3.6 Results of AceMath-Instruct"}, {"figure_path": "https://arxiv.org/html/2412.15084/x4.png", "caption": "Figure 4: rm@k\ud835\udc58kitalic_k evaluation on average accuracy of 7 datasets for AceMath-7B-Instruct.", "description": "This figure shows the performance of the AceMath-7B-Instruct model on seven different math datasets, evaluating the model's ability to select the correct answer from a list of k candidates.  The x-axis represents the number of candidates (k) considered, while the y-axis shows the average accuracy across the seven datasets for that k value. The graph visualizes how the model's accuracy changes as the number of considered answers increases.  This allows one to evaluate the model's performance when presented with varying degrees of uncertainty in the answer selection process.", "section": "3.6 Results of AceMath-Instruct"}, {"figure_path": "https://arxiv.org/html/2412.15084/x5.png", "caption": "Figure 5: Learning curves for reward model training. All models are trained from Qwen2.5-Instruct family.", "description": "This figure presents the learning curves for reward model training, illustrating how the model's accuracy improves as training progresses.  The curves are shown for various model sizes, ranging from 0.5B to 32B parameters, across several mathematical reasoning benchmarks (GSM8K, Math500, Minerva Math, Gaokao2023EN, Olympiad Bench, College Math, MMLU STEM). This provides insights into the relationship between model size, training data, and performance on different types of mathematical problems.  The data shows that larger models generally achieve better accuracy, but the rate of improvement varies depending on the complexity of the benchmark.", "section": "4.4.7. Learning curves of reward model training"}]