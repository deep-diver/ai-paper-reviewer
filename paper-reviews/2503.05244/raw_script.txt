[{"Alex": "Hey everyone, and welcome to the podcast! Today, we're diving into the wild world of AI writing! Get ready to hear how the latest AI models are shaking things up...and maybe even outsmarting us writers!", "Jamie": "Oh, this sounds fun! I'm so ready to learn how these AI models are changing writing. Is it really getting to the point where computers can write better than us?"}, {"Alex": "Well, Jamie, it's not quite 'better' yet, but they're definitely getting smarter. We have a fascinating research paper to discuss called 'WritingBench: A Comprehensive Benchmark for Generative Writing.' It\u2019s all about how we measure the writing skills of these AI models.", "Jamie": "A benchmark, got it. So, like a standardized test for AI writers? What exactly does this WritingBench test, and why do we need it?"}, {"Alex": "Exactly! Current methods mostly focus on generic writing or specific tasks, missing the full range of real-world writing needs. WritingBench, on the other hand, evaluates LLMs across six core domains\u2014creative, persuasive, informative, technical\u2014and 100 subdomains.", "Jamie": "Wow, 100 subdomains is detailed. Ummm, could you give me an example of a subdomain or two? Just to get a better picture."}, {"Alex": "Sure. Think 'market analysis' under 'Finance & Business,' or 'screenplay' under 'Literature & Art'. It covers a huge spectrum! And it doesn't just look at grammar, it assesses style, format, and even the ability to incorporate source materials effectively.", "Jamie": "Okay, I see, so it goes beyond just basic writing skills. What makes it better than other benchmarks, and why is the ability to use source material so important?"}, {"Alex": "That's the key! Current benchmarks lack specialized tasks and diverse inputs. WritingBench uses varied materials\u2014from short sentences to thousands of words\u2014mimicking real-world writing tasks. For example, imagine an AI writing a video script incorporating skincare product placement based on past reviews.", "Jamie": "Hmm, so it\u2019s about AI being able to handle the messy reality of writing with actual client briefs and requirements. How does WritingBench actually evaluate if the AI has done a good job with a particular task?"}, {"Alex": "That's where it gets interesting. WritingBench uses a query-dependent evaluation framework. Instead of fixed criteria, it dynamically generates instance-specific assessment criteria using another LLM.", "Jamie": "Wait, so an AI judging another AI? That sounds a little like letting the fox guard the henhouse. How does that work?"}, {"Alex": "It's not as crazy as it sounds! The evaluation LLM generates five specific criteria based on the writing prompt, ensuring the assessment is tailored to the task. Then, a fine-tuned 'critic model' scores the response based on these criteria.", "Jamie": "Okay, so the criteria are AI-generated but specific to the task. What does the 'critic model' add to the process, and how do you know it's any good at judging?"}, {"Alex": "The critic model is trained to score based on the generated criteria. The researchers found that this approach aligns with human judgment 83% of the time, which is significantly better than using static criteria.", "Jamie": "That is impressive! So, human alignment of 83%... It's better than static criteria? That really does sound effective. How does the research team make use of this benchmark?"}, {"Alex": "The WritingBench allows the creation of high-quality data sets for training smaller, more efficient models. When they trained 7B parameter models on WritingBench-filtered data, their writing performance approached state-of-the-art!", "Jamie": "Wow, so this isn't just about evaluation, but also about improving models, and the writing performance goes way up? What can we expect from this framework to do?"}, {"Alex": "Exactly! By filtering samples, high-quality datasets are created, allowing to train small-scale model. And they have open-sourced the benchmark, the evaluation tools and their trained models! The team anticipates more research and advancement in writing.", "Jamie": "That sounds amazing! But is this like, an actual guarantee for good AI writing in the future? Like, what could be improved?"}, {"Alex": "It's not a guarantee, Jamie, but it's a big step forward. The researchers acknowledge some limitations. For instance, the models are primarily trained using conventional methods, so there's room to explore more advanced training strategies.", "Jamie": "Hmm, so there are still training improvements to be made, what else is considered a limitation in this research?"}, {"Alex": "Another limitation is handling complex length requirements, like specific word counts for different sections of a document. And getting perfect agreement from human evaluators on creative tasks is always tricky.", "Jamie": "Yeah, I can see how something like 'creativity' is super subjective, where do you see things going from here?"}, {"Alex": "That's the million-dollar question! I see potential in combining WritingBench with techniques like chain-of-thought prompting, which could further boost the creative capabilities of these models.", "Jamie": "That's an interesting concept, it will be exciting to see how AI models will evolve to be like, novel writing!"}, {"Alex": "It will! I also think future work will focus on improving the evaluation framework to handle more complex requirements and to better capture the nuances of human preferences.", "Jamie": "Okay, that sounds super promising, I am exciting about new AI models from now, what's the key take away from this research?"}, {"Alex": "The take away is that the future of AI writing isn\u2019t just about generating text, but also about evaluating it intelligently and using that evaluation to build better models. What's next?", "Jamie": "What is next?"}, {"Alex": "I think that it will have the potential to be able to create professional documents like contracts, reports, or any legal documents. What you think?", "Jamie": "I think that will be amazing and will save us a lot of time! But what about human feelings on those AI generate writing? Do you think it will be the same as human writing?"}, {"Alex": "I think that those AI-generate writing it won't be the same as human writing, because the models still needs a lot more improvement.", "Jamie": "True! I agree, so, the model can be the AI to write the story, and human improve it. what do you think about that?"}, {"Alex": "I agreed! Human improve the story, human help models to train on high-quality writing. The AI writing can still do with a lot of help.", "Jamie": "Yep, what would be the next step for those AI models do?"}, {"Alex": "I would say, the next step would be to have the AI models learn to recognize the tone/emotion of the user/writing, and try to match that", "Jamie": "Ooo, AI is trying to read human emotions? It is kinda creepy...But if the AI can learn that, it would be really helpful for a lot of professional writers!"}, {"Alex": "Absolutely! And that\u2019s where WritingBench comes in. It\u2019s a comprehensive approach to push the boundaries of AI writing and to ensure that AI writing meets all the requirements! So, thank you all for joining us, see you next time!", "Jamie": "That was super insightful, I will be looking forward to our next conversations, thanks Alex!"}]