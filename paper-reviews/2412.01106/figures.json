[{"figure_path": "https://arxiv.org/html/2412.01106/x2.png", "caption": "Figure 1: Given a one-shot image (e.g., your favorite photo) as input, our method reconstructs a fully expressive whole-body talking avatar that captures personalized details and supports realistic animation, including vivid body gestures and natural expression changes. Project page: https://ustc3dv.github.io/OneShotOneTalk/", "description": "This figure demonstrates the capabilities of the One Shot, One Talk method.  A single input image (like a personal photo) is used to generate a detailed, fully expressive whole-body avatar. The generated avatar is capable of realistic animation, including natural facial expressions and body movements.  This showcases the ability of the system to create personalized avatars from minimal input.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2412.01106/x3.png", "caption": "Figure 2: Overview. Our method constructs an expressive whole-body talking avatar from a single image. We begin by generating pseudo body and head frames using pre-trained generative models, driven by a collected video dataset with diverse poses. Per-pixel supervision on the input image, perceptual supervision on imperfect pseudo labels, and mesh-related constraints are then applied to guide the 3DGS-mesh coupled avatar representation, ensuring realistic and expressive avatar reconstruction and animation.", "description": "This figure illustrates the pipeline for creating a talking avatar from a single image.  It starts by using pre-trained models and a diverse video dataset to generate pseudo-body and head video frames. These frames, along with the input single image, are used to train a 3DGS-mesh coupled avatar.  Three types of supervision guide the training: per-pixel supervision on the input image, perceptual supervision on the imperfect pseudo labels, and mesh-related constraints. The result is a realistic and expressive avatar that can be animated.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2412.01106/x4.png", "caption": "Figure 3: Qualitative comparisons with representative methods\u00a0[65, 36, 18] in the cross-identity motion reenactment task. Our method achieves accurate and realistic animation with almost all fine details preserved and identity unchanged.", "description": "Figure 3 presents a qualitative comparison of the proposed method against three representative methods (MimicMotion [65], ExAvatar [36], and ELICIT [18]) for cross-identity motion reenactment.  The task involves animating a target person with the motions of a different person. The figure shows that the proposed method produces highly accurate and realistic animation, preserving fine details and maintaining the identity of the target person, unlike the other methods which show various degrees of artifacts, such as blurry textures, identity mismatches, or missing details.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.01106/x5.png", "caption": "Figure 4: Qualitative comparisons with representative methods\u00a0[65, 36, 18, 20] in the self-driven motion reenactment task. Our method well models facial and hand regions, which match the input image most in global identity preservation and local details modeling, even compared with some methods trained on captured videos.", "description": "Figure 4 presents a qualitative comparison of different avatar generation methods on a self-driven motion reenactment task.  The goal was to recreate the subject's movements and expressions using only the input image and a motion sequence. The methods compared are MimicMotion [65], ExAvatar [36], ELICIT [18], and Make-Your-Anchor [20]. The figure visually demonstrates that the proposed method generates a more realistic and detailed avatar, especially in facial and hand areas, even surpassing some models trained on full video data which have more information to work with.  The global identity is better preserved, and finer details are more accurately represented.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.01106/x6.png", "caption": "Figure 5: More examples of cross-identity pose reenactment. Different subjects can be accurately animated with the same poses.", "description": "This figure showcases additional examples of cross-identity pose reenactment.  It demonstrates the model's ability to accurately animate diverse subjects using the same set of poses. The consistent animation quality across different individuals highlights the generalization capabilities of the proposed approach.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.01106/x7.png", "caption": "Figure 6: Perceptual diffusion guidance is of great importance to inpainting unseen regions and modeling natural and realistic textures.", "description": "Figure 6 demonstrates the importance of perceptual guidance in enhancing the quality of pseudo-labels generated by diffusion models.  The figure compares the results of using perceptual guidance with those using only L1 and SSIM losses, and also with no guidance at all.  Perceptual guidance significantly improves the generation of realistic textures, particularly in regions where data is missing from the input image, resulting in more natural and accurate facial and hand details.", "section": "3.3 Objective Functions"}, {"figure_path": "https://arxiv.org/html/2412.01106/x8.png", "caption": "Figure 7: Soft mesh constraints together with Gaussian Laplacian help preserve geometric integrity and model fine details.", "description": "Figure 7 presents an ablation study demonstrating the importance of mesh-related constraints and Gaussian Laplacian regularization in preserving geometric integrity and detailing for the generated 3D human avatar.  The figure visually compares the quality of avatar generation under different conditions: with all constraints and regularization applied (full model), without mesh-specific constraints (w/o mesh SC), and without both mesh constraints and Laplacian smoothing (w/o mesh SC & Lap).  The results highlight how the combination of these techniques leads to more accurate, detailed, and realistic results in the final avatar, particularly in challenging areas such as hands and facial features.", "section": "3.3 Objective Functions"}, {"figure_path": "https://arxiv.org/html/2412.01106/x10.png", "caption": "Figure 8: Re-Tracking step preserves better texture structures and avoid texture loss.", "description": "This figure shows an ablation study on the impact of the re-tracking step in the proposed pipeline.  The re-tracking step refines the pose parameters obtained from the initial pseudo-labels generated by a motion diffusion model. By comparing the results with and without this re-tracking step, the figure visually demonstrates that re-tracking preserves better texture structures and helps prevent texture loss in the generated video frames. The comparison highlights the improved quality and accuracy of the final avatar animation achieved through this refinement step.", "section": "3.3 Objective Functions"}, {"figure_path": "https://arxiv.org/html/2412.01106/x11.png", "caption": "Figure 9: A detailed illustration of our pipeline.", "description": "Figure 9 illustrates the detailed steps of the proposed pipeline for generating a whole-body talking avatar from a single image.  It begins with the training stage where a single input image is used along with motion data from a large-scale dataset.  Generative models are used to produce pseudo-frames, and these frames are used with the single image for training a 3DGS-mesh coupled avatar representation.  The inference stage shows how, given new inference poses, this avatar is animated, leveraging perceptual supervision and mesh-related constraints to ensure realism and expressiveness.", "section": "3. Method"}]