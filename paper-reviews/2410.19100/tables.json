[{"figure_path": "2410.19100/tables/table_4_0.html", "caption": "Table 1: Video statistics for VideoWebArena.", "description": "Table 1 presents video statistics for the VideoWebArena benchmark, including the number of videos, total duration, and average number of tasks per video.", "section": "3 VIDEOWEBARENA ENVIRONMENT"}, {"figure_path": "2410.19100/tables/table_4_1.html", "caption": "Table 1: Video statistics for VideoWebArena.", "description": "The table presents video statistics for the VideoWebArena benchmark, including the number of videos per domain, total video duration, average video duration, and the average number of factual and skill retention tasks per video.", "section": "3 VIDEOWEBARENA ENVIRONMENT"}, {"figure_path": "2410.19100/tables/table_5_0.html", "caption": "Table 1: Video statistics for VideoWebArena.", "description": "Table 1 presents the video statistics for the VideoWebArena benchmark, including the number of videos, total duration, and average duration for each domain.", "section": "3 VIDEOWEBARENA ENVIRONMENT"}, {"figure_path": "2410.19100/tables/table_6_0.html", "caption": "Table 4: Examples of Each Task in the VideoWebArena Taxonomy: Given a video tutorial, the agent is asked to perform the intent. The intermediate intent tests the multimodal agent, s ability to extract the necessary information to perform the task from the video. Skill retention tasks do not have intermediate intents as they do not require recalling specific information that factual retention tasks will require.", "description": "This table provides examples of tasks in the VideoWebArena benchmark, illustrating the intent of each task and, for factual retention tasks, the intermediate intent required to complete the task.", "section": "3.5 VIDEO CREATION AND SKILL RETENTION TASKS"}, {"figure_path": "2410.19100/tables/table_9_0.html", "caption": "Table 5: Results on VideoWebArena Factual Retention Tasks. Performance of GPT4-0, Gemini 1.5 Pro, and human performance on 400 factual retention tasks broken down by task domain. Final scores indicate the overall task performance (i.e., if the task is completed successfully in its entirety), while intermediate scores measure the performance on the intermediate intents.", "description": "Table 5 presents the performance of different models on factual retention tasks in VideoWebArena, showing final task success rates, intermediate intent success rates, and the average number of steps taken.", "section": "5 RESULTS"}, {"figure_path": "2410.19100/tables/table_9_1.html", "caption": "Table 6: Results on VideoWebArena Skill Retention Tasks. Overall performance comparison of GPT4-0 and human performance on skill retention tasks. Human performance shows tutorials should help task performance success and efficiency. However, adding tutorials in-context to the model does not necessarily help, but in fact hurts performance by a significant margin. See the failure modes in Appendix B for more analysis. Dashes (-) indicate that data is unavailable for that particular metric.", "description": "Table 6 presents a comparison of the overall performance of GPT-40 and human participants on skill retention tasks within the VideoWebArena benchmark, highlighting the impact of providing tutorials in-context.", "section": "5 RESULTS"}, {"figure_path": "2410.19100/tables/table_10_0.html", "caption": "Table 7: Factual Retention Results Breakdown: Overall performance breakdown of the baseline agents across all task categories and difficulties in the factual retention set. The summary agent has the best task performance, even without having any visual aspect of the video in context. However, it lags behind in the intermediate VQA intents, as the video frame and video agents all perform very similarly on intermediate tasks.", "description": "Table 7 presents a breakdown of the performance of different baseline agents across various task categories and difficulty levels within the factual retention task set of the VideoWebArena benchmark.", "section": "5 Results"}, {"figure_path": "2410.19100/tables/table_15_0.html", "caption": "Table 8: List of VideoWebArena evaluator functions and descriptions: All rewards are binary. We adopt our evaluators from WebArena (Zhou et al., 2024) and VisualWebArena (Koh et al., 2024a).", "description": "This table lists the reward functions used in VideoWebArena, specifying their names, inputs, and conditions for returning a reward of 1.", "section": "3.4 Task Design"}, {"figure_path": "2410.19100/tables/table_16_0.html", "caption": "Table 9: Model Comparison - Average Steps per Task Type", "description": "The table presents a comparison of the average number of steps taken by different models across various task types in the VideoWebArena benchmark.", "section": "5.1 MODEL PERFORMANCE"}]