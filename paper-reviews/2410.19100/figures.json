[{"figure_path": "2410.19100/figures/figures_2_0.png", "caption": "Figure 1: Overview of VideoWebArena. VideoWebArena is a visually grounded benchmark that tests the video understanding of agentic models across various realistic domains and environments, mirroring real-life tasks. All tasks require video input and consist of Q/A to test agentic abilities in video information retrieval, video understanding, and more.", "description": "Figure 1 is an overview of the VideoWebArena benchmark, showing its various domains, video-based tasks, and automatic evaluation process.", "section": "VideoWebArena Environment"}, {"figure_path": "2410.19100/figures/figures_8_0.png", "caption": "Figure 4: VideoWebArena Baseline Agent Framework: We use 3 baseline agents: 1.) Video Summary Agent, where the video summary is fed in-context. 2.) Video Frame Agent, where a set number of frames and audio transcription is fed in-context. 3.) Video Agent, where the video is fed in as an .mov file in-context. The video information is put in-context along with the Set-of-Marks state representation to generate a singular action, following the multimodal SoM agent in VisualWebArena (Koh et al., 2024a).", "description": "This figure illustrates the framework used to evaluate three different baseline agents in the VideoWebArena benchmark, showcasing how video data (summary, frames, or full video) is processed with webpage information to generate actions.", "section": "4 BASELINE AGENTS"}, {"figure_path": "2410.19100/figures/figures_13_0.png", "caption": "Figure 5: Dataset Creation Process A walkthrough of the VideoWebArena dataset creation. From 1641 existing tasks in WebArena and VisualWebArena, the authors grouped these tasks by their intent templates. For each intent template, the authors created a new video tutorial showing how to perform the tasks. For each video, the authors made at minimum 4 factual retention tasks. This led to 1641 skill retention and 400 factual retention tasks.", "description": "The figure illustrates the process of creating the VideoWebArena dataset, starting from existing WebArena and VisualWebArena tasks and creating video tutorials to generate new video-based and skill retention tasks.", "section": "3 VIDEOWEBARENA ENVIRONMENT"}, {"figure_path": "2410.19100/figures/figures_14_0.png", "caption": "Figure 6: VideoWebArena Task Taxonomy We define a taxonomy for all the tasks in our benchmark, namely splitting them into a factual and skill retention groups. Under the factual retention group, there are 4 types of tasks: Visual Perception, Audio Perception, Full Video Understanding, and Temporal Reasoning.", "description": "The figure shows a taxonomy of video-based agent tasks, categorized into skill retention and factual retention, with further sub-categories for factual retention.", "section": "3 VIDEOWEBARENA ENVIRONMENT"}, {"figure_path": "2410.19100/figures/figures_16_0.png", "caption": "Figure 4: VideoWebArena Baseline Agent Framework: We use 3 baseline agents: 1.) Video Summary Agent, where the video summary is fed in-context. 2.) Video Frame Agent, where a set number of frames and audio transcription is fed in-context. 3.) Video Agent, where the video is fed in as an .mov file in-context. The video information is put in-context along with the Set-of-Marks state representation to generate a singular action, following the multimodal SoM agent in VisualWebArena (Koh et al., 2024a).", "description": "The figure illustrates the framework for three baseline agents used to evaluate the VideoWebArena benchmark, showing how video data (summary, frames, or full video) and website information are processed to generate actions.", "section": "4 BASELINE AGENTS"}]