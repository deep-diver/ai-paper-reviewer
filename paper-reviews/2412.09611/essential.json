{"importance": "**Disentangled image editing** is crucial for responsible image manipulation. This work offers a **scalable method** within the **powerful flow transformer architecture**, allowing precise edits without retraining. It advances research in **interpretable generative models**, impacting areas like **content creation** & **digital media authenticity**, while raising important ethical considerations.", "summary": "Edit images precisely with AI, no masks needed!", "takeaways": ["FluxSpace offers domain-agnostic, training-free image editing within flow transformers.", "Edits are disentangled, preserving unrelated aspects of the image.", "FluxSpace supports both fine-grained (e.g., smile) and coarse (e.g., style) edits.", "Linearly adjustable editing with different controlling mechanism is introduced within the flow-based generation models"], "tldr": "**High-quality image generation** is key in AI, but **precisely editing** these images remains a challenge.  Existing methods struggle to isolate specific features, leading to **unwanted changes** in other image areas. This makes it hard to do things like changing a person's smile without affecting their other facial features, or changing an image's style without altering the content. **Rectified flow transformers**, like Flux, produce high-quality images, but existing editing techniques often don't work well with them. This makes precise editing difficult within this powerful class of generative models. Existing works can also require long training hours for edit-per-domain, making it not suitable for editing without retraining.  Moreover, many of the state-of-the-art editing methods require manual mask inputs from users to identify editable parts of an image, which may not always be convenient and accurate. This work aims to propose a method that enables image editing without manual mask inputs. Existing methods also mainly focus on the editing capabilities within diffusion-based image generation models, where the rectified flow-based image generation is relatively unexplored. This work aims to bridge this gap and enables a method for editing images with high-fidelity generated with rectified flow models.\nThis paper introduces **FluxSpace**, a new method for editing images generated by flow transformers. It leverages the **attention layers** within these models to offer **disentangled control** over image features. Unlike previous methods, **FluxSpace doesn't require additional training** and applies edits directly at inference time. It also allows both **fine-grained adjustments** (e.g., adding eyeglasses) and **coarse edits** (e.g. stylization) without requiring mask inputs, opening new possibilities for creative image editing and better disentanglement within rectified flow models. Quantitative and qualitative experiments demonstrate that FluxSpace preserves image identity better than state-of-the-art methods while achieving precise edits. The results also show that FluxSpace is both able to edit real and generated images, increasing its versatility in different tasks. The method allows a more flexible image editing method by introducing linear edit scale control within flow-based generative models, opening new directions for future research.", "affiliation": "Virginia Tech", "categories": {"main_category": "Computer Vision", "sub_category": "Image Generation"}, "podcast_path": "2412.09611/podcast.wav"}