[{"figure_path": "https://arxiv.org/html/2503.18923/x2.png", "caption": "Figure 1: (a) The taxonomy of Video SimpleQA benchmark; (b) Illustrations of existing knowledge-based video benchmarks [21, 84, 30, 85, 28] which may involve hypothetical or subjective reasoning; (c) Illustrations of our Video SimpleQA benchmark with the fact-seeking question and definitive & short-form answer with external-source verified.", "description": "Figure 1 illustrates three key aspects of the Video SimpleQA benchmark.  Panel (a) presents a hierarchical taxonomy showing how different question types are categorized within the benchmark. Panel (b) displays examples from existing video question answering benchmarks (KnowIT-VQA, WorldQA, Video-MMMU, MMVU, and MMWorld), highlighting their reliance on knowledge that may require subjective interpretation or hypothetical reasoning. In contrast, Panel (c) showcases examples from the Video SimpleQA benchmark, emphasizing fact-seeking questions with definitive, short answers that can be verified against external sources. This contrast highlights the key improvements introduced by Video SimpleQA.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2503.18923/x3.png", "caption": "Figure 2: Sampled examples in Video SimpleQA and the responses of typical LVLMs [53, 62, 5].", "description": "This figure showcases several examples from the Video SimpleQA benchmark dataset, highlighting the challenges involved in evaluating the factuality of Large Video Language Models (LVLMs).  Each example shows a video still, a question about the video, the correct answer (ground truth), and the responses generated by three different LVLMs (GPT-40, Gemini-1.5-Pro, and Qwen-2.5-VL-72B).  The responses illustrate the varying levels of accuracy and confidence exhibited by the LVLMs in answering fact-based questions related to videos. The discrepancies between the LVLMs' answers and the ground truth demonstrate the need for a comprehensive benchmark like Video SimpleQA to evaluate the factuality of LVLMs.", "section": "3. Video SimpleQA"}, {"figure_path": "https://arxiv.org/html/2503.18923/x4.png", "caption": "Figure 3: An overview of the construction pipeline of Video SimpleQA including the video & encyclopedia collection (Sec. 3.1), QA annotation (Sec. 3.2), and quality control (Sec. 3.3).", "description": "Figure 3 illustrates the three-stage pipeline for creating the Video SimpleQA dataset.  First, videos are collected from Wikimedia Commons and their associated encyclopedic descriptions are gathered via a Retrieval-Augmented Generation (RAG) process.  Second, a two-stage QA annotation process is undertaken: 1) an iterative LLM-based approach for initial QA pair generation, refined by 2) human-in-the-loop verification.  Finally, a quality control phase incorporates difficulty filtering and human cross-verification to ensure high-quality QA pairs. ", "section": "3. Video SimpleQA"}, {"figure_path": "https://arxiv.org/html/2503.18923/x5.png", "caption": "Figure 4: The encyclopedia collection process including the raw associated description in Wikimedia and the RAG results\u2020 \u2023 3.1 for the specialized terms extracted by GPT-4o.", "description": "This figure illustrates the process of creating a comprehensive encyclopedia for the Video SimpleQA benchmark.  It begins with raw descriptions from Wikimedia Commons. Then, GPT-40 is used with a Retrieval Augmented Generation (RAG) method to extract key terms and find more detailed definitions for them. These additional definitions from search engines like Google and Bing are then added to create a richer and more accurate knowledge base for the dataset.  This ensures that the questions in the benchmark require knowledge beyond what is explicitly shown in the video itself.", "section": "3. Video & Encyclopedia Collection"}, {"figure_path": "https://arxiv.org/html/2503.18923/x6.png", "caption": "(a)", "description": "This figure shows a pie chart that visualizes the taxonomy of the Video SimpleQA benchmark. The chart is divided into four main categories: Nature, Engineering, Science, and Society & Culture.  Each of these categories is further subdivided into secondary and tertiary categories representing various sub-topics. The size of each slice in the pie chart corresponds to the number of videos in that particular category, illustrating the distribution of video content across different topics in the Video SimpleQA dataset.", "section": "3. Video SimpleQA"}, {"figure_path": "https://arxiv.org/html/2503.18923/x7.png", "caption": "(b)", "description": "This figure shows an example of an existing knowledge-based video benchmark.  The example uses a question about a video demonstrating a Euclidean geometry principle and the correct answer, which is \"Pythagorean theorem\". The benchmark includes information about the required knowledge, whether the question is factual, whether the answer is definitive, and whether it was verified by an external source. This illustrates that existing benchmarks may involve hypothetical or subjective reasoning, unlike the proposed Video SimpleQA benchmark.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.18923/x8.png", "caption": "(c)", "description": "This figure illustrates the Video SimpleQA benchmark, which is a novel dataset for evaluating the factuality of Large Video Language Models (LVLMs). It contrasts with existing benchmarks by focusing on fact-seeking questions that necessitate integrating external knowledge beyond the video's explicit narrative.  The answers provided are unambiguous, definitive, and short-form, allowing for automated evaluation.  The questions necessitate both static single-frame understanding and dynamic temporal reasoning, testing LVLMs' ability to handle long-context dependencies.", "section": "3. Video SimpleQA"}, {"figure_path": "https://arxiv.org/html/2503.18923/x9.png", "caption": "Figure 5: (a) Video distribution at the secondary level; (b) Question type distribution; (c) Key statistics of Video SimpleQA .", "description": "Figure 5 presents a comprehensive overview of the Video SimpleQA dataset. Subfigure (a) shows the distribution of videos across 15 secondary categories, illustrating the diversity of video content in the benchmark. Subfigure (b) visualizes the distribution of question types, revealing the balance between different question forms. Subfigure (c) provides key statistics of the dataset, including the total number of question-answer pairs, video lengths, and the number of categories at different levels of granularity.", "section": "3. Video SimpleQA"}, {"figure_path": "https://arxiv.org/html/2503.18923/x10.png", "caption": "Figure 6: Calibration curves based on the self-stated confidence scores and interval-level accuracy; Brier scores to quantify the deviation from the ideal calibration line.", "description": "This figure displays the calibration of several large video language models (LVLMs) by plotting their self-reported confidence scores against their actual accuracy.  The x-axis represents the self-stated confidence levels, categorized into intervals (bins), while the y-axis shows the accuracy within each confidence interval.  A perfectly calibrated model would have a diagonal line representing perfect agreement between confidence and accuracy. Deviations from this ideal diagonal line reveal overconfidence or underconfidence in the model's predictions.  The Brier score, displayed numerically in the figure, quantifies the overall deviation from perfect calibration, providing a single metric for comparing the calibration quality across different LVLMs.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.18923/x11.png", "caption": "Figure 7: Evaluations of test-time compute including Best-of-N and Self-refine.", "description": "This figure displays the results of experiments evaluating the impact of test-time compute strategies on the Video SimpleQA benchmark.  Two strategies were tested: Best-of-N, which involves generating N independent responses and selecting the best one, and Self-refine, which involves iteratively refining initial outputs using self-generated feedback. The graphs show the accuracy (proportion of correct answers) against the number of models used for Best-of-N and the number of refinement iterations for Self-refine, for several different models. The results reveal that neither strategy consistently improves accuracy and that in some cases, performance even degrades.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.18923/x12.png", "caption": "Figure 8: The relationship between model size and F-score.", "description": "This figure illustrates the correlation between the size of large video language models (LVLMs) and their performance on the Video SimpleQA benchmark.  The x-axis represents the model size in billions of parameters, while the y-axis shows the F1-score achieved by each model on the benchmark. The plot visually demonstrates the general trend of improved performance with increasing model size, although the relationship isn't perfectly linear. Some larger models may underperform compared to smaller models, indicating that model size is not the sole determinant of performance.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.18923/x13.png", "caption": "Figure 9: Frame number scaling experiments.", "description": "This figure displays the results of experiments examining the effect of the number of video frames used as input on the performance of various Large Video Language Models (LVLMs).  The x-axis likely represents the number of frames, and the y-axis likely represents a performance metric, such as the F-score.  Each line likely represents a different LVLMs. The plot aims to demonstrate whether increasing the number of frames improves the models' ability to answer factually grounded questions about the video content.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.18923/x14.png", "caption": "Figure 10: Results with different judge models.", "description": "This figure displays the F-scores achieved by different Large Video Language Models (LVLMs) when evaluated using various judge models (GPT-40, Gemini-1.5-Pro, Claude-3.5-SonnetV2, GPT-40-mini).  The consistent ranking of the LVLMs across different judge models highlights the robustness of the evaluation methodology, showing that even smaller judge models can provide consistent results.  The variation in F-scores across models reveals differences in their performance across various subtopics.", "section": "6.2. More Experiments"}, {"figure_path": "https://arxiv.org/html/2503.18923/x15.png", "caption": "Figure 11: The performance of different models across 15 secondary categories in Video SimpleQA .", "description": "This figure visualizes the performance of various large video language models (LVLMs) across 15 secondary categories within the Video SimpleQA benchmark.  It allows for a comparison of model strengths and weaknesses in different domains or topical areas.  Each bar represents the F1-score achieved by a specific model within a particular category, offering insights into the relative performance of various models for different types of factual video understanding tasks.  Higher bars indicate better performance.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.18923/x16.png", "caption": "Figure 12: Visualizations of typical error types including (a) perception error; (b) lack of knowledge; (c) refusal to answer; (d) failure to follow instructions.", "description": "Figure 12 presents four examples illustrating common error types made by Large Video Language Models (LVLMs) when answering questions about video content.  (a) shows a \nperception error, where the model fails to correctly identify an object (a bat) within the video. (b) demonstrates a lack of knowledge error, where the model correctly identifies an object (a beetle) but lacks the contextual information (its origin) to answer the question.  (c) exemplifies a refusal to answer error, in which the model acknowledges its inability to confidently answer the question. Finally, (d) illustrates an error stemming from a failure to follow instructions, where the model correctly identifies an object (a tower) but provides an answer that doesn't directly address the question (it gives the city instead of the country where the tower is located).  These examples highlight the diverse challenges in achieving factual accuracy in video question answering for LVLMs.", "section": "3. Video SimpleQA"}, {"figure_path": "https://arxiv.org/html/2503.18923/x17.png", "caption": "Figure 13: Error type distributions across Qwen2.5-VL-72B [5], GPT-4o [53] and Gemini-1.5-Pro [62].", "description": "This figure presents a comparison of error types across three large video language models (LVLMs): Qwen2.5-VL-72B, GPT-4o, and Gemini-1.5-Pro.  It shows the percentage breakdown of four error categories for each model: Perception Error (incorrect identification of objects in the video), Lack of Knowledge (correct identification, but the model lacks the required factual information to answer accurately), Refusal to Answer (the model correctly recognizes it cannot answer confidently and chooses to abstain), and Failure to Follow Instructions (the model understands the instruction but doesn't follow the format or generates an irrelevant answer). The figure allows for a quantitative comparison of the models' error profiles and strengths/weaknesses in various aspects of video question answering.", "section": "3.4. Dataset Statistics"}, {"figure_path": "https://arxiv.org/html/2503.18923/x18.png", "caption": "Figure 14: Prompt for grading: Part 1", "description": "This figure shows the prompt used for the human evaluators to grade the quality of the LLM generated answers.  The prompt provides examples of answers that are considered CORRECT, INCORRECT, and NOT ATTEMPTED. This helps ensure consistency in evaluation across different annotators.  The examples illustrate various scenarios and edge cases in the responses that impact the final grading decision.  Specifically, it covers cases with partial answers, answers containing additional information,  hedging or uncertainty in the answers, contradictory statements, and responses that are simply incorrect or refuse to answer.", "section": "3.2. QA Annotations"}, {"figure_path": "https://arxiv.org/html/2503.18923/x19.png", "caption": "Figure 15: Prompt for grading: Part 2", "description": "This prompt instructs the evaluator to grade the correctness of a large language model's response by comparing it to the ground truth.  The task involves considering various factors like numerical precision, semantic equivalence, and tolerance of minor errors.  The evaluator is asked to classify each answer as CORRECT, INCORRECT, or NOT_ATTEMPTED based on specific guidelines, illustrating these classifications with provided examples of each grade.", "section": "3.2. QA Annotations"}, {"figure_path": "https://arxiv.org/html/2503.18923/x20.png", "caption": "Figure 16: Prompt for calibration experiments.", "description": "This figure shows the prompt used in the calibration experiments. The prompt instructs the model to process video frames, a question, a gold target answer, and a predicted answer. Then, the model should classify the predicted answer as either \"CORRECT\", \"INCORRECT\", or \"NOT_ATTEMPTED\", based on whether it aligns with the gold target.  The prompt provides examples of each classification and clarifies the criteria for each, addressing edge cases like minor discrepancies or hedging in answers.  It emphasizes that only factual accuracy matters, ignoring issues like grammar or punctuation.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.18923/x21.png", "caption": "Figure 17: Sampled examples in Video SimpleQA and the responses of typical LVLMs: part 1", "description": "This figure showcases three example questions from the Video SimpleQA benchmark dataset, each accompanied by the ground truth answer and the model's response from three different large video language models (LVLMs): GPT-40, Gemini-1.5-Pro, and Qwen2.5-VL-72B.  The examples illustrate the diversity of questions within the dataset and the varying capabilities of current LVLMs in accurately answering factually-grounded questions about video content.  The responses highlight the models' strengths and weaknesses in understanding and correctly interpreting visual information and integrating external knowledge.", "section": "3. Video SimpleQA"}, {"figure_path": "https://arxiv.org/html/2503.18923/x22.png", "caption": "Figure 18: Sampled examples in Video SimpleQA and the responses of typical LVLMs: part 2", "description": "This figure shows two example questions from the Video SimpleQA benchmark dataset and the corresponding answers generated by three different large video language models (LVLMs): GPT-4, Gemini-1.5-Pro, and Qwen2.5-VL-72B.  The first example involves identifying the pioneers associated with an aircraft shown in a video, while the second involves identifying the physical principle demonstrated.  The answers highlight the varying levels of accuracy and detail provided by different LVLMs in addressing factual video understanding tasks.", "section": "3. Video SimpleQA"}]