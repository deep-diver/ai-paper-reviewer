[{"figure_path": "2410.17131/charts/charts_1_0.png", "caption": "Figure 1: Results of SSO in Online, Offline, and RM Training. Detailed results will be presented in Section 3.2. In these figures, SFT indicates Llama3.1-8B-SFT, which we trained from Llama3.1-8B. Instruct indicates Llama3.1-8B-Instruct. Skywork is the dataset leading to the SOTA reward model for RewardBench.", "description": "The chart displays the results of Self-Steering Optimization (SSO) in online, offline, and reward model training, comparing its performance against standard fine-tuning (SFT) and instruction-tuning methods.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.17131/charts/charts_8_0.png", "caption": "Figure 4: Quality analysis of synthetic data for Llama3.1-SFT training.", "description": "The chart displays the accuracy and on-policy nature of synthetic preference signals generated by SSO and PBAA across three training iterations.", "section": "3.2 MAIN RESULTS"}, {"figure_path": "2410.17131/charts/charts_8_1.png", "caption": "Figure 4: Quality analysis of synthetic data for Llama3.1-SFT training.", "description": "The chart compares the accuracy and on-policy nature of synthetic preference signals generated by SSO and IPO across three training iterations.", "section": "3.2 MAIN RESULTS"}, {"figure_path": "2410.17131/charts/charts_9_0.png", "caption": "Figure 5: Results of Different Optimization Loss on Llama3.1-Instruct.", "description": "The chart displays the performance of Llama3.1-Instruct model with different optimization losses (W and W') across multiple iterations on AlpacaEval 2.0 and MT Bench.", "section": "3.2 MAIN RESULTS"}]