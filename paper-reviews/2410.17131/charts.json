[{"figure_path": "2410.17131/charts/charts_1_0.png", "caption": "Figure 1: Results of SSO in Online, Offline, and RM Training. Detailed results will be presented in Section 3.2. In these figures, SFT indicates Llama3.1-8B-SFT, which we trained from Llama3.1-8B. Instruct indicates Llama3.1-8B-Instruct. Skywork is the dataset leading to the SOTA reward model for RewardBench.", "description": "The chart displays the performance improvements achieved by Self-Steering Optimization (SSO) in online, offline, and reward model training across different base models.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.17131/charts/charts_8_0.png", "caption": "Figure 4: Quality analysis of synthetic data for Llama3.1-SFT training.", "description": "The chart displays the accuracy and on-policy rate of synthetic data generated by SSO and PBAA across three training iterations.", "section": "3.2 MAIN RESULTS"}, {"figure_path": "2410.17131/charts/charts_8_1.png", "caption": "Figure 4: Quality analysis of synthetic data for Llama3.1-SFT training.", "description": "The chart compares the average log probabilities of chosen and rejected responses generated by SSO and IPO across three iterations of training, showing SSO generates better on-policy data.", "section": "3.2 MAIN RESULTS"}, {"figure_path": "2410.17131/charts/charts_9_0.png", "caption": "Figure 5: Results of Different Optimization Loss on Llama3.1-Instruct.", "description": "The chart displays the performance of Llama3.1-Instruct model with different optimization losses (W and W') across iterations on AlpacaEval 2.0 and MT Bench.", "section": "3.2 MAIN RESULTS"}]