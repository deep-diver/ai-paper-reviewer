[{"figure_path": "2410.17131/figures/figures_2_0.png", "caption": "Figure 2: The philosophical motivation of our methods. Greater overlap on the x-axis (performance) between the generated distributions (red and blue) and the original distribution (orange) indicates stronger on-policy behavior. Previous automated methods extract chosen and rejected distributions through different methods, which may be less learnable for the policy model and hard to distinguish after iterative training. Our approach (SSO) optimizes models to generate near-on-policy signals where there remains a gap between chosen and rejected distributions, which benefits the automated alignment process.", "description": "The figure illustrates how the proposed Self-Steering Optimization (SSO) method generates near-on-policy preference signals, contrasting it with previous off-policy methods.", "section": "INTRODUCTION"}, {"figure_path": "2410.17131/figures/figures_2_1.png", "caption": "Figure 2: The philosophical motivation of our methods. Greater overlap on the x-axis (performance) between the generated distributions (red and blue) and the original distribution (orange) indicates stronger on-policy behavior. Previous automated methods extract chosen and rejected distributions through different methods, which may be less learnable for the policy model and hard to distinguish after iterative training. Our approach (SSO) optimizes models to generate near-on-policy signals where there remains a gap between chosen and rejected distributions, which benefits the automated alignment process.", "description": "The figure illustrates how the proposed Self-Steering Optimization (SSO) method generates near on-policy preference signals, unlike previous off-policy methods, by progressively optimizing the model to maintain a gap between chosen and rejected responses throughout iterative training.", "section": "INTRODUCTION"}, {"figure_path": "2410.17131/figures/figures_3_0.png", "caption": "Figure 3: Our approach consists of two iterative steps: 1) Constructing contrastive prompts and sampling responses. Given a query, the policy model first identifies the most relevant features and principles to the query. We then construct a pair of contrastive prompts based on these principles and sample corresponding responses. These responses are then used to form three preference pairs for alignment. 2) Training the model with a weighted objective incorporating three distinct losses.", "description": "The figure illustrates the two-step process of Self-Steering Optimization (SSO), showing how contrastive prompts are constructed and used to train a model with three preference pairs.", "section": "2 SELF-STEERING OPTIMIZATION"}, {"figure_path": "2410.17131/figures/figures_4_0.png", "caption": "Figure 1: Results of SSO in Online, Offline, and RM Training. Detailed results will be presented in Section 3.2. In these figures, SFT indicates Llama3.1-8B-SFT, which we trained from Llama3.1-8B. Instruct indicates Llama3.1-8B-Instruct. Skywork is the dataset leading to the SOTA reward model for RewardBench.", "description": "Figure 1 shows the results of Self-Steering Optimization (SSO) in online, offline, and reward model (RM) training on two foundation models, comparing the performance with and without SSO.", "section": "1 INTRODUCTION"}]