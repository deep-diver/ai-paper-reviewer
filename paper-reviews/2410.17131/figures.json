[{"figure_path": "2410.17131/figures/figures_2_0.png", "caption": "Figure 2: The philosophical motivation of our methods. Greater overlap on the x-axis (performance) between the generated distributions (red and blue) and the original distribution (orange) indicates stronger on-policy behavior. Previous automated methods extract chosen and rejected distributions through different methods, which may be less learnable for the policy model and hard to distinguish after iterative training. Our approach (SSO) optimizes models to generate near-on-policy signals where there remains a gap between chosen and rejected distributions, which benefits the automated alignment process.", "description": "The figure illustrates how the proposed Self-Steering Optimization (SSO) method generates near on-policy preference signals that are more learnable and accurate compared to previous off-policy methods.", "section": "INTRODUCTION"}, {"figure_path": "2410.17131/figures/figures_2_1.png", "caption": "Figure 2: The philosophical motivation of our methods. Greater overlap on the x-axis (performance) between the generated distributions (red and blue) and the original distribution (orange) indicates stronger on-policy behavior. Previous automated methods extract chosen and rejected distributions through different methods, which may be less learnable for the policy model and hard to distinguish after iterative training. Our approach (SSO) optimizes models to generate near-on-policy signals where there remains a gap between chosen and rejected distributions, which benefits the automated alignment process.", "description": "The figure illustrates how the proposed Self-Steering Optimization (SSO) method generates near on-policy preference signals, contrasting it with previous off-policy methods.", "section": "INTRODUCTION"}, {"figure_path": "2410.17131/figures/figures_3_0.png", "caption": "Figure 3: Our approach consists of two iterative steps: 1) Constructing contrastive prompts and sampling responses. Given a query, the policy model first identifies the most relevant features and principles to the query. We then construct a pair of contrastive prompts based on these principles and sample corresponding responses. These responses are then used to form three preference pairs for alignment. 2) Training the model with a weighted objective incorporating three distinct losses.", "description": "The figure illustrates the two-step process of Self-Steering Optimization (SSO): constructing contrastive prompts and sampling responses, and then training the model using three preference pairs with a weighted objective.", "section": "2 SELF-STEERING OPTIMIZATION"}, {"figure_path": "2410.17131/figures/figures_4_0.png", "caption": "Figure 1: Results of SSO in Online, Offline, and RM Training. Detailed results will be presented in Section 3.2. In these figures, SFT indicates Llama3.1-8B-SFT, which we trained from Llama3.1-8B. Instruct indicates Llama3.1-8B-Instruct. Skywork is the dataset leading to the SOTA reward model for RewardBench.", "description": "The figure shows the results of Self-Steering Optimization (SSO) in online, offline, and reward model (RM) training on two foundation models, comparing its performance against standard fine-tuning (SFT) and instruction-tuning methods.", "section": "1 INTRODUCTION"}]