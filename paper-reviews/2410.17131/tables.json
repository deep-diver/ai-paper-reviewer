[{"figure_path": "2410.17131/tables/table_6_0.html", "caption": "Table 1: Results on Llama3.1-8B-SFT and Qwen2-7B-SFT. We conduct experiments with Ultra-feedback, modified PBAA (principle-based automated alignment), and SSO. In this table, \u201cAE2\u201d represents \u201cAlpacaEval 2.0 Length Control Win Rate\u201d. \u201cMT\u201d represents \u201cMT-Bench\u201d.", "description": "Table 1 presents the results of iterative online training on Llama3.1-8B-SFT and Qwen2-7B-SFT models using Ultra-feedback, modified PBAA, and SSO, evaluating performance across multiple benchmarks.", "section": "3.2.1 How SSO PERFORMS IN ITERATIVE ONLINE TRAINING"}, {"figure_path": "2410.17131/tables/table_6_1.html", "caption": "Table 2: Results on Llama3.1-8B-Instruct and Qwen2-7B-Instruct.", "description": "Table 2 presents the results of applying SSO to already aligned models, Llama3.1-8B-Instruct and Qwen2-7B-Instruct, showing improvements in both subjective and objective benchmarks.", "section": "3.2 MAIN RESULTS"}, {"figure_path": "2410.17131/tables/table_7_0.html", "caption": "Table 3: Results on Llama3.1 trained with synthetic offline data.", "description": "Table 3 presents the results of Llama3.1 model trained with synthetic offline data generated by SSO on various benchmarks including AE2, MT, GPQA, MATH, GSM8K, and MMLU Pro.", "section": "3.2 MAIN RESULTS"}, {"figure_path": "2410.17131/tables/table_7_1.html", "caption": "Table 4: Our Reward Models", "description": "The table presents the performance of reward models trained with different datasets on RewardBench, showing the average scores for chat, chat-hard, safety, and reason.", "section": "3.2.3 How SSO PERFORM IN RM TRAINING"}, {"figure_path": "2410.17131/tables/table_7_2.html", "caption": "Table 5: Results on Qwen2-7B-Instruct under different ablations (Iteration 3).", "description": "This table presents the ablation study results on Qwen2-7B-Instruct model with different components removed, showing the impact on the performance metrics of AE2 and MT.", "section": "3.2 MAIN RESULTS"}, {"figure_path": "2410.17131/tables/table_8_0.html", "caption": "Table 6: Results with DPO-Based SSO.", "description": "Table 6 presents the experimental results of SSO based on DPO Loss for Qwen2-7B-Instruct and Llama3.1-8B-Instruct.", "section": "3.2 MAIN RESULTS"}, {"figure_path": "2410.17131/tables/table_8_1.html", "caption": "Table 7: Results on Infinity-Instruct-7M-Gen-Llama3.1-8B", "description": "Table 7 presents the results of applying SSO to a stronger SFT model of Llama3.1-8B, demonstrating improved performance on several benchmarks.", "section": "3.2 MAIN RESULTS"}, {"figure_path": "2410.17131/tables/table_15_0.html", "caption": "Table 1: Results on Llama3.1-8B-SFT and Qwen2-7B-SFT. We conduct experiments with Ultra-feedback, modified PBAA (principle-based automated alignment), and SSO. In this table, \"AE2\" represents \"AlpacaEval 2.0 Length Control Win Rate\". \"MT\" represents \"MT-Bench\".", "description": "Table 1 presents the results of experiments on Llama3.1-8B-SFT and Qwen2-7B-SFT models using Ultra-feedback, modified PBAA, and SSO, evaluating performance across various metrics including AlpacaEval 2.0, MT-Bench, GPQA, MMLU Pro, MATH, and GSM8K.", "section": "3.2.1 How SSO PERFORMS IN ITERATIVE ONLINE TRAINING"}, {"figure_path": "2410.17131/tables/table_15_1.html", "caption": "Table 8: Results on Llama3.1-8B-Instruct and Qwen2-7B-Instruct.", "description": "Table 8 presents the detailed results of experiments conducted on Llama3.1-8B-Instruct and Qwen2-7B-Instruct models, comparing their performance across various metrics with different training methods.", "section": "3.2.1 How SSO Performs in Iterative Online Training"}, {"figure_path": "2410.17131/tables/table_16_0.html", "caption": "Table 1: Results on Llama3.1-8B-SFT and Qwen2-7B-SFT. We conduct experiments with Ultra-feedback, modified PBAA (principle-based automated alignment), and SSO. In this table, \"AE2\" represents \"AlpacaEval 2.0 Length Control Win Rate\". \"MT\" represents \"MT-Bench\".", "description": "Table 1 presents the results of experiments conducted on Llama3.1-8B-SFT and Qwen2-7B-SFT models using Ultra-feedback, modified PBAA, and SSO, comparing their performance across various metrics, including AlpacaEval 2.0, MT-Bench, GPQA, MMLU Pro, MATH, and GSM8K.", "section": "3.2.1 How SSO PERFORMS IN ITERATIVE ONLINE TRAINING"}, {"figure_path": "2410.17131/tables/table_16_1.html", "caption": "Table 1: Results on Llama3.1-8B-SFT and Qwen2-7B-SFT. We conduct experiments with Ultra-feedback, modified PBAA (principle-based automated alignment), and SSO. In this table, \"AE2\" represents \"AlpacaEval 2.0 Length Control Win Rate\". \"MT\" represents \"MT-Bench\".", "description": "Table 1 presents the results of experiments conducted on Llama3.1-8B-SFT and Qwen2-7B-SFT models using Ultra-feedback, modified PBAA, and SSO, evaluating performance across various metrics including AlpacaEval 2.0, MT-Bench, GPQA, MMLU Pro, MATH, and GSM8K.", "section": "3.2.1 How SSO PERFORMS IN ITERATIVE ONLINE TRAINING"}, {"figure_path": "2410.17131/tables/table_17_0.html", "caption": "Table 12: Results on Qwen2-7B-Instruct and Llama3.1-8B-Instruct under different ablations.", "description": "Table 12 presents the results of an ablation study on Qwen2-7B-Instruct and Llama3.1-8B-Instruct models under different ablation settings, showing the impact of removing the weight function (W), self-steering loss (G), or both on the model performance.", "section": "A.2 DETAIL ABLATION"}, {"figure_path": "2410.17131/tables/table_17_1.html", "caption": "Table 1: Results on Llama3.1-8B-SFT and Qwen2-7B-SFT. We conduct experiments with Ultra-feedback, modified PBAA (principle-based automated alignment), and SSO. In this table, \"AE2\" represents \"AlpacaEval 2.0 Length Control Win Rate\". \"MT\" represents \"MT-Bench\".", "description": "Table 1 presents the results of iterative online training experiments comparing the performance of Self-Steering Optimization (SSO) against modified principle-based alignment and Ultra-feedback on Llama3.1-8B-SFT and Qwen2-7B-SFT models across various metrics.", "section": "3.2.1 How SSO PERFORMS IN ITERATIVE ONLINE TRAINING"}, {"figure_path": "2410.17131/tables/table_18_0.html", "caption": "Table 1: Results on Llama3.1-8B-SFT and Qwen2-7B-SFT. We conduct experiments with Ultra-feedback, modified PBAA (principle-based automated alignment), and SSO. In this table, \"AE2\" represents \"AlpacaEval 2.0 Length Control Win Rate\". \"MT\" represents \"MT-Bench\".", "description": "Table 1 presents the results of experiments comparing the performance of Self-Steering Optimization (SSO) against modified principle-based alignment and Ultra-feedback on Llama3.1-8B-SFT and Qwen2-7B-SFT across multiple benchmarks.", "section": "3.2.1 How SSO PERFORMS IN ITERATIVE ONLINE TRAINING"}, {"figure_path": "2410.17131/tables/table_19_0.html", "caption": "Table 1: Results on Llama3.1-8B-SFT and Qwen2-7B-SFT. We conduct experiments with Ultra-feedback, modified PBAA (principle-based automated alignment), and SSO. In this table, \"AE2\" represents \"AlpacaEval 2.0 Length Control Win Rate\". \"MT\" represents \"MT-Bench\".", "description": "Table 1 presents the results of experiments conducted on Llama3.1-8B-SFT and Qwen2-7B-SFT models using Ultra-feedback, modified PBAA, and SSO, evaluating performance across multiple benchmarks.", "section": "3.2.1 How SSO PERFORMS IN ITERATIVE ONLINE TRAINING"}, {"figure_path": "2410.17131/tables/table_20_0.html", "caption": "Table 1: Results on Llama3.1-8B-SFT and Qwen2-7B-SFT. We conduct experiments with Ultra-feedback, modified PBAA (principle-based automated alignment), and SSO. In this table, \"AE2\" represents \"AlpacaEval 2.0 Length Control Win Rate\". \"MT\" represents \"MT-Bench\".", "description": "Table 1 presents the results of experiments conducted on Llama3.1-8B-SFT and Qwen2-7B-SFT models using Ultra-feedback, modified PBAA, and SSO, evaluating performance across various metrics.", "section": "3.2.1 How SSO PERFORMS IN ITERATIVE ONLINE TRAINING"}]