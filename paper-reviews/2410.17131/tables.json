[{"figure_path": "2410.17131/tables/table_6_0.html", "caption": "Table 1: Results on Llama3.1-8B-SFT and Qwen2-7B-SFT. We conduct experiments with Ultra-feedback, modified PBAA (principle-based automated alignment), and SSO. In this table, \"AE2\" represents \"AlpacaEval 2.0 Length Control Win Rate\". \"MT\" represents \"MT-Bench\".", "description": "Table 1 presents the results of iterative online training on Llama3.1-8B-SFT and Qwen2-7B-SFT models using Ultra-feedback, modified PBAA, and SSO, comparing their performance across various metrics.", "section": "3.2.1 How SSO PERFORMS IN ITERATIVE ONLINE TRAINING"}, {"figure_path": "2410.17131/tables/table_6_1.html", "caption": "Table 2: Results on Llama3.1-8B-Instruct and Qwen2-7B-Instruct.", "description": "Table 2 presents the results of applying SSO and other methods on two pre-trained instruction-following language models, Llama3.1-8B and Qwen2-7B, across multiple benchmark datasets.", "section": "3.2 MAIN RESULTS"}, {"figure_path": "2410.17131/tables/table_7_0.html", "caption": "Table 3: Results on Llama3.1 trained with synthetic offline data.", "description": "Table 3 presents the results of Llama3.1 model trained with synthetic offline data generated by SSO on various evaluation metrics.", "section": "3.2 MAIN RESULTS"}, {"figure_path": "2410.17131/tables/table_7_1.html", "caption": "Table 4: Our Reward Models", "description": "Table 4 shows the performance of reward models trained with different datasets on RewardBench, indicating the effectiveness of SSO in enhancing reward model performance.", "section": "3.2.3 How SSO PERFORM IN RM TRAINING"}, {"figure_path": "2410.17131/tables/table_7_2.html", "caption": "Table 5: Results on Qwen2-7B-Instruct under different ablations (Iteration 3).", "description": "Table 5 presents the ablation study results on Qwen2-7B-Instruct, showing the impact of removing the weight function (W) and self-steering loss (G) on the model's performance across various metrics.", "section": "3.2 MAIN RESULTS"}, {"figure_path": "2410.17131/tables/table_8_0.html", "caption": "Table 6: Results with DPO-Based SSO.", "description": "Table 6 presents experimental results of SSO based on DPO Loss for Qwen2-7B-Instruct and Llama3.1-8B-Instruct.", "section": "3.2 MAIN RESULTS"}, {"figure_path": "2410.17131/tables/table_8_1.html", "caption": "Table 7: Results on Infinity-Instruct-7M-Gen-Llama3.1-8B", "description": "This table presents the results of applying SSO to a stronger SFT model of Llama3.1-8B trained on Infinity Instruct, showing that the model outperforms Llama3.1-8B-Instruct on some benchmarks.", "section": "3.2 MAIN RESULTS"}, {"figure_path": "2410.17131/tables/table_15_0.html", "caption": "Table 8: Results on Llama3.1-8B-Instruct and Qwen2-7B-Instruct.", "description": "Table 8 presents detailed results of experiments conducted on Llama3.1-8B-Instruct and Qwen2-7B-Instruct models using UltraFeedBack+IPO, Modified PBAA(IPO Based), and SSO(IPO Based) methods across multiple iterations.", "section": "3.2.1 How SSO PERFORMS IN ITERATIVE ONLINE TRAINING"}, {"figure_path": "2410.17131/tables/table_15_1.html", "caption": "Table 8: Results on Llama3.1-8B-Instruct and Qwen2-7B-Instruct.", "description": "Table 8 presents detailed results of experiments conducted on Llama3.1-8B-Instruct and Qwen2-7B-Instruct models using different methods (UltraFeedBack+IPO, Modified PBAA, and SSO).", "section": "3.2.1 How SSO Performs in Iterative Online Training"}, {"figure_path": "2410.17131/tables/table_16_0.html", "caption": "Table 1: Results on Llama3.1-8B-SFT and Qwen2-7B-SFT. We conduct experiments with Ultra-feedback, modified PBAA (principle-based automated alignment), and SSO. In this table, \"AE2\" represents \"AlpacaEval 2.0 Length Control Win Rate\". \"MT\" represents \"MT-Bench\".", "description": "Table 1 presents the results of experiments comparing the performance of Self-Steering Optimization (SSO) against modified principle-based alignment and Ultra-feedback on Llama3.1-8B-SFT and Qwen2-7B-SFT models across multiple iterations, evaluating metrics such as AlpacaEval 2.0, MT-Bench, GPQA, MMLU Pro, MATH, and GSM8K.", "section": "3.2.1 How SSO PERFORMS IN ITERATIVE ONLINE TRAINING"}, {"figure_path": "2410.17131/tables/table_16_1.html", "caption": "Table 1: Results on Llama3.1-8B-SFT and Qwen2-7B-SFT. We conduct experiments with Ultra-feedback, modified PBAA (principle-based automated alignment), and SSO. In this table, \"AE2\" represents \"AlpacaEval 2.0 Length Control Win Rate\". \"MT\" represents \"MT-Bench\".", "description": "Table 1 presents the results of experiments comparing Self-Steering Optimization (SSO) against modified principle-based automated alignment and Ultra-feedback on Llama3.1-8B-SFT and Qwen2-7B-SFT models across multiple benchmarks.", "section": "3.2.1 How SSO PERFORMS IN ITERATIVE ONLINE TRAINING"}, {"figure_path": "2410.17131/tables/table_17_0.html", "caption": "Table 12: Results on Qwen2-7B-Instruct and Llama3.1-8B-Instruct under different ablations.", "description": "Table 12 presents the detailed results of an ablation study on Qwen2-7B-Instruct and Llama3.1-8B-Instruct models under different ablations, showing the performance of each model under different experimental settings.", "section": "A.2 DETAIL ABLATION"}, {"figure_path": "2410.17131/tables/table_17_1.html", "caption": "Table 1: Results on Llama3.1-8B-SFT and Qwen2-7B-SFT. We conduct experiments with Ultra-feedback, modified PBAA (principle-based automated alignment), and SSO. In this table, \"AE2\" represents \"AlpacaEval 2.0 Length Control Win Rate\". \"MT\" represents \"MT-Bench\".", "description": "Table 1 presents the results of experiments conducted on Llama3.1-8B-SFT and Qwen2-7B-SFT models using Ultra-feedback, modified PBAA, and SSO, evaluating performance across multiple benchmarks.", "section": "3.2.1 How SSO PERFORMS IN ITERATIVE ONLINE TRAINING"}, {"figure_path": "2410.17131/tables/table_18_0.html", "caption": "Table 1: Results on Llama3.1-8B-SFT and Qwen2-7B-SFT. We conduct experiments with Ultra-feedback, modified PBAA (principle-based automated alignment), and SSO. In this table, \"AE2\" represents \"AlpacaEval 2.0 Length Control Win Rate\". \"MT\" represents \"MT-Bench\".", "description": "Table 1 presents the results of experiments comparing the performance of Self-Steering Optimization (SSO) against modified principle-based alignment and Ultra-feedback on Llama3.1-8B-SFT and Qwen2-7B-SFT models across multiple benchmarks.", "section": "3.2.1 How SSO PERFORMS IN ITERATIVE ONLINE TRAINING"}, {"figure_path": "2410.17131/tables/table_19_0.html", "caption": "Table 1: Results on Llama3.1-8B-SFT and Qwen2-7B-SFT. We conduct experiments with Ultra-feedback, modified PBAA (principle-based automated alignment), and SSO. In this table, \"AE2\" represents \"AlpacaEval 2.0 Length Control Win Rate\". \"MT\" represents \"MT-Bench\".", "description": "Table 1 presents the results of iterative online training on Llama3.1-8B-SFT and Qwen2-7B-SFT models using Ultra-feedback, modified PBAA, and SSO, evaluating performance across multiple benchmarks.", "section": "3.2.1 How SSO PERFORMS IN ITERATIVE ONLINE TRAINING"}, {"figure_path": "2410.17131/tables/table_20_0.html", "caption": "Table 1: Results on Llama3.1-8B-SFT and Qwen2-7B-SFT. We conduct experiments with Ultra-feedback, modified PBAA (principle-based automated alignment), and SSO. In this table, \"AE2\" represents \"AlpacaEval 2.0 Length Control Win Rate\". \"MT\" represents \"MT-Bench\".", "description": "Table 1 presents the results of experiments conducted on Llama3.1-8B-SFT and Qwen2-7B-SFT models using Ultra-feedback, modified PBAA, and SSO, comparing their performance across various metrics including AlpacaEval 2.0, MT-Bench, GPQA, MMLU Pro, MATH, and GSM8K.", "section": "3.2.1 How SSO PERFORMS IN ITERATIVE ONLINE TRAINING"}]