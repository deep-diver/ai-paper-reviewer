{"references": [{" publication_date": "2022", "fullname_first_author": "Jean-Baptiste Alayrac", "paper_title": "Flamingo: a visual language model for few-shot learning", "reason": "This paper is highly relevant because it introduces Flamingo, a foundational visual language model that serves as a basis for many subsequent LVLMs.  The authors' work on Flamingo directly addresses the challenge of multimodal understanding, which is crucial for mitigating object hallucination in LVLMs. Understanding Flamingo's architecture and capabilities provides a vital context for comprehending the advancements and limitations of the proposed CCA method.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Josh Achiam", "paper_title": "GPT-4 technical report", "reason": "This paper is crucial as it details the capabilities of GPT-4, a leading large language model.  The comparative analysis of GPT-4 with the authors' proposed method helps establish the state-of-the-art performance in generating accurate and factual multimodal responses, which is essential for evaluating the success of the CCA in reducing object hallucination.", "section_number": 5}, {" publication_date": "2018", "fullname_first_author": "Anna Rohrbach", "paper_title": "Object hallucination in image captioning", "reason": "This seminal work is highly important because it first introduces the concept of object hallucination in image captioning, a crucial precursor to understanding object hallucination in large vision-language models. The authors' early identification of the problem and their insights provide a fundamental framework for research in this field. This forms a critical starting point for the authors' work, as they build on this existing understanding of the core challenge.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Yifan Li", "paper_title": "Evaluating object hallucination in large vision-language models", "reason": "This paper is critical because it provides a rigorous evaluation framework (POPE) for assessing object hallucination in LVLMs. The authors' use of POPE as a benchmark allows for a direct comparison of their proposed method's performance against existing techniques, providing strong empirical evidence of its effectiveness in reducing object hallucination.", "section_number": 5}, {" publication_date": "2020", "fullname_first_author": "Alexey Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "reason": "This foundational paper introduces the Vision Transformer (ViT) architecture, a key component in many LVLMs.  Understanding ViT's architecture and its impact on visual representation is crucial for assessing the effectiveness of CCA, as it directly influences how visual information is processed and integrated with textual information in LVLMs.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Wei-Lin Chiang", "paper_title": "Vicuna: An open-source chatbot impressing GPT-4 with 90%* ChatGPT quality", "reason": "This paper introduces Vicuna, an open-source chatbot model, and is relevant to the paper because Vicuna's strong performance is used as a benchmark to compare against the authors' proposed model. Vicuna's architecture and training methodologies, especially its instruction tuning, provide a context for analyzing the improvement achieved by the authors' method in addressing object hallucination in LVLMs.", "section_number": 5}, {" publication_date": "2014", "fullname_first_author": "Tsung-Yi Lin", "paper_title": "Microsoft COCO: Common Objects in Context", "reason": "This paper introduces the COCO dataset, which is a fundamental benchmark dataset used for evaluating object detection and related tasks in computer vision. As a result, COCO's importance lies in its use as a benchmark for evaluating object hallucination mitigation techniques, providing a widely recognized and standardized means of assessing the performance of various models.", "section_number": 5}, {" publication_date": "2020", "fullname_first_author": "Pengcheng He", "paper_title": "DeBERTa: Decoding-enhanced BERT with disentangled attention", "reason": "This paper is significant because it introduces DeBERTa, an improved version of BERT, focusing on enhancing decoding and disentangling attention mechanisms in language models.  These improvements in language understanding are indirectly relevant as they can potentially enhance the accuracy and factual consistency of the visual-language interactions in LVLMs, thereby indirectly addressing object hallucination.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Sicong Leng", "paper_title": "Mitigating object hallucinations in large vision-language models through visual contrastive decoding", "reason": "This paper is directly relevant because it proposes a state-of-the-art method (VCD) for mitigating object hallucination in LVLMs. The authors directly compare their proposed CCA method against VCD, which serves as a crucial benchmark for evaluating the improvement achieved. This comparison demonstrates the effectiveness of CCA in addressing the limitations of existing techniques.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Qidong Huang", "paper_title": "Opera: Alleviating hallucination in multi-modal large language models via over-trust penalty and retrospection-allocation", "reason": "This paper proposes OPERA, a method for mitigating hallucinations in multimodal large language models.  The authors compare their CCA method's performance against OPERA, highlighting the relative advantages of CCA. This comparison is particularly important for evaluating the effectiveness of CCA in terms of accuracy and efficiency.", "section_number": 5}, {" publication_date": "2017", "fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "reason": "This paper is highly influential as it introduces the Transformer architecture, which is foundational to many modern large language models, including those used in LVLMs. Understanding the principles behind the Transformer architecture is crucial for comprehending the impact of positional encoding (RoPE) and the rationale behind the proposed CCA method for improving multimodal alignment.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Haotian Liu", "paper_title": "Visual instruction tuning", "reason": "This paper introduces the concept of visual instruction tuning for LVLMs. This is highly relevant because the authors' method builds upon this concept, using a similar approach to improve multimodal alignment and reduce object hallucination.  Analyzing the training method and its impact on model performance is essential for evaluating the improvements achieved by CCA.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Zhiqing Sun", "paper_title": "Aligning large multimodal models with factually augmented RLHF", "reason": "This paper focuses on aligning large multimodal models using reinforcement learning from human feedback (RLHF), a crucial aspect of improving model accuracy and reducing hallucination. The authors compare their approach to this state-of-the-art method, demonstrating CCA's effectiveness in achieving comparable results without the need for computationally intensive RLHF.", "section_number": 5}, {" publication_date": "2022", "fullname_first_author": "Junnan Li", "paper_title": "BLIP: Bootstrapping language-image pre-training for unified vision-language understanding and generation", "reason": "This paper introduces BLIP, a significant advancement in vision-language models, which utilizes a powerful large language model to process multimodal inputs.  Understanding the architectural design and training methods of BLIP provides a context for understanding the authors' work, as it directly influences how the proposed CCA method can be used to enhance multimodal alignment.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "Guolin Ke", "paper_title": "Rethinking positional encoding in language pre-training", "reason": "This paper provides valuable insights into the importance of positional encoding in language models. This is highly relevant as it directly relates to the authors' focus on the role of Rotary Position Encoding (RoPE) in LVLMs and how the long-term decay of RoPE affects object hallucination.  Understanding the different approaches to positional encoding is crucial for assessing the novelty and effectiveness of the authors' proposed CCA method.", "section_number": 2}, {" publication_date": "2019", "fullname_first_author": "Amanpreet Singh", "paper_title": "Towards VQA models that can read", "reason": "This paper focuses on improving Visual Question Answering (VQA) models' ability to understand and process textual information, which is highly relevant to the challenges in mitigating object hallucination.  By improving the model's ability to accurately interpret textual descriptions of images, the chance of hallucination is reduced. Thus it helps to contextualize the authors' contribution within the broader field of vision-language understanding.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Chaoyou Fu", "paper_title": "MME: A comprehensive evaluation benchmark for multimodal large language models", "reason": "This paper is significant because it introduces MME, a comprehensive evaluation benchmark specifically designed for assessing multimodal large language models. The authors' inclusion of MME in their experimental evaluation demonstrates the robustness of their method across various tasks and metrics, validating the generalizability of CCA in mitigating object hallucination.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Tianyu Yu", "paper_title": "RLHF-V: Towards trustworthy MLLMs via behavior alignment from fine-grained correctional human feedback", "reason": "This paper proposes RLHF-V, focusing on aligning large multimodal models using reinforcement learning from human feedback (RLHF). The authors compare their CCA method against RLHF-V to highlight that CCA can achieve comparable results without extensive RLHF, making it a more efficient and cost-effective solution.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Jianlin Su", "paper_title": "RoFormer: Enhanced transformer with rotary position embedding", "reason": "This paper is highly relevant because it introduces RoFormer, which utilizes rotary position embedding (RoPE). The authors' research is directly focused on understanding and addressing the limitations of RoPE and their proposed CCA method is explicitly designed to mitigate the negative impacts of RoPE's long-term decay. The strong connection between the authors' work and this paper underlines the importance of RoPE in the context of their contribution.", "section_number": 3}, {" publication_date": "2018", "fullname_first_author": "Peter Shaw", "paper_title": "Self-attention with relative position representations", "reason": "This paper examines self-attention mechanisms with relative position representations, a key aspect of transformer-based models. This is highly relevant because the authors' work focuses on improving the positional encoding within the transformer architecture to mitigate object hallucination. Understanding self-attention and positional encoding is crucial for comprehending the rationale behind the proposed CCA method and its effectiveness in addressing the limitations of RoPE.", "section_number": 2}]}