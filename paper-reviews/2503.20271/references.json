{"references": [{"fullname_first_author": "Shuai Bai", "paper_title": "Qwen2. 5-vl technical report", "publication_date": "2025-02-13", "reason": "This paper introduces Qwen2.5-VL, the base model upon which the ViLPRM is built."}, {"fullname_first_author": "Yuntao Bai", "paper_title": "Constitutional ai: Harmlessness from ai feedback", "publication_date": "2022-12-08", "reason": "This paper is important as it explores AI feedback, a core element of the reward modeling approach used in the current study."}, {"fullname_first_author": "Dongping Chen", "paper_title": "Mllm-as-a-judge: Assessing multimodal Ilm-as-a-judge with vision-language benchmark", "publication_date": "2024-01-01", "reason": "This paper is crucial as it establishes the MLLM-as-a-judge paradigm, which the current study utilizes to benchmark VLLMs."}, {"fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-01-01", "reason": "This paper lays the groundwork for instruction following, a key aspect of how reward models are used to guide language models."}, {"fullname_first_author": "Rafael Rafailov", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "publication_date": "2023-01-01", "reason": "This paper presents direct preference optimization, an alternative method that views the language model itself as a reward model, providing a crucial perspective for reward modelling"}]}