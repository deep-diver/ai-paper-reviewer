[{"figure_path": "https://arxiv.org/html/2503.20271/ICCV25/pics/mcts2.pdf", "caption": "Figure 1: MCTS tree we have constructed for geometry problem datasets (e.g., MAVIS-Geometry). One path in the tree yields a correct result, while the remaining paths result in incorrect answers. It is worth noting that we use ellipses to omit some nodes in the original MCTS tree for better presentation.", "description": "This figure shows a Monte Carlo Tree Search (MCTS) tree used in constructing the ViLReward-73K dataset.  The MCTS process explores different reasoning paths to solve geometry problems. Each node in the tree represents a step in the reasoning process, and the edges connect steps. One path in the MCTS tree leads to the correct solution, while other paths result in incorrect answers.  The value associated with each node represents the estimated quality of that reasoning step. The ellipses indicate that some nodes in the original MCTS tree have been omitted for clarity.", "section": "3. Part II: ViLPRM: A Vision-Language Process Reward Model"}, {"figure_path": "https://arxiv.org/html/2503.20271/ICCV25/pics/prm_examples2.pdf", "caption": "Figure 2: An example from o1\u2019s generation of a medical reasoning example from our ViLBench.", "description": "This figure shows an example from OpenAI's model, o1,  demonstrating its performance on a medical reasoning task within the ViLBench dataset.  It highlights the step-wise reasoning process of the model, displaying the intermediate steps, associated scores (from both URSA and the proposed ViLPRM model), and the final answer. The example visually compares how ViLPRM and URSA, two different process reward models, score the reasoning steps. ViLPRM gives higher scores to the steps that lead to a correct answer, showcasing its effectiveness in evaluating the quality of the reasoning process.", "section": "4.2. Results and Analysis"}]