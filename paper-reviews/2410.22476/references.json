{"references": [{"fullname_first_author": "Jacob Devlin", "paper_title": "Bert: Pre-training of deep bidirectional transformers for language understanding", "publication_date": "2019-00-00", "reason": "This paper introduces BERT, a highly influential language model used as a foundation for many subsequent NLP models, including the one proposed in this paper."}, {"fullname_first_author": "Yinhan Liu", "paper_title": "Roberta: A robustly optimized bert pretraining approach", "publication_date": "2019-00-00", "reason": "This paper presents RoBERTa, an improved version of BERT, which is directly compared to and used in the experiments of this paper."}, {"fullname_first_author": "Alice Coucke", "paper_title": "Snips voice platform: an embedded spoken language understanding system for private-by-design voice interfaces", "publication_date": "2018-00-00", "reason": "This paper introduces the SNIPS dataset, a crucial benchmark dataset used for training and evaluating multi-intent detection models in this research, demonstrating the model's real-world applicability."}, {"fullname_first_author": "Inigo Casanueva", "paper_title": "Efficient intent detection with dual sentence encoders", "publication_date": "2020-00-00", "reason": "This paper explores efficient intent detection methods, providing a relevant comparison point for the novel approach proposed and evaluated in this research."}, {"fullname_first_author": "Libo Qin", "paper_title": "AGIF: An adaptive graph-interactive framework for joint multiple intent detection and slot filling", "publication_date": "2020-00-00", "reason": "This paper addresses a similar problem of joint intent detection and slot filling, offering a comparative approach against which the proposed pointer network model is assessed."}]}