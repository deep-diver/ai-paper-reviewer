[{"figure_path": "https://arxiv.org/html/2503.21620/extracted/6315182/images/Fig0_v2.png", "caption": "Figure 1: Left: Overall performance of UI-R1-3B on both in-domain (i.e., AndroidControl) and out-of-domain (i.e., ScreenSpot-Pro, ScreenSpot desktop and web subsets) tasks; Right: Employing reinforcement fine-tuning (RFT), UI-R1-3B achieves performance comparable to SFT models with significantly fewer data and GPU hours. The circle radius indicates the model size.", "description": "This figure presents a comparison of the UI-R1-3B model's performance against other models.  The left panel shows a radar chart illustrating the model's performance across various in-domain (AndroidControl) and out-of-domain (ScreenSpot-Pro, ScreenSpot's desktop and web subsets) tasks.  Each axis represents a different task or aspect of performance, and the distance of the UI-R1-3B point from the center shows the model's relative performance on that axis.  The right panel is a bar chart comparing the performance of UI-R1-3B, trained using reinforcement fine-tuning (RFT) with fewer data points, to larger models trained with supervised fine-tuning (SFT). It highlights that UI-R1-3B achieves comparable or better performance despite using significantly less training data and computational resources (GPU hours).  The size of the circles in the bar chart visually represents the size of the models.", "section": "Experiment"}, {"figure_path": "https://arxiv.org/html/2503.21620/extracted/6315182/images/Fig_1_v5.png", "caption": "Figure 2: Overview of UI-R1 training framework. Given a GUI screenshot and a text instruction from the user, the policy model (i.e., Qwen2.5-VL-3B) generates multiple action planning responses with reasoning. Our proposed rule-based action reward function is then applied, and the policy model is updated using a policy gradient optimization algorithm.", "description": "The UI-R1 training framework starts with a GUI screenshot and a user's text instruction.  The Qwen2.5-VL-3B policy model generates multiple action plans, each including reasoning steps. A custom rule-based reward function assesses these plans.  The policy model is then refined using a policy gradient optimization algorithm based on the rewards received.", "section": "3 Method"}, {"figure_path": "https://arxiv.org/html/2503.21620/extracted/6315182/images/study.png", "caption": "Figure 3: Left: Impact of data selection methods and data size; Right: Study of relation between answering accuracy and reasoning length.", "description": "This figure presents a two-part analysis of the UI-R1 model's performance.  The left panel shows how different data selection strategies and varying training dataset sizes affect the model's accuracy on the ScreenSpot benchmark.  It compares results using randomly selected data versus data specifically chosen for difficulty, revealing the impact of data quality and quantity on performance. The right panel investigates the correlation between the length of the model's reasoning process and its accuracy in answering the questions. It illustrates how accuracy may decrease as reasoning complexity increases, suggesting that the model faces more difficulty in providing correct answers for more complex tasks.", "section": "4 Experiment"}, {"figure_path": "https://arxiv.org/html/2503.21620/extracted/6315182/images/ablation.png", "caption": "Figure 4: Left: Ablation on reward function; Right: Ablation on data selection method.", "description": "This figure presents ablation study results, investigating the impact of different reward functions and data selection methods on model performance. The left panel shows a comparison of using only the action reward, only the coordinate reward, both action and bounding box reward, and the combination of both action and coordinate reward.  The right panel compares different data selection methods, illustrating the effect of using only randomly chosen data versus using a high-quality subset of data selected by difficulty, demonstrating the quality and efficiency of the proposed data selection method.  This analysis is crucial to evaluating the model's sensitivity and effectiveness to the design choices made for the reward structure and training data.", "section": "4.4 Ablation Study"}, {"figure_path": "https://arxiv.org/html/2503.21620/extracted/6315182/images/loss_reward.png", "caption": "Figure 5: UI-R1 training process.", "description": "This figure visualizes the training progress of the UI-R1 model by plotting various metrics over training steps.  These metrics include reward-related values (accuracy rewards for action and coordinates, format reward, reward standard deviation, total reward), loss, KL divergence, and completion length.  The plots allow for observation of trends in these metrics throughout the training process, giving insights into the model's learning dynamics.", "section": "3 Method"}, {"figure_path": "https://arxiv.org/html/2503.21620/extracted/6315182/images/iteration.png", "caption": "Figure 6: Accuracy change over rounds.", "description": "This figure shows how the accuracy of the model changes over training rounds.  Separate lines represent accuracy on different subsets of the data (mobile, web, and desktop).  The graph shows the model's performance improves in all three subsets over eight training rounds, stabilizing by round 7 or 8.", "section": "C Other Ablation"}, {"figure_path": "https://arxiv.org/html/2503.21620/extracted/6315182/images/user_case.png", "caption": "Figure 7: An example of use case.", "description": "This figure showcases a practical example of the UI-R1 model's capabilities.  It presents a screenshot of a login page with a \"Remember me\" checkbox. The text describes the task (selecting the checkbox), the model's reasoning process (identifying the checkbox and its location), and the resulting action (clicking the checkbox's coordinates). This demonstrates the model's ability to understand user instructions, reason about the GUI elements, and execute the corresponding actions accurately.", "section": "D Case Study"}]