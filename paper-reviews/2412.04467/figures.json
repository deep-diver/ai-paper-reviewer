[{"figure_path": "https://arxiv.org/html/2412.04467/x1.png", "caption": "Figure 1: VisionZip Performance and Efficiency. (a) Our VisionZip significantly outperforms the current SOTA EfficientVLM model, like FastV, SparseVLM, achieving nearly 95% of the performance with only 10% of the tokens across 11 benchmarks on LLaVA-1.5. (b) VisionZip could reduce 8\u00d7\\times\u00d7 prefilling time for LLaVA-NeXT 7B. (c) VisionZip reduces GPU inference time by 2\u00d7\\times\u00d7 across 11 benchmarks, enabling the LLaVA-NeXT 13B model to infer faster than the 7B model while achieving better results.", "description": "Figure 1 demonstrates the performance and efficiency gains achieved by VisionZip.  Panel (a) showcases VisionZip's superior performance compared to state-of-the-art efficient Vision-Language Models (VLMs) like FastV and SparseVLM across eleven benchmarks on the LLaVA-1.5 dataset.  Remarkably, VisionZip achieves nearly 95% of the performance using only 10% of the original tokens. Panel (b) highlights the significant reduction in prefilling time (8 times faster) for the LLaVA-NeXT 7B model.  Lastly, panel (c) illustrates that VisionZip improves inference speed by a factor of two for the LLaVA-NeXT 13B model while still outperforming the 7B model, showcasing its ability to achieve better results with improved efficiency.", "section": "3. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.04467/x2.png", "caption": "Figure 2: Redundancy Visualization. The visualization and distribution statistics of attention scores show attention concentrated on only a few tokens, while many tokens display very low attention scores, indicating significant redundancy in the visual tokens.", "description": "This figure visualizes the redundancy in visual tokens generated by popular vision encoders like CLIP and SigLIP.  The left panel shows visualizations of attention weights for CLIP and SigLIP on a single image example, demonstrating that the attention is highly concentrated on a small subset of tokens. The right panel presents the distribution statistics of attention weights across all the visual tokens in a validation dataset. This clearly shows that most tokens have very low attention weights, while only a few tokens receive high attention. This visual and statistical evidence strongly supports the claim of significant redundancy within visual tokens.", "section": "2. VisionZip"}, {"figure_path": "https://arxiv.org/html/2412.04467/x3.png", "caption": "Figure 3: Framework of VisionZip. VisionZip\u00a0 selects dominant tokens that aggregate substantial information based on visual token attention scores. Remaining tokens are merged based on semantic similarity to produce contextual tokens. \u00a0VisionZip\u00a0is a training-free method significantly reduces the number of image tokens, accelerating inference while maintaining performance. With efficient fine-tuning of the projector, even better results can be achieved with minimal performance loss compared to using the full token.", "description": "VisionZip is a novel method for improving the efficiency of Vision Language Models (VLMs) by reducing the number of visual tokens. It operates in two main steps. First, it identifies and selects \"dominant\" visual tokens that carry the most significant information, as determined by their attention scores.  Second, the remaining tokens are merged using a semantic similarity approach to create \"contextual\" tokens. This process significantly reduces the number of input tokens to the Language Model without sacrificing performance, leading to faster inference speeds.  Furthermore, fine-tuning the projector layer of the VLM can slightly improve performance, bringing it closer to the results obtained with the full set of tokens. The overall architecture is shown in the figure, highlighting VisionZip's training-free nature and its integration within a typical VLM pipeline.", "section": "2. VisionZip"}, {"figure_path": "https://arxiv.org/html/2412.04467/x4.png", "caption": "Figure 4: Performance of VisionZip on the Mini-Gemini.", "description": "The figure shows the performance comparison of VisionZip against several other methods on the Mini-Gemini model.  It demonstrates the effect of reducing the number of visual tokens on the model's accuracy across three different benchmarks (TextVQA, GQA, and POPE). The x-axis represents the number of visual tokens used, and the y-axis represents the accuracy. The lines show that VisionZip maintains a high level of accuracy even with a drastically reduced number of tokens, outperforming other efficient methods. This illustrates the efficiency and effectiveness of VisionZip in reducing the computational cost of VLMs without significant performance loss.", "section": "3. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.04467/x5.png", "caption": "Figure 5: Visualization of attention distribution across layers", "description": "This figure visualizes how the attention mechanism of a vision encoder changes across different layers of a Vision Language Model (VLM). It shows that in the initial layers, attention is broadly distributed across the entire image. However, as the network deepens, attention becomes increasingly concentrated on a small subset of tokens.  This concentration intensifies until reaching a peak around layer 23 before becoming slightly more dispersed again in the final layer.  This pattern of attention distribution illustrates how the VLM progressively aggregates visual information into a smaller number of highly informative tokens while discarding less salient features.", "section": "2. Redundancy Observation"}, {"figure_path": "https://arxiv.org/html/2412.04467/x6.png", "caption": "Figure 6: Reason of redundancy and feature misalignment", "description": "Figure 6(a) shows the gradient of the softmax function used in calculating the model's loss.  The steep rise for large z values and near-zero gradient for small z values explain how the attention mechanism concentrates on a small number of tokens, as those tokens with high attention scores will have a stronger gradient, while tokens with lower attention scores will have a weaker gradient, leading to a concentration of attention on a few dominant tokens. Figure 6(b) illustrates the feature misalignment problem. The attention heatmap for a 'man' is shown and highlights that the highest attention is not focused on the man himself, but rather on the background and less relevant areas. This shows that visual tokens from important features do not necessarily have high attention values, resulting in a misalignment where important visual information is not captured effectively.", "section": "4. Analysis and Discussion"}, {"figure_path": "https://arxiv.org/html/2412.04467/x7.png", "caption": "Table 6: Compatibility of VisionZip on various quantization levels for ScienceQA. \u2020 represents use of VisionZip.", "description": "This table presents the results of applying VisionZip with different quantization levels (8-bit and 4-bit) to the ScienceQA benchmark.  It demonstrates the compatibility of VisionZip with quantization techniques and shows that VisionZip maintains good performance even with reduced precision.  The table compares the performance (Precision and Memory Accuracy) and inference speed (Size and Time) of the full-precision model against the model using VisionZip with 8-bit and 4-bit quantization. The '\u2020' symbol indicates that VisionZip was used. This highlights the efficiency gains achievable through VisionZip without significant performance loss.", "section": "3. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.04467/x8.png", "caption": "Table 7: VisionZip boosts the 13B model\u2019s performance and efficiency over the 7B model on TextVQA. \u2020 represents use of VisionZip.", "description": "This table compares the performance and efficiency of different Vision Language Models (VLMs) on the TextVQA benchmark.  It shows the inference time (Size, Time), accuracy (Acc), and memory usage (Memory) for a 7B parameter VLM and a 13B parameter VLM.  Both baselines and the models enhanced with VisionZip (indicated by \u2020) are included for comparison.  The results demonstrate that VisionZip improves the 13B VLM's performance while making it more efficient than the 7B baseline, highlighting its advantages in optimizing both model size and processing speed.", "section": "3. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.04467/x9.png", "caption": "Figure 7: Example comparison of VisionZip and previous text-relevant method in multi-turn conversation", "description": "This figure demonstrates the effectiveness of VisionZip in multi-turn conversations compared to previous text-relevant methods for visual token selection. The example shows that previous methods struggle to maintain relevance across multiple turns because their visual token selection relies on text-based attention, leading to irrelevant information in later turns. In contrast, VisionZip, selecting tokens in a text-agnostic manner, consistently focuses on relevant visual information throughout the conversation.", "section": "2.5 Usage of VisionZip"}, {"figure_path": "https://arxiv.org/html/2412.04467/x10.png", "caption": "Figure 8: Advantage of \u00a0VisionZip\u00a0in video understanding task. With the same visual token length, using\u00a0VisionZip\u00a0allows encoding more frames, significantly enhancing the model\u2019s capacity to understand longer video sequences and capture more detailed information.", "description": "This figure demonstrates the impact of VisionZip on video understanding.  The top row shows a 3-minute video clip processed by the Video-LLaVA model. Because Video-LLaVA only processes a small number of frames, its understanding is limited. The bottom row shows the same clip processed by VisionZip. VisionZip achieves the same length in visual tokens by utilizing a significantly larger number of frames. This increased frame count allows VisionZip to capture more information and produce a more detailed and accurate understanding of the video.", "section": "3. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.04467/x11.png", "caption": "Figure 9: Visualization of Redundancy in the CLIP Model", "description": "This figure visualizes the redundancy in visual tokens generated by the CLIP model.  It displays a series of images, each overlaid with a heatmap showing the attention weights assigned to different visual tokens.  The color intensity of each point in the heatmap corresponds to the attention weight, with brighter colors indicating higher attention. This visualization demonstrates that a relatively small subset of visual tokens receive most of the attention, while the majority of tokens have very low attention weights. This highlights the presence of significant redundancy within the visual token representations produced by CLIP.", "section": "2. Redundancy Observation"}, {"figure_path": "https://arxiv.org/html/2412.04467/x12.png", "caption": "Figure 10: Visualization of Redundancy in the CLIP Model", "description": "This figure visualizes the redundancy in visual tokens generated by the CLIP model.  It shows a grid of images, each overlaid with a heatmap representing the attention weights of the visual tokens.  The heatmaps illustrate that a small number of tokens receive high attention weights, indicating that most visual tokens are redundant and do not contribute significantly to information representation.", "section": "2. Redundancy Observation"}, {"figure_path": "https://arxiv.org/html/2412.04467/x13.png", "caption": "Figure 11: Visualization of Redundancy in the SigLIP Model", "description": "This figure visualizes the redundancy of visual tokens generated by the SigLIP model.  It shows a series of images, each overlaid with a heatmap indicating the attention weights of different visual tokens.  The heatmaps demonstrate that a small number of tokens receive high attention, while most tokens receive very low attention. This indicates that a significant portion of the visual tokens are redundant and could likely be removed without affecting performance significantly.", "section": "2. Redundancy Observation"}, {"figure_path": "https://arxiv.org/html/2412.04467/x14.png", "caption": "Figure 12: Visualization of Attention Distribution Change", "description": "This figure visualizes how the attention distribution in the CLIP model changes across different layers.  In the initial layers, attention is spread broadly across the image. As the network processes the image through deeper layers, attention becomes increasingly concentrated on only a few key tokens, indicating a concentration of information into a smaller subset of tokens. The final layer shows a slightly more diffuse attention pattern, potentially due to the alignment with text tokens. This visualization helps to understand the redundancy of visual tokens because much of the information is already captured by a few dominant tokens.", "section": "2.2. Redundancy Observation"}, {"figure_path": "https://arxiv.org/html/2412.04467/x15.png", "caption": "Figure 13: Visualization of Attention Distribution Change", "description": "This figure visualizes how the attention distribution across different layers of a CLIP model changes.  It shows that in the early layers, attention is spread broadly across the image. However, as the network goes deeper, attention becomes increasingly concentrated on a smaller subset of tokens.  By the final layers, most of the attention is focused on only a few tokens, indicating that these tokens carry the majority of the information while many other tokens contain redundant information.", "section": "2.2 Redundancy Observation"}]