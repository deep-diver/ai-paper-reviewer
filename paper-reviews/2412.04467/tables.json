[{"content": "Method | GQA | MMB | MME | POPE | SQA | VQA<sup>V2</sup> | VQA<sup>Text</sup> | MMMU | SEED | MMVet | LLaVA-B | Avg.\n---|---|---|---|---|---|---|---|---|---|---|---|---\nUpper Bound, 576 Tokens (100%) |  |  |  |  |  |  |  |  |  |  |  | \nVanilla<sup>(CVPR24)</sup> | 61.9 | 64.7 | 1862 | 85.9 | 69.5 | 78.5 | 58.2 | 36.3 | 58.6 | 31.1 | 66.8 | 100%\n | 100% | 100% | 100% | 100% | 100% | 100% | 100% | 100% | 100% | 100% | 100% | \nRetain 192 Tokens (\u2193 66.7%) |  |  |  |  |  |  |  |  |  |  |  | \nFastV<sup>(ECCV24)</sup> | 52.7 | 61.2 | 1612 | 64.8 | 67.3 | 67.1 | 52.5 | 34.3 | 57.1 | 27.7 | 49.4 | 88.2%\n | 85.1% | 94.6% | 86.6% | 75.4% | 96.8% | 85.5% | 90.2% | 94.5% | 97.4% | 89.7% | 74.0% | \nSparseVLM<sup>(2024.10)</sup> | 57.6 | 62.5 | 1721 | 83.6 | 69.1 | 75.6 | 56.1 | 33.8 | 55.8 | 31.5 | 66.1 | 96.4%\n | 93.1% | 96.6% | 92.4% | 97.3% | 99.4% | 96.3% | 96.4% | 93.1% | 95.2% | 101.3% | 99.0% | \nVisionZip | 59.3 | 63.0 | 1782.6 | 85.3 | 68.9 | 76.8 | 57.3 | 36.6 | 56.4 | 31.7 | 67.7 | **98.5%**\n | 95.8% | 97.4% | 95.7% | 99.3% | 99.1% | 97.8% | 98.5% | 100.8% | 96.2% | 101.9% | 101.3% | \nVisionZip \u2021 | 60.1 | 63.4 | 1834 | 84.9 | 68.2 | 77.4 | 57.8 | 36.2 | 57.1 | 32.6 | 66.7 | **99.1%**\n | 97.1% | 98.0% | 98.5% | 98.8% | 98.1% | 98.6% | 99.3% | 99.7% | 97.4% | 104.8% | 99.9% | \nRetain 128 Tokens (\u2193 77.8%) |  |  |  |  |  |  |  |  |  |  |  | \nFastV<sup>(ECCV24)</sup> | 49.6 | 56.1 | 1490 | 59.6 | 60.2 | 61.8 | 50.6 | 34.9 | 55.9 | 28.1 | 52.0 | 83.5%\n | 80.1% | 86.7% | 80.0% | 69.4% | 86.6% | 78.7% | 86.9% | 96.1% | 95.4% | 90.9% | 77.8% | \nSparseVLM<sup>(2024.10)</sup> | 56.0 | 60.0 | 1696 | 80.5 | 67.1 | 73.8 | 54.9 | 33.8 | 53.4 | 30 | 62.7 | 93.4%\n | 90.5% | 92.7% | 91.1% | 93.7% | 96.5% | 94.0% | 94.3% | 93.1% | 91.1% | 96.5% | 93.9% | \nVisionZip | 57.6 | 62.0 | 1761.7 | 83.2 | 68.9 | 75.6 | 56.8 | 37.9 | 54.9 | 32.6 | 64.8 | **97.6%**\n | 93.1% | 95.8% | 94.6% | 96.9% | 99.1% | 96.3% | 97.6% | 104.4% | 93.7% | 104.8% | 97.6% | \nVisionZip \u2021 | 58.9 | 62.6 | 1823 | 83.7 | 68.3 | 76.6 | 57.0 | 37.3 | 55.8 | 32.9 | 64.8 | **98.4%**\n | 95.2% | 96.8% | 97.9% | 97.4% | 98.3% | 97.6% | 97.9% | 102.8% | 95.2% | 105.8% | 97.0% | \nRetain 64 Tokens (\u2193 88.9%) |  |  |  |  |  |  |  |  |  |  |  | \nFastV<sup>(ECCV24)</sup> | 46.1 | 48.0 | 1256 | 48.0 | 51.1 | 55.0 | 47.8 | 34.0 | 51.9 | 25.8 | 46.1 | 75.6%\n | 74.5% | 74.2% | 67.5% | 55.9% | 73.5% | 70.1% | 82.1% | 93.7% | 88.6% | 83.0% | 69.0% | \nSparseVLM<sup>(2024.10)</sup> | 52.7 | 56.2 | 1505 | 75.1 | 62.2 | 68.2 | 51.8 | 32.7 | 51.1 | 23.3 | 57.5 | 85.8%\n | 85.1% | 86.9% | 80.8% | 87.4% | 89.4% | 86.9% | 89.0% | 90.1% | 87.2% | 74.5% | 86.1% | \nVisionZip | 55.1 | 60.1 | 1690 | 77.0 | 69.0 | 72.4 | 55.5 | 36.2 | 52.2 | 31.7 | 62.9 | **94.0%**\n | 89.0% | 92.9% | 90.8% | 89.6% | 99.3% | 92.2% | 95.4% | 99.7% | 89.1% | 101.9% | 94.2% | \nVisionZip \u2021 | 57.0 | 61.5 | 1756 | 80.9 | 68.8 | 74.2 | 56.0 | 35.6 | 53.4 | 30.2 | 63.6 | **95.2%**\n | 92.1% | 95.1% | 94.3% | 94.2% | 99.0% | 94.5% | 96.2% | 98.1% | 91.1% | 97.1% | 95.2% | ", "caption": "Table 1: Performance of \u00a0VisionZip\u00a0on LLaVA 1.5. The vanilla number of visual tokens is 576576576576. The first line of each method shows the raw benchmark accuracy, and the second line is the proportion relative to the upper limit. The last column is the average value. VisionZip\u2021\u00a0 indicates that fine-tuning the multimodal projector with 1/101101/101 / 10 LLaVA-1.5 datasets, which takes 30 minutes for 8A800 GPU.", "description": "Table 1 presents a comprehensive comparison of VisionZip's performance against other state-of-the-art methods on the LLaVA-1.5 benchmark.  It evaluates performance across eleven different image understanding tasks.  The table shows raw accuracy scores (the first line for each method) and then shows these scores as a percentage of the baseline model's performance (the second line). Three different visual token reduction levels are evaluated (192, 128, and 64) to show how the efficiency and accuracy trade-off changes.  A special notation (VisionZip\u2021) indicates when the multimodal projector was fine-tuned using a small subset (1/10) of the LLaVA-1.5 training data, to further optimize performance with the reduced number of visual tokens. The final column presents the average accuracy score across all eleven tasks.", "section": "3. Experiments"}, {"content": "Method|GQA|MMB|MME|SQA|VQA<sup>V2</sup>|VQA<sup>Text</sup>|MMMU|Avg.\n---|---|---|---|---|---|---|---|---\nUpper Bound, 2880 Tokens **(100%)**| | | | | | | |100%\nVanilla|64.2|67.9|1842|70.2|80.1|61.3|35.1|100%\n100%|100%|100%|100%|100%|100%|100%\nRetain 640 Tokens **(\u2193 77.8%)**| | | | | | | |96.1%\nSparseVLM|60.3|65.7|1772|67.7|77.1|57.8|34.6|96.1%\n93.9%|96.8%|96.2%|96.4%|96.3%|94.3%|98.6%\nVisionZip|61.3|66.3|1787|68.1|79.1|60.2|34.7|**97.6%**\n95.5%|97.6%|97.0%|97.0%|98.8%|98.2%|98.9%\nVisionZip \u2021|62.4|65.9|1778|67.9|79.9|60.8|37.2|**98.9%**\n97.2%|97.1%|96.5%|96.7%|99.8%|99.2%|106.0%\nRetain 320 Tokens **(\u2193 88.9%)**| | | | | | | |93.3%\nSparseVLM|57.7|64.3|1694|67.3|73.4|55.9|34.4|93.3%\n89.9%|94.7%|92.0%|95.9%|91.6%|91.2%|98.0%\nVisionZip|59.3|63.1|1702|67.3|76.2|58.9|35.3|**95.0%**\n92.3%|92.9%|92.4%|95.9%|95.1%|96.1%|100.5%\nVisionZip \u2021|61.0|64.4|1770|67.5|78.4|59.3|38.0|**97.9%**\n95.0%|94.8%|96.1%|96.2%|97.9%|96.7%|108.3%\nRetain 160 Tokens **(\u2193 94.4%)**| | | | | | | |86.4%\nSparseVLM|51.2|63.1|1542|67.5|66.3|46.4|32.8|86.4%\n79.8%|92.9%|83.7%|96.2%|82.8%|75.7%|93.4%\nVisionZip|55.5|60.1|1630|68.3|71.4|56.2|36.1|**92.0%**\n86.4%|88.5%|88.5%|97.3%|89.1%|91.7%|102.8%\nVisionZip \u2021|58.2|63.9|1699|67.5|75.6|57.3|37.7|**95.5%**\n90.7%|94.1%|92.2%|96.2%|94.4%|93.5%|107.4%", "caption": "Table 2: Performance of VisionZip on LLaVA-NeXT. The vanilla number of visual tokens is 2880288028802880. For VisionZip\u2021, we use 1/101101/101 / 10 LLaVA-1.5 datasets to fine-tune the multimodal projector.", "description": "This table presents the performance of the VisionZip model on the LLaVA-NeXT benchmark for image understanding.  It compares VisionZip's performance against baseline models (Vanilla), as well as other state-of-the-art efficient VLMs such as FastV and SparseVLM. The table shows results for three different configurations of VisionZip, each using a reduced number of visual tokens (640, 320, and 160) compared to the baseline model (2880 tokens).  Results are expressed as percentages relative to the baseline's performance (100%).  The results demonstrate the effectiveness of VisionZip in maintaining performance while significantly reducing the number of visual tokens.  A version of VisionZip, denoted VisionZip\u2021, includes additional fine-tuning of the multimodal projector using a small subset (1/10) of the LLaVA-1.5 dataset to further improve performance after the reduction of visual tokens.", "section": "3. Experiments"}, {"content": "| Method | TGIF | MSVD | MSRVTT | ActivityNet | Avg |\n|---|---|---|---|---|---| \n| Video-LLaVA | 47.1 | 69.8 | 56.7 | 43.1 | 100.0% |\n| FastV | 23.1 | 38.0 | 19.3 | 30.6 | 52.1% |\n|  | 49.0% | 54.4% | 34.0% | 71.0% |  |\n| SparseVLM | 44.7 | 68.2 | 31.0 | 42.6 | 86.5% |\n|  | 94.9% | 97.7% | 54.7% | 98.8% |  |\n| VisionZip | 42.4 | 63.5 | 52.1 | 43.0 | 93.2% |\n|  | 90.0% | 91.0% | 91.9% | 99.8% |  |", "caption": "Table 3: Performance of \u00a0VisionZip\u00a0on Video-LLaVA. The original Video-LLaVa\u2019s video token number is 2048204820482048, while our VisionZip only retain the 136136136136 tokens.", "description": "This table presents the performance comparison of VisionZip against baseline methods (FastV and SparseVLM) on the Video-LLaVA benchmark.  The original Video-LLaVA model uses 2048 visual tokens.  VisionZip drastically reduces this number to only 136 tokens while aiming to maintain or improve performance. The table shows the performance of each method on four video question-answering datasets (TGIF-QA, MSVD-QA, MSRVTT-QA, and ActivityNet-QA).  The results are displayed as percentages, with the Video-LLaVA's performance serving as the 100% baseline. This demonstrates VisionZip's efficiency in reducing computational cost while preserving a high level of accuracy.", "section": "3.2. Effectiveness on Video Understanding"}, {"content": "| Token | Accuracy | \u0394 |\n|---|---|---|\n| Baseline 576\u219264 | 51.1 |  |\n| Ex1 526\u219264 | 46.4 | -9.2% |\n| Ex2 128\u219264 | 52.5 | +2.7% |", "caption": "Table 4: Efficiency analysis of \u00a0VisionZip\u00a0on LLaVA-NeXT 7B. The detailed metrics include practical total time for one A800 GPU on POPE, Prefilling time(latency). \u0394\u0394\\Deltaroman_\u0394 denotes the reduction ratio.", "description": "This table presents a detailed efficiency analysis of the VisionZip model on the LLaVA-NeXT 7B benchmark, focusing on the POPE dataset.  It compares the total inference time and the prefilling time (latency, the time to generate the first token) of VisionZip against baseline methods.  The reduction ratio (\u0394) shows how much faster VisionZip is in comparison to these methods.  The metrics provide insights into the performance improvement and efficiency gains achieved by VisionZip in reducing visual token redundancy.", "section": "3. Experiments"}, {"content": "| Precision | Memory | Acc |\n|---|---|---|\n| 7B-Full | 18,952 | 70.2 |\n| 13B-Full | 36,721 | 73.5 |\n| 13B-8bit-\u2020 | 16,632 | 70.8 |\n| 13B-4bit-\u2020 | 10,176 | 70.3 |", "caption": "Table 5: Quantitative analysis for the feature misalignment", "description": "This table presents a quantitative analysis of the impact of feature misalignment on model performance.  It demonstrates how selecting only a subset of the most important tokens, instead of using all available visual tokens, affects the model's accuracy.  The experiment compares a baseline model using all tokens against reduced-token models (various counts), highlighting the trade-off between computational efficiency and accuracy. It specifically investigates whether a text-agnostic token selection approach (like VisionZip) can overcome information loss associated with discarding many tokens. The results are shown for three different scenarios to evaluate how effective different token-reduction strategies are, showcasing the effects of feature misalignment that occurs when using a smaller set of tokens for representing the image.", "section": "4. Analysis and Discussion"}, {"content": "| Size | Time | Acc |\n|---|---|---|\n| 7B | 1,714s | 61.3 |\n| 13B | 2,516s | 64.3 |\n| 13B\u2020 | 1,246s | 62.2 |", "caption": "Table 8: Token number settings for VisionZip\u00a0 in LLaVA-1.5\u00a0[32] and Mini-Gemini\u00a0[30]", "description": "This table shows the number of dominant and contextual visual tokens used by the VisionZip model for different settings on two vision language models: LLaVA-1.5 and Mini-Gemini. Dominant tokens are the most informative tokens selected by VisionZip, while contextual tokens are generated by merging the remaining, less informative tokens.  The table shows how many dominant and contextual tokens are used with different numbers of retained total tokens.", "section": "2. VisionZip"}, {"content": "|                     | Retain 64                     |                     | Retain 128                    |                     | Retain 192                    |                     |\n| :------------------ | :-----------------------------: | :-----------------------------: | :-----------------------------: | :-----------------------------: | :-----------------------------: | :-----------------------------: |\n|                     | Dominant | Contextual | Dominant | Contextual | Dominant | Contextual |\n| LLaVA-1.5           | 54                           | 10                          | 108                          | 20                          | 162                          | 30                          |\n| Mini-Gemini         | 54                           | 10                          | 108                          | 20                          | 162                          | 30                          |", "caption": "Table 9: Token number settings for VisionZip\u00a0 in LLaVA-NeXT\u00a0[33]", "description": "This table details the number of dominant and contextual visual tokens used in the VisionZip method for the LLaVA-NeXT model [33]. It shows different configurations, with varying numbers of retained tokens, to demonstrate the flexibility of the method. For each configuration, the counts of dominant tokens (highly informative tokens) and contextual tokens (merged tokens summarizing additional details) are given. These numbers illustrate the trade-off between model efficiency and performance.", "section": "2. VisionZip"}, {"content": "|                   | Retain 160          |                   | Retain 320          |                   | Retain 640          |                   |\n| :----------------: | :------------------: | :----------------: | :------------------: | :----------------: | :------------------: | :----------------: |\n|                   | Dominant            | Contextual         | Dominant            | Contextual         | Dominant            | Contextual         |\n| **LLaVA NeXT** | 135                 | 25                  | 270                 | 50                  | 540                 | 100                |", "caption": "Table 10: Performance of \u00a0VisionZip\u00a0on LLaVA 1.5 13B. The vanilla number of visual tokens is 576576576576. The first line of each method shows the raw benchmark accuracy, and the second line is the proportion relative to the upper limit. The last column is the average value. VisionZip\u2021\u00a0indicates that fine-tuning the multimodal projector with 1/101101/101 / 10 LLaVA-1.5 datasets. SEED-I represents SEED-IMG, which uses the metric from LMMs-Eval\u00a0[64]. The Avg calculation process does not include the results from LLaVA-B and MMVet, as the benchmark is small and the results are not stable.", "description": "This table presents the performance comparison of the VisionZip method against the baseline LLaVA 1.5 13B model and other state-of-the-art methods (FastV and SparseVLM) across eleven image understanding benchmarks.  The baseline model uses 576 visual tokens. VisionZip is tested with reduced numbers of visual tokens (192, 128, and 64), both with and without fine-tuning the model's projector using a small subset of the LLaVA-1.5 dataset (indicated by \u2021).  The table shows the accuracy of each method for each benchmark, expressed as a percentage of the baseline's performance (100%).  The average accuracy across all benchmarks (excluding LLaVA-B and MMVet due to their small size and instability) is also provided for each configuration.  The SEED-I column uses the metric from the LMMs-Eval [64] benchmark.", "section": "3. Experiments, 3.1. Effectiveness on Image Understanding"}, {"content": "| Method | GQA | MMB | MME | POPE | SQA | VQA<sup>V2</sup> | VQA<sup>Text</sup> | MMMU | SEED-I | MMVet | LLaVA-B | Avg. |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| Upper Bound, 576 Tokens (100%) |  |  |  |  |  |  |  |  |  |  |  |  |\n| Vanilla<sup>(CVPR24)</sup> | 63.2 | 67.7 | 1818 | 85.9 | 72.8 | 80.0 | 61.3 | 36.4 | 66.9 | 35.3 | 70.8 | 100% |\n|  | 100% | 100% | 100% | 100% | 100% | 100% | 100% | 100% | 100% | 100% | 100% |  |\n| Retain 192 Tokens (\u2193 66.7%) |  |  |  |  |  |  |  |  |  |  |  |  |\n| VisionZip | 59.1 | 66.9 | 1754 | 85.1 | 73.5 | 78.1 | 59.5 | 36.4 | 65.2 | 37.5 | 77.5 | **97.9%** |\n|  | 93.5% | 98.8% | 96.5% | 99.1% | 101.0% | 97.6% | 97.1% | 100% | 97.5% | 106.2% | 109.5% |  |\n| VisionZip \u2021 | 61.6 | 67.1 | 1790 | 84.5 | 72.7 | 78.6 | 59.9 | 36.4 | 66.1 | 37.7 | 73.9 | **98.7%** |\n|  | 97.5% | 99.1% | 98.5% | 98.4% | 99.9% | 98.3% | 97.7% | 100% | 98.8% | 106.7% | 104.3% |  |\n| Retain 128 Tokens (\u2193 77.8%) |  |  |  |  |  |  |  |  |  |  |  |  |\n| VisionZip | 57.9 | 66.7 | 1743 | 85.2 | 74.0 | 76.8 | 58.7 | 36.1 | 63.8 | 37.5 | 70.8 | **97.0%** |\n|  | 91.6% | 98.5% | 95.9% | 99.2% | 101.6% | 96.0% | 95.8% | 99.2% | 95.4% | 106.2% | 100% |  |\n| VisionZip \u2021 | 60.1 | 67.6 | 1736 | 83.8 | 73.0 | 77.6 | 59.2 | 35.4 | 64.9 | 38.3 | 72.3 | **97.4%** |\n|  | 95.1% | 99.9% | 95.5% | 97.6% | 100.2% | 97.0% | 96.6% | 97.3% | 97.0% | 108.5% | 102.1% |  |\n| Retain 64 Tokens (\u2193 88.9%) |  |  |  |  |  |  |  |  |  |  |  |  |\n| VisionZip | 56.2 | 64.9 | 1676 | 76.0 | 74.4 | 73.7 | 57.4 | 36.4 | 60.4 | 33.9 | 70.3 | **93.7%** |\n|  | 88.9% | 95.9% | 92.2% | 88.5% | 102.2% | 92.1% | 93.3% | 100% | 90.3% | 96.0% | 99.3% |  |\n| VisionZip \u2021 | 58.1 | 65.6 | 1671 | 81.6 | 72.3 | 75.2 | 58.5 | 35.3 | 61.4 | 36.7 | 68.7 | **94.8%** |\n|  | 91.9% | 96.9% | 91.9% | 95.0% | 99.3% | 94.0% | 95.4% | 97.0% | 91.8% | 104.0% | 97.0% |  |", "caption": "Table 11: Using \u00a0VisionZip\u00a0train the LLaVA 1.5 7B. The vanilla number of visual tokens is 576576576576. The first line of each method shows the raw benchmark accuracy, and the second line is the proportion relative to the upper limit. The last column is the average value. VisionZip\u2021\u00a0 indicates that fine-tuning the multimodal projector with 1/101101/101 / 10 LLaVA-1.5 datasets. The Avg calculation process does not include the results from LLaVA-B and MMVet, as the benchmark is small and the results are not stable.", "description": "Table 11 presents a comprehensive evaluation of VisionZip's performance when integrated into the training process of the LLaVA 1.5 7B model.  It compares the model's accuracy across eleven benchmark datasets, varying the number of visual tokens used (192, 128, and 64) while using VisionZip in the training stage.  The table shows both the raw accuracy scores and the accuracy relative to a baseline model using the full 576 visual tokens (100%). A separate row also displays the results when fine-tuning the multimodal projector of VisionZip using a subset (1/10) of the LLaVA-1.5 dataset.  Note that two benchmarks, LLaVA-B and MMVet, are excluded from the average accuracy calculation due to their smaller size and reported instability of results.", "section": "3. Experiments"}, {"content": "| Method | GQA | MMB | MME | POPE | SQA | VQA<sup>V2</sup> | VQA<sup>Text</sup> | MMMU | SEED | MMVet | VizWiz | LLaVA-B | Avg. |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| Vanilla (CVPR24) | 61.9 | 64.7 | 1862 | 85.9 | 69.5 | 78.5 | 58.2 | 36.3 | 58.6 | 31.1 | 50.0 | 66.8 | 100% |\n|  | 100% | 100% | 100% | 100% | 100% | 100% | 100% | 100% | 100% | 100% | 100% | 100% |  |\n| VisionZip 192 Tokens | 61.5 | 67.4 | 1820 | 85.2 | 69.3 | 78.5 | 57.8 | 36.1 | 59.6 | 33 | 52.6 | 71.3 | **100.6%** |\n|  | 99.4% | 104.2% | 97.7% | 99.2% | 99.7% | 100% | 99.3% | 99.4% | 101.7% | 106.1% | 105.2 | 106.7% |  |\n| VisionZip 128 Tokens | 60.0 | 66.6 | 1814 | 84.3 | 69.4 | 77.8 | 57.6 | 36.9 | 59.0 | 31.4 | 49.9 | 66.7 | **99.6%** |\n|  | 96.9% | 102.9% | 97.4% | 98.1% | 99.9% | 99.1% | 99.0% | 101.7% | 100.7% | 101% | 99.8% | 99.9% |  |\n| VisionZip 64 Tokens | 58.9 | 63.7 | 1785 | 84.1 | 69.3 | 76.0 | 57.1 | 36.2 | 55.8 | 29.9 | 46.8 | 63.5 | **97.1%** |\n|  | 95.2% | 98.5% | 95.9% | 97.9% | 99.7% | 96.8% | 98.1% | 99.7% | 95.2% | 96.1% | 93.6% | 95.1% |  |", "caption": "Table 12: Performance of \u00a0VisionZip\u00a0on LLaVA NeXT 7B. The vanilla number of visual tokens is 2880288028802880. The first line of each method shows the raw benchmark accuracy, and the second line is the proportion relative to the upper limit. The last column is the average value. VisionZip\u2021\u00a0indicates that fine-tuning the multimodal projector with 1/101101/101 / 10 LLaVA-1.5 datasets. SEED-I represents SEED-IMG, which uses the metric from LMMs-Eval\u00a0[64].", "description": "This table presents the performance comparison of VisionZip against baseline and other state-of-the-art methods on the LLaVA-NeXT 7B model for image understanding tasks.  It shows the performance (accuracy) for each method across various benchmarks,  with results presented as both raw scores and percentages relative to a baseline with the full number of visual tokens (2880).  Different configurations of VisionZip, using reduced numbers of visual tokens (640, 320, and 160), are shown.  The table also indicates where the VisionZip results include fine-tuning of the model's multimodal projector using a subset of the LLaVA-1.5 dataset. Finally, an average performance across all benchmarks is provided.", "section": "3. Experiments"}, {"content": "| Method | GQA | MMB | MME | POPE | SQA | VQA<sup>V2</sup> | VQA<sup>Text</sup> | MMMU | SEED-I | Avg. |\n|---|---|---|---|---|---|---|---|---|---|---|\n| Upper Bound, 2880 Tokens (100%) |  |  |  |  |  |  |  |  |  | 100% |\n| Vanilla | 64.2 | 67.9 | 1842 | 86.4 | 70.2 | 80.1 | 61.3 | 35.1 | 70.2 | 100% |\n|  | 100% | 100% | 100% | 100% | 100% | 100% | 100% | 100% | 100% |  |\n| Retain 640 Tokens (\u2193 77.8%) |  |  |  |  |  |  |  |  |  |  |\n| VisionZip | 61.3 | 66.3 | 1787 | 86.3 | 68.1 | 79.1 | 60.2 | 34.7 | 66.7 | 97.5% |\n|  | 95.5% | 97.6% | 97.0% | 99.9% | 97.0% | 98.8% | 98.2% | 98.9% | 95.0% |  |\n| VisionZip \u2021 | 62.4 | 65.9 | 1778 | 87.6 | 67.9 | 79.9 | 60.8 | 37.2 | 67.8 | 98.9% |\n|  | 97.2% | 97.1% | 96.5% | 101.4% | 96.7% | 99.8% | 99.2% | 106.0% | 96.6% |  |\n| Retain 320 Tokens (\u2193 88.9%) |  |  |  |  |  |  |  |  |  |  |\n| VisionZip | 59.3 | 63.1 | 1702 | 82.1 | 67.3 | 76.2 | 58.9 | 35.3 | 63.4 | 94.5% |\n|  | 92.3% | 92.9% | 92.4% | 95.0% | 95.9% | 95.1% | 96.1% | 100.5% | 90.3% |  |\n| VisionZip \u2021 | 61.0 | 64.4 | 1770 | 86.2 | 67.5 | 78.4 | 59.3 | 38.0 | 65.9 | 97.6% |\n|  | 95.0% | 94.8% | 96.1% | 99.8% | 96.2% | 97.9% | 96.7% | 108.3% | 93.9% |  |\n| Retain 160 Tokens (\u2193 94.4%) |  |  |  |  |  |  |  |  |  |  |\n| VisionZip | 55.5 | 60.1 | 1630 | 74.8 | 68.3 | 71.4 | 56.2 | 36.1 | 58.3 | 91.5% |\n|  | 86.4% | 88.5% | 88.5% | 86.6% | 97.3% | 89.1% | 91.7% | 102.8% | 83.0% |  |\n| VisionZip \u2021 | 58.2 | 63.9 | 1699 | 83.4 | 67.5 | 75.6 | 57.3 | 37.7 | 62.9 | 95.0% |\n|  | 90.7% | 94.1% | 92.2% | 96.5% | 96.2% | 94.4% | 93.5% | 107.4% | 89.6% |  |", "caption": "Table 13: Performance of \u00a0VisionZip\u00a0on LLaVA NeXT 13B. The vanilla number of visual tokens is 2880288028802880. The first line of each method shows the raw benchmark accuracy, and the second line is the proportion relative to the upper limit. The last column is the average value. VisionZip\u2021\u00a0indicates that fine-tuning the multimodal projector with 1/101101/101 / 10 LLaVA-1.5 datasets. SEED-I represents SEED-IMG, which uses the metric from LMMs-Eval\u00a0[64].", "description": "Table 13 presents the performance comparison of the VisionZip method on the LLaVA-NeXT 13B model for image understanding tasks.  It evaluates the model's performance with varying numbers of visual tokens (2880, 640, 320, and 160) against the baseline model. The table shows both raw accuracy and the percentage relative to the baseline's performance. VisionZip\u2021 denotes results obtained after fine-tuning the model's multimodal projector using a subset (1/10) of the LLaVA-1.5 dataset.  SEED-I refers to the SEED-IMG benchmark which uses metrics from LMMs-Eval [64]. The average score across all benchmarks is also included.", "section": "3. Experiments"}, {"content": "| Method | GQA | MMB | MME | POPE | SQA | VQA<sup>V2</sup> | VQA<sup>Text</sup> | MMMU | SEED-I | Avg. |\n|---|---|---|---|---|---|---|---|---|---|---|\n| Upper Bound, 2880 Tokens (100%) |  |  |  |  |  |  |  |  |  |  |\n| Vanilla 13B | 65.4 | 70.0 | 1901 | 86.2 | 73.5 | 81.8 | 64.3 | 36.2 | 71.9 | 100% |\n|  | 100% | 100% | 100% | 100% | 100% | 100% | 100% | 100% | 100% |  |\n| Vanilla 7B | 64.2 | 67.9 | 1842 | 86.4 | 70.2 | 80.1 | 61.3 | 35.1 | 70.2 | 97.2% |\n|  | 98.2% | 96.3% | 96.9% | 100.2% | 95.5% | 97.9% | 95.3% | 97.0% | 97.6% |  |\n| Retain 640 Tokens (\u2193 77.8%) |  |  |  |  |  |  |  |  |  |  |\n| VisionZip | 63.0 | 68.6 | 1871 | 85.7 | 71.2 | 79.7 | 62.2 | 36.4 | 68.8 | 97.5% |\n|  | 96.3% | 98.0% | 98.4% | 99.4% | 96.7% | 96.9% | 96.7% | 100.5% | 95.7% |  |\n| VisionZip \u2021 | 63.7 | 66.6 | 1829 | 86.3 | 73.2 | 81.2 | 64.4 | 38.1 | 69.2 | 98.8% |\n|  | 97.4% | 95.1% | 96.2% | 100.1% | 99.6% | 99.3% | 100.2% | 105.2% | 96.2% |  |\n| Retain 320 Tokens (\u2193 88.9%) |  |  |  |  |  |  |  |  |  |  |\n| VisionZip | 60.7 | 67.2 | 1805 | 82.0 | 70.3 | 76.8 | 60.9 | 35.6 | 65.2 | 94.7% |\n|  | 92.8% | 96.0% | 95.0% | 95.1% | 95.6% | 93.9% | 94.7% | 98.3% | 90.7% |  |\n| VisionZip \u2021 | 62.5 | 66.9 | 1861 | 85.7 | 72.7 | 80.0 | 63.2 | 36.9 | 67.9 | 97.8% |\n|  | 95.6% | 95.6% | 97.9% | 99.4% | 98.9% | 97.8% | 98.3% | 101.9% | 94.4% |  |\n| Retain 160 Tokens (\u2193 94.4%) |  |  |  |  |  |  |  |  |  |  |\n| VisionZip | 57.8 | 64.9 | 1739 | 76.6 | 69.3 | 72.4 | 58.4 | 37.0 | 61.1 | 91.3% |\n|  | 88.4% | 92.7% | 91.5% | 88.9% | 94.3% | 88.5% | 90.8% | 102.2% | 84.8% |  |\n| VisionZip \u2021 | 59.7 | 65.3 | 1766 | 84.0 | 72.0 | 77.6 | 60.8 | 36.0 | 64.4 | 94.6% |\n|  | 91.3% | 93.3% | 92.9% | 97.4% | 98.0% | 94.9% | 94.6% | 99.4% | 89.6% |  |", "caption": "Table 14: Performance of \u00a0VisionZip\u00a0on mini-Gemini 7B. The vanilla number of visual tokens is 576576576576. The first line of each method shows the raw benchmark accuracy, and the second line is the proportion relative to the upper limit. The last column is the average value. VisionZip\u2021\u00a0indicates that fine-tuning the multimodal projector with 1/101101/101 / 10 LLaVA-1.5 datasets. SEED-I represents SEED-IMG, which uses the metric from LMMs-Eval\u00a0[64].", "description": "Table 14 presents the performance comparison of VisionZip against several baseline methods on the Mini-Gemini 7B model for image understanding tasks.  The model's performance is evaluated across multiple benchmarks, with the original model's performance on all benchmarks considered as 100%. VisionZip is tested under three different configurations of retained visual tokens (192, 128, and 64), showing its performance with and without fine-tuning of the multimodal projector using a subset of the LLaVA-1.5 dataset (indicated by VisionZip\u2021).  Each row represents a method, with the first line showing the raw accuracy and the second line showing the percentage relative to the baseline model. The final column provides the average performance across all benchmarks.", "section": "3. Experiments"}, {"content": "| Method | GQA | MMB | MME | POPE | SQA | VQA<sup>V2</sup> | VQA<sup>Text</sup> | MMMU | SEED-I | Avg. |\n|---|---|---|---|---|---|---|---|---|---|---|\n| Upper Bound, 576 Tokens (100%) |  |  |  |  |  |  |  |  |  |  |\n| Vanilla 7B | 62.4 | 69.3 | 1841 | 85.8 | 70.7 | 80.4 | 65.2 | 36.1 | 69.7 | 100% |\n|  | 100% | 100% | 100% | 100% | 100% | 100% | 100% | 100% | 100% |  |\n| Retain 192 Tokens (\u2193 66.7%) |  |  |  |  |  |  |  |  |  |  |\n| VisionZip | 60.3 | 68.9 | 1846 | 82.3 | 70.1 | 79.1 | 63.4 | 36.1 | 67.5 | **98.2%** |\n|  | 96.6% | 99.4% | 100.2% | 95.9% | 99.2% | 98.4% | 97.2% | 100% | 96.8% |  |\n| VisionZip \u2021 | 61.6 | 67.2 | 1804 | 85.5 | 70.2 | 78.9 | 63.6 | 36.1 | 67.0 | **98.3%** |\n|  | 98.7% | 97.0% | 98.0% | 99.7% | 99.3% | 98.1% | 97.5% | 100% | 96.1% |  |\n| Retain 128 Tokens (\u2193 77.8%) |  |  |  |  |  |  |  |  |  |  |\n| VisionZip | 58.7 | 68.1 | 1841 | 78.5 | 70.0 | 77.5 | 61.3 | 34.8 | 65.6 | **96.0%** |\n|  | 94.1% | 98.3% | 100% | 91.5% | 99.0% | 96.4% | 94.0% | 96.4% | 94.1% |  |\n| VisionZip \u2021 | 60.0 | 67.0 | 1810 | 83.2 | 70.1 | 78.3 | 61.6 | 34.8 | 65.9 | **96.7%** |\n|  | 96.2% | 96.7% | 98.3% | 97.0% | 99.2% | 97.4% | 94.5% | 96.4% | 94.5% |  |\n| Retain 64 Tokens (\u2193 88.9%) |  |  |  |  |  |  |  |  |  |  |\n| VisionZip | 55.8 | 65.9 | 1737 | 69.6 | 70.7 | 73.9 | 59.1 | 35.6 | 61.7 | **92.2%** |\n|  | 89.4% | 95.1% | 94.4% | 81.4% | 100% | 91.9% | 90.6% | 98.6% | 88.5% |  |\n| VisionZip \u2021 | 57.7 | 66.3 | 1779 | 80.0 | 71.0 | 75.9 | 60.1 | 36.2 | 62.6 | **95.0%** |\n|  | 92.5% | 95.7% | 96.6% | 93.2% | 100.4% | 94.4% | 92.2% | 100.3% | 89.8% |  |", "caption": "Table 15: Impact of Fine-Tuning Dataset Compatibility. The first column indicates which dataset was used to sample 1/10 of the data for fine-tuning the multimodality projector.", "description": "This table presents an ablation study analyzing the impact of using different fine-tuning datasets on the performance of VisionZip.  It shows the results of fine-tuning the multi-modal projector with 1/10th of either the LLaVA-1.5 or LLaVA-NeXT datasets, for three different visual token counts (640, 320, and 160).  The goal is to determine the effect of dataset compatibility on VisionZip's performance when reducing the number of visual tokens.", "section": "3. Experiments"}, {"content": "| Dataset | GQA | MMB | MME | SQA | VQA<sup>V2</sup> | VQA<sup>Text</sup> | MMMU | Avg. |\n|---|---|---|---|---|---|---|---|---|\n| *Retain 640 Tokens* **(**\u2193 77.8%**) |  |  |  |  |  |  |  |  |\n| LLaVA-1.5 | 62.4 | 65.9 | 1778 | 67.9 | 79.9 | 60.8 | 37.2 | 98.9% |\n| LLaVA-NeXT | 63.0 | 66.8 | 1738 | 68.4 | 80.1 | 61.2 | 38.8 | 99.3% |\n| *Retain 320 Tokens* **(**\u2193 88.9%**) |  |  |  |  |  |  |  |  |\n| LLaVA-1.5 | 61.0 | 64.4 | 1770 | 67.5 | 78.4 | 59.3 | 38.0 | 97.6% |\n| LLaVA-NeXT | 61.6 | 64.7 | 1771 | 67.5 | 78.8 | 60.1 | 36.3 | 97.3% |\n| *Retain 160 Tokens* **(**\u2193 94.4%**) |  |  |  |  |  |  |  |  |\n| LLaVA-1.5 | 58.2 | 63.9 | 1699 | 67.5 | 75.6 | 57.3 | 37.7 | 95.2% |\n| LLaVA-NeXT | 58.4 | 63.2 | 1763 | 68.0 | 76.0 | 58.2 | 36.9 | 95.7% |", "caption": "Table 16: Performance and Memory of \u00a0VisionZip\u00a0on LLaVA NeXT 13B with the Quantization. The vanilla number of visual tokens is 2880288028802880. The first line of each method shows the raw benchmark accuracy, and the second line is the proportion relative to the upper limit. The last column is the average value. SEED-I represents SEED-IMG, which uses the metric from LMMs-Eval\u00a0[64]. The memory refers to the practical CUDA memory usage on a single Nvidia A800 GPU for SQA.", "description": "This table presents a performance comparison of different vision models on the LLaVA-NeXT 13B benchmark.  It shows the results of using VisionZip (with and without quantization) against a baseline vanilla model and other relevant efficient VLMs. The raw accuracy, accuracy relative to the baseline (100%), and average accuracy are provided across multiple benchmark tasks. CUDA memory usage for a single NVIDIA A800 GPU on the SQA task is also included for comparison.  The table highlights the tradeoff between accuracy and efficiency when using VisionZip for reduction in visual tokens.", "section": "3. Experiments"}, {"content": "| Method | Memory | GQA | MMB | MME | POPE | SQA | VQA<sup>V2</sup> | VQA<sup>Text</sup> | MMMU | SEED-I | Avg. |\n|---|---|---|---|---|---|---|---|---|---|---|---| \n| Upper Bound, 2880 Tokens (100%) |  |  |  |  |  |  |  |  |  |  |  |\n| Vanilla 13B | 36721Mb | 65.4 | 70.0 | 1901 | 86.2 | 73.5 | 81.8 | 64.3 | 36.2 | 71.9 | 100% |\n|  | 100% | 100% | 100% | 100% | 100% | 100% | 100% | 100% | 100% | 100% |\n| Vanilla 7B | 18952Mb | 64.2 | 67.9 | 1842 | 86.4 | 70.2 | 80.1 | 61.3 | 35.1 | 70.2 | 97.2% |\n|  | 98.2% | 96.3% | 96.9% | 100.2% | 95.5% | 97.9% | 95.3% | 97.0% | 97.6% |\n| Retain 320 Tokens (\u2193 88.9%) |  |  |  |  |  |  |  |  |  |  |  |\n| VisionZip | 28810Mb | 60.7 | 67.2 | 1805 | 82.0 | 70.3 | 76.8 | 60.9 | 35.6 | 65.2 | 94.7% |\n|  | 92.8% | 96.0% | 95.0% | 95.1% | 95.6% | 93.9% | 94.7% | 98.3% | 90.7% |\n| VisionZip-8bit | 16632Mb | 60.6 | 67.1 | 1798 | 81.4 | 70.8 | 76.8 | 60.5 | 37.0 | 65.4 | 95.0% |\n|  | 92.7% | 95.9% | 94.6% | 94.4% | 96.3% | 93.9% | 94.1% | 102.2% | 91.0% |\n| VisionZip-4bit | 10176Mb | 60.3 | 65.1 | 1773 | 82.1 | 70.3 | 76.6 | 60.0 | 36.1 | 65.1 | 94.0% |\n|  | 92.2% | 93.0% | 93.3% | 95.2% | 95.6% | 93.6% | 93.3% | 99.7% | 90.5% |", "caption": "Table 17: Performance and Training Time of \u00a0VisionZip\u00a0on LLaVA NeXT 7B. The vanilla number of visual tokens is 2880288028802880. The first line of each method shows the raw benchmark accuracy, and the second line is the proportion relative to the upper limit. The last column is the average value. SEED-I represents SEED-IMG, which uses the metric from LMMs-Eval\u00a0[64]. The time refers to the practical Training time usage on 8 Nvidia A800 GPUs for training.", "description": "This table presents a comparison of the performance and training time of the VisionZip model against a baseline LLaVA-NeXT 7B model across multiple benchmarks.  It shows the impact of VisionZip's visual token reduction strategy on accuracy and training efficiency.  The results are shown for different numbers of retained visual tokens (640, 320, and 160).  The percentage accuracy relative to a baseline (full 2880 tokens) is displayed, alongside the training time in hours.  The average performance across all benchmarks is provided for each model.  The experiment was conducted on 8 Nvidia A800 GPUs.", "section": "3. Experiments"}, {"content": "| Method | Time | Memory | GQA | MMB | MME | POPE | SQA | VQA<sup>V2</sup> | VQA<sup>Text</sup> | MMMU | SEED-I | Avg. |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| Upper Bound, 2880 Tokens (100%) |  |  |  |  |  |  |  |  |  |  |  |  |\n| Vanilla 7B | 33.8h | 63558Mb | 64.2 | 67.9 | 1842 | 86.4 | 70.2 | 80.1 | 61.3 | 35.1 | 70.2 | 100% |\n|  |  |  | 100% | 100% | 100% | 100% | 100% | 100% | 100% | 100% | 100% |  |\n| Retain 640 Tokens (\u2193 77.8%) |  |  |  |  |  |  |  |  |  |  |  |  |\n| VisionZip-Inference |  |  | 61.3 | 66.3 | 1787 | 86.3 | 68.1 | 79.1 | 60.2 | 34.7 | 66.7 | **97.5%** |\n|  |  |  | 95.5% | 97.6% | 97.0% | 99.9% | 97.0% | 98.8% | 98.2% | 98.9% | 95.0% |  |\n| VisionZip-Train | 15.9h | 35326Mb | 62.5 | 67.1 | 1728 | 86.0 | 70.2 | 80.6 | 64.1 | 35.1 | 67.8 | **99.0%** |\n|  |  |  | 97.4% | 98.8% | 93.8% | 99.5% | 100% | 100.6% | 104.6% | 100% | 96.6% |  |", "caption": "Table 18: Performance of \u00a0VisionZip\u00a0on LLaVA NeXT 13B. The vanilla number of visual tokens is 2880288028802880. The first line of each method shows the raw benchmark accuracy, and the second line is the proportion relative to the upper limit. The last column is the average value. \u201cPrefilling\u201d represents the prefilling time, and \u201cTotal\u201d represents the actual testing time of the model on the TextVQA benchmark.", "description": "This table presents the performance of the VisionZip method on the LLaVA-NeXT 13B model for image understanding tasks.  It compares the performance of VisionZip with different numbers of visual tokens (640, 320, 160) against the baseline (full 2880 tokens) and other state-of-the-art methods. The results are shown as both raw accuracy and the percentage relative to the full-token baseline.  Additionally, the table includes prefilling time (latency to generate the first token) and total inference time on the TextVQA benchmark, highlighting the efficiency gains achieved by VisionZip.", "section": "3. Experiments"}]