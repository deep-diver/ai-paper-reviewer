{"references": [{"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-01", "reason": "This paper introduces CLIP, a foundational vision-language model used extensively in the VisionZip paper's experiments and analysis."}, {"fullname_first_author": "Haotian Liu", "paper_title": "LLaVA: Large language and vision assistant", "publication_date": "2023-10-26", "reason": "LLaVA is a key VLM benchmark used for evaluating VisionZip's performance, highlighting its importance in the field."}, {"fullname_first_author": "Haotian Liu", "paper_title": "LLaVA-Next: Improved reasoning, OCR, and world knowledge", "publication_date": "2024-01-01", "reason": "This paper introduces LLaVA-Next, another significant VLM used for evaluating VisionZip, showcasing its impact on the field."}, {"fullname_first_author": "Jinze Bai", "paper_title": "Qwen-VL: A frontier large vision-language model with versatile abilities", "publication_date": "2023-08-01", "reason": "Qwen-VL is a large vision-language model used in VisionZip's experiments, highlighting the importance of similar models in the field."}, {"fullname_first_author": "Liang Chen", "paper_title": "An image is worth 1/2 tokens after layer 2: Plug-and-play inference acceleration for large vision-language models", "publication_date": "2024-01-01", "reason": "This paper introduces a state-of-the-art efficient VLM, FastV, which is a key comparative method to evaluate VisionZip's efficiency and effectiveness."}]}