{"references": [{" publication_date": "2020", "fullname_first_author": "Dimo Angelov", "paper_title": "Top2vec: Distributed representations of topics", "reason": "This paper introduces Top2Vec, a novel method for distributed representations of topics, which is relevant to the current research due to its focus on semantic relationships between words and sentences.  The ability to effectively capture semantic relationships is crucial for the proposed Meta-Chunking approach, as it aims to identify logically connected sentences within paragraphs. Therefore, Top2Vec provides a foundational technique that aligns with and supports the goals of Meta-Chunking.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Yushi Bai", "paper_title": "Longbench: A bilingual, multitask benchmark for long context understanding", "reason": "This paper introduces LongBench, a comprehensive benchmark designed to evaluate the performance of large language models (LLMs) on long-context tasks. This is highly relevant to the current research as Meta-Chunking deals with long documents and the ability of LLMs to handle long contexts significantly impacts the effectiveness of the Meta-Chunking strategies.  Therefore, LongBench serves as a crucial benchmark for evaluating the performance of Meta-Chunking in practical scenarios.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Bhagyashree Vyankatrao Barde", "paper_title": "An overview of topic modeling methods and tools", "reason": "This paper offers a comprehensive overview of topic modeling methods and tools.  Topic modeling techniques, such as Latent Dirichlet Allocation (LDA), play an essential role in understanding the semantic structure of documents, which is important for the proposed Meta-Chunking approach.  By offering a broader understanding of existing methods, the paper strengthens the context and justification for the Meta-Chunking approach.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Maciej Besta", "paper_title": "Multi-head rag: Solving multi-aspect problems with llms", "reason": "This paper discusses the use of LLMs in retrieval-augmented generation (RAG) systems for solving multi-aspect problems.  The concept of dealing with multiple aspects is directly relevant to the Meta-Chunking approach, where the method aims to improve the quality of text segmentation and information retrieval by considering logical relationships between sentences and paragraphs. This makes the paper an important reference.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Stella Biderman", "paper_title": "Pythia: A suite for analyzing large language models across training and scaling", "reason": "This paper introduces Pythia, a comprehensive suite for analyzing large language models (LLMs). This is highly relevant to the current research as Meta-Chunking relies on LLMs for its core functionalities. By providing a detailed analysis of various LLM aspects, Pythia helps in understanding the strengths and limitations of LLMs used for text chunking.  This context is vital to the understanding of the Meta-Chunking strategy.", "section_number": 3}, {" publication_date": "2003", "fullname_first_author": "David M Blei", "paper_title": "Latent dirichlet allocation", "reason": "This seminal paper introduces Latent Dirichlet Allocation (LDA), a highly influential topic modeling technique widely used in natural language processing.  LDA's capability to discover underlying semantic structures in text data is relevant to Meta-Chunking, as it is fundamental to the understanding and organization of textual information for improved segmentation. Therefore, understanding LDA is critical for appreciating the context and novelty of Meta-Chunking.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Zheng Cai", "paper_title": "Internlm2 technical report", "reason": "This technical report provides detailed information on Internlm2, a large language model (LLM) used in the experiments for Meta-Chunking.  Understanding the characteristics of this specific LLM is crucial for interpreting the experimental results and analyzing the performance of Meta-Chunking across different LLM models. Thus, the technical report provides critical information about a key component of the study.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Tong Chen", "paper_title": "Dense x retrieval: What retrieval granularity should we use?", "reason": "This paper explores different retrieval granularities in information retrieval systems. The discussion of retrieval granularity is highly relevant to Meta-Chunking, which aims to improve the efficiency and effectiveness of Retrieval-Augmented Generation (RAG) systems by optimizing text segmentation granularity. This paper provides a broader context for the Meta-Chunking approach by addressing related challenges in information retrieval.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Yuyan Chen", "paper_title": "Hallucination detection: Robustly discerning reliable answers in large language models", "reason": "This paper focuses on detecting hallucinations in large language models (LLMs), which is a crucial problem addressed by Retrieval-Augmented Generation (RAG). Meta-Chunking aims to improve the quality of RAG responses by optimizing text segmentation, and this paper highlights the impact of hallucinations on RAG\u2019s effectiveness, contextualizing the importance and potential benefits of Meta-Chunking.", "section_number": 1}, {" publication_date": "1997", "fullname_first_author": "SS Dragomir", "paper_title": "Some bounds on entropy measures in information theory", "reason": "This paper provides mathematical bounds on entropy measures, which is directly relevant to the theoretical analysis of the Perplexity Chunking strategy in the proposed Meta-Chunking method. The paper provides key mathematical tools and insights for proving the crucial properties of entropy and cross-entropy in the theoretical analysis, strengthening the methodological rigor of the research.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Andr\u00e9 V Duarte", "paper_title": "Lumberchunker: Long-form narrative document segmentation", "reason": "This paper introduces LumberChunker, a novel approach to text chunking using LLMs, specifically designed for long-form narrative texts. This work directly addresses the limitations of traditional text chunking methods and thus provides a relevant comparison point and context for the novel Meta-Chunking approach proposed in this paper. Thus, a comparison strengthens the justification for the approach.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Robert Friel", "paper_title": "Ragbench: Explainable benchmark for retrieval-augmented generation systems", "reason": "This paper presents RAGBench, a benchmark specifically designed for evaluating retrieval-augmented generation (RAG) systems.  The proposed Meta-Chunking approach is applied within the context of RAG systems, making RAGBench a highly relevant benchmark for validating and comparing the performance of Meta-Chunking against other RAG approaches. Therefore, it serves as an important validation tool for the current work.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Yunfan Gao", "paper_title": "Retrieval-augmented generation for large language models: A survey", "reason": "This paper provides a comprehensive survey of retrieval-augmented generation (RAG) for large language models (LLMs).  The study of Meta-Chunking directly relates to RAG systems, and this survey helps contextualize the research by summarizing the existing state-of-the-art, highlighting the advancements and challenges in RAG techniques, strengthening the overall context and providing valuable background knowledge.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Maarten Grootendorst", "paper_title": "Bertopic: Neural topic modeling with a class-based tf-idf procedure", "reason": "This paper introduces BERTopic, a neural topic modeling technique that is relevant to the current research because of its focus on topic modeling and clustering, which are foundational to understanding the semantic structure of documents.  Meta-Chunking's aim to capture logical connections between sentences benefits from a solid understanding of underlying semantic structures in the text, which makes this paper highly relevant.", "section_number": 5}, {" publication_date": "2020", "fullname_first_author": "Kelvin Guu", "paper_title": "Retrieval augmented language model pre-training", "reason": "This paper introduces retrieval augmented language model pre-training, which is highly relevant to the current research due to its focus on improving the performance of large language models (LLMs) by augmenting them with retrieved information.  Meta-Chunking directly relates to this concept as it aims to improve the quality and efficiency of information retrieval in LLM-based systems.", "section_number": 5}, {" publication_date": "2022", "fullname_first_author": "Hangfeng He", "paper_title": "Rethinking with retrieval: Faithful large language model inference", "reason": "This paper explores the use of retrieval in improving the faithfulness of large language model (LLM) inference.  The proposed Meta-Chunking approach directly impacts the quality and accuracy of information retrieval in RAG systems, therefore, this work provides critical context on how to ensure faithfulness in information retrieval, which is central to the goals of Meta-Chunking.", "section_number": 1}, {" publication_date": "1999", "fullname_first_author": "Thomas Hofmann", "paper_title": "Probabilistic latent semantic analysis", "reason": "This paper introduces Probabilistic Latent Semantic Analysis (PLSA), a foundational topic modeling technique that is highly relevant to the current research due to its focus on uncovering the underlying semantic structure of text data.  The ability to understand and capture the underlying semantic structure is crucial for the proposed Meta-Chunking approach, as it seeks to identify logically coherent chunks of text.  Therefore, PLSA\u2019s impact on understanding semantic relationships is directly relevant to the context and justification of Meta-Chunking.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Huiqiang Jiang", "paper_title": "Longllmlingua: Accelerating and enhancing llms in long context scenarios via prompt compression", "reason": "This paper explores techniques for accelerating and enhancing large language models (LLMs) in long-context scenarios.  Meta-Chunking deals with long documents and the ability of LLMs to efficiently process long contexts is critical to the performance of the proposed method. This makes the research in this paper directly relevant to the current work, as it explores complementary methods to address similar challenges in LLM performance.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Youna Kim", "paper_title": "Adaptive contrastive decoding in retrieval-augmented generation for handling noisy contexts", "reason": "This paper proposes adaptive contrastive decoding for handling noisy contexts in retrieval-augmented generation. This is highly relevant to the current research as Meta-Chunking deals with the problem of improving the quality of text segmentation and information retrieval in noisy datasets. The work addresses related challenges and offers valuable insights into how to deal with noisy datasets within the context of retrieval-augmented generation.  Thus, it provides valuable context and potential solutions.", "section_number": 1}]}