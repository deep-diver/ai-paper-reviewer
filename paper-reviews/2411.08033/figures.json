[{"figure_path": "https://arxiv.org/html/2411.08033/x1.png", "caption": "Figure 1: Our method generates high-quality and editable surfel Gaussians through a cascaded 3D diffusion pipeline, given single-view images or texts as the conditions.", "description": "This figure illustrates the process of 3D object generation using the GAUSSIANANYTHING method. Starting from single-view images or text descriptions as input, the method employs a cascaded 3D diffusion pipeline to produce high-quality and editable surfel Gaussians.  The pipeline involves several stages, shown in the image as a flow chart,  that progressively refine the 3D representation, ultimately resulting in detailed and easily manipulated 3D models.", "section": "ABSTRACT"}, {"figure_path": "https://arxiv.org/html/2411.08033/x2.png", "caption": "Figure 2: Pipeline of the 3D VAE of GaussianAnything.\nIn the 3D latent space learning stage, our proposed 3D VAE \u2130\u03d5subscript\u2130bold-italic-\u03d5\\mathcal{E}_{\\bm{\\phi}}caligraphic_E start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT encodes V\u2212limit-from\ud835\udc49V-italic_V -views of posed RGB-D(epth)-N(ormal) renderings \u211b\u211b\\mathcal{R}caligraphic_R into a point-cloud structured latent space. This is achieved by first processing the multi-view inputs into the un-structured set latent, which is further projected onto the 3D manifold through a cross attention block, yielding the point-cloud structured latent code \ud835\udc33\ud835\udc33{\\mathbf{z}}bold_z.\nThe structured 3D latent is further decoded by a 3D-aware DiT transformer, giving the coarse Gaussian prediction.\nFor high-quality rendering, the base Gaussian is further up-sampled by a series of cascaded upsampler \ud835\udc9fUksuperscriptsubscript\ud835\udc9f\ud835\udc48\ud835\udc58\\mathcal{D}_{U}^{k}caligraphic_D start_POSTSUBSCRIPT italic_U end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT towards a dense Gaussian for high-resolution rasterization.\nThe 3D VAE training objective is detailed in Eq.\u00a0(9).", "description": "This figure illustrates the architecture of the 3D Variational Autoencoder (VAE) within the GaussianAnything model.  The VAE takes in multiple views (V) of posed RGB-D-N (Red-Green-Blue, Depth, Normal) renderings of a 3D object as input.  These views are initially encoded into an unstructured set latent representation. A cross-attention block then projects this set latent onto a 3D manifold, creating a point-cloud structured latent code (z).  A 3D-aware Diffusion Transformer (DiT) decodes this point-cloud latent code, producing an initial, coarse Gaussian prediction for the 3D object. To improve rendering quality, this coarse Gaussian prediction undergoes a series of cascaded upsampling operations (D<sub>U</sub><sup>k</sup>), generating a dense Gaussian representation suitable for high-resolution rendering. The training objective for this VAE is detailed in Equation 9 of the paper.", "section": "4.1 Point-Cloud Structured 3D VAE"}, {"figure_path": "https://arxiv.org/html/2411.08033/x3.png", "caption": "Figure 3: Diffusion training of GaussianAnything.\nBased on the point-cloud structure 3D VAE, we perform cascaded 3D diffusion learning given text (a) and image (b) conditions. We adopt DiT architecture with AdaLN-single\u00a0(Chen et\u00a0al., 2023) and QK-Norm\u00a0(Dehghani et\u00a0al., 2023; Esser et\u00a0al., 2021). For both condition modality, we send in the conditional feature with cross attention block, but at different positions. The 3D generation is achieved in two stages (c), where a point cloud diffusion model first generates the 3D layout \ud835\udc33x,0subscript\ud835\udc33\ud835\udc650{\\mathbf{z}}_{x,0}bold_z start_POSTSUBSCRIPT italic_x , 0 end_POSTSUBSCRIPT, and a texture diffusion model further generates the corresponding point-cloud features \ud835\udc33h,0subscript\ud835\udc33\u210e0{\\mathbf{z}}_{h,0}bold_z start_POSTSUBSCRIPT italic_h , 0 end_POSTSUBSCRIPT. The generated latent code \ud835\udc330subscript\ud835\udc330{\\mathbf{z}}_{0}bold_z start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT is decoded into the final 3D object with the pre-trained VAE decoder.", "description": "This figure illustrates the cascaded diffusion process within the GaussianAnything model for 3D generation.  The process begins with a point cloud structured 3D Variational Autoencoder (VAE). Conditional inputs, either text or images, are fed into the DiT architecture (using AdaLN-single and QK-Norm for normalization), interacting via cross-attention blocks at different stages. 3D generation proceeds in two stages: (1) a point cloud diffusion model generates the 3D object's layout (\ud835\udc33x,0), and (2) a texture diffusion model generates the corresponding point cloud features (\ud835\udc33h,0), given the initial layout. The combined latent code (\ud835\udc330) is then decoded by the pre-trained VAE to produce the final 3D object.", "section": "4 GAUSSIANANYTHING"}, {"figure_path": "https://arxiv.org/html/2411.08033/x4.png", "caption": "Figure 4: Qualitative Comparison of Image-to-3D. We showcase the novel view 3D reconstruction of all methods given a single image from unseen GSO dataset. Our proposed method achieves consistently stable performance across all cases.\nNote that though feed-forward 3D reconstruction methods achieve sharper texture reconstruction, these method fail to yield intact 3D predictions under challenging cases (e.g., the rhino in row 2).\nIn contrast, our proposed native 3D diffusion model achieve consistently better performance.\nBetter zoom in.", "description": "Figure 4 presents a qualitative comparison of different image-to-3D reconstruction methods on the unseen GSO dataset.  Each method is given a single input image, and the results are shown as novel-view 3D reconstructions.  The figure highlights the consistent, stable performance of the proposed method across various input images, in contrast to feed-forward methods that, while producing sharper textures, sometimes fail to generate complete and accurate 3D models, especially in challenging scenarios (such as the rhino in the second row).  The figure visually demonstrates the superior performance of the proposed native 3D diffusion model in terms of overall 3D reconstruction accuracy.", "section": "5.1 METRICS AND BASELINES"}, {"figure_path": "https://arxiv.org/html/2411.08033/x5.png", "caption": "Figure 5: Qualitative Comparison of Text-to-3D. We present text-conditioned 3D objects generated by GaussianAnything, displaying two views of each sample.\nThe top section compares our results with baseline methods, while the bottom shows additional samples from our method along with their geometry maps.\nOur approach consistently yields better quality in terms of geometry, texture, and text-3D alignment.", "description": "Figure 5 showcases a qualitative comparison of text-to-3D generation results achieved by GaussianAnything and several baseline methods. The figure presents two views of 3D objects generated from text prompts. The top section provides a direct comparison between GaussianAnything and baseline methods, demonstrating the superior quality of GaussianAnything's output. The bottom section presents additional examples generated by GaussianAnything, alongside their corresponding geometry maps, further highlighting the model's ability to generate high-quality 3D shapes with accurate textures and strong alignment between the generated content and the input text prompt.", "section": "5.1 Metrics and Baselines"}, {"figure_path": "https://arxiv.org/html/2411.08033/x6.png", "caption": "Figure 6: 3D editing. Given two text prompts, we generate the corresponding point cloud \ud835\udc330,xsubscript\ud835\udc330\ud835\udc65{\\mathbf{z}}_{0,x}bold_z start_POSTSUBSCRIPT 0 , italic_x end_POSTSUBSCRIPT with stage-1 diffusion model with \u03f5\u0398xsuperscriptsubscriptbold-italic-\u03f5\u0398\ud835\udc65\\bm{\\epsilon}_{\\Theta}^{x}bold_italic_\u03f5 start_POSTSUBSCRIPT roman_\u0398 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_x end_POSTSUPERSCRIPT, and the corresponding point cloud features \ud835\udc330,hsubscript\ud835\udc330\u210e{\\mathbf{z}}_{0,h}bold_z start_POSTSUBSCRIPT 0 , italic_h end_POSTSUBSCRIPT can be further generated with \u03f5\u0398hsuperscriptsubscriptbold-italic-\u03f5\u0398\u210e\\bm{\\epsilon}_{\\Theta}^{h}bold_italic_\u03f5 start_POSTSUBSCRIPT roman_\u0398 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_h end_POSTSUPERSCRIPT. As can be seen, the samples from stage-2 are consistent in overall 3D structures but with diverse textures. Thanks to the proposed Point Cloud-structured Latent space, our method supports interactive 3D structure editing. This is achieved by first modifying the stage-1 point cloud \ud835\udc330,x\u2192\ud835\udc330,x\u2032\u2192subscript\ud835\udc330\ud835\udc65superscriptsubscript\ud835\udc330\ud835\udc65\u2032{\\mathbf{z}}_{0,x}\\rightarrow{{\\mathbf{z}}_{0,x}^{\\prime}}bold_z start_POSTSUBSCRIPT 0 , italic_x end_POSTSUBSCRIPT \u2192 bold_z start_POSTSUBSCRIPT 0 , italic_x end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT, and then regenerate the 3D object with the same Gaussian noise.", "description": "This figure demonstrates the 3D editing capabilities of the GAUSSIANANYTHING model.  Two text prompts are used to generate a point cloud representing the 3D object's structure (z0,x) using a stage-1 diffusion model and its corresponding features (z0,h) using a stage-2 diffusion model.  The stage-2 samples maintain consistent 3D structure but offer diverse textures.  The point cloud-structured latent space allows for interactive 3D editing.  This is shown by modifying the stage-1 point cloud (z0,x) to (z0,x') and then regenerating the 3D object using the same Gaussian noise, highlighting the disentanglement of geometry and texture. ", "section": "4.2 CASCADED 3D GENERATION WITH FLOW MATCHING"}, {"figure_path": "https://arxiv.org/html/2411.08033/x7.png", "caption": "Figure 7: Qualitative ablation of Cascaded diffusion and latent space editing. We first show the effectiveness of our two-stage cascaded diffusion framework in (a). Compared to Fig.\u00a05, the single-stage 3D diffusion yields worse texture details and 3D structure intactness. In (b), we validate the latent point cloud editing yields less 3D artifacts compared to direct 3D editing on the 3D Gaussians.", "description": "This figure demonstrates the benefits of the two-stage cascaded diffusion process and the advantages of latent space editing.  Subfigure (a) compares the results of a single-stage diffusion model (generating both geometry and texture at once) with the two-stage approach (geometry first, then texture) from Figure 5. The single-stage method shows inferior texture quality and structural fidelity compared to the cascaded method, highlighting the effectiveness of the two-stage design. Subfigure (b) shows that editing in the point cloud latent space (before decoding to surfel Gaussians) produces cleaner and more realistic results than directly editing the surfel Gaussians themselves.  The comparison showcases a reduced chance of introducing artifacts or inconsistencies during the editing process.", "section": "4.2 CASCADED 3D GENERATION WITH FLOW MATCHING"}]