[{"content": "| Method | FID\u2193 | KID(%)\u2193 | MUSIQ\u2191 | P-FID\u2193 | P-KID(%)\u2193 | COV(%)\u2191 | MMD(\u2030)\u2193 |\n|---|---|---|---|---|---|---|---| \n| OpenLRM | 38.41 | 1.87 | 45.46 | 35.74 | 12.60 | 39.33 | 29.08 |\n| Splatter-Image | 48.80 | 3.65 | 30.33 | 19.72 | 7.03 | 37.66 | 30.69 |\n| One-2-3-45 (V=12) | 88.39 | 6.34 | 59.02 | 72.40 | 30.83 | 33.33 | 35.09 |\n| CRM (V=6) | 45.53 | 1.93 | 64.10 | 35.21 | 13.19 | 38.83 | 28.91 |\n| Lara (V=4) | 43.74 | 1.95 | 39.37 | 32.37 | 12.44 | 39.33 | 28.84 |\n| LGM (V=4) | 19.93 | 0.55 | 54.78 | 40.17 | 19.45 | 50.83 | 22.06 |\n| Shape-E | 138.53 | 11.95 | 31.51 | 20.98 | 7.41 | 61.33 | 19.17 |\n| LN3Diff | 29.08 | 0.89 | 50.39 | 27.17 | 10.02 | 55.17 | 19.94 |\n| **Ours** | 24.21 | 0.76 | 65.17 | 8.72 | 3.22 | 59.50 | 15.48 |", "caption": "Table 1: \nQuantitative evaluation of image-conditioned 3D generation. Here, quality of both 2D rendering and 3D shapes is evaluated. As shown below, the proposed method demonstrates strong performance across all metrics. Although multi-view images-to-3D approaches like LGM achieves better performance on the FID/KID metrics, they fall short on more advanced image quality assessment metrics such as MUSIQ and performs significantly worse in 3D shape quality. For multi-view to 3D methods, we also include the number of input views (V=##\\##).", "description": "This table presents a quantitative comparison of image-conditioned 3D generation methods, evaluating both the quality of the 2D renderings and the 3D shapes.  The metrics used include FID, KID, MUSIQ, P-FID, P-KID, Coverage Score, and Minimum Matching Distance.  The results show that the proposed method outperforms existing techniques across all metrics. While some multi-view methods (like LGM) achieve better FID and KID scores, they perform poorly on higher-level image quality (MUSIQ) and 3D shape quality metrics. The table also indicates the number of input views used for multi-view methods.", "section": "5.1 METRICS AND BASELINES"}, {"content": "| Method | ViT-B/32 | ViT-L/14 |\n|---|---|---|\n| Point-E | 26.35 | 21.40 |\n| Shape-E | 27.84 | 25.84 |\n| LN3Diff | 29.12 | 27.80 |\n| 3DTopia | 30.10 | 28.11 |\n| Ours | **31.80** | **29.38** |", "caption": "Table 2: \nQuantitative Evaluation on Text-to-3D. The proposed method outperforms existing methods on CLIP scores over two different backbones.", "description": "This table presents a quantitative comparison of different text-to-3D generation methods, focusing on CLIP scores.  The CLIP score measures the alignment between the generated 3D model and the input text prompt. Two different versions of the CLIP backbone model (ViT-B/32 and ViT-L/14) were used to evaluate the models, providing a more robust assessment. The results demonstrate that the proposed method in the paper achieves higher CLIP scores than existing approaches, indicating better alignment between the generated 3D models and text prompts.", "section": "5.1 METRICS AND BASELINES"}]