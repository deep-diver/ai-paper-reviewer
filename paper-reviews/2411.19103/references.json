{"references": [{"fullname_first_author": "Josh Achiam", "paper_title": "GPT-4 technical report", "publication_date": "2023-03-00", "reason": "GPT-4 is a highly influential and widely used large language model, serving as a strong baseline and comparison point for the research."}, {"fullname_first_author": "Pravesh Agrawal", "paper_title": "Pixtral 12B", "publication_date": "2024-10-00", "reason": "Pixtral 12B is a significant open-source vision-language model that provides a strong comparative baseline due to its size and availability."}, {"fullname_first_author": "Bo Li", "paper_title": "LLaVA-OneVision", "publication_date": "2024-08-00", "reason": "LLaVA-OneVision is a key foundational model for the proposed VARCO-VISION model, and its training framework is extended in this work."}, {"fullname_first_author": "Bohao Li", "paper_title": "SEED-Bench: Benchmarking multimodal large language models", "publication_date": "2024-00-00", "reason": "SEED-Bench is used as a basis for the Korean benchmark datasets, showcasing its importance in establishing evaluation standards for multimodal models."}, {"fullname_first_author": "Yuan Liu", "paper_title": "MMBench: Is your multi-modal model an all-around player?", "publication_date": "2025-00-00", "reason": "MMBench is another crucial benchmark, providing various evaluation dimensions, used for the creation of Korean benchmarks."}]}