[{"content": "| KOREAN BENCHMARKS | Image Understanding (MCQA) | Generation (Image-based) |\n|---|---|---|\n| **Model** | K-MMB (dev) | K-SEED | K-MMSTAR | K-DTCBench | K-LLaVA-W |\n| **VARCO-VISION-14B** | 82.21 | 75.39 | 57.33 | 84.58 | 84.74 |\n| **Pangea-7B [36]** | 71.64 | 73.34 | 35.00 | 48.33 | 69.70 |\n| **Pixtral-12B [3]** | 57.47 | 46.44 | 23.93 | 27.50 | 82.00 |\n| **Molmo-7B-D [7]** | 63.83 | 69.53 | 47.40 | 45.83 | 63.90 |\n| **Qwen2-VL-7B-Instruct [33]** | 78.26 | 74.08 | 50.67 | 75.00 | 62.00 |\n| **LLaVA-OneVision-7B [16]** | 76.28 | 73.21 | 54.00 | 52.91 | 48.80 |\n| **Qwen2-VL-72B-Instruct [33]** | 84.27 | 78.25 | 63.53 | 88.75 | 97.40 |\n| **LLaVA-OneVision-72B [16]** | 88.01 | 77.86 | 62.66 | 60.83 | 84.10 |\n| **GPT-4o-mini [2]** | 74.48 | 73.30 | 42.33 | 74.58 | 101.90 |\n| **GPT-4V [2]** | 77.92 | 71.66 | 35.20 | 47.50 | 98.90 |\n| **GPT-4o [2]** | 81.01 | 76.98 | 56.20 | 85.80 | 103.90 |", "caption": "Table 1: Model Comparison on Korean Benchmarks. The models in the first upper block are open-source models with similar scale, and the second block are open-source 72B models. The last block shows the performance of proprietary GPT models. We primarily compare VARCO-VISION\u00a0with the models mentioned in the first block, as they are similar in size to our model.", "description": "This table compares the performance of VARCO-VISION-14B against other vision-language models on Korean benchmarks.  The comparison is broken down into three groups: open-source models of similar size to VARCO-VISION (under 20B parameters), larger open-source models (around 72B parameters), and proprietary GPT models. The table highlights the performance of each model across multiple tasks, allowing for a comprehensive evaluation of VARCO-VISION's capabilities relative to existing models. The main focus of the comparison is between VARCO-VISION and the first group of models due to their similar scale.", "section": "3.2.1 Korean Benchmarks"}, {"content": "| Model | Image Understanding (MCQA) | SEED (image) | MMStar (val) | MMMU (val) | OCR | \n|---|---|---|---|---|---| \n| **ENGLISH BENCHMARKS** |  |  |  |  |  | \n| V<span class=\"ltx_text\" id=\"S3.T2.1.1.4.4.1.1.1.1\" style=\"font-size:90%;color:#172B4D;\">ARCO</span>-V<span class=\"ltx_text\" id=\"S3.T2.1.1.4.4.1.1.1.2\" style=\"font-size:90%;color:#172B4D;\">ISION</span>-14B | 84.28 | 76.73 | 63.33 | <s>51.33</s> | <s>820</s> | \n| Pangea-7B [36] | 76.23 | 74.88 | 43.26 | 43.55 | 620 | \n| Pixtral-12B [3] | 72.98 | 74.34 | 48.33 | 49.00 | 682 | \n| Molmo-7B-D [7] | 72.05 | 74.36 | 52.73 | 45.30 | 708 | \n| Qwen2-VL-7B-Instruct [33] | <s>80.95</s> | <s>76.45</s> | 60.00 | 54.10 | 866 | \n| LLaVA-OneVision-7B [16] | 80.80 | 76.41 | <s>61.33</s> | 47.67 | 630 | \n| Qwen2-VL-72B-Instruct [33] | 86.91 | 77.86 | 67.60 | 56.66 | 877 | \n| LLaVA-OneVision-72B [16] | 85.44 | 77.43 | 65.33 | 56.80 | 741 | \n| GPT-4o-mini [2] | 76.31 | 72.80 | 54.80 | 60.00 | 785 | \n| GPT-4V [2] | 79.41 | 73.00 | 56.00 | 62.30 | 656 | \n| GPT-4o [2] | 81.73 | 76.70 | 64.70 | 69.90 | 805 | ", "caption": "Table 2: Model Comparison on English Benchmarks. MMBench\u00a0[22], SEED\u00a0[17], MMStar\u00a0[5], and MMMU\u00a0[35] are multi-choice question answering tasks. MMBench and SEED are for comprehension evaluation, whereas MMStar is focused more on vision-indispensible reasoning. MMMU tests college-level subject knowledge of VLMs. OCRBench\u00a0[23] is a specialized benchmark in OCR for VLMs, composed of 1000 question-answer pairs. The values in OCRBench indicate the number of questions correctly answered by models.", "description": "Table 2 presents a comparative analysis of various Vision-Language Models (VLMs) on several English benchmarks.  The benchmarks assess different aspects of VLM capabilities: MMBench, SEED, and MMStar evaluate comprehension and reasoning skills through multiple-choice question answering tasks, with MMStar focusing on vision-centric reasoning. MMMU tests the models' knowledge of college-level subjects. OCRBench specifically evaluates Optical Character Recognition (OCR) performance by counting correctly answered questions out of 1000 provided pairs. The table allows for a comprehensive comparison of VLMs across diverse capabilities, enabling a nuanced understanding of their strengths and weaknesses in different visual and linguistic tasks.", "section": "3.2 English Benchmarks"}, {"content": "| TEXT-ONLY BENCHMARKS | Korean | KoMT-Bench | English\n|---|---|---|---|\n| **Model** |  |  |  |\n| **VARCO-VISION-14B** | **8.69** | **8.39** | **8.80** |\n| Pangea-7B [36] | 5.06 | 5.06 | 7.29 |\n| Pixtral-12B [3] | <s>7.71</s> | <s>8.11</s> | <s>8.40</s> |\n| Molmo-7B-D [7] | 2.64 | 3.58 | 6.93 |\n| Qwen2-VL-7B-Instruct [33] | 4.62 | 4.54 | 7.13 |\n| LLaVA-OneVision-7B [16] | 2.23 | 3.52 | 7.52 |\n| Qwen2-VL-72B-Instruct [33] | 7.74 | 7.49 | 8.53 |\n| LLaVA-OneVision-72B [16] | 8.22 | 7.87 | 8.78 |\n| EXAONE 3.0 7.8B Inst.(LLM) [29] | 8.62 | 8.92 | 9.01 |\n| GPT-4o-mini [2] | 9.14 | 8.88 | 9.09 |\n| GPT-4V [2] | 8.66 | 9.25 | 9.41 |\n| GPT-4o [2] | 9.57 | 9.24 | 9.30 |", "caption": "Table 3: Model Performance on Korean and English Text-only Benchmarks. MT-Bench\u00a0[40] is an English multi-turn dialogue benchmark, and KoMT-Bench\u00a0[29] is built by translating MT-Bench. LogicKor11 consists of multi-turn Korean dialogues across six categories.", "description": "This table presents a comparison of model performance on English and Korean text-only benchmarks.  The English benchmark MT-Bench involves multi-turn dialogue, while the Korean benchmark KoMT-Bench is a translation of MT-Bench.  A third Korean benchmark, LogicKor, uses multi-turn dialogues across six distinct categories.  The table allows for assessment of models' capabilities in understanding and generating text in both languages, independent of any visual input.", "section": "3.2.3 Text-only Benchmarks"}, {"content": "| Model | CORD | ICDAR2013 | ICDAR2015 | Average |\n|---|---|---|---|---|\n| **VARCO-VISION-14B** | 82.69 | **94.42** | 72.95 | 83.35 |\n| **EasyOCR**<sup>10</sup> | 79.56 | 84.97 | 57.90 | 74.14 |\n| **Pororo** [9] | 78.73 | 84.29 | 64.65 | 75.89 |\n| **PaddleOCR**<sup>11</sup> | **92.71** | 92.01 | **73.73** | **86.15** |\n| **CLOVA OCR**<sup>12</sup> | 95.32 | 94.39 | 84.06 | 91.26 |", "caption": "Table 4: OCR Benchmark Performance. EasyOCR, Pororo, and PaddleOCR are open-source OCR models. CLOVA OCR is a proprietary OCR model. PopEval\u00a0[15] was used as the metric for all benchmarks.", "description": "This table presents a comparison of the performance of various OCR models on benchmark datasets. The models include EasyOCR, Pororo, PaddleOCR (all open-source), and CLOVA OCR (proprietary).  The PopEval metric [15] was used to evaluate the performance of each model. The results demonstrate the accuracy of each model across different benchmarks, providing insights into their relative strengths and weaknesses for Optical Character Recognition tasks.", "section": "3.2.4 OCR Benchmarks"}]