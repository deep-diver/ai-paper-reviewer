[{"figure_path": "https://arxiv.org/html/2411.19103/x1.png", "caption": "Figure 1: VARCO-VISION\u00a0Application Examples: Visual Question Answering (VQA), Optical Character Recognition (OCR), Referring, and Grounding. Our model excels at both Korean/English vision-text and text-only tasks. Please see B for more detailed examples.", "description": "This figure showcases VARCO-VISION's capabilities across various vision-language tasks, including Visual Question Answering (VQA), Optical Character Recognition (OCR), referring expression generation, and grounding.  The examples highlight the model's proficiency in understanding and generating text in both Korean and English, demonstrating its strong bilingual capabilities in multimodal contexts.  More detailed examples can be found in Appendix B.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2411.19103/x2.png", "caption": "Figure 2: K-MMStar Example", "description": "This figure shows examples from the K-MMStar benchmark, a Korean version of the MMStar benchmark.  It illustrates three types of question modifications done to adapt the original English MMStar dataset for Korean.  Type 1 questions are direct translations. Type 2 questions require modifications to account for differences in terminology (e.g., 'football' vs. 'soccer'). Type 3 questions needed complete re-creation because the original options in MMStar were unanswerable or vague, requiring new images and questions to be generated. This highlights the challenges of directly translating and adapting existing multimodal benchmarks for different languages.", "section": "3.1 Korean Evaluation Benchmarks"}, {"figure_path": "https://arxiv.org/html/2411.19103/x3.png", "caption": "Figure 3: K-DTCBench Example", "description": "This figure displays example images from the K-DTCBench dataset.  K-DTCBench is a Korean multimodal benchmark dataset designed to evaluate the ability of vision-language models to process diverse image types, including documents, tables, and charts. The examples show sample images, multiple-choice questions, and their respective answers.  The dataset is constructed from scratch with a mix of synthetic images and real-world scanned documents/tables/charts to ensure a wide representation of typical image formats that a vision-language model might encounter.", "section": "3.1 Korean Evaluation Benchmarks"}, {"figure_path": "https://arxiv.org/html/2411.19103/x4.png", "caption": "Figure 4: K-LLaVA-W Example", "description": "Figure 4 shows examples from the K-LLaVA-W benchmark dataset.  The K-LLaVA-W benchmark is an open-set, Korean language version of the LLaVA-W benchmark, designed to evaluate the model's ability to generate fluent and relevant responses to open-ended questions about images. This figure displays example image-question pairs and illustrates how English text within images has been translated into Korean, while images without text were left unchanged. This illustrates the methods used to create a Korean version of the open-set benchmark. ", "section": "3.1 Korean Evaluation Benchmarks"}, {"figure_path": "https://arxiv.org/html/2411.19103/x5.png", "caption": "Figure 5: K-LLaVA-W Evaluation Prompt. We translated the LLaVA-W prompts and added specific guidelines in the JudgeLLM prompt.", "description": "This figure displays the evaluation prompt used for the K-LLaVA-W benchmark.  The prompt, originally in English for the LLaVA-W benchmark, has been translated into Korean.  Crucially, specific guidelines were added to help the JudgeLLM (the large language model used for automated evaluation) more accurately assess aspects like helpfulness, relevance, accuracy, detail, and Korean language generation quality of the model's responses. The structure of the prompt is shown, including the spaces for the caption, question, model's response, and the evaluator's scoring instructions.", "section": "3.1 Korean Evaluation Benchmarks"}, {"figure_path": "https://arxiv.org/html/2411.19103/x6.png", "caption": "Figure 6: Text Recognition and Analysis Example (English)", "description": "This figure shows an example of the model's text recognition and analysis capabilities.  The input is a screenshot of a sleep tracking app displaying sleep data. The model accurately extracts the relevant data points (e.g., hours of sleep, sleep stages) and then provides a concise summary of the user's sleep quality and actionable advice on how to improve it.", "section": "Application Examples"}, {"figure_path": "https://arxiv.org/html/2411.19103/x7.png", "caption": "Figure 7: Mathematical Reasoning Example (Korean)", "description": "This figure demonstrates the model's mathematical reasoning capabilities using a Korean language example.  It presents a diagram showing a shaded region within a 90-degree sector of a circle with a 4cm radius. The task is to calculate the area of this shaded region.  The figure shows the model's step-by-step solution, including identifying the shapes involved (sector and triangle), applying the relevant formulas, and calculating the final answer. This example showcases the model's ability to both understand the geometrical problem and perform the necessary calculations accurately.", "section": "3.2.1 Korean Benchmarks"}, {"figure_path": "https://arxiv.org/html/2411.19103/x8.png", "caption": "Figure 8: Information Extraction and Calculation Example (Korean)", "description": "This figure demonstrates the model's ability to extract relevant information from an image of a menu board and perform calculations based on user requests.  A user asks the model how much it would cost to order a cheeseburger and a soda, given the prices displayed on the menu. The model correctly identifies the prices, adds them up, and provides the total cost in the specified currency (1000 won). This showcases the model's combined capabilities of visual information extraction and numerical computation.", "section": "3.2 Benchmark Results"}, {"figure_path": "https://arxiv.org/html/2411.19103/x9.png", "caption": "Figure 9: Grounding Example (Korean)", "description": "The figure displays an example of the VARCO-VISION model's grounding capabilities in Korean.  It shows an image of a child in a cart pulled by a donkey, and the model's response provides a detailed description of the image, including the location and identification of objects within the image (child, cart, donkey) and their attributes.", "section": "Application Examples"}, {"figure_path": "https://arxiv.org/html/2411.19103/x10.png", "caption": "Figure 10: Grounding Example (English)", "description": "The figure displays an example of the grounding task in English.  The image shows two cats lying on a pink blanket, with two remote controls nearby. The model correctly identifies and localizes the objects (the cats and the blanket) and provides a detailed description of the scene.", "section": "Application Examples"}, {"figure_path": "https://arxiv.org/html/2411.19103/x11.png", "caption": "Figure 11: Referring Example (Korean)", "description": "This figure demonstrates the model's referring capabilities using a Korean example. The input is a request to read the text visible on a Starbucks storefront and describe what kind of place it is.  The model correctly identifies the text as \"STARBUCKS,\" and then provides a detailed description of Starbucks as a coffee shop chain that sells various drinks and snacks, offering customers a space to relax and socialize. The response also mentions the global presence of Starbucks and the typical ambiance of the store.", "section": "Application Examples"}, {"figure_path": "https://arxiv.org/html/2411.19103/x12.png", "caption": "Figure 12: OCR Example", "description": "This figure shows an example of Optical Character Recognition (OCR) performed by the VARCO-VISION model.  The input is an image of a speed limit sign at night. The model correctly identifies and transcribes the text on the sign as \"SPEED LIMIT 20\". This demonstrates the model's ability to accurately extract textual information from images, even under challenging conditions like low light.", "section": "3.2 Benchmark Results"}, {"figure_path": "https://arxiv.org/html/2411.19103/x13.png", "caption": "Figure 13: OCR Example", "description": "This figure showcases an example of Optical Character Recognition (OCR) performed by the VARCO-VISION model.  The input image contains a Korean street sign with multiple lines of text. The model successfully extracts and outputs the text from the image, demonstrating its ability to accurately identify and transcribe Korean characters within a real-world context.", "section": "3.2.4 OCR Benchmarks"}, {"figure_path": "https://arxiv.org/html/2411.19103/x14.png", "caption": "Figure 14: Summarization Example (English)", "description": "The figure displays an example of the model's summarization capabilities.  It shows a news article about the reunion of the British rock band Oasis and the model's generated summary of the article in a lyrical style, mimicking the style requested in the caption. This showcases the model's ability to not only understand the content of the article but also to adapt its writing style to a specific user request.", "section": "3.2 Benchmark Results"}, {"figure_path": "https://arxiv.org/html/2411.19103/x15.png", "caption": "Figure 15: Text Recognition Example (Korean)", "description": "This figure showcases an example of text recognition in Korean.  The model accurately extracts and displays all the text present in the provided image, which includes both text and images.  The image depicts a diagram explaining the training process for a Multimodal Large Language Model (MLLM). The model's response shows it not only identified the text but also structured it appropriately, separating the two phases (Pre-training and Fine-tuning) clearly. This demonstrates the model's capability to correctly interpret and extract textual information from complex images containing both text and visuals.", "section": "3 Evaluation"}]