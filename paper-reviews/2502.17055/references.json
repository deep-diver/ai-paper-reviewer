{"references": [{"fullname_first_author": "Kingma, D. P.", "paper_title": "Adam: A method for stochastic optimization", "publication_date": "2014-12-01", "reason": "This paper introduced the Adam optimizer, which is a foundational algorithm widely used in deep learning and serves as a key baseline in this work."}, {"fullname_first_author": "Huang, T.", "paper_title": "Spam: Spike-aware adam with momentum reset for stable llm training", "publication_date": "2025-01-09", "reason": "This paper introduces the SPAM optimizer, which is a direct predecessor to the proposed Stable-SPAM and is extensively analyzed and improved upon in this paper."}, {"fullname_first_author": "Shazeer, N.", "paper_title": "Adafactor: Adaptive learning rates with sublinear memory cost", "publication_date": "2018-07-01", "reason": "This paper introduces the Adafactor optimizer, which is an adaptive learning rate method evaluated as a baseline and demonstrates robustness in the study."}, {"fullname_first_author": "Touvron, H.", "paper_title": "Llama 2: Open foundation and fine-tuned chat models", "publication_date": "2023-07-11", "reason": "This paper describes the LLaMA2 model family, which is used as the experimental testbed for evaluating the proposed optimization techniques in this study."}, {"fullname_first_author": "Micikevicius, P.", "paper_title": "Mixed precision training", "publication_date": "2017-10-05", "reason": "This paper introduces mixed precision training, an approach that is foundational for many techniques that are evaluated and compared to in the current study."}]}