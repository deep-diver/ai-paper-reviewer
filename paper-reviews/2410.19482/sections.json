[{"page_end_idx": 3, "page_start_idx": 1, "section_number": 1, "section_title": "Introduction", "details": {"details": "Large language models (LLMs) are prone to **memorizing training data**, which poses a significant risk due to the potential leakage of sensitive information.  Current methods for assessing memorization, such as discoverable extraction, utilize single-sequence sampling, potentially underestimating the true extent of the problem. This paper introduces a **probabilistic relaxation of discoverable extraction** which quantifies the probability of extracting a target sequence, considering various sampling schemes and multiple attempts. This approach offers a more nuanced and realistic evaluation of LLM memorization, addressing the limitations of previous techniques.", "first_cons": "Current memorization measurement methods, primarily discoverable extraction, rely on single-sequence greedy sampling, potentially underestimating the true extent of memorization.", "first_pros": "Introduces a probabilistic relaxation of discoverable extraction, offering a more comprehensive and realistic assessment of LLM memorization.", "keypoints": ["LLMs memorize training data, posing risks of sensitive information leakage.", "Discoverable extraction, a common method, underestimates memorization using single-sequence sampling.", "Proposed probabilistic method quantifies extraction probability across multiple samples and schemes.", "Addresses limitations of existing approaches; improves realistic assessment of risk."], "second_cons": "Existing methods, such as discoverable extraction, only consider single-sequence sampling, potentially misrepresenting the reality of LLM memorization.", "second_pros": "The proposed probabilistic measure considers various sampling schemes and multiple attempts, which leads to a more accurate and realistic evaluation of memorization rates.", "summary": "This paper introduces a probabilistic approach to measuring LLM memorization, addressing the limitations of existing methods by considering various sampling schemes and multiple attempts to provide a more realistic assessment of memorization and its associated risks."}}, {"page_end_idx": 4, "page_start_idx": 3, "section_number": 2, "section_title": "Preliminaries", "details": {"details": "This section lays out the groundwork for understanding discoverable extraction, a key concept in the paper. It introduces **notation** for sequences and probability distributions, formally defines **discoverable extraction**  as a method to determine if a model has memorized a training sequence (**Definition 2.1**), and discusses common **sampling schemes** used in the process, such as **greedy sampling**, **temperature sampling**, **top-k sampling**, and **top-p (nucleus) sampling**.  Understanding these sampling methods is crucial to interpreting the results of experiments later in the paper. The definition of discoverable extraction highlights that, in practice, the sampling process is often instantiated with greedy sampling.  This section makes sure that readers are on the same page about the fundamental concepts and methods.", "first_cons": "The formal definition might be difficult for readers without a background in machine learning or probability.", "first_pros": "Provides the necessary foundation and formal definitions for the subsequent sections of the paper. By clearly defining terms and notation upfront, the rest of the paper can be more concise and rigorous.", "keypoints": ["**Notation** for sequences and probability distributions is introduced.", "**Discoverable extraction** is formally defined (Definition 2.1).", "Common **sampling schemes** (greedy, temperature, top-k, top-p) are described.", "**Greedy sampling** is highlighted as the common instantiation of the discoverable extraction process.", "Understanding sampling methods is important for interpreting results later in the paper"], "second_cons": "The description of sampling schemes could be more detailed, maybe providing illustrative examples of how different settings affect the outcome.", "second_pros": "Precise and concise definitions of key terms make the paper self-contained and easy to follow for those familiar with machine learning concepts. This sets up the subsequent analysis based on the formalization presented here. ", "summary": "Section 2 establishes the fundamental terminology and definitions of discoverable extraction and various sampling schemes used to evaluate large language model memorization, setting the stage for a new probabilistic definition."}}, {"page_end_idx": 4, "page_start_idx": 4, "section_number": 3, "section_title": "A probabilistic definition of discoverable extraction", "details": {"details": "This section introduces a new definition called **(n,p)-discoverable extraction** which enhances the traditional discoverable extraction by considering multiple sampling attempts and a probability threshold.  It addresses the limitation of single-sequence greedy sampling in traditional approaches by quantifying the probability of extracting a target sequence within *n* attempts with a probability of at least *p*.  The authors argue that this probabilistic approach provides a more realistic assessment of LLM memorization, acknowledging the inherent probabilistic nature of LLMs and diverse user interaction patterns.  The definition uses the probability of generating the suffix at least once in *n* attempts to exceed a threshold *p*.  The authors show this improves upon the previous methods which is based on greedy sampling only, which is less likely to capture diverse possibilities and the repeated querying behavior of a user.", "first_cons": "The existing definition of discoverable extraction relies on single-sequence greedy sampling, which may underestimate memorization rates by overlooking less likely yet still extractable sequences. This approach is not realistic when considering diverse user interaction patterns.", "first_pros": "The proposed definition provides a comprehensive and realistic assessment of LLM memorization by accounting for the probabilistic nature of LLMs and user interaction patterns.", "keypoints": ["New definition: **(n,p)-discoverable extraction** considers multiple sampling attempts (*n*) and probability threshold (*p*).", "Addresses limitations of single-sequence greedy sampling.", "Provides more realistic memorization assessment.", "Captures user interaction patterns and probabilistic nature of LLMs.", "Improved upon traditional methods by considering multiple sampling and probability threshold"], "second_cons": "While offering a more nuanced perspective, the introduction of two new parameters (*n* and *p*) requires careful consideration and tuning for different contexts and datasets.", "second_pros": "The probabilistic approach provides a more flexible and comprehensive evaluation of memorization risk, allowing the analysis of the risk of extraction given a certain number of sampling attempts and probability. This approach allows users to define a threshold that balances the risks of false positives and false negatives, according to their tolerance and the sensitivity of the data.", "summary": "This section introduces (n,p)-discoverable extraction, a probabilistic measure of memorization that accounts for multiple sampling attempts and a probability threshold for extracting target sequences, thus providing a more nuanced and realistic assessment of LLM memorization compared to traditional methods."}}, {"page_end_idx": 5, "page_start_idx": 5, "section_number": 5, "section_title": "Experiments", "details": {"details": "This section demonstrates the effectiveness of the new probabilistic definition of extraction using various experiments. The results show that **(n,p)-discoverable extraction rates significantly exceed greedy extraction rates for modest values of n and p**, indicating that greedy methods may underestimate memorization.  The risk of extracting training data is considerably higher than for test data, even with larger n and smaller p. This gap widens with larger models and repeated data in training.  The study also demonstrates that (n,p)-discoverable extraction provides a better comparison of memorization across models than greedy extraction, showing that larger models tend to memorize more and that repeated data in training leads to significantly higher extraction rates.", "first_cons": "The experiments primarily focus on the Pythia model, potentially limiting the generalizability of findings to other LLMs.", "first_pros": "The results clearly illustrate the benefits of the proposed probabilistic approach over conventional methods.", "keypoints": ["(n,p)-discoverable extraction surpasses greedy extraction rates", "Training data extraction risk significantly higher than test data", "Gap widens with larger models and repeated data", "(n,p) method provides better cross-model comparison"], "second_cons": "The choice of hyperparameters n and p requires careful consideration and may be context-dependent.", "second_pros": "The methodology is computationally efficient, avoiding the need for retraining models.", "summary": "Experiments using (n,p)-discoverable extraction reveal significantly higher memorization rates in LLMs than previously measured with greedy methods, highlighting the importance of considering the probabilistic nature of LLMs and user interaction patterns."}}, {"page_end_idx": 7, "page_start_idx": 6, "section_number": 5, "section_title": "Modeling extraction risks from multiple queries", "details": {"details": "This section explores how the risk of data extraction changes when considering multiple queries from a user to a language model.  It highlights the fact that users can and do query LLMs multiple times before successfully extracting sensitive information.  This contrasts with single-query approaches like standard discoverable extraction, which may underestimate actual memorization rates.  The authors emphasize that the ability to verify the extracted information through external means makes even a small probability of extraction in a single trial problematic, as repeated attempts can lead to success. The section introduces the concept of considering the compounded risk over multiple attempts and advocates for a more nuanced approach that considers the combined probability of extraction across multiple queries. The authors mention how this approach allows for a more nuanced quantification of memorization risk, while addressing the limitations of single-query methods, particularly for users who may query a model multiple times.  It touches on the need for practical parameters to define 'extractability' in this context and discusses setting practical parameters 'n' (number of attempts) and 'p' (probability) and acknowledges the contextual nature of what constitutes an acceptable risk level.", "first_cons": "Single-query approaches like standard discoverable extraction might not fully capture the memorization risks, especially when users can make repeated queries.", "first_pros": "This new approach provides a more realistic assessment of memorization risks by accounting for the possibility of multiple queries.", "keypoints": ["Users often make multiple queries to LLMs, increasing the risk of sensitive data extraction.", "Verification of extracted information through external means magnifies the risk of even low single-query extraction probabilities.", "(n, p)-discoverable extraction offers a more nuanced quantification of memorization risk.", "This approach addresses the limitations of single-query assessments by considering repeated attempts.", "Practical parameter choices (n and p) are context-dependent and require careful consideration of different types of data"], "second_cons": "The new approach introduces two new hyperparameters (n,p) that may need fine-tuning and require consideration of various types of data.", "second_pros": "The (n,p) approach facilitates a finer-grained analysis of extraction tolerance, enabling better risk management across diverse data.", "summary": "The risk of data extraction from large language models is significantly underestimated by single-query methods, as users can make multiple queries and verify results through external means, making a probabilistic multi-query assessment crucial for accurate risk evaluation."}}, {"page_end_idx": 11, "page_start_idx": 7, "section_number": 6, "section_title": "Discussion", "details": {"details": "The (n,p)-discoverable extraction offers a **cost-effective** way to quantify memorization risk by considering multiple sampling attempts.  It addresses limitations of existing methods that rely on single-sequence greedy sampling and provides a more nuanced understanding of memorization by incorporating the probabilistic nature of LLMs and varying user interaction patterns.  The flexibility of choosing parameters (n and p) allows for a **granular assessment** of risks depending on the sensitivity of the data and the adversary's computational budget. This definition bridges the gap between theoretical memorization measures and real-world user behavior, offering a more comprehensive and realistic evaluation of LLM memorization.", "first_cons": "The (n,p) definition introduces new hyperparameters (n and p), requiring careful consideration and potentially subjective choices for tuning.", "first_pros": "It is computationally inexpensive, aligning with the cost of greedy-based methods.", "keypoints": ["**Cost-effective** method to quantify memorization risk", "Considers multiple sampling attempts, reflecting realistic user interactions", "Allows granular risk assessment based on data sensitivity and adversary's capabilities", "Bridges gap between theoretical measures and real-world user behavior"], "second_cons": "The choice of n and p values may be subjective and require careful consideration depending on the type of data involved.", "second_pros": "The nuanced definition of (n, p)-discoverable extraction provides a better understanding of the relationship between memorization and various sampling schemes, potentially mitigating the underestimation of memorization rates observed in greedy sampling.", "summary": "The (n,p)-discoverable extraction method offers a computationally inexpensive way to quantify memorization risk more accurately than traditional methods by incorporating multiple sampling attempts and allowing flexible risk assessments tailored to the specific context."}}]