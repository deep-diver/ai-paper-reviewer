[{"content": "| Train \u2192 Test | Method | Active Params in Million | PrT | No. Instructions in a Row (1000 chains) |  |  |  |  | **Avg. Len.** |\n|---|---|---|---|---|---|---|---|---|---| \n| ABCD \u2192 D | Diff-P-CNN | 321 | \u00d7 | 86.3% | 72.7% | 60.1% | 51.2% | 41.7% | 3.16 \u00b1 0.06 |\n| | Diff-P-T | 194 | \u00d7 | 78.3% | 53.9% | 33.8% | 20.4% | 11.3% | 1.98 \u00b1 0.09 |\n| | RoboFlamingo | 1000 | \u2713 | 96.4% | 89.6% | 82.4% | 74.0% | 66.0% | 4.09 \u00b1 0.00 |\n| | GR-1 | 130 | \u2713 | 94.9% | 89.6% | 84.4% | 78.9% | 73.1% | 4.21 \u00b1 0.00 |\n| | **MoDE** | 277 | \u00d7 | 96.6% | 90.6% | 86.6% | 80.9% | 75.5% | 4.30 \u00b1 0.02 |\n| | **MoDE** | 436 | \u2713 | **97.1%** | **92.5%** | **87.9%** | **83.5%** | **77.9%** | **4.39 \u00b1 0.04** |\n| ABC \u2192 D | Diff-P-CNN | 321 | \u00d7 | 63.5% | 35.3% | 19.4% | 10.7% | 6.4% | 1.35 \u00b1 0.05 |\n| | Diff-P-T | 194 | \u00d7 | 62.2% | 30.9% | 13.2% | 5.0% | 1.6% | 1.13 \u00b1 0.02 |\n| | RoboFlamingo | 1000 | \u2713 | 82.4% | 61.9% | 46.6% | 33.1% | 23.5% | 2.47 \u00b1 0.00 |\n| | SuSIE | 860+ | \u2713 | 87.0% | 69.0% | 49.0% | 38.0% | 26.0% | 2.69 \u00b1 0.00 |\n| | GR-1 | 130 | \u2713 | 85.4% | 71.2% | 59.6% | 49.7% | 40.1% | 3.06 \u00b1 0.00 |\n| | **MoDE** | 307 | \u00d7 | 91.5% | 79.2% | 67.3% | 55.8% | 45.3% | 3.39 \u00b1 0.03 |\n| | **MoDE** | 436 | \u2713 | **96.2%** | **88.9%** | **81.1%** | **71.8%** | **63.5%** | **4.01 \u00b1 0.04** |", "caption": "Table 1: \nPerformance comparison on the two CALVIN challenges.\nThe table reports average success rates for individual tasks within instruction chains and the average rollout length (Avg. Len.) to complete 5 consecutive instructions, based on 1000 chains.\nZero standard deviation indicates methods without reported average performance.\n\"Prt\" denotes models requiring policy pretraining.\nParameter counts exclude language encoders.", "description": "This table compares the performance of different models on the CALVIN benchmark challenges, including ABCD\u2192D and ABC\u2192D. It presents average success rates for completing individual tasks within instruction chains and the average rollout length to complete five consecutive instructions. The results are based on 1000 instruction chains. Models marked with \"Prt\" require policy pretraining. Reported parameter counts exclude the language encoders.  Some methods show a zero standard deviation because the papers did not report these values.", "section": "4 EVALUATION"}, {"content": "|                       | Avg. Success. |\n| :-------------------- | :----------- |\n| **MoDE**             | **0.92**     |\n| - Input Noise Token   | 0.90         |\n| - Noise-cond Attention | 0.85         |\n| FiLM Noise Conditioning | 0.81         |\n| **TopK=1**           | **0.91**     |\n| Shared Expert        | 0.90         |\n| $\ngamma=0.1$          | 0.90         |\n| $\ngamma=0.001$       | 0.86         |\n| 256 Embed Dim       | 0.86         |\n| 512 Embed Dim       | 0.87         |", "caption": "Table 2: Ablation Studies for MoDE on LIBERO-10. All results are averaged over 3333 seeds with 20202020 rollouts each.", "description": "This table presents the ablation study results of the MoDE policy on the LIBERO-10 benchmark.  It investigates the impact of different design choices on MoDE's performance by evaluating variations in noise injection methods, Mixture-of-Expert (MoE) strategies, and latent dimension. Reported metrics (average success rate) are averaged over 3 seeds, each with 20 rollouts per task.", "section": "4.5.1 What Design Decisions Affect MoDE's Performance?"}, {"content": "| Hyperparameter | CALVIN ABCD | CALVIN ABC | LIBERO-10 | LIBERO-90 | Pret-MoDE |\n|---|---|---|---|---|---|---|---|---| \n| Number of Transformer Layers | 8 | 8 | 8 | 8 | 12 |\n| Number Experts | 4 | 4 | 4 | 4 | 4 |\n| Attention Heads | 8 | 8 | 8 | 8 | 8 |\n| Action Chunk Size | 10 | 10 | 10 | 10 | 10 |\n| History Length | 1 | 1 | 1 | 1 | 1 |\n| Embedding Dimension | 1024 | 1024 | 1024 | 1024 | 1024 |\n| Image Encoder | FiLM-ResNet18 | FiLM-ResNet50 | FiLM-ResNet18 | FiLM-ResNet18 | FiLM-ResNet50 |\n| Goal Lang Encoder | CLIP ViT-B/32 | CLIP ViT-B/32 | CLIP ViT-B/32 | CLIP ViT-B/32 | CLIP ViT-B/32 |\n| Attention Dropout | 0.3 | 0.3 | 0.3 | 0.3 | 0.3 |\n| Residual Dropout | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 |\n| MLP Dropout | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 |\n| Optimizer | AdamW | AdamW | AdamW | AdamW | AdamW |\n| Betas | [0.9, 0.95] | [0.9, 0.95] | [0.9, 0.95] | [0.9, 0.95] | [0.9, 0.95] |\n| Learning Rate | 1e-4 | 1e-4 | 1e-4 | 1e-4 | 1e-4 |\n| Transformer Weight Decay | 0.05 | 0.05 | 0.05 | 0.05 | 0.1 |\n| Other weight decay | 0.05 | 0.05 | 0.05 | 0.05 | 0.1 |\n| Batch Size | 512 | 512 | 512 | 512 | 512 |\n| Train Steps in Thousands | 30 | 25 | 15 | 30 | 300 |\n| \\(\\sigma_{\\text{max}}\\) | 80 | 80 | 80 | 80 | 80 |\n| \\(\\sigma_{\\text{min}}\\) | 0.001 | 0.001 | 0.001 | 0.001 | 0.001 |\n| \\(\\sigma_{t}\\) | 0.5 | 0.5 | 0.5 | 0.5 | 0.5 |\n| EMA | True | True | True | True | True |\n| Time steps | Exponential | Exponential | Exponential | Exponential | Exponential |\n| Sampler | DDIM | DDIM | DDIM | DDIM | DDIM |\n| Parameter Count (Millions) | 460 | 460 | 460 | 460 | 685 |", "caption": "Table 3: Summary of all the Hyperparameters for the MoDE policy used in our experiments.", "description": "This table provides a comprehensive overview of the hyperparameters used for the Mixture-of-Denoising-Experts (MoDE) policy in the described experiments. It details the specific settings for various components of the MoDE architecture, including the number of transformer layers, the number of experts used in the mixture-of-experts setup, the dimensions of embedding vectors, and the choice of image and goal encoders. Optimization parameters like the learning rate, weight decay, and choice of optimizer (AdamW) are also specified. The table covers configurations used for the CALVIN benchmarks (ABCD and ABC), the LIBERO benchmarks (LIBERO-10 and LIBERO-90), and a pretrained version of MoDE.  Additionally, details about data augmentation, batch size, training steps, sampler and parameter count are included to ensure reproducibility and facilitate comparison with other methods.", "section": "Method"}, {"content": "| Dataset | Weight |\n|---|---| \n| BC-Z | 0.258768 |\n| LIBERO-10 | 0.043649 |\n| BRIDGE | 0.188043 |\n| CMU Play-Fusion | 0.101486 |\n| Google Fractal | 0.162878 |\n| DOBB-E | 0.245176 |\n| **Total** | 1.000000 |", "caption": "Table 4: Dataset sampling weights used for training MoDE on a small subset of trajectories. The total dataset consists of 196k trajectories.", "description": "This table details how the Mixture-of-Denoising-Experts (MoDE) policy is pretrained on a diverse dataset of robot demonstrations from six different sources (BC-Z, LIBERO-10, BRIDGE, CMU Play-Fusion, Google Fractal, and DOBB-E). Each source has an associated sampling weight, reflecting its proportional contribution to the training mix.  This pretraining dataset totals 196k trajectories and aims to improve the model's generalization capabilities, especially in zero-shot settings.", "section": "A.1 PRETRAINING DETAILS"}, {"content": "| Benchmark | MoDE | DP-T | DP-CNN | Avg. Baseline | Improvement |\n|---|---|---|---|---|---| \n| CALVIN ABC\u2192D (norm.) | **0.678** | 0.226 | 0.270 | 0.248 | +151.1% |\n| CALVIN ABCD\u2192D (norm.) | **0.860** | 0.396 | 0.632 | 0.514 | +36.1% |\n| LIBERO-90 | **0.910** | 0.690 | 0.780 | 0.735 | +16.7% |\n| LIBERO-10 | **0.920** | 0.510 | 0.730 | 0.620 | +26.0% |\n| Average Improvement Over Second-Best: | **57.5%** | | | | |", "caption": "Table 5: Detailed Performance Improvement Analysis. CALVIN scores are normalized by dividing by 5 to align with LIBERO scale. Improvement calculated as: (MoDE - Avg. Baseline) / Avg. Baseline \u00d7 100. Final average is the mean of improvements across all four benchmarks compared to the second best Diffusion Policy variant on each one.", "description": "This table presents a detailed analysis of the performance improvement achieved by MoDE compared to other Diffusion Policy variants across four different benchmarks: CALVIN ABC\u2192D, CALVIN ABCD\u2192D, LIBERO-90, and LIBERO-10. The CALVIN scores are normalized to be on the same scale as LIBERO.  The improvement is calculated as the percentage difference between MoDE's performance and the average performance of the baseline models (Diffusion Transformer and Diffusion CNN).  The final average represents the overall improvement achieved by MoDE over the second-best performing variant across all four benchmarks.", "section": "4 Evaluation"}, {"content": "| | OpenVLA | | Octo Base | | MoDe (ours) | |\n|---|---|---|---|---|---|---| \n| **Metric** | **Score** | **Rank** | **Score** | **Rank** | **Score** | **Rank** |\n| Drawer Open | **16%** | **1** | 0% | 3 | 4.23% | 2 |\n| Drawer Close | 20% | 2 | 2% | 3 | **34.92%** | **1** |\n| Pick Can Horizontal | **71%** | **1** | 0% | 3 | 33.78% | 2 |\n| Pick Can Vertical | 27% | 2 | 0% | 3 | **29.78%** | **1** |\n| Pick Can Standing | **65%** | **1** | 0% | 3 | 36.44% | 2 |\n| Move Near | **48%** | **1** | 3% | 3 | 30% | 2 |\n| Drawer Open | 19% | 2 | 1% | 3 | **21.30%** | **1** |\n| Drawer Close | 52% | 2 | 44% | 3 | **76.85%** | **1** |\n| Pick Can Horizontal | **27%** | **1** | 21% | 3 | 22% | 2 |\n| Pick Can Vertical | 3% | 3 | 21% | 2 | **40%** | **1** |\n| Pick Can Standing | 19% | 2 | 9% | 3 | **35%** | **1** |\n| Move Near | **46%** | **1** | 4% | 3 | 45.42% | 2 |\n| Partial Put Spoon on Tablecloth | 4% | 3 | **35%** | **1** | 29.17% | 2 |\n| Put Spoon on Tablecloth | 0% | 3 | 12% | **1** | **12.5%** | **1** |\n| Partial Put Carrot on Plate | 33% | 2 | **53%** | **1** | 29.17% | 3 |\n| Put Carrot on Plate | 0% | 3 | 8% | **1** | **8.33%** | 1 |\n| Partial Stack Green Block on Yellow Block | 12% | 2 | **32%** | **1** | 8.33% | 3 |\n| Stack Green Block on Yellow Block | 0% | 2 | 0% | 2 | 0% | 2 |\n| Partial Put Eggplant in Basket | 8% | 3 | **67%** | **1** | 37.5% | 2 |\n| Put Eggplant in Basket | 4% | 3 | **43%** | **1** | 8.33% | 2 |\n| **Average** | 23.70% | 1.95 | 17.75% | 2.1 | **26.30%** | **1.65** |", "caption": "Table 6: Detailed comparison of MoDE against two sota Generalist Policies OpenVLA Kim et\u00a0al. (2024) and Octo Octo Model Team et\u00a0al. (2023) tested on all SIMPLER tasks with 2952 evals.", "description": "This table presents a thorough evaluation of MoDE, a novel diffusion policy, compared to two leading generalist policies, OpenVLA and Octo, on the SIMPLER benchmark.  SIMPLER offers real-to-sim tasks spanning diverse manipulation skills. The table details performance across specific tasks within the benchmark (e.g., drawer manipulation, object picking) showing score and rank for each model on each task. This comparison highlights MoDE's overall superior performance with limited compute, particularly on complex manipulation actions.", "section": "A.2.1 MODE EVALUATION ON SIMPLER"}, {"content": "| Model | Block Push | Relay Kitchen | CAL ABC | CAL ABCD | L-10 | Average |\n|---|---|---|---|---|---|---| \n| Dense T | 0.96\u00b10.02 | 3.73\u00b10.12 | **2.83\u00b10.19** | 4.13\u00b10.11 | 0.91\u00b10.02 | 0.839\u00b10.144 |\n| Token-Router | 0.97\u00b10.01 | **3.85\u00b10.03** | 2.67\u00b10.04 | 4.29\u00b10.08 | 0.90\u00b10.01 | 0.845\u00b10.161 |\n| \u03c3_t-Router | *0.97\u00b10.01* | 3.79\u00b10.04 | *2.79\u00b10.16* | **4.30\u00b10.02** | **0.92\u00b10.02** | **0.851\u00b10.151** |", "caption": "Table 7: Overview of the performance of all different token routing strategies used for MoDE across 5555 benchmarks.\nWe mark the best result for each environment in bold and the second best in cursive. We use CAL to represent CALVIN.\nTo average the results, we normalize all scores and compute the average over all environments.", "description": "This table compares the performance of different token routing strategies for the Mixture-of-Denoising-Experts (MoDE) policy across five benchmarks: Block Push, Relay Kitchen, CALVIN ABC, CALVIN ABCD, LIBERO-10.  Two routing strategies are evaluated: one based on token embeddings (Token-Router) and another based on noise level (\u03c3\u03c4-Router), along with a dense transformer baseline. The table presents average scores and standard deviations for each strategy across each benchmark, highlighting the best and second-best performing strategy with bold and cursive text respectively. CAL is used as an abbreviation for CALVIN.  The average performance is calculated by normalizing all scores and averaging them across all environments to determine the most effective routing strategy for MoDE across various tasks.", "section": "A.2.3 LIBERO BENCHMARK"}, {"content": "| Method | Active Params (M) | Total Params (M) | GFLOPS | PrT | Avg. Length | SF-Ratio | Inf. Time [ms] |\n|---|---|---|---|---|---|---|---|\n| Diff-P-CNN | 321 | 321 | 1.28 | \u00d7 | 1.35 | 1.05 | **11.7** |\n| Diff-P-T | 194 | 194 | 2.16 | \u00d7 | 1.13 | 0.53 | 16.2 |\n| RoboFlamingo | 1000 | 1000 | 690 | \u2713 | 2.47 | 0.004 | 65 |\n| SuSIE | 860+ | 860+ | 60 | \u2713 | 2.69 | 0.045 | 199 |\n| GR-1 | **130** | **130** | 27.5 | \u2713 | 3.06 | 0.11 | 12.6 |\n| **MoDE (ours)** | 436 | 740 | 1.53 | \u2713 | **4.01** | **2.6** | 12.2 |", "caption": "Table 8: Comparison of total and active number of parameters of methods used in the CALVIN benchmark. Additional overview of average FLOPS required by the different methods together with their average performance on the ABC benchmark. SF-Ratio compares average rollout length with GFLOPS.", "description": "This table presents a comparison of various methods used in the CALVIN benchmark, including Diffusion Policy (CNN and Transformer variants), Robo Flamingo, SuSIE, GR-1, and the proposed MoDE method. It compares the total number of parameters, the number of active parameters, the average GFLOPS (billions of floating-point operations per second) required for inference, whether or not the method uses pretraining, the average rollout length achieved, a performance-to-computational-cost ratio, and the average inference time per action.  This comparison helps demonstrate the efficiency and performance advantages of MoDE over existing state-of-the-art methods.", "section": "4.4. COMPUTATIONAL EFFICIENCY OF MODE"}, {"content": "| | Block Push | Relay Kitchen |\n|---|---|---| \n| C-BeT | 0.87\u00b1(0.07) | 3.09\u00b1(0.12) |\n| VQ-BeT | 0.87\u00b1(0.02) | 3.78\u00b1(0.04) |\n| BESO | 0.96\u00b1(0.02) | 3.73\u00b1(0.05) |\n| **MoDE** | **0.97\u00b1(0.01)** | **3.79\u00b1(0.02)** |", "caption": "Table 9: Comparison of the performance of different policies on the state-based goal-conditioned relay-kitchen and block-push environment averaged over 4444 seeds. MoDE outperforms the dense transformer variant BESO and other policy representations on all baselines.", "description": "This table compares the performance of several state-of-the-art goal-conditioned policies, including Mixture-of-Denoising-Experts (MoDE), on two simulated robotics environments: Relay Kitchen and Block Push.  The policies are evaluated based on their ability to achieve specified goals, like manipulating objects in a virtual kitchen or pushing blocks to target locations. The results show that MoDE outperforms other approaches, including a dense transformer model and methods using discrete latent action representations.", "section": "A.6 STATE-BASED EXPERIMENTS"}]