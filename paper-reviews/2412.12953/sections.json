[{"heading_title": "Scaling Diffusion Policies", "details": {"summary": "**Scaling diffusion policies** involves increasing model size and data to handle complex behaviors.  This poses a **computational challenge** due to larger models and numerous denoising steps.  **Mixture-of-Experts (MoE)** offers a promising solution by activating only a subset of parameters during each forward pass.  Noise-conditioned routing allows specialized experts for different denoising phases. Combining noise-conditioned self-attention further improves efficiency.  This enables **scaling model capacity while reducing computational costs**, which is crucial for real-time robotics applications."}}, {"heading_title": "MoDE Architecture", "details": {"summary": "The MoDE architecture presented leverages a Mixture of Denoising Experts (MoE) within a transformer framework for diffusion-based policies.  A **key innovation** is the **noise-conditioned routing**, where expert selection depends on the noise level during the denoising process. This specialization allows experts to focus on specific denoising phases and enables efficient **expert caching**, reducing computational overhead during inference. The design incorporates **noise-conditioned self-attention**  and noise input tokens for enhanced noise injection. These elements together enable MoDE to efficiently scale to larger model capacities while maintaining a smaller active parameter footprint. Notably, the architecture facilitates a flexible utilization of experts, allowing the model to dynamically adapt to the denoising requirements at different noise levels."}}, {"heading_title": "Noise-Conditioned Routing", "details": {"summary": "**Noise-conditioned routing** presents a novel approach in Mixture-of-Experts (MoE) models, where the routing of input tokens to expert networks is determined by the noise level in the data. This strategy proves particularly advantageous in Diffusion Policies, which are increasingly used for tasks like imitation learning in robotics.  Traditional MoE routing often relies on input content, while noise-conditioned routing allows for specialization of experts across different denoising phases in diffusion models. This specialization is crucial for efficient handling of the varying levels of noise present during the denoising process. It enables **improved performance and computational efficiency**, as experts can be pre-computed and cached based on noise level. This reduces computational overhead during inference. Furthermore, this routing method leads to **better scaling and generalization** compared to standard MoE approaches by allowing experts to focus on specific noise regimes rather than the entire data distribution."}}, {"heading_title": "Multitask Efficiency", "details": {"summary": "**MoDE**, a Mixture-of-Denoising-Experts policy, exhibits remarkable multitask efficiency in imitation learning.  Its **noise-conditioned routing** mechanism activates specific expert denoisers based on the noise level, enabling **sparse parameter usage and expert caching**. This significantly reduces computational costs, achieving up to **90% fewer FLOPs and faster inference** compared to dense transformer models.  Evaluations across diverse benchmarks like CALVIN and LIBERO show MoDE's superior performance on a wide range of tasks, confirming its potential for efficient and scalable multitask learning."}}, {"heading_title": "Expert Utilization Analysis", "details": {"summary": "**MoDE's noise-conditioned routing** enables dynamic expert allocation during action denoising.  Visualizations reveal specialized expert roles across noise levels.  Early layers differentiate strongly between high and low noise, while middle layers exhibit more distributed utilization, suggesting a transition from coarse to fine action refinement.  Later layers show a return to specialization, indicating a focus on final action adjustments.  The first layer prioritizes low-noise scenarios. Notably, different experts handle distinct noise ranges.  Some specialize in initial high-noise denoising, others in low-noise refinement, and some in transitional phases.  **This division of labor** across noise levels, without explicit supervision, highlights the effectiveness of noise-conditioned routing for enabling specialized denoising expertise within MoDE's architecture."}}]