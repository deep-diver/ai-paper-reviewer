{"importance": "This work introduces a versatile video inpainting framework, **MTV-Inpaint**, offering multimodal control and addressing long video challenges. It advances video editing capabilities, streamlines complex tasks, and opens avenues for future research in video generation and manipulation.", "summary": "MTV-Inpaint: A unified framework for multi-task long video inpainting, enabling versatile object insertion, scene completion, editing, and removal.", "takeaways": ["MTV-Inpaint unifies object insertion and scene completion in a single framework using a dual-branch spatial attention mechanism.", "The framework enhances controllability via I2V inpainting mode, integrating external image inpainting tools.", "A two-stage pipeline (keyframe inpainting + in-between) effectively handles long videos with improved temporal coherence."], "tldr": "Video inpainting, while powerful, often struggles with controllable object insertion and handling long videos. Existing methods primarily focus on scene completion and lack the flexibility to insert new objects with customized trajectories. Text-to-video diffusion models offer promise, but adapting them for inpainting is limited by task unification, input controllability, and long video processing issues.\n\nThis paper introduces **MTV-Inpaint**, a multi-task video inpainting framework built on a T2V diffusion model. It unifies object insertion and scene completion using a dual-branch spatial attention mechanism in the U-Net. It enhances control through an I2V mode, integrating image inpainting tools, and addresses long videos with a two-stage pipeline: keyframe inpainting followed by in-between frame propagation, ensuring temporal consistency.", "affiliation": "City University of Hong Kong", "categories": {"main_category": "Computer Vision", "sub_category": "Video Understanding"}, "podcast_path": "2503.11412/podcast.wav"}