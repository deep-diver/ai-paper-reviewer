{"references": [{"fullname_first_author": "Robin Rombach", "paper_title": "High-resolution image synthesis with latent diffusion models", "publication_date": "2022-01-01", "reason": "This paper introduces Latent Diffusion Models, which are foundational to many subsequent works in image and video generation, including the diffusion U-Net architecture used in MTV-Inpaint."}, {"fullname_first_author": "Jonathan Ho", "paper_title": "Denoising diffusion probabilistic models", "publication_date": "2020-01-01", "reason": "This paper lays the groundwork for denoising diffusion probabilistic models, a core technology used in MTV-Inpaint for generating images and videos from noise."}, {"fullname_first_author": "Yuwei Guo", "paper_title": "Animatediff: Animate your personalized text-to-image diffusion models without specific tuning", "publication_date": "2023-01-01", "reason": "This work offers a text-to-video generation approach adapted to inpainting, which shows how text prompt can drive diffusion models for video generation."}, {"fullname_first_author": "Jay Zhangjie Wu", "paper_title": "Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation", "publication_date": "2023-01-01", "reason": "This paper introduces a framework on text-to-video creation using one-shot tuning of image diffusion models, illustrating tuning for video generation."}, {"fullname_first_author": "Andreas Blattmann", "paper_title": "Stable video diffusion: Scaling latent video diffusion models to large datasets", "publication_date": "2023-01-01", "reason": "This paper introduces Stable Video Diffusion, a model which scales latent video diffusion models, and which can be adapted into in-paint framework."}]}