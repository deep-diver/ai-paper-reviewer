[{"figure_path": "https://arxiv.org/html/2412.03085/x2.png", "caption": "Figure 1: \nSamples generated by Mimir. Our model demonstrates a powerful spatiotemporal imagination for input text prompts, e.g., (row-3) physically accurate petals, (row-4) the desert with illumination harmonization, which closely match human cognition.", "description": "This figure showcases four video clips generated by the Mimir model, each illustrating its ability to vividly render detailed spatiotemporal scenes from concise text prompts.  The first row shows an eagle soaring over a snow-covered forest, highlighting the model's ability to create realistic and immersive natural landscapes. The second row depicts a woman in a red dress in a futuristic neon-lit setting, emphasizing its capacity for capturing atmospheric details and conveying a sense of mood.  The third row demonstrates the model's ability to generate physically accurate details, such as the petals of a flower in full bloom. Finally, the fourth row highlights Mimir's ability to produce videos with accurate and nuanced illumination, as seen in the desert landscape which seamlessly transitions from day to night.", "section": "Abstract"}, {"figure_path": "https://arxiv.org/html/2412.03085/x3.png", "caption": "Figure 2: The core idea of Mimir.\nText Encoder is well suited for fine-tuning pre-trained T2V models\u00a0(\u2713), however it struggles with limited text comprehension\u00a0(\u2717).\nIn contrast, Decoder-only LLM excels at precise text understanding\u00a0(\u2713), but cannot be directly used in established video generation models since the feature distribution gap and the feature volatility\u00a0(\u2717) .\nTherefore, we propose the token fuser in Mimir to harmonize multiple tokens, achieving precise text understanding\u00a0(\u2713) in T2V generation\u00a0(\u2713).", "description": "The figure illustrates the core concept of the Mimir model.  Traditional text encoders, while effective for fine-tuning existing text-to-video (T2V) models, suffer from limitations in text comprehension.  Conversely, decoder-only large language models (LLMs) excel at understanding text but cannot be directly integrated into existing T2V models due to differences in feature distributions and the inherent instability of their features. The Mimir model overcomes this challenge by introducing a token fuser that harmonizes the outputs from both text encoders and LLMs, enabling precise text understanding and high-quality video generation.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2412.03085/x4.png", "caption": "Figure 3: The framework of Mimir. Given a text prompt, we employ a text encoder and a decoder-only large language model to obtain e\u03b8subscript\ud835\udc52\ud835\udf03e_{\\theta}italic_e start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT and e\u03b2subscript\ud835\udc52\ud835\udefde_{\\beta}italic_e start_POSTSUBSCRIPT italic_\u03b2 end_POSTSUBSCRIPT. Additionally, we add an instruction prompt which, after processing by the decoder-only model, yields the corresponding instruction token eisubscript\ud835\udc52\ud835\udc56e_{i}italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. See token details in Sec.\u00a02.2. To prevent any convergence issue in training caused by the feature distribution gap of e\u03b8subscript\ud835\udc52\ud835\udf03e_{\\theta}italic_e start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT and e\u03b2subscript\ud835\udc52\ud835\udefde_{\\beta}italic_e start_POSTSUBSCRIPT italic_\u03b2 end_POSTSUBSCRIPT, the proposed token fuser first applies a normalization layer and a learnable scale to e\u03b2subscript\ud835\udc52\ud835\udefde_{\\beta}italic_e start_POSTSUBSCRIPT italic_\u03b2 end_POSTSUBSCRIPT. It then uses Zero-Conv to preserve the original semantic space in the early of training. These modified tokens are then summed to produce e\u2208\u211dn\u00d74096\ud835\udc52superscript\u211d\ud835\udc5b4096e\\in\\mathbb{R}^{n\\times 4096}italic_e \u2208 blackboard_R start_POSTSUPERSCRIPT italic_n \u00d7 4096 end_POSTSUPERSCRIPT. Meanwhile, we initialize four learnable tokens elsubscript\ud835\udc52\ud835\udc59e_{l}italic_e start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT, which are added to eisubscript\ud835\udc52\ud835\udc56e_{i}italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT to stabilize divergent semantic features. Finally, the token fuser concatenates e\ud835\udc52eitalic_e and essubscript\ud835\udc52\ud835\udc60e_{s}italic_e start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT to generate videos.", "description": "Mimir uses a text encoder and a decoder-only large language model to process text prompts, generating embeddings  e<sub>\u03b8</sub> and e<sub>\u03b2</sub> respectively.  An instruction prompt is also processed by the decoder-only model, creating instruction token e<sub>i</sub>. To address potential training convergence issues due to differing feature distributions between e<sub>\u03b8</sub> and e<sub>\u03b2</sub>, Mimir's token fuser normalizes and scales e<sub>\u03b2</sub>, applies Zero-Conv to preserve semantic space, and sums the results with e<sub>\u03b8</sub> to produce a combined embedding e \u2208 \u211d<sup>n x 4096</sup>. Four learnable tokens e<sub>l</sub> are initialized and added to e<sub>i</sub> to stabilize semantic features, resulting in e<sub>s</sub>. Finally, the fuser concatenates e and e<sub>s</sub> to generate the video.", "section": "2. Methodology"}, {"figure_path": "https://arxiv.org/html/2412.03085/x5.png", "caption": "Figure 4: Comparison between CogVideoX-5B with Mimir in T2V, where Mimir generates the vivid stunning moment of rocket launch.", "description": "This figure showcases a comparison of video generation results between two models: CogVideoX-5B and Mimir. The models were prompted to generate a video of a rocket launch.  The comparison highlights Mimir's superior ability to generate more vivid and stunning visuals of the rocket launch, with more realistic details and a more dynamic depiction of the event.", "section": "3. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.03085/x6.png", "caption": "Figure 5: Mimir demonstrates spatial comprehension and imagination, e.g., quantities, spatial relationships, colors, etc.", "description": "This figure showcases Mimir's ability to understand and generate images based on complex spatial descriptions.  It demonstrates the model's capabilities in several key areas: \n\n* **Quantities:** Accurately representing the specified number of objects (e.g., \"two apples,\" \"three apples\").\n* **Spatial Relationships:** Correctly positioning objects relative to each other (e.g., \"sitting to the left of,\" \"sitting to the right of\").\n* **Colors:** Accurately rendering the specified colors of objects (e.g., \"neon pink elephant,\" \"chartreuse zebra\").\n\nThe examples illustrate Mimir's capacity for spatial reasoning and creative image synthesis beyond simply following basic instructions.", "section": "3. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.03085/x7.png", "caption": "Figure 6: Mimir demonstrates temporal comprehension and imagination, e.g., direction, order of motion and appearance / disappearance.", "description": "Figure 6 showcases Mimir's ability to understand and generate videos depicting temporal concepts such as direction of movement, sequences of actions, and appearances/disappearances of objects over time.  Examples include a puppy looking left and right, a lion looking right and then left, a bird turning its head and then flapping its wings, a rabbit raising its ears and then jumping, and the moon changing phases. The figure highlights Mimir's capability to accurately represent and synchronize these temporal elements within the generated video content.", "section": "3. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.03085/x8.png", "caption": "Figure 7: Visualization by t-SNE: (a) Given 50 prompts, we obtain the corresponding tokens using Encoder branch, Decoder-only branch and their sum, i.e., Mimir. (b) We feed one prompt into Decoder-only branch for 50 times to generate 50 query tokens, answer tokens and final tokens. Differences in feature distribution: (c) The original distribution of T5 encoder and Phi-3.5 Decoder. (d) The distribution of T5 encoder and Phi-3.5 Decoder after normalization across different value ranges.", "description": "This figure uses t-SNE to visualize the token distributions from different components of the Mimir model. Panel (a) shows the combined token embeddings from the text encoder and decoder-only LLM for 50 different prompts, representing the Mimir model's integrated output. Panel (b) demonstrates the variability of the decoder-only LLM by showing 50 separate sets of query, answer, and final tokens generated from the same prompt, highlighting the LLM's inherent dynamism and capacity for diverse outputs.  Panels (c) and (d) compare the distributions of T5 encoder tokens and Phi-3.5 decoder tokens, showing the difference between their original distributions (c) and how normalization brings them closer together (d), illustrating the effectiveness of the normalization strategy used in Mimir's token fuser.", "section": "Visualization by t-SNE"}, {"figure_path": "https://arxiv.org/html/2412.03085/x9.png", "caption": "Figure 8: We present more cases generated by Mimir.", "description": "Figure 8 showcases additional examples of videos generated by the Mimir model.  These examples highlight Mimir's ability to generate high-quality videos from various text prompts, demonstrating the model's capabilities in handling different aspects of video generation including detailed descriptions of scenes, animation, and complex spatiotemporal relationships. The examples serve as further evidence of Mimir's strong text comprehension and video generation capabilities.", "section": "3. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.03085/x10.png", "caption": "Figure 9: The pipeline for preparing data.", "description": "This figure illustrates the data preprocessing pipeline used in the paper.  The pipeline consists of several stages: First, collected videos undergo basic filtration, removing videos that do not meet basic criteria (e.g., too short, incorrect aspect ratio). Next, quality filtration removes videos with poor quality based on metrics like black area percentage, brightness, and black frame rate. Aesthetic filtration removes videos that do not meet aesthetic criteria. Finally, watermark filtration removes videos containing watermarks and a re-captioning step generates new captions for the videos. The result of this pipeline is a set of high-quality videos with accurate captions, ready for use in training the model. ", "section": "A. Data Processing"}, {"figure_path": "https://arxiv.org/html/2412.03085/x11.png", "caption": "Figure 10: The comparison between results with short & course prompts and long & fine prompts.", "description": "This figure compares the video generation results of Mimir using both short, imprecise prompts and long, detailed prompts.  It showcases Mimir's ability to generate high-quality videos even with concise instructions, highlighting the effectiveness of its token fuser in expanding semantic understanding.", "section": "Additional Experimental Results"}, {"figure_path": "https://arxiv.org/html/2412.03085/x12.png", "caption": "Figure 11: More examples in terms of color rendering.", "description": "This figure showcases the model's ability to accurately render colors specified in text prompts.  It demonstrates that the model can generate videos where objects have colors that precisely match the input descriptions.  This is particularly impressive in cases with multiple objects, each requiring a different, specific color. The color accuracy highlights the effectiveness of the proposed token fusion method in ensuring semantic consistency between the text and the generated video.", "section": "D.2. More Interesting Prompts"}, {"figure_path": "https://arxiv.org/html/2412.03085/x13.png", "caption": "Figure 12: More examples in terms of absolute & relative position.", "description": "This figure demonstrates the model's ability to understand and accurately represent spatial relationships described in text prompts.  It showcases examples where objects are correctly positioned relative to each other (e.g., left, right, top, bottom) and examples demonstrating understanding of absolute positions.", "section": "D.2.2 Temporal Semantic Understanding"}, {"figure_path": "https://arxiv.org/html/2412.03085/x14.png", "caption": "Figure 13: More examples in terms of counting.", "description": "This figure visually demonstrates Mimir's capacity for accurate quantitative understanding.  It presents several examples of video clips generated based on prompts specifying precise counts of objects. Each row features a different prompt and showcases the video sequence generated by Mimir, correctly reflecting the stated number of objects (e.g., one lotus flower, two butterflies, three birds). This highlights the model's ability to process and accurately represent numerical information within the video generation process.", "section": "D.2. More Interesting Prompts"}, {"figure_path": "https://arxiv.org/html/2412.03085/x15.png", "caption": "Figure 14: More examples in terms of action sequence over time.", "description": "This figure showcases Mimir's ability to generate videos depicting sequential actions performed by an object over time.  Multiple example videos are shown, each demonstrating accurate temporal understanding.  The model correctly sequences actions, even in scenarios involving more complex patterns of motion.", "section": "D.2.2. Temporal Semantic Understanding"}, {"figure_path": "https://arxiv.org/html/2412.03085/x16.png", "caption": "Figure 15: More examples in terms of light changes, showcasing the illumination harmonization over time.", "description": "This figure showcases the model's ability to realistically depict changes in lighting conditions over time.  The examples demonstrate a smooth and natural progression of light, from the gradual fading of stars at dawn to the fiery brilliance of a sunrise over the sea, and finally to the subtle illumination of sunlight filtering through forest leaves in the evening, and ending with moonlight reflecting on a glacier. The model accurately captures and harmonizes changes in illumination, including transitions between day and night and subtle shifts in light intensity and color.", "section": "D.2.2. Temporal Semantic Understanding"}]