[{"figure_path": "https://arxiv.org/html/2503.19757/x1.png", "caption": "Figure 1: Illustrations of different generalist robot policy architectures. Left head: the common robot Transformer architecture with discretization actions, e.g., Robot Transformer\u00a0[8, 9] and OpenVLA\u00a0[32]. Middle head: the Transformer architecture with diffusion action head which denoises the individual continuous action with a small network condition on each embedding from the causal Transformer, e.g., Octo\u00a0[72] and \u03c00subscript\ud835\udf0b0\\pi_{0}italic_\u03c0 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT\u00a0[5]. Right head: the proposed Dita architecture that denoises actions inherently in an in-context conditioning style.", "description": "Figure 1 illustrates three different approaches to building generalist robot policy architectures. The leftmost architecture represents traditional robot transformers that utilize discrete actions.  Examples include Robot Transformer and OpenVLA. The middle architecture shows a transformer with a diffusion action head. This approach denoises continuous actions using a small network conditioned on embeddings from a causal transformer.  Octo and  \u03c00 are examples of this approach. The rightmost architecture is the proposed Dita architecture.  Dita denoises actions directly within the context of its conditioning.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.19757/x2.png", "caption": "Figure 2: Our model employs a Transformer-based diffusion architecture, integrating a pretrained CLIP network to extract language instruction tokens. The DinoV2\u00a0[53] model encodes image observations, followed by a Q-Former that queries features for each image. The instruction tokens, image features, timestep embeddings, and noised action are concatenated to construct a token sequence, which is then fed into the network to denoise the raw actions.", "description": "This figure illustrates the architecture of Dita, a Transformer-based diffusion model for generalist robotic learning.  Language instructions are processed by a pretrained CLIP model to generate instruction tokens.  Image observations are encoded using a pretrained DINOv2 model, and a Q-Former network selects relevant image features based on the instruction context.  These instruction tokens, image features, timestep embeddings, and a noised version of the action are concatenated into a single sequence. This sequence is then fed into a causal Transformer network, which denoises the action sequence to generate the final, refined action.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.19757/x3.png", "caption": "Figure 3: The experimental platform consists of Franka Emika Panda robot arm, Robotiq 2F-85 gripper and RealSense D435i positioned in third-person view.", "description": "The figure shows the experimental setup used for the real-robot experiments.  It consists of a Franka Emika Panda robot arm equipped with a Robotiq 2F-85 gripper. A RealSense D435i depth camera is positioned to provide a third-person view of the robot's workspace, capturing RGB-D images during the experiments. This setup enables the robot to perform various manipulation tasks while the camera provides visual input for the vision-language-action model.", "section": "5. Real-Robot Experiments"}, {"figure_path": "https://arxiv.org/html/2503.19757/x4.png", "caption": "Figure 4: Quantitative results in real-robot experiments. Each task is manually divided into two sequential steps, except for the last two single-step tasks. In each stacked bar, the light-colored region represents the model\u2019s success rate in the first stage, while the dark-colored region indicates the contribution of second-stage success to the overall success rate. A larger proportion of the dark-colored region signifies a stronger capability of the model in completing long-horizon tasks. Since the open/close drawer tasks are single-step, they are excluded from the calculation of the average success rate.", "description": "This figure presents quantitative results from real-robot experiments evaluating the success rate of different tasks. Each task is broken down into two sequential steps (except for the last two, which are single-step). The stacked bars visualize the success rate: the light-colored portion shows the first-stage success, and the dark-colored portion represents the contribution of the second stage to the overall success. A larger dark-colored area indicates better performance on long-horizon tasks. Single-step tasks (opening/closing drawers) are excluded from the average success rate calculation.", "section": "5. Real-Robot Experiments"}, {"figure_path": "https://arxiv.org/html/2503.19757/x5.png", "caption": "Figure 5: Qualitative comparison in real-robot experiments. Failures are highlighted with red circles. For a direct comparison, we initialize the layout consistently across all methods.", "description": "This figure presents a qualitative comparison of Dita's performance against other methods (Octo, OpenVLA, and a diffusion head policy baseline) in real-world robotic experiments.  The image shows the results of several trials, with failures indicated by red circles. Notably, all methods started from the same initial setup, providing a direct visual comparison of their ability to successfully complete a series of manipulation tasks.", "section": "5. Real-Robot Experiments"}, {"figure_path": "https://arxiv.org/html/2503.19757/x6.png", "caption": "Figure 6: Qualitative results of Dita under variances in Google Robot.", "description": "This figure showcases the robustness of the Dita model across various challenging conditions.  It presents qualitative results demonstrating the model's ability to successfully complete tasks even with changes in background, object arrangements, and lighting.  Each row represents a different task performed by the robot under these varied conditions, visually demonstrating the model's generalizability and resilience.", "section": "4 Simulation Experiments"}, {"figure_path": "https://arxiv.org/html/2503.19757/x7.png", "caption": "Figure 7: Qualitative results of Dita on LIBERO benchmark.", "description": "This figure showcases qualitative results obtained by Dita on the LIBERO benchmark.  It provides a visual comparison of Dita's performance across various tasks within the LIBERO dataset, illustrating the model's ability to handle diverse scenarios and object interactions. The images likely demonstrate successful and unsuccessful attempts at completing tasks, highlighting Dita's strengths and weaknesses in different contexts.", "section": "4. Simulation Experiments"}, {"figure_path": "https://arxiv.org/html/2503.19757/x8.png", "caption": "Figure 8: Qualitative results of Dita on CALVIN ABC\u2192\u2192\\rightarrow\u2192D benchmark.", "description": "This figure showcases qualitative results from the CALVIN ABC\u2192D benchmark, demonstrating the performance of the Dita model on long-horizon tasks.  Each row represents a distinct task, illustrating the model's ability to successfully complete complex sequences of actions. The images depict both successful and failed attempts, offering a visual comparison of Dita's capabilities and robustness across a range of manipulations.", "section": "4. Simulation Experiments"}, {"figure_path": "https://arxiv.org/html/2503.19757/x9.png", "caption": "Figure 9: Qualitative comparison between Dita (top) and Diffusion Action Head baseline \u2130\u03b8\u223csD\u2062i\u2062f\u2062fsuperscriptsubscript\u2130similar-to\ud835\udf03\ud835\udc60\ud835\udc37\ud835\udc56\ud835\udc53\ud835\udc53\\mathcal{E}_{\\theta\\sim s}^{Diff}caligraphic_E start_POSTSUBSCRIPT italic_\u03b8 \u223c italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_D italic_i italic_f italic_f end_POSTSUPERSCRIPT (bottom) on ManiSkill2 (PickClutterYCB).", "description": "This figure presents a qualitative comparison of the performance of the proposed Dita model and a baseline diffusion action head model on the ManiSkill2 benchmark's PickClutterYCB task. It visually showcases the results, allowing for a direct comparison of the model's ability to successfully complete the task.  The top row illustrates Dita's execution, while the bottom row displays the results of the baseline diffusion action head model.  This comparison highlights the differences in the approach taken by each model to complete the task and the resulting successes and failures.", "section": "4.5 ManiSkill2"}, {"figure_path": "https://arxiv.org/html/2503.19757/extracted/6301744/ICLR2025/convergen_analysis.jpg", "caption": "Figure 10: Convergence Analysis on OXE dataset\u00a0[9]. The blue line is DiT Policy, and the orange line is Diffusion action head strategy with the same number of parameters.", "description": "This figure displays a comparison of the convergence speed between two different approaches for training a robot policy: the DiT (Diffusion Transformer) Policy and a simpler Diffusion Action Head strategy.  Both methods were trained on the OXE dataset and used the same number of parameters for a fair comparison.  The x-axis represents the training steps, while the y-axis shows the MSE (Mean Squared Error) loss, a measure of the model's error during training.  The blue line illustrates the convergence of the DiT Policy, and the orange line shows the convergence of the Diffusion Action Head strategy. The graph visually demonstrates that the DiT Policy converges faster and achieves a lower MSE loss than the Diffusion Action Head approach.", "section": "D. Analysis, Ablations, and Discussions"}]