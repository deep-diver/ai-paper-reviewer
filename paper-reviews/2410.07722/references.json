{"references": [{" publication_date": "2019", "fullname_first_author": "Jacob Devlin", "paper_title": "Bert: Pre-training of deep bidirectional transformers for language understanding", "reason": "This paper introduces BERT, a powerful pre-trained transformer model that is widely used in various NLP tasks, including the LSR models described in this paper. BERT's contextualized word embeddings are crucial for achieving state-of-the-art performance in many LSR models.  The effectiveness and influence of BERT is repeatedly emphasized in the current paper, making this foundational paper very important.", "section_number": 1}, {" publication_date": "2019", "fullname_first_author": "Rodrigo Frassetto Nogueira", "paper_title": "Passage re-ranking with BERT", "reason": "This paper is highly relevant because it explores the use of BERT for passage re-ranking, a key component of two-stage retrieval systems including those using LSR.  The paper's findings and techniques directly inform and inspire the current paper's approach to improving LSR performance by incorporating contextualized embeddings and re-ranking.", "section_number": 1}, {" publication_date": "2021", "fullname_first_author": "Thibault Formal", "paper_title": "Splade: Sparse lexical and expansion model for first stage ranking", "reason": "SPLADE is a state-of-the-art LSR model that uses a masked language model (MLM) architecture for both query and document encoders.  This method forms the basis of the sparse encoders employed in the current work, making SPLADE a crucial reference point for understanding and enhancing LSR systems.  Understanding the strengths and weaknesses of SPLADE is critical to the DyVo approach presented in the paper.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Thibault Formal", "paper_title": "Splade: Sparse lexical and expansion model for first stage ranking", "reason": "This paper extends the work in SPLADE (2021) presenting further improvements and refinements.  Given that SPLADE is a foundation for the current work, this paper is crucial in understanding and refining the core methodology of LSR.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Jeffrey M Dudek", "paper_title": "Learning sparse lexical representations over specified vocabularies for retrieval", "reason": "This paper explores enriching sparse representations by using additional vocabularies in a context that directly relates to the current paper's aim of enriching sparse representations. This work provides insights into methods for expanding LSR's capabilities and handling expanded vocabularies, making it highly relevant to understanding techniques for incorporating entities into LSR.", "section_number": 1}, {" publication_date": "2020", "fullname_first_author": "Jimmy Lin", "paper_title": "Pretrained transformers for text ranking: BERT and beyond", "reason": "This is an important paper on text ranking using pretrained transformers, a topic central to the paper. Understanding the state-of-the-art and various approaches in this area is crucial for positioning the current research and assessing the novel contributions of this work.", "section_number": 1}, {" publication_date": "2019", "fullname_first_author": "Sean MacAvaney", "paper_title": "Cedr: Contextualized embeddings for document ranking", "reason": "This paper introduces CEDR, which uses contextualized embeddings for document ranking. CEDR is related to the current paper's focus on improving LSR through richer representations.  It provides insights into relevant techniques and potential enhancements that have informed the current research.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Amin Abolghasemi", "paper_title": "Measuring bias in a ranked list using term-based representations", "reason": "This paper explores biases in ranked lists, which is directly relevant to the fairness and trustworthiness of the LSR models discussed in this paper. Understanding how to identify and mitigate biases is essential for producing robust and reliable LSR systems, and therefore, this is a relevant reference.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Carlos Lassance", "paper_title": "An efficiency study for splade models", "reason": "This paper studies efficiency aspects of SPLADE models, a state-of-the-art LSR model that is central to the research described in this paper. Since this paper is focusing on developing efficient methods for retrieval, it is very important for the reader to know the efficiency of the methods.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Negar Arabzadeh", "paper_title": "Laque: Enabling entity search at scale", "reason": "This paper presents LaQue, a powerful entity search model used for generating entity embeddings in the current work.  LaQue's performance and capabilities are directly relevant to evaluating the effectiveness of the current approach to entity retrieval and integration into LSR.", "section_number": 3}, {" publication_date": "2020", "fullname_first_author": "Ikuya Yamada", "paper_title": "Wikipedia2Vec: An efficient toolkit for learning and visualizing the embeddings of words and entities from Wikipedia", "reason": "Wikipedia2Vec is a key tool in the current work for generating entity embeddings. This paper is vital in understanding the source of these embeddings, its capabilities, and potential limitations, all of which impact the performance and accuracy of the overall system.", "section_number": 3}, {" publication_date": "2021", "fullname_first_author": "Fabio Petroni", "paper_title": "KILT: a benchmark for knowledge intensive language tasks", "reason": "KILT is a benchmark dataset used for evaluating the performance of knowledge-intensive language tasks, relevant to the current work, which aims to improve LSR by incorporating knowledge from external sources.  The use of KILT highlights the importance of benchmarking and evaluating performance based on widely-recognized standards.", "section_number": 3}, {" publication_date": "2021", "fullname_first_author": "Nicola De Cao", "paper_title": "Autoregressive entity retrieval", "reason": "This paper introduces an autoregressive approach for entity retrieval, which offers an alternative method to the current approach based on LLMs.  Understanding this alternative method helps contextualize the current approach and evaluate its novelty and potential benefits.", "section_number": 3}, {" publication_date": "2022", "fullname_first_author": "Emma J Gerritse", "paper_title": "Entity-aware transformers for entity search", "reason": "This work explores entity-aware transformers for entity search, relevant to this paper's task of incorporating entities into LSR.  The insights from this paper help contextualize the use of transformers and entity embeddings within the current LSR model and demonstrate an approach to integrating entity information into search.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Shubham Chatterjee", "paper_title": "Dreq: Document re-ranking using entity-based query understanding", "reason": "This paper explores document re-ranking methods using entity-based query understanding, relevant to the current paper's task of improving re-ranking by incorporating entities. This reference is crucial for illustrating the state-of-the-art in this specific area and for understanding the limitations of existing methods.  The paper provides insights and a comparison to inform the current research.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Canjia Li", "paper_title": "Parade: Passage representation aggregation fordocument reranking", "reason": "This paper explores passage representation aggregation for document reranking, relevant to the current paper's focus on improving LSR re-ranking by incorporating entities.  This paper gives insights into techniques for enhancing the re-ranking phase of a two-stage retrieval system, which directly influences the performance of LSR.", "section_number": 3}, {" publication_date": "2022", "fullname_first_author": "Sean MacAvaney", "paper_title": "Streamlining evaluation with ir-measures", "reason": "This paper introduces ir_measures, the toolkit used for evaluating the performance of information retrieval systems in the current work. It provides crucial information on the evaluation metrics used and the methods for conducting a thorough evaluation of the proposed retrieval model, ensuring accuracy and reliability of the results.", "section_number": 4}, {" publication_date": "2022", "fullname_first_author": "Nils Reimers", "paper_title": "Sentence-bert: Sentence embeddings using siamese bert-networks", "reason": "Sentence-BERT is a well-known model for generating sentence embeddings that this research uses as one of the baselines for experiments. This paper is foundational to understanding the methodology of sentence embedding and the comparison with the proposed methods.", "section_number": 5}, {" publication_date": "2022", "fullname_first_author": "Jianmo Ni", "paper_title": "Sentence-T5-base", "reason": "This paper presents Sentence-T5-base, a large language model used as a baseline in this paper's experiments.  Understanding the performance and characteristics of this model is essential for establishing a strong baseline and accurately assessing the improvements achieved by the proposed methods.", "section_number": 5}]}