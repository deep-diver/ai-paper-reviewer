[{"figure_path": "2410.12957/figures/figures_4_0.png", "caption": "Figure 1: The architecture of MuVi. The main model and the input/output are illustrated in the middle, where the visual encoder is frozen during the training stage. The visual compression strategies are listed on the left, where \"CLS\" indicates the CLS token of certain visual encoders, such as CLIP. The architecture of the diffusion Transformer is illustrated on the right.", "description": "This figure illustrates the architecture of MuVi, a video-to-music generation framework, showing its visual encoder, visual adaptor, and diffusion transformer-based music generator.", "section": "3 METHOD"}, {"figure_path": "2410.12957/figures/figures_8_0.png", "caption": "Figure 2: Visualization of the attention distribution of Softmax aggregation. The yellower the patch, the more it is related to the generated music. We mask the video frames with the averaged attention scores. We transform the patches corresponding to the weights after applying Softmax into masks, and then adjust the colors of the masks accordingly. When the weights are smaller (close to 0.0), the mask appears bluer; conversely (close to 1.0), it appears yellower. This reflects the attention distribution of the adaptor.", "description": "Figure 2 visualizes the attention distribution of the Softmax aggregation method used in the MuVi model, showing how the model focuses on specific parts of the video frames when generating music.", "section": "3.1 VISUAL REPRESENTATION MODELING"}]