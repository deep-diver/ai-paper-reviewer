[{"figure_path": "https://arxiv.org/html/2501.12599/x1.png", "caption": "Figure 1: Kimi k1.5 long-CoT results.", "description": "This figure presents a comparison of the Kimi K1.5 model's performance on various reasoning benchmarks against other leading language models, both open-source and proprietary.  The benchmarks cover different domains and modalities including mathematics (AIME 2024, MATH 500), code (Codeforces, LiveCodeBench), and vision-language tasks (MathVista).  The bar chart visually displays the superior performance of Kimi K1.5 in achieving state-of-the-art results across most tasks.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2501.12599/x3.png", "caption": "Figure 2: Kimi k1.5 short-CoT results.", "description": "This figure presents a comparison of Kimi K1.5's performance on various short-Chain of Thought (short-CoT) reasoning tasks against other leading language models, both open-source and proprietary.  It showcases Kimi K1.5's performance across multiple benchmarks and modalities (Math, Code, Vision), highlighting its state-of-the-art results and significant improvements over existing models.", "section": "2 Approach: Reinforcement Learning with LLMs"}, {"figure_path": "https://arxiv.org/html/2501.12599/x4.png", "caption": "(a) System overview", "description": "This figure provides a high-level overview of the large-scale reinforcement learning training system used for the LLM. It illustrates the interactions between various components, including rollout workers, trainer workers, a central master, reward models, and a replay buffer. The rollout workers generate rollout trajectories by interacting with the model, while the trainer workers update model weights based on the experiences stored in the replay buffer. The central master coordinates these processes, ensuring efficient data processing. Reward models evaluate the quality of model outputs, and the replay buffer enables diverse and unbiased data for training.", "section": "2.6 RL Infrastructure"}, {"figure_path": "https://arxiv.org/html/2501.12599/x5.png", "caption": "(b) Partial Rollout", "description": "This figure illustrates the partial rollout mechanism used in the large-scale reinforcement learning system for LLMs.  Partial rollouts optimize the handling of complex reasoning trajectories by managing rollouts of both long and short trajectories.  If a trajectory exceeds a token limit, the unfinished portion is saved and continued in the next iteration. This ensures that no single lengthy trajectory monopolizes resources, and that shorter rollouts can be processed while longer ones are underway, maximizing efficiency.  The diagram shows how segments of long responses are stored in a replay buffer and used across iterations, reducing computational overhead.", "section": "2.6 RL Infrastructure"}]