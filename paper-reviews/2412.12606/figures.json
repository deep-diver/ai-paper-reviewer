[{"figure_path": "https://arxiv.org/html/2412.12606/x1.png", "caption": "Figure 1: The MDI-Benchmark includes real needs of different age groups in six major real-world scenarios.", "description": "The figure showcases example images from the Multi-Dimensional Insights (MDI) Benchmark, categorized by real-world scenarios relevant to different age groups.  These scenarios include kitchen and home layouts (Architecture), library and campus scenes (Education), household items like laundry detergent and a digital clock (Housework), restaurant menus and travel information (Social Services), sports images like basketball and racing (Sport), and airport departure boards and road signs (Transport).  Each image is paired with multiple questions that aim to gauge an LMM's understanding of the scene and its ability to address the specific needs of different age demographics.", "section": "MDI-Benchmark"}, {"figure_path": "https://arxiv.org/html/2412.12606/x2.png", "caption": "Figure 2: The overview of the MDI Benchmark\u2019s six real-world multimodal scenarios, each comprising three sub-domains.", "description": "The figure provides a high-level overview of the six real-world multimodal scenarios included in the MDI Benchmark. Each scenario, representing a different aspect of daily human life, is further divided into three more specific sub-domains, resulting in a total of 18 distinct sub-domains.  These scenarios and sub-domains are:\n\n*   **Architecture:** House Planning, Work Scenes, Measuring\n*   **Education:** Studying, Teaching, Campus\n*   **Housework:** Home Arrangements, Housework Activities, Household Appliances\n*   **Social Services:** Travel, Shopping, Communal Facilities\n*   **Sport:** Powerlifting, Race, Ball\n*   **Transport:** Signpost, Rail transit, Airport", "section": "3 MDI-BENCHMARK"}, {"figure_path": "https://arxiv.org/html/2412.12606/x3.png", "caption": "Figure 3: The average performance of different LMMs on different difficulty levels of the MDI-Benchmark.", "description": "This bar chart compares the performance of different Large MultiModal Models (LMMs) on two difficulty levels (Level 1 and Level 2) of the Multi-Dimensional Insights (MDI) Benchmark.  Level 1 represents easier tasks focused on basic perception, while Level 2 involves more complex reasoning tasks. The chart visually displays the accuracy scores achieved by each LMM on both levels, indicating their strengths and weaknesses in handling different task complexities. The LMMs evaluated include GPT-40, GPT-4V, Gemini 1.5 Pro, Qwen-VL-Plus, and various open-source models.", "section": "4 EXPERIMENTS"}, {"figure_path": "https://arxiv.org/html/2412.12606/x4.png", "caption": "Figure 4: Performance of the model at different difficulty levels and the overall performance results of the model under the score metric.", "description": "This figure visualizes the performance of Large Multimodal Models (LMMs) on the Multi-Dimensional Insights (MDI) Benchmark across different complexity levels (Level 1 and Level 2).  **(a) Leaderboard:**  Ranks various open-source LMMs based on their overall performance scores, calculated using a weighted average of Level 1 and Level 2 accuracies. The size of the model (in billions of parameters) is plotted against the score, showcasing the impact of model scale on performance.  **(b) Radar Charts:** Two radar charts illustrate the performance of 14 LMMs (both open-source and closed-source) across six real-world scenarios (Architecture, Education, Housework, Social Service, Sport, and Transport) at both Level 1 (basic perception) and Level 2 (logical reasoning). These charts visually compare the models' strengths and weaknesses in different scenarios and at different levels of complexity. **(c) Bar and Variance Charts:** These charts display the average accuracy and variance of the LMMs across the six scenarios at both Level 1 and Level 2. This visualization helps analyze the models' overall performance and consistency across different tasks.", "section": "4 Experiments"}, {"figure_path": "https://arxiv.org/html/2412.12606/x6.png", "caption": "Figure 5: The average accuracy and variance of LLMs across six domains at Level 1 and Level 2", "description": "This figure presents bar plots illustrating the average accuracy and variance of various Large Multimodal Models (LMMs) across six domains at two levels of question complexity (Level 1 and Level 2). Level 1 focuses on basic perceptual abilities, while Level 2 involves logical reasoning. The x-axis in both subfigures represents different LMMs, with closed-source models (GPT-40, GPT-4V, Gemini 1.5 Pro, Qwen-VL-Plus) grouped on the left and open-source models to the right. The first subfigure displays the average accuracy (y-axis) of each LMM for Level 1 and Level 2, showing the overall performance trends. The second subfigure depicts the variance in accuracy (y-axis), indicating the consistency of model performance across domains. Lower variance suggests more stable performance. This analysis reveals which models perform consistently and highlights potential areas for improvement in addressing specific question complexities within varied scenarios.", "section": "4.4 Complexity Dimension Analysis"}, {"figure_path": "https://arxiv.org/html/2412.12606/x7.png", "caption": "Figure 6: Performance of different LMMs across the age dimension.", "description": "This stacked bar chart showcases the performance of various Large MultiModal Models (LMMs) across three age demographics (old, middle-aged, and young) in correctly answering questions.  Each segment of the stacked bar represents the model's accuracy within a specific age group, and the total height of the bar signifies the overall performance across all ages. This visualization helps to highlight the models' ability to generalize across different age-related needs and preferences, which is crucial for real-world applications where LMMs interact with diverse user groups. The color coding distinguishes between the age groups: blue for old, orange for middle-aged, and green for young.", "section": "4.5 AGE DIMENSION ANALYSIS"}, {"figure_path": "https://arxiv.org/html/2412.12606/x8.png", "caption": "Figure 7: Examples of Architecture Scenario.", "description": "Examples of image-question pairs in the architecture scenario, including questions about color coding on a floor plan, measurement of a cable diameter using a tool, angle measurement of a corner, and identification of the object being polished by a worker.", "section": "C.1 EXAMPLE OF SCENARIO DIMENSION"}, {"figure_path": "https://arxiv.org/html/2412.12606/x9.png", "caption": "Figure 8: Examples of Education Scenario.", "description": "This figure presents two distinct scenes within an educational context. The first scene showcases a map of Johns Hopkins University highlighting parking areas, while an image depicts individuals playing table tennis. The second scene displays the exterior of Hayden Library alongside a whiteboard displaying an algorithm flow.", "section": "C. MORE DETAIL ON MDI-BENCHMARK"}, {"figure_path": "https://arxiv.org/html/2412.12606/x10.png", "caption": "Figure 9: Examples of Housework Scenario.", "description": "This figure showcases examples from the Housework scenario within the MDI-Benchmark, illustrating different scenes and situations related to housework activities and appliances.", "section": "C. MORE DETAIL ON MDI-BENCHMARK"}, {"figure_path": "https://arxiv.org/html/2412.12606/x11.png", "caption": "Figure 10: Examples of Social Service.", "description": "This figure showcases examples from the Social Service scenario within the MDI-Benchmark. It includes images related to sub-domains like travel (featuring a subway menu board and an airline flight schedule), communal facilities (showing an image of the Leaning Tower of Pisa), and shopping (with a picture of neatly stacked towels). Each image is paired with two example questions, one for each complexity level defined in the benchmark.", "section": "3 MDI-BENCHMARK"}, {"figure_path": "https://arxiv.org/html/2412.12606/x12.png", "caption": "Figure 11: Examples of Sport Scenario.", "description": "This figure presents a collage of images related to sports, showcasing different sports scenarios like a basketball game, a marathon, and a fighting match. These scenarios exemplify the diverse range of sports-related visual information within the MDI-Benchmark, which is designed to evaluate large multimodal models' ability to understand and respond to real-world sports scenarios.", "section": "3 MDI-BENCHMARK"}, {"figure_path": "https://arxiv.org/html/2412.12606/x13.png", "caption": "Figure 12: Examples of Transport Scenario.", "description": "Examples of images and questions related to the Transport scenario in the MDI-Benchmark. This includes questions and multiple-choice answers about airport flight information displays, bus stop signs, highway exit signs, and subway maps.  These examples illustrate the types of visual information and queries found in the Transport section of the benchmark. Questions target different levels of complexity, from simple information extraction to multi-step reasoning.", "section": "C. MORE DETAIL ON MDI-BENCHMARK"}, {"figure_path": "https://arxiv.org/html/2412.12606/x14.png", "caption": "Figure 13: Examples of Architecture Scenario Questions.", "description": "This figure presents two pairs of example questions for the Architecture scenario of the MDI-Benchmark, illustrating the difference between Level 1 and Level 2 questions. The first pair relates to a room plan. The Level 1 question asks about the color used to distinguish the bathroom, while the Level 2 question asks about calculating the area of the garage. The second pair involves an image of a micrometer. The Level 1 question requires reading the measurement displayed on the device, while the Level 2 question presents a scenario related to manufacturing, asking if a steel plate with a measured thickness falls within the required range.", "section": "C.2 EXAMPLE OF PROBLEM COMPLEXITY DIMENSION"}, {"figure_path": "https://arxiv.org/html/2412.12606/x15.png", "caption": "Figure 14: Examples of Education Scenario Questions.", "description": "Figure 14, located in Section C.2 (Example of Problem Complexity Dimension), presents two sets of questions for each image within the Education scenario of the MDI-Benchmark. Each image is paired with a Level 1 question and a Level 2 question, demonstrating the varying complexity of the benchmark. Level 1 questions assess basic comprehension of image content, like counting parking lots on a campus map or identifying a building's entrance. In contrast, Level 2 questions require more advanced reasoning and knowledge application, such as determining a suitable visit location based on a child's dental appointment or identifying the year a university was founded. This figure showcases how MDI-Benchmark evaluates Large Multimodal Models (LMMs) abilities to handle different complexity levels within real-world situations.", "section": "C.2 EXAMPLE OF PROBLEM COMPLEXITY DIMENSION"}, {"figure_path": "https://arxiv.org/html/2412.12606/x16.png", "caption": "Figure 15: Examples of Housework Scenario Questions.", "description": "This figure presents two sets of example questions related to a housework scenario, categorized by complexity level (Level 1 and Level 2). The first example involves a digital thermostat displaying a temperature of 72\u00b0F. Level 1 question asks about the displayed temperature, while the Level 2 question asks which button should be pressed to increase the fan speed. The second example shows a TV screen displaying movie options. The Level 1 question asks how many movies are available, while the Level 2 question asks for a movie recommendation based on a preference for DC Comics.", "section": "MDI-Benchmark"}, {"figure_path": "https://arxiv.org/html/2412.12606/x17.png", "caption": "Figure 16: Examples of Social Service Scenario Questions.", "description": "This figure, belonging to the supplementary material section \"More Detail on MDI-Benchmark\", presents two distinct examples of questions categorized under the Social Service scenario within the MDI-Benchmark. Each example includes two questions, one labeled \"Level 1\" and the other \"Level 2\", to illustrate the varying levels of complexity in the benchmark.  The first example shows an image of a restaurant menu, with Level 1 question asking about the restaurant's signature dish and the Level 2 question presenting a cost calculation scenario given customer orders. The second example displays an image of a movie schedule board, where the Level 1 question inquires about the number of movies available that day, while the Level 2 question involves selecting an age-appropriate movie for a family with young children. These demonstrate how MDI-Benchmark assesses different reasoning and knowledge application abilities within real-world scenarios.", "section": "More Detail on MDI-Benchmark"}, {"figure_path": "https://arxiv.org/html/2412.12606/x18.png", "caption": "Figure 17: Examples of Sport Scenario Questions.", "description": "The figure presents two examples of questions from the Sport scenario in the MDI-Benchmark, categorized by question complexity. The first example, at Level 1, asks to identify the sport being played, showing an image of an American football game. The second example, at Level 2, relates to a weightlifting competition, asking to calculate the body weight ratio and identify the athlete with the highest ratio. Each example includes question, multiple-choice options, and the ground truth answer.", "section": "C. MORE DETAIL ON MDI-BENCHMARK"}, {"figure_path": "https://arxiv.org/html/2412.12606/x19.png", "caption": "Figure 18: Examples of Transport Scenario Question.", "description": "This figure presents two sample questions related to the Transport scenario, showcasing varying levels of complexity.  The first question, categorized as Level 1, involves identifying the current color of a traffic signal. The second question, classified as Level 2, involves determining the appropriate exit to use when entering a city via Route 4, which demands greater contextual awareness and reasoning compared to the Level 1 questions. These examples highlight how the MDI-Benchmark assesses multimodal models' ability to extract information and perform visual reasoning in real-world scenarios.", "section": "C. MORE DETAIL ON MDI-BENCHMARK"}, {"figure_path": "https://arxiv.org/html/2412.12606/x20.png", "caption": "Figure 19: Example of Architecture Scenario Age Questions.", "description": "Figure 19 shows example questions posed to different age groups (old, middle-aged, and young) regarding an architectural floor plan image and a close-up image of a worker using tools.  The questions probe different aspects of the scenarios, illustrating how needs and queries vary across age demographics. For instance, the older person asks about calculating the area of a porch for planting, the middle-aged person inquires about bathroom fixture suitability based on the plan, and the younger person asks about furniture placement within a bedroom, given existing closet placement. The second image prompts questions about tool usage and suitability for electrical work, bookshelf fitting, and alternative measurement applications.  This demonstrates how the benchmark tailors questions to different age groups and their respective priorities in a given scenario.", "section": "C. MORE DETAIL ON MDI-BENCHMARK"}, {"figure_path": "https://arxiv.org/html/2412.12606/x21.png", "caption": "Figure 20: Example of Education Scenario Age Questions.", "description": "Example questions posed to different age groups (old, middle-aged, young) within the Education scenario of the MDI benchmark. The first row showcases questions about campus facilities and accessibility. The second row presents queries related to a university's history and curriculum.", "section": "C. MORE DETAIL ON MDI-BENCHMARK"}, {"figure_path": "https://arxiv.org/html/2412.12606/x22.png", "caption": "Figure 21: Example of Housework Scenario Age Questions.", "description": "This figure presents examples of questions tailored to different age groups within the \"Housework\" scenario of the MDI-Benchmark.  Each row displays an image related to a housework task, followed by three questions marked as \"Old_Q\", \"Mid_Q\", and \"Young_Q\", representing questions posed by older, middle-aged, and younger individuals, respectively. These questions vary in their complexity and the type of information they seek to extract from the image. For instance, the first image shows a water meter, with the older person's question focusing on reading the meter value, the middle-aged person's question involving a calculation based on the meter reading, and the younger person's question asking about the device's function. The second image depicts a bookshelf, with questions addressing the author of a specific book, identifying a toy based on a description, and determining the manufacturer of a particular item. This diversity in questions illustrates the varied needs and perspectives of different age groups when interacting with everyday scenarios.", "section": "C. MORE DETAIL ON MDI-BENCHMARK"}, {"figure_path": "https://arxiv.org/html/2412.12606/x23.png", "caption": "Figure 22: Example of Social Service Scenario Age Questions.", "description": "This figure presents example questions posed to different age groups (old, middle-aged, young) within the Social Service scenario of the MDI-Benchmark. The first image depicts a supermarket aisle containing eggs, with questions related to quantity and price comparisons.  The second image shows a museum entrance with admission prices, and the questions pertain to ticket costs for various age groups and family configurations.", "section": "C. MORE DETAIL ON MDI-BENCHMARK"}, {"figure_path": "https://arxiv.org/html/2412.12606/x24.png", "caption": "Figure 23: Example of Sport Scenario Age Questions.", "description": "This figure showcases example questions posed to different age groups (old, middle-aged, young) within the Sport scenario of the MDI-Benchmark.  The first image displays a shooting competition result board, with questions about the year the sport was added to the Olympics and the bullet caliber used. The second image depicts two fighters in a boxing ring, with questions about the referee, weight class, and match duration.", "section": "C. MORE DETAIL ON MDI-BENCHMARK"}, {"figure_path": "https://arxiv.org/html/2412.12606/x25.png", "caption": "Figure 24: Example of Transport Scenario Age Questions.", "description": "This figure presents example questions posed to different age groups (old, middle-aged, and young) within the Transport scenario of the MDI-Benchmark. Each question focuses on a different aspect of transport, such as flight delays, cruise ship amenities, or highway exits, and reflects the varying needs and priorities of different age demographics. The questions also include ground truth labels and serve to demonstrate the kind of complex, multimodal reasoning required to correctly interpret information present in a real-world visual context.", "section": "C. MORE DETAIL ON MDI-BENCHMARK"}, {"figure_path": "https://arxiv.org/html/2412.12606/x26.png", "caption": "Figure 25: Example of GPT-4o Architecture Scenario Correct Answers.", "description": "This figure presents two examples of correct answers generated by GPT-40 in the architecture scenario of the MDI-Benchmark. The first example showcases a room layout where the model correctly identifies the appropriate dining table size based on the room's dimensions. The second example displays a measuring tool and a floor tile, with GPT-40 accurately calculating the number of tiles needed to cover a specific length.", "section": "E. CORRECT RESPONDS FROM GPT-40"}, {"figure_path": "https://arxiv.org/html/2412.12606/x27.png", "caption": "Figure 26: Example of GPT-4o Education Scenario Correct Answers.", "description": "This figure presents two examples of questions and GPT-4o's responses within the Education scenario of the MDI-Benchmark.  The first example involves a campus map question where GPT-4o correctly identifies the building associated with a physical fitness test based on its label, \"The Human Performance Center.\" The second example showcases GPT-4o's ability to correctly determine the age of Loyola University Maryland by subtracting the founding year, visible on a sign in the image, from the current year.", "section": "C. MORE DETAIL ON MDI-BENCHMARK"}, {"figure_path": "https://arxiv.org/html/2412.12606/x28.png", "caption": "Figure 27: Example of GPT-4o Housework Scenario Correct Answers.", "description": "This figure showcases GPT-4o's performance on two examples from the Housework scenario of the MDI-Benchmark.  The first example involves reading a digital electricity meter to determine daily usage. GPT-4o correctly identifies the readings for both \"Today\" and \"Yesterday\" and then subtracts to find the difference. The second example displays a TV showing movie options, with the prompt asking for a DC Comics movie recommendation. GPT-4o successfully identifies \"The Flash\" as the appropriate choice from the list.", "section": "C. MORE DETAIL ON MDI-BENCHMARK"}, {"figure_path": "https://arxiv.org/html/2412.12606/x29.png", "caption": "Figure 28: Example of GPT-4o Social Service Scenario Correct Answers.", "description": "This figure showcases two examples of correctly answered questions within the Social Service scenario of the MDI-Benchmark, using GPT-40.  The first question asks for the total cost of a Subway order for a group with specific items listed. GPT-40 correctly calculates the total by summing the price of each item. The second question inquires about a filming location shown in a photograph of Griffith Observatory.  GPT-40 accurately identifies that the movie *Transformers* was filmed there.", "section": "E. CORRECT RESPONDS FROM GPT-40"}, {"figure_path": "https://arxiv.org/html/2412.12606/x30.png", "caption": "Figure 29: Example of GPT-4o Sport Scenario Correct Answers.", "description": "Figure 29 presents two examples of correct answers and reasoning processes generated by GPT-40 for the Sport scenario in the MDI-Benchmark.  The first example involves a scoreboard from a women's soccer (football) match between the USA and Thailand, asking which player scored the most goals. GPT-40 correctly identifies Alex Morgan as the player with the most goals by analyzing the scoreboard data. The second example shows runners in a marathon passing a 39.1km marker.  The question asks how far the runners are from the finish line, assuming a standard marathon distance of 42.195km. GPT-40 correctly calculates the remaining distance as 3.095km by subtracting the current distance from the total distance.", "section": "C.3 EXAMPLE OF AGE DIMENTION"}, {"figure_path": "https://arxiv.org/html/2412.12606/x31.png", "caption": "Figure 30: Example of GPT-4o Trans Scenario Correct Answers.", "description": "This figure presents two examples demonstrating GPT-40's ability to correctly answer questions in the Transport scenario of the MDI-Benchmark.  The first example involves an airplane flight path from Toronto to London, questioning which ocean the flight primarily crosses. GPT-40 accurately identifies the Atlantic Ocean. The second example involves a highway road sign, asking which highway is accessible if continuing on the current road. GPT-40 correctly identifies SR 826 South.  Both examples show not only the correct answer but also GPT-40's chain of thought or reasoning to arrive at the answer.", "section": "E. CORRECT RESPONDS FROM GPT-40"}, {"figure_path": "https://arxiv.org/html/2412.12606/x32.png", "caption": "Figure 31: Example of Information Extraction Error.", "description": "This figure presents three examples where Large Multimodal Models (LMMs) failed to correctly extract visual information, leading to incorrect answers. The first example involves a micrometer reading, where GPT-40 incorrectly extracts the value as 22.4. The second example shows a campus map with marked parking lots; Gemini 1.5 Pro incorrectly counts them as 8.  The third example involves a thermometer image, and Phi-3-Vision incorrectly interprets the temperature reading as 25\u00b0C.", "section": "F. BAD CASE"}, {"figure_path": "https://arxiv.org/html/2412.12606/x33.png", "caption": "Figure 32: Example of Knowledge Deficiency Error.", "description": "This figure presents three examples of knowledge deficiency errors from different LMMs (Large Multimodal Models). The first example involves an image concerning airport immigration and a question about eligible countries. Qwen-VL-Plus incorrectly answers based on counting flags instead of understanding the text. The second shows an image of the Hayden Library and a question about its opening hours. LLaVA-NeXT-110B incorrectly reasons about typical library hours without extracting specific information from the image. The third example presents an image of a basketball game and asks about the final score. MiniCPM-V2.5 hallucinates an incorrect score, failing to extract the information from the image. These examples highlight the models' struggles in retrieving and applying external knowledge relevant to the image context.", "section": "C. MORE DETAIL ON MDI-BENCHMARK"}]