<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><title>Multi-Dimensional Insights: Benchmarking Real-World Personalization in Large Multimodal Models &#183; HF Daily Paper Reviews by AI</title>
<meta name=title content="Multi-Dimensional Insights: Benchmarking Real-World Personalization in Large Multimodal Models &#183; HF Daily Paper Reviews by AI"><meta name=description content="New benchmark reveals how well AI understands and meets real-world human needs."><meta name=keywords content="Multimodal Learning,Vision-Language Models,üè¢ Beijing University of Posts and Telecommunications,"><link rel=canonical href=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12606/><link type=text/css rel=stylesheet href=/ai-paper-reviewer/css/main.bundle.min.595affd4445a931ea6d6e3a5a3c709930fa52a60be10b21c6f81fdb8fecaacea33aacedf80cdc88be45f189be14ed4ce53ea74a1e1406fad9cbf90c5ed409173.css integrity="sha512-WVr/1ERakx6m1uOlo8cJkw+lKmC+ELIcb4H9uP7KrOozqs7fgM3Ii+RfGJvhTtTOU+p0oeFAb62cv5DF7UCRcw=="><script type=text/javascript src=/ai-paper-reviewer/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script><script defer type=text/javascript id=script-bundle src=/ai-paper-reviewer/js/main.bundle.min.efbf3b6b987689fffaf2d7b73173d2690c0279a04d444b0537a77d7f4ff6e6d493445400cb0cf56bc0f0f123e19f15394e63cae34e67f069bd013dd5c73df56e.js integrity="sha512-7787a5h2if/68te3MXPSaQwCeaBNREsFN6d9f0/25tSTRFQAywz1a8Dw8SPhnxU5TmPK405n8Gm9AT3Vxz31bg==" data-copy data-copied></script><script src=/ai-paper-reviewer/lib/zoom/zoom.min.37d2094687372da3f7343a221a470f6b8806f7891aa46a5a03966af7f0ebd38b9fe536cb154e6ad28f006d184b294525a7c4054b6bbb4be62d8b453b42db99bd.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ=="></script><link rel=apple-touch-icon sizes=180x180 href=/ai-paper-reviewer/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/ai-paper-reviewer/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/ai-paper-reviewer/favicon-16x16.png><link rel=manifest href=/ai-paper-reviewer/site.webmanifest><meta property="og:url" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12606/"><meta property="og:site_name" content="HF Daily Paper Reviews by AI"><meta property="og:title" content="Multi-Dimensional Insights: Benchmarking Real-World Personalization in Large Multimodal Models"><meta property="og:description" content="New benchmark reveals how well AI understands and meets real-world human needs."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="paper-reviews"><meta property="article:published_time" content="2024-12-17T00:00:00+00:00"><meta property="article:modified_time" content="2024-12-17T00:00:00+00:00"><meta property="article:tag" content="Multimodal Learning"><meta property="article:tag" content="Vision-Language Models"><meta property="article:tag" content="üè¢ Beijing University of Posts and Telecommunications"><meta property="og:image" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12606/cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12606/cover.png"><meta name=twitter:title content="Multi-Dimensional Insights: Benchmarking Real-World Personalization in Large Multimodal Models"><meta name=twitter:description content="New benchmark reveals how well AI understands and meets real-world human needs."><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"Paper Reviews by AI","name":"Multi-Dimensional Insights: Benchmarking Real-World Personalization in Large Multimodal Models","headline":"Multi-Dimensional Insights: Benchmarking Real-World Personalization in Large Multimodal Models","abstract":"New benchmark reveals how well AI understands and meets real-world human needs.","inLanguage":"en","url":"https:\/\/deep-diver.github.io\/ai-paper-reviewer\/paper-reviews\/2412.12606\/","author":{"@type":"Person","name":"Hugging Face Daily Papers"},"copyrightYear":"2024","dateCreated":"2024-12-17T00:00:00\u002b00:00","datePublished":"2024-12-17T00:00:00\u002b00:00","dateModified":"2024-12-17T00:00:00\u002b00:00","keywords":["Multimodal Learning","Vision-Language Models","üè¢ Beijing University of Posts and Telecommunications"],"mainEntityOfPage":"true","wordCount":"5510"}]</script><meta name=author content="Hugging Face Daily Papers"><link href=https://github.com/deep-diver/paper-reviewer/ rel=me><link href=https://twitter.com/algo_diver/ rel=me><script src=/ai-paper-reviewer/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj+KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script><script defer src=/ai-paper-reviewer/lib/typeit/typeit.umd.1b3200cb448f5cd1f548f2781452643d3511a43584b377b82c03a58055da4fdb7bc8f6c6c2ce846480c7677ff25bfd0d75f15823c09443ab18e0fd2cad792587.js integrity="sha512-GzIAy0SPXNH1SPJ4FFJkPTURpDWEs3e4LAOlgFXaT9t7yPbGws6EZIDHZ3/yW/0NdfFYI8CUQ6sY4P0srXklhw=="></script><script defer src=/ai-paper-reviewer/lib/packery/packery.pkgd.min.js integrity></script><script type=text/javascript src=/ai-paper-reviewer/js/shortcodes/gallery.min.9b4cb28f931ed922c26fb9b2510c2debb370f6a63305050c2af81740b2919883715e24efbbdf3a081496718ec751df3a72729d4d0bc71d6071297563a97ce1ee.js integrity="sha512-m0yyj5Me2SLCb7myUQwt67Nw9qYzBQUMKvgXQLKRmINxXiTvu986CBSWcY7HUd86cnKdTQvHHWBxKXVjqXzh7g=="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-KX0S6Q55Y7"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-KX0S6Q55Y7")</script><meta name=theme-color><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-app.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-firestore.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-auth.js></script><script>const firebaseConfig={apiKey:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",authDomain:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",projectId:"neurips2024-f3065",storageBucket:"neurips2024-f3065.firebasestorage.app",messagingSenderId:"982475958898",appId:"1:982475958898:web:2147e5d7753d6ac091f0eb",measurementId:"G-YQ46HXQ9JS"};var app=firebase.initializeApp(firebaseConfig),db=firebase.firestore(),auth=firebase.auth()</script></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a></div><div class=min-h-[148px]></div><div class="fixed inset-x-0 pl-[24px] pr-[24px]" style=z-index:100><div id=menu-blur class="absolute opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl shadow-2xl"></div><div class="relative max-w-[64rem] ml-auto mr-auto"><div style=padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start space-x-3"><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/ai-paper-reviewer/ class="text-base font-medium text-gray-500 hover:text-gray-900">HF Daily Paper Reviews by AI</a></nav><nav class="hidden md:flex items-center space-x-5 md:ml-12 h-12"><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title="About This Project">About</p></a><a href=/ai-paper-reviewer/2025-01-27/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title=2025-01-27s>2025-01-27</p></a><a href=/ai-paper-reviewer/2025-01-28/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title=2025-01-28s>2025-01-28</p></a><a href=/ai-paper-reviewer/2025-01-29/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title=2025-01-29s>2025-01-29</p></a><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title="Paper Reviews by AI">Archive</p></a><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title=Tags>Tags</p></a><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></span><p class="text-base font-medium" title></p></a><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span><p class="text-base font-medium" title></p></a><button id=search-button aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button><div class="ltr:mr-14 rtl:ml-14 flex items-center"><button id=appearance-switcher aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></nav><div class="flex md:hidden items-center space-x-5 md:ml-12 h-12"><span></span>
<button id=search-button-mobile aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></button>
<button id=appearance-switcher-mobile aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400" style=margin-right:5px><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div><div class="-my-2 -mr-2 md:hidden"><label id=menu-button class=block><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper style=padding-top:5px class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li id=menu-close-button><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title="About This Project">About</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-01-27/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title=2025-01-27s>2025-01-27</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-01-28/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title=2025-01-28s>2025-01-28</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-01-29/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title=2025-01-29s>2025-01-29</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title="Paper Reviews by AI">Archive</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title=Tags>Tags</p></a></li><li class=mt-1><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li><li class=mt-1><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li></ul></div></label></div></div></div></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("menu-blur");n.style.opacity=t/300})</script><div class="relative flex flex-col grow"><main id=main-content class=grow><article><div id=hero class="h-[150px] md:h-[200px]"></div><div class="fixed inset-x-0 top-0 h-[800px] single_hero_background nozoom" style=background-image:url(/ai-paper-reviewer/paper-reviews/2412.12606/cover_hu_320080194b6e728c.png)><div class="absolute inset-0 bg-gradient-to-t from-neutral dark:from-neutral-800 to-transparent mix-blend-normal"></div><div class="absolute inset-0 opacity-60 bg-gradient-to-t from-neutral dark:from-neutral-800 to-neutral-100 dark:to-neutral-800 mix-blend-normal"></div></div><div id=background-blur class="fixed opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl"></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("background-blur");n.style.opacity=t/300})</script><header id=single_header class="mt-5 max-w-prose"><ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden"><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/>HF Daily Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class=inline><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/>Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/2412.12606/>Multi-Dimensional Insights: Benchmarking Real-World Personalization in Large Multimodal Models</a><span class="px-1 text-primary-500">/</span></li></ol><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">Multi-Dimensional Insights: Benchmarking Real-World Personalization in Large Multimodal Models</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime=2024-12-17T00:00:00+00:00>17 December 2024</time><span class="px-2 text-primary-500">&#183;</span><span>5510 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">26 mins</span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=views_paper-reviews/2412.12606/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=views>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 576 512"><path fill="currentcolor" d="M288 32c-80.8.0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7.0 24.6C17.3 304 48.6 356 95.4 399.4 142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1 3.3-7.9 3.3-16.7.0-24.6-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144 64.5-144 144-144 144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64-11.5.0-22.3-3-31.6-8.4-.2 2.8-.4 5.5-.4 8.4.0 53 43 96 96 96s96-43 96-96-43-96-96-96c-2.8.0-5.6.1-8.4.4 5.3 9.3 8.4 20.1 8.4 31.6z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=likes_paper-reviews/2412.12606/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=likes>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<button id=button_likes class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400" onclick=process_article()>
<span id=button_likes_heart style=display:none class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span><span id=button_likes_emtpty_heart class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M244 84l11.1 12 12-11.98C300.6 51.37 347 36.51 392.6 44.1 461.5 55.58 512 115.2 512 185.1V190.9c0 41.5-17.2 81.2-47.6 109.5L283.7 469.1c-7.5 7-17.4 10.9-27.7 10.9S235.8 476.1 228.3 469.1L47.59 300.4C17.23 272.1.0 232.4.0 190.9V185.1c0-69.9 50.52-129.52 119.4-141 44.7-7.59 92 7.27 124.6 39.9C243.1 84 244 84.01 244 84zm11.1 79.9-45-46.8c-21.7-20.82-52.5-30.7-82.8-25.66C81.55 99.07 48 138.7 48 185.1V190.9c0 28.2 11.71 55.2 32.34 74.4L256 429.3l175.7-164c20.6-19.2 32.3-46.2 32.3-74.4V185.1c0-46.4-33.6-86.03-79.3-93.66C354.4 86.4 323.6 96.28 301.9 117.1l-46.8 46.8z"/></svg>
</span></span><span id=button_likes_text>&nbsp;Like</span></button></span></div><div class="flex flex-row flex-wrap items-center"><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/ai-generated/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">AI Generated
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/-daily-papers/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">ü§ó Daily Papers
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/multimodal-learning/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Multimodal Learning
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/vision-language-models/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Vision-Language Models
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/-beijing-university-of-posts-and-telecommunications/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">üè¢ Beijing University of Posts and Telecommunications</span></span></span></div></div><div class="flex author"><img class="!mt-0 !mb-0 h-24 w-24 rounded-full ltr:mr-4 rtl:ml-4" width=96 height=96 alt="Hugging Face Daily Papers" src=/ai-paper-reviewer/img/avatar_hu_97e7d424fadd1c26.png><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">Author</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">Hugging Face Daily Papers</div><div class="text-sm text-neutral-700 dark:text-neutral-400">I am AI, and I review papers on HF Daily Papers</div><div class="text-2xl sm:text-lg"><div class="flex flex-wrap text-neutral-400 dark:text-neutral-500"><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://github.com/deep-diver/paper-reviewer/ target=_blank aria-label=Github rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://twitter.com/algo_diver/ target=_blank aria-label=Twitter rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span></a></div></div></div></div><div class=mb-5></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first lg:ml-auto px-0 lg:order-last ltr:lg:pl-8 rtl:lg:pr-8"><div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-[140px]"><details open id=TOCView class="toc-right mt-0 overflow-y-scroll overscroll-contain scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600 rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 hidden lg:block"><summary class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="min-w-[220px] py-2 border-dotted ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#lmm-personalization>LMM Personalization</a></li><li><a href=#mdi-benchmark-design>MDI Benchmark Design</a></li><li><a href=#multi-modal-lmm-gaps>Multi-Modal LMM Gaps</a></li><li><a href=#age-aware-lmms>Age-Aware LMMs</a></li><li><a href=#future-lmm-design>Future LMM Design</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><details class="toc-inside mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 lg:hidden"><summary class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#lmm-personalization>LMM Personalization</a></li><li><a href=#mdi-benchmark-design>MDI Benchmark Design</a></li><li><a href=#multi-modal-lmm-gaps>Multi-Modal LMM Gaps</a></li><li><a href=#age-aware-lmms>Age-Aware LMMs</a></li><li><a href=#future-lmm-design>Future LMM Design</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><script>var margin=200,marginError=50;(function(){var t=$(window),e=$("#TOCView"),s=e.height();function n(){var n=t.height()-margin;s>=n?(e.css("overflow-y","scroll"),e.css("max-height",n+marginError+"px")):(e.css("overflow-y","hidden"),e.css("max-height","9999999px"))}t.on("resize",n),$(document).ready(n)})()</script></div></div><div class="min-w-0 min-h-0 max-w-fit"><div class="article-content max-w-prose mb-20"><br><div class="flex flex-row flex-wrap items-center space-x-2"><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 48 48" fill="none"><rect width="48" height="48" fill="#fff" fill-opacity=".01"/><path d="M18 43V22c0-3.3137 2.6863-6 6-6s6 2.6863 6 6V43" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M12 40V22c0-6.6274 5.3726-12 12-12s12 5.3726 12 12V40" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M6 35V22C6 12.0589 14.0589 4 24 4s18 8.0589 18 18V35" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 44V31" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 24.625v-2.75" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/></svg>
</span></span><span>2412.12606</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg fill="#000" height="800" width="800" id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 511.999 511.999"><g><g><path d="M421.578 190.264l-99.847-99.847c-2.439-2.439-6.391-2.439-8.829.0L82.824 320.495c-2.439 2.439-2.439 6.392.0 8.829l99.847 99.847c2.439 2.439 6.391 2.439 8.829.0l230.078-230.078C424.017 196.655 424.017 192.703 421.578 190.264z"/></g></g><g><g><path d="M506.511 87.672 424.323 5.484c-7.308-7.31-19.175-7.315-26.488.0L348.219 55.1c-2.439 2.439-2.439 6.391.0 8.829l99.847 99.847c2.439 2.437 6.391 2.437 8.829.0l49.616-49.616C513.826 106.847 513.826 94.987 506.511 87.672z"/></g></g><g><g><path d="M508.133 491.11c-1.054-9.556-9.489-16.599-19.104-16.599H111.633l36.058-15.163c4.088-1.719 5.131-7.034 1.994-10.17l-86.854-86.854c-3.137-3.135-8.451-2.094-10.17 1.994C52.224 365.359 2.052 484.66 1.627 485.707c-5.815 13.208 4.855 27.01 18.107 26.263H489.52C500.566 511.97 509.379 502.408 508.133 491.11z"/></g></g></svg>
</span></span><span>YiFan Zhang et el.</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span>ü§ó 2024-12-18</span></span></span></div></div><p><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://arxiv.org/abs/2412.12606 target=_self role=button>‚Üó arXiv
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://huggingface.co/papers/2412.12606 target=_self role=button>‚Üó Hugging Face
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://paperswithcode.com/paper/multi-dimensional-insights-benchmarking-real target=_self role=button>‚Üó Papers with Code</a></p><audio controls><source src=https://ai-paper-reviewer.com/2412.12606/podcast.wav type=audio/wav>Your browser does not support the audio element.</audio><h3 class="relative group">TL;DR<div id=tldr class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#tldr aria-label=Anchor>#</a></span></h3><div class="lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl">Creating personalized AI assistants that meet diverse user needs is a long-standing goal. While Large Multimodal Models (LMMs) have shown potential, current benchmarks mainly focus on technical metrics and don&rsquo;t adequately assess if these models understand and meet actual human needs, especially across different demographics like age groups. This gap hinders the development of truly personalized and helpful AI assistants.Existing LMM benchmarks evaluate technical skills but not real-world applications. They don&rsquo;t consider diverse needs based on factors like age. This makes it hard to build personalized AI that truly helps people in everyday situations. The MDI benchmark aims to bridge this gap.The Multi-Dimensional Insights (MDI) benchmark addresses this by using images of real-world scenarios paired with questions of varying complexity and relevance to different age groups. This design allows for a more nuanced evaluation of LMMs, exploring their ability to understand and respond to practical, age-specific needs in everyday situations. The benchmark data and evaluation code are available to aid further research.</div><h4 class="relative group">Key Takeaways<div id=key-takeaways class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#key-takeaways aria-label=Anchor>#</a></span></h4><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-144344ce3b4de89c314637d7f5dd7842></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-144344ce3b4de89c314637d7f5dd7842",{strings:[" The MDI benchmark evaluates large multimodal models (LMMs) on their ability to meet real-world human needs across different age groups and complexities. "],speed:10,lifeLike:!0,startDelay:0,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-84fa80e2de50a9707a3012802fe658a5></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-84fa80e2de50a9707a3012802fe658a5",{strings:[" GPT-4 performed the best but still has room for improvement, especially in personalizing responses for different demographics. "],speed:10,lifeLike:!0,startDelay:1e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-ba1b56d1d484759c2b01b0e270d9f6a8></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-ba1b56d1d484759c2b01b0e270d9f6a8",{strings:[" The benchmark highlights the need for better datasets and enhanced LMM capabilities in handling complex real-world scenarios and age-specific needs. "],speed:10,lifeLike:!0,startDelay:2e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><h4 class="relative group">Why does it matter?<div id=why-does-it-matter class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#why-does-it-matter aria-label=Anchor>#</a></span></h4><p><strong>This benchmark is crucial for LMM researchers</strong> as it provides <strong>a standardized way to evaluate and improve LMM&rsquo;s ability to address diverse, real-world needs</strong> across different demographics. It also <strong>highlights the shortcomings of current LMMs</strong> in handling complex, age-specific scenarios, opening new avenues for research into personalization and <strong>improving real-world application</strong> of these models.</p><hr><h4 class="relative group">Visual Insights<div id=visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#visual-insights aria-label=Anchor>#</a></span></h4><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.12606/x1.png alt></figure></p><blockquote><p>üîº The figure showcases example images from the Multi-Dimensional Insights (MDI) Benchmark, categorized by real-world scenarios relevant to different age groups. These scenarios include kitchen and home layouts (Architecture), library and campus scenes (Education), household items like laundry detergent and a digital clock (Housework), restaurant menus and travel information (Social Services), sports images like basketball and racing (Sport), and airport departure boards and road signs (Transport). Each image is paired with multiple questions that aim to gauge an LMM&rsquo;s understanding of the scene and its ability to address the specific needs of different age demographics.</p><details><summary>read the caption</summary>Figure 1: The MDI-Benchmark includes real needs of different age groups in six major real-world scenarios.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Scenarios</th><th>Number of images</th><th>Number of L1 questions</th><th>Number of L2 questions</th><th>Number of old questions</th><th>Number of mid questions</th><th>Number of young questions</th></tr></thead><tbody><tr><td>Architecture</td><td>85</td><td>121</td><td>112</td><td>77</td><td>74</td><td>82</td></tr><tr><td>Education</td><td>85</td><td>114</td><td>115</td><td>80</td><td>79</td><td>70</td></tr><tr><td>Housework</td><td>86</td><td>103</td><td>109</td><td>71</td><td>74</td><td>67</td></tr><tr><td>Social services</td><td>86</td><td>95</td><td>108</td><td>65</td><td>66</td><td>72</td></tr><tr><td>Sports</td><td>86</td><td>107</td><td>103</td><td>70</td><td>73</td><td>67</td></tr><tr><td>Transport</td><td>86</td><td>109</td><td>102</td><td>73</td><td>70</td><td>68</td></tr><tr><td>Total</td><td>86</td><td>649</td><td>649</td><td>436</td><td>436</td><td>426</td></tr></tbody></table></table></figure><blockquote><p>üîº This table provides a statistical overview of the MDI-Benchmark dataset, detailing the distribution of images and questions across different scenarios. It breaks down the number of images and the number of questions for both Level 1 and Level 2 complexity, as well as the distribution of questions across the three age groups (old, mid, and young). The scenarios included are Architecture, Education, Housework, Social Services, Sports, and Transport.</p><details><summary>read the caption</summary>Table 1: Statistical details of MDI-Benchmark.</details></blockquote><h3 class="relative group">In-depth insights<div id=in-depth-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#in-depth-insights aria-label=Anchor>#</a></span></h3><h4 class="relative group">LMM Personalization<div id=lmm-personalization class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#lmm-personalization aria-label=Anchor>#</a></span></h4><p><strong>Personalized LMMs</strong>, crucial for real-world impact, face limitations in adapting to diverse user needs. Current benchmarks lack a focus on personalization, hindering accurate assessment of LMM&rsquo;s ability to address varying preferences across demographics, especially age. This emphasizes the <strong>need for new benchmarks</strong> like the proposed MDI benchmark, to evaluate real-world personalized needs. Existing LMMs exhibit <strong>insufficient generalization across age</strong>, revealing the complexity of tailoring responses for different demographics. Scaling language model parameters generally improves personalization, highlighting the role of extensive training data. Future LMMs should <strong>prioritize personalized data generation and integrate user feedback</strong>, advancing the development of <strong>reliable and truly personalized AI assistants</strong> capable of effectively meeting diverse human needs.</p><h4 class="relative group">MDI Benchmark Design<div id=mdi-benchmark-design class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#mdi-benchmark-design aria-label=Anchor>#</a></span></h4><p>The MDI Benchmark design prioritizes <strong>real-world relevance</strong>, addressing limitations of existing evaluations. It employs two key dimensions. <strong>Question Complexity</strong>, with two levels: basic comprehension and complex reasoning, mirrors real-world demands on LMMs. The <strong>Age Dimension</strong> acknowledges the diverse needs across young, middle-aged, and older demographics, a crucial aspect of personalized AI. This layered approach assesses LMM performance beyond simple accuracy metrics, offering valuable insights into their capabilities across diverse scenarios and user needs. The MDI benchmark evaluates across six domains: <strong>architecture, education, housework, social service, sport, and transport</strong>, each further divided into sub-domains, enhancing its real-world applicability.</p><h4 class="relative group">Multi-Modal LMM Gaps<div id=multi-modal-lmm-gaps class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#multi-modal-lmm-gaps aria-label=Anchor>#</a></span></h4><p>Large Multi-Modal Models (LMMs) show promising results, but significant gaps remain. A key challenge is <strong>contextual understanding</strong>, where LMMs struggle to integrate visual and textual information effectively for complex reasoning. This is evident in tasks requiring <strong>knowledge integration</strong> or <strong>multi-step inference</strong>. Another gap lies in <strong>personalization</strong>, where current LMMs lack the ability to tailor responses to individual user needs, preferences, or demographics. <strong>Data limitations</strong> also hinder progress, with current datasets often lacking diversity in scenarios, complexities, and user demographics. This highlights the need for more comprehensive benchmarks to accurately assess and address these limitations.</p><h4 class="relative group">Age-Aware LMMs<div id=age-aware-lmms class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#age-aware-lmms aria-label=Anchor>#</a></span></h4><p><strong>Age-aware LMMs</strong> represent a crucial step towards personalized AI. The paper&rsquo;s MDI benchmark highlights how current models struggle with age-related nuances in understanding and responding to real-world scenarios. This underscores the need for LMMs to move beyond general comprehension and cater to the <strong>diverse needs of different age groups</strong>. Developing age-aware models requires careful consideration of varying cognitive abilities, life experiences, and communication styles. Training data should reflect these age-specific characteristics. Moreover, evaluation benchmarks must include <strong>age as a key dimension</strong> to measure progress effectively. Ultimately, age-aware LMMs will enable the creation of more empathetic and useful AI assistants capable of truly personalized interactions.</p><h4 class="relative group">Future LMM Design<div id=future-lmm-design class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#future-lmm-design aria-label=Anchor>#</a></span></h4><p><strong>Future LMM design</strong> must prioritize enhanced <strong>real-world</strong> problem-solving. Current benchmarks lack real-world scenarios and diverse human needs. Models should understand context, reason logically, and address varying complexities and user preferences across age groups. This requires datasets reflecting real-life scenarios, diverse information needs, and age-specific requirements. Developing robust evaluation metrics for personalization and knowledge grounding is also <strong>crucial</strong> for practical application. Addressing these <strong>challenges</strong> will improve LMM&rsquo;s ability to assist humans in everyday life.</p><h3 class="relative group">More visual insights<div id=more-visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#more-visual-insights aria-label=Anchor>#</a></span></h3><details><summary>More on figures</summary><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.12606/x2.png alt></figure></p><blockquote><p>üîº The figure provides a high-level overview of the six real-world multimodal scenarios included in the MDI Benchmark. Each scenario, representing a different aspect of daily human life, is further divided into three more specific sub-domains, resulting in a total of 18 distinct sub-domains. These scenarios and sub-domains are: * <strong>Architecture:</strong> House Planning, Work Scenes, Measuring * <strong>Education:</strong> Studying, Teaching, Campus * <strong>Housework:</strong> Home Arrangements, Housework Activities, Household Appliances * <strong>Social Services:</strong> Travel, Shopping, Communal Facilities * <strong>Sport:</strong> Powerlifting, Race, Ball * <strong>Transport:</strong> Signpost, Rail transit, Airport</p><details><summary>read the caption</summary>Figure 2: The overview of the MDI Benchmark‚Äôs six real-world multimodal scenarios, each comprising three sub-domains.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.12606/x3.png alt></figure></p><blockquote><p>üîº This bar chart compares the performance of different Large MultiModal Models (LMMs) on two difficulty levels (Level 1 and Level 2) of the Multi-Dimensional Insights (MDI) Benchmark. Level 1 represents easier tasks focused on basic perception, while Level 2 involves more complex reasoning tasks. The chart visually displays the accuracy scores achieved by each LMM on both levels, indicating their strengths and weaknesses in handling different task complexities. The LMMs evaluated include GPT-40, GPT-4V, Gemini 1.5 Pro, Qwen-VL-Plus, and various open-source models.</p><details><summary>read the caption</summary>Figure 3: The average performance of different LMMs on different difficulty levels of the MDI-Benchmark.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.12606/x4.png alt></figure></p><blockquote><p>üîº This figure visualizes the performance of Large Multimodal Models (LMMs) on the Multi-Dimensional Insights (MDI) Benchmark across different complexity levels (Level 1 and Level 2). <strong>(a) Leaderboard:</strong> Ranks various open-source LMMs based on their overall performance scores, calculated using a weighted average of Level 1 and Level 2 accuracies. The size of the model (in billions of parameters) is plotted against the score, showcasing the impact of model scale on performance. <strong>(b) Radar Charts:</strong> Two radar charts illustrate the performance of 14 LMMs (both open-source and closed-source) across six real-world scenarios (Architecture, Education, Housework, Social Service, Sport, and Transport) at both Level 1 (basic perception) and Level 2 (logical reasoning). These charts visually compare the models&rsquo; strengths and weaknesses in different scenarios and at different levels of complexity. <strong>(c) Bar and Variance Charts:</strong> These charts display the average accuracy and variance of the LMMs across the six scenarios at both Level 1 and Level 2. This visualization helps analyze the models&rsquo; overall performance and consistency across different tasks.</p><details><summary>read the caption</summary>Figure 4: Performance of the model at different difficulty levels and the overall performance results of the model under the score metric.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.12606/x6.png alt></figure></p><blockquote><p>üîº This figure presents bar plots illustrating the average accuracy and variance of various Large Multimodal Models (LMMs) across six domains at two levels of question complexity (Level 1 and Level 2). Level 1 focuses on basic perceptual abilities, while Level 2 involves logical reasoning. The x-axis in both subfigures represents different LMMs, with closed-source models (GPT-40, GPT-4V, Gemini 1.5 Pro, Qwen-VL-Plus) grouped on the left and open-source models to the right. The first subfigure displays the average accuracy (y-axis) of each LMM for Level 1 and Level 2, showing the overall performance trends. The second subfigure depicts the variance in accuracy (y-axis), indicating the consistency of model performance across domains. Lower variance suggests more stable performance. This analysis reveals which models perform consistently and highlights potential areas for improvement in addressing specific question complexities within varied scenarios.</p><details><summary>read the caption</summary>Figure 5: The average accuracy and variance of LLMs across six domains at Level 1 and Level 2</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.12606/x7.png alt></figure></p><blockquote><p>üîº This stacked bar chart showcases the performance of various Large MultiModal Models (LMMs) across three age demographics (old, middle-aged, and young) in correctly answering questions. Each segment of the stacked bar represents the model&rsquo;s accuracy within a specific age group, and the total height of the bar signifies the overall performance across all ages. This visualization helps to highlight the models&rsquo; ability to generalize across different age-related needs and preferences, which is crucial for real-world applications where LMMs interact with diverse user groups. The color coding distinguishes between the age groups: blue for old, orange for middle-aged, and green for young.</p><details><summary>read the caption</summary>Figure 6: Performance of different LMMs across the age dimension.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.12606/x8.png alt></figure></p><blockquote><p>üîº Examples of image-question pairs in the architecture scenario, including questions about color coding on a floor plan, measurement of a cable diameter using a tool, angle measurement of a corner, and identification of the object being polished by a worker.</p><details><summary>read the caption</summary>Figure 7: Examples of Architecture Scenario.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.12606/x9.png alt></figure></p><blockquote><p>üîº This figure presents two distinct scenes within an educational context. The first scene showcases a map of Johns Hopkins University highlighting parking areas, while an image depicts individuals playing table tennis. The second scene displays the exterior of Hayden Library alongside a whiteboard displaying an algorithm flow.</p><details><summary>read the caption</summary>Figure 8: Examples of Education Scenario.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.12606/x10.png alt></figure></p><blockquote><p>üîº This figure showcases examples from the Housework scenario within the MDI-Benchmark, illustrating different scenes and situations related to housework activities and appliances.</p><details><summary>read the caption</summary>Figure 9: Examples of Housework Scenario.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.12606/x11.png alt></figure></p><blockquote><p>üîº This figure showcases examples from the Social Service scenario within the MDI-Benchmark. It includes images related to sub-domains like travel (featuring a subway menu board and an airline flight schedule), communal facilities (showing an image of the Leaning Tower of Pisa), and shopping (with a picture of neatly stacked towels). Each image is paired with two example questions, one for each complexity level defined in the benchmark.</p><details><summary>read the caption</summary>Figure 10: Examples of Social Service.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.12606/x12.png alt></figure></p><blockquote><p>üîº This figure presents a collage of images related to sports, showcasing different sports scenarios like a basketball game, a marathon, and a fighting match. These scenarios exemplify the diverse range of sports-related visual information within the MDI-Benchmark, which is designed to evaluate large multimodal models&rsquo; ability to understand and respond to real-world sports scenarios.</p><details><summary>read the caption</summary>Figure 11: Examples of Sport Scenario.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.12606/x13.png alt></figure></p><blockquote><p>üîº Examples of images and questions related to the Transport scenario in the MDI-Benchmark. This includes questions and multiple-choice answers about airport flight information displays, bus stop signs, highway exit signs, and subway maps. These examples illustrate the types of visual information and queries found in the Transport section of the benchmark. Questions target different levels of complexity, from simple information extraction to multi-step reasoning.</p><details><summary>read the caption</summary>Figure 12: Examples of Transport Scenario.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.12606/x14.png alt></figure></p><blockquote><p>üîº This figure presents two pairs of example questions for the Architecture scenario of the MDI-Benchmark, illustrating the difference between Level 1 and Level 2 questions. The first pair relates to a room plan. The Level 1 question asks about the color used to distinguish the bathroom, while the Level 2 question asks about calculating the area of the garage. The second pair involves an image of a micrometer. The Level 1 question requires reading the measurement displayed on the device, while the Level 2 question presents a scenario related to manufacturing, asking if a steel plate with a measured thickness falls within the required range.</p><details><summary>read the caption</summary>Figure 13: Examples of Architecture Scenario Questions.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.12606/x15.png alt></figure></p><blockquote><p>üîº Figure 14, located in Section C.2 (Example of Problem Complexity Dimension), presents two sets of questions for each image within the Education scenario of the MDI-Benchmark. Each image is paired with a Level 1 question and a Level 2 question, demonstrating the varying complexity of the benchmark. Level 1 questions assess basic comprehension of image content, like counting parking lots on a campus map or identifying a building&rsquo;s entrance. In contrast, Level 2 questions require more advanced reasoning and knowledge application, such as determining a suitable visit location based on a child&rsquo;s dental appointment or identifying the year a university was founded. This figure showcases how MDI-Benchmark evaluates Large Multimodal Models (LMMs) abilities to handle different complexity levels within real-world situations.</p><details><summary>read the caption</summary>Figure 14: Examples of Education Scenario Questions.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.12606/x16.png alt></figure></p><blockquote><p>üîº This figure presents two sets of example questions related to a housework scenario, categorized by complexity level (Level 1 and Level 2). The first example involves a digital thermostat displaying a temperature of 72¬∞F. Level 1 question asks about the displayed temperature, while the Level 2 question asks which button should be pressed to increase the fan speed. The second example shows a TV screen displaying movie options. The Level 1 question asks how many movies are available, while the Level 2 question asks for a movie recommendation based on a preference for DC Comics.</p><details><summary>read the caption</summary>Figure 15: Examples of Housework Scenario Questions.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.12606/x17.png alt></figure></p><blockquote><p>üîº This figure, belonging to the supplementary material section &lsquo;More Detail on MDI-Benchmark&rsquo;, presents two distinct examples of questions categorized under the Social Service scenario within the MDI-Benchmark. Each example includes two questions, one labeled &lsquo;Level 1&rsquo; and the other &lsquo;Level 2&rsquo;, to illustrate the varying levels of complexity in the benchmark. The first example shows an image of a restaurant menu, with Level 1 question asking about the restaurant&rsquo;s signature dish and the Level 2 question presenting a cost calculation scenario given customer orders. The second example displays an image of a movie schedule board, where the Level 1 question inquires about the number of movies available that day, while the Level 2 question involves selecting an age-appropriate movie for a family with young children. These demonstrate how MDI-Benchmark assesses different reasoning and knowledge application abilities within real-world scenarios.</p><details><summary>read the caption</summary>Figure 16: Examples of Social Service Scenario Questions.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.12606/x18.png alt></figure></p><blockquote><p>üîº The figure presents two examples of questions from the Sport scenario in the MDI-Benchmark, categorized by question complexity. The first example, at Level 1, asks to identify the sport being played, showing an image of an American football game. The second example, at Level 2, relates to a weightlifting competition, asking to calculate the body weight ratio and identify the athlete with the highest ratio. Each example includes question, multiple-choice options, and the ground truth answer.</p><details><summary>read the caption</summary>Figure 17: Examples of Sport Scenario Questions.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.12606/x19.png alt></figure></p><blockquote><p>üîº This figure presents two sample questions related to the Transport scenario, showcasing varying levels of complexity. The first question, categorized as Level 1, involves identifying the current color of a traffic signal. The second question, classified as Level 2, involves determining the appropriate exit to use when entering a city via Route 4, which demands greater contextual awareness and reasoning compared to the Level 1 questions. These examples highlight how the MDI-Benchmark assesses multimodal models&rsquo; ability to extract information and perform visual reasoning in real-world scenarios.</p><details><summary>read the caption</summary>Figure 18: Examples of Transport Scenario Question.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.12606/x20.png alt></figure></p><blockquote><p>üîº Figure 19 shows example questions posed to different age groups (old, middle-aged, and young) regarding an architectural floor plan image and a close-up image of a worker using tools. The questions probe different aspects of the scenarios, illustrating how needs and queries vary across age demographics. For instance, the older person asks about calculating the area of a porch for planting, the middle-aged person inquires about bathroom fixture suitability based on the plan, and the younger person asks about furniture placement within a bedroom, given existing closet placement. The second image prompts questions about tool usage and suitability for electrical work, bookshelf fitting, and alternative measurement applications. This demonstrates how the benchmark tailors questions to different age groups and their respective priorities in a given scenario.</p><details><summary>read the caption</summary>Figure 19: Example of Architecture Scenario Age Questions.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.12606/x21.png alt></figure></p><blockquote><p>üîº Example questions posed to different age groups (old, middle-aged, young) within the Education scenario of the MDI benchmark. The first row showcases questions about campus facilities and accessibility. The second row presents queries related to a university&rsquo;s history and curriculum.</p><details><summary>read the caption</summary>Figure 20: Example of Education Scenario Age Questions.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.12606/x22.png alt></figure></p><blockquote><p>üîº This figure presents examples of questions tailored to different age groups within the &lsquo;Housework&rsquo; scenario of the MDI-Benchmark. Each row displays an image related to a housework task, followed by three questions marked as &lsquo;Old_Q&rsquo;, &lsquo;Mid_Q&rsquo;, and &lsquo;Young_Q&rsquo;, representing questions posed by older, middle-aged, and younger individuals, respectively. These questions vary in their complexity and the type of information they seek to extract from the image. For instance, the first image shows a water meter, with the older person&rsquo;s question focusing on reading the meter value, the middle-aged person&rsquo;s question involving a calculation based on the meter reading, and the younger person&rsquo;s question asking about the device&rsquo;s function. The second image depicts a bookshelf, with questions addressing the author of a specific book, identifying a toy based on a description, and determining the manufacturer of a particular item. This diversity in questions illustrates the varied needs and perspectives of different age groups when interacting with everyday scenarios.</p><details><summary>read the caption</summary>Figure 21: Example of Housework Scenario Age Questions.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.12606/x23.png alt></figure></p><blockquote><p>üîº This figure presents example questions posed to different age groups (old, middle-aged, young) within the Social Service scenario of the MDI-Benchmark. The first image depicts a supermarket aisle containing eggs, with questions related to quantity and price comparisons. The second image shows a museum entrance with admission prices, and the questions pertain to ticket costs for various age groups and family configurations.</p><details><summary>read the caption</summary>Figure 22: Example of Social Service Scenario Age Questions.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.12606/x24.png alt></figure></p><blockquote><p>üîº This figure showcases example questions posed to different age groups (old, middle-aged, young) within the Sport scenario of the MDI-Benchmark. The first image displays a shooting competition result board, with questions about the year the sport was added to the Olympics and the bullet caliber used. The second image depicts two fighters in a boxing ring, with questions about the referee, weight class, and match duration.</p><details><summary>read the caption</summary>Figure 23: Example of Sport Scenario Age Questions.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.12606/x25.png alt></figure></p><blockquote><p>üîº This figure presents example questions posed to different age groups (old, middle-aged, and young) within the Transport scenario of the MDI-Benchmark. Each question focuses on a different aspect of transport, such as flight delays, cruise ship amenities, or highway exits, and reflects the varying needs and priorities of different age demographics. The questions also include ground truth labels and serve to demonstrate the kind of complex, multimodal reasoning required to correctly interpret information present in a real-world visual context.</p><details><summary>read the caption</summary>Figure 24: Example of Transport Scenario Age Questions.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.12606/x26.png alt></figure></p><blockquote><p>üîº This figure presents two examples of correct answers generated by GPT-40 in the architecture scenario of the MDI-Benchmark. The first example showcases a room layout where the model correctly identifies the appropriate dining table size based on the room&rsquo;s dimensions. The second example displays a measuring tool and a floor tile, with GPT-40 accurately calculating the number of tiles needed to cover a specific length.</p><details><summary>read the caption</summary>Figure 25: Example of GPT-4o Architecture Scenario Correct Answers.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.12606/x27.png alt></figure></p><blockquote><p>üîº This figure presents two examples of questions and GPT-4o&rsquo;s responses within the Education scenario of the MDI-Benchmark. The first example involves a campus map question where GPT-4o correctly identifies the building associated with a physical fitness test based on its label, &lsquo;The Human Performance Center.&rsquo; The second example showcases GPT-4o&rsquo;s ability to correctly determine the age of Loyola University Maryland by subtracting the founding year, visible on a sign in the image, from the current year.</p><details><summary>read the caption</summary>Figure 26: Example of GPT-4o Education Scenario Correct Answers.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.12606/x28.png alt></figure></p><blockquote><p>üîº This figure showcases GPT-4o&rsquo;s performance on two examples from the Housework scenario of the MDI-Benchmark. The first example involves reading a digital electricity meter to determine daily usage. GPT-4o correctly identifies the readings for both &lsquo;Today&rsquo; and &lsquo;Yesterday&rsquo; and then subtracts to find the difference. The second example displays a TV showing movie options, with the prompt asking for a DC Comics movie recommendation. GPT-4o successfully identifies &lsquo;The Flash&rsquo; as the appropriate choice from the list.</p><details><summary>read the caption</summary>Figure 27: Example of GPT-4o Housework Scenario Correct Answers.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.12606/x29.png alt></figure></p><blockquote><p>üîº This figure showcases two examples of correctly answered questions within the Social Service scenario of the MDI-Benchmark, using GPT-40. The first question asks for the total cost of a Subway order for a group with specific items listed. GPT-40 correctly calculates the total by summing the price of each item. The second question inquires about a filming location shown in a photograph of Griffith Observatory. GPT-40 accurately identifies that the movie <em>Transformers</em> was filmed there.</p><details><summary>read the caption</summary>Figure 28: Example of GPT-4o Social Service Scenario Correct Answers.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.12606/x30.png alt></figure></p><blockquote><p>üîº Figure 29 presents two examples of correct answers and reasoning processes generated by GPT-40 for the Sport scenario in the MDI-Benchmark. The first example involves a scoreboard from a women&rsquo;s soccer (football) match between the USA and Thailand, asking which player scored the most goals. GPT-40 correctly identifies Alex Morgan as the player with the most goals by analyzing the scoreboard data. The second example shows runners in a marathon passing a 39.1km marker. The question asks how far the runners are from the finish line, assuming a standard marathon distance of 42.195km. GPT-40 correctly calculates the remaining distance as 3.095km by subtracting the current distance from the total distance.</p><details><summary>read the caption</summary>Figure 29: Example of GPT-4o Sport Scenario Correct Answers.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.12606/x31.png alt></figure></p><blockquote><p>üîº This figure presents two examples demonstrating GPT-40&rsquo;s ability to correctly answer questions in the Transport scenario of the MDI-Benchmark. The first example involves an airplane flight path from Toronto to London, questioning which ocean the flight primarily crosses. GPT-40 accurately identifies the Atlantic Ocean. The second example involves a highway road sign, asking which highway is accessible if continuing on the current road. GPT-40 correctly identifies SR 826 South. Both examples show not only the correct answer but also GPT-40&rsquo;s chain of thought or reasoning to arrive at the answer.</p><details><summary>read the caption</summary>Figure 30: Example of GPT-4o Trans Scenario Correct Answers.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.12606/x32.png alt></figure></p><blockquote><p>üîº This figure presents three examples where Large Multimodal Models (LMMs) failed to correctly extract visual information, leading to incorrect answers. The first example involves a micrometer reading, where GPT-40 incorrectly extracts the value as 22.4. The second example shows a campus map with marked parking lots; Gemini 1.5 Pro incorrectly counts them as 8. The third example involves a thermometer image, and Phi-3-Vision incorrectly interprets the temperature reading as 25¬∞C.</p><details><summary>read the caption</summary>Figure 31: Example of Information Extraction Error.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.12606/x33.png alt></figure></p><blockquote><p>üîº This figure presents three examples of knowledge deficiency errors from different LMMs (Large Multimodal Models). The first example involves an image concerning airport immigration and a question about eligible countries. Qwen-VL-Plus incorrectly answers based on counting flags instead of understanding the text. The second shows an image of the Hayden Library and a question about its opening hours. LLaVA-NeXT-110B incorrectly reasons about typical library hours without extracting specific information from the image. The third example presents an image of a basketball game and asks about the final score. MiniCPM-V2.5 hallucinates an incorrect score, failing to extract the information from the image. These examples highlight the models&rsquo; struggles in retrieving and applying external knowledge relevant to the image context.</p><details><summary>read the caption</summary>Figure 32: Example of Knowledge Deficiency Error.</details></blockquote></details><details><summary>More on tables</summary><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption>| Model | Final Score | Level 1 | Level 2 | |&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;| |&mdash;| Avg | Arc | Edu | Hou | Soc | Spo | Tra | Avg | Arc | Edu | Hou | Soc | Spo | Tra | | <strong><em>Closed-source</em></strong> | | | | | | | | | | | | | | | | | <strong>GPT-4o</strong> | <strong>78.46</strong> | 87.46 | 76.47 | 94.12 | 92.16 | 90.20 | 86.27 | 94.12 | 69.45 | 70.59 | 70.59 | 78.43 | 82.35 | 54.90 | 66.67 | | <strong>GPT-4V</strong> | 74.92 | 87.46 | 86.27 | 92.16 | 86.27 | 90.20 | 88.24 | 90.20 | 62.38 | 72.55 | 70.59 | 74.51 | 60.78 | 45.10 | 56.86 | | <strong>Gemini 1.5 Pro</strong> | 69.13 | 82.32 | 68.63 | 92.16 | 76.47 | 88.24 | 86.27 | 90.20 | 55.95 | 52.94 | 56.86 | 54.90 | 74.51 | 43.14 | 58.82 | | <strong>Qwen-VL-Plus</strong> | 43.57 | 56.59 | 43.14 | 64.71 | 62.75 | 78.43 | 50.98 | 45.10 | 30.55 | 35.29 | 41.18 | 37.25 | 25.49 | 23.53 | 23.53 | | <strong><em>Open-source</em></strong> | | | | | | | | | | | | | | | | | <strong>LLaVA-NeXT-110B</strong> | <strong>65.59</strong> | 79.10 | 60.78 | 92.16 | 78.43 | 84.31 | 78.43 | 88.24 | 52.09 | 66.67 | 56.86 | 54.90 | 64.71 | 31.37 | 43.14 | | <strong>LLaVA-NeXT-72B</strong> | 63.67 | 76.21 | 68.63 | 88.24 | 80.39 | 82.35 | 70.59 | 74.51 | 51.13 | 66.67 | 54.90 | 52.94 | 60.78 | 33.33 | 43.14 | | <strong>MiniCPM-LLaMA3-V 2.5</strong> | 55.95 | 72.67 | 52.94 | 86.27 | 70.59 | 82.35 | 70.59 | 80.39 | 39.23 | 45.10 | 49.02 | 49.02 | 31.37 | 27.45 | 37.25 | | <strong>mPLUG-Owl2-7B</strong> | 52.57 | 64.63 | 49.02 | 70.59 | 74.51 | 70.59 | 58.82 | 70.59 | 40.51 | 41.18 | 41.18 | 47.06 | 39.22 | 29.41 | 49.02 | | <strong>DeepSeek-VL-7B</strong> | 52.09 | 68.49 | 49.02 | 70.59 | 74.51 | 80.39 | 62.75 | 80.39 | 35.69 | 41.18 | 33.33 | 39.22 | 41.18 | 21.57 | 41.18 | | <strong>Phi3-Vision-4.2B</strong> | 50.80 | 67.20 | 50.98 | 76.47 | 60.78 | 80.39 | 62.75 | 78.43 | 34.41 | 37.25 | 33.33 | 41.18 | 43.14 | 21.57 | 33.33 | | <strong>CogVLM-chat</strong> | 49.84 | 60.77 | 49.02 | 72.55 | 62.75 | 56.86 | 68.63 | 60.78 | 38.91 | 49.02 | 33.33 | 43.14 | 41.18 | 27.45 | 43.14 | | <strong>DeepSeek-VL-1.3B</strong> | 46.30 | 58.20 | 45.10 | 56.86 | 66.67 | 56.86 | 66.67 | 62.75 | 34.41 | 35.29 | 29.41 | 29.41 | 39.22 | 27.45 | 49.02 | | <strong>CogAgent-vqa</strong> | 41.16 | 49.52 | 35.29 | 45.10 | 66.67 | 54.90 | 56.86 | 43.14 | 32.80 | 31.37 | 35.29 | 35.29 | 37.25 | 25.49 | 35.29 | | <strong>LLaVA-NeXT-7B</strong> | 33.60 | 43.09 | 31.37 | 52.94 | 43.14 | 49.02 | 39.22 | 47.06 | 24.12 | 35.29 | 13.73 | 37.25 | 23.53 | 9.80 | 27.45 |}</table></figure><blockquote><p>üîº This table presents the performance of various Large Multimodal Models (LMMs) on the Multi-Dimensional Insights (MDI) Benchmark, categorized by question complexity (Level 1: basic perception, Level 2: logical reasoning) and real-world scenarios (Architecture, Education, Housework, Social Services, Sport, Transport). The table is split into two sections: closed-source and open-source models. Each model&rsquo;s overall score, calculated using a weighted average of Level 1 and Level 2 performance, is also presented. Higher accuracy percentages indicate better performance, with the best scores for each model category (closed/open) highlighted.</p><details><summary>read the caption</summary>Table 2: LMMs Performance on MDI-Benchmark in Terms of Level and Scenario. Vertically, the table is composed of a model score and two Level sub-tables, where the model score is obtained from Formula ¬†1. Each sub-table consists of seven columns showing the accuracy rates of LMMs in different scenarios. The first column of each sub-table represents the mean value of the subsequent six columns, reflecting the overall performance at different levels. The annotations for Level and Scenario are as follows: Level 1: assessment questions that focus only on basic perceptual ability; Level 2: assessment questions that involve logical reasoning. The scenarios are abbreviated as follows: Arc (architecture), Edu (education), Hou (housework), Soc (social service), Spo (sport), Tra (transport). Horizontally, the table is divided into two blocks. For better statistics and analysis, we will display the blocks as closed-source model statistics and open-source model statistics. The best performance in each block is highlighted in blue and green.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Model</th><th>Avg</th><th>old</th><th>middle-aged</th><th>young</th></tr></thead><tbody><tr><td><strong><em>Closed-source</em></strong></td><td></td><td></td><td></td><td></td></tr><tr><td><strong>GPT-4o</strong></td><td>79.74</td><td>77.94</td><td>78.43</td><td>82.84</td></tr><tr><td><strong>GPT-4V</strong></td><td>76.14</td><td>75.49</td><td>75.49</td><td>77.45</td></tr><tr><td><strong>Gemini 1.5 Pro</strong></td><td>70.26</td><td>70.10</td><td>68.63</td><td>72.06</td></tr><tr><td><strong>Qwen-VL-Plus</strong></td><td>44.28</td><td>41.67</td><td>40.20</td><td>50.98</td></tr><tr><td><strong><em>Open-source</em></strong></td><td></td><td></td><td></td><td></td></tr><tr><td><strong>LLaVA-NeXT-110B</strong></td><td>66.67</td><td>69.12</td><td>63.24</td><td>67.65</td></tr><tr><td><strong>LLaVA-NeXT-72B</strong></td><td>64.71</td><td>66.67</td><td>63.73</td><td>63.73</td></tr><tr><td><strong>MiniCPM-LLaMA3-V 2.5</strong></td><td>56.86</td><td>55.88</td><td>54.90</td><td>59.80</td></tr><tr><td><strong>mPLUG-Owl2-7B</strong></td><td>53.43</td><td>55.39</td><td>50.98</td><td>53.92</td></tr><tr><td><strong>DeepSeek-VL-7B</strong></td><td>52.94</td><td>53.43</td><td>51.96</td><td>53.43</td></tr><tr><td><strong>Phi3-Vision-4.2B</strong></td><td>51.63</td><td>53.43</td><td>49.02</td><td>52.45</td></tr><tr><td><strong>CogVLM-chat</strong></td><td>50.65</td><td>52.94</td><td>51.96</td><td>47.06</td></tr><tr><td><strong>DeepSeek-VL-1.3B</strong></td><td>47.06</td><td>49.02</td><td>39.71</td><td>52.45</td></tr><tr><td><strong>CogAgent-vqa</strong></td><td>41.83</td><td>44.12</td><td>42.65</td><td>38.73</td></tr><tr><td><strong>LLaVA-NeXT-7B</strong></td><td>34.15</td><td>37.75</td><td>33.82</td><td>30.88</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents the performance of various Large Multimodal Models (LMMs) across three different age groups: old, middle-aged, and young. The performance is evaluated based on the average accuracy achieved by each model on a set of questions tailored to each age group. This evaluation is designed to assess the model&rsquo;s ability to understand and respond effectively to the diverse needs and preferences of different age demographics in various real-world scenarios. The table includes both closed-source models (like GPT-40 and GPT-4V) and open-source models (like LLaVA-NeXT and MiniCPM). The &lsquo;Avg&rsquo; column represents the unweighted average across all three age groups for each model.</p><details><summary>read the caption</summary>Table 3: Performance of Various Models Across Different Age Groups.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Type</th><th>Prompt Template</th></tr></thead><tbody><tr><td>Multiple</td><td></td></tr><tr><td>Choice</td><td>Now, we require you to solve a multiple-choice real-world question. Please briefly describe your thought process and provide the final answer(option).</td></tr><tr><td><strong>Question</strong>: &lt;Question></td><td></td></tr><tr><td><strong>Option</strong>: &lt;Option></td><td></td></tr><tr><td>Regarding the format, please answer following the template below, and be sure to include two &lt;> symbols:</td><td></td></tr><tr><td><strong>&lt;Thought process></strong>: &lt;&lt;your thought process>> <strong>&lt;Answer></strong>: &lt;&lt;your option>></td><td></td></tr></tbody></table></table></figure><blockquote><p>üîº This table shows the different prompt templates used for generating model responses. It includes the prompt template for multiple-choice questions, which asks the model to solve a real-world multiple-choice question, briefly describe its thought process, and provide the final answer (option). The table also specifies the desired response format, requesting the model to include its thought process and answer within specific tags.</p><details><summary>read the caption</summary>Table 4: Prompt templates for response generations.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Multiple</th><th></th></tr></thead><tbody><tr><td>Choice</td><td></td></tr></tbody></table></table></figure><blockquote><p>üîº This table provides the release time and source (download link if applicable) for each Large Multimodal Model (LMM) used in the MDI-Benchmark. This information allows researchers to access the specific models and understand their development timeline. This is crucial for reproducibility and for understanding how model performance relates to architecture and training data. The table includes both open-source and closed-source models.</p><details><summary>read the caption</summary>Table 5: The release time and model source of LMMs used in MDI-Benchmark</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Now, we require you to solve a multiple-choice real-world question. Please briefly</th><th></th></tr></thead><tbody><tr><td>describe your thought process and provide the final answer(option).</td><td></td></tr><tr><td><strong>Question</strong>: <question></td><td></td></tr><tr><td><strong>Option</strong>:<option></td><td></td></tr><tr><td>Regarding the format, please answer following the template below, and be</td><td></td></tr><tr><td>sure to include two &lt;> symbols:</td><td></td></tr><tr><td><strong><thought process></strong>: &#171;your thought process&#187; <strong><answer></strong>: &#171;your option&#187;</td><td></td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents a comprehensive breakdown of the performance of various Large Multimodal Models (LMMs) across different age demographics (old, middle-aged, and young) within six real-world scenarios (Architecture, Education, Housework, Social Services, Sport, and Transport). The table is organized to present the accuracy rates for each model within each age group and scenario combination. This granular view allows for a detailed analysis of model performance, highlighting strengths and weaknesses in catering to specific demographics and scenario types. The highest accuracy for each scenario-age group combination within both closed-source and open-source model categories is emphasized for benchmark comparison.</p><details><summary>read the caption</summary>Table 6: Performance of models across different age groups. The best performance in each block is highlighted in blue and green.</details></blockquote></details><h3 class="relative group">Full paper<div id=full-paper class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#full-paper aria-label=Anchor>#</a></span></h3><div id=gallery-084f18b9020a0eb830833f73c71c74d4 class=gallery><img src=https://ai-paper-reviewer.com/2412.12606/1.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.12606/2.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.12606/3.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.12606/4.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.12606/5.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.12606/6.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.12606/7.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.12606/8.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.12606/9.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.12606/10.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.12606/11.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.12606/12.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.12606/13.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.12606/14.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.12606/15.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.12606/16.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.12606/17.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.12606/18.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.12606/19.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.12606/20.png class="grid-w50 md:grid-w33 xl:grid-w25"></div></div><section class="flex flex-row flex-wrap justify-center pt-4 text-xl"><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12606/&amp;title=Multi-Dimensional%20Insights:%20Benchmarking%20Real-World%20Personalization%20in%20Large%20Multimodal%20Models" title="Share on LinkedIn" aria-label="Share on LinkedIn"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://twitter.com/intent/tweet/?url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12606/&amp;text=Multi-Dimensional%20Insights:%20Benchmarking%20Real-World%20Personalization%20in%20Large%20Multimodal%20Models" title="Tweet on Twitter" aria-label="Tweet on Twitter"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="mailto:?body=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12606/&amp;subject=Multi-Dimensional%20Insights:%20Benchmarking%20Real-World%20Personalization%20in%20Large%20Multimodal%20Models" title="Send via email" aria-label="Send via email"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1-27.64 140.9 68.65 266.2 199.1 285.1 19.01 2.888 36.17-12.26 36.17-31.49l1e-4-.6631c0-15.74-11.44-28.88-26.84-31.24-84.35-12.98-149.2-86.13-149.2-174.2.0-102.9 88.61-185.5 193.4-175.4 91.54 8.869 158.6 91.25 158.6 183.2v16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98.0036c-7.299.0-13.2 4.992-15.12 11.68-24.85-12.15-54.24-16.38-86.06-5.106-38.75 13.73-68.12 48.91-73.72 89.64-9.483 69.01 43.81 128 110.9 128 26.44.0 50.43-9.544 69.59-24.88 24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3c0-149.2-133.9-265.632-287.3-235.57zM239.1 304.3c-26.47.0-48-21.56-48-48.05s21.53-48.05 48-48.05 48 21.56 48 48.05-20.6 48.05-48 48.05z"/></svg></span></a></section></div><script>var oid="views_paper-reviews/2412.12606/index.md",oid_likes="likes_paper-reviews/2412.12606/index.md"</script><script type=text/javascript src=/ai-paper-reviewer/js/page.min.0860cf4e04fa2d72cc33ddba263083464d48f67de06114529043cb4623319efed4f484fd7f1730df5abea0e2da6f3538855634081d02f2d6e920b956f063e823.js integrity="sha512-CGDPTgT6LXLMM926JjCDRk1I9n3gYRRSkEPLRiMxnv7U9IT9fxcw31q+oOLabzU4hVY0CB0C8tbpILlW8GPoIw=="></script></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="flex group mr-3" href=/ai-paper-reviewer/paper-reviews/2412.13018/><span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&larr;</span>
<span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">OmniEval: An Omnidirectional and Automatic RAG Evaluation Benchmark in Financial Domain</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2024-12-17T00:00:00+00:00>17 December 2024</time>
</span></span></a></span><span><a class="flex text-right group ml-3" href=/ai-paper-reviewer/paper-reviews/2412.13185/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Move-in-2D: 2D-Conditioned Human Motion Generation</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2024-12-17T00:00:00+00:00>17 December 2024</time>
</span></span><span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&larr;</span></a></span></div></div><div class=pt-3><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class=pt-3><script src=https://utteranc.es/client.js repo=pmnxis/pmnxis.github.io issue-term=pathname label=Comment theme=dark-blue crossorigin=anonymous async></script></div></div></footer></article><div id=top-scroller class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer id=site-footer class="py-10 print:hidden"><nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400"><ul class="flex flex-col list-none sm:flex-row"><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=/ai-paper-reviewer/tags/ title=Tags>Tags</a></li><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=https://deep-diver.github.io/neurips2024/ title>NeurIPS2024</a></li></ul></nav><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2025
Hugging Face Daily Papers</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a></p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/ai-paper-reviewer/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh+sCQ0E53ghYrxgYqw+0GCRyIEpA=="></script></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://deep-diver.github.io/ai-paper-reviewer/ style=z-index:500><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body><script data-name=BMC-Widget data-cfasync=false src=https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js data-id=chansung data-description="Support me on Buy me a coffee!" data-message data-color=#FFDD00 data-position=Left data-x_margin=18 data-y_margin=18></script></html>