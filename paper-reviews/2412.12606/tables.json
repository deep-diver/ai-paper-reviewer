[{"content": "| Scenarios | Number of images | Number of L1 questions | Number of L2 questions | Number of old questions | Number of mid questions | Number of young questions |\n|---|---|---|---|---|---|---| \n| Architecture | 85 | 121 | 112 | 77 | 74 | 82 |\n| Education | 85 | 114 | 115 | 80 | 79 | 70 |\n| Housework | 86 | 103 | 109 | 71 | 74 | 67 |\n| Social services | 86 | 95 | 108 | 65 | 66 | 72 |\n| Sports | 86 | 107 | 103 | 70 | 73 | 67 |\n| Transport | 86 | 109 | 102 | 73 | 70 | 68 |\n| Total | 86 | 649 | 649 | 436 | 436 | 426 |", "caption": "Table 1: Statistical details of MDI-Benchmark.", "description": "This table provides a statistical overview of the MDI-Benchmark dataset, detailing the distribution of images and questions across different scenarios.  It breaks down the number of images and the number of questions for both Level 1 and Level 2 complexity, as well as the distribution of questions across the three age groups (old, mid, and young). The scenarios included are Architecture, Education, Housework, Social Services, Sports, and Transport.", "section": "3 MDI-BENCHMARK"}, {"content": "| Model | Final Score | Level 1 | Level 2 | |---|---|---|---|---|---|---|---|---|---|---|---|---|---|---| |---| Avg | Arc | Edu | Hou | Soc | Spo | Tra | Avg | Arc | Edu | Hou | Soc | Spo | Tra | | **_Closed-source_** | | | | | | | | | | | | | | | | | **GPT-4o** | **78.46** | 87.46 | 76.47 | 94.12 | 92.16 | 90.20 | 86.27 | 94.12 | 69.45 | 70.59 | 70.59 | 78.43 | 82.35 | 54.90 | 66.67 | | **GPT-4V** | 74.92 | 87.46 | 86.27 | 92.16 | 86.27 | 90.20 | 88.24 | 90.20 | 62.38 | 72.55 | 70.59 | 74.51 | 60.78 | 45.10 | 56.86 | | **Gemini 1.5 Pro** | 69.13 | 82.32 | 68.63 | 92.16 | 76.47 | 88.24 | 86.27 | 90.20 | 55.95 | 52.94 | 56.86 | 54.90 | 74.51 | 43.14 | 58.82 | | **Qwen-VL-Plus** | 43.57 | 56.59 | 43.14 | 64.71 | 62.75 | 78.43 | 50.98 | 45.10 | 30.55 | 35.29 | 41.18 | 37.25 | 25.49 | 23.53 | 23.53 | | **_Open-source_** | | | | | | | | | | | | | | | | | **LLaVA-NeXT-110B** | **65.59** | 79.10 | 60.78 | 92.16 | 78.43 | 84.31 | 78.43 | 88.24 | 52.09 | 66.67 | 56.86 | 54.90 | 64.71 | 31.37 | 43.14 | | **LLaVA-NeXT-72B** | 63.67 | 76.21 | 68.63 | 88.24 | 80.39 | 82.35 | 70.59 | 74.51 | 51.13 | 66.67 | 54.90 | 52.94 | 60.78 | 33.33 | 43.14 | | **MiniCPM-LLaMA3-V 2.5** | 55.95 | 72.67 | 52.94 | 86.27 | 70.59 | 82.35 | 70.59 | 80.39 | 39.23 | 45.10 | 49.02 | 49.02 | 31.37 | 27.45 | 37.25 | | **mPLUG-Owl2-7B** | 52.57 | 64.63 | 49.02 | 70.59 | 74.51 | 70.59 | 58.82 | 70.59 | 40.51 | 41.18 | 41.18 | 47.06 | 39.22 | 29.41 | 49.02 | | **DeepSeek-VL-7B** | 52.09 | 68.49 | 49.02 | 70.59 | 74.51 | 80.39 | 62.75 | 80.39 | 35.69 | 41.18 | 33.33 | 39.22 | 41.18 | 21.57 | 41.18 | | **Phi3-Vision-4.2B** | 50.80 | 67.20 | 50.98 | 76.47 | 60.78 | 80.39 | 62.75 | 78.43 | 34.41 | 37.25 | 33.33 | 41.18 | 43.14 | 21.57 | 33.33 | | **CogVLM-chat** | 49.84 | 60.77 | 49.02 | 72.55 | 62.75 | 56.86 | 68.63 | 60.78 | 38.91 | 49.02 | 33.33 | 43.14 | 41.18 | 27.45 | 43.14 | | **DeepSeek-VL-1.3B** | 46.30 | 58.20 | 45.10 | 56.86 | 66.67 | 56.86 | 66.67 | 62.75 | 34.41 | 35.29 | 29.41 | 29.41 | 39.22 | 27.45 | 49.02 | | **CogAgent-vqa** | 41.16 | 49.52 | 35.29 | 45.10 | 66.67 | 54.90 | 56.86 | 43.14 | 32.80 | 31.37 | 35.29 | 35.29 | 37.25 | 25.49 | 35.29 | | **LLaVA-NeXT-7B** | 33.60 | 43.09 | 31.37 | 52.94 | 43.14 | 49.02 | 39.22 | 47.06 | 24.12 | 35.29 | 13.73 | 37.25 | 23.53 | 9.80 | 27.45 |}  ", "caption": "Table 2: \nLMMs Performance on MDI-Benchmark in Terms of Level and Scenario.\nVertically, the table is composed of a model score and two Level sub-tables, where the model score is obtained from Formula \u00a01. Each sub-table consists of seven columns showing the accuracy rates of LMMs in different scenarios. The first column of each sub-table represents the mean value of the subsequent six columns, reflecting the overall performance at different levels. The annotations for Level and Scenario are as follows: Level 1: assessment questions that focus only on basic perceptual ability; Level 2: assessment questions that involve logical reasoning. The scenarios are abbreviated as follows: Arc (architecture), Edu (education), Hou (housework), Soc (social service), Spo (sport), Tra (transport). Horizontally, the table is divided into two blocks. For better statistics and analysis, we will display the blocks as closed-source model statistics and open-source model statistics. The best performance in each block is highlighted in blue and green.", "description": "This table presents the performance of various Large Multimodal Models (LMMs) on the Multi-Dimensional Insights (MDI) Benchmark, categorized by question complexity (Level 1: basic perception, Level 2: logical reasoning) and real-world scenarios (Architecture, Education, Housework, Social Services, Sport, Transport).  The table is split into two sections: closed-source and open-source models. Each model's overall score, calculated using a weighted average of Level 1 and Level 2 performance, is also presented.  Higher accuracy percentages indicate better performance, with the best scores for each model category (closed/open) highlighted.", "section": "4 EXPERIMENTS"}, {"content": "| Model | Avg | old | middle-aged | young |\n|---|---|---|---|---| \n| **_Closed-source_** | | | | |\n| **GPT-4o** | 79.74 | 77.94 | 78.43 | 82.84 |\n| **GPT-4V** | 76.14 | 75.49 | 75.49 | 77.45 |\n| **Gemini 1.5 Pro** | 70.26 | 70.10 | 68.63 | 72.06 |\n| **Qwen-VL-Plus** | 44.28 | 41.67 | 40.20 | 50.98 |\n| **_Open-source_** | | | | |\n| **LLaVA-NeXT-110B** | 66.67 | 69.12 | 63.24 | 67.65 |\n| **LLaVA-NeXT-72B** | 64.71 | 66.67 | 63.73 | 63.73 |\n| **MiniCPM-LLaMA3-V 2.5** | 56.86 | 55.88 | 54.90 | 59.80 |\n| **mPLUG-Owl2-7B** | 53.43 | 55.39 | 50.98 | 53.92 |\n| **DeepSeek-VL-7B** | 52.94 | 53.43 | 51.96 | 53.43 |\n| **Phi3-Vision-4.2B** | 51.63 | 53.43 | 49.02 | 52.45 |\n| **CogVLM-chat** | 50.65 | 52.94 | 51.96 | 47.06 |\n| **DeepSeek-VL-1.3B** | 47.06 | 49.02 | 39.71 | 52.45 |\n| **CogAgent-vqa** | 41.83 | 44.12 | 42.65 | 38.73 |\n| **LLaVA-NeXT-7B** | 34.15 | 37.75 | 33.82 | 30.88 |", "caption": "Table 3: Performance of Various Models Across Different Age Groups.", "description": "This table presents the performance of various Large Multimodal Models (LMMs) across three different age groups: old, middle-aged, and young. The performance is evaluated based on the average accuracy achieved by each model on a set of questions tailored to each age group. This evaluation is designed to assess the model's ability to understand and respond effectively to the diverse needs and preferences of different age demographics in various real-world scenarios.  The table includes both closed-source models (like GPT-40 and GPT-4V) and open-source models (like LLaVA-NeXT and MiniCPM).  The \"Avg\" column represents the unweighted average across all three age groups for each model.", "section": "4 EXPERIMENTS"}, {"content": "| Type | Prompt Template | \n|---|---| \n| Multiple\nChoice | Now, we require you to solve a multiple-choice real-world question. Please briefly describe your thought process and provide the final answer(option).\n**Question**: &lt;Question&gt;\n**Option**: &lt;Option&gt;\nRegarding the format, please answer following the template below, and be sure to include two &lt;&gt; symbols:\n**&lt;Thought process&gt;**: &lt;&lt;your thought process&gt;&gt; **&lt;Answer&gt;**: &lt;&lt;your option&gt;&gt; |", "caption": "Table 4: Prompt templates for response generations.", "description": "This table shows the different prompt templates used for generating model responses. It includes the prompt template for multiple-choice questions, which asks the model to solve a real-world multiple-choice question, briefly describe its thought process, and provide the final answer (option). The table also specifies the desired response format, requesting the model to include its thought process and answer within specific tags.", "section": "A.1 DETAILS OF THE PROMPT INFORMATION"}, {"content": "| Multiple |\n|---|---| \n| Choice |", "caption": "Table 5: The release time and model source of LMMs used in MDI-Benchmark", "description": "This table provides the release time and source (download link if applicable) for each Large Multimodal Model (LMM) used in the MDI-Benchmark. This information allows researchers to access the specific models and understand their development timeline. This is crucial for reproducibility and for understanding how model performance relates to architecture and training data. The table includes both open-source and closed-source models.", "section": "A.2 DETAILS OF THE EVALUATED MODELS"}, {"content": "| Now, we require you to solve a multiple-choice real-world question. Please briefly |\n|---|---| \n| describe your thought process and provide the final answer(option). |\n| **Question**: <Question> |\n| **Option**: <Option> |\n| Regarding the format, please answer following the template below, and be |\n| sure to include two <> symbols: |\n| **<Thought process>**: <<your thought process>> **<Answer>**: <<your option>> |", "caption": "Table 6: Performance of models across different age groups. The best performance in each block is highlighted in blue and green.", "description": "This table presents a comprehensive breakdown of the performance of various Large Multimodal Models (LMMs) across different age demographics (old, middle-aged, and young) within six real-world scenarios (Architecture, Education, Housework, Social Services, Sport, and Transport).  The table is organized to present the accuracy rates for each model within each age group and scenario combination. This granular view allows for a detailed analysis of model performance, highlighting strengths and weaknesses in catering to specific demographics and scenario types. The highest accuracy for each scenario-age group combination within both closed-source and open-source model categories is emphasized for benchmark comparison.", "section": "D. MORE DETAILS ON EXPERIMENT RESULTS"}]