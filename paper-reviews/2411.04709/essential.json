{"importance": "This paper is crucial because **it addresses the critical need for a dedicated dataset in image-to-video prompt research.**  Existing datasets lack the specific focus on user-provided text and image prompts alongside generated videos, hindering advancements in model safety and user experience.  TIP-I2V facilitates research on user preference analysis, model safety enhancement, and improved benchmark creation, thus **significantly advancing image-to-video technology.**", "summary": "TIP-I2V: A million-scale dataset provides 1.7 million real user text & image prompts for image-to-video generation, boosting model development and safety.", "takeaways": ["TIP-I2V dataset offers over 1.7 million real user-provided text and image prompts for image-to-video generation.", "The dataset includes videos generated by five state-of-the-art models, enabling better evaluation of multi-dimensional model performance.", "TIP-I2V facilitates research in user preference analysis, model safety enhancement (addressing misinformation), and creation of more practical benchmarks for image-to-video models."], "tldr": "Current image-to-video models rely heavily on user-provided text and image prompts, yet there's a lack of comprehensive datasets studying these prompts. This limits progress in understanding user preferences and creating safer models. Existing datasets either focus on text-to-video or text-to-image tasks, failing to capture the nuances of image-to-video.\nThe paper introduces TIP-I2V, a large-scale dataset with over 1.7 million unique user prompts (text and image) and corresponding videos generated by five state-of-the-art models.  This allows researchers to analyze user preferences, improve model safety by addressing misinformation, and build more comprehensive benchmarks.  TIP-I2V's unique structure, scale, and scope significantly advance image-to-video research and its practical applications.", "affiliation": "University of Technology Sydney", "categories": {"main_category": "Multimodal Learning", "sub_category": "Vision-Language Models"}}