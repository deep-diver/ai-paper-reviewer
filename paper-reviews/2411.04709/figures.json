[{"figure_path": "https://arxiv.org/html/2411.04709/x2.png", "caption": "Figure 1: TIP-I2V is the first dataset comprising over 1.70 million unique user-provided text and image prompts. Besides the prompts, TIP-I2V also includes videos generated by five state-of-the-art image-to-video models (\ud835\ude7f\ud835\ude92\ud835\ude94\ud835\ude8a\ud835\ude7f\ud835\ude92\ud835\ude94\ud835\ude8a\\mathtt{Pika}typewriter_Pika [5], \ud835\ude82\ud835\ude9d\ud835\ude8a\ud835\ude8b\ud835\ude95\ud835\ude8e\ud835\ude82\ud835\ude9d\ud835\ude8a\ud835\ude8b\ud835\ude95\ud835\ude8e\\mathtt{Stable}typewriter_Stable \ud835\ude85\ud835\ude92\ud835\ude8d\ud835\ude8e\ud835\ude98\ud835\ude85\ud835\ude92\ud835\ude8d\ud835\ude8e\ud835\ude98\\mathtt{Video}typewriter_Video \ud835\ude73\ud835\ude92\ud835\ude8f\ud835\ude8f\ud835\ude9e\ud835\ude9c\ud835\ude92\ud835\ude98\ud835\ude97\ud835\ude73\ud835\ude92\ud835\ude8f\ud835\ude8f\ud835\ude9e\ud835\ude9c\ud835\ude92\ud835\ude98\ud835\ude97\\mathtt{Diffusion}typewriter_Diffusion [8], \ud835\ude7e\ud835\ude99\ud835\ude8e\ud835\ude97\u2062-\u2062\ud835\ude82\ud835\ude98\ud835\ude9b\ud835\ude8a\ud835\ude7e\ud835\ude99\ud835\ude8e\ud835\ude97-\ud835\ude82\ud835\ude98\ud835\ude9b\ud835\ude8a\\mathtt{Open\\text{-}Sora}typewriter_Open - typewriter_Sora [73], \ud835\ude78\ud835\udff8\ud835\ude85\ud835\ude76\ud835\ude8e\ud835\ude97\u2062-\u2062\ud835\ude87\ud835\ude7b\ud835\ude78\ud835\udff8\ud835\ude85\ud835\ude76\ud835\ude8e\ud835\ude97-\ud835\ude87\ud835\ude7b\\mathtt{I2VGen\\text{-}XL}typewriter_I2VGen - typewriter_XL [71], and \ud835\ude72\ud835\ude98\ud835\ude90\ud835\ude85\ud835\ude92\ud835\ude8d\ud835\ude8e\ud835\ude98\ud835\ude87\u2062-\u2062\ud835\udffb\u2062\ud835\ude71\ud835\ude72\ud835\ude98\ud835\ude90\ud835\ude85\ud835\ude92\ud835\ude8d\ud835\ude8e\ud835\ude98\ud835\ude87-5\ud835\ude71\\mathtt{CogVideoX\\text{-}5B}typewriter_CogVideoX - typewriter_5 typewriter_B [69]). The TIP-I2V contributes to the development of better and safer image-to-video models.", "description": "Figure 1 shows the TIP-I2V dataset, which contains over 1.7 million unique text and image prompts created by real users.  These prompts were used to generate videos using five different state-of-the-art image-to-video models: Pika, Stable Video Diffusion, Open-Sora, I2VGen-XL, and CogVideoX-5B.  The figure visually represents a small sample of these prompts and resulting videos to illustrate the dataset's diversity and scale. The TIP-I2V dataset aims to advance the development of improved and safer image-to-video generation models.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2411.04709/x3.png", "caption": "Figure 2: A data point in our TIP-I2V includes UUID, timestamp, text and image prompt, subject, NSFW status of text and image, text and image embedding, and the corresponding generated videos.", "description": "The figure displays a sample data point from the TIP-I2V dataset.  It shows the various components included for each data point: a unique identifier (UUID), a timestamp indicating when the data was collected, the text prompt provided by the user, the image prompt used, the subject of the prompt, NSFW (Not Safe For Work) status flags for both the text and image,  embeddings representing the text and image prompts, and finally, the corresponding videos generated by five different image-to-video models. This comprehensive structure makes the dataset valuable for researching user prompts and improving image-to-video models.", "section": "3. Curating TIP-I2V"}, {"figure_path": "https://arxiv.org/html/2411.04709/x4.png", "caption": "Table 1: Comparison of our TIP-I2V (image-to-video) with popular VidProM (text-to-video) and DiffusionDB (text-to-image) in terms of basic information. Our TIP-I2V is comparable in scale to these datasets but focuses on different aspects of visual generation.", "description": "This table compares the TIP-I2V dataset with two other popular datasets, VidProM and DiffusionDB, highlighting key differences in their scope and focus.  All three datasets are large-scale, but TIP-I2V is unique in its concentration on image-to-video generation, using both text and image prompts, unlike VidProM (text-to-video) and DiffusionDB (text-to-image). The table provides a detailed breakdown of the number of unique prompts, embedding methods, prompt length, data collection time span, and number of generation sources. This comparison emphasizes the unique characteristics of TIP-I2V and its contribution to the field of image-to-video research.", "section": "4. Comparing TIP-I2V with Similar Datasets"}, {"figure_path": "https://arxiv.org/html/2411.04709/x5.png", "caption": "Figure 3: Our TIP-I2V (image-to-video) differs from popular VidProM (text-to-video) and DiffusionDB (text-to-image) in terms of semantics. Top: Example prompts from the three datasets. Bottom: The WizMap [65] visualization of our TIP-I2V compared to VidProM/DiffusionDB. Please \\faSearch\u00a0zoom in to see the detailed semantic focus of text prompts across the three datasets.", "description": "Figure 3 demonstrates the key differences between TIP-I2V and two other popular prompt datasets: VidProM (text-to-video) and DiffusionDB (text-to-image).  The top part of the figure shows example prompts from each dataset, highlighting the varying levels of specificity and semantic focus.  The bottom part utilizes a WizMap visualization to compare the semantic distributions of the text prompts across the three datasets. This visual representation allows for a deeper understanding of how the prompts in TIP-I2V differ semantically from prompts in VidProM and DiffusionDB, showcasing a different style of prompt crafting oriented around animating elements within an existing image.", "section": "Differences between TIP-I2V and other similar datasets in basic and semantic information"}, {"figure_path": "https://arxiv.org/html/2411.04709/x6.png", "caption": "Figure 4: The top 25252525 subjects (top) and directions (bottom) preferred by users when generating videos from images.", "description": "This figure shows the top 25 most frequent subjects and directions chosen by users when using the TIP-I2V dataset for image-to-video generation.  The top panel displays a bar chart representing the frequency of subjects (categories of objects/scenes), while the bottom panel shows the frequency of directions (actions or movements applied to the subjects).  This visualization helps to understand user preferences and biases in terms of what kinds of scenes and actions are commonly requested for image-to-video synthesis, informing the design and evaluation of image-to-video models.", "section": "5.1. Catering Users Better"}, {"figure_path": "https://arxiv.org/html/2411.04709/x7.png", "caption": "Figure 5: The ratio of the sum of the top N\ud835\udc41Nitalic_N subjects (top) or directions (bottom) to the total frequencies.", "description": "This figure shows two line graphs. The top graph displays the cumulative proportion of the top N subjects, indicating the percentage of total subject frequency accounted for by the top N most frequent subjects.  The bottom graph presents the same analysis but for the top N most frequent directions used in video generation prompts.  Both graphs illustrate the uneven distribution of subject and direction preferences among users, showing that a relatively small number of subjects and directions represent a significant portion of all prompts.", "section": "5.1. Catering Users Better"}, {"figure_path": "https://arxiv.org/html/2411.04709/x8.png", "caption": "Table 2: A comparison of the proposed benchmark with existing ones. Our TIP-Eval is more comprehensive and practical.", "description": "Table 2 compares the proposed TIP-Eval benchmark with existing benchmarks (VBench-I2V, I2V-Bench, AIGCBench) in terms of comprehensiveness and practicality for evaluating image-to-video models.  TIP-Eval uses 1000 subjects and 10,000 real user prompts, providing a more comprehensive and practical evaluation than previous benchmarks, which had limited subjects and/or prompts generated by algorithms rather than real users.", "section": "5. Comparing TIP-I2V with Similar Datasets"}, {"figure_path": "https://arxiv.org/html/2411.04709/x9.png", "caption": "Figure 6: Benchmarking results using 10,0001000010,00010 , 000 prompts in TIP-Eval and 10 dimensions from [25, 49, 18]. Similar to VBench [25], results are normalized per dimension for clearer comparisons.", "description": "Figure 6 presents a radar chart visualizing the performance of five different image-to-video diffusion models across ten evaluation dimensions.  The models are compared using TIP-Eval, a new benchmark dataset comprising 10,000 prompts, which ensures a more practical and real-world evaluation compared to existing benchmarks.  Each dimension represents a different aspect of video quality, such as temporal consistency, aesthetic quality, and alignment between the video and its text or image prompts.  The results are normalized across dimensions for ease of comparison, allowing for a direct visual assessment of the relative strengths and weaknesses of each model in various aspects of video generation.", "section": "5.2. More Comprehensive and Practical Evaluation"}, {"figure_path": "https://arxiv.org/html/2411.04709/x10.png", "caption": "Figure 7: A case illustrating the misuse of image-to-video models, resulting in misinformation: given a friendly image of \ud835\ude74\ud835\ude95\ud835\ude98\ud835\ude97\ud835\ude74\ud835\ude95\ud835\ude98\ud835\ude97\\mathtt{Elon}typewriter_Elon \ud835\ude7c\ud835\ude9e\ud835\ude9c\ud835\ude94\ud835\ude7c\ud835\ude9e\ud835\ude9c\ud835\ude94\\mathtt{Musk}typewriter_Musk and \ud835\ude73\ud835\ude98\ud835\ude97\ud835\ude8a\ud835\ude95\ud835\ude8d\ud835\ude73\ud835\ude98\ud835\ude97\ud835\ude8a\ud835\ude95\ud835\ude8d\\mathtt{Donald}typewriter_Donald \ud835\ude83\ud835\ude9b\ud835\ude9e\ud835\ude96\ud835\ude99\ud835\ude83\ud835\ude9b\ud835\ude9e\ud835\ude96\ud835\ude99\\mathtt{Trump}typewriter_Trump shaking hands, an image-to-video model can easily generate a video of them fighting, which fuels political rumors.", "description": "The figure shows an example of how image-to-video models can generate misinformation.  A friendly image of Elon Musk and Donald Trump shaking hands is used as input. An image-to-video model easily creates a video of them fighting, which can spread false narratives and fuel political rumors.  This highlights the risk of using these models to manipulate the meaning of images and generate misleading content.", "section": "5.3 Identifying Generated Videos from Images"}, {"figure_path": "https://arxiv.org/html/2411.04709/x11.png", "caption": "Table 3: The generalization experiments of existing fake image detection methods to identify generated videos from images.", "description": "This table presents the results of evaluating several existing fake image detection methods on videos generated from images.  It demonstrates the generalization ability of these methods by testing their performance across videos created by different image-to-video models. The results are expressed as accuracy percentages, showing how well each method can distinguish between real and generated video frames.  The inclusion of 'Blind Guess' provides a baseline for comparison.", "section": "5.3 Identifying Generated Videos from Images"}, {"figure_path": "https://arxiv.org/html/2411.04709/x12.png", "caption": "Table 4: Our trained strong detector\u2019s performance in classifying videos as real, text-generated, or image-generated. \u2018Same/Cross Domain\u2019 refers to training and testing on the same or different diffusion models, respectively.", "description": "This table presents the performance of a trained model designed to distinguish between real videos and videos generated using text or image prompts by diffusion models.  The results are categorized by whether the model was trained and tested on the same diffusion model ('Same Domain') or different diffusion models ('Cross Domain').  The table shows the accuracy (in percentage) achieved by the model in classifying videos into these three categories.", "section": "5.3 Identifying Generated Videos from Images"}]