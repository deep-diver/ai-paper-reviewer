{"references": [{"fullname_first_author": "Aakanksha Chowdhery", "paper_title": "PaLM: Scaling language modeling with pathways", "publication_date": "2023-01-01", "reason": "This paper is important as it establishes mixture weights based on the performance for large language models."}, {"fullname_first_author": "Niklas Muennighoff", "paper_title": "Crosslingual generalization through multitask finetuning", "publication_date": "2022-11-01", "reason": "This paper is important because the dataset xP3 is used to address the high variance in downstream tasks."}, {"fullname_first_author": "Sang Michael Xie", "paper_title": "Doremi: Optimizing data mixtures speeds up language model pretraining", "publication_date": "2024-01-01", "reason": "This paper is one of the learning-based methods that involves training of small proxy models across domains to generate optimal domain weights."}, {"fullname_first_author": "Leo Gao", "paper_title": "The pile: An 800gb dataset of diverse text for language modeling", "publication_date": "2020-01-01", "reason": "This paper is important because the Pile dataset is used in this paper to simulate separate collection of training and validation data."}, {"fullname_first_author": "Daria Soboleva", "paper_title": "SlimPajama: A 627B token cleaned and deduplicated version of RedPajama", "publication_date": "2023-01-01", "reason": "This paper is important because the SlimPajama dataset is used as dataset for the experiments."}]}