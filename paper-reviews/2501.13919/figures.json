[{"figure_path": "https://arxiv.org/html/2501.13919/x1.png", "caption": "Figure 1: Temporal Preference Optimization (TPO) is a self-improvement preference optimization technique designed to enhance video comprehension in video-LMMs by modeling temporal preferences at two granular levels: localized and comprehensive TPO. In localized TPO (upper-left), we generate queries focused on short segments, with contrastive responses that retain or exclude the target segment. For comprehensive TPO (lower-left), queries are designed for high-level understanding, using intact video versus sparse downsampled video for contrasting responses. After post-filtering, the contrast response pairs are serving as the preference dataset to train a video-LMM, guiding the model to prioritize preferred responses for improved video understanding.", "description": "This figure illustrates the Temporal Preference Optimization (TPO) framework, a self-improvement method enhancing video-LMMs' understanding.  It operates at two levels: localized TPO, focusing on short video segments and contrasting responses that either include or exclude the target segment; and comprehensive TPO, using full versus downsampled videos to contrast responses to high-level queries.  The resulting preference data, after filtering, trains the video-LMM to favor temporally accurate responses.", "section": "3. Temporal Preference Optimization"}, {"figure_path": "https://arxiv.org/html/2501.13919/x2.png", "caption": "Figure 2: The performance of LongVA-TPO and LongVA on MLVU with different input lengths. LongVA-TPO consistently shows performance improvements with longer inputs, whereas LongVA experiences performance degradation when the input exceeds 64 frames.", "description": "This figure displays a comparison of the performance of the LongVA and LongVA-TPO models on the MLVU benchmark across various video lengths. The x-axis represents the input video length (number of frames) on a logarithmic scale, and the y-axis shows the performance score of each model.  The results reveal that LongVA-TPO consistently demonstrates improved performance as input video length increases, showcasing better handling of longer contexts. Conversely, the performance of the LongVA model plateaus and even degrades when the input exceeds 64 frames, indicating limitations in processing extended temporal information.", "section": "4.3 Ablation Study"}, {"figure_path": "https://arxiv.org/html/2501.13919/extracted/6150550/sec/length.png", "caption": "Figure 3: Comparison of our LongVA-TPO model and the original LongVA model on the needle-in-a-haystack task.", "description": "Figure 3 presents a visual comparison of the LongVA-TPO model's performance against the original LongVA model on the 'needle-in-a-haystack' task.  This task tests the ability of the models to locate a specific, rare event within a lengthy video. The figure uses heatmaps to represent model performance at different video lengths, showing the percentage of correct responses across varying video durations. The heatmaps highlight how the LongVA-TPO model maintains consistently better performance compared to the original LongVA, especially as the video length increases.", "section": "4.3 Ablation Study"}, {"figure_path": "https://arxiv.org/html/2501.13919/extracted/6150550/sec/question_type2.png", "caption": "Figure 4: Qualitative comparison between LongVA-TPO model and LongVA on two videos from VideoMME benchmark. Our LongVA-TPO model demonstrates superior generation quality on both comprehensive and localized questions.", "description": "This figure showcases a qualitative comparison of the LongVA and LongVA-TPO models' performance on two videos sourced from the VideoMME benchmark dataset.  The comparison focuses on the quality of generated responses to both comprehensive (high-level understanding) and localized (specific segment) questions.  The results illustrate that the LongVA-TPO model consistently produces superior response quality, demonstrating the effectiveness of the Temporal Preference Optimization (TPO) framework in enhancing the temporal grounding capabilities of video-LMMs.", "section": "4. Results"}, {"figure_path": "https://arxiv.org/html/2501.13919/x5.png", "caption": "Figure 5: The distribution of lengths for 8K crawled videos.", "description": "This figure shows a pie chart illustrating the distribution of video lengths within a dataset of 8,000 videos used for training.  The dataset is comprised of videos with durations categorized into four ranges: less than 1 minute, 1 to 3 minutes, 3 to 10 minutes, and more than 10 minutes. The chart visually represents the proportion of videos falling into each of these duration categories, offering insight into the temporal characteristics of the video data used in the study.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2501.13919/x6.png", "caption": "Figure 6: The distribution of question types for 10K curated preference dataset for LongVA-TPO.", "description": "This figure shows a pie chart visualizing the distribution of various question types within a curated dataset of 10,000 question-answer pairs. This dataset was specifically created for training the LongVA-TPO model, a model focused on enhancing temporal understanding in videos. Each slice of the pie chart represents a different question type, indicating the relative frequency of each type in the dataset.  The categories included are: Temporal Reasoning, Action Reasoning, Causal Reasoning, Information Extraction, Descriptive Questions, Summarization, Object Reasoning, and Spatial Reasoning. The sizes of the slices visually represent the proportion of each question type within the dataset.", "section": "4. Experiments"}]