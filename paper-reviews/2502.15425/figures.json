[{"figure_path": "https://arxiv.org/html/2502.15425/x1.png", "caption": "Figure 1: Three- and two-level hierarchical agents used in the four-agent MPE-Spread environment. Yellow boxes represent the hierarchy levels, while blue connections indicate what each agent perceives as its environment. Red connections illustrate how the agents in the real environment are controlled, and green boxes represent the goals that the agents must reach.", "description": "This figure illustrates the hierarchical agent structures used in the four-agent Multi-Agent Particle World (MAPW) Spread environment.  Two different hierarchies are shown: one with three levels and another with two levels.  Yellow boxes depict the hierarchical levels, with the top-level agent coordinating lower levels. Blue lines show the information flow from each level to the next higher level.  Red lines illustrate how each agent controls its corresponding agent in the environment.  Green boxes represent the goals that the agents must reach.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2502.15425/x2.png", "caption": "Figure 2: Representation of the information flows between a level l\ud835\udc59litalic_l with two agents and the levels above and below. The top-down flow of actions is shown in blue. The bottom-up flux of messages and rewards is shown in red and green, respectively.", "description": "Figure 2 illustrates the information flow within a hierarchical multi-agent system using the TAG framework.  It focuses on a single level (level \ud835\udc59) containing two agents. Blue arrows depict the top-down flow of actions from level \ud835\udc59+1 to level \ud835\udc59.  Red and green arrows show the bottom-up flow of messages and rewards, respectively, from level \ud835\udc59-1 to level \ud835\udc59. This diagram helps visualize how TAG facilitates communication and coordination between adjacent levels in a hierarchy, while maintaining agent autonomy.", "section": "3. TAG Framework"}, {"figure_path": "https://arxiv.org/html/2502.15425/x3.png", "caption": "Figure 3: Mean average reward in the MPE-Spread environment (a) and Balance environment (b). Mean is calculated over 5 random seeds. Shaded areas represent 95% confidence intervals. Dotted red line in (a) shows the performance of an hand-designed heuristic.", "description": "Figure 3 presents a comparison of the average rewards obtained by various multi-agent reinforcement learning (MARL) algorithms across two benchmark environments: MPE-Spread and Balance.  The results are averaged over five independent runs with different random seeds to account for stochasticity. Shaded regions indicate the 95% confidence intervals around the mean reward for each algorithm.  For the MPE-Spread environment, a hand-designed heuristic's performance is included as a dotted red line, serving as a performance upper bound. This figure highlights the relative performance of different approaches, including various hierarchical and non-hierarchical methods, across different task complexities, showing how hierarchical organization might affect performance and sample efficiency.", "section": "5. Empirical Validation"}]