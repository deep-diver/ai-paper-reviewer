[{"content": "| Benchmarks | #Videos | #QAs | Perception Level | Data source | Dataset Feature |\n|---|---|---|---|---|---| \n| MVBench [23] | 4,000 | 4,000 | general, motion&lt;30% | existing datasets | general |\n| TempCompass [28] | 410 | 1,580 | general, motion&lt;20% | ShutterStock | temporal concept |\n| VideoMME [8] | 900 | 2,700 | general, motion&lt;20% | Youtube | general |\n| AutoEval-Video [4] | 327 | 327 | event level | Youtube | open-ended QA |\n| EgoSchema [31] | 5,031 | 5031 | event level | ego-centric video | ego-centric |\n| LVBench [39] | 103 | 1,549 | event & story level | Youtube | long video |\n| LongVideoBench [41] | 3,763 | 6,678 | event & story level | web channels | long videos |\n| MovieChat-1K [35] | 130 | 1,950 | story level | movies | movie |\n| Short Film Dataset [9] | 1,078 | 4,885 | story level | short films | story-level |\n| MotionBench | 5,385 | 8,052 | motion level | web videos, movies, synthetic videos, datasets | motion perception |", "caption": "Table 1: The comparison of existing video VLM benchmarks with MotionBench. MotionBench collects various video sources including web videos and synthetic videos, and provides a new evaluation perspective in motion level perception.", "description": "This table compares MotionBench with other existing video VLM benchmarks.  The comparison includes the number of videos and questions/answers (QAs) in each benchmark.  It also highlights the level of perception each benchmark focuses on (general, motion, event, or story level), the data sources used, and the types of features present in the datasets (general, temporal concepts, etc.).  MotionBench is distinct in its focus on motion-level perception, using a diverse range of video sources and providing a more granular evaluation of fine-grained motion understanding than other benchmarks.", "section": "2. Related Work"}, {"content": "| Category             | \n|----------------------|\n| web videos, movies   |\n| synthetic videos, datasets |", "caption": "Table 2: The MotionBench curation process. Categories [1-3] refer to \u201cvideos with intricate interactions\u201d, \u201cvideos from specific fields\u201d and \u201cvirtual videos\u201d, detailed in Sec.\u00a03.1. N. Vid/QA refers to the number of videos and queries in a category. min(H, W) is the minimum of the height and width of the video frames. len refers to the processed video duration. We automatically construct the queries in Virtual scenes, and manually annotate the other QA pairs in MotinBench.", "description": "Table 2 details the process of creating the MotionBench dataset. It breaks down the dataset creation into three categories: videos with complex interactions, videos from specific domains (like medical or sports), and synthetic videos.  For each category, it shows the number of videos and associated questions (N. Vid/QA), the source of the videos (whether collected directly or from an existing dataset), how the videos were processed (e.g., scene segmentation), and whether the questions and answers were automatically generated or manually annotated.  The table also lists the minimum resolution (min(H,W)) and minimum duration (len) of the processed video clips.  This provides a detailed overview of how the MotionBench dataset was compiled, highlighting the different sources, processing steps, and annotation methods used for each category.", "section": "3. MotionBench: Motion-Level Benchmarking"}, {"content": "| Category | # Videos/QAs | Source | Collection | Post-process | Annotation |\n|---|---|---|---|---|---| \n| 1 | 2,355/4,922 | Pexels | Self-collected | Directly adopt | Caption & QA |\n|  |  | Pandas-70M [3] | Open-sourced | Segment with scene detection | Caption & QA |\n|  |  | Movie clips | Self-collected | Segment with scene detection | Caption & QA |\n| 2 | 2,430/2,530 | MedVid\u00a0[14] | Open-sourced | min(H,W)>448 & len\u2208[3,60]sec | QA |\n|  |  | SportsSloMo\u00a0[2] | Open-sourced | min(H,W)>448 & len\u2208[3,60]sec | QA |\n|  |  | HA-ViD\u00a0[52] | Open-sourced | min(H,W)>448 & len\u2208[3,60]sec | QA |\n| 3 | 600/600 | Virtual scenes | Self-collected | Remove renderings with occlusion | Automatic QA |", "caption": "Table 3: Evaluation results of the existing video VLMs. Abbreviations: MR (Motion Recognition), LM (Location-related Motion), CM (Camera Motion), MO (Motion-related Objects), AO (Action Order), RC (Repetition Count). We randomly split MotionBench into \u201cdev\u201d and \u201ctest\u201d. We will release the ground truth answers in the \u201cdev\u201d set and set up an online platform for results submission in the \u201ctest\u201d set.", "description": "This table presents the performance of various existing Video Vision Language Models (VLMs) on the MotionBench benchmark.  The benchmark assesses six categories of motion-related tasks: Motion Recognition (MR), Location-related Motion (LM), Camera Motion (CM), Motion-related Objects (MO), Action Order (AO), and Repetition Count (RC).  The table shows the average performance across a randomly split development set ('dev') and a held-out test set ('test') of the MotionBench dataset.  The 'dev' set's ground truth answers are released to facilitate model evaluation, while an online platform will be established for submitting and evaluating results on the 'test' set.  The results highlight that even well-established VLMs struggle with fine-grained motion comprehension.", "section": "5. Experiments"}, {"content": "| Model | LLM | # Frames | Dev AVG (4020) | Test AVG (4034) | MR | LM | CM | MO | AO | RC |\n|---|---|---|---|---|---|---|---|---|---|---| \n| Random | - | - | 0.25 | 0.25 | 0.25 | 0.25 | 0.25 | 0.25 | 0.25 | 0.25 |\n| *LLM: **Text** as Input* |  |  |  |  |  |  |  |  |  |  |\n| GPT-4o [33] | - | - | 0.33 | 0.33 | 0.31 | 0.34 | 0.36 | 0.37 | 0.42 | 0.23 |\n| *Video VLMs : **Text + Multiple Frames** as Input* |  |  |  |  |  |  |  |  |  |  |\n| Gemini 1.5 Pro [34] | - | 1fps | 0.51 | 0.50 | 0.51 | 0.52 | 0.54 | 0.67 | 0.40 | 0.22 |\n| Qwen2VL-2B [36] | Qwen2 [37] | 1fps | 0.48 | 0.47 | 0.49 | 0.49 | 0.42 | 0.62 | 0.32 | 0.28 |\n| Qwen2VL-7B [36] | Qwen2 [37] | 1fps | 0.52 | 0.52 | 0.52 | 0.55 | 0.49 | 0.68 | 0.39 | 0.32 |\n| Qwen2VL-72B [36] | Qwen2 [37] | 1fps | 0.57 | **0.58** | 0.58 | **0.61** | **0.63** | 0.72 | **0.47** | 0.31 |\n| InternVL-40B [6] | NH-2-Yi-34B [32] | 8 | 0.55 | 0.54 | 0.54 | 0.58 | 0.49 | **0.76** | 0.41 | 0.30 |\n| PLLaVA-34B [44] | Yi-34B [32] | 16 | 0.52 | 0.51 | 0.55 | 0.51 | 0.47 | 0.66 | 0.38 | 0.31 |\n| CogVLM2-Video [15] | LLaMA3-8B [1] | 24 | 0.41 | 0.44 | 0.43 | 0.39 | 0.38 | 0.64 | 0.37 | 0.33 |\n| GLM-4V-plus [15] | GLM4 [10] | 30 | 0.54 | 0.55 | 0.57 | 0.57 | 0.54 | 0.69 | 0.40 | 0.37 |\n| LLaVA-NeXT [50] | Yi-34B [32] | 32 | 0.48 | 0.40 | 0.53 | 0.45 | 0.36 | 0.66 | 0.39 | 0.23 |\n| MiniCPM-V2.6 [46] | Qwen2 [37] | 64 | 0.52 | 0.53 | 0.56 | 0.49 | 0.45 | 0.72 | 0.39 | 0.33 |\n| Oryx-34B [29] | Yi-34B [32] | 64 | 0.49 | 0.49 | 0.48 | 0.52 | 0.44 | 0.65 | 0.42 | 0.32 |\n| TE Fusion (ours) | GLM4-9B [10] | 16 | **0.58** | **0.58** | **0.64** | 0.59 | 0.51 | 0.69 | 0.41 | **0.39** |", "caption": "Table 4: Benchmark results for different compression methods at various compression rates, all using the same sequence length in the VLM decoder. We set Ninputk=4subscript\ud835\udc41input\ud835\udc584\\frac{N_{\\text{input}}}{k}=4divide start_ARG italic_N start_POSTSUBSCRIPT input end_POSTSUBSCRIPT end_ARG start_ARG italic_k end_ARG = 4, with the baseline representing video models that process 4 frames without compression. Note that each compression method is re-implemented on the GLM-4V-9B backbone to ensure a fair comparison.", "description": "This table presents a comparison of different video compression methods' performance on the MotionBench benchmark.  The experiment controls for the length of the input sequence to the VLM decoder, holding it constant while varying the compression ratio (k) and the number of input frames (Ninput).  The results show the performance for each compression technique (QFormer, Qwen2-VL, PLLaVA, Kangaroo, and TE Fusion) at different compression ratios, with a baseline representing a model that uses 4 frames without compression. All methods were implemented using the same GLM-4V-9B backbone for fair comparison.", "section": "5. Experiments"}, {"content": "| Dev AVG |\n|---|---| \n| (4020) |", "caption": "Table 5:", "description": "Table 5 presents the evaluation results of existing video Vision Language Models (VLMs) on the MotionBench benchmark.  It shows the average performance (across a development and test set) of various models on six motion-related tasks: Motion Recognition (MR), Location-related Motion (LM), Camera Motion (CM), Motion-related Objects (MO), Action Order (AO), and Repetition Count (RC).  The table includes the model name, the underlying Large Language Model (LLM), the number of frames used as input, and the average accuracy scores on the development and test sets for each task, as well as an overall average accuracy.  A random baseline is also provided for comparison.", "section": "5. Experiments"}, {"content": "| Test AVG |\n|---|---| \n| (4034) |", "caption": "Table 6: The model configurations of all ablated architectures.", "description": "This table details the specific configurations of the Vision Language Models (VLMs) used in the experiments.  It lists the hyperparameters for the decoder and the visual encoder components of the various models, allowing for comparison across different architectures and highlighting the key settings that impact performance.  The configurations help in understanding and replicating the experimental setup.", "section": "4. Model Design: Motion-Level Perception"}, {"content": "| k | Method | MotionBench | MVBench | VideoMME short | VideoMME medium | VideoMME long |\n|---|---|---|---|---|---|---|\n| 1 | baseline | 47.6 | 64.5 | 51.4 | 41.0 | 38.3 |\n| 2 | QFormer | 43.5 | 62.1 | 42.8 | 39.6 | 36.3 |\n| 2 | Qwen2-VL | 48.0 | 66.5 | 54.1 | 43.1 | 37.8 |\n| 2 | PLLaVA | 48.5 | 68.8 | 54.9 | 44.9 | 39.6 |\n| 2 | Kangaroo | 48.4 | 69.2 | 55.4 | 43.0 | 38.8 |\n| 2 | TE Fusion (ours) | 49.1 | 69.0 | 55.2 | 46.3 | 40.0 |\n| 4 | QFormer | 44.3 | 63.8 | 45.2 | 41.0 | 36.8 |\n| 4 | Qwen2-VL | 47.6 | 65.6 | 51.8 | 43.4 | 39.4 |\n| 4 | PLLaVA | 50.5 | 70.2 | 58.9 | 46.4 | 41.3 |\n| 4 | Kangaroo | 50.0 | 69.8 | 55.3 | 45.6 | 39.5 |\n| 4 | TE Fusion (ours) | 51.0 | 72.1 | 61.0 | 47.3 | 42.1 |", "caption": "Table 7: Benchmark results for different compression methods at various compression rates, all using the same sequence length in the VLM decoder. We set Ninputk=4,8subscript\ud835\udc41input\ud835\udc5848\\frac{N_{\\text{input}}}{k}=4,8divide start_ARG italic_N start_POSTSUBSCRIPT input end_POSTSUBSCRIPT end_ARG start_ARG italic_k end_ARG = 4 , 8, with the baseline representing video models that process 4 frames without compression. Note that each compression method is re-implemented on the GLM-4V-9B backbone to ensure a fair comparison.", "description": "This table presents a comparison of different video compression methods' performance on the MotionBench benchmark.  The experiment controls for the total number of frames processed by the VLM decoder (Ldecoder) by adjusting the compression ratio (k) and number of input frames (Ninput).  The table shows accuracy results across various metrics for four different compression methods (QFormer, Qwen2-VL, PLLAVA, Kangaroo) and the proposed Through-Encoder Fusion (TE Fusion) method. The baseline represents a model without temporal compression processing only 4 frames. Results are shown for different compression ratios (k=2, 4, 8) and numbers of input frames (Ninput=4, 8).  The goal is to evaluate how different compression techniques affect model accuracy while maintaining a consistent sequence length within the VLM decoder.", "section": "5.2. Experiments on Video Feature Compression"}, {"content": "| Configurations           |\n|------------------------|\n| **Total steps**         | 10,000                  |\n| **Warmup steps**        | 1,000                   |\n| **Global batch size**   | 768                     |\n| **Learning rate**       | 8e-6                    |\n| **Minimal learning rate** | 1e-6                    |\n| **Learning rate decay** | cosine                  |\n| **Optimizer**           | Adam                     |\n| Adam \u03f5                   | 1e-8                    |\n| Adam \u03b21                  | 0.9                     |\n| Adam \u03b22                  | 0.95                    |\n| **Precision**          | bf16                    |", "caption": "Table 8: Model performance variation with respect to different compression ratios k=2,4,8,16\ud835\udc5824816k=2,4,8,16italic_k = 2 , 4 , 8 , 16, given a fixed VLM input frame count of Ninput=16subscript\ud835\udc41input16N_{\\text{input}}=16italic_N start_POSTSUBSCRIPT input end_POSTSUBSCRIPT = 16. Note that each compression method is re-implemented on the GLM-4V-9B backbone to ensure a fair comparison.", "description": "Table 8 presents the results of an experiment evaluating the performance of various video compression techniques on video language models (VLMs). The experiment uses a fixed number of input frames (Ninput = 16) for each VLM and varies the compression ratio (k = 2, 4, 8, 16). The table shows how different compression methods, including pre-encoder fusion, post-encoder fusion, through-encoder fusion (TE Fusion), and a baseline without temporal fusion, affect the performance of the model on the MotionBench, MVBench, LVBench, and VideoMME datasets.  Each compression method was implemented using the GLM-4V-9B backbone for a fair comparison.  The table allows for assessment of how different compression strategies affect model performance across different datasets and compression ratios.", "section": "5.2. Experiments on Video Feature Compression"}]