[{"figure_path": "https://arxiv.org/html/2501.02955/x1.png", "caption": "Figure 1: State-of-the-art video understanding models struggle with basic motion-level perception. Compared to existing benchmarks, our proposed MotionBench focuses on assessing the model\u2019s Motion level perception capability, which is critical in understanding videos with fast and instant interactions and motions.", "description": "The figure illustrates that state-of-the-art video understanding models struggle with basic motion-level perception.  It compares the performance of several models (LVBench, VideoMME, MVBench, and the proposed MotionBench) on motion-level perception tasks. MotionBench outperforms other benchmarks because it specifically focuses on evaluating the fine-grained motion understanding capability, crucial for comprehending videos with rapid and immediate interactions and movements.", "section": "Motion-level perception"}, {"figure_path": "https://arxiv.org/html/2501.02955/x2.png", "caption": "Figure 2: \nWe propose MotionBench, a collection of manually curated multi-choice queries with video clips featuring dynamic changes from various scenes such as daily life and medical instructions. We devise six primary tasks to evaluate the capability of motion-level perception. Unlike previous story-level and event-level benchmarks, MotionBench is characterized by a significantly higher annotation density, allowing for the assessment of fine-grained motions.", "description": "MotionBench is a new benchmark dataset for evaluating the fine-grained motion understanding capabilities of vision-language models (VLMs).  It consists of 8052 multiple-choice questions paired with short video clips showing diverse dynamic actions from various settings, including daily life and medical scenarios. The questions are categorized into six primary tasks designed to test different aspects of motion-level perception (motion recognition, location-related motion, action order, repetition count, motion-related objects, and camera motion).  MotionBench's key feature is its significantly higher annotation density compared to existing video understanding benchmarks, which allows for a more precise evaluation of fine-grained movements.", "section": "3. MotionBench: Motion-Level Benchmarking"}, {"figure_path": "https://arxiv.org/html/2501.02955/x3.png", "caption": "(a) Option distribution", "description": "This figure shows the distribution of option selections across all questions in the MotionBench dataset.  It helps to illustrate the balance (or imbalance) of the multiple choice answers provided for each question in the benchmark, revealing whether the answers are evenly distributed or if some options are favored more than others. This is a key aspect of evaluating the quality and fairness of a multiple choice question benchmark. ", "section": "3. MotionBench: Motion-Level Benchmarking"}, {"figure_path": "https://arxiv.org/html/2501.02955/x4.png", "caption": "(b) Video duration", "description": "This histogram shows the distribution of video durations within the MotionBench dataset.  The x-axis represents the duration of videos in seconds, and the y-axis represents the frequency or count of videos with that duration. The distribution shows the range of video lengths included in the MotionBench benchmark, indicating the dataset's diversity in terms of video length.", "section": "3. MotionBench: Motion-Level Benchmarking"}, {"figure_path": "https://arxiv.org/html/2501.02955/x5.png", "caption": "(c) Annotation length", "description": "This histogram displays the distribution of annotation lengths (number of words) for the questions within the MotionBench dataset.  It shows the frequency of different annotation lengths, giving insight into the level of detail present in the questions and the complexity of the descriptions needed to adequately capture fine-grained motion.", "section": "3. MotionBench: Motion-Level Benchmarking"}, {"figure_path": "https://arxiv.org/html/2501.02955/x6.png", "caption": "(d) QA per video", "description": "This figure shows the distribution of the number of questions per video in the MotionBench dataset.  It reveals the variability in the number of questions asked about each video, indicating a range of complexity and detail in the annotations.", "section": "3. MotionBench: Motion-Level Benchmarking"}, {"figure_path": "https://arxiv.org/html/2501.02955/extracted/6113584/figs/video_dynamic_annotation_ver2.jpg", "caption": "Figure 3: Basic statistics of MotionBench.", "description": "Figure 3 presents a detailed statistical overview of the MotionBench dataset.  The subfigures illustrate the distribution of options in multiple-choice questions (a), video durations (b), the number of questions per video (d), and the lengths of annotations (c).  These visualizations provide insights into the dataset's composition and characteristics, aiding in a comprehensive understanding of its scale, complexity, and diversity of video content.", "section": "3. MotionBench: Motion-Level Benchmarking"}, {"figure_path": "https://arxiv.org/html/2501.02955/x7.png", "caption": "Figure 4: Example of dynamic information annotation", "description": "This figure showcases an example of how dynamic information is annotated in the MotionBench dataset.  It includes a short video clip description highlighting changes that occur over time. This caption is then followed by a multiple-choice question related to the video, demonstrating the level of detail and specificity in the MotionBench annotations. The purpose is to illustrate the richness of the annotations and highlight the focus on fine-grained motion-level understanding, differentiating it from existing benchmarks that often rely solely on the initial frame or static descriptions.", "section": "3. MotionBench: Motion-Level Benchmarking"}, {"figure_path": "https://arxiv.org/html/2501.02955/x8.png", "caption": "Figure 5: Summarization of prevalent paradigms for video compression and our proposed Through-Encoder Fusion (TE Fusion). Here we only illustrate the part before the VLM decoder where temporal compression performs.", "description": "Figure 5 illustrates four different approaches to video compression before feeding the data into a Vision Language Model (VLM) decoder.  The first method ('Without Temporal Fusion') processes each frame individually. The second ('Pre-Encoder Fusion') merges frames *before* visual encoding. The third ('Post-Encoder Fusion') encodes frames separately and then combines their features. Finally, the authors' proposed method ('Through-Encoder Fusion') integrates the fusion of frames *within* the visual encoder, resulting in a more efficient and effective compression strategy. The figure focuses specifically on the steps preceding the VLM decoder to highlight the differences in how temporal compression is handled.", "section": "4. Model Design: Motion-Level Perception"}, {"figure_path": "https://arxiv.org/html/2501.02955/x9.png", "caption": "Figure 6: Model performance variation with respect to different compression ratios k=2,4,8,16\ud835\udc5824816k=2,4,8,16italic_k = 2 , 4 , 8 , 16, given a fixed VLM input frame count of Ninput=16subscript\ud835\udc41input16N_{\\text{input}}=16italic_N start_POSTSUBSCRIPT input end_POSTSUBSCRIPT = 16. The pink dotted line represents the performance of the baseline model, which processes 16 frames without temporal compression. Note that each compression method is re-implemented on the GLM-4V-9B backbone to ensure a fair comparison.", "description": "Figure 6 presents a comparative analysis of various video compression techniques' impact on model performance.  The experiment uses a fixed number of input frames (Ninput = 16) to the Vision Language Model (VLM), while varying the compression ratio (k = 2, 4, 8, 16). The results are shown for four different video compression methods:  QFormer, PLLaVA, Kangaroo, and the authors' proposed Through-Encoder Fusion (TE Fusion). A baseline representing a model processing 16 frames without compression is also included (pink dotted line). Each compression method was re-implemented using the same GLM-4V-9B backbone for a fair comparison.  The figure displays the accuracy achieved by each method on four benchmarks: MotionBench, MVBench, LVBench, and VideoMME across different compression ratios, illustrating how well each technique balances compression with accuracy in video understanding.", "section": "5.2. Experiments on Video Feature Compression"}]