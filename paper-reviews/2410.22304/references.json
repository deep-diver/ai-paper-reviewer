{"references": [{" publication_date": "2021", "fullname_first_author": "Cobbe, K.", "paper_title": "Training verifiers to solve math word problems", "reason": "This paper introduces the GSM8K dataset, which is a crucial component of the MetaMath dataset used for training and evaluation in the target paper. The GSM8K dataset's focus on mathematical reasoning makes it highly relevant to the research presented in the target paper.", "section_number": 1}, {" publication_date": "2021", "fullname_first_author": "Hendrycks, D.", "paper_title": "Measuring mathematical problem solving with the math dataset", "reason": "This paper introduces the MATH dataset, another crucial component of the MetaMath dataset used in the target paper.  The MATH dataset's focus on mathematical problem-solving capabilities is directly relevant to the research and evaluation.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Lightman, H.", "paper_title": "Let's verify step by step", "reason": "This paper explores the concept of process reward, which is closely related to the DPO method used in the target paper. The focus on step-by-step verification for improving reasoning traces is directly relevant to the goals of the target paper.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Yu, L.", "paper_title": "Metamath: Bootstrap your own mathematical questions for large language models", "reason": "This paper introduces the MetaMath dataset, a key dataset used for training and evaluation in the target paper.  MetaMath combines GSM8K and MATH, offering a comprehensive benchmark for mathematical reasoning in LLMs.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Zelikman, E.", "paper_title": "Star: Self-taught reasoner", "reason": "This paper presents a self-training method for LLMs that is related to the self-improvement aspect of the target paper's approach.  The focus on self-teaching and improving model performance through self-generated data is relevant to the methodology.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Zelikman, E.", "paper_title": "Quiet-star: Language models can teach themselves to think before speaking", "reason": "This paper explores a self-teaching technique for LLMs, which is conceptually relevant to the self-improvement approach of the target paper. The concept of models learning to generate better reasoning traces aligns well with the target paper's focus.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Yuan, X.", "paper_title": "V-star: Training verifiers for self-taught reasoners", "reason": "This paper presents a method for training verifiers for LLMs that is indirectly relevant to the target paper. The concept of using verifiers to improve the quality of generated reasoning traces aligns with the goal of the target paper.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Pang, R.", "paper_title": "Iterative reasoning preference optimization", "reason": "This paper introduces a preference optimization technique that is directly relevant to the DPO method used in the target paper. The focus on preference learning is highly relevant to the online learning approach.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Rafailov, R.", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "reason": "This paper introduces Direct Preference Optimization (DPO), the core learning method used in the target paper. Understanding DPO is essential for understanding the training methodology of the target paper.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Singh, A.", "paper_title": "Beyond human data: Scaling self-training for problem-solving with language models", "reason": "This paper discusses scaling self-training methods for LLMs, which is conceptually relevant to the target paper's approach of using self-generated data for improvement. The focus on scalability is also pertinent to the target paper's goals.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Mineiro, P.", "paper_title": "Online joint fine-tuning of multi-agent flows", "reason": "This paper is highly relevant because it provides background information on the use of multi-agent flows in online learning, which is the core approach of the target paper. The principles and techniques discussed are essential to understanding the target paper's methodology.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Zhang, D.", "paper_title": "Llama-berry: Pairwise optimization for o1-like olympiad-level mathematical reasoning", "reason": "This paper focuses on improving LLM performance in mathematical reasoning, which is directly relevant to the target paper's goals.  The use of pairwise optimization is also relevant to the DPO method used.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Zhang, D.", "paper_title": "Rest-mcts*: Llm self-training via process reward guided tree search", "reason": "This paper explores self-training methods for LLMs using process reward, which is closely related to the DPO method used in the target paper. The focus on process reward is particularly relevant to the goals.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Zhang, R.", "paper_title": "Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems?", "reason": "This paper focuses on evaluating LLMs' ability to solve mathematical problems, which is directly relevant to the target paper's goals. The focus on visual aspects is indirectly related, highlighting the broad scope of the research area.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Gao, B.", "paper_title": "Omni-math: A universal olympiad level mathematic benchmark for large language models", "reason": "This paper introduces a new benchmark dataset for evaluating LLMs' mathematical reasoning capabilities. While not directly used in the target paper, its existence and focus on high-level mathematical problems is relevant to the broader context of the research.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Liu, H.", "paper_title": "Mathbench: Evaluating the theory and application proficiency of llms with a hierarchical mathematics benchmark", "reason": "This paper introduces a new benchmark dataset for evaluating LLMs' mathematical reasoning capabilities, similar to the previous entry. The focus on hierarchical mathematical problems is particularly relevant to the target paper's focus on complex reasoning tasks.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Lu, P.", "paper_title": "Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts", "reason": "This paper focuses on evaluating LLMs' mathematical reasoning capabilities in visual contexts. Although the target paper does not directly focus on visual aspects, the broader context of mathematical reasoning makes it relevant.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Wang, P.", "paper_title": "Math-shepherd: Verify and reinforce llms step-by-step without human annotations", "reason": "This paper is highly relevant due to its focus on step-by-step verification of LLMs' reasoning processes without human intervention. This is a significant challenge addressed by the target paper using a novel approach.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Wang, Z.", "paper_title": "Multi-step problem solving through a verifier: An empirical analysis on model-induced process supervision", "reason": "This paper uses a verifier to improve LLM performance in multi-step problem solving, which is conceptually similar to the approach used in the target paper.  The use of a verifier to enhance reasoning accuracy is a key point of similarity.", "section_number": 3}]}