{"references": [{" publication_date": "2021", "fullname_first_author": "Cobbe et al.", "paper_title": "Training verifiers to solve math word problems", "reason": "This paper introduces the MATH dataset, a benchmark for evaluating mathematical reasoning in large language models.  The dataset's focus on mathematical problem solving is directly relevant to the paper's focus on improving LLM mathematical reasoning, making it a crucial foundational reference.  The dataset's development is also closely related to the development of other mathematical reasoning datasets, showing its significance in the field.", "section_number": 1}, {" publication_date": "2021", "fullname_first_author": "Hendrycks et al.", "paper_title": "Measuring mathematical problem solving with the math dataset", "reason": "This paper is fundamental because it introduces the MATH dataset, which is a benchmark for evaluating large language models' mathematical problem-solving capabilities.  The paper's focus on measuring mathematical problem-solving abilities is highly relevant to the current paper's objective of improving LLMs' performance in mathematical reasoning tasks, making it a core reference.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Yu et al.", "paper_title": "Metamath: Bootstrap your own mathematical questions for large language models", "reason": "This paper presents MetaMath, a dataset for evaluating mathematical reasoning in LLMs, significantly impacting the research community working on mathematical reasoning in LLMs and providing a relevant benchmark for the current paper to assess performance.  The dataset's creation and characteristics are directly relevant to the current paper's methodology.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Lu et al.", "paper_title": "Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts", "reason": "This paper is highly relevant due to its evaluation of LLMs' mathematical reasoning capabilities within visual contexts, broadening the scope of the research on LLM mathematical reasoning.  By considering the visual aspect, this paper provides a more holistic view of the challenges and opportunities, which indirectly informs the current work.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Gao et al.", "paper_title": "Omni-math: A universal olympiad level mathematic benchmark for large language models", "reason": "This paper introduces a new benchmark dataset, Omni-Math, for evaluating the capabilities of LLMs in solving mathematical problems, especially complex ones.  The significance lies in providing a challenging benchmark for the current paper to measure and compare the quality of reasoning traces generated by the proposed method against.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Liu et al.", "paper_title": "Mathbench: Evaluating the theory and application proficiency of LLMs with a hierarchical mathematics benchmark", "reason": "This paper introduces a new benchmark dataset, Mathbench, focusing on evaluating LLMs' proficiency in both mathematical theory and application. Its relevance lies in providing a comprehensive evaluation framework that is comparable to the current research, covering both theoretical and application aspects.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Lai et al.", "paper_title": "Step-DPO: Step-wise preference optimization for long-chain reasoning of LLMs", "reason": "This paper proposes Step-DPO, a step-wise preference optimization method for improving long-chain reasoning in LLMs, which addresses similar challenges to the current paper's focus on generating detailed reasoning traces. This provides a related method for comparison and contextualization.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Lightman et al.", "paper_title": "Let's verify step by step", "reason": "This work highlights the significance of step-by-step verification in enhancing the quality of reasoning traces.  This is directly relevant to the current paper's approach, which emphasizes the generation of high-quality reasoning traces through incremental steps. The comparison provides context and highlights a similar approach.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Wang et al.", "paper_title": "Math-Shepherd: Verify and reinforce LLMs step-by-step without human annotations", "reason": "This paper focuses on verifying and reinforcing LLM reasoning step-by-step, which is highly related to the current paper's goal of improving reasoning trace quality. This paper provides a comparison of similar methods to enhance the quality of reasoning generation.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Wang et al.", "paper_title": "Multi-step problem solving through a verifier: An empirical analysis on model-induced process supervision", "reason": "This paper explores multi-step problem solving through a verifier, which is closely aligned with the current paper's approach of generating detailed reasoning traces. By providing an empirical analysis on model-induced process supervision, this paper offers valuable insights for comparative analysis.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Zelikman et al.", "paper_title": "Star: Self-taught reasoner", "reason": "This paper introduces a self-taught reasoning method for LLMs, which is a pioneering approach for improving reasoning capabilities without extensive human supervision. The direct relevance lies in its focus on self-improvement and the utilization of generated traces, which the current paper also leverages.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Zelikman et al.", "paper_title": "Quiet-star: Language models can teach themselves to think before speaking", "reason": "This paper is relevant because it explores the idea of language models teaching themselves to think before speaking, a concept related to self-improvement and the generation of high-quality reasoning traces. This provides a relevant conceptual comparison and context for the current paper's focus on self-improvement in mathematical reasoning.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Zhang et al.", "paper_title": "Llama-berry: Pairwise optimization for o1-like olympiad-level mathematical reasoning", "reason": "This paper is important because it introduces Llama-berry, a pairwise optimization method specifically designed for improving mathematical reasoning capabilities in LLMs.  This makes it highly relevant as a comparative method focusing on similar challenges.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Zhang et al.", "paper_title": "Rest-MCTS*: LLM self-training via process reward guided tree search", "reason": "This paper is relevant because it explores self-training in LLMs using a process reward guided tree search, which addresses the challenge of generating high-quality reasoning traces efficiently.  The use of a tree search mechanism, similar to other methods, provides context and a different approach for comparison.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Zhang et al.", "paper_title": "Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems?", "reason": "This paper expands the scope of LLM mathematical reasoning to include multi-modal aspects, particularly visual diagrams, which relates to the broader context of the current paper.  It highlights the challenges of interpreting complex information in mathematical reasoning tasks, informing the current work's scope.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Singh et al.", "paper_title": "Beyond human data: Scaling self-training for problem-solving with language models", "reason": "This paper emphasizes the scaling of self-training for problem-solving in language models, which is directly relevant to the current paper's approach of leveraging self-generated reasoning traces for improving model performance.  The focus on scaling provides a relevant perspective on the broader context and implications.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Rafailov et al.", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "reason": "This paper is highly relevant due to its introduction of Direct Preference Optimization (DPO), a key method used in the current paper.  The explanation and theoretical foundation of DPO are essential for understanding the core methodology of the proposed approach.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Pang et al.", "paper_title": "Iterative reasoning preference optimization", "reason": "This paper presents Iterative Reasoning Preference Optimization (IRPO), which addresses the challenge of improving reasoning in LLMs through iterative optimization.  Its relevance lies in providing a related method for comparison and contextualization of the proposed DPO-based approach.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Hosseini et al.", "paper_title": "V-star: Training verifiers for self-taught reasoners", "reason": "This paper is significant because it explores the concept of training verifiers for self-taught reasoners, a related approach to the current paper's focus on self-improvement in LLMs' mathematical reasoning.  It offers a comparative perspective on methods for enhancing the quality of reasoning traces generated by models.", "section_number": 1}]}