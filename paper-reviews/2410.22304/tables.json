[{"figure_path": "2410.22304/tables/table_4_0.html", "caption": "Table 1: Main results of comparing the quality of traces used for SFT. We report the accuracy (%) for each model fine-tuned on an identical set of prompts, but with varying answer sources. For Phi-3, we does not include GSM8K due to its already optimized performance on the dataset.", "description": "Table 1 compares the accuracy of different LLMs fine-tuned using different reasoning traces (ground-truth, self-generated, and Flow-generated) on GSM8K and MATH datasets.", "section": "3.3 Compile"}, {"figure_path": "2410.22304/tables/table_9_0.html", "caption": "Table 2: Online DPO Fine-tuning hyperparameters.", "description": "This table presents the hyperparameters used during the online direct preference optimization (DPO) fine-tuning process.", "section": "3.3 Compile"}, {"figure_path": "2410.22304/tables/table_9_1.html", "caption": "Table 3: Comiple (SFT) hyperparameters.", "description": "Table 3 presents the hyperparameters used during the Compile (SFT) step of the proposed model.", "section": "3.3 Compile"}]