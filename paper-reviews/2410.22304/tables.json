[{"figure_path": "2410.22304/tables/table_4_0.html", "caption": "Table 1: Main results of comparing the quality of traces used for SFT. We report the accuracy (%) for each model fine-tuned on an identical set of prompts, but with varying answer sources. For Phi-3, we does not include GSM8K due to its already optimized performance on the dataset.", "description": "Table 1 presents a comparison of the accuracy achieved by different fine-tuning methods (ground-truth, self-generated, and Flow-generated traces) on the GSM8K and MATH datasets for Llama-3-Instruct (8B) and Phi-3-Medium (14B) language models.", "section": "3.3 Compile"}, {"figure_path": "2410.22304/tables/table_9_0.html", "caption": "Table 2: Online DPO Fine-tuning hyperparameters.", "description": "This table shows the hyperparameters used for online direct preference optimization (DPO) fine-tuning in the Flow-DPO model.", "section": "3.3 Compile"}]