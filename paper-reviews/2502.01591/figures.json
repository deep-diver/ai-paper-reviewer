[{"figure_path": "https://arxiv.org/html/2502.01591/x6.png", "caption": "Figure 1: \n[Left]\nReward on Craftax-classic.\nOur best MBRL and MFRL agents outperform all the previously published MFRL and MBRL results, and for the first time, surpass the reward achieved by a human expert.\nWe display published methods which report the reward at 1M steps with horizontal line from 900k to 1M steps.\n[Middle] The Craftax-classic observation is a 63\u00d763636363\\times 6363 \u00d7 63 pixel image, composed of 9\u00d79999\\times 99 \u00d7 9 patches of 7\u00d77777\\times 77 \u00d7 7 pixels.\nThe observation shows the map around the agent and the agent\u2019s health and inventory. Here we have rendered the image at 144\u00d7144144144144\\times 144144 \u00d7 144 pixels for visibility.\n[Right] 64646464 different patches.", "description": "Figure 1 presents a comprehensive overview of the experimental results and the environment used in the study. The left panel shows a reward curve for various model-based reinforcement learning (MBRL) and model-free reinforcement learning (MFRL) algorithms on the Craftax-classic benchmark. The plot demonstrates that the proposed MBRL and MFRL agents surpass previously published results, achieving a reward exceeding human-level performance for the first time. The middle panel provides a visual representation of the Craftax-classic observation space. The agent's observation consists of a 63x63 pixel image partitioned into 9x9 patches of 7x7 pixels, representing the agent's vicinity and also including information about the agent's inventory and health. The provided image has been upscaled to 144x144 pixels for clarity. The right panel displays 64 distinct image patches illustrating the variety of visual elements within the observation space. This figure thus effectively summarizes the key aspects of the environment, the performance of the proposed agents, and the data used for training.", "section": "Results"}, {"figure_path": "https://arxiv.org/html/2502.01591/x12.png", "caption": "Figure 2: \nApproaches for TWM training with L=2\ud835\udc3f2L=2italic_L = 2, T=2\ud835\udc472T=2italic_T = 2.\nqt\u2113superscriptsubscript\ud835\udc5e\ud835\udc61\u2113q_{t}^{\\ell}italic_q start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_\u2113 end_POSTSUPERSCRIPT denotes token \u2113\u2113\\ellroman_\u2113 of timestep t\ud835\udc61titalic_t. Tokens in the same timestep have the same color.\nWe exclude action tokens for simplicity.\n[Left] Usual autoregressive model training with teacher forcing.\n[Right] Block teacher forcing predicts token qt+1\u2113superscriptsubscript\ud835\udc5e\ud835\udc611\u2113q_{t+1}^{\\ell}italic_q start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_\u2113 end_POSTSUPERSCRIPT from input token qt\u2113superscriptsubscript\ud835\udc5e\ud835\udc61\u2113q_{t}^{\\ell}italic_q start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_\u2113 end_POSTSUPERSCRIPT with block causal attention.", "description": "Figure 2 illustrates two different approaches to training a Transformer World Model (TWM).  The TWM takes a sequence of image tokens as input and predicts future tokens. The figure focuses on a simplified scenario with only two timesteps (T=2) and two tokens per timestep (L=2). Each token represents a patch of the input image.  The left panel shows the standard autoregressive approach with teacher forcing, where the model predicts tokens sequentially, using previously predicted tokens as context. The right panel presents the block teacher forcing approach. In this method, all tokens for the next timestep are predicted simultaneously (in a block) using only the past timesteps' tokens as context. The use of block causal attention allows the model to consider the relationships between all tokens in the next timestep before making any predictions. This is in contrast to the autoregressive approach which predicts tokens one at a time and only uses the already generated tokens from that timestep. The color-coding in the figure visually highlights the tokens belonging to the same timestep.", "section": "3.6. Block teacher forcing"}, {"figure_path": "https://arxiv.org/html/2502.01591/x13.png", "caption": "Figure 3: \nThe ladder of improvements presented in Section 3 progressively transforms our baseline MBRL agent into a state-of-the-art method on Craftax-classic. Training in imagination\nstarts at step 200k, indicated\nby the dotted vertical line.", "description": "This figure shows the performance of different model-based reinforcement learning (MBRL) agents on the Craftax-classic environment.  It demonstrates a series of improvements to a baseline MBRL agent, showing how each improvement cumulatively leads to better performance. Each line represents a different version of the agent, with the final model achieving state-of-the-art results. The dotted vertical line highlights the point (200k environment steps) at which the model starts training using imagined trajectories generated by a world model, rather than relying solely on real experience.  This illustrates the contribution of the model-based component to improved sample efficiency.", "section": "4. Results"}, {"figure_path": "https://arxiv.org/html/2502.01591/x14.png", "caption": "Figure 4: Ablations results on Craftax-classic after 1M environment interactions.", "description": "This ablation study investigates the impact of various components of the proposed model-based reinforcement learning (MBRL) approach on the Craftax-classic benchmark.  It measures the reward achieved after 1 million environment interactions, showing the effect of removing key elements such as Dyna (background planning), the nearest-neighbor tokenizer (NNT), and block teacher forcing (BTF). The experiment also examines the sensitivity of the NNT to patch sizes and the importance of quantization for the 7x7 patches. The results highlight the contributions of each component to the overall performance and demonstrate the necessity of these improvements for achieving state-of-the-art results.", "section": "4. Results"}, {"figure_path": "https://arxiv.org/html/2502.01591/x17.png", "caption": "Figure 5: [Left] MBRL performance decreases when NNT uses patches of smaller or larger size than the ground truth, but it remains competitive. However, performance collapses if the patches are not quantized.\n[Middle] Removing any rung of the ladder of improvements leads to a drop in performance.\n[Right] Warming up the world model before using it to train the policy on imaginary rollouts is required for good performance. BP denotes background planning. For each method, training in imagination\nstarts at the color-coded vertical line, and leads to an initial drop in performance.", "description": "Figure 5 presents an ablation study to analyze the impact of different components of the proposed MBRL approach on its performance. The left panel shows that while using patches of different sizes with the nearest-neighbor tokenizer (NNT) maintains competitive performance, not quantizing the patches causes a significant drop in performance. The middle panel demonstrates that each improvement in the 'ladder of improvements' significantly contributes to the overall performance, highlighting the cumulative effect of each component. The right panel illustrates the importance of warming up the world model before starting background planning, preventing the model from negatively affecting the learning of the policy.", "section": "4. Results"}]