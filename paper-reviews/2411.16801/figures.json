[{"figure_path": "https://arxiv.org/html/2411.16801/x2.png", "caption": "Figure 1: Generated images by BootComp. (a) BootComp\u00a0generates high-quality human images wearing multiple reference garments, with support for extended categories such as bag, shoes, even in unusual garment combinations (e.g., swimming suit with soccer cleats). We show BootComp\u2019s\u00a0generalization capability through various conditional image generations, such as (b) virtual try-on, (c) pose guided generation, (d) stylization, and (e) text guided generation, even though BootComp\u00a0is not directly trained or fine-tuned for each task.", "description": "Figure 1 showcases BootComp's ability to generate high-quality images of humans wearing multiple garments.  Panel (a) demonstrates this core functionality, highlighting the model's capacity to handle a wide range of clothing items, including accessories like bags and shoes, and even unusual combinations like a swimsuit and cleats. The figure further illustrates BootComp's versatility by demonstrating its ability to generate images conditioned on different factors. This is shown in panels (b) through (e), which depict virtual try-ons, pose-controlled generation, stylization, and text-guided generation, respectively.  Crucially, the model achieves these varied capabilities without any direct training or fine-tuning for these specific tasks, highlighting its strong generalization ability.", "section": "Abstract"}, {"figure_path": "https://arxiv.org/html/2411.16801/x3.png", "caption": "Figure 2: Limitations of previous data curation approaches used in controllable generation.\nPrevious approaches on controllable generation often use a paired dataset consisting of low-quality segmented garments and human images for training. It leads to several undesirable artifacts as shown in right (generated with baselines).\nFor example, garments are directly replicated from the reference images in (a), shirts and skirts are blended together in (b), and generated skirts fail to resemble the reference in (c).", "description": "Figure 2 illustrates the shortcomings of prior methods for creating datasets to train controllable human image generation models. These methods typically use pairs of low-quality segmented garment images and human images.  The figure shows three examples of the problems this causes. In (a), the generated garment is a direct copy of the reference image, lacking any variation or integration with the human. In (b), parts of different garments (shirt and skirt) are blended together in the generated image, resulting in a nonsensical output. In (c), the generated skirt significantly differs from the reference, indicating a failure to learn accurate garment representation.  These issues highlight the need for improved data curation techniques to achieve higher-quality controllable image generation.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2411.16801/x4.png", "caption": "Figure 3: Overview of BootComp.\nWe propose a two-stage framework: synthetic data generation and composition module training for controllable human image generation.\n(a) We train a decomposition network that maps from a segmented garment image to a product garment image. (b) We bootstrap synthetic paired data of human and multiple garment images. (c) We finally train our composition module with the synthetic paired dataset enabling it to generate human images with multiple reference garment images.", "description": "BootComp is a two-stage framework for controllable human image generation.  The first stage involves creating a synthetic dataset. A decomposition network is trained to transform segmented garment images from human images into product-view garment images. This allows bootstrapping a large dataset of human images with multiple garments. The second stage trains a composition module using this synthetic dataset. This module generates human images conditioned on multiple reference garment images, resulting in controllable image generation.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2411.16801/x6.png", "caption": "Figure 4: Extended self-attention architecture. In a extended self-attention layer,  reference hidden states are concatenated with the  target hidden states in the key and value matrices. This architecture enables injecting reference image features within the target image. Note that decomposition module also uses same structure but works within a single network.", "description": "This figure illustrates the extended self-attention mechanism used in the BootComp model.  The architecture enhances the standard self-attention layer by concatenating reference hidden states (from the garment images) with the target hidden states (from the human image) within the key and value matrices. This crucial modification allows the model to effectively integrate the garment features into the generation of the human image, ensuring that the generated human image accurately reflects the clothing being worn. Notably, this same architectural design is employed within the decomposition module, although confined to a single network.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2411.16801/x7.png", "caption": "Figure 5: Examples of high&low-quality generated garments. When human parsing results are not precise, the decomposition network struggles to generate product garment images accurately, resulting in low-quality garment images. We filter out these cases.", "description": "Figure 5 illustrates the impact of inaccurate human parsing on the garment generation process.  The figure shows examples of generated garments categorized as either high-quality or low-quality.  Low-quality results occur when the initial segmentation of the human image (to isolate individual garments) is imprecise.  This imprecision makes it difficult for the decomposition network to accurately generate realistic product-view images of the garments. To maintain a high level of quality in the dataset, these low-quality garment generation attempts are filtered out before being used to train the main image generation model.", "section": "3.1 Training data generation"}, {"figure_path": "https://arxiv.org/html/2411.16801/x8.png", "caption": "Figure 6: Qualitative comparison of human image generation with multiple garments. BootComp\u00a0generates realistic human images with multiple reference garments even with non-straightforward combinations of garments without losing details of each reference. For example, Parts2Whole replaces reference soccer cleats with stilettos, while ours accurately generates each reference (left, middle row).", "description": "Figure 6 presents a qualitative comparison of human image generation models, focusing on their ability to handle multiple clothing items.  The figure shows that the model BootComp successfully generates realistic images of people wearing multiple garments, even in unusual combinations.  BootComp preserves the details of each garment, whereas other methods, such as Parts2Whole, might replace some items with visually dissimilar ones (e.g., replacing soccer cleats with stilettos). This highlights BootComp's superior performance in accurately generating images with multiple clothing items.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2411.16801/x9.png", "caption": "Figure 7: More applications of BootComp. We showcase the extensive applications of our method, BootComp. BootComp\u00a0creates human images by controlling the (a) poses and (b) styles of the generated human images. BootComp\u00a0also enables (c) personalized human image generation by taking user\u2019s images as conditions (e.g., face, full body).", "description": "Figure 7 demonstrates the versatility of the BootComp model for generating human images under various conditions.  Subfigure (a) shows the model's ability to generate images with controlled poses, demonstrating its capability to faithfully render clothing details and human anatomy across a range of postures. Subfigure (b) illustrates the method's effectiveness in generating human images in different styles, such as cartoonish renderings, showcasing its adaptability to diverse aesthetic preferences. Finally, subfigure (c) highlights the model's power for personalized image generation, which allows users to input their own images (e.g., facial features, full body shots) to create custom-tailored results. This demonstrates BootComp's capacity for both creative control and personalized applications.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2411.16801/x10.png", "caption": "Figure 8: Visualization of segmented paired data and our synthetic paired data.\nWe provide a visual comparison between segmented and synthetic pairs. Given a single garment and a human image pair, we segment out other garments from the human image in the segmented paired data.", "description": "Figure 8 visually compares two approaches for creating paired datasets for training a clothing generation model. The 'segmented paired data' method involves manually segmenting out individual garments from an image of a person wearing multiple garments. This approach is time-consuming and prone to inaccuracies.  In contrast, the 'synthetic paired data' method leverages the authors' proposed decomposition network to generate images of individual garments from a single garment-human image pair. This method offers a more efficient and scalable way to create a larger, higher-quality dataset.", "section": "4.3 Analysis and ablation studies"}, {"figure_path": "https://arxiv.org/html/2411.16801/x11.png", "caption": "Figure 9: Visual comparison on data construction methods.\nVisual comparison between generated human images where each model is trained on segmented and synthetic pairs. The model trained on segmented pair data struggles to generate naturally harmonized human images (red).", "description": "Figure 9 presents a comparison of human image generation results using two different training datasets: segmented paired data and synthetic paired data.  The images demonstrate that models trained on segmented data, where garments are extracted directly from existing images, struggle to produce realistic and harmoniously composed images of people wearing multiple garments. The generated human images often appear unnatural or have artifacts.  In contrast, images from the model trained on the synthetic data show significantly improved results, producing more natural and realistic human image generation.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2411.16801/x12.png", "caption": "Figure 10: Examples of training data for decomposition module. We collect pairs of a human image and a single reference garment image from public datasets including VITON-HD, DressCode, and LAION-Fashion. It consists of various garments in different categories, e.g., shirts, pants, shoes and bags etc.", "description": "Figure 10 shows example images from the dataset used to train the decomposition module of the BootComp model.  The dataset consists of paired images: a human wearing clothing, and a separate, single product shot of one of the garments the person is wearing.  The images are sourced from publicly available datasets such as VITON-HD, DressCode, and LAION-Fashion. The figure showcases the variety of garments included in the dataset, illustrating the diversity of clothing categories such as shirts, pants, shoes, and bags.", "section": "A. Implementation Details"}, {"figure_path": "https://arxiv.org/html/2411.16801/x13.png", "caption": "Figure 11: Examples of pairs filtered out by different similarity metrics. We present examples of generated garment images and their corresponding human images that were excluded based on various image similarity metrics. Using LPIPS, garments with complicated patterns are filtered out, and using CLIP score, inner layer garments are filtered out even when they are considered identical in human perception. In contrast, DreamSim captures the distance between images in a way aligned with human perception, filtering out undesirable pairs.", "description": "Figure 11 demonstrates the effectiveness of the DreamSim metric for filtering generated garment-human image pairs.  The figure showcases examples where different similarity metrics (LPIPS and CLIP) fail to accurately assess the similarity between generated garments and their real-world counterparts in human images. LPIPS struggles with garments containing intricate patterns, while CLIP misjudges the similarity of inner-layer garments, even when perceptually identical to a human observer. In contrast, DreamSim, aligned with human visual perception, successfully identifies and filters out low-quality pairs, ensuring only high-quality synthetic data are used for training.", "section": "Synthetic Data Generation"}, {"figure_path": "https://arxiv.org/html/2411.16801/x14.png", "caption": "Figure 12: Examples of generated garment images with different image distance values. We provide examples of generated garment images and corresponding human images, varying the distance values measured by DreamSim. With the distance value d\u22650.4\ud835\udc510.4d\\geq 0.4italic_d \u2265 0.4, generated garments are inconsistent with the actual garment, while for d<0.4\ud835\udc510.4d<0.4italic_d < 0.4, the generated garments closely resemble the actual garment.", "description": "This figure visualizes the results of a filtering process applied to synthetically generated garment images.  The goal was to ensure the quality of the generated garments by comparing them to the actual garments worn in the original images.  The comparison metric was DreamSim, which measures perceptual similarity between images.  The image pairs shown are categorized by their DreamSim distance score (d): those with d \u2265 0.4 show inconsistencies between the generated and real garments, while those with d < 0.4 indicate a close resemblance, signifying successful generation. The figure thus provides a visual demonstration of how effective the filtering step was in ensuring data quality.", "section": "3.1 Training data generation"}, {"figure_path": "https://arxiv.org/html/2411.16801/x15.png", "caption": "Figure 13: Examples of our synthetic paired data. We visualize our synthetic pairs of a human image and multiple garment images. Our decomposition module generates high-quality garment images in product view on different categories including shirts, pants, shoes and bags.", "description": "Figure 13 presents examples from the synthetic dataset created by the BootComp model's decomposition module.  The figure showcases multiple examples of paired images:  a human image and corresponding product views of the garments worn.  The generated garment images are of high quality and represent several garment categories including shirts, pants, shoes, and bags. This demonstrates the module's ability to produce realistic garment images from a single image of a person wearing the garments.", "section": "3.1 Training data generation"}, {"figure_path": "https://arxiv.org/html/2411.16801/x16.png", "caption": "Figure 14: Examples of synthetic paired data generated by the decomposition module trained on MVImgNet\u00a0[51]. We show the potential extension of our decomposition module to the general domain. Given an image containing common objects such as cups, chairs, and broccoli, the decomposition module generates each object in a different view, constructing paired data. Reference images are obtained from COCO\u00a0[26].", "description": "This figure demonstrates the versatility of the decomposition module by applying it beyond the fashion domain.  Using the MVImgNet dataset [51], which contains images of various objects in multiple views, the model is tasked with taking an image containing several everyday objects (e.g., cups, chairs, broccoli) and generating a product view for each individual object. This process effectively creates synthetic paired data, where each pair consists of a single object image and its corresponding product-view image. The reference images used in this experiment were sourced from the COCO dataset [26].  The results showcase the potential of the decomposition module for broader applications in general-purpose image generation and data augmentation.", "section": "C.1. Synthetic Paired Data on General Domain"}, {"figure_path": "https://arxiv.org/html/2411.16801/x17.png", "caption": "Figure 15: Examples of generated subjects in multi-view by the decomposition module trained on MVImgNet. The decomposition module can serve as a multi-view generator for single-subject images. Subject images are from DreamBooth\u00a0[40].", "description": "Figure 15 showcases the versatility of the decomposition module, a key component of the BootComp framework, when trained on the MVImgNet dataset.  It demonstrates that this module, initially designed for extracting garment images, can function as a robust multi-view image generator for single subjects. The input subject images used in this demonstration were generated using the DreamBooth method, highlighting the potential for seamless integration with other image generation techniques. The figure displays several examples of single-subject images processed by the decomposition module to create various viewpoints of the same subject, illustrating its ability to generate multiple views from a single image while preserving the subject's identity and key features.", "section": "C.2. Multi-view Image Generator"}, {"figure_path": "https://arxiv.org/html/2411.16801/x18.png", "caption": "Figure 16: Limitations of BootComp. BootComp\u00a0struggles on naturally dressing hats and preserving tiny details like letters.", "description": "Figure 16 shows examples where BootComp, despite its strengths, faces limitations.  The model struggles with the realistic placement of hats on human subjects, often resulting in unnatural or inaccurate positioning.  Furthermore, fine details such as small text on clothing are not consistently preserved in the generated images. These limitations highlight areas where the model could benefit from further improvements or additional training data.", "section": "Limitations"}, {"figure_path": "https://arxiv.org/html/2411.16801/x19.png", "caption": "Figure 17: More qualitative comparisons. BootComp\u00a0generates realistic human images wearing multiple reference garments, faithfully preserving fine-details of each garment, while baselines often generate inconsistent garment images and blend reference garments.", "description": "Figure 17 presents a qualitative comparison of human image generation results between BootComp and two baseline methods (MIP-Adapter and Parts2Whole).  It showcases the superior performance of BootComp in generating realistic human images wearing multiple garments.  BootComp faithfully preserves the fine details of each garment, while the baseline methods frequently produce images with inconsistencies in garment appearance and often blend garments together unrealistically. This demonstrates BootComp's ability to handle complex combinations of clothing items accurately and with high fidelity.", "section": "4.2. Results"}]