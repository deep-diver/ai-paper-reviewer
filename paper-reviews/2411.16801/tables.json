[{"content": "| Method | MP-LPIPS \u2193 | DINO \u2191 | M-DINO \u2191 | FID \u2193 |\n|---|---|---|---|---|\n| MIP-Adapter [15] | 0.276 | 0.308 | 0.025 | 59.99 |\n| Parts2Whole [16] | 0.267 | 0.362 | 0.036 | 28.39 |\n| **BootComp (ours)** | **0.187** | **0.379** | **0.046** | **27.63** |", "caption": "Table 1: Quantitative comparisons. We compare BootComp\u00a0with baselines on garment similarity and image fidelity. We see that BootComp\u00a0outperforms other methods, preserving fine-details of garments and naturally generating human images.", "description": "This table presents a quantitative comparison of BootComp against other state-of-the-art methods for controllable human image generation.  The comparison focuses on two key aspects: garment similarity (how accurately the generated garments match the reference garments) and image fidelity (how realistic and natural the generated human images appear).  Four metrics are used for evaluation: MP-LPIPS (a perceptual metric focusing on the differences between generated and reference images), DINO and M-DINO (semantic similarity scores reflecting the alignment between generated and reference garments in the image), and FID (Fr\u00e9chet Inception Distance, measuring the overall quality and realism of the generated images). The results show that BootComp significantly outperforms the other methods in all four metrics, demonstrating its superior ability to generate high-fidelity human images with precisely rendered garments.", "section": "4.2. Results"}, {"content": "| Dataset | MP-LPIPS \u2193 | DINO \u2191 | M-DINO \u2191 | FID \u2193 |\n|---|---|---|---|---|\n| Segmented | 0.374 | 0.284 | 0.025 | 59.27 |\n| **Synthetic** | **0.197** | **0.365** | **0.043** | **29.41** |", "caption": "Table 2: Comparison on dataset construction methods. The model trained on the segmented paired dataset shows worse performance compared to one trained on our synthetic paired dataset both in  garment similarity and  image fidelity.", "description": "This table compares the performance of a model trained on a synthetic dataset generated using the proposed decomposition method with that of a model trained on a segmented dataset.  The segmented dataset involved manually extracting garment images from existing human images. The table uses four metrics (MP-LPIPS, DINO, M-DINO, FID) to evaluate garment similarity and overall image fidelity. Results show that the model trained on the synthetic data produced significantly better results than the one trained on the segmented data, demonstrating the effectiveness of the synthetic data generation pipeline in improving model quality.", "section": "4.3. Analysis and ablation studies"}, {"content": "| Dataset size | DINO \u2191 | M-DINO \u2191 | FID \u2193 |\n|---|---|---|---|\n| 5K | 0.337 | 0.248 | 34.15 |\n| 15K | 0.338 | 0.251 | 32.32 |\n| 30K | 0.344 | 0.261 | 26.99 |\n| 50K | **0.360** | **0.285** | **25.88** |", "caption": "Table 3: Comparison on dataset scale. Training with a larger datatset (after filtered) improves the model\u2019s overall performance in both  garment similarity and  image fidelity.", "description": "This table presents the results of an experiment assessing the impact of dataset size on the performance of the BootComp model.  Specifically, it shows how increasing the size of the training dataset (after filtering out low-quality data) affects the model's ability to accurately generate images with similar garment appearances and overall human image fidelity.  Larger datasets generally lead to improved performance on both garment similarity and image quality metrics, suggesting a positive correlation between training data volume and the model's accuracy and detail preservation.", "section": "4.3. Analysis and ablation studies"}, {"content": "| \u03c4 | 0.4 | 0.5 | 0.6 | 0.7 | 1.0 |\n|---|---|---|---|---|---| \n| DINO\u2191 | **0.360** | 0.347 | 0.343 | 0.342 | 0.338 |", "caption": "Table 4: Ablation study for threshold value \u03c4\ud835\udf0f\\tauitalic_\u03c4 on filtering. The data quality improves with a stricter threshold value, leading to better performance. We adopt \u03c4=0.4\ud835\udf0f0.4\\tau=0.4italic_\u03c4 = 0.4 when applying the filtering.", "description": "This ablation study investigates the impact of different threshold values (\u03c4) used in a data filtering process on the performance of a model.  The filtering step removes low-quality synthetic data generated during training. The table shows how varying \u03c4 affects the model's performance, measured by the DINO metric. A stricter threshold (lower \u03c4) indicates a more stringent filtering process, leading to a smaller dataset. The study found that stricter filtering, while reducing the dataset size, improves the model's performance as indicated by higher DINO scores.  The optimal threshold value selected was \u03c4 = 0.4.", "section": "4.3. Analysis and ablation studies"}]