[{"figure_path": "https://arxiv.org/html/2412.04440/x3.png", "caption": "Figure 1: \nThe first row illustrates our multi-agent collaboration approach, showcasing the collaborative workflow, task decomposition in the Redesign stage, and adaptive self-routing for correction agents.\nThe second row presents videos generated by GenMAC based on complex compositional prompts involving multiple objects, attribute binding, quantity, and dynamic motion binding.", "description": "Figure 1 demonstrates GenMAC's multi-agent collaborative text-to-video generation process. The top row illustrates the workflow's three stages: Design, Generation, and Redesign.  The Redesign stage is broken down into four sub-tasks handled by specialized agents (verification, suggestion, correction, and output structuring), each responsible for a specific aspect of refining the generated video.  The adaptive self-routing mechanism ensures that the appropriate agent is chosen for the task. The bottom row displays example videos produced by GenMAC showcasing its ability to handle complex prompts involving multiple objects, their attributes, quantities, and dynamic interactions.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2412.04440/x4.png", "caption": "Figure 2: \nFramework of GenMAC. Collaborative workflow includes three stages with an iterative loop: Design, Generation, and Redesign (Section\u00a03.1). Task decomposition decomposes the redesign stage into four sub-tasks, handled by four agents: verification agent, suggestion agent, correction agent, and output structuring agent (Section\u00a03.2). Self-routing mechanism allows for adaptive selection of suitable correction agent to address the diverse requirements for compositional text-to-video generation (Section\u00a03.3).", "description": "GenMAC's framework consists of three stages: Design, Generation, and Redesign.  The Design stage uses an LLM to create a structured layout based on the text prompt. The Generation stage uses a text-to-video model to create a video based on this layout. The Redesign stage is iterative and crucial; it verifies if the generated video matches the prompt, and if not, it uses four specialized agents (verification, suggestion, correction, and output structuring agents) to iteratively refine the layout, prompt, and other parameters for subsequent generation attempts. A self-routing mechanism intelligently selects the most suitable correction agent depending on the specific issues detected.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2412.04440/x5.png", "caption": "Figure 3: \nIllustration of Task Decomposition for the Redesign stage (Section\u00a03.2). The diagram illustrates the allocation of roles: verification agent, suggestion agent, correction agent, and output structuring agent within a sequential task breakdown, highlighting the clear responsibilities of each agent.", "description": "The figure illustrates the workflow of the REDESIGN stage in the GENMAC framework.  The REDESIGN stage is broken down into four sequential sub-tasks, each handled by a specialized agent: a verification agent (checks for misalignments between the generated video and the text prompt), a suggestion agent (suggests corrections based on the verification results), a correction agent (makes the corrections based on the suggestions, choosing an appropriate agent based on the scenario), and an output structuring agent (formats the suggestions for the next iteration of the generation stage). The diagram clearly shows the flow of information and responsibilities between these four agents.", "section": "3.2 Task Decomposition for Redesign Stage"}, {"figure_path": "https://arxiv.org/html/2412.04440/x6.png", "caption": "Figure 4: \nQualitative Comparison. Our proposed\u00a0GenMAC\u00a0generates videos that accurately adhere to complex compositional scenarios, demonstrating a clear advantage in handling such requirements in comparision with\nSOTA text-to-video models.", "description": "Figure 4 presents a qualitative comparison of video generation results between the proposed GenMAC model and state-of-the-art (SOTA) text-to-video generation models.  The figure showcases several example video prompts requiring complex compositional elements such as multiple objects with specific attributes, spatial relationships, and dynamic actions.  For each prompt, GenMAC's generated video is compared alongside the outputs from other SOTA models, highlighting GenMAC's superior ability to accurately capture and represent the intricate details and relationships specified in the complex prompts. This demonstrates GenMAC's clear advantage in handling compositional text-to-video generation tasks compared to existing models.", "section": "4. Qualitative Comparisons"}, {"figure_path": "https://arxiv.org/html/2412.04440/x7.png", "caption": "Figure 5: \nQualitative Results. Our proposed\u00a0GenMAC\u00a0generates videos that highly aligned with complex compositional prompts, including attribute binding, multiple objects, quantity, and dynamic motion binding.", "description": "Figure 5 showcases example videos generated by GenMAC, highlighting its ability to handle complex and detailed text prompts.  These examples demonstrate GenMAC's capacity to accurately represent various compositional elements within a video, including the correct attributes of multiple objects, the specified number of objects, and dynamic movements as described in the text prompt. This figure visually supports the claims made in the paper regarding GenMAC's superior compositional capabilities in text-to-video generation.", "section": "Qualitative Results"}, {"figure_path": "https://arxiv.org/html/2412.04440/x8.png", "caption": "Figure 6: Visualization of the iterative refinement process in our multi-agent framework, demonstrating iterations enhance scene accuracy by progressively aligning video content with compositional prompts.", "description": "This figure visualizes how GENMAC iteratively refines video generation through multi-agent collaboration.  It shows an example where the initial generated video doesn't fully capture the prompt's instructions.  The subsequent iterations, facilitated by the REDESIGN stage agents, progressively improve the video by correcting misalignments and introducing missing elements.  This iterative process demonstrates how GENMAC aligns the video content with complex compositional aspects outlined in the text prompt, enhancing scene accuracy step-by-step.", "section": "3. Overall Collaborative Workflow"}, {"figure_path": "https://arxiv.org/html/2412.04440/x9.png", "caption": "Figure 7: Cumulative Corrected Ratio. For each subset in T2V-CompBench, we calculate the ratio of prompts that have completed the refinement and exited the GenMAC loop to the total size of the subset in each iteration. Dynamic attribute binding remains challenging, while generative numeracy, spatial relationships, and motion binding show substantial improvements from iteration 1 to 9.", "description": "This figure displays the cumulative corrected ratio for each subset within the T2V-CompBench benchmark across nine iterations of the GenMAC model.  The cumulative corrected ratio represents the proportion of prompts successfully refined and completed within the GenMAC loop relative to the total number of prompts in each subset. The graph reveals that while \"dynamic attribute binding\" consistently proves challenging, notable improvements are observed across multiple iterations for \"generative numeracy,\" \"spatial relationships,\" and \"motion binding.\"", "section": "4. Analysis on Iterative Generation"}, {"figure_path": "https://arxiv.org/html/2412.04440/x10.png", "caption": "(a) Visualization of multi-agent collaboration. Initial generation lacks \u201ctug\u201d motion between the rope and boat; Redesign agents adjust spatial alignment and visual tension, leading to a final video that aligns with the prompt\u2019s interaction requirements.", "description": "This figure visualizes the iterative refinement process in the GENMAC framework using a multi-agent approach.  The top row depicts the workflow. The initial video generation (left) fails to capture the 'tug' motion between the rope and boat specified in the text prompt. The Redesign stage (middle) uses multiple specialized agents to detect the issue and suggest corrections, such as refining bounding boxes and visual cues to better represent the interaction. The final generated video (right) successfully incorporates the intended 'tug' motion, aligning the output with the compositional prompt requirements. The bottom row shows frames from the video at different stages of refinement.", "section": "3. Overall Collaborative Workflow"}, {"figure_path": "https://arxiv.org/html/2412.04440/x11.png", "caption": "(b) Visualization of the iterative refinement in correcting object quantity and motion direction. The Redesign agents adjust guidance scale and alignment over successive iterations, progressively enhancing adherence to the prompt.", "description": "This figure shows how the iterative refinement process in the REDESIGN stage corrects errors in object quantity and motion direction. The REDESIGN agents use multiple iterations to adjust the guidance scales and alignment of objects, improving the generated video's adherence to the text prompt. The figure visualizes this process and highlights the collaborative and iterative nature of the approach.", "section": "3. Overall Collaborative Workflow"}, {"figure_path": "https://arxiv.org/html/2412.04440/x12.png", "caption": "Figure A8: Visualization of the multi-agent collaboration.", "description": "This figure visualizes the multi-agent collaboration process in GENMAC through two examples.  (a) shows how the agents iteratively refine a video of a rope pulling a boat, correcting initial misalignments in the interaction between the rope and boat. The agents iteratively refine the video through adjusting the bounding boxes to reflect the tension between the objects, achieving a final version that accurately reflects the prompt. (b) illustrates the iterative correction of object quantity and motion direction in a video of a car on the moon. The agents detect the mismatch, and through successive iterations of adjustment (bounding box position and guidance scale) refine the video to match the direction and quantity specified in the prompt.", "section": "B. Additional Experimental Results"}, {"figure_path": "https://arxiv.org/html/2412.04440/x13.png", "caption": "(a) The number of corrections.", "description": "The bar chart visualizes the number of corrections needed in each iteration of the GENMAC's iterative refinement loop for different compositional aspects.  It shows how many times adjustments were required to the generated video across various aspects of composition like attribute consistency, dynamic attributes, spatial relationships, etc., across multiple iterations.", "section": "4.5. Ablation Study"}, {"figure_path": "https://arxiv.org/html/2412.04440/x14.png", "caption": "(b) \nThe contribution (%) of different guidance types to the video scores with Design and Redesign stages.", "description": "This figure shows a breakdown of how different types of guidance (structured layout, guidance scale, and new text prompt) contribute to the overall video quality scores.  The analysis is performed after both the Design and Redesign stages of the generation process. It illustrates the relative importance of each guidance type in achieving the final video quality, showing their respective percentage contributions.", "section": "4.5. Ablation Study"}, {"figure_path": "https://arxiv.org/html/2412.04440/x15.png", "caption": "(c)  The contribution (%) of different guidance types to the video scores with only the Redesign stage.", "description": "This figure shows a pie chart that breaks down the contributions of different types of guidance (structured layout, guidance scale, and new text prompt) to the overall quality of generated videos, specifically focusing on the results achieved during the REDESIGN stage of the GENMAC process.  It quantifies the influence each guidance type had on the video scores while only using the REDESIGN stage; excluding the DESIGN and GENERATION stages.", "section": "Analysis of Various Guidance Settings"}, {"figure_path": "https://arxiv.org/html/2412.04440/x16.png", "caption": "Figure A9: Illustration of the number of corrections and contributions (%) in T2V-CompBench of different guidance types: structured layout, guidance scale, and new text prompt.", "description": "Figure A9 shows the number of corrections made during each iteration of the GENMAC model's refinement process, broken down by the type of guidance used (structured layout, guidance scale, new text prompt).  It also displays the percentage contribution of each guidance type to the overall video quality scores. This allows for analysis of which guidance method is most effective and how their impact changes across the iterative refinement process.", "section": "B. Additional Experimental Results"}, {"figure_path": "https://arxiv.org/html/2412.04440/x17.png", "caption": "Figure A10: \nMore qualitative comparisons.", "description": "This figure provides a qualitative comparison of video frames generated by different models (GENMAC, VideoCrafter2, CogVideoX, Gen-3, VideoTetris, Open-Sora-Plan) for three complex compositional prompts. Each row represents a different prompt and shows the outputs of each model for comparison. This illustrates the ability of the proposed method GENMAC to generate videos that accurately match the complex composition and dynamics specified in the prompts, outperforming other existing methods.", "section": "B.2 Qualitative Comparisons"}]