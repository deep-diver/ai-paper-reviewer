[{"content": "| Model | Layers | Hidden size | Heads | #Param |\n|---|---|---|---|---|\n| DiT-S | 12 | 384 | 6 | 22.5M |\n| DiT-B | 12 | 768 | 12 | 89.5M |\n| DiT-L | 24 | 1024 | 16 | 310.0M |\n| DiT-XL | 28 | 1152 | 16 | 456.0M |", "caption": "Table 1: Details of DiT model sizes.", "description": "This table presents the specifications of four different sizes of Diffusion Transformer (DiT) models used in the experiments.  It shows the number of layers, hidden size, number of attention heads, and the total number of parameters for each model size (DiT-S, DiT-B, DiT-L, DiT-XL). These details are crucial for understanding the computational cost and capacity differences between the models.", "section": "3.1 Fundamental Physical Scenarios"}, {"content": "| Model | #Templates | FVD (\u2193) | SSIM (\u2191) | PSNR (\u2191) | LPIPS (\u2193) | Abnormal (\u2193) |\n|---|---|---|---|---|---|---|\n| DiT-XL | 6 | 18.2 / 22.1 | **0.973** / 0.943 | **32.8** / 25.5 | **0.028** / 0.082 | 3% / 67% |\n| DiT-XL | 30 | 19.5 / 19.7 | 0.973 / 0.950 | 32.7 / 27.1 | 0.028 / 0.065 | 3% / 18% |\n| DiT-XL | 60 | **17.6** / **18.7** | 0.972 / **0.951** | 32.4 / **27.3** | 0.030 / **0.062** | **2%** / **10%** |\n| DiT-B | 60 | 18.4 / 21.4 | 0.967 / 0.949 | 30.9 / 27.0 | 0.035 / 0.066 | 3% / 24% |", "caption": "Table 2: Combinatorial generalization results.\nThe results are presented in the format of\n\n{in-template result} / {out-of-template result}.", "description": "This table presents the results of evaluating combinatorial generalization in video generation models.  It shows the performance of models on both in-distribution (in-template) and out-of-distribution (out-of-template) generalization tasks.  The metrics used to evaluate the model's performance are Frechet Video Distance (FVD), Structural Similarity Index (SSIM), Peak Signal-to-Noise Ratio (PSNR), Learned Perceptual Image Patch Similarity (LPIPS), and the percentage of generated videos deemed \"abnormal\" by human evaluators.  The results are presented in a format showing in-template scores followed by a slash and then out-of-template scores for easier comparison.", "section": "4 Combinatorial Generalization"}, {"content": "| Scenario | Ground Truth Error | VAE Reconstruction Error |\n|---|---|---|\n| Uniform Motion | 0.0099 | 0.0105 |\n| Collision | 0.0117 | 0.0131 |\n| Parabola | 0.0210 | 0.0212 |", "caption": "Table 3: Comparison of errors for ground truth videos and VAE reconstruction videos.", "description": "This table presents a quantitative comparison of reconstruction errors between the ground truth videos and those reconstructed using a Variational Autoencoder (VAE).  The goal is to demonstrate the VAE's accuracy in encoding and decoding videos of physical events.  The lower the reconstruction error (compared to the ground truth error), the better the VAE's performance in capturing and reproducing the key information in the videos.", "section": "2.2 VIDEO GENERATION MODEL"}]