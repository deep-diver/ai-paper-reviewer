[{"figure_path": "https://arxiv.org/html/2411.02385/x1.png", "caption": "Figure 1: Categorization of generalization patterns.\u25cb\u25cb\\ocircle\u25cb denotes training data. \u00d7\\times\u00d7 denotes testing data.", "description": "This figure categorizes generalization patterns in machine learning models by illustrating the relationship between training and testing data.  The symbols \u25cb represent training data points, while \u00d7 symbols represent test data points.  Different panels show in-distribution (ID) generalization where training and testing data come from the same distribution, out-of-distribution (OOD) generalization where testing data comes from a different distribution, and combinatorial generalization where testing data involves novel combinations of concepts observed during training. This visualization aids in understanding how well a model generalizes beyond its training data and in different contexts.", "section": "3 IN-DISTRIBUTION AND OUT-OF-DISTRIBUTION GENERALIZATION"}, {"figure_path": "https://arxiv.org/html/2411.02385/x2.png", "caption": "Figure 2: Downsampled video visualization. The arrow indicates the progression of time.", "description": "This figure visualizes downsampled videos from a 2D physics simulation used in the paper.  Three distinct scenarios are shown, each demonstrating a different fundamental physical law: 1) Uniform Linear Motion (a ball moving at a constant velocity), 2) Perfectly Elastic Collision (two balls colliding), and 3) Parabolic Motion (a ball following a parabolic trajectory due to gravity).  The arrow in each video segment indicates the progression of time, showing the evolution of the physical system.  The figure is a simplified representation to facilitate quantitative evaluation of video generation models' ability to learn and extrapolate physical laws.", "section": "Fundamental Physical Scenarios"}, {"figure_path": "https://arxiv.org/html/2411.02385/x3.png", "caption": "Figure 3: \nThe error in the velocity of balls between the ground truth state in the simulator and the values parsed from the generated video by the diffusion model, given the first 3 frames.", "description": "This figure displays the velocity error for three different physical scenarios (Uniform Motion, Collision, Parabola) across various model and dataset sizes.  The velocity error represents the difference between the actual velocity of the balls calculated from the simulator's ground truth and the velocity estimated from the video generated by the diffusion model.  The first three frames of each video serve as input to the model.  The results show how the model's velocity prediction accuracy changes with the scale of both the model and training data.", "section": "3.2 MAIN RESULT OF SCALING DATA AND MODEL"}, {"figure_path": "https://arxiv.org/html/2411.02385/x4.png", "caption": "Figure 4: Downsampled videos. The black objects are fixed and others are dynamic.", "description": "This figure visualizes example videos from a 2D physics simulation used in the paper. Each video shows multiple objects with various shapes and colors interacting under the influence of gravity and collisions. Black objects represent fixed elements in the environment, while other objects (red ball and others) are dynamic and move according to the laws of physics. The videos serve to demonstrate the complexity of the physical interactions that the model must learn from and make predictions about.", "section": "4 Combinatorial Generalization"}, {"figure_path": "https://arxiv.org/html/2411.02385/x5.png", "caption": "Figure 5: \nUniform motion video generation. Models are trained on datasets with a missing middle velocity range.\nFor example, in the first figure, training velocities cover [1.0,1.25]1.01.25[1.0,1.25][ 1.0 , 1.25 ] and [3.75,4.0]3.754.0[3.75,4.0][ 3.75 , 4.0 ], excluding the middle range.\nWhen evaluated with velocity condition from the missing range [1.25,3.75]1.253.75[1.25,3.75][ 1.25 , 3.75 ], the generated velocity tends to shift away from the initial condition, breaking the Law of Inertia.", "description": "This figure demonstrates the limitations of video generation models when extrapolating beyond their training data.  The experiment focuses on uniform linear motion of a ball, a simple physical phenomenon governed by Newton's First Law of Motion (Inertia).  Multiple models are trained on datasets where a range of velocities are intentionally omitted (the 'missing middle velocity range'). When the model is then tested with velocities within this missing range, it fails to correctly predict the constant velocity, instead generating videos where the velocity deviates significantly from the expected constant value, violating the Law of Inertia. This demonstrates a 'case-based' generalization approach rather than true understanding of the physical law. The model appears to 'mimic' the closest training example rather than extrapolate based on a learned principle.", "section": "5.1 UNDERSTANDING GENERALIZATION FROM INTERPOLATION AND EXTRAPOLATION"}, {"figure_path": "https://arxiv.org/html/2411.02385/x6.png", "caption": "Figure 6: \nCollision video generation. Models are trained on the yellow region and evaluated on data points in both the yellow (ID) and red (OOD) regions. When the OOD range is surrounded by the training region, the OOD generalization error remains relatively small and comparable to the ID error.", "description": "This figure visualizes the results of collision video generation experiments.  The models were trained using data within the yellow region. Then, they were evaluated on data points both inside the yellow region (in-distribution, or ID) and the red region (out-of-distribution, or OOD).  The key finding highlighted is that when the OOD data points are surrounded by the training data, the generalization error for the OOD data remains low and similar to the error for the ID data. This suggests that the model's ability to generalize to unseen scenarios is related to the proximity of those scenarios to the training data.", "section": "3.2 MAIN RESULT OF SCALING DATA AND MODEL"}, {"figure_path": "https://arxiv.org/html/2411.02385/x7.png", "caption": "Figure 7: The example of uniform motion illustrating memorization.", "description": "This figure demonstrates the model's memorization behavior during generalization.  The model was trained on videos of uniform linear motion with velocities in the range of 2.5 to 4.0 units. It was trained on two datasets: one only containing objects moving in one direction, and another containing movements in both directions, achieved by horizontal flipping during training. During testing, the model was given low-speed objects (velocity 1.0 to 2.5). The results show that a model trained only on one direction generated videos with velocities biased toward the higher range and only in the trained direction.  In contrast, the model trained with both directions occasionally produced videos moving in the opposite direction, showcasing the model's tendency to 'memorize' training examples rather than learn the underlying physical law of uniform motion.", "section": "5.1 UNDERSTANDING GENERALIZATION FROM INTERPOLATION AND EXTRAPOLATION"}, {"figure_path": "https://arxiv.org/html/2411.02385/x8.png", "caption": "Figure 8: \nUniform motion. (1) Color v.s. shape, (2) Size v.s. shape, (3) Velocity v.s. shape.\nThe arrow \u21d2\u21d2\\Rightarrow\u21d2 signifies that the generated videos shift from their specified conditions to resemble similar training cases. For example, in the first figure, the model is trained on videos of blue balls and red squares. When conditioned with a blue ball, as shown in the bottom, it transforms into a blue square, i.e., mimicking the training case by color.", "description": "This figure demonstrates how a video generation model generalizes based on different attributes (color, size, and velocity) when dealing with shape.  It shows three experiments comparing pairs of these attributes.  In each experiment, the model is trained on videos featuring two distinct combinations of attributes. The model is then tested with videos that combine the attributes in novel ways. Arrows indicate that the generated videos tend to shift their visual properties from the testing data's initial conditions to more closely resemble similar training examples. For instance, in the first experiment comparing color and shape, when trained on red squares and blue balls and tested with a blue ball, the model changes the ball into a blue square.", "section": "5.3 How DOES DIFFUSION MODEL RETRIEVE DATA?"}, {"figure_path": "https://arxiv.org/html/2411.02385/x11.png", "caption": "Figure 9: \nUniform motion.\n(1) Velocity v.s. size: The arrow \u2192\u2192\\rightarrow\u2192 indicates the direction of generated videos shifting from their initial conditions.\n(2) Color v.s. size: Models are trained with small red balls and large blue balls, and evaluated on reversed color-size pair conditions. All generated videos retain the initial color but show slight size shifts from the original.\n(3) Color v.s. velocity: Models are trained with low-speed red balls and high-speed blue balls, and evaluated on reversed color-velocity pair conditions. All generated videos retain the initial color but show large velocity shifts from the original.", "description": "Figure 9 presents a detailed analysis of how a video generation model generalizes based on various attributes.  It explores three scenarios, each comparing two attributes: (1) Velocity vs. Size:  The model's predictions are shown when presented with initial conditions outside its training data.  The arrows indicate the direction of the generated video's velocity changing from the initial state. (2) Color vs. Size: The model is trained on videos featuring small red balls and large blue balls. Testing is performed on reversed conditions (large red balls and small blue balls). Results show that generated videos generally maintain the initial color but often exhibit size variations. (3) Color vs. Velocity:  Similar to (2), training uses low-speed red balls and high-speed blue balls, with testing on reversed conditions. Generated videos preserve the initial color but demonstrate significant discrepancies in velocity compared to the initial conditions. This figure helps explain how the model's generalization process favors specific attributes over others.", "section": "DEEPER ANALYSIS"}, {"figure_path": "https://arxiv.org/html/2411.02385/x12.png", "caption": "Figure 10: \nFirst row: Ground truth; second row: generated video.\nAmbiguities in visual representation result in inaccuracies in fine-grained physics modeling.", "description": "This figure demonstrates the limitations of relying solely on visual information for accurate physics modeling in video generation. The top row shows the ground truth frames of a video, while the bottom row displays the corresponding frames generated by a video generation model.  The subtle differences between the ground truth and generated video highlight a key problem: when fine-grained details, like the exact position of a ball relative to a gap, are visually ambiguous, the model produces plausible-looking but inaccurate results. This indicates that visual information alone may be insufficient for precise physical modeling, particularly in scenarios involving subtle spatial relationships.", "section": "5 IS VIDEO SUFFICIENT FOR COMPLETE PHYSICS MODELING?"}, {"figure_path": "https://arxiv.org/html/2411.02385/x13.png", "caption": "Figure 11: \nSpatial and temporal combinatorial generalization. The two subsets of the training set contain disjoint physical events. However, the trained model can combine these two types of events across spatial and temporal dimensions.", "description": "Figure 11 demonstrates the model's ability to generalize beyond simple scenarios by combining elements from different situations in both space and time. The training data is divided into two sets: one showing a blue square moving horizontally while a red ball remains stationary, and another showing a red ball bouncing off a wall while a blue square is stationary.  Importantly, these scenarios are distinct; the model was never shown both events happening simultaneously. However, when presented with a test scenario where both events occur (the blue square moves horizontally, and the red ball bounces), the model correctly predicts the combined outcome.  This shows the model is not simply memorizing training examples but can synthesize new behaviors by integrating disparate learned skills.", "section": "4 COMBINATORIAL GENERALIZATION"}, {"figure_path": "https://arxiv.org/html/2411.02385/x14.png", "caption": "Figure 12: Comparison of different modal conditions for video generation.", "description": "This figure displays a comparison of video generation results under different input conditions.  It shows velocity error as a function of training data size, contrasting results when the model is conditioned only on visual data, visual data plus numerical data, and visual data plus textual descriptions.  The goal is to assess whether incorporating additional information like numbers or text improves physical law learning and generalization to out-of-distribution (OOD) scenarios.", "section": "4 COMBINATORIAL GENERALIZATION"}, {"figure_path": "https://arxiv.org/html/2411.02385/x15.png", "caption": "Figure 13: \nUniform motion. Color vs. shape. The shapes are a ball and a ring. Transforming from a ring to a ball leads to a large pixel variation.", "description": "This figure demonstrates the effect of color and shape on a video generation model's ability to generalize to unseen scenarios.  The model is trained on videos showing red squares and blue balls moving uniformly. During testing, the model is conditioned on frames showing a blue ring. Because the model prioritizes color, it transforms the blue ring into a blue ball instead of preserving the shape of the ring.  This highlights the model's reliance on visual similarities rather than underlying physical laws in its generalization.  The caption emphasizes the large pixel variation involved in changing a ring into a ball, suggesting this is a factor contributing to the model's reliance on color in its decision-making process.", "section": "5.3 How DOES DIFFUSION MODEL RETRIEVE DATA?"}, {"figure_path": "https://arxiv.org/html/2411.02385/x16.png", "caption": "Figure 14: Failure cases in combinatorial generalization. Note that the bounce cases in the training set do not include the red ball.", "description": "Figure 14 illustrates instances where the model fails to generalize combinatorially. The model struggles to produce videos with the expected outcomes when presented with test scenarios that combine elements not seen together during training.  Specifically, the training data included scenarios with bouncing balls but excluded cases where a red ball bounced. Consequently, when a test scenario involving a red ball bounce was presented, the model failed to correctly predict the resulting video.  The failure highlights the model's reliance on memorizing specific training examples rather than learning generalizable rules about physics.", "section": "4 Combinatorial Generalization"}, {"figure_path": "https://arxiv.org/html/2411.02385/x17.png", "caption": "Figure 15: \nThe visualization of in-distribution evaluation cases with very small prediction errors.", "description": "Figure 15 visualizes several example video sequences generated by the model for in-distribution testing scenarios. Each example demonstrates successful prediction of object motion, indicating that the model accurately captures the underlying physical laws within its training data distribution. The videos showcase scenarios of uniform linear motion, perfectly elastic collisions, and parabolic motion, all of which are accurately predicted by the model. The close alignment between the generated videos and ground truth in these examples signifies strong in-distribution generalization capability. The model's accurate prediction of these simple physical phenomena is a crucial aspect of its overall physical law discovery ability. The precise matching between generated and ground truth videos in Figure 15 provides strong evidence of the model's capability to learn and apply physical laws within a constrained setting.", "section": "3.2 MAIN RESULT OF SCALING DATA AND MODEL"}, {"figure_path": "https://arxiv.org/html/2411.02385/x18.png", "caption": "Figure 16: \nThe visualization of out-of-distribution evaluation cases with large prediction errors.", "description": "This figure visualizes examples from the out-of-distribution (OOD) test set where the model's predictions significantly deviate from the ground truth. It showcases instances of uniform linear motion, collision, and parabolic motion where the model fails to accurately predict the velocity or trajectory of the objects, resulting in large prediction errors.  The visualization helps illustrate the model's limitations in generalizing to unseen scenarios outside the training distribution.", "section": "3.2 MAIN RESULT OF SCALING DATA AND MODEL"}, {"figure_path": "https://arxiv.org/html/2411.02385/x19.png", "caption": "Figure 17: \nThe visualization of out-of-template evaluation cases that appear plausible and adhere to physical laws, generated by DiT-XL trained on 6M data (60 templates). Zoom in for details. Notably, the first four cases generated by the model are nearly identical to the ground truth. In some cases, such as the rightmost example, the generated video seems physically plausible but differs from the ground truth due to visual ambiguity, as discussed in\u00a0Section\u00a05.5.", "description": "Figure 17 visualizes the results of out-of-template evaluation of a video generation model (DiT-XL). The model was trained on 6 million video samples representing 60 unique scenarios (templates).  The figure shows several video examples where the model generated videos which are visually very similar to the actual ground truth videos, thus appearing plausible and obeying physical laws.  However, while many of the generated videos are near-perfect matches, there are cases (like the rightmost example) where minor visual discrepancies exist between the generated video and the ground truth. These discrepancies, while visually subtle, indicate that the model hasn't perfectly captured and replicated the underlying physical process, highlighting the limitations of using visual information alone for learning physical laws (further elaborated in Section 5.5).", "section": "5 DEEPER ANALYSIS"}]