{"references": [{" publication_date": "2021", "fullname_first_author": "Alexey Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "reason": "This paper is highly influential in the field of computer vision, introducing the vision transformer (ViT) architecture, which significantly impacted the understanding and development of image processing methods using transformers. The authors' work on ViT directly relates to the current study's focus on understanding the properties of post-trained large-scale models because ViT's architecture is commonly used as the backbone for post-training tasks.  This makes this paper highly relevant to the current work's context and analysis.", "section_number": 2}, {" publication_date": "2019", "fullname_first_author": "Jacob Devlin", "paper_title": "BERT: pre-training of deep bidirectional transformers for language understanding", "reason": "This paper introduced BERT, a highly influential model in natural language processing (NLP). BERT's architecture and pre-training methodology are fundamental to the development of many large-scale language models, which form the basis for much of the research on post-training methods discussed in this paper.  The relevance comes from the fact that many of the post-trained models analyzed here are built upon or inspired by BERT-like architectures.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "Jesse Dodge", "paper_title": "Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping", "reason": "This paper explores effective fine-tuning strategies for pre-trained language models.  It is directly relevant to the current paper's research on post-training because fine-tuning is a prominent post-training technique. The insights into weight initialization, data ordering, and early stopping from this paper provide valuable context and background for understanding the broader implications of post-training. This is particularly relevant because the methods investigated in this paper are often used in conjunction with fine-tuning.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Abhimanyu Dubey", "paper_title": "The llama 3 herd of models", "reason": "This paper presents the LLaMA 3 family of language models. As LLaMA 3 models are used as examples in this work, the details of the model's architecture and training process are critical for understanding and validating experimental results. The paper provides detailed information about these models, enabling a more thorough analysis of the methods presented in the current study.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Albert Q.", "paper_title": "Mistral 7b", "reason": "This paper introduces the Mistral 7B language model, which is used in this research as an example. Since this paper details the model's design, training, and experimental setup, it helps us understand and interpret results obtained using this model in the current paper's experiments, validating the conclusions made about delta parameter editing techniques.", "section_number": 4}, {" publication_date": "2021", "fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "reason": "This paper details the training and characteristics of the ViT-B-32 model. As this is one of the models used for experiments in the current paper, understanding the specifics of this model is crucial for interpreting experimental results and verifying claims regarding delta parameter editing. The paper offers valuable background and context on ViT-B-32, enriching our understanding of the experimental setup.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Ganqu Cui", "paper_title": "Ultrafeedback: Boosting language models with scaled AI feedback", "reason": "This paper introduces a method for improving the alignment of language models. The concept of alignment is directly related to the current paper's investigation of delta parameter editing, as alignment tasks often benefit from techniques that refine or reduce the delta parameters.  This paper provides valuable insights into the connection between model alignment, delta parameters, and post-training improvements, enhancing the broader context of the current work.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Pala Tej Deep", "paper_title": "Della-merging: Reducing interference in model merging through magnitude-based sampling", "reason": "This work proposes DELLA-Merging, an approach to delta parameter editing used in the current study. The details of DELLA-Merging's methods are essential for understanding and comparing its effectiveness with other methods. This paper allows for a deeper comparison of the approaches and contributes to a broader analysis of the state-of-the-art in delta parameter editing techniques.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Zeyu Han", "paper_title": "Parameter-efficient fine-tuning for large models: A comprehensive survey", "reason": "This survey paper provides a comprehensive overview of parameter-efficient fine-tuning methods.  This is highly relevant to the current work because parameter-efficient fine-tuning is a key approach to post-training, which is the central theme of this paper. The survey paper offers a broader context for understanding the various approaches to parameter-efficient post-training, providing valuable background information for this paper's specific analysis.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Xuehai He", "paper_title": "Parameter-efficient model adaptation for vision transformers", "reason": "This paper discusses parameter-efficient methods for adapting vision transformers, relevant as vision transformers are used in this work. The paper offers methods for achieving high performance while being mindful of computational cost, directly relevant to the objectives of minimizing delta parameters.", "section_number": 2}, {" publication_date": "2019", "fullname_first_author": "Neil Houlsby", "paper_title": "Parameter-efficient transfer learning for NLP", "reason": "This paper focuses on parameter-efficient transfer learning in NLP, which is a closely related area to post-training.  Techniques for efficient adaptation and transfer learning are highly relevant to the methods discussed in this paper, especially those concerned with reducing or compressing delta parameters to improve efficiency and maintain performance.  This paper provides a valuable comparison for understanding the challenges and efficiency improvements related to parameter editing.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Edward J.", "paper_title": "Lora: Low-rank adaptation of large language models", "reason": "This paper introduces LoRA, a method for parameter-efficient fine-tuning of large language models.  LoRA is a significant advancement in the field of post-training, directly relevant to the work on editing delta parameters presented here.  The technique of low-rank adaptation provides insights into managing the complexity of delta parameters and improving the efficiency of post-training techniques which makes this paper a strong reference.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Gabriel Ilharco", "paper_title": "Editing models with task arithmetic", "reason": "This paper examines the process of model editing with task arithmetic. Understanding how model editing affects performance, especially through altering parameters, is relevant to the current paper's exploration of delta parameter editing techniques. The focus on parameter editing provides a direct comparison and insightful perspective to this research.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Albert Q.", "paper_title": "Mistral 7b", "reason": "The Mistral 7B model is used in this study as a test model. Therefore, understanding the design, training, and evaluation of the model provides critical context for the current paper's experiments. The specifics of this model, including its architecture, training methodology, and performance on benchmark tasks, are important for interpreting and contextualizing the results of the current study.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "James Liu", "paper_title": "Bitdelta: Your fine-tune may only be worth one bit", "reason": "This paper introduces BitDelta, a technique for quantizing delta parameters used in the current paper's analysis.  The methodology and results of BitDelta are crucial for comparing and evaluating the relative effectiveness of different delta parameter editing methods that may lead to reduced performance.  Understanding the workings of BitDelta is essential for conducting a comprehensive comparison within the broader context of delta parameter editing.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Zhenyi Lu", "paper_title": "Twin-merging: Dynamic integration of modular expertise in model merging", "reason": "Twin-Merging is another delta parameter editing method analyzed in the current research. This paper explains Twin-Merging's methodology and its effect on model performance. Understanding the nuances of this method is essential for proper comparison and analysis within the broader context of delta parameter editing.  The specifics of this technique are critical for interpreting the results and evaluating the effectiveness within this study's broader context.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Prateek Yadav", "paper_title": "TIES-merging: Resolving interference when merging models", "reason": "TIES-Merging is one of the methods examined in this research. Thus, a deep understanding of the method is crucial.  This paper offers a detailed explanation of the technique, allowing for a thorough evaluation of its effectiveness and a robust comparison with other delta parameter editing methods analyzed here.  The specifics of TIES-Merging are essential for interpreting the results and evaluating its place within this study's broader context.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Le Yu", "paper_title": "Language models are super mario: Absorbing abilities from homologous models as a free lunch", "reason": "This paper introduces DARE, a prominent delta parameter editing method that forms a basis for much of the current study.  Understanding the workings of DARE, including its advantages and limitations, is essential for a complete analysis of delta parameter editing techniques.  The methodology of DARE and the conclusions derived from its application provide a crucial foundation for evaluating and comparing other related methods in this paper.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Chujie Zheng", "paper_title": "Weak-to-strong extrapolation expedites alignment", "reason": "This paper presents EXPO, a technique for extrapolating delta parameters to improve model performance, especially in alignment tasks. EXPO is one of the primary methods analyzed in this work.  Understanding its methodology and results is critical for comparing its effectiveness and evaluating the overall impact of extrapolation strategies in the context of delta parameter editing.", "section_number": 6}]}