[{"figure_path": "https://arxiv.org/html/2503.14478/x2.png", "caption": "Figure 1: Our Motivation for Creation-MMBench. The triarchic theory of intelligence divides intelligence into three forms. Current MLLM benchmarks have significant gaps in evaluating visual-creative intelligence compared to the other forms. Additionally, existing benchmarks feature simple questions that fail to assess model performance in real-life creative tasks. Therefore, we proposed Creation-MMBench, which includes four categories, more creative and discriminative questions, and better evaluation of visual creative intelligence.", "description": "The figure illustrates the motivation behind the creation of Creation-MMBench.  It highlights the limitations of existing Multimodal Large Language Model (MLLM) benchmarks in assessing visual-creative intelligence. These benchmarks often lack comprehensive evaluation of visual creativity and rely on simple questions that do not reflect real-world creative tasks. In contrast, Creation-MMBench offers a more robust evaluation by incorporating four categories of tasks, more creative and discriminative questions, and a more thorough assessment of visual creative intelligence.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.14478/x3.png", "caption": "Figure 2: Brain regions related to creativity and their respective functions\u00a0[11, 6].", "description": "The figure illustrates the brain regions associated with creativity, highlighting the frontal lobe's crucial role in functions like concentration, planning, and problem-solving, which are essential components of the creative process.  It visually represents the neural networks involved in creative thinking.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.14478/x4.png", "caption": "Figure 3: Overview of Creation-MMBench. Contains four task categories, each category consists of multiple tasks, and the types of images are diverse. Only a few representative tasks of each category are shown here. Complete list of tasks is detailed in the Appendix A.", "description": "Creation-MMBench is a multimodal benchmark for evaluating the creative capabilities of large language models (LLMs), particularly in image-based tasks.  This figure provides a visual overview of the benchmark's structure and content. It shows the four main task categories within Creation-MMBench (Literary Writing, Common Functional Writing, Professional Functional Writing, and Creative Multimodal Understanding), each containing numerous sub-tasks.  A selection of representative example tasks within each category is visually displayed, along with sample images illustrating the diverse visual content used across all tasks. The caption points out that this figure shows only a subset of the total tasks, with a complete list available in Appendix A of the paper.", "section": "3. Creation-MMBench"}, {"figure_path": "https://arxiv.org/html/2503.14478/x5.png", "caption": "Figure 4: Evaluation Result of MLLMs w/o visual input.", "description": "The bar chart displays the performance of various Multimodal Large Language Models (MLLMs) on the Creation-MMBench benchmark when visual input is removed.  It compares the reward scores for each model, providing insights into their creative capabilities in a text-only setting.  Lower reward scores indicate weaker performance without visual context.", "section": "4. Experiment"}, {"figure_path": "https://arxiv.org/html/2503.14478/x6.png", "caption": "(a) Distribution of query lengths.", "description": "The figure shows a bar chart illustrating the distribution of query lengths in the Creation-MMBench dataset. The x-axis represents the range of query lengths, categorized into bins (e.g., <50, 50-500, 500-1000, >1000). The y-axis represents the frequency or proportion of queries falling within each length range. This visualization helps to understand the length distribution characteristics of questions used in Creation-MMBench for evaluating MLLMs' creative capabilities.", "section": "3. Creation-MMBench"}, {"figure_path": "https://arxiv.org/html/2503.14478/x7.png", "caption": "(b) Roles in Creation-MMBench.", "description": "The figure shows the diversity of roles incorporated in the Creation-MMBench benchmark's queries.  The variety of roles reflects real-world scenarios and aims to stimulate creative responses from MLLMs by requiring them to leverage diverse disciplinary knowledge and prior experience.", "section": "3. Creation-MMBench"}, {"figure_path": "https://arxiv.org/html/2503.14478/x8.png", "caption": "(c) Example Case of Creation-MMBench.", "description": "This figure shows an example case from Creation-MMBench. It showcases two tasks, one focused on design and the other on creative writing. The design task presents a floor plan and asks the model to analyze the plan and provide improvement suggestions considering a family of four. The creative writing task shows an image and asks the model to write a short story based on the image. The figure illustrates the benchmark's diverse tasks and its multimodal nature.", "section": "3 Creation-MMBench"}, {"figure_path": "https://arxiv.org/html/2503.14478/x9.png", "caption": "Figure 6: Statistics and Cases of Creation-MMBench. Compared to other widely used MLLM benchmarks, Creation-MMBench features a more comprehensive query design to capture abundant creative contexts. Diverse roles are introduced into the queries to stimulate MLLMs\u2019 utilization of disciplinary and prior knowledge. As an MLLM benchmark, Creation-MMBench includes a rich variety of images to thoroughly evaluate multiple capabilities of MLLMs.", "description": "Creation-MMBench is a new benchmark designed to evaluate the creative capabilities of multimodal large language models (MLLMs).  Figure 6 highlights key aspects that differentiate Creation-MMBench from other existing benchmarks.  First, the query design is more comprehensive, encompassing rich contexts to fully test creative abilities.  Second, diverse roles are included in the prompts to encourage MLLMs to draw upon their existing knowledge and expertise in various fields.  Finally, the benchmark utilizes a wide variety of images, enabling a thorough evaluation of MLLMs' multifaceted capabilities.", "section": "3. Creation-MMBench"}, {"figure_path": "https://arxiv.org/html/2503.14478/extracted/6287371/figures/full_task_v3.png", "caption": "Figure 7: Comparing OC Score and Creation-MMBench Reward. This figure shows the model performance on the OpenVLM Leaderboard and Creation-MMBench, highlighting a significant gap between objective performance and visual creativity in some open-source models.", "description": "This figure compares the performance of various large language models (LLMs) on two different benchmarks: the OpenVLM Leaderboard and Creation-MMBench. The OpenVLM Leaderboard measures general objective performance, while Creation-MMBench specifically evaluates visual creativity.  The graph highlights a significant difference between the scores achieved on these two benchmarks for some open-source models.  While some models perform well on the OpenVLM Leaderboard (indicating strong general capabilities), their performance on Creation-MMBench is substantially lower, demonstrating a weakness in visual creativity. This discrepancy emphasizes that general objective metrics may not accurately reflect the visual creative capabilities of LLMs, especially for open-source models.", "section": "4. Experiment"}, {"figure_path": "https://arxiv.org/html/2503.14478/x10.png", "caption": "Figure 8: Qualitative study Case between InternVL-2.5-78B and Reference Answer (GPT4o-1120).", "description": "This figure presents a qualitative comparison of responses generated by InternVL-2.5-78B and GPT-40-1120 for a creative multimodal understanding task within the Creation-MMBench benchmark.  It highlights the differences in the models' abilities to interpret visual information and formulate coherent, creative responses. The analysis focuses on aspects like the accuracy of visual description, the clarity and coherence of the explanation, and the overall engagement level of the response.  It showcases how GPT-40-1120 produces a superior response demonstrating a better grasp of the image's key elements and a more compelling narrative.  The detailed breakdown helps illustrate the strengths and weaknesses of each model in a complex creative task.", "section": "4.5 Qualitative Study"}, {"figure_path": "https://arxiv.org/html/2503.14478/x11.png", "caption": "Figure 9: Overview of Creation-MMBench Complete Task. Contains four task categories, each category consists of multiple tasks.", "description": "Creation-MMBench is a multimodal benchmark designed to evaluate the creative capabilities of Multimodal Large Language Models (MLLMs).  This figure provides a visual overview of the benchmark's structure. It shows the four main task categories within Creation-MMBench: Literary Writing, Common Functional Writing, Professional Functional Writing, and Creative Multimodal Understanding. Each category contains multiple individual tasks, illustrating the breadth of creative abilities assessed by the benchmark. The figure highlights the diversity of tasks within each category and the benchmark as a whole.", "section": "3. Creation-MMBench"}, {"figure_path": "https://arxiv.org/html/2503.14478/x12.png", "caption": "(a) Distribution of query lengths.", "description": "The figure shows a bar chart illustrating the distribution of query lengths in the Creation-MMBench dataset.  The x-axis represents different ranges of query lengths (likely categorized into bins such as 0-50 words, 50-100 words, etc.), and the y-axis shows the relative frequency or proportion of queries falling into each length category.  This visualization helps to understand the complexity and length variation of the prompts used in the benchmark.", "section": "3. Creation-MMBench"}, {"figure_path": "https://arxiv.org/html/2503.14478/x13.png", "caption": "(b) Roles in Creation-MMBench.", "description": "The figure shows the various roles that large language models (LLMs) are tasked to assume when responding to prompts in the Creation-MMBench benchmark.  The wide variety of roles highlights the diverse contexts and situations in which the LLMs need to generate creative text.  Examples of roles include:  'media head', 'customer', 'writer', 'guide', and many others that span numerous professional and non-professional fields.  The distribution of these roles aims to test the LLMs' ability to adapt their creative responses to different perspectives and situations.", "section": "3. Creation-MMBench"}, {"figure_path": "https://arxiv.org/html/2503.14478/x14.png", "caption": "Figure 10: Redundancy Analysis of Creation-MMBench with other widely used MLLM Benchmarks.", "description": "This figure displays the results of a redundancy analysis comparing Creation-MMBench with several other widely used Multimodal Large Language Model (MLLM) benchmarks.  The analysis measures the correlation between Creation-MMBench's evaluation scores and those of other benchmarks. The heatmaps visualize the Spearman's Rank Correlation Coefficient (SRCC) and the coefficient of determination (R-squared) for all benchmark pairs.  High correlations indicate redundancy, suggesting that those benchmarks assess similar aspects of MLLM capabilities. Low correlations indicate that Creation-MMBench measures unique aspects of MLLM performance not captured by the other benchmarks.", "section": "D Advanced Analysis of Creation-MMBench"}, {"figure_path": "https://arxiv.org/html/2503.14478/extracted/6287371/figures/data_voter.png", "caption": "(a) Distribution of reference answers lengths.", "description": "This histogram displays the distribution of reference answer lengths across different benchmarks, including Creation-MMBench, MM-Vet, MLLM-Bench, AlignMMBench, MEGABench_core, and MEGABench_open. The x-axis represents the length of the reference answers, categorized into bins (e.g., <50, 50-1500, 1500-3000, >3000 tokens), and the y-axis shows the proportion of reference answers falling into each bin. The bars in the histogram visually compare the distribution of reference answer lengths across the different benchmarks.", "section": "3.2 Dataset Statistics"}, {"figure_path": "https://arxiv.org/html/2503.14478/x15.png", "caption": "(b) Instructions in Creation-MMBench.", "description": "The figure shows a word cloud summarizing the instructions given to large language models (LLMs) in the Creation-MMBench benchmark.  The words represent the various tasks or types of creative text generation that the LLMs are instructed to perform, reflecting the diverse range of creative tasks in the benchmark.  The size of each word roughly corresponds to its frequency of occurrence in the instructions, indicating the relative emphasis placed on certain types of creative writing tasks.", "section": "3. Creation-MMBench"}, {"figure_path": "https://arxiv.org/html/2503.14478/x16.png", "caption": "(c) Top 15 Image Categories in Creation-MMBench.", "description": "This figure shows the top 15 image categories used in Creation-MMBench, a multimodal benchmark designed to evaluate the creative capabilities of Multimodal Large Language Models (MLLMs).  The categories illustrate the diverse range of visual content included in the benchmark, encompassing various genres, such as animation, people, products, architecture, events, education, art, food and beverages, nature, science and technology, news, user interfaces, interior design, history and culture, and statistical data.  This diversity is a key feature of the benchmark, enabling a comprehensive assessment of MLLMs' ability to handle varied visual inputs and generate creative outputs.", "section": "3. Creation-MMBench"}, {"figure_path": "https://arxiv.org/html/2503.14478/extracted/6287371/figures/category_case/LW_story_continue.png", "caption": "Figure 11: Other Statistics of Creation-MMBench.", "description": "This figure presents supplementary statistics for Creation-MMBench, comparing it against other widely used MLLM benchmarks.  Subfigure (a) shows the distribution of reference answer lengths, highlighting that Creation-MMBench features significantly longer answers than others. Subfigure (b) displays a word cloud of the instructions used in Creation-MMBench's tasks, demonstrating the diversity and complexity of the tasks. Finally, subfigure (c) illustrates the distribution of image categories present in the dataset, emphasizing the rich and diverse visual content.", "section": "3.2 Dataset Statistics"}, {"figure_path": "https://arxiv.org/html/2503.14478/extracted/6287371/figures/category_case/LW_daily_conversation_creation.png", "caption": "Figure 12: The Process of Human Pairwise Comparison.", "description": "This figure illustrates the process of human pairwise comparison used to evaluate the responses generated by different language models.  Specifically, it shows how two model responses are presented side-by-side to human evaluators, along with detailed criteria for comparison. Evaluators are asked to determine which response is superior, or if they are equal.  The process is designed to minimize bias by randomly changing the positions of the model responses.", "section": "Evaluation Strategy"}, {"figure_path": "https://arxiv.org/html/2503.14478/extracted/6287371/figures/category_case/LW_landscape_to_poem.png", "caption": "Figure 13: Qualitative Case in Professional Functional Writing. This case comes from Software Engineering Diagram Explanation Task, Assistant A is GPT-4o-1120, assistant B is Qwen2.5-VL-72B.", "description": "Figure 13 presents a qualitative comparison of two different large language models (LLMs), GPT-40-1120 and Qwen2.5-VL-72B, on a software engineering diagram explanation task.  The figure showcases how each model interprets a swimlane diagram.  Assistant A (GPT-40-1120) accurately identifies the type of diagram, clearly explains its purpose and stages in software development, and provides a comprehensive step-by-step explanation of the credit approval process shown.  In contrast, Assistant B (Qwen2.5-VL-72B) misidentifies the diagram type, leading to inaccuracies in its explanation. The evaluation highlights Assistant A's superior performance due to its correct diagram identification and more thorough explanation.", "section": "3. Creation-MMBench"}, {"figure_path": "https://arxiv.org/html/2503.14478/extracted/6287371/figures/category_case/LW_historical_story_creation.png", "caption": "Figure 14: Qualitative Case in Creative Multimodal Understanding. This case comes from Travel Itinerary Planning and Recommendations Task, Assistant A is GPT-4o-1120, assistant B is InternVL2.5-78B.", "description": "This figure showcases a qualitative comparison of two different large language models (LLMs), GPT-40-1120 (Assistant A) and InternVL-2.5-78B (Assistant B), in performing a travel itinerary planning task.  The task required generating a 2-day travel plan for art and architecture enthusiasts in Barcelona, utilizing a provided map.  The figure highlights the differences in the quality and completeness of the travel plans generated by each model.  GPT-40-1120's plan is presented as superior in organization, detail, and adherence to the user's preferences. ", "section": "4.2 Qualitative Study"}, {"figure_path": "https://arxiv.org/html/2503.14478/extracted/6287371/figures/category_case/CFW_daily_achievement_show_off.png", "caption": "Figure 15: Example Case of Literary Writing, from Task story continue.", "description": "This figure shows an example from the Creation-MMBench dataset, specifically from the \"Story continue\" task within the Literary Writing category.  It displays a series of images from a children's animation, along with the task prompt, requirements, and evaluation criteria. The task requires the model to continue the story based on the provided images. The evaluation criteria assess aspects like narrative coherence, vivid descriptions, character consistency, and the introduction of new challenges.", "section": "3. Creation-MMBench"}, {"figure_path": "https://arxiv.org/html/2503.14478/extracted/6287371/figures/category_case/CFW_social_media_travel_content.png", "caption": "Figure 16: Example Case of Literary Writing, from Task daily conversation creation.", "description": "This figure shows an example of a creative writing task from the Creation-MMBench benchmark.  The task is to generate a daily conversation between two people based on a given image. The figure displays the image used in the task, and excerpts from two model responses, demonstrating the diversity of outputs that can be achieved.  One response is marked as significantly better, illustrating the benchmark's ability to differentiate between various levels of creative writing quality. The criteria used to assess each response are also presented.", "section": "3. Creation-MMBench"}, {"figure_path": "https://arxiv.org/html/2503.14478/extracted/6287371/figures/category_case/CFW_daily_affairs_inquiries.png", "caption": "Figure 17: Example Case of Literary Writing, from Task landscape to poem.", "description": "This figure shows an example of a literary writing task from the Creation-MMBench benchmark.  The task was \"landscape to poem.\"  It displays the prompt given to the large language model (LLM), including instructions and evaluation criteria, along with two example responses (Assistant A and Assistant B) from different LLMs. The evaluation criteria included subjective aspects like coherence and the use of imagery, as well as objective aspects such as adherence to the sonnet form.  The image used in the prompt is also included, to show how the generated poems should reflect the visual aspects of the image. The goal of this example is to demonstrate the capability of Creation-MMBench to evaluate the creative writing abilities of LLMs in response to visual prompts.", "section": "3. Creation-MMBench"}, {"figure_path": "https://arxiv.org/html/2503.14478/extracted/6287371/figures/category_case/CFW_personal_event_summaries.png", "caption": "Figure 18: Example Case of Literary Writing, from Task historical story creation.", "description": "This figure displays an example from the Creation-MMBench benchmark dataset, specifically showcasing a task that involves generating a historical narrative based on a provided image. The image in question shows a photograph from 1944 of the Central Epidemic Prevention Office in Xishan, Kunming, Yunnan Province. The task requires the model to create a story based on this image, incorporating relevant historical context and imagining the life experiences of the people depicted.  The criteria for evaluation include historical accuracy, emotional engagement, and narrative connection to the image itself.  The response should create a compelling story that is both historically grounded and emotionally resonant.", "section": "3. Creation-MMBench"}, {"figure_path": "https://arxiv.org/html/2503.14478/extracted/6287371/figures/category_case/PFW_teaching_plan.png", "caption": "Figure 19: Example Case of Common Functional Writing, from Task daily achievement show off.", "description": "This figure shows an example of a response from the Common Functional Writing task in Creation-MMBench. The specific task is to write a Facebook post about a daily achievement: receiving a certificate of achievement. The figure displays the images used as input to the model, the prompt given to the model, and the generated response (Facebook post).  The response includes a description of the event, the student's reflections on their achievement, and the significance of receiving the award.", "section": "3.1 Benchmark Construction"}, {"figure_path": "https://arxiv.org/html/2503.14478/extracted/6287371/figures/category_case/PFW_product_marketing_strategy.png", "caption": "Figure 20: Example Case of Common Functional Writing, from Task social media travel content.", "description": "Figure 20 shows an example from the Creation-MMBench dataset, specifically from the Common Functional Writing section. It presents a Reddit post written by an LLM in response to an image prompt depicting a trip to Krakow, Poland. The goal of this task is to assess the LLM's ability to generate engaging and informative social media content based on visual input.  The figure showcases the LLM's response, along with the evaluation criteria used to assess aspects like engagement, clarity, and factual accuracy related to the image.", "section": "3.1 Benchmark Construction"}, {"figure_path": "https://arxiv.org/html/2503.14478/extracted/6287371/figures/category_case/PFW_nutritional_formulation_of_recipe.png", "caption": "Figure 21: Example Case of Common Functional Writing, from Task daily affairs inquiries.", "description": "This figure shows an example from the Common Functional Writing section of the Creation-MMBench benchmark.  It specifically illustrates a task where the model is asked to compose a Reddit post seeking help with a technical problem. The image shows a screenshot of an iPhone with several apps frozen. The associated text provides detailed criteria for evaluating the generated Reddit post, including aspects like clarity, tone, and accuracy of problem description.  It highlights the multimodal nature of the Creation-MMBench by incorporating visual information into a creative writing task.", "section": "3.1 Benchmark construction"}, {"figure_path": "https://arxiv.org/html/2503.14478/extracted/6287371/figures/category_case/PFW_clothing_match_design.png", "caption": "Figure 22: Example Case of Common Functional Writing, from Task personal event summaries.", "description": "This figure shows an example from the Creation-MMBench dataset, specifically from the \"personal event summaries\" task within the Common Functional Writing category. It displays the Reddit year-end report of a user, including their top categories (Psychology, Q&As, and Memes), a significant post, and engagement metrics. The task requires generating a blog post summarizing this report, explaining a user's question about placing an object on Mars, and encouraging interaction.", "section": "3. Creation-MMBench"}, {"figure_path": "https://arxiv.org/html/2503.14478/extracted/6287371/figures/category_case/CMU_advertisement_explanation.png", "caption": "Figure 23: Example Case of Professional Functional Writing, from Task teaching plan.", "description": "This figure shows an example of a professional functional writing task from the Creation-MMBench benchmark.  It displays a teaching plan created by an AI model for a biology lesson on prokaryotic cells. The plan includes teaching objectives, a breakdown of lesson sections with specific content, and an estimated time allocation for each section.  The plan also demonstrates consideration of differentiated instruction for different student levels using visual materials from the textbook. This example showcases the complexity and detailed evaluation criteria involved in assessing AI's ability to generate functional and coherent text suitable for real-world applications.", "section": "3. Creation-MMBench"}, {"figure_path": "https://arxiv.org/html/2503.14478/extracted/6287371/figures/category_case/CMU_document_understanding.png", "caption": "Figure 24: Example Case of Professional Functional Writing, from Task product marketing strategy.", "description": "Figure 24 shows an example from the Creation-MMBench benchmark's \"Professional Functional Writing\" task, specifically the \"Product marketing strategy\" subtask.  It presents a case where a model generates a marketing strategy and promotional copy for Estee Lauder skincare products targeting the Asia-Pacific market.  The figure highlights the model's response, the evaluation criteria (both general subjective and visual factuality criteria), and the visual input (images of Estee Lauder products).  It illustrates how the benchmark evaluates both the creativity and factual accuracy of MLLM responses in a professional context.", "section": "3. Creation-MMBench"}, {"figure_path": "https://arxiv.org/html/2503.14478/extracted/6287371/figures/category_case/CMU_snapshot_analysis.png", "caption": "Figure 25: Example Case of Professional Functional Writing, from Task nutritional formulation of recipe.", "description": "This figure shows an example from the Creation-MMBench dataset, specifically from the \"Professional Functional Writing\" task category focusing on \"nutritional formulation of recipe\".  It presents the prompt given to the model, the model's response, evaluation criteria (both general subjective and visual factuality), and the scoring of the model's response.  The prompt requires the model to analyze a recipe (shown in an image), identify its advantages and disadvantages for a marathon runner, and suggest supplementary foods to optimize the diet. The evaluation criteria assess the completeness and accuracy of the analysis, the depth of nutritional insights provided, and the alignment of the response with the dish description and nutritional needs. This demonstrates the benchmark's ability to evaluate nuanced, practical creative tasks involving multimodal reasoning.", "section": "3. Creation-MMBench"}, {"figure_path": "https://arxiv.org/html/2503.14478/extracted/6287371/figures/category_case/CMU_travel_itinerary_planning_and_recommendations.png", "caption": "Figure 26: Example Case of Professional Functional Writing, from Task clothing match design.", "description": "Figure 26 presents an example from the \"Professional Functional Writing\" task category, specifically focusing on \"clothing match design\".  It showcases a response from a model to a prompt requesting a clothing outfit suitable for working from home while remaining presentable for video calls. The figure highlights the model's ability to propose a well-reasoned outfit considering both comfort and professionalism, along with detailed explanations justifying the choices made.", "section": "Category Qualitative Case Study"}]