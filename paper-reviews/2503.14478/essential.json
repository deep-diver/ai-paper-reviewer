{"importance": "This study introduces Creation-MMBench, a benchmark to evaluate creative capabilities of MLLMs, which will provide a valuable resource for researchers working on **advancing MLLM creativity and generative intelligence.** The benchmark and analysis can open new avenues for research in multimodal learning and evaluation.", "summary": "Creation-MMBench: Assessing Context-Aware Creative Intelligence in MLLMs", "takeaways": ["Current open-source MLLMs significantly underperform proprietary models in creative tasks.", "Visual fine-tuning can negatively impact the base LLM's creative abilities.", "Creation-MMBench offers valuable insights for advancing MLLM creativity."], "tldr": "Current MLLM benchmarks have gaps evaluating **visual-creative intelligence**. Existing benchmarks feature simple questions that fail to assess model performance in real-life creative tasks. To address this, the paper introduces Creation-MMBench to divide the intelligence into three forms and comprehensively evaluate visual creative intelligence. \n\nThe paper proposed Creation-MMBench, a multimodal benchmark specifically designed to evaluate the creative capabilities of MLLMs in real-world, image-based tasks. **Creation-MMBench provides more creative and discriminative questions** and better evaluation of visual creative intelligence. The benchmark comprises 765 test cases spanning 51 fine-grained tasks.", "affiliation": "Zhejiang University", "categories": {"main_category": "Multimodal Learning", "sub_category": "Vision-Language Models"}, "podcast_path": "2503.14478/podcast.wav"}