[{"content": "| Method | Depth | #Param | Iters | IS \u2191 | FID \u2193 | sFID \u2193 | Prec. \u2191 | Recall \u2191 | Sampling it/s \u2191 |\n|---|---|---|---|---|---|---|---|---|---| \n| DiT-XL/2 [40] | 28 | 675 M | 7,000 K | 278.24 | 2.27 | 4.60 | 0.83 | 0.57 | 6.91 |\n| DiT-XL/2 [40] | 28 | 675 M | 2,000 K | 240.22 | 2.73 | 4.46 | 0.83 | 0.55 | 6.91 |\n| DiT-XL/2 [40] | 28 | 675 M | 1,000 K | 157.83 | 5.53 | 4.60 | 0.80 | 0.53 | 6.91 |\n| U-ViT-H/2 [1] | 29 | 501 M | 500 K | 265.30 | 2.30 | 5.60 | 0.82 | 0.58 | 8.21 |\n| ShortGPT [36] | 28 \u21d2 19 | 459 M | 100 K | 132.79 | 7.93 | 5.25 | 0.76 | 0.53 | 10.07 |\n| TinyDiT-D19 (KD) | 28 \u21d2 19 | 459 M | 100 K | 242.29 | 2.90 | 4.63 | 0.84 | 0.54 | 10.07 |\n| TinyDiT-D19 (KD) | 28 \u21d2 19 | 459 M | 500 K | 251.02 | 2.55 | 4.57 | 0.83 | 0.55 | 10.07 |\n| DiT-L/2 [40] | 24 | 458 M | 1,000 K | 196.26 | 3.73 | 4.62 | 0.82 | 0.54 | 9.73 |\n| U-ViT-L [1] | 21 | 287 M | 300 K | 221.29 | 3.44 | 6.58 | 0.83 | 0.52 | 13.48 |\n| U-DiT-L [50] | 22 | 204 M | 400 K | 246.03 | 3.37 | 4.49 | 0.86 | 0.50 | - |\n| Diff-Pruning-50% [12] | 28 | 338 M | 100 K | 186.02 | 3.85 | 4.92 | 0.82 | 0.54 | 10.43 |\n| Diff-Pruning-75% [12] | 28 | 169 M | 100 K | 83.78 | 14.58 | 6.28 | 0.72 | 0.53 | 13.59 |\n| ShortGPT [36] | 28 \u21d2 14 | 340 M | 100 K | 66.10 | 22.28 | 6.20 | 0.63 | 0.56 | 13.54 |\n| Flux-Lite [6] | 28 \u21d2 14 | 340 M | 100 K | 54.54 | 25.92 | 5.98 | 0.62 | 0.55 | 13.54 |\n| Sensitivity Analysis [18] | 28 \u21d2 14 | 340 M | 100 K | 70.36 | 21.15 | 6.22 | 0.63 | 0.57 | 13.54 |\n| Oracle (BK-SDM) [23] | 28 \u21d2 14 | 340 M | 100 K | 141.18 | 7.43 | 6.09 | 0.75 | 0.55 | 13.54 |\n| TinyDiT-D14 | 28 \u21d2 14 | 340 M | 100 K | 151.88 | 5.73 | 4.91 | 0.80 | 0.55 | 13.54 |\n| TinyDiT-D14 | 28 \u21d2 14 | 340 M | 500 K | 198.85 | 3.92 | 5.69 | 0.78 | 0.58 | 13.54 |\n| TinyDiT-D14 (KD) | 28 \u21d2 14 | 340 M | 100 K | 207.27 | 3.73 | 5.04 | 0.81 | 0.54 | 13.54 |\n| TinyDiT-D14 (KD) | 28 \u21d2 14 | 340 M | 500 K | 234.50 | 2.86 | 4.75 | 0.82 | 0.55 | 13.54 |\n| DiT-B/2 [40] | 12 | 130 M | 1,000 K | 119.63 | 10.12 | 5.39 | 0.73 | 0.55 | 28.30 |\n| U-DiT-B [50] | 22 | - | 400 K | 85.15 | 16.64 | 6.33 | 0.64 | 0.63 | - |\n| TinyDiT-D7 (KD) | 14 \u21d2 7 | 173 M | 500 K | 166.91 | 5.87 | 5.43 | 0.78 | 0.53 | 26.81 |", "caption": "Table 1: Layer pruning results for pre-trained DiT-XL/2. We focus on two settings: fast training with 100K optimization steps and sufficient fine-tuning with 500K steps. Both fine-tuning and Masked Knowledge Distillation (a variant of KD, see Sec.\u00a04.4) are used for recovery.", "description": "This table presents the results of depth pruning experiments performed on a pre-trained DiT-XL/2 model.  The experiments compare different depth pruning methods and explore the impact of fine-tuning duration (100K vs. 500K optimization steps).  Metrics evaluated include Inception Score (IS), Fr\u00e9chet Inception Distance (FID), and Sliding FID (sFID), along with Precision and Recall.  The table also shows the effect of applying Masked Knowledge Distillation (a variant of knowledge distillation) during the recovery phase after pruning.  The number of parameters, sampling speed (iterations per second), and the depth of the pruned model are also included. The table highlights the trade-off between model efficiency and performance.", "section": "4.2 Results on Diffusion Transformers"}, {"content": "| Method | Depth | Params | Epochs | FID | IS |\n|---|---|---|---|---|---| \n| MAR-Large | 32 | 479 M | 400 | 1.78 | 296.0 |\n| MAR-Base | 24 | 208 M | 400 | 2.31 | 281.7 |\n| TinyMAR-D16 | 32 => 16 | 277 M | 40 | 2.28 | 283.4 |\n| SiT-XL/2 | 28 | 675 M | 1,400 | 2.06 | 277.5 |\n| TinySiT-D14 | 28 => 14 | 340 M | 100 | 3.02 | 220.1 |", "caption": "Table 2: Depth pruning results on MARs\u00a0[29] and SiTs\u00a0[34].", "description": "This table presents the results of applying depth pruning techniques to two different types of diffusion transformer models: Masked Autoregressive models (MARs) and Scalable Interpolant Transformers (SiTs).  It shows the depth of the original and pruned models, the number of parameters, the number of training epochs used, and the resulting Fr\u00e9chet Inception Distance (FID) and Inception Score (IS) after pruning.  The FID and IS scores are commonly used metrics to evaluate the quality of images generated by these models; lower FID and higher IS scores generally indicate better image quality.", "section": "4.2 Results on Diffusion Transformers"}, {"content": "| Strategy | Loss | IS | FID | Prec. | Recall |\n|---|---|---|---|---|---| \n| Max. Loss | 37.69 | NaN | NaN | NaN | NaN |\n| Med. Loss | 0.99 | 149.51 | 6.45 | 0.78 | 0.53 |\n| Min. Loss | **0.20** | 73.10 | 20.69 | 0.63 | 0.58 |\n| Sensitivity | 0.21 | 70.36 | 21.15 | 0.63 | **0.57** |\n| ShortGPT [36] | 0.20 | 66.10 | 22.28 | 0.63 | 0.56 |\n| Flux-Lite [6] | 0.85 | 54.54 | 25.92 | 0.62 | 0.55 |\n| Oracle (BK-SDM) | 1.28 | 141.18 | 7.43 | 0.75 | 0.55 |\n| Learnable | **0.98** | **151.88** | **5.73** | **0.80** | 0.55 |", "caption": "Table 3: Directly minimizing the calibration loss may lead to non-optimal solutions. All pruned models are fine-tuned without knowledge distillation (KD) for 100K steps. We evaluate the following baselines: (1) Loss \u2013 We randomly prune a DiT-XL model to generate 100,000 models and select models with different calibration losses for fine-tuning; (2) Metric-based Methods \u2013 such as Sensitivity Analysis and ShortGPT; (3) Oracle \u2013 We retain the first and last layers while uniformly pruning the intermediate layers following [23]; (4) Learnable \u2013 The proposed learnable method.", "description": "This table compares different depth pruning methods for diffusion transformers, focusing on the impact of minimizing calibration loss versus maximizing post-fine-tuning performance.  It shows that simply minimizing calibration loss doesn't guarantee optimal results after fine-tuning. The methods compared are: (1) Random pruning with varying calibration losses, demonstrating the lack of correlation between initial loss and final performance. (2) Metric-based methods (Sensitivity Analysis and ShortGPT) which are based on heuristics and importance scores to identify layers to remove, which also underperform. (3) An oracle method (retaining first and last layers while uniformly pruning the rest), which provides a reasonably good baseline. (4) The proposed 'Learnable' method which aims to directly optimize for post-fine-tuning performance. All methods are fine-tuned for 100,000 steps without knowledge distillation.", "section": "4.3. Analytical Experiments"}, {"content": "| Pattern | **\u0394W** | IS \u2191 | FID \u2193 | sFID \u2193 | Prec. \u2191 | Recall \u2191 |\n|---|---|---|---|---|---|---|\n| 1:2 | LoRA | 54.75 | 33.39 | 29.56 | 0.56 | 0.62 |\n| 2:4 | LoRA | 53.07 | 34.21 | 27.61 | 0.55 | 0.63 |\n| 7:14 | LoRA | 34.97 | 49.41 | 28.48 | 0.46 | 0.56 |\n| 1:2 | Full | 53.11 | 35.77 | 32.68 | 0.54 | 0.61 |\n| 2:4 | Full | 53.63 | 34.41 | 29.93 | 0.55 | 0.62 |\n| 7:14 | Full | 45.03 | 38.76 | 31.31 | 0.52 | 0.62 |\n| 1:2 | Frozen | 45.08 | 39.56 | 31.13 | 0.52 | 0.60 |\n| 2:4 | Frozen | 48.09 | 37.82 | 31.91 | 0.53 | 0.62 |\n| 7:14 | Frozen | 34.09 | 49.75 | 31.06 | 0.46 | 0.56 |", "caption": "Table 4: Performance comparison of TinyDiT-D14 models compressed using various pruning schemes and recoverability estimation strategies. All models are fine-tuned for 10,000 steps, and FID scores are computed on 10,000 sampled images with 64 timesteps.", "description": "This table compares the performance of different TinyDiT-D14 models created using various pruning techniques and recoverability estimation strategies.  The performance is measured by FID score (Frechet Inception Distance), IS (Inception Score), SFID (Sliding Fr\u00e9chet Inception Distance), Precision and Recall. Each model was fine-tuned for 10,000 steps using 10,000 samples generated with 64 timesteps. The comparison allows us to assess the impact of different pruning methods and strategies on the overall quality and efficiency of the model.", "section": "4.2 Results on Diffusion Transformers"}, {"content": "| fine-tuning Strategy | Init. Distill. Loss | FID @ 100K |\n|---|---|---|\n| fine-tuning | - | 5.79 |\n| Logits KD | - | 4.66 |\n| RepKD | 2840.1 | NaN |\n| Masked KD (0.1\u03c3) | 15.4 | NaN |\n| Masked KD (2\u03c3) | 387.1 | 3.73 |\n| Masked KD (4\u03c3) | 391.4 | 3.75 |", "caption": "Table 5: Evaluation of different fine-tuning strategies for recovery. Masked RepKD ignores those massive activations (|x|>k\u2062\u03c3x\ud835\udc65\ud835\udc58subscript\ud835\udf0e\ud835\udc65|x|>k\\sigma_{x}| italic_x | > italic_k italic_\u03c3 start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT) in both teacher and student, which enables effective knowledge transfer between diffusion transformers.", "description": "This table presents the results of different fine-tuning strategies used for recovering the performance of pruned diffusion transformer models.  The strategies compared include standard fine-tuning, logits KD (knowledge distillation),  RepKD (representation distillation), and a novel Masked KD.  Masked KD is a variation of RepKD designed to mitigate the negative effects of extremely large activation values in the hidden layers of both teacher and student networks by ignoring the loss associated with these values above a certain threshold (k*\u03c3x, where k is a hyperparameter and \u03c3x is the standard deviation of the activations). The table shows that Masked KD significantly improves the final FID (Fr\u00e9chet Inception Distance) score, indicating better performance recovery compared to the other methods, likely due to the more effective transfer of knowledge between models. The FID score is a lower-is-better metric for evaluating the quality of generated images.  Lower FID scores indicate that the generated images are more similar to real images.", "section": "4.4. Knowledge Distillation for Recovery"}, {"content": "| Model | Optimizer | Cosine Sched. | Teacher | \u03b1<sub>KD</sub> | \u03b1<sub>GT</sub> | \u03b2 | Grad. Clip | Pruning Configs |\n|---|---|---|---|---|---|---|---|---|\n| DiT-D19 | AdamW(lr=2e-4, wd=0.0) | \u03b7<sub>min</sub>=1e-4 | DiT-XL | 0.9 | 0.1 | 1e-2 \u2192 0 | 1.0 | LoRA-1:2 |\n| DiT-D14 | AdamW(lr=2e-4, wd=0.0 | \u03b7<sub>min</sub>=1e-4 | DiT-XL | 0.9 | 0.1 | 1e-2 \u2192 0 | 1.0 | LoRA-1:2 |\n| DiT-D7 | AdamW(lr=2e-4, wd=0.0) | \u03b7<sub>min</sub>=1e-4 | DiT-D14 | 0.9 | 0.1 | 1e-2 \u2192 0 | 1.0 | LoRA-1:2 |\n| SiT-D14 | AdamW(lr=2e-4, wd=0.0) | \u03b7<sub>min</sub>=1e-4 | SiT-XL | 0.9 | 0.1 | 2e-4 \u2192 0 | 1.0 | LoRA-1:2 |\n| MAR-D16 | AdamW(lr=2e-4, wd=0.0) | \u03b7<sub>min</sub>=1e-4 | MAR-Large | 0.9 | 0.1 | 1e-2 \u2192 0 | 1.0 | LoRA-1:2 |", "caption": "Table 6: Training details and hyper-parameters for mask training", "description": "This table details the training configurations used for the learnable depth pruning method in the TinyFusion model.  It lists the model variations (DiT-D19, DiT-D14, DiT-D7, SiT-D14, MAR-D16), the optimizer used (AdamW), the cosine scheduler parameters (minimum learning rate), the teacher model used for knowledge distillation (DiT-XL, SiT-XL, MAR-Large), the hyperparameters for the loss function (\u03b1KD, \u03b1GT, \u03b2), gradient clipping parameters, and the pruning configuration (LORA-1:2).  These parameters dictate how the model learns to effectively prune layers while maintaining performance.", "section": "4. Experiments"}, {"content": "| Teacher Model | Pruned From | IS | FID | sFID | Prec. | Recall |\n|---|---|---|---|---|---|---|\n| DiT-XL/2 | DiT-XL/2 | 29.46 | 56.18 | 26.03 | 0.43 | 0.51 |\n| DiT-XL/2 | TinyDiT-D14 | 51.96 | 36.69 | 28.28 | 0.53 | 0.59 |\n| TinyDiT-D14 | DiT-XL/2 | 28.30 | 58.73 | 29.53 | 0.41 | 0.50 |\n| TinyDiT-D14 | TinyDiT-D14 | 57.97 | 32.47 | 26.05 | 0.55 | 0.60 |", "caption": "Table 7: TinyDiT-D7 is pruned and distilled with different teacher models for 10k, sample steps is 64, original weights are used for sampling rather than EMA.", "description": "This table presents the results of experiments evaluating different approaches to training a smaller, more efficient diffusion transformer model (TinyDiT-D7).  The model is created by pruning a larger pre-trained model (DiT-XL/2), and then fine-tuned using knowledge distillation from different teacher models.  The table shows the Inception Score (IS), Fr\u00e9chet Inception Distance (FID), and Sliding FID (SFID), along with precision and recall metrics. These metrics help evaluate the quality and efficiency of the generated images from the pruned and fine-tuned model.  Note that the sampling uses the original weights instead of the Exponential Moving Average (EMA).", "section": "4.3. Analytical Experiments"}, {"content": "| Learning Rate | IS | FID | sFID | Prec. | Recall |\n|---|---|---|---|---|---|---|\n| lr=2e-4 | 207.27 | 3.73 | 5.04 | 0.8127 | 0.5401 |\n| lr=1e-4 | 194.31 | 4.10 | 5.01 | 0.8053 | 0.5413 |\n| lr=5e-5 | 161.40 | 6.63 | 6.69 | 0.7419 | 0.5705 |", "caption": "Table 8: The effect of Learning rato for TinyDiT-D14 finetuning w/o knowledge distillation", "description": "This table shows the impact of different learning rates on the performance of the TinyDiT-D14 model when fine-tuned without knowledge distillation.  It presents the Inception Score (IS), Fr\u00e9chet Inception Distance (FID), and Sliding FID (SFID) along with Precision and Recall metrics for three different learning rates (2e-4, 1e-4, and 5e-5).  The results demonstrate how the choice of learning rate affects the model's performance after fine-tuning.", "section": "4.2 Results on Diffusion Transformers"}]