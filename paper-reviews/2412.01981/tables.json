[{"content": "| Type | Reward Model | Mistral-7B-Inst-v0.2 |  |  | Llama-3.1-8B-Inst |  |  | Llama-3.1-70B-Inst |  |  | Avg. |\n|---|---|---|---|---|---|---|---|---|---|---|---|\n|  |  | Pass@1: 9.6 | @4 | @16 | @64 | Pass@1: 44.6 | @4 | @16 | Pass@1: 63.2 | @4 | @16 |  |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| **ORM** | <a href=\"https://huggingface.co/openbmb/Eurus-RM-7b\">EurusRM-7B</a> | 17.2 | 21.0 | 20.4 | 49.6 | 51.6 | 51.8 | 69.0 | 69.6 | 72.2 | 46.9 |\n|  | <a href=\"https://huggingface.co/Skywork/Skywork-Reward-Llama-3.1-8B-v0.2\">SkyworkRM-Llama3.1-8B</a> | 16.0 | 19.6 | 23.4 | 49.0 | 50.4 | 48.2 | 70.4 | 72.6 | 72.0 | 46.8 |\n|  | <a href=\"https://huggingface.co/RLHFlow/ArmoRM-Llama3-8B-v0.1\">ArmoRM-Llama3-8B</a> | 16.6 | 21.0 | 23.2 | 47.8 | 48.6 | 49.4 | 70.6 | 70.8 | 71.0 | 46.6 |\n| **PRM** | <a href=\"https://huggingface.co/peiyi9979/math-shepherd-mistral-7b-prm\">Math-Shepherd-7B</a> | 16.0 | 21.0 | 20.4 | 50.0 | 52.4 | 52.8 | 66.4 | 65.8 | 65.6 | 45.6 |\n|  | <a href=\"https://huggingface.co/RLHFlow/Llama3.1-8B-PRM-Mistral-Data\">RLHFlow-8B-Mistral-Data</a> | 19.4 | 25.2 | 30.2 | 51.8 | 52.0 | 50.6 | 70.8 | 71.0 | 71.2 | 49.1 |\n|  | <a href=\"https://huggingface.co/RLHFlow/Llama3.1-8B-PRM-Deepseek-Data\">RLHFlow-8B-DS-Data</a> | 17.2 | 23.0 | 25.2 | 54.4 | 54.2 | 55.8 | 68.6 | 70.4 | 73.0 | 49.1 |\n| **Baselines** | **Math-Shepherd** | 17.6 | 24.4 | 26.8 | 50.0 | 51.4 | 52.8 | 68.6 | 69.4 | 68.8 | 47.8 |\n|  | **AutoPSV** | 16.6 | 20.6 | 22.2 | 52.2 | 51.4 | 52.2 | 68.4 | 65.4 | 62.4 | 45.7 |\n| **Implicit PRM** | **DPO** | 18.6 | 24.4 | 28.8 | 54.0 | 55.4 | 57.0 | 71.8 | 71.2 | 72.2 | 50.4 |\n|  | **KTO** | 15.6 | 18.4 | 18.6 | 49.6 | 51.8 | 50.8 | 72.6 | 67.0 | 67.2 | 45.7 |\n|  | **NCA** | 18.6 | 23.8 | 28.0 | 52.4 | 53.4 | 55.2 | 69.0 | 73.0 | 71.6 | 49.4 |\n|  | **CE** | 18.8 | 24.0 | 28.0 | 52.6 | 54.4 | 53.0 | 70.6 | 67.0 | 67.2 | 48.4 |\n|  | **CE (Dataset-wise Balanced)** | 18.0 | 23.6 | 27.0 | 52.6 | 54.2 | 52.6 | 68.6 | 66.8 | 67.0 | 47.8 |\n|  | **CE (Inst.-wise Balanced)** | 17.6 | 22.6 | 26.2 | 52.6 | 55.2 | 54.6 | 69.4 | 71.2 | 72.0 | 49.0 |", "caption": "Table 1: Different reward models\u2019 best-of-N sampling performance on MATH test set with three different generation models. When completing instructions with a temperature of 0.5, the three generation models\u2019 accuracies are 9.6%, 44.6%, and 63.2% respectively.", "description": "This table presents a comparison of various reward models' performance on the MATH dataset, using best-of-N sampling.  Three different large language models (LLMs) were used as generation models: Mistral-7B-Inst-v0.2, Llama-3.1-8B-Inst, and Llama-3.1-70B-Inst.  The models' individual pass@1 accuracies (when completing instructions with a temperature of 0.5) are shown in the caption (9.6%, 44.6%, and 63.2% respectively). The table shows the performance (pass@k, where k = 4, 16, and 64) of different reward models, categorized as outcome reward models (ORMs) and process reward models (PRMs), including both open-source and custom-implemented models. The goal is to evaluate the effectiveness of various reward models in improving the accuracy of the LLMs during best-of-N sampling.", "section": "4 Experiments"}, {"content": "| Setup | Mistral-7B-Inst-v0.2 |  |  | Llama-3.1-8B-Inst |  |  | Llama-3.1-70B-Inst |  |  | Avg. |\n|---|---|---|---|---|---|---|---|---|---|---|\n|  | @4 | @16 | @64 | @4 | @16 | @64 | @4 | @16 | @64 |  |\n| Implicit PRM | 18.6 | 24.4 | 28.8 | 54.0 | 55.4 | 57.0 | 71.8 | 71.2 | 72.2 | 49.3 |\n| + UltraFeedback | 19.4 | 24.4 | 29.0 | 53.8 | 55.0 | 55.8 | 71.6 | 70.6 | 72.2 | 49.2 |\n| + UltraInteract (Code) | 19.2 | 24.6 | 28.0 | 54.6 | 54.0 | 56.8 | 71.4 | 70.8 | 70.0 | 49.2 |\n| + Dedup. | 18.2 | 22.8 | 26.8 | 52.0 | 53.2 | 51.6 | 69.8 | 69.4 | 70.4 | 47.6 |\n| + Base Resp. | 17.8 | 23.2 | 27.6 | 54.0 | 55.0 | 54.8 | 71.4 | 72.4 | 73.2 | 48.7 |\n| + Step Label | 18.8 | 25.4 | 28.8 | 53.8 | 54.8 | 54.6 | 70.8 | 71.2 | 73.0 | 49.2 |", "caption": "Table 2: Factors that may affect PRM performance. To our surprise, none of them consistently improve our implicit PRM.", "description": "Table 2 investigates the impact of various factors on implicit Process Reward Model (PRM) performance.  It explores whether incorporating additional data (instructions from different sources, removal of duplicate responses, or replacement with diverse responses) or incorporating step-level labels from a Math-Shepherd model improves the performance of the implicit PRM. The results show that none of these factors consistently enhance the implicit PRM's performance. This suggests that the implicit PRM is surprisingly robust and data efficient, achieving good results even without these extra factors.", "section": "5.3 ARE THERE ANY OTHER FACTORS CAN IMPROVE IMPLICIT PRM PERFORMANCE?"}, {"content": "| Model | Accuracy |\n|---|---| \n| Llama-3.1-8B-Inst | 45.2 |\n| + DPO | 25.8 |\n| + KTO | 46.6 |\n| + NCA | 35.6 |\n| + CE | 28.6 |", "caption": "Table 3: \nImplicit PRMs\u2019 performance on MATH500 when used to solve the problems directly.", "description": "This table presents the results of using implicit PRMs (trained using different reward modeling objectives) as policy models to directly solve problems from the MATH500 dataset.  It contrasts the performance of the implicit PRMs, each instantiated with a different objective (DPO, KTO, NCA, and CE), against the baseline performance of Llama-3.1-8B-Inst to assess whether the ability of a model to function as an effective process reward model correlates with its ability to perform well as a policy model on the same tasks.", "section": "5.4 PRM ABILITY DOES NOT TRANSLATE INTO POLICY PERFORMANCE"}, {"content": "| Source of Cost | Method | Mistral-7B-Inst-v0.2 | Llama-3.1-8B-Inst | Llama-3.1-70B-Inst |\n|---|---|---|---|---|\n| Generation Model | - | 100.0 | 100.0 | 100.0 |\n| Reward Model | Baselines | 33.5 | 29.4 | 9.1 |\n|  | Implicit PRM | 201.6 | 141.7 | 22.2 |\n| Total | Baselines | 200.9 | 171.1 | 111.1 |\n|  | Implicit PRM | 301.6 | 241.7 | 122.2 |", "caption": "Table 4: GPU time costs during best-of-N sampling relative to the cost of generation model (%). The overall inference overhead of baselines on three test sets are 66.6%, 70.8%, and 90.9% of that of our implicit PRM, respectively. Namely, the reference model does not double the inference cost in practice, and the extra inference overhead becomes more marginal as the generation model gets larger.", "description": "This table compares the GPU time costs for best-of-N sampling using different reward models, relative to the time cost of the generation model.  It shows the breakdown of time spent on generation versus reward model calculation for several baseline methods and the proposed implicit PRM. The key finding is that the additional cost of the reference model in the implicit PRM does not significantly increase the overall inference time, especially when the generation model is substantially larger than the reward model.  The percentages provided illustrate that the baselines have considerably higher overall inference overheads than the implicit PRM.", "section": "5.5.1 THE REFERENCE MODEL DOES NOT DOUBLE OVERALL INFERENCE OVERHEAD"}, {"content": "| Setup | Mistral-7B-Inst-v0.2 | Llama-3.1-8B-Inst | Llama-3.1-70B-Inst | Avg. |\n|---|---|---|---|---|\n| Train | Inference | @4 | @16 | @64 | @4 | @16 | @64 | @4 | @16 | @64 |  |\n| Llama-3.1-8B-Instruct<br>w/o Ref | 14.8 | 16.2 | 18.4 | 49.0 | 50.4 | 52.2 | 69.6 | 71.0 | 71.0 | 45.8 |\n| + DPO w/ Ref<br>w/ Ref | 18.6 | 24.4 | 28.8 | 54.0 | 55.4 | 57.0 | 71.8 | 71.2 | 72.2 | 50.4 |\n| + DPO w/ Ref<br>w/o Ref | 17.8 | 23.4 | 27.8 | 54.2 | 56.6 | 57.6 | 71.6 | 73.6 | 73.2 | 50.6 |\n| + DPO w/o Ref<br>w/ Ref | 17.8 | 23.4 | 28.4 | 54.0 | 55.2 | 57.6 | 70.6 | 72.0 | 73.2 | 50.2 |\n| + DPO w/o Ref<br>w/o Ref | 17.4 | 22.6 | 25.6 | 54.8 | 56.4 | 58.2 | 70.4 | 73.2 | 74.0 | 50.3 |", "caption": "Table 5: Ablating reference model in both training and inference. Neither consistently hurts our implicit PRM. More surprisingly, the reference model, Llama-3.1-8B-Instruct, already perfroms well on Best-of-N sampling.", "description": "This table investigates the impact of removing the reference model from both the training and inference phases of the implicit PRM.  The results show that removing the reference model does not consistently harm the performance of the implicit PRM.  Surprisingly, the reference model (Llama-3.1-8B-Instruct) itself performs well in best-of-N sampling, suggesting that a strong pre-trained model might obviate the need for the extra reference model in certain situations. The table compares the best-of-N sampling performance on three different generation models (Mistral-7B-Inst-v0.2, Llama-3.1-8B-Inst, Llama-3.1-70B-Inst) under different setups: with and without the reference model during training and inference, along with the performance of the reference model used as a standalone reward model.", "section": "5.5 CAN WE REDUCE THE INFERENCE OVERHEAD OF THE REFERENCE MODEL?"}]