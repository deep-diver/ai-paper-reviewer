{"references": [{"fullname_first_author": "Mohammad Gheshlaghi Azar", "paper_title": "A general theoretical paradigm to understand learning from human preferences", "publication_date": "2024-00-00", "reason": "This paper provides a general theoretical framework for understanding learning from human preferences, which is relevant to the paper's exploration of reward models in reinforcement learning."}, {"fullname_first_author": "Meng Cao", "paper_title": "Enhancing reinforcement learning with dense rewards from language model critic", "publication_date": "2024-00-00", "reason": "This paper is highly relevant to the paper's focus on process reward models (PRMs) and their effectiveness in reinforcement learning."}, {"fullname_first_author": "Alex J. Chan", "paper_title": "Dense reward for free in reinforcement learning from human feedback", "publication_date": "2024-00-00", "reason": "This paper directly addresses the challenge of efficiently training PRMs by proposing a method to obtain dense rewards without additional cost, aligning with the core focus of the target paper."}, {"fullname_first_author": "Huayu Chen", "paper_title": "Noise contrastive alignment of language models with explicit rewards", "publication_date": "2024-00-00", "reason": "This paper explores reward parameterization techniques for language models, a key aspect of the target paper's proposed method for implicitly learning PRMs."}, {"fullname_first_author": "Rafael Rafailov", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "publication_date": "2023-00-00", "reason": "This paper introduces the concept of direct preference optimization, which is closely related to the target paper's approach of implicitly learning PRMs through reward parameterization."}]}