[{"content": "| Models | Data Processing Pipeline | Reproducible Pretraining Dataset | Large-scale SFT Dataset (&gt;1M) | Intermediate Checkpoints | Training Tokens | HumanEval Pass@1 |\n|---|---|---|---|---|---|---|\n| _Open Model Weights & Reproducible Datasets_ |\n| OpenCoder-8B | \u2713 | \u2713 | \u2713 | \u2713 | 2.5T | 83.5 |\n| StarCoder2-15B | \u2713 | \u2713 | \u2717 | \u2717 | 4.1T | 72.6 |\n| Crystal-7B | \u2717 | \u2713 | \u2717 | \u2713 | 1.3T | 34.1 |\n| _Open Model Weights_ |\n| CodeLlama-7B | \u2717 | \u2717 | \u2717 | \u2717 | 2.5T | 34.8 |\n| CodeGemma-7B | \u2717 | \u2717 | \u2717 | \u2717 | 6.5T | 56.1 |\n| DS-Coder-V2-Lite | \u2717 | \u2717 | \u2717 | \u2717 | 10.2T | 81.1 |\n| Yi-Coder-9B | \u2717 | \u2717 | \u2717 | \u2717 | 6.0T | 85.4 |\n| Qwen2.5-Coder-7B | \u2717 | \u2717 | \u2717 | \u2717 | 23.5T | 88.4 |", "caption": "Table 1: The comparison of released resources between our OpenCoder with other popular open-sourced code LLMs. HumanEval scores are reported for the corresponding chat models.", "description": "This table compares the resources released by the OpenCoder large language model (LLM) with those of other popular open-source code LLMs.  The comparison includes whether the model weights, intermediate checkpoints, the training dataset, the data processing pipeline, and a large-scale supervised fine-tuning (SFT) dataset are publicly available.  HumanEval Pass@1 scores (a measure of code generation performance) for the corresponding chat models are also provided. This allows for a comprehensive assessment of the openness and reproducibility of each LLM and allows researchers to easily compare the performance and capabilities of different models.", "section": "2 Pretraining Data"}, {"content": "| Category | Data Source | # Tokens | Percentage |\n|---|---|---|---| \n| Raw Code Data | Github Code | 755 B | 78.4% |\n|  | Jupyter Notebooks | 11 B | 1.1% |\n|  | The Stack v2 | 120 B | 12.5% |\n| Code-related Web Data | Processed CC | 13 B | 1.4% |\n|  | Processed SkyPile | 3 B | 0.3% |\n|  | Processed FineWeb | 55 B | 5.7% |\n| OpenSource Data | Processed AutoMathText | 3 B | 0.3% |", "caption": "Table 2: The Composition of RefineCode.", "description": "Table 2 presents a breakdown of the RefineCode dataset, detailing the composition of its different data sources and their respective sizes (in tokens and percentage).  It shows how much of RefineCode comes from GitHub code, Jupyter Notebooks, The Stack v2 dataset, and different processed web corpora. This provides crucial context for understanding the dataset's scale and diversity, and how various sources contributed to the final dataset.", "section": "2 Pretraining Data"}, {"content": "| Category | Dataset | # Token |\n|---|---|---|\n| Original Data | RefineCode | 84.21 B |\n|  | Algorithmic Corpus | 12.44 B |\n| Synthetic Data | High Quality Code Snippet | 2.71 B |\n|  | Code Textbooks | 0.91 B |", "caption": "Table 3: Detailed data mixture for annealing data.", "description": "This table details the composition of the data used in the annealing phase of the OpenCoder model's training.  It breaks down the total number of tokens contributed by different data sources:  the original RefineCode dataset, algorithmically generated code, high-quality synthetic code snippets, and code textbooks.  The proportions of each dataset are shown to illustrate the mixture of data used to fine-tune the model during the annealing stage.", "section": "2 Pretraining Data"}, {"content": "| Model Parameter | OpenCoder-1.5B | OpenCoder-8B |\n|---|---|---|\n| Layers | 24 | 32 |\n| Model Dimension | 2240 | 4096 |\n| Attention Heads | 14 | 32 |\n| Key / Value Heads | 14 | 8 |\n| Activation Function | SwiGLU | SwiGLU |\n| Vocab Size | 96640 | 96640 |\n| Positional Embedding | RoPE(\u03b8=10000) | RoPE(\u03b8=500000) |\n| Context Window Size | 4096 | 8192 |", "caption": "Table 4: Overview of the key hyperparameters of OpenCoder, including 1.5B and 8B.", "description": "This table details the key architectural hyperparameters of the two OpenCoder models: the 1.5 billion parameter model and the 8 billion parameter model.  It provides a comparison of their configurations, including the number of layers, hidden dimension size, number of attention heads, activation function used, vocabulary size, and context window size. This information is crucial for understanding the differences in model capacity and computational requirements between the two variants.", "section": "3 Pretraining"}, {"content": "| Stage | Data Source | # Examples |\n|---|---|---|\n| Stage1 | RealUser-Instruct | 0.7 M |\n|  | Large-scale Diverse-Instruct | 2.3 M |\n|  | Filtered Infinity-Instruct | 1.0 M |\n| Stage2 | McEval-Instruct | 36 K |\n|  | Evol-Instruct | 111 K |\n|  | Educational-Instruct | 110 K |\n|  | Package-Instruct | 110 K |", "caption": "Table 5: Detailed data composition of our two-stage instruction-tuning.", "description": "This table details the data used in the two-stage instruction tuning process for the OpenCoder model.  Stage 1 focuses on general theoretical computer science concepts, while Stage 2 concentrates on practical coding tasks using high-quality code from GitHub.  The table lists the data source and the number of examples for each stage of the tuning process. This two-stage approach aims to enhance the model's abilities in both theoretical understanding and practical code generation.", "section": "4 Post Training"}, {"content": "| Model | Size | HumanEvalHE | HumanEvalHE+ | MBPP | MBPP+ | MBPP3-shot | MBPPFull | BigCodeBenchHard | BigCodeBench | \n|---|---|---|---|---|---|---|---|---|---| \n| **1B+ Models** |  |  |  |  |  |  |  |  |  | \n| DeepSeek-Coder-1.3B-Base | 1.3B | 34.8 | 26.8 | 55.6 | 46.9 | 46.2 | 26.1 | 3.4 |  | \n| Yi-Coder-1.5B | 1.5B | 41.5 | 32.9 | 27.0 | 22.2 | 51.6 | 23.5 | 3.4 |  | \n| CodeGemma-2B | 2B | 31.1 | 16.5 | 51.1 | 43.1 | 45.4 | 23.9 | 7.4 |  | \n| Qwen2.5-Coder-1.5B | 1.5B | 43.9 | 36.6 | 69.2 | 58.6 | **59.2** | **34.6** | **9.5** |  | \n| StarCoder2-3B | 3B | 31.7 | 27.4 | 60.2 | 49.1 | 46.4 | 21.4 | 4.7 |  | \n| OpenCoder-1.5B-Base | 1.5B | **54.3** | **49.4** | **70.6** | **58.7** | 51.8 | 24.5 | 5.4 |  | \n| **6B+ Models** |  |  |  |  |  |  |  |  |  | \n| CodeLlama-7B | 7B | 33.5 | 26.2 | 55.3 | 46.8 | 41.4 | 28.7 | 5.4 |  | \n| CodeGemma-7B | 7B | 39.0 | 32.3 | 50.5 | 40.7 | 55.0 | 38.3 | 10.1 |  | \n| DS-Coder-6.7B-Base | 6.7B | 47.6 | 39.6 | 70.2 | 56.6 | 60.6 | 41.1 | 11.5 |  | \n| DS-Coder-V2-Lite-Base(MoE) | 16B | 40.9 | 34.1 | 71.9 | 59.4 | 62.6 | 30.6 | 8.1 |  | \n| CodeQwen1.5-7B-Base | 7B | 51.8 | 45.7 | 72.2 | 60.2 | 61.8 | 45.6 | 15.6 |  | \n| Yi-Coder-9B | 9B | 53.7 | 46.3 | 48.4 | 40.7 | **69.4** | 42.9 | 14.2 |  | \n| Qwen2.5-Coder-7B-Base | 7B | 61.6 | 53.0 | 76.9 | 62.9 | 68.8 | **45.8** | **16.2** |  | \n| Crystal-7B | 7B | 22.6 | 20.7 | 38.6 | 31.7 | 31.0 | 10.8 | 4.1 |  | \n| StarCoder2-7B | 7B | 35.4 | 29.9 | 54.4 | 45.6 | 55.2 | 27.7 | 8.8 |  | \n| StarCoder2-15B | 15B | 46.3 | 37.8 | 66.2 | 53.1 | 15.2 | 38.4 | 12.2 |  | \n| OpenCoder-8B-Base | 8B | **68.9** | **63.4** | **79.9** | **70.4** | 60.6 | 40.5 | 9.5 |  | ", "caption": "Table 6: Performance of various base models on HumanEval, MBPP, and the \u201ccomplete\u201d task of BigCodeBench. Models trained on reproducible datasets are marked with green.", "description": "Table 6 presents a comparative analysis of various base code language models' performance on three prominent benchmarks: HumanEval, MBPP, and BigCodeBench's \"complete\" task.  The table highlights the performance scores achieved by each model across these benchmarks.  Models trained using openly accessible and reproducible datasets are visually distinguished with a green marker, emphasizing the importance of transparency and reproducibility in model development. This comparison allows for a nuanced understanding of the relative strengths and weaknesses of different code models and the impact of data availability on model performance.", "section": "5 Experimental Results"}, {"content": "| Model | Size | HumanEval HE | HumanEval HE+ | MBPP MBPP | MBPP MBPP+ | BigCodeBench Full | BigCodeBench Hard | LiveCodeBench Avg |\n|---|---|---|---|---|---|---|---|---|\n| **1B+ Models** |  |  |  |  |  |  |  |  |\n| DS-coder-1.3B-Instruct | 1.3B | 65.2 | 61.6 | 61.6 | 52.6 | 22.8 | 3.4 | 9.3 |\n| Qwen2.5-Coder-1.5B-Instruct | 1.5B | 70.7 | 66.5 | 69.2 | 59.4 | 32.5 | 6.8 | **15.7** |\n| Yi-Coder-1.5B-Chat | 1.5B | 67.7 | 63.4 | 68.0 | 59.0 | 24.0 | 6.8 | 11.6 |\n| OpenCoder-1.5B-Instruct | 1.5B | **72.5** | **67.7** | **72.7** | **61.9** | **33.3** | **11.5** | 12.8 |\n| **6B+ Models** |  |  |  |  |  |  |  |  |\n| DS-Coder-V2-Lite-Instruct | 16B | 81.1 | 75.0 | 82.3 | 68.8 | 36.8 | 16.2 | 24.3 |\n| CodeLlama-7B-Instruct | 7B | 45.7 | 39.6 | 39.9 | 33.6 | 21.9 | 3.4 | 2.8 |\n| CodeGemma-7B-It | 7B | 59.8 | 47.0 | 69.8 | 59.0 | 32.3 | 7.4 | 14.7 |\n| DS-Coder-6.7B-Instruct | 6.7B | 78.6 | 70.7 | 75.1 | 66.1 | 35.5 | 10.1 | 20.5 |\n| Yi-Coder-9B-Chat | 9B | 82.3 | 72.6 | 81.5 | 69.3 | 38.1 | 11.5 | 23.4 |\n| CodeQwen1.5-7B-Chat | 7B | 86.0 | 79.3 | 83.3 | 71.4 | 39.6 | **18.9** | 20.1 |\n| Qwen2.5-Coder-7B-Instruct | 7B | **88.4** | **84.1** | **83.5** | **71.7** | **41.0** | 18.2 | **37.6** |\n| CrystalChat-7B | 7B | 34.1 | 31.7 | 39.1 | 32.7 | 26.7 | 2.3 | 6.1 |\n| StarCoder2-15B-Instruct-v0.1 | 15B | 72.6 | 63.4 | 75.2 | 61.2 | 37.6 | 12.2 | 20.4 |\n| OpenCoder-8B-Instruct | 8B | 83.5 | 78.7 | 79.1 | 69.0 | 40.3 | 16.9 | 23.2 |", "caption": "Table 7: Performance of various chat models on HumanEval, MBPP, the \u201cinstruct\u201d task of BigCodeBench and LiveCodeBench. Models trained on reproducible datasets are marked with green.", "description": "This table compares the performance of different chat models on four code-related benchmarks: HumanEval, MBPP, BigCodeBench's \"instruct\" task, and LiveCodeBench.  It shows the Pass@1 scores (percentage of correctly solved problems) for each model across these benchmarks. The table highlights models trained using publicly available data (reproducible datasets) in green to emphasize the transparency and reproducibility of their training processes.  The benchmarks cover different aspects of code understanding and generation ability.", "section": "5 Experimental Results"}, {"content": "| Model | Size | Python | Java | C++ | C# | TS | JS | PHP | Bash | Average |\n|---|---|---|---|---|---|---|---|---|---|---|\n| **1B+ Models** |  |  |  |  |  |  |  |  |  |  |\n| DS-Coder-1.3B-Instruct | 1.3B | 65.2 | 51.9 | 45.3 | 55.1 | 59.7 | 52.2 | 45.3 | 12.7 | 48.4 |\n| Yi-Coder-1.5B-Chat | 1.5B | 67.7 | 51.9 | 49.1 | 57.6 | 57.9 | 59.6 | 52.2 | 19.0 | 51.9 |\n| Qwen2.5-Coder-1.5B-Instruct | 1.5B | 71.2 | 55.7 | 50.9 | 64.6 | 61.0 | 62.1 | 59.0 | 29.1 | 56.7 |\n| OpenCoder-1.5B-Instruct | 1.5B | 72.5 | 64.6 | 50.9 | 61.4 | 63.5 | 62.1 | 55.3 | 29.7 | 57.5 |\n| **6B+ Models** |  |  |  |  |  |  |  |  |  |  |\n| DS-Coder-6.7B-Instruct | 6.7B | 78.6 | 68.4 | 63.4 | 72.8 | 67.2 | 72.7 | 68.9 | 36.7 | 66.1 |\n| DS-Coder-V2-Lite-Instruct | 16B | 81.1 | 76.6 | 75.8 | 76.6 | 80.5 | 77.6 | 74.5 | 43.0 | 73.2 |\n| CodeLlama-7B-Instruct | 7B | 45.7 | 32.2 | 28.6 | 32.9 | 39.0 | 43.5 | 31.7 | 10.1 | 33.0 |\n| CodeGemma-7B-It | 7B | 59.8 | 48.1 | 46.6 | 51.9 | 54.7 | 54.0 | 46.6 | 10.1 | 46.5 |\n| CodeQwen1.5-7B-Chat | 7B | 83.5 | 70.9 | 72.0 | 75.9 | 76.7 | 77.6 | 73.9 | 41.8 | 71.6 |\n| Yi-Coder-9B-Chat | 9B | 85.4 | 76.0 | 67.7 | 76.6 | 72.3 | 78.9 | 72.1 | 45.6 | 71.8 |\n| Qwen2.5-Coder-7B-Instruct | 7B | 87.8 | 76.5 | 75.6 | 80.3 | 81.8 | 83.2 | 78.3 | 48.7 | 76.5 |\n| OpenCoder-8B-Instruct | 8B | 83.5 | 72.2 | 61.5 | 75.9 | 78.0 | 79.5 | 73.3 | 44.3 | 71.0 |", "caption": "Table 8: Performance of various chat models on the MultiPL-E benchmark across different programming languages.", "description": "This table presents a comprehensive comparison of different large language models (LLMs) on their ability to generate code in multiple programming languages.  The MultiPL-E benchmark evaluates the models' performance across various languages, providing insights into their cross-lingual code generation capabilities and identifying strengths and weaknesses in handling different programming paradigms and syntaxes. The table shows the performance metrics for each model across various languages, offering a detailed analysis of the models' proficiency in multilingual code generation.", "section": "5 Experimental Results"}, {"content": "| Deduplication Level | # Total Rows | # Retained Rows | # Retained Tokens |\n|---|---|---|---|\n| File level | 485,817,123 | 30,488,834 | 32.74 B |\n| Repository level | 11,037,352 | 7,480,488 | 99.47 B |", "caption": "Table 9: The statistics for file level deduplication and repository level deduplication on Python code. Rows for file level and repository level represent the number of files and repositories, respectively.", "description": "This table presents a comparison of file-level and repository-level deduplication techniques applied to a Python code dataset.  It shows the initial number of files and repositories, the number of files and repositories retained after deduplication, and the total number of tokens retained. This comparison highlights the impact of different deduplication strategies on data size and potentially on model training performance. The results are crucial for understanding the trade-offs between data size reduction and data diversity in building code large language models (LLMs).", "section": "6 Analysis"}, {"content": "|               | HE   | HE+  | MBPP | MBPP+ | BigCodeBench | Code Arena |\n| :------------ | :---: | :---: | :---: | :---: | :------------: | :--------: |\n| Stage1        | 52.4 | 48.1 | 68.7 | 57.4 |      22.1      |     5.3     |\n| Stage1 + Stage2 | **70.1** | **64.0** | **74.6** | **64.8** |     **31.5**     |     **6.9**     |\n| Mix Training   | 55.5 | 51.2 | 52.0 | 58.7 |      23.9      |     3.8     |", "caption": "Table 10: Performance of different training strategies across benchmarks. Mix Training refers to the process of combining and shuffling the data from Stage 1 and Stage 2 for joint training.", "description": "This table compares the performance of three different instruction tuning strategies for a 1.5B parameter language model: training only on Stage 1 data, training on both Stage 1 and Stage 2 data sequentially, and training on a mixture of both Stage 1 and Stage 2 data.  The comparison is made across multiple code generation benchmarks (HumanEval, HumanEval+, MBPP, MBPP+, BigCodeBench, and Code Arena).  The results show the impact of different data compositions and training approaches on the model's ability to generate high-quality code.", "section": "4 Post Training"}, {"content": "| Description | Explanation | Filtering Quota |\n|---|---|---|\n| The proportion of lines in strings with a word count exceeding. | Files with too many long strings indicate a lack of code logic. | score \">\" 0.2 |\n| The proportion of characters in words from strings with a character count exceeding 20. | String variables containing long sequences of characters are often indicative of meaningless content such as base64 data, Hash encoding, url, etc. | score \">\" 0.4 |\n| The proportion of hexadecimal characters. | Files with two many hexadecimal characters indicate a lack of code logic. | score \">\" 0.4 |\n| The proportion of lines like \"you code here\", \"TODO\" or \"FIXME\". | We found that these elements tend to be excessively repeated in the dataset, which increases the likelihood that the model, during code completion, will output placeholders like the ones mentioned above instead of generating actual code. | score \">\" 0.01 |\n| The proportion of lines containing an \"assert\" statement. | Files containing a large number of \u2019assert\u2019 statements are often test files, which tend to have relatively simple and repetitive code patterns. | score \">\" 0.4 |", "caption": "Table 11: Examples of general code filtering rules.", "description": "Table 11 presents examples of general heuristic filtering rules used in the data cleaning pipeline.  These rules are not language-specific and apply to various code files.  The table details the specific criteria used in the filtering process, along with an explanation and the filtering threshold value used for each rule.  These rules aim to remove low-quality code, such as those with excessive long strings, hexadecimal characters, or comments like \"You code here\". The filtering quota is a score that helps to evaluate how well the rule performs. The goal is to identify and remove code that contains low-quality or non-informative elements to improve overall data quality for model training.", "section": "A Filtering Rules"}, {"content": "| Description | Explanation | Filtering Quota |\n|---|---|---|\n| The proportion of the number of python functions to the total number of lines. | A higher number of Python functions in a file may indicate that the functions are overly simple, with limited code logic, or have a bad code format. | score > 0.2 |\n| Whether the file can be parsed into an python abstract syntax tree (AST). | Files that cannot be parsed into an AST contain syntax errors and should be filtered out. | score == False |\n| The proportion of lines that are \"import\" statements. | A file with exceeding prportion of \"import\" statements indicates to have sparse code logic. | score > 0.3 |", "caption": "Table 12: Examples of python-specific filtering rules.", "description": "Table 12 presents examples of filtering rules specifically designed for Python code within the data preprocessing pipeline. These rules leverage Python-specific syntax and characteristics to identify and remove low-quality code snippets, improving the overall quality of the training dataset.  Each rule includes a description of the characteristic being checked, an explanation of why that characteristic is indicative of low-quality code, and the filtering threshold applied.", "section": "A Filtering Rules"}, {"content": "| Level | # Total Lines | # Retained Lines | # Retained Tokens |\n|---|---|---|---| \n| Chunk-level | 333,007,812 | 79,272,460 | 324.70 B |\n| File-level | 485,817,123 | 30,488,834 | 32.74 B |\n| File-level + Chunk-level | 333,007,812 | 7,993,164 | 32.70 B |\n| Repo-level | 11,037,352 | 7,480,488 | 99.47 B |\n| Repo-level + Chunk-level | 333,007,812 | 17,675,781 | 72.40 B |", "caption": "Table 13: Comparison of deduplication strategies on Python data. At the File level, \"Lines\" refers to the number of lines in individual files; at the Repo level, it indicates the line count of aggregated strings; Note that for all deduplication strategies involving the Chunk level, \"Lines\" specifically refers to 4096-token chunks.", "description": "This table compares different deduplication methods used on Python code data for model training. It shows the total number of lines of code before deduplication, the number of lines retained after applying various deduplication strategies (file-level, repository-level, and chunk-level), and the resulting number of tokens.  The key difference is how deduplication is performed: file-level considers individual files, repository-level treats all files within a repository as one unit, and chunk-level works on 4096-token segments of code. The table clarifies the line count units for each strategy to avoid ambiguity.", "section": "B Analysis on Chunk-level Deduplication"}, {"content": "| Domain | Prefix | Tag |\n|---|---|---|\n| cloud.tencent.com | %cloud.tencent.com/developer/article% | Code |\n| cloud.tencent.com | %cloud.tencent.com/ask% | Code |\n| cloud.tencent.com | %cloud.tencent.com/developer/information% | Code |\n| cloud.tencent.com | %cloud.tencent.com/document% | Code |\n| my.oschina.net | %my.oschina.net%blog% | Code |\n| ask.csdn.net | %ask.csdn.net/questions% | Code |\n| www.cnblogs.com | %www.cnblogs.com% | Code |\n| forum.ubuntu.org.cn | %forum.ubuntu.org.cn% | Code |\n| q.cnblogs.com | %q.cnblogs.com/q% | Code |\n| segmentfault.com | %segmentfault.com/q% | Code |\n| segmentfault.com | %segmentfault.com/a% | Code |\n| woshipm.com | %woshipm.com/data-analysis% | Code |\n| zgserver.com | %zgserver.com/server% | Code |\n| zgserver.com | %zgserver.com/linux% | Code |\n| zgserver.com | %zgserver.com/ubuntu% | Code |\n| juejin.cn | %juejin.cn/post% | Code |\n| jiqizhixin.com | %jiqizhixin.com/articles% | Code |\n| help.aliyun.com | %help.aliyun.com/zh% | Code |\n| jyeoo.com | %jyeoo.com% | Math |\n| www.haihongyuan.com | %haihongyuan.com%shuxue% | Math |\n| www.03964.com | %www.03964.com% | Math |\n| www.nbhkdz.com | %www.nbhkdz.com% | Math |\n| 9512.net | %9512.net% | Math |\n| lanxicy.com | %lanxicy.com% | Math |\n| bbs.emath.ac.cn | %bbs.emath.ac.cn% | Math |\n| math.pro | %math.pro% | Math |\n| mathschina.com | %mathschina.com% | Math |\n| shuxue.chazidian.com | %shuxue.chazidian.com% | Math |\n| shuxue.ht88.com | %shuxue.ht88.com% | Math |", "caption": "Table 14: We manually annotate code-like and math-like Chinese domains, utilizing the \u2019%\u2019 symbol as a wildcard in our pattern matching. For example, the URL \u2019https://my.oschina.net/u/4/blog/11\u2019 is matched by the pattern \u2019%my.oschina.net%blog%\u2019.", "description": "This table details the manually annotated Chinese web domains categorized as either code-related or math-related.  The annotation uses the '%' symbol as a wildcard to match URL patterns, allowing for flexible identification of relevant domains.  For example, the pattern '%my.oschina.net%blog%' would match URLs like 'https://my.oschina.net/u/4/blog/11'. This list of domains was used as seed data for identifying similar web pages during data collection.", "section": "C Extra Data Processing"}, {"content": "| Model | # Tokens | # Languages | # Web Data Tokens | # Rules | LS Rules |\n|---|---|---|---|---|---| \n| The Stack v1 | 200 B | 88 | \\ | ~15 | \u2717 |\n| The Stack v2 | 900 B | 619 | ~30 B | ~15 | \u2717 |\n| **RefineCode** | 960 B | 607 | ~75 B | ~130 | \u2713 |", "caption": "Table 15: The Comparison of training data between RefineCode and series of The Stack. \u201cLS\u201d denotes \u201cLanguage Specific\u201d.", "description": "This table compares the training data used in RefineCode with that of two previous versions of The Stack dataset.  It highlights key differences in the size of the datasets (measured in tokens and the number of programming languages included), and details the number of filtering rules applied during dataset creation. Importantly, it notes whether language-specific rules were used in the process, indicating a more sophisticated approach to data refinement in RefineCode compared to The Stack.", "section": "2 Pretraining Data"}, {"content": "| Language | # Files (After deduplication) | Vol(GB) (After deduplication) | Ratio(%) (After deduplication) | # Files (After filtering) | Vol(GB) (After filtering) | Ratio(%) (After filtering) |\n|---|---|---|---|---|---|---|\n| html | 141,081,897 | 3,175.4 | 8.56 | 45,100,466 | 582.4 | 18.08 |\n| java | 215,177,833 | 706.8 | 1.90 | 124,751,295 | 474.3 | 14.72 |\n| python | 109,725,362 | 493.3 | 1.33 | 58,640,346 | 271.1 | 8.41 |\n| csharp | 88,825,202 | 364.2 | 0.98 | 57,910,485 | 232.4 | 7.21 |\n| javascript | 190,670,421 | 1,925.0 | 5.19 | 69,579,517 | 226.9 | 7.04 |\n| php | 84,378,361 | 374.4 | 1.01 | 60,089,397 | 222.7 | 6.91 |\n| cpp | 51,362,503 | 375.2 | 1.01 | 38,037,406 | 176.9 | 5.49 |\n| go | 35,649,865 | 301.1 | 0.81 | 26,723,829 | 153.7 | 4.77 |\n| typescript | 40,211,985 | 287.4 | 0.77 | 20,621,755 | 140.4 | 4.35 |\n| ruby | 15,735,042 | 244.5 | 0.66 | 8,285,561 | 122.7 | 3.81 |\n| perl | 16,354,543 | 121.7 | 0.33 | 9,532,620 | 65.6 | 2.04 |\n| rust | 10,605,421 | 63.6 | 0.17 | 6,086,150 | 39.9 | 1.24 |\n| r | 6,132,978 | 92.5 | 0.25 | 4,803,109 | 34.7 | 1.08 |\n| swift | 4,238,754 | 47.9 | 0.13 | 2,938,498 | 31.8 | 0.99 |\n| kotlin | 4,493,548 | 56.4 | 0.15 | 3,123,156 | 29.8 | 0.94 |\n| dart | 4,087,329 | 33.0 | 0.09 | 2,161,462 | 18.5 | 0.57 |\n| java-pages | 6,174,654 | 31.0 | 0.08 | 4,145,336 | 15.4 | 0.48 |\n| css | 39,822,744 | 241.5 | 0.65 | 15,771,061 | 15.3 | 0.47 |\n| lua | 4,027,221 | 116.0 | 0.31 | 2,538,234 | 14.4 | 0.45 |\n| xml | 61,171,289 | 1,934.2 | 5.21 | 3,173,128 | 12.8 | 0.40 |\n| scala | 5,897,567 | 19.7 | 0.05 | 4,204,979 | 11.7 | 0.36 |\n| shell | 12,054,632 | 23.0 | 0.06 | 6,043,070 | 11.2 | 0.35 |\n| pascal | 1,306,130 | 27.8 | 0.07 | 960,497 | 9.5 | 0.29 |\n| fortran | 2,274,663 | 39.7 | 0.10 | 1,218,491 | 8.6 | 0.27 |\n| perl6 | 1,943,430 | 16.4 | 0.04 | 1,034,748 | 8.6 | 0.27 |\n| rmarkdown | 1,317,760 | 14.0 | 0.04 | 827,951 | 7.9 | 0.25 |\n| html+erb | 7,618,377 | 11.4 | 0.03 | 4,452,355 | 7.8 | 0.24 |\n| smali | 3,457,531 | 37.9 | 0.10 | 1,408,274 | 7.4 | 0.23 |\n| scss | 18,061,278 | 35.6 | 0.10 | 7,705,822 | 7.4 | 0.23 |\n| gettext catalog | 1,100,044 | 51.3 | 0.14 | 442,385 | 6.3 | 0.19 |\n| haskell | 1,746,444 | 24.0 | 0.06 | 1,218,491 | 6.8 | 0.27 |\n| tcl | 253,345 | 4.2 | 0.01 | 136,171 | 1.0 | 0.03 |\n| gradle | 2,431,985 | 2.9 | 0.01 | 724,609 | 1.0 | 0.03 |\n| scheme | 357,909 | 4.7 | 0.01 | 201,170 | 1.0 | 0.03 |\n| qml | 354,756 | 1.8 | 0.01 | 252,621 | 1.0 | 0.03 |\n| mdx | 795,525 | 6.4 | 0.17 | 222,013 | 1.0 | 0.03 |\n| classic asp | 220,344 | 2.8 | 0.08 | 141,236 | 0.9 | 0.03 |\n| xbase | 192,780 | 2.5 | 0.07 | 80,396 | 0.9 | 0.03 |\n| ini | 7,232,136 | 19.1 | 0.05 | 1,517,099 | 1.3 | 0.04 |\n| objective-c++ | 197,416 | 2.4 | 0.01 | 149,223 | 1.3 | 0.04 |\n| motorola68k | 1,066,095 | 26.5 | 0.07 | 220,218 | 1.2 | 0.04 |\n| gap | 752,261 | 2.6 | 0.01 | 510,420 | 1.2 | 0.04 |", "caption": "Table 16: Overview of the data composition of in RefineCode. The items in the table are sorted in descending order according to the file volume after filtering.", "description": "Table 16 presents a detailed breakdown of the composition of the RefineCode dataset, specifically focusing on the top 85 programming languages. It shows the number of files and the volume (in GB) before and after deduplication and filtering for each language. The languages are listed in descending order based on their file volume after the filtering process, offering insights into the data's distribution and the impact of data cleaning steps.", "section": "2 Pretraining Data"}]