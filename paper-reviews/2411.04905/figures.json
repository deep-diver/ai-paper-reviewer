[{"figure_path": "https://arxiv.org/html/2411.04905/x1.png", "caption": "Figure 1: OpenCoder surpasses all previous fully open models (i.e., with open model weights and reproducible datasets) and other open-access models (i.e., with open model weights only) at the 6B+ parameter scale, pushing the frontier of fully open models to new heights.", "description": "The figure shows a graph comparing the performance of OpenCoder with other large language models (LLMs) for code.  The x-axis represents the number of training tokens (in billions), and the y-axis represents the MBPP Pass@1 (%) metric for a 1.5B parameter model and HumanEval (Zero-shot Pass@1) for 6B+ parameter models.  OpenCoder significantly outperforms other fully open models (those with both open weights and reproducible datasets) and open-access models (those with only open weights) in both metrics, indicating its superior performance and the value of its fully open nature. The graph visually demonstrates OpenCoder pushing the frontier of fully open models to new heights.", "section": "Experimental Results"}, {"figure_path": "https://arxiv.org/html/2411.04905/x2.png", "caption": "Figure 2: The illustration of our pretraining data processing workflow.", "description": "This figure shows the data processing pipeline for the pretraining stage of the OpenCoder model. It details the steps involved in creating a high-quality dataset for training, starting from raw code data and code-related web data. The pipeline involves several key stages, including preprocessing, deduplication, transformation, filtering, and data sampling, each designed to improve data quality and remove undesirable elements.  The left panel focuses on processing the raw code data, while the right panel demonstrates the processing of code-related web data. This figure helps illustrate the comprehensive approach OpenCoder takes to creating a reliable and effective pretraining dataset.", "section": "2 Pretraining Data"}, {"figure_path": "https://arxiv.org/html/2411.04905/x3.png", "caption": "Figure 3: Visualization on the PCA data distributions of RefineCode and The Stack v2.", "description": "This figure uses Principal Component Analysis (PCA) to visualize the differences in data distribution between the RefineCode dataset and the Stack v2 dataset.  RefineCode, a dataset created by the authors, is designed to be higher quality than Stack v2.  The PCA plot shows distinct clusters for the two datasets, indicating that they have different characteristics.  RefineCode's data points are more tightly clustered, suggesting greater homogeneity and higher quality, while Stack v2's points are more scattered, suggesting greater heterogeneity and potentially lower quality. The plot helps illustrate the authors' claim of creating a more refined and homogenous dataset suitable for training high-performing code LLMs.", "section": "2 Pretraining Data"}, {"figure_path": "https://arxiv.org/html/2411.04905/x4.png", "caption": "Figure 4: The distribution of top program languages in RefineCode.", "description": "This bar chart visualizes the distribution of the top programming languages included in the RefineCode dataset, a crucial component of the OpenCoder large language model.  The x-axis lists the programming languages, and the y-axis displays two metrics: the total file size (in gigabytes) and the number of files for each language. This illustrates the relative prevalence of different languages within the dataset, providing insights into the dataset's composition and potential biases or strengths that could influence the model's capabilities in various programming languages.", "section": "2 Pretraining Data"}, {"figure_path": "https://arxiv.org/html/2411.04905/x5.png", "caption": "Figure 5: The illustration of our instruction data synthesis workflow.", "description": "This figure illustrates the three different methods used to synthesize the instruction data for training OpenCoder.  (a) shows large-scale diverse instruction synthesis, leveraging a filtered web corpus, task-specific prompt engineering, and answer generation from an LLM. (b) details educational instruction synthesis, starting from a seed corpus, incorporating LLM prompt engineering, test case generation, code verification, and ultimately creating educational instructions. Finally, (c) illustrates package-related instruction synthesis that leverages pretraining and package corpora, employing retrieval, prompt engineering, and generating package instructions.", "section": "4 Post Training"}, {"figure_path": "https://arxiv.org/html/2411.04905/x6.png", "caption": "Figure 6: The McEval performance of OpenCoder-8B-Instruct in comparison to other open-source code models of comparable size.", "description": "Figure 6 presents a detailed comparison of the performance of OpenCoder-8B-Instruct against other open-source, similarly sized code models on the McEval benchmark. McEval is a comprehensive multilingual code evaluation benchmark that assesses various coding capabilities across 40 programming languages. The figure provides a visual representation of each model's performance across different languages, allowing for a direct comparison of their strengths and weaknesses in various coding contexts. This is particularly useful for identifying potential areas for improvement or specialization in multilingual code generation.", "section": "5 Experimental Results"}, {"figure_path": "https://arxiv.org/html/2411.04905/x7.png", "caption": "Figure 7: The MdEval performance of OpenCoder-8B-Instruct in comparison to other open-source code models of comparable size.", "description": "This figure presents a bar chart comparing the performance of various open-source code large language models (LLMs) on the MdEval benchmark.  MdEval is a multilingual code debugging benchmark that assesses a model's ability to identify and fix bugs in code across different programming languages.  The chart shows the average performance across multiple languages, with separate bars for each language highlighting the relative strengths and weaknesses of each LLM.  OpenCoder-8B-Instruct is included, and its performance is compared to that of other models of similar size.  The chart visually demonstrates the relative performance of OpenCoder-8B-Instruct compared to competing LLMs on a challenging, multilingual code debugging task.", "section": "5 Experimental Results"}, {"figure_path": "https://arxiv.org/html/2411.04905/x8.png", "caption": "Figure 8: Impact of using different deduplication strategies.", "description": "This figure compares the performance of different deduplication strategies on code datasets used for training large language models.  Two different metrics (HumanEval and MBPP) measuring code generation performance are shown, plotted against the number of training tokens used after applying either file-level or repository-level deduplication.  The results illustrate the impact of the chosen deduplication method on the final model's performance.", "section": "6 Analysis"}, {"figure_path": "https://arxiv.org/html/2411.04905/x9.png", "caption": "Figure 9: Impact of using high-quality data in the annealing stage.", "description": "This figure displays the impact of incorporating high-quality data during the annealing phase of the model's training. Two 1.5B parameter LLMs were trained, one with the original annealing data and another without the high-quality components (Algorithmic Corpus and Synthetic Data). The plots show the performance of both models on the HumanEval and MBPP benchmarks as a function of the number of tokens processed during the annealing phase.  The results clearly demonstrate a significant performance drop for the model trained without high-quality data, underscoring its importance in the annealing stage.", "section": "6 Analysis"}, {"figure_path": "https://arxiv.org/html/2411.04905/x10.png", "caption": "Figure 10: Impact of star-based data filtering on model performance.", "description": "This figure displays the impact of filtering data based on GitHub stars on the performance of a language model.  Two 1.5B parameter models were trained, one using the original data and the other using data where only repositories with 5 or more stars were included. The graph shows the performance of each model on HumanEval and MBPP over the course of training.  It reveals that using the original data, without filtering by stars, produced better results compared to the filtered data. Although filtering data by stars led to lower training losses, the performance was worse, suggesting that prioritizing repositories with high stars counts decreases the diversity and quality of the data which ultimately reduces the model's performance.", "section": "6 Analysis"}, {"figure_path": "https://arxiv.org/html/2411.04905/x11.png", "caption": "Figure 11: Left figure: Losses of using different training data with different distributions. Right figure: Visualization of the embeddings for original data and filtered data. Note that filtering based on the number of stars can reduce data diversity and result in a lower overall loss for pretraining.", "description": "Figure 11 presents a comparative analysis of training loss and embedding distribution using different datasets.  The left panel displays the training loss curves for models trained on datasets with different characteristics. The original data, representing a more diverse dataset with both high-quality and lower-quality code, shows a higher loss compared to the filtered data. The filtered data, containing only high-quality code (filtered by the number of Github stars), exhibits a lower training loss. This indicates that using a filter reduces training loss but is likely at the cost of reduced data diversity. The right panel visualizes the embedding distributions of these original and filtered datasets using PCA (Principal Component Analysis), showing a clear distinction between them. This further confirms that filtering based on the number of Github stars leads to a less diverse dataset, despite potentially improving model training efficiency.", "section": "6 Analysis"}, {"figure_path": "https://arxiv.org/html/2411.04905/x12.png", "caption": "Figure 12: Comparison of Pass@1 performance on HumanEval & MBPP for different dedup strategies (File-Level, Repo-Level, and Repo-level + Chunk-Level) across RefineCode Python corpus.", "description": "Figure 12 illustrates the impact of different deduplication strategies on the performance of a language model.  Three strategies were compared: file-level deduplication, repository-level deduplication, and a combined repository and chunk-level approach. The x-axis represents the number of tokens (in billions) processed, while the y-axis shows the Pass@1 score on the HumanEval and MBPP benchmarks. The results demonstrate that file-level deduplication yields the best performance, outperforming both repository-level deduplication and the combined approach.", "section": "5 Experimental Results"}]