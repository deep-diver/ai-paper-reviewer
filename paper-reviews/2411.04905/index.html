<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><title>OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models &#183; AI Paper Reviews by AI</title>
<meta name=title content="OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models &#183; AI Paper Reviews by AI"><meta name=description content="OpenCoder, a top-tier open-source code LLM, is introduced, providing not only model weights and code but also reproducible training data, data processing pipelines, and training protocols, enabling co..."><meta name=keywords content="Natural Language Processing,Large Language Models,üè¢ INF,"><link rel=canonical href=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04905/><link type=text/css rel=stylesheet href=/ai-paper-reviewer/css/main.bundle.min.595affd4445a931ea6d6e3a5a3c709930fa52a60be10b21c6f81fdb8fecaacea33aacedf80cdc88be45f189be14ed4ce53ea74a1e1406fad9cbf90c5ed409173.css integrity="sha512-WVr/1ERakx6m1uOlo8cJkw+lKmC+ELIcb4H9uP7KrOozqs7fgM3Ii+RfGJvhTtTOU+p0oeFAb62cv5DF7UCRcw=="><script type=text/javascript src=/ai-paper-reviewer/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script><script defer type=text/javascript id=script-bundle src=/ai-paper-reviewer/js/main.bundle.min.efbf3b6b987689fffaf2d7b73173d2690c0279a04d444b0537a77d7f4ff6e6d493445400cb0cf56bc0f0f123e19f15394e63cae34e67f069bd013dd5c73df56e.js integrity="sha512-7787a5h2if/68te3MXPSaQwCeaBNREsFN6d9f0/25tSTRFQAywz1a8Dw8SPhnxU5TmPK405n8Gm9AT3Vxz31bg==" data-copy data-copied></script><script src=/ai-paper-reviewer/lib/zoom/zoom.min.37d2094687372da3f7343a221a470f6b8806f7891aa46a5a03966af7f0ebd38b9fe536cb154e6ad28f006d184b294525a7c4054b6bbb4be62d8b453b42db99bd.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ=="></script><link rel=apple-touch-icon sizes=180x180 href=/ai-paper-reviewer/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/ai-paper-reviewer/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/ai-paper-reviewer/favicon-16x16.png><link rel=manifest href=/ai-paper-reviewer/site.webmanifest><meta property="og:url" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04905/"><meta property="og:site_name" content="AI Paper Reviews by AI"><meta property="og:title" content="OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models"><meta property="og:description" content="OpenCoder, a top-tier open-source code LLM, is introduced, providing not only model weights and code but also reproducible training data, data processing pipelines, and training protocols, enabling co‚Ä¶"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="paper-reviews"><meta property="article:published_time" content="2024-11-07T00:00:00+00:00"><meta property="article:modified_time" content="2024-11-07T00:00:00+00:00"><meta property="article:tag" content="Natural Language Processing"><meta property="article:tag" content="Large Language Models"><meta property="article:tag" content="üè¢ INF"><meta property="og:image" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04905/cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04905/cover.png"><meta name=twitter:title content="OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models"><meta name=twitter:description content="OpenCoder, a top-tier open-source code LLM, is introduced, providing not only model weights and code but also reproducible training data, data processing pipelines, and training protocols, enabling co‚Ä¶"><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"Paper Reviews by AI","name":"OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models","headline":"OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models","abstract":"OpenCoder, a top-tier open-source code LLM, is introduced, providing not only model weights and code but also reproducible training data, data processing pipelines, and training protocols, enabling co\u0026hellip;","inLanguage":"en","url":"https:\/\/deep-diver.github.io\/ai-paper-reviewer\/paper-reviews\/2411.04905\/","author":{"@type":"Person","name":"AI Paper Reviews by AI"},"copyrightYear":"2024","dateCreated":"2024-11-07T00:00:00\u002b00:00","datePublished":"2024-11-07T00:00:00\u002b00:00","dateModified":"2024-11-07T00:00:00\u002b00:00","keywords":["Natural Language Processing","Large Language Models","üè¢ INF"],"mainEntityOfPage":"true","wordCount":"5600"}]</script><meta name=author content="AI Paper Reviews by AI"><link href=https://github.com/deep-diver/paper-reviewer/ rel=me><link href=https://twitter.com/algo_diver/ rel=me><script src=/ai-paper-reviewer/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj+KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script><script defer src=/ai-paper-reviewer/lib/typeit/typeit.umd.1b3200cb448f5cd1f548f2781452643d3511a43584b377b82c03a58055da4fdb7bc8f6c6c2ce846480c7677ff25bfd0d75f15823c09443ab18e0fd2cad792587.js integrity="sha512-GzIAy0SPXNH1SPJ4FFJkPTURpDWEs3e4LAOlgFXaT9t7yPbGws6EZIDHZ3/yW/0NdfFYI8CUQ6sY4P0srXklhw=="></script><script defer src=/ai-paper-reviewer/lib/packery/packery.pkgd.min.js integrity></script><script type=text/javascript src=/ai-paper-reviewer/js/shortcodes/gallery.min.9b4cb28f931ed922c26fb9b2510c2debb370f6a63305050c2af81740b2919883715e24efbbdf3a081496718ec751df3a72729d4d0bc71d6071297563a97ce1ee.js integrity="sha512-m0yyj5Me2SLCb7myUQwt67Nw9qYzBQUMKvgXQLKRmINxXiTvu986CBSWcY7HUd86cnKdTQvHHWBxKXVjqXzh7g=="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-KX0S6Q55Y7"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-KX0S6Q55Y7")</script><meta name=theme-color><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-app.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-firestore.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-auth.js></script><script>const firebaseConfig={apiKey:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",authDomain:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",projectId:"neurips2024-f3065",storageBucket:"neurips2024-f3065.firebasestorage.app",messagingSenderId:"982475958898",appId:"1:982475958898:web:2147e5d7753d6ac091f0eb",measurementId:"G-YQ46HXQ9JS"};var app=firebase.initializeApp(firebaseConfig),db=firebase.firestore(),auth=firebase.auth()</script></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a></div><div class=min-h-[148px]></div><div class="fixed inset-x-0 pl-[24px] pr-[24px]" style=z-index:100><div id=menu-blur class="absolute opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl shadow-2xl"></div><div class="relative max-w-[64rem] ml-auto mr-auto"><div style=padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start space-x-3"><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/ai-paper-reviewer/ class="text-base font-medium text-gray-500 hover:text-gray-900">AI Paper Reviews by AI</a></nav><nav class="hidden md:flex items-center space-x-5 md:ml-12 h-12"><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>About</p></a><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Paper Reviews</p></a><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Tags</p></a><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></span><p class="text-base font-medium" title></p></a><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span><p class="text-base font-medium" title></p></a><button id=search-button aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button><div class="ltr:mr-14 rtl:ml-14 flex items-center"><button id=appearance-switcher aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></nav><div class="flex md:hidden items-center space-x-5 md:ml-12 h-12"><span></span>
<button id=search-button-mobile aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></button>
<button id=appearance-switcher-mobile aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400" style=margin-right:5px><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div><div class="-my-2 -mr-2 md:hidden"><label id=menu-button class=block><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper style=padding-top:5px class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li id=menu-close-button><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>About</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Paper Reviews</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Tags</p></a></li><li class=mt-1><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li><li class=mt-1><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li></ul></div></label></div></div></div></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("menu-blur");n.style.opacity=t/300})</script><div class="relative flex flex-col grow"><main id=main-content class=grow><article><div id=hero class="h-[150px] md:h-[200px]"></div><div class="fixed inset-x-0 top-0 h-[800px] single_hero_background nozoom" style=background-image:url(/ai-paper-reviewer/paper-reviews/2411.04905/cover_hu7652632346366300351.png)><div class="absolute inset-0 bg-gradient-to-t from-neutral dark:from-neutral-800 to-transparent mix-blend-normal"></div><div class="absolute inset-0 opacity-60 bg-gradient-to-t from-neutral dark:from-neutral-800 to-neutral-100 dark:to-neutral-800 mix-blend-normal"></div></div><div id=background-blur class="fixed opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl"></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("background-blur");n.style.opacity=t/300})</script><header id=single_header class="mt-5 max-w-prose"><ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden"><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/>AI Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class=inline><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/>Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/2411.04905/>OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models</a><span class="px-1 text-primary-500">/</span></li></ol><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime=2024-11-07T00:00:00+00:00>7 November 2024</time><span class="px-2 text-primary-500">&#183;</span><span>5600 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">27 mins</span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=views_paper-reviews/2411.04905/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=views>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 576 512"><path fill="currentcolor" d="M288 32c-80.8.0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7.0 24.6C17.3 304 48.6 356 95.4 399.4 142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1 3.3-7.9 3.3-16.7.0-24.6-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144 64.5-144 144-144 144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64-11.5.0-22.3-3-31.6-8.4-.2 2.8-.4 5.5-.4 8.4.0 53 43 96 96 96s96-43 96-96-43-96-96-96c-2.8.0-5.6.1-8.4.4 5.3 9.3 8.4 20.1 8.4 31.6z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=likes_paper-reviews/2411.04905/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=likes>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<button id=button_likes class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400" onclick=process_article()>
<span id=button_likes_heart style=display:none class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span><span id=button_likes_emtpty_heart class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M244 84l11.1 12 12-11.98C300.6 51.37 347 36.51 392.6 44.1 461.5 55.58 512 115.2 512 185.1V190.9c0 41.5-17.2 81.2-47.6 109.5L283.7 469.1c-7.5 7-17.4 10.9-27.7 10.9S235.8 476.1 228.3 469.1L47.59 300.4C17.23 272.1.0 232.4.0 190.9V185.1c0-69.9 50.52-129.52 119.4-141 44.7-7.59 92 7.27 124.6 39.9C243.1 84 244 84.01 244 84zm11.1 79.9-45-46.8c-21.7-20.82-52.5-30.7-82.8-25.66C81.55 99.07 48 138.7 48 185.1V190.9c0 28.2 11.71 55.2 32.34 74.4L256 429.3l175.7-164c20.6-19.2 32.3-46.2 32.3-74.4V185.1c0-46.4-33.6-86.03-79.3-93.66C354.4 86.4 323.6 96.28 301.9 117.1l-46.8 46.8z"/></svg>
</span></span><span id=button_likes_text>&nbsp;Like</span></button></span></div><div class="flex flex-row flex-wrap items-center"><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/ai-generated/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">AI Generated
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/-daily-papers/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">ü§ó Daily Papers
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/natural-language-processing/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Natural Language Processing
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/large-language-models/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Large Language Models
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/-inf/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">üè¢ INF</span></span></span></div></div><div class="flex author"><img class="!mt-0 !mb-0 h-24 w-24 rounded-full ltr:mr-4 rtl:ml-4" width=96 height=96 alt="AI Paper Reviews by AI" src=/ai-paper-reviewer/img/avatar_hu14127527184135390686.png><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">Author</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">AI Paper Reviews by AI</div><div class="text-sm text-neutral-700 dark:text-neutral-400">I am AI, and I review papers in the field of AI</div><div class="text-2xl sm:text-lg"><div class="flex flex-wrap text-neutral-400 dark:text-neutral-500"><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://github.com/deep-diver/paper-reviewer/ target=_blank aria-label=Github rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://twitter.com/algo_diver/ target=_blank aria-label=Twitter rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span></a></div></div></div></div><div class=mb-5></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first lg:ml-auto px-0 lg:order-last ltr:lg:pl-8 rtl:lg:pr-8"><div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-[140px]"><details open id=TOCView class="toc-right mt-0 overflow-y-scroll overscroll-contain scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600 rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 hidden lg:block"><summary class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="min-w-[220px] py-2 border-dotted ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#open-code-llms>Open Code LLMs</a></li><li><a href=#data-deduplication>Data Deduplication</a></li><li><a href=#annealing-impact>Annealing Impact</a></li><li><a href=#two-stage-tuning>Two-Stage Tuning</a></li><li><a href=#future-research>Future Research</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><details class="toc-inside mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 lg:hidden"><summary class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#open-code-llms>Open Code LLMs</a></li><li><a href=#data-deduplication>Data Deduplication</a></li><li><a href=#annealing-impact>Annealing Impact</a></li><li><a href=#two-stage-tuning>Two-Stage Tuning</a></li><li><a href=#future-research>Future Research</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><script>var margin=200,marginError=50;(function(){var t=$(window),e=$("#TOCView"),s=e.height();function n(){var n=t.height()-margin;s>=n?(e.css("overflow-y","scroll"),e.css("max-height",n+marginError+"px")):(e.css("overflow-y","hidden"),e.css("max-height","9999999px"))}t.on("resize",n),$(document).ready(n)})()</script></div></div><div class="min-w-0 min-h-0 max-w-fit"><div class="article-content max-w-prose mb-20"><br><div class="flex flex-row flex-wrap items-center space-x-2"><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 48 48" fill="none"><rect width="48" height="48" fill="#fff" fill-opacity=".01"/><path d="M18 43V22c0-3.3137 2.6863-6 6-6s6 2.6863 6 6V43" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M12 40V22c0-6.6274 5.3726-12 12-12s12 5.3726 12 12V40" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M6 35V22C6 12.0589 14.0589 4 24 4s18 8.0589 18 18V35" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 44V31" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 24.625v-2.75" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/></svg>
</span></span><span>2411.04905</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg fill="#000" height="800" width="800" id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 511.999 511.999"><g><g><path d="M421.578 190.264l-99.847-99.847c-2.439-2.439-6.391-2.439-8.829.0L82.824 320.495c-2.439 2.439-2.439 6.392.0 8.829l99.847 99.847c2.439 2.439 6.391 2.439 8.829.0l230.078-230.078C424.017 196.655 424.017 192.703 421.578 190.264z"/></g></g><g><g><path d="M506.511 87.672 424.323 5.484c-7.308-7.31-19.175-7.315-26.488.0L348.219 55.1c-2.439 2.439-2.439 6.391.0 8.829l99.847 99.847c2.439 2.437 6.391 2.437 8.829.0l49.616-49.616C513.826 106.847 513.826 94.987 506.511 87.672z"/></g></g><g><g><path d="M508.133 491.11c-1.054-9.556-9.489-16.599-19.104-16.599H111.633l36.058-15.163c4.088-1.719 5.131-7.034 1.994-10.17l-86.854-86.854c-3.137-3.135-8.451-2.094-10.17 1.994C52.224 365.359 2.052 484.66 1.627 485.707c-5.815 13.208 4.855 27.01 18.107 26.263H489.52C500.566 511.97 509.379 502.408 508.133 491.11z"/></g></g></svg>
</span></span><span>Siming Huang et el.</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span>ü§ó 2024-11-08</span></span></span></div></div><p><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://arxiv.org/abs/2411.04905 target=_self role=button>‚Üó arXiv
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://huggingface.co/papers/2411.04905 target=_self role=button>‚Üó Hugging Face
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://paperswithcode.com/paper/opencoder-the-open-cookbook-for-top-tier-code target=_self role=button>‚Üó Papers with Code</a></p><h3 class="relative group">TL;DR<div id=tldr class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#tldr aria-label=Anchor>#</a></span></h3><div class="lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl"><p>Current top-tier code LLMs are largely closed-source, hindering open scientific investigation and community progress. This limits reproducibility, understanding of model strengths and weaknesses, and exploration of better training methodologies. This lack of transparency also contributes to resource inequality within the AI research community.</p><p>OpenCoder directly addresses these issues by providing a fully open-source code LLM. This includes not only the model weights and inference code but also the training data, complete data processing pipeline, detailed training protocols, and rigorous experimental results. The paper identifies key factors contributing to the model&rsquo;s success: improved data cleaning heuristics, high-quality synthetic data, and effective text corpus recall. This transparency promotes reproducibility and fosters faster advancements in code AI research.</p></div><h4 class="relative group">Key Takeaways<div id=key-takeaways class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#key-takeaways aria-label=Anchor>#</a></span></h4><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-6d549b547db07cf48bdbcce4875d2821></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-6d549b547db07cf48bdbcce4875d2821",{strings:[" OpenCoder achieves performance comparable to leading closed-source code LLMs. "],speed:10,lifeLike:!0,startDelay:0,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-44affa738ec3ada4efcdd91a29ac4ece></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-44affa738ec3ada4efcdd91a29ac4ece",{strings:[" OpenCoder's release includes not just model weights but also reproducible training data and a complete data processing pipeline. "],speed:10,lifeLike:!0,startDelay:1e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-4f8532e1e57bfb8d3d331fde2198491e></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-4f8532e1e57bfb8d3d331fde2198491e",{strings:[" The research identifies key ingredients for building top-tier code LLMs, including optimized data cleaning, relevant text corpus recall, and high-quality synthetic data. "],speed:10,lifeLike:!0,startDelay:2e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><h4 class="relative group">Why does it matter?<div id=why-does-it-matter class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#why-does-it-matter aria-label=Anchor>#</a></span></h4><p>This paper is crucial because it addresses the critical need for high-quality, reproducible code LLMs. By open-sourcing a top-tier model along with its training data and methodology, it accelerates research and fosters collaboration within the code AI community. It sets a new standard for transparency in code LLM research, potentially prompting others to follow suit and further democratizing access to cutting-edge technologies. This also opens avenues for improving training data, model architectures and training processes.</p><hr><h4 class="relative group">Visual Insights<div id=visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#visual-insights aria-label=Anchor>#</a></span></h4><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2411.04905/x1.png alt></figure></p><blockquote><p>üîº The figure shows a graph comparing the performance of OpenCoder with other large language models (LLMs) for code. The x-axis represents the number of training tokens (in billions), and the y-axis represents the MBPP Pass@1 (%) metric for a 1.5B parameter model and HumanEval (Zero-shot Pass@1) for 6B+ parameter models. OpenCoder significantly outperforms other fully open models (those with both open weights and reproducible datasets) and open-access models (those with only open weights) in both metrics, indicating its superior performance and the value of its fully open nature. The graph visually demonstrates OpenCoder pushing the frontier of fully open models to new heights.</p><details><summary>read the caption</summary>Figure 1: OpenCoder surpasses all previous fully open models (i.e., with open model weights and reproducible datasets) and other open-access models (i.e., with open model weights only) at the 6B+ parameter scale, pushing the frontier of fully open models to new heights.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Models</th><th>Data Processing Pipeline</th><th>Reproducible Pretraining Dataset</th><th>Large-scale SFT Dataset (>1M)</th><th>Intermediate Checkpoints</th><th>Training Tokens</th><th>HumanEval Pass@1</th></tr></thead><tbody><tr><td><em>Open Model Weights & Reproducible Datasets</em></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>OpenCoder-8B</td><td>‚úì</td><td>‚úì</td><td>‚úì</td><td>‚úì</td><td>2.5T</td><td>83.5</td></tr><tr><td>StarCoder2-15B</td><td>‚úì</td><td>‚úì</td><td>‚úó</td><td>‚úó</td><td>4.1T</td><td>72.6</td></tr><tr><td>Crystal-7B</td><td>‚úó</td><td>‚úì</td><td>‚úó</td><td>‚úì</td><td>1.3T</td><td>34.1</td></tr><tr><td><em>Open Model Weights</em></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>CodeLlama-7B</td><td>‚úó</td><td>‚úó</td><td>‚úó</td><td>‚úó</td><td>2.5T</td><td>34.8</td></tr><tr><td>CodeGemma-7B</td><td>‚úó</td><td>‚úó</td><td>‚úó</td><td>‚úó</td><td>6.5T</td><td>56.1</td></tr><tr><td>DS-Coder-V2-Lite</td><td>‚úó</td><td>‚úó</td><td>‚úó</td><td>‚úó</td><td>10.2T</td><td>81.1</td></tr><tr><td>Yi-Coder-9B</td><td>‚úó</td><td>‚úó</td><td>‚úó</td><td>‚úó</td><td>6.0T</td><td>85.4</td></tr><tr><td>Qwen2.5-Coder-7B</td><td>‚úó</td><td>‚úó</td><td>‚úó</td><td>‚úó</td><td>23.5T</td><td>88.4</td></tr></tbody></table></table></figure><blockquote><p>üîº This table compares the resources released by the OpenCoder large language model (LLM) with those of other popular open-source code LLMs. The comparison includes whether the model weights, intermediate checkpoints, the training dataset, the data processing pipeline, and a large-scale supervised fine-tuning (SFT) dataset are publicly available. HumanEval Pass@1 scores (a measure of code generation performance) for the corresponding chat models are also provided. This allows for a comprehensive assessment of the openness and reproducibility of each LLM and allows researchers to easily compare the performance and capabilities of different models.</p><details><summary>read the caption</summary>Table 1: The comparison of released resources between our OpenCoder with other popular open-sourced code LLMs. HumanEval scores are reported for the corresponding chat models.</details></blockquote><h3 class="relative group">In-depth insights<div id=in-depth-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#in-depth-insights aria-label=Anchor>#</a></span></h3><h4 class="relative group">Open Code LLMs<div id=open-code-llms class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#open-code-llms aria-label=Anchor>#</a></span></h4><p>Open Code LLMs represent a significant advancement in the field of artificial intelligence, offering the potential for more accessible and reproducible research. <strong>Openness</strong> is key, as it facilitates collaboration, allows for scrutiny of model architectures and training data, and promotes further development by the broader research community. However, <strong>challenges remain</strong> in achieving performance parity with closed-source models. These challenges include the cost and effort required to collect, clean, and curate high-quality training datasets, which often involve significant computational resources and specialized expertise. Furthermore, the need for <strong>transparency</strong> and <strong>reproducibility</strong> must be balanced with the competitive landscape of the AI industry, where proprietary models often hold an advantage. Despite these challenges, ongoing research is actively addressing these issues, with the ultimate aim of creating open LLMs that are not only comparable in performance to their closed-source counterparts but also serve as robust platforms for advancing the field of AI in a more ethical and collaborative manner.</p><h4 class="relative group">Data Deduplication<div id=data-deduplication class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#data-deduplication aria-label=Anchor>#</a></span></h4><p>Data deduplication plays a crucial role in optimizing large language model (LLM) training, particularly for code LLMs. The paper highlights the significant impact of deduplication on both data efficiency and model performance. <strong>Aggressive deduplication strategies</strong>, such as file-level deduplication, are shown to be superior to repository-level methods in terms of improving downstream task performance on benchmarks like HumanEval and MBPP. This is because repository-level deduplication retains a higher volume of redundant data, ultimately hindering model efficiency. <strong>File-level deduplication followed by fuzzy deduplication</strong> is identified as an effective and efficient process. The authors demonstrate that chunk-level deduplication doesn&rsquo;t offer additional benefits, while excessive deduplication can lead to data sparsity and negatively impact model performance. Therefore, a carefully balanced approach to deduplication, prioritizing data quality and diversity, is essential for optimal LLM training.</p><h4 class="relative group">Annealing Impact<div id=annealing-impact class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#annealing-impact aria-label=Anchor>#</a></span></h4><p>The concept of &lsquo;Annealing Impact&rsquo; in the context of large language models (LLMs) training refers to the effect of the annealing phase on the model&rsquo;s performance. <strong>Annealing, a gradual reduction in the learning rate</strong>, is a crucial post-pretraining stage designed to refine the model&rsquo;s abilities and improve generalization. The impact of annealing is multifaceted. The choice of <strong>high-quality annealing data significantly enhances performance</strong>, demonstrating the importance of curating datasets with diverse yet relevant examples. <strong>Data deduplication strategies</strong>, employed during both pretraining and annealing phases, play a significant role in determining the effectiveness of the process. File-level deduplication, as shown in the study, is more beneficial than repository-level deduplication. In essence, the annealing phase allows for a fine-tuning of the model&rsquo;s initial learning, improving its capacity to handle varied tasks with higher accuracy. The results suggest that a <strong>well-defined annealing stage, incorporating high-quality data and effective deduplication</strong>, is a key ingredient in training top-tier LLMs.</p><h4 class="relative group">Two-Stage Tuning<div id=two-stage-tuning class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#two-stage-tuning aria-label=Anchor>#</a></span></h4><p>The concept of &ldquo;Two-Stage Tuning&rdquo; in the context of large language model (LLM) training for code generation is a powerful technique. It involves a two-phased approach: <strong>Stage 1 focuses on broad capability acquisition</strong>, using a diverse and extensive instruction dataset. This allows the model to grasp general programming concepts and a wide array of coding styles, establishing a strong foundation. <strong>Stage 2 then refines this foundation</strong>, concentrating on higher-quality, code-specific data to enhance performance on precise, practical tasks. This approach combines the benefits of breadth and depth, resulting in a model that is both versatile and proficient. By initially building a strong, generalized understanding, Stage 1 prepares the model for targeted improvements in Stage 2. This strategy is demonstrably superior to a single-stage approach, resulting in models that achieve better performance on various benchmarks that test both general knowledge and focused skill. The two-stage strategy helps avoid catastrophic forgetting; knowledge from Stage 1 isn&rsquo;t lost during Stage 2&rsquo;s specialization. Therefore, adopting a two-stage tuning strategy is crucial for achieving superior LLMs, especially in complex domains like code generation where both theoretical and practical expertise are vital.</p><h4 class="relative group">Future Research<div id=future-research class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#future-research aria-label=Anchor>#</a></span></h4><p>Future research directions for OpenCoder should prioritize <strong>improving the model&rsquo;s reasoning and problem-solving capabilities</strong>, particularly for complex, multi-step tasks. This could involve exploring advanced training techniques like reinforcement learning or incorporating external knowledge bases. <strong>Expanding the model&rsquo;s multilingual capabilities</strong> is crucial, focusing on supporting a wider range of programming languages and addressing the nuances of different coding styles and conventions. <strong>Enhanced data curation methods</strong> are needed to improve data quality and diversity. Investigating techniques for efficient data deduplication and strategies for integrating diverse data sources, like code repositories and documentation, are vital. Further research should also focus on <strong>mitigating bias in the training data</strong> and improving the model&rsquo;s reliability and safety. This includes designing robust evaluation methods that specifically target potential biases and vulnerabilities. Finally, <strong>investigating the efficiency of the training process</strong> and exploring methods for training even larger and more powerful models while maintaining resource efficiency is essential for future advancements in code LLMs. By addressing these research avenues, the OpenCoder project can continue to push the boundaries of code AI and contribute meaningfully to the broader software development community.</p><h3 class="relative group">More visual insights<div id=more-visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#more-visual-insights aria-label=Anchor>#</a></span></h3><details><summary>More on figures</summary><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2411.04905/x2.png alt></figure></p><blockquote><p>üîº This figure shows the data processing pipeline for the pretraining stage of the OpenCoder model. It details the steps involved in creating a high-quality dataset for training, starting from raw code data and code-related web data. The pipeline involves several key stages, including preprocessing, deduplication, transformation, filtering, and data sampling, each designed to improve data quality and remove undesirable elements. The left panel focuses on processing the raw code data, while the right panel demonstrates the processing of code-related web data. This figure helps illustrate the comprehensive approach OpenCoder takes to creating a reliable and effective pretraining dataset.</p><details><summary>read the caption</summary>Figure 2: The illustration of our pretraining data processing workflow.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2411.04905/x3.png alt></figure></p><blockquote><p>üîº This figure uses Principal Component Analysis (PCA) to visualize the differences in data distribution between the RefineCode dataset and the Stack v2 dataset. RefineCode, a dataset created by the authors, is designed to be higher quality than Stack v2. The PCA plot shows distinct clusters for the two datasets, indicating that they have different characteristics. RefineCode&rsquo;s data points are more tightly clustered, suggesting greater homogeneity and higher quality, while Stack v2&rsquo;s points are more scattered, suggesting greater heterogeneity and potentially lower quality. The plot helps illustrate the authors&rsquo; claim of creating a more refined and homogenous dataset suitable for training high-performing code LLMs.</p><details><summary>read the caption</summary>Figure 3: Visualization on the PCA data distributions of RefineCode and The Stack v2.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2411.04905/x4.png alt></figure></p><blockquote><p>üîº This bar chart visualizes the distribution of the top programming languages included in the RefineCode dataset, a crucial component of the OpenCoder large language model. The x-axis lists the programming languages, and the y-axis displays two metrics: the total file size (in gigabytes) and the number of files for each language. This illustrates the relative prevalence of different languages within the dataset, providing insights into the dataset&rsquo;s composition and potential biases or strengths that could influence the model&rsquo;s capabilities in various programming languages.</p><details><summary>read the caption</summary>Figure 4: The distribution of top program languages in RefineCode.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2411.04905/x5.png alt></figure></p><blockquote><p>üîº This figure illustrates the three different methods used to synthesize the instruction data for training OpenCoder. (a) shows large-scale diverse instruction synthesis, leveraging a filtered web corpus, task-specific prompt engineering, and answer generation from an LLM. (b) details educational instruction synthesis, starting from a seed corpus, incorporating LLM prompt engineering, test case generation, code verification, and ultimately creating educational instructions. Finally, (c) illustrates package-related instruction synthesis that leverages pretraining and package corpora, employing retrieval, prompt engineering, and generating package instructions.</p><details><summary>read the caption</summary>Figure 5: The illustration of our instruction data synthesis workflow.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2411.04905/x6.png alt></figure></p><blockquote><p>üîº Figure 6 presents a detailed comparison of the performance of OpenCoder-8B-Instruct against other open-source, similarly sized code models on the McEval benchmark. McEval is a comprehensive multilingual code evaluation benchmark that assesses various coding capabilities across 40 programming languages. The figure provides a visual representation of each model&rsquo;s performance across different languages, allowing for a direct comparison of their strengths and weaknesses in various coding contexts. This is particularly useful for identifying potential areas for improvement or specialization in multilingual code generation.</p><details><summary>read the caption</summary>Figure 6: The McEval performance of OpenCoder-8B-Instruct in comparison to other open-source code models of comparable size.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2411.04905/x7.png alt></figure></p><blockquote><p>üîº This figure presents a bar chart comparing the performance of various open-source code large language models (LLMs) on the MdEval benchmark. MdEval is a multilingual code debugging benchmark that assesses a model&rsquo;s ability to identify and fix bugs in code across different programming languages. The chart shows the average performance across multiple languages, with separate bars for each language highlighting the relative strengths and weaknesses of each LLM. OpenCoder-8B-Instruct is included, and its performance is compared to that of other models of similar size. The chart visually demonstrates the relative performance of OpenCoder-8B-Instruct compared to competing LLMs on a challenging, multilingual code debugging task.</p><details><summary>read the caption</summary>Figure 7: The MdEval performance of OpenCoder-8B-Instruct in comparison to other open-source code models of comparable size.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2411.04905/x8.png alt></figure></p><blockquote><p>üîº This figure compares the performance of different deduplication strategies on code datasets used for training large language models. Two different metrics (HumanEval and MBPP) measuring code generation performance are shown, plotted against the number of training tokens used after applying either file-level or repository-level deduplication. The results illustrate the impact of the chosen deduplication method on the final model&rsquo;s performance.</p><details><summary>read the caption</summary>Figure 8: Impact of using different deduplication strategies.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2411.04905/x9.png alt></figure></p><blockquote><p>üîº This figure displays the impact of incorporating high-quality data during the annealing phase of the model&rsquo;s training. Two 1.5B parameter LLMs were trained, one with the original annealing data and another without the high-quality components (Algorithmic Corpus and Synthetic Data). The plots show the performance of both models on the HumanEval and MBPP benchmarks as a function of the number of tokens processed during the annealing phase. The results clearly demonstrate a significant performance drop for the model trained without high-quality data, underscoring its importance in the annealing stage.</p><details><summary>read the caption</summary>Figure 9: Impact of using high-quality data in the annealing stage.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2411.04905/x10.png alt></figure></p><blockquote><p>üîº This figure displays the impact of filtering data based on GitHub stars on the performance of a language model. Two 1.5B parameter models were trained, one using the original data and the other using data where only repositories with 5 or more stars were included. The graph shows the performance of each model on HumanEval and MBPP over the course of training. It reveals that using the original data, without filtering by stars, produced better results compared to the filtered data. Although filtering data by stars led to lower training losses, the performance was worse, suggesting that prioritizing repositories with high stars counts decreases the diversity and quality of the data which ultimately reduces the model&rsquo;s performance.</p><details><summary>read the caption</summary>Figure 10: Impact of star-based data filtering on model performance.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2411.04905/x11.png alt></figure></p><blockquote><p>üîº Figure 11 presents a comparative analysis of training loss and embedding distribution using different datasets. The left panel displays the training loss curves for models trained on datasets with different characteristics. The original data, representing a more diverse dataset with both high-quality and lower-quality code, shows a higher loss compared to the filtered data. The filtered data, containing only high-quality code (filtered by the number of Github stars), exhibits a lower training loss. This indicates that using a filter reduces training loss but is likely at the cost of reduced data diversity. The right panel visualizes the embedding distributions of these original and filtered datasets using PCA (Principal Component Analysis), showing a clear distinction between them. This further confirms that filtering based on the number of Github stars leads to a less diverse dataset, despite potentially improving model training efficiency.</p><details><summary>read the caption</summary>Figure 11: Left figure: Losses of using different training data with different distributions. Right figure: Visualization of the embeddings for original data and filtered data. Note that filtering based on the number of stars can reduce data diversity and result in a lower overall loss for pretraining.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2411.04905/x12.png alt></figure></p><blockquote><p>üîº Figure 12 illustrates the impact of different deduplication strategies on the performance of a language model. Three strategies were compared: file-level deduplication, repository-level deduplication, and a combined repository and chunk-level approach. The x-axis represents the number of tokens (in billions) processed, while the y-axis shows the Pass@1 score on the HumanEval and MBPP benchmarks. The results demonstrate that file-level deduplication yields the best performance, outperforming both repository-level deduplication and the combined approach.</p><details><summary>read the caption</summary>Figure 12: Comparison of Pass@1 performance on HumanEval & MBPP for different dedup strategies (File-Level, Repo-Level, and Repo-level + Chunk-Level) across RefineCode Python corpus.</details></blockquote></details><details><summary>More on tables</summary><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Category</th><th>Data Source</th><th># Tokens</th><th>Percentage</th></tr></thead><tbody><tr><td>Raw Code Data</td><td>Github Code</td><td>755 B</td><td>78.4%</td></tr><tr><td></td><td>Jupyter Notebooks</td><td>11 B</td><td>1.1%</td></tr><tr><td></td><td>The Stack v2</td><td>120 B</td><td>12.5%</td></tr><tr><td>Code-related Web Data</td><td>Processed CC</td><td>13 B</td><td>1.4%</td></tr><tr><td></td><td>Processed SkyPile</td><td>3 B</td><td>0.3%</td></tr><tr><td></td><td>Processed FineWeb</td><td>55 B</td><td>5.7%</td></tr><tr><td>OpenSource Data</td><td>Processed AutoMathText</td><td>3 B</td><td>0.3%</td></tr></tbody></table></table></figure><blockquote><p>üîº Table 2 presents a breakdown of the RefineCode dataset, detailing the composition of its different data sources and their respective sizes (in tokens and percentage). It shows how much of RefineCode comes from GitHub code, Jupyter Notebooks, The Stack v2 dataset, and different processed web corpora. This provides crucial context for understanding the dataset&rsquo;s scale and diversity, and how various sources contributed to the final dataset.</p><details><summary>read the caption</summary>Table 2: The Composition of RefineCode.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Category</th><th>Dataset</th><th># Token</th></tr></thead><tbody><tr><td>Original Data</td><td>RefineCode</td><td>84.21 B</td></tr><tr><td></td><td>Algorithmic Corpus</td><td>12.44 B</td></tr><tr><td>Synthetic Data</td><td>High Quality Code Snippet</td><td>2.71 B</td></tr><tr><td></td><td>Code Textbooks</td><td>0.91 B</td></tr></tbody></table></table></figure><blockquote><p>üîº This table details the composition of the data used in the annealing phase of the OpenCoder model&rsquo;s training. It breaks down the total number of tokens contributed by different data sources: the original RefineCode dataset, algorithmically generated code, high-quality synthetic code snippets, and code textbooks. The proportions of each dataset are shown to illustrate the mixture of data used to fine-tune the model during the annealing stage.</p><details><summary>read the caption</summary>Table 3: Detailed data mixture for annealing data.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Model Parameter</th><th>OpenCoder-1.5B</th><th>OpenCoder-8B</th></tr></thead><tbody><tr><td>Layers</td><td>24</td><td>32</td></tr><tr><td>Model Dimension</td><td>2240</td><td>4096</td></tr><tr><td>Attention Heads</td><td>14</td><td>32</td></tr><tr><td>Key / Value Heads</td><td>14</td><td>8</td></tr><tr><td>Activation Function</td><td>SwiGLU</td><td>SwiGLU</td></tr><tr><td>Vocab Size</td><td>96640</td><td>96640</td></tr><tr><td>Positional Embedding</td><td>RoPE(Œ∏=10000)</td><td>RoPE(Œ∏=500000)</td></tr><tr><td>Context Window Size</td><td>4096</td><td>8192</td></tr></tbody></table></table></figure><blockquote><p>üîº This table details the key architectural hyperparameters of the two OpenCoder models: the 1.5 billion parameter model and the 8 billion parameter model. It provides a comparison of their configurations, including the number of layers, hidden dimension size, number of attention heads, activation function used, vocabulary size, and context window size. This information is crucial for understanding the differences in model capacity and computational requirements between the two variants.</p><details><summary>read the caption</summary>Table 4: Overview of the key hyperparameters of OpenCoder, including 1.5B and 8B.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Stage</th><th>Data Source</th><th># Examples</th></tr></thead><tbody><tr><td>Stage1</td><td>RealUser-Instruct</td><td>0.7 M</td></tr><tr><td></td><td>Large-scale Diverse-Instruct</td><td>2.3 M</td></tr><tr><td></td><td>Filtered Infinity-Instruct</td><td>1.0 M</td></tr><tr><td>Stage2</td><td>McEval-Instruct</td><td>36 K</td></tr><tr><td></td><td>Evol-Instruct</td><td>111 K</td></tr><tr><td></td><td>Educational-Instruct</td><td>110 K</td></tr><tr><td></td><td>Package-Instruct</td><td>110 K</td></tr></tbody></table></table></figure><blockquote><p>üîº This table details the data used in the two-stage instruction tuning process for the OpenCoder model. Stage 1 focuses on general theoretical computer science concepts, while Stage 2 concentrates on practical coding tasks using high-quality code from GitHub. The table lists the data source and the number of examples for each stage of the tuning process. This two-stage approach aims to enhance the model&rsquo;s abilities in both theoretical understanding and practical code generation.</p><details><summary>read the caption</summary>Table 5: Detailed data composition of our two-stage instruction-tuning.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Model</th><th>Size</th><th>HumanEvalHE</th><th>HumanEvalHE+</th><th>MBPP</th><th>MBPP+</th><th>MBPP3-shot</th><th>MBPPFull</th><th>BigCodeBenchHard</th><th>BigCodeBench</th></tr></thead><tbody><tr><td><strong>1B+ Models</strong></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>DeepSeek-Coder-1.3B-Base</td><td>1.3B</td><td>34.8</td><td>26.8</td><td>55.6</td><td>46.9</td><td>46.2</td><td>26.1</td><td>3.4</td><td></td></tr><tr><td>Yi-Coder-1.5B</td><td>1.5B</td><td>41.5</td><td>32.9</td><td>27.0</td><td>22.2</td><td>51.6</td><td>23.5</td><td>3.4</td><td></td></tr><tr><td>CodeGemma-2B</td><td>2B</td><td>31.1</td><td>16.5</td><td>51.1</td><td>43.1</td><td>45.4</td><td>23.9</td><td>7.4</td><td></td></tr><tr><td>Qwen2.5-Coder-1.5B</td><td>1.5B</td><td>43.9</td><td>36.6</td><td>69.2</td><td>58.6</td><td><strong>59.2</strong></td><td><strong>34.6</strong></td><td><strong>9.5</strong></td><td></td></tr><tr><td>StarCoder2-3B</td><td>3B</td><td>31.7</td><td>27.4</td><td>60.2</td><td>49.1</td><td>46.4</td><td>21.4</td><td>4.7</td><td></td></tr><tr><td>OpenCoder-1.5B-Base</td><td>1.5B</td><td><strong>54.3</strong></td><td><strong>49.4</strong></td><td><strong>70.6</strong></td><td><strong>58.7</strong></td><td>51.8</td><td>24.5</td><td>5.4</td><td></td></tr><tr><td><strong>6B+ Models</strong></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>CodeLlama-7B</td><td>7B</td><td>33.5</td><td>26.2</td><td>55.3</td><td>46.8</td><td>41.4</td><td>28.7</td><td>5.4</td><td></td></tr><tr><td>CodeGemma-7B</td><td>7B</td><td>39.0</td><td>32.3</td><td>50.5</td><td>40.7</td><td>55.0</td><td>38.3</td><td>10.1</td><td></td></tr><tr><td>DS-Coder-6.7B-Base</td><td>6.7B</td><td>47.6</td><td>39.6</td><td>70.2</td><td>56.6</td><td>60.6</td><td>41.1</td><td>11.5</td><td></td></tr><tr><td>DS-Coder-V2-Lite-Base(MoE)</td><td>16B</td><td>40.9</td><td>34.1</td><td>71.9</td><td>59.4</td><td>62.6</td><td>30.6</td><td>8.1</td><td></td></tr><tr><td>CodeQwen1.5-7B-Base</td><td>7B</td><td>51.8</td><td>45.7</td><td>72.2</td><td>60.2</td><td>61.8</td><td>45.6</td><td>15.6</td><td></td></tr><tr><td>Yi-Coder-9B</td><td>9B</td><td>53.7</td><td>46.3</td><td>48.4</td><td>40.7</td><td><strong>69.4</strong></td><td>42.9</td><td>14.2</td><td></td></tr><tr><td>Qwen2.5-Coder-7B-Base</td><td>7B</td><td>61.6</td><td>53.0</td><td>76.9</td><td>62.9</td><td>68.8</td><td><strong>45.8</strong></td><td><strong>16.2</strong></td><td></td></tr><tr><td>Crystal-7B</td><td>7B</td><td>22.6</td><td>20.7</td><td>38.6</td><td>31.7</td><td>31.0</td><td>10.8</td><td>4.1</td><td></td></tr><tr><td>StarCoder2-7B</td><td>7B</td><td>35.4</td><td>29.9</td><td>54.4</td><td>45.6</td><td>55.2</td><td>27.7</td><td>8.8</td><td></td></tr><tr><td>StarCoder2-15B</td><td>15B</td><td>46.3</td><td>37.8</td><td>66.2</td><td>53.1</td><td>15.2</td><td>38.4</td><td>12.2</td><td></td></tr><tr><td>OpenCoder-8B-Base</td><td>8B</td><td><strong>68.9</strong></td><td><strong>63.4</strong></td><td><strong>79.9</strong></td><td><strong>70.4</strong></td><td>60.6</td><td>40.5</td><td>9.5</td><td></td></tr></tbody></table></table></figure><blockquote><p>üîº Table 6 presents a comparative analysis of various base code language models&rsquo; performance on three prominent benchmarks: HumanEval, MBPP, and BigCodeBench&rsquo;s &lsquo;complete&rsquo; task. The table highlights the performance scores achieved by each model across these benchmarks. Models trained using openly accessible and reproducible datasets are visually distinguished with a green marker, emphasizing the importance of transparency and reproducibility in model development. This comparison allows for a nuanced understanding of the relative strengths and weaknesses of different code models and the impact of data availability on model performance.</p><details><summary>read the caption</summary>Table 6: Performance of various base models on HumanEval, MBPP, and the ‚Äúcomplete‚Äù task of BigCodeBench. Models trained on reproducible datasets are marked with green.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Model</th><th>Size</th><th>HumanEval HE</th><th>HumanEval HE+</th><th>MBPP MBPP</th><th>MBPP MBPP+</th><th>BigCodeBench Full</th><th>BigCodeBench Hard</th><th>LiveCodeBench Avg</th></tr></thead><tbody><tr><td><strong>1B+ Models</strong></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>DS-coder-1.3B-Instruct</td><td>1.3B</td><td>65.2</td><td>61.6</td><td>61.6</td><td>52.6</td><td>22.8</td><td>3.4</td><td>9.3</td></tr><tr><td>Qwen2.5-Coder-1.5B-Instruct</td><td>1.5B</td><td>70.7</td><td>66.5</td><td>69.2</td><td>59.4</td><td>32.5</td><td>6.8</td><td><strong>15.7</strong></td></tr><tr><td>Yi-Coder-1.5B-Chat</td><td>1.5B</td><td>67.7</td><td>63.4</td><td>68.0</td><td>59.0</td><td>24.0</td><td>6.8</td><td>11.6</td></tr><tr><td>OpenCoder-1.5B-Instruct</td><td>1.5B</td><td><strong>72.5</strong></td><td><strong>67.7</strong></td><td><strong>72.7</strong></td><td><strong>61.9</strong></td><td><strong>33.3</strong></td><td><strong>11.5</strong></td><td>12.8</td></tr><tr><td><strong>6B+ Models</strong></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>DS-Coder-V2-Lite-Instruct</td><td>16B</td><td>81.1</td><td>75.0</td><td>82.3</td><td>68.8</td><td>36.8</td><td>16.2</td><td>24.3</td></tr><tr><td>CodeLlama-7B-Instruct</td><td>7B</td><td>45.7</td><td>39.6</td><td>39.9</td><td>33.6</td><td>21.9</td><td>3.4</td><td>2.8</td></tr><tr><td>CodeGemma-7B-It</td><td>7B</td><td>59.8</td><td>47.0</td><td>69.8</td><td>59.0</td><td>32.3</td><td>7.4</td><td>14.7</td></tr><tr><td>DS-Coder-6.7B-Instruct</td><td>6.7B</td><td>78.6</td><td>70.7</td><td>75.1</td><td>66.1</td><td>35.5</td><td>10.1</td><td>20.5</td></tr><tr><td>Yi-Coder-9B-Chat</td><td>9B</td><td>82.3</td><td>72.6</td><td>81.5</td><td>69.3</td><td>38.1</td><td>11.5</td><td>23.4</td></tr><tr><td>CodeQwen1.5-7B-Chat</td><td>7B</td><td>86.0</td><td>79.3</td><td>83.3</td><td>71.4</td><td>39.6</td><td><strong>18.9</strong></td><td>20.1</td></tr><tr><td>Qwen2.5-Coder-7B-Instruct</td><td>7B</td><td><strong>88.4</strong></td><td><strong>84.1</strong></td><td><strong>83.5</strong></td><td><strong>71.7</strong></td><td><strong>41.0</strong></td><td>18.2</td><td><strong>37.6</strong></td></tr><tr><td>CrystalChat-7B</td><td>7B</td><td>34.1</td><td>31.7</td><td>39.1</td><td>32.7</td><td>26.7</td><td>2.3</td><td>6.1</td></tr><tr><td>StarCoder2-15B-Instruct-v0.1</td><td>15B</td><td>72.6</td><td>63.4</td><td>75.2</td><td>61.2</td><td>37.6</td><td>12.2</td><td>20.4</td></tr><tr><td>OpenCoder-8B-Instruct</td><td>8B</td><td>83.5</td><td>78.7</td><td>79.1</td><td>69.0</td><td>40.3</td><td>16.9</td><td>23.2</td></tr></tbody></table></table></figure><blockquote><p>üîº This table compares the performance of different chat models on four code-related benchmarks: HumanEval, MBPP, BigCodeBench&rsquo;s &lsquo;instruct&rsquo; task, and LiveCodeBench. It shows the Pass@1 scores (percentage of correctly solved problems) for each model across these benchmarks. The table highlights models trained using publicly available data (reproducible datasets) in green to emphasize the transparency and reproducibility of their training processes. The benchmarks cover different aspects of code understanding and generation ability.</p><details><summary>read the caption</summary>Table 7: Performance of various chat models on HumanEval, MBPP, the ‚Äúinstruct‚Äù task of BigCodeBench and LiveCodeBench. Models trained on reproducible datasets are marked with green.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Model</th><th>Size</th><th>Python</th><th>Java</th><th>C++</th><th>C#</th><th>TS</th><th>JS</th><th>PHP</th><th>Bash</th><th>Average</th></tr></thead><tbody><tr><td><strong>1B+ Models</strong></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>DS-Coder-1.3B-Instruct</td><td>1.3B</td><td>65.2</td><td>51.9</td><td>45.3</td><td>55.1</td><td>59.7</td><td>52.2</td><td>45.3</td><td>12.7</td><td>48.4</td></tr><tr><td>Yi-Coder-1.5B-Chat</td><td>1.5B</td><td>67.7</td><td>51.9</td><td>49.1</td><td>57.6</td><td>57.9</td><td>59.6</td><td>52.2</td><td>19.0</td><td>51.9</td></tr><tr><td>Qwen2.5-Coder-1.5B-Instruct</td><td>1.5B</td><td>71.2</td><td>55.7</td><td>50.9</td><td>64.6</td><td>61.0</td><td>62.1</td><td>59.0</td><td>29.1</td><td>56.7</td></tr><tr><td>OpenCoder-1.5B-Instruct</td><td>1.5B</td><td>72.5</td><td>64.6</td><td>50.9</td><td>61.4</td><td>63.5</td><td>62.1</td><td>55.3</td><td>29.7</td><td>57.5</td></tr><tr><td><strong>6B+ Models</strong></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>DS-Coder-6.7B-Instruct</td><td>6.7B</td><td>78.6</td><td>68.4</td><td>63.4</td><td>72.8</td><td>67.2</td><td>72.7</td><td>68.9</td><td>36.7</td><td>66.1</td></tr><tr><td>DS-Coder-V2-Lite-Instruct</td><td>16B</td><td>81.1</td><td>76.6</td><td>75.8</td><td>76.6</td><td>80.5</td><td>77.6</td><td>74.5</td><td>43.0</td><td>73.2</td></tr><tr><td>CodeLlama-7B-Instruct</td><td>7B</td><td>45.7</td><td>32.2</td><td>28.6</td><td>32.9</td><td>39.0</td><td>43.5</td><td>31.7</td><td>10.1</td><td>33.0</td></tr><tr><td>CodeGemma-7B-It</td><td>7B</td><td>59.8</td><td>48.1</td><td>46.6</td><td>51.9</td><td>54.7</td><td>54.0</td><td>46.6</td><td>10.1</td><td>46.5</td></tr><tr><td>CodeQwen1.5-7B-Chat</td><td>7B</td><td>83.5</td><td>70.9</td><td>72.0</td><td>75.9</td><td>76.7</td><td>77.6</td><td>73.9</td><td>41.8</td><td>71.6</td></tr><tr><td>Yi-Coder-9B-Chat</td><td>9B</td><td>85.4</td><td>76.0</td><td>67.7</td><td>76.6</td><td>72.3</td><td>78.9</td><td>72.1</td><td>45.6</td><td>71.8</td></tr><tr><td>Qwen2.5-Coder-7B-Instruct</td><td>7B</td><td>87.8</td><td>76.5</td><td>75.6</td><td>80.3</td><td>81.8</td><td>83.2</td><td>78.3</td><td>48.7</td><td>76.5</td></tr><tr><td>OpenCoder-8B-Instruct</td><td>8B</td><td>83.5</td><td>72.2</td><td>61.5</td><td>75.9</td><td>78.0</td><td>79.5</td><td>73.3</td><td>44.3</td><td>71.0</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents a comprehensive comparison of different large language models (LLMs) on their ability to generate code in multiple programming languages. The MultiPL-E benchmark evaluates the models&rsquo; performance across various languages, providing insights into their cross-lingual code generation capabilities and identifying strengths and weaknesses in handling different programming paradigms and syntaxes. The table shows the performance metrics for each model across various languages, offering a detailed analysis of the models&rsquo; proficiency in multilingual code generation.</p><details><summary>read the caption</summary>Table 8: Performance of various chat models on the MultiPL-E benchmark across different programming languages.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Deduplication Level</th><th># Total Rows</th><th># Retained Rows</th><th># Retained Tokens</th></tr></thead><tbody><tr><td>File level</td><td>485,817,123</td><td>30,488,834</td><td>32.74 B</td></tr><tr><td>Repository level</td><td>11,037,352</td><td>7,480,488</td><td>99.47 B</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents a comparison of file-level and repository-level deduplication techniques applied to a Python code dataset. It shows the initial number of files and repositories, the number of files and repositories retained after deduplication, and the total number of tokens retained. This comparison highlights the impact of different deduplication strategies on data size and potentially on model training performance. The results are crucial for understanding the trade-offs between data size reduction and data diversity in building code large language models (LLMs).</p><details><summary>read the caption</summary>Table 9: The statistics for file level deduplication and repository level deduplication on Python code. Rows for file level and repository level represent the number of files and repositories, respectively.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th style=text-align:left></th><th style=text-align:center>HE</th><th style=text-align:center>HE+</th><th style=text-align:center>MBPP</th><th style=text-align:center>MBPP+</th><th style=text-align:center>BigCodeBench</th><th style=text-align:center>Code Arena</th></tr></thead><tbody><tr><td style=text-align:left>Stage1</td><td style=text-align:center>52.4</td><td style=text-align:center>48.1</td><td style=text-align:center>68.7</td><td style=text-align:center>57.4</td><td style=text-align:center>22.1</td><td style=text-align:center>5.3</td></tr><tr><td style=text-align:left>Stage1 + Stage2</td><td style=text-align:center><strong>70.1</strong></td><td style=text-align:center><strong>64.0</strong></td><td style=text-align:center><strong>74.6</strong></td><td style=text-align:center><strong>64.8</strong></td><td style=text-align:center><strong>31.5</strong></td><td style=text-align:center><strong>6.9</strong></td></tr><tr><td style=text-align:left>Mix Training</td><td style=text-align:center>55.5</td><td style=text-align:center>51.2</td><td style=text-align:center>52.0</td><td style=text-align:center>58.7</td><td style=text-align:center>23.9</td><td style=text-align:center>3.8</td></tr></tbody></table></table></figure><blockquote><p>üîº This table compares the performance of three different instruction tuning strategies for a 1.5B parameter language model: training only on Stage 1 data, training on both Stage 1 and Stage 2 data sequentially, and training on a mixture of both Stage 1 and Stage 2 data. The comparison is made across multiple code generation benchmarks (HumanEval, HumanEval+, MBPP, MBPP+, BigCodeBench, and Code Arena). The results show the impact of different data compositions and training approaches on the model&rsquo;s ability to generate high-quality code.</p><details><summary>read the caption</summary>Table 10: Performance of different training strategies across benchmarks. Mix Training refers to the process of combining and shuffling the data from Stage 1 and Stage 2 for joint training.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Description</th><th>Explanation</th><th>Filtering Quota</th></tr></thead><tbody><tr><td>The proportion of lines in strings with a word count exceeding.</td><td>Files with too many long strings indicate a lack of code logic.</td><td>score &ldquo;>&rdquo; 0.2</td></tr><tr><td>The proportion of characters in words from strings with a character count exceeding 20.</td><td>String variables containing long sequences of characters are often indicative of meaningless content such as base64 data, Hash encoding, url, etc.</td><td>score &ldquo;>&rdquo; 0.4</td></tr><tr><td>The proportion of hexadecimal characters.</td><td>Files with two many hexadecimal characters indicate a lack of code logic.</td><td>score &ldquo;>&rdquo; 0.4</td></tr><tr><td>The proportion of lines like &ldquo;you code here&rdquo;, &ldquo;TODO&rdquo; or &ldquo;FIXME&rdquo;.</td><td>We found that these elements tend to be excessively repeated in the dataset, which increases the likelihood that the model, during code completion, will output placeholders like the ones mentioned above instead of generating actual code.</td><td>score &ldquo;>&rdquo; 0.01</td></tr><tr><td>The proportion of lines containing an &ldquo;assert&rdquo; statement.</td><td>Files containing a large number of ‚Äôassert‚Äô statements are often test files, which tend to have relatively simple and repetitive code patterns.</td><td>score &ldquo;>&rdquo; 0.4</td></tr></tbody></table></table></figure><blockquote><p>üîº Table 11 presents examples of general heuristic filtering rules used in the data cleaning pipeline. These rules are not language-specific and apply to various code files. The table details the specific criteria used in the filtering process, along with an explanation and the filtering threshold value used for each rule. These rules aim to remove low-quality code, such as those with excessive long strings, hexadecimal characters, or comments like &lsquo;You code here&rsquo;. The filtering quota is a score that helps to evaluate how well the rule performs. The goal is to identify and remove code that contains low-quality or non-informative elements to improve overall data quality for model training.</p><details><summary>read the caption</summary>Table 11: Examples of general code filtering rules.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Description</th><th>Explanation</th><th>Filtering Quota</th></tr></thead><tbody><tr><td>The proportion of the number of python functions to the total number of lines.</td><td>A higher number of Python functions in a file may indicate that the functions are overly simple, with limited code logic, or have a bad code format.</td><td>score > 0.2</td></tr><tr><td>Whether the file can be parsed into an python abstract syntax tree (AST).</td><td>Files that cannot be parsed into an AST contain syntax errors and should be filtered out.</td><td>score == False</td></tr><tr><td>The proportion of lines that are &ldquo;import&rdquo; statements.</td><td>A file with exceeding prportion of &ldquo;import&rdquo; statements indicates to have sparse code logic.</td><td>score > 0.3</td></tr></tbody></table></table></figure><blockquote><p>üîº Table 12 presents examples of filtering rules specifically designed for Python code within the data preprocessing pipeline. These rules leverage Python-specific syntax and characteristics to identify and remove low-quality code snippets, improving the overall quality of the training dataset. Each rule includes a description of the characteristic being checked, an explanation of why that characteristic is indicative of low-quality code, and the filtering threshold applied.</p><details><summary>read the caption</summary>Table 12: Examples of python-specific filtering rules.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Level</th><th># Total Lines</th><th># Retained Lines</th><th># Retained Tokens</th></tr></thead><tbody><tr><td>Chunk-level</td><td>333,007,812</td><td>79,272,460</td><td>324.70 B</td></tr><tr><td>File-level</td><td>485,817,123</td><td>30,488,834</td><td>32.74 B</td></tr><tr><td>File-level + Chunk-level</td><td>333,007,812</td><td>7,993,164</td><td>32.70 B</td></tr><tr><td>Repo-level</td><td>11,037,352</td><td>7,480,488</td><td>99.47 B</td></tr><tr><td>Repo-level + Chunk-level</td><td>333,007,812</td><td>17,675,781</td><td>72.40 B</td></tr></tbody></table></table></figure><blockquote><p>üîº This table compares different deduplication methods used on Python code data for model training. It shows the total number of lines of code before deduplication, the number of lines retained after applying various deduplication strategies (file-level, repository-level, and chunk-level), and the resulting number of tokens. The key difference is how deduplication is performed: file-level considers individual files, repository-level treats all files within a repository as one unit, and chunk-level works on 4096-token segments of code. The table clarifies the line count units for each strategy to avoid ambiguity.</p><details><summary>read the caption</summary>Table 13: Comparison of deduplication strategies on Python data. At the File level, 'Lines' refers to the number of lines in individual files; at the Repo level, it indicates the line count of aggregated strings; Note that for all deduplication strategies involving the Chunk level, 'Lines' specifically refers to 4096-token chunks.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Domain</th><th>Prefix</th><th>Tag</th></tr></thead><tbody><tr><td>cloud.tencent.com</td><td>%cloud.tencent.com/developer/article%</td><td>Code</td></tr><tr><td>cloud.tencent.com</td><td>%cloud.tencent.com/ask%</td><td>Code</td></tr><tr><td>cloud.tencent.com</td><td>%cloud.tencent.com/developer/information%</td><td>Code</td></tr><tr><td>cloud.tencent.com</td><td>%cloud.tencent.com/document%</td><td>Code</td></tr><tr><td>my.oschina.net</td><td>%my.oschina.net%blog%</td><td>Code</td></tr><tr><td>ask.csdn.net</td><td>%ask.csdn.net/questions%</td><td>Code</td></tr><tr><td><a href=https://www.cnblogs.com target=_blank>www.cnblogs.com</a></td><td>%www.cnblogs.com%</td><td>Code</td></tr><tr><td>forum.ubuntu.org.cn</td><td>%forum.ubuntu.org.cn%</td><td>Code</td></tr><tr><td>q.cnblogs.com</td><td>%q.cnblogs.com/q%</td><td>Code</td></tr><tr><td>segmentfault.com</td><td>%segmentfault.com/q%</td><td>Code</td></tr><tr><td>segmentfault.com</td><td>%segmentfault.com/a%</td><td>Code</td></tr><tr><td>woshipm.com</td><td>%woshipm.com/data-analysis%</td><td>Code</td></tr><tr><td>zgserver.com</td><td>%zgserver.com/server%</td><td>Code</td></tr><tr><td>zgserver.com</td><td>%zgserver.com/linux%</td><td>Code</td></tr><tr><td>zgserver.com</td><td>%zgserver.com/ubuntu%</td><td>Code</td></tr><tr><td>juejin.cn</td><td>%juejin.cn/post%</td><td>Code</td></tr><tr><td>jiqizhixin.com</td><td>%jiqizhixin.com/articles%</td><td>Code</td></tr><tr><td>help.aliyun.com</td><td>%help.aliyun.com/zh%</td><td>Code</td></tr><tr><td>jyeoo.com</td><td>%jyeoo.com%</td><td>Math</td></tr><tr><td><a href=https://www.haihongyuan.com target=_blank>www.haihongyuan.com</a></td><td>%haihongyuan.com%shuxue%</td><td>Math</td></tr><tr><td><a href=https://www.03964.com target=_blank>www.03964.com</a></td><td>%www.03964.com%</td><td>Math</td></tr><tr><td><a href=https://www.nbhkdz.com target=_blank>www.nbhkdz.com</a></td><td>%www.nbhkdz.com%</td><td>Math</td></tr><tr><td>9512.net</td><td>%9512.net%</td><td>Math</td></tr><tr><td>lanxicy.com</td><td>%lanxicy.com%</td><td>Math</td></tr><tr><td>bbs.emath.ac.cn</td><td>%bbs.emath.ac.cn%</td><td>Math</td></tr><tr><td>math.pro</td><td>%math.pro%</td><td>Math</td></tr><tr><td>mathschina.com</td><td>%mathschina.com%</td><td>Math</td></tr><tr><td>shuxue.chazidian.com</td><td>%shuxue.chazidian.com%</td><td>Math</td></tr><tr><td>shuxue.ht88.com</td><td>%shuxue.ht88.com%</td><td>Math</td></tr></tbody></table></table></figure><blockquote><p>üîº This table details the manually annotated Chinese web domains categorized as either code-related or math-related. The annotation uses the &lsquo;%&rsquo; symbol as a wildcard to match URL patterns, allowing for flexible identification of relevant domains. For example, the pattern &lsquo;%my.oschina.net%blog%&rsquo; would match URLs like &lsquo;<a href=https://my.oschina.net/u/4/blog/11%27 target=_blank>https://my.oschina.net/u/4/blog/11'</a>. This list of domains was used as seed data for identifying similar web pages during data collection.</p><details><summary>read the caption</summary>Table 14: We manually annotate code-like and math-like Chinese domains, utilizing the ‚Äô%‚Äô symbol as a wildcard in our pattern matching. For example, the URL ‚Äôhttps://my.oschina.net/u/4/blog/11‚Äô is matched by the pattern ‚Äô%my.oschina.net%blog%‚Äô.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Model</th><th># Tokens</th><th># Languages</th><th># Web Data Tokens</th><th># Rules</th><th>LS Rules</th></tr></thead><tbody><tr><td>The Stack v1</td><td>200 B</td><td>88</td><td>\</td><td>~15</td><td>‚úó</td></tr><tr><td>The Stack v2</td><td>900 B</td><td>619</td><td>~30 B</td><td>~15</td><td>‚úó</td></tr><tr><td><strong>RefineCode</strong></td><td>960 B</td><td>607</td><td>~75 B</td><td>~130</td><td>‚úì</td></tr></tbody></table></table></figure><blockquote><p>üîº This table compares the training data used in RefineCode with that of two previous versions of The Stack dataset. It highlights key differences in the size of the datasets (measured in tokens and the number of programming languages included), and details the number of filtering rules applied during dataset creation. Importantly, it notes whether language-specific rules were used in the process, indicating a more sophisticated approach to data refinement in RefineCode compared to The Stack.</p><details><summary>read the caption</summary>Table 15: The Comparison of training data between RefineCode and series of The Stack. ‚ÄúLS‚Äù denotes ‚ÄúLanguage Specific‚Äù.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Language</th><th># Files (After deduplication)</th><th>Vol(GB) (After deduplication)</th><th>Ratio(%) (After deduplication)</th><th># Files (After filtering)</th><th>Vol(GB) (After filtering)</th><th>Ratio(%) (After filtering)</th></tr></thead><tbody><tr><td>html</td><td>141,081,897</td><td>3,175.4</td><td>8.56</td><td>45,100,466</td><td>582.4</td><td>18.08</td></tr><tr><td>java</td><td>215,177,833</td><td>706.8</td><td>1.90</td><td>124,751,295</td><td>474.3</td><td>14.72</td></tr><tr><td>python</td><td>109,725,362</td><td>493.3</td><td>1.33</td><td>58,640,346</td><td>271.1</td><td>8.41</td></tr><tr><td>csharp</td><td>88,825,202</td><td>364.2</td><td>0.98</td><td>57,910,485</td><td>232.4</td><td>7.21</td></tr><tr><td>javascript</td><td>190,670,421</td><td>1,925.0</td><td>5.19</td><td>69,579,517</td><td>226.9</td><td>7.04</td></tr><tr><td>php</td><td>84,378,361</td><td>374.4</td><td>1.01</td><td>60,089,397</td><td>222.7</td><td>6.91</td></tr><tr><td>cpp</td><td>51,362,503</td><td>375.2</td><td>1.01</td><td>38,037,406</td><td>176.9</td><td>5.49</td></tr><tr><td>go</td><td>35,649,865</td><td>301.1</td><td>0.81</td><td>26,723,829</td><td>153.7</td><td>4.77</td></tr><tr><td>typescript</td><td>40,211,985</td><td>287.4</td><td>0.77</td><td>20,621,755</td><td>140.4</td><td>4.35</td></tr><tr><td>ruby</td><td>15,735,042</td><td>244.5</td><td>0.66</td><td>8,285,561</td><td>122.7</td><td>3.81</td></tr><tr><td>perl</td><td>16,354,543</td><td>121.7</td><td>0.33</td><td>9,532,620</td><td>65.6</td><td>2.04</td></tr><tr><td>rust</td><td>10,605,421</td><td>63.6</td><td>0.17</td><td>6,086,150</td><td>39.9</td><td>1.24</td></tr><tr><td>r</td><td>6,132,978</td><td>92.5</td><td>0.25</td><td>4,803,109</td><td>34.7</td><td>1.08</td></tr><tr><td>swift</td><td>4,238,754</td><td>47.9</td><td>0.13</td><td>2,938,498</td><td>31.8</td><td>0.99</td></tr><tr><td>kotlin</td><td>4,493,548</td><td>56.4</td><td>0.15</td><td>3,123,156</td><td>29.8</td><td>0.94</td></tr><tr><td>dart</td><td>4,087,329</td><td>33.0</td><td>0.09</td><td>2,161,462</td><td>18.5</td><td>0.57</td></tr><tr><td>java-pages</td><td>6,174,654</td><td>31.0</td><td>0.08</td><td>4,145,336</td><td>15.4</td><td>0.48</td></tr><tr><td>css</td><td>39,822,744</td><td>241.5</td><td>0.65</td><td>15,771,061</td><td>15.3</td><td>0.47</td></tr><tr><td>lua</td><td>4,027,221</td><td>116.0</td><td>0.31</td><td>2,538,234</td><td>14.4</td><td>0.45</td></tr><tr><td>xml</td><td>61,171,289</td><td>1,934.2</td><td>5.21</td><td>3,173,128</td><td>12.8</td><td>0.40</td></tr><tr><td>scala</td><td>5,897,567</td><td>19.7</td><td>0.05</td><td>4,204,979</td><td>11.7</td><td>0.36</td></tr><tr><td>shell</td><td>12,054,632</td><td>23.0</td><td>0.06</td><td>6,043,070</td><td>11.2</td><td>0.35</td></tr><tr><td>pascal</td><td>1,306,130</td><td>27.8</td><td>0.07</td><td>960,497</td><td>9.5</td><td>0.29</td></tr><tr><td>fortran</td><td>2,274,663</td><td>39.7</td><td>0.10</td><td>1,218,491</td><td>8.6</td><td>0.27</td></tr><tr><td>perl6</td><td>1,943,430</td><td>16.4</td><td>0.04</td><td>1,034,748</td><td>8.6</td><td>0.27</td></tr><tr><td>rmarkdown</td><td>1,317,760</td><td>14.0</td><td>0.04</td><td>827,951</td><td>7.9</td><td>0.25</td></tr><tr><td>html+erb</td><td>7,618,377</td><td>11.4</td><td>0.03</td><td>4,452,355</td><td>7.8</td><td>0.24</td></tr><tr><td>smali</td><td>3,457,531</td><td>37.9</td><td>0.10</td><td>1,408,274</td><td>7.4</td><td>0.23</td></tr><tr><td>scss</td><td>18,061,278</td><td>35.6</td><td>0.10</td><td>7,705,822</td><td>7.4</td><td>0.23</td></tr><tr><td>gettext catalog</td><td>1,100,044</td><td>51.3</td><td>0.14</td><td>442,385</td><td>6.3</td><td>0.19</td></tr><tr><td>haskell</td><td>1,746,444</td><td>24.0</td><td>0.06</td><td>1,218,491</td><td>6.8</td><td>0.27</td></tr><tr><td>tcl</td><td>253,345</td><td>4.2</td><td>0.01</td><td>136,171</td><td>1.0</td><td>0.03</td></tr><tr><td>gradle</td><td>2,431,985</td><td>2.9</td><td>0.01</td><td>724,609</td><td>1.0</td><td>0.03</td></tr><tr><td>scheme</td><td>357,909</td><td>4.7</td><td>0.01</td><td>201,170</td><td>1.0</td><td>0.03</td></tr><tr><td>qml</td><td>354,756</td><td>1.8</td><td>0.01</td><td>252,621</td><td>1.0</td><td>0.03</td></tr><tr><td>mdx</td><td>795,525</td><td>6.4</td><td>0.17</td><td>222,013</td><td>1.0</td><td>0.03</td></tr><tr><td>classic asp</td><td>220,344</td><td>2.8</td><td>0.08</td><td>141,236</td><td>0.9</td><td>0.03</td></tr><tr><td>xbase</td><td>192,780</td><td>2.5</td><td>0.07</td><td>80,396</td><td>0.9</td><td>0.03</td></tr><tr><td>ini</td><td>7,232,136</td><td>19.1</td><td>0.05</td><td>1,517,099</td><td>1.3</td><td>0.04</td></tr><tr><td>objective-c++</td><td>197,416</td><td>2.4</td><td>0.01</td><td>149,223</td><td>1.3</td><td>0.04</td></tr><tr><td>motorola68k</td><td>1,066,095</td><td>26.5</td><td>0.07</td><td>220,218</td><td>1.2</td><td>0.04</td></tr><tr><td>gap</td><td>752,261</td><td>2.6</td><td>0.01</td><td>510,420</td><td>1.2</td><td>0.04</td></tr></tbody></table></table></figure><blockquote><p>üîº Table 16 presents a detailed breakdown of the composition of the RefineCode dataset, specifically focusing on the top 85 programming languages. It shows the number of files and the volume (in GB) before and after deduplication and filtering for each language. The languages are listed in descending order based on their file volume after the filtering process, offering insights into the data&rsquo;s distribution and the impact of data cleaning steps.</p><details><summary>read the caption</summary>Table 16: Overview of the data composition of in RefineCode. The items in the table are sorted in descending order according to the file volume after filtering.</details></blockquote></details><h3 class="relative group">Full paper<div id=full-paper class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#full-paper aria-label=Anchor>#</a></span></h3><div id=gallery-ace5262b4796822fd8a252320da3f6fa class=gallery><img src=https://ai-paper-reviewer.com/2411.04905/1.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.04905/2.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.04905/3.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.04905/4.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.04905/5.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.04905/6.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.04905/7.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.04905/8.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.04905/9.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.04905/10.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.04905/11.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.04905/12.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.04905/13.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.04905/14.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.04905/15.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.04905/16.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.04905/17.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.04905/18.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.04905/19.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.04905/20.png class="grid-w50 md:grid-w33 xl:grid-w25"></div></div><section class="flex flex-row flex-wrap justify-center pt-4 text-xl"><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04905/&amp;title=OpenCoder:%20The%20Open%20Cookbook%20for%20Top-Tier%20Code%20Large%20Language%20Models" title="Share on LinkedIn" aria-label="Share on LinkedIn"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://twitter.com/intent/tweet/?url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04905/&amp;text=OpenCoder:%20The%20Open%20Cookbook%20for%20Top-Tier%20Code%20Large%20Language%20Models" title="Tweet on Twitter" aria-label="Tweet on Twitter"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="mailto:?body=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04905/&amp;subject=OpenCoder:%20The%20Open%20Cookbook%20for%20Top-Tier%20Code%20Large%20Language%20Models" title="Send via email" aria-label="Send via email"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1-27.64 140.9 68.65 266.2 199.1 285.1 19.01 2.888 36.17-12.26 36.17-31.49l1e-4-.6631c0-15.74-11.44-28.88-26.84-31.24-84.35-12.98-149.2-86.13-149.2-174.2.0-102.9 88.61-185.5 193.4-175.4 91.54 8.869 158.6 91.25 158.6 183.2v16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98.0036c-7.299.0-13.2 4.992-15.12 11.68-24.85-12.15-54.24-16.38-86.06-5.106-38.75 13.73-68.12 48.91-73.72 89.64-9.483 69.01 43.81 128 110.9 128 26.44.0 50.43-9.544 69.59-24.88 24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3c0-149.2-133.9-265.632-287.3-235.57zM239.1 304.3c-26.47.0-48-21.56-48-48.05s21.53-48.05 48-48.05 48 21.56 48 48.05-20.6 48.05-48 48.05z"/></svg></span></a></section></div><script>var oid="views_paper-reviews/2411.04905/index.md",oid_likes="likes_paper-reviews/2411.04905/index.md"</script><script type=text/javascript src=/ai-paper-reviewer/js/page.min.0860cf4e04fa2d72cc33ddba263083464d48f67de06114529043cb4623319efed4f484fd7f1730df5abea0e2da6f3538855634081d02f2d6e920b956f063e823.js integrity="sha512-CGDPTgT6LXLMM926JjCDRk1I9n3gYRRSkEPLRiMxnv7U9IT9fxcw31q+oOLabzU4hVY0CB0C8tbpILlW8GPoIw=="></script></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="flex group mr-3" href=/ai-paper-reviewer/paper-reviews/2411.05003/><span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&larr;</span>
<span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">ReCapture: Generative Video Camera Controls for User-Provided Videos using Masked Video Fine-Tuning</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2024-11-07T00:00:00+00:00>7 November 2024</time>
</span></span></a></span><span><a class="flex text-right group ml-3" href=/ai-paper-reviewer/paper-reviews/2411.05000/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Needle Threading: Can LLMs Follow Threads through Near-Million-Scale Haystacks?</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2024-11-07T00:00:00+00:00>7 November 2024</time>
</span></span><span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&larr;</span></a></span></div></div><div class=pt-3><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class=pt-3><script src=https://utteranc.es/client.js repo=pmnxis/pmnxis.github.io issue-term=pathname label=Comment theme=dark-blue crossorigin=anonymous async></script></div></div></footer></article><div id=top-scroller class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer id=site-footer class="py-10 print:hidden"><nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400"><ul class="flex flex-col list-none sm:flex-row"><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=/ai-paper-reviewer/tags/ title>Tags</a></li></ul></nav><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2024
AI Paper Reviews by AI</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a></p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/ai-paper-reviewer/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh+sCQ0E53ghYrxgYqw+0GCRyIEpA=="></script></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://deep-diver.github.io/ai-paper-reviewer/ style=z-index:500><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body><script data-name=BMC-Widget data-cfasync=false src=https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js data-id=chansung data-description="Support me on Buy me a coffee!" data-message data-color=#FFDD00 data-position=Left data-x_margin=18 data-y_margin=18></script></html>