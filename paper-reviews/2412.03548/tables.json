[{"content": "| Model | Direct Labeling Data |  | Depth Generation Data |  | CoT Data |  | BLINK 2 Points |  | HardBLINK 3 Points |  | HardBLINK 4 Points |  | HardBLINK 5 Points |  | Average |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| LLaVA OneVision | \u2717 |  | \u2717 |  | \u2717 |  | 51.6 |  | 33.1 |  | 22.6 |  | 18.5 |  | 31.4 |\n| LLaVA 1.5 13B | \u2717 |  | \u2717 |  | \u2717 |  | 54.0 |  | 35.5 |  | 37.9 |  | 29 |  | 39.1 |\n| Fine-tunned LLaVA | \u2713 |  | \u2717 |  | \u2717 |  | 68.5 |  | 58.9 |  | 52.4 |  | 41.1 |  | 55.2 |\n| LLaVA-Aurora (Ours) | \u2713 |  | \u2713 |  | \u2713 |  | 64.5 |  | **66.9** |  | **60.5** |  | **54.8** |  | **61.6** |\n| <span style=\"color:#808080;\">GPT-4o</span> | \u2717 |  | \u2717 |  | \u2717 |  | <span style=\"color:#808080;\">53.2</span> |  | <span style=\"color:#808080;\">58.9</span> |  | <span style=\"color:#808080;\">50</span> |  | <span style=\"color:#808080;\">36.3</span> |  | <span style=\"color:#808080;\">49.6</span> |\n| <span style=\"color:#808080;\">GPT-4 Turbo</span> | \u2717 |  | \u2717 |  | \u2717 |  | <span style=\"color:#808080;\">58.1</span> |  | <span style=\"color:#808080;\">54.8</span> |  | <span style=\"color:#808080;\">41.9</span> |  | <span style=\"color:#808080;\">32.2</span> |  | <span style=\"color:#808080;\">46.7</span> |\n| <span style=\"color:#808080;\">GPT-4 Turbo + Tool</span> | \u2717 |  | \u2717 |  | \u2717 |  | <span style=\"color:#808080;\">**70.2**</span> |  | <span style=\"color:#808080;\">57.2</span> |  | <span style=\"color:#808080;\">44.3</span> |  | <span style=\"color:#808080;\">26.6</span> |  | <span style=\"color:#808080;\">49.6</span> |", "caption": "Table 1: Performance comparison between our LLaVA-Aurora model, the fine-tunning baseline, and the original base model on the relative depth accuracy (%) task. Results demonstrate that our approach, utilizing depth tokens and intermediate reasoning steps, significantly outperforms both the baseline and the base model, particularly on more challenging configurations with 3, 4, and 5 points sampled from the image\u2019s mid-height region.", "description": "This table presents a performance comparison of three different models on a relative depth estimation task.  The models compared are: the original LLaVA 1.5 13B model, a fine-tuned version of LLaVA, and the LLaVA-AURORA model (the authors' proposed method). The task involves determining which of two or more points in an image is closest to the camera. The performance is measured as the accuracy (%) of the model's predictions.  The difficulty of the task is varied by using different numbers of points (2, 3, 4, and 5 points) sampled from the image's mid-height region (increasing difficulty with a larger number of points).  The results show that LLaVA-AURORA, which uses depth tokens and intermediate reasoning steps significantly outperforms both the baseline and the fine-tuned models, especially when dealing with the more challenging configurations of 3, 4, and 5 points.", "section": "4. Experiments"}, {"content": "| Model | Direct Labeling Data |  | Bounding Box Data |  | CoT Data |  | CV-Bench Counting |  | SEED-Bench Counting |  | BLINK Counting |\n|---|---|---|---|---|---|---|---|---|---|---|---| \n| **LLaVA One Vision** | \u2717 |  | \u2717 |  | \u2717 |  | 34.4 |  | 31.7 |  | 35.8 |\n| **LLaVA 1.5 13B** | \u2717 |  | \u2717 |  | \u2717 |  | 40.9 |  | 52.2 |  | 35.0 |\n| **Fine-tunned LlaVA** | \u2713 |  | \u2717 |  | \u2717 |  | 44.7 |  | 46.3 |  | 0.2 |\n| **LLaVA-Aurora (Ours)** | \u2713 |  | \u2713 |  | \u2713 |  | 56.0 |  | 54.6 |  | 45.8 |\n| <span style=\"color:#808080;\">GPT-4o</span> | \u2717 |  | \u2717 |  | \u2717 |  | <span style=\"color:#808080;\">70.18</span> |  | <span style=\"color:#808080;\">64.6</span> |  | <span style=\"color:#808080;\">47.5</span> |\n| <span style=\"color:#808080;\">GPT-4 Turbo</span> | \u2717 |  | \u2717 |  | \u2717 |  | <span style=\"color:#808080;\">61.3</span> |  | <span style=\"color:#808080;\">64.8</span> |  | <span style=\"color:#808080;\">57.5</span> |\n| <span style=\"color:#808080;\">GPT-4 Turbo + Tool</span> | \u2717 |  | \u2717 |  | \u2717 |  | <span style=\"color:#808080;\">48.6</span> |  | <span style=\"color:#808080;\">29.9</span> |  | <span style=\"color:#808080;\">26.7</span> |", "caption": "Table 2: Comparison of object counting accuracy (%) across three benchmarks (CV-Bench, SEED-Bench, and BLINK). Our LlaVA-Aurora model, using auxiliary perception tokens to encode bounding box information for intermediate reasoning, demonstrates superior performance compared to the fine-tunning baseline models and the original base model.", "description": "Table 2 presents a comparative analysis of object counting accuracy across three established benchmarks: CV-Bench, SEED-Bench, and BLINK.  The results showcase the performance of various models, including the original LLaVA 1.5 13B model, a fine-tuned version of LLaVA, and the novel LLaVA-AURORA model.  LLaVA-AURORA leverages the technique of incorporating perception tokens, representing bounding box information, to enhance intermediate reasoning during the object counting process. This table highlights the superior performance of LLaVA-AURORA compared to both the fine-tuned LLaVA model and the original base model, demonstrating the effectiveness of the proposed method.", "section": "4. Experiments"}, {"content": "| Model | BLINK [11] 2 points |  | HardBLINK 3 Points |  | HardBLINK 4 Points |  | HardBLINK 5 Points |  | Average |\n|---|---|---|---|---|---|---|---|---|---|\n| VQGAN [10] (16384 codes) | 82.2 |  | 66.1 |  | 53 |  | 37 |  | 59.6 |\n| Unified-IO | 70.2 |  | 75.8 |  | 75.8 |  | 75.8 |  | 74.4 |\n| Unified-IO2 | 54 |  | 37.9 |  | 21 |  | 27.4 |  | 35.1 |\n| LLaVA-Aurora (Ours) | **91.9** |  | **78.2** |  | 71.7 |  | **75.8** |  | **79.2** |\n| Our VQVAE (128 codes) | 96.7 |  | 94.3 |  | 95.2 |  | 96.7 |  | 95.7 |", "caption": "Table 3: While not the main aim of our work, we report the depth generation performance across benchmarks with 2, 3, 4, and 5 marked points using BLINK\u00a0[11]\u2019s relative depth subtask images. We report relative depth estimation accuracy (%), calculated by programmatically extracting depth values at specific coordinates from model-generated depth maps. Our model consistently outperforms other multimodal models, including Unified-IO\u00a0[33] and Unified-IO 2\u00a0[34]", "description": "This table presents a supplementary analysis of depth map generation quality using the BLINK dataset's relative depth subtask.  It compares the performance of various models, including the authors' model (LLaVA-AURORA), Unified-IO, and Unified-IO 2, in generating accurate depth maps.  The evaluation metric is relative depth estimation accuracy, calculated by extracting depth values from the generated maps at specific coordinates and comparing them to ground truth.  The varying number of marked points (2, 3, 4, and 5) in each benchmark image provides a complexity gradient for assessing model performance. The results show that LLaVA-AURORA consistently outperforms other models.", "section": "4. Experiments"}, {"content": "| Model | Coordinates |  | Depth |  | HardBLINK 3 Points |  | HardBLINK 4 Points |  | HardBLINK 5 Points |\n|---|---|---|---|---|---|---|---|---|---| \n| **Direct Labeling Baseline** | \u2717 |  | \u2717 |  | 58.9 |  | 52.4 |  | 41.1 |\n| **Step (2) only** | \u2717 |  | \u2713 |  | 56.4 |  | 56.4 |  | 50 |\n| **LLaVA-Aurora (Ours)** | \u2713 |  | \u2713 |  | **66.9** |  | **60.5** |  | **54.8** |", "caption": "Table 4: Performance comparison of models trained with different Chain of Thought question prompt variations for relative depth estimation on the harder BLINK datasets. Models with both steps in the prompts (Aurora) achieve the best performance.", "description": "This table presents the results of an ablation study on the relative depth estimation task using the harder BLINK dataset.  Three model variations were compared: a direct labeling baseline (no chain-of-thought), a model using only the depth map generation step in the prompt, and the AURORA model (which includes both the coordinate identification and depth map generation steps). The table shows that the AURORA model, incorporating both steps in the chain-of-thought, achieved significantly better performance across all configurations of the harder BLINK dataset (3, 4, and 5 points). This highlights the importance of guiding the model through a multi-step reasoning process for improved accuracy in depth estimation tasks.", "section": "6.1. Chain-of-thought steps"}, {"content": "| Model |  | Token Type |  | CV-Bench Counting |  | SEED-Bench Counting |  | BLINK Counting |\n|---|---|---|---|---|---|---|---|---|\n| LLaVA-Aurora |  | Standard |  | 52.2 |  | 50.6 |  | 38.3 |\n| LLaVA-Aurora |  | Perception |  | 56.0 |  | 54.6 |  | 45.8 |", "caption": "Table 5: Comparison of model performance using perception tokens and standard tokens for the object counting task across three benchmarks: BLINK, SEED-Bench, and CV-Bench. Perception tokens consistently improve accuracy.", "description": "This table presents a comparison of the performance of a model using perception tokens versus standard text tokens for object counting tasks across three benchmark datasets: BLINK, SEED-Bench, and CV-Bench. The results show that using perception tokens consistently leads to improved accuracy in object counting.", "section": "4. Experiments"}, {"content": "| Model |  | Recons Loss |  | BLINK |  | Visual Genome |\n|---|---|---|---|---|---|---|\n| LLaVA 1.5 |  | \u2713 |  | 0.092 |  | **0.074** |\n| LLaVA 1.5 |  | \u2717 |  | **0.087** |  | 0.076 |", "caption": "Table 6: MSE evaluation of models with and without reconstruction loss on subsets BLINK and Visual Genome datasets.", "description": "This table presents a comparison of Mean Squared Error (MSE) for depth map prediction using two models: one trained with a reconstruction loss and another without it.  The MSE is calculated on two datasets: the BLINK dataset (a subset used for evaluating relative depth) and the Visual Genome dataset (a subset used for evaluating depth map generation). Lower MSE values indicate better depth map prediction accuracy.  The table helps to evaluate the contribution of the reconstruction loss to the model's overall performance in terms of depth estimation accuracy.", "section": "6.3. Perception token reconstruction loss"}, {"content": "| Model | CV-Bench Depth |\n|---|---| \n| LLaVA 1.5 13B | 62.2 |\n| Fine-tunned LLaVA | 60.0 |\n| Aurora (Ours) | **64.8** |", "caption": "Table 7: Performance comparison on the CV-Bench Depth subtask, highlighting our model\u2019s generalization ability.", "description": "Table 7 presents a performance comparison on the CV-Bench Depth subtask, demonstrating the generalization capability of the AURORA model. It compares the performance of three models: the LLaVA 1.5 13B base model, a fine-tuned version of LLaVA, and the AURORA model. The results show that AURORA achieves the best performance on this depth estimation task, highlighting its ability to generalize to unseen data and different tasks.", "section": "4. Experiments"}]