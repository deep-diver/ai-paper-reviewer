[{"figure_path": "https://arxiv.org/html/2412.03548/extracted/6053340/figures/aurora_teaser.png", "caption": "Figure 1: We introduce Perception Tokens, intermediate reasoning tokens that allow MLMs to go beyond using language in reasoning. With it, we develop Aurora, a framework that trains multimodal language models to leverage visual perception tokens, allowing them to use depth estimation and bounding box predictions while reasoning.", "description": "The figure illustrates the concept of Perception Tokens and the Aurora framework. Perception Tokens are intermediate reasoning tokens that enhance visual reasoning in multimodal language models (MLMs). They act as auxiliary tokens, similar to chain-of-thought prompts, allowing MLMs to incorporate visual perception information, such as depth maps and bounding boxes, into their reasoning process.  Aurora is a training framework that leverages Perception Tokens to enable MLMs to perform depth estimation and bounding box prediction during visual reasoning tasks. The example in the figure shows how a depth map, represented as Perception Tokens, is used to determine which point is closer to the camera.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2412.03548/extracted/6053340/figures/files/main_figure.png", "caption": "Figure 2: We demonstrate relative depth estimation and counting questions where LLaVA fails. In contrast, by learning to utilize visual perception tokens as intermediate reasoning steps, LLaVA-Aurora successfully complete these tasks requiring perceptual understanding.", "description": "Figure 2 showcases two example tasks: relative depth estimation and object counting.  The left side displays how the original LLaVA model fails on these tasks.  The right side shows how the improved LLaVA-AURORA model, incorporating visual perception tokens as intermediate reasoning steps, successfully completes these tasks. The addition of perception tokens allows LLaVA-AURORA to effectively leverage visual information, thus improving performance on tasks requiring perceptual understanding, which the standard LLaVA model struggled with.  The figure visually demonstrates the significant improvement achieved by integrating visual perception tokens.", "section": "3. Perception Tokens & Aurora"}, {"figure_path": "https://arxiv.org/html/2412.03548/x1.png", "caption": "Figure 3: The overall Aurora training framework. We first learn visual perception tokens using VQVAE. We then finetune MLMs with a multi-task training approach where we distill intrinsic image representations (e.g., depth map) into MLMs by training them to decode the visual tokens as intermediate reasoning steps towards completing the tasks.", "description": "The figure illustrates the AURORA training framework.  AURORA leverages a Vector Quantized Variational Autoencoder (VQVAE) to convert visual representations (like depth maps) into a set of discrete tokens called \"perception tokens\". These tokens are then integrated into a multimodal language model (MLM) during a multi-task training phase. The MLM learns to generate and utilize these perception tokens as intermediate reasoning steps to solve visual reasoning tasks.  This process allows the MLM to incorporate low-level image features into its reasoning process, improving performance on tasks where pure language processing is insufficient.", "section": "3. Perception Tokens & Aurora"}, {"figure_path": "https://arxiv.org/html/2412.03548/extracted/6053340/figures/files/depth_vis.png", "caption": "Figure 4: Depth maps generated by Aurora are imperfect but resemble the ground-truths from Depth Anything\u00a0[51].", "description": "Figure 4 presents a qualitative comparison of depth maps generated by the AURORA model and ground truth depth maps from the Depth Anything model [51]. The AURORA model's depth maps, while not perfect, show a reasonable resemblance to the ground truth, indicating the model's ability to generate depth information that is useful for visual reasoning tasks.  The comparison highlights the model's capacity to produce depth maps which, although imperfect, capture the relative depth of objects in a scene and are useful as input to downstream visual reasoning tasks.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.03548/extracted/6053340/figures/files/recons.png", "caption": "Figure 5:  Qualitative comparison of predicted depth maps with and without reconstruction loss.", "description": "This figure presents a qualitative comparison of depth maps generated by a model trained with and without a reconstruction loss.  It visually demonstrates the impact of the reconstruction loss on the accuracy and detail of the predicted depth maps, allowing for a direct visual assessment of the improvement in the quality of depth estimation.", "section": "6. Ablation study"}, {"figure_path": "https://arxiv.org/html/2412.03548/extracted/6053340/figures/files/depthdata.png", "caption": "Figure 6: Examples of sub-datasets for the depth task: (1) depth generation, (2) Chain-of-Thought reasoning, and (3) direct labeling.", "description": "Figure 6 illustrates the three types of data used to train the model for the relative depth estimation task.  The first, \"Depth Generation Data,\" shows the model generating a depth map represented as a sequence of tokens.  The second, \"Chain of Thought Data,\" demonstrates a multi-step reasoning process to determine the closest point to the camera, using both the coordinates of the points and the depth map generated as intermediate steps. Finally, \"Direct Labeling Data\" shows a simple question-answering approach where the model directly identifies the nearest point.", "section": "3. Perception Tokens & Aurora"}, {"figure_path": "https://arxiv.org/html/2412.03548/extracted/6053340/figures/files/countdata.png", "caption": "Figure 7: Examples of sub-datasets for the counting task: (1) bounding box prediction, (2) Chain-of-Thought reasoning, and (3) direct labeling.", "description": "Figure 7 illustrates three types of data used for training a model to count objects in images.  The first example shows bounding box predictions, where the model identifies objects and outputs the coordinates of their bounding boxes using perception tokens (PIXEL_X). The second example demonstrates Chain-of-Thought reasoning, where the model goes through a step-by-step process, first identifying bounding boxes for the objects of interest and then counting them. Finally, the third example showcases direct labeling, in which the model is directly given the question and answer, thus learning to count objects without intermediate reasoning steps.  This multifaceted training approach helps the model learn to count accurately through different reasoning strategies.", "section": "4. Experiments"}]