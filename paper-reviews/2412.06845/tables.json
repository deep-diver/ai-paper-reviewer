[{"content": "| Parameter | Value |\n|---|---| \n| n_layers | 36 |\n| dim | 4096 |\n| head_dim | 128 |\n| hidden_dim | 14336 |\n| n_heads | 32 |\n| n_kv_heads | 8 |", "caption": "Table 1: Parameter setting.", "description": "This table details the hyperparameters used in the architecture of the Moxin 7B model.  It shows the values set for key parameters that define the model's structure and behavior, including the number of layers, dimensions, head dimensions, hidden dimensions, and number of heads. These parameters are crucial for understanding the model's complexity and computational requirements.", "section": "3 Model Training"}, {"content": "| Model | MTbench |\n|---|---| \n| **Moxin-7B-chat** | **6.42** |\n| Llama 2 13B Chat | 6.65 |\n| Vicuna 13B | 6.57 |\n| Llama 2 7B Chat | 6.27 |\n| Vicuna 7B | 6.17 |\n| Alpaca 13B | 4.53 |", "caption": "Table 2: Performance for various chat models.", "description": "This table presents a comparison of the performance of several chat models on the MTBench benchmark. MTBench is a two-round conversation dataset that evaluates various conversational capabilities across different dimensions, including reasoning, role-playing, mathematics, coding, writing, and more. Each model's score reflects the quality of its responses as judged by GPT-4, providing a comprehensive assessment of its conversational abilities.", "section": "4.3 Alignment Evaluation"}]