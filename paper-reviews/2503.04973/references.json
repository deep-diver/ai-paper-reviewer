{"references": [{"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-01-01", "reason": "This paper introduces the Transformer architecture, which is the foundation for most modern large language models, making it fundamentally important."}, {"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-01-01", "reason": "This paper demonstrates the in-context learning abilities of large language models, which is a key motivation for the task-aware compression strategy presented in the main paper."}, {"fullname_first_author": "Patrick Lewis", "paper_title": "Retrieval-augmented generation for knowledge-intensive nlp tasks", "publication_date": "2020-01-01", "reason": "This paper introduces Retrieval-Augmented Generation (RAG), a key baseline method that the main paper aims to improve upon with task-aware KV cache compression."}, {"fullname_first_author": "Yushi Bai", "paper_title": "Longbench: A bilingual, multitask benchmark for long context understanding", "publication_date": "2023-01-01", "reason": "This paper presents LongBench, which is a key benchmark for evaluating performance of long-context language models; it is used to evaluate the methods introduced in the main paper."}, {"fullname_first_author": "Giulio Corallo", "paper_title": "Finch: Prompt-guided key-value cache compression for large language models", "publication_date": "2024-01-01", "reason": "This paper introduces a query-aware method to compress the key-value (KV) cache, and is used as the basis to build the novel Task-aware, Query-Agnostic Compression strategy introduced in the main paper."}]}