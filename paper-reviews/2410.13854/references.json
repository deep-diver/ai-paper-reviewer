{"references": [{" publication_date": "2023", "fullname_first_author": "Haotian Liu", "paper_title": "Improved baselines with visual instruction tuning", "reason": "This paper is highly relevant due to its focus on visual instruction tuning for multimodal large language models (MLLMs), a crucial aspect of enhancing their understanding of visual content, which is central to this study's focus on deeper interpretation of culturally significant images.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Haotian Liu", "paper_title": "Visual instruction tuning", "reason": "This paper focuses on improving MLLMs' ability to understand and process visual instructions, enhancing the model's ability to understand complex scenes, which is essential for interpreting the deeper implications within images that are culturally relevant.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Haotian Liu", "paper_title": "LLaVA-Next: Improved reasoning, OCR, and world knowledge", "reason": "This paper is directly relevant because it addresses the limitations of previous MLLMs in reasoning, OCR, and knowledge, all of which are necessary for successfully understanding culturally significant imagery.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Yuan Liu", "paper_title": "MMBench: Is your multi-modal model an all-around player?", "reason": "This paper is important because it offers a comprehensive benchmark for evaluating multimodal LLMs. CII-Bench aims to build on this by creating a more advanced and culturally sensitive benchmark focusing on nuanced understanding.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Ziqiang Liu", "paper_title": "Ii-bench: An image implication understanding benchmark for multimodal large language models", "reason": "This paper is highly relevant as it proposes a benchmark for evaluating image implication understanding in MLLMs.  CII-Bench extends upon this work by focusing on the Chinese cultural context.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Pan Lu", "paper_title": "Learn to explain: Multimodal reasoning via thought chains for science question answering", "reason": "This paper is significant as it introduces a novel approach to multimodal reasoning using thought chains.  This approach is highly relevant to understanding the deeper implications within culturally-rich images, which is a core focus of the current research.", "section_number": 2}, {" publication_date": "2015", "fullname_first_author": "Stanislaw Antol", "paper_title": "VQA: Visual question answering", "reason": "This paper is highly relevant as it presents a benchmark for visual question answering (VQA), a fundamental task in multimodal understanding. The current work aims to go beyond simple VQA by incorporating deeper cultural understanding in Chinese image implication.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Chaoyou Fu", "paper_title": "Mme: A comprehensive evaluation benchmark for multimodal large language models", "reason": "This paper is highly relevant to this study's focus on benchmarking multimodal LLMs. The current research aims to extend upon this by developing a benchmark specifically tailored to assess MLLMs' cultural sensitivity.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Zhe Chen", "paper_title": "How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites", "reason": "This paper is important as it highlights the development of open-source multimodal LLMs aiming to match the performance of closed-source models.  The current research contributes to this trend by providing a benchmark to further assess their capabilities.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Lin Chen", "paper_title": "Are we on the right way for evaluating large vision-language models?", "reason": "This paper is highly relevant because it addresses the challenges of effectively evaluating large vision-language models. The current work contributes by focusing specifically on evaluating the higher-order capabilities of MLLMs with Chinese images.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Shengbang Tong", "paper_title": "Cambrian-1: A fully open, vision-centric exploration of multimodal llms", "reason": "This paper is important for introducing a vision-centric benchmark that is fully open. The current research builds upon this by incorporating the culturally rich Chinese imagery context.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Bohao Li", "paper_title": "Seed-bench-2: Benchmarking multimodal large language models", "reason": "This paper is highly relevant as it provides a comprehensive benchmark for evaluating multimodal LLMs. The current research aims to build on this by focusing specifically on the deeper implications of Chinese images.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Bohao Li", "paper_title": "Seed-bench: Benchmarking multimodal llms with generative comprehension", "reason": "This paper is important because it proposes a benchmark for assessing generative comprehension in multimodal LLMs. The current research builds upon this foundation to create a benchmark that particularly emphasizes culturally sensitive image interpretation.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Sharan Aakanksha", "paper_title": "Palm: Scaling language modeling with pathways", "reason": "This paper is significant as it showcases advancements in scaling language models. This is relevant to the development of MLLMs which require large-scale models for effective multimodal understanding.", "section_number": 2}, {" publication_date": "2019", "fullname_first_author": "Harsh Agrawal", "paper_title": "Nocaps: Novel object captioning at scale", "reason": "This paper is important because it addresses the task of object captioning at scale, which is a prerequisite for developing sophisticated image implication understanding models, which is the focus of this study.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "Alexey Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "reason": "This paper is significant because it introduces transformers for image recognition, a key technique used in many MLLMs. The current work improves upon the work of image recognition by assessing cultural sensitivity and nuanced understanding.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Rizhao Cai", "paper_title": "Benchlmm: Benchmarking cross-style visual capability of large multimodal models", "reason": "This paper is important for introducing a benchmark for assessing cross-style visual capability.  CII-Bench builds upon this by extending the evaluation to include deeper cultural understanding.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Zheng Cai", "paper_title": "InternLM2 technical report", "reason": "This paper is highly relevant because it presents a technical report on InternLM2, a powerful multimodal model. The current work builds upon this by assessing the cultural and nuanced understanding of InternLM2.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Lin Chen", "paper_title": "Are we on the right way for evaluating large vision-language models?", "reason": "This paper provides a valuable critique of existing vision-language model evaluation methods, paving the way for more sophisticated methods like CII-Bench that incorporate cultural and contextual understanding.  It helps establish the need for a more nuanced approach.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Ziqiang Liu", "paper_title": "Ii-bench: An image implication understanding benchmark for multimodal large language models", "reason": "This paper is directly relevant to the study as it introduces II-Bench, a benchmark for image implication understanding in MLLMs. The current research builds upon this by developing a benchmark specifically focused on the cultural nuances of Chinese images.", "section_number": 2}]}