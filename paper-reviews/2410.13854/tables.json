[{"figure_path": "2410.13854/tables/table_4_0.html", "caption": "Table 1: Overall results of different MLLMs, LLMs and humans on different domains and emotions. The best-performing model in each category is in-bold, and the second best is underlined.", "description": "Table 1 presents the overall performance of various multimodal large language models (MLLMs), large language models (LLMs), and humans on the CII-Bench, broken down by different domains and emotional categories.", "section": "4.2 Main Results"}, {"figure_path": "2410.13854/tables/table_7_0.html", "caption": "Table 1: Overall results of different MLLMs, LLMs and humans on different domains and emotions. The best-performing model in each category is in-bold, and the second best is underlined.", "description": "Table 1 presents the overall performance of various Multimodal Large Language Models (MLLMs), Large Language Models (LLMs), and humans across different domains and emotional polarities in the CII-Bench benchmark.", "section": "4.2 Main Results"}, {"figure_path": "2410.13854/tables/table_8_0.html", "caption": "Table 2: Overall results of different prompts on CII-Bench. The label (Emotion, Domain, Rhetoric) means providing corresponding information for the images in the prompt. The best-performing model in each category is in-bold, and the second best is underlined.", "description": "Table 2 presents the overall results of different prompts (None, CoT, Domain, Emotion, Rhetoric) on the CII-Bench benchmark, showing the accuracy of various open-source and closed-source models.", "section": "4.2 Main Results"}, {"figure_path": "2410.13854/tables/table_9_0.html", "caption": "Table 3: Few-shot results of different models on the CII-Bench.", "description": "Table 3 shows the few-shot performance of different models on the CII-Bench benchmark, indicating the impact of providing a small number of examples on model accuracy.", "section": "4.3 EVALUATION OF CHINESE TRADITIONAL CULTURE"}, {"figure_path": "2410.13854/tables/table_10_0.html", "caption": "Table 4: Overall result of Chinese traditional painting.", "description": "Table 4 presents the overall performance and results of Chinese traditional painting evaluation based on GPT-40, categorized by difficulty levels and emotional polarities.", "section": "4.3.2 LLM-BASED CHINESE TRADITIONAL PAINTING AUTOMATIC EVALUATION"}, {"figure_path": "2410.13854/tables/table_16_0.html", "caption": "Table 1: Overall results of different MLLMs, LLMs and humans on different domains and emotions. The best-performing model in each category is in-bold, and the second best is underlined.", "description": "Table 1 presents the overall performance of various MLLMs, LLMs, and humans on the CII-Bench across different domains and emotional polarities.", "section": "4.2 Main Results"}, {"figure_path": "2410.13854/tables/table_16_1.html", "caption": "Table 1: Overall results of different MLLMs, LLMs and humans on different domains and emotions. The best-performing model in each category is in-bold, and the second best is underlined.", "description": "Table 1 presents the overall performance of various multimodal large language models (MLLMs), large language models (LLMs), and humans across different domains and emotional polarities on the Chinese Image Implication understanding Benchmark (CII-Bench).", "section": "4.2 Main Results"}, {"figure_path": "2410.13854/tables/table_20_0.html", "caption": "Table 1: Overall results of different MLLMs, LLMs and humans on different domains and emotions. The best-performing model in each category is in-bold, and the second best is underlined.", "description": "Table 1 presents the overall performance of various multi-modal large language models (MLLMs), large language models (LLMs), and humans across different domains and emotional categories on the CII-Bench benchmark.", "section": "4.2 MAIN RESULTS"}, {"figure_path": "2410.13854/tables/table_20_1.html", "caption": "Table 1: Overall results of different MLLMs, LLMs and humans on different domains and emotions. The best-performing model in each category is in-bold, and the second best is underlined.", "description": "Table 1 presents the overall performance of various multi-modal large language models (MLLMs), large language models (LLMs), and humans across different domains and emotional polarities on the CII-Bench benchmark.", "section": "4.2 Main Results"}, {"figure_path": "2410.13854/tables/table_20_2.html", "caption": "Table 1: Overall results of different MLLMs, LLMs and humans on different domains and emotions. The best-performing model in each category is in-bold, and the second best is underlined.", "description": "Table 1 presents the overall performance of various Multimodal Large Language Models (MLLMs), Large Language Models (LLMs), and humans across different domains and emotional categories on the CII-Bench.", "section": "4.2 Main Results"}, {"figure_path": "2410.13854/tables/table_22_0.html", "caption": "Table 6: Overall results of different MLLMs on different image types. The best-performing model in each category is in-bold, and the second best is underlined. For brevity, Illus. refers to Illustration, Paint. refers to Painting, Single-C. refers to Single-panel Comic, Multi-C. refers to Multi-panel Comic.", "description": "Table 6 presents the overall performance of various Multimodal Large Language Models (MLLMs) and humans across different image types (Illustration, Painting, Poster, Single-panel Comic, Multi-panel Comic, Meme).", "section": "4.2 Main Results"}, {"figure_path": "2410.13854/tables/table_22_1.html", "caption": "Table 7: Overall results of different MLLMs on various difficulty levels. The best-performing model in each category is in-bold, and the second best is underlined. The numbers in parentheses indicate the number of samples in each category.", "description": "Table 7 presents the overall performance of various multi-modal large language models (MLLMs) and humans across different difficulty levels (easy, medium, hard) on a benchmark for understanding image implications.", "section": "4.2 MAIN RESULTS"}, {"figure_path": "2410.13854/tables/table_23_0.html", "caption": "Table 8: Overall results of different MLLMs and humans on different rhetoric. The best-performing model in each category is in-bold, and the second best is underlined. For brevity, Meta. refers to Metaphor, Exag. refers to Exaggerate, Symb. refers to Symbolism, VisD. refers to Visual Dislocation, Anti. refers to Antithesis, Anal. refers to Analogy, Pers. refers to Personification", "description": "Table 8 presents the overall results of various multimodal large language models and human participants on different rhetoric types, showing the best-performing model in each rhetoric category.", "section": "4.2 Main Results"}, {"figure_path": "2410.13854/tables/table_24_0.html", "caption": "Table 1: Overall results of different MLLMs, LLMs and humans on different domains and emotions. The best-performing model in each category is in-bold, and the second best is underlined.", "description": "Table 1 presents the overall performance of various multimodal large language models (MLLMs), large language models (LLMs), and humans across different domains and emotional categories on the CII-Bench benchmark.", "section": "4.2 Main Results"}, {"figure_path": "2410.13854/tables/table_24_1.html", "caption": "Table 1: Overall results of different MLLMs, LLMs and humans on different domains and emotions. The best-performing model in each category is in-bold, and the second best is underlined.", "description": "Table 1 presents the overall performance of various multimodal large language models (MLLMs), large language models (LLMs), and humans across different domains and emotional polarities on the CII-Bench benchmark.", "section": "4.2 Main Results"}, {"figure_path": "2410.13854/tables/table_25_0.html", "caption": "Table 1: Overall results of different MLLMs, LLMs and humans on different domains and emotions. The best-performing model in each category is in-bold, and the second best is underlined.", "description": "Table 1 presents the overall performance of various multimodal large language models (MLLMs), large language models (LLMs), and humans across different domains (Life, Art, Society, Politics, Environment, and Chinese Traditional Culture) and emotional polarities (Positive, Negative, Neutral).", "section": "4.2 Main Results"}, {"figure_path": "2410.13854/tables/table_25_1.html", "caption": "Table 1: Overall results of different MLLMs, LLMs and humans on different domains and emotions. The best-performing model in each category is in-bold, and the second best is underlined.", "description": "Table 1 presents the overall performance of various multimodal large language models (MLLMs), large language models (LLMs), and humans across different domains and emotional categories on the Chinese Image Implication Understanding Benchmark (CII-Bench).", "section": "4.2 Main Results"}, {"figure_path": "2410.13854/tables/table_26_0.html", "caption": "Table 1: Overall results of different MLLMs, LLMs and humans on different domains and emotions. The best-performing model in each category is in-bold, and the second best is underlined.", "description": "Table 1 presents the overall performance of various MLLMs, LLMs, and humans across different domains and emotional categories on the CII-Bench.", "section": "4.2 Main Results"}]