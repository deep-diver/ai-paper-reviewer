{"references": [{" publication_date": "2021", "fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "reason": "This paper introduces a foundational model, ViT, that is widely used as a backbone for many vision-language models, including those discussed in the current paper.  Its impact on the field of computer vision and its influence on subsequent architectures make it a highly significant reference.", "section_number": 4}, {" publication_date": "2022", "fullname_first_author": "Jean-Baptiste Alayrac", "paper_title": "Flamingo: a visual language model for few-shot learning", "reason": "This paper presents Flamingo, a notable vision-language model that employs a Perceiver Resampler as its vision-language connector.  The architecture and approach are directly relevant to the current paper's design choices and represent a highly influential model in the field.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Junnan Li", "paper_title": "BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models", "reason": "BLIP-2 is directly referenced as the basis for the current paper's image-based vision-language model, BLIP-3.  Its design, training methodologies, and capabilities provide a key foundation for the current work. The use of a Q-Former is a direct inspiration for the current paper's methods.", "section_number": 4}, {" publication_date": "2022", "fullname_first_author": "Andrew Jaegle", "paper_title": "Perceiver IO: A general architecture for structured inputs & outputs", "reason": "Perceiver IO is explicitly mentioned as a foundational model for attentional pooling, a key component of BLIP-3-Video's architecture. Its contributions to efficient attention mechanisms and the ability to handle variable-sized inputs directly impact the performance and design of the proposed model. ", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Michael Ryoo", "paper_title": "Tokenlearner: Adaptive space-time tokenization for videos", "reason": "TokenLearner, the spatio-temporal attentional pooling method used in this paper, is introduced in this highly relevant reference.  The approach significantly impacts the efficiency and accuracy of the proposed model, justifying its importance as a key citation.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Michael Ryoo", "paper_title": "Token turing machines", "reason": "This paper introduces Token Turing Machines (TTM), a crucial component used as one of the temporal encoders in BLIP-3-Video. Its unique sequential modeling capabilities are directly used and discussed in the current paper, making it a fundamental contribution to the proposed method's architecture and design.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Marah Abdin", "paper_title": "Phi-3 technical report: A highly capable language model locally on your phone", "reason": "Phi-3, a large language model (LLM), serves as the backbone of the BLIP-3-Video model. The capabilities and architecture of this LLM are integral to the proposed model's ability to perform video question-answering and captioning tasks, making it a highly relevant and important reference.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Muhammad Maaz", "paper_title": "Video-chatgpt: Chat-centric video understanding", "reason": "This paper is cited as one of the datasets used for training BLIP-3-Video, specifically mentioning the VideoChatGPT instruction tuning data. The dataset's role in fine-tuning the model for video question-answering is substantial, highlighting its importance in the experimental results.", "section_number": 2}, {" publication_date": "2017", "fullname_first_author": "Dejing Xu", "paper_title": "Video question answering via gradually refined attention over appearance and motion", "reason": "This paper introduces MSVD-QA, a key dataset used for evaluating and training BLIP-3-Video. MSVD-QA's role in establishing the model's performance and generalizability on open-ended question-answering benchmarks is significant, highlighting its importance in the current work's evaluation.", "section_number": 3}, {" publication_date": "2017", "fullname_first_author": "Dejing Xu", "paper_title": "Video question answering via gradually refined attention over appearance and motion", "reason": "This paper introduces MSRVTT-QA, another key dataset used in the evaluation and training of BLIP-3-Video. This dataset's inclusion in the experiments is critical for evaluating the model's performance on open-ended question-answering tasks and comparing it to other state-of-the-art models.", "section_number": 3}, {" publication_date": "2019", "fullname_first_author": "Zhou Yu", "paper_title": "Activitynet-qa: A dataset for understanding complex web videos via question answering", "reason": "ActivityNet-QA, introduced in this paper, serves as a significant dataset utilized in the evaluation and training of BLIP-3-Video. The dataset's inclusion and the results obtained from it contribute significantly to the validation of the proposed model's performance.", "section_number": 3}, {" publication_date": "2017", "fullname_first_author": "Yunseok Jang", "paper_title": "Tgif-qa: Toward spatio-temporal reasoning in visual question answering", "reason": "TGIF-QA is specifically mentioned as a key dataset in evaluating and training the BLIP-3-Video model.  The performance on this dataset provides crucial validation of the model's ability to handle temporal reasoning in video question-answering.", "section_number": 3}, {" publication_date": "2021", "fullname_first_author": "Junbin Xiao", "paper_title": "Next-qa: Next phase of question-answering to explaining temporal actions", "reason": "This paper introduces NExT-QA, a key dataset used for evaluating BLIP-3-Video on multiple-choice question answering tasks. The model's performance on this dataset demonstrates its ability to handle a different type of question-answering task, broadening the scope of the evaluation.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Lin Xu", "paper_title": "Pllava: Parameter-free llava extension from images to videos for video dense captioning", "reason": "This paper introduces PLLaVA, a key model compared against BLIP-3-Video's results in the experimental section.  The comparison allows for a detailed evaluation of the proposed model's efficiency and performance against a state-of-the-art model for video understanding.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Hang Zhang", "paper_title": "Video-llama: An instruction-tuned audio-visual language model for video understanding", "reason": "This paper introduces Video-LLaMA, another key model for comparison in the experimental section of the current work. The comparison enables a detailed evaluation of BLIP-3-Video's performance and efficiency against a state-of-the-art video understanding model.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Anas Awadalla", "paper_title": "Mint-1t: Scaling open-source multimodal data by 10x: A multimodal dataset with one trillion tokens", "reason": "This paper introduces the Mira dataset, a key dataset used in the evaluation and training of BLIP-3-Video.  Its comprehensive nature and scale are highlighted as reasons for its inclusion in the evaluation process.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Le Xue", "paper_title": "xgen-mm (blip-3): A family of open large multimodal models", "reason": "BLIP-3, the foundation model for BLIP-3-Video, is introduced in this paper. This citation is crucial for understanding the baseline model and the modifications applied to create BLIP-3-Video. Its architecture and pre-training are directly relevant to the understanding of the current model.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Hang Zhang", "paper_title": "Video-llama: An instruction-tuned audio-visual language model for video understanding", "reason": "This work is directly compared against BLIP-3-Video in the experimental section, providing a direct benchmark for evaluating the efficiency and performance of the proposed method.  It highlights the trade-offs between model size, token count, and accuracy.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Peng Jin", "paper_title": "Chat-univi: Unified visual representation empowers large language models with image and video understanding", "reason": "This paper introduces Chat-UniVi, a model compared against BLIP-3-Video. This comparison helps to highlight the unique aspects of the temporal encoder in BLIP-3-Video and its impact on model size and performance compared to other state-of-the-art models.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Yanwei Li", "paper_title": "Llama-vid: An image is worth 2 tokens in large language models", "reason": "This paper introduces LLaMA-VID, which is compared against BLIP-3-Video. The comparison highlights the trade-off between model size, the number of visual tokens used, and achieved accuracy. The findings reinforce the efficiency of BLIP-3-Video.", "section_number": 3}]}