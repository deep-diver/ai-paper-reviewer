{"references": [{" publication_date": "2021", "fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "reason": "This paper introduces a foundational model, ViT, that is widely used as a pre-trained image encoder in many vision-language models, including the one presented in this paper. Its impact on the field of computer vision and the development of subsequent models is undeniable.", "section_number": 4}, {" publication_date": "2022", "fullname_first_author": "Jean-Baptiste Alayrac", "paper_title": "Flamingo: a visual language model for few-shot learning", "reason": "Flamingo is a significant advancement in visual language models, demonstrating the effectiveness of the 'perceiver resampler' for creating vision tokens, a technique also explored in this work, thus influencing the design choices of the present model.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Junnan Li", "paper_title": "BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models", "reason": "BLIP-2 is a crucial predecessor to the current work.  It introduces the concept of vision tokens from a pre-trained vision encoder, which are processed by an LLM. This design is directly adapted and further enhanced in the BLIP-3-Video model presented in this paper.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Hang Zhang", "paper_title": "Video-LLaMA", "reason": "This paper proposes a video language model that explicitly addresses the temporal aspect of video understanding.  It is directly compared and contrasted with the proposed model in the experiment results, thereby establishing it as a significant benchmark.", "section_number": 4}, {" publication_date": "2022", "fullname_first_author": "Andrew Jaegle", "paper_title": "Perceiver IO: A general architecture for structured inputs & outputs", "reason": "The Perceiver IO architecture, with its flexible attention mechanism, heavily influenced the design of the spatio-temporal attentional pooling used in the proposed BLIP-3-Video model, specifically the TokenLearner module, thus making it directly relevant to the core methodology.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Michael Ryoo", "paper_title": "TokenLearner: Adaptive space-time tokenization for videos", "reason": "The TokenLearner model presented in this paper is a direct component of the BLIP-3-Video architecture.  The authors' previous work on efficient spatio-temporal attentional pooling directly informs the core design of the proposed model.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Michael Ryoo", "paper_title": "Token Turing Machines", "reason": "The Token Turing Machines (TTM) model, introduced in this previous work by the authors, is directly used as one of the key components in the BLIP-3-Video architecture, for efficient sequential modeling of video tokens, hence is directly relevant to the core functionality.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Muhammad Maaz", "paper_title": "Video-ChatGPT: Towards detailed video understanding via large vision and language models", "reason": "Video-ChatGPT is a very strong benchmark model in the field of video LLMs and is closely related to the proposed model's approach. It also provides a direct comparison point for results, and shows similar strengths and weaknesses with the new model.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Lin Xu", "paper_title": "PLLaVA: Parameter-free LLaVA extension from images to videos for video dense captioning", "reason": "PLLaVA is directly compared with the proposed model. The presented work improves upon PLLaVA's methodology by using fewer tokens with a temporal encoder, and hence represents a strong comparison model.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Le Xue", "paper_title": "xGen-MM (BLIP-3): A family of open large multimodal models", "reason": "BLIP-3 forms the foundation of the BLIP-3-Video model, providing the pre-trained LLM backbone.  Understanding BLIP-3 is essential for appreciating the novel contributions of the temporal encoder in the proposed architecture.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Marah Abdin", "paper_title": "Phi-3 technical report: A highly capable language model locally on your phone", "reason": "Phi-3 serves as the foundational Large Language Model (LLM) in the BLIP-3-Video architecture.  Its capabilities and characteristics significantly influence the overall performance and capabilities of the proposed model.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Peng Jin", "paper_title": "Chat-UniVi: Unified visual representation empowers large language models with image and video understanding", "reason": "Chat-UniVi is a strong benchmark model that is used for comparison. Its results in the experiments highlight the improved efficiency of BLIP-3-Video in comparison, thus showing its importance in establishing context.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Yanwei Li", "paper_title": "LLaMA-VID: An image is worth 2 tokens in large language models", "reason": "LLaMA-VID is a notable prior work that also explores reducing the number of tokens to represent video data. Its approach is compared and contrasted in the ablation studies, highlighting the superiority of the temporal encoder in BLIP-3-Video.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Bin Lin", "paper_title": "Video-LLaVA: Learning united visual representation by alignment before projection", "reason": "Video-LLaVA is used as a benchmark and a comparison point for the BLIP-3-Video model.  Its performance is directly compared and contrasted to highlight the computational efficiency and accuracy gains made by BLIP-3-Video.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Jiawei Wang", "paper_title": "Tarsier: Recipes for training and evaluating large video description models", "reason": "Tarsier is a strong benchmark model that is used for performance comparison on video question answering and captioning tasks. It's particularly relevant because it also utilizes a large number of tokens for video representation, thereby contrasting it with BLIP-3-Video's efficient approach.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Xiaohan Wang", "paper_title": "VideoAgent: Long-form video understanding with large language model as agent", "reason": "VideoAgent is a relevant model for comparison, as it addresses the challenge of handling long videos and is used in experiments to show the accuracy and efficiency of BLIP-3-Video against other models with comparable scale.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Daniel Bolya", "paper_title": "Token merging: Your ViT but faster", "reason": "This paper presents a token merging technique for ViTs that's directly relevant to the concept of efficient token representation, making it a significant contribution in the related field.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Shuhuai Ren", "paper_title": "Testa: Temporal-spatial token aggregation for long-form video-language understanding", "reason": "This paper addresses a similar challenge: efficient video representation, providing a different approach to reducing the number of tokens in video processing.  It serves as a relevant related work and comparison point to the new model.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Rafael Rafailov", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "reason": "Direct Preference Optimization (DPO) is used in the training of BLIP-3-Video, which is a significant training methodology.  This makes understanding DPO and its application crucial to understanding the model's training process.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Junbin Xiao", "paper_title": "Next-QA: Next phase of question-answering to explaining temporal actions", "reason": "The NExT-QA dataset used in the experiments is introduced in this paper.  The results on this dataset showcase the model's capacity to address a more complex question-answering type and provide a benchmark against other models, making this paper directly relevant to the experimental section.", "section_number": 3}]}