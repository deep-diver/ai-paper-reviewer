[{"figure_path": "2410.16267/figures/figures_2_0.png", "caption": "Figure 2: An illustration of the BLIP-3-Video model architecture. It has the explicit temporal encoder inserted to BLIP-3.", "description": "The figure illustrates the architecture of the BLIP-3-Video model, showing its components including a temporal encoder inserted into BLIP-3.", "section": "2 BLIP-3-VIDEO"}, {"figure_path": "2410.16267/figures/figures_3_0.png", "caption": "Figure 3: Visually comparing different types of temporal encoders we explored in our model architecture. (c) and (d) are particularly effective, as we discuss further in the experiments.", "description": "Figure 3 visually compares four different types of temporal encoders used in the BLIP-3-Video model architecture, highlighting the attentional pooling and sequential model as particularly effective.", "section": "2.2 TEMPORAL ENCODERS"}, {"figure_path": "2410.16267/figures/figures_7_0.png", "caption": "Figure 1: SOTA video VLM model comparison: (Left) Number of visual tokens vs. video-QA accuracy. (Right) Model size vs. video-QA accuracy.", "description": "The figure compares state-of-the-art video VLMs in terms of their video question answering accuracy against the number of visual tokens used and model size.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.16267/figures/figures_7_1.png", "caption": "Figure 1: SOTA video VLM model comparison: (Left) Number of visual tokens vs. video-QA accuracy. (Right) Model size vs. video-QA accuracy.", "description": "The figure compares state-of-the-art video Vision-Language Models in terms of their size, number of visual tokens, and video question answering accuracy.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.16267/figures/figures_7_2.png", "caption": "Figure 1: SOTA video VLM model comparison: (Left) Number of visual tokens vs. video-QA accuracy. (Right) Model size vs. video-QA accuracy.", "description": "The figure compares state-of-the-art video VLMs in terms of model size, number of visual tokens, and video question answering accuracy.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.16267/figures/figures_7_3.png", "caption": "Figure 1: SOTA video VLM model comparison: (Left) Number of visual tokens vs. video-QA accuracy. (Right) Model size vs. video-QA accuracy.", "description": "The figure compares state-of-the-art video VLMs in terms of video question answering accuracy against the number of visual tokens and model size.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.16267/figures/figures_8_0.png", "caption": "Figure 4: Example video captioning results on Mira dataset, formed in question-answering style.", "description": "The figure shows example video captioning results from the BLIP-3-Video model and compares its performance against Tarsier and LLaVA-OneVision on the Mira dataset, highlighting differences in caption quality and hallucination rates.", "section": "3 EXPERIMENTS AND RESULTS"}, {"figure_path": "2410.16267/figures/figures_8_1.png", "caption": "Figure 4: Example video captioning results on Mira dataset, formed in question-answering style.", "description": "The figure shows example video captioning results of three different models on the Mira dataset, presented in a question-answering format, comparing the models' outputs with the ground truth captions.", "section": "3.4 VIDEO CAPTIONING EVALUATION"}, {"figure_path": "2410.16267/figures/figures_8_2.png", "caption": "Figure 1: SOTA video VLM model comparison: (Left) Number of visual tokens vs. video-QA accuracy. (Right) Model size vs. video-QA accuracy.", "description": "The figure compares state-of-the-art video VLMs in terms of their video question answering accuracy, number of visual tokens, and model size.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.16267/figures/figures_8_3.png", "caption": "Figure 1: SOTA video VLM model comparison: (Left) Number of visual tokens vs. video-QA accuracy. (Right) Model size vs. video-QA accuracy.", "description": "The figure compares state-of-the-art video VLMs in terms of the number of visual tokens used versus video question answering accuracy and model size.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.16267/figures/figures_14_0.png", "caption": "Figure 1: SOTA video VLM model comparison: (Left) Number of visual tokens vs. video-QA accuracy. (Right) Model size vs. video-QA accuracy.", "description": "The figure compares state-of-the-art video Vision-Language Models in terms of their video question answering accuracy against the number of visual tokens used and model size.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.16267/figures/figures_14_1.png", "caption": "Figure 1: SOTA video VLM model comparison: (Left) Number of visual tokens vs. video-QA accuracy. (Right) Model size vs. video-QA accuracy.", "description": "The figure shows a comparison of state-of-the-art video Vision-Language Models in terms of the number of visual tokens used and model size against video question answering accuracy.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.16267/figures/figures_15_0.png", "caption": "Figure 1: SOTA video VLM model comparison: (Left) Number of visual tokens vs. video-QA accuracy. (Right) Model size vs. video-QA accuracy.", "description": "The figure compares state-of-the-art video Vision-Language Models in terms of their video question answering accuracy against the number of visual tokens used and their model sizes.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.16267/figures/figures_15_1.png", "caption": "Figure 1: SOTA video VLM model comparison: (Left) Number of visual tokens vs. video-QA accuracy. (Right) Model size vs. video-QA accuracy.", "description": "The figure shows a comparison of state-of-the-art video vision-language models in terms of their size, number of visual tokens, and video question answering accuracy.", "section": "1 INTRODUCTION"}]