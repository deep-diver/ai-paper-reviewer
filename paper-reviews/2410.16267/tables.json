[{"figure_path": "2410.16267/tables/table_6_0.html", "caption": "Table 1: Comparison against reported numbers of other models on open-ended question answering evaluation. The number of visual tokens are also reported. The numbers after '/' are answer quality scores. * indicates our evaluation using the checkpoint and inference code provided by the author, with the identical videos used in our model (8 frames of 384\u00d7384 resolution).", "description": "Table 1 compares the performance of BLIP-3-Video against other state-of-the-art models on open-ended video question answering tasks, considering model size and the number of visual tokens used.", "section": "3.2 PUBLIC BENCHMARKS"}, {"figure_path": "2410.16267/tables/table_6_1.html", "caption": "Table 2: Comparison against reported numbers of other models on multiple choice question-answering (MCQ) benchmark.", "description": "Table 2 compares the performance of BLIP-3-Video against other models on multiple-choice video question answering tasks, showing its accuracy using different numbers of tokens.", "section": "3.2 Public Benchmarks"}, {"figure_path": "2410.16267/tables/table_6_2.html", "caption": "Table 3: Ablations comparing different temporal encoders: 128 tokens. *A slightly different training recipe using a subset of the entire dataset (without Mira data) was used for the ablations.", "description": "Table 3 shows the ablation study comparing different temporal encoders in terms of question-answering accuracy on four datasets, using 128 tokens for each video.", "section": "3.3 ABLATIONS"}, {"figure_path": "2410.16267/tables/table_7_0.html", "caption": "Table 4: Ablations comparing different pooling strategies for 32 tokens.", "description": "The table compares different pooling strategies for 32 tokens, showing their effects on MSVD-QA, TGIF-QA, and NEXT-QA.", "section": "3.3 ABLATIONS"}, {"figure_path": "2410.16267/tables/table_8_0.html", "caption": "Table 6: Video caption evaluation results using 8 frames. We employ VideoChatGPT's LLM evaluation and report Average Accuracy / Average Score in this table. The \u2018captioning-only model\u2019 was trained only using Mira video caption data (without QA data), making it specialized for the captioning.", "description": "Table 6 compares the video captioning performance of BLIP-3-Video with other state-of-the-art models on MSVD-Caption, MSRVTT-Caption, and Mira-Cap datasets, showing BLIP-3-Video's superior performance despite its smaller size and fewer visual tokens.", "section": "3.4 VIDEO CAPTIONING EVALUATION"}]