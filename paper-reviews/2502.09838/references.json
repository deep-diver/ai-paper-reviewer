{"references": [{"fullname_first_author": "Abdin, M.", "paper_title": "Phi-4 technical report", "publication_date": "2024-12-08", "reason": "This paper introduces the Phi-4 model, a large language model that serves as the foundation for one of HealthGPT's versions, HealthGPT-L14, making it crucial to the model's architecture and performance."}, {"fullname_first_author": "Chen, J.", "paper_title": "Huatuogpt-vision, towards injecting medical visual knowledge into multimodal llms at scale", "publication_date": "2024-06-19", "reason": "This paper introduces HuatuoGPT-Vision, a key comparative model in the HealthGPT evaluation, highlighting the state-of-the-art in medical visual language models and providing a benchmark for HealthGPT's performance."}, {"fullname_first_author": "Li, C.", "paper_title": "LLaVA-Med: Training a large language-and-vision assistant for biomedicine in one day", "publication_date": "2024-00-00", "reason": "LLaVA-Med is a significant comparative model, frequently referenced in the paper's evaluation, demonstrating the state-of-the-art in medical visual language models and providing a benchmark for HealthGPT's capabilities."}, {"fullname_first_author": "Liu, B.", "paper_title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models", "publication_date": "2023-00-00", "reason": "BLIP-2 is a major comparative model in the HealthGPT evaluation, illustrating the leading techniques in visual language models and serving as a benchmark against which HealthGPT's performance is measured."}, {"fullname_first_author": "Lu, J.", "paper_title": "Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision Language Audio and Action", "publication_date": "2024-00-00", "reason": "Unified-IO 2 is a key comparative model, frequently mentioned in the evaluation sections, illustrating current leading methods in unified multimodal models and providing a crucial benchmark for HealthGPT's performance in handling diverse multimodal tasks."}]}