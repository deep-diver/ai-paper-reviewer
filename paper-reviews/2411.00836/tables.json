[{"content": "| Statistic | Number |\n|---|---| \n| Total *seed* questions (programs) | 501 |\n| - Created from existing dataset | 227 (45.3%) |\n| - Newly designed questions | 274 (54.7%) |\n| Topics |  |\n| - Solid geometry (SG) | 15 (3.0%) |\n| - Puzzle test (PT) | 17 (3.4%) |\n| - Arithmetic (AR) | 26 (5.2%) |\n| - Scientific figure (SF) | 45 (9.0%) |\n| - Graph theory (GT) | 48 (9.6%) |\n| - Algebra (AL) | 51 (10.2%) |\n| - Plane geometry (PG) | 77 (15.4%) |\n| - Analytic geometry (AG) | 97 (19.4%) |\n| - Statistics (ST) | 125 (25.0%) |\n| Levels |  |\n| - Elementary school (EL) | 63 (12.6%) |\n| - High school (HI) | 277 (55.3%) |\n| - Undergraduate (UN) | 161 (32.1%) |\n| Question Types |  |\n| - Multiple-choice questions | 177 (35.3%) |\n| - Free-form questions | 324 (64.7%) |", "caption": "Table 1: Statistics of DynaMath.", "description": "Table 1 provides a comprehensive overview of the DYNAMATH dataset, detailing its composition and characteristics.  It shows the total number of seed questions (programs), categorized by whether they originate from existing datasets or are newly designed.  Furthermore, it breaks down the seed questions by topic (covering areas like geometry, algebra, statistics, etc.), difficulty level (elementary, high school, undergraduate), and question type (multiple-choice versus free-form). This granular breakdown helps to understand the scope and diversity of the dataset, highlighting the balance between established problems and novel questions across different mathematical domains and difficulty levels.", "section": "3. Benchmark Design"}, {"content": "| Model | ALL | PG | SG | AG | AL | PT | GT | ST | SF | AR | EL | HI | UN |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---| \n| *Closed-sourced Large Multimodal Models (LMMs)* |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| Zero-shot GPT-4o | 63.7 | 56.8 | 52.0 | 61.0 | 76.9 | 51.8 | 58.1 | 69.3 | 62.4 | 61.5 | 68.6 | 61.8 | 36.8 |\n| Zero-shot Claude-3.5 | 64.8 | 49.9 | 49.3 | 55.3 | 81.0 | 44.1 | 69.4 | 78.2 | 62.2 | 61.2 | 66.7 | 62.6 | 33.3 |\n| Zero-shot Gemini Pro 1.5 | 60.5 | 52.7 | 42.7 | 61.6 | 70.8 | 20.6 | 65.2 | 69.8 | 50.2 | 54.2 | 62.9 | 59.2 | 37.1 |\n| 3-shot CoT GPT-4o | 64.9 | 58.1 | 59.3 | 57.7 | 84.1 | 51.2 | 61.9 | 71.0 | 60.9 | 57.7 | 66.2 | 62.5 | 34.8 |\n| 3-shot CoT Claude-3.5 | 62.5 | 49.1 | 48.0 | 50.6 | 80.2 | 37.1 | 58.1 | 78.2 | 64.9 | 55.0 | 63.0 | 61.5 | 30.5 |\n| 3-shot CoT Gemini Pro 1.5 | 58.7 | 52.6 | 45.3 | 56.7 | 72.9 | 21.8 | 57.9 | 66.0 | 54.9 | 48.1 | 59.0 | 58.3 | 34.2 |\n| *Open-sourced Vision Language Models (VLMs)* |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| Qwen2-VL-72B | 55.1 | 48.1 | 48.7 | 50.9 | 57.6 | 28.2 | 45.0 | 68.9 | 56.4 | 54.2 | 61.3 | 57.4 | 30.7 |\n| Qwen2-VL-7B | 42.1 | 40.3 | 38.7 | 39.9 | 37.1 | 8.2 | 44.8 | 52.1 | 41.1 | 39.2 | 47.6 | 42.2 | 24.4 |\n| InternVL2-76B | 54.0 | 44.5 | 34.7 | 43.8 | 67.6 | 35.3 | 51.0 | 66.7 | 55.1 | 51.5 | 60.3 | 52.9 | 26.4 |\n| InternVL2-40B | 41.8 | 31.3 | 21.3 | 38.8 | 42.9 | 15.3 | 38.3 | 58.1 | 43.1 | 38.1 | 51.0 | 41.5 | 23.4 |\n| InternVL2-26B | 41.0 | 35.8 | 26.0 | 37.3 | 38.8 | 13.5 | 46.9 | 51.9 | 39.6 | 40.4 | 52.1 | 38.5 | 22.5 |\n| InternVL2-8B | 39.7 | 33.9 | 37.3 | 32.5 | 46.9 | 15.9 | 42.1 | 47.8 | 39.1 | 37.3 | 51.1 | 37.4 | 19.6 |\n| Llama-3.2-90B | 44.0 | 47.5 | 37.3 | 36.8 | 46.5 | 12.4 | 44.8 | 56.8 | 39.8 | 30.0 | 45.4 | 43.8 | 22.2 |\n| Deepseek-VL-7B-chat | 21.5 | 16.0 | 13.3 | 26.5 | 12.9 | 4.7 | 32.7 | 24.3 | 24.2 | 15.0 | 28.3 | 19.0 | 16.0 |\n| Llava-v1.6-34B | 27.1 | 21.4 | 25.3 | 27.6 | 14.9 | 7.6 | 32.7 | 36.8 | 27.8 | 23.1 | 35.9 | 23.8 | 16.6 |\n| Llava-v1.6-vicuna-13B | 19.8 | 14.7 | 10.0 | 23.4 | 8.2 | 10.0 | 21.5 | 28.2 | 19.6 | 10.0 | 27.1 | 16.5 | 14.1 |\n| Llava-v1.5-7B | 16.6 | 10.5 | 7.3 | 19.5 | 6.5 | 8.2 | 32.3 | 17.5 | 20.2 | 10.8 | 18.9 | 13.3 | 11.7 |\n| *Human* |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| Human performance | 75.8 | 80.5 | 60.0 | 83.5 | 78.4 | 76.5 | 64.6 | 74.4 | 77.8 | 61.5 | 74.6 | 78.3 | 72.0 |", "caption": "Table 2: Average-case accuracy \ud835\udc9ca\u2062v\u2062gsubscript\ud835\udc9c\ud835\udc4e\ud835\udc63\ud835\udc54\\mathcal{A}_{avg}caligraphic_A start_POSTSUBSCRIPT italic_a italic_v italic_g end_POSTSUBSCRIPT on DynaMath with 5,010 generated questions. \u201cALL\u201d represents overall accuracy. Question topics and difficulty levels (PG, EL, etc) are defined in Table\u00a01.", "description": "Table 2 presents the average-case accuracy of various vision-language models (VLMs) on the DynaMath benchmark.  DynaMath consists of 5,010 dynamically generated visual math questions, derived from 501 seed questions. The table shows the performance of each model across different question topics (Plane Geometry (PG), Solid Geometry (SG), Analytic Geometry (AG), Algebra (AL), Puzzle Tests (PT), Graph Theory (GT), Statistics (ST), Scientific Figures (SF), Arithmetic (AR)), and difficulty levels (Elementary school (EL), High school (HI), Undergraduate (UN)).  The \"ALL\" column shows the overall average accuracy across all questions.  The results are useful for comparing the performance of different models on various types of visual mathematical reasoning tasks and assessing their strengths and weaknesses.", "section": "4.2 EXPERIMENTAL RESULTS"}, {"content": "| Model | ALL | PG | SG | AG | AL | PT | GT | ST | SF | AR | EL | HI | UN |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---| \n| *Closed-sourced Large Multimodal Models (LMMs)* |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| Zero-shot GPT-4o | 34.7 | 37.7 | 33.3 | 25.8 | 54.9 | 11.8 | 18.8 | 38.4 | 35.6 | 46.2 | 46.0 | 34.3 | 31.1 |\n| Zero-shot Claude-3.5 | 35.3 | 22.1 | 26.7 | 18.6 | 62.7 | 23.5 | 27.1 | 53.6 | 24.4 | 42.3 | 49.2 | 33.2 | 33.5 |\n| Zero-shot Gemini Pro 1.5 | 26.9 | 28.6 | 20.0 | 19.6 | 39.2 | 5.9 | 22.9 | 35.2 | 15.6 | 30.8 | 41.3 | 26.7 | 21.7 |\n| 3-shot CoT GPT-4o | 32.3 | 31.2 | 40.0 | 21.6 | 54.9 | 17.6 | 20.8 | 36.8 | 26.7 | 46.2 | 47.6 | 30.7 | 29.2 |\n| 3-shot CoT Claude-3.5 | 32.1 | 27.3 | 26.7 | 11.3 | 54.9 | 0.0 | 10.4 | 56.0 | 31.1 | 30.8 | 39.7 | 32.9 | 28.0 |\n| 3-shot CoT Gemini Pro 1.5 | 23.6 | 27.3 | 26.7 | 14.4 | 39.2 | 5.9 | 18.8 | 27.2 | 17.8 | 26.9 | 33.3 | 23.1 | 20.5 |\n| *Open-sourced Vision Language Models (VLMs)* |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| Qwen2-VL-72B | 28.3 | 27.3 | 33.3 | 15.5 | 31.4 | 0.0 | 16.7 | 43.2 | 26.7 | 42.3 | 41.3 | 30.3 | 19.9 |\n| Qwen2-VL-7B | 13.8 | 22.1 | 6.7 | 7.2 | 13.7 | 0.0 | 12.5 | 16.8 | 11.1 | 19.2 | 25.4 | 12.3 | 11.8 |\n| InternVL2-76B | 24.6 | 24.7 | 20.0 | 15.5 | 37.3 | 5.9 | 12.5 | 32.8 | 20.0 | 38.5 | 39.7 | 23.1 | 21.1 |\n| InternVL2-40B | 14.2 | 14.3 | 6.7 | 9.3 | 13.7 | 0.0 | 10.4 | 21.6 | 13.3 | 19.2 | 28.6 | 14.1 | 8.7 |\n| InternVL2-26B | 14.4 | 19.5 | 0.0 | 6.2 | 9.8 | 0.0 | 18.8 | 20.0 | 11.1 | 26.9 | 34.9 | 12.3 | 9.9 |\n| InternVL2-8B | 10.4 | 13.0 | 20.0 | 5.2 | 15.7 | 0.0 | 10.4 | 9.6 | 11.1 | 15.4 | 23.8 | 9.4 | 6.8 |\n| Llama-3.2-90B | 13.0 | 22.1 | 20.0 | 7.2 | 7.8 | 0.0 | 12.5 | 16.8 | 13.3 | 3.8 | 15.9 | 14.1 | 9.9 |\n| Deepseek-VL-7B-chat | 4.2 | 7.8 | 0.0 | 3.1 | 0.0 | 0.0 | 10.4 | 4.0 | 2.2 | 3.8 | 7.9 | 2.9 | 5.0 |\n| Llava-v1.6-34B | 6.0 | 10.4 | 13.3 | 4.1 | 2.0 | 0.0 | 4.2 | 6.4 | 6.7 | 7.7 | 15.9 | 5.1 | 3.7 |\n| Llava-v1.6-vicuna-13B | 2.8 | 7.8 | 0.0 | 4.1 | 0.0 | 0.0 | 2.1 | 2.4 | 0.0 | 0.0 | 6.3 | 2.9 | 1.2 |\n| Llava-v1.5-7B | 1.8 | 3.9 | 0.0 | 2.1 | 0.0 | 0.0 | 4.2 | 0.8 | 0.0 | 3.8 | 3.2 | 1.8 | 1.2 |", "caption": "Table 3: Worst-case accuracy \ud835\udc9cw\u2062s\u2062tsubscript\ud835\udc9c\ud835\udc64\ud835\udc60\ud835\udc61\\mathcal{A}_{wst}caligraphic_A start_POSTSUBSCRIPT italic_w italic_s italic_t end_POSTSUBSCRIPT on DynaMath with 5,010 generated questions. \u201cALL\u201d represents overall accuracy. Question topics and difficulty levels (PG, EL, etc) are defined in Table\u00a01.", "description": "Table 3 presents the worst-case accuracy (the lowest accuracy across 10 variations of each question) of various vision-language models (VLMs) on the DynaMath benchmark.  It shows the performance of each model on different mathematical question types and difficulty levels (Elementary, High School, Undergraduate)  as well as an overall worst-case accuracy. The table helps assess how robust each model is to variations in question presentation, emphasizing its ability to generalize. The question types and difficulty levels are defined in Table 1 of the paper.", "section": "4.2 EXPERIMENTAL RESULTS"}, {"content": "| Model name | GPT-4o | Gemini | Qwen2-72B | InternVL2-76B |\n|---|---|---|---|---|\n| Repetition Consistency (%) | 94.1 | 92.5 | 98.9 | 99.0 |", "caption": "Table 4: The Repetition Consistency (R\u2062C\ud835\udc45\ud835\udc36RCitalic_R italic_C) for different models over 5 repetitions.", "description": "This table presents the repetition consistency (RC) scores for various vision-language models.  Repetition consistency measures the consistency of a model's responses to the same question across multiple repetitions.  A higher RC indicates greater confidence and less inherent randomness in the model's answers. The results are calculated from 5 repetitions for each question in the dataset.  The table helps assess the reliability of each model, identifying those that provide consistent answers even when facing the same prompt multiple times.", "section": "4.3 QUALITY STUDY"}, {"content": "| Answer type | prompt |\n|---|---| \n| multiple choice | If the problem is a multiple choice problem, just provide the corresponing choice option, such as \u2019A\u2019, \u2019B\u2019, \u2019C\u2019, or \u2019D\u2019. |\n| float | If the answer is a numerical value, format it as a three-digit floating-point number. |\n| text | Please answer the question in the following form: (specific requirement in question). |", "caption": "Table 5: The prompt for different questions and answer types in answer generation.", "description": "This table presents the different prompts used for generating answers based on the question type.  The prompt engineering approach is tailored to guide the model to produce responses in specific formats, depending on whether the question is multiple-choice, requires a numerical (floating-point) answer, or needs a text-based response. This ensures consistency and facilitates accurate evaluation of the model's performance.", "section": "3.3 EVALUATION PROTOCOLS"}, {"content": "| Model | Hyperparameters |\n|---|---| \n| GPT-4o | model = `gpt-4o-0806`, temperature = 0.0, max_tokens = 4096 |\n| Claude-3.5 | model = `claude-3-5-sonnet-20240620`, temperature = 0.0, max_tokens = 8192 |\n| Gemini Pro 1.5 | model = `gemini-1.5-pro`, temperature = 0.0, max_tokens = 8192 |\n| Qwen2-VL-72B | model = `Qwen/Qwen2-VL-72B-Instruct`, temperature = 0.0, max_tokens = 2048 |\n| QWen2-VL-7B | model = `Qwen/Qwen2-VL-7B-Instruct`, temperature = 0.0, max_tokens = 2048 |\n| InternVL2-76B | model = `OpenGVLab/InternVL2-Llama3-76B`, temperature = 0.0, max_tokens = 2048 |\n| InternVL2-40B | model = `OpenGVLab/InternVL2-40B`, temperature = 0.0, max_tokens = 2048 |\n| InternVL2-26B | model = `OpenGVLab/InternVL2-26B`, temperature = 0.0, max_tokens = 2048 |\n| InternVL2-8B | model = `OpenGVLab/InternVL2-8B`, temperature = 0.0, max_tokens = 2048 |\n| Deepseek-VL-7B-chat | model = `deepseek-ai/deepseek-vl-7b-chat`, temperature = 0.0, max_tokens = 2048 |\n| Llama-3.2-90B | model = `meta-llama/Llama-3.2-90B-Vision-Instruct`, temperature = 0.0, max_tokens = 2048 |\n| Llava-v1.6-34B | model = `liuhaotian/llava-v1.6-34b`, temperature = 0.0, max_tokens = 2048 |\n| Llava-v1.6-vicuna-13B | model = `liuhaotian/llava-v1.6-vicuna-13b`, temperature = 0.0, max_tokens = 2048 |\n| Llava-v1.5-7B | model = `liuhaotian/llava-v1.5-7b`, temperature = 0.0, max_tokens = 2048 |", "caption": "Table 6: Hyperparameters for various VLMs.", "description": "This table lists the hyperparameters used for different Vision-Language Models (VLMs) during the experiments in the paper.  For each model, it specifies the model name, the specific model version used (e.g., model size), the temperature setting, which controls the randomness of the model's outputs, and the maximum number of tokens allowed in the model's response.", "section": "3.3 EVALUATION PROTOCOLS"}]