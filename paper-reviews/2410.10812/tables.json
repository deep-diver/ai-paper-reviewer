[{"figure_path": "2410.10812/tables/table_1_0.html", "caption": "Table 2: The performance of HART on MJHQ-30K, GenEval and DPG-Bench benchmarks. We compare HART with open-source diffusion models and autoregressive models. Results demonstrate that HART can achieve comparable performance to state-of-the-art diffusion models with <1B parameters, surpassing prior autoregressive models by a large margin.", "description": "Table 2 compares the performance of HART against other state-of-the-art diffusion and autoregressive models on three benchmark datasets, showing that HART achieves comparable performance to state-of-the-art diffusion models with fewer parameters.", "section": "4.2 Main Results"}, {"figure_path": "2410.10812/tables/table_8_0.html", "caption": "Table 2: The performance of HART on MJHQ-30K, GenEval and DPG-Bench benchmarks. We compare HART with open-source diffusion models and autoregressive models. Results demonstrate that HART can achieve comparable performance to state-of-the-art diffusion models with <1B parameters, surpassing prior autoregressive models by a large margin.", "description": "Table 2 compares the performance of HART with other state-of-the-art diffusion and autoregressive models on various image generation benchmarks, showing HART achieves comparable performance to top models with fewer parameters.", "section": "4.2 Main Results"}, {"figure_path": "2410.10812/tables/table_8_1.html", "caption": "Table 3: Compared to state-of-the-art diffusion models, HART achieves 5.0\u20139.6\u00d7 higher throughput and 4.0\u20134.7\u00d7 lower latency at 512\u00d7512 resolution. At 1024\u00d71024 resolution, it demonstrates 4.5\u20137.7\u00d7 higher throughput and 3.1\u20135.9\u00d7 lower latency.", "description": "This table compares the efficiency of HART against state-of-the-art diffusion models in terms of latency, throughput, and MACs at 512x512 and 1024x1024 resolutions.", "section": "3.3 EFFICIENCY ENHANCEMENTS"}, {"figure_path": "2410.10812/tables/table_8_2.html", "caption": "Table 1: HART significantly outperforms VAR and matches SDXL tokenizer performance on MJHQ-30K and ImageNet datasets.", "description": "Table 1 shows a comparison of the reconstruction fidelity (rFID) achieved by different tokenizers on the MJHQ-30K and ImageNet datasets, demonstrating that HART's hybrid tokenizer significantly outperforms the discrete VAR tokenizer and matches the performance of the continuous SDXL tokenizer.", "section": "4.2 Main Results"}, {"figure_path": "2410.10812/tables/table_9_0.html", "caption": "Table 4: HART achieves better class-conditioned image generation results compared to MAR (Li et al., 2024b) with 10.7\u00d7 lower MACs and 12.9\u00d7 faster runtime. It also offers 7.8% FID reduction with 4% runtime overhead compared with VAR (Tian et al., 2024). Time: bs=64 on A100.", "description": "Table 4 presents a comparison of class-conditioned image generation results between HART and other autoregressive models, highlighting HART's superior performance in terms of FID, IS, MACs, and inference time.", "section": "4.2 Main Results"}, {"figure_path": "2410.10812/tables/table_9_1.html", "caption": "Table 4: HART achieves better class-conditioned image generation results compared to MAR (Li et al., 2024b) with 10.7\u00d7 lower MACs and 12.9\u00d7 faster runtime. It also offers 7.8% FID reduction with 4% runtime overhead compared with VAR (Tian et al., 2024). Time: bs=64 on A100.", "description": "Table 4 compares the performance of HART against MAR and VAR models on class-conditioned image generation, showing that HART achieves better FID and IS scores with significantly fewer computations and faster inference time.", "section": "4.2 Main Results"}, {"figure_path": "2410.10812/tables/table_9_2.html", "caption": "Table 5: HART learns residual tokens, which enhance conditioned image generation as evidenced by improvements in FID, inception score, and CLIP score. The HART-VAR results are obtained by omitting residual diffusion from the full HART model. Left: class-to-image, right: text-to-image, *: results obtained using the official VAR quantizer.", "description": "Table 5 shows the ablation study results on the impact of residual tokens, alternating training, and scalable resolution transformer on FID, IS, and inference time.", "section": "4.3 ABLATION STUDIES AND ANALYSIS"}]