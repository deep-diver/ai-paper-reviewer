[{"figure_path": "2410.10812/charts/charts_10_0.png", "caption": "Figure 7: Left: residual tokens in HART are much easier to learn than full tokens in MAR. Middle/Right: Despite achieving similar reconstruction FID, single decoder with alternating training enables faster and better generation convergence.", "description": "The chart compares the performance of HART and MAR in terms of ImageNet Inception Score and FID, showing HART's superior efficiency and convergence with alternating training.", "section": "3.2 HYBRID AUTOREGRESSIVE MODELING WITH RESIDUAL DIFFUSION"}, {"figure_path": "2410.10812/charts/charts_16_0.png", "caption": "Figure 9: Left: The VAR attention in HART exhibits a sink + local pattern: for stages 8-10 visualized here, attention scores concentrate in the most recent two stages and the first three stages, akin to StreamingLLM. Right: Within the final stage, the attention score distribution is predominantly local. Note: For clearer visualization, we apply a sigmoid function to the attention scores in the rightmost three subfigures.", "description": "The chart visualizes the attention patterns of the VAR transformer in HART, showing a \"sink + local\" pattern across stages and a predominantly local pattern within the final stage.", "section": "A.1 ATTENTION PATTERN ANALYSIS"}]