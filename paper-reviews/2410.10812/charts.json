[{"figure_path": "2410.10812/charts/charts_10_0.png", "caption": "Figure 7: Left: residual tokens in HART are much easier to learn than full tokens in MAR. Middle/Right: Despite achieving similar reconstruction FID, single decoder with alternating training enables faster and better generation convergence.", "description": "The chart compares the performance of HART and MAR models in terms of Inception Score and FID, showing that HART achieves better results with fewer training steps and sampling steps.", "section": "3.2 HYBRID AUTOREGRESSIVE MODELING WITH RESIDUAL DIFFUSION"}, {"figure_path": "2410.10812/charts/charts_16_0.png", "caption": "Figure 9: Left: The VAR attention in HART exhibits a sink + local pattern: for stages 8-10 visualized here, attention scores concentrate in the most recent two stages and the first three stages, akin to StreamingLLM. Right: Within the final stage, the attention score distribution is predominantly local. Note: For clearer visualization, we apply a sigmoid function to the attention scores in the rightmost three subfigures.", "description": "The chart visualizes the attention patterns of the VAR (Visual Autoregressive) transformer in HART (Hybrid Autoregressive Transformer) across different stages and layers, showing a localized attention pattern in the final stage.", "section": "A.1 ATTENTION PATTERN ANALYSIS"}]