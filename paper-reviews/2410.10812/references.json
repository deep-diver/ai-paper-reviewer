{"references": [{" publication_date": "2020", "fullname_first_author": "Jonathan Ho", "paper_title": "Denoising diffusion probabilistic models", "reason": "This paper is foundational to the field of diffusion models, which HART is compared against.  Understanding the core concepts of diffusion models is crucial for evaluating HART's contributions and placing its achievements within the context of the broader visual generation landscape.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Robin Rombach", "paper_title": "High-resolution image synthesis with latent diffusion models", "reason": "This work significantly advanced diffusion models, making them capable of generating high-resolution images. HART directly competes with these models, and understanding their capabilities and limitations is critical for evaluating HART's novelty and impact.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "William Peebles", "paper_title": "Scalable diffusion models with transformers", "reason": "This paper is relevant because it presents a highly efficient diffusion model architecture (DiT).  HART's efficiency is a key selling point, and directly comparing its performance and efficiency against DiT's helps establish the extent of HART's improvement in this area.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Dustin Podell", "paper_title": "SDXL: Improving latent diffusion models for high-resolution image synthesis", "reason": "SDXL represents a state-of-the-art diffusion model. HART is benchmarked against SDXL, and thus, understanding SDXL's capabilities is necessary to properly contextualize and assess HART's achievements in terms of both quality and efficiency.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "Ming Chen", "paper_title": "Generative pretraining from pixels", "reason": "This paper is highly influential in the field of autoregressive models.  HART builds upon the ideas and techniques introduced in this work, particularly regarding autoregressive image generation. Its impact on the broader methodology of HART is substantial.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Patrick Esser", "paper_title": "Taming transformers for high-resolution image synthesis", "reason": "This paper significantly improved the quality of autoregressive image generation using transformers, which is directly related to HART's approach.  Understanding its contributions sets the context for evaluating HART's advancements.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Katherine Crowson", "paper_title": "VQGAN-CLIP: Open domain image generation and editing with natural language guidance", "reason": "This paper introduced a crucial component, VQGAN, which uses vector quantization to convert images into discrete tokens.  Understanding how this technique works, and its strengths and limitations, is critical to comprehending HART's hybrid tokenization approach.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Daiqing Li", "paper_title": "Playground v2.5: Three insights towards enhancing aesthetic quality in text-to-image generation", "reason": "This paper details a state-of-the-art diffusion model.  HART's performance is evaluated against Playground v2.5. Understanding its achievements provides critical context for gauging the quality and impact of HART.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Junsong Chen", "paper_title": "PixArt-\u03a3: Weak-to-strong training of diffusion transformer for 4k text-to-image generation", "reason": "This paper demonstrates strong results in diffusion models at high resolution.  HART's capabilities are compared to PixArt-\u03a3's, hence understanding this paper is crucial to assessing HART's capabilities.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Keyu Tian", "paper_title": "Visual autoregressive modeling: Scalable image generation via next-scale prediction", "reason": "This paper introduces a method for generating images that is closely related to HART\u2019s autoregressive approach.  Understanding this prior work helps establish the novelty and significance of HART\u2019s approach.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Ji Lin", "paper_title": "VILA: On pre-training for visual language models", "reason": "This paper concerns efficient large vision-language models that integrate visual and textual information.  Understanding this related technology is important since HART may benefit from similar methods, and thus the paper provides context.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Michael Tschannen", "paper_title": "GIVT: Generative infinite-vocabulary transformers", "reason": "This paper is highly relevant as it presents a hybrid model using autoregressive and continuous tokenization techniques, similar to HART.  Direct comparison to GIVT helps highlight the novel aspects of the hybrid approach in HART.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Ming Ding", "paper_title": "Cogview2: Faster and better text-to-image generation via hierarchical transformers", "reason": "This paper demonstrates a fast and high-quality text-to-image generation approach using transformers.  HART aims for similar efficiency, and understanding Cogview2 helps in evaluating the efficiency and quality gains achieved by HART.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Aditya Ramesh", "paper_title": "Zero-shot text-to-image generation", "reason": "This paper is highly influential and presents a seminal method for text-to-image generation.   Understanding its core contributions is crucial for evaluating HART's performance, especially given that HART also addresses text-to-image generation.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Aditya Ramesh", "paper_title": "Hierarchical text-conditional image generation with CLIP latents", "reason": "This paper significantly advanced image generation with CLIP latents, providing a strong benchmark for text-to-image generation. HART's text-to-image performance is measured against this approach, making understanding this crucial for evaluation.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Jiahui Yu", "paper_title": "Scaling autoregressive models for content-rich text-to-image generation", "reason": "This paper explored scaling up autoregressive models for high-resolution image generation, addressing a key challenge that HART also addresses. A comparison provides context to assess the effectiveness of HART's scaling strategy.", "section_number": 2}, {" publication_date": "2017", "fullname_first_author": "Aaron Van Den Oord", "paper_title": "Neural discrete representation learning", "reason": "This paper introduces the concept of vector quantization (VQ), a technique extensively used in autoregressive image generation models.  Understanding VQ is crucial for comprehending the underlying mechanisms of HART's hybrid tokenizer.", "section_number": 1}, {" publication_date": "2020", "fullname_first_author": "Jonathan Ho", "paper_title": "Denoising diffusion probabilistic models", "reason": "This paper introduced diffusion models which HART improves upon.  As diffusion models serve as a benchmark for comparison, understanding its foundation is essential for the proper assessment of HART's contribution.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Cheng Lu", "paper_title": "DPM-Solver: A fast ODE solver for diffusion probabilistic model sampling in around 10 steps", "reason": "This paper proposed an efficient sampling technique for diffusion models. Since HART aims to improve the efficiency of image generation, understanding existing efficient methods is essential to assess the magnitude of HART\u2019s improvement.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Keyu Tian", "paper_title": "Visual autoregressive modeling: Scalable image generation via next-scale prediction", "reason": "This paper is highly relevant because it proposes a state-of-the-art autoregressive method for image generation. Comparing HART\u2019s performance and approach to this work is crucial for demonstrating its novelty and advancement in the field.", "section_number": 3}]}