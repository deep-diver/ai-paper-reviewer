[{"figure_path": "2410.22330/figures/figures_1_0.png", "caption": "Figure 1: Vision-and-language models (VLMs) map inputs to abstract task representations that are consistent across modalities and specifications. For example, the task of mapping a country to its capital can be expressed in various ways (a), all of which lead to similar task representations (b).", "description": "The figure illustrates how vision-and-language models (VLMs) generate consistent task representations from various input modalities and specifications.", "section": "ABSTRACT"}, {"figure_path": "2410.22330/figures/figures_1_1.png", "caption": "Figure 1: Vision-and-language models (VLMs) map inputs to abstract task representations that are consistent across modalities and specifications. For example, the task of mapping a country to its capital can be expressed in various ways (a), all of which lead to similar task representations (b).", "description": "The figure illustrates how vision-and-language models (VLMs) generate consistent task representations across different input modalities and specifications.", "section": "ABSTRACT"}, {"figure_path": "2410.22330/figures/figures_2_0.png", "caption": "Figure 2: Cross-modal transfer. Task representations can be computed in one modality (left) and patched to guide VLMs to perform a task on queries from a different modality (right). We observe that certain tasks are more effectively represented in one modality and therefore benefit from transfer.", "description": "The figure illustrates how task representations learned from one modality (text or image examples) can be transferred to improve the performance of a vision-language model on queries from a different modality.", "section": "2 CROSS-MODAL TASK VECTORS"}]