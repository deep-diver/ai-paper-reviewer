{"references": [{" publication_date": "2020", "fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "reason": "This paper is foundational for understanding the capabilities of large language models (LLMs), which are crucial to the study of vision-language models (VLMs).  The concept of few-shot learning is central to the paper's investigation of in-context learning (ICL) and task vectors, which heavily rely on the LLM's ability to quickly adapt to new tasks with limited examples.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Roee Hendel", "paper_title": "In-context learning creates task vectors", "reason": "This paper introduces the concept of task vectors, which are central to the current study. It establishes the foundation for understanding how models implicitly create task representations during in-context learning, laying the groundwork for the current paper's investigation of cross-modal task vectors in VLMs.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Eric Todd", "paper_title": "Function vectors in large language models", "reason": "This paper directly relates to the current study by expanding on the concept of task vectors within LLMs, exploring how task information is encoded in token representations within in-context learning (ICL) examples. This provides crucial context for the paper's investigation of cross-modal task vectors in VLMs.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Alberto Hojel", "paper_title": "Finding visual task vectors", "reason": "This work is highly relevant to the current paper as it directly addresses the issue of task vectors in the context of computer vision models.  Understanding how task vectors are formed in vision-only models is crucial to building a complete understanding of cross-modal task vectors in VLMs, which is the primary focus of the current paper.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Jean-Baptiste Alayrac", "paper_title": "Flamingo: a visual language model for few-shot learning", "reason": "This paper introduces a significant VLM architecture (Flamingo) which is used as a model in the experimental setup. The Flamingo architecture's capabilities and properties directly influence the current study's results and analysis of cross-modal task vector behavior.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Rohan Bavishi", "paper_title": "Fuyu-8b: A multimodal architecture for ai agents", "reason": "This paper introduces the Mantis-Fuyu model, which is used in the experiments of the current paper. Understanding this model's architecture and capabilities are essential to interpreting the findings of the study concerning cross-modal task vector performance.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Hugo Lauren\u00e7on", "paper_title": "What matters when building vision-language models?", "reason": "This paper directly addresses the design choices and considerations involved in creating VLMs, providing critical background information for interpreting the findings of the current study. The focus on vision-language model design choices aids in contextualizing the results of the current study's cross-modal transfer experiments.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Wei-Lin Chiang", "paper_title": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality", "reason": "This paper introduces the LLaVA-v1.5 model, which is utilized in the experimental portion of the current study. The qualities and characteristics of this model are directly relevant to interpreting the results and understanding the behaviors of cross-modal task vectors.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Albert Q. Jiang", "paper_title": "Mistral 7b", "reason": "This paper details the Idefics2 model used in the experiments, providing vital background for understanding the model's architecture and how it handles cross-modal information processing, which directly relates to the paper's findings on cross-modal task vectors.", "section_number": 3}, {" publication_date": "2021", "fullname_first_author": "Mostafa Abdou", "paper_title": "Can language models encode perceptual structure without grounding?", "reason": "This paper explores the relationship between language models and their ability to encode perceptual information, offering a relevant theoretical perspective for understanding the mechanisms underlying the cross-modal task vector representations investigated in the current paper.", "section_number": 4}, {" publication_date": "2022", "fullname_first_author": "Jean-Baptiste Alayrac", "paper_title": "Flamingo: a visual language model for few-shot learning", "reason": "This paper is important due to its discussion of few-shot learning in the context of VLMs. The paper's investigation of few-shot learning is directly relevant to the current study's exploration of in-context learning (ICL) and task vectors, providing additional context for evaluating and interpreting the results.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Folco Bertini Baldassini", "paper_title": "What makes multimodal in-context learning work?", "reason": "This paper is highly relevant as it directly investigates multimodal in-context learning (ICL), a key aspect of the current study. The paper's analysis of multimodal ICL provides critical background information for understanding the challenges and opportunities related to cross-modal task vectors and the overall process of answer generation.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Junnan Li", "paper_title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models", "reason": "This paper presents a significant VLM architecture (BLIP-2) that is relevant to the current paper's investigation. Understanding the architecture of different VLMs is crucial for comparing experimental results and gaining insights into the diverse approaches to handling cross-modal information.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Haotian Liu", "paper_title": "Visual instruction tuning", "reason": "This paper is directly relevant to the current paper because it introduces the concept of visual instruction tuning which is closely related to the concept of instruction-based task vectors explored in the current paper. Instruction-based task vectors are a key component of the current paper's cross-modal analysis.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Sivan Doveh", "paper_title": "Towards multimodal in-context learning for vision & language models", "reason": "This paper's focus on multimodal in-context learning directly relates to the central theme of the current study. Investigating multimodal in-context learning within VLMs provides valuable context and comparative analysis for evaluating the paper's findings on cross-modal task vectors.", "section_number": 4}, {" publication_date": "2022", "fullname_first_author": "Roma Patel", "paper_title": "Mapping language models to grounded conceptual spaces", "reason": "This paper provides a theoretical background for understanding the relationships between language models and their ability to represent conceptual information.  This is highly relevant to the current study because it provides a theoretical foundation for interpreting how language models might represent cross-modal tasks.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Ellie Pavlick", "paper_title": "Symbols and grounding in large language models", "reason": "This paper delves into the important issue of grounding within large language models (LLMs). This is particularly relevant to the current study because it directly addresses how LLMs and VLMs might represent the world and how this representation impacts their ability to process cross-modal data.", "section_number": 4}, {" publication_date": "2022", "fullname_first_author": "Judea Pearl", "paper_title": "Direct and indirect effects", "reason": "This paper is highly relevant to the current study's investigation of the mechanistic interpretability of VLMs.  The causal inference methods presented provide a theoretical framework for understanding the complex relationships between different aspects of the VLM architecture and their interactions.", "section_number": 4}, {" publication_date": "2021", "fullname_first_author": "Catherine Olsson", "paper_title": "In-context learning and induction heads", "reason": "This paper is important due to its investigation of in-context learning (ICL) mechanisms. The paper's analysis of ICL mechanisms provides a valuable theoretical framework for understanding how models learn tasks from limited examples, and how this process impacts task vector formation within VLMs.", "section_number": 4}, {" publication_date": "2020", "fullname_first_author": "nostalgebraist", "paper_title": "interpreting gpt: the logit lens", "reason": "This paper introduces a useful methodology (logit lens) for analyzing the internal representations of language models. This methodology is directly applied in the current paper for analyzing the token representations within VLMs and understanding their evolution during the process of answer generation.", "section_number": 4}]}