[{"content": "| Model | NIH (AUROC) |  |  | VinDr (AUROC) |  |  |\n|---|---|---|---|---|---|---|\n|  | 1% | 10% | 100% | 1% | 10% | 100% |\n| ConVIRT | 77.0 \u00b1 0.1 | 81.5 \u00b1 0.01 | 84.2 \u00b1 0.06 | 88.1 \u00b1 0.1 | 90.5 \u00b1 0.1 | 90.9 \u00b1 0.2 |\n| GLoRIA | 74.2 \u00b1 0.5 | 81.0 \u00b1 0.16 | 83.8 \u00b1 0.15 | 87.5 \u00b1 0.1 | 90.3 \u00b1 0.2 | 91.3 \u00b1 0.1 |\n| MedCLIP-R50 | 74.2 \u00b1 0.6 | 79.5 \u00b1 0.36 | 83.9 \u00b1 0.08 | 83.0 \u00b1 2.0 | 87.7 \u00b1 0.3 | 89.8 \u00b1 0.4 |\n| MedCLIP-ViT | 76.1 \u00b1 0.3 | 81.4 \u00b1 0.25 | 84.5 \u00b1 0.17 | 83.6 \u00b1 1.5 | 89.7 \u00b1 0.5 | 88.7 \u00b1 0.4 |\n| MedKLIP | 75.2 \u00b1 0.1 | 80.3 \u00b1 0.08 | 83.9 \u00b1 0.08 | 77.5 \u00b1 1.9 | 85.8 \u00b1 2.1 | 89.9 \u00b1 0.5 |\n| M-FLAG | 66.5 \u00b1 0.5 | 78.4 \u00b1 0.55 | 84.0 \u00b1 0.04 | 69.2 \u00b1 2.1 | 81.7 \u00b1 0.8 | 86.6 \u00b1 0.9 |\n| MGCA-R50 | 73.2 \u00b1 0.3 | 79.9 \u00b1 0.08 | 83.5 \u00b1 0.04 | 84.5 \u00b1 0.5 | 89.1 \u00b1 0.3 | 90.6 \u00b1 0.2 |\n| MGCA-ViT | 78.2 \u00b1 0.1 | 82.4 \u00b1 0.03 | 84.4 \u00b1 0.05 | 88.3 \u00b1 0.1 | 91.5 \u00b1 0.2 | 91.8 \u00b1 0.3 |\n| MRM | 80.1 \u00b1 0.1 | 83.5 \u00b1 0.10 | 85.3 \u00b1 0.05 | 87.1 \u00b1 0.1 | 89.9 \u00b1 0.1 | 91.2 \u00b1 0.3 |\n| REFERS | 76.4 \u00b1 0.3 | 81.3 \u00b1 0.01 | 83.7 \u00b1 0.06 | 87.1 \u00b1 0.1 | 89.4 \u00b1 0.3 | 90.0 \u00b1 0.5 |", "caption": "Table 1: Multi-label classification performance (%percent\\%%) of MedVLP methods (Best, Second Best).", "description": "This table presents the results of a multi-label image classification task, comparing the performance of various Medical Vision-Language Pretraining (MedVLP) models.  The performance is measured using the Area Under the Receiver Operating Characteristic curve (AUROC), a common metric for evaluating the effectiveness of classification models in distinguishing between multiple classes. Results are shown for three different training data sizes (1%, 10%, and 100%), highlighting the impact of data availability on model performance.  The table indicates the best and second-best AUROC scores achieved by each MedVLP model on two benchmark datasets, NIH and VinDr.", "section": "4.1 Medical Image Classification"}, {"content": "COVIDx (F1)\n\n| Model | 1% | 10% | 100% | SIIM (F1) | 1% | 10% | 100% | RSNA (F1) | 1% | 10% | 100% |\n|---|---|---|---|---|---|---|---|---|---|---|---| \n| ConVIRT | 67.4\u00b10.6 | 68.7\u00b10.1 | 68.1\u00b10.1 | 62.8\u00b10.7 | 64.8\u00b11.7 | 72.8\u00b10.8 | 58.0\u00b10.5 | 63.3\u00b10.3 | 65.0\u00b10.8 |\n| GLoRIA | 66.6\u00b10.6 | 68.2\u00b10.1 | 68.3\u00b10.0 | 59.3\u00b11.0 | 63.4\u00b11.1 | 69.0\u00b12.3 | 60.1\u00b10.6 | 62.0\u00b11.1 | 64.7\u00b11.0 |\n| MedCLIP-R50 | 68.5\u00b11.7 | 68.3\u00b10.2 | 68.3\u00b10.1 | 64.8\u00b11.1 | 68.4\u00b11.1 | 73.2\u00b11.7 | 62.9\u00b10.5 | 63.9\u00b10.3 | 65.3\u00b10.8 |\n| MedCLIP-ViT | 67.1\u00b10.5 | 68.7\u00b10.4 | 68.3\u00b10.1 | 68.6\u00b10.8 | 71.5\u00b11.1 | 75.7\u00b10.2 | 63.5\u00b10.5 | 65.3\u00b11.0 | 66.2\u00b10.8 |\n| MedKLIP | 66.5\u00b10.2 | 69.3\u00b10.6 | 68.3\u00b10.3 | 61.4\u00b10.3 | 64.4\u00b12.1 | 72.7\u00b11.4 | 60.4\u00b10.6 | 61.9\u00b11.4 | 66.0\u00b10.6 |\n| M-FLAG | 67.6\u00b10.3 | 69.2\u00b11.0 | 68.1\u00b10.1 | 47.1\u00b10.3 | 61.8\u00b11.5 | 72.1\u00b11.6 | 56.0\u00b10.9 | 60.3\u00b11.4 | 64.4\u00b10.3 |\n| MGCA-R50 | 68.2\u00b11.1 | 68.4\u00b10.2 | 68.0\u00b10.1 | 59.7\u00b11.2 | 61.3\u00b11.0 | 69.4\u00b10.8 | 57.3\u00b10.5 | 61.9\u00b10.6 | 64.0\u00b11.3 |\n| MGCA-ViT | 66.5\u00b10.9 | 68.1\u00b10.1 | 68.2\u00b10.0 | 66.3\u00b10.3 | 68.6\u00b10.9 | 73.3\u00b10.8 | 61.0\u00b11.3 | 64.3\u00b10.4 | 66.9\u00b11.4 |\n| MRM | 67.4\u00b10.6 | 68.2\u00b10.4 | 68.3\u00b10.2 | 65.0\u00b10.5 | 69.3\u00b11.0 | 75.6\u00b10.7 | 62.6\u00b11.1 | 66.6\u00b10.3 | 66.5\u00b10.2 |\n| REFERS | 66.7\u00b10.0 | 66.6\u00b11.0 | 68.5\u00b10.8 | 60.8\u00b11.0 | 66.9\u00b10.7 | 72.6\u00b10.3 | 61.7\u00b10.7 | 63.8\u00b10.1 | 67.2\u00b10.3 |", "caption": "Table 2: Binary classification performance (%percent\\%%) of MedVLP methods (Best, Second Best).", "description": "This table presents the results of binary classification experiments using various Medical Vision-Language Pretraining (MedVLP) methods.  It shows the performance, measured as the F1 score (%), across three different datasets: COVIDx, RSNA, and SIIM. Results are presented for three training set sizes (1%, 10%, and 100%) to illustrate the effect of data availability. The best and second-best performing models are highlighted for each dataset and training set size.", "section": "4.1 Medical Image Classification"}, {"content": "| Method | Obj-CXR | RSNA | SIIM | TBX11K |\n|---|---|---|---|---|\n| ConVIRT | 79.82 \u00b1 0.59 | 74.72 \u00b1 0.12 | 76.02 \u00b1 0.44 | 84.98 \u00b1 0.59 |\n| GLoRIA | 77.23 \u00b1 0.13 | 74.41 \u00b1 0.41 | 73.39 \u00b1 0.43 | 83.17 \u00b1 0.36 |\n| MedCLIP-R50 | 79.88 \u00b1 0.23 | 75.45 \u00b1 0.11 | 76.35 \u00b1 0.44 | 85.52 \u00b1 0.17 |\n| MedCLIP-ViT | 79.64 \u00b1 0.35 | 73.29 \u00b1 1.41 | 76.48 \u00b1 0.38 | 85.62 \u00b1 0.07 |\n| MedKLIP | 78.17 \u00b1 0.29 | 74.68 \u00b1 0.42 | 77.78 \u00b1 0.69 | 87.06 \u00b1 0.31 |\n| M-FLAG | 73.96 \u00b1 0.30 | 67.86 \u00b1 0.63 | 68.13 \u00b1 0.75 | 79.12 \u00b1 0.16 |\n| MGCA-R50 | 80.27 \u00b1 0.07 | 75.04 \u00b1 0.59 | 77.04 \u00b1 0.48 | 87.05 \u00b1 0.19 |\n| MGCA-ViT | 81.68 \u00b1 0.26 | 75.48 \u00b1 0.28 | 77.22 \u00b1 0.51 | 86.89 \u00b1 0.39 |\n| MRM | 80.45 \u00b1 0.02 | 75.69 \u00b1 0.56 | 78.66 \u00b1 0.52 | 87.85 \u00b1 0.47 |\n| PTUnifier | 80.64 \u00b1 0.10 | 74.54 \u00b1 0.50 | 74.91 \u00b1 0.58 | 85.78 \u00b1 0.05 |\n| REFERS | 80.47 \u00b1 0.08 | 75.52 \u00b1 0.34 | 75.33 \u00b1 0.85 | 86.39 \u00b1 0.26 |", "caption": "Table 3: Segmentation performance (%percent\\%%) in mDice score (Best, Second Best).", "description": "This table presents the performance of various Medical Vision-Language Pretraining (MedVLP) models on medical image segmentation tasks.  The mDice score, a common metric for evaluating segmentation accuracy, is reported for each model on four different chest X-ray datasets (Obj-CXR, RSNA, SIIM, and TBX11K). The table shows the best and second-best performing models for each dataset, providing a detailed comparison of the MedVLP methods' ability to perform accurate medical image segmentation.", "section": "4.2 Medical Image Segmentation"}, {"content": "Method|BLEU1|BLEU2|BLEU3|BLEU4|ROUGEL|METEOR\n---|---|---|---|---|---|---\nBaseline|0.415 \u00b1 0.047|0.256 \u00b1 0.030|0.179 \u00b1 0.023|0.133 \u00b1 0.018|0.329 \u00b1 0.019|0.165 \u00b1 0.022\nConVIRT|0.443 \u00b1 0.017|0.286 \u00b1 0.013|0.201 \u00b1 0.008|0.148 \u00b1 0.006|0.368 \u00b1 0.013|0.187 \u00b1 0.007\nGLoRIA|0.466 \u00b1 0.052|0.316 \u00b1 0.028|0.227 \u00b1 0.017|0.170 \u00b1 0.011|0.387 \u00b1 0.007|0.202 \u00b1 0.010\nMedCLIP-R50|0.440 \u00b1 0.031|0.295 \u00b1 0.013|0.216 \u00b1 0.007|0.163 \u00b1 0.006|0.380 \u00b1 0.010|0.189 \u00b1 0.006\nMedCLIP-ViT|0.421 \u00b1 0.046|0.280 \u00b1 0.032|0.201 \u00b1 0.026|0.151 \u00b1 0.020|0.382 \u00b1 0.011|0.180 \u00b1 0.009\nMedKLIP|0.470 \u00b1 0.011|0.310 \u00b1 0.022|0.222 \u00b1 0.021|0.167 \u00b1 0.016|0.379 \u00b1 0.009|0.194 \u00b1 0.005\nPTUnifier|0.468 \u00b1 0.022|0.307 \u00b1 0.019|0.217 \u00b1 0.011|0.162 \u00b1 0.007|0.380 \u00b1 0.006|0.194 \u00b1 0.011\nM-FLAG|0.412 \u00b1 0.029|0.274 \u00b1 0.024|0.196 \u00b1 0.019|0.147 \u00b1 0.016|0.371 \u00b1 0.009|0.185 \u00b1 0.004\nMGCA-R50|0.457 \u00b1 0.033|0.300 \u00b1 0.027|0.213 \u00b1 0.018|0.159 \u00b1 0.014|0.375 \u00b1 0.016|0.191 \u00b1 0.013\nMGCA-ViT|0.462 \u00b1 0.034|0.311 \u00b1 0.031|0.225 \u00b1 0.026|0.170 \u00b1 0.021|0.384 \u00b1 0.019|0.195 \u00b1 0.010\nREFERS|0.466 \u00b1 0.022|0.305 \u00b1 0.009|0.216 \u00b1 0.009|0.161 \u00b1 0.009|0.377 \u00b1 0.007|0.195 \u00b1 0.002", "caption": "Table 4: Radiology report generation resutls on the IUXray dataset (Best, Second Best).", "description": "This table presents the quantitative results of radiology report generation on the IUXray dataset.  It compares the performance of various Medical Vision-Language Pretraining (MedVLP) models against a baseline method. The evaluation metrics used are BLEU (1-4), ROUGE-L, and METEOR, all commonly used in Natural Language Generation (NLG) to assess the quality and similarity of generated text to reference text.  The \"Best\" and \"Second Best\" columns indicate the top-performing MedVLP models for each metric.", "section": "4.3 Radiology Report Generation"}, {"content": "| Model | H@1 | H@5 | H@10 | P@1 | P@5 | P@10 |\n|---|---|---|---|---|---|---|\n| ConVIRT | 61.9 | 88.2 | 94.2 | 61.9 | 54.9 | 52.5 |\n| GLoRIA | 54.6 | 86.3 | 93.6 | 54.6 | 49.7 | 47.2 |\n| MedCLIP-R50 | 16.1 | 35.1 | 46.4 | 16.1 | 16.6 | 18.8 |\n| MedCLIP-ViT | 42.0 | 77.9 | 88.8 | 42.0 | 41.0 | 40.6 |\n| MGCA-R50 | 57.9 | 87.9 | 95.8 | 57.9 | 53.0 | 50.2 |\n| MGCA-ViT | 63.3 | 90.4 | 95.5 | 63.3 | 56.4 | 52.6 |\n| PTUnifier | 78.7 | 99.5 | 100.0 | 78.7 | 38.4 | 23.4 |\n| REFERS | 54.4 | 83.4 | 90.5 | 54.4 | 52.5 | 50.5 |", "caption": "Table 5: Image-text retrieval results on the MIMIC 5x200 datasets (Best, Second Best).", "description": "This table presents the results of image-text retrieval experiments conducted on the MIMIC 5x200 dataset.  The MIMIC 5x200 dataset is a subset of the larger MIMIC-CXR dataset, specifically focusing on 5 different medical findings (Atelectasis, Cardiomegaly, Edema, Pleural Effusion, and Consolidation).  The task involves using an image as a query and retrieving the most relevant text reports describing that image. The table shows the performance of various MedVLP (Medical Vision-Language Pretraining) models, measured using two metrics: Hit@K (the percentage of correctly retrieved reports within the top K results) and Precision@K (the proportion of correctly retrieved reports among the top K results).  The results are presented for K=1, 5, and 10.  The table highlights the best and second-best performing models for each metric.", "section": "4.4 Medical Image-Text Retrieval"}, {"content": "| Method | None | +DLR | +DLR+LN | All |\n|---|---|---|---|---|\n| ConVIRT | 71.7 | 76.9 \u2191 | 74.5 \u2193 | 77.0 \u2191 |\n| GLoRIA | 72.8 | 74.2 \u2191 | 70.6 \u2193 | 74.9 \u2191 |\n| MedCLIP-R50 | 74.1 | 73.7 \u2193 | 74.2 \u2191 | 73.8 \u2193 |\n| MedCLIP-ViT | 75.5 | 75.7 \u2191 | 75.9 \u2191 | 70.7 \u2193 |\n| MedKLIP | 74.4 | 71.9 \u2193 | 75.2 \u2191 | 73.7 \u2193 |\n| MGCA-R50 | 72.8 | 73.0 \u2191 | 69.6 \u2193 | 73.8 \u2191 |\n| MGCA-ViT | 77.7 | 78.1 \u2191 | 78.2 \u2191 | 78.2 = |\n| MRM | 77.9 | 80.0 \u2191 | 79.5 \u2193 | 80.1 \u2191 |\n| REFERS | 76.8 | 75.9 \u2193 | 76.2 \u2193 | 75.6 \u2193 |", "caption": "Table 6: Classification results (AUROC) with different training strategies on the NIH dataset with 1%percent11\\%1 % training data.", "description": "This table presents the Area Under the Receiver Operating Characteristic Curve (AUROC) scores for different medical vision-language pretraining (MedVLP) models on the NIH Chest X-ray dataset.  The models are evaluated using only 1% of the training data.  Crucially, it showcases the impact of three different training strategies: Layer Normalization (LN), Truncated Normal Initialization (TNI), and Discriminative Learning Rates (DLR). By comparing AUROC scores across various combinations of these strategies, the table quantifies the impact of training choices on MedVLP model performance.", "section": "4.6 Impact of Training Strategies"}, {"content": "| Method | M-CLS (AUC) \u2191 | B-CLS (F1) \u2191 | SEG (mDice) \u2191 | RRG (BLEU4) \u2191 | Avg. Rank \u2193 |\n|---|---|---|---|---|---| \n| ConVIRT | 85.37 | 65.56 | 78.89 | 14.8 | 6.38 |\n| GLoRIA | 84.68 | 64.06 | 77.05 | 17.0 | 5.88 |\n| MedCLIP-R50 | 83.02 | 67.17 | 79.80 | 16.3 | 5.25 |\n| MedCLIP-ViT | 84.00 | 68.33 | 78.76 | 15.1 | 5.75 |\n| MedKLIP | 82.77 | 65.56 | 79.42 | 16.7 | 6.13 |\n| M-FLAG | 77.73 | 62.96 | 72.77 | 14.7 | 10.00 |\n| MGCA-R50 | 83.47 | 64.69 | 79.85 | 15.9 | 6.50 |\n| MGCA-ViT | 86.10 | 67.03 | 80.32 | 17.0 | 2.38 |\n| MRM | 86.18 | 67.72 | 80.66 | 16.5 | 2.00 |\n| REFERS | 84.65 | 66.06 | 79.93 | 16.1 | 4.75 |", "caption": "Table 7: Overall performance (%percent\\%%) of each MedVLP method across different tasks (Best, Second Best).", "description": "This table presents a comprehensive comparison of nine Medical Vision-Language Pretraining (MedVLP) models across four distinct downstream medical tasks: multi-label classification, binary classification, segmentation, and radiology report generation.  For each task, the table shows the average performance of each MedVLP model, expressed as a percentage, based on the best and second-best results achieved. The models are ranked based on their overall performance across all four tasks, offering insights into their relative strengths and weaknesses in handling different types of medical image analysis.", "section": "4 Benchmark Results"}, {"content": "| Dataset | Image Size | Dataset Size | Task | Annotation |\n|---|---|---|---|---|\n| NIH ChestX-ray 14 | 224x224 | 112,120 | CLS | 14 Classes |\n| VinDr-CXR | 512x640 | 18,000 | CLS | 28 classes, BBoxes |\n| COVIDx CXR-4 | 1024x1024 | 84,818 | CLS | 2 Classes |\n| SIIM-ACR PTX | 512x512 | 12,047 | CLS, SEG | 2 Classes, Masks |\n| RSNA Pneumonia | 1024x1024 | 26,684 | CLS, SEG | BBoxes |\n| IU-Xray | 512x640 | 3,955 | RRG | Image-Report Pairs |\n| Object CXR | 2048x2624 | 10,000 | DET | BBoxes, Ellipse, Polygons |\n| TBX11K | 512x512 | 11,200 | CLS, SEG | 3 classes, BBoxes |\n| MIMIC 5x200 | 512x512 | 1,000 | RET | Image-Report Pairs |", "caption": "Table 8: Statistics of the test datasets.", "description": "This table presents a summary of the nine chest X-ray datasets used for evaluating the performance of various Medical Vision-Language Pretraining (MedVLP) methods.  For each dataset, it lists the image size, the number of images, the type of task(s) it is used for (classification, segmentation, report generation, or image-text retrieval), and the type of annotations available (e.g., class labels, bounding boxes, masks, or image-report pairs).", "section": "3 The BenchX Framework"}, {"content": "| Method | Learning Rate | Batch Size | Optimizer | LN | DLR |\n|---|---|---|---|---|---| \n| ConVIRT | 1e-4 | 64 | Adam | Yes | Yes |\n| GLoRIA | 1e-4 | 64 | Adam | Yes | Yes |\n| MedCLIP-R50 | 1e-5 | 64 | Adam | No | No |\n| MedCLIP-ViT | 1e-5 | 32 | Adam | No | No |\n| MedKLIP | 1e-4 | 128 | Adam | No | Yes |\n| M-FLAG | 1e-4 | 32 | Adam | Yes | No |\n| MGCA-R50 | 1e-5 | 32 | Adam | Yes | No |\n| MGCA-ViT | 1e-2 | 64 | SGD | Yes | Yes |\n| MRM | 3e-2 | 64 | SGD | Yes | Yes |\n| REFERS | 3e-2 | 32 | SGD | Yes | No |", "caption": "Table 9: Selected hyper-parameters per method on the NIH dataset.", "description": "This table lists the hyperparameters used for each of the nine MedVLP methods evaluated on the NIH ChestX-Ray dataset.  For each method, it shows the learning rate, batch size, optimizer used (Adam or SGD), whether layer normalization (LN) was applied, and whether discriminative learning rates (DLR) were used.  These hyperparameters were chosen to optimize performance on the NIH dataset during the experiments.", "section": "3.2 Standardizing Data Preprocessing and Training Strategies"}, {"content": "| Method | Learning Rate | Batch Size | Optimizer | LN | DLR |\n|---|---|---|---|---|---| \n| ConVIRT | 5e-05 | 32 | Adam | Yes | Yes |\n| GLoRIA | 1e-04 | 64 | Adam | Yes | Yes |\n| MedCLIP-R50 | 1e-04 | 128 | Adam | No | No |\n| MedCLIP-ViT | 1e-04 | 128 | Adam | No | No |\n| MedKLIP | 1e-04 | 64 | Adam | No | Yes |\n| M-FLAG | 1e-04 | 64 | Adam | Yes | No |\n| MGCA-R50 | 5e-05 | 64 | Adam | Yes | No |\n| MGCA-ViT | 0.03 | 64 | SGD | Yes | Yes |\n| MRM | 0.01 | 64 | SGD | Yes | Yes |\n| REFERS | 0.03 | 128 | SGD | Yes | No |", "caption": "Table 10: Selected hyper-parameters per method on the VinDr dataset.", "description": "This table details the hyperparameters used for each of the nine MedVLP methods evaluated on the VinDr dataset.  For each method, it lists the learning rate, batch size, optimizer used (Adam or SGD), whether layer normalization (LN) was applied, and whether discriminative learning rates (DLR) were used. This information is crucial for understanding and reproducing the experimental results, showcasing the fine-tuning choices made to optimize each method's performance on this specific dataset.", "section": "3.3 Unifying Task Adaptation Pipelines"}, {"content": "| Method | Learning Rate | Batch Size | Optimizer | LN | DLR |\n|---|---|---|---|---|---| \n| ConVIRT | 5e-04 | 64 | Adam | Yes | Yes |\n| GLoRIA | 5e-04 | 32 | Adam | Yes | Yes |\n| MedCLIP-R50 | 5e-04 | 64 | Adam | No | No |\n| MedCLIP-ViT | 1e-04 | 64 | Adam | No | No |\n| MedKLIP | 1e-04 | 64 | Adam | No | Yes |\n| M-FLAG | 5e-04 | 128 | Adam | Yes | No |\n| MGCA-R50 | 5e-04 | 128 | Adam | Yes | No |\n| MGCA-ViT | 5e-04 | 32 | Adam | Yes | Yes |\n| MRM | 5e-04 | 64 | Adam | Yes | Yes |\n| REFERS | 5e-04 | 64 | Adam | Yes | No |", "caption": "Table 11: Selected hyper-parameters per method on the COVIDx dataset.", "description": "This table details the optimal hyperparameters used for each of the nine MedVLP (Medical Vision-Language Pretraining) models evaluated on the COVIDx dataset.  The hyperparameters include the learning rate, batch size, optimizer used (Adam or SGD), and whether layer normalization (LN) and discriminative learning rates (DLR) were applied during training.  This information is crucial for understanding the experimental setup and reproducibility of the results reported for each MedVLP model on this specific dataset.", "section": "3.3 Unifying Task Adaptation Pipelines"}, {"content": "| Method | Learning Rate | Batch Size | Optimizer | LN | DLR |\n|---|---|---|---|---|---| \n| ConVIRT | 1e-4 | 128 | Adam | Yes | Yes |\n| GLoRIA | 1e-5 | 128 | Adam | Yes | Yes |\n| MedCLIP-R50 | 1e-5 | 128 | Adam | No | No |\n| MedCLIP-ViT | 1e-5 | 32 | Adam | No | No |\n| MedKLIP | 1e-4 | 64 | Adam | No | Yes |\n| M-FLAG | 1e-4 | 64 | Adam | Yes | No |\n| MGCA-R50 | 1e-5 | 128 | Adam | Yes | No |\n| MGCA-ViT | 1e-2 | 128 | SGD | Yes | Yes |\n| MRM | 1e-2 | 64 | SGD | Yes | Yes |\n| REFERS | 3e-2 | 64 | SGD | Yes | No |", "caption": "Table 12: Selected hyper-parameters per method on the SIIM dataset.", "description": "This table details the hyperparameters used for each of the nine MedVLP (Medical Vision-Language Pretraining) methods tested on the SIIM (Society for Imaging Informatics in Medicine) dataset.  It lists the learning rate, batch size, optimizer used, and whether layer normalization (LN) and discriminative learning rates (DLR) were applied during training.  These settings are crucial for ensuring fair comparison between different MedVLP models on the SIIM dataset's image segmentation task.", "section": "3.3 Unifying Task Adaptation Pipelines"}, {"content": "| Method | Learning Rate | Batch Size | Optimizer | LN | DLR |\n|---|---|---|---|---|---| \n| ConVIRT | 5e-05 | 64 | Adam | Yes | Yes |\n| GLoRIA | 1e-04 | 32 | Adam | Yes | Yes |\n| MedCLIP-R50 | 1e-05 | 32 | Adam | No | No |\n| MedCLIP-ViT | 1e-05 | 32 | Adam | No | No |\n| MedKLIP | 1e-04 | 128 | Adam | No | Yes |\n| M-FLAG | 1e-04 | 64 | Adam | Yes | No |\n| MGCA-R50 | 1e-05 | 32 | Adam | Yes | No |\n| MGCA-ViT | 0.01 | 32 | SGD | Yes | Yes |\n| MRM | 0.01 | 32 | SGD | Yes | Yes |\n| REFERS | 0.01 | 32 | SGD | Yes | No |", "caption": "Table 13: Selected hyper-parameters per method on the RSNA dataset.", "description": "This table details the hyperparameters used for each of the nine MedVLP methods evaluated on the RSNA dataset.  It lists the learning rate, batch size, optimizer used (Adam or SGD), and whether layer normalization (LN) and discriminative learning rates (DLR) were employed. This information is crucial for reproducibility and understanding the experimental setup of the study.", "section": "3.3 Unifying Task Adaptation Pipelines"}]