[{"page_end_idx": 1, "page_start_idx": 1, "section_number": 1, "section_title": "Introduction", "details": {"details": "Large language models (LLMs) are expensive to deploy due to their substantial memory and computational demands. Parameter sharing is a potential solution to reduce the costs associated with LLMs, however, its effectiveness in modern LLMs remains limited. This work revisits parameter sharing in Transformers and proposes novel methods to convert existing LLMs into smaller, more efficient Recursive Transformers that share parameters across layers, while maintaining strong performance.", "first_cons": "Larger models with more parameters are expensive to deploy due to their substantial memory and computational demands.", "first_pros": "Parameter sharing approaches can lower costs by reducing memory footprint, allowing for the use of fewer or lower-grade accelerators, or larger batch sizes for better throughput.", "keypoints": ["**High deployment cost of LLMs** due to memory and computational demands", "**Parameter sharing** as a potential solution but limited effectiveness in modern LLMs", "Novel methods to convert existing LLMs into smaller, efficient **Recursive Transformers**", "Minimal performance loss with parameter sharing across layers"], "second_cons": "While parameter sharing has shown encouraging capabilities, its application to modern LLMs has yielded limited reported success.", "second_pros": "Recursive Transformers offer a more efficient architecture for LLMs, potentially leading to significant gains in inference throughput.", "summary": "This paper explores efficient LLM deployment by revisiting parameter sharing in Transformers, introducing Recursive Transformers that share parameters across layers with minimal performance loss, and proposing novel methodologies for converting existing models."}}, {"page_end_idx": 4, "page_start_idx": 2, "section_number": 2, "section_title": "Effective Model Compression with Recursive Patterns", "details": {"details": "This section details methods for converting vanilla Transformer models into smaller, more efficient Recursive Transformers.  It introduces two main approaches: **Recursive Transformers**, which reuse a single block of layers recursively, and **Relaxed Recursive Transformers**, which add layer-specific low-rank adaptation (LoRA) modules to the recursive structure.  Effective initialization methods for the shared layers using weights from the original, pretrained model (**Stepwise**, **Average**, and **Lower** methods) are presented, aiming to mitigate potential performance degradation from weight sharing.  The section also introduces a novel inference paradigm, **Continuous Depth-wise Batching**, to further improve the throughput of recursive models, especially when combined with early exiting.", "first_cons": "While the Recursive Transformer is efficient, its performance can be limited by the constraints of parameter sharing across layers. Initializing the shared layers effectively is crucial to achieve good performance.", "first_pros": "Recursive Transformers significantly reduce the number of trainable parameters, leading to smaller model sizes and potential efficiency gains in terms of memory and computation.", "keypoints": ["Recursive Transformer reuses a block of layers, reducing parameters.", "Relaxed version adds LoRA modules for better performance.", "Effective initialization (Stepwise, Average, Lower) is crucial.", "Continuous Depth-wise Batching enhances inference throughput."], "second_cons": "Relaxing the parameter sharing constraint with LoRA modules adds complexity and slightly increases the number of parameters, potentially offsetting some of the efficiency gains.", "second_pros": "The Relaxed Recursive Transformer offers a flexible way to control the trade-off between model size and performance by adjusting the rank of the LoRA matrices.  It seamlessly transitions between fully tied and vanilla Transformer architectures.", "summary": "This section presents novel methods for compressing large language models using recursive patterns and layer-wise parameter sharing, enabling smaller, more efficient models with minimal performance loss."}}, {"page_end_idx": 5, "page_start_idx": 5, "section_number": 2, "section_title": "Continuous Depth-wise Batching and Early-Exiting", "details": {"details": "The core idea is to leverage the recursive nature of Recursive Transformers to enable a new inference paradigm called **Continuous Depth-wise Batching**.  This paradigm combines the already established **Continuous Sequence-wise Batching** with **early exiting**.  Unlike standard batching where all model stages must complete for one batch before the next begins, Continuous Depth-wise Batching allows new requests to fill the slots created when responses finish early, significantly improving throughput. Early exiting further speeds this by allowing predictions for high-confidence samples at earlier stages. The recursive structure allows for dynamic grouping of computations across depths and time steps, maximizing resource utilization.  A theoretical analysis suggests that this method has the potential to yield a significant throughput gain (2-3x).", "first_cons": "Early exiting has synchronization challenges in standard Transformers, requiring specialized mechanisms to efficiently manage incomplete computations and avoid performance degradation.", "first_pros": "Continuous Depth-wise Batching, enabled by the recursive architecture of the Recursive Transformer, allows requests to be scheduled continuously, thus increasing throughput.", "keypoints": ["Recursive architecture enables continuous Depth-wise Batching.", "Early exiting further improves throughput by generating high-confidence samples earlier.", "Theoretical analysis predicts 2-3x throughput gains.", "Dynamic grouping across depths and timesteps optimizes resource use."], "second_cons": "Real-world implementations of continuous depth-wise batching and early exiting require efficient algorithms that prevent performance degradation and address synchronization challenges.", "second_pros": "Early exiting is naturally compatible with the recursive structure of the model.  This method can further improve throughput gains.", "summary": "Continuous Depth-wise Batching and early exiting in Recursive Transformers significantly boosts inference throughput by continuously scheduling requests and making predictions opportunistically for samples with high confidence at earlier stages."}}, {"page_end_idx": 7, "page_start_idx": 6, "section_number": 3, "section_title": "Experiments", "details": {"details": "This section details experimental evaluations of the proposed Relaxed Recursive Transformers on three large language models: Gemma 2B, TinyLlama 1.1B, and Pythia 1B.  The models were converted to Recursive Transformers and uptrained on the SlimPajama dataset.  **Non-recursive baselines**, including full-size and reduced-size models, were also evaluated.  **Initialization techniques** for the Recursive Transformers were compared. The results show that Recursive Transformers, particularly when using the Stepwise initialization method, achieve comparable or even superior performance to full-size models while significantly reducing the number of parameters.  **Relaxed Recursive Transformers**, which incorporate LoRA modules, offer a flexible trade-off between size and performance. Continuous Depth-wise Batching, a novel inference paradigm enabled by the recursive architecture paired with early exiting, is explored, demonstrating the potential for significant (2-3x) gains in throughput. ", "first_cons": "Uptraining on a dataset with different quality or distribution than the original pretraining dataset can hinder performance. Reduced size models show performance lower than the full size model.", "first_pros": "Recursive Transformers, especially with Stepwise initialization, match or exceed the performance of similar-sized non-recursive models. Relaxed Recursive Transformers offer a flexible size-performance trade-off.  Continuous Depth-wise Batching shows promise for significant inference speedup.", "keypoints": ["Recursive models show comparable or better performance than full-size models with fewer parameters.", "Stepwise initialization is most effective for Recursive Transformers.", "Relaxed Recursive Transformers with LoRA modules provide flexible size-performance trade-offs.", "Continuous Depth-wise Batching shows the potential for 2-3x throughput improvement."], "second_cons": "The achievable performance upper bound can be influenced by the quality of the uptraining dataset, which can affect the comparison to the full-size model.", "second_pros": "Knowledge distillation helps Recursive Transformers achieve performance on par with the full-size model.", "summary": "Experiments on three LLMs demonstrate that Recursive Transformers, especially with Stepwise initialization, achieve comparable or superior performance to non-recursive models of similar size while offering significant parameter efficiency and potential for large throughput improvements through Continuous Depth-wise Batching."}}]