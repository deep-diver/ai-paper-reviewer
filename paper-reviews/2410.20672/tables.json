[{"figure_path": "2410.20672/tables/table_6_0.html", "caption": "Table 1 | Key parameters and pretraining details of three models. The sizes of each model refer to the number of embedding parameters (embedding matrices and classifier heads), and all other non-embedding parameters. Gemma and TinyLlama utilize Multi-Query (Shazeer, 2019) and Grouped-Query (Ainslie et al., 2023) attention mechanisms, which leads to a reduced number of key-value heads.", "description": "Table 1 presents key parameters and pretraining details for three large language models: Gemma 2B, TinyLlama 1.1B, and Pythia 1B.", "section": "3.1 Experimental Setup"}, {"figure_path": "2410.20672/tables/table_7_0.html", "caption": "Table 2 | Uptraining the pretrained models on datasets that differ significantly in quality or distribution from their pretraining datasets can lead to decreased performance. We evaluated models after uptraining on the SlimPajama dataset. We measured perplexity on test sets of the SlimPajama, RedPajama, and PG19, and few-shot accuracy on LAMBADA, HellaSwag, PIQA, WinoGrande, ARC-easy, ARC-challenge, and OpenBookQA benchmarks.", "description": "Table 2 presents the perplexity and few-shot accuracy results of three large language models (LLMs) after fine-tuning on the SlimPajama dataset, comparing their performance against different baselines.", "section": "3.2 Non-Recursive Model Baselines"}, {"figure_path": "2410.20672/tables/table_11_0.html", "caption": "Table J.1 | Ablation studies on early-exit training for recursive Gemma models. We evaluated performance in a static-exiting scenario (Bae et al., 2023; Schuster et al., 2022), where all tokens exit at either first or second iteration loops (9th or 18th depths). We explored post-training (after uptraining) and co-training (during uptraining) approaches. Moreover, we explored freezing uptrained weights and adding LoRA with the rank of 128 to the classifier head. Different coefficient values were tested for the aggressive CE loss function. Early-exit training utilized 15 billion tokens, either overlapping with uptraining data or entirely new. Delta (\u25b3) indicates the performance changes of the final loop outputs. We highlight the final configuration: post-training with aggressive CE and KD loss on 15 billion new tokens.", "description": "This table presents the ablation study results on various early-exit training strategies for recursive Gemma models, showing their impact on final and intermediate loop outputs' performance and comparing different training methods, loss functions, and data usage.", "section": "J. Expanded Results of Early-Exit Training"}, {"figure_path": "2410.20672/tables/table_12_0.html", "caption": "Table K.1 | Measurements of generation time across three models using a single A100 40GB GPU. We measured time per token for both a batch size of 1 and the maximum batch size achievable by each model. The prefix length was set to 512 tokens, and the decoded output length to 2048 tokens. We then averaged the total elapsed time by the output length of 2048. Dummy input and dummy tensors were used for measurement.", "description": "This table presents measurements of generation time per token for three different large language models under varying batch sizes and model configurations.", "section": "3.8. Hypothetical Generation Speedup via Continuous Depth-wise Batching"}, {"figure_path": "2410.20672/tables/table_47_0.html", "caption": "Table 1 | Key parameters and pretraining details of three models. The sizes of each model refer to the number of embedding parameters (embedding matrices and classifier heads), and all other non-embedding parameters. Gemma and TinyLlama utilize Multi-Query (Shazeer, 2019) and Grouped-Query (Ainslie et al., 2023) attention mechanisms, which leads to a reduced number of key-value heads.", "description": "Table 1 presents key parameters and pre-training details for three large language models.", "section": "3.1. Experimental Setup"}]