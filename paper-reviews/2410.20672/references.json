{"references": [{" publication_date": "2023", "fullname_first_author": "S. Bae", "paper_title": "Fast and robust early-exiting framework for autoregressive language models with synchronized parallel decoding", "reason": "This paper is highly relevant because it introduces a novel early-exiting framework specifically designed for autoregressive language models.  It is directly applicable to the Recursive Transformer architecture proposed in the main paper, enhancing its efficiency by allowing for faster generation. The synergy between the recursive structure and early exiting is explored in detail, showing how this combination can lead to significant gains in throughput, thus addressing the resource constraints associated with large language models.", "section_number": 2}, {" publication_date": "2019", "fullname_first_author": "M. Dehghani", "paper_title": "Universal transformers", "reason": "This paper introduced the Universal Transformer, a recurrent self-attentive model that demonstrated superior performance to non-recursive counterparts with significantly fewer parameters. This directly relates to the main paper's exploration of parameter sharing in transformers, demonstrating the feasibility and potential benefits of achieving efficient model architectures through cross-layer parameter reuse. The Universal Transformer serves as a key antecedent to the recursive models explored in this work.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "Z. Lan", "paper_title": "ALBERT: A lite BERT for self-supervised learning of language representations", "reason": "This paper presents ALBERT, a lightweight version of BERT, demonstrating that efficient language models can be achieved with fewer parameters while maintaining competitive performance. This is particularly relevant to the main paper because it directly addresses the challenge of high computational costs of LLMs, a key motivation for exploring parameter sharing techniques. ALBERT demonstrates that it's possible to substantially reduce the model size while preserving satisfactory performance, lending further support to the methods in the main paper.", "section_number": 2}, {" publication_date": "2019", "fullname_first_author": "J. Frankle", "paper_title": "The lottery ticket hypothesis: Finding sparse, trainable neural networks", "reason": "This paper introduced the 'lottery ticket hypothesis,' demonstrating that sparse, trainable neural networks can be obtained from larger models via pruning. This directly relates to the main paper's goal of model compression and is particularly relevant for initializing the shared layers in the Recursive Transformers, as it suggests that a smaller, more efficient architecture can retain high performance after appropriate training.", "section_number": 1}, {" publication_date": "2020", "fullname_first_author": "A. Fan", "paper_title": "Reducing transformer depth on demand with structured dropout", "reason": "This paper explores methods for reducing the depth of transformer models without significant performance degradation.  This is directly related to the main paper's exploration of recursive architectures where a single block of layers is repeated multiple times. It suggests that reducing depth can lead to efficient models while potentially retaining high accuracy, a central theme of this paper.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "S. Bae", "paper_title": "Fast and robust early-exiting framework for autoregressive language models with synchronized parallel decoding", "reason": "This paper is directly relevant to the main paper's introduction of Continuous Depth-wise Batching and early exiting. The method is specifically designed for autoregressive language models and aims to significantly improve throughput. Early-exiting has emerged as a crucial technique to improve the efficiency of LLMs, and this paper offers a method for combining it with parallel decoding techniques.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "E. J. Hu", "paper_title": "Lora: Low-rank adaptation of large language models", "reason": "This paper introduces LoRA (Low-Rank Adaptation), a parameter-efficient fine-tuning technique for LLMs, which is directly used in the main paper's Relaxed Recursive Transformers.  LoRA allows for the incorporation of low-rank updates to the weights of existing models, enhancing their performance without significantly increasing their size, thus addressing the critical challenges of efficiency and resource constraints in LLMs.", "section_number": 2}, {" publication_date": "2016", "fullname_first_author": "A. Graves", "paper_title": "Adaptive computation time for recurrent neural networks", "reason": "This paper is highly relevant due to its introduction of Adaptive Computation Time (ACT) for recurrent neural networks, a technique that enables efficient computation by adjusting the computational steps based on the input's complexity.  This concept is directly related to the main paper's proposed Continuous Depth-wise Batching and early exiting, where the number of computational steps is dynamically adjusted based on the confidence of predictions.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "W. Kwon", "paper_title": "Efficient memory management for large language model serving with pagedattention", "reason": "This paper proposes a method for efficient memory management in large language model serving, directly addressing a major challenge in deploying LLMs. The main paper focuses on reducing the memory footprint of LLMs through parameter sharing, and this work offers a complementary approach for optimizing memory usage, therefore offering insights into resource-efficient LLM deployment techniques.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "M. Elbayad", "paper_title": "Depth-adaptive transformer", "reason": "This paper introduced the Depth-Adaptive Transformer which adjusts the depth of the model during inference based on the input's complexity. This is highly relevant to the main paper's proposed method of Continuous Depth-wise Batching and early exiting. Both aim to reduce computation by dynamically adjusting the number of layers or computational steps depending on the input's characteristics.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "S. Bae", "paper_title": "Fast and robust early-exiting framework for autoregressive language models with synchronized parallel decoding", "reason": "This paper is highly relevant due to its focus on improving the efficiency of autoregressive language models through early-exiting techniques. The main paper also explores early-exiting in conjunction with its proposed Continuous Depth-wise Batching approach, which aims to substantially increase throughput and address the challenges of resource constraints of large language models. This paper provides insights into the challenges and effective strategies for implementing early exiting.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "G. Team", "paper_title": "Gemma: Open models based on gemini research and technology", "reason": "This paper introduces the Gemma language model, which is used as one of the primary models in the experiments of the main paper.  Gemma's architecture and performance characteristics are directly relevant to the evaluation and comparison of the proposed Recursive Transformers, allowing for a thorough assessment of the effectiveness of the proposed methods in reducing model size while preserving strong performance. This paper is crucial for understanding the context and baseline performance of the Recursive Transformer evaluation.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "P. Zhang", "paper_title": "Tinyllama: An open-source small language model", "reason": "This paper introduces the TinyLlama language model, a key model used in the experiments of the main paper.  TinyLlama's architecture and performance characteristics are directly relevant for evaluating the effectiveness of the proposed Recursive Transformers and assessing the potential of achieving competitive performance with reduced model sizes.  The comparison with TinyLlama provides valuable insights into the trade-offs between model size and performance.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "S. Biderman", "paper_title": "Pythia: A suite for analyzing large language models across training and scaling", "reason": "This paper introduces the Pythia suite of language models, one of the models employed in the main paper's experimental evaluations.  Pythia's architecture and pretraining details are highly relevant for a robust comparison of the performance of the proposed Recursive Transformers in the context of various pretrained language models.  The use of Pythia helps in establishing a comprehensive benchmark for comparing the efficiency and performance of different model compression techniques.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "D. Zeng", "paper_title": "Learning to skip for language modeling", "reason": "This paper directly addresses the challenges of parameter efficiency and computational cost in large language models by exploring layer skipping techniques. This aligns perfectly with the core concept of parameter sharing explored in the main paper.  Layer skipping is a powerful technique for model compression, and understanding its performance characteristics in the context of LLMs is highly relevant to the main paper's goal.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Z. Zhou", "paper_title": "A survey on efficient inference for large language models", "reason": "This paper offers a comprehensive overview of methods for efficient inference in large language models. This is highly relevant to the main paper's introduction of Continuous Depth-wise Batching, a novel inference paradigm that significantly improves throughput in Recursive Transformers.  The survey provides a broader context for understanding the challenges of efficient LLM inference and highlights the potential for further improvements.", "section_number": 3}, {" publication_date": "2017", "fullname_first_author": "A. Vaswani", "paper_title": "Attention is all you need", "reason": "This paper introduced the Transformer architecture, which is the foundation for most modern large language models.  Understanding the Transformer architecture is crucial for appreciating the main paper's innovation of Recursive Transformers. The paper\u2019s architecture is essential for comprehending the proposed method of parameter sharing and how it affects the efficiency and performance of language models.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "M. Schuster", "paper_title": "Confident adaptive language modeling", "reason": "This paper proposes a novel adaptive language modeling technique that is relevant to the main paper's introduction of early exiting. Adaptive language modeling adjusts the model's behavior during inference, potentially leading to significant speedups.  This is highly relevant to the main paper's proposed method of Continuous Depth-wise Batching and early exiting, which aims to improve throughput by dynamically adjusting the number of computational steps.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "T. Takase", "paper_title": "Lessons on parameter sharing across layers in transformers", "reason": "This paper directly addresses the problem of parameter sharing in transformer models, a core focus of the main paper. The authors explore different parameter sharing strategies, which are highly relevant for understanding the context and challenges of the main paper's proposed methods for creating Recursive Transformers with minimal performance degradation.  The paper provides valuable insights into the various techniques for parameter sharing and their implications for model efficiency.", "section_number": 2}, {" publication_date": "2015", "fullname_first_author": "G. E. Hinton", "paper_title": "Distilling the knowledge in a neural network", "reason": "This paper introduced the concept of knowledge distillation, a technique for compressing large neural networks by training a smaller \"student\" network to mimic the behavior of a larger \"teacher\" network.  Knowledge distillation is employed in the main paper's experiments to improve the performance of the Recursive Transformers.  This paper provides the foundation for the knowledge distillation techniques used in the main paper, offering insights into its effectiveness for achieving comparable performance with smaller model sizes.", "section_number": 3}]}