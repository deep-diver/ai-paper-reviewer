[{"figure_path": "2410.20672/figures/figures_2_0.png", "caption": "Figure 1 | Overview of the conversion from a vanilla N-layer Transformer to a Recursive Transformer with N/K blocks of K shared layers. The Recursive Transformer is obtained by repeating a single block of K layers multiple times, resulting in a looped architecture. The Recursive Transformer can also be converted into a Relaxed Recursive Transformer by adding layer-specific LoRA modules. This preserves many of the advantages of weight sharing, but also allows for better performance.", "description": "The figure illustrates the transformation of a vanilla transformer into a recursive transformer and further into a relaxed recursive transformer by applying parameter sharing and low-rank adaptation.", "section": "2. Effective Model Compression with Recursive Patterns"}, {"figure_path": "2410.20672/figures/figures_4_0.png", "caption": "Figure 2 | Left: An example of unshared, full-size model with 6 layers. Middle: Three proposed methodologies for initializing looped layers in a Recursive Transformer. Each layer number indicates the source layer in the full-size model used for initialization. Right: Example of a Relaxed Recursive Transformer initialized by SVD method. Here, looped layers are initialized using the Average method.", "description": "The figure illustrates three initialization techniques (Stepwise, Average, Lower) for looped layers in Recursive and Relaxed Recursive Transformers, along with an example of Relaxed Recursive Transformer with SVD initialization.", "section": "2.2. Recursive Transformer: Looped Layer Tying"}, {"figure_path": "2410.20672/figures/figures_25_0.png", "caption": "Figure 1 | Overview of the conversion from a vanilla N-layer Transformer to a Recursive Transformer with N/K blocks of K shared layers. The Recursive Transformer is obtained by repeating a single block of K layers multiple times, resulting in a looped architecture. The Recursive Transformer can also be converted into a Relaxed Recursive Transformer by adding layer-specific LoRA modules. This preserves many of the advantages of weight sharing, but also allows for better performance.", "description": "The figure illustrates the conversion of a vanilla transformer into a recursive transformer, showing how parameters are shared across layers and how low-rank adapters (LoRA) can be added to improve performance.", "section": "2. Effective Model Compression with Recursive Patterns"}, {"figure_path": "2410.20672/figures/figures_26_0.png", "caption": "Figure 2 | Left: An example of unshared, full-size model with 6 layers. Middle: Three proposed methodologies for initializing looped layers in a Recursive Transformer. Each layer number indicates the source layer in the full-size model used for initialization. Right: Example of a Relaxed Recursive Transformer initialized by SVD method. Here, looped layers are initialized using the Average method.", "description": "The figure illustrates three proposed initialization methods for looped layers in Recursive and Relaxed Recursive Transformers, using an example of a 6-layer unshared model.", "section": "2.2. Recursive Transformer: Looped Layer Tying"}]