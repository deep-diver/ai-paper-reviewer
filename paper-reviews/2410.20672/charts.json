[{"figure_path": "2410.20672/charts/charts_5_0.png", "caption": "Figure 3 | An illustrative example of a continuous depth-wise batching strategy together with early-exiting. We assume a maximum batch size of 32, three model \u201cstages\u201d (e.g., layer blocks), and a stream of batched inputs that arrive sequentially in time. In (a), all three model stages must complete for the first (non-maximal) batch of 16 before the second batch of 32 examples that arrives next can be started. In (b), however, half of second batch of 32 examples can share computation with the first batch of 16 that is still finishing. Finally, (c) demonstrates a situation where some examples within each batch can early-exit after stage 2; their vacant slots in the batch are then immediately filled.", "description": "The chart illustrates the difference between vanilla batching, depth-wise batching, and depth-wise batching with early exiting in processing batched inputs sequentially.", "section": "2.4. Continuous Depth-wise Batching and Early-Exiting"}, {"figure_path": "2410.20672/charts/charts_8_0.png", "caption": "Figure 4 | Recursive and Relaxed Recursive Transformers achieve comparable performance to full-size models, and significantly outperform reduced-size models. Recursive models were initialized using the Stepwise method, while relaxed models utilized Average and SVD methods for looped layers and LoRA modules. We show the performance of four different rank values: 64, 128, 256, and 512. Recursive and reduced-size models were either uptrained (recursive model) and pretrained from scratch (reduced-size model) on 60 billion tokens using a knowledge distillation objective.", "description": "The chart compares the few-shot accuracy of full-size, reduced-size, recursive, and relaxed recursive transformer models across three different model architectures (Gemma, TinyLlama, and Pythia) with varying model sizes and LoRA ranks.", "section": "3. Main Results"}, {"figure_path": "2410.20672/charts/charts_9_0.png", "caption": "Figure F.1 | Training loss curves of Stepwise and Average initialization variants across three models with two blocks. (a) \u201cFixed-start\u201d indicates that the first layer of the pretrained model is selected initially, and subsequent layers are repeatedly chosen at a fixed interval. \u201cFixed-ends\u201d means that the first and last layers are included, and intermediate layers are selected at specific step intervals. (b) When initializing the weights of normalization layer (RMSNorm in Gemma and TinyLlama, and LayerNorm in Pythia), we consider whether to average the weights (Norm-avg), select a single layer\u2019s weights (Norm-choice), or use zero initialization (Norm-zero).", "description": "The chart displays the training loss curves for different initialization methods (Stepwise, Average, Lower, and Random) across three language models (Gemma, TinyLlama, and Pythia) and shows how the Stepwise method consistently outperforms other methods.", "section": "F. Expanded Results of Initialization Methods for Looped Layers"}, {"figure_path": "2410.20672/charts/charts_10_0.png", "caption": "Figure G.2 | Comparison of average few-shot accuracy between zero and SVD initialization methods across three models. Performance gains due to LoRA relaxation are indicated by hatched bars, while cases where performance is lower than the recursive counterpart (without LoRA modules) are represented by dotted lines.", "description": "The chart compares the average few-shot accuracy of three models (Gemma, TinyLlama, and Pythia) using different initialization methods (Stepwise, Average, and Lower) for LoRA modules, showing the impact of SVD initialization and LoRA relaxation on model performance.", "section": "G. Expanded Results of Relaxed Recursive Transformers"}, {"figure_path": "2410.20672/charts/charts_11_0.png", "caption": "Figure 4 | Recursive and Relaxed Recursive Transformers achieve comparable performance to full-size models, and significantly outperform reduced-size models. Recursive models were initialized using the Stepwise method, while relaxed models utilized Average and SVD methods for looped layers and LoRA modules. We show the performance of four different rank values: 64, 128, 256, and 512. Recursive and reduced-size models were either uptrained (recursive model) and pretrained from scratch (reduced-size model) on 60 billion tokens using a knowledge distillation objective.", "description": "The chart compares the few-shot accuracy of recursive and relaxed recursive transformer models to full-size and reduced-size models across different model sizes and LoRA rank values.", "section": "3. Main Results"}, {"figure_path": "2410.20672/charts/charts_12_0.png", "caption": "Figure 4 | Recursive and Relaxed Recursive Transformers achieve comparable performance to full-size models, and significantly outperform reduced-size models. Recursive models were initialized using the Stepwise method, while relaxed models utilized Average and SVD methods for looped layers and LoRA modules. We show the performance of four different rank values: 64, 128, 256, and 512. Recursive and reduced-size models were either uptrained (recursive model) and pretrained from scratch (reduced-size model) on 60 billion tokens using a knowledge distillation objective.", "description": "The chart compares the few-shot accuracy of recursive and relaxed recursive transformer models against full-size and reduced-size models across different sizes, initialization methods, and LoRA ranks.", "section": "3. Main Results"}, {"figure_path": "2410.20672/charts/charts_29_0.png", "caption": "Figure F.1 | Training loss curves of Stepwise and Average initialization variants across three models with two blocks. (a) \u201cFixed-start\u201d indicates that the first layer of the pretrained model is selected initially, and subsequent layers are repeatedly chosen at a fixed interval. \u201cFixed-ends\u201d means that the first and last layers are included, and intermediate layers are selected at specific step intervals. (b) When initializing the weights of normalization layer (RMSNorm in Gemma and TinyLlama, and LayerNorm in Pythia), we consider whether to average the weights (Norm-avg), select a single layer\u2019s weights (Norm-choice), or use zero initialization (Norm-zero).", "description": "The chart displays training loss curves for different initialization methods (Stepwise and Average) across three language models, showing the impact of various techniques on model training.", "section": "F. Expanded Results of Initialization Methods for Looped Layers"}, {"figure_path": "2410.20672/charts/charts_29_1.png", "caption": "Figure F.1 | Training loss curves of Stepwise and Average initialization variants across three models with two blocks. (a) \u201cFixed-start\u201d indicates that the first layer of the pretrained model is selected initially, and subsequent layers are repeatedly chosen at a fixed interval. \u201cFixed-ends\u201d means that the first and last layers are included, and intermediate layers are selected at specific step intervals. (b) When initializing the weights of normalization layer (RMSNorm in Gemma and TinyLlama, and LayerNorm in Pythia), we consider whether to average the weights (Norm-avg), select a single layer\u2019s weights (Norm-choice), or use zero initialization (Norm-zero).", "description": "The chart displays the training loss curves for different initialization methods (Stepwise and Average) across three models with different numbers of blocks, showing the impact of various initialization strategies on training loss.", "section": "F. Expanded Results of Initialization Methods for Looped Layers"}, {"figure_path": "2410.20672/charts/charts_31_0.png", "caption": "Figure F.3 | Few-shot performance on seven benchmarks and their average accuracy based on four looping initialization methods. Full-size model performance is represented by a gray dotted line.", "description": "The chart compares the few-shot performance of recursive transformers initialized with different methods (Stepwise, Average, Lower, Random) across seven benchmarks, showing the Stepwise method consistently outperforms others.", "section": "F. Expanded Results of Initialization Methods for Looped Layers"}, {"figure_path": "2410.20672/charts/charts_32_0.png", "caption": "Figure G.1 | Comparison of training loss for recursive and relaxed recursive models with two blocks. The LoRA rank is set to 512, and the SVD initialization method is used for LoRA modules.", "description": "The chart compares the training loss curves across three different models (Gemma, TinyLlama, and Pythia) for recursive and relaxed recursive approaches, highlighting the impact of LoRA modules and SVD initialization on model training.", "section": "G. Expanded Results of Relaxed Recursive Transformers"}, {"figure_path": "2410.20672/charts/charts_32_1.png", "caption": "Figure G.2 | Comparison of average few-shot accuracy between zero and SVD initialization methods across three models. Performance gains due to LoRA relaxation are indicated by hatched bars, while cases where performance is lower than the recursive counterpart (without LoRA modules) are represented by dotted lines.", "description": "The chart compares the average few-shot accuracy of three models (Gemma, TinyLlama, and Pythia) using different initialization methods (zero, SVD) for LoRA modules, showing the impact of LoRA relaxation on model performance.", "section": "G. Expanded Results of Relaxed Recursive Transformers"}, {"figure_path": "2410.20672/charts/charts_33_0.png", "caption": "Figure F.3 | Few-shot performance on seven benchmarks and their average accuracy based on four looping initialization methods. Full-size model performance is represented by a gray dotted line.", "description": "The chart displays a comparison of few-shot performance across seven benchmarks for four different looping initialization methods of recursive transformers, with a dotted line representing the performance of full-size models.", "section": "F. Expanded Results of Initialization Methods for Looped Layers"}, {"figure_path": "2410.20672/charts/charts_37_0.png", "caption": "Figure 4 | Recursive and Relaxed Recursive Transformers achieve comparable performance to full-size models, and significantly outperform reduced-size models. Recursive models were initialized using the Stepwise method, while relaxed models utilized Average and SVD methods for looped layers and LoRA modules. We show the performance of four different rank values: 64, 128, 256, and 512. Recursive and reduced-size models were either uptrained (recursive model) and pretrained from scratch (reduced-size model) on 60 billion tokens using a knowledge distillation objective.", "description": "The chart compares the few-shot accuracy of recursive and relaxed recursive transformers to full-size and reduced-size models across different model sizes and LoRA ranks.", "section": "3. Main Results"}]