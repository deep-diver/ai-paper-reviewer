[{"figure_path": "https://arxiv.org/html/2411.18197/x2.png", "caption": "Figure 1: Given a 3D character represented by mesh or 3D Gaussian Splats with arbitrary pose and shape, our framework can produce high-quality results of rigging, skinning, and pose resetting for it within one second. The output 3D model is fully animatable with a fine-grained skeleton and optional bone topology of extra body structures.", "description": "This figure shows the input and output of the proposed Make-It-Animatable framework.  Given a 3D character model (either a mesh or a point cloud represented by 3D Gaussian Splats), regardless of its pose or shape, the system automatically generates a rig, performs skinning, and resets the pose to a neutral position, all within one second. The resulting model has a detailed skeleton and allows for animation. The framework can even handle characters with extra body parts (like ears or tails) not typically found in standard humanoid models.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2411.18197/x3.png", "caption": "Figure 2: \nPipeline of the proposed framework.\nGiven an input 3D character, we produce high-quality blend weights, bones, and pose-to-rest transformation for it, so that any animation is within easy reach.\nFirst, we coarsely localize the joints with a pre-trained lite version of this framework, which enables a finer shape representation.\nThen the shape is encoded into a neural field with a particle-based autoencoder. The decoding process involves spatial and learnable queries for different animation assets.\nFinally, the structure-aware modeling of bones is proposed to better align the predictions with skeleton topology priors.", "description": "This figure illustrates the pipeline of the Make-It-Animatable framework, which efficiently generates animation-ready 3D characters from various input formats.  The process begins with coarse joint localization using a pre-trained model, refining the shape representation.  A particle-based autoencoder then encodes the shape into a neural field.  Spatial and learnable queries decode this field to generate blend weights and bone structures.  Finally, a structure-aware model refines bone predictions to better match skeletal topology, resulting in a fully animatable 3D character.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2411.18197/x4.png", "caption": "Figure 3: \nPipeline of the proposed structure-aware transformer.\nThe per-bone shape-aware embedding is first added with its parent bone\u2019s latent, which is encoded from the autoregressive outputs (in inference) or the ground-truth values (in training). The summation is then fused with the ancestral bones\u2019 features via the masked causal attention. Eventually, bone attributes are decoded from the output shape- and structure-aware embeddings.\nIn inference, the whole process follows the paradigm of next-child-bone prediction.", "description": "This figure illustrates the architecture of the structure-aware transformer, a key component of the Make-It-Animatable framework.  The transformer processes per-bone shape embeddings, combining them with latent information from parent bones (using autoregressive outputs during inference or ground truth values during training). This combined information, along with features from ancestral bones, is refined through masked causal attention. Finally, the transformer decodes the enriched shape and structure embeddings to generate bone attributes (joint positions and pose transformations). The inference process is sequential, predicting the attributes of child bones based on the already-predicted attributes of their parent bones.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2411.18197/x5.png", "caption": "Figure 4: \nComparison with Meshy\u00a0[meshy] and Tripo\u00a0[tripo].\nWe feed them the same image as reference and compare the performance based on their generated 3D models respectively.\nThe blend weights of two joints, i.e., Left Shoulder and Right Leg, are visualized.\nGiven that these baselines can only apply preset motions and their rest-pose models cannot be exported, we apply a similar \u201crunning\u201d sequence to all the methods for fair comparison.\nThe T-pose models predicted by our method are also included as the front-view animating results.", "description": "This figure compares the results of the proposed method with two existing methods (Meshy and Tripo) for generating and rigging 3D character models from a reference image.  The comparison focuses on the quality of blend weights (visualized for the left shoulder and right leg joints), the ability to handle arbitrary poses (a 'running' animation sequence was applied to all models for consistent comparison), and the ability to export rest-pose models (Meshy and Tripo lack this functionality). The figure includes the results from the proposed method, showing both the 'running' sequence results and also the model in a T-pose (rest pose) for better visualization of the generated model.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2411.18197/x6.png", "caption": "Figure 5: \nComparison with RigNet\u00a0[xu2020rignet].\nWe visualize the blend weights of selected joints and manually deform them to assess the impact of rigging quality on skinning results.", "description": "This figure compares the results of the proposed method with RigNet, a data-driven automatic rigging method.  It visualizes the blend weights of selected joints for several characters.  By manually deforming these characters, the impact of rigging quality (accuracy of bone placement and assignment of weights) on the resulting skinning (deformation of the mesh) is assessed.  The comparison helps demonstrate the superior quality and accuracy of the proposed method's rigging compared to RigNet.", "section": "4.2 Comparison Results"}, {"figure_path": "https://arxiv.org/html/2411.18197/x7.png", "caption": "Figure 6: \nComparison with TADA\u00a0[liao2024tada] and HumanGaussian\u00a0[liu2024humangaussian] (HG).\nWe use the generated meshes from TADA and 3D Gaussians from HG for comparison.\nNote that the skeletons of these two baselines are identical to the shape-specific SMPL\u00a0[loper2015smpl] templates (without bone tail), with their weights interpolated from the template meshes.\nZoom in to better view the details.", "description": "Figure 6 compares the proposed method with two template-based baselines: TADA, which generates mesh-based human models, and HumanGaussian, which uses Gaussian splats.  It highlights that while both baselines use SMPL templates (without bone tails) and interpolate weights from these templates, the proposed method achieves comparable or superior results without relying on a pre-defined template. The figure shows examples of 3D characters, their skeletons, blend weights, and animation results from each method to visually demonstrate the quality difference. Zooming in is recommended to appreciate the detail.", "section": "4.2 Comparison Results"}, {"figure_path": "https://arxiv.org/html/2411.18197/x8.png", "caption": "Figure 7: \nResults of more cases to demonstrate the advantage of our method.\nThe detailed explanations can be found in the supplementary material.", "description": "Figure 7 showcases various examples illustrating the effectiveness of the proposed method in handling diverse 3D character models.  These examples highlight the method's ability to produce high-quality rigging and skinning results across different body shapes, poses, and skeletal structures.  Specific examples include fine-grained finger control, handling of unconventional shapes, management of complex poses, efficient processing of high-polygon models, successful handling of asymmetric shapes, adaptation to models missing bones, and inclusion of extra bones such as ears and tails.  Detailed analysis of each example is available in the supplementary material.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2411.18197/x9.png", "caption": "Figure 8: \nVisualizations of some ablative experiments.\nWe show the effectiveness of the proposed modules and design choices by visualizing the predicted bones, blend weights, and pose transformations.", "description": "Figure 8 presents an ablation study to demonstrate the impact of different modules and design choices within the proposed framework.  By visualizing the predicted bones, blend weights, and pose transformations for several variations, the study highlights how each component contributes to the overall accuracy and effectiveness of the animation-ready 3D character generation.", "section": "4.3 Ablation Study"}, {"figure_path": "https://arxiv.org/html/2411.18197/x10.png", "caption": "Figure S1: \nThe coarse (upper) and fine (lower) stages of training our framework.\nIn the coarse stage, the input shape is uniformly sampled and only the bone positions are predicted. We apply data augmentation to the inputs via random 3D rotations, so that the coarse model is generalizable to global transformations of in-the-wild cases with an acceptable accuracy.\nIn the fine stage, we apply canonical transformation and hierarchical sampling to the shapes in advance based on the ground-truth bone positions.\nThen during inference, a 3D character is fed into the coarse framework to get its bone positions, which guide the establishment of coarse-to-fine shape representation later in the fine framework.\nNote that the body prior loss (Sec.\u00a03.4) is directly applied to the bone positions. As for pose prediction, we take the ground-truth bones as a proxy and use the predicted pose to transform them, thereby indirectly affecting the pose optimization.", "description": "This figure illustrates the two-stage training process of the proposed framework. The upper part shows the coarse stage, where the input 3D shape is uniformly sampled, and only bone positions are predicted using data augmentation (random 3D rotations). This aims to make the coarse model robust to various global transformations. The lower part illustrates the fine stage.  Here, canonical transformation and hierarchical sampling are applied to refine the shape representation based on the ground-truth bone positions obtained in the coarse stage. During inference, a 3D character is first processed by the coarse framework to predict bone positions; these positions then guide the creation of the refined shape representation in the fine framework. The body prior loss is directly applied to bone positions in both stages, and the ground-truth bones are used as a proxy for pose prediction to indirectly optimize the pose.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2411.18197/x11.png", "caption": "Figure S2: \nSome samples from the collected Mixamo\u00a0[mixamo] dataset.\nThe dataset contains bipedal humanoids with different shapes, ranging from realistic humans to cartoon or fantasy creatures. Each character is preprocessed to be animatable by any of the motion sequences.\nThe proposed framework is trained on this dataset.", "description": "This figure displays a subset of the 3D character models used to train the Make-It-Animatable framework.  The Mixamo dataset contains a variety of bipedal humanoid characters, demonstrating diverse body shapes and styles, ranging from realistic human figures to stylized cartoon and fantasy characters. Each model in the dataset has been pre-processed to ensure compatibility with the animation sequences used for training. This preprocessing step makes each character readily animatable.", "section": "S3.1. Mixamo Dataset"}, {"figure_path": "https://arxiv.org/html/2411.18197/x12.png", "caption": "Figure S3: \nSome samples of the anime characters with additional accessories for the extra-bone fine-tuning.\nThese characters are all manually created using Vroid Studio\u00a0[vroid] and then preprocessed to be compatible with the standard skeleton definition of Mixamo\u00a0[mixamo].", "description": "Figure S3 shows examples of anime-style 3D characters created using Vroid Studio and modified to be compatible with the Mixamo standard skeleton. These models include additional accessories such as ears and tails, allowing for the training and evaluation of the proposed method's ability to handle characters with non-standard bone structures (extra bones).  The modifications to fit the Mixamo skeleton demonstrate a practical application of the framework's adaptability and robustness.", "section": "S3.2 Vroid Dataset"}, {"figure_path": "https://arxiv.org/html/2411.18197/x13.png", "caption": "Figure S4: \nAdditional Comparison with generative 3D methods, i.e., Meshy\u00a0[meshy] and Tripo\u00a0[tripo].\nWe feed them the same image as reference and compare the performance based on their generated 3D models respectively.\nThe blend weights of two joints, i.e., Left Shoulder and Right Leg, are visualized.\nThe T-pose models predicted by our method are included as the front-view animating results.\nZoom in to better view the details.", "description": "This figure compares the proposed method with two other generative 3D modeling methods, Meshy and Tripo.  All three methods were given the same reference image as input. The resulting 3D models from each method are shown, with a focus on the blend weights assigned to the left shoulder and right leg joints. This helps illustrate the differences in the quality of the rigging and skinning generated by each method. The figure also includes a visualization of the T-pose (a standard pose used for animation) generated by the proposed method for a front-view comparison.", "section": "4.2 Comparison Results"}, {"figure_path": "https://arxiv.org/html/2411.18197/x14.png", "caption": "Figure S5: \nAdditional comparison with template-based avatar generation methods, i.e., TADA\u00a0[liao2024tada] and HumanGaussian\u00a0[liu2024humangaussian] (HG).\nWe use the generated meshes from TADA and 3D Gaussians from HG for comparison.\nNote that the skeletons of these two baselines are identical to the shape-specific SMPL\u00a0[loper2015smpl] templates (without bone tail), with their weights interpolated from the template meshes.\nZoom in to better view the details.", "description": "Figure S5 presents a comparison of the proposed method with two template-based avatar generation methods: TADA and HumanGaussian.  The comparison uses 3D models generated by TADA (meshes) and HumanGaussian (3D Gaussian splats). Importantly, both TADA and HumanGaussian rely on the SMPL template for their skeletons (excluding the bone tails), and their weights are interpolated from the template meshes. The figure visually compares the results of the three methods, highlighting differences in the generated skeletons, blend weights, and animation results.  Zooming in is recommended to see the details clearly.", "section": "4.2 Comparison Results"}, {"figure_path": "https://arxiv.org/html/2411.18197/x15.png", "caption": "Figure S6: \nAdditional comparison with auto-rigging methods, i.e., RigNet\u00a0[xu2020rignet], TARig\u00a0[ma2023tarig], and Neural Blend Shapes\u00a0[li2021learning] (Neural BS).\nWe visualize the blend weights of selected joints and manually deform them to assess the impact of rigging quality on skinning results.\n*: Neural Blend Shapes only support T-pose inputs, so for the non-rest cases (lower two), we feed it the T-pose meshes transformed by our pose-to-rest predictions.", "description": "Figure S6 presents a comparison of the proposed method's rigging and skinning capabilities against three existing auto-rigging techniques: RigNet, TARig, and Neural Blend Shapes.  The figure visually demonstrates the quality of blend weights generated by each method for selected joints. To assess the impact of rigging quality on the resulting skinning, the authors manually deformed the meshes.  Importantly, Neural Blend Shapes only accepts T-pose (a standardized pose) input.  To incorporate non-T-pose examples fairly, T-pose versions of the non-T-pose models were created using the authors' pose-to-rest transformations prior to input into Neural Blend Shapes for a consistent evaluation.", "section": "S4. More Results"}, {"figure_path": "https://arxiv.org/html/2411.18197/x16.png", "caption": "Figure S7: \nComparison with commercial auto-rigging software, i.e., Mixamo\u00a0[mixamo] and Anything World\u00a0[anythingworld].\nNote that these two tools can only deal with simple input poses (T- or A-pose is recommended) and often raise errors when faced with complex ones.", "description": "This figure compares the results of using the proposed method against two commercial auto-rigging software packages, Mixamo and Anything World.  The comparison highlights the limitations of Mixamo and Anything World, which perform optimally only with simple input poses like the T-pose or A-pose. When presented with more complex poses, these software packages frequently produce errors.  The figure demonstrates the superior performance and robustness of the proposed method in handling diverse and complex poses.", "section": "S4. More Results"}, {"figure_path": "https://arxiv.org/html/2411.18197/x17.png", "caption": "Figure S8: \nQualitative comparison with RigNet\u00a0[xu2020rignet] and TARig\u00a0[ma2023tarig] on cases from the test split of \u201cModelsResource-RigNetv1\u201d dataset\u00a0[xu2019predicting].\nWhile both baselines are exactly trained on this dataset and ours are not, we still achieve the best performance.", "description": "This figure compares the performance of the proposed method against two state-of-the-art auto-rigging methods, RigNet and TARig, on a subset of the ModelsResource-RigNetv1 dataset.  The key takeaway is that despite RigNet and TARig being trained specifically on this dataset, while the proposed method was not, it still achieves superior performance in terms of rigging and skinning quality.", "section": "S4. More Results"}, {"figure_path": "https://arxiv.org/html/2411.18197/x18.png", "caption": "Figure S9: \nResults of more cases to demonstrate the advantage of our method.\n(a) Fine-grained control of fingers; (b) Capacity of abnormal shapes; (c) Complex input poses; (d) Efficiency for high polygon models; (e) Support of asymmetric inputs; (f) Adaptation to non-existing bones; (g) & (h): Extension to extra bones (e.g., long ears and tails).", "description": "This figure showcases additional examples to highlight the strengths of the proposed method.  Panel (a) demonstrates the fine-grained control achieved over finger articulation. Panel (b) shows the method's ability to handle characters with unusual proportions or shapes (abnormal shapes).  Panel (c) illustrates successful rigging and skinning of characters in complex or challenging poses. Panel (d) exhibits the efficiency of the method even when processing high-polygon models. Panel (e) presents successful results for characters with asymmetrical features. Panel (f) shows how the method adapts when dealing with missing body parts (non-existing bones). Finally, panels (g) and (h) demonstrate the extensibility of the method to incorporate extra bones, such as long ears or tails, illustrating the capacity for creating diverse and detailed character models.", "section": "4.2 Additional Visualizations"}, {"figure_path": "https://arxiv.org/html/2411.18197/x19.png", "caption": "Figure S10: \nQualitative analysis of our geometry-aware attention module and its injecting method.\nThe proposed attention-based injection can benefit from normal information without any side effects.", "description": "This figure demonstrates the effectiveness and lack of side effects of the geometry-aware attention module, a key part of the proposed framework.  The module incorporates normal information to improve the quality of shape representations and blend weight predictions.  The results show that using attention-based injection of normal information leads to improved results without any negative consequences.", "section": "S4. More Results"}, {"figure_path": "https://arxiv.org/html/2411.18197/x20.png", "caption": "Figure S11: \nVisualization of the attention score of our geometry-aware attention module.\nThese per-sampled-point values are extracted from the first attention head (out of 8 heads in total).\nThe brighter color (yellower) indicates more attention to normals rather than coordinates.\nWe also use green bounding boxes to label some clusters where high-attention-score points are densely distributed.\nIt can be observed that the module adaptively learns to rely more on normals in regions like the inner thigh since coordinates become less discriminative there.", "description": "This figure visualizes the attention weights learned by the geometry-aware attention module within the shape autoencoder.  The visualization shows the attention scores for each sampled point, with brighter colors (yellow) representing higher attention to surface normals.  Areas where high attention scores are clustered are highlighted with green bounding boxes. The visualization demonstrates that the model learns to prioritize surface normal information over coordinate information in ambiguous regions such as the inner thighs where coordinate-based information is less discriminative.", "section": "3.2. Particle-based Shape Autoencoder"}]