[{"figure_path": "2410.16215/charts/charts_1_0.png", "caption": "Figure 1: Results of the pre-trained 1.9B, 3.8B, and 6.8B student LLMs, using only LM loss, vanilla PD configuration (\u00a7 3.1), and a better PD configuration (PD*) after our exploration. Details are placed in appendix A.6.", "description": "The chart displays the accuracy of pre-trained language models (LLMs) with varying sizes (1.9B, 3.8B, and 6.8B parameters) using different pre-training distillation configurations (LM loss, vanilla PD, and optimized PD*).", "section": "1 Introduction"}, {"figure_path": "2410.16215/charts/charts_4_0.png", "caption": "Figure 2: Relative improvements compared to LLM-LM using different p in top-p-100 logits truncation and logits sizes per token with different p. The sizes are estimated using 10 million tokens.", "description": "The chart shows the relative improvements and logit sizes per token when using different values of p in top-p-100 logits truncation.", "section": "3.2 Design Dimension #1: Logits Processing"}, {"figure_path": "2410.16215/charts/charts_4_1.png", "caption": "Figure 3: Relative improvements compared to LLM-LM using different k in top-0.95-k logits truncation and logits sizes per token with different k.", "description": "The chart illustrates the relationship between the number of logits (using top-0.95-k and top-1.0-k truncation methods) and the relative improvement in performance compared to a baseline LLM, across different values of k.", "section": "3.2 Design Dimension #1: Logits Processing"}, {"figure_path": "2410.16215/charts/charts_6_0.png", "caption": "Figure 4: Relative improvements compared to LLM-LM using varying sizes of student and teacher LLMs.", "description": "The chart displays the relative improvement in performance of student LLMs of different sizes when trained using pre-training distillation with GLM-4-9B and GLM-4-32B as teacher models.", "section": "3.4 Design Dimension #3: Scaling Law"}, {"figure_path": "2410.16215/charts/charts_7_0.png", "caption": "Figure 5: Experimental results of the checkpoints saved every 10,000 step (about 83B tokens) during the pre-training of 1.9B and 3.8B LLMs on 500B tokens. The last data point is from the checkpoint saved at the end.", "description": "The chart displays the accuracy of 1.9B and 3.8B LLMs pre-trained with and without knowledge distillation (KD) across varying numbers of consumed tokens during the pre-training phase.", "section": "3.4 Design Dimension #3: Scaling Law"}]