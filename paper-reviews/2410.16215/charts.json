[{"figure_path": "2410.16215/charts/charts_1_0.png", "caption": "Figure 1: Results of the pre-trained 1.9B, 3.8B, and 6.8B student LLMs, using only LM loss, vanilla PD configuration (\u00a7 3.1), and a better PD configuration (PD*) after our exploration. Details are placed in appendix A.6.", "description": "The chart displays the accuracy of three different pre-trained LLMs (1.9B, 3.8B, and 6.8B parameters) using LM loss, vanilla pre-training distillation, and an improved pre-training distillation configuration.", "section": "1 Introduction"}, {"figure_path": "2410.16215/charts/charts_4_0.png", "caption": "Figure 2: Relative improvements compared to LLM-LM using different p in top-p-100 logits truncation and logits sizes per token with different p. The sizes are estimated using 10 million tokens.", "description": "The chart shows the relative improvements in performance compared to a baseline model (LLM-LM) when using different top-p values in the top-p-100 logits truncation method, along with the number of logits per token for both top-p-100 and top-p-inf methods.", "section": "3.2 Design Dimension #1: Logits Processing"}, {"figure_path": "2410.16215/charts/charts_4_1.png", "caption": "Figure 3: Relative improvements compared to LLM-LM using different k in top-0.95-k logits truncation and logits sizes per token with different k.", "description": "The chart displays the relative improvements in LLM performance and the logit size per token using different values of k in top-0.95-k logits truncation.", "section": "3.2 Design Dimension #1: Logits Processing"}, {"figure_path": "2410.16215/charts/charts_6_0.png", "caption": "Figure 4: Relative improvements compared to LLM-LM using varying sizes of student and teacher LLMs.", "description": "The chart displays the relative improvements of distilled LLMs compared to baseline LLMs trained using only LM loss with varying sizes of student and teacher LLMs.", "section": "3.4 Design Dimension #3: Scaling Law"}, {"figure_path": "2410.16215/charts/charts_7_0.png", "caption": "Figure 5: Experimental results of the checkpoints saved every 10,000 step (about 83B tokens) during the pre-training of 1.9B and 3.8B LLMs on 500B tokens. The last data point is from the checkpoint saved at the end.", "description": "The chart shows the accuracy of 1.9B and 3.8B LLMs pre-trained with and without knowledge distillation (KD) across varying amounts of training tokens.", "section": "3.4 Design Dimension #3: Scaling Law"}]