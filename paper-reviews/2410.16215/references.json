{"references": [{" publication_date": "2015", "fullname_first_author": "Geoffrey Hinton", "paper_title": "Distilling the knowledge in a neural network", "reason": "This paper is foundational for knowledge distillation (KD), introducing the core concept and techniques.  Its impact across various machine learning fields, including the subsequent application to large language models, makes it a highly important foundational paper for this work.", "section_number": 1}, {" publication_date": "2021", "fullname_first_author": "Jianping Gou", "paper_title": "Knowledge distillation: A survey", "reason": "This comprehensive survey provides a broad overview of knowledge distillation techniques and their applications across different domains, including computer vision and natural language processing. It is valuable for understanding the context and state-of-the-art of KD before focusing on its application in LLMs.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Rohan Taori", "paper_title": "Alpaca: A strong, replicable instruction-following model", "reason": "This paper showcases a practical and impactful application of post-training knowledge distillation in LLMs, demonstrating the efficacy of the approach and inspiring the exploration of similar methods during pre-training. The Alpaca model serves as a significant milestone in open-source LLM development.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Ganqu Cui", "paper_title": "Ultrafeedback: Boosting language models with scaled ai feedback", "reason": "This paper is a recent example of successful post-training knowledge distillation in the field of LLMs. Its success and the methodology highlight the current state of the art, which motivates exploring similar benefits during pre-training.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Collin Burns", "paper_title": "Weak-to-strong generalization: Eliciting strong capabilities with weak supervision", "reason": "This recent paper explores the generalization capabilities of large language models, a field closely related to the enhancement of LLMs through pre-training distillation. It provides valuable context for evaluating the effectiveness of the proposed method.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Team GLM", "paper_title": "Chatglm: A family of large language models from glm-130b to glm-4 all tools", "reason": "This paper details the development of a family of large language models (GLM) which are used in several experiments in this study, forming a core part of the empirical evaluation and results reported. It is therefore crucial to understand the base models.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Zhihao Chi", "paper_title": "Normkd: Normalized logits for knowledge distillation", "reason": "This paper introduces a novel method for knowledge distillation, which is directly used in the experimental setup for adaptive temperature calculations. The methodology and results are relevant to the study's focus on logits processing and loss functions.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Ke Tan", "paper_title": "Towards model compression for deep learning based speech enhancement", "reason": "This paper explores model compression techniques in the context of speech enhancement using deep learning.  It's relevant to the broader goal of creating more compact and efficient language models, which is a motivation behind the paper's use of knowledge distillation.", "section_number": 1}, {" publication_date": "2019", "fullname_first_author": "Victor Sanh", "paper_title": "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter", "reason": "This paper showcases a successful application of knowledge distillation to BERT, reducing its size while preserving performance.  It provides a relevant example of efficient knowledge transfer which is relevant to the goal of reducing the size of student LLMs.", "section_number": 4}, {" publication_date": "2020", "fullname_first_author": "Xiaoqi Jiao", "paper_title": "Tinybert: Distilling bert for natural language understanding", "reason": "This paper demonstrates the effective application of knowledge distillation to create a smaller and faster version of BERT, relevant to the paper's focus on creating efficient and compact LLMs.  The use of BERT, a prominent language model, adds significance.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Yuxian Gu", "paper_title": "Minillm: Knowledge distillation of large language models", "reason": "This paper is directly relevant to the current research on pre-training distillation for LLMs. It provides a recent and relevant example focusing specifically on LLMs, advancing the exploration of this method and offering comparable results.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Tom Gunter", "paper_title": "Apple intelligence foundation language models", "reason": "This paper is highly relevant as it describes the development of foundation language models and directly relates to the study's exploration of pre-training distillation in large language models. The models are also used in the experimental setup.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Saurav Muralidharan", "paper_title": "Compact language models via pruning and knowledge distillation", "reason": "This paper directly addresses the topic of LLMs and knowledge distillation, focusing on creating compact models through pruning and distillation. It offers valuable insights on techniques for improving the efficiency of LLMs.", "section_number": 4}, {" publication_date": "2019", "fullname_first_author": "Ari Holtzman", "paper_title": "The curious case of neural text degeneration", "reason": "This paper is relevant to logits processing and truncation. The authors introduce the concept of top-p truncation, a key technique used to reduce the memory overhead of handling logits during the experiments. The theoretical background is relevant for implementing the approach.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Justin Kiefel", "paper_title": "Lokilm: Technical report", "reason": "This paper explores the use of pre-training distillation in LLMs, making it highly relevant to this study. It addresses the challenges and potential benefits of improving model efficiency during pre-training and provides comparable findings.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Sharath Turuvekere Sreenivas", "paper_title": "Llm pruning and distillation in practice: The minitron approach", "reason": "This paper provides practical insights and methods for optimizing the training of compact and efficient LLMs, using pruning and distillation techniques which is directly relevant to the focus on efficient LLM development.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Shengding Hu", "paper_title": "Minicpm: Unveiling the potential of small language models with scalable training strategies", "reason": "This paper explores scaling strategies for training small language models, which is closely related to this research.  The authors investigate the efficient training of smaller models using novel strategies, providing context and suggesting possible future directions.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Qwen Team", "paper_title": "Qwen2.5: A party of foundation models", "reason": "This paper describes the recent advancement in foundational language models which are very relevant to the study's discussion of scaling law and the performance of large language models. The results and findings provide insight into the trends in LLMs and the potential for further improvement using pre-training distillation.", "section_number": 3}, {" publication_date": "2017", "fullname_first_author": "Junho Yim", "paper_title": "A gift from knowledge distillation: Fast optimization, network minimization and transfer learning", "reason": "This paper provides insights into the benefits of knowledge distillation beyond model compression, specifically relating to faster optimization and transfer learning. The techniques and findings are relevant for the broader context of improving LLM training efficiency.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Abhimanyu Dubey", "paper_title": "The llama 3 herd of models", "reason": "This paper discusses a recent advancement in large language models, providing context for the study\u2019s work on improving LLMs through pre-training distillation. The results of the development highlight current trends in LLMs and how pre-training distillation can contribute to the advancement of these models. ", "section_number": 4}]}