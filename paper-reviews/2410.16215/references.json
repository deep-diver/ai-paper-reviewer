{"references": [{" publication_date": "2015", "fullname_first_author": "Geoffrey Hinton", "paper_title": "Distilling the knowledge in a neural network", "reason": "This paper is foundational to the field of knowledge distillation (KD), introducing the core concept and methodology.  It's the seminal work that inspired much of the subsequent research in KD, including the current paper's exploration of pre-training distillation for large language models. The core ideas and techniques presented here have had a profound impact on various machine learning applications and remain highly relevant.", "section_number": 1}, {" publication_date": "2021", "fullname_first_author": "Jianping Gou", "paper_title": "Knowledge distillation: A survey", "reason": "This is a comprehensive survey paper that provides a detailed overview of the history, techniques, and applications of knowledge distillation (KD) across various domains.  It lays the groundwork for understanding KD's potential and limitations, which is crucial for the current paper's focus on pre-training distillation (PD) for LLMs.  As a survey paper, it is important in providing context and relevant background information about this key technique.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Rohan Taori", "paper_title": "Alpaca: A strong, replicable instruction-following model", "reason": "This paper represents a significant advancement in the application of knowledge distillation to LLMs, demonstrating the effectiveness of post-training distillation using a practical approach.  Its success showcases the power of KD in creating high-quality LLMs and makes the idea of extending KD to the pre-training stage highly relevant, as explored in the current research.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Team GLM", "paper_title": "Chatglm: A family of large language models from glm-130b to glm-4 all tools", "reason": "This paper introduces a series of large language models (LLMs), including GLM-4-9B which serves as a teacher model in the current study's experiments.  Its high performance establishes a strong baseline for comparison and provides the foundation for the experiments with pre-training distillation.", "section_number": 3}, {" publication_date": "2019", "fullname_first_author": "Victor Sanh", "paper_title": "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter", "reason": "This work demonstrates the successful application of knowledge distillation to a popular language model (BERT), resulting in a more efficient and smaller model.  It showcases the potential of KD to create lightweight, efficient language models, an important aspect relevant to the exploration of pre-training distillation in the current paper.", "section_number": 4}, {" publication_date": "2020", "fullname_first_author": "Xiaoqi Jiao", "paper_title": "Tinybert: Distilling bert for natural language understanding", "reason": "This work demonstrates successful knowledge distillation applied to BERT, further highlighting the effectiveness of this technique in creating smaller, faster language models. It's a direct example demonstrating the successful use of KD for model compression, which is relevant to the primary focus of the current paper.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Yuxian Gu", "paper_title": "Minillm: Knowledge distillation of large language models", "reason": "This paper is particularly relevant because it directly addresses knowledge distillation applied to large language models (LLMs) and provides a related approach to pre-training distillation.  Comparing the results and findings with the current paper's exploration of the design space of pre-training distillation offers valuable insights.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Tom Gunter", "paper_title": "Apple intelligence foundation language models", "reason": "This work introduces a series of powerful language models that provide a strong context for the exploration of pre-training distillation.  The performance and characteristics of these models serve as a benchmark against which to measure the effectiveness of the proposed pre-training distillation methods in the current paper.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Justin Kiefel", "paper_title": "Lokilm: Technical report", "reason": "This paper presents a specific approach to knowledge distillation for LLMs, providing another comparative point of reference for evaluating the design choices and results in the current paper. It is relevant to understand how other approaches use pre-training distillation, and how they compare to the proposed techniques.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Sharath Turuvekere Sreenivas", "paper_title": "Llm pruning and distillation in practice: The minitron approach", "reason": "This work focuses on pruning and distillation of large language models (LLMs), and thus it closely relates to the current research. The methods, results, and conclusions of this work offer a comparison to the pre-training distillation methods explored in this paper.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Gemma Team", "paper_title": "Gemma 2: Improving open language models at a practical size", "reason": "This paper shows the successful application of knowledge distillation techniques in producing compact and efficient large language models, which is strongly relevant to the current paper's focus on creating more efficient language models through pre-training distillation.  The results provide a comparison against which the effectiveness of the current research can be measured.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Sungsoo Ahn", "paper_title": "Variational information distillation for knowledge transfer", "reason": "This paper introduces a variation of knowledge distillation and shows how it can be applied to the transfer of knowledge from a teacher to a student model.  Its techniques and findings are relevant in understanding and comparing the overall effectiveness of the pre-training distillation approach proposed in this paper.", "section_number": 4}, {" publication_date": "2020", "fullname_first_author": "Paul Bergmann", "paper_title": "Uninformed students: Student-teacher anomaly detection with discriminative latent embeddings", "reason": "This paper is related to the concept of student-teacher learning and how it applies to improving the performance of student models.  The concepts presented in this work offer some relevant background information and ideas applicable to knowledge distillation in general, making it a valuable reference.", "section_number": 4}, {" publication_date": "2017", "fullname_first_author": "Takashi Fukuda", "paper_title": "Efficient knowledge distillation from an ensemble of teachers", "reason": "This paper introduces an efficient method for knowledge distillation from an ensemble of teacher models, providing a valuable comparative approach to the distillation methods proposed in the current paper.  The effectiveness of ensembles and how to distill knowledge from them are both relevant considerations in the context of pre-training distillation.", "section_number": 4}, {" publication_date": "2022", "fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "reason": "This work introduces a method for training language models to better follow instructions by using human feedback.  The concepts and techniques demonstrated here are relevant to improving the quality and behavior of LLMs which directly impacts how knowledge distillation techniques are applied and evaluated in the current research.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Ganqu Cui", "paper_title": "Ultrafeedback: Boosting language models with scaled ai feedback", "reason": "This paper discusses boosting language models using scaled AI feedback, which is relevant to the paper\u2019s work on pre-training distillation.  The concepts and techniques presented here provide another perspective on improving the overall performance and capability of LLMs, a closely related goal of pre-training distillation.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Saurav Muralidharan", "paper_title": "Compact language models via pruning and knowledge distillation", "reason": "This paper tackles improving the efficiency of large language models (LLMs) through pruning and distillation, an area strongly related to the current paper's pre-training distillation approach.  Comparing and contrasting the methods and conclusions of this paper to the current paper's work provides important insights.", "section_number": 4}, {" publication_date": "2019", "fullname_first_author": "Ari Holtzman", "paper_title": "The curious case of neural text degeneration", "reason": "This paper discusses the challenges of neural text generation, a critical component of large language models (LLMs). The issues raised in this work are relevant to understanding the context of pre-training distillation, as the quality and coherence of the teacher model's output directly affect the effectiveness of the distillation process.  The methods and considerations in this paper provide a relevant background to the challenges and complexities of training and using LLMs effectively.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Zhihao Chi", "paper_title": "Normkd: Normalized logits for knowledge distillation", "reason": "This paper presents a specific method for knowledge distillation that is directly applicable to the techniques explored in the current study, using normalization methods to improve distillation effectiveness.  The specific approach presented here offers a valuable comparison against the methods explored in the current research, helping to evaluate its strengths and weaknesses.", "section_number": 3}]}