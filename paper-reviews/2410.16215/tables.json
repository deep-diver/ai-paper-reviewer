[{"figure_path": "2410.16215/tables/table_4_0.html", "caption": "Table 1: Preliminary experimental results on the evaluation datasets. \u0394 is relative to LLM-LM.", "description": "Table 1 presents the preliminary experimental results comparing the performance of LLMs pre-trained with only LM loss and LLMs pre-trained with distillation (LLM-KD) across various evaluation datasets.", "section": "3.1 Preliminary Experiment"}, {"figure_path": "2410.16215/tables/table_4_1.html", "caption": "Table 1: Preliminary experimental results on the evaluation datasets. A is relative to LLM-LM.", "description": "The table presents the preliminary experimental results on several evaluation datasets, comparing the performance of LLMs pre-trained with and without pre-training distillation.", "section": "3.1 Preliminary Experiment"}, {"figure_path": "2410.16215/tables/table_5_0.html", "caption": "Table 3: Experimental results of LLMs pre-trained with different adaptive temperature \u03c4 methods.", "description": "Table 3 presents the experimental results of LLMs pre-trained with different adaptive temperature methods, comparing their performance across various evaluation datasets.", "section": "3.2 Design Dimension #1: Logits Processing"}, {"figure_path": "2410.16215/tables/table_5_1.html", "caption": "Table 4: Relative improvements (%) compared to LLM-LM using different a in combination of Lim and Lkd.", "description": "Table 4 presents the relative improvements in performance compared to the baseline LLM-LM model,  showing the impact of varying the combination factor (\u03b1) of language modeling loss and distillation loss on the overall performance across various datasets.", "section": "3.3 Design Dimension #2: Loss Selection"}, {"figure_path": "2410.16215/tables/table_6_0.html", "caption": "Table 5: Experimental results of LLMs pre-trained with different pre-training loss. \u0394 is relative to LLM-LM. 0-\u03b1 and 1-\u03b1 denote setting \u03b1 = 0 and \u03b1 = 1.0, respectively. 0-\u03b1+WSD-LR represents LLM-LM training with the WSD scheduler, which serves as a baseline. Cos-LR means a cosine learning rate scheduler. \u03b2 = 1 - \u03b1, and WSD-\u03b2 denotes applying the WSD scheduler to the proportion of LM loss.", "description": "Table 5 presents the experimental results of LLMs pre-trained with different loss functions, showing the relative improvements compared to a baseline LLM trained only with language modeling loss.", "section": "3.3 Design Dimension #2: Loss Selection"}, {"figure_path": "2410.16215/tables/table_7_0.html", "caption": "Table 6: Experimental results of different LLMs pre-trained with online logits. A is relative to LLM-LM.", "description": "Table 6 presents the performance comparison of three LLMs pre-trained with online logits, showing that using online logits in pre-training distillation can yield comparable performance to offline logits, particularly when the teacher LLM is closer to convergence.", "section": "3.5 Design Dimension #4: Offline or Online"}, {"figure_path": "2410.16215/tables/table_12_0.html", "caption": "Table 7: Model architectures of student LLMs of varying sizes. \"#Query Groups\" denotes the number of query groups in grouped-query attention (GQA, Ainslie et al., 2023). \u201cTie\u201d represents whether to tie the word embeddings and output weights. All the models are trained with BFLOAT16 (Kalamkar et al., 2019) format.", "description": "This table presents the model architectures of student LLMs with different sizes, including hidden size, FFN hidden size, number of layers, attention heads, and query groups.", "section": "3.1 Preliminary Experiment"}, {"figure_path": "2410.16215/tables/table_14_0.html", "caption": "Table 7: Model architectures of student LLMs of varying sizes. \"#Query Groups\" denotes the number of query groups in grouped-query attention (GQA, Ainslie et al., 2023). \u201cTie\u201d represents whether to tie the word embeddings and output weights. All the models are trained with BFLOAT16 (Kalamkar et al., 2019) format.", "description": "Table 7 shows the different architectures of student LLMs with varying sizes used in the experiments.", "section": "3.1 Preliminary Experiment"}, {"figure_path": "2410.16215/tables/table_14_1.html", "caption": "Table 8: Experimental results on all the evaluation datasets using different p and k in top-p-k truncation.", "description": "Table 8 presents the relative improvements compared to LLM-LM using different p and k values in top-p-k logits truncation, showing the impacts of different p and k values on pre-training distillation performance across various datasets.", "section": "3.2 Design Dimension #1: Logits Processing"}, {"figure_path": "2410.16215/tables/table_14_2.html", "caption": "Table 9: Experimental results on all the evaluation datasets using different \u03c4 in logits normalization.", "description": "Table 9 presents the performance of LLMs pre-trained with different normalization temperatures (\u03c4) across various evaluation datasets.", "section": "3.2 Design Dimension #1: Logits Processing"}, {"figure_path": "2410.16215/tables/table_14_3.html", "caption": "Table 10: Experimental results on all the evaluation datasets using different a in Equation 1.", "description": "Table 10 presents the experimental results on various evaluation datasets using different values of the combination factor (a) for the language modeling loss and distillation loss in Equation 1, demonstrating the impact of loss selection on the performance of pre-training distillation.", "section": "3.3 Design Dimension #2: Loss Selection"}, {"figure_path": "2410.16215/tables/table_15_0.html", "caption": "Table 1: Preliminary experimental results on the evaluation datasets. \u0394 is relative to LLM-LM.", "description": "Table 1 presents the preliminary experimental results on several evaluation datasets, comparing the performance of a 1.9B student LLM pre-trained with and without knowledge distillation.", "section": "3.1 Preliminary Experiment"}, {"figure_path": "2410.16215/tables/table_15_1.html", "caption": "Table 12: Experimental results on all the evaluation datasets of different checkpoints saved every 10,000 optimization step when pre-training the LLMs on 500 billion tokens. \u201c59604\u201d is the last checkpoint saved at the end.", "description": "Table 12 presents the performance of 1.9B and 3.8B LLMs pre-trained with LM loss and KD loss at various checkpoints during the 500B tokens pre-training.", "section": "3.4 Design Dimension #3: Scaling Law"}, {"figure_path": "2410.16215/tables/table_16_0.html", "caption": "Table 13: Experimental results on all the evaluation datasets of a better pre-training distillation configuration.", "description": "Table 13 presents the performance of 1.9B, 3.8B and 6.8B LLMs trained with a better pre-training distillation configuration on eight different evaluation datasets.", "section": "3.2 Design Dimension #1: Logits Processing"}]