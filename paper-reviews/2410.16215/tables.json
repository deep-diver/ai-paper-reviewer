[{"figure_path": "2410.16215/tables/table_4_0.html", "caption": "Table 1: Preliminary experimental results on the evaluation datasets. A is relative to LLM-LM.", "description": "The table presents the preliminary experimental results of LLMs pre-trained with only LM loss and LLMs pre-trained with distillation on eight evaluation datasets.", "section": "3.1 Preliminary Experiment"}, {"figure_path": "2410.16215/tables/table_4_1.html", "caption": "Table 1: Preliminary experimental results on the evaluation datasets. A is relative to LLM-LM.", "description": "The table presents the preliminary experimental results of LLMs pre-trained with and without knowledge distillation on various evaluation datasets, showing a marginal performance improvement with knowledge distillation.", "section": "3.1 Preliminary Experiment"}, {"figure_path": "2410.16215/tables/table_5_0.html", "caption": "Table 3: Experimental results of LLMs pre-trained with different adaptive temperature \\(\\tau\\) methods.", "description": "Table 3 presents the experimental results of LLMs pre-trained with different adaptive temperature methods, comparing their performance across various evaluation datasets and highlighting the best-performing method.", "section": "3.2 Design Dimension #1: Logits Processing"}, {"figure_path": "2410.16215/tables/table_5_1.html", "caption": "Table 4: Relative improvements (%) compared to LLM-LM using different a in combination of Lim and Lkd.", "description": "Table 4 shows the relative improvements in performance compared to a baseline LLM-LM model when using different values of alpha (\u03b1) to combine language modeling loss and distillation loss during training.", "section": "3.3 Design Dimension #2: Loss Selection"}, {"figure_path": "2410.16215/tables/table_6_0.html", "caption": "Table 5: Experimental results of LLMs pre-trained with different pre-training loss. \u0394 is relative to LLM-LM. 0-\u03b1 and 1-\u03b1 denote setting \u03b1 = 0 and \u03b1 = 1.0, respectively. 0-\u03b1+WSD-LR represents LLM-LM training with the WSD scheduler, which serves as a baseline. Cos-LR means a cosine learning rate scheduler. \u03b2 = 1 - \u03b1, and WSD-\u03b2 denotes applying the WSD scheduler to the proportion of LM loss.", "description": "This table presents the experimental results comparing the performance of LLMs pre-trained with various loss functions and their combinations, showing the impact of different loss selection strategies on the performance of the models.", "section": "3.3 Design Dimension #2: Loss Selection"}, {"figure_path": "2410.16215/tables/table_7_0.html", "caption": "Table 6: Experimental results of different LLMs pre-trained with online logits. A is relative to LLM-LM.", "description": "Table 6 presents the performance comparison of LLMs pre-trained with online logits against a baseline LLM, showing that while online logits can be effective, they yield better results when the teacher model is closer to convergence.", "section": "3.5 Design Dimension #4: Offline or Online"}, {"figure_path": "2410.16215/tables/table_12_0.html", "caption": "Table 7: Model architectures of student LLMs of varying sizes. \"#Query Groups\" denotes the number of query groups in grouped-query attention (GQA, Ainslie et al., 2023). \u201cTie\u201d represents whether to tie the word embeddings and output weights. All the models are trained with BFLOAT16 (Kalamkar et al., 2019) format.", "description": "Table 7 presents the model architectures of student LLMs with varying sizes, showing their hidden size, feed-forward network hidden size, number of layers, attention heads, and query groups.", "section": "3.1 Preliminary Experiment"}, {"figure_path": "2410.16215/tables/table_14_0.html", "caption": "Table 7: Model architectures of student LLMs of varying sizes. \"#Query Groups\" denotes the number of query groups in grouped-query attention (GQA, Ainslie et al., 2023). \u201cTie\u201d represents whether to tie the word embeddings and output weights. All the models are trained with BFLOAT16 (Kalamkar et al., 2019) format.", "description": "Table 7 presents the architectures of student LLMs with varying sizes, detailing their hidden size, feed-forward network (FFN) hidden size, number of layers, attention heads, query groups, and whether word embeddings and output weights are tied.", "section": "3.1 Preliminary Experiment"}, {"figure_path": "2410.16215/tables/table_14_1.html", "caption": "Table 8: Experimental results on all the evaluation datasets using different p and k in top-p-k truncation.", "description": "Table 8 presents the relative improvements compared to LLM-LM using different p and k values in top-p-k logits truncation, showing the impact on various evaluation datasets.", "section": "3.2 Design Dimension #1: Logits Processing"}, {"figure_path": "2410.16215/tables/table_14_2.html", "caption": "Table 9: Experimental results on all the evaluation datasets using different  \u03c4 in logits normalization.", "description": "Table 9 shows the experimental results on multiple datasets using different temperatures (\u03c4) for logits normalization in pre-training distillation.", "section": "3.2 Design Dimension #1: Logits Processing"}, {"figure_path": "2410.16215/tables/table_14_3.html", "caption": "Table 10: Experimental results on all the evaluation datasets using different \u03b1 in Equation 1.", "description": "Table 10 presents the experimental results on eight evaluation datasets using different values of \u03b1 (the combination factor of language modeling loss and distillation loss) in Equation 1, showing the impact of loss selection on the performance of pre-training distillation.", "section": "3.3 Design Dimension #2: Loss Selection"}, {"figure_path": "2410.16215/tables/table_15_0.html", "caption": "Table 11: Experimental results on all the evaluation datasets of baseline LLMs trained with only LM loss and distilled LLMs using varying sizes of teacher and student LLMs.", "description": "Table 11 presents the performance comparison of baseline LLMs trained solely with LM loss against distilled LLMs using different sizes of teacher and student LLMs across various evaluation datasets.", "section": "3.4 Design Dimension #3: Scaling Law"}, {"figure_path": "2410.16215/tables/table_15_1.html", "caption": "Table 12: Experimental results on all the evaluation datasets of different checkpoints saved every 10,000 optimization step when pre-training the LLMs on 500 billion tokens. \u201c59604\u201d is the last checkpoint saved at the end.", "description": "Table 12 presents the performance of 1.9B and 3.8B LLMs pre-trained with LM loss and KD loss at various checkpoints during the 500B tokens pre-training.", "section": "3.4 Design Dimension #3: Scaling Law"}, {"figure_path": "2410.16215/tables/table_16_0.html", "caption": "Table 13: Experimental results on all the evaluation datasets of a better pre-training distillation configuration.", "description": "Table 13 presents the performance of 1.9B, 3.8B, and 6.8B LLMs on eight evaluation datasets after using a better pre-training distillation configuration.", "section": "3.2 Design Dimension #1: Logits Processing"}]