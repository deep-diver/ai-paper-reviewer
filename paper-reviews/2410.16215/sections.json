[{"page_end_idx": 2, "page_start_idx": 1, "section_number": 1, "section_title": "Introduction", "details": {"details": "Knowledge distillation (KD) is a technique to transfer knowledge from a large teacher model to a smaller student model, achieving model compression.  While KD has been widely applied in various fields like computer vision and natural language processing, its application to Large Language Models (LLMs) has primarily focused on the post-training phase. This paper introduces pre-training distillation (PD), extending KD to the pre-training phase of LLMs.  The authors conduct a preliminary experiment using GLM-4-9B as the teacher and a 1.9B parameter student LLM, demonstrating the effectiveness of PD with a 1.6% average performance improvement across English and Chinese datasets. This success motivates a deeper exploration into the design space of PD, investigating key factors like logits processing, loss selection, scaling laws, and offline versus online logits generation.", "first_cons": "The paper primarily focuses on the experimental results and design space exploration, lacking a detailed theoretical analysis to explain why pre-training distillation works better than traditional methods.", "first_pros": "The paper introduces a novel approach, pre-training distillation (PD), extending knowledge distillation to the pre-training phase of LLMs, a previously less explored area.", "keypoints": ["The paper introduces pre-training distillation (PD), extending knowledge distillation to the pre-training phase of LLMs.", "A preliminary experiment shows a 1.6% average performance improvement using GLM-4-9B as the teacher and a 1.9B parameter student LLM.", "The design space exploration of PD considers four key aspects: logits processing, loss selection, scaling law, and offline/online logits.", "The study uses a two-stage paradigm: storing teacher LLM's logits on disk and then using them to train the student LLM to enhance training efficiency, reducing storage space by 4000x (from 58.6PB to 15TB)"], "second_cons": "The analysis of the design space is somewhat descriptive, lacking a deeper dive into the underlying mechanisms driving the performance changes observed with different configurations.", "second_pros": "The systematic exploration of the design space of pre-training distillation, considering logits processing, loss selection, scaling law, and offline/online logits, provides valuable insights and guidance for future research.", "summary": "This paper introduces pre-training distillation (PD), a novel approach that extends knowledge distillation to the pre-training phase of large language models (LLMs).  A preliminary experiment demonstrates its effectiveness, improving performance by 1.6% on average.  The authors then systematically explore the design space of PD, analyzing the impact of logits processing, loss selection, scaling laws, and offline vs. online logits generation to provide guidance for future research."}}, {"page_end_idx": 3, "page_start_idx": 2, "section_number": 2, "section_title": "Design Space for PD", "details": {"details": "This section delves into the design space of pre-training distillation (PD) for large language models (LLMs).  It formalizes the objective of PD by combining the traditional language modeling loss (L<sub>lm</sub>) with a distillation loss (L<sub>kd</sub>), controlled by a parameter *\u03b1*. The core of the section lies in exploring four key dimensions of this design space: \n\n1.  **Logits Processing (F):**  This involves how the teacher LLM's logits are processed before being used in the distillation loss. Techniques like top-p-k truncation and temperature normalization are explored to reduce memory overhead and improve efficiency.  Experiments show that a temperature of 0.5 for normalization yields good results, and while different p and k values in the top-p-k truncation show similar effects, smaller values effectively reduce storage space.\n\n2.  **Loss Selection (L):**  This focuses on the choice of distillation loss function (L) and how it's combined with the LM loss using *\u03b1*.  Three loss functions are evaluated: Kullback-Leibler divergence (KLD), negative log-likelihood (NLL), and Mean Squared Error (MSE).  The authors also discuss different scheduling strategies for *\u03b1*, like linearly increasing or decreasing it, and the Warmup-Stable-Decay (WSD) method. The study highlights the superiority of KL divergence and NLL compared to MSE, and suggests that WSD scheduling with a final value of *\u03b1* around 0.9 gives optimal performance.\n\n3.  **Scaling Law:** This investigates the impact of the size of student and teacher LLMs on PD's effectiveness.  The results indicate that larger student LLMs benefit more significantly from PD, while a larger teacher LLM does not necessarily translate into better outcomes, possibly due to a capacity gap between student and teacher.\n\n4.  **Offline/Online Logits:**  This dimension explores the source of logits, either from a pre-trained teacher (offline) or generated simultaneously during the teacher's pre-training (online). Online logits are computationally cheaper; however, experiments indicate that offline logits generally produce better results, likely due to the teacher's convergence status.\n\nThe section concludes by emphasizing the significance of these design space explorations for future developments in pre-training distillation techniques.", "first_cons": "The exploration of the design space is not exhaustive.  Interactions between the four dimensions are not deeply studied; thus, the findings might not cover all possible optimal scenarios.", "first_pros": "The systematic exploration of the design space for pre-training distillation is valuable. This helps understand the effects of key factors influencing distillation, giving practical guidance on choosing the optimal configuration.", "keypoints": ["The objective function combines language modeling loss and distillation loss with a parameter \u03b1.", "Four dimensions of the design space are explored: Logits Processing, Loss Selection, Scaling Law, and Offline/Online Logits.", "Larger student LLMs benefit more from PD, but a larger teacher LLM doesn't guarantee better results.", "Kullback-Leibler divergence and negative log-likelihood loss outperform MSE loss.  The WSD method for \u03b1 scheduling is recommended.", "Offline logits generally yield better results than online logits due to teacher model convergence issues. Top-p-k truncation with a smaller p and k value can effectively reduce storage space,  Normalization with temperature 0.5 offers good performance, and  a value of \u03b1 around 0.9 is optimal for combining LM and KD losses"], "second_cons": "The study is computationally expensive due to the large models involved, limiting the depth of some explorations, especially interaction effects between factors.", "second_pros": "The section provides a clear formalization of pre-training distillation and offers practical recommendations, such as the use of Kullback-Leibler divergence or negative log-likelihood loss, and WSD scheduling for the \u03b1 parameter. This provides tangible guidelines for researchers and practitioners.", "summary": "This section systematically explores the design space of pre-training distillation for LLMs, focusing on four key dimensions: logits processing, loss selection, scaling law, and offline vs. online logits.  It introduces a formal objective function that combines language modeling loss and distillation loss and evaluates different configurations within each dimension, providing valuable insights and practical guidelines for future research."}}, {"page_end_idx": 5, "page_start_idx": 3, "section_number": 3, "section_title": "Experiments", "details": {"details": "This section details the empirical evaluation of pre-training distillation (PD) for large language models (LLMs). It begins with a preliminary experiment using GLM-4-9B as the teacher and a 1.9B parameter student model, achieving a 1.6% average performance improvement across various English and Chinese datasets.  The core of the section systematically explores the design space of PD, focusing on four key aspects: logits processing, loss selection, scaling law, and the offline/online distinction for obtaining logits. \n\nLogits processing investigates techniques like top-p-k truncation and temperature normalization to mitigate memory overhead.  The study reveals that smaller p and k values effectively reduce storage space without significant performance loss;  adaptive temperature offers no significant benefit compared to static temperature. In loss selection, the experiment compares various loss functions (Kullback-Leibler divergence, negative log-likelihood, and mean squared error). Kullback-Leibler divergence and negative log-likelihood yield similar results and outperform MSE loss.  Combining LM and KD loss shows that a warmup-stable-decay scheduler for the proportion of KD loss effectively enhances performance.  The scaling law analysis examines the effect of varying student and teacher LLM sizes.  Larger student LLMs benefit more from PD, while a larger teacher LLM does not necessarily guarantee better results. Finally, online logits, generated concurrently with teacher model training, show improvement but not as much as offline logits generated from a pre-trained teacher model.\n\nIn summary, the experiments show that PD is effective and improve performance, but optimal configurations depend on several design choices.  A careful consideration of these choices, balancing efficiency and performance, is crucial for effective PD implementation.  The results provide valuable insights into the design space for future research.", "first_cons": "The exploration of the design space is not exhaustive; interactions between the four aspects are not fully investigated due to computational constraints.", "first_pros": "The study systematically explores four key aspects impacting pre-training distillation and provides empirically-validated recommendations for each aspect.", "keypoints": ["1.6% average performance improvement in preliminary experiment", "Logits processing: Top-p-k truncation with smaller p and k values are effective.", "Loss selection: Kullback-Leibler divergence and negative log-likelihood loss outperform MSE loss.", "Scaling Law: Larger student LLMs benefit significantly from PD, but larger teacher LLMs don't guarantee better results.", "Offline/online logits: Offline logits are slightly more effective than online logits"], "second_cons": "The study primarily focuses on logits-based KD; other KD methods are not explored.", "second_pros": "The findings are supported by comprehensive experimental results on multiple datasets, providing a solid empirical basis for understanding the design space of pre-training distillation.", "summary": "This section presents a systematic empirical study of pre-training distillation for LLMs.  A preliminary experiment shows a performance improvement, followed by a detailed exploration of four design dimensions: logits processing, loss selection, scaling law, and offline/online logits,  leading to useful insights and best practices for optimizing pre-training distillation."}}, {"page_end_idx": 7, "page_start_idx": 5, "section_number": 4, "section_title": "Related Work", "details": {"details": "Knowledge distillation (KD), aiming to transfer knowledge from a large teacher model to a smaller student model for model compression, has been widely applied in various fields.  This section focuses specifically on KD's application to large language models (LLMs), particularly during the pre-training phase.  Prior work predominantly concentrated on post-training KD, where student LLMs learn directly from teacher-generated instructions and responses. This contrasts with the pre-training distillation (PD) explored in this paper, which applies KD during the initial pre-training stage. Two main categories of pre-training distillation are identified: one focusing on smaller models before the emergence of ChatGPT (generally several million parameters), and another focusing on LLMs (billion-level parameters).  Existing research on LLM pre-training distillation provides limited detail on the distillation process, often lacking a systematic exploration of the design space.  The authors highlight several prior efforts on LLM pre-training KD, including MiniLLM, Gemma 2, AFM, LokiLM, Mini-tron, and studies focused on pruning and distillation of LLMs.  However, these studies are noted as lacking a comprehensive exploration of the design space of pre-training KD, a gap the current research aims to fill.", "first_cons": "The review of existing work on pre-training distillation for LLMs is somewhat brief and lacks a deep dive into the methodologies and results of specific studies.  More in-depth analysis and comparative study of these methods would strengthen the paper's contribution.", "first_pros": "This section provides a concise yet informative overview of the existing research on knowledge distillation for LLMs, clearly differentiating between pre- and post-training approaches.  The classification of pre-training distillation into two categories\u2014pre- and post-ChatGPT era\u2014provides a useful framework for understanding the evolution of this technique.", "keypoints": ["Post-training KD is the dominant approach in LLMs, focusing on student LLMs learning from teacher-generated examples.", "Pre-training distillation (PD), applying KD during the initial pre-training phase, is less explored.", "Two main categories of pre-training distillation are noted: one for smaller models (pre-ChatGPT era, usually several million parameters) and one for billion-level parameter LLMs.", "Prior work on LLM pre-training distillation often lacks detailed descriptions of the distillation process, necessitating a more thorough exploration of the design space. ", "Several relevant studies are mentioned, including MiniLLM, Gemma 2, AFM, LokiLM, and Mini-tron, highlighting the need for a more systematic approach to explore the design space of pre-training KD"], "second_cons": "While the authors point out the limitations of existing research, they don't offer concrete suggestions for future research directions to address these limitations.  A more proactive approach in suggesting future research avenues would enhance the paper's impact.", "second_pros": "The section effectively highlights the gap in existing literature, namely the lack of a thorough exploration of the design space for pre-training distillation in LLMs.  This clearly motivates the current research and demonstrates its significance.", "summary": "This section reviews related work on knowledge distillation for large language models (LLMs), focusing particularly on the less-explored area of pre-training distillation.  It highlights the dominance of post-training distillation methods and categorizes existing pre-training distillation research into pre- and post-ChatGPT eras, noting the limitations of current approaches in comprehensively exploring the design space.  The authors emphasize the need for a more systematic exploration of pre-training distillation techniques for LLMs, a gap that their current research aims to address."}}]