[{"page_end_idx": 2, "page_start_idx": 1, "section_number": 1, "section_title": "Introduction", "details": {"details": "Knowledge distillation (KD), a technique to transfer knowledge from a large teacher model to a smaller student model, has been widely used in various fields like computer vision and natural language processing.  In the context of large language models (LLMs), KD is typically applied during the post-training phase. This involves the student LLM directly learning from instructions and responses generated by the teacher model.  However, this paper proposes extending KD to the pre-training phase of LLMs, introducing the concept of pre-training distillation (PD).  The authors briefly mention the success of post-training KD in advancing LLM development, citing examples such as GPT-4 and several others that significantly advanced the field. This success motivates the exploration of whether similar benefits can be achieved by applying distillation during the pre-training stage.  The paper aims to systematically investigate the design space of PD, thereby providing valuable insights for future practice.", "first_cons": "The introduction lacks concrete examples of the challenges or limitations encountered in applying knowledge distillation to the pre-training phase, making the motivation for PD less compelling.", "first_pros": "The introduction clearly defines knowledge distillation and its existing applications, providing a strong foundation for understanding the proposed pre-training distillation (PD) method.", "keypoints": ["Knowledge distillation (KD) is typically applied in the post-training phase of LLMs, where the student LLM learns directly from instructions and responses generated by a teacher model.", "This paper proposes extending KD to the pre-training phase of LLMs, a novel approach called pre-training distillation (PD).", "The success of post-training KD in advancing LLM development motivates the exploration of PD.", "The paper intends to conduct a systematic exploration of the design space of PD across multiple dimensions, informing future practices in pre-training distillation."], "second_cons": "The introduction is primarily descriptive and does not offer a detailed overview of the technical challenges or potential benefits of applying PD compared to traditional pre-training methods.", "second_pros": "The introduction effectively highlights the novelty of the proposed approach by clearly differentiating it from existing knowledge distillation techniques applied to LLMs.  The paper also sets the stage for a detailed design space exploration by identifying key parameters that will be explored in subsequent sections.", "summary": "This paper introduces pre-training distillation (PD), a novel approach that extends knowledge distillation (KD) to the pre-training phase of large language models (LLMs). Unlike traditional post-training KD, PD aims to distill knowledge during the pre-training stage itself. The authors highlight the success of post-training KD and posit that a similar approach during pre-training could yield significant benefits. The paper focuses on systematically exploring the design space of PD to provide valuable insights for future research and application of this technique.  The proposed exploration is motivated by the observation that successful post-training KD has substantially advanced the LLM field and that there is potential for similar progress by improving the pre-training phase itself. "}}, {"page_end_idx": 2, "page_start_idx": 2, "section_number": 2, "section_title": "Design Space for PD", "details": {"details": "This section formalizes the objective of pre-training distillation (PD) and defines its design space.  The objective is to minimize a loss function that's a weighted combination of traditional language modeling loss (`Llm`) and a distillation loss (`Lkd`).  The distillation loss measures the difference between the student's and teacher's logit distributions, with the teacher's logits potentially processed through a transformation function, F.  The design space of PD is explored across four dimensions:\n\n1.  **Logits Processing (F):** This involves methods for handling the teacher's logits, focusing on truncation (top-p-k) and temperature scaling for normalization to reduce memory overhead.  The goal is to find efficient truncation strategies that minimize loss in information while greatly reducing storage requirements, reducing storage space by up to 4,000x. \n2.  **Loss Selection:** This explores various choices for the distillation loss function (`L`) and the weight (`\u03b1`) given to the distillation loss versus the language modeling loss in the overall objective function (Equation 1).  This includes investigating the Kullback-Leibler divergence, negative log-likelihood loss, and mean squared error (MSE) loss. The use of a warmup-stable-decay (WSD) schedule for \u03b1, paired with a similar schedule for the learning rate, shows promising results. \n3.  **Scaling Law:**  Investigates the effect of varying the sizes of both student and teacher LLMs on the performance of PD.  The study aims to determine the relationship between model sizes and the effectiveness of knowledge distillation. There are also studies on the impact of corpus size for pre-training.\n4.  **Offline or Online Logits:** This explores whether the teacher's logits are generated offline (from a pre-trained teacher model) or online (simultaneously during teacher model training).", "first_cons": "The section primarily focuses on the mathematical formalization and doesn't provide a comprehensive analysis of the practical implications or challenges associated with implementing different configurations within the design space of pre-training distillation.", "first_pros": "The section clearly and concisely defines the objective function and the four key dimensions of the design space for pre-training distillation, providing a solid framework for further research and experimentation.", "keypoints": ["The objective function for PD is a weighted combination of language modeling loss and distillation loss (Equation 1).", "The design space of PD is explored in four dimensions: Logits Processing, Loss Selection, Scaling Law, and Offline/Online Logits.", "Top-p-k truncation is used for efficient logit processing, resulting in storage space reduction by a factor of 4000.", "Kullback-Leibler divergence and negative log-likelihood loss perform better than MSE loss for knowledge distillation in the context of LLMs.", "Larger student LLMs benefit more from PD than smaller ones. A larger teacher LLM doesn't necessarily guarantee better results.", "Online logits are also effective for PD but may yield slightly lower performance than offline logits. Using a WSD schedule for the loss weight (\u03b1) yields an improvement."], "second_cons": "While the study mentions the computational cost of exploring all combinations of factors, it does not quantitatively assess the trade-offs between the computational resources needed and the potential gains in accuracy.", "second_pros": "The systematic exploration of the design space for PD, divided into four clearly defined dimensions, offers a valuable contribution to the field, guiding future research towards more effective strategies for pre-training distillation.", "summary": "This section lays out the mathematical framework for pre-training distillation (PD) in LLMs.  It defines the objective function as a combination of language modeling and distillation loss, and meticulously details four dimensions of the design space: logits processing (focusing on efficient truncation techniques), loss selection (exploring different loss functions and their weighting), scaling laws (analyzing how model sizes affect results), and online versus offline logits generation.  The goal is to optimize the training of smaller student LLMs by leveraging knowledge from larger teacher LLMs during the pre-training phase. This exploration aims to optimize PD by efficiently managing storage and balancing training losses while considering model scaling and training process differences between offline and online methods.  The analysis provides quantitative results on the relative performance of different approaches within these dimensions, highlighting the importance of thoughtful configuration choices for successful application of PD.  A combination of Kullback-Leibler divergence loss function and a dynamic schedule (Warmup-Stable-Decay) for the weighting parameter (alpha) is found to yield good results."}}, {"page_end_idx": 5, "page_start_idx": 3, "section_number": 3, "section_title": "Experiments", "details": {"details": "This section details experiments conducted to validate pre-training distillation (PD) for large language models (LLMs). It begins with a preliminary experiment using GLM-4-9B as the teacher LLM to distill a 1.9B parameter student LLM, showing a 1.6% average performance improvement.  The core of the section then systematically explores the design space of PD across four key aspects: logits processing, loss selection, scaling law, and offline vs. online logits. Logits processing investigates truncation methods (top-p-k) to manage memory, finding that smaller p and k values effectively reduce storage while maintaining performance. Loss selection explores different loss functions (Kullback-Leibler divergence, negative log-likelihood, MSE), with Kullback-Leibler and negative log-likelihood showing similar improvements while MSE performs significantly worse. The scaling law experiments vary the sizes of both student and teacher LLMs and the pre-training corpus size, revealing that larger student LLMs generally benefit more from PD, while a larger teacher LLM doesn't necessarily guarantee better results. Finally, offline and online logits strategies are compared, with offline logits consistently yielding better results.  The experiments use various datasets for evaluation, including English and Chinese language understanding and commonsense reasoning datasets, showcasing the effectiveness and generalizability of the findings.", "first_cons": "The study focuses on a limited set of LLMs (primarily GLM-based) and hyperparameter configurations, limiting the generalizability of results to other LLM architectures and configurations.  A more extensive exploration across a wider range of LLMs is needed for broader conclusions.", "first_pros": "The systematic exploration of the design space provides valuable insights into effective pre-training distillation strategies, offering practical guidance for future LLM development.", "keypoints": ["A preliminary experiment shows a 1.6% average performance improvement using GLM-4-9B as teacher and 1.9B student LLM.", "Smaller p and k values in top-p-k logits truncation effectively reduce storage while maintaining performance.", "Kullback-Leibler and negative log-likelihood loss functions yield comparable improvements, while MSE performs significantly worse.", "Larger student LLMs benefit more from PD than smaller ones, while larger teacher LLMs do not always provide better results.", "Offline logits consistently outperform online logits in improving model performance."], "second_cons": "The research lacks a comprehensive error analysis and discussion of potential biases in the datasets or experimental setup, which could impact the reliability and interpretation of findings.", "second_pros": "The controlled experimental design and detailed reporting of results allow for clear reproducibility and verification by other researchers, facilitating further research and development in the field.", "summary": "This experimental section systematically explores the design space of pre-training distillation (PD) for LLMs.  It starts with a preliminary experiment demonstrating the effectiveness of PD, followed by a controlled investigation across four key aspects: logits processing, loss selection, scaling law, and offline/online logits generation.  Key findings include the effectiveness of smaller top-p-k truncation values, superior performance of Kullback-Leibler and negative log-likelihood losses compared to MSE, and a greater benefit of PD for larger student LLMs. Offline logits consistently showed better performance than online logits.  These results provide valuable insights for optimizing pre-training distillation techniques."}}, {"page_end_idx": 6, "page_start_idx": 5, "section_number": 3, "section_title": "Design Dimension #2: Loss Selection", "details": {"details": "This section investigates the impact of different loss functions on the performance of pre-training distillation.  Three common loss functions were examined: Negative Log-Likelihood (NLL), Kullback-Leibler Divergence (KLD), and Mean Squared Error (MSE).  Initially, only the distillation loss was used (\u03b1 = 1), showing that both NLL and KLD resulted in better performance than the baseline LLM-LM, with KLD slightly outperforming NLL. However, MSE significantly underperformed the baseline.  Further experiments combined the distillation loss with the Language Modeling (LM) loss, testing various proportions (\u03b1) using static values and dynamic scheduling. The best results came from using a dynamic scheduler Warmup-Stable-Decay (WSD) for the proportion of KD loss, along with a WSD learning rate scheduler.  This dynamic approach optimized the proportion of KD loss over the training process and yielded the best performance. The WSD method generally improved the model's performance, achieving an improvement up to 8%.", "first_cons": "The experiments primarily focus on static and dynamic scheduling for the loss functions and do not consider other techniques such as curriculum learning or other advanced methods that might yield further improvements.", "first_pros": "The systematic comparison of three common loss functions provides a clear understanding of their individual effectiveness in the context of pre-training distillation. The exploration of static and dynamic scheduling methods offers valuable insights into optimizing the balance of LM and KD loss.", "keypoints": ["NLL and KLD losses significantly outperformed MSE loss when used alone (\u03b1 = 1).", "Combining LM and KD loss generally improved performance, with the optimal balance varying.", "Using the WSD dynamic scheduler for loss proportion with WSD learning rate scheduler yielded the best results, with improvements up to 8%."], "second_cons": "The study lacks a detailed theoretical analysis explaining the observed differences in performance across loss functions and their combinations. It mainly focuses on empirical results which limits the generalizability of findings.", "second_pros": "The investigation is comprehensive in its examination of different loss functions and their dynamic schedules. The clear presentation of results and the consistent methodology across experiments make the findings easily understandable and reproducible.", "summary": "This section explores loss function selection for pre-training distillation, comparing Negative Log-Likelihood (NLL), Kullback-Leibler Divergence (KLD), and Mean Squared Error (MSE).  While NLL and KLD generally outperformed MSE, dynamically scheduling the loss proportion using a Warmup-Stable-Decay (WSD) method, along with a WSD learning rate scheduler, produced the best results, with significant improvements in model performance."}}, {"page_end_idx": 7, "page_start_idx": 6, "section_number": 3, "section_title": "Design Dimension #3: Scaling Law", "details": {"details": "This section investigates the scaling law of pre-training distillation by examining the impact of varying student and teacher LLM sizes, as well as the pre-training corpus size.  Experiments use teacher LLMs of 9B and 32B parameters to distill student LLMs ranging from 330M to 6.8B parameters. The results show a clear scaling effect: larger student LLMs (6.8B) benefit significantly more from pre-training distillation than smaller ones (330M), achieving relative improvements of up to 8%. Interestingly, using a larger teacher LLM (32B) doesn't automatically guarantee better results compared to a smaller teacher (9B). Experiments with 500 billion tokens further confirm the consistent improvement provided by pre-training distillation across different student LLM sizes, although the improvement gradually plateaus as the corpus size increases.  The impact of corpus size on the performance of the pre-training is also analyzed, showcasing consistent improvement with a gradual plateau as the size increases. The study highlights that the relative size between student and teacher models plays a significant role, suggesting the existence of an optimal ratio yet to be determined.", "first_cons": "The study does not explore the optimal ratio between student and teacher LLM sizes, leaving a gap in understanding the ideal configuration for pre-training distillation.", "first_pros": "The findings demonstrate a clear scaling effect: Larger student LLMs benefit significantly more from pre-training distillation, showing relative improvements up to 8%.", "keypoints": ["Larger student LLMs benefit significantly more from pre-training distillation than smaller ones (up to 8% relative improvement for the largest model).", "A larger teacher LLM does not automatically result in better performance compared to a smaller teacher LLM.", "The performance improvement from pre-training distillation plateaus as the corpus size increases beyond a certain point.", "Optimal ratio between teacher and student LLM sizes is not definitively determined in this experiment."], "second_cons": "Computational constraints prevented exploration of even larger LLMs and corpus sizes, limiting the generalizability of the findings to extremely large language models.", "second_pros": "The experiments consistently demonstrate the effectiveness of pre-training distillation across various student LLM sizes and a large corpus of 500 billion tokens, providing strong support for its efficacy.", "summary": "This section explores the scaling law in pre-training distillation, revealing that larger student LLMs benefit more from the technique (up to 8% improvement), while larger teacher LLMs do not guarantee superior results.  Performance gains plateau with increasing corpus size, and optimal student-teacher size ratios remain to be fully investigated due to computational limits."}}, {"page_end_idx": 8, "page_start_idx": 7, "section_number": 3, "section_title": "Design Dimension #4: Offline or Online", "details": {"details": "This section investigates how obtaining logits impacts pre-training distillation.  It compares offline methods (logits from a pre-trained teacher) with online methods (logits generated concurrently during teacher LLM pre-training).  Experiments used a 9B parameter teacher LLM to distill student LLMs.  The offline approach, used in previous experiments, showed consistent improvements. The online approach, while potentially advantageous due to reduced inference costs and similarities to curriculum learning, yielded mixed results.  Using logits from the later stages of teacher LLM pre-training is better for online methods, while using offline logits is better for training a single LLM. The results highlight a trade-off between convenience and performance. Using the last 100B tokens from the 400B token teacher LLM training produced slightly better results than using the first 100B tokens. When comparing offline vs online, offline logits consistently showed better results, except when the online approach incorporated the final 100B tokens, but even then, the results did not match the consistent performance obtained with offline logits.", "first_cons": "The online approach yielded inconsistent and often inferior results compared to the consistently superior performance of the offline approach, particularly when using the initial stages of the teacher's pre-training.", "first_pros": "Online logits offer the potential for significant cost savings by eliminating the need for additional inference from a pre-trained teacher LLM.", "keypoints": ["Offline logits consistently outperformed online logits in most experiments.", "Using the final 100 billion tokens in the online experiment provided some improvement, but still didn't reach the level of offline results.", "The online method is more convenient for training multiple LLMs of different sizes.", "The offline method proved to be more reliable and efficient for training single LLMs.", "Significant cost savings are possible using the online method due to the elimination of additional teacher LLM inference, but not at the cost of much lower performance"], "second_cons": "The online method's reliance on the teacher LLM's convergence stage can be a source of variability in results, depending on the model's training status.", "second_pros": "The online approach mirrors curriculum learning, which can enhance learning efficiency and mitigate capacity gaps between teacher and student LLMs.", "summary": "This experiment explores the impact of using offline (pre-trained teacher LLM logits) versus online (concurrently generated during teacher LLM training) logits on pre-training distillation. While offline logits consistently yielded better results, the online approach offered potential cost savings but only performed similarly to the offline approach when utilizing data from the final stages of the teacher LLM training process."}}, {"page_end_idx": 8, "page_start_idx": 8, "section_number": 4, "section_title": "Related Work", "details": {"details": "Knowledge distillation (KD), the process of transferring knowledge from a larger teacher model to a smaller student model, is a well-established technique in machine learning.  This section focuses specifically on KD's application to large language models (LLMs), particularly regarding pre-training.  It highlights that while post-training KD, where the student LLM learns directly from teacher-generated responses, is common and has led to advancements like Alpaca (a model distilled from ChatGPT),  the exploration of pre-training KD remains limited. The section categorizes existing pre-training KD research into two groups:  (1) earlier work on smaller models (pre-ChatGPT era), and (2) more recent work focusing on LLMs. It notes that much of this more recent work offers limited detail on the distillation process itself and doesn't comprehensively explore the design space. This paper, in contrast, aims to systematically examine the design space of pre-training KD for LLMs.", "first_cons": "The review of existing literature on pre-training KD for LLMs is somewhat superficial, lacking detailed analysis of specific methodologies and their strengths/weaknesses.", "first_pros": "The section provides a clear distinction between post-training and pre-training knowledge distillation in the context of LLMs, highlighting the relative scarcity of research in the latter area and the potential benefits.", "keypoints": ["Post-training KD is common and has led to advancements like Alpaca (distilled from ChatGPT), but pre-training KD is less explored.", "Existing literature on pre-training KD for LLMs is categorized into two groups: smaller models (pre-ChatGPT era) and LLMs.", "Much of the recent work on pre-training KD for LLMs lacks detail on the process and doesn't fully explore design space. This paper aims to address this gap."], "second_cons": "The section lacks a detailed comparative analysis of different pre-training KD techniques or a clear recommendation for the best approach.", "second_pros": "The categorization of existing research into pre-ChatGPT era and LLM-focused research is helpful in contextualizing the current state of the field and highlighting the contributions of this paper.", "summary": "This section reviews existing research on knowledge distillation (KD) for large language models (LLMs), focusing on the less-explored area of pre-training KD. While post-training KD has seen significant success, pre-training KD remains under-researched, with existing work often lacking detailed descriptions of methodologies or comprehensive explorations of design space.  The paper positions its work as a systematic investigation into pre-training KD, contrasting its approach to the prior art."}}]