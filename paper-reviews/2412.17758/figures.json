[{"figure_path": "https://arxiv.org/html/2412.17758/x1.png", "caption": "Figure 1: Difference between ARC Challenge and ARC Easy accuracies when considering each answer separately compared to seeing all options. The gap is vastly reduced, up to six times in this comparison.", "description": "The figure shows the accuracy difference between ARC Challenge and ARC Easy when using two different evaluation setups: one considering answers separately and the other showing all options together. The results reveal a significant reduction in the accuracy gap between the two datasets when the model can see all options, thus highlighting the impact of evaluation setup on model performance.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2412.17758/x2.png", "caption": "Figure 2: Model considers particular choices in \n\u2005Ayseparation\u2005 without knowing the alternative (prompt includes only the question). Because options may vary in length, it is a good practice to normalize them Gao (2021).", "description": "Figure 2 illustrates the 'separation' evaluation setup in multi-choice question answering.  The model is presented with a single answer choice and asked to assess its correctness without being shown the other options.  This is contrasted with the 'options' setup (Figure 3) which provides all choices simultaneously. This setup is discussed as commonly overused and potentially misleading, as it doesn't reflect natural reasoning where comparing options is integral. The caption also notes that when answer choices vary in length, normalization (as suggested by Gao, 2021) is a good practice to ensure fair comparison.", "section": "Impact on evaluation results"}, {"figure_path": "https://arxiv.org/html/2412.17758/x3.png", "caption": "Figure 3: Model sees the context of all possible \n\u2005Ayoptions\u2005 in the prompt. Because all of the options are single letters (likely single tokens), scores require no normalization.", "description": "This figure illustrates an improved method for evaluating model performance in multiple-choice question answering. Unlike Figure 2, where the model assesses each answer choice in isolation, Figure 3 presents all choices simultaneously within the prompt. This contextual approach is considered more natural and fairer because it reflects how humans approach such problems. The simplicity of using single-letter options eliminates the need for score normalization, a common issue when dealing with various option lengths.", "section": "1.1 Hardly answerable in separation"}, {"figure_path": "https://arxiv.org/html/2412.17758/x4.png", "caption": "Figure 4: ARC Challenge evaluation results depending on whether the model sees other options or considers each answer separately. Differences reach up to 35%, and assumed setup impacts model rankings.", "description": "This figure displays the results of the ARC Challenge evaluation conducted using two different setups: one where the model considers each answer choice in isolation ('separation'), and another where the model is presented with all answer choices at once ('options'). The bar chart visually represents the accuracy scores achieved by several different large language models (LLMs) under each setup.  The key takeaway is the significant difference in accuracy scores between the two setups, with differences reaching up to 35%. This demonstrates that the evaluation setup significantly impacts the perceived difficulty of the task and the resulting model rankings. The 'options' setup generally leads to substantially higher accuracy scores.", "section": "2 Impact on evaluation results"}, {"figure_path": "https://arxiv.org/html/2412.17758/x5.png", "caption": "Figure 5: OpenBookQA evaluation results depending on whether the model sees other options or considers each answer separately. In a setup with options, current models outperform human test takers.", "description": "This figure displays the OpenBookQA evaluation results, comparing model performance when evaluating answers individually (separation) versus evaluating them within the context of all answer choices (options). The results show that, when considering all options simultaneously, current large language models surpass human performance on this benchmark, highlighting the impact of evaluation methodology on perceived model capabilities.", "section": "2 Impact on evaluation results"}, {"figure_path": "https://arxiv.org/html/2412.17758/x6.png", "caption": "Figure 6: SIQA evaluation results depending on whether the model sees other options or considers each answer separately. Reformulation leads to up to 24% improvement.", "description": "This figure displays the results of evaluating the SIQA benchmark using two different multiple-choice question evaluation setups.  The 'separation' setup evaluates each answer choice independently of the others, while the 'options' setup provides all choices to the model simultaneously.  The graph compares the accuracy of various large language models (LLMs) under each condition.  The key finding is that presenting all options together ('options' setup) leads to a substantial improvement in accuracy (up to 24%), suggesting the previous evaluation methodology underestimated LLM capabilities on this benchmark.", "section": "Impact on evaluation results"}]