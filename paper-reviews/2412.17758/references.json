{"references": [{"fullname_first_author": "Todor Mihaylov", "paper_title": "Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering", "publication_date": "2018-00-00", "reason": "This paper introduces OpenBookQA, a benchmark dataset heavily analyzed in the target paper to illustrate the impact of evaluation setup on model performance and perceived difficulty."}, {"fullname_first_author": "Peter Clark", "paper_title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge", "publication_date": "2018-00-00", "reason": "This paper introduces the ARC Challenge and ARC Easy datasets, which are central to the target paper's analysis of evaluation methodologies in question answering benchmarks."}, {"fullname_first_author": "Yonatan Bisk", "paper_title": "PIQA: Reasoning about Physical Commonsense in Natural Language", "publication_date": "2019-00-00", "reason": "PIQA is another benchmark dataset used in the target paper to demonstrate the effects of different evaluation methods, highlighting the influence of evaluation design on model capabilities."}, {"fullname_first_author": "Maarten Sap", "paper_title": "SocialiQA: Commonsense Reasoning about Social Interactions", "publication_date": "2019-00-00", "reason": "The SIQA dataset is used in the target paper to further support the claim that evaluation setup significantly impacts model performance and the interpretation of model capabilities."}, {"fullname_first_author": "Dan Hendrycks", "paper_title": "Measuring Massive Multitask Language Understanding", "publication_date": "2021-00-00", "reason": "The MMLU benchmark is referenced to show the widespread use of the problematic evaluation setup in other prominent LLM evaluation benchmarks."}]}