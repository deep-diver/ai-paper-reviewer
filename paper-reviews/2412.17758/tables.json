[{"content": "| Model | Reported | Measured \\(s\\) / \\(o\\) | Assessment |\n|---|---|---|---|\n| Llama 65B <cite class=\"ltx_cite ltx_citemacro_cite\">Touvron et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2412.17758v1#bib.bib26\" title=\"\">2023a</a>)</cite> | 56.0 | 55.6 / 70.2 | <span class=\"ltx_text\" style=\"background-color:#E6F4FB;\">&#8194;<span class=\"ltx_text ltx_phantom\"><span style=\"visibility:hidden\">Ay</span></span>separation</span> |\n| Llama 2 70B <cite class=\"ltx_cite ltx_citemacro_cite\">Touvron et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2412.17758v1#bib.bib27\" title=\"\">2023b</a>)</cite> | 57.4 | 57.4 / 79.6 | <span class=\"ltx_text\" style=\"background-color:#E6F4FB;\">&#8194;<span class=\"ltx_text ltx_phantom\"><span style=\"visibility:hidden\">Ay</span></span>separation</span> |\n| Llama 3 70B <cite class=\"ltx_cite ltx_citemacro_cite\">Grattafiori et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2412.17758v1#bib.bib14\" title=\"\">2024</a>)</cite> | 92.9 | 64.2 / 91.3 | <span class=\"ltx_text\" style=\"background-color:#E1F2DB;\">&#8194;<span class=\"ltx_text ltx_phantom\"><span style=\"visibility:hidden\">Ay</span></span>options</span> |\n| Mistral 7B <cite class=\"ltx_cite ltx_citemacro_cite\">Jiang et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2412.17758v1#bib.bib16\" title=\"\">2023</a>)</cite> | 55.5 | 54.1 / 74.6 | <span class=\"ltx_text\" style=\"background-color:#E6F4FB;\">&#8194;<span class=\"ltx_text ltx_phantom\"><span style=\"visibility:hidden\">Ay</span></span>separation</span> |\n| Mixtral 8x7B <cite class=\"ltx_cite ltx_citemacro_cite\">Jiang et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2412.17758v1#bib.bib17\" title=\"\">2024</a>)</cite> | 59.7 | 59.9 / 83.3 | <span class=\"ltx_text\" style=\"background-color:#E6F4FB;\">&#8194;<span class=\"ltx_text ltx_phantom\"><span style=\"visibility:hidden\">Ay</span></span>separation</span> |\n| Mixtral 8x22B <cite class=\"ltx_cite ltx_citemacro_cite\">Mistral AI (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2412.17758v1#bib.bib20\" title=\"\">2024</a>)</cite> | 91.3<sup class=\"ltx_sup\">\u2020</sup> | 70.7 / 91.8 | <span class=\"ltx_text\" style=\"background-color:#E1F2DB;\">&#8194;<span class=\"ltx_text ltx_phantom\"><span style=\"visibility:hidden\">Ay</span></span>options</span> |\n| DeepSeek 67B <cite class=\"ltx_cite ltx_citemacro_cite\">DeepSeek AI et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2412.17758v1#bib.bib7\" title=\"\">2024a</a>)</cite> | 59.0 | 60.1 / 84.6 | <span class=\"ltx_text\" style=\"background-color:#E1F2DB;\">&#8194;<span class=\"ltx_text ltx_phantom\"><span style=\"visibility:hidden\">Ay</span></span>options</span> |\n| DeepSeek V2 <cite class=\"ltx_cite ltx_citemacro_cite\">DeepSeek AI et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2412.17758v1#bib.bib8\" title=\"\">2024b</a>)</cite> | 92.4<sup class=\"ltx_sup\">\u2020</sup> | 70.3 / 92.2 | <span class=\"ltx_text\" style=\"background-color:#E1F2DB;\">&#8194;<span class=\"ltx_text ltx_phantom\"><span style=\"visibility:hidden\">Ay</span></span>options</span> |\n| Qwen 14B <cite class=\"ltx_cite ltx_citemacro_cite\">Bai et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2412.17758v1#bib.bib2\" title=\"\">2023</a>)</cite> | 84.4 | 47.3 / 86.6 | <span class=\"ltx_text\" style=\"background-color:#E1F2DB;\">&#8194;<span class=\"ltx_text ltx_phantom\"><span style=\"visibility:hidden\">Ay</span></span>options</span> |\n| Yi 6B <cite class=\"ltx_cite ltx_citemacro_cite\">01. AI et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2412.17758v1#bib.bib1\" title=\"\">2024</a>)</cite> | 50.3<sup class=\"ltx_sup\">\u2020</sup> | 55.7 / 80.5 | <span class=\"ltx_text\" style=\"background-color:#E6F4FB;\">&#8194;<span class=\"ltx_text ltx_phantom\"><span style=\"visibility:hidden\">Ay</span></span>separation</span> |\n| Gemma 7B <cite class=\"ltx_cite ltx_citemacro_cite\">Gemma Team et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2412.17758v1#bib.bib12\" title=\"\">2024b</a>)</cite> | 53.2 | 53.2 / 79.0 | <span class=\"ltx_text\" style=\"background-color:#E6F4FB;\">&#8194;<span class=\"ltx_text ltx_phantom\"><span style=\"visibility:hidden\">Ay</span></span>separation</span> |\n| Gemma 2 27B <cite class=\"ltx_cite ltx_citemacro_cite\">Gemma Team et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2412.17758v1#bib.bib11\" title=\"\">2024a</a>)</cite> | 71.4 | 65.8 / 90.0 | <span class=\"ltx_text\" style=\"background-color:#E6F4FB;\">&#8194;<span class=\"ltx_text ltx_phantom\"><span style=\"visibility:hidden\">Ay</span></span>separation</span> |", "caption": "Table 1: Measured and reported ARC Challenge scores with our assessment of the setup used by authors. The 25-shot prompting used in contrast to the 0-shot is denoted by \u2020\u2020\\dagger\u2020 (in the case authors use such a setup in their report).", "description": "This table presents the measured and reported ARC Challenge scores from various sources, along with the author's evaluation setup (separation or options).  It compares the reported scores with the scores obtained using both 'separation' and 'options' evaluation methods. A dagger symbol ( \u2020 ) indicates that 25-shot prompting was used instead of 0-shot prompting.", "section": "2 Impact on evaluation results"}, {"content": "| Model | Reported | Measured  s / o | Assessment |\n|---|---|---|---| \n| Llama 65B Touvron et al. (2023a) | 52.3 | 50.3 / 60.1 |  separation |\n| Llama 2 70B Touvron et al. (2023b) | 50.7 | 50.8 / 66.9 |  separation |\n| Llama 3 70B Grattafiori et al. (2024) | 52.2 | 51.2 / 72.9 |  separation |\n| Mistral 7B Jiang et al. (2023) | \u2014<sup>\u22c4</sup> | 50.9 / 62.4 | \u2014 |\n| Mixtral 8x7B Jiang et al. (2024) | \u2014<sup>\u22c4</sup> | 49.4 / 65.1 | \u2014 |\n| Mixtral 8x22B Mistral AI (2024) | \u2014 | 51.1 / 67.3 | \u2014 |\n| DeepSeek 67B DeepSeek AI et al. (2024a) | \u2014 | 51.6 / 61.6 | \u2014 |\n| DeepSeek V2 DeepSeek AI et al. (2024b) | \u2014 | 52.2 / 70.0 | \u2014 |\n| Qwen 14B Bai et al. (2023) | 77.9 | 56.2 / 78.6 |  options |\n| Yi 6B 01. AI et al. (2024) | \u2014 | 52.5 / 71.0 | \u2014 |\n| Gemma 7B Gemma Team et al. (2024b) | 51.8 | 51.8 / 60.0 |  separation |\n| Gemma 2 27B Gemma Team et al. (2024a) | 53.7 | 58.3 / 70.0 |  separation |", "caption": "Table 2: Measured and reported SIQA scores with our assessment of the setup used by authors. Some authors do not directly report scores but average them with other commonsense reasoning problems (denoted by \u22c4), making our assessment unlikely to succeed.", "description": "This table presents the measured and reported SIQA scores from various studies.  It also includes an assessment of the evaluation setup used by each study (separation or options), indicating whether the model evaluated answer choices individually or in the context of all choices. Note that some authors don't report raw SIQA scores; they average them with other datasets, leading to less certain assessment of the setup in those cases (indicated by a \u22c4 symbol).", "section": "3 Are other benchmarks affected?"}, {"content": "| Model | Reported | Measured  s / o / s<sub>b</sub> / o<sub>b</sub> | Assessment |\n|---|---|---|---| \n| Llama 65B [Touvron et al. (2023a)](https://arxiv.org/html/2412.17758/bib26.png) | 60.2 | 47.0 / 59.0 / 60.2 / 56.2 | s<sub>b</sub> separation |\n| Llama 2 70B [Touvron et al. (2023b)](https://arxiv.org/html/2412.17758/bib27.png) | 60.2 | 48.8 / 73.0 / 60.0 / 65.8 | s<sub>b</sub> separation |\n| Llama 3 70B [Grattafiori et al. (2024)](https://arxiv.org/html/2412.17758/bib14.png) | 47.6 | 48.6 / 88.4 / 59.4 / 88.5 | separation |\n| Mistral 7B [Jiang et al. (2023)](https://arxiv.org/html/2412.17758/bib16.png) | \u2014<sup>\u22c4</sup> | 44.2 / 71.6 / 55.0 / 57.8 | \u2014 |\n| Mixtral 8x7B [Jiang et al. (2024)](https://arxiv.org/html/2412.17758/bib17.png) | \u2014<sup>\u22c4</sup> | 47.0 / 80.2 / 55.2 / 78.0 | \u2014 |\n| Mixtral 8x22B [Mistral AI (2024)](https://arxiv.org/html/2412.17758/bib20.png) | \u2014 | 49.6 / 81.6 / 61.2 / 78.4 | \u2014 |\n| DeepSeek 67B [DeepSeek AI et al. (2024a)](https://arxiv.org/html/2412.17758/bib7.png) | 60.2 | 47.6 / 76.6 / 62.0 / 76.2 | s<sub>b</sub> separation |\n| DeepSeek V2 [DeepSeek AI et al. (2024b)](https://arxiv.org/html/2412.17758/bib8.png) | \u2014 | 38.6 / 82.8 / 62.4 / 84.2 | \u2014 |\n| Qwen 14B [Bai et al. (2023)](https://arxiv.org/html/2412.17758/bib2.png) | \u2014 | 43.8 / 87.0 / 54.6 / 79.8 | \u2014 |\n| Yi 6B [01. AI et al. (2024)](https://arxiv.org/html/2412.17758/bib1.png) | \u2014<sup>\u22c4</sup> | 40.4 / 68.2 / 53.6 / 67.6 | \u2014 |\n| Gemma 7B [Gemma Team et al. (2024b)](https://arxiv.org/html/2412.17758/bib12.png) | \u2014 | 44.8 / 65.2 / 58.2 / 65.8 | \u2014 |\n| Gemma 2 27B [Gemma Team et al. (2024a)](https://arxiv.org/html/2412.17758/bib11.png) | \u2014 | 47.6 / 83.0 / 59.8 / 81.4 | \u2014 |", "caption": "Table 3: Measured and reported OpenBookQA scores with our assessment of the setup used by authors. Some authors do not directly report scores but average them with other commonsense reasoning problems (denoted by \u22c4).", "description": "This table presents the OpenBookQA scores reported by various authors, along with the authors' evaluation setup (separation or options) and the scores measured by the authors of this paper. The table highlights discrepancies between reported and measured scores.  A '\u22c4' symbol indicates cases where authors averaged OpenBookQA scores with other commonsense reasoning tasks, making it difficult to accurately assess their evaluation setup.", "section": "3 Are other benchmarks affected?"}, {"content": "| Model |\n|---|---| \n| `huggyllama/llama-65b` |\n| `meta-llama/Llama-2-70b-hf` |\n| `meta-llama/Meta-Llama-3-70B` |\n| `mistralai/Mistral-7B-v0.1` |\n| `mistralai/Mixtral-8x7B-v0.1` |\n| `mistralai/Mixtral-8x22B-v0.1` |\n| `deepseek-ai/deepseek-llm-67b-base` |\n| `deepseek-ai/DeepSeek-V2` |\n| `Qwen/Qwen-14B` |\n| `01-ai/Yi-6B` |\n| `google/gemma-7b` |\n| `google/gemma-2-27b` |", "caption": "Table 4: Exact variants of models used for evaluation.", "description": "This table lists the specific model variants used in the experiments for evaluating the impact of different evaluation setups on various benchmarks.  For each benchmark (ARC, OpenBookQA, SIQA), the authors used various large language models (LLMs) from different sources (e.g., Meta, Mistral, DeepSeek, Google) and versions, ensuring consistency in the comparison of experimental results across models.", "section": "C Evaluation details"}]