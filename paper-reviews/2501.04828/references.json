{"references": [{"fullname_first_author": "Devlin, J.", "paper_title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "publication_date": "2019-06-01", "reason": "This paper introduced BERT, a foundational model for many NLP tasks, whose architecture and pre-training approach are widely used and adapted in this paper's models for historical Turkish."}, {"fullname_first_author": "Raffel, C.", "paper_title": "Exploring the limits of transfer learning with a unified text-to-text transformer", "publication_date": "2020-07-01", "reason": "This paper introduced the T5 model, which is the basis of the TURNA model used in this paper, demonstrating the effectiveness of a unified text-to-text transformer approach for various NLP tasks."}, {"fullname_first_author": "Tay, Y.", "paper_title": "UL2: Unifying language learning paradigms", "publication_date": "2022-05-01", "reason": "This paper presents the UL2 framework, which underpins the TURNA model used in this study, highlighting its capacity for unifying various language learning paradigms."}, {"fullname_first_author": "\u00d6zate\u015f, \u015e. B.", "paper_title": "Deep learning-based Dependency Parsing for Turkish", "publication_date": "2022-01-01", "reason": "This PhD thesis provides extensive background on dependency parsing for Turkish, a crucial aspect of the work presented in this paper, and includes related tasks and methods for tackling syntactic challenges."}, {"fullname_first_author": "Uludo\u011fan, G.", "paper_title": "TURNA: A Turkish encoder-decoder language model for enhanced understanding and generation", "publication_date": "2024-07-01", "reason": "This paper introduces TURNA, a significant Turkish language model used in this research, demonstrating its capabilities in both understanding and generation tasks which are relevant to the paper's goals."}]}