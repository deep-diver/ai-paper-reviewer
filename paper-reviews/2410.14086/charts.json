[{"figure_path": "2410.14086/charts/charts_3_0.png", "caption": "Figure 1: Illustration of prequential coding, a method for estimating K(D, \u03b8) = K(D|p\u03b8) + K(p\u03b8) using \u03b8\u2019s learning algorithm T. a. Pseudocode of the prequential coding program, which jointly compresses D and p\u03b8 by incrementally training a model using T on increasingly more data. The primary contribution to total program length comes from specifying each next datapoint di+1 using the current model p\u03b8i, which takes \u2013 log2 p\u03b8i(di+1) bits. b. A visual illustration of prequential coding. As the learner T sees more data, it outputs models that assign a higher likelihood to new observations, and can thus better compress them. The total prequential code length Lpreq(D; T) is given by the area under the curve. The area underneath the curve\u2019s last point is equal to the complexity of the dataset given the final model, K(D|p\u03b8). Since Lpreq(D; T) = K(D|p\u03b8) + K(p\u03b8), the area above the curve\u2019s last point is equal to K(p\u03b8). Prequential coding formalizes the intuition that simple models generalize better from less data.", "description": "Figure 1 illustrates prequential coding, showing how it jointly compresses data and a model by incrementally training on data, and visualizing how minimizing prequential code length minimizes both training error and model complexity.", "section": "2.2 Prequential coding"}, {"figure_path": "2410.14086/charts/charts_6_0.png", "caption": "Figure 2: Experimental results comparing different learners. Figures show average prequential coding curves for a meta-dataset, which is the mean prediction error on unseen data (generalization error, y-axis) given observed contexts of increasing length (datapoints seen, x-axis). The area underneath these curves corresponds to prequential code length. Error is measured using MSE for linear and sinusoid regression and cross-entropy for Mastermind. a. ICL from next-token prediction objectives (prequential ICL, blue) yields lower prequential code lengths than ICL from past-token prediction objectives (train-risk ICL, orange), with greater effects in low-data regimes. An SGD-based learner (green) fits more complex models than prequential ICL and performs poorly in low-data regimes, but can generalize better in large-data regimes on a difficult Mastermind task due to underfitting in ICL. b. The architecture used to parameterize T has substantial influence on ICL's ability to minimize prequential code length.", "description": "The chart compares the generalization error of three different learning methods (prequential ICL, train-risk ICL, and SGD) across three different tasks (linear regression, sinusoid regression, and Mastermind) and shows that prequential ICL consistently outperforms the other methods in low-data regimes.", "section": "3 Experiments"}, {"figure_path": "2410.14086/charts/charts_6_1.png", "caption": "Figure 2: Experimental results comparing different learners. Figures show average prequential coding curves for a meta-dataset, which is the mean prediction error on unseen data (generalization error, y-axis) given observed contexts of increasing length (datapoints seen, x-axis). The area underneath these curves corresponds to prequential code length. Error is measured using MSE for linear and sinusoid regression and cross-entropy for Mastermind. a. ICL from next-token prediction objectives (prequential ICL, blue) yields lower prequential code lengths than ICL from past-token prediction objectives (train-risk ICL, orange), with greater effects in low-data regimes. An SGD-based learner (green) fits more complex models than prequential ICL and performs poorly in low-data regimes, but can generalize better in large-data regimes on a difficult Mastermind task due to underfitting in ICL. b. The architecture used to parameterize T has substantial influence on ICL's ability to minimize prequential code length.", "description": "The chart compares the generalization performance of different meta-learners (prequential ICL, train-risk ICL, and SGD) across three tasks (linear regression, sinusoid regression, and Mastermind) with varying context lengths, showing that prequential ICL generally achieves lower prequential code lengths and better generalization, especially in low-data regimes.", "section": "3.1 Comparisons to in-context learning with a train-risk objective"}, {"figure_path": "2410.14086/charts/charts_8_0.png", "caption": "Figure 3: Experimental results for LLM and data manipulation strategies. Figures show average prequential coding curves for a meta-dataset, which is the mean prediction error on unseen data (generalization error, y-axis) given observed contexts of increasing length (datapoints seen, x-axis). The area underneath these curves corresponds to prequential code length. Error bars show standard error across 5 seeds. a. An LLM (GPT-4, red) fails to meaningfully minimize prequential code length on a novel Mastermind task, performing far worse than small ICL models trained on a distribution of Mastermind tasks (blue) and a naive baseline that predicts the marginal class distribution over the context (purple). Error is measured using cross-entropy. b. On a synthetic HMM dataset designed to mimic natural language, preferentially training on shorter contexts (red) yields lower prequential code lengths than training uniformly over context lengths (purple). Error is measured using reverse KL divergence between model and oracle conditioned on seen context.", "description": "Figure 3 shows the comparison of generalization performance on unseen data between large pretrained language model and smaller in-context learning models, with and without data manipulation strategies.", "section": "3 Experiments"}, {"figure_path": "2410.14086/charts/charts_20_0.png", "caption": "Figure E.1: Validation loss as a function of the number of tokens seen during training. The curve is averaged over 5 different datasets (seeds). We can see that the models trained on sequences with shorter length converge faster.", "description": "The chart displays the validation loss for two different training methods on a Hidden Markov Model dataset over the number of tokens seen during training, showing faster convergence for the method using shorter training sequences.", "section": "E.4 Hidden Markov Model experiment"}, {"figure_path": "2410.14086/charts/charts_21_0.png", "caption": "Figure E.2: Prequential code curves at different stages of training Reproduction of Figure 3b but with the prequential curve at 610M tokens also. At this point, the models trained with uniform context length have essentially the same performance as the ones trained with smaller context lengths.", "description": "The chart displays the generalization error as a function of the number of datapoints seen during training for models trained with uniform and skewed short context lengths, showing that models trained with shorter contexts converge faster, but the difference diminishes as more data is seen.", "section": "E.4 Hidden Markov Model experiment"}]