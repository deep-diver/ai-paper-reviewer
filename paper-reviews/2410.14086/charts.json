[{"figure_path": "2410.14086/charts/charts_3_0.png", "caption": "Figure 1: Illustration of prequential coding, a method for estimating K(D, \u03b8) = K(D|p\u03b8) + K(p\u03b8) using \u03b8\u2019s learning algorithm T. a. Pseudocode of the prequential coding program, which jointly compresses D and p\u03b8 by incrementally training a model using T on increasingly more data. The primary contribution to total program length comes from specifying each next datapoint di+1 using the current model p\u03b8i, which takes \u2212log2 p\u03b8i(di+1) bits. b. A visual illustration of prequential coding. As the learner T sees more data, it outputs models that assign a higher likelihood to new observations, and can thus better compress them. The total prequential code length Lpreq(D; T) is given by the area under the curve. The area underneath the curve\u2019s last point is equal to the complexity of the dataset given the final model, K(D|p\u03b8). Since Lpreq(D; T) = K(D|p\u03b8) + K(p\u03b8), the area above the curve\u2019s last point is equal to K(p\u03b8). Prequential coding formalizes the intuition that simple models generalize better from less data.", "description": "Figure 1 illustrates prequential coding, a method for estimating the joint complexity of a dataset and a model by incrementally training a model on increasingly more data and compressing each datapoint using the model.", "section": "2.2 Prequential coding"}, {"figure_path": "2410.14086/charts/charts_6_0.png", "caption": "Figure 2: Experimental results comparing different learners. Figures show average prequential coding curves for a meta-dataset, which is the mean prediction error on unseen data (generalization error, y-axis) given observed contexts of increasing length (datapoints seen, x-axis). The area underneath these curves corresponds to prequential code length. Error is measured using MSE for linear and sinusoid regression and cross-entropy for Mastermind. a. ICL from next-token prediction objectives (prequential ICL, blue) yields lower prequential code lengths than ICL from past-token prediction objectives (train-risk ICL, orange), with greater effects in low-data regimes. An SGD-based learner (green) fits more complex models than prequential ICL and performs poorly in low-data regimes, but can generalize better in large-data regimes on a difficult Mastermind task due to underfitting in ICL. b. The architecture used to parameterize T\u00f8 has substantial influence on ICL's ability to minimize prequential code length.", "description": "The chart compares the generalization error of prequential ICL, train-risk ICL, and SGD across three different tasks (linear regression, sinusoid regression, and Mastermind) with varying context lengths, showing that prequential ICL outperforms the others, especially in low-data settings.", "section": "3 Experiments"}, {"figure_path": "2410.14086/charts/charts_6_1.png", "caption": "Figure 2: Experimental results comparing different learners. Figures show average prequential coding curves for a meta-dataset, which is the mean prediction error on unseen data (generalization error, y-axis) given observed contexts of increasing length (datapoints seen, x-axis). The area underneath these curves corresponds to prequential code length. Error is measured using MSE for linear and sinusoid regression and cross-entropy for Mastermind. a. ICL from next-token prediction objectives (prequential ICL, blue) yields lower prequential code lengths than ICL from past-token prediction objectives (train-risk ICL, orange), with greater effects in low-data regimes. An SGD-based learner (green) fits more complex models than prequential ICL and performs poorly in low-data regimes, but can generalize better in large-data regimes on a difficult Mastermind task due to underfitting in ICL. b. The architecture used to parameterize T\u00f8 has substantial influence on ICL's ability to minimize prequential code length.", "description": "The chart compares the performance of different meta-learners (with different architectures) in minimizing prequential code length across various tasks, showing the impact of architecture and objective on generalization.", "section": "3.1 Comparisons to in-context learning with a train-risk objective"}, {"figure_path": "2410.14086/charts/charts_8_0.png", "caption": "Figure 3: Experimental results for LLM and data manipulation strategies. Figures show average prequential coding curves for a meta-dataset, which is the mean prediction error on unseen data (generalization error, y-axis) given observed contexts of increasing length (datapoints seen, x-axis). The area underneath these curves corresponds to prequential code length. Error bars show standard error across 5 seeds. a. An LLM (GPT-4, red) fails to meaningfully minimize prequential code length on a novel Mastermind task, performing far worse than small ICL models trained on a distribution of Mastermind tasks (blue) and a naive baseline that predicts the marginal class distribution over the context (purple). Error is measured using cross-entropy. b. On a synthetic HMM dataset designed to mimic natural language, preferentially training on shorter contexts (red) yields lower prequential code lengths than training uniformly over context lengths (purple). Error is measured using reverse KL divergence between model and oracle conditioned on seen context.", "description": "The chart compares the performance of a large pretrained language model (LLM), a smaller transformer model trained with in-context learning (ICL), and a naive baseline on a Mastermind task, showing that ICL achieves lower prequential code lengths, especially when using shorter context lengths in training.", "section": "3 Experiments"}, {"figure_path": "2410.14086/charts/charts_20_0.png", "caption": "Figure E.1: Validation loss as a function of the number of tokens seen during training. The curve is averaged over 5 different datasets (seeds). We can see that the models trained on sequences with shorter length converge faster.", "description": "The chart displays the validation loss as a function of the number of tokens seen during training, showing faster convergence for models trained on shorter sequences.", "section": "E.4 Hidden Markov Model experiment"}, {"figure_path": "2410.14086/charts/charts_21_0.png", "caption": "Figure E.2: Prequential code curves at different stages of training Reproduction of Figure 3b but with the prequential curve at 610M tokens also. At this point, the models trained with uniform context length have essentially the same performance as the ones trained with smaller context lengths.", "description": "The chart displays prequential coding curves for models trained with uniform and skewed short context lengths, showing generalization error as a function of datapoints seen at different training stages.", "section": "E.4 Hidden Markov Model experiment"}]