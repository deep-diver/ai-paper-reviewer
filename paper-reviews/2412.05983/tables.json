[{"content": "| Stage 1 | General: | \n|---|---| \n|  | ShareGPT4v [10], ShareGPT4-o [10] | \n| Table: | TableX [75] | \n| Chart: | ChartQA [48], PlotQA [53], ChartX [76], SimChart [74] | \n| Math: | MAVIS-Caption [80] | \n| Stage 2: | Language: | \n|---|---| \n|  | Kaggle-science-exam [36], MathInstruct [78], MathQA [3], SciInstruct [79], Orcamath [54] | \n| General: | ShareGPT4v [10], ShareGPT4-o [10], LLaVAR [82], AI2D (GPT4V) [28], AI2D (InternVL [12]), AI2D (Original) [25], MathVision [70], IconQA [45], MapQA [8], ScienceQA [59], ArxivQA [31], TQA [26], CLEVR-Math [19], Super-CLEVR [34], Cambrian Data Engine [66] | \n| Table: | TableX [75], TabMWP [47], MMTab [83] | \n| Chart: | PlotQA [53], ChartX [76], SimChart [74], Chart2Text [23], ChartQA [48], LRV Chart [41], ChartGemma [51], DVQA [21], FigureQA [22], VisText [63] | \n| Math: | MAVIS-Caption [80], Geo170K [16], GeoMVerse [24], MAVIS Manual Collection [80], MAVIS Data Engine [80], Geometry3K [44], GeoQA+ [9], InterGPS [44] | ", "caption": "Table 1: Dataset used for Chimera-Reasoner. Stage 1 and Stage 2 represent Domain-General Knowledge Alignment and Visual Instruction Tuning separately.", "description": "This table details the datasets used to train the Chimera-Reasoner model.  The training process is divided into two stages. Stage 1 focuses on Domain-General Knowledge Alignment, where the model learns to align general knowledge with domain-specific knowledge from various datasets.  Stage 2, Visual Instruction Tuning, further refines the model using instruction-following datasets from different domains to improve its performance on specialized tasks. Datasets are categorized into general, table, chart, and math domains, with each category including multiple datasets for comprehensive training.", "section": "3. Methodology"}, {"content": "| Stage | Datasets |\n|---|---| \n| Stage 1 | ChartQA [48], PlotQA [53], ChartX [76], SimChart [74], TableX [75] |\n| Stage 2 | DocGenome [75], DocStruct4M [18], DocVQA [52] |", "caption": "Table 2: Datasets used for Chimera-Extractor. Stage 1 represents Domain-General Knowledge Alignment, and Stage 2 represents Visual Instruction Tuning.", "description": "This table details the datasets used to train the Chimera-Extractor model.  The training process is divided into two stages. Stage 1, Domain-General Knowledge Alignment, focuses on aligning the model's understanding with general domain knowledge.  Stage 2, Visual Instruction Tuning, further refines the model using instruction-following datasets. The table lists the datasets used in each stage, categorized by their contribution to model training.", "section": "3. Methodology"}, {"content": "Model | #Params. | ALL | FQA | GPS | MWP | TQA | VQA | ALG | ARI | GEO | LOG | NUM | SCI | STA\n---|---|---|---|---|---|---|---|---|---|---|---|---|---|---\n**Close Source LMMs** |  |  |  |  |  |  |  |  |  |  |  |  |  | \nInternVL2-Pro [12] | - | 66.8 | 70.6 | 65.4 | 76.9 | 71.5 | 48.0 | 66.5 | 62.3 | 63.6 | 27.0 | 40.3 | 65.6 | 81.1\nGemini 1.5 Pro [65] | - | 63.9 | - | - | - | - | - | - | - | - | - | - | - | -\nGPT-4o | - | 63.8 | - | - | - | - | - | - | - | - | - | - | - | -\nGrok-1.5V | - | 52.8 | - | - | - | - | - | - | - | - | - | - | - | -\nClaude 3 Opus [1] | - | 50.5 | - | - | - | - | - | - | - | - | - | - | - | -\nGPT-4V (Playground) | - | 49.9 | 43.1 | 50.5 | 57.5 | 65.2 | 38.0 | 53.0 | 49.0 | 51.0 | 21.6 | 20.1 | 63.1 | 55.8\n**Open Source LMMs** |  |  |  |  |  |  |  |  |  |  |  |  |  | \nLLaVA-OneVision [28] | 72B | 67.5 | - | - | - | - | - | - | - | - | - | - | - | -\nMath-LLaVA [62] | 13B | 46.6 | 37.2 | 57.7 | 56.5 | 51.3 | 33.5 | 53.0 | 40.2 | 56.5 | 16.2 | 33.3 | 49.2 | 43.9\nPixtral [2] | 12B | 58.0 | - | - | - | - | - | - | - | - | - | - | - | -\nSPHINX-MoE [38] | 8\u00d77B | 42.7 | - | - | - | - | - | - | - | - | - | - | - | -\nInternLM-XComposer2 [15] | 7B | 57.6 | 55.0 | 63.0 | 73.7 | 56.3 | 39.7 | 56.6 | 52.4 | 62.3 | 8.1 | 42.4 | 59.0 | 64.1\nLLaVA-OneVision [28] | 7B | 63.2 | - | - | - | - | - | - | - | - | - | - | - | -\nMath-PUMA-DeepSeek-Math [84] | 7B | 44.7 | 42.8 | 39.9 | 67.7 | 42.4 | 31.3 | 39.2 | 41.9 | 41.4 | 8.1 | 36.8 | 48.4 | 52.5\nQwen2-VL [71] | 2B | 43.0 | - | - | - | - | - | - | - | - | - | - | - | -\nQwen2-VL [71] | 7B | 58.2 | - | - | - | - | - | - | - | - | - | - | - | -\nIntenrVL2 [12] | 2B | 48.3 | 51.3 | 45.7 | 40.9 | 50.6 | 52.5 | 43.4 | 47.3 | 42.3 | 13.5 | 28.5 | 53.3 | 56.8\nIntenrVL2 [12] | 4B | 57.0 | 58.0 | 58.2 | 62.4 | 57.0 | 48.6 | 55.9 | 53.8 | 55.2 | 13.5 | 30.6 | 59.0 | 65.1\nIntenrVL2 [12] | 8B | 61.6 | 62.5 | 64.4 | 61.3 | 64.6 | 54.7 | 63.0 | 58.9 | 61.9 | 18.9 | 34.0 | 59.0 | 70.1\nChimera-Reasoner | 2B | 53.1 | 52.4 | 56.7 | 62.9 | 51.9 | 40.8 | 52.7 | 47.6 | 56.1 | 10.8 | 34.0 | 52.5 | 61.1\nChimera-Reasoner | 4B | 61.3 | 58.4 | 66.8 | 72.0 | 61.4 | 48.0 | 63.3 | 54.7 | 65.7 | 24.3 | 39.6 | 60.7 | 66.4\nChimera-Reasoner | 8B | 64.9 | 62.8 | 71.6 | 72.6 | 65.2 | 52.0 | 67.6 | 57.8 | 69.5 | 21.6 | 45.8 | 61.5 | 69.4\nHuman performance | - | 60.3 | 59.7 | 48.4 | 73.0 | 63.2 | 55.9 | 50.9 | 59.2 | 51.4 | 40.7 | 53.8 | 64.9 | 63.9", "caption": "Table 3: Accuracy scores on the testmini subset of MathVista. Task types: FQA: figure QA, GPS: geometry problem solving, MWP: math word problem, TQA: textbook QA, VQA: visual QA.\nMath reasoning types: ALG: algebraic, ARI: arithmetic, GEO: geometry, LOG: logical , NUM: numeric, SCI: scientific, STA: statistical.", "description": "This table presents the performance of various large multi-modal models (LMMs) on the testmini subset of the MathVista benchmark.  The MathVista benchmark evaluates the ability of LMMs to perform multi-modal reasoning tasks, focusing on mathematical problems. The table shows accuracy scores across different task types within MathVista: Figure QA (FQA), Geometry Problem Solving (GPS), Math Word Problem (MWP), Textbook QA (TQA), and Visual QA (VQA).  Furthermore, it breaks down the mathematical reasoning types into six categories: Algebraic (ALG), Arithmetic (ARI), Geometric (GEO), Logical (LOG), Numeric (NUM), Scientific (SCI), and Statistical (STA). This allows for a detailed analysis of model performance across various problem types and reasoning skills.", "section": "4. Experiments"}, {"content": "| Model | #Params. | All Acc | Text Dominant | Text Lite | Vision Intensive | Vision Dominant | Vision Only |\n|---|---|---|---|---|---|---|---| \n| **Closed-source MLLMs** |  |  |  |  |  |  |  |\n| Gemini-Pro [64] | - | 23.5 | 26.3 | 23.5 | 23.0 | 22.3 | 22.2 |\n| Qwen-VL-Max [5] | - | 25.3 | 30.7 | 26.1 | 24.1 | 24.1 | 21.4 |\n| GPT-4V | - | 39.4 | 54.7 | 41.4 | 34.9 | 34.4 | 31.6 |\n| **Open-source MLLMs** |  |  |  |  |  |  |  |\n| SPHINX-Plus [38] | 13B | 14.0 | 16.3 | 12.8 | 12.9 | 14.7 | 13.2 |\n| SPHINX-MoE [38] | 8\u00d77B | 15.0 | 22.2 | 16.4 | 14.8 | 12.6 | 9.1 |\n| LLaVA-NeXT [27] | 110B | 24.5 | 31.7 | 24.1 | 24.0 | 22.1 | 20.7 |\n| LLaVA-NeXT [27] | 8B | 19.3 | 24.9 | 20.9 | 20.8 | 16.1 | 13.8 |\n| InternLM-XComposer2 [15] | 7B | 16.5 | 22.3 | 17.0 | 15.7 | 16.4 | 11.0 |\n| Math-LLaVA [62] | 13B | 19.0 | 21.2 | 19.8 | 20.2 | 17.6 | 16.4 |\n| MAVIS-7B [80] | 7B | 27.5 | 41.4 | 29.1 | 27.4 | 24.9 | 14.6 |\n| Math-PUMA-DeepSeek-Math [84] | 7B | 31.8 | 43.4 | 35.4 | 33.6 | 31.6 | 14.7 |\n| InternVL2 [12] | 2B | 21.4 | 24.1 | 22.5 | 22.8 | 21.1 | 16.6 |\n|  | 4B | 26.3 | 32.0 | 28.6 | 28.0 | 24.4 | 18.8 |\n|  | 8B | 31.3 | 38.8 | 34.5 | 33.6 | 32.6 | 17.0 |\n| Chimera-Reasoner | 2B | 22.6 | 27.3 | 23.9 | 22.3 | 22.8 | 16.9 |\n|  | 4B | 27.2 | 31.4 | 30.8 | 29.7 | 25.7 | 18.2 |\n|  | 8B | 32.4 | 39.6 | 35.8 | 34.8 | 32.7 | 19.3 |", "caption": "Table 4: Performance Comparison on MathVerse with the accuracy metric.", "description": "This table presents a comparison of different models' performance on the MathVerse benchmark, specifically focusing on the accuracy metric. It shows how various large multimodal models (LLMs), including both closed-source and open-source options, perform on a range of tasks within the MathVerse dataset.  The table allows for a comparison of performance between generalist LMMs and specialized models, and how Chimera-Reasoner compares against other approaches.", "section": "4. Experiments"}, {"content": "| Model | ALL | General | Chart | Table | Math |\n|---|---|---|---|---|---| \n| InternVL2-2B [12] | 48.3 | 45.3 | 58.9 | 50.0 | 44.2 |\n| InternVL2-4B [12] | 57.0 | 50.1 | 66.2 | 65.7 | 58.3 |\n| InternVL2-8B [12] | 61.6 | 52.7 | 71.2 | 67.1 | 66.5 |\n| Chimera-Reasoner-2B | 53.1 | 46.0 | 60.3 | 62.9 | 56.1 |\n| Chimera-Reasoner-4B | 61.3 | 54.0 | 64.8 | 72.9 | 66.9 |\n| Chimera-Reasoner-8B | 64.9 | 57.5 | 71.2 | 62.9 | 71.9 |", "caption": "Table 5: Accuracy scores of different visual content domain on the testmini subset of MathVista.Those do not belong to the last three domains are uniformly classified as General for simplicity.", "description": "This table presents the accuracy results of the Chimera model on the MathVista benchmark's testmini subset, categorized by visual content domain.  The benchmark tests various aspects of multi-modal reasoning.  MathVista's 'testmini' subset focuses on evaluating the model's ability to answer questions related to specific visual content. For simplicity, any visual content not belonging to the chart, table, or math domains is grouped under a general category. The table displays the accuracy in percentage for different model sizes and visual content domains.", "section": "4. Experiments"}, {"content": "| Task | Metric | Deplot [40] | UniChart [49] | ChartVLM [76] | GPT-4V | Qwen-VL [5] | GOT [73] | InternVL-2 [12] | Chimera-Reasoner |\n|---|---|---|---|---|---|---|---|---|---| \n| ChartQA-SE | AP@strict | 61.4 | 42.3 | 71.8 | 50.4 | 58.6 | **74.7** | 73.7 | 74.1 |\n|  | AP@slight | 70.9 | 53.1 | 81.4 | 60.6 | 68.5 | **84.5** | 83.9 | 84.4 |\n|  | AP@high | 72.9 | 56.0 | 84.2 | 64.3 | 72.7 | 86.7 | 87.2 | **87.6** |\n| PlotQA-SE | AP@strict | 3.1 | 10.5 | 3.8 | 7.3 | 0.5 | **13.3** | 5.7 | 5.9 |\n|  | AP@slight | 16.5 | 26.0 | 46.8 | 19.4 | 4.2 | 59.6 | 55.0 | **62.1** |\n|  | AP@high | 26.5 | 26.9 | 54.0 | 22.3 | 12.0 | 64.0 | 61.8 | **71.0** |", "caption": "Table 6: Performance comparison on ChartQA-SE and PlotQA-SE benchmarks. Metrics include Average Precision (AP) at strict, slight, and high levels.", "description": "Table 6 presents a performance comparison of different models on two chart question answering benchmarks: ChartQA-SE and PlotQA-SE.  The Average Precision (AP) metric is used to evaluate the performance of each model at three levels of strictness: strict, slight, and high. A higher AP score indicates better performance.  The table allows for a comparison of how well each model can answer questions about charts with varying degrees of tolerance for errors in the answers.", "section": "4.3 Comparison on Visual Structural Extraction"}, {"content": "| Method | Edit Distance \u2193 | TEDS \u2191 | TEDS (structure only) \u2191 |\n|---|---|---|---|\n| InternVL-2 [12] | 0.229 | 0.676 | 0.762 |\n| Qwen2-VL [71] | 0.231 | 0.690 | 0.773 |\n| StructEqTable [75] | 0.226 | 0.706 | 0.787 |\n| GOT [73] | 0.257 | **0.745** | **0.830** |\n| Chimera-Reasoner | **0.165** | 0.740 | 0.828 |", "caption": "Table 9: Ablation results on different visual content domain on the testmini subset of MathVista. InternVL2-4B-NF represents naive finetune of baseline with the same settings, Chimera-4B-R\ud835\udc45Ritalic_R means Chimera model trained with mask ratio R\ud835\udc45Ritalic_R in GSCM.", "description": "This table presents ablation study results on the MathVista benchmark's testmini subset.  It compares the performance of different model variations: a naive finetuned baseline (InternVL2-4B-NF) and Chimera models trained with various Generalist-Specialist Collaboration Masking (GSCM) ratios (Chimera-4B-R, where R represents the mask ratio). The goal is to analyze how the GSCM ratio affects the model's performance across different visual content domains within MathVista.", "section": "4.4 Ablation Study"}, {"content": "| Method | Edit Distance<br>\u2193 en | Edit Distance<br>\u2193 zh | Precision<br>\u2191 en | Precision<br>\u2191 zh | BLEU<br>\u2191 en | BLEU<br>\u2191 zh | METEOR<br>\u2191 en | METEOR<br>\u2191 zh |\n|---|---|---|---|---|---|---|---|---|\n| InternVL [12] | 0.504 | 0.604 | 65.4 | 66.0 | 38.4 | 33.1 | 52.6 | 50.6 |\n| GOT [73] | 0.355 | 0.510 | 67.9 | 71.2 | 52.5 | 34.3 | 65.3 | 53.9 |\n| Chimera-Extractor | 0.304 | 0.461 | 69.6 | 66.1 | 49.8 | 40.5 | 64.8 | 56.9 |", "caption": "Table 10: Statistical information of Doc-SE task.", "description": "Table 10 provides a detailed breakdown of the data used in the Document Structural Extraction (Doc-SE) task.  It lists various categories of documents included in the dataset, along with the count of documents in each category. The categories are broken down by document type (like PPT2PDF, Academic Literature, etc.) and language (Simplified Chinese and English).  The table also presents information on document layout (e.g. number of columns), giving a comprehensive overview of the dataset's composition.", "section": "6. Details of Table-SE and Doc-SE"}, {"content": "| Model | Ratio | ALL | General | Chart | Table | Math |\n|---|---|---|---|---|---|---|\n| InternVL2-4B [12] | N/A | 57.0 | 50.1 | 66.2 | 65.7 | 58.3 |\n| InternVL2-4B-NF [12] | N/A | 58.5 | 51.5 | 67.1 | 74.3 | 58.6 |\n| Chimera-4B-0.0 | 0.0 | 59.4 | 50.8 | 66.2 | 67.1 | 65.5 |\n| Chimera-4B | 0.3 | 61.3 | 54.0 | 64.8 | 72.9 | 66.9 |\n| Chimera-4B-0.5 | 0.5 | 60.4 | 51.3 | 68.5 | 70.0 | 65.8 |\n| Chimera-4B-1.0 | 1.0 | 56.2 | 51.5 | 63.5 | 72.9 | 53.6 |", "caption": "Table 11: Statistical information of Table-SE task.", "description": "Table 11 presents a detailed breakdown of the characteristics of the Table-SE (Table Structural Extraction) dataset used in the paper.  It shows the distribution of various factors relevant to the types of tables included. These factors include the presence or absence of background elements, equations, specific table layouts, and the languages used in the tables.", "section": "6. Details of Table-SE and Doc-SE"}, {"content": "| Document Categories | Count |\n|---|---| \n| **PPT2PDF** | 43 |\n| **Academic Literature** | 42 |\n| **Book** | 13 |\n| **Colorful Textbook** | 37 |\n| **Magazine** | 30 |\n| **Exam Paper** | 7 |\n| **Note** | 18 |\n| **Newspaper** | 15 |\n| **Language** |  |\n| Simplified Chinese | 128 |\n| English | 77 |\n| **Layout** |  |\n| 1 and More Column | 27 |\n| Single Column | 91 |\n| Other Layout | 43 |\n| Double Column | 40 |\n| Three Column | 4 |\n| # Total | 205 |", "caption": "Table 12: Accuracy scores of different visual content domain on the testmini subset of MathVista. Those do not belong to the last three domains are uniformly classified as General for simplicity. InternVL2-4B w/ Chart Expert represent the case only integrating chart expert model. Chimera-Reasoner-4B integrates more expert models, including specialized models in chart, table, and math domains.", "description": "This table presents a comparison of accuracy scores achieved by different models on the 'testmini' subset of the MathVista benchmark dataset.  The models compared include the baseline InternVL2-4B model, a version of InternVL2-4B that only incorporates a chart expert model (InternVL2-4B w/ Chart Expert), and the Chimera-Reasoner-4B model which integrates multiple expert models, specialized in chart, table, and math domains.  Accuracy is measured across various visual content domains; those not explicitly belonging to the chart, table, and math categories are grouped under a 'General' category for simplicity and clarity in analysis.", "section": "4. Experiments"}, {"content": "| Feature          | Count |\n|-----------------|-------|\n| **Background** |       |\n| w/o Background   | 80    |\n| w/ Background    | 20    |\n| **Equation**     |       |\n| w/o Equation    | 78    |\n| w/ Equation     | 22    |\n| **Language**     |       |\n| English          | 45    |\n| English & Chinese Mixed | 5     |\n| Chinese          | 50    |\n| **Table Format** |       |\n| Three-line Table | 47    |\n| Full-bordered Table | 39    |\n| Partial-bordered Table | 14    |\n| w/o Merged Cells | 58    |\n| w/ Merged Cells | 42    |\n| **Layout**       |       |\n| Horizontal       | 97    |\n| Vertical         | 3     |\n| **# Total**      | 100   |", "caption": "Table 13: \nDetailed configuration for each training stage of Chimera-Reasoner-2B, Chimera-Reasoner-4B and Chimera-Reasoner-8B. The table outlines the progression of vision parameters, dataset characteristics and training hyperparameters.\nFor elements containing \u201c/\u201d, the left side represents configurations used by the 2B and 4B model, while the right side represents configurations used by the 8B model.", "description": "Table 13 details the configurations used for training the Chimera-Reasoner models (2B, 4B, and 8B).  It shows how the vision parameters, dataset characteristics, and hyperparameters evolved across the two training stages (Domain-General Knowledge Alignment and Visual Instruction Tuning).  Values separated by a '/' indicate that the left-hand value applies to the 2B and 4B models, while the right-hand value applies to the 8B model.", "section": "3. Methodology"}, {"content": "| Model | ALL | General | Chart | Table | Math |\n|---|---|---|---|---|---| \n| InternVL2-4B | 57.0 | 50.1 | 66.2 | 65.7 | 58.3 |\n| InternVL2-4B w/ Chart Expert | 59.4 | 52.0 | 68.0 | 72.9 | 60.8 |\n| Chimera-Reasoner-4B | 61.3 | 54.0 | 64.8 | 72.9 | 66.9 |", "caption": "Table 14: \nDetailed configuration for each training stage of Chimera-Extractor-1B. The table outlines the progression of vision parameters, dataset characteristics and training hyperparameters.", "description": "Table 14 details the configuration used for each training phase of the Chimera-Extractor-1B model. It shows how vision parameters, dataset characteristics, and training hyperparameters changed across the two-stage training process: Domain-General Knowledge Alignment and Visual Instruction Tuning.  Specifically, it illustrates the changes in resolution, the number of tokens, the use of dynamic high resolution, training samples, GSCM ratio, trainable parameters, batch size, learning rate, learning rate scheduler, maximum length, weight decay, and epochs.", "section": "3. Methodology"}]