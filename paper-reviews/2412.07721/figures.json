[{"figure_path": "https://arxiv.org/html/2412.07721/x1.png", "caption": "Figure 1: ObjCtrl-2.5D enables versatile object motion control for image-to-video generation. It accepts 2D trajectories, 3D trajectories, or camera poses as control guidance (all transformed to camera poses) and achieves precise motion control by utilizing an existing camera motion control module without additional training. Unlike existing methods based on 2D trajectories, ObjCtrl-2.5D supports complex motion control beyond planar movement, such as object rotation, as demonstrated in the last row. We strongly recommend viewing the project page for dynamic results.", "description": "ObjCtrl-2.5D is a novel method for image-to-video generation that allows for versatile object motion control using 2D trajectories, 3D trajectories, or camera poses.  The method leverages an existing camera motion control module, eliminating the need for additional training.  It improves upon existing methods by enabling precise control and supporting complex motions (such as rotation) that go beyond simple planar movements. The figure visually demonstrates the diverse control capabilities of ObjCtrl-2.5D with several examples.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2412.07721/x2.png", "caption": "Figure 2: Object control results using 2D and 3D trajectories. On the right, the red line represents the 2D trajectory, the blue line indicates the 3D trajectory extracted from real-world video in DAVIS\u00a0[31], and the green point marks the starting point of the trajectory. The training-based method DragAnything\u00a0[58], which controls objects using a 2D trajectory, closely follows the specified path; however, it results in the car appearing to move horizontally toward the grass, which is atypical in real-world settings. By incorporating depth information from a 3D trajectory, our proposed method generates videos that not only follow the spatial trajectory but also achieve more realistic movement.", "description": "This figure compares object control using 2D and 3D trajectories in image-to-video generation.  The left side shows the input: a 2D trajectory (red line) and a 3D trajectory (blue line) extracted from a real-world video, with the green dot representing the starting point. The right side presents the results.  DragAnything [58], a training-based method using the 2D trajectory, accurately follows the path but unrealistically moves the car horizontally toward the grass, ignoring the depth.  In contrast, the proposed ObjCtrl-2.5D method, using the 3D trajectory, correctly guides the car's movement, demonstrating better accuracy and more realistic behavior, including its approach towards the camera (indicated by the decreasing depth).  The depth information from the 3D trajectory allows for more accurate and natural object movement.", "section": "Object Control Results"}, {"figure_path": "https://arxiv.org/html/2412.07721/x3.png", "caption": "Figure 3: Framework of ObjCtrl-2.5D. ObjCtrl-2.5D first extends the provided 2D trajectory \ud835\udcaf2\u2062dsubscript\ud835\udcaf2\ud835\udc51\\mathcal{T}_{2d}caligraphic_T start_POSTSUBSCRIPT 2 italic_d end_POSTSUBSCRIPT to a 3D trajectory \ud835\udcaf3\u2062dsubscript\ud835\udcaf3\ud835\udc51\\mathcal{T}_{3d}caligraphic_T start_POSTSUBSCRIPT 3 italic_d end_POSTSUBSCRIPT using depth information from the conditioning image. This 3D trajectory is then transformed into a camera pose \ud835\udc04\ud835\udc28subscript\ud835\udc04\ud835\udc28\\mathbf{E_{o}}bold_E start_POSTSUBSCRIPT bold_o end_POSTSUBSCRIPT via Algrithm\u00a01. To achieve object motion control within a frozen camera motion control module, ObjCtrl-2.5D integrates a Layer Control Module (LCM) that separates the object and background with distinct camera poses (\ud835\udc04\ud835\udc28subscript\ud835\udc04\ud835\udc28\\mathbf{E_{o}}bold_E start_POSTSUBSCRIPT bold_o end_POSTSUBSCRIPT and \ud835\udc04\ud835\udc1b\ud835\udc20subscript\ud835\udc04\ud835\udc1b\ud835\udc20\\mathbf{E_{bg}}bold_E start_POSTSUBSCRIPT bold_bg end_POSTSUBSCRIPT). After extracting camera pose features via a Camera Encoder, LCM spatially combines these features using a series of scale-wise masks. Additionally, ObjCtrl-2.5D introduces a Shared Warping Latent (SWL) technique, which enhances control by sharing low-frequency initialized noise across frames within the warped areas of the object.", "description": "ObjCtrl-2.5D uses a 2D trajectory and depth information from an image to create a 3D trajectory.  This 3D trajectory is converted into camera poses using Algorithm 1.  To control object movement within an existing camera motion control module, a Layer Control Module (LCM) separates the object and background using distinct camera poses.  A camera encoder extracts features from these poses, which are then spatially combined using scale-wise masks by the LCM. Finally, a Shared Warping Latent (SWL) technique improves control accuracy by sharing low-frequency noise across frames within the object's warped region.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2412.07721/x4.png", "caption": "Figure 4: 3D Trajectory to Camera Poses. We model the object movement in a video, indicated by a 3D trajectory, as the camera\u2019s location translation in 3D space. Details refer to Sec.\u00a03.2.1 and Algorithm.\u00a01.", "description": "This figure illustrates the method used to translate a 3D trajectory of an object's movement into a series of camera poses.  The object's movement from one point to the next in the 3D trajectory is represented by a camera translation.  The rotation of the camera is not considered, thus simplifying the transformation. This process leverages triangulation to map points in the 3D trajectory to camera positions, thereby enabling control of object motion using an existing camera motion control model. Algorithm 1 provides the detailed pseudocode for this transformation.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2412.07721/x5.png", "caption": "Figure 5: Qualitative Comparison with Training-free Methods. While PEEKABOO\u00a0[19] and FreeTraj\u00a0[32] can move the object coarsely within the bounding boxes generated from the trajectory, they lack control precision. In contrast, ObjCtrl-2.5D achieves higher trajectory alignment by extending the 2D trajectory to 3D and accurately transforming it into camera poses through a geometric projection algorithm (triangulation\u00a0[38, 39]).", "description": "This figure compares the object control capabilities of ObjCtrl-2.5D against two other training-free methods, PEEKABOO and FreeTraj.  PEEKABOO and FreeTraj use 2D trajectories and bounding boxes to control object movement, resulting in less precise control and alignment with the intended trajectory. ObjCtrl-2.5D, however, improves accuracy by expanding the 2D trajectory into 3D using depth information, and then converting the 3D trajectory into camera poses using a geometric projection (triangulation).  This allows for more precise control and better alignment of the object's movement to the desired trajectory.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.07721/x6.png", "caption": "Figure 6: Qualitative Comparison with Training-based Methods. Due to their training strategy, DragAnything\u00a0[58] tends to apply global movement to objects (both potted plants shift downward, despite only the right plant being specified to move), and DragNUWA\u00a0[64] often moves only part of the target object. In contrast, our proposed ObjCtrl-2.5D achieves precise, targeted object control thanks to its Layer Control Module. Additionally, ObjCtrl-2.5D is capable of performing more versatile object control when given a trajectory with a fixed spatial position (the green point in the second sample), such as front-to-back-to-front movement, while DragAnything\u00a0[58] generates a relatively static video.", "description": "This figure compares the object control capabilities of ObjCtrl-2.5D with two training-based methods: DragAnything and DragNUWA.  The comparison highlights how the training strategies of the existing methods lead to less precise and less versatile object control.  DragAnything applies global movement, unintentionally affecting all objects, even when only one object's movement is specified.  DragNUWA often only moves a portion of the targeted object. In contrast, ObjCtrl-2.5D, with its Layer Control Module, demonstrates precise and targeted control, even when the trajectory involves complex movements like front-to-back-to-front motion (as indicated by the green point).  This shows the superior versatility of ObjCtrl-2.5D.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.07721/x7.png", "caption": "Figure 7: Qualitative Results of Ablation Studies on LCM, Scale-wise Mask, and SWL. Without the Layer Control Module (LCM), ObjCtrl-2.5D applies motion control to the entire scene (a) rather than isolating the specific object (d). Removing the Shared Warping Latent (SWL) reduces controllability (c), while omitting the scale-wise mask may eliminate controllability (b).", "description": "This ablation study evaluates the individual contributions of the Layer Control Module (LCM), scale-wise mask, and Shared Warping Latent (SWL) to ObjCtrl-2.5D's object control capabilities.  (a) shows that without the LCM, the entire scene is affected by the motion control, leading to unnatural results. (b) demonstrates the importance of the scale-wise mask, as removing it results in a loss of object controllability. (c) shows reduced control accuracy without the SWL, highlighting its role in refining object movement.  (d) presents the complete ObjCtrl-2.5D model with all components, showcasing effective object motion control.", "section": "4.2. Ablation Study"}, {"figure_path": "https://arxiv.org/html/2412.07721/extracted/6059641/figures/user_study.png", "caption": "Figure 8: User Study. The majority of participants preferred the results obtained with ObjCtrl-2.5D over both training-free and training-based methods, attributing this preference to its better trajectory alignment and more natural motion generation.", "description": "A user study was conducted to compare the video generation results of ObjCtrl-2.5D against other state-of-the-art training-free and training-based methods. Participants were asked to evaluate the quality of object motion control in generated videos. The results show that a clear majority of participants favored ObjCtrl-2.5D, citing its superior trajectory alignment and more natural-looking object movements.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.07721/x8.png", "caption": "Figure 9: Qualitative Results of Ablation Studies on SWL and Copy-pasting Shared Latent. The Shared Warping Latent (SWL) in ObjCtrl-2.5D restricts the shared latent specifically within the object\u2019s warping areas, effectively avoiding unintended effects on the background while controlling the target object. In contrast, the copy-pasting mechanism used in FreeTraj\u00a0[32] coarsely applies the shared latent within bounding boxes, resulting in pronounced artifacts in the generated video.", "description": "This ablation study compares the performance of ObjCtrl-2.5D's Shared Warping Latent (SWL) method against FreeTraj's [32] copy-pasting approach.  SWL selectively applies shared latent information only to the object's warped region, thus preventing unintentional modifications to the background. In contrast, FreeTraj's method broadly applies the shared latent within bounding boxes, leading to noticeable artifacts in the generated video.  The figure visually demonstrates the superior control and reduced artifacts achieved by SWL.", "section": "4.2. Ablation Study"}, {"figure_path": "https://arxiv.org/html/2412.07721/x9.png", "caption": "Figure 10: Additional Results with User-Defined Camera Poses. ObjCtrl-2.5D allows both the object and background to be manipulated using user-defined camera poses, enabling effects like zooming in, as shown in these examples. More results can be found in the supplementary materials.", "description": "This figure showcases the versatility of ObjCtrl-2.5D in controlling both the foreground object and the background simultaneously using user-provided camera pose sequences.  Unlike methods restricted to pre-defined trajectories, ObjCtrl-2.5D allows for complex camera movements, such as zooming, panning, and other custom effects, demonstrating its adaptability and potential for creating highly customized video generation.", "section": "4.3. More Extensions"}, {"figure_path": "https://arxiv.org/html/2412.07721/x10.png", "caption": "Figure 11: Failure Cases. Due to the limitations of SVD\u00a0[3] in handling large motions, ObjCtrl-2.5D with high-speed camera poses results in the object fading out of the scene, leaving only the background. Interestingly, this outcome reveals potential for image inpainting applications, as seen in the last frames of the generated videos.", "description": "Figure 11 shows examples where ObjCtrl-2.5D fails to accurately control object motion when using high-speed camera movements.  The limitations of the underlying SVD video generation model in handling rapid changes in motion cause the target object to disappear from the scene in the later frames, leaving only the background visible. While this is a limitation of the current method, it also suggests a potential application for image inpainting techniques to fill in the missing object.", "section": "4.4. Limitation"}, {"figure_path": "https://arxiv.org/html/2412.07721/x11.png", "caption": "Figure 12: Guidelines for Drawing Trajectories. Drawing 2D trajectories directly on the depth image is recommended, as it ensures smoother depth transitions and avoids abrupt changes (refer to (a)) with the intrinsic depth information. Furthermore, trajectories can be drawn anywhere on the depth image to achieve appropriate depth values without affecting the movement of the target object.", "description": "Figure 12 illustrates the recommended approach for drawing 2D trajectories when using depth information for improved 3D object control in video generation.  Directly drawing the trajectory onto the depth map ensures smoother depth transitions between frames and prevents unnatural, abrupt changes in depth. The method is superior to drawing on the original image as the depth information is inherently smooth in the depth map. Additionally, the flexibility to draw the trajectory anywhere on the depth image, rather than restricting it to start on the object, allows for a more intuitive interaction and more appropriate depth values without impacting the movement of the target object.", "section": "A. More Details about 2D Trajectories to 3D"}, {"figure_path": "https://arxiv.org/html/2412.07721/x12.png", "caption": "Figure 13: Additional Results with User-Defined Camera Poses. ObjCtrl-2.5D can drive the same sample differently with different camera poses. We strongly recommend viewing the project page for dynamic results.", "description": "This figure shows additional results obtained using user-defined camera poses with ObjCtrl-2.5D.  It demonstrates the versatility of the model by showcasing how it can produce different video outputs from the same input image, simply by changing the camera movement instructions.  The top row shows examples of changing the camera's zoom level, while the bottom shows panning left, right, and zoom.  The variations highlight the precise control ObjCtrl-2.5D offers over object motion within a video generation context.", "section": "D. More Results of ObjCtrl-2.5D"}, {"figure_path": "https://arxiv.org/html/2412.07721/x13.png", "caption": "Figure 14: More Compared Results with Previous Methods. ObjCtrl-2.5D outperforms training-free methods (PEEKABOO\u00a0[19] and FreeTraj\u00a0[32]) in trajectory alignment and achieves more precise target object movement compared to training-based methods (DragNUWA\u00a0[64] and DragAnything\u00a0[58]), which often result in either global scene movement or partial object movement. We strongly recommend viewing the project page for dynamic results.", "description": "Figure 14 presents a qualitative comparison of object control results between ObjCtrl-2.5D and other state-of-the-art methods, both training-free and training-based.  The figure showcases three example scenarios, each demonstrating the differences in object control precision.  ObjCtrl-2.5D exhibits superior trajectory alignment and more precise control over the target object, while the other methods, particularly the training-free ones, either suffer from imprecise alignment or unintentionally affect the background or only parts of the target object. The training-based approaches, while achieving good alignment, sometimes inadvertently move the entire scene instead of just the specified object. This highlights ObjCtrl-2.5D's ability to effectively control object movement within a complex scene without affecting unwanted elements.", "section": "4. Experiments"}]