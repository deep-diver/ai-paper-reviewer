[{"content": "| Model | RealWorldQA [53] | MMStar [6] | MMBench [30] | SEEDBench [23] | ScienceQA [32] | MMT-Bench [58] | MathVista [33] | MathVerse [63] |\n|---|---|---|---|---|---|---|---|---|\n| Llama-3.2-11B-Vision [36] | 57.8 | 49.8 | 65.8 | 62.2 | 67.8 | 47.9 | 48.6 | 24.31 |\n| MiniCPM-V 2.6 [57] | 65.2 | 57.5 | 78.0 | 71.7 | 90.9 | 56.6 | 60.6 | 24.1 |\n| InternVL2-8B [7] | 64.4 | 61.5 | 79.4 | 76.2 | 89.2 | 54.8 | 58.3 | 30.3 |\n| GPT-4V [54] | 61.4 | 57.1 | 74.3 | 71.6 | 81.4 | 55.5 | 49.9 | 54.4 |\n| GeminiPro-Vision [47] | 67.5 | 42.6 | 68.1 | 64.3 | 80.6 | 55.1 | 36.0 | 35.3 |\n| LLaVA-v1.5-13B [28] | 55.3 | 32.8 | 68.6 | 68.1 | 72.2 | 45.7 | 26.4 | 12.7 |\n| ShareGPT4V-7B [5] | 56.9 | 33.0 | 69.5 | 69.4 | 69.4 | 45.1 | 25.7 | 17.4 |\n| InternLM2-XC2 [10] | 63.8 | 55.4 | 78.1 | 74.9 | 96.7 | 50.0 | 57.4 | 25.9 |\n| Qwen2-VL-7B [49] | 70.1 | 60.7 | 80.7 | 74.7 | 73.4(mm-only) | 60.4 | 61.4 | 25.8 |\n| **Qwen2-VL-7B+Critic-V** | **74.9(+4.8)** | 56.2(-4.5) | **82.8(+2.1)** | **76.5(+1.8)** | 74.5(mm-only, +1.1) | **62.0(+1.6)** | **73.2(+11.8)** | 32.9(+7.1) |\n| DeepSeek-VL-7B [31] | 58.1 | 37.1 | 73.5 | 70.2 | 61.7(mm-only) | 46.5 | 35.3 | 18.4 |\n| **DeepSeek-VL-7B+Critic-V** | 62.1(+4.0) | 41.4(+4.3) | 79.0(+5.5) | 70.6(+0.4) | 67.1(mm-only, +5.4) | 53.6(+7.1) | 53.1(+17.8) | 28.9(+10.5) |\n| LLaVA-v1.5-7B [28] | 50.7 | 32.2 | 68.4 | 65.6 | 60.8 | 36.0 | 37.8 | 26.0 |\n| **LLaVA-v1.5-7B+Critic-V** | 63.5(+12.8) | 38.4(+6.2) | 73.8(+5.4) | 70.1(+4.5) | 65.2(+4.4) | 47.4(+11.4) | 53.1(+15.3) | 30.5(+4.5) |", "caption": "Table 1: Main results of VLMs on various benchmarks, reported as percentage scores. The bolded scores indicate the best performance on each benchmark. Additionally, we report the score improvements of Qwen2-VL-7B and DeepSeek-VL-7B compared to their original scores with the application of our method (+Critic-V).", "description": "This table presents the performance comparison of various Vision-Language Models (VLMs) on eight different benchmark datasets.  The results are reported as percentage scores, with the highest score for each benchmark highlighted in bold.  The table includes both state-of-the-art closed-source and open-source models, allowing for a comprehensive analysis across different model scales and architectures. Notably, it also shows the performance improvement achieved by applying the Critic-V method to two specific models, Qwen2-VL-7B and DeepSeek-VL-7B, highlighting the effectiveness of the proposed approach.", "section": "3. Evaluation"}, {"content": "| Model | RealWorldQA [53] | MMStar [6] | MMBench [30] | SEEDBench [23] | ScienceQA [32] | MMT-Bench [58] |\n|---|---|---|---|---|---|---|\n| LLaVA-V1.5-7B | 50.7 | 32.2 | 68.4 | 65.6 | 60.8 | 36.0 |\n| +POVID [66] | 51.8 | 33.6 | 71.6 | 65.4 | 65.0 | 33.4 |\n| +CSR [67] | 51.8 | 32.4 | 70.6 | 65.4 | 66.0 | 33.2 |\n| +SIMA [50] | 49.3 | 32.6 | 70.6 | 65.2 | 64.2 | 34.0 |\n| +SCL [14] | 53.2 | 35.8 | 70.8 | 68.6 | **67.8** | 39.6 |\n| **+Critic-V(Ours)** | **63.5** | **38.4** | **73.8** | **70.1** | 65.2 | **49.7** |", "caption": "Table 2: Quantitative comparison of LLaVA-V1.5-7B with SCL and four baseline methods. The best results are highlighted in bold. The results underscore Critic-V\u2019s strong reasoning capabilities.", "description": "This table presents a quantitative comparison of the performance of the LLaVA-V1.5-7B model on various reasoning benchmarks, both with and without the application of different reasoning enhancement methods.  It compares LLaVA-V1.5-7B's performance to four baseline methods (POVID, CSR, SIMA, and SCL), as well as with the addition of the Self-Correcting Learning (SCL) technique.  The best-performing method for each benchmark is highlighted in bold. The results demonstrate the significant improvement in reasoning capabilities achieved by integrating the Critic-V framework, especially when compared to existing state-of-the-art methods.", "section": "3. Evaluation"}, {"content": "| Model | MathVista | MMT-Bench | MMBench |\n|---|---|---|---|\n| Qwen2-VL-7B | 61.4 | 60.4 | 80.7 |\n| Qwen2-VL-7B+ Self-Refine | 63.4 | 57.8 | 82.1 |\n| **Qwen2-VL-7B+Critic-V** | **73.2** | **62.0** | **82.8** |", "caption": "Table 3: Comparison between Self-Refine and Baseline. We conduct a comparison of Qwen2-VL-7B using Self-Refine, Critic-V, and baseline methods. The results demonstrate the superiority of Critic-V over Self-Refine.", "description": "This table compares the performance of the Qwen2-VL-7B model on three different settings: using the baseline model only, using the Self-Refine method, and using the Critic-V framework.  The results showcase that the Critic-V method significantly outperforms both the baseline and the Self-Refine method across various benchmarks, highlighting its effectiveness in improving the accuracy and efficiency of the model, particularly in complex reasoning tasks.", "section": "3.4 Ablation Study"}, {"content": "| Model | MathVista | MMT-Bench | MMBench |\n|---|---|---|---|\n| Qwen2-VL-7B | 61.4 | 60.4 | 80.7 |\n| Qwen2-VL-7B+ _special-prompt-only_ | 61.8 | 59.0 | 81.0 |\n| **Qwen2-VL-7B+Critic-V** | **73.2** | **62.0** | **82.8** |", "caption": "Table 4: Ablation of different prompts. We report the scores of each method, along with the respective increases or decreases relative to the original scores.", "description": "This table presents an ablation study analyzing the impact of different prompts on the performance of the Qwen2-VL-7B model across three benchmarks: MathVista, MMT-Bench, and MMBench.  It compares the original model's scores to scores obtained using a specialized prompt designed for the study and scores obtained with the full Critic-V framework.  The table shows the raw scores for each condition and calculates the percentage change from the original scores to highlight the effects of the different prompts on model performance.", "section": "3.4 Ablation Study"}, {"content": "| Evaluation Criterion | Question Description | Response Format |\n|---|---|---|\n| Coverage Analysis | Did the model identify all the hallucinations mentioned in the correct answer? | Yes / No |\n|  | Are there any significant hallucinations that were missed? |  |\n| Accuracy Assessment | Are the detected items genuine hallucinations (true positives)? | Yes / No |\n|  | Are there any false detections (false positives)? |  |\n| Precision of Description | How precise and clear are the model\u2019s descriptions of the detected hallucinations? | Yes / No |\n|  | Is the explanation specific enough to understand what exactly is wrong? |  |\n| Overall Effectiveness | How effective is this detection compared to an ideal detection? | Yes / No |\n|  | Does it provide practical value for hallucination identification? |  |\n| Comprehensive Evaluation | Based on your analysis, please provide a brief explanation of your evaluation. | Text Input |\n| Final Score | Based on the scoring criteria, provide a final score from 0 to 10. | 0-10 |", "caption": "Table 5: GPT-4o Evaluation for Erroneous Detection", "description": "This table details the rubric used by GPT-4 to evaluate the quality of critiques generated by Vision-Language Models (VLMs).  It outlines four key criteria for assessing the critiques: Coverage Analysis (whether the critique identified all hallucinations), Accuracy Assessment (whether the critique correctly identified hallucinations), Precision of Description (how clear and precise the descriptions of hallucinations are), and Overall Effectiveness (a holistic assessment of the critique's value). Each criterion uses a binary Yes/No response format except for Overall Effectiveness, which uses a numerical score from 0 to 10, based on a comprehensive evaluation of the critique.  This rubric is used to rank critiques and provide high-quality training data for the Critic Model within the Critic-V framework.", "section": "3. Evaluation"}, {"content": "| Benchmark | Average of token count | standard deviation of token count |\n|---|---|---|\n| MathVista | 40.64 | 51.42 |\n| MMBench | 50.26 | 41.54 |\n| MMStar | 39.39 | 44.96 |\n| MMT-Bench | 84.43 | 86.93 |\n| ScienceQA | 84.64 | 18.11 |\n| RealWorldQA | 30.29 | 10.70 |\n| SEED | 41.50 | 28.58 |\n| MathVerse | 43.13 | 34.76 |", "caption": "Table 6: Token consuming analysis of Critic-V across benchmarks.", "description": "This table presents a quantitative analysis of the token consumption by the Critic-V model across various benchmark datasets.  It shows the average and standard deviation of the token count for each benchmark, providing insights into the computational resource demands of the Critic-V model during its operation on different types of tasks and data. This information is crucial for understanding the efficiency and scalability of the proposed model for real-world applications.", "section": "3. Evaluation"}, {"content": "| Part | Max length | Min length | Avg Length |\n|---|---|---|---|\n| Question | 679 | 41 | 181.96 |\n| Chosen Critique | 714 | 5 | 60.48 |\n| Reject Critique | 1048 | 5 | 49.32 |", "caption": "Table 7: Details of training set. Number of tokens counted.", "description": "Table 7 provides a detailed breakdown of the training dataset's characteristics in terms of token counts. It shows the maximum, minimum, and average lengths (in tokens) for questions, chosen critiques, and rejected critiques, offering a comprehensive overview of the data's size and variability.", "section": "14. Details of Training data and Benchmarks for Evaluation"}, {"content": "| Benchmark | Description | #samples |\n|---|---|---|\n| MathVista | Multimodal Math QA | 1000(testmini) |\n| MMBench | Multimodal QA | 4329 |\n| MMStar | Multimodal QA | 1500 |\n| MMT-Bench | Multimodal QA | 3127 |\n| RealWorldQA | Multimodal QA | 764 |\n| ScienceQA | Multimodal/Text Scientific QA | 4241 |\n| SEED | Multimodal QA | 14233 |\n| MathVerse | Multimodal Math QA | 3940 |", "caption": "Table 8: Details of evaluation benchmarks.", "description": "Table 8 presents a detailed overview of the eight evaluation benchmarks used in the paper to assess the performance of the Critic-V framework.  Each benchmark's name, a concise description of its focus, and the total number of examples (samples) used for evaluation are provided. The benchmarks cover diverse aspects of multimodal reasoning, including mathematical problem-solving, scientific knowledge comprehension, and general-purpose question-answering tasks. This table helps readers understand the scope and nature of the evaluation process and the variety of reasoning capabilities being assessed.", "section": "3. Evaluation"}]