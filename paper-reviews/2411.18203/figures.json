[{"figure_path": "https://arxiv.org/html/2411.18203/x1.png", "caption": "Figure 1: Offline training of critic model and response supervision for VLM. ywisubscriptsuperscript\ud835\udc66\ud835\udc56\ud835\udc64y^{i}_{w}italic_y start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT is preferred critique and ylisubscriptsuperscript\ud835\udc66\ud835\udc56\ud835\udc59y^{i}_{l}italic_y start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT is disfavored critique.", "description": "This figure illustrates the offline training process for the Critic-V model, a key component of the proposed framework.  The training leverages a dataset of vision-language model (VLM) responses paired with human-generated critiques. Each response is associated with two critiques: a preferred critique (y\u1d62\u02b7) and a disfavored critique (y\u1d62\u02e1). The preferred critique represents a higher quality evaluation of the VLM response, while the disfavored critique is considered of lower quality.  The Critic model is trained to distinguish between these two types of critiques using Direct Preference Optimization (DPO). This process allows the Critic to learn to provide more constructive and nuanced feedback to improve the accuracy of VLM responses.", "section": "Method"}, {"figure_path": "https://arxiv.org/html/2411.18203/x2.png", "caption": "Figure 2: The scoring method combines GPT\u2019s evaluation with several predefined rules and the Jaccard index.", "description": "This figure illustrates the scoring mechanism used to assess the quality of critiques generated by different Vision-Language Models (VLMs). The process involves two stages. First, GPT (likely GPT-4) provides an initial evaluation of the critique.  Second, predefined rules and a Jaccard index are applied. These rules evaluate the critiques based on criteria such as comprehensiveness in identifying hallucinations and clarity of description. The Jaccard index measures the overlap between the set of hallucinations identified by the VLM and the ground truth.  These evaluations are then combined to produce a final score, enabling a quantitative comparison of critique quality and supporting the training of a preference-based Critic model.", "section": "2. Method"}, {"figure_path": "https://arxiv.org/html/2411.18203/x3.png", "caption": "Figure 3: The annotation framework for our critique on the VisualQA\u00a0(critique-VQA) dataset. We collect questions and images from various sources, then use GPT-4o to generate a fake answer and employ three different VLMs to identify incorrect elements. Finally, we apply our proposed scoring method to calculate preference between different assessments.", "description": "This figure illustrates the process of creating a dataset for training a critic model within a vision-language model (VLM) framework.  It starts by gathering questions and images from various VQA datasets. A large language model (GPT-40) then generates a 'fake' answer containing deliberate errors. Three different VLMs are used to independently identify these errors, producing critiques. Finally, a scoring mechanism, described in the paper, ranks these critiques, creating a preference dataset used for training the critic model, enabling the model to learn to differentiate between high-quality and low-quality critiques.", "section": "2. Method"}, {"figure_path": "https://arxiv.org/html/2411.18203/x4.png", "caption": "Figure 4: The comparison between GPT-4V and Qwen2-VL-7B+Critic-V across multiple benchmarks.", "description": "Figure 4 presents a radar chart comparing the performance of GPT-4V and Qwen2-VL-7B enhanced with Critic-V across eight benchmark datasets.  Each axis represents a different benchmark (RealWorldQA, MMStar, MMBench, SEEDBench, ScienceQA, MMT-Bench, MathVista, and MathVerse), and the distance of each model's point from the center reflects its performance on that specific benchmark.  The chart visually highlights the performance improvements achieved by integrating the Critic-V framework with the Qwen2-VL-7B model, showing its superiority over GPT-4V on most of the evaluated benchmarks.", "section": "3. Result and Analysis"}, {"figure_path": "https://arxiv.org/html/2411.18203/x5.png", "caption": "Figure 5: Case studies on evaluation samples from ScienceQA (left) and SEEDBench (right). Our Critic-V accurately identifies Salem as the capital of Oregon, unaffected by the initial incorrect answer, and correctly selects \u201cGuitars and keyboards\u201d as the answer in the right image.", "description": "Figure 5 presents two case studies showcasing Critic-V's ability to improve the accuracy of vision-language models (VLMs). The left panel shows a ScienceQA example where the VLM initially incorrectly identifies Portland as the capital of Oregon.  Critic-V corrects this error by providing feedback, leading to the correct answer, Salem. The right panel shows a SEEDBench example where the VLM initially incorrectly identifies \"bass guitar and drums\" as the instruments being played. Critic-V, again through feedback, corrects this to the accurate answer, \"guitars and keyboards.\"  These examples highlight how Critic-V refines the VLM's reasoning process, leading to more accurate responses even when the initial response is incorrect.", "section": "3.3 Case Study"}, {"figure_path": "https://arxiv.org/html/2411.18203/x6.png", "caption": "Figure 6: Training loss vs. training steps.", "description": "This plot shows the training loss curve for the Critic model over a series of training steps.  The x-axis represents the number of training steps completed, while the y-axis shows the value of the loss function. The loss function measures the difference between the Critic's predictions and the true labels, with lower loss values indicating better performance. The curve visualizes the model's learning progress, with the goal of minimizing the loss and improving the Critic's accuracy in evaluating the Reasoner's responses.", "section": "3. Evaluation"}, {"figure_path": "https://arxiv.org/html/2411.18203/x7.png", "caption": "Figure 7: Training accuracy vs. training steps.", "description": "This figure shows a plot illustrating the model's training accuracy over a series of training steps. The x-axis represents the training steps, and the y-axis shows the accuracy. The plot visually demonstrates the improvement in accuracy as the model undergoes more training iterations.", "section": "3. Evaluation"}, {"figure_path": "https://arxiv.org/html/2411.18203/x8.png", "caption": "Figure 8: Learning rate vs. training steps.", "description": "This figure shows the learning rate's change over training steps.  The plot illustrates how the learning rate adjusted during the training process of the Critic-V model.  The x-axis represents the training steps, and the y-axis shows the learning rate value at each step.  The specific learning rate schedule used (e.g., constant, linear decay, cosine annealing) can be inferred from the curve's shape.  Analyzing this curve provides insights into the optimization process and helps determine if the learning rate was appropriately tuned for the model's convergence.", "section": "3. Evaluation"}, {"figure_path": "https://arxiv.org/html/2411.18203/x9.png", "caption": "Figure 9: A math example. Fake Answer indicates the answer is inserted some errors by GPT-4o.", "description": "Figure 9 presents a geometry problem and its solution. The correct answer is derived logically from the given information, while the 'Fake Answer' contains errors introduced by GPT-40. The provided critiques highlight these errors, distinguishing between valid and invalid reasoning steps. This example demonstrates Critic-V's ability to identify and correct errors in multimodal reasoning, particularly focusing on identifying inconsistencies in problem-solving processes.", "section": "3.3 Case Study"}, {"figure_path": "https://arxiv.org/html/2411.18203/x10.png", "caption": "Figure 10: A real-world example of public market signage. Fake Answer indicates the answer is inserted some errors by GPT-4o.", "description": "Figure 10 shows a real-world example from the critique-VQA dataset. The image depicts a public market sign.  The original question asked about how the sign was supported. A large language model (LLM) provided a fake answer that included fabricated details (hallucinations) like a golden octopus supporting the sign and a clock providing fortune cookie messages. The 'Chosen Critique' highlights the inaccuracies in the fake answer, pointing out the nonsensical additions.", "section": "3.3 Case Study"}, {"figure_path": "https://arxiv.org/html/2411.18203/x11.png", "caption": "Figure 11: A driving car example. Fake Answer indicates the answer is inserted some errors by GPT-4o.", "description": "This figure showcases an example from the critique-VQA dataset focusing on a driving scene. The initial response, generated by a Vision-Language Model (VLM), contains inaccuracies introduced by GPT-40.  The image depicts a road with a speed limit sign, and the VLM's answer incorrectly incorporates additional details not present in the image, such as lane markers painted blue for bike lanes and a caution sign about wild ponies. This demonstrates the kind of errors and hallucination that Critic-V aims to address by providing constructive feedback to refine the VLM's reasoning.", "section": "3.3 Case Study"}]