[{"figure_path": "https://arxiv.org/html/2503.08689/x1.png", "caption": "Figure 1: Comparative analysis of Video-MME [7] when implementing attention-based token assignment methods AIM [50] and FrameFusion [9], alongside our proposed query-oriented QuoTA within LLaVA-Video-7B [48] and LLaVA-OV-7B [13] across varied relative visual token budgets. QuoTA demonstrates superior efficacy while exhibiting consistent performance enhancement across diverse token budget configurations relative to the baseline.", "description": "Figure 1 presents a comparative performance analysis of different token assignment methods on the Video-MME benchmark.  It specifically examines the impact of three techniques \u2013 AIM [50], FrameFusion [9], and the proposed QuoTA \u2013 when integrated with two large video-language models (LVLMs): LLaVA-Video-7B [48] and LLaVA-OV-7B [13]. The x-axis represents the relative visual token budget (percentage of total tokens used), while the y-axis shows the overall performance on Video-MME.  The figure demonstrates that QuoTA consistently outperforms both AIM and FrameFusion across various token budget settings, showcasing its effectiveness in enhancing the performance of the base LVLMs without requiring additional computational resources.", "section": "Abstract"}, {"figure_path": "https://arxiv.org/html/2503.08689/x2.png", "caption": "Figure 2: The framework of QuoTA. Initially, a dynamic frame sampler extracts T\ud835\udc47Titalic_T frames from the video based on its duration, which are subsequently processed by ViT to generate visual embeddings \ud835\udc04\ud835\udc04\\bm{\\mathrm{E}}bold_E. Then, the based LVLM decouples the input query using Chain-of-Thoughts [36] reasoning into a decoupled clue to generate frame-wise importance scores through scoring LVLM in parallel, thus evaluating the relevance to the query of each frame. Finally, a token assigner rescales the frame embeddings to \ud835\udc04^bold-^\ud835\udc04\\bm{\\mathrm{\\hat{E}}}overbold_^ start_ARG bold_E end_ARG based on these importance scores.", "description": "QuoTA's framework consists of four stages: First, a dynamic frame sampler extracts frames from the video based on its duration. Second, a Vision Transformer (ViT) processes these frames to produce visual embeddings. Third, the \"based LVLM\" uses Chain-of-Thought reasoning to transform the input query into a more specific question. This question is then used to prompt a lightweight \"scoring LVLM\" to generate frame-wise importance scores, evaluating each frame's relevance to the query.  Finally, a token assigner uses these scores to adjust the visual embeddings, effectively weighting them according to their relevance to the query, producing rescaled frame embeddings.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.08689/x3.png", "caption": "Figure 3: Qualitative result shown in Video-MME [7] benchmark when applying QuoTA with LLaVA-Video-7B [48]. The video frames with a blue border are query-oriented keyframes, and the bar chart shows the normalized scores of QuoTA for each frame.", "description": "This figure displays a qualitative analysis of QuoTA's performance on the Video-MME benchmark using the LLaVA-Video-7B model.  It showcases two example queries and their corresponding video segments.  Frames deemed important by QuoTA (query-oriented keyframes) are highlighted with blue borders.  A bar chart visually represents the normalized importance scores assigned by QuoTA to each frame within the video segment, illustrating how the model prioritizes certain frames based on their relevance to the query. This visualization helps understand how QuoTA focuses the model's attention on the most pertinent parts of the video for accurate response generation.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.08689/x4.png", "caption": "Figure 4: CoT-driven decouple prompt for object list.", "description": "This figure shows the detailed prompt engineering strategy used in the paper for object list-based chain of thought reasoning. The prompt guides the large language model (LLM) to first determine if identifying specific objects is necessary to answer the question. If yes, the model is then prompted to list those objects, followed by a filtering step to eliminate any abstract concepts and leave only physical entities.  The example prompts help illustrate this three-step process and ensure the LLM focuses on relevant objects in the video frames.", "section": "3.3. CoT-Driven Query Decouple"}, {"figure_path": "https://arxiv.org/html/2503.08689/x5.png", "caption": "Figure 5: CoT-driven decouple prompt for video event.", "description": "This figure demonstrates the Chain-of-Thought (CoT) driven query decoupling prompt specifically designed for video events.  The prompt guides the large language model (LLM) to break down a complex query into a simpler, more focused question about the presence of key video events or elements within a frame.  This process aids in generating more accurate frame-level importance scores by focusing the LLM on specific, easily identifiable aspects of the video content rather than the entire, potentially ambiguous original query.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.08689/x6.png", "caption": "Figure 6: Sub-task results shown in Video-MME [7] benchmark when applying distinct frame scoring strategy of LLaVA-Video-7B [48].", "description": "Figure 6 presents a detailed breakdown of the performance achieved by various frame scoring strategies within the LLaVA-Video-7B model on the Video-MME benchmark.  It showcases a comparison of different approaches for assigning importance scores to video frames, including methods based on LVLMs with and without Chain-of-Thought (CoT) reasoning, and even CLIP-based scoring. The graph displays the performance on individual sub-tasks within Video-MME, offering a granular view of how each method performs in aspects such as temporal perception, spatial perception, action recognition, and more. This allows for a direct comparison of the effectiveness of different query-oriented token selection strategies and their relative strengths and weaknesses across a range of video understanding tasks.", "section": "4.4. Ablation Studies"}]