{"references": [{" publication_date": "2024", "fullname_first_author": "Tianqi Du", "paper_title": "On the role of discrete tokenization in visual representation learning", "reason": "This paper is highly relevant as it directly addresses the topic of discrete tokenization, a key component of the proposed method.  The research explores the role of tokenization in visual representation learning and introduces techniques for effective tokenization in this specific domain. While focused on visual data, the techniques discussed are relevant and applicable to the speech tokenization problem tackled by the authors. The paper's analysis of the impact of different tokenization strategies and their effects on downstream performance is relevant and highly valuable for understanding the significance of efficient and appropriate tokenization.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Alexandre D\u00e9fossez", "paper_title": "High fidelity neural audio compression", "reason": "This paper is important due to its introduction of EnCodec, a state-of-the-art neural audio codec that uses residual vector quantization (RVQ) for high-fidelity speech compression.  The authors' work on EnCodec is highly relevant, as the proposed DM-Codec model builds upon several architectural and methodological concepts of EnCodec, particularly its use of RVQ and a streamlined encoder-decoder framework.  The EnCodec work demonstrates the utility and effectiveness of the core components adopted in DM-Codec, establishing a benchmark for comparison and providing a foundation for further improvements.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Xin Zhang", "paper_title": "Speechtokenizer: Unified speech tokenizer for speech large language models", "reason": "This work directly addresses the challenge of speech tokenization, introducing a unified speech tokenizer that combines acoustic and semantic representations. The paper's approach, employing semantic distillation from HuBERT, is relevant to the authors' approach of using distillation for incorporating contextual information.  The results reported in this paper provide a crucial benchmark for comparison, establishing a clear reference point for assessing the performance improvement gained through the use of DM-Codec.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Zeqian Ju", "paper_title": "Naturalspeech 3: Zero-shot speech synthesis with factorized codec and diffusion models", "reason": "This paper presents FACodec, another state-of-the-art speech tokenization model, and is included as a baseline model. The methodology used in FACodec, particularly the use of a factorized vector quantizer for disentangling speech representations, is an important contribution to the field. The paper provides additional benchmark results against which to evaluate the performance of the proposed DM-Codec, allowing for a comprehensive and robust comparison against existing methodologies.", "section_number": 1}, {" publication_date": "2021", "fullname_first_author": "Neil Zeghidour", "paper_title": "SoundStream: An end-to-end neural audio codec", "reason": "This paper is highly relevant due to its introduction of SoundStream, another state-of-the-art neural audio codec that leverages RVQ for efficient and high-fidelity speech synthesis.  The authors' approach to SoundStream is closely related to the DM-Codec model.  Like DM-Codec, SoundStream employs RVQ and a streamlined encoder-decoder framework, demonstrating the potential of using these techniques for improved speech representation and generation. This prior work establishes a baseline for comparison against which the authors assess their improvements and highlights the relevance of the chosen architectural components for speech processing tasks.", "section_number": 1}, {" publication_date": "2021", "fullname_first_author": "Wei-Ning Hsu", "paper_title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units", "reason": "This paper introduces HuBERT, a self-supervised learning model for speech representation that is highly relevant to DM-Codec due to its use of speech representations in the training of the model.  The self-supervised learning approach is particularly relevant as the proposed DM-Codec utilizes a self-supervised model for incorporating semantic features.  The HuBERT model is a crucial component in other state-of-the-art methods, providing a foundation for the DM-Codec's approach and providing a strong baseline for comparison.", "section_number": 1}, {" publication_date": "2020", "fullname_first_author": "Alexei Baevski", "paper_title": "wav2vec 2.0: A framework for self-supervised learning of speech representations", "reason": "This paper introduces wav2vec 2.0, a prominent self-supervised learning model for speech representation, which serves as a baseline and is used as a component in DM-Codec. Wav2vec 2.0's approach to self-supervised learning of speech representations is critical to the overall design of DM-Codec, which utilizes a self-supervised speech model for incorporating semantic features. This method provides another state-of-the-art baseline, improving the context and enhancing comparison.", "section_number": 1}, {" publication_date": "2019", "fullname_first_author": "Alexei Baevski", "paper_title": "vq-wav2vec: Self-supervised learning of discrete speech representations", "reason": "This paper is highly relevant as it introduces vq-wav2vec, a model that utilizes vector quantization for speech representation.  The concept of using vector quantization is directly applicable to DM-Codec, which uses RVQ for quantizing speech features.  Understanding the methods and techniques presented in vq-wav2vec provides a necessary foundation for evaluating the improvements made in DM-Codec.", "section_number": 1}, {" publication_date": "2019", "fullname_first_author": "Jacob Devlin", "paper_title": "Bert: Pre-training of deep bidirectional transformers for language understanding", "reason": "This paper is crucial because it introduces BERT, a powerful language model widely used in natural language processing. The authors use a BERT-based language model as part of DM-Codec's architecture for incorporating contextual representations.  Leveraging the strengths of BERT is essential to the proposed method.  The work serves as a foundation upon which DM-Codec's language modeling component is based, providing a solid basis for evaluating the efficacy of DM-Codec's approach.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "Marco Tagliasacchi", "paper_title": "SEANet: A Multi-Modal Speech Enhancement Network", "reason": "This paper is chosen because the encoder-decoder architecture in DM-Codec is based on SEANet.  The use of SEANet as the foundation for DM-Codec's architecture is critical to the proposed method.  By using a well-established architecture as a base, the authors are able to focus on the novel distillation methods and integration of multimodal representations, rather than developing a completely new architecture from scratch.  The paper serves as an essential component of the overall architecture.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "Jungil Kong", "paper_title": "Hifi-gan: generative adversarial networks for efficient and high fidelity speech synthesis", "reason": "This paper is chosen due to the adoption of the multi-discriminator framework from HiFi-GAN in the DM-Codec architecture.  The multi-discriminator framework of HiFi-GAN significantly contributes to the overall quality and realism of the generated speech in DM-Codec, thus making the paper crucial for evaluating the performance and effectiveness of DM-Codec.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Zal\u00e1n Borsos", "paper_title": "AudioLM: a language modeling approach to audio generation", "reason": "This paper introduces AudioLM, a state-of-the-art model that generates audio.  While not directly related to tokenization, this study provides a valuable comparison point for speech quality and naturalness.  The comparison against AudioLM in terms of speech quality allows for an indirect evaluation of the success of the proposed model in improving the quality of synthesized speech.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Kevin Clark", "paper_title": "Electra: Pre-training text encoders as discriminators rather than generators", "reason": "This paper is highly relevant because it introduces ELECTRA, a strong language model, tested in the ablation study.  ELECTRA is chosen as a variant in the ablation study, which helps to demonstrate the effectiveness of the proposed distillation methods using different models. This paper offers a valuable comparison point and enables more nuanced analysis of DM-Codec's performance compared to other methods.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Alec Radford", "paper_title": "Robust speech recognition via large-scale weak supervision", "reason": "This paper introduces a novel approach to robust speech recognition using large-scale weak supervision. It's crucial because the authors use a similar model for speech-to-text transcription in their experimental setup. Using a similar model provides consistent results across different parts of their experimental setup, improving the reliability and reproducibility of the findings. The paper enhances the methodological soundness of the experimental section.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Vassil Panayotov", "paper_title": "Librispeech: An asr corpus based on public domain audio books", "reason": "This paper is seminal because it introduces the LibriSpeech corpus, a large-scale speech recognition dataset.  The authors use LibriSpeech in their experimental setup, providing a solid foundation for comparison of DM-Codec's performance against other speech tokenizers. The use of this widely-accepted benchmark dataset ensures the validity and generalizability of the results.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Phillip K Rubenstein", "paper_title": "AudioPaLM: A large language model that can speak and listen", "reason": "This paper introduces AudioPaLM, a state-of-the-art model capable of both speech generation and comprehension. It is included to show a significant advancement in the integration of language and speech models.  While not directly a speech tokenizer, this paper shows a direction the field is taking, and DM-Codec aligns with the core ideas of integrating LMs and speech.  The paper is relevant for the discussion of broader impacts and related work.", "section_number": 5}, {" publication_date": "2021", "fullname_first_author": "Yu-An Chung", "paper_title": "W2v-bert: Combining contrastive learning and masked language modeling for self-supervised speech pre-training", "reason": "This paper is important because it introduces W2v-BERT, a model for self-supervised speech representation learning.  The paper's approach is related to the authors' self-supervised learning component in their model. The combined self-supervised learning component used in DM-Codec is relevant to the methods and findings of this paper, providing an additional baseline against which the performance and novelty of DM-Codec can be evaluated.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Ziqiang Zhang", "paper_title": "SpeechLM: Enhanced speech pre-training with unpaired textual data", "reason": "This paper is relevant because it introduces SpeechLM, a large language model for speech processing.  The paper is included because it discusses a recent and relevant development in speech processing; the combination of language models and speech models.  The paper highlights the growing integration of language models and speech, aligning with the core theme and motivation of DM-Codec.  The work offers an additional comparison point and contextualizes the authors' contribution within the larger research landscape.", "section_number": 5}]}