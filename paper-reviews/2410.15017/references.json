{"references": [{" publication_date": "2021", "fullname_first_author": "Wei-Ning Hsu", "paper_title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units", "reason": "This paper is highly relevant because it introduces HuBERT, a self-supervised speech representation learning model.  HuBERT is used as a component in DM-Codec's combined LM and SM-guided distillation approach, and thus its contribution to the field of speech representation learning is directly relevant to the effectiveness of DM-Codec.  The choice to use HuBERT as a building block for the innovative technique of DM-Codec highlights its significance within the paper's methodological design.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "Alexei Baevski", "paper_title": "wav2vec 2.0: A framework for self-supervised learning of speech representations", "reason": "This paper is crucial because it introduces wav2vec 2.0, a seminal work in self-supervised speech representation learning that is leveraged in DM-Codec for speech-to-text transcription and semantic feature extraction.  The use of wav2vec 2.0 as an essential component in the DM-Codec framework underscores its contribution to the paper's design.  wav2vec 2.0's capabilities in extracting semantic features directly impact DM-Codec's ability to capture the full spectrum of information during the speech tokenization process.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Alexandre D\u00e9fossez", "paper_title": "High fidelity neural audio compression", "reason": "This paper is important because it introduces EnCodec, a high-fidelity neural audio codec that forms the basis of the encoder-decoder framework in DM-Codec. EnCodec's architecture is adapted and incorporated into DM-Codec, thus highlighting the critical contribution of EnCodec to the overall model's performance and efficiency.  Understanding the core elements of EnCodec is vital to understanding the functioning of DM-Codec, and the direct adaptation of EnCodec's features is a significant part of DM-Codec's novelty.", "section_number": 2}, {" publication_date": "2019", "fullname_first_author": "Alexei Baevski", "paper_title": "vq-wav2vec: Self-supervised learning of discrete speech representations", "reason": "This work is highly relevant as it introduces the concept of self-supervised learning of discrete speech representations, forming the foundation upon which the semantic distillation process in DM-Codec is built. The method is directly used as the building block for generating semantic representations. The use of this technique is integral to the performance enhancement of DM-Codec and understanding it is crucial to fully grasping DM-Codec's methodology.", "section_number": 2}, {" publication_date": "2019", "fullname_first_author": "Jacob Devlin", "paper_title": "Bert: Pre-training of deep bidirectional transformers for language understanding", "reason": "This paper is highly relevant because it introduces BERT, a pre-trained language model that is employed in DM-Codec's LM-guided and combined LM and SM-guided distillation approaches.  BERT provides the contextual representations used in DM-Codec's distillation process; thus, BERT's contribution to the field of language modeling is integral to the understanding of DM-Codec's functionality and performance.  The choice to incorporate BERT reflects its established prowess in contextual understanding.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "Yu-An Chung", "paper_title": "W2v-bert: Combining contrastive learning and masked language modeling for self-supervised speech pre-training", "reason": "This paper presents W2v-BERT, a model that combines contrastive learning and masked language modeling for self-supervised speech pre-training.  While not directly used as a pre-trained model, this work is highly relevant because it reflects the background of self-supervised learning in speech representation and is closely related to the underlying principles driving the SM-guided distillation in DM-Codec.  The use of this method is directly related to the methodology used in the second distillation approach.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "Jungil Kong", "paper_title": "Hifi-gan: generative adversarial networks for efficient and high fidelity speech synthesis", "reason": "This paper introduces HiFi-GAN, a generative adversarial network used in DM-Codec's multi-discriminator framework. HiFi-GAN's architecture and training methodology significantly influence DM-Codec's capacity to produce high-quality, realistic speech during the reconstruction process. The design and use of discriminators within the DM-Codec model is crucial for achieving high-fidelity speech and understanding HiFi-GAN is critical to understanding that aspect of DM-Codec.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Neil Zeghidour", "paper_title": "SoundStream: An end-to-end neural audio codec", "reason": "This paper is important because it introduces SoundStream, an end-to-end neural audio codec which is used in DM-Codec as an initial inspiration for the encoder-decoder framework.  While SoundStream is not directly integrated into DM-Codec's architecture, its foundational contribution to neural audio codecs and the overall methodology directly influences the design and implementation of DM-Codec, particularly in shaping the fundamental architecture and serving as a critical inspiration for this design.", "section_number": 2}, {" publication_date": "2024a", "fullname_first_author": "Xin Zhang", "paper_title": "Speechtokenizer: Unified speech tokenizer for speech large language models", "reason": "This paper introduces SpeechTokenizer, a key baseline model compared against DM-Codec in the experimental evaluation. SpeechTokenizer is directly compared against the proposed DM-Codec, thus, understanding its capabilities and limitations are crucial to fully understanding the performance enhancement of DM-Codec.   The comparative results demonstrate the effectiveness of DM-Codec against a prominent existing model, showcasing its strengths and improvements in different aspects of speech tokenization.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Zeqian Ju", "paper_title": "Naturalspeech 3: Zero-shot speech synthesis with factorized codec and diffusion models", "reason": "This paper introduces FACodec, another crucial baseline model compared against DM-Codec.  Similar to SpeechTokenizer, FACodec is a direct point of comparison which helps evaluate and understand the enhanced performance of DM-Codec compared to an existing model in different aspects of speech tokenization.  The direct comparison against FACodec strengthens the claim of performance improvement and allows for a precise quantification of the improvement.", "section_number": 4}, {" publication_date": "2015", "fullname_first_author": "Vassil Panayotov", "paper_title": "Librispeech: An asr corpus based on public domain audio books", "reason": "This paper is highly important because it introduces the LibriSpeech corpus, the dataset used for training and evaluating DM-Codec.  The choice of LibriSpeech as the benchmark dataset directly influences the experimental results and comparative analysis. The characteristics and properties of this dataset are vital to understanding the scope and limitations of the experimental results, rendering this reference highly critical to interpreting the findings.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Paul K Rubenstein", "paper_title": "Audiopalm: A large language model that can speak and listen", "reason": "This paper presents AudioPalm, which while not directly utilized in DM-Codec, is a significant advancement in the field of speech modeling that integrates large language models.  The paper highlights a relevant research area and contextualizes the trend of integrating textual LMs in speech processing.  This contextualization within the related work section provides a clearer understanding of the broader significance and future directions of research in speech tokenization.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Tianqi Du", "paper_title": "On the role of discrete tokenization in visual representation learning", "reason": "This paper explores the role of discrete tokenization in visual representation learning, providing a broader perspective on the importance of tokenization in machine learning. This paper demonstrates how discrete tokenization is essential for the success of various machine learning algorithms. This is indirectly relevant to DM-Codec as it highlights the general need for effective tokenization approaches, which is central to the proposed model.", "section_number": 1}, {" publication_date": "2021", "fullname_first_author": "Alexandre D\u00e9fossez", "paper_title": "High fidelity neural audio compression", "reason": "This paper is significant because it introduces EnCodec, a model utilized for comparison against the proposed DM-Codec.  EnCodec serves as one of the primary baselines against which DM-Codec's performance is compared. Understanding EnCodec's capabilities and limitations are thus crucial to interpreting the findings reported for DM-Codec.  This comparative analysis helps establish the improvements achieved by DM-Codec in various speech metrics.", "section_number": 4}, {" publication_date": "2019", "fullname_first_author": "Leyuan Sheng", "paper_title": "High-quality speech synthesis using super-resolution mel-spectrogram", "reason": "This paper, while not directly related to the methodology of DM-Codec, provides context for the challenges of traditional speech tokenization methods. It utilizes mel-spectrograms, a technique mentioned in the introduction as insufficient for capturing the full complexity of speech. The contrast between this traditional method and the advanced multimodal approach of DM-Codec underscores the significance of the proposed model. This reference serves to highlight the limitations of previous approaches before introducing DM-Codec's novel solution.", "section_number": 1}, {" publication_date": "2018", "fullname_first_author": "Lauri Juvela", "paper_title": "Speech waveform synthesis from mfcc sequences with generative adversarial networks", "reason": "This paper is relevant as it highlights the use of MFCCs (Mel-frequency cepstral coefficients) in speech waveform synthesis, a technique that DM-Codec improves upon.  The introduction notes the limitations of MFCCs for capturing the complexity of speech, whereas DM-Codec is presented as a solution that addresses these limitations.   This reference provides additional context and emphasizes the advancement that DM-Codec represents.", "section_number": 1}, {" publication_date": "2021", "fullname_first_author": "Phillip Rust", "paper_title": "How good is your tokenizer? on the monolingual performance of multilingual language models", "reason": "This work is important because it directly addresses the importance of tokenization in language models, a crucial aspect that underlies DM-Codec's functionality.  The paper discusses the importance of tokenizer selection and its impact on the overall performance of language models, making it highly relevant to the work on speech tokenization.  This is a key contextual paper that further strengthens the introduction's argument regarding the importance of tokenization.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Ziqiang Zhang", "paper_title": "Speechlm: Enhanced speech pre-training with unpaired textual data", "reason": "This paper is relevant because it presents a novel approach for speech pre-training that uses unpaired textual data. While not directly incorporated into DM-Codec, this approach exemplifies the advancements in speech representation learning.  The reference highlights a different yet parallel method of speech modeling that contributes to the broader context of DM-Codec and showcases the various approaches to this complex problem.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Ziqiang Zhang", "paper_title": "Speechtokenizer: Unified speech tokenizer for speech large language models", "reason": "This paper is relevant because it presents SpeechTokenizer, a state-of-the-art speech tokenizer, which DM-Codec aims to surpass. The experimental section directly compares the performance of DM-Codec against this baseline model and this comparison forms a critical aspect of assessing DM-Codec's contribution to the field. By providing a direct performance comparison, it enables a clear evaluation of the proposed method.", "section_number": 4}]}