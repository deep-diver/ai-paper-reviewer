[{"content": "| Dataset Name | Number of Clips | Number of Frames per Clip | Number of Objects per Clip | Number of Object Classes | Number of Instances per Object Class | Instance Captions | Instance Masks | Range of Average Instance Mask Size Per Video (%) |\n|---|---|---|---|---|---|---|---|---|\n| TGVE [59] & TGVE+ [47] | 76 | 32-128 | 1-2 | No Info | 1-2 | \u2713 | \u00d7 | No Masks |\n| EVA [63] | 26 | 16-32 | 1-2 | No Info | 1-2 | \u2713 | \u2713 | No Info |\n| MIVE Dataset (Ours) | 200 | 12-46 | 3-12 | 110 | 1-20 | \u2713 | \u2713 | 0.01~98.68 |", "caption": "Table 1: \nComparison between our multi-instance video editing dataset with other text-guided video editing datasets.", "description": "This table presents a comparison of several video editing datasets, including TGVE, TGVE+, EVA, and the proposed MIVE dataset. The comparison focuses on characteristics relevant to multi-instance video editing, such as the number of clips, frames per clip, objects per clip, object classes, instances per object class, presence of instance captions and masks, and the range of average instance mask sizes. This highlights the advantages of the MIVE dataset, particularly its larger size and richer annotations, making it suitable for evaluating multi-instance video editing.", "section": "4.1 MIVE Dataset Construction"}, {"content": "| Method | Venue | Editing | GTC \u2191 | GTF \u2191 | FA \u2191 | LTC \u2191 | LTF \u2191 | IA \u2191 | CIA (**Ours**) \u2191 | BP \u2193 | TC \u2191 | TF \u2191 | Leakage \u2191 | User Study | User Study | User Study |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---| \n| ControlVideo [70] | ICLR\u201924 | Global | **0.9743** | **0.2738** | **0.8856** | **0.9548** | **0.1960** | **0.4941** | 0.4967 | 72.8690 | 6.97 | **14.00** | 6.26 | - | - | - |\n| FLATTEN [11] | ICLR\u201924 | Global | 0.9679 | 0.2388 | 0.2637 | 0.9507 | 0.1881 | 0.2469 | 0.5111 | 62.8136 | **32.32** | 2.45 | **9.74** | - | - | - |\n| RAVE [26] | CVPR\u201924 | Global | 0.9675 | 0.2727 | 0.5777 | **0.9551** | 0.1869 | 0.3512 | 0.4945 | 64.8703 | 10.45 | 3.61 | 4.52 | - | - | - |\n| TokenFlow [16] | ICLR\u201924 | Global | **0.9686** | 0.2569 | 0.5622 | 0.9478 | 0.1868 | 0.3501 | 0.5307 | 68.6688 | 7.61 | 3.16 | 4.26 | - | - | - |\n| FreSCo [62] | CVPR\u201924 | Global | 0.9541 | 0.2527 | 0.4202 | 0.9324 | 0.1860 | 0.2962 | 0.5172 | 85.1843 | 3.55 | 1.81 | 3.42 | - | - | - |\n| GAV [25] | ICLR\u201924 | Local, Multiple | 0.9660 | 0.2566 | 0.5504 | 0.9514 | 0.1893 | 0.3703 | **0.5492** | **60.0773** | 8.90 | 7.74 | **9.74** | - | - | - |\n| **MIVE (Ours)** | - | Local, Multiple | 0.9604 | **0.2750** | **0.8557** | 0.9478 | **0.2138** | **0.6419** | **0.7100** | **54.3452** | **30.20** | **67.23** | **62.06** | - | - | - |", "caption": "Table 2: \u00a0Quantitative comparison for multi-instance video editing. The best and second best scores are shown in red and blue, respectively.", "description": "This table presents quantitative results comparing the proposed MIVE framework with other state-of-the-art zero-shot video editing methods. It uses several metrics, including Global Temporal Consistency (GTC), Global Textual Faithfulness (GTF), Frame Accuracy (FA), Local Temporal Consistency (LTC), Local Textual Faithfulness (LTF), Instance Accuracy (IA), Cross-Instance Accuracy (CIA), and Background Preservation (BP) to evaluate the performance of each method on multi-instance video editing tasks. The best and second-best scores for each metric are highlighted in red and blue, respectively. The table demonstrates that MIVE outperforms other methods on key multi-instance video editing metrics (LTF, IA, CIA, and BP) while maintaining competitive performance on temporal consistency metrics.", "section": "5.1. Experimental Results"}, {"content": "| Methods |     | LTC \u2191 | LTF \u2191 | IA \u2191 | CIA \u2191 | BP \u2193 |\n| :------ | :-- | :---- | :---- | :--- | :---- | :--- |\n| DMS | Only NPS | 0.9460 | 0.2072 | 0.5587 | 0.6663 | 54.6597 |\n|       | Only LPS | **0.9483** | 0.2068 | 0.5716 | 0.6688 | **50.8549** |\n|       | LPS + NPS w/o Re-Inversion | **0.9485** | **0.2080** | **0.5776** | **0.6783** | **52.3240** |\n|       | **Ours, Full** | 0.9478 | **0.2138** | **0.6419** | **0.7100** | 54.3452 |\n| IPR | No Modulation [44] | **0.9535** | 0.2060 | 0.5225 | 0.6553 | **50.1319** |\n|       | Dense Diffusion [28] | **0.9482** | **0.2136** | **0.6215** | 0.6891 | 59.2100 |\n|       | **Ours, Full** | 0.9478 | **0.2138** | **0.6419** | **0.7100** | **54.3452** |", "caption": "Table 3: \nAblation study results on DMS (Sec.\u00a03.2) and IPR (Sec.\u00a03.3).\nLPS and NPS denotes Latent Parallel Sampling and Noise Parallel Sampling, respectively.\nThe best and second best scores are shown in red and blue, respectively.", "description": "This table presents the ablation study results on Disentangled Multi-instance Sampling (DMS) and Instance-centric Probability Redistribution (IPR), which are two key components of the MIVE framework.  It evaluates the performance of different DMS and IPR configurations using metrics such as Local Temporal Consistency (LTC), Local Textual Faithfulness (LTF), Instance Accuracy (IA), Cross-Instance Accuracy (CIA), and Background Preservation (BP).  The results show that using Latent Parallel Sampling (LPS) and Noise Parallel Sampling (NPS) in conjunction with a re-inversion step leads to the best overall performance in terms of faithfulness and leakage reduction for MIVE. The comparison between the baseline cross-attention mechanism without modulation and two other attention modulation methods demonstrates that the proposed Instance-centric Probability Redistribution (IPR) is more effective in achieving faithful editing with fewer artifacts.  The table uses red and blue colors to highlight the best and second-best performing configurations, respectively.", "section": "5.2. Ablation Studies"}, {"content": "| Use Case | Number of Clips | Number of Frames per Clip | Number of Objects per Clip | Number of Object Classes | Number of Instances per Object Class | Range of Average Instance Mask Size Per Video (%) |\n|---|---|---|---|---|---|---| \n| MIVE Dataset ( *full set*) | 200 | 12-46 | 3-12 | 110 | 1-20 | 0.01~98.68 |\n| For Editing (Things without Stuff) | 200 | 12-46 | 1-9 | 54 | 1-17 | 0.02~77.35 |\n| For User Study | 50 | 13-46 | 2-9 | 38 | 1-16 | 0.05~75.62 |\n| For Demo | 40 | 13-46 | 2-9 | 35 | 1-16 | 0.05~69.74 |", "caption": "Table 4: \nStatistics of our multi-instance video editing dataset in various use cases.\n(i) MIVE Dataset is the full set of our dataset including both \u201cstuff\u201d and \u201cthing\u201d categories.\n(ii) For Editing, we only edit objects in the \u201cthing\u201d categories, thus, decreasing some statistics.\n(iii) For User Study, we only select 50 videos that cover diverse scenarios.\n(iv) For the Demo, we only select 40 videos that cover diverse scenarios.", "description": "This table presents statistics of the MIVE dataset, a new dataset for multi-instance video editing, across different use cases including the full dataset, a subset for editing \"thing\" categories only, a subset for user studies covering diverse scenarios, and a subset for demo purposes. The statistics presented include the number of clips, frames per clip, objects per clip, object classes, instances per object class, and the range of average instance mask size per video.  The different use cases demonstrate the versatility and diversity of the MIVE dataset for evaluating multi-instance video editing tasks.", "section": "C. Dataset and Metrics Additional Details"}, {"content": "| Method | Venue | Editing | Scope | LTC \u2191 | LTF \u2191 | IA \u2191 | CIA \u2191 | LTC \u2191 | LTF \u2191 | IA \u2191 | CIA \u2191 | LTC \u2191 | LTF \u2191 | IA \u2191 | CIA \u2191 |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| ControlVideo [70] | ICLR\u201924 | Global | | **0.9546** | 0.1684 | 0.3479 | 0.3875 | **0.9516** | 0.1852 | 0.3248 | 0.5220 | **0.9580** | **0.2048** | **0.5845** | 0.5003 |\n| FLATTEN [11] | ICLR\u201924 | Global | | 0.9519 | 0.1789 | **0.4230** | 0.4215 | 0.9457 | 0.1902 | 0.2338 | 0.5766 | 0.9547 | 0.1876 | 0.2371 | 0.4949 |\n| RAVE [26] | CVPR\u201924 | Global | |0.9547 | 0.1752 | 0.3953 | 0.4310 | 0.9527 | 0.1830 | 0.2932 | 0.5444 | 0.9581 | 0.1910 | 0.3684 | 0.4815 |\n| TokenFlow [16] | ICLR\u201924 | Global | | 0.9486 | 0.1783 | **0.4517** | 0.4434 | 0.9406 | 0.1876 | 0.3297 | **0.6085** | 0.9522 | 0.1880 | 0.3536 | 0.5090 |\n| FreSCo [62] | CVPR\u201924 | Global | | 0.9288 | 0.1790 | 0.4044 | 0.4283 | 0.9226 | 0.1892 | 0.2937 | 0.5945 | 0.9383 | 0.1852 | 0.2709 | 0.4958 |\n| GAV [25] | ICLR\u201924 | Local, Multiple | | 0.9529 | **0.1803** | 0.4224 | **0.4680** | 0.9498 | **0.1932** | **0.3298** | 0.5913 | 0.9550 | 0.1889 | 0.3740 | **0.5420** |\n| **MIVE (Ours)** | - | Local, Multiple | | 0.9537 | 0.1794 | 0.4051 | **0.6059** | 0.9441 | 0.1997 | 0.4647 | **0.6883** | 0.9509 | **0.2243** | **0.7414** | **0.7331** |", "caption": "Table 5: \u00a0Quantitative comparison based on instance size. We only show Local Scores since these are the only scores that can be computed depending on the instance size.\nWe follow the categorization of instance size from COCO dataset [30], where: (i) small instance has area <322absentsuperscript322<32^{2}< 32 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT, (ii) medium instance has area between 322\u2062\u00a0and\u00a0\u2062962superscript322\u00a0and\u00a0superscript96232^{2}\\text{ and }96^{2}32 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT and 96 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT, and (iii) large instance has area >962absentsuperscript962>96^{2}> 96 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT.\nThe best and second best scores are shown in red and blue, respectively.", "description": "This table presents a quantitative comparison of various video editing methods based on instance size, categorized as small (<32x32 pixels), medium (between 32x32 and 96x96 pixels), and large (>96x96 pixels), following the COCO dataset categorization.  Only Local Temporal Consistency (LTC), Local Textual Faithfulness (LTF), Instance Accuracy (IA), and Cross-Instance Accuracy (CIA) are shown, as global metrics are unsuitable for evaluating multi-instance edits. The table analyzes the performance of each method across these size categories, highlighting the best and second-best scores in red and blue, respectively. It aims to demonstrate how instance size affects the performance of different video editing methods in terms of temporal consistency, textual faithfulness, and cross-instance accuracy, especially in handling attention leakage, which is the key focus of the presented MIVE method.", "section": "5.1. Experimental Results"}, {"content": "| Method | Venue | Editing | Global Scores |  |  | Local Scores |  |  | Leakage Scores |  |\n|---|---|---|---|---|---|---|---|---|---|---| \n| | | Scope | GTC \u2191 | GTF \u2191 | FA \u2191 | LTC \u2191 | LTF \u2191 | FA \u2191 | CIA (**Ours**) \u2191 | BP \u2193 |\n|---|---|---|---|---|---|---|---|---|---|---|---| \n| | | | | | | | | | | | |\n| **Editing on 1-3 Instances (Easy Video) - 116 Videos** | | | | | | | | | | | |\n| ControlVideo [70] | ICLR\u201924 | Global | **0.9729** | 0.2724 | **0.8839** | 0.9513 | 0.2020 | 0.5374 | 0.6192 | 79.3548 |\n| FLATTEN [11] | ICLR\u201924 | Global | 0.9661 | 0.2415 | 0.2924 | 0.9484 | 0.1893 | 0.2568 | 0.6060 | 67.1713 |\n| RAVE [26] | CVPR\u201924 | Global | 0.9661 | 0.2698 | 0.5452 | **0.9533** | 0.1886 | 0.3620 | 0.5964 | 71.2598 |\n| TokenFlow [16] | ICLR\u201924 | Global | 0.9686 | 0.2578 | 0.5710 | 0.9465 | 0.1880 | 0.3480 | 0.6248 | 74.2420 |\n| FreSCo [62] | CVPR\u201924 | Global | 0.9534 | 0.2491 | 0.4138 | 0.9327 | 0.1877 | 0.2841 | 0.6085 | 92.0291 |\n| GAV [25] | ICLR\u201924 | Local, Multiple | 0.9643 | 0.2518 | 0.5583 | 0.9477 | 0.1915 | 0.3851 | **0.6466** | **63.3588** |\n| **MIVE (Ours)** | - | Local, Multiple | 0.9583 | **0.2738** | **0.8589** | 0.9441 | **0.2203** | **0.6932** | **0.7983** | **58.4894** |\n| **Editing on 4-7 Instances (Medium Video) - 66 Videos** | | | | | | | | | | | |\n| ControlVideo [70] | ICLR\u201924 | Global | **0.9757** | **0.2775** | **0.8834** | **0.9579** | **0.1875** | **0.4543** | 0.3704 | 65.3667 |\n| FLATTEN [11] | ICLR\u201924 | Global | 0.9700 | 0.2373 | 0.2305 | 0.9526 | 0.1853 | 0.2288 | 0.4199 | **56.7246** |\n| RAVE [26] | CVPR\u201924 | Global | 0.9689 | **0.2777** | 0.5819 | 0.9570 | 0.1842 | 0.3396 | 0.3951 | 57.2210 |\n| TokenFlow [16] | ICLR\u201924 | Global | 0.9686 | 0.2559 | 0.5425 | 0.9486 | 0.1845 | 0.3631 | 0.4498 | 62.3709 |\n| FreSCo [62] | CVPR\u201924 | Global | 0.9555 | 0.2589 | 0.4276 | 0.9318 | 0.1835 | 0.3177 | 0.4389 | 76.8818 |\n| GAV [25] | ICLR\u201924 | Local, Multiple | 0.9674 | 0.2648 | 0.5549 | 0.9549 | 0.1859 | 0.3660 | **0.4676** | 57.8612 |\n| **MIVE (Ours)** | - | Local, Multiple | 0.9614 | 0.2763 | **0.8397** | 0.9501 | **0.2060** | **0.5872** | **0.6483** | **48.2502** |\n| **Editing on >7 Instances (Hard Video) - 18 Videos** | | | | | | | | | | | |\n| ControlVideo [70] | ICLR\u201924 | Global | **0.9781** | 0.2692 | **0.9051** | **0.9651** | 0.1885 | **0.3602** | 0.1698 | 58.5803 |\n| FLATTEN [11] | ICLR\u201924 | Global | 0.9717 | 0.2274 | 0.2007 | 0.9584 | **0.1898** | 0.2499 | **0.2339** | 57.0569 |\n| RAVE [26] | CVPR\u201924 | Global | 0.9711 | **0.2735** | 0.7714 | 0.9602 | 0.1857 | 0.3242 | 0.2026 | 51.7408 |\n| TokenFlow [16] | ICLR\u201924 | Global | 0.9682 | 0.2552 | 0.5778 | 0.9529 | 0.1878 | 0.3162 | 0.2212 | 55.8445 |\n| FreSCo [62] | CVPR\u201924 | Global | 0.9538 | 0.2536 | 0.4339 | 0.9322 | 0.1844 | 0.2954 | 0.2156 | 71.5167 |\n| GAV [25] | ICLR\u201924 | Local, Multiple | 0.9715 | 0.2580 | 0.4825 | 0.9628 | 0.1870 | 0.2911 | 0.2210 | **47.0554** |\n| **MIVE (Ours)** | - | Local, Multiple | 0.9697 | **0.2784** | **0.8937** | 0.9626 | **0.2002** | **0.5118** | **0.3669** | **49.9864** |", "caption": "Table 6: \u00a0Quantitative comparison for multi-instance video editing on various number of instances. We categorize 200 videos of MIVE Dataset depending on the number of edited instances: (i) Easy Video (EV): video that contains 1-3 edited instances, (ii) Medium Video (MV): video that contains 4-7 edited instances, and (iii) Hard Video (HV): video that contains >>> 7 edited instances. The best and second best scores are shown in red and blue, respectively.", "description": "This table presents quantitative results of different video editing methods on the MIVE dataset, categorized by the number of instances edited per video: Easy (1-3 instances), Medium (4-7 instances), and Hard (7+ instances).  It compares performance across various metrics, including global and local temporal consistency, global and local textual faithfulness, frame and instance accuracy, cross-instance accuracy, and background preservation.", "section": "5. Experiments"}, {"content": "| Methods |             | GTC | GTF | FA  | LTC \u2191 | LTF \u2191 | IA \u2191 | CIA \u2191 | BP \u2193 |\n|---------|-------------|-----|-----|-----|-------|-------|------|-------|------|\n| DMS     | Only NPS    | 0.9591 | 0.2667 | 0.7907 | 0.9460 | 0.2072 | 0.5587 | 0.6663 | 54.6597 |\n|         | Only LPS    | 0.9602 | 0.2645 | 0.7690 | 0.9483 | 0.2068 | 0.5716 | 0.6688 | 50.8549 |\n|         | LPS + NPS w/o Re-Inversion | 0.9615 | 0.2674 | 0.7810 | 0.9485 | 0.2080 | 0.5776 | 0.6783 | 52.3240 |\n|         | **Ours, Full** | 0.9604 | 0.2750 | 0.8557 | 0.9478 | 0.2138 | 0.6419 | 0.7100 | 54.3452 |\n| IPR     | No Modulation [44] | 0.9642 | 0.2642 | 0.7468 | 0.9535 | 0.2060 | 0.5225 | 0.6553 | 50.1319 |\n|         | Dense Diffusion [28] | 0.9611 | 0.2760 | 0.9029 | 0.9482 | 0.2136 | 0.6215 | 0.6891 | 59.2100 |\n|         | **Ours, Full** | 0.9604 | 0.2750 | 0.8557 | 0.9478 | 0.2138 | 0.6419 | 0.7100 | 54.3452 |", "caption": "Table 7: \nThe full results (Global Scores and Local Scores) of our ablation study on Disentangled Multi-instance Sampling (DMS) and Instance-centric Probability Redistribution (IPR).\nLPS and NPS denotes Latent Parallel Sampling and Noise Parallel Sampling, respectively.\nThe best and second best scores are shown in red and blue, respectively.", "description": "This table presents the complete results, including both Global and Local Scores, from an ablation study on Disentangled Multi-instance Sampling (DMS) and Instance-centric Probability Redistribution (IPR).  The abbreviations LPS and NPS stand for Latent Parallel Sampling and Noise Parallel Sampling, respectively.  The highest scores in each metric are highlighted in red, while the second-highest scores are highlighted in blue.", "section": "5.2. Ablation Studies"}, {"content": "| Method | GTC \u2191 | GTF \u2191 | FA \u2191 | LTC \u2191 | LTF \u2191 | IA \u2191 | CIA (**Ours**) \u2191 | BP \u2193 |\n|---|---|---|---|---|---|---|---|---| \n| (a) Ablation on Alternating LPS and NPS on All Sampling Steps (50) | | | | | | | | |\n| (1) Alternate (50): LPS = 1 + NPS = 1 | **0.9608** | **0.2691** | **0.8098** | 0.9472 | **0.2090** | **0.5883** | **0.6851** | 55.1253 |\n| (2) Alternate (50): LPS = 4 + NPS = 1 | **0.9610** | 0.2667 | 0.7908 | **0.9481** | 0.2080 | **0.5914** | **0.6809** | 52.4647 |\n| (3) Alternate (50): LPS = 9 + NPS = 1 | 0.9605 | 0.2654 | 0.7779 | **0.9483** | 0.2075 | 0.5748 | 0.6713 | **51.4300** |\n| (4) Alternate (50): LPS = 14 + NPS = 1 | 0.9604 | 0.2649 | 0.7706 | **0.9483** | 0.2073 | 0.5743 | 0.6697 | **51.1941** |\n| (5) Alternate (50): NPS = 4 + LPS = 1 | 0.9598 | **0.2690** | **0.8181** | 0.9465 | **0.2081** | 0.5804 | 0.6727 | 54.8790 |\n| (b) Ablation on Last NPS after Alternating Sampling | | | | | | | | |\n| (1) Alternate (50): LPS = 9 + NPS = 1; NPS = 0 | 0.9605 | 0.2654 | 0.7779 | **0.9483** | 0.2075 | 0.5748 | 0.6713 | **51.4300** |\n| (2) Alternate (40): LPS = 9 + NPS = 1; NPS = 10 | **0.9613** | 0.2673 | 0.7824 | **0.9485** | 0.2081 | **0.5786** | **0.6792** | **52.3441** |\n| (3) Alternate (30): LPS = 9 + NPS = 1; NPS = 20 | **0.9614** | **0.2688** | **0.7890** | **0.9483** | **0.2083** | **0.5752** | **0.6739** | 52.9035 |\n| (4) Alternate (20): LPS = 9 + NPS = 1; NPS = 30 | **0.9614** | **0.2680** | **0.7990** | **0.9485** | **0.2084** | 0.5743 | 0.6734 | 53.4412 |\n| (c) Ablation on Re-Inversion Step only on Alternating Sampling | | | | | | | | |\n| (1) Alternate (40): LPS = 9 + Re-Inv L = 1 + NPS = 1; NPS = 10 | **0.9607** | 0.2697 | 0.8012 | **0.9479** | 0.2094 | 0.5930 | 0.6867 | **52.8582** |\n| (2) Alternate (40): LPS = 9 + Re-Inv L = 2 + NPS = 1; NPS = 10 | **0.9603** | **0.2712** | **0.8162** | **0.9475** | **0.2107** | **0.6180** | **0.6970** | **53.2689** |\n| (3) Alternate (40): LPS = 9 + Re-Inv L = 3 + NPS = 1; NPS = 10 | 0.9599 | **0.2724** | **0.8305** | 0.9471 | **0.2114** | **0.6233** | **0.6989** | 53.7494 |\n| (d) Ablation on Re-Inversion Step of Last NPS=10 with Alternating LPS=9 & NPS=1 & Re-Inversion L=3 | | | | | | | | |\n| (1) Alternate (40): LPS = 9 + Re-Inv L = 3 + NPS = 1; NPS = 10 + Re-Inv L = 1 | 0.9603 | 0.2740 | 0.8413 | **0.9477** | 0.2129 | 0.6315 | 0.7068 | **53.9953** |\n| (2) Alternate (40): LPS = 9 + Re-Inv L = 3 + NPS = 1; NPS = 10 + Re-Inv L = 2 (**Ours**, Full) | **0.9604** | **0.2750** | **0.8557** | **0.9478** | **0.2138** | **0.6419** | **0.7100** | **54.3452** |\n| (3) Alternate (40): LPS = 9 + Re-Inv L = 3 + NPS = 1; NPS = 10 + Re-Inv L = 3 | **0.9606** | **0.2751** | **0.8591** | 0.9476 | **0.2143** | **0.6401** | **0.7090** | 54.7422 |\n| (e) Ablation on Re-Inversion Step using 2D vs 3D Model | | | | | | | | |\n| (1) Alternate (40): LPS = 9 + Re-Inv L = 3 + NPS = 1; NPS = 10 + Re-Inv (2D) L = 2 | **0.9638** | **0.2749** | **0.8497** | **0.9505** | **0.2138** | **0.6385** | **0.7078** | **54.9499** |\n| (2) Alternate (40): LPS = 9 + Re-Inv L = 3 + NPS = 1; NPS = 10 + Re-Inv (3D) L = 2 (**Ours**, Full) | **0.9604** | **0.2750** | **0.8557** | **0.9478** | **0.2138** | **0.6419** | **0.7100** | **54.3452** |\n| (f) Ablation on Another Alternative Configuration | | | | | | | | |\n| (1) Alternate (40): LPS = 4 + Re-Inv L = 3 + NPS = 1; NPS = 10 + Re-Inv (3D) L = 2 | **0.9597** | **0.2775** | **0.8774** | **0.9467** | **0.2151** | **0.6626** | **0.7159** | **55.6459** |\n| (2) Alternate (40): LPS = 9 + Re-Inv L = 3 + NPS = 1; NPS = 10 + Re-Inv (3D) L = 2 (**Ours**, Full) | **0.9604** | **0.2750** | **0.8557** | **0.9478** | **0.2138** | **0.6419** | **0.7100** | **54.3452** |", "caption": "Table 8: \nAblation study on various hyperparameter configurations for our Disentangled Multi-instance Sampling (DMS). The best and second best scores are shown in red and blue, respectively.", "description": "This table presents the ablation study results for Disentangled Multi-instance Sampling (DMS), a key component of the MIVE framework.  It explores variations in alternating Latent Parallel Sampling (LPS) and Noise Parallel Sampling (NPS) steps, the use of re-inversion, and the choice of 2D vs. 3D models during re-inversion. The impact of these configurations is measured using metrics like GTC, GTF, FA, LTC, LTF, IA, CIA, and BP.  The table highlights the trade-offs between different sampling strategies and the importance of re-inversion for achieving high editing faithfulness while maintaining temporal consistency and minimizing artifacts. The goal is to identify the optimal DMS configuration for the MIVE framework.", "section": "5.2 Ablation Studies"}, {"content": "| Method | GTC \u2191 | GTF \u2191 | FA \u2191 | LTC \u2191 | LTF \u2191 | IA \u2191 | CIA (**Ours**) \u2191 | BP \u2193 |\n|---|---|---|---|---|---|---|---|---| \n| (a) Ablation on _\u03bb_ | | | | | | | | |\n| _\u03bb_=0.2 | **0.9605** | 0.2745 | 0.8469 | <ins>0.9477</ins> | 0.2130 | 0.6299 | 0.7072 | **53.8473** |\n| _\u03bb_=0.4 (Ours) | <ins>0.9604</ins> | <ins>0.2750</ins> | **0.8557** | **0.9478** | <ins>0.2138</ins> | <ins>0.6419</ins> | <ins>0.7100</ins> | <ins>54.3452</ins> |\n| _\u03bb_=0.6 | **0.9605** | **0.2753** | <ins>0.8525</ins> | 0.9476 | **0.2139** | **0.6459** | **0.7111** | 54.5119 |\n| (b) Ablation on _\u03bb__r_ | | | | | | | | |\n| _\u03bb__r_=0.0 | 0.9599 | <ins>0.2757</ins> | **0.8770** | 0.9478 | 0.2137 | **0.6439** | 0.7041 | 54.8389 |\n| _\u03bb__r_=0.2 | 0.9599 | 0.2752 | <ins>0.8663</ins> | 0.9475 | 0.2137 | 0.6396 | 0.7023 | 54.6357 |\n| _\u03bb__r_=0.4 | **0.9610** | **0.2759** | 0.8589 | 0.9477 | **0.2140** | 0.6386 | **0.7101** | 54.5377 |\n| _\u03bb__r_=0.6 (Ours) | 0.9604 | 0.2750 | 0.8557 | 0.9478 | <ins>0.2138</ins> | 0.6419 | <ins>0.7100</ins> | 54.3452 |\n| _\u03bb__r_=0.8 | 0.9600 | 0.2746 | 0.8457 | <ins>0.9479</ins> | 0.2130 | <ins>0.6438</ins> | 0.7004 | <ins>54.2150</ins> |\n| _\u03bb__r_=1.0 | <ins>0.9605</ins> | 0.2728 | 0.8302 | **0.9484** | 0.2121 | 0.6277 | 0.6999 | **53.9180** |\n| (c) Ablation on Percentage of Sampling Step that Utilizes IPR | | | | | | | | |\n| Applying IPR on The First 60% Sampling Steps | **0.9608** | **0.2750** | **0.8579** | **0.9485** | <ins>0.2134</ins> | <ins>0.6350</ins> | <ins>0.7096</ins> | <ins>54.3023</ins> |\n| Applying IPR on The First 80% Sampling Steps (Ours) | <ins>0.9604</ins> | **0.2750** | <ins>0.8557</ins> | <ins>0.9478</ins> | **0.2138** | **0.6419** | **0.7100** | 54.3452 |\n| Applying IPR on The First 100% Sampling Steps | 0.9577 | <ins>0.2740</ins> | 0.8447 | 0.9452 | 0.2128 | 0.6363 | 0.7082 | **54.0010** |", "caption": "Table 9: \u00a0Ablation study on various hyperparameter configurations for our Instance-centric Probability Redistribution (IPR). The best and second best scores are shown in red and blue, respectively.", "description": "This table presents the ablation study results on Instance-centric Probability Redistribution (IPR), a key module of MIVE to achieve faithful editing with less artifacts. The table shows how varying different hyperparameters, such as \u03bb, \u03bbr, and the percentage of sampling steps that utilizes IPR affects various metrics, including GTC, GTF, FA, LTC, LTF, IA, CIA, and BP. The table highlights the best and second-best scores in red and blue.", "section": "5.2 Ablation Studies"}]