{"references": [{" publication_date": "2018", "fullname_first_author": "Philip J Ball", "paper_title": "Efficient online reinforcement learning with offline data", "reason": "This paper introduces a novel method for efficient online reinforcement learning that incorporates offline data, directly addressing the sample complexity challenge in real-world robotic manipulation.  Its focus on sample efficiency and ability to leverage prior information aligns perfectly with the key challenges and goals of the current research, making it highly relevant and impactful.", "section_number": 5}, {" publication_date": "2013", "fullname_first_author": "Volodymyr Mnih", "paper_title": "Playing atari with deep reinforcement learning", "reason": "This seminal work demonstrated the power of deep reinforcement learning in achieving superhuman performance on challenging tasks, setting a benchmark for applying deep RL in complex domains.  Its impact on the field of deep reinforcement learning is undeniable, inspiring numerous subsequent works on deep RL for robotic control and manipulation.", "section_number": 1}, {" publication_date": "2018", "fullname_first_author": "Tuomas Haarnoja", "paper_title": "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor", "reason": "Soft Actor-Critic (SAC) is a highly influential off-policy reinforcement learning algorithm known for its sample efficiency and stability. This method has proven particularly effective for complex tasks with continuous action spaces, making it a strong and relevant baseline for comparison in the context of real-world robotic manipulation.", "section_number": 4}, {" publication_date": "2020", "fullname_first_author": "Lin F. Yang", "paper_title": "Sample-optimal parametric Q-learning using linearly additive features", "reason": "This paper offers theoretical guarantees on the sample complexity of parametric Q-learning, providing insights into the fundamental limitations of reinforcement learning algorithms.  Understanding these theoretical limits is crucial to making informed choices in the design and implementation of real-world RL systems for robotic manipulation.", "section_number": 4}, {" publication_date": "2018", "fullname_first_author": "Carlos Florensa", "paper_title": "Automatic goal generation for reinforcement learning agents", "reason": "This work is closely related to the current paper's approach to handle long-horizon tasks. The authors present methods for automatically decomposing tasks into a sequence of simpler subgoals, which can significantly reduce the sample complexity in reinforcement learning. This is highly relevant since long-horizon manipulation tasks require significant amounts of data for learning.", "section_number": 4}, {" publication_date": "2019", "fullname_first_author": "Jemin Hwangbo", "paper_title": "Learning agile and dynamic motor skills for legged robots", "reason": "This paper showcases the state-of-the-art in robotic learning, focusing on learning complex, dynamic motor skills for legged robots. The techniques and challenges discussed are highly relevant in context of the current work, since the paper considers learning complex, dynamic motor skills on robotic manipulators. ", "section_number": 4}, {" publication_date": "2020", "fullname_first_author": "Jianlan Luo", "paper_title": "Provably efficient reinforcement learning with linear function approximation", "reason": "This work presents a theoretical analysis of reinforcement learning algorithms with linear function approximation, providing valuable insights into the sample complexity and convergence properties of RL algorithms. This is highly relevant as it directly addresses the sample complexity issues associated with real-world robotic manipulation tasks.", "section_number": 4}, {" publication_date": "2018", "fullname_first_author": "Aravind Rajeswaran", "paper_title": "Learning complex dexterous manipulation with deep reinforcement learning and demonstrations", "reason": "This paper is a highly relevant baseline to compare with. It introduces an approach to learn complex dexterous manipulation skills using deep reinforcement learning and demonstrations, achieving strong performance on a variety of robotic manipulation tasks. The approach and its limitations serve as a benchmark for the current work.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Philip J Ball", "paper_title": "Efficient online reinforcement learning with offline data", "reason": "This paper presents a novel method for efficient online reinforcement learning that incorporates offline data, directly addressing the sample complexity challenge in real-world robotic manipulation.  Its focus on sample efficiency and ability to leverage prior information makes it highly relevant and impactful to the current work.", "section_number": 4}, {" publication_date": "2020", "fullname_first_author": "Carlos Florensa", "paper_title": "Automatic goal generation for reinforcement learning agents", "reason": "This work is closely related to the current paper's approach to handle long-horizon tasks.  The methods for automatically decomposing tasks into simpler subgoals can significantly reduce the sample complexity in RL, which is highly relevant to this research on long-horizon manipulation tasks.", "section_number": 4}, {" publication_date": "2016", "fullname_first_author": "Sergey Levine", "paper_title": "End-to-end training of deep visuomotor policies", "reason": "This work is a seminal paper in deep reinforcement learning for robotic control. It demonstrated the feasibility of training deep neural networks end-to-end for visuomotor control, laying the foundation for many subsequent works in the field, including this current work. This early work's impact is evident in many areas of deep RL.", "section_number": 4}, {" publication_date": "2019", "fullname_first_author": "Tuomas Haarnoja", "paper_title": "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor", "reason": "Soft Actor-Critic (SAC) is a highly influential off-policy reinforcement learning algorithm known for its sample efficiency and stability.  This method has proven particularly effective for complex tasks with continuous action spaces, making it a strong and relevant baseline for comparison in the context of real-world robotic manipulation. The method's efficiency is directly relevant to the current work.", "section_number": 4}, {" publication_date": "2021", "fullname_first_author": "Abhishek Gupta", "paper_title": "Reset-free reinforcement learning via multi-task learning: Learning dexterous manipulation behaviors without human intervention", "reason": "This paper presents a novel approach to reset-free reinforcement learning, directly addressing the challenge of dealing with sample inefficiency in real-world robotic manipulation tasks.  The focus on learning without resets and the use of multi-task learning are highly relevant to the current research, making it a significant contribution.", "section_number": 4}, {" publication_date": "2018", "fullname_first_author": "Michael J. Kearns", "paper_title": "Near-optimal reinforcement learning in polynomial time", "reason": "This paper provides theoretical insights into the sample complexity of reinforcement learning algorithms, which is extremely relevant to the challenges of applying RL to complex robotic manipulation tasks in the real world. The theoretical underpinnings laid out in this paper help guide practical choices in algorithm design and implementation.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Jianlan Luo", "paper_title": "Interactive imitation learning as reinforcement learning", "reason": "This paper is directly relevant because it proposes a novel method that combines imitation learning with reinforcement learning to address the sample inefficiency of reinforcement learning in real-world settings, especially in robotic manipulation tasks. The method and its results are highly relevant to this current research.", "section_number": 4}, {" publication_date": "2020", "fullname_first_author": "Cheng Chi", "paper_title": "Diffusion policy: Visuomotor policy learning via action diffusion", "reason": "This paper presents a novel approach to visuomotor policy learning using diffusion models, a technique that is highly relevant to the current work, which also focuses on vision-based manipulation policies.  Diffusion models have proven effective in generating high-quality samples and capturing complex data distributions, making them a potentially powerful tool for learning complex manipulation skills.", "section_number": 4}, {" publication_date": "2018", "fullname_first_author": "Carlos Florensa", "paper_title": "Reverse curriculum generation for reinforcement learning", "reason": "This paper introduces the concept of reverse curriculum generation, a method for generating a sequence of increasingly difficult training tasks, which can significantly improve the sample efficiency of reinforcement learning.  This method is highly relevant to the current research because it directly addresses the challenge of sample inefficiency in learning complex robotic manipulation skills.", "section_number": 4}, {" publication_date": "2021", "fullname_first_author": "Alexey Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "reason": "This is a highly influential paper in computer vision that introduces the use of transformer networks for image recognition, achieving state-of-the-art results on various image classification benchmarks.  The use of transformer networks has significantly impacted the field of computer vision and has become a standard approach in many recent works, including the vision-based manipulation methods used in this current work.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Henry Zhu", "paper_title": "The ingredients of real-world robotic reinforcement learning", "reason": "This paper provides a comprehensive overview of the challenges and key factors involved in successful real-world robotic reinforcement learning. It discusses various techniques and design choices for building robust, efficient, and generalizable reinforcement learning policies for robotic manipulation, making it highly relevant and impactful to this current research.", "section_number": 4}, {" publication_date": "2021", "fullname_first_author": "Jianlan Luo", "paper_title": "Robust Multi-Modal Policies for Industrial Assembly via Reinforcement Learning and Demonstrations: A Large-Scale Study", "reason": "This paper is highly relevant because it focuses on applying reinforcement learning to industrial assembly tasks. It directly addresses the challenges associated with multi-modal policies and the need for robustness and demonstrates a large-scale study. The focus on multi-modal policies is especially relevant since this current research also explores various types of policies for different tasks and challenges.", "section_number": 4}]}