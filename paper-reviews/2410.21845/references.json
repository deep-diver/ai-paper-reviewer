{"references": [{" publication_date": "2018", "fullname_first_author": "Philip J Ball", "paper_title": "Efficient online reinforcement learning with offline data", "reason": "This paper introduces RLPD, a sample-efficient off-policy RL algorithm that is crucial for handling the sample complexity issue in real-world robotic RL.  The algorithm's ability to incorporate prior data through demonstrations and corrections contributes significantly to the overall efficiency and performance of HIL-SERL.  This method forms the core RL methodology upon which the entire paper's real-world experiments are based, making it one of the most critical references in the paper.", "section_number": 3}, {" publication_date": "2018", "fullname_first_author": "Tuomas Haarnoja", "paper_title": "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor", "reason": "This paper introduces the Soft Actor-Critic (SAC) algorithm, which is a key component of the HIL-SERL framework. SAC is a sample-efficient off-policy RL algorithm that enables stable and effective training in complex environments. Its use contributes to achieving faster and more stable learning, making it a vital component to the success of the proposed methodology.", "section_number": 3}, {" publication_date": "2013", "fullname_first_author": "Volodymyr Mnih", "paper_title": "Playing atari with deep reinforcement learning", "reason": "This seminal paper introduced the Deep Q-Network (DQN) algorithm which is a foundational algorithm used in many deep reinforcement learning applications, including the present work.  The DQN methodology is specifically used in the paper as the basis of the gripper control mechanism, and its effectiveness in learning discrete action spaces is important for several tasks discussed in the experimental results.", "section_number": 3}, {" publication_date": "2018", "fullname_first_author": "Carlos Florensa", "paper_title": "Automatic goal generation for reinforcement learning agents", "reason": "This paper discusses using reward shaping to accelerate the learning process in RL. While not directly used in HIL-SERL, the concept of shaping rewards is relevant as it relates to the challenge of designing reward functions for complex robotic manipulation tasks, making it an important theoretical background for the paper. This concept motivates the use of a different approach involving a combination of demonstrations and corrections to guide the learning process.", "section_number": 3}, {" publication_date": "2016", "fullname_first_author": "Sergey Levine", "paper_title": "End-to-end training of deep visuomotor policies", "reason": "This paper demonstrates the feasibility of end-to-end training of deep visuomotor policies for robotic manipulation.  This is highly relevant to the HIL-SERL approach, which also leverages end-to-end training with vision-based inputs. The success of the referenced work motivates the approach used in HIL-SERL which is also an end-to-end approach that trains vision-based policies directly in the real world, and it establishes the end-to-end methodology as a viable and efficient approach for real-world robotic manipulation.", "section_number": 3}, {" publication_date": "2020", "fullname_first_author": "Chi Jin", "paper_title": "Provably efficient reinforcement learning with linear function approximation", "reason": "This paper is essential for the theoretical understanding of the sample complexity of reinforcement learning, a key concern addressed by HIL-SERL. The theoretical analysis in this paper is crucial to understanding the limitations of conventional reinforcement learning in handling complex tasks and motivates the design choices in HIL-SERL, such as using a sample-efficient algorithm (RLPD) and incorporating human-in-the-loop feedback to improve efficiency.", "section_number": 3}, {" publication_date": "2015", "fullname_first_author": "Kaiming He", "paper_title": "Deep residual learning for image recognition", "reason": "This highly influential paper introduced the ResNet architecture, a crucial building block for many computer vision tasks. Since HIL-SERL uses a pretrained ResNet as a visual backbone for policy learning, understanding the capabilities and characteristics of this architecture is essential for grasping the core visual processing components of the paper.", "section_number": 3}, {" publication_date": "2018", "fullname_first_author": "Aravind Rajeswaran", "paper_title": "Learning complex dexterous manipulation with deep reinforcement learning and demonstrations", "reason": "This is a highly relevant reference demonstrating the capacity of RL methods to learn complex dexterous manipulation skills. HIL-SERL builds upon this work by addressing the challenges of applying RL to real-world settings. Specifically, it incorporates human feedback and corrections, improving the training efficiency and the overall performance of the learned policies. This establishes the importance of the task complexity addressed by the paper.", "section_number": 4}, {" publication_date": "2018", "fullname_first_author": "Michael Kelly", "paper_title": "Hg-dagger: Interactive imitation learning with human experts", "reason": "This work presents the HG-DAgger algorithm, a key imitation learning baseline which is compared against the HIL-SERL method in the experimental results. The HG-DAgger algorithm, which combines demonstrations and human corrections, is a state-of-the-art imitation learning method. Its performance in comparison to HIL-SERL highlights the benefits of incorporating reinforcement learning to further optimize policies beyond imitation learning alone.", "section_number": 4}, {" publication_date": "2020", "fullname_first_author": "Cheng Chi", "paper_title": "Diffusion policy: Visuomotor policy learning via action diffusion", "reason": "The diffusion policy is a relevant baseline which is compared against the HIL-SERL method in the experimental results.  The comparison is essential for illustrating the advantages of the HIL-SERL approach over alternative RL methods for solving real-world robotic manipulation tasks, which addresses the complex challenges such as acquiring a diverse range of manipulation skills with varying physical characteristics and dynamic behaviors.", "section_number": 4}, {" publication_date": "2019", "fullname_first_author": "Jemin Hwangbo", "paper_title": "Learning agile and dynamic motor skills for legged robots", "reason": "This paper is a highly relevant reference showcasing the potential of RL in learning agile and dynamic motor skills for legged robots.  It is relevant to the current work because the paper focuses on achieving human-level performance in dynamic, dexterous manipulation tasks, and it highlights the difficulty of training such policies in real-world settings using RL-based methods. The paper provides an important background for understanding the complexity of the manipulation tasks addressed in this paper.", "section_number": 4}, {" publication_date": "2019", "fullname_first_author": "N. Fazeli", "paper_title": "See, feel, act: Hierarchical learning for complex manipulation skills with multisensory fusion", "reason": "This paper is highly relevant to HIL-SERL as it also addresses the challenges of learning complex manipulation skills using hierarchical RL.  The comparison to HIL-SERL is relevant as the two papers address similar challenges and adopt different strategies, offering a meaningful comparison and insightful analysis to the approach and performance of HIL-SERL.", "section_number": 4}, {" publication_date": "2013", "fullname_first_author": "Justin Fu", "paper_title": "Variational inverse control with events: A general framework for data-driven reward definition", "reason": "This paper introduced a general framework for data-driven reward definition in reinforcement learning, which is crucial for designing effective reward functions for robotic manipulation tasks.  The framework provided in this paper is relevant as it provides a systematic approach to define reward functions for complex robotic manipulation tasks and therefore provides theoretical support for the proposed approach.", "section_number": 4}, {" publication_date": "2021", "fullname_first_author": "Abhishek Gupta", "paper_title": "Reset-free reinforcement learning via multi-task learning: Learning dexterous manipulation behaviors without human intervention", "reason": "This paper tackles the sample efficiency issue in real-world RL by proposing a method for learning dexterous manipulation behaviors without relying on explicit resets, a significant challenge in training robotic RL agents.  This is important as HIL-SERL also addresses the sample complexity issue, and this paper provides a valuable perspective on alternative strategies for enhancing sample efficiency, thereby providing relevant context and alternatives to the approach adopted in HIL-SERL.", "section_number": 4}, {" publication_date": "2020", "fullname_first_author": "Jianlan Luo", "paper_title": "Robust Multi-Modal Policies for Industrial Assembly via Reinforcement Learning and Demonstrations: A Large-Scale Study", "reason": "This paper is closely related to the present work as it also deals with real-world robotic manipulation using a combination of reinforcement learning and demonstrations. This method uses demonstrations and multiple modalities to train robust policies for assembly tasks, highlighting the potential of combining these techniques for efficient real-world robotic manipulation. The comparison of the two methods helps highlight the specific design choices and contributions of the current work.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Jianlan Luo", "paper_title": "Interactive imitation learning as reinforcement learning", "reason": "This paper demonstrates the potential of combining imitation learning with reinforcement learning to achieve more efficient learning.  This is relevant to HIL-SERL because HIL-SERL also combines human demonstrations and corrections with reinforcement learning to improve efficiency.  This work offers a closely related comparison to the approach used in HIL-SERL that motivates the combined approach.", "section_number": 4}, {" publication_date": "2021", "fullname_first_author": "Antonio Loquercio", "paper_title": "Learning high-speed flight in the wild", "reason": "This paper demonstrates the success of RL in learning complex skills directly in the real world, which is relevant to HIL-SERL.  It showcases the feasibility of applying RL to dynamic manipulation tasks, and its successes further highlight the potential of RL in solving real-world robotic manipulation problems.  The work addresses challenges such as handling noisy sensor data in a real-world environment which establishes the basis of the approach presented in this paper.", "section_number": 4}, {" publication_date": "2017", "fullname_first_author": "Carlos Florensa", "paper_title": "Reverse curriculum generation for reinforcement learning", "reason": "This paper introduces the concept of reverse curriculum generation, which is a technique for shaping the reward function in reinforcement learning. This is highly relevant to HIL-SERL as it demonstrates an alternative way to address the sample complexity issue in reinforcement learning by gradually increasing the complexity of the task over time. The work explores the relationship between the curriculum design and learning efficiency which inspires the approach used in HIL-SERL, demonstrating the importance of the technique.", "section_number": 4}, {" publication_date": "2021", "fullname_first_author": "Henry Zhu", "paper_title": "The ingredients of real world robotic reinforcement learning", "reason": "This paper provides a comprehensive analysis of the challenges and techniques used in real-world robotic reinforcement learning, which is closely related to the HIL-SERL approach. The analysis of the challenges and solutions provides valuable insights into the design choices of HIL-SERL and helps highlight the unique contributions and advantages of HIL-SERL. The analysis of the challenges and solutions in this paper offers support and context for the motivation of the approach adopted in HIL-SERL.", "section_number": 4}]}