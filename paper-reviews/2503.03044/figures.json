[{"figure_path": "https://arxiv.org/html/2503.03044/x1.png", "caption": "Figure 1: A summary of the QE4PE study. Documents are translated by a neural MT model and reviewed by professional editors across two translation directions and four highlight modalities. These include a no highlight baseline and three settings using unsupervised, supervised, and oracle word-level QE methods. Editing effort, productivity and usability across modalities are estimated from editing logs and questionnaires. Finally, the quality of MT and edited outputs is assessed with MQM/ESA human annotations and automatic metrics.", "description": "The figure illustrates the QE4PE study's workflow.  Professional translators post-edit machine translations (MT) generated by a neural MT model. The study compares four conditions: no highlights, unsupervised word-level quality estimation (QE), supervised word-level QE, and oracle highlights (representing perfect QE).  Two translation directions (English-Italian and English-Dutch) are used. Editing logs and questionnaires assess the impact of each highlight modality on editing effort, productivity, and usability.  Finally, human annotations (MQM and ESA) and automatic metrics evaluate the quality of both the initial MT and the post-edited translations.", "section": "3 Experimental Setup"}, {"figure_path": "https://arxiv.org/html/2503.03044/x2.png", "caption": "Figure 2: An example of the QE4PE GroTE setup for two segments in an English\u2192\u2192\\rightarrow\u2192Italian document.", "description": "Figure 2 shows a screenshot of the GROTE interface used in the QE4PE study. The interface is designed for professional translators to post-edit machine translation (MT) outputs. It displays the source sentence in one column and the machine-translated sentence in an adjacent column.  In this example, the interface highlights potential errors in the Italian translation, allowing the translators to focus their edits.  The highlights correspond to different QE models, including supervised and unsupervised methods, used to identify and classify errors at the word level. Translators can interact with the interface by making edits and confirming their changes via checkmarks.  This screenshot demonstrates the controlled experimental environment used to study the effects of different QE methods on professional post-editing workflows.", "section": "3 Experimental Setup"}, {"figure_path": "https://arxiv.org/html/2503.03044/x3.png", "caption": "Figure 3: Productivity of post-editors across QE4PE stages (Pre, Main, Post).\nThe \u2794 marks outliers and \u2715 marks missing data.\nEach row corresponds to the same three translators across all stages.", "description": "This figure displays the productivity of post-editors throughout the QE4PE study across three stages: Pre-task, Main task, and Post-task. Productivity is measured in characters per minute.  The data points represent individual post-editors' productivity in each stage.  Outliers are marked with a \u2794 symbol, and missing data points are indicated by a \u2715. Each row shows the productivity of the same three post-editors across the three stages, allowing for a within-subjects comparison of productivity changes.", "section": "3 Experimental Setup"}, {"figure_path": "https://arxiv.org/html/2503.03044/x4.png", "caption": "Figure 4: Median quality improvement for post-edited segments at various initial MT quality levels across domains and highlight modalities. Quality scores are estimated using XCOMET segment-level QE (top) and professional ESA annotations (bottom). Histograms show example counts across quality bins for the two metrics. Dotted lines show upper bounds for quality improvements given starting MT quality.", "description": "This figure displays the median quality improvements after post-editing, categorized by initial Machine Translation (MT) quality levels, domains (biomedical and social media), and highlight modalities.  The top half uses XCOMET segment-level Quality Estimation (QE) scores to measure quality, while the bottom half utilizes professional ESA (Expert-based Segment Assessment) annotations for a more human-centric evaluation. Histograms alongside each line graph show the number of data points within each quality bin. Dotted lines represent the maximum achievable quality improvement based on the initial MT quality, providing context for the results.", "section": "4.3 Quality Assessment"}, {"figure_path": "https://arxiv.org/html/2503.03044/x5.png", "caption": "Figure 5: Distribution of MQM error categories for MT and post-edits across highlight modalities.", "description": "This figure displays a breakdown of errors categorized using the Multidimensional Quality Metrics (MQM) framework.  It compares the distribution of errors in the original machine translation (MT) outputs to the errors found in post-edited translations.  The comparison is shown across four different highlight modalities: No Highlight, Oracle, Unsupervised, and Supervised.  These modalities represent different methods of highlighting potential errors in the MT output, with \"Oracle\" indicating error spans identified through consensus amongst human post-editors. The figure visually represents the frequency of various error types (Accuracy, Linguistic, and Style) for both Biomedical and Social Media domains across both English-to-Italian and English-to-Dutch translation directions.", "section": "4.2 Highlights and Edits"}, {"figure_path": "https://arxiv.org/html/2503.03044/x6.png", "caption": "Figure 6: Top:QA interface with cropped examples of biomedical and social media texts with error annotations (Biomedical: post-edited segments with No Highlight; Social media: MT outputs). Bottom: Annotation instructions for our MQM-inspired error taxonomy.", "description": "Figure 6 shows the quality assessment process of the study. The top half displays screenshots of the Groningen Translation Environment (GROTE) interface, showcasing examples of biomedical and social media texts with and without error highlights. The biomedical examples show post-edited segments without highlights, while the social media examples are the original machine translation outputs.  The bottom half provides a detailed description of the annotation guidelines and the taxonomy used for error categorization, which follows the Multidimensional Quality Metrics (MQM) scheme.", "section": "3 Experimental Setup"}, {"figure_path": "https://arxiv.org/html/2503.03044/x7.png", "caption": "Figure 7: Top: Post-editing rate across highlight modalities, domains and directions. Bottom: Proportion of edits in highlighted spans across highlight modalities. *** =p<0.001absent\ud835\udc5d0.001=p<0.001= italic_p < 0.001, ** =p<0.01absent\ud835\udc5d0.01=p<0.01= italic_p < 0.01, * =p<0.05absent\ud835\udc5d0.05=p<0.05= italic_p < 0.05, ns === not significant with Bonferroni correction.", "description": "Figure 7 presents a comparative analysis of post-editing behavior across different highlight modalities (no highlight, oracle, unsupervised, and supervised), domains (biomedical and social media), and translation directions (English-Italian and English-Dutch).  The top panel displays the overall post-editing rate for each condition, illustrating how quickly editors completed their work.  The bottom panel shows the proportion of edits made *within* the highlighted spans provided by the different QE methods.  Statistical significance levels are indicated using asterisks (*** for p<0.001, ** for p<0.01, * for p<0.05)  and 'ns' denotes results that were not statistically significant after correcting for multiple comparisons using the Bonferroni method.", "section": "4.2 Highlights and Edits"}, {"figure_path": "https://arxiv.org/html/2503.03044/x8.png", "caption": "Figure 8: Post-editing agreement across various modalities (Section\u00a04.2). Results are averaged across all translator pairs for the two modalities (n=3\ud835\udc5b3n=3italic_n = 3 intra-modality, n=9\ud835\udc5b9n=9italic_n = 9 inter-modality for every language) and all segments.", "description": "Figure 8 displays the level of agreement between different post-editing methods.  The data is averaged across all translator pairs for each modality (3 for intra-modality comparisons and 9 for inter-modality comparisons) and across all the segments. The results show the extent to which different highlighting methods lead to similar editing choices.", "section": "4.2 Highlights and Edits"}, {"figure_path": "https://arxiv.org/html/2503.03044/x9.png", "caption": "Figure 9: ESA ratings for MT outputs and post-edits across domains and translation directions, showing comparable quality distributions for MT and post-edits across domains and directions.", "description": "Figure 9 presents a comparison of the quality scores obtained from the ESA (Error Severity Assessment) metric, applied to both machine translation (MT) outputs and their corresponding post-edited versions. The comparison is done across two different domains (biomedical and social media) and two translation directions (English-Italian and English-Dutch).  The figure visually demonstrates the distribution of ESA scores for each condition, showing that post-editing generally improves translation quality across both domains and translation pairs. The comparable distributions suggest that the post-editing process yields relatively consistent improvements in quality regardless of the source text domain or language.", "section": "4.3 Quality Assessment"}]