[{"figure_path": "https://arxiv.org/html/2502.09614/x1.png", "caption": "Figure 1:  DexTrack\u00a0 learns a generalizable neural tracking controller for dexterous manipulation from human references.\n\nIt generates hand action commands from kinematic references, ensuring\nclose tracking of input trajectories (Fig. (a)), generalizes to novel and challenging tasks involving thin objects, complex movements and intricate in-hand manipulations (Fig. (b)),\nand demonstrates robustness to large kinematics noise and utility in real-world scenarios (Fig. (c)).\nKinematic references are illustrated in orange rectangles and background.", "description": "This figure illustrates DexTrack's neural tracking controller, trained on human-demonstrated dexterous manipulations.  Panel (a) shows the controller's inference process, generating hand commands from kinematic references to closely track input trajectories. Panel (b) highlights DexTrack's ability to generalize to complex, novel tasks involving thin objects and intricate in-hand manipulations. Panel (c) demonstrates the controller's robustness to noisy kinematic data and its effectiveness in real-world settings.  Orange rectangles in the figure represent kinematic references.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2502.09614/x2.png", "caption": "Figure 2: DexTrack\u00a0 \nlearns a generalizable neural tracking controller for dexterous manipulation from human references. It alternates between training the tracking controller using abundant and high-quality robot tracking demonstrations and improving the data\nvia the tracking controller through a homotopy optimization scheme.", "description": "DexTrack uses a neural network to control a robot hand's dexterous manipulation by learning from human-demonstrated examples. The system iteratively improves its performance by alternating between two phases: (1) Training the neural tracking controller using a large dataset of high-quality robot demonstrations of dexterous manipulations; and (2) Enhancing the dataset through a homotopy optimization scheme, where the controller itself guides the generation of improved demonstrations by solving challenging trajectory tracking problems.  This iterative approach leads to a more robust and generalizable controller.", "section": "3 METHOD"}, {"figure_path": "https://arxiv.org/html/2502.09614/x3.png", "caption": "Figure 3: Robustness w.r.t. unreasonable states. \nPlease check our website and video for animated results.", "description": "This figure demonstrates the robustness of DexTrack, the neural tracking controller, when handling unreasonable states.  It showcases scenarios where the robot hand encounters significant deviations from expected kinematic references, such as deep penetrations or impossible configurations.  The caption encourages viewers to visit the project website and accompanying video for a dynamic demonstration of the controller's ability to successfully track and complete the manipulation task despite these challenging conditions.", "section": "4.2 Generalizable Tracking Control for Dexterous Manipulation"}, {"figure_path": "https://arxiv.org/html/2502.09614/x4.png", "caption": "Figure 4: Qualitative comparisons. \nPlease check our website and the accompanying video for animated results.", "description": "This figure presents a qualitative comparison of the proposed DexTrack method against two baseline methods (PPO with OmniGrasp reward and PPO without supervision, using a tracking reward) across various dexterous manipulation tasks.  Each row shows a different manipulation task involving a variety of objects and movements, including in-hand object reorientations and interactions with thin objects.  The images showcase the resulting robot hand and object trajectories generated by each method, highlighting DexTrack's superior ability to accurately track the human-demonstrated kinematic references.  For a complete and dynamic view of these results, the authors recommend visiting their website and accompanying video.", "section": "4.2 Generalizable Tracking Control for Dexterous Manipulation"}, {"figure_path": "https://arxiv.org/html/2502.09614/x5.png", "caption": "Figure 5: Scaling the amount of demonstrations.", "description": "This figure shows a plot illustrating the relationship between the number of demonstrations used to train the model and the model's performance on the TACO dataset.  The x-axis represents the proportion of the total demonstrations used for training, ranging from 0 to 1. The y-axis displays the success rate achieved by the model. The plot demonstrates a clear upward trend, indicating that increasing the number of demonstrations improves the model's success rate in dexterous manipulation tasks. The plot suggests that even with a large number of demonstrations, there might still be room for improvement in the model's performance.", "section": "5 ABLATION STUDIES"}, {"figure_path": "https://arxiv.org/html/2502.09614/x6.png", "caption": "Figure 6: DexTrack\u00a0learns a generalizable neural tracking controller for dexterous manipulation from human references. It alternates between training the tracking controller using abundant and high-quality robot tracking demonstrations, and improving the data\nvia the tracking controller through a homotopy optimization scheme.", "description": "This figure illustrates the DexTrack framework, which learns a neural tracking controller for dexterous manipulation using human references.  The process involves two alternating stages: (1) Training the controller using many high-quality robot tracking demonstrations. These demonstrations consist of paired human kinematic references (showing desired hand and object movements) and corresponding robot actions. (2) Improving the quality of the demonstrations themselves by using the already trained controller in a homotopy optimization scheme.  This scheme refines the trajectories, making them more suitable for controller training and improving the controller's ability to generalize to novel tasks.", "section": "3 METHOD"}, {"figure_path": "https://arxiv.org/html/2502.09614/x7.png", "caption": "Figure 7: \nRobustness towards out-of-distribution objects and manipulations. \nPlease refer to our website and the accompanying video for animated results.", "description": "This figure demonstrates the robustness of the DexTrack neural tracking controller when handling objects and manipulations unseen during training.  The controller successfully tracks diverse, complex movements involving novel objects with varied shapes and sizes, including thin and delicate objects. The video and website provide additional context and animated results for a more thorough understanding of the controller's capabilities.", "section": "4.2 Generalizable Tracking Control for Dexterous Manipulation"}, {"figure_path": "https://arxiv.org/html/2502.09614/x8.png", "caption": "Figure 8: \nAdditional qualitative comparisons. \nPlease refer to our website and the accompanying video for animated results.", "description": "This figure provides additional qualitative comparisons of the DexTrack method's performance against a baseline method on various dexterous manipulation tasks.  The tasks showcase the model's ability to handle complex in-hand object reorientations, interactions with thin objects, and other challenging manipulation scenarios.  Because the image itself is limited, the caption directs the reader to the project website and accompanying video for a complete and dynamic visualization of the results.", "section": "4.2 Generalizable Tracking Control for Dexterous Manipulation"}, {"figure_path": "https://arxiv.org/html/2502.09614/x9.png", "caption": "Figure 9: \nAdditional real-world qualitative results. \nPlease refer to our website and the accompanying video for animated results.", "description": "This figure shows additional qualitative results from real-world experiments.  It provides a visual comparison of the DexTrack controller's performance against a baseline method on various dexterous manipulation tasks.  The images showcase the robot's ability to handle complex movements, intricate in-hand object reorientations, and interactions with thin objects. Because the still images don't fully capture the dynamic nature of the robot's movements, the caption suggests referring to the accompanying video and project website for a better understanding of the results.", "section": "Real-World Evaluations"}, {"figure_path": "https://arxiv.org/html/2502.09614/x10.png", "caption": "Figure 10: \nFailure cases in real-world experiments. \nPlease refer to our website for animated results.", "description": "This figure showcases instances where the DexTrack system failed during real-world experiments.  The failures primarily involve the object dropping from the robot's grasp due to variations in contact.  These failures highlight the challenges of applying a simulated controller to the real-world's unpredictable dynamics and underscore areas for future improvement in robustness and contact management. The animated results on the project website provide further visual detail.", "section": "Additional Experimental Results (Sec. B)"}, {"figure_path": "https://arxiv.org/html/2502.09614/x11.png", "caption": "Figure 11: \nEffectiveness of the homotopy optimization scheme. \nPlease refer to our website and the accompanying video for animated results.", "description": "Figure 11 demonstrates the effectiveness of the homotopy optimization scheme by comparing the results of lifting a thin flute, passing a small sphere, and grasping an apple with and without the scheme.  The homotopy optimization scheme helps the model achieve better tracking results, particularly in challenging scenarios.  The accompanying video on the project website provides animated results for a better understanding.", "section": "Additional Experimental Results"}, {"figure_path": "https://arxiv.org/html/2502.09614/x12.png", "caption": "Figure 12: \nFailure Cases. \nPlease refer to our website and the accompanying video for animated results.", "description": "This figure showcases instances where the DexTrack method struggles.  The scenarios depicted involve objects from novel categories or situations with challenging thin geometries that the model has not been adequately trained to handle. The accompanying video and website provide animated demonstrations to illustrate these failures more comprehensively.", "section": "Additional Experimental Results"}, {"figure_path": "https://arxiv.org/html/2502.09614/x13.png", "caption": "Figure 13: \nExamples of novel objects from the seen object category (TACO).", "description": "This figure displays various novel objects from the TACO dataset that belong to object categories already present in the training set.  This showcases the model's ability to generalize to new objects within familiar categories, demonstrating the impact of the data augmentation technique used in the paper.", "section": "4.1 EXPERIMENTAL SETTINGS"}, {"figure_path": "https://arxiv.org/html/2502.09614/x14.png", "caption": "Figure 14: \nExamples of objects from new object categories (TACO).", "description": "This figure shows examples of objects from categories not seen during the training of the dexterous manipulation model.  These objects are used to test the model's ability to generalize to unseen object types, highlighting its capacity for handling novel manipulation tasks and showcasing its adaptability beyond objects encountered during the training process.", "section": "4.1 Experimental Settings"}, {"figure_path": "https://arxiv.org/html/2502.09614/x15.png", "caption": "Figure 15: \nReal-world experiment setup.", "description": "The figure shows the setup for real-world experiments.  A Franka Emika Panda robot arm is used, equipped with a LEAP hand to interact with real-world objects.  A RealSense camera provides the necessary visual information for object pose estimation (using FoundationPose). The setup demonstrates the transfer of the learned tracking controller from simulation to the real world.", "section": "4.1 EXPERIMENTAL SETTINGS"}]