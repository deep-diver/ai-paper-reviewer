[{"figure_path": "https://arxiv.org/html/2503.03746/x1.png", "caption": "Figure 1: Illustration of our Process-based Self-Rewarding paradigm. (1) We get EFT data by tree-search, initial data filtering and data annotation. And we get IFT data by step segmentation. (2) The model is initialized on EFT and IFT data. (3) The model conducts step-by-step search-based reasoning and performs step-wise LLM-as-a-Judge to select the chosen step and generate the step-wise preference pair at each step. (4) We perform step-wise preference optimization on the model. (5) The model enters the next iteration cycle.", "description": "This figure illustrates the Process-based Self-Rewarding paradigm.  It begins with obtaining two types of data: EFT data (obtained through tree search, initial data filtering, and annotation) and IFT data (created by segmenting solution steps). The model is initialized using both datasets. Then, a step-by-step process unfolds:  The model performs search-based reasoning, uses an LLM-as-a-Judge to select the best reasoning step and create pairwise preference data, and finally, undergoes step-wise preference optimization. This entire cycle is then repeated for iterative model improvement.", "section": "3 Process-based Self-Rewarding Language Models"}, {"figure_path": "https://arxiv.org/html/2503.03746/x2.png", "caption": "(a) Prompt Distributions", "description": "This figure visualizes the distribution of prompts and responses from three different datasets: EFT (red), IFT (blue), and PPD (grey).  The left panel (a) shows the distribution of prompts, while the right panel (b) shows the distribution of the model's responses. The visualization helps to understand how the data from each source differs and how this difference might influence the model's learning process during iterative training. EFT data is for model evaluation, IFT data is for model initialization, and PPD data is the generated data from the process-based self-rewarding pipeline.", "section": "3 Process-based Self-Rewarding Language Models"}, {"figure_path": "https://arxiv.org/html/2503.03746/x3.png", "caption": "(b) Response Distributions", "description": "This figure visualizes the distribution of responses generated by the model during different stages of training. Specifically, it compares the response distributions of the model's initial outputs (EFT), intermediate outputs (IFT), and outputs generated during process-based self-rewarding optimization (PPD). By comparing these distributions, we can observe how the model's response patterns evolve throughout the training process, providing insights into the effectiveness of the self-rewarding mechanism.", "section": "3 Process-based Self-Rewarding Language Models"}, {"figure_path": "https://arxiv.org/html/2503.03746/x4.png", "caption": "Figure 2: The data distribution of prompts and responses in EFT (red), IFT (blue) and PPD (grey) data.", "description": "This figure visualizes the distribution of prompts and responses from three different datasets: EFT (Evaluation Fine-Tuning), IFT (Instruction Fine-Tuning), and PPD (Pair-wise Preference Data).  The visualization helps illustrate how the data points cluster and whether there's significant overlap or separation between the datasets. This is useful for understanding how the model's training data is structured and might influence its performance.", "section": "3 Process-based Self-Rewarding Language Models"}, {"figure_path": "https://arxiv.org/html/2503.03746/x5.png", "caption": "Figure 3: The prompt for converting the the given solution into step-by-step format logically without altering any information in the original solution.", "description": "This figure displays the prompt used to instruct the model to convert a given solution into a step-by-step format. The prompt explicitly instructs the model to maintain the original information and only separate the solution into logical steps, each prefixed with 'Step n:', where n represents the step number.", "section": "3.2 Model Initialization"}, {"figure_path": "https://arxiv.org/html/2503.03746/x6.png", "caption": "Figure 4: The prompt for LLMs conducting step-by-step long-thought reasoning.", "description": "This figure shows the prompt template used for the step-by-step long-thought reasoning task in the Process-based Self-Rewarding Language Model experiments.  The prompt instructs the LLM to solve a given math problem step-by-step, clearly labeling each reasoning step with the prefix \"Step n:\", where n is a positive integer.  The final answer should be enclosed in boxes.  This approach encourages the LLM to break down complex problems into smaller, manageable steps, improving the reasoning process and reducing the chance of errors.", "section": "3 Process-based Self-Rewarding Language Models"}]