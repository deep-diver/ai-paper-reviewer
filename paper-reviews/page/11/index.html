<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><title>Paper Reviews by AI &#183; AI Paper Reviews by AI</title>
<meta name=title content="Paper Reviews by AI &#183; AI Paper Reviews by AI"><meta name=description content="Explore AI papers with thorough reviews generated by AI"><link rel=canonical href=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/><link rel=alternate type=application/rss+xml href=/ai-paper-reviewer/paper-reviews/index.xml title="AI Paper Reviews by AI"><link type=text/css rel=stylesheet href=/ai-paper-reviewer/css/main.bundle.min.67eb0befb1fb57e6528684f3a2ff6e0606a82100504705b66488d54a9f3fda9c2605c9bf3b70163208427288637a7b4d1a168384c1115e417458eae6b44b329c.css integrity="sha512-Z+sL77H7V+ZShoTzov9uBgaoIQBQRwW2ZIjVSp8/2pwmBcm/O3AWMghCcohjentNGhaDhMERXkF0WOrmtEsynA=="><script type=text/javascript src=/ai-paper-reviewer/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script><script defer type=text/javascript id=script-bundle src=/ai-paper-reviewer/js/main.bundle.min.efbf3b6b987689fffaf2d7b73173d2690c0279a04d444b0537a77d7f4ff6e6d493445400cb0cf56bc0f0f123e19f15394e63cae34e67f069bd013dd5c73df56e.js integrity="sha512-7787a5h2if/68te3MXPSaQwCeaBNREsFN6d9f0/25tSTRFQAywz1a8Dw8SPhnxU5TmPK405n8Gm9AT3Vxz31bg==" data-copy data-copied></script><script src=/ai-paper-reviewer/lib/zoom/zoom.min.37d2094687372da3f7343a221a470f6b8806f7891aa46a5a03966af7f0ebd38b9fe536cb154e6ad28f006d184b294525a7c4054b6bbb4be62d8b453b42db99bd.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ=="></script><link rel=apple-touch-icon sizes=180x180 href=/ai-paper-reviewer/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/ai-paper-reviewer/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/ai-paper-reviewer/favicon-16x16.png><link rel=manifest href=/ai-paper-reviewer/site.webmanifest><meta property="og:url" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/"><meta property="og:site_name" content="AI Paper Reviews by AI"><meta property="og:title" content="Paper Reviews by AI"><meta property="og:description" content="Explore AI papers with thorough reviews generated by AI"><meta property="og:locale" content="en"><meta property="og:type" content="website"><meta property="og:image" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/cover.png"><meta name=twitter:title content="Paper Reviews by AI"><meta name=twitter:description content="Explore AI papers with thorough reviews generated by AI"><meta name=author content="AI Paper Reviews by AI"><link href=https://github.com/deep-diver/paper-reviewer/ rel=me><link href=https://twitter.com/algo_diver/ rel=me><script src=/ai-paper-reviewer/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj+KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script><meta name=theme-color></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a></div><div class=min-h-[148px]></div><div class="fixed inset-x-0 pl-[24px] pr-[24px]" style=z-index:100><div id=menu-blur class="absolute opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl shadow-2xl"></div><div class="relative max-w-[64rem] ml-auto mr-auto"><div style=padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start space-x-3"><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/ai-paper-reviewer/ class="text-base font-medium text-gray-500 hover:text-gray-900">AI Paper Reviews by AI</a></nav><nav class="hidden md:flex items-center space-x-5 md:ml-12 h-12"><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>About</p></a><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Paper Reviews</p></a><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Tags</p></a><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></span><p class="text-base font-medium" title></p></a><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span><p class="text-base font-medium" title></p></a><button id=search-button aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button><div class="ltr:mr-14 rtl:ml-14 flex items-center"><button id=appearance-switcher aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></nav><div class="flex md:hidden items-center space-x-5 md:ml-12 h-12"><span></span>
<button id=search-button-mobile aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></button>
<button id=appearance-switcher-mobile aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400" style=margin-right:5px><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div><div class="-my-2 -mr-2 md:hidden"><label id=menu-button class=block><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper style=padding-top:5px class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li id=menu-close-button><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>About</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Paper Reviews</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Tags</p></a></li><li class=mt-1><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li><li class=mt-1><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li></ul></div></label></div></div></div></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("menu-blur");n.style.opacity=t/300})</script><div class="relative flex flex-col grow"><main id=main-content class=grow><div class="fixed inset-x-0 top-0 h-[800px] single_hero_background nozoom" style=background-image:url(/ai-paper-reviewer/paper-reviews/cover_hu1795002977392341634.png)><div class="absolute inset-0 bg-gradient-to-t from-neutral dark:from-neutral-800 to-transparent mix-blend-normal"></div><div class="absolute inset-0 opacity-60 bg-gradient-to-t from-neutral dark:from-neutral-800 to-neutral-100 dark:to-neutral-800 mix-blend-normal"></div></div><div id=background-blur class="fixed opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl"></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("background-blur");n.style.opacity=t/300})</script><header><ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden"><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/>AI Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/>Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li></ol><h1 class="mt-5 text-4xl font-extrabold text-neutral-900 dark:text-neutral">Paper Reviews by AI</h1><div class="mt-1 mb-2 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"></div></div><script>var oid="views_paper-reviews/_index.md",oid_likes="likes_paper-reviews/_index.md"</script><script type=text/javascript src=/ai-paper-reviewer/js/page.min.0860cf4e04fa2d72cc33ddba263083464d48f67de06114529043cb4623319efed4f484fd7f1730df5abea0e2da6f3538855634081d02f2d6e920b956f063e823.js integrity="sha512-CGDPTgT6LXLMM926JjCDRk1I9n3gYRRSkEPLRiMxnv7U9IT9fxcw31q+oOLabzU4hVY0CB0C8tbpILlW8GPoIw=="></script></header><section class="mt-0 prose flex max-w-full flex-col dark:prose-invert lg:flex-row"><div class="min-w-0 min-h-0 max-w-prose"></div></section><h2 class="mt-12 mb-3 text-2xl font-bold text-neutral-700 first:mt-8 dark:text-neutral-300">0001</h2><section class="w-full grid gap-4 sm:grid-cols-2 md:grid-cols-3"><a href=/ai-paper-reviewer/paper-reviews/2410.21252/tables/table_8_1/ class=min-w-full><div class="min-h-full border border-neutral-200 dark:border-neutral-700 border-2 rounded overflow-hidden shadow-2xl relative"><div class="px-6 py-4"><div class="font-bold text-xl text-neutral-800 decoration-primary-500 hover:underline hover:underline-offset-2 dark:text-neutral" href=/ai-paper-reviewer/paper-reviews/2410.21252/tables/table_8_1/></div><div class="text-sm text-neutral-500 dark:text-neutral-400"><div class="flex flex-row flex-wrap items-center"><time datetime=0001-01-01T00:00:00+00:00>1 January 0001</time><span class="px-2 text-primary-500">&#183;</span><span>6 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">1 min</span></div><div class="flex flex-row flex-wrap items-center"></div></div><div class="py-1 prose dark:prose-invert"><table id=2 style=font-size:16px><tr><td>Method</td><td>Accuracy</td></tr><tr><td>SRM</td><td>0.583</td></tr><tr><td>Paired comparison</td><td>0.571</td></tr><tr><td>LongReward</td><td>0.662</td></tr><tr><td>w/o Helpfulness</td><td>0.631</td></tr><tr><td>w/o Logicality</td><td>0.623</td></tr><tr><td>w/o Faithfulness</td><td>0.578</td></tr><tr><td>w/o Completeness</td><td>0.578</td></tr></table></div></div><div class="px-6 pt-4 pb-2"></div></div></a><a href=/ai-paper-reviewer/paper-reviews/2410.21252/tables/table_8_0/ class=min-w-full><div class="min-h-full border border-neutral-200 dark:border-neutral-700 border-2 rounded overflow-hidden shadow-2xl relative"><div class="px-6 py-4"><div class="font-bold text-xl text-neutral-800 decoration-primary-500 hover:underline hover:underline-offset-2 dark:text-neutral" href=/ai-paper-reviewer/paper-reviews/2410.21252/tables/table_8_0/></div><div class="text-sm text-neutral-500 dark:text-neutral-400"><div class="flex flex-row flex-wrap items-center"><time datetime=0001-01-01T00:00:00+00:00>1 January 0001</time><span class="px-2 text-primary-500">&#183;</span><span>8 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">1 min</span></div><div class="flex flex-row flex-wrap items-center"></div></div><div class="py-1 prose dark:prose-invert"><table id=0 style=font-size:14px><tr><td rowspan=2>Model</td><td rowspan=2>Preference Data</td><td colspan=2>Long Benchmark</td><td colspan=2>Short Benchmark</td></tr><tr><td>LongBench-Chat</td><td>LongBench</td><td>MT-Bench</td><td>AlpacaEval2</td></tr><tr><td rowspan=3>Llama-3.1-8B</td><td>Short</td><td>70.6</td><td>54.5</td><td>7.48</td><td>15.8</td></tr><tr><td>Long</td><td>72.6</td><td>55.6</td><td>7.24</td><td>14.2</td></tr><tr><td>Short + Long</td><td>73.0</td><td>57.3</td><td>7.51</td><td>14.9</td></tr><tr><td rowspan=3>GLM-4-9B</td><td>Short</td><td>67.0</td><td>56.3</td><td>7.62</td><td>14.7</td></tr><tr><td>Long</td><td>69.2</td><td>59.7</td><td>7.58</td><td>15.2</td></tr><tr><td>Short + Long</td><td>70.2</td><td>58.7</td><td>7.61</td><td>15.4</td></tr></table></div></div><div class="px-6 pt-4 pb-2"></div></div></a><a href=/ai-paper-reviewer/paper-reviews/2410.21252/tables/table_7_2/ class=min-w-full><div class="min-h-full border border-neutral-200 dark:border-neutral-700 border-2 rounded overflow-hidden shadow-2xl relative"><div class="px-6 py-4"><div class="font-bold text-xl text-neutral-800 decoration-primary-500 hover:underline hover:underline-offset-2 dark:text-neutral" href=/ai-paper-reviewer/paper-reviews/2410.21252/tables/table_7_2/></div><div class="text-sm text-neutral-500 dark:text-neutral-400"><div class="flex flex-row flex-wrap items-center"><time datetime=0001-01-01T00:00:00+00:00>1 January 0001</time><span class="px-2 text-primary-500">&#183;</span><span>15 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">1 min</span></div><div class="flex flex-row flex-wrap items-center"></div></div><div class="py-1 prose dark:prose-invert"><br><table id=6 style=font-size:14px><tr><td>Method</td><td>MT-Bench</td><td>AlpacaEval2</td></tr><tr><td>Llama-3.1-8B</td><td></td><td></td></tr><tr><td>officially post-trained</td><td>8.13</td><td>22.9</td></tr><tr><td>SFT</td><td>7.12</td><td>12.4</td></tr><tr><td>DPO w/ SRM</td><td>7.58</td><td>13.7</td></tr><tr><td>DPO w/ Contrast</td><td>7.58</td><td>13.8</td></tr><tr><td>DPO w/ LongReward</td><td>7.24</td><td>14.2</td></tr><tr><td>GLM-4-9B</td><td></td><td></td></tr><tr><td>officially post-trained</td><td>8.09</td><td>22.4</td></tr><tr><td>SFT</td><td>7.37</td><td>12.5</td></tr><tr><td>DPO w/ SRM</td><td>7.50</td><td>14.2</td></tr><tr><td>DPO w/ Contrast</td><td>7.54</td><td>14.5</td></tr><tr><td>DPO w/ LongReward</td><td>7.58</td><td>15.4</td></tr></table></div></div><div class="px-6 pt-4 pb-2"></div></div></a><a href=/ai-paper-reviewer/paper-reviews/2410.21252/tables/table_7_1/ class=min-w-full><div class="min-h-full border border-neutral-200 dark:border-neutral-700 border-2 rounded overflow-hidden shadow-2xl relative"><div class="px-6 py-4"><div class="font-bold text-xl text-neutral-800 decoration-primary-500 hover:underline hover:underline-offset-2 dark:text-neutral" href=/ai-paper-reviewer/paper-reviews/2410.21252/tables/table_7_1/></div><div class="text-sm text-neutral-500 dark:text-neutral-400"><div class="flex flex-row flex-wrap items-center"><time datetime=0001-01-01T00:00:00+00:00>1 January 0001</time><span class="px-2 text-primary-500">&#183;</span><span>1 word</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">1 min</span></div><div class="flex flex-row flex-wrap items-center"></div></div><div class="py-1 prose dark:prose-invert"><table id=2 style=font-size:14px><tr><td></td><td>Win</td><td>Tie</td><td>Loss</td><td>△(Win-Loss)</td></tr><tr><td>Helpfulness</td><td>0.14</td><td>0.84</td><td>0.02</td><td>0.12</td></tr><tr><td>Logicality</td><td>0.14</td><td>0.86</td><td>0.00</td><td>0.14</td></tr><tr><td>Faithfulness</td><td>0.32</td><td>0.64</td><td>0.04</td><td>0.28</td></tr><tr><td>Completeness</td><td>0.26</td><td>0.64</td><td>0.10</td><td>0.16</td></tr><tr><td>Overall</td><td>0.54</td><td>0.38</td><td>0.08</td><td>0.46</td></tr></table></div></div><div class="px-6 pt-4 pb-2"></div></div></a><a href=/ai-paper-reviewer/paper-reviews/2410.21252/tables/table_7_0/ class=min-w-full><div class="min-h-full border border-neutral-200 dark:border-neutral-700 border-2 rounded overflow-hidden shadow-2xl relative"><div class="px-6 py-4"><div class="font-bold text-xl text-neutral-800 decoration-primary-500 hover:underline hover:underline-offset-2 dark:text-neutral" href=/ai-paper-reviewer/paper-reviews/2410.21252/tables/table_7_0/></div><div class="text-sm text-neutral-500 dark:text-neutral-400"><div class="flex flex-row flex-wrap items-center"><time datetime=0001-01-01T00:00:00+00:00>1 January 0001</time><span class="px-2 text-primary-500">&#183;</span><span>5 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">1 min</span></div><div class="flex flex-row flex-wrap items-center"></div></div><div class="py-1 prose dark:prose-invert"><table id=0 style=font-size:14px><tr><td>Method</td><td>#Facts</td><td>FactScore</td></tr><tr><td>Llama-3.1-8B</td><td></td><td></td></tr><tr><td>SFT</td><td>21.76</td><td>91.94</td></tr><tr><td>DPO w/ LongReward</td><td>32.86</td><td>92.85</td></tr><tr><td>GLM-4-9B</td><td></td><td></td></tr><tr><td>SFT</td><td>18.41</td><td>91.43</td></tr><tr><td>DPO w/ LongReward</td><td>28.05</td><td>93.62</td></tr></table></div></div><div class="px-6 pt-4 pb-2"></div></div></a><a href=/ai-paper-reviewer/paper-reviews/2410.21252/tables/table_6_1/ class=min-w-full><div class="min-h-full border border-neutral-200 dark:border-neutral-700 border-2 rounded overflow-hidden shadow-2xl relative"><div class="px-6 py-4"><div class="font-bold text-xl text-neutral-800 decoration-primary-500 hover:underline hover:underline-offset-2 dark:text-neutral" href=/ai-paper-reviewer/paper-reviews/2410.21252/tables/table_6_1/></div><div class="text-sm text-neutral-500 dark:text-neutral-400"><div class="flex flex-row flex-wrap items-center"><time datetime=0001-01-01T00:00:00+00:00>1 January 0001</time><span class="px-2 text-primary-500">&#183;</span><span>17 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">1 min</span></div><div class="flex flex-row flex-wrap items-center"></div></div><div class="py-1 prose dark:prose-invert"><table id=2 style=font-size:14px><tr><td rowspan=2>Model</td><td rowspan=2>Method</td><td rowspan=2>LongBench-Chat</td><td colspan=3>LongBench</td><td rowspan=2>Avg</td></tr><tr><td>S-Doc QA</td><td>M-Doc QA</td><td>Summ</td></tr><tr><td rowspan=5>Llama-3.1-8B</td><td>officially post-trained</td><td>60.2</td><td>59.3</td><td>42.9</td><td>35.3</td><td>49.4</td></tr><tr><td>SFT</td><td>69.8</td><td>66.1</td><td>44.5</td><td>39.6</td><td>55.0</td></tr><tr><td>DPO w/ SRM</td><td>67.4</td><td>65.0</td><td>49.6</td><td>42.7</td><td>56.2</td></tr><tr><td>DPO w/ Contrast</td><td>70.6</td><td>67.8</td><td>46.2</td><td>40.3</td><td>56.2</td></tr><tr><td>DPO w/ LongReward</td><td>72.6</td><td>67.8</td><td>55.8</td><td>43.2</td><td>59.9</td></tr><tr><td rowspan=5>GLM-4-9B</td><td>officially post-trained</td><td>68.6</td><td>67.8</td><td>56.9</td><td>47.9</td><td>60.3</td></tr><tr><td>SFT</td><td>64.8</td><td>68.4</td><td>50.9</td><td>42.1</td><td>56.6</td></tr><tr><td>DPO w/ SRM</td><td>66.6</td><td>67.5</td><td>57.4</td><td>48.2</td><td>59.9</td></tr><tr><td>DPO w/ Contrast</td><td>68.2</td><td>67.8</td><td>58.0</td><td>47.8</td><td>60.5</td></tr><tr><td>DPO w/ LongReward</td><td>69.2</td><td>71.9</td><td>58.8</td><td>48.5</td><td>62.1</td></tr></table></div></div><div class="px-6 pt-4 pb-2"></div></div></a><a href=/ai-paper-reviewer/paper-reviews/2410.21252/tables/table_6_0/ class=min-w-full><div class="min-h-full border border-neutral-200 dark:border-neutral-700 border-2 rounded overflow-hidden shadow-2xl relative"><div class="px-6 py-4"><div class="font-bold text-xl text-neutral-800 decoration-primary-500 hover:underline hover:underline-offset-2 dark:text-neutral" href=/ai-paper-reviewer/paper-reviews/2410.21252/tables/table_6_0/></div><div class="text-sm text-neutral-500 dark:text-neutral-400"><div class="flex flex-row flex-wrap items-center"><time datetime=0001-01-01T00:00:00+00:00>1 January 0001</time><span class="px-2 text-primary-500">&#183;</span><span>17 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">1 min</span></div><div class="flex flex-row flex-wrap items-center"></div></div><div class="py-1 prose dark:prose-invert"><table id=0 style=font-size:16px><tr><td>Dataset</td><td>Task Type</td><td>#Data</td><td>Avg Len</td><td>Language</td><td>Metric</td><td>Judge Model</td></tr><tr><td colspan=7>Long-context Benchmark</td></tr><tr><td>LongBench-Chat</td><td>Multi-Task</td><td>50</td><td>35,571</td><td>English/Chinese</td><td>Point-wise Rate</td><td>GPT-4o</td></tr><tr><td></td><td>Single-Doc QA</td><td>750</td><td>8,573</td><td>English/Chinese</td><td>Point-wise Rate</td><td>GPT-4o</td></tr><tr><td>LongBench</td><td>Multi-Doc QA</td><td>800</td><td>1,0255</td><td>English/Chinese</td><td>Point-wise Rate</td><td>GPT-4o</td></tr><tr><td></td><td>Summarization</td><td>800</td><td>9,210</td><td>English/Chinese</td><td>Point-wise Rate</td><td>GPT-4o</td></tr><tr><td colspan=7>Short-context Benchmark</td></tr><tr><td>MT-Bench</td><td>Instruction Following</td><td>80</td><td>-</td><td>English</td><td>Point-wise Rate</td><td>GPT-4</td></tr><tr><td>AlpacaEval2</td><td>Instruction Following</td><td>805</td><td>-</td><td>English</td><td>LC Win Rate</td><td>GPT-4-turbo</td></tr></table></div></div><div class="px-6 pt-4 pb-2"></div></div></a><a href=/ai-paper-reviewer/paper-reviews/2410.21169/tables/table_56_0/ class=min-w-full><div class="min-h-full border border-neutral-200 dark:border-neutral-700 border-2 rounded overflow-hidden shadow-2xl relative"><div class="px-6 py-4"><div class="font-bold text-xl text-neutral-800 decoration-primary-500 hover:underline hover:underline-offset-2 dark:text-neutral" href=/ai-paper-reviewer/paper-reviews/2410.21169/tables/table_56_0/></div><div class="text-sm text-neutral-500 dark:text-neutral-400"><div class="flex flex-row flex-wrap items-center"><time datetime=0001-01-01T00:00:00+00:00>1 January 0001</time><span class="px-2 text-primary-500">&#183;</span><span>456 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">3 mins</span></div><div class="flex flex-row flex-wrap items-center"></div></div><div class="py-1 prose dark:prose-invert"><table id=0 style=font-size:18px><tr><td>[374]</td><td>Junyu Luo, Zekun Li, Jinpeng Wang, and Chin- Yew Lin. Chartocr: Data extraction from charts images via a deep hybrid framework. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 1917-1925, 2021.</td></tr><tr><td>[375]</td><td>Joseph Shtok, Sivan Harary, Ophir Azulai, Adi Raz Goldfarb, Assaf Arbelle, and Leonid Karlin- sky. Charter: heatmap-based multi-type chart data extraction. arXiv preprint arXiv:2111.14103, 2021.</td></tr><tr><td>[376]</td><td>Renqiu Xia, Bo Zhang, Haoyang Peng, Hancheng Ye, Xiangchao Yan, Peng Ye, Botian Shi, Yu Qiao, and Junchi Yan. Structchart: Perception, structuring, reasoning for visual chart understanding. arXiv preprint arXiv:2309.11268, 2023.</td></tr><tr><td>[377]</td><td>Jinyue Chen, Lingyu Kong, Haoran Wei, Chenglong Liu, Zheng Ge, Liang Zhao, Jianjian Sun, Chunrui Han, and Xiangyu Zhang. Onechart: Purify the chart structural extraction via one auxiliary token. arXiv preprint arXiv:2404.09987, 2024.</td></tr><tr><td>[378]</td><td>Jason Obeid and Enamul Hoque. Chart-to-text: Generating natural language descriptions for charts by adapting the transformer model. arXiv preprint arXiv:2010.09142, 2020.</td></tr><tr><td>[379]</td><td>Yucheng Han, Chi Zhang, Xin Chen, Xu Yang, Zhibin Wang, Gang Yu, Bin Fu, and Hanwang Zhang. Chartllama: A multimodal llm for chart understanding and generation. arXiv preprint arXiv:2311.16483, 2023.</td></tr><tr><td>[380]</td><td>Zheng Huang, Kai Chen, Jianhua He, Xiang Bai, Dimosthenis Karatzas, Shijian Lu, and CV Jawahar. Icdar2019 competition on scanned receipt ocr and information extraction. In 2019 International Conference on Document Analysis and Recognition (ICDAR), pages 1516- 1520. IEEE, 2019.</td></tr><tr><td>[381]</td><td>Anni Zou, Wenhao Yu, Hongming Zhang, Kaixin Ma, Deng Cai, Zhuosheng Zhang, Hai Zhao, and Dong Yu. Docbench: A benchmark for evaluating llm-based document reading systems. arXiv preprint arXiv:2407.10701, 2024.</td></tr><tr><td>[382]</td><td>Karin Verspoor, Dat Quoc Nguyen, Saber A Akhondi, Christian Druckenbrodt, Camilo Thorne, Ralph Hoessel, Jiayuan He, and Zenan Zhai. Chemu dataset for information extraction from chemical patents. Mendeley Data, 2(10):17632, 2020.</td></tr><tr><td>[383]</td><td>Shivalika Tanwar, Patrick Auberger, Germain Gillet, Mario DiPaola, Katya Tsaioun, and Bruno 0 Villoutreix. A new chembl dataset for the similarity-based target fishing engine fasttargetpred: Annotation of an exhaustive list of linear tetrapeptides. Data in Brief, 42: 108159, 2022.</td></tr><tr><td>[384]</td><td>Jan Hajic and Pavel Pecina. The muscima++ dataset for handwritten optical music recognition. In 2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR), volume 1, pages 39-46. IEEE, 2017.</td></tr><tr><td>[385]</td><td>Lukas Tuggener, Ismail Elezi, Jurgen Schmidhuber, Marcello Pelillo, and Thilo Stadelmann. Deepscores-a dataset for segmentation, detection and classification of tiny objects. In 2018 24th International Conference on Pattern Recognition (ICPR), pages 3704-3709. IEEE, 2018.</td></tr><tr><td>[386]</td><td>Zelun Wang and Jyh-Charn Liu. Translating math formula images to latex sequences using deep neural networks with sequence-level training. International Journal on Document Analysis and Recognition (IJDAR), 24(1):63-75, 2021.</td></tr><tr><td>[387]</td><td>Bin Wang, Fan Wu, Linke Ouyang, Zhuangcheng Gu, Rui Zhang, Renqiu Xia, Bo Zhang, and Conghui He. Cdm: A reliable metric for fair and accurate formula recognition evaluation. arXiv preprint arXiv:2409.03643, 2024.</td></tr><tr><td>[388]</td><td>Pratik Kayal, Mrinal Anand, Harsh Desai, and Mayank Singh. Tables to latex: structure and content extraction from scientific tables. International Journal on Document Analysis and Recognition (IJDAR), 26(2):121-130, 2023.</td></tr></table></div></div><div class="px-6 pt-4 pb-2"></div></div></a><a href=/ai-paper-reviewer/paper-reviews/2410.21169/tables/table_51_0/ class=min-w-full><div class="min-h-full border border-neutral-200 dark:border-neutral-700 border-2 rounded overflow-hidden shadow-2xl relative"><div class="px-6 py-4"><div class="font-bold text-xl text-neutral-800 decoration-primary-500 hover:underline hover:underline-offset-2 dark:text-neutral" href=/ai-paper-reviewer/paper-reviews/2410.21169/tables/table_51_0/></div><div class="text-sm text-neutral-500 dark:text-neutral-400"><div class="flex flex-row flex-wrap items-center"><time datetime=0001-01-01T00:00:00+00:00>1 January 0001</time><span class="px-2 text-primary-500">&#183;</span><span>509 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">3 mins</span></div><div class="flex flex-row flex-wrap items-center"></div></div><div class="py-1 prose dark:prose-invert"><table id=0 style=font-size:18px><tr><td>[298]</td><td>Chinmayee Rane, Seshasayee Mahadevan Subramanya, Devi Sandeep Endluri, Jian Wu, and C Lee Giles. Chartreader: Automatic parsing of bar-plots. In 2021 IEEE 22nd International Conference on Information Reuse and Integration for Data Science (IRI), pages 318-325. IEEE, 2021.</td></tr><tr><td>[299]</td><td>Renqiu Xia, Bo Zhang, Hancheng Ye, Xiangchao Yan, Qi Liu, Hongbin Zhou, Zijun Chen, Min Dou, Botian Shi, Junchi Yan, et al. Chartx & chartvlm: A versatile benchmark and foundation model for complicated chart reasoning. arXiv preprint arXiv:2402.12185, 2024.</td></tr><tr><td>[300]</td><td>Muhammad Yusuf Hassan, Mayank Singh, et al. Lineex: data extraction from scientific line charts. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 6213-6221, 2023.</td></tr><tr><td>[301]</td><td>Ceres Carton, Aurelie Lemaitre, and Bertrand Couasnon. Fusion of statistical and structural information for flowchart recognition. In 2013 12th International Conference on Document Analysis and Recognition, pages 1210-1214. IEEE, 2013.</td></tr><tr><td>[302]</td><td>Mar�al Rusinol, Lluis-Pere de las Heras, Joan Mas, Oriol Ramos Terrades, Dimosthenis Karatzas, Anjan Dutta, Gemma Sanchez, and Josep Llados. Cvc-uab's participation in the flowchart recognition task of clef-ip 2012. In CLEF (Online Working Notes/Labs/Workshop), 2012.</td></tr><tr><td>[303]</td><td>Hugh A Chipman, Edward I George, Robert E McCulloch, and Thomas S Shively. mbart: multidimensional monotone bart. Bayesian Analysis, 17(2):515-544, 2022.</td></tr><tr><td>[304]</td><td>Haoran Wei, Lingyu Kong, Jinyue Chen, Liang Zhao, Zheng Ge, Jinrong Yang, Jianjian Sun, Chunrui Han, and Xiangyu Zhang. Vary: Scaling up the vision vocabulary for large vision-language models. arXiv preprint arXiv:2312.06109, 2023.</td></tr><tr><td>[305]</td><td>Jiawei Wang, Kai Hu, Zhuoyao Zhong, Lei Sun, and Qiang Huo. Detect-order-construct: A tree construction based approach for hierarchical document structure analysis. arXiv preprint arXiv:2401.11874, 2024.</td></tr><tr><td>[306]</td><td>Jianqiang Wan, Sibo Song, Wenwen Yu, Yuliang Liu, Wenqing Cheng, Fei Huang, Xiang Bai, Cong Yao, and Zhibo Yang. Omniparser: A unified framework for text spotting key information extraction and table recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15641-15653, 2024.</td></tr><tr><td>[307]</td><td>Christos Papadopoulos, Stefan Pletschacher, Christian Clausner, and Apostolos Antonacopou- los. The impact dataset of historical document images. In Proceedings of the 2Nd international workshop on historical document imaging and processing, pages 123-130, 2013.</td></tr><tr><td>[308]</td><td>Mukkai Krishnamoorthy, George Nagy, Sharad Seth, and Mahesh Viswanathan. Syntactic segmentation and labeling of digitized pages from technical journals. IEEE Transactions on Pattern Analysis and Machine Intelligence, 15(7):737-747, 1993.</td></tr><tr><td>[309]</td><td>David Lewis, Gady Agam, Shlomo Argamon, Ophir Frieder, David Grossman, and Jefferson Heard. Building a test collection for complex document information processing. In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 665-666, 2006.</td></tr><tr><td>[310]</td><td>Apostolos Antonacopoulos, David Bridson, Christos Papadopoulos, and Stefan Pletschacher. A realistic dataset for performance evaluation of document layout analysis. In 2009 10th International Conference on Document Analysis and Recognition, pages 296-300. IEEE, 2009.</td></tr><tr><td>[311]</td><td>Rana SM Saad, Randa I Elanwar, NS Abdel Kader, Samia Mashali, and Margrit Betke. Bce-arabic-v1 dataset: Towards interpreting arabic document images for people with vi- sual impairments. In Proceedings of the 9th ACM International Conference on PErvasive Technologies Related to Assistive Environments, pages 1-8, 2016.</td></tr><tr><td>[312]</td><td>Fotini Simistira, Manuel Bouillon, Mathias Seuret, Marcel W�rsch, Michele Alberti, Rolf Ingold, and Marcus Liwicki. Icdar2017 competition on layout analysis for challenging medieval manuscripts. In 2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR), volume 1, pages 1361-1370. IEEE, 2017.</td></tr></table></div></div><div class="px-6 pt-4 pb-2"></div></div></a><a href=/ai-paper-reviewer/paper-reviews/2410.21169/tables/table_50_0/ class=min-w-full><div class="min-h-full border border-neutral-200 dark:border-neutral-700 border-2 rounded overflow-hidden shadow-2xl relative"><div class="px-6 py-4"><div class="font-bold text-xl text-neutral-800 decoration-primary-500 hover:underline hover:underline-offset-2 dark:text-neutral" href=/ai-paper-reviewer/paper-reviews/2410.21169/tables/table_50_0/></div><div class="text-sm text-neutral-500 dark:text-neutral-400"><div class="flex flex-row flex-wrap items-center"><time datetime=0001-01-01T00:00:00+00:00>1 January 0001</time><span class="px-2 text-primary-500">&#183;</span><span>470 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">3 mins</span></div><div class="flex flex-row flex-wrap items-center"></div></div><div class="py-1 prose dark:prose-invert"><table id=0 style=font-size:18px><tr><td>[282]</td><td>Zengyuan Guo, Yuechen Yu, Pengyuan Lv, Chengquan Zhang, Haojie Li, Zhihui Wang, Kun Yao, Jingtuo Liu, and Jingdong Wang. Trust: An accurate and end-to-end table structure recognizer using splitting-based transformers. arXiv preprint arXiv:2208.14687, 2022.</td></tr><tr><td>[283]</td><td>Tao Zhang, Yi Sui, Shunyao Wu, Fengjing Shao, and Rencheng Sun. Table structure recog- nition method based on lightweight network and channel attention. Electronics, 12(3):673, 2023.</td></tr><tr><td>[284]</td><td>Fangyu Liu, Julian Martin Eisenschlos, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton Lee, Mandar Joshi, Wenhu Chen, Nigel Collier, and Yasemin Altun. Deplot: One-shot visual language reasoning by plot-to-table translation. arXiv preprint arXiv:2212.10505, 2022.</td></tr><tr><td>[285]</td><td>Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems, 25, 2012.</td></tr><tr><td>[286]</td><td>Christopher Clark and Santosh Divvala. Pdffigures 2.0: Mining figures from research papers. In Proceedings of the 16th ACM/IEEE-CS on Joint Conference on Digital Libraries, pages 143-152, 2016.</td></tr><tr><td>[287]</td><td>Noah Siegel, Nicholas Lourie, Russell Power, and Waleed Ammar. Extracting scientific figures with distantly supervised neural networks. In Proceedings of the 18th ACM/IEEE on joint conference on digital libraries, pages 223-232, 2018.</td></tr><tr><td>[288]</td><td>Manolis Savva, Nicholas Kong, Arti Chhajta, Li Fei-Fei, Maneesh Agrawala, and Jeffrey Heer. Revision: Automated classification, analysis and redesign of chart images. In Proceedings of the 24th annual ACM symposium on User interface software and technology, pages 393-402, 2011.</td></tr><tr><td>[289]</td><td>Ales Mishchenko and Natalia Vassilieva. Chart image understanding and numerical data extraction. In 2011 Sixth International Conference on Digital Information Management, pages 115-120. IEEE, 2011.</td></tr><tr><td>[290]</td><td>Haixia Liu and Tim Brailsford. Reproducing show, attend and tell: Neural image caption generation with visual attention. In Journal of Physics: Conference Series, volume 2589, page 012012. IOP Publishing, 2023.</td></tr><tr><td>[291]</td><td>Junqi Jin, Kun Fu, Runpeng Cui, Fei Sha, and Changshui Zhang. Aligning where to see and what to tell: image caption with region-based attention and scene factorization. arXiv preprint arXiv:1506.06272, 2015.</td></tr><tr><td>[292]</td><td>Sameer Antani, Dina Demner-Fushman, Jiang Li, Balaji V Srinivasan, and George R Thoma. Exploring use of images in clinical articles for decision support in evidence-based medicine. In Document Recognition and Retrieval XV, volume 6815, pages 230-239. SPIE, 2008.</td></tr><tr><td>[293]</td><td>Beibei Cheng, Sameer Antani, R Joe Stanley, and George R Thoma. Automatic segmentation of subfigure image panels for multimodal biomedical document retrieval. In Document Recognition and Retrieval XVIII, volume 7874, pages 294-304. SPIE, 2011.</td></tr><tr><td>[294]</td><td>Daliang Xu, Hao Zhang, Liming Yang, Ruiqi Liu, Gang Huang, Mengwei Xu, and Xuanzhe Liu. Empowering 1000 tokens/second on-device llm prefilling with mllm-npu. arXiv preprint arXiv:2407.05858, 2024.</td></tr><tr><td>[295]</td><td>Weihua Huang, Chew Lim Tan, and Wee Kheng Leow. Associating text and graphics for scientific chart understanding. In Eighth International Conference on Document Analysis and Recognition (ICDAR '05), pages 580-584. IEEE, 2005.</td></tr><tr><td>[296]</td><td>Weihua Huang and Chew Lim Tan. A system for understanding imaged infographics and its applications. In Proceedings of the 2007 ACM symposium on Document engineering, pages 9-18, 2007.</td></tr><tr><td>[297]</td><td>Sagnik Ray Choudhury, Shuting Wang, Prasenjit Mitra, and C Lee Giles. Automated data extraction from scholarly line graphs. In Proc. Int. Workshop Graph. Recognit, 2015.</td></tr></table></div></div><div class="px-6 pt-4 pb-2"></div></div></a><a href=/ai-paper-reviewer/paper-reviews/2410.21169/tables/table_38_0/ class=min-w-full><div class="min-h-full border border-neutral-200 dark:border-neutral-700 border-2 rounded overflow-hidden shadow-2xl relative"><div class="px-6 py-4"><div class="font-bold text-xl text-neutral-800 decoration-primary-500 hover:underline hover:underline-offset-2 dark:text-neutral" href=/ai-paper-reviewer/paper-reviews/2410.21169/tables/table_38_0/></div><div class="text-sm text-neutral-500 dark:text-neutral-400"><div class="flex flex-row flex-wrap items-center"><time datetime=0001-01-01T00:00:00+00:00>1 January 0001</time><span class="px-2 text-primary-500">&#183;</span><span>481 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">3 mins</span></div><div class="flex flex-row flex-wrap items-center"></div></div><div class="py-1 prose dark:prose-invert"><table id=0 style=font-size:18px><tr><td>[94]</td><td>Jianshu Zhang, Jun Du, and Lirong Dai. Multi-scale attention with dense encoder for hand- written mathematical expression recognition. In 2018 24th international conference on pattern recognition (ICPR), pages 2245-2250. IEEE, 2018.</td></tr><tr><td>[95]</td><td>Zhe Li, Lianwen Jin, Songxuan Lai, and Yecheng Zhu. Improving attention-based handwritten mathematical expression recognition with scale augmentation and drop attention. In 2020 17th International Conference on Frontiers in Handwriting Recognition (ICFHR), pages 175-180. IEEE, 2020.</td></tr><tr><td>[96]</td><td>Bin Wang, Zhuangcheng Gu, Chao Xu, Bo Zhang, Botian Shi, and Conghui He. Unimernet: A universal network for real-world mathematical expression recognition. arXiv preprint arXiv:2404.15254, 2024.</td></tr><tr><td>[97]</td><td>Wei Zhang, Zhiqiang Bai, and Yuesheng Zhu. An improved approach based on cnn-rnns for mathematical expression recognition. In Proceedings of the 2019 4th international conference on multimedia systems and signal processing, pages 57-61, 2019.</td></tr><tr><td>[98]</td><td>Wenqi Zhao, Liangcai Gao, Zuoyu Yan, Shuai Peng, Lin Du, and Ziyin Zhang. Handwritten mathematical expression recognition with bidirectionally trained transformer. In Document analysis and recognition-ICDAR 2021: 16th international conference, Lausanne, Switzerland, September 5-10, 2021, proceedings, part II 16, pages 570-584. Springer, 2021.</td></tr><tr><td>[99]</td><td>Wenqi Zhao and Liangcai Gao. Comer: Modeling coverage for transformer-based handwritten mathematical expression recognition. In European conference on computer vision, pages 392-408. Springer, 2022.</td></tr><tr><td>[100]</td><td>Bohan Li, Ye Yuan, Dingkang Liang, Xiao Liu, Zhilong Ji, Jinfeng Bai, Wenyu Liu, and Xiang Bai. When counting meets hmer: counting-aware network for handwritten mathematical expression recognition. In European conference on computer vision, pages 197-214. Springer, 2022.</td></tr><tr><td>[101]</td><td>Jianhua Zhu, Liangcai Gao, and Wenqi Zhao. Ical: Implicit character-aided learning for enhanced handwritten mathematical expression recognition. In International Conference on Document Analysis and Recognition, pages 21-37. Springer, 2024.</td></tr><tr><td>[102]</td><td>Chungkwong Chan. Stroke extraction for offline handwritten mathematical expression recog- nition. IEEE Access, 8:61565-61575, 2020.</td></tr><tr><td>[103]</td><td>Jiaming Wang, Jun Du, Jianshu Zhang, and Zi-Rui Wang. Multi-modal attention network for handwritten mathematical expression recognition. In 2019 International Conference on Document Analysis and Recognition (ICDAR), pages 1181-1186. IEEE, 2019.</td></tr><tr><td>[104]</td><td>Leipeng Hao, Liangcai Gao, Xiaohan Yi, and Zhi Tang. A table detection method for pdf documents based on convolutional neural networks. In 2016 12th IAPR Workshop on Document Analysis Systems (DAS), pages 287-292. IEEE, 2016.</td></tr><tr><td>[105]</td><td>Azka Gilani, Shah Rukh Qasim, Imran Malik, and Faisal Shafait. Table detection using deep learning. In 2017 14th IAPR international conference on document analysis and recognition (ICDAR), volume 1, pages 771-776. IEEE, 2017.</td></tr><tr><td>[106]</td><td>Sebastian Schreiber, Stefan Agne, Ivo Wolf, Andreas Dengel, and Sheraz Ahmed. Deepdesrt: Deep learning for detection and structure recognition of tables in document images. In 2017 14th IAPR international conference on document analysis and recognition (ICDAR), volume 1, pages 1162-1167. IEEE, 2017.</td></tr><tr><td>[107]</td><td>Shoaib Ahmed Siddiqui, Muhammad Imran Malik, Stefan Agne, Andreas Dengel, and Sheraz Ahmed. Decnt: Deep deformable cnn for table detection. IEEE access, 6:74151-74161, 2018.</td></tr><tr><td>[108]</td><td>Yilun Huang, Qinqin Yan, Yibo Li, Yifan Chen, Xiong Wang, Liangcai Gao, and Zhi Tang. A yolo-based table detection method. In 2019 International Conference on Document Analysis and Recognition (ICDAR), pages 813-818. IEEE, 2019.</td></tr><tr><td>[109]</td><td>Bin Xiao, Murat Simsek, Burak Kantarci, and Ala Abu Alkheir. Table detection for visually rich document images. Knowledge-Based Systems, 282:111080, 2023.</td></tr></table></div></div><div class="px-6 pt-4 pb-2"></div></div></a><a href=/ai-paper-reviewer/paper-reviews/2410.21169/tables/table_29_0/ class=min-w-full><div class="min-h-full border border-neutral-200 dark:border-neutral-700 border-2 rounded overflow-hidden shadow-2xl relative"><div class="px-6 py-4"><div class="font-bold text-xl text-neutral-800 decoration-primary-500 hover:underline hover:underline-offset-2 dark:text-neutral" href=/ai-paper-reviewer/paper-reviews/2410.21169/tables/table_29_0/></div><div class="text-sm text-neutral-500 dark:text-neutral-400"><div class="flex flex-row flex-wrap items-center"><time datetime=0001-01-01T00:00:00+00:00>1 January 0001</time><span class="px-2 text-primary-500">&#183;</span><span>211 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">1 min</span></div><div class="flex flex-row flex-wrap items-center"></div></div><div class="py-1 prose dark:prose-invert"><br><table id=2 style=font-size:14px><tr><td>Tools</td><td>Developer</td><td>Time</td><td>Introduction</td></tr><tr><td>GROBID</td><td>Patrice Lopez</td><td>2011</td><td>A machine learning library that focuses on extracting and restructuring original documents, converting them into structured formats such as XML/TEI encoding.</td></tr><tr><td>PyMuPDF</td><td>Jorj X. McKie</td><td>2011</td><td>A Python library for extracting, analyzing, converting, and processing data from PDFs and other documents, supporting tables, figures, and other types of content.</td></tr><tr><td>doc2text</td><td>Joe Sutherland</td><td>2016.9</td><td>Specializes in extracting low-quality documents; only ensures compatibility in Linux.</td></tr><tr><td>pdfplumber</td><td>Jeremy Singer- Vine</td><td>2019.1</td><td>Tools for extraction and parsing of characters, images, lines, tables, and other elements from digital PDF documents.</td></tr><tr><td>Parsr</td><td>axa-group</td><td>2019.8</td><td>A tool for cleaning, parsing, and extracting content from various document types, with outputs including JSON, Markdown, CSV/pandasDF, and txt formats.</td></tr><tr><td>PP-StructureV2</td><td>Baidu</td><td>2021.8</td><td>Intelligent document analysis system, supports layout analysis of Chinese and English documents, table recognition, and semantic recognition.</td></tr><tr><td>DocxChain</td><td>Alibaba</td><td>2023.9</td><td>A system for non-structured or semi-structured document conversion into various information and formats, including complex document applications based on computational capabilities.</td></tr><tr><td>pdf2htmlEX</td><td>Lu Wang</td><td>2023.12</td><td>A project to convert PDF documents into HTML format.</td></tr><tr><td>MinerU</td><td>OpenDataLab</td><td>2024.4</td><td>A system for extracting content from PDF and converting it into markdown or JSON formats.</td></tr><tr><td>PDF-Extract-Kit</td><td>OpenDataLab</td><td>2024.7</td><td>A system based on MinerU to extract various content from PDF, including layout analysis, OCR, table recognition, and formula recognition tasks.</td></tr><tr><td>OmniParser</td><td>Adithya s Kolavi</td><td>2024.6</td><td>A platform for extracting and parsing any unstructured data, transforming it into structured, actionable data optimized for GenAI applications.</td></tr><tr><td>LLM_aided_ocr</td><td>Jeff Emanuel</td><td>2024.8</td><td>Uses Tesseract for document OCR, followed by LLM-based error correction, with final output in markdown or similar formats.</td></tr></table></div></div><div class="px-6 pt-4 pb-2"></div></div></a><a href=/ai-paper-reviewer/paper-reviews/2410.21169/tables/table_28_0/ class=min-w-full><div class="min-h-full border border-neutral-200 dark:border-neutral-700 border-2 rounded overflow-hidden shadow-2xl relative"><div class="px-6 py-4"><div class="font-bold text-xl text-neutral-800 decoration-primary-500 hover:underline hover:underline-offset-2 dark:text-neutral" href=/ai-paper-reviewer/paper-reviews/2410.21169/tables/table_28_0/></div><div class="text-sm text-neutral-500 dark:text-neutral-400"><div class="flex flex-row flex-wrap items-center"><time datetime=0001-01-01T00:00:00+00:00>1 January 0001</time><span class="px-2 text-primary-500">&#183;</span><span>157 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">1 min</span></div><div class="flex flex-row flex-wrap items-center"></div></div><div class="py-1 prose dark:prose-invert"><table id=2 style=font-size:14px><tr><td>Metric</td><td>Definition</td><td>Description</td></tr><tr><td>Purity</td><td>k 1 Purity = max ICinL⌀l N j i=1 k 1</td><td>Measures the level of noise contained in the detected results.</td></tr><tr><td>Completeness</td><td>Completeness max |Lj n Cil N 2 j=1</td><td>Measure the proportion of table areas detected within the tables.</td></tr><tr><td>CAR</td><td>�i=1 1(predicted adjacency(Ci) = true adjacency(Ci)) CAR n</td><td>Evaluates boundary detection and relative positioning of table cells, reflecting the structural relationships of the table.</td></tr><tr><td>TEDS</td><td>TED(T1 , T2) TEDS(T1,T2) = 1 - max(size(T1), size(T2)) AcolEd (i)H</td><td>Measures similarity based on tree edit distance, focusing on table structure, including tags and content.</td></tr><tr><td>Aall</td><td>K⌀il ArowSt (i) n ArowEd (i) n AcolSt(i) n Aall = N</td><td>A cell's prediction is considered correct if and only if all four of its logical positions are accurately predicted.</td></tr><tr><td>F_beta</td><td>(1 +0.52) . H . Aall F�=0.5 = 0.52 . H + Aall</td><td>Combines spatial positioning and logical accuracy, balancing evaluation better than F1-score. layout and spatial location</td></tr><tr><td>WAF</td><td>� =1 IoUi · F�= 1 @IoUi W AF = �1=1 IoU⌀</td><td>Evaluates adjacency relation prediction based on intersection over union (IoU).</td></tr></table></div></div><div class="px-6 pt-4 pb-2"></div></div></a><a href=/ai-paper-reviewer/paper-reviews/2410.21169/tables/table_27_1/ class=min-w-full><div class="min-h-full border border-neutral-200 dark:border-neutral-700 border-2 rounded overflow-hidden shadow-2xl relative"><div class="px-6 py-4"><div class="font-bold text-xl text-neutral-800 decoration-primary-500 hover:underline hover:underline-offset-2 dark:text-neutral" href=/ai-paper-reviewer/paper-reviews/2410.21169/tables/table_27_1/></div><div class="text-sm text-neutral-500 dark:text-neutral-400"><div class="flex flex-row flex-wrap items-center"><time datetime=0001-01-01T00:00:00+00:00>1 January 0001</time><span class="px-2 text-primary-500">&#183;</span><span>97 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">1 min</span></div><div class="flex flex-row flex-wrap items-center"></div></div><div class="py-1 prose dark:prose-invert"><table id=4 style=font-size:16px><tr><td>Metric</td><td>Definition</td><td>Description</td></tr><tr><td>ExpRate</td><td>Number of exact matches ExpRate Total number of samples</td><td>Measures the proportion of samples that are com- pletely correct, suitable for scenarios requiring high accuracy.</td></tr><tr><td>MSE</td><td>m n 1 MSE ��(I(i,j) - K(i,j))2 mn i=1 j=1</td><td>Measures the average squared difference between corresponding pixels in two images.</td></tr><tr><td>SSIM</td><td>(2�x�� + C1)(2�xy + C2) SSIM(x, y) = (사로 + 사립 + C1)(⌀2 + ⌀2 + C2)</td><td>Measures the structural similarity of images, taking into account brightness, contrast, and structural in- formation.</td></tr><tr><td>CDM</td><td>2x TP CDM = 2 x TP + FP + FN</td><td>Converts LaTeX mathematical expression into im- age and matches it with the corresponding image structure.</td></tr></table></div></div><div class="px-6 pt-4 pb-2"></div></div></a><a href=/ai-paper-reviewer/paper-reviews/2410.21169/tables/table_27_0/ class=min-w-full><div class="min-h-full border border-neutral-200 dark:border-neutral-700 border-2 rounded overflow-hidden shadow-2xl relative"><div class="px-6 py-4"><div class="font-bold text-xl text-neutral-800 decoration-primary-500 hover:underline hover:underline-offset-2 dark:text-neutral" href=/ai-paper-reviewer/paper-reviews/2410.21169/tables/table_27_0/></div><div class="text-sm text-neutral-500 dark:text-neutral-400"><div class="flex flex-row flex-wrap items-center"><time datetime=0001-01-01T00:00:00+00:00>1 January 0001</time><span class="px-2 text-primary-500">&#183;</span><span>114 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">1 min</span></div><div class="flex flex-row flex-wrap items-center"></div></div><div class="py-1 prose dark:prose-invert"><table id=1 style=font-size:14px><tr><td>Metric</td><td>Definition</td><td>Description</td></tr><tr><td>CER</td><td>S+ D + I CER N</td><td>Measures the character-level discrepancy between recognized and ground truth text, suitable for OCR tasks requiring high precision.</td></tr><tr><td>Edit Distance</td><td>D(i-1,j)+1 D(i,j) = min [ D(i,j -1)+1 D(i - 1, j -1) + Cost(s1 [i], s2[j]) N</td><td>Measures the minimum edit distance needed to convert recognized text into ground truth text.</td></tr><tr><td>BLEU</td><td>BLEU = BP x exp M Wn log Pn ) n=1</td><td>Measures the minimum edit distance needed to convert recognized text into ground truth text.</td></tr><tr><td>METEOR</td><td>METEOR = Fmean x (1 - Penalty)</td><td>Accounts for both precision and recall, and supports stem and synonym matching.</td></tr><tr><td>ROUGE-N</td><td>� ngramE Reference min(Countmatch (ngram), Countcandidate (ngram)) ROUGE-N = ngramEReference Countreference (ngram)</td><td>An improved version of BLEU that focuses on recall rather than precision.</td></tr></table></div></div><div class="px-6 pt-4 pb-2"></div></div></a><a href=/ai-paper-reviewer/paper-reviews/2410.21169/tables/table_26_0/ class=min-w-full><div class="min-h-full border border-neutral-200 dark:border-neutral-700 border-2 rounded overflow-hidden shadow-2xl relative"><div class="px-6 py-4"><div class="font-bold text-xl text-neutral-800 decoration-primary-500 hover:underline hover:underline-offset-2 dark:text-neutral" href=/ai-paper-reviewer/paper-reviews/2410.21169/tables/table_26_0/></div><div class="text-sm text-neutral-500 dark:text-neutral-400"><div class="flex flex-row flex-wrap items-center"><time datetime=0001-01-01T00:00:00+00:00>1 January 0001</time><span class="px-2 text-primary-500">&#183;</span><span>85 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">1 min</span></div><div class="flex flex-row flex-wrap items-center"></div></div><div class="py-1 prose dark:prose-invert"><table id=6 style=font-size:14px><tr><td>Metric</td><td>Definition</td><td>Description</td></tr><tr><td>IoU</td><td>Area of Overlap IoU = Area of Union TP</td><td>Measures the overlap between predicted and ground truth boxes.</td></tr><tr><td>ReCall</td><td>ReCall = TP + FN N</td><td>Measures how many true positive samples are correctly predicted by the model.</td></tr><tr><td>mAP</td><td>1 mAP = APi N i=1 M 1</td><td>Average precision across all classes, assessing overall model performance.</td></tr><tr><td>mAP@IoU[a:b]</td><td>mAP@IoU[a:b] = mAPI⌀U j M j=1</td><td>Computes over a range of IoU thresholds [a, b], calculating at specified intervals and averaged.</td></tr><tr><td>F1-score</td><td>Precision X Recall F1-score = 2 x Precision + Recall</td><td>Balances precision and recall and useful in imbalanced class scenarios.</td></tr></table></div></div><div class="px-6 pt-4 pb-2"></div></div></a><a href=/ai-paper-reviewer/paper-reviews/2410.21169/tables/table_25_0/ class=min-w-full><div class="min-h-full border border-neutral-200 dark:border-neutral-700 border-2 rounded overflow-hidden shadow-2xl relative"><div class="px-6 py-4"><div class="font-bold text-xl text-neutral-800 decoration-primary-500 hover:underline hover:underline-offset-2 dark:text-neutral" href=/ai-paper-reviewer/paper-reviews/2410.21169/tables/table_25_0/></div><div class="text-sm text-neutral-500 dark:text-neutral-400"><div class="flex flex-row flex-wrap items-center"><time datetime=0001-01-01T00:00:00+00:00>1 January 0001</time><span class="px-2 text-primary-500">&#183;</span><span>144 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">1 min</span></div><div class="flex flex-row flex-wrap items-center"></div></div><div class="py-1 prose dark:prose-invert"><table id=1 style=font-size:14px><tr><td>Dataset</td><td>Year</td><td>Instance</td><td>Class</td><td>Task</td><td>Feature</td></tr><tr><td>DeepChart [368]</td><td>2015</td><td>5000</td><td>5</td><td>Chart Classification</td><td></td></tr><tr><td>VIEW [369]</td><td>2012</td><td>300</td><td>3</td><td>Chart Classification</td><td>-</td></tr><tr><td>ReVision [288]</td><td>2011</td><td>2601</td><td>10</td><td>Chart Classification</td><td>Based on ChartSense dataset</td></tr><tr><td>CHART 2019 [370] - PMC</td><td>2019</td><td>4242</td><td>multi-class</td><td>Chart Classification</td><td>Real charts from scientific publications</td></tr><tr><td>CHART 2019 - Syn- thetic [371]</td><td>2019</td><td>202,550</td><td>multi-class</td><td>Chart Classification</td><td>Synthetic charts</td></tr><tr><td>DocFigure [370]</td><td>2019</td><td>33000</td><td>28</td><td>Chart Classification</td><td>Includes various figure images</td></tr><tr><td>UB-PMC 2019 [370]</td><td>2019</td><td>4242</td><td>7</td><td>Chart Classification</td><td>Competition dataset</td></tr><tr><td>UB-PMC 2020 [372]</td><td>2020</td><td>2123</td><td>4</td><td>Chart Data Extraction</td><td>Real charts from PubMedCentra</td></tr><tr><td>UM-PMC 2021 [373]</td><td>2021</td><td>22924</td><td>15</td><td>Chart Classification</td><td>Competition dataset</td></tr><tr><td>UB-PMC 2022 [136]</td><td>2022</td><td>33186</td><td>15</td><td>Chart Classification</td><td>Competition dataset</td></tr><tr><td>Synth 2020 [373]</td><td>2020</td><td>9600</td><td>4</td><td>Chart Data Extraction</td><td>Synthetic charts</td></tr><tr><td>LINEEX430k [300]</td><td>2023</td><td>430,000</td><td>Line charts</td><td>Chart Data Extraction</td><td>Focused on line charts</td></tr><tr><td>ICPR 2022 [136]</td><td>2022</td><td>26,596</td><td>15</td><td>Chart Classification</td><td>Charts with embedded text</td></tr><tr><td>ExcelChart400K</td><td>[3742021</td><td>400,0000</td><td>Pie and bar charts</td><td>Chart Data Extraction</td><td>Extracted from Excel charts with JSON annotations</td></tr><tr><td>CHARTER [375]</td><td>2021</td><td>32334</td><td>4</td><td>Chart Data Extraction</td><td>Sourced from document pages, web pages, PubMed, FigureQA, etc.</td></tr><tr><td>StructChart dataset [376]</td><td>2023</td><td>16466</td><td>Organization and structure charts</td><td>Chart Structure Extraction</td><td>-</td></tr><tr><td>OneChart [377]</td><td>2023</td><td>10000000</td><td>5</td><td>Chart Information Extraction, QA, and Inference</td><td>Synthesized using Matplotlib</td></tr><tr><td>Chart-to-Text [378]</td><td>2023</td><td>8305</td><td>6</td><td>Chart Information Extraction</td><td>Contains chart samples and corresponding data</td></tr><tr><td>ChartLlama [379]</td><td>2023</td><td>1500</td><td>10</td><td>7 comprehensive chart tasks in- cluding chart information extrac- tion</td><td>GPT-4 generates charts and instruction data</td></tr><tr><td>ChartX [299]</td><td>2024</td><td>48000</td><td>18</td><td>7 comprehensive chart tasks in- cluding chart information extrac- tion</td><td>Automatically generated by GPT-4 and manually checked</td></tr></table></div></div><div class="px-6 pt-4 pb-2"></div></div></a><a href=/ai-paper-reviewer/paper-reviews/2410.21169/tables/table_23_1/ class=min-w-full><div class="min-h-full border border-neutral-200 dark:border-neutral-700 border-2 rounded overflow-hidden shadow-2xl relative"><div class="px-6 py-4"><div class="font-bold text-xl text-neutral-800 decoration-primary-500 hover:underline hover:underline-offset-2 dark:text-neutral" href=/ai-paper-reviewer/paper-reviews/2410.21169/tables/table_23_1/></div><div class="text-sm text-neutral-500 dark:text-neutral-400"><div class="flex flex-row flex-wrap items-center"><time datetime=0001-01-01T00:00:00+00:00>1 January 0001</time><span class="px-2 text-primary-500">&#183;</span><span>168 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">1 min</span></div><div class="flex flex-row flex-wrap items-center"></div></div><div class="py-1 prose dark:prose-invert"><table id=3 style=font-size:16px><tr><td>Dataset</td><td>Instance</td><td>Type</td><td>Language</td><td>Task</td><td>Feature</td></tr><tr><td>ICDAR2013 [349]</td><td>150</td><td>Government Documents</td><td>English</td><td>TD & TSR</td><td>Covers complex structures and cross-page tables</td></tr><tr><td>ICDAR2017 POD [342]</td><td>1548</td><td>Scientific papers</td><td>English</td><td>TD</td><td>Includes shape and formula detec- tion</td></tr><tr><td>ICDAR2019 [350]</td><td>2439</td><td>Multiple Types</td><td>English</td><td>TD & TSR</td><td>Includes historical and modern ta- bles</td></tr><tr><td>TABLE2LATEX-450K [124]</td><td>140000</td><td>Scientific papers</td><td>English</td><td>TSR</td><td></td></tr><tr><td>RVL-CDIP (subset) [351]</td><td>518</td><td>Receipts</td><td>English</td><td>TD</td><td>Derived from RVL-CDIP</td></tr><tr><td>IIIT-AR-13K [352]</td><td>17,000 (not only tables)</td><td>Annual Reports</td><td>Multi-langugae</td><td>TD</td><td>Does not only contain tables</td></tr><tr><td>CamCap [353]</td><td>85</td><td>Table images</td><td>English</td><td>TD & TSR</td><td>Used for evaluating table detection in camera-captured images</td></tr><tr><td>UNLV Table [354]</td><td>2889</td><td>Journals, Newspapers, Business Letters</td><td>English</td><td>TD</td><td></td></tr><tr><td>UW-3 Table [355]</td><td>1,600 (around 120 tables)</td><td>Books, Magazines</td><td>English</td><td>TD</td><td>Manually labeled bounding boxes</td></tr><tr><td>Marmot [356]</td><td>2000</td><td>Conference Papers</td><td>English and Chinese</td><td>TD</td><td>Includes diversified table types; still expanding</td></tr><tr><td>TableBank [357]</td><td>417234</td><td>Multiple Types</td><td>English</td><td>TD & TSR</td><td>Automatically created by weakly su- pervised methods</td></tr><tr><td>DeepFigures [287]</td><td>5,500,000 (tables and figures)</td><td>Scientific papers</td><td>English</td><td>TD</td><td>Supports figure extraction</td></tr><tr><td>PubTabNet [125]</td><td>568000</td><td>Scientific papers</td><td>English</td><td>TSR</td><td>Structure and content recognition of tables</td></tr><tr><td>PubTables-1M [358]</td><td>1000000</td><td>Scientific papers</td><td>English</td><td>TSR [122]</td><td>Evaluates the oversegmentation is- sue</td></tr><tr><td>SciTSR [359]</td><td>15000</td><td>Scientific papers</td><td>English</td><td>TSR</td><td></td></tr><tr><td>FinTable [359]</td><td>112887</td><td>Scientific and Financial Tables</td><td>English</td><td>TD & TSR</td><td>Automatic Annotation methods</td></tr><tr><td>SynthTabNet [360]</td><td>600000</td><td>Multiple Types</td><td>English</td><td>TD & TSR</td><td>Synthetic tables</td></tr><tr><td>Wired Table in the Wild [121]</td><td>14582 (pages)</td><td>Photos, Files, and Web Pages</td><td>English</td><td>TSR</td><td>Deformed and occluded images</td></tr><tr><td>WikiTableSet [361]</td><td>50000000</td><td>Wikipedia</td><td>English, Japanese, French</td><td>TSR</td><td></td></tr><tr><td>STDW [362]</td><td>7000</td><td>Multiple Types</td><td>English</td><td>TD</td><td></td></tr><tr><td>TableGraph-350K [363]</td><td>358,767</td><td>Academic Table</td><td>English</td><td>TSR</td><td>including TableGraph-24K</td></tr><tr><td>TabRecSet [364]</td><td>38100</td><td>Multiple Types</td><td>English and Chinese</td><td>TSR</td><td></td></tr><tr><td>DECO [365]</td><td>1165</td><td>Multiple Types</td><td>English</td><td>TD</td><td>Enron document electronic table files</td></tr><tr><td>iFLYTAB [366]</td><td>17291</td><td>Multiple Types</td><td>Chinese and English</td><td>TD & TSR</td><td>Online and offline tables from vari- ous scenarios</td></tr><tr><td>FinTab [367]</td><td>1,600</td><td>Financial Table</td><td>Chinese</td><td>TSR</td><td></td></tr></table></div></div><div class="px-6 pt-4 pb-2"></div></div></a><a href=/ai-paper-reviewer/paper-reviews/2410.21169/tables/table_23_0/ class=min-w-full><div class="min-h-full border border-neutral-200 dark:border-neutral-700 border-2 rounded overflow-hidden shadow-2xl relative"><div class="px-6 py-4"><div class="font-bold text-xl text-neutral-800 decoration-primary-500 hover:underline hover:underline-offset-2 dark:text-neutral" href=/ai-paper-reviewer/paper-reviews/2410.21169/tables/table_23_0/></div><div class="text-sm text-neutral-500 dark:text-neutral-400"><div class="flex flex-row flex-wrap items-center"><time datetime=0001-01-01T00:00:00+00:00>1 January 0001</time><span class="px-2 text-primary-500">&#183;</span><span>41 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">1 min</span></div><div class="flex flex-row flex-wrap items-center"></div></div><div class="py-1 prose dark:prose-invert"><table id=1 style=font-size:18px><tr><td>Dataset</td><td>Image</td><td>Instance</td><td>Type</td><td>Task</td></tr><tr><td>UW-III [339]</td><td>100</td><td>/</td><td>Inline and displayed Formula</td><td>MED</td></tr><tr><td>InftyCDB-1 [340]</td><td>467</td><td>21000</td><td>Inline and displayed Formula</td><td>MED</td></tr><tr><td>Marmo [341]t</td><td>594</td><td>9500</td><td>Inline and displayed Formula</td><td>MED</td></tr><tr><td>ICDAR-2017 POD [342]</td><td>3900</td><td>5400</td><td>Only displayed Formula</td><td>MED</td></tr><tr><td>TFD-ICDAR 2019 [343]</td><td>851</td><td>38000</td><td>Inline and displayed Formula</td><td>MED</td></tr><tr><td>ICDAR-2021 IBEM [344]</td><td>8900</td><td>166000</td><td>Inline and displayed Formula</td><td>MED</td></tr><tr><td>FormulaNet [345]</td><td>46,672</td><td>1000,00</td><td>Inline and displayed Formula</td><td>MED</td></tr><tr><td>ArxivFormula [91]</td><td>700000</td><td>813.3</td><td>Inline and displayed Formula</td><td>MED</td></tr><tr><td>Pix2tex [346]</td><td colspan=2>189117</td><td>Printed</td><td>MER</td></tr><tr><td>CROHME [347]</td><td colspan=2>12178</td><td>Handwritten</td><td>MER</td></tr><tr><td>HME100K [348]</td><td colspan=2>99109</td><td>Handwritten</td><td>MER</td></tr><tr><td>UniMERNet [96]</td><td colspan=2>1,061,791</td><td>Printed and Handwritten</td><td>MER</td></tr></table></div></div><div class="px-6 pt-4 pb-2"></div></div></a><a href=/ai-paper-reviewer/paper-reviews/2410.21169/tables/table_22_0/ class=min-w-full><div class="min-h-full border border-neutral-200 dark:border-neutral-700 border-2 rounded overflow-hidden shadow-2xl relative"><div class="px-6 py-4"><div class="font-bold text-xl text-neutral-800 decoration-primary-500 hover:underline hover:underline-offset-2 dark:text-neutral" href=/ai-paper-reviewer/paper-reviews/2410.21169/tables/table_22_0/></div><div class="text-sm text-neutral-500 dark:text-neutral-400"><div class="flex flex-row flex-wrap items-center"><time datetime=0001-01-01T00:00:00+00:00>1 January 0001</time><span class="px-2 text-primary-500">&#183;</span><span>87 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">1 min</span></div><div class="flex flex-row flex-wrap items-center"></div></div><div class="py-1 prose dark:prose-invert"><table id=3 style=font-size:14px><tr><td>Dataset</td><td>Instance</td><td>Task</td><td>Feature</td><td>Language</td></tr><tr><td>IIIT5K [324]</td><td>5000</td><td>TR</td><td>Real-world scene text</td><td>English</td></tr><tr><td>Street View Text [325]</td><td>647</td><td>TD</td><td>Street View</td><td>English</td></tr><tr><td>Street View Text Per- spective [326]</td><td>645</td><td>TD</td><td>Street View with per- spective distortion</td><td>English</td></tr><tr><td>ICDAR 2003 [327]</td><td>507</td><td>TD & TR</td><td>Real-world short scene text</td><td>English</td></tr><tr><td>ICDAR 2013 [328]</td><td>462</td><td>TD & TR</td><td>Real-world short scene text</td><td>English</td></tr><tr><td>MSRA-TD500 [329]</td><td>500</td><td>TD</td><td>Rotated text</td><td>English, Chinese</td></tr><tr><td>CUTE80 [330]</td><td>13000</td><td>TD & TR</td><td>Curved text</td><td>English</td></tr><tr><td>COCO-Text [331]</td><td>63,686</td><td>TD & TR</td><td>Real-world short scene text</td><td>English</td></tr><tr><td>ICDAR 2015 [332]</td><td>1500</td><td>TD & TR & TS</td><td>Incidental Scene Text</td><td>English</td></tr><tr><td>SCUT-CTW1500 [333]</td><td>1500</td><td>TD</td><td>Curved text</td><td>English, Chinese</td></tr><tr><td>Total-Text [334]</td><td>1555</td><td>TD & TR</td><td>Multi-oriented scene text</td><td>English, Chinese</td></tr><tr><td>SynthText [335]</td><td>800,000</td><td>TD & TR</td><td>Synthetic images</td><td>English</td></tr><tr><td>SynthAdd [336]</td><td>1,200,000</td><td>TD & TR</td><td>Synthetic images</td><td>English</td></tr><tr><td>Occlusion Scene Text [80]</td><td>4832</td><td>TD</td><td>Occlusion text</td><td>English</td></tr><tr><td>WordArt [337]</td><td>6316</td><td>TR</td><td>Artistic text</td><td>English</td></tr><tr><td>ICDAR2019-ReCTS [338]</td><td>25,000</td><td>TD & TR & TS</td><td>TD & TR & Document Structure Analysis</td><td>Chinese</td></tr></table></div></div><div class="px-6 pt-4 pb-2"></div></div></a></section><ul class="flex flex-row mt-8 justify-center"><li><a href=/ai-paper-reviewer/paper-reviews/page/10/ class="mx-1 block min-w-[1.8rem] rounded text-center hover:bg-primary-600 hover:text-neutral" rel=prev>&larr;</a></li><li><a href=/ai-paper-reviewer/paper-reviews/ class="mx-1 block min-w-[1.8rem] rounded text-center hover:bg-primary-600 hover:text-neutral">1</a></li><li class=page-item><span class=page-link aria-hidden=true>&mldr;</span></li><li><a href=/ai-paper-reviewer/paper-reviews/page/9/ class="mx-1 block min-w-[1.8rem] rounded text-center hover:bg-primary-600 hover:text-neutral">9</a></li><li><a href=/ai-paper-reviewer/paper-reviews/page/10/ class="mx-1 block min-w-[1.8rem] rounded text-center hover:bg-primary-600 hover:text-neutral">10</a></li><li><a href=/ai-paper-reviewer/paper-reviews/page/11/ class="bg-primary-200 dark:bg-primary-400 dark:text-neutral-800
mx-1 block min-w-[1.8rem] rounded text-center hover:bg-primary-600 hover:text-neutral">11</a></li><li><a href=/ai-paper-reviewer/paper-reviews/page/12/ class="mx-1 block min-w-[1.8rem] rounded text-center hover:bg-primary-600 hover:text-neutral">12</a></li><li><a href=/ai-paper-reviewer/paper-reviews/page/12/ class="mx-1 block min-w-[1.8rem] rounded text-center hover:bg-primary-600 hover:text-neutral" rel=next>&rarr;</a></li></ul></main><footer id=site-footer class="py-10 print:hidden"><nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400"><ul class="flex flex-col list-none sm:flex-row"><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=/ai-paper-reviewer/tags/ title>Tags</a></li></ul></nav><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2024
AI Paper Reviews by AI</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a></p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/ai-paper-reviewer/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh+sCQ0E53ghYrxgYqw+0GCRyIEpA=="></script></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://deep-diver.github.io/ai-paper-reviewer/ style=z-index:500><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body><script data-name=BMC-Widget data-cfasync=false src=https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js data-id=chansung data-description="Support me on Buy me a coffee!" data-message data-color=#FFDD00 data-position=Left data-x_margin=18 data-y_margin=18></script></html>