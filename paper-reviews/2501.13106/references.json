{"references": [{"fullname_first_author": "Bo Li", "paper_title": "LLaVA-OneVision: Easy visual task transfer", "publication_date": "2024-08-08", "reason": "This paper introduces LLaVA-OneVision, a foundational model for image understanding that is leveraged in VideoLLaMA3's training paradigm, showcasing its influence on the model's overall design and performance."}, {"fullname_first_author": "Zhe Chen", "paper_title": "InternVL: Scaling up vision foundation models and aligning for generic visual-linguistic tasks", "publication_date": "2023-12-12", "reason": "InternVL, a strong baseline model for image understanding, is compared against VideoLLaMA3 across multiple benchmarks, demonstrating the importance of InternVL's research in the field."}, {"fullname_first_author": "Zesen Cheng", "paper_title": "VideoLLaMA 2: Advancing spatial-temporal modeling and audio understanding in video-LLMs", "publication_date": "2024-06-06", "reason": "VideoLLaMA 2 serves as a direct predecessor to VideoLLaMA3, highlighting the iterative development and improvements made between the models."}, {"fullname_first_author": "Muhammad Maaz", "paper_title": "VideoGPT+: Integrating image and video encoders for enhanced video understanding", "publication_date": "2024-04-04", "reason": "VideoGPT+, another significant baseline model, is evaluated against VideoLLaMA3 to showcase its performance and contribution to the field of video understanding."}, {"fullname_first_author": "Hang Zhang", "paper_title": "VideoLLaMA: An instruction-tuned audio-visual language model for video understanding", "publication_date": "2023-06-06", "reason": "VideoLLaMA, an earlier model by the same research group, is referenced to provide context for VideoLLaMA3's advancements, demonstrating the progression in model development."}]}