[{"figure_path": "https://arxiv.org/html/2503.19990/x1.png", "caption": "Figure 1: Problem Statistics in LEGO-Puzzles.", "description": "This figure shows the breakdown of the 1100+ visual question answering pairs in the LEGO-Puzzles benchmark dataset.  It displays the number of questions categorized into three main task types: Spatial Understanding (fundamental spatial reasoning), Single-Step Sequential Reasoning (reasoning across one step in a LEGO construction sequence), and Multi-Step Sequential Reasoning (reasoning across multiple steps). The specific tasks within each category are also listed, along with the percentage of questions belonging to each.", "section": "3. LEGO-Puzzles"}, {"figure_path": "https://arxiv.org/html/2503.19990/x2.png", "caption": "Figure 2: Task examples of LEGO-Puzzles. From left to right, the columns represent tasks in Spatial Understanding, Single-Step Sequential Reasoning, and Multi-Step Sequential Reasoning. Note: The questions above are slightly simplified for clarity and brevity.", "description": "Figure 2 presents example questions from the LEGO-Puzzles benchmark, categorized by task type.  The figure showcases the visual complexity and diversity of the questions, which range from basic spatial understanding (e.g., identifying the relative heights of LEGO pieces, determining adjacency, understanding rotations and viewpoints) to single-step sequential reasoning (e.g., determining the next assembly state given instructions, identifying required pieces, assessing rotation needs) and multi-step sequential reasoning (e.g., determining the correct order of steps, identifying incorrect intermediate states).  This visual representation helps illustrate the progression of task difficulty and the range of spatial reasoning skills assessed by the LEGO-Puzzles benchmark.", "section": "3. LEGO-Puzzles"}, {"figure_path": "https://arxiv.org/html/2503.19990/x3.png", "caption": "Figure 3: Data curation pipeline.\nOur pipeline first collects a diverse set of LEGO building instructions to render and extract LEGO images in a unified format. Next, we generate question-answer pairs by using a combination of human annotation and predefined question templates. Finally, we implement three quality control strategies to ensure the accuracy, consistency, and reliability of the data.", "description": "This figure illustrates the process of creating the LEGO-Puzzles dataset.  It starts with collecting diverse LEGO building instructions. These instructions are then rendered into images with consistent formatting.  Next, question-answer pairs are generated, combining human annotation with pre-defined templates.  Finally, a three-stage quality control process is implemented to ensure accuracy, consistency and reliability of the resulting dataset.", "section": "3. LEGO-Puzzles"}, {"figure_path": "https://arxiv.org/html/2503.19990/x4.png", "caption": "Figure 4: Task-specific template. Our question-answer template includes instructions, questions, and answers. Here, we provide an example from the Position task for reference.", "description": "Figure 4 shows a template used in the LEGO-Puzzles benchmark for creating question-answer pairs.  It illustrates the structure of the template, which includes three parts: instructions explaining the task to the model (in this case, determining the correct assembly point of a LEGO piece), a question presenting a specific scenario with images of the current state, the next piece, and the state after installation, and finally the ground truth answer providing the correct choice from several options given.", "section": "3. LEGO-Puzzles"}, {"figure_path": "https://arxiv.org/html/2503.19990/x5.png", "caption": "Figure 5: Task Similarity Heatmap. The heatmap illustrates the pairwise correlation between tasks in our benchmark, measured using SRCC, PLCC, and R\u00b2 scores.", "description": "This heatmap visualizes the pairwise correlations between different tasks within the LEGO-Puzzles benchmark.  It shows how strongly the performance on one task is related to performance on another.  The correlations are calculated using three different methods: Spearman Rank Correlation Coefficient (SRCC), Pearson Linear Correlation Coefficient (PLCC), and R-squared (R\u00b2).  Stronger correlations (closer to 1) are represented by darker colors, indicating tasks that share underlying skills or cognitive processes. Weaker correlations (closer to 0) are shown in lighter colors, implying greater independence between the tasks. This analysis helps to understand the structure of the benchmark and ensures diversity among the tasks, preventing redundancy and bias in the overall evaluation of MLLM capabilities.", "section": "4.6. Task Similarity"}, {"figure_path": "https://arxiv.org/html/2503.19990/x6.png", "caption": "Figure 6: Visualization of sample failure cases in Height and Ordering. The Ground Truth answer is marked in blue, while the MLLM\u2019s answer is marked in red. Note: The questions above are slightly simplified for clarity and brevity.", "description": "This figure showcases examples where large language models (LLMs) failed to correctly answer questions from the LEGO-Puzzles benchmark.  It highlights two specific task types: 'Height', which assesses the ability to determine the relative height of LEGO pieces, and 'Ordering', which tests the capacity to arrange steps in a LEGO assembly sequence.  The figure presents the original question, the correct answer (in blue), and the incorrect answer generated by the LLM (in red).  This visually demonstrates the challenges that LLMs face in spatial reasoning, particularly when handling 2D representations of 3D objects and understanding the order of sequential steps.", "section": "5. Error Analysis"}, {"figure_path": "https://arxiv.org/html/2503.19990/x7.png", "caption": "Figure 7: Qualitative image generation results for Rotation* and Multiview* tasks. Note: The questions above are slightly simplified for clarity and brevity.", "description": "This figure showcases the qualitative results of image generation tests performed on two specific tasks within the LEGO-Puzzles benchmark: Rotation* and Multiview*.  The Rotation* task evaluates the model's ability to generate an image of a LEGO object rotated by a specified angle compared to a reference image.  The Multiview* task assesses the model's capacity to produce an image of a LEGO object viewed from a different perspective than shown in a reference image.  The figure displays example images generated by various models, highlighting differences in their ability to both accurately depict the LEGO structure and correctly follow the instructions for generating the images. Note that the image questions shown in the caption are slightly simplified versions.", "section": "4.3. Image Generation Evaluation"}]