[{"figure_path": "2410.18362/tables/table_1_0.md", "caption": "Table 2: Main results on the WebSight-Test dataset.", "description": "This table presents the performance comparison of different techniques on the WebSight-Test dataset.  It shows the results for HTML-Match (percentage of perfectly matched generated images), CW-SSIM (complex wavelet structural similarity index), CLIP (similarity between rendered webpage and ground truth based on CLIP embeddings), and Low-Level Element Matching (LLEM) which measures the percentage of matched text blocks, text content, position, and font color.  The techniques compared include prompting with several large language models (Gemini 1.5 Pro, GPT-40 mini, GPT-40) and standard fine-tuning (FT) with two backbones (Moondream2 and VLM-WebSight), with WAFFLE showing significant improvements over the standard fine-tuning.", "section": "4 Results"}, {"figure_path": "2410.18362/tables/table_6_0.md", "caption": "Main results on the WebSight-Test dataset.", "description": "This table presents the performance comparison of different techniques on the WebSight-Test dataset.  The techniques compared include prompting using various models (Gemini 1.5 Pro, GPT-4o mini, GPT-4o), standard fine-tuning (FT), and WAFFLE.  Metrics used for comparison are HTML-Match (percentage of generated images perfectly matching ground truth at the pixel level), CW-SSIM (complex wavelet structural similarity index between images), CLIP (similarity between rendered webpages using CLIP embeddings), and Low-Level Element Matching (LLEM; percentage of matched text blocks, text content, position, and font color). The table shows that WAFFLE significantly outperforms standard fine-tuning on all metrics, with Moondream2 and VLM-WebSight as backbones, and also provides comparisons against prompting-based approaches with various large language models.", "section": "4 Results"}, {"figure_path": "2410.18362/tables/table_7_0.md", "caption": "Table 4: Ablation studies on the two test datasets. LLEM refers to the averaged Low-Level Element Matching.", "description": "This table presents the ablation study results on two datasets, WebSight-Test and Design2Code, comparing four different model training techniques: Standard FT (standard fine-tuning), WAFFLE-attn (WAFFLE with only contrastive learning), WAFFLE-contra (WAFFLE with only structure-aware attention), and WAFFLE (the full model with both contrastive learning and structure-aware attention).  For both Moondream2 and VLM-WebSight backbones, the table shows the performance of each technique across multiple metrics: HTML-Match (percentage of generated images perfectly matching ground truth), CW-SSIM (complex wavelet structural similarity index), CLIP (similarity score using CLIP embeddings), and LLEM (low-level element matching).  The results demonstrate the individual and combined effects of contrastive learning and structure-aware attention on the overall model performance.", "section": "4.2 Ablation Studies"}, {"figure_path": "2410.18362/tables/table_7_1.md", "caption": "Table 5: Human evaluation on two datasets using VLM-WebSight as the backbone. The numbers are shown as \"xly (x+y)\", where x is the result on WebSight-Test and y is the result on Design2Code.", "description": "This table presents the results of a human evaluation comparing four different techniques: Standard FT, WAFFLE-attn, WAFFLE-contra, and WAFFLE.  The evaluation was performed on two datasets, WebSight-Test and Design2Code, using the VLM-WebSight model as the backbone.  For each technique, the table shows the number of times each technique received a rank of 1, 2, and 3 by human evaluators, along with the average ranking across both datasets.  The numbers in parentheses represent the sum of ranks across the two datasets.", "section": "4.3 Structure-Aware Attention's Effect"}, {"figure_path": "2410.18362/tables/table_7_2.md", "caption": "Table 6: CW-SSIM on 20 samples using the VLM-WebSight backbone. \u201cPrior\u201d refers to \u201cwithout intermediate mistakes\u201d, and \u201cCurrent\u201d to \u201cwith intermediate mistakes\u201d.", "description": "Table 6 presents the results of a study evaluating the robustness of two models, WAFFLE-attn and WAFFLE, to intermediate errors during HTML code generation.  Twenty samples where generation errors occurred mid-process were selected. The models were then tasked with re-completing the HTML code, starting from the error point. The table shows the average Complex Wavelet Structural Similarity Index (CW-SSIM) score for each model across the 20 samples, comparing the results when intermediate errors are present ('Current') against when errors are not present ('Prior').  The data illustrates the impact of the structure-aware attention mechanism on the model's ability to recover from intermediate errors.", "section": "4.3 Structure-Aware Attention's Effect"}, {"figure_path": "2410.18362/tables/table_11_0.md", "caption": "Table 7: Specification for Mutation Rules to construct the Contrastive dataset.", "description": "This table specifies the mutation rules used to create the contrastive learning dataset.  It outlines the failure types for both CSS styles and HTML structures and details the specifications for each. For CSS, it covers color, size, margin, font, display, and position properties, listing possible values or keywords for each.  For HTML structure mutations, it describes the process of randomly duplicating specific HTML elements while excluding certain key elements like , , , and .  These mutations introduce variations in the data to aid in the contrastive learning process.", "section": "A.1 Mutation Rules"}, {"figure_path": "2410.18362/tables/table_12_0.md", "caption": "Table 8: Distance (d) and similarity (sim) between averaged image embeddings v<sup>i</sup> and text embeddings t<sup>i</sup>, using Moondream2 as the backbone.", "description": "This table presents a comparison of the distance (d) and similarity (sim) between averaged image embeddings (v<sup>i</sup>) and text embeddings (t<sup>i</sup>) for two different techniques: Standard FT and WAFFLE-attn, using the Moondream2 model as the backbone.  The distance metric used is Euclidean distance, and the similarity metric is cosine similarity. Lower distance and higher similarity indicate better alignment between image and text representations. The results show that WAFFLE-attn achieves significantly lower distance (0.8447 vs 1.3395) and higher similarity (0.6244 vs 0.1027) compared to Standard FT, suggesting a much better alignment between visual and textual understanding in the WAFFLE-attn model.", "section": "4.2 Ablation Studies"}, {"figure_path": "2410.18362/tables/table_13_0.md", "caption": "Table 9: Distance (d) and similarity (sim) between each averaged image embeddings v\u00b2 with the corresponding centroid c of the group of mutants, with Moondream2 backbone.", "description": "This table presents the results of an experiment designed to assess the ability of two models (Standard FT and WAFFLE-attn) to distinguish subtle differences in images.  It shows the computed Euclidean distance (d) and cosine similarity (sim) between each averaged image embedding (v\u00b2) and the centroid (c) of its corresponding group of mutated images. The values indicate that WAFFLE-attn achieves a substantially larger distance (0.7590 vs 0.1224) and lower similarity (0.6202 vs 0.9910) compared to Standard FT, demonstrating its superior ability to capture and differentiate subtle variations in image data.", "section": "4.2 Ablation Studies"}]