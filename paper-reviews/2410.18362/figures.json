[{"figure_path": "2410.18362/figures/figures_1_0.png", "caption": "Figure 1: Removing the children of the element <div id=\"left-column\"> highlighted in yellow does not affect the structure of the visual layout of itself or its sibling element <div id=\"right-column\">.", "description": "Figure 1 shows that removing child elements from a parent element in HTML does not affect the visual layout of the parent element or its sibling elements.", "section": "1 Introduction"}, {"figure_path": "2410.18362/figures/figures_3_0.png", "caption": "Figure 3: Overview of WAFFLE, including training data mutation, structure-aware attention, and contrastive learning.", "description": "The figure illustrates the overall architecture of WAFFLE, showing its training data mutation process, structure-aware attention mechanism, and contrastive learning objective.", "section": "2 Approach"}, {"figure_path": "2410.18362/figures/figures_4_0.png", "caption": "Figure 4: Example of structure-aware attention.", "description": "The figure illustrates WAFFLE's structure-aware attention mechanism, showing how tokens attend to parent, sibling, and self elements in the HTML code.", "section": "2.2 Structure-Aware Attention"}, {"figure_path": "2410.18362/figures/figures_12_0.png", "caption": "Figure 5: Example test instance from WebSight-Test dataset, with the generated images by GPT-40, Standard FT, and WAFFLE.", "description": "The figure shows a comparison of webpage generation results from GPT-40, standard fine-tuning, and WAFFLE on a sample from the WebSight-Test dataset, highlighting WAFFLE's superior performance.", "section": "4.2 Ablation Studies"}, {"figure_path": "2410.18362/figures/figures_12_1.png", "caption": "Figure 6: Illustration of the tuning process of the parameter that controls the effect of structure-aware attention. In (b), the green line almost overlaps with the blue line.", "description": "The figure shows the effect of different portions of structure-aware attention heads on validation LLEM score and training loss.", "section": "2.2 Structure-Aware Attention"}]