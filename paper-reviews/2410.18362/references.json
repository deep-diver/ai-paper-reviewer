{"references": [{" publication_date": "2021", "fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "reason": "This paper is foundational for the field of multi-modal large language models (MLLMs), introducing a method for learning transferable visual models from natural language supervision.  This is directly relevant to WAFFLE, which leverages MLLMs for UI-to-HTML code generation, and thus, understanding the advancements and limitations of this core approach is crucial for the paper's context.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Jean-Baptiste Alayrac", "paper_title": "Flamingo: a visual language model for few-shot learning", "reason": "This paper introduces Flamingo, a visual language model, which directly relates to WAFFLE's use of MLLMs. Understanding the capabilities and limitations of Flamingo, as a state-of-the-art MLLM, is important in evaluating WAFFLE's improvements and contributions to the field.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Jinze Bai", "paper_title": "Qwen-vl: A frontier large vision-language model with versatile abilities", "reason": "This paper introduces Qwen-VL, a large vision-language model, which serves as a significant comparison point for WAFFLE.  Analyzing Qwen-VL's performance and characteristics provides insights into the state-of-the-art capabilities of MLLMs, enriching the assessment of WAFFLE's novelty and efficacy.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Lukas Blecher", "paper_title": "Nougat: Neural optical understanding for academic documents", "reason": "While not directly related to UI-to-HTML generation, this paper discusses techniques for neural optical understanding, which is indirectly relevant to WAFFLE's goal of bridging the gap between visual understanding of UI images and text understanding of HTML code.  The advancements in this area contribute to the broader context of multi-modal learning.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Lin Chen", "paper_title": "Sharegpt4v: Improving large multi-modal models with better captions", "reason": "This paper focuses on improving large multi-modal models by generating better captions. This is relevant to WAFFLE's objective of improving the generation of HTML code from UI images, as both tasks involve understanding visual and textual information and aligning them to produce accurate results.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Wenliang Dai", "paper_title": "Instructblip: Towards general-purpose vision-language models with instruction tuning", "reason": "This paper explores general-purpose vision-language models using instruction tuning. This approach is relevant to WAFFLE, which also fine-tunes an MLLM for a specific task. Understanding the techniques used in InstructBLIP helps to contextualize WAFFLE's approach and evaluate its effectiveness.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Daniel Fried", "paper_title": "Incoder: A generative model for code infilling and synthesis", "reason": "This paper discusses Incoder, a generative model for code, which is highly related to WAFFLE's objective of generating HTML code.  Analyzing Incoder's performance and methods provides valuable context for evaluating WAFFLE's efficiency and innovativeness in the code generation domain.", "section_number": 1}, {" publication_date": "2021", "fullname_first_author": "Tianyu Gao", "paper_title": "SimCSE: Simple contrastive learning of sentence embeddings", "reason": "This paper introduces SimCSE, a simple contrastive learning method for sentence embeddings. This technique is directly relevant to WAFFLE, which uses contrastive learning to align the model's understanding of UI images and HTML code.  The insights from SimCSE are crucial to understanding and evaluating WAFFLE's approach.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Yi Gui", "paper_title": "Vision2ui: A real-world dataset with layout for code generation from ui designs", "reason": "This paper introduces a new dataset, Vision2UI, specifically designed for UI-to-code generation. This is relevant to WAFFLE, which uses a modified version of the WebSight dataset. Understanding Vision2UI's design and characteristics helps contextualize WAFFLE's dataset selection and evaluation.", "section_number": 5}, {" publication_date": "2022", "fullname_first_author": "Junnan Li", "paper_title": "BLIP: Bootstrapping language-image pre-training for unified vision-language understanding and generation", "reason": "This paper introduces BLIP, a foundational model for vision-language understanding and generation.  This is crucial for WAFFLE's context as it builds upon the advancements in MLLMs for image-to-text tasks, providing insights into the effectiveness and potential limitations of this approach.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Hugo Lauren\u00e7on", "paper_title": "Unlocking the conversion of web screenshots into html code with the websight dataset", "reason": "This paper is highly relevant to WAFFLE as it introduces the WebSight dataset, which is used as the basis for WAFFLE's training data. Understanding the creation, characteristics, and limitations of the WebSight dataset is essential for interpreting the results and the overall contributions of the WAFFLE model.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Haotian Liu", "paper_title": "Improved baselines with visual instruction tuning", "reason": "This paper proposes improvements to baselines using visual instruction tuning, a technique directly related to WAFFLE's approach of contrastive learning. Understanding this related work helps to contextualize WAFFLE's methods and evaluate its relative advancements.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Anton Lozhkov", "paper_title": "Obelics: An open web-scale filtered dataset of interleaved image-text documents", "reason": "This paper focuses on the creation of Obelics, a dataset which, while not directly used in the WAFFLE paper, is relevant to the development of multi-modal models and provides context for the research's focus on the interplay of visual and textual data in the UI-to-HTML code generation context.", "section_number": 5}, {" publication_date": "2019", "fullname_first_author": "Ilya Loshchilov", "paper_title": "Decoupled weight decay regularization", "reason": "This paper introduces decoupled weight decay regularization, an optimization technique used in training WAFFLE's model.  Understanding this technique is essential for interpreting the training process and results.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Raymond Li", "paper_title": "Starcoder: may the source be with you!", "reason": "This paper introduces StarCoder, a large language model for code, which provides a comparative context for WAFFLE.  Understanding StarCoder's strengths and weaknesses helps to evaluate the effectiveness and novelty of WAFFLE in the context of code generation.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Haotian Liu", "paper_title": "Visual instruction tuning", "reason": "This paper introduces visual instruction tuning, which is crucial for understanding WAFFLE's approach to contrastive learning and aligning the model's understanding of UI images and HTML code. Understanding this technique is essential for evaluating the method's efficiency and contribution to the field.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Chenglei Si", "paper_title": "Design2code: How far are we from automating front-end engineering?", "reason": "This paper is highly relevant to WAFFLE as it introduces the Design2Code dataset, which is used as one of the benchmark datasets for evaluating WAFFLE's performance.  Understanding the properties and challenges of this dataset is essential for interpreting the results and contributions of WAFFLE.", "section_number": 3}, {" publication_date": "2017", "fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "reason": "This paper introduces the Transformer architecture, which is foundational for many modern MLLMs, including those used in WAFFLE.  Understanding the Transformer architecture and the attention mechanism is essential for grasping the core workings of WAFFLE and its structure-aware attention mechanism.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Haoran Wei", "paper_title": "Vary: Scaling up the vision vocabulary for large vision-language models", "reason": "This paper discusses scaling up vision vocabulary for large vision-language models, which is relevant to WAFFLE's aim of improving the model's visual understanding. Understanding the advancements in vision vocabulary directly contributes to the context of assessing WAFFLE's capabilities.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "X. Zhai", "paper_title": "Sigmoid loss for language image pre-training", "reason": "This paper introduces a sigmoid loss function for language-image pre-training, which is relevant to WAFFLE's use of contrastive learning.  Understanding the rationale behind this loss function and its effectiveness helps to contextualize WAFFLE's choices and assess its performance.", "section_number": 2}]}