{"references": [{" publication_date": "2021", "fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "reason": "This paper is foundational for the work on multi-modal large language models (MLLMs), which are central to WAFFLE's approach.  The introduction of CLIP (Contrastive Language-Image Pre-training) and its ability to learn transferable visual representations from natural language supervision is directly relevant to WAFFLE's use of contrastive learning to align the understanding of UI images and HTML code.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Hugo Lauren\u00e7on", "paper_title": "Unlocking the conversion of web screenshots into HTML code with the websight dataset", "reason": "This paper introduces the WebSight dataset, which is crucial for training and evaluating WAFFLE.  The dataset's focus on UI image-to-HTML code generation makes it directly relevant to WAFFLE's objective, and the paper's discussion of challenges in this area provides context for WAFFLE's approach.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Chenglei Si", "paper_title": "Design2Code: How far are we from automating front-end engineering?", "reason": "This paper introduces the Design2Code dataset, a valuable benchmark for evaluating WAFFLE's performance.  The Design2Code dataset focuses on real-world webpages, providing a more challenging and realistic evaluation scenario compared to synthetic datasets.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Albert Q. Jiang", "paper_title": "Mistral 7b", "reason": "This paper presents Mistral 7B, a large language model that is used as a backbone for training WAFFLE.  The model's performance in various NLP tasks demonstrates its potential for code generation, which is directly relevant to the UI-to-HTML task.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Hugo Touvron", "paper_title": "Llama: Open and efficient foundation language models", "reason": "This paper introduces the LLaMA large language model, which serves as a backbone model for WAFFLE.  The model's efficiency and open nature make it suitable for research purposes, and its strengths in code generation are directly applicable to WAFFLE's aim of automating UI-to-HTML translation.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Daniel Fried", "paper_title": "Incoder: A generative model for code infilling and synthesis", "reason": "This paper showcases advancements in code generation using large language models. The techniques and findings in Incoder can be applied to or adapted for the specific challenge of HTML code generation from UI designs, making it highly relevant to WAFFLE.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Jean-Baptiste Alayrac", "paper_title": "Flamingo: a visual language model for few-shot learning", "reason": "This paper details Flamingo, a visual language model that showcases the capabilities of multi-modal models. Its ability to perform few-shot learning on visual and textual data is relevant to WAFFLE's task of generating code from UI images.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Jinze Bai", "paper_title": "Qwen-vl: A frontier large vision-language model with versatile abilities", "reason": "This paper presents Qwen-VL, a large vision-language model that has shown promise in multi-modal tasks.  Its versatility and strength in handling visual and textual data make it relevant to WAFFLE's approach.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Lukas Blecher", "paper_title": "Nougat: Neural optical understanding for academic documents", "reason": "While not directly focused on code generation, this paper's work on neural optical understanding is highly relevant to WAFFLE's goal of bridging the gap between the visual understanding of UI images and the textual understanding of HTML code.  The techniques used to process visual information could be adapted for UI image analysis.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Lin Chen", "paper_title": "Sharegpt4v: Improving large multi-modal models with better captions", "reason": "This paper explores improving large multi-modal models, a key aspect of WAFFLE. The techniques used in improving multi-modal models are directly relevant to WAFFLE's goal of integrating visual and textual information for code generation.  The focus on improving captions, which is a multi-modal task, relates to WAFFLE's aim of generating coherent HTML from UI images.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Wenliang Dai", "paper_title": "Instructblip: Towards general-purpose vision-language models with instruction tuning", "reason": "This paper is significant because it explores instruction tuning in vision-language models. The techniques and insights gained from this work are highly relevant to WAFFLE's fine-tuning strategy, as instruction tuning can be adapted for improving code generation tasks.  The concept of instruction tuning is important for guiding the MLLM to understand and fulfill the specific task of UI-to-HTML generation.", "section_number": 1}, {" publication_date": "2021", "fullname_first_author": "Tianyu Gao", "paper_title": "SimCSE: Simple contrastive learning of sentence embeddings", "reason": "This paper is crucial for understanding the contrastive learning aspect of WAFFLE.  SimCSE introduces a simple yet effective contrastive learning framework for sentence embeddings, which directly relates to WAFFLE's use of contrastive learning to align visual and textual embeddings.  Understanding SimCSE is important for grasping the technical details of WAFFLE's contrastive learning component.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Yi Gui", "paper_title": "Vision2ui: A real-world dataset with layout for code generation from ui designs", "reason": "The Vision2UI dataset is highly relevant to WAFFLE as it focuses on generating code from UI designs.  This paper's introduction of a dataset with layout information for code generation directly addresses one of the main challenges faced in UI-to-HTML generation: understanding the layout aspects of UI designs. Analyzing the dataset's features and challenges can help enhance WAFFLE's performance.", "section_number": 2}, {" publication_date": "2017", "fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "reason": "This paper introduced the Transformer architecture, which is the foundation of many modern LLMs.  WAFFLE utilizes and adapts the attention mechanism within the transformer architecture to create its novel structure-aware attention, which is key to the model's success.  Understanding the Transformer architecture is crucial for interpreting the design and workings of WAFFLE.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Haotian Liu", "paper_title": "Improved baselines with visual instruction tuning", "reason": "This paper focuses on improving baselines for vision-language tasks, which is directly relevant to the improvements made in WAFFLE.  The techniques and methods explored in this paper relate to WAFFLE's approach and inform the design decisions made in the model.  The discussion on baselines helps contextualize the performance gains achieved by WAFFLE.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Haotian Liu", "paper_title": "Visual instruction tuning", "reason": "This paper focuses on visual instruction tuning, a technique that is closely related to the training method employed in WAFFLE.  Visual instruction tuning aims to improve the alignment between visual and textual information, which aligns with WAFFLE's aim of aligning UI images and HTML code.  Understanding this technique is vital for comprehending the nuances of WAFFLE's training process.", "section_number": 3}, {" publication_date": "2019", "fullname_first_author": "Ilya Loshchilov", "paper_title": "Decoupled weight decay regularization", "reason": "This paper introduces the AdamW optimizer, a crucial component of WAFFLE's training process.  AdamW is used in both the initial fine-tuning phase and the subsequent fine-tuning with structure-aware attention and contrastive learning.  Understanding AdamW's properties and its impact on training is key to interpreting WAFFLE's results.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Zian Su", "paper_title": "Codeart: Better code models by attention regularization when symbols are lacking", "reason": "This paper addresses the issue of attention regularization in code models, a problem relevant to WAFFLE's structure-aware attention.  The techniques and findings discussed in this paper inform the design choices and optimization strategies used in WAFFLE's structure-aware attention mechanism.", "section_number": 4}, {" publication_date": "2016", "fullname_first_author": "Zichao Yang", "paper_title": "Hierarchical attention networks for document classification", "reason": "This paper explores hierarchical attention networks, a concept relevant to WAFFLE's structure-aware attention. While focusing on document classification, the concepts and techniques of hierarchical attention are applicable to WAFFLE's task of processing the hierarchical structure of HTML code. The hierarchical nature of attention is important to understand WAFFLE's approach.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "vikhyat", "paper_title": "Moondream: tiny vision language model", "reason": "This paper introduces the Moondream model, one of the two backbones used in WAFFLE. The Moondream model's characteristics and performance are directly relevant to WAFFLE's results and the choice of backbone models. Understanding Moondream is crucial to assess the generalizability and limitations of the WAFFLE approach.", "section_number": 4}]}