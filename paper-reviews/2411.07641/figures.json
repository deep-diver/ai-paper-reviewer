[{"figure_path": "https://arxiv.org/html/2411.07641/x1.png", "caption": "(a) Distribution of logits", "description": "The figure shows the distribution of pre-softmax logits from the LLaMA3-8B-Instruct model on an AQUA dataset sample.  The left panel (a) presents a histogram of the logits, revealing a distinct bimodal distribution. A large portion of logits cluster around a central mean, resembling a Gaussian distribution, representing 'noise'.  A smaller, but more significant number of tokens have substantially larger logit values forming the 'informative' region, which is separate from the noise. The right panel (b) displays the token probabilities (post-softmax) sorted in descending order. It visually emphasizes how the few tokens with the largest logits contribute most of the probability mass. This illustrates that the informative tokens are easily distinguishable from the noise tokens by looking at the logits.", "section": "Insights"}, {"figure_path": "https://arxiv.org/html/2411.07641/x2.png", "caption": "(b) Descendingly sorted Probabilities. Only the top 20 tokens are shown.", "description": "This figure's (b) part shows the probabilities of the top 20 tokens after applying the softmax function to the logits.  It visually demonstrates how a small number of tokens (the most likely ones) account for the majority of the probability mass, while the vast majority of tokens have very low probabilities. This highlights the key concept of the paper: that logits naturally separate into a Gaussian-distributed noisy region and a distinct informative region, with the informative tokens having much higher logits.", "section": "Insights"}, {"figure_path": "https://arxiv.org/html/2411.07641/x3.png", "caption": "Figure 1: Distribution of logits and descendingly sorted probabilities of LLaMA3-8B-Instruct on an AQuA sample. Note that the leading tokens in the right plot (with higher probabilities) correspond to the right-side region of the logits distribution. The maximum logit is approximately 10\u2062\u03c310\ud835\udf0e10\\sigma10 italic_\u03c3 above the mean of the distribution.", "description": "This figure visualizes the distribution of pre-softmax logits and the resulting probabilities after applying the softmax function for a single sample from the AQuA dataset using the LLaMA3-8B-Instruct language model.  The left panel (a) shows a histogram of the logits, highlighting their approximately Gaussian distribution with a significant outlier tail.  A Kernel Density Estimate (KDE) curve is overlaid to emphasize the Gaussian component. The right panel (b) displays the probabilities of the top 20 tokens in descending order.  The key observation is the strong correspondence between the tokens with the highest probabilities (on the right of plot (b)) and the high-logit outliers in the right tail of the logit distribution (on the right of plot (a)).  The maximum logit is considerably larger than the mean of the distribution (approximately 10 standard deviations greater), clearly distinguishing a small number of 'informative' tokens from the bulk of 'noisy' tokens.", "section": "Insights"}, {"figure_path": "https://arxiv.org/html/2411.07641/x4.png", "caption": "(a) \u03c3\ud835\udf0e\\sigmaitalic_\u03c3-distance during generation", "description": "The figure shows the \u03c3-distance, which is the number of standard deviations between the maximum probability and the mean value of the logit distribution, over the course of text generation.  It visually represents how much the maximum logit value deviates from the average logit values throughout the generation process. This metric is used to assess the model's confidence at different generation stages.  A higher \u03c3-distance implies higher confidence because the maximum logit is significantly above the average, and a lower \u03c3-distance suggests less certainty.", "section": "2 Insights"}]