[{"figure_path": "2410.14649/charts/charts_5_0.png", "caption": "Figure 1: Removing twelve transformer blocks from Llama-3-8B under the constraint that only pairs of consecutive blocks can be removed. EvoPress finds the optimal configuration from the 8008 possible removal combinations in generation 6.", "description": "The chart displays the fast convergence of EvoPress in finding the optimal configuration for removing twelve transformer blocks from Llama-3-8B, achieving the optimum in only six generations.", "section": "4 Experiments"}, {"figure_path": "2410.14649/charts/charts_8_0.png", "caption": "Figure 2: Depth pruning results, on Mistral-7B-v0.3. (Left) Relative to all prior methods, EvoPress shows significantly lower PPL gap relative to the uncompressed model, with remarkably large gaps at medium compression rates. (Right) Examining the blocks dropped, we observe that EvoPress isolates completely different profiles relative to ShortGPT (which scores by cosine similarity).", "description": "The chart compares the perplexity of different depth pruning methods on the Mistral-7B-v0.3 model across various sparsity levels, showing EvoPress's superior performance and distinct block removal patterns.", "section": "4.1 Application 1: Depth Pruning"}, {"figure_path": "2410.14649/charts/charts_9_0.png", "caption": "Figure 5: Convergence of EvoPress when removing 8 transformer blocks (left) and 16 transformer blocks (right) of Mistral-7B-v0.3.", "description": "The chart displays the convergence speed of EvoPress in terms of perplexity and KL-divergence when removing different numbers of transformer blocks from the Mistral-7B-v0.3 model.", "section": "4.1 Application 1: Depth Pruning"}, {"figure_path": "2410.14649/charts/charts_26_0.png", "caption": "Figure 4: Convergence of EvoPress for unstructured sparsity (left) and quantization (right) for different fitness functions.", "description": "The chart displays the convergence of EvoPress for unstructured sparsity and quantization using different fitness functions (perplexity and KL-divergence).", "section": "Experiments"}, {"figure_path": "2410.14649/charts/charts_29_0.png", "caption": "Figure 5: Convergence of EvoPress when removing 8 transformer blocks (left) and 16 transformer blocks (right) of Mistral-7B-v0.3.", "description": "Figure 5 shows the convergence speed of EvoPress for removing 8 and 16 transformer blocks from Mistral-7B-v0.3, illustrating its rapid convergence to high-quality solutions.", "section": "4.1 Application 1: Depth Pruning"}, {"figure_path": "2410.14649/charts/charts_29_1.png", "caption": "Figure 5: Convergence of EvoPress when removing 8 transformer blocks (left) and 16 transformer blocks (right) of Mistral-7B-v0.3.", "description": "The chart displays the convergence of EvoPress's perplexity and KL-divergence over generations when pruning 8 and 16 transformer blocks from the Mistral-7B-v0.3 model.", "section": "4.1 Application 1: Depth Pruning"}, {"figure_path": "2410.14649/charts/charts_29_2.png", "caption": "Figure 6: Optimal removal configurations identified by EvoPress for different models.", "description": "The chart visualizes optimal block removal configurations identified by EvoPress for various LLMs under different sparsity levels, showcasing the model's ability to determine optimal configurations that balance compression and accuracy.", "section": "D.3 Locality of Dropped Blocks"}, {"figure_path": "2410.14649/charts/charts_30_0.png", "caption": "Figure 7: Effect of removing random subsets of blocks for Llama-3-8B.", "description": "The chart displays the correlation between the average cosine similarity, average squared error, and average normalized squared error of random subsets of removed blocks with their corresponding perplexity for Llama-3-8B.", "section": "D.4 Correlation of Scores with Perplexity"}, {"figure_path": "2410.14649/charts/charts_32_0.png", "caption": "Figure 8: Comparison of different block-level sparsity profiles for Llama-3.1-8B at 70% sparsity.", "description": "The chart compares the sparsity profiles generated by EvoPress, OWL, and uniform sparsity methods across different layers of the Llama-3.1-8B model at 70% overall sparsity.", "section": "E.2 Sparsity Profiles"}, {"figure_path": "2410.14649/charts/charts_32_1.png", "caption": "Figure 9: Average sparsity per projection type for Llama-3.1-8B at 70% sparsity for EvoPress.", "description": "The chart displays the average sparsity per projection type for the Llama-3.1-8B model at 70% sparsity using the EvoPress method.", "section": "E.2 Sparsity Profiles"}, {"figure_path": "2410.14649/charts/charts_33_0.png", "caption": "Figure 10: Convergence of EvoPress for 2.25 bit quantization on Llama-3.1-8B (left) and 3 bit quantization on Llama-3-8B (right).", "description": "The chart displays the convergence of EvoPress for 2.25-bit and 3-bit quantization on Llama-3.1-8B and Llama-3-8B respectively, showing the perplexity and KL-divergence over generations.", "section": "F.2 Practical Convergence"}, {"figure_path": "2410.14649/charts/charts_33_1.png", "caption": "Figure 10: Convergence of EvoPress for 2.25 bit quantization on Llama-3.1-8B (left) and 3 bit quantization on Llama-3-8B (right).", "description": "The chart displays the convergence of EvoPress for 2.25-bit and 3-bit quantization on Llama-3.1-8B and Llama-3-8B, respectively, showing perplexity and KL-divergence over generations.", "section": "Additional Quantization Results"}, {"figure_path": "2410.14649/charts/charts_33_2.png", "caption": "Figure 11: Block-level quantization profiles for Llama-3.1-8B at 3 bit compression on average.", "description": "The chart visualizes the block-level quantization profiles generated by EvoPress for Llama-3.1-8B, showing the bitwidth allocated to each block during 3-bit compression on average.", "section": "F.3 Quantization Profiles"}, {"figure_path": "2410.14649/charts/charts_33_3.png", "caption": "Figure 9: Average sparsity per projection type for Llama-3.1-8B at 70% sparsity for EvoPress.", "description": "The bar chart displays the average sparsity per projection type for the Llama-3.1-8B model at 70% sparsity using the EvoPress method.", "section": "E.2 Sparsity Profiles"}]