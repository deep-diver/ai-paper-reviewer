[{"figure_path": "2410.14649/charts/charts_5_0.png", "caption": "Figure 1: Removing twelve transformer blocks from Llama-3-8B under the constraint that only pairs of consecutive blocks can be removed. EvoPress finds the optimal configuration from the 8008 possible removal combinations in generation 6.", "description": "The chart displays the fast convergence of EvoPress in finding the optimal configuration for removing transformer blocks from Llama-3-8B.", "section": "Experiments"}, {"figure_path": "2410.14649/charts/charts_8_0.png", "caption": "Figure 2: Depth pruning results, on Mistral-7B-v0.3. (Left) Relative to all prior methods, EvoPress shows significantly lower PPL gap relative to the uncompressed model, with remarkably large gaps at medium compression rates. (Right) Examining the blocks dropped, we observe that EvoPress isolates completely different profiles relative to ShortGPT (which scores by cosine similarity).", "description": "The chart compares the performance of EvoPress against other depth pruning methods across different sparsity levels on the Mistral-7B-v0.3 model, showing EvoPress's superior performance and unique block removal patterns.", "section": "4.1 Application 1: Depth Pruning"}, {"figure_path": "2410.14649/charts/charts_9_0.png", "caption": "Figure 5: Convergence of EvoPress when removing 8 transformer blocks (left) and 16 transformer blocks (right) of Mistral-7B-v0.3.", "description": "The chart displays the convergence speed of EvoPress in terms of perplexity and KL-divergence when pruning 8 and 16 transformer blocks from the Mistral-7B-v0.3 model.", "section": "4.1 Application 1: Depth Pruning"}, {"figure_path": "2410.14649/charts/charts_26_0.png", "caption": "Figure 4: Convergence of EvoPress for unstructured sparsity (left) and quantization (right) for different fitness functions.", "description": "The chart displays the convergence of EvoPress for unstructured sparsity and quantization using two different fitness functions (perplexity and KL-divergence).", "section": "Experiments"}, {"figure_path": "2410.14649/charts/charts_29_0.png", "caption": "Figure 5: Convergence of EvoPress when removing 8 transformer blocks (left) and 16 transformer blocks (right) of Mistral-7B-v0.3.", "description": "The chart displays the convergence speed of EvoPress in terms of perplexity and KL-divergence when removing 8 and 16 transformer blocks from the Mistral-7B-v0.3 model, showing a rapid convergence to near-optimal solutions within a few hours.", "section": "4.1 Application 1: Depth Pruning"}, {"figure_path": "2410.14649/charts/charts_29_1.png", "caption": "Figure 5: Convergence of EvoPress when removing 8 transformer blocks (left) and 16 transformer blocks (right) of Mistral-7B-v0.3.", "description": "The chart displays the convergence of EvoPress in terms of perplexity and KL-divergence over generations when removing 8 and 16 transformer blocks from the Mistral-7B-v0.3 model.", "section": "4.1 Application 1: Depth Pruning"}, {"figure_path": "2410.14649/charts/charts_29_2.png", "caption": "Figure 5: Convergence of EvoPress when removing 8 transformer blocks (left) and 16 transformer blocks (right) of Mistral-7B-v0.3.", "description": "The chart displays the convergence speed of EvoPress for removing 8 and 16 transformer blocks from the Mistral-7B-v0.3 model, showing the perplexity and KL divergence over generations.", "section": "4.1 Application 1: Depth Pruning"}, {"figure_path": "2410.14649/charts/charts_30_0.png", "caption": "Figure 7: Effect of removing random subsets of blocks for Llama-3-8B.", "description": "The chart displays the correlation between different metrics (cosine similarity, squared error, and normalized squared error) and perplexity when randomly removing subsets of blocks from Llama-3-8B, showing how these correlations change with sparsity levels.", "section": "D.4 Correlation of Scores with Perplexity"}, {"figure_path": "2410.14649/charts/charts_32_0.png", "caption": "Figure 8: Comparison of different block-level sparsity profiles for Llama-3.1-8B at 70% sparsity.", "description": "The chart compares the sparsity profiles generated by EvoPress, OWL, and uniform sparsity methods across different layers of Llama-3.1-8B model at 70% average sparsity.", "section": "E.2 Sparsity Profiles"}, {"figure_path": "2410.14649/charts/charts_32_1.png", "caption": "Figure 9: Average sparsity per projection type for Llama-3.1-8B at 70% sparsity for EvoPress.", "description": "The bar chart displays the average sparsity achieved per projection type (q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj) for the Llama-3.1-8B model using EvoPress at 70% overall sparsity.", "section": "E.2 Sparsity Profiles"}, {"figure_path": "2410.14649/charts/charts_33_0.png", "caption": "Figure 10: Convergence of EvoPress for 2.25 bit quantization on Llama-3.1-8B (left) and 3 bit quantization on Llama-3-8B (right).", "description": "The chart displays the convergence speed of EvoPress for 2.25-bit and 3-bit quantization on Llama-3.1-8B and Llama-3-8B, respectively, over generations.", "section": "4.3 Application 3: Quantization"}, {"figure_path": "2410.14649/charts/charts_33_1.png", "caption": "Figure 10: Convergence of EvoPress for 2.25 bit quantization on Llama-3.1-8B (left) and 3 bit quantization on Llama-3-8B (right).", "description": "The chart displays the convergence speed of EvoPress for 2.25-bit and 3-bit quantization on Llama-3.1-8B and Llama-3-8B, respectively, showing the perplexity and KL-divergence values over generations.", "section": "F Additional Quantization Results"}, {"figure_path": "2410.14649/charts/charts_33_2.png", "caption": "Figure 11: Block-level quantization profiles for Llama-3.1-8B at 3 bit compression on average.", "description": "The chart displays the block-level quantization profiles for the Llama-3.1-8B model at an average compression of 3 bits, showing the bit allocation for each block.", "section": "F.3 Quantization Profiles"}, {"figure_path": "2410.14649/charts/charts_33_3.png", "caption": "Figure 9: Average sparsity per projection type for Llama-3.1-8B at 70% sparsity for EvoPress.", "description": "The chart visualizes the average sparsity achieved per projection type (q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj) for the Llama-3.1-8B model when applying EvoPress at a 70% overall sparsity level.", "section": "E.2 Sparsity Profiles"}]