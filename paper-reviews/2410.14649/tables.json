[{"figure_path": "2410.14649/tables/table_2_0.html", "caption": "Table 1: Depth pruning is not monotone. In this example (Llama-3-8B with Fineweb-Edu calibration), removing strictly more blocks (depicted in orange) can improve perplexity across sources. Left half of block corresponds to attention layer, right half to MLP.", "description": "The table shows that removing more blocks from a Llama-3-8B model does not always lead to lower perplexity, demonstrating that error monotonicity does not hold for LLMs.", "section": "1 Introduction"}, {"figure_path": "2410.14649/tables/table_9_0.html", "caption": "Table 2: Performance of various methods at 70% average sparsity. EvoPress outperforms prior methods both in terms of validation perplexity (PPL) and zero-shot accuracy.", "description": "Table 2 presents a comparison of different model compression methods at 70% average sparsity across multiple LLMs, showing EvoPress's superior performance in terms of perplexity and zero-shot accuracy.", "section": "4.2 APPLICATION 2: UNSTRUCTURED SPARSITY"}, {"figure_path": "2410.14649/tables/table_24_0.html", "caption": "Table 1: Depth pruning is not monotone. In this example (Llama-3-8B with Fineweb-Edu calibration), removing strictly more blocks (depicted in orange) can improve perplexity across sources. Left half of block corresponds to attention layer, right half to MLP.", "description": "The table shows an example where removing more blocks in a Llama-3-8B model, contrary to the assumption of error monotonicity, improves perplexity across different sources.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.14649/tables/table_24_1.html", "caption": "Table 2: Performance of various methods at 70% average sparsity. EvoPress outperforms prior methods both in terms of validation perplexity (PPL) and zero-shot accuracy.", "description": "The table presents a comparison of different methods for achieving 70% average sparsity in various LLMs, showing EvoPress's superior performance in terms of perplexity and zero-shot accuracy.", "section": "4.2 APPLICATION 2: UNSTRUCTURED SPARSITY"}, {"figure_path": "2410.14649/tables/table_25_0.html", "caption": "Table 1: Depth pruning is not monotone. In this example (Llama-3-8B with Fineweb-Edu calibration), removing strictly more blocks (depicted in orange) can improve perplexity across sources. Left half of block corresponds to attention layer, right half to MLP.", "description": "The table shows an example where removing more blocks from a Llama-3-8B model, contrary to the assumption of error monotonicity, leads to improved perplexity.", "section": "1 Introduction"}, {"figure_path": "2410.14649/tables/table_25_1.html", "caption": "Table 2: Performance of various methods at 70% average sparsity. EvoPress outperforms prior methods both in terms of validation perplexity (PPL) and zero-shot accuracy.", "description": "Table 2 presents a comparison of different model compression methods at 70% average sparsity, showing EvoPress's superior performance in terms of perplexity and zero-shot accuracy.", "section": "4.2 APPLICATION 2: UNSTRUCTURED SPARSITY"}, {"figure_path": "2410.14649/tables/table_26_0.html", "caption": "Table 2: Performance of various methods at 70% average sparsity. EvoPress outperforms prior methods both in terms of validation perplexity (PPL) and zero-shot accuracy.", "description": "Table 2 presents a comparison of different methods for unstructured sparsity at 70% sparsity, showing EvoPress's superior performance in terms of perplexity and zero-shot accuracy across various LLMs.", "section": "4.2 APPLICATION 2: UNSTRUCTURED SPARSITY"}, {"figure_path": "2410.14649/tables/table_27_0.html", "caption": "Table 1: Depth pruning is not monotone. In this example (Llama-3-8B with Fineweb-Edu calibration), removing strictly more blocks (depicted in orange) can improve perplexity across sources. Left half of block corresponds to attention layer, right half to MLP.", "description": "The table shows that removing more blocks from a Llama-3-8B model, as measured by perplexity, does not always lead to a decrease in performance, demonstrating that error monotonicity does not hold for LLMs.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.14649/tables/table_27_1.html", "caption": "Table 10: Depth pruning of Llama-2-7B.", "description": "The table presents the results of depth pruning experiments on Llama-2-7B model, comparing the perplexity scores of various depth pruning methods at different sparsity levels.", "section": "4.1 Application 1: Depth Pruning"}, {"figure_path": "2410.14649/tables/table_28_0.html", "caption": "Table 1: Depth pruning is not monotone. In this example (Llama-3-8B with Fineweb-Edu calibration), removing strictly more blocks (depicted in orange) can improve perplexity across sources. Left half of block corresponds to attention layer, right half to MLP.", "description": "The table shows that removing more blocks from a Llama-3-8B model, contrary to the assumption of error monotonicity, can sometimes lead to better perplexity.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.14649/tables/table_28_1.html", "caption": "Table 12: Depth pruning of Llama-3.1-8B.", "description": "The table shows the perplexity results of different depth pruning methods on the Llama-3.1-8B model at various sparsity levels.", "section": "4.1 Application 1: Depth Pruning"}, {"figure_path": "2410.14649/tables/table_30_0.html", "caption": "Table 1: Depth pruning is not monotone. In this example (Llama-3-8B with Fineweb-Edu calibration), removing strictly more blocks (depicted in orange) can improve perplexity across sources. Left half of block corresponds to attention layer, right half to MLP.", "description": "The table shows that in large language models, removing more blocks does not always lead to lower perplexity, contradicting the assumption of error monotonicity in dynamic model compression.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.14649/tables/table_31_0.html", "caption": "Table 2: Performance of various methods at 70% average sparsity. EvoPress outperforms prior methods both in terms of validation perplexity (PPL) and zero-shot accuracy.", "description": "Table 2 presents the performance comparison of different methods for unstructured sparsity at 70% average sparsity across various LLMs, showcasing EvoPress's superior performance in terms of perplexity and zero-shot accuracy.", "section": "4.2 APPLICATION 2: UNSTRUCTURED SPARSITY"}, {"figure_path": "2410.14649/tables/table_31_1.html", "caption": "Table 2: Performance of various methods at 70% average sparsity. EvoPress outperforms prior methods both in terms of validation perplexity (PPL) and zero-shot accuracy.", "description": "Table 2 presents a comparison of different methods for achieving 70% average sparsity in LLMs, showing EvoPress's superior performance in both perplexity and zero-shot accuracy.", "section": "4.2 APPLICATION 2: UNSTRUCTURED SPARSITY"}, {"figure_path": "2410.14649/tables/table_32_0.html", "caption": "Table 2: Performance of various methods at 70% average sparsity. EvoPress outperforms prior methods both in terms of validation perplexity (PPL) and zero-shot accuracy.", "description": "Table 2 presents a comparison of different methods for achieving 70% sparsity in various LLMs, showing that EvoPress outperforms existing techniques in terms of both perplexity and zero-shot accuracy.", "section": "4.2 APPLICATION 2: UNSTRUCTURED SPARSITY"}]