[{"figure_path": "2410.14649/tables/table_2_0.html", "caption": "Table 1: Depth pruning is not monotone. In this example (Llama-3-8B with Fineweb-Edu calibration), removing strictly more blocks (depicted in orange) can improve perplexity across sources. Left half of block corresponds to attention layer, right half to MLP.", "description": "The table shows that removing more blocks from a Llama-3-8B model does not always result in lower perplexity, demonstrating that error monotonicity does not hold generally for LLM compression.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.14649/tables/table_9_0.html", "caption": "Table 2: Performance of various methods at 70% average sparsity. EvoPress outperforms prior methods both in terms of validation perplexity (PPL) and zero-shot accuracy.", "description": "Table 2 presents a comparison of various methods for achieving 70% average sparsity across different LLMs, showing that EvoPress outperforms existing methods in terms of perplexity and zero-shot accuracy.", "section": "4.2 Application 2: Unstructured Sparsity"}, {"figure_path": "2410.14649/tables/table_24_0.html", "caption": "Table 1: Depth pruning is not monotone. In this example (Llama-3-8B with Fineweb-Edu calibration), removing strictly more blocks (depicted in orange) can improve perplexity across sources. Left half of block corresponds to attention layer, right half to MLP.", "description": "The table shows that removing more blocks from a Llama-3-8B language model does not always lead to lower perplexity, refuting the assumption of error monotonicity in LLM compression.", "section": "1 Introduction"}, {"figure_path": "2410.14649/tables/table_24_1.html", "caption": "Table 2: Performance of various methods at 70% average sparsity. EvoPress outperforms prior methods both in terms of validation perplexity (PPL) and zero-shot accuracy.", "description": "The table presents a comparison of various methods' performance at 70% average sparsity across different LLMs, showing EvoPress's superior performance in terms of perplexity and zero-shot accuracy.", "section": "4.2 APPLICATION 2: UNSTRUCTURED SPARSITY"}, {"figure_path": "2410.14649/tables/table_25_0.html", "caption": "Table 2: Performance of various methods at 70% average sparsity. EvoPress outperforms prior methods both in terms of validation perplexity (PPL) and zero-shot accuracy.", "description": "The table presents a comparison of different model compression methods at 70% average sparsity, showing that EvoPress outperforms existing methods in terms of validation perplexity and zero-shot accuracy.", "section": "4.2 APPLICATION 2: UNSTRUCTURED SPARSITY"}, {"figure_path": "2410.14649/tables/table_25_1.html", "caption": "Table 2: Performance of various methods at 70% average sparsity. EvoPress outperforms prior methods both in terms of validation perplexity (PPL) and zero-shot accuracy.", "description": "Table 2 presents a comparison of different methods for achieving 70% average sparsity across various LLMs, showing that EvoPress achieves the best performance in terms of both perplexity and zero-shot accuracy.", "section": "4.2 APPLICATION 2: UNSTRUCTURED SPARSITY"}, {"figure_path": "2410.14649/tables/table_26_0.html", "caption": "Table 2: Performance of various methods at 70% average sparsity. EvoPress outperforms prior methods both in terms of validation perplexity (PPL) and zero-shot accuracy.", "description": "Table 2 shows the performance of different methods on various LLMs at 70% average sparsity, comparing validation perplexity and average zero-shot accuracy across different metrics.", "section": "4.2 APPLICATION 2: UNSTRUCTURED SPARSITY"}, {"figure_path": "2410.14649/tables/table_27_0.html", "caption": "Table 1: Depth pruning is not monotone. In this example (Llama-3-8B with Fineweb-Edu calibration), removing strictly more blocks (depicted in orange) can improve perplexity across sources. Left half of block corresponds to attention layer, right half to MLP.", "description": "The table shows an example where removing more blocks in a Llama-3-8B model, contrary to the assumption of error monotonicity, leads to better perplexity.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.14649/tables/table_27_1.html", "caption": "Table 10: Depth pruning of Llama-2-7B.", "description": "The table presents the results of depth pruning experiments on Llama-2-7B at various sparsity levels, comparing EvoPress with other baseline methods.", "section": "4.1 Application 1: Depth Pruning"}, {"figure_path": "2410.14649/tables/table_28_0.html", "caption": "Table 1: Depth pruning is not monotone. In this example (Llama-3-8B with Fineweb-Edu calibration), removing strictly more blocks (depicted in orange) can improve perplexity across sources. Left half of block corresponds to attention layer, right half to MLP.", "description": "The table demonstrates that removing more blocks from a Llama language model does not always result in worse perplexity, refuting the assumption of error monotonicity in LLM compression.", "section": "1 Introduction"}, {"figure_path": "2410.14649/tables/table_28_1.html", "caption": "Table 12: Depth pruning of Llama-3.1-8B.", "description": "The table shows the perplexity scores achieved by different depth pruning methods on the Llama-3.1-8B model at various sparsity levels.", "section": "4.1 Application 1: Depth Pruning"}, {"figure_path": "2410.14649/tables/table_30_0.html", "caption": "Table 1: Depth pruning is not monotone. In this example (Llama-3-8B with Fineweb-Edu calibration), removing strictly more blocks (depicted in orange) can improve perplexity across sources. Left half of block corresponds to attention layer, right half to MLP.", "description": "The table demonstrates that removing more blocks from a Llama-3-8B language model does not always lead to lower perplexity, contradicting the assumption of error monotonicity in dynamic model compression.", "section": "1 Introduction"}, {"figure_path": "2410.14649/tables/table_31_0.html", "caption": "Table 2: Performance of various methods at 70% average sparsity. EvoPress outperforms prior methods both in terms of validation perplexity (PPL) and zero-shot accuracy.", "description": "Table 2 presents a comparison of different methods for achieving 70% average sparsity across various LLMs, showing EvoPress's superior performance in terms of perplexity and zero-shot accuracy.", "section": "4.2 APPLICATION 2: UNSTRUCTURED SPARSITY"}, {"figure_path": "2410.14649/tables/table_31_1.html", "caption": "Table 2: Performance of various methods at 70% average sparsity. EvoPress outperforms prior methods both in terms of validation perplexity (PPL) and zero-shot accuracy.", "description": "Table 2 presents the performance comparison of different methods for unstructured sparsity at 70% sparsity level across multiple LLMs, showing EvoPress's superior performance.", "section": "4.2 APPLICATION 2: UNSTRUCTURED SPARSITY"}, {"figure_path": "2410.14649/tables/table_32_0.html", "caption": "Table 2: Performance of various methods at 70% average sparsity. EvoPress outperforms prior methods both in terms of validation perplexity (PPL) and zero-shot accuracy.", "description": "Table 2 presents a comparison of different methods for achieving 70% average sparsity in various LLMs, showing that EvoPress outperforms existing methods in terms of both perplexity and zero-shot accuracy.", "section": "4.2 APPLICATION 2: UNSTRUCTURED SPARSITY"}]