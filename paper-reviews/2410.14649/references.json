{"references": [{" publication_date": "2022", "fullname_first_author": "Tim Dettmers", "paper_title": "The case for 4-bit precision: k-bit inference scaling laws", "reason": "This paper is highly relevant because it establishes the baseline for 4-bit quantization, a technique central to the study of LLM compression.  The findings of this paper regarding the scaling laws associated with reduced bit precision are directly relevant to understanding the trade-offs in accuracy vs. compression inherent to quantization methods, a core focus of the EvoPress paper.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Elias Frantar", "paper_title": "SPDY: Accurate pruning with speedup guarantees", "reason": "This paper introduces SPDY, a significant advancement in LLM compression that uses a unified framework for pruning and quantization. Its importance stems from its use as a baseline for comparison. The paper also highlights limitations that EvoPress attempts to address, which further underscores its relevance to the overall research context.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Elias Frantar", "paper_title": "Massive language models can be accurately pruned in one-shot", "reason": "This paper provides valuable insights into one-shot pruning, a crucial technique in LLM compression. The study directly relates to the EvoPress paper's focus on efficiently finding optimal compression configurations, demonstrating an advancement over prior methods that require iterative pruning or retraining.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Elias Frantar", "paper_title": "GPTQ: Accurate post-training quantization for generative pre-trained transformers", "reason": "This paper is foundational for EvoPress's quantization experiments because it introduces GPTQ, a highly effective post-training quantization technique for LLMs. Its application in EvoPress showcases its efficacy, especially when combined with EvoPress's dynamic optimization approach.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Vage Egiazarian", "paper_title": "Extreme compression of large language models via additive quantization", "reason": "This recent work explores additive quantization for LLM compression and provides valuable insights into the challenges and limitations of this approach. It sets the stage for the EvoPress experiments by providing a context for the challenges faced in achieving both high compression ratios and accuracy.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Bo-Kyeong Kim", "paper_title": "Shortened llama: A simple depth pruning for large language models", "reason": "This paper presents a depth pruning method for LLMs, which is a key baseline method for comparison in the EvoPress paper. The importance lies in its direct relation to the depth pruning experiments conducted by the authors, providing a benchmark against which to measure the effectiveness of EvoPress.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Eldar Kurtic", "paper_title": "Ziplm: Inference-aware structured pruning of language models", "reason": "This paper introduces ZipLM, a structured pruning method, which offers a comparison point for EvoPress's method in the context of depth pruning. It highlights the tradeoffs and challenges associated with different compression approaches, forming a backdrop for the authors' claims regarding EvoPress's improvements.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Andrey Gromov", "paper_title": "The unreasonable ineffectiveness of the deeper layers", "reason": "This paper is crucial for EvoPress because it directly challenges the assumption of error monotonicity in LLMs, which is a key motivation for the authors' work. By demonstrating that deeper layers are not always more important, the paper provides a strong rationale for EvoPress's dynamic compression approach.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Xin Men", "paper_title": "ShortGPT: Layers in large language models are more redundant than you expect", "reason": "This paper offers another depth pruning method that serves as a relevant baseline for comparison in EvoPress's depth pruning experiments. Its inclusion is critical because it offers a different approach to evaluating the importance of layers, thus contributing to the comprehensive nature of the experiments and allowing for a more thorough evaluation of EvoPress's performance.", "section_number": 4}, {" publication_date": "2021", "fullname_first_author": "Yuhang Li", "paper_title": "BRECQ: Pushing the limit of post-training quantization by block reconstruction", "reason": "BRECQ provides a state-of-the-art quantization technique that serves as a strong benchmark against which to compare EvoPress.  Its inclusion is important for showcasing the innovative nature of EvoPress. Moreover, it explores an aspect of quantization (block reconstruction) which differs from the GPTQ method used in EvoPress.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Lu Yin", "paper_title": "Outlier weighed layerwise sparsity (OWL): A missing secret sauce for pruning LLMs to high sparsity", "reason": "This paper introduces OWL, a competitive method for unstructured sparsity, directly addressing the issue of non-uniform compression that EvoPress tackles.  OWL serves as a critical benchmark for comparing EvoPress's performance and illustrating its improvements in unstructured sparsity, particularly at high compression ratios.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Tim Dettmers", "paper_title": "Spqr: A sparse-quantized representation for near-lossless llm weight compression", "reason": "This paper introduces SpQR, a method for near-lossless LLM weight compression.  This is highly relevant to EvoPress because it represents the state-of-the-art in near-lossless compression techniques at the time of publishing, setting a high standard for EvoPress to demonstrate its capabilities. EvoPress indirectly relates to SpQR as it addresses the shortcomings of near-lossless compression methods that are also computationally intensive and often suffer from suboptimal solutions.", "section_number": 4}, {" publication_date": "2018", "fullname_first_author": "Jun He", "paper_title": "AMC: AutoML for model compression and acceleration on mobile devices", "reason": "This paper discusses AutoML for model compression and acceleration.  It is significant because it emphasizes the importance of automation in model compression, a theme directly addressed by EvoPress, which aims to automate the process of finding the optimal compression strategy.  EvoPress can be considered an evolution of this earlier AutoML approach.", "section_number": 2}, {" publication_date": "2018", "fullname_first_author": "Anubhav Ashok", "paper_title": "N2N learning: Network to network compression via policy gradient reinforcement learning", "reason": "This paper presents an early approach to non-uniform model compression using reinforcement learning.  It's significant because it represents a foundational work in the area of dynamically adjusting compression levels for neural networks.  EvoPress builds upon this by offering a more efficient and provably convergent solution to the dynamic compression problem.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Itay Hubara", "paper_title": "Accelerated sparse neural training: A provable and efficient method to find N:M transposable masks", "reason": "This paper focuses on accelerated sparse neural training, a technique closely related to pruning, a core method addressed by EvoPress.  Its importance lies in its focus on efficiency and provable guarantees, which directly connects to EvoPress's emphasis on efficiency and theoretical convergence properties.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "Daoyuan Chen", "paper_title": "Adabert: Task-adaptive bert compression with differentiable neural architecture search", "reason": "This paper uses differentiable neural architecture search for BERT compression.  Its relevance to EvoPress comes from its exploration of automated model compression and architecture search, a field that shares common ground with EvoPress\u2019s approach to finding the optimal compression configuration.  This paper demonstrates that structured compression methods like EvoPress hold significant promise.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Jin Xu", "paper_title": "NAS-bert: Task-agnostic and adaptive-size bert compression with neural architecture search", "reason": "This paper introduces NAS-BERT, another method that uses neural architecture search for model compression, similar to EvoPress in its approach. Its relevance lies in the application of neural architecture search to the problem of model compression, which highlights the potential of automated methods in achieving efficient and optimal results. EvoPress advances beyond this by focusing on dynamic compression and providing provable guarantees.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Hugo Touvron", "paper_title": "Llama: Open and efficient foundation language models", "reason": "The Llama models, introduced in this paper, are used as the primary experimental benchmarks for EvoPress. The Llama model family is important because it's widely used and represents a significant advancement in LLM technology, thus providing a strong context and validation for EvoPress's experiments.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Albert Tseng", "paper_title": "Quip#: Even better llm quantization with hadamard incoherence and lattice codebooks", "reason": "This paper provides a very recent quantization method for LLMs that serves as a state-of-the-art benchmark for the EvoPress paper. It's important because it represents a direct competitor in the field of LLM quantization, highlighting the novelty and competitive performance of EvoPress's approach to the problem.", "section_number": 4}]}