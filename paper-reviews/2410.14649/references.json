{"references": [{" publication_date": "2022", "fullname_first_author": "Tim Dettmers", "paper_title": "The case for 4-bit precision: k-bit inference scaling laws", "reason": "This paper is highly relevant because it investigates the trade-offs between precision and inference speed in LLMs, directly addressing one of the central challenges in LLM compression. The findings are critical in understanding the limitations of existing low-bit quantization techniques and in guiding the design of new, more efficient methods.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Tim Dettmers", "paper_title": "Spqr: A sparse-quantized representation for near-lossless llm weight compression", "reason": "This work is significant because it proposes a novel approach to LLM compression that combines sparsity and quantization. The combination of these techniques offers a potential pathway to achieving significantly higher compression ratios while minimizing the accuracy loss, a key challenge highlighted in the introduction.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Elias Frantar", "paper_title": "SPDY: Accurate pruning with speedup guarantees", "reason": "This paper presents SPDY, a dynamic pruning algorithm that provides speed guarantees. This is directly relevant to the paper's goal of developing efficient dynamic compression techniques. The algorithm's focus on speedup is important because it complements the accuracy-oriented focus of many existing methods.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Elias Frantar", "paper_title": "Massive language models can be accurately pruned in one-shot", "reason": "This work is important because it presents a one-shot pruning method, which is significantly more efficient than iterative pruning methods. The ability to prune LLMs quickly is highly desirable given the computational cost of training and evaluating these models. This is directly relevant to EvoPress, which aims for both efficiency and optimality.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Elias Frantar", "paper_title": "GPTQ: Accurate post-training quantization for generative pre-trained transformers", "reason": "This paper introduces GPTQ, a post-training quantization technique that is highly accurate.  Quantization is one of the main LLM compression methods, and the accuracy of GPTQ is crucial for achieving high compression ratios without significant performance degradation. EvoPress builds upon such existing techniques to achieve more optimal compression.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Andrey Gromov", "paper_title": "The unreasonable ineffectiveness of the deeper layers", "reason": "This paper challenges the common assumption that deeper layers of LLMs are more important, which is often a basis for layer dropping methods. EvoPress also addresses the layer importance issue, by developing a model-agnostic approach and questioning the reliance on heuristics.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Bo-Kyeong Kim", "paper_title": "Shortened llama: A simple depth pruning for large language models", "reason": "This paper explores depth pruning, which is one of the LLM compression techniques discussed and compared in the paper. By providing a state-of-the-art baseline in depth pruning, this work serves as a strong benchmark against which the proposed method can be evaluated and compared.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Xin Men", "paper_title": "ShortGPT: Layers in large language models are more redundant than you expect", "reason": "Similar to \"Shortened Llama,\" this paper also focuses on depth pruning, a technique that EvoPress aims to improve upon.  This paper provides another competitive baseline for evaluating EvoPress' performance in depth pruning, thereby allowing for a more comprehensive comparison.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Guilherme Penedo", "paper_title": "The fineweb datasets: Decanting the web for the finest text data at scale", "reason": "This paper provides a large-scale, high-quality dataset for calibrating and evaluating LLMs, which is essential for the robust evaluation of compression techniques. The availability of this dataset allows for a more comprehensive and reliable evaluation of the methods tested and provides a consistent benchmark.", "section_number": 4}, {" publication_date": "2016", "fullname_first_author": "Stephen Merity", "paper_title": "Pointer sentinel mixture models", "reason": "This paper provides one of the standard datasets used for evaluating perplexity, which is one of the evaluation metrics used in the paper. Its relevance stems from the fact that the evaluation of LLM compression often involves measuring the perplexity on standard language modeling benchmarks.", "section_number": 4}, {" publication_date": "2019", "fullname_first_author": "Colin Raffel", "paper_title": "Exploring the limits of transfer learning with a unified text-to-text transformer", "reason": "This paper introduces another widely used dataset (C4) used as a benchmark for evaluating the language modeling capabilities of LLMs. The dataset is used for evaluating the performance of the proposed compression methods on language modeling tasks and provides a standard for comparison.", "section_number": 4}, {" publication_date": "2021", "fullname_first_author": "Keisuke Sakaguchi", "paper_title": "Winogrande: an adversarial winograd schema challenge at scale", "reason": "This paper introduces a challenging dataset used to measure the performance of LLMs on commonsense reasoning tasks. This is directly relevant to the paper, as the evaluation of compression techniques often involves assessing the performance of compressed models on such downstream tasks.", "section_number": 4}, {" publication_date": "2003", "fullname_first_author": "Sandeep Tata", "paper_title": "PiQA: An algebra for querying protein data sets", "reason": "This paper introduces a benchmark dataset which tests the LLM's ability to answer questions related to a specific domain. The inclusion of this dataset in the evaluation showcases the broader applicability of the proposed compression methods to various downstream tasks.", "section_number": 4}, {" publication_date": "2019", "fullname_first_author": "Rowan Zellers", "paper_title": "Hellaswag: Can a machine really finish your sentence?", "reason": "This paper contributes to the evaluation of LLMs by introducing a dataset designed to measure the performance on common sense reasoning. This is highly relevant to the current study, as the effectiveness of the proposed compression techniques can be evaluated by testing their impact on various downstream tasks.", "section_number": 4}, {" publication_date": "2021", "fullname_first_author": "Albert Q Jiang", "paper_title": "Mistral 7b", "reason": "This paper introduces the Mistral family of models, which are used as the base models for one of the experiments of this paper, making it a key reference for understanding the experimental setup and the performance results.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Hugo Touvron", "paper_title": "Llama: Open and efficient foundation language models", "reason": "This paper is critical because it introduces the Llama family of large language models, which are used as one of the baselines for the experiments in the paper. It is essential for understanding the context of the experiments and comparing the proposed method's performance against state-of-the-art models.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Marah Abdin", "paper_title": "Phi-3 technical report: A highly capable language model locally on your phone", "reason": "This paper introduces another model family (Phi-3) used in the experiments. By providing the details of the architecture and performance of the Phi-3 models, this reference helps to establish a more comprehensive experimental setup, enabling a thorough comparison between different models and compression techniques.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Vage Egiazarian", "paper_title": "Extreme compression of large language models via additive quantization", "reason": "This paper is highly relevant because it provides a state-of-the-art method for additive quantization, which is one of the compression techniques investigated in the paper. Its inclusion provides a baseline for comparison, and its results help contextulize the paper's contributions.", "section_number": 4}, {" publication_date": "2021", "fullname_first_author": "Yichun Yin", "paper_title": "Autotinybert: Automatic hyper-parameter optimization for efficient pre-trained language models", "reason": "This paper proposes an automated hyperparameter optimization technique for efficient pre-trained language models. It's relevant because this is an alternative approach to the task and helps to clarify the contributions of the proposed method.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Lu Yin", "paper_title": "Outlier weighed layerwise sparsity (owl): A missing secret sauce for pruning llms to high sparsity", "reason": "This paper is highly relevant because it presents a state-of-the-art method for non-uniform pruning of LLMs. By comparing EvoPress's performance against OWL's, the authors can demonstrate that their proposed method significantly outperforms existing state-of-the-art techniques.", "section_number": 2}]}