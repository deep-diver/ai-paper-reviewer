{"references": [{" publication_date": "2022", "fullname_first_author": "Jonathan Ho", "paper_title": "Classifier-free diffusion guidance", "reason": "This paper introduces Classifier-Free Guidance (CFG), a crucial technique in enhancing sample quality for visual generative models.  It is central to the paper's background and motivation as CFG's limitations (high computational cost and training inconsistencies) drive the need for the proposed CCA method.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Prafulla Dhariwal", "paper_title": "Diffusion models beat gans on image synthesis", "reason": "This paper is a landmark achievement in the field of image generation, showcasing the power of diffusion models.  It provides crucial background on the capabilities and limitations of generative models, directly relevant to the context of autoregressive visual models discussed in this paper.", "section_number": 2}, {" publication_date": "2017", "fullname_first_author": "Aaron van den Oord", "paper_title": "Neural discrete representation learning", "reason": "This paper introduces vector-quantized (VQ) methods for encoding images into discrete tokens, a technique essential for applying autoregressive models to image generation.  The use of VQ is fundamental to the paper's approach and is discussed extensively in the background section.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Rafael Rafailov", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "reason": "This paper presents Direct Preference Optimization (DPO), a highly efficient method for directly aligning language models with human preferences.  DPO is critically relevant to the paper's proposed approach as CCA draws inspiration from DPO's direct optimization technique for improved alignment, shifting away from sampling-based methods.", "section_number": 2}, {" publication_date": "2018", "fullname_first_author": "Alec Radford", "paper_title": "Improving language understanding by generative pre-training", "reason": "This paper is a seminal work in the field of large language models (LLMs), introducing the concept of generative pre-training.  It is relevant because the paper's method, CCA, is inspired by the successful fine-tuning alignment techniques used for LLMs, demonstrating the potential of similar strategies in the visual domain.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "Jonathan Ho", "paper_title": "Denoising diffusion probabilistic models", "reason": "This paper lays the groundwork for diffusion models, a popular class of generative models.  The understanding of diffusion models provides critical context for the discussion of generative modeling approaches in the paper, contrasting them with autoregressive methods.", "section_number": 6}, {" publication_date": "2014", "fullname_first_author": "Ian Goodfellow", "paper_title": "Generative adversarial nets", "reason": "This paper is a foundational work in the field of generative adversarial networks (GANs).  It provides relevant background on the history and development of generative models, setting the stage for the discussion of other generative modeling methods used in this paper and their limitations.", "section_number": 6}, {" publication_date": "2024", "fullname_first_author": "Peize Sun", "paper_title": "Autoregressive model beats diffusion: Llama for scalable image generation", "reason": "This paper introduces LlamaGen, one of the two autoregressive visual models used in the paper's experiments. It is essential because the experimental results show that the proposed method, CCA, significantly improves the performance of this model, demonstrating CCA's effectiveness in practice.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Keyu Tian", "paper_title": "Visual autoregressive modeling: Scalable image generation via next-scale prediction", "reason": "This paper introduces VAR, the other autoregressive visual model used in the paper's experiments. Similar to the LlamaGen reference, the experimental results involving VAR reinforce the practical impact and generalizability of the proposed method.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Huiwen Chang", "paper_title": "Muse: Text-to-image generation via masked generative transformers", "reason": "This paper is directly cited as a closely related work.  The results and approach are discussed in this paper's experimental section as well.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Huayu Chen", "paper_title": "Noise contrastive alignment of language models with explicit rewards", "reason": "This paper presents the theoretical foundation of CCA by introducing Noise Contrastive Estimation and its implications for alignment.  The theoretical contributions provide a strong rationale for the technical approach employed by CCA.", "section_number": 3}, {" publication_date": "2021", "fullname_first_author": "Patrick Esser", "paper_title": "Taming transformers for high-resolution image synthesis", "reason": "This paper is a seminal work in applying transformers to image generation. This is directly related to the paper's research on autoregressive visual models and relevant to the background section.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Robin Rombach", "paper_title": "High-resolution image synthesis with latent diffusion models", "reason": "This paper presents a significant advancement in generating high-resolution images using diffusion models. It is relevant to the context of visual generative models and the paper's focus on improving image quality.", "section_number": 6}, {" publication_date": "2022", "fullname_first_author": "John Schulman", "paper_title": "ChatGPT: Optimizing language models for dialogue", "reason": "This paper details the optimization strategies used in training ChatGPT, a large language model. This is relevant to the context of language model alignment and the paper's inspiration in using direct training-based alignment for improved model performance.", "section_number": 6}, {" publication_date": "2018", "fullname_first_author": "Andrew Brock", "paper_title": "Large scale gan training for high fidelity natural image synthesis", "reason": "This paper introduces significant improvements in GAN training that resulted in high-fidelity image generation.  It's important for background on visual generative models.", "section_number": 6}, {" publication_date": "2023", "fullname_first_author": "Kevin Black", "paper_title": "Training diffusion models with reinforcement learning", "reason": "This paper explores the use of reinforcement learning to train diffusion models, a different approach than CCA but relevant to the overall theme of optimizing generative models.", "section_number": 6}, {" publication_date": "2023", "fullname_first_author": "Huayu Chen", "paper_title": "Aligning diffusion behaviors with q-functions for efficient continuous control", "reason": "This paper is another theoretical contribution by the authors, focused on aligning diffusion models using reinforcement learning.  It shows the theoretical link between alignment and reinforcement learning that is relevant to the context of CCA's approach.", "section_number": 6}, {" publication_date": "2012", "fullname_first_author": "Michael U Gutmann", "paper_title": "Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics", "reason": "This paper introduces Noise Contrastive Estimation (NCE), a fundamental technique underlying CCA's theoretical framework. NCE is used for estimating unnormalized probability distributions efficiently, which forms the basis of CCA\u2019s contrastive learning approach.", "section_number": 3}, {" publication_date": "1952", "fullname_first_author": "Ralph Allan Bradley", "paper_title": "Rank analysis of incomplete block designs: I. the method of paired comparisons", "reason": "This paper introduces the Bradley-Terry model, a fundamental framework for preference modeling.  The Bradley-Terry model is a core component of Direct Preference Optimization (DPO), upon which the theoretical foundation of CCA partly relies.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Tianhong Li", "paper_title": "Mage: Masked generative encoder to unify representation learning and image synthesis", "reason": "This paper introduces MAGE, a masked generative model for image synthesis that is closely related to CCA in terms of its approach and methodology.  The paper's results and methodology are useful for comparison and validating the CCA method\u2019s performance.", "section_number": 5}]}