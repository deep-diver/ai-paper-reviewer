{"references": [{" publication_date": "2017", "fullname_first_author": "Paulius Micikevicius", "paper_title": "Mixed precision training", "reason": "This paper is foundational to the field of low-precision training for deep learning, introducing the concept and demonstrating its effectiveness.  The work on mixed precision training is directly relevant to COAT's approach of using lower-precision formats for certain computations to reduce memory footprint and increase speed.  Its impact and wide adoption within the deep learning community makes it highly significant.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Paulius Micikevicius", "paper_title": "Fp8 formats for deep learning", "reason": "This NVIDIA paper directly introduces and analyzes the use of FP8 formats for deep learning.  This is the most directly relevant paper to COAT since FP8 precision is the focus of the proposed method.  The detailed analysis of the benefits and limitations of FP8 is critical for understanding the technical challenges and design decisions in COAT.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Bo Adler", "paper_title": "Nemotron-4 340b technical report", "reason": "This report provides comprehensive insights into training extremely large language models.  COAT focuses on optimizing the memory efficiency of such large models, and this report contains vital information on the challenges and scaling strategies that are directly relevant to the goals of COAT. The sheer scale of the model described makes it highly relevant.", "section_number": 1}, {" publication_date": "2019", "fullname_first_author": "Rohan Anil", "paper_title": "Memory efficient adaptive optimization", "reason": "This work explores techniques for memory efficient optimization, crucial for training large models. COAT focuses on reducing the memory footprint of optimizer states, and this paper introduces methods that are relevant to that aim. The memory efficiency focus aligns directly with COAT's objectives.", "section_number": 2}, {" publication_date": "2016", "fullname_first_author": "Jimmy Lei Ba", "paper_title": "Layer normalization", "reason": "Layer Normalization is a widely used normalization technique in deep learning, and its impact on memory usage and computational efficiency is significant.  COAT deals with efficient activation quantization, and understanding the role and computational demands of Layer Normalization within modern architectures is key. The paper describes the normalization technique used in many modern architectures.", "section_number": 5}, {" publication_date": "2020", "fullname_first_author": "Han Cai", "paper_title": "Tinytl: Reduce activations, not trainable parameters for efficient on-device learning", "reason": "This paper tackles memory efficiency in on-device learning by reducing the number of activations.  COAT addresses the activation memory footprint, and this paper presents a related approach that can inform the strategies used to achieve the memory reduction in COAT.", "section_number": 5}, {" publication_date": "2020", "fullname_first_author": "Jianfei Chen", "paper_title": "A statistical framework for low-bitwidth training of deep neural networks", "reason": "This paper explores low-bitwidth training, a closely related technique to the FP8 training that COAT employs.  Understanding the statistical properties of low-bitwidth training is crucial to the success of COAT's quantization methods, ensuring that the reduced precision doesn't result in unacceptable accuracy loss.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Jianfei Chen", "paper_title": "ActNN: Reducing training memory footprint via 2-bit activation compressed training", "reason": "This paper proposes a 2-bit activation compression technique, which is closely related to COAT's goal of reducing the activation memory footprint. The comparison between different low-bit precision activation methods is essential to designing efficient and accurate methods for training large models.", "section_number": 2}, {" publication_date": "2018", "fullname_first_author": "Steve Dai", "paper_title": "Vs-quant: Per-vector scaled quantization for accurate low-precision neural network inference", "reason": "This paper introduces a per-vector scaled quantization technique relevant to COAT's mixed-granularity approach.  COAT uses fine-grained quantization for non-linear layers, and VS-Quant is a related method that demonstrates the benefits of this approach in maintaining accuracy. This is specifically used for non-linear layers in COAT.", "section_number": 5}, {" publication_date": "2021", "fullname_first_author": "Alex Davies", "paper_title": "Advancing mathematics by guiding human intuition with ai", "reason": "While not directly related to low-precision training, this paper highlights the increasing capabilities of large language models in mathematical reasoning.  COAT aims to improve the efficiency of training such models, and understanding their advanced capabilities underscores the importance of the efficiency improvements proposed by COAT.", "section_number": 6}, {" publication_date": "2021", "fullname_first_author": "Tim Dettmers", "paper_title": "8-bit optimizers via block-wise quantization", "reason": "This paper explores quantization of optimizer states, a key area COAT also focuses on.  This specific work on quantizing optimizers is critical to the understanding of how to compress optimizer states efficiently. This is directly related to COAT's memory saving in optimizer states.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Abhimanyu Dubey", "paper_title": "The llama 3 herd of models", "reason": "This report details the training of large language models, providing insights relevant to the challenges addressed by COAT.  Training large models involves significant memory usage, and this report's description of the training process helps understand and refine the memory optimization techniques in COAT.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Maxim Fishman", "paper_title": "Scaling fp8 training to trillion-token llms", "reason": "This paper directly addresses FP8 training at a large scale, which is the core of COAT's contribution. It explores the challenges and techniques to enable efficient FP8 training, and as such this is highly relevant to the approach of COAT.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "Han Cai", "paper_title": "Tinytl: Reduce activations, not trainable parameters for efficient on-device learning", "reason": "This paper is relevant to the activation quantization aspect of COAT because it focuses on reducing the memory footprint of neural networks by reducing activation memory.  COAT also focuses on efficiently managing activation memory, making this paper highly relevant to that aspect of COAT.", "section_number": 5}, {" publication_date": "2017", "fullname_first_author": "Paulius Micikevicius", "paper_title": "Mixed precision training", "reason": "This paper is foundational to the field of low-precision training and is highly relevant to COAT's approach. The introduction of mixed precision training is a fundamental concept underlying COAT's approach. The early work on mixed precision is highly relevant to the motivation and technical considerations in COAT.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Georgii Sergeevich Novikov", "paper_title": "Few-bit backward: Quantized gradients of activation functions for memory footprint reduction", "reason": "This paper proposes a technique for reducing memory footprint by quantizing gradients of activation functions.  COAT focuses on activation quantization, and this method could be considered as an alternative or complementary approach.  Comparing different activation compression approaches informs the design decisions and demonstrates related work.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "Colin Raffel", "paper_title": "Exploring the limits of transfer learning with a unified text-to-text transformer", "reason": "This paper introduces a powerful text-to-text transformer model, which is highly relevant to the LLM training that COAT is evaluated on.  The architecture and training methodology used in this model are directly relevant to the considerations of training large language models and understanding efficiency improvements.", "section_number": 6}, {" publication_date": "2020", "fullname_first_author": "Jeff Rasley", "paper_title": "Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters", "reason": "Deepspeed is a widely used large-scale model training framework, and its optimizations are directly relevant to COAT's attempts at efficient training of large models.  Understanding Deepspeed's techniques can help to compare and contrast the specific memory optimization strategies in COAT.", "section_number": 6}, {" publication_date": "2018", "fullname_first_author": "Noam Shazeer", "paper_title": "Adafactor: Adaptive learning rates with sublinear memory cost", "reason": "Adafactor is a memory-efficient optimizer, which is directly relevant to COAT's aim of reducing memory usage.  COAT focuses on reducing the memory footprint of both optimizer states and activations, and this paper demonstrates an approach towards efficient optimization that could be complementary to COAT's approach.", "section_number": 2}, {" publication_date": "2019", "fullname_first_author": "Mohammad Shoeybi", "paper_title": "Megatron-lm: Training multi-billion parameter language models using model parallelism", "reason": "Megatron-LM demonstrates the training of large language models and highlights the challenges of memory and computation scalability that COAT addresses directly.  Its large-scale model training techniques inform the approaches used in COAT to improve efficiency.", "section_number": 1}]}