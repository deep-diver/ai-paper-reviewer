[{"figure_path": "2410.19313/tables/table_5_0.html", "caption": "Table 1: Quantization error of m under different quantization settings. +Expand means applying our Dynamic Range Expansion method.", "description": "Table 1 shows the quantization error of the first-order momentum (m) under different quantization settings, both before and after applying the Dynamic Range Expansion method.", "section": "4 DYNAMIC RANGE EXPANSION FOR ACCURATE OPTIMIZER QUANTIZATION"}, {"figure_path": "2410.19313/tables/table_6_0.html", "caption": "Table 2: Activation memory footprint of different operators. U is a unit to measure memory usage, where 1U = Batch Size \u00d7 Sequence Length \u00d7 Hidden Size \u00d7 2 bytes (for BF16). For Llama-style model, Act Func refers to SiLU & Multiply, and Linear refers to the summation of QKV/Attn/Up/Gate/Down projection. RMSNorm is upcast to float32 in transformers implementation, so the memory usage of LayerNorm in BF16 is 4U. Our method reduces activation memory by quantizing them to FP8. More details about FlashAttention in Appendix D.", "description": "Table 2 shows the memory footprint of different operators in BF16, TransformerEngine, and COAT, highlighting the memory reduction achieved by COAT through FP8 quantization.", "section": "5 MIXED-GRANULARITY ACTIVATION QUANTIZATION"}, {"figure_path": "2410.19313/tables/table_8_0.html", "caption": "Table 3: OLMo-1B pretraining performance on downstream tasks. TE refers to TransformerEngine.", "description": "Table 3 presents a comparison of OLMo-1B's performance on several downstream tasks using BF16, TransformerEngine, and COAT, showcasing the similar performance across all three methods.", "section": "6.1.1 LLM PRETRAINING"}, {"figure_path": "2410.19313/tables/table_8_1.html", "caption": "Table 4: Evaluation result of fine-tuning Llama-2-7B on math corpus. Llama-2-7B refers to the evaluation metric before fine-tuning. TE refers to TransformerEngine.", "description": "Table 4 presents the evaluation results of fine-tuning Llama-2-7B on various mathematical reasoning datasets, comparing BF16, TransformerEngine, and COAT.", "section": "6.1.2 LLM FINE-TUNING"}, {"figure_path": "2410.19313/tables/table_9_0.html", "caption": "Table 5: VILA1.5-7B Stage-3 SFT performance on downstream tasks. * means it has seen the training data.", "description": "Table 5 presents the performance of VILA1.5-7B after Stage-3 SFT on various downstream tasks, comparing BF16, TransformerEngine, and COAT.", "section": "6.1.3 VLM TRAINING"}, {"figure_path": "2410.19313/tables/table_9_1.html", "caption": "Table 6: Memory Saving and Speedup for a single Transformer Layer. Memory refers to Activation Memory. Our method achieves better speedup than TransformerEngine and significantly reduces the activation memory footprint by 1.65\u00d7.", "description": "Table 6 shows the memory saving and speedup results for a single transformer layer with different hidden sizes and sequence lengths, comparing BF16, TransformerEngine, and COAT.", "section": "6.2.1 MEMORY SAVING AND SPEEDUP FOR A SINGLE TRANSFORMER LAYER"}, {"figure_path": "2410.19313/tables/table_10_0.html", "caption": "Table 7: End-to-end memory reduction and speedup results. BS refers to batch size. CL refers to context length. We report token/s per GPU for speed results. \u2021 means CL=1024.", "description": "Table 7 presents a detailed comparison of end-to-end memory reduction and speedup results across different configurations for transformer models, specifically Llama-2-7B, Llama-2-13B, and Llama-30B, with variations in the number of GPUs used (1, 2, 4, and 8).", "section": "6.2 Memory Saving and Speedup"}, {"figure_path": "2410.19313/tables/table_10_1.html", "caption": "Table 8: Dynamic Range Expansion is compatible with DE8 (8-bit dynamic quantization).", "description": "Table 8 presents quantization error of first and second order momentum under different quantization methods, showing that Dynamic Range Expansion consistently improves accuracy regardless of the underlying quantization method.", "section": "6.3 ABLATION STUDIES"}]