{"importance": "This paper is crucial for researchers working with large language models (LLMs) because it introduces a novel self-supervised method for improving citation quality in LLM-generated text.  **This addresses a significant limitation of current LLMs, namely their tendency to hallucinate or misattribute information.** The self-supervised nature of the approach reduces reliance on expensive and time-consuming human annotation, making it more practical for real-world applications.  The findings also open avenues for further research into self-supervised alignment techniques and improving the reliability of LLMs.", "summary": "SelfCite: A self-supervised approach boosts LLM citation accuracy via context ablation.  By removing or isolating cited text, SelfCite trains LLMs to generate high-quality citations without manual annotation, significantly improving citation F1 scores.", "takeaways": ["SelfCite, a novel self-supervised method, significantly improves the quality of citations generated by LLMs.", "The approach uses context ablation to generate a reward signal, eliminating the need for manual annotation.", "SelfCite achieves state-of-the-art results on the LongBench-Cite benchmark, demonstrating its effectiveness."], "tldr": "Large language models (LLMs) are increasingly used for information gathering, but they often produce inaccurate or fabricated information (hallucinations) and lack proper context attribution.  Existing methods to mitigate these issues typically rely on manual annotations which are costly and time-consuming.  This creates a need for self-supervised methods that can improve LLM reliability and citation quality without human intervention. \nSelfCite tackles this challenge by using a self-supervised approach that aligns LLMs to generate high-quality citations.  It leverages a reward signal derived from context ablation\u2014removing or isolating cited text to evaluate citation necessity and sufficiency.  This reward signal guides a best-of-N sampling strategy and preference optimization to improve citation quality, leading to improvements in citation F1 scores of up to 5.3 points on the LongBench-Cite benchmark.  **The self-supervised nature of SelfCite makes it a more efficient and scalable solution compared to methods relying on human annotation.**", "affiliation": "MIT", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}, "podcast_path": "2502.09604/podcast.wav"}