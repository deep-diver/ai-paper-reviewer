{"references": [{" publication_date": "2024", "fullname_first_author": "Ishaan Watts", "paper_title": "MEGA-VERSE: benchmarking large language models across languages, modalities, models and tasks", "reason": "This paper introduces a comprehensive benchmark (MEGA-VERSE) for evaluating LLMs across multiple languages and tasks, directly addressing the problem of limited multilingual benchmarks mentioned in the introduction.  Its focus on various modalities and tasks makes it highly relevant to the broader goals of multilingual LLM evaluation discussed in this paper.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Sumanth Doddapaneni", "paper_title": "Towards leaving no Indic language behind: Building monolingual corpora, benchmark and models for Indic languages", "reason": "This paper tackles the specific challenge of evaluating LLMs for low-resource Indic languages. Its creation of monolingual corpora, a benchmark and models is critical for advancing multilingual evaluation by providing resources specifically targeted at this under-represented language family. The work is highly relevant to the CIA Suite's goal of supporting low-resource scenarios.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Sumanth Doddapaneni", "paper_title": "Finding blind spots in evaluator LLMs with interpretable checklists", "reason": "This paper is highly relevant because it directly addresses the limitations of LLM-based evaluations, a key concern highlighted in the introduction. By proposing 'interpretable checklists' to evaluate LLMs, this work contributes to improving the accuracy and reliability of LLM-based evaluations, a major aspect of the CIA Suite's approach.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "David Cheng-Han Chiang", "paper_title": "Can large language models be an alternative to human evaluations?", "reason": "This paper is central to the discussion in the introduction regarding the limitations of human evaluation methods, especially the subjectivity of 'vibe checks'. By exploring the potential of LLMs as alternatives, it addresses the main challenges of evaluation methods discussed in this work.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Wei-Lin Chiang", "paper_title": "Chatbot arena: An open platform for evaluating llms by human preference", "reason": "This paper introduces an open-source platform for evaluating LLMs through human preference, addressing the subjective nature of human evaluations. It enhances the discussion on the challenges and potential solutions in the introduction regarding human evaluations and the CIA suite's approach to address these shortcomings.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Viraat Aryabumi", "paper_title": "Aya 23: Open weight releases to further multilingual progress", "reason": "This paper introduces a multilingual LLM (AYA23) with 8B parameters that is evaluated in the results section.  Its large parameter size and multilingual focus make it a significant model for comparison in the context of multilingual LLM assessment; directly relevant to the experimental setup of this paper.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Gemini Team", "paper_title": "The llama 3 herd of models", "reason": "This work provides the foundational LLMs (Llama-3.1) upon which the HERCULE models are built and evaluated. The Llama models represent state-of-the-art models; their characteristics influence the design and performance of the HERCULE evaluators.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Seungone Kim", "paper_title": "Prometheus: Inducing fine-grained evaluation capability in language models", "reason": "This paper is highly relevant because it introduces a fine-tuned LLM-based evaluation approach. This approach is similar to what the proposed HERCULE models do and is therefore used as a comparative baseline in the experiments.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Seungone Kim", "paper_title": "Feedback-Collection", "reason": "This dataset is the foundation for the INTEL training data used in this work. The direct usage of this dataset for training and the importance of the dataset for the study make this reference crucial.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Seungone Kim", "paper_title": "Feedback-Bench", "reason": "The Feedback-Bench dataset is used for validation in this paper, demonstrating its importance in the training and evaluation process for the proposed methodology.  The usage of this benchmark for validation makes this reference directly relevant to the experimental methodology.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Ning Ding", "paper_title": "Enhancing chat language models by scaling high-quality instructional conversations", "reason": "This paper is relevant because it discusses enhancing LLMs' instruction-following capabilities. Given that instruction-following is a key aspect of multilingual LLM evaluation, this work informs the related tasks addressed in the current paper.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Abhimanyu Dubey", "paper_title": "Finding blind spots in evaluator LLMs with interpretable checklists", "reason": "This paper introduces the concept of interpretable checklists for evaluating LLMs, a method used to analyze the performance of the HERCULE model. The usage of this evaluation method in this study makes this paper directly relevant to the methodological framework.", "section_number": 5}, {" publication_date": "2021", "fullname_first_author": "Tetsuya Sakai", "paper_title": "Evaluating evaluation measures for ordinal classification and ordinal quantification", "reason": "This paper provides the theoretical foundation for the Cohen's Kappa (\u03ba) metric, which is used as a primary metric to evaluate the LLM evaluators in this paper.  The choice of this metric for quantitative evaluation highlights the relevance of this work.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Yang Liu", "paper_title": "G-eval: NLG evaluation using GPT-4 with better human alignment", "reason": "This paper introduces GPT-4 as an evaluator LLM.  Since GPT-4 is a strong baseline used in the results section, and also used as a comparison model, understanding its method is crucial to interpreting the results of the current study.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Lianmin Zheng", "paper_title": "Judging LLM-as-a-judge with mt-bench and chatbot arena", "reason": "This paper introduces MT-Bench, a multilingual benchmark for evaluating LLMs, which is directly relevant to the goals of the proposed CIA suite. It also discusses LLMs as evaluators, which are central to this study.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Chunting Zhou", "paper_title": "LIMA: less is more for alignment", "reason": "This paper is relevant to the discussion on large language models in the introduction and particularly relevant to section 5 where the authors compare the performance of their model with other large language models like GPT-4 and Gemini. Its use as a comparison model in the results necessitates including it.", "section_number": 5}, {" publication_date": "2022", "fullname_first_author": "Mitchell Wortsman", "paper_title": "Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time", "reason": "This paper explores model averaging techniques for improving performance, a method investigated in this work using weight merging. The relevance of this paper stems from the adoption of similar model merging techniques in the experimental setup.", "section_number": 6}, {" publication_date": "2023", "fullname_first_author": "Joel Jang", "paper_title": "Exploring the benefits of training expert language models over instruction tuning", "reason": "This paper focuses on training language models for specific tasks, similar to the training approach used for HERCULE. This relevance stems from the similar approach adopted for training the evaluator LLMs in this work.", "section_number": 6}, {" publication_date": "2023", "fullname_first_author": "Prateek Yadav", "paper_title": "TIES-merging: resolving interference when merging models", "reason": "This paper discusses techniques for merging multiple models effectively, which is directly relevant to the weight merging method used in this study (Section 6.4).  Understanding this methodology is crucial for interpreting the results and evaluating the effectiveness of the weight merging technique.", "section_number": 6}]}