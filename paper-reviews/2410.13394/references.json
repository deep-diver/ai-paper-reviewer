{"references": [{" publication_date": "2024", "fullname_first_author": "Sumanth Doddapaneni", "paper_title": "Cross-Lingual Auto Evaluation for Assessing Multilingual LLMs", "reason": "This is the main paper under review.  It introduces a novel cross-lingual auto evaluation framework (CIA) for assessing multilingual LLMs, addressing the significant challenge of evaluating machine-generated text in multiple languages.  It presents a novel test set and evaluator LLM, pushing the boundaries of current multilingual evaluation.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Ishaan Watts", "paper_title": "MEGA-VERSE: Benchmarking large language models across languages, modalities, models and tasks", "reason": "This paper introduces MEGA-VERSE, a multilingual benchmark that is relevant for evaluating the general purpose multilingual LLMs used in the paper. This benchmark is referred to when the authors discuss the lack of such benchmarks for multilingual LLM evaluation.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Sumanth Doddapaneni", "paper_title": "Finding blind spots in evaluator LLMs with interpretable checklists", "reason": "This paper is relevant because it discusses the use of LLMs for evaluation, addressing some limitations of using LLMs as evaluators and demonstrating better alignment with human judgments when compared with models from prior work, thus providing a good comparison for the work presented.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "David Cheng-Han Chiang", "paper_title": "Can Large Language Models Be an Alternative to Human Evaluations?", "reason": "This paper explores the use of LLMs as alternatives to human evaluations, which is relevant to the work presented since it proposes a different approach to evaluation.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Wei-Lin Chiang", "paper_title": "Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference", "reason": "This paper introduces a new framework for evaluating LLMs based on human preferences, addressing some limitations of the traditional evaluation approaches.", "section_number": 1}, {" publication_date": "2020", "fullname_first_author": "Ricardo Rei", "paper_title": "COMET: A Neural Framework for MT Evaluation", "reason": "This paper is relevant to the study because it proposes a neural framework for machine translation evaluation which can be used to compare with the proposed cross-lingual evaluation model. ", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Viraat Aryabumi", "paper_title": "Aya 23: Open weight releases to further multilingual progress", "reason": "This paper introduces a new multilingual LLM which can be used to compare the performance of the proposed cross-lingual LLM evaluator, showcasing the potential of trained evaluators.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Seungone Kim", "paper_title": "Prometheus: Inducing Fine-Grained Evaluation Capability in Language Models", "reason": "This paper is relevant to the work presented since it proposes a framework for evaluation of LLMs which is used to train the INTEL training data. The proposed model is also compared with the models presented in this paper.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Ning Ding", "paper_title": "Enhancing Chat Language Models by Scaling High-Quality Instructional Conversations", "reason": "This paper is referred to when discussing the RECON test set, providing context for the design choices made.  It provides another perspective on multilingual evaluation and highlights the significance of human-generated data for enhanced LLM capabilities.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Yann Dubois", "paper_title": "AlpacaEval: An Automatic Evaluator of Instruction-Following Models", "reason": "This paper discusses the use of LLMs for automated evaluation, which is relevant to the work presented, comparing the approach taken in this paper with the proposed approach.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Abhimanyu Dubey", "paper_title": "GEMMA: An Open-Source LLM Trained on 6T Tokens with Better Tokenizer Fertility", "reason": "This paper is relevant to the work presented because it introduces a new open-source LLM which can be used to compare the performance of the proposed cross-lingual LLM evaluator.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Ishaan Watts", "paper_title": "MEGA-VERSE: Benchmarking Large Language Models Across Languages, Modalities, Models, and Tasks", "reason": "This paper provides a relevant benchmark for evaluating the performance of the proposed cross-lingual LLM evaluator, allowing for a comparative analysis.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Sumanth Doddapaneni", "paper_title": "Finding Blind Spots in Evaluator LLMs with Interpretable Checklists", "reason": "This paper discusses the use of LLMs for evaluation, providing a comparison for the work presented in terms of effectiveness and alignment with human judgments. This helps to contextualize the findings of the current study.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Lianmin Zheng", "paper_title": "Judging LLMs-as-a-Judge with MT-Bench and Chatbot Arena", "reason": "This paper provides insights into the evaluation of LLMs using other LLMs, directly relevant to the core methodology of the paper under review.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Yang Liu", "paper_title": "G-Eval: NLG Evaluation Using GPT-4 with Better Human Alignment", "reason": "This paper is relevant because it discusses the use of GPT-4 for evaluation, providing a comparison for the work presented in terms of effectiveness and alignment with human judgments. This helps to contextualize the findings of the current study.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Mitchell Wortsman", "paper_title": "Model Soups: Averaging Weights of Multiple Fine-Tuned Models Improves Accuracy Without Increasing Inference Time", "reason": "This paper is cited in the context of the weight merging techniques used in the current research, specifically to improve performance across multiple tasks or languages.", "section_number": 6}, {" publication_date": "2023", "fullname_first_author": "Prateek Yadav", "paper_title": "TIES-Merging: Resolving Interference When Merging Models", "reason": "This paper is relevant to the methodology used in the current research, focusing on the weight merging techniques utilized to combine multiple models for improved efficiency and performance across various languages.", "section_number": 6}, {" publication_date": "2024", "fullname_first_author": "Gemini Team", "paper_title": "The Llama 3 Herd of Models", "reason": "This paper is highly relevant to the present work as the LLMs being used for comparison are from the Llama family of models. The paper provides context for the models under consideration and gives details about the training data and other features of the LLMs, helping to put the results of the current study in perspective.", "section_number": 6}, {" publication_date": "2024", "fullname_first_author": "Gemma Team", "paper_title": "Gemma: Open Models Based on Gemini Research and Technology", "reason": "This paper presents a family of open-source LLMs, some of which are used in the study's ablations, providing a foundation for comparing different model architectures and training techniques.", "section_number": 6}, {" publication_date": "2023", "fullname_first_author": "Zhiyuan Zeng", "paper_title": "Evaluating Large Language Models at Evaluating Instruction Following", "reason": "This paper discusses the use of LLMs for automated evaluation and compares the approach with the proposed approach.", "section_number": 6}]}