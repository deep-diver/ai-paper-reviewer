[{"figure_path": "2410.13394/figures/figures_1_0.png", "caption": "Figure 1: We present cross-lingual Evaluator LLM, HERCULE, where the Instruction & Response provided to the model are in the target language, while all other fields are in English. The model generates feedback & score in English for a given evaluation example.", "description": "The figure illustrates the CIA (Cross-Lingual Auto Evaluation) suite architecture, showing how the HERCULE evaluator LLM processes multilingual instructions and responses to generate feedback and scores.", "section": "3 CIA: Cross Lingual Auto Evaluation"}, {"figure_path": "2410.13394/figures/figures_3_0.png", "caption": "Figure 2: Distribution of task capabilities in RECON.", "description": "The figure shows the distribution of different task categories in the RECON benchmark dataset.", "section": "3.1 RECON: Test Data"}]