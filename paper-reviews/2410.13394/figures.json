[{"figure_path": "2410.13394/figures/figures_1_0.png", "caption": "Figure 1: We present cross-lingual Evaluator LLM, HERCULE, where the Instruction & Response provided to the model are in the target language, while all other fields are in English. The model generates feedback & score in English for a given evaluation example.", "description": "The figure illustrates the CIA (Cross-Lingual Auto Evaluation) Suite's architecture, showcasing the HERCULE evaluator LLM's cross-lingual evaluation process.", "section": "3 CIA: Cross Lingual Auto Evaluation"}, {"figure_path": "2410.13394/figures/figures_3_0.png", "caption": "Figure 2: Distribution of task capabilities in RECON.", "description": "The figure shows a donut chart illustrating the distribution of different task capabilities within the RECON benchmark dataset.", "section": "3 CIA: Cross Lingual Auto Evaluation"}]