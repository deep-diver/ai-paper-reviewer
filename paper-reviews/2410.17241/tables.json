[{"figure_path": "2410.17241/tables/table_5_0.html", "caption": "TABLE 2\nSummary of classification models in colonoscopy. Dataset: CU=CU-ColonDB [25], CDS=ColonoscopicDS [26], Private=private data, HK=HyperKvasir [43], KC=Kvasir-Capsule [56]. Backbone: CaffeNet [97], D-121=DenseNet121 [98], R-12/-18/-50/-101=ResNet12/18/50/101 [99], VIT-S16 or ViT-B16 [100], MobV2=MobileNetV2 [101], R50-Att=ResNet50 with attention module [102], C3D [103], Inc-v3=Inceptionv3 [104], I3D [105]. \"Customised\" means a base network modified for the current task or a model independent of the base network choice. Head: classifier implemented by the fully connected (FC) and support vector machine (SVM) layers, or using the l\u00b2 norm to measure the disparity between the input and output. Arch: the architectures shown in Fig. 3. Sup: learning strategies such as fully supervised (FS), semi-supervised (SS), unsupervised (US), and weakly supervised (WS). For simplicity, the following tables use consistent abbreviations unless specified otherwise.", "description": "Table 2 summarizes 18 image-based and 5 video-based classification models for colonoscopy, detailing their core designs, datasets, backbones, architectures, heads, and learning strategies.", "section": "4 Classification models"}, {"figure_path": "2410.17241/tables/table_10_0.html", "caption": "TABLE 6\nDetails of instruction tuning dataset ColonINST. For each task, we provide five templates for human instructions, the data sources used to organise human-machine dialogues, and an example of a human-machine conversation.", "description": "Table 6 presents details of the ColonINST dataset, including instruction templates, data sources, and sample human-machine dialogues for four tasks: classification, region proposal, region comprehension, and captioning.", "section": "5.1 Established instruction tuning dataset: ColonINST"}, {"figure_path": "2410.17241/tables/table_12_0.html", "caption": "TABLE 7\nMultimodal benchmark for three conversational tasks. \"LoRA\" refers to fine-tuning using low-rank adaptation [282]. \"EXT\" indicates the use of\npre-trained weights on extra data. We compare the results on the seen samples from the validation set and the unseen samples from the testing\nset of ColonINST. The symbol \u2191 signifies that a higher score reflects better performance.", "description": "This table presents a multimodal benchmark comparing eight popular MLMs on three conversational tasks (classification, referring expression generation, and referring expression comprehension) using the ColonINST dataset, showing performance on both seen and unseen samples with and without LoRA fine-tuning.", "section": "5.3 Experiments"}, {"figure_path": "2410.17241/tables/table_13_0.html", "caption": "TABLE 8\nDiagnostic studies of three core components in ColonGPT. \"*\": interpolate the position embeddings for higher resolution, specifically from\n224px to 384px. Our default configurations are shaded with a gray background.", "description": "Table 8 presents diagnostic studies of three core components in ColonGPT, showing the impact of different visual encoders, multimodal adapters, and fine-tuning strategies on the model's performance across three conversational tasks.", "section": "5.3 Experiments"}]