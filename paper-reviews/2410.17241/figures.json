[{"figure_path": "2410.17241/figures/figures_1_0.png", "caption": "Fig. 1. Introductory diagram. We depict (a) the anatomy of the large intestine (colon) within the digestive tract, the polypectomy procedure during colonoscopy examination, and the components of a colonoscope. The bottom figure (b) summarises three highlights of this study.", "description": "The figure illustrates the anatomy of the large intestine, the polypectomy procedure, colonoscope components, and summarizes the study's three highlights: literature investigation, instruction tuning dataset, and multimodal language model.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.17241/figures/figures_2_0.png", "caption": "Fig. 2. Colonscopic scene perception from visual to multimodal perspectives. In clinical practice, purely visual tasks, including (a) classification, (b) detection, and (c) segmentation, are applied to identify targets of interest such as polyps and instruments. (d) Multimodal applications improve colonoscopy procedures by performing interactive, user-driven tasks aligned with clinical needs. The chatbot provides personalised advice, automated reporting, and streamline procedural workflows.", "description": "The figure illustrates four colonoscopic scene perception tasks (classification, detection, segmentation, and multimodal applications) and how they can be used to improve colonoscopy procedures.", "section": "2 BACKGROUND"}, {"figure_path": "2410.17241/figures/figures_5_0.png", "caption": "Fig. 3. Gallery of deep-based architectures. The single-stream framework (SF) features a single input and output with sequential data flow. Multi-stream frameworks predict a single output but involve parallel processing streams, either at the decoding stage (MF#1) or the encoding stage (MF#2). Branched frameworks extend multi-stream framework to produce multiple outputs from either a single input (BF#1) or multiple inputs (BF#2). These side outputs typically receive supervision from additional supervisory signals, such as boundary cues.", "description": "The figure illustrates five deep-learning architectures used for colonoscopic image analysis, categorized by data flow and processing streams.", "section": "4 Revisiting Colonoscopy Models"}, {"figure_path": "2410.17241/figures/figures_9_0.png", "caption": "Fig. 4. Details of the established ColonINST. (a) Three sequential steps to create the instruction tuning dataset for multimodal research. (b) Numbers of colonoscopy images designated for training, validation, and testing purposes. (c) Data taxonomy of three-level categories. (d) A word cloud of the category distribution by name size. (e) Caption generation pipeline using the VL prompting mode of GPT-4V [4]. (f) Numbers of human-machine dialogues created for four downstream tasks.", "description": "Figure 4 shows the creation process of ColonINST dataset, including data statistics, taxonomy, caption generation pipeline and human-machine dialogues.", "section": "5.1 Established instruction tuning dataset: ColonINST"}, {"figure_path": "2410.17241/figures/figures_10_0.png", "caption": "Fig. 5. Response comparison for colonoscopy image classification. We evaluate the zero-shot language responses from three AI chatbots against the response from our multimodal model, ColonGPT.", "description": "The figure compares the zero-shot language responses of three AI chatbots and ColonGPT for colonoscopy image classification, highlighting ColonGPT's superior accuracy.", "section": "5.2 Proposed multimodal language model: ColonGPT"}, {"figure_path": "2410.17241/figures/figures_11_0.png", "caption": "Fig. 6. Details of our multimodal language model, ColonGPT.", "description": "The figure illustrates the architecture of ColonGPT, a multimodal language model designed for interactive colonoscopy tasks, highlighting its visual encoder, multimodal adapter, and language model components.", "section": "5.2 Proposed multimodal language model: ColonGPT"}, {"figure_path": "2410.17241/figures/figures_12_0.png", "caption": "Fig. 7. Illustration of ColonGPT\u2019s multimodal capabilities. Our model can execute various multimodal colonoscopy tasks through conversational interactions, including comprehension (CLS, REG), localisation (REC), and captioning (CAP) based.", "description": "Figure 7 shows examples of ColonGPT performing different colonoscopy tasks through conversational interactions.", "section": "5.3 Experiments"}, {"figure_path": "2410.17241/figures/figures_12_1.png", "caption": "Fig. 7. Illustration of ColonGPT's multimodal capabilities. Our model can execute various multimodal colonoscopy tasks through conversational interactions, including comprehension (CLS, REG), localisation (REC), and captioning (CAP) based.", "description": "Figure 7 showcases ColonGPT's ability to perform various colonoscopy tasks (classification, referring expression generation/comprehension, and captioning) through conversational interactions.", "section": "5.3 Experiments"}]