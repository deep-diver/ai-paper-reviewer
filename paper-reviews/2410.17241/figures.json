[{"figure_path": "2410.17241/figures/figures_1_0.png", "caption": "Fig. 1. Introductory diagram. We depict (a) the anatomy of the large intestine (colon) within the digestive tract, the polypectomy procedure during colonoscopy examination, and the components of a colonoscope. The bottom figure (b) summarises three highlights of this study.", "description": "Figure 1 is an introductory diagram showing the anatomy of the large intestine, the polypectomy procedure, colonoscope components, and a summary of the study's three highlights.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.17241/figures/figures_2_0.png", "caption": "Fig. 2. Colonscopic scene perception from visual to multimodal perspectives. In clinical practice, purely visual tasks, including (a) classification, (b) detection, and (c) segmentation, are applied to identify targets of interest such as polyps and instruments. (d) Multimodal applications improve colonoscopy procedures by performing interactive, user-driven tasks aligned with clinical needs. The chatbot provides personalised advice, automated reporting, and streamline procedural workflows.", "description": "The figure illustrates the four colonoscopic scene perception tasks (classification, detection, segmentation, and multimodal applications) and their clinical implications.", "section": "2 BACKGROUND"}, {"figure_path": "2410.17241/figures/figures_5_0.png", "caption": "Fig. 3. Gallery of deep-based architectures. The single-stream framework (SF) features a single input and output with sequential data flow. Multi-stream frameworks predict a single output but involve parallel processing streams, either at the decoding stage (MF#1) or the encoding stage (MF#2). Branched frameworks extend multi-stream framework to produce multiple outputs from either a single input (BF#1) or multiple inputs (BF#2). These side outputs typically receive supervision from additional supervisory signals, such as boundary cues.", "description": "The figure illustrates five deep-based architectures used in colonoscopy scene perception, categorized by their data flow management strategies.", "section": "4 Revisiting Colonoscopy Models"}, {"figure_path": "2410.17241/figures/figures_9_0.png", "caption": "Fig. 4. Details of the established ColonINST. (a) Three sequential steps to create the instruction tuning dataset for multimodal research. (b) Numbers of colonoscopy images designated for training, validation, and testing purposes. (c) Data taxonomy of three-level categories. (d) A word cloud of the category distribution by name size. (e) Caption generation pipeline using the VL prompting mode of GPT-4V [4]. (f) Numbers of human-machine dialogues created for four downstream tasks.", "description": "Figure 4 shows the creation process of ColonINST, a multimodal instruction tuning dataset for colonoscopy, including data statistics, taxonomy, caption generation pipeline, and human-machine dialogue statistics.", "section": "5.1 Established instruction tuning dataset: ColonINST"}, {"figure_path": "2410.17241/figures/figures_10_0.png", "caption": "Fig. 5. Response comparison for colonoscopy image classification. We evaluate the zero-shot language responses from three AI chatbots against the response from our multimodal model, ColonGPT.", "description": "The figure compares the zero-shot language responses of three AI chatbots against ColonGPT for colonoscopy image classification, highlighting ColonGPT's superior accuracy.", "section": "5.2 Proposed multimodal language model: ColonGPT"}, {"figure_path": "2410.17241/figures/figures_11_0.png", "caption": "Fig. 6. Details of our multimodal language model, ColonGPT.", "description": "The figure illustrates the architecture of ColonGPT, a multimodal language model designed for interactive colonoscopy tasks, showing its visual encoder, multimodal adapter, language model, and multigranularity views.", "section": "5.2 Proposed multimodal language model: ColonGPT"}, {"figure_path": "2410.17241/figures/figures_12_0.png", "caption": "Fig. 7. Illustration of ColonGPT\u2019s multimodal capabilities. Our model can execute various multimodal colonoscopy tasks through conversational interactions, including comprehension (CLS, REG), localisation (REC), and captioning (CAP) based.", "description": "Figure 7 shows examples of ColonGPT's abilities to perform classification, region classification, localization, and captioning tasks through conversational interactions.", "section": "5.3 Experiments"}, {"figure_path": "2410.17241/figures/figures_12_1.png", "caption": "Fig. 7. Illustration of ColonGPT\u2019s multimodal capabilities. Our model can execute various multimodal colonoscopy tasks through conversational interactions, including comprehension (CLS, REG), localisation (REC), and captioning (CAP) based.", "description": "The figure illustrates ColonGPT's ability to perform four colonoscopy tasks (classification, referring expression generation, referring expression comprehension, and image captioning) through conversational interactions.", "section": "5.3 Experiments"}]