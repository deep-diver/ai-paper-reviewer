[{"figure_path": "https://arxiv.org/html/2502.05178/extracted/6182739/figures/teaser.png", "caption": "Figure 1: State-of-the-art visual tokenizers excel at either understanding (high zero-shot accuracy,\u00a0e.g. SigLIP\u00a0[96]) or reconstruction (low reconstruction FID,\u00a0e.g. MAGVIT2\u00a0[93]), but not both.\nQLIP can perform well on both understanding and reconstruction with a marginal performance drop, opening up an opportunity for unified multi-modal understanding and generation.", "description": "The figure illustrates the trade-off between zero-shot image classification accuracy and reconstruction quality (measured by FID) in state-of-the-art visual tokenizers.  Existing methods like SigLIP prioritize accuracy, achieving high scores in zero-shot tasks but poor reconstruction quality; methods like MAGVIT2 prioritize reconstruction, showing low FID but poor accuracy in zero-shot settings.  In contrast, QLIP achieves a balance between these two objectives, demonstrating strong performance in both zero-shot classification and image reconstruction, with only a minor decrease in performance compared to the best-performing models in each category.  This balance allows QLIP to be used in a unified multimodal framework for both understanding and generation tasks.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2502.05178/x1.png", "caption": "Figure 2: \nOverview.\n(a-b) Two-stage training pipeline of QLIP.\n(a) In Stage 1, we train QLIP with a combination of alignment loss and MSE loss.\n(b) In Stage 2, we drop the text encoder, freeze the visual encoder, and no longer optimize the contrastive loss.\nOnly the bottleneck quantizer and the decoder are fine-tuned.\n(c) With the text-aligned visual tokenizer, we transform the image into visual tokens, concatenate them with text tokens, and use an auto-regressive multi-modal model (Sec\u00a04.1) to model jointly.", "description": "Figure 2 illustrates the QLIP model training and its application. (a) and (b) detail the two-stage training process. Stage 1 involves simultaneous training of image-text alignment and image reconstruction, while stage 2 focuses solely on enhancing reconstruction quality by fine-tuning specific components after freezing the visual encoder. (c) shows how a text-aligned visual tokenizer processes images into visual tokens for integration with text tokens within a unified multimodal autoregressive model, enabling joint modeling of both modalities.", "section": "Quantized Language-Image Pre-training"}, {"figure_path": "https://arxiv.org/html/2502.05178/x2.png", "caption": "Figure 3: Memory usage of QLIP.", "description": "This figure shows a plot illustrating the GPU memory usage of the Quantized Language-Image Pretraining (QLIP) model during training.  The x-axis represents the batch size used per device, while the y-axis shows the peak GPU memory consumption in gigabytes. Two lines are shown, one for training QLIP without the Learned Perceptual Image Patch Similarity (LPIPS) and Generative Adversarial Network (GAN) losses, and another for training with these losses included. The plot demonstrates the significant increase in memory usage when incorporating LPIPS and GAN losses, highlighting a key challenge addressed by the two-stage training strategy employed in the QLIP method.", "section": "Two-stage training"}, {"figure_path": "https://arxiv.org/html/2502.05178/x3.png", "caption": "Figure 4: \nComparison of reconstruction results to the input image after the first and second stage.\nThe second-stage model produces more high-frequency details.\nThe figure is best viewed on a PDF viewer with zoom-in.", "description": "This figure displays a comparison of image reconstruction results at different stages of the QLIP training process.  The leftmost column shows the original input image. The middle column shows the reconstruction after the first training stage, which primarily focuses on contrastive language-image alignment and MSE loss for reconstruction. The rightmost column displays the reconstruction after the second training stage. In this stage, only the quantizer and decoder are fine-tuned, while the encoder is frozen. This second stage uses a weighted sum of MSE, perceptual, and GAN loss functions. The comparison highlights how the second stage leads to better reconstruction of high-frequency details, resulting in a more refined image. It's recommended to view this figure using a PDF viewer at a larger zoom level to fully appreciate the differences in detail.", "section": "4 Quantized Language-Image Pre-training"}, {"figure_path": "https://arxiv.org/html/2502.05178/x4.png", "caption": "Figure 5: \nComparison of gradient magnitude.\nHere, \ud835\udc98\ud835\udc98{\\bm{w}}bold_italic_w refers to the linear layer in the visual encoder\u2019s last MLP.", "description": "This figure compares the gradient magnitude for the alignment and reconstruction objectives during the training of QLIP. It visualizes the difference in the convergence rates between these objectives, highlighting the challenge of balancing them effectively. The gradient magnitudes are shown for a linear layer in the visual encoder's last Multilayer Perceptron (MLP), demonstrating the disparity in the training dynamics between the two objectives.", "section": "4 Quantized Language-Image Pre-training"}, {"figure_path": "https://arxiv.org/html/2502.05178/x5.png", "caption": "Figure 6: Comparison of generated images with conditioning captions in the bottom.\nFor each pair, the left is from LlamaGen+VQGAN and the right is from LlamaGen+QLIP-B/16 (ours).\nThe caption is also provided at the bottom.", "description": "Figure 6 presents a comparison of images generated using two different visual tokenizers in conjunction with the LlamaGen text-to-image generation model.  Each row shows an image pair; the left image is generated using LlamaGen with a VQGAN visual tokenizer, while the right image uses LlamaGen with the QLIP-B/16 tokenizer (the authors' model). The captions underneath each pair describe the image content.", "section": "5.3 Experiment Results on QLIP"}, {"figure_path": "https://arxiv.org/html/2502.05178/x6.png", "caption": "Figure 7: Comparison of generated images with conditioning captions in the bottom.\nFor each pair, the left is from LlamaGen+VQGAN and the right is from LlamaGen+QLIP-B/16 (ours).\nThe caption is also provided at the bottom.", "description": "Figure 7 presents a comparison of images generated using two different visual tokenizers within the LlamaGen framework.  Each row shows an image-caption pair. The left image in each pair was generated using LlamaGen with VQGAN as the visual tokenizer, while the right image was generated using LlamaGen with the QLIP-B/16 tokenizer (the authors' proposed method).  The captions accompanying each image are displayed below the image pair for context and comparison.", "section": "5.3 Experiment Results on QLIP"}]