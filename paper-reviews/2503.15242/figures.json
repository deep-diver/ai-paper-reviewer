[{"figure_path": "https://arxiv.org/html/2503.15242/extracted/6297511/figures/dataset_composition.png", "caption": "Figure 1: \nBigO(Bench) framework overview: Given a coding problem and human solutions, the framework evaluates language models on three key tasks: (1) predicting time-space complexities of existing solutions, (2) generating new code that meets specified complexity requirements, and (3) ranking solutions against human-written code with similar complexity profiles. The complexity framework automatically validates model outputs by computing runtime distributions and curve coefficients.", "description": "The figure illustrates the BigO(Bench) framework, which evaluates large language models (LLMs) on their ability to handle code complexity.  Given a coding problem and existing human-generated solutions, BigO(Bench) assesses the LLMs' performance in three areas: (1) predicting the time and space complexity of the solutions, (2) generating new code that satisfies specified complexity constraints, and (3) ranking solutions based on their complexity compared to human solutions. The framework uses profiling to automatically verify the LLMs' responses by analyzing runtime distributions and curve coefficients.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2503.15242/extracted/6297511/figures/benchmark_complexity.jpg", "caption": "Figure 2: \nOutline of the dynamic complexity inference framework. The framework takes a code snippet and a single example of inputs to this code snippet. Then, it processes the code snippet and proceeds with extensive inputs generation, based on the provided example of inputs: inputs are independently or interdependently increased in size, using several expansion methods that can be the identity or random, among else. This forms a queue of synthetic inputs on which to execute the provided code snippet. These executions happen independently in sandboxes, where runtime and memory footprint measures are taken. Once all the measures are collected, the framework can model the code snippet time and space dependencies to the different inputs. Using curve fitting, the time and space complexity of the code is computed on each input separately and then altogether. The global time and space complexity over all inputs is what is being returned.", "description": "The dynamic complexity inference framework analyzes code complexity.  It starts with a code snippet and sample inputs.  These inputs are systematically expanded using various methods (identity, random, etc.) creating a queue of test cases. Each test case runs in isolation within a sandboxed environment, capturing runtime and memory usage.  The collected data reveals the relationship between input size and resource consumption. Curve fitting techniques determine time and space complexity for each individual test and these are combined for a final, overall complexity assessment.", "section": "Dynamic Complexity Inference Framework"}, {"figure_path": "https://arxiv.org/html/2503.15242/x3.png", "caption": "Figure 3: \nDistribution of time-space complexity classes across BigO(Bench) dataset of 3,105 coding problems. Each problem is included when at least one solution exists with that specific time-space complexity pair.\nLinear time O(n) represents 38% of solutions, while constant space O(1) accounts for 25%.\nThe chart orders classes by computational efficiency, with less common classes grouped under \u201cother\u201d.\nProblems for which the framework can not infer a time complexity and/or a space complexity are not counted.", "description": "This figure visualizes the distribution of time and space complexity classes within the BigO(Bench) dataset, which comprises 3,105 coding problems.  Each problem is represented if at least one solution exists matching a specific time-space complexity pair. The visualization highlights the prevalence of certain complexities: linear time complexity, O(n), accounts for 38% of the solutions, whereas constant space complexity, O(1), represents 25%. Complexities are ordered from most to least computationally efficient, with less frequent complexities grouped under the 'other' category.  Problems where the complexity framework couldn't determine either time or space complexity are excluded from this analysis.", "section": "Benchmark Data Release"}, {"figure_path": "https://arxiv.org/html/2503.15242/extracted/6297511/figures/complexity_per_algorithmic_notion_time.png", "caption": "Figure 4: \nFailure rate analysis of the complexity inference framework. The top plot shows the overall distribution of framework failures across all problems. The bottom heatmap breaks down failure rates by input type and number of distinct inputs.\nApproximately 84% of problems have failure rates below 30%, demonstrating robust performance across most input configurations.", "description": "This figure analyzes the performance of the complexity inference framework. The top graph displays the distribution of problems with different failure rates, showing that 84% of problems have failure rates under 30%. The bottom heatmap provides a more detailed breakdown of failure rates based on the type and number of inputs used in each problem. This visualization highlights the framework's robustness across various input configurations.", "section": "Benchmark Data Release"}, {"figure_path": "https://arxiv.org/html/2503.15242/extracted/6297511/figures/complexity_per_algorithmic_notion_space.png", "caption": "Figure 5: LLM results aggregated by time complexity class and by algorithmic notions for all models part of BigO(Bench).", "description": "This figure visualizes the performance of various Large Language Models (LLMs) on code generation tasks categorized by time complexity and algorithmic approaches.  The x-axis represents different algorithmic categories used in the problems (e.g., graph algorithms, string manipulation, etc.). The y-axis represents the accuracy or success rate of the LLMs. Different colored bars represent the various LLMs included in the benchmark. The figure allows for a comparison of LLM performance across various problem types and complexities, highlighting strengths and weaknesses of different models in handling specific algorithmic challenges.", "section": "5 Evaluation"}, {"figure_path": "https://arxiv.org/html/2503.15242/extracted/6297511/figures/complexity_per_difficulty.png", "caption": "Figure 6: Model performance per coding benchmarks: HumanEval, MBPP and BigCodeBench main metrics are all P\u2062a\u2062s\u2062s\u2062@\u20621\ud835\udc43\ud835\udc4e\ud835\udc60\ud835\udc60@1Pass@1italic_P italic_a italic_s italic_s @ 1; for BigO(Bench), we display A\u2062l\u2062l\u2062@\u20621\ud835\udc34\ud835\udc59\ud835\udc59@1All@1italic_A italic_l italic_l @ 1 results.", "description": "Figure 6 compares the performance of various large language models (LLMs) across four different coding benchmarks: HumanEval, MBPP, BigCodeBench, and BigO(Bench).  HumanEval, MBPP, and BigCodeBench evaluate the models' ability to generate correct code solutions to programming problems, using the Pass@1 metric which measures the percentage of problems for which the model produces the correct solution on its first try.  In contrast, BigO(Bench) assesses not only the correctness but also the efficiency (time and space complexity) of the generated solutions.  The BigO(Bench) results utilize the All@1 metric, indicating whether the model successfully generates correct code solutions for all problems while adhering to specific time and space constraints.  The figure thus provides a comprehensive overview of LLM capabilities in terms of both code generation accuracy and algorithmic efficiency.", "section": "5 Evaluation"}]