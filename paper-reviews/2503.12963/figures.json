[{"figure_path": "https://arxiv.org/html/2503.12963/x1.png", "caption": "Figure 1: The proposed KDTalker: A keypoint-based spatiotemporal diffusion framework that generates synchronized, high-fidelity talking videos from audio and a single image, enhancing pose diversity and expression detail with realistic, temporally consistent animations.", "description": "KDTalker is a novel framework for generating high-fidelity talking videos from a single image and audio. It uses a keypoint-based spatiotemporal diffusion model that produces realistic and temporally consistent animations.  The model combines implicit 3D keypoints to model diverse head poses and fine facial details, enhancing both pose diversity and expression detail, with a custom designed spatiotemporal attention mechanism for accurate lip synchronization. ", "section": "3 Method"}, {"figure_path": "https://arxiv.org/html/2503.12963/extracted/6286149/fig/Inference_Time_vs_Head_Diversity_LSE-D.png", "caption": "Figure 2: Inference Time vs Head Diversity &\\&& LSE-D. The value of LSE-D (Lip Sync Error Distance), a metric quantifying the alignment between lip movements and audio, is represented by the size of the circle. A smaller circle indicates a lower LSE-D value, reflecting better lip sync performance.", "description": "This figure shows a comparison of different talking portrait generation methods across three metrics: inference time (frames per second), head pose diversity, and lip synchronization accuracy (measured by Lip Sync Error Distance, or LSE-D).  Each method is represented by a point on the graph, with its horizontal position determined by inference time and its vertical position by head pose diversity. The size of the point corresponds to the LSE-D score; smaller points indicate better lip synchronization. This visualization allows for a direct comparison of the trade-offs between speed, diversity of head movements, and the accuracy of lip movements in relation to the audio.", "section": "4 Experiments"}, {"figure_path": "https://arxiv.org/html/2503.12963/x2.png", "caption": "Figure 3: Overview of the proposed KDTalker for talking portrait synthesis.", "description": "The figure shows a detailed overview of the KDTalker framework architecture.  It illustrates the process of generating talking portrait videos using a combination of audio and image inputs.  The input audio is processed by an encoder to extract audio features, while the reference image undergoes extraction of motion and appearance features. These features are then integrated with a noise sequence in the 'Reference-Guided Priors' module, which provides input to the core 'Keypoint-based Spatiotemporal Diffusion' module. This module predicts implicit 3D keypoints and transformation parameters (scaling, rotation, and translation), which are then used in the 'Face Render' module to generate the final talking head video.", "section": "3 Method"}, {"figure_path": "https://arxiv.org/html/2503.12963/x3.png", "caption": "Figure 4: Reference-Guided Priors.", "description": "The Reference-Guided Priors module combines motion information from two key reference frames with a noise sequence to guide the diffusion model.  The canonical keypoints extracted from the reference image are used as input for the first frame, with zero-padding applied to other parameters. This provides prior information on facial structure, ensuring that canonical keypoints serve as a foundation for subsequent frames.  The expression deformation aligns with the canonical keypoints (xe), while other parameters are initialized with zero-padding. These priors are then integrated with a noise sequence to create the input for the diffusion model. By initializing the process with reference frames, the model conditions the noise sequence on structured motion information. This enables temporally coherent motion generation aligned with the original facial structure and adapting to audio-driven dynamics.", "section": "3.3 Reference-Guided Priors"}, {"figure_path": "https://arxiv.org/html/2503.12963/x4.png", "caption": "Figure 5: Spatiotemporal-Aware Attention Network.", "description": "This figure illustrates the architecture of the Spatiotemporal-Aware Attention Network, a key component of the KDTalker framework.  The network takes as input the timestep (which indicates the stage of the denoising process), audio features, and the output from the ResNet blocks.  These inputs are processed through several modules including a Time MLP for temporal embedding, rotary embedding, and a softmax attention mechanism. The final output is a set of refined motion parameters that incorporate both spatial and temporal consistency.", "section": "3.4 Keypoint-based Spatiotemporal Diffusion"}, {"figure_path": "https://arxiv.org/html/2503.12963/x5.png", "caption": "Figure 6: Qualitative comparison with the state-of-the-art methods on HDTF dataset.", "description": "Figure 6 presents a qualitative comparison of talking head videos generated by different state-of-the-art methods and the proposed method (KDTalker) using the HDTF dataset.  For each method, several frames from a generated video are shown alongside the corresponding ground truth frames. This visual comparison allows for a direct assessment of the realism, accuracy of lip synchronization, and overall quality of the generated videos. The figure highlights KDTalker's ability to generate high-fidelity animations that closely match the natural head movements and lip synchronization of the ground truth videos.", "section": "4 Experiments"}, {"figure_path": "https://arxiv.org/html/2503.12963/x6.png", "caption": "Figure 7: Qualitative comparison of head motion diversity between existing methods and our proposed approach.", "description": "Figure 7 presents a qualitative comparison of head motion diversity between several existing audio-driven talking portrait generation methods and the proposed KDTalker approach. It uses motion heatmaps to visualize the diversity of head movements generated by each method, including SadTalker, Real3DPortrait, AniTalker, AniPortrait, and the proposed method. The heatmaps provide a visual representation of the range and variation of head poses generated, allowing for a direct comparison of the dynamic head motion generated by each method. The ground truth (GT) heatmap is also shown for reference.", "section": "4.3 User Studies"}]