{"importance": "This paper is important because it presents a novel approach to solving a significant challenge in the field of computer architecture: the efficient and accurate translation of assembly code between different instruction set architectures (ISAs).  This is especially relevant given the increasing industry adoption of ARM-based systems and the need to migrate legacy x86 software.  The method developed, a language-model based approach, presents a unique solution that overcomes limitations of current virtualization approaches while providing valuable insights for researchers in machine learning and compiler optimization.", "summary": "A novel LLM-based transpiler, CRT, efficiently converts x86 assembly to ARM and RISC-V assembly, achieving high accuracy and significant performance improvements over existing virtualization methods.", "takeaways": ["CRT, a novel lightweight LLM-based transpiler, directly converts x86 assembly code to ARM and RISC-V assembly code.", "CRT achieves high translation accuracy (79.25% for ARMv5, 88.68% for RISC-V), outperforming existing methods.", "CRT demonstrates significant performance gains (1.73x speedup, 1.47x energy efficiency, 2.41x memory efficiency) compared to Apple's Rosetta 2 on Apple M2 hardware."], "tldr": "The shift towards ARM architecture presents a significant challenge due to the extensive legacy x86 software ecosystem and the lack of efficient, accurate translation methods between different instruction set architectures (ISAs). Existing virtualization methods introduce performance overhead, and direct recompilation is not always feasible. This paper introduces a novel solution to this problem: a lightweight language model (LLM)-based transpiler called CRT.\n\nCRT utilizes the power of LLMs to learn the mapping between x86 and ARM/RISC-V assembly instructions. The model is trained on a large dataset of paired assembly codes and then used to automatically translate x86 code to ARM/RISC-V equivalents. **CRT achieves high translation accuracy (79.25% for ARMv5 and 88.68% for RISC-V) and demonstrates significant performance improvements (1.73x speedup, 1.47x energy efficiency, and 2.41x memory efficiency) compared to existing methods.**  The study also evaluates the impact of various model parameters and quantization techniques on the transpiler's efficiency and performance. **The research successfully navigates the CISC/RISC divide, generating correct and efficient RISC code and demonstrating potential for significant improvements in software migration and compatibility.**", "affiliation": "Mohamed bin Zayed University of Artificial Intelligence", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}, "podcast_path": "2411.16341/podcast.wav"}