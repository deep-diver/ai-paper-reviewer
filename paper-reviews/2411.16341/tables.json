[{"content": "| Input | Tokenizer | DeepSeek/Yi-Coder | Our Extended Tokenizer |\n|---|---|---|---| \n| `ldr r1, r2` | Tokens | `ld<font color=\"#FFD9D9\">r</font><font color=\"#FFFFD9\">1</font><font color=\"#D9FFD9\">\u2423</font><font color=\"#D9D9FF\">r</font><font color=\"#FFFFD9\">2</font><font color=\"#FFECD9\">,</font><font color=\"#F5D9E2\">\u2423</font>` | `ldr<font color=\"#D9FFD9\">\u2423</font><font color=\"#D9D9FF\">r1</font><font color=\"#FFECD9\">,</font><font color=\"#F5D9E2\">\u2423</font><font color=\"#D9FFFF\">r2</font>` |", "caption": "Table 1: Comparison of tokenization approaches between DeepSeek/Yi-Coder and our extended tokenizer. Spaces are represented as \u2423 and shown with colored backgrounds to highlight token boundaries. Note how our tokenizer groups related tokens (e.g., ldr and r1) as singular units.", "description": "This table compares the tokenization methods used by the pre-trained language models DeepSeek and Yi-Coder with the extended tokenizer developed by the authors.  The extended tokenizer is designed to improve the accuracy of assembly language translation by treating related tokens (such as instruction mnemonics and registers) as single units rather than splitting them, thus enhancing the model's understanding of the low-level semantics of assembly language instructions. The table uses visual cues, such as colored backgrounds and the symbol \u2423 to represent spaces, to highlight the differences in how each tokenizer processes and breaks down a simple assembly instruction into constituent tokens.", "section": "3.3 Tokenizer Extension"}, {"content": "| ld r \u2423 r 1 , \u2423 r 2 |", "caption": "Table 2: Comparison of models\u2019 performance on the x86 to ARM transpilation task, measured by Edit Distance, Exact Match, and Test Accuracy. The top portion lists pre-existing models, while the bottom portion lists models trained by us. Arrows\u00a0(\u2191\u2191\\uparrow\u2191,\u00a0\u2193\u2193\\downarrow\u2193) indicate whether higher or lower values are better for each metric. The best results are highlighted in bold.", "description": "This table compares the performance of various large language models (LLMs) on the task of translating x86 assembly code to ARM assembly code.  The models are evaluated using three metrics: Edit Distance (lower is better, measuring the difference between the generated ARM code and the ground truth), Exact Match (higher is better, indicating whether the generated code perfectly matches the ground truth), and Test Accuracy (higher is better, reflecting the percentage of test cases passed by the generated code). The top half of the table shows results for pre-trained, publicly available LLMs, while the bottom half displays results for models fine-tuned by the authors of the paper.  The arrows in the caption indicate whether a higher or lower value is preferable for each metric. The best performing model for each metric is shown in bold.", "section": "4 Experiments and Evaluation"}, {"content": "| ldr r1, r2 |", "caption": "Table 3: Comparison of models\u2019 performance on the x86 to RISCv64 transpilation task.", "description": "This table presents a comparison of different large language models (LLMs) on their performance in translating x86 assembly code to RISC-V64 assembly code.  The metrics used for comparison include average edit distance (a measure of the syntactic similarity between the generated and ground truth assembly code), exact match (the percentage of translations that are exactly identical to the ground truth), and test accuracy (the percentage of translations that pass all the associated test cases, indicating functional correctness).  Lower edit distance and higher exact match and test accuracy scores indicate better performance.", "section": "5 Results"}, {"content": "| Model | Average Edit Distance (\u2193) | Exact Match (\u2191) | Test Accuracy (\u2191) |\n|---|---|---|---|\n| GPT4o [OpenAI (2024)](https://arxiv.org/html/2411.16341v1#bib.bib38) | 1296 | 0% | 8.18% |\n| DeepSeekCoder2-16B [Zhu et al. (2024)](https://arxiv.org/html/2411.16341v1#bib.bib53) | 1633 | 0% | 7.36% |\n| Yi-Coder-9B [01.AI (2024)](https://arxiv.org/html/2411.16341v1#bib.bib1) | 1653 | 0% | 6.33% |\n| Yi-coder-1.5B | 275 | 16.98% | 49.69% |\n| DeepSeekCoder-1.3B | 107 | 45.91% | 77.23% |\n| DeepSeekCoder-1.3B-xTokenizer-int4 | 119 | 46.54% | 72.96% |\n| DeepSeekCoder-1.3B-xTokenizer-int8 | 96 | 49.69% | 75.47% |\n| DeepSeekCoder-1.3B-xTokenizer | 165 | 50.32% | 79.25% |", "caption": "Table 4: Correctness comparison between CRT implementations on ARMv5 and ARMv8 architectures. Metrics include Average Edit Distance (AED), Exact Match (EM), and Test Accuracy (Acc.).", "description": "This table presents a comparison of the performance of the CRT (CISC to RISC transpiler) model on two different ARM architectures: ARMv5 and ARMv8.  The comparison is made using three key metrics: Average Edit Distance (AED), Exact Match (EM), and Test Accuracy (Acc.).  AED quantifies the difference between the generated ARM assembly code and the ground truth, with a lower score indicating better similarity. EM indicates the percentage of test cases where the generated code produced the exact same output as the ground truth.  Test Accuracy shows the overall functional correctness, representing the fraction of test cases where the generated code successfully passed all tests. This comparison reveals the model's performance consistency and its behavior when handling varying levels of architectural complexity within the ARM ISA.", "section": "7 Case Study"}, {"content": "| Model | Average Edit Distance (\u2193) | Exact Match (\u2191) | Test Accuracy (\u2191) |\n|---|---|---|---|\n| GPT4o [2024] | 1293 | 0% | 7.55% |\n| DeepSeekCoder2-16B [2024] | 1483 | 0% | 6.29% |\n| DeepSeekCoder-1.3B-xTokenizer-int4 | 112 | 14.47% | 68.55% |\n| DeepSeekCoder-1.3B-xTokenizer-int8 | 31 | 69.81% | 88.05% |\n| DeepSeekCoder-1.3B-xTokenizer | 27 | 69.81% | 88.68% |", "caption": "Table 5: Simple Variation Patterns in Functionally Equivalent Code", "description": "This table presents examples of functionally equivalent assembly code snippets that exhibit syntactic variations, despite having the same logical outcome.  It showcases instances where seemingly minor differences in register allocation, operand ordering (for commutative operations), and stack variable locations do not affect the program's functionality. The goal is to illustrate that superficial dissimilarities between the ground-truth assembly code and the model's generated code do not always indicate errors, as semantically equivalent implementations can vary.", "section": "A.1 Analysis of Assembly Code Variations"}, {"content": "| Model | AED (<img src=\"https://arxiv.org/html/2411.16341/equation_down.png\" alt=\"\u2193\"/>) | EM (<img src=\"https://arxiv.org/html/2411.16341/equation_up.png\" alt=\"\u2191\"/>) | Acc. (<img src=\"https://arxiv.org/html/2411.16341/equation_up.png\" alt=\"\u2191\"/>) |\n|---|---|---|---| \n| CRT ARMv5 | 165 | 50.32% | 79.25% |\n| CRT ARMv8 | 105 | 50.61% | 75.0% |", "caption": "Table 6: Complex Variation Patterns with Multiple Differences", "description": "This table details instances where functionally equivalent assembly code exhibits multiple types of differences between the ground truth and the model's output. These differences include variations in operand order for commutative operations, register selection, memory locations, instruction combinations, and constant handling.  The table shows how these variations can appear in combination to produce functionally correct but syntactically different assembly code.", "section": "A.1 Analysis of Assembly Code Variations"}, {"content": "| Prog ID | Edit Dist | Example |\n|---|---|---|\n| P75 | 8 | Operands in arithmetic operations can be reordered if operation is commutative |\n|  |  | Ground truth: `add` `r1`, `r2`, `r3` |\n|  |  | Predicted: `add` `r1`, `r3`, `r2` |\n| P108 | 16 | Different registers can be chosen for temporary values while maintaining same data flow |\n|  |  | Ground truth: `mov` `r2`, `r0`; `add` `r2`, `r2`, `#1` |\n|  |  | Predicted: `mov` `r3`, `r0`; `add` `r3`, `r3`, `#1` |\n| P8 | 12 | Local variables can be stored at different stack locations while maintaining correct access patterns |\n|  |  | Ground truth: `str` `r1`, `[fp, #-8]`; `str` `r2`, `[fp, #-12]` |\n|  |  | Predicted: `str` `r1`, `[fp, #-12]`; `str` `r2`, `[fp, #-8]` |\n| P119 | 6 | Compiler-generated symbol names can differ while referring to same data |\n|  |  | Ground truth: `.word` `out.4781` |\n|  |  | Predicted: `.word` `out.4280` |\n| P135 | 12 | Multiple instructions can be combined into single equivalent instruction |\n|  |  | Ground truth: `mov` `r3`, `r0`; |\n|  |  | \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0`str` `r3`, `[fp, #-8]` |\n|  |  | Predicted: `str` `r0`, `[fp, #-8]` |\n| P162 | 4 | Stack frame offsets can vary while maintaining correct variable access |\n|  |  | Ground truth: `strb` `r3`, `[fp, #-21]` |\n|  |  | Predicted: `strb` `r3`, `[fp, #-17]` |\n| P88 | 23 | Memory allocation sizes can vary if sufficient for program needs |\n|  |  | Ground truth: `mov` `r0`, `#400` |\n|  |  | Predicted: `mov` `r0`, `#800` |\n| P103 | 52 | Different instruction sequences can achieve same logical result |\n|  |  | Ground truth: `cmp` `r3`, `#0`; `and` `r3`, `r3`, `#1`; `rsblt` `r3`, `r3`, `#0` |\n|  |  | Predicted: `rsbs` `r2`, `r3`, `#0`; `and` `r3`, `r3`, `#1`; `and` `r2`, `r2`, `#1`; `rsbpl` `r3`, `r2`, `#0` |\n| P69 | 50 | Constants can be loaded directly or from literal pool |\n|  |  | Ground truth: `mvn` `r3`, `#-2147483648` |\n|  |  | Predicted: `ldr` `r3`, `.L8`; `.L8:` `.word` `2147483647` |", "caption": "Table 7: Analysis of Critical Errors with Small Edit Distances", "description": "This table presents a detailed analysis of critical errors in assembly code transpilation where the differences between the ground truth and the predicted assembly code are minimal (small edit distances). It focuses on cases where the errors cause the transpiled code to fail functional tests, providing insights into how subtle differences can lead to significant problems.  For each example, the table shows the program ID (Prog ID), the edit distance (Edit Dist), and the specific error type observed with illustrative code snippets from both the ground truth and the incorrectly transpiled assembly.", "section": "A.1 Analysis of Assembly Code Variations"}]