{"references": [{"fullname_first_author": "M. Chen", "paper_title": "Evaluating large language models trained on code", "publication_date": "2021-07-07", "reason": "This paper introduces the HumanEval benchmark, used for evaluating the model's performance in the paper."}, {"fullname_first_author": "A. F. Da Silva", "paper_title": "Anghabench: A suite with one million compilable C benchmarks for code-size reduction", "publication_date": "2021-00-00", "reason": "This paper provides the AnghaBench dataset, which is the primary training dataset used in the paper."}, {"fullname_first_author": "M. -A. Lachaux", "paper_title": "Unsupervised translation of programming languages", "publication_date": "2020-06-06", "reason": "This paper explores neural machine translation for high-level languages which is related to the paper's approach for low-level languages."}, {"fullname_first_author": "C. Lee", "paper_title": "GUESS & SKETCH: Language model guided transpilation", "publication_date": "2024-00-00", "reason": "This paper is the closest related work using LLMs for assembly-level translation, which the paper improves upon."}, {"fullname_first_author": "H. Tan", "paper_title": "LLM4Decompile: Decompiling binary code with large language models", "publication_date": "2024-00-00", "reason": "This paper introduces LLM4Decompile, used in the paper for evaluation of the model's performance and providing data"}]}