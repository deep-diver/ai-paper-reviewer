{"references": [{" publication_date": "2024", "fullname_first_author": "Randall Balestriero", "paper_title": "How learning by reconstruction produces uninformative features for perception", "reason": "This paper is highly relevant because it directly addresses the core limitation that BiGR aims to overcome. It reveals that reconstruction-based learning, while successful in generating visually appealing images, often produces weak feature representations unsuitable for downstream tasks. This directly supports BiGR's motivation to build a model that excels in both generation and representation learning by employing binary latent codes and a unified framework. The findings serve as a strong foundation for BiGR's design and its expected performance improvement.", "section_number": 1}, {" publication_date": "2020", "fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "reason": "This seminal paper is crucial because BiGR adopts the masked language modeling approach similar to the transformer-based language models in this paper, which demonstrates the capability of large language models to handle tasks with minimal task-specific modifications. BiGR leverages this approach to create a unified framework for both generative and discriminative tasks, leading to improved performance and flexibility.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Hangbo Bao", "paper_title": "BEIT: BERT pre-training of image transformers", "reason": "This paper is highly relevant because it introduces masked image modeling, which is a critical component of BiGR's architecture.  BiGR uses a masked modeling approach to learn from images and predict masked tokens, drawing inspiration from the success of masked modeling in language models, as highlighted in this paper. The techniques and concepts from this method directly influenced the development of BiGR's unified generative and discriminative framework.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Patrick Esser", "paper_title": "Taming transformers for high-resolution image synthesis", "reason": "This paper introduces the transformer architecture for image generation, which is closely related to the underlying architecture employed by BiGR. The authors' utilization of transformers in image generation serves as foundational work for BiGR's adoption of the decoder-only transformer backbone for its image generation and representation learning tasks. The experience gained from this work has significantly influenced BiGR's design and its potential.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "Jean-Bastien Grill", "paper_title": "Bootstrap your own latent - a new approach to self-supervised learning", "reason": "This paper is relevant because it explores self-supervised learning techniques for visual representation learning, which is a related area to BiGR's goal of improving visual representation capabilities. The concepts and techniques used for self-supervised learning provide valuable insights into the design of efficient and effective representation learning methods, which have significantly influenced BiGR's development.  It explores novel approaches for efficient self-supervised training that BiGR adapts and extends in its specific task.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Huiwen Chang", "paper_title": "MaskGIT: Masked generative image transformer", "reason": "This paper is directly relevant because it introduces a masked generative transformer, a key component of BiGR's design and approach.  The masked modeling approach and its application in image generation, as presented in this paper, directly informed BiGR's architecture and training strategy.  The similarities in the model architecture and training methodology make this paper highly relevant to the development and understanding of BiGR.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Huiwen Chang", "paper_title": "Muse: Text-to-image generation via masked generative transformers", "reason": "This paper introduces a masked generative transformer model for text-to-image generation, which is relevant to BiGR because both models leverage a masked modeling approach and transformer architecture. BiGR shares several key architectural similarities with this work and further extends upon these ideas by unifying both generative and discriminative tasks within the same framework using binary latent codes, leading to improved overall performance.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "Kaiming He", "paper_title": "Momentum contrast for unsupervised visual representation learning", "reason": "This paper focuses on unsupervised visual representation learning, which is a closely related area to BiGR's goal of enhancing visual representation capabilities.  The techniques and insights gained from unsupervised representation learning, as detailed in this work, have influenced BiGR's design and its approach to improving feature representations without relying on explicit discriminative losses during training.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Kaiming He", "paper_title": "Masked autoencoders are scalable vision learners", "reason": "This paper introduced the Masked Autoencoder (MAE) framework, a method that strongly influences BiGR's design.  BiGR adopts a masked modeling approach, similar to MAE, to train the model, predicting masked tokens to improve the model's learning of image features.  The success of MAE in representation learning and its application to visual tasks directly informed BiGR's architecture and helped achieve its unified generative and discriminative capabilities.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "Jonathan Ho", "paper_title": "Denoising diffusion probabilistic models", "reason": "This paper introduces denoising diffusion models, a significant advancement in the field of image generation that has influenced many subsequent models.  BiGR, while employing a different approach (masked modeling on binary latent codes), draws inspiration from the advancements made in diffusion models in achieving high-quality image generation, incorporating key lessons learned regarding efficient sampling and training strategies.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Jiaming Song", "paper_title": "Denoising diffusion implicit models", "reason": "This paper is relevant as it advances diffusion models for image generation, which are closely related to BiGR's approach despite BiGR using a different technique (masked modeling).  Advancements in diffusion models offer insights into efficient sampling strategies and the use of noise for generative tasks, knowledge which has implicitly influenced BiGR's design and approach to improving both generation quality and sampling efficiency.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Robin Rombach", "paper_title": "High-resolution image synthesis with latent diffusion models", "reason": "This paper introduced a diffusion model that can generate high-resolution images. While BiGR uses a different model architecture, the focus on high-resolution image generation and the need for efficient generation processes are important aspects of the research landscape that have influenced the design choices and goals of BiGR.  The success of this model in generating high-resolution images has prompted the development of more efficient techniques in the field.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Peize Sun", "paper_title": "Autoregressive model beats diffusion: Llama for scalable image generation", "reason": "This paper is highly relevant because it provides a state-of-the-art conditional image generation model (LlamaGen) that BiGR is directly compared against in experiments. LlamaGen's performance serves as a crucial benchmark against which BiGR's results are evaluated.  The comparison highlights BiGR's advancements in terms of FID score, representation learning capability, and efficient inference speed.  This comparison allows for a robust evaluation of BiGR's performance.", "section_number": 4}, {" publication_date": "2022", "fullname_first_author": "Jonathan Ho", "paper_title": "Classifier-free diffusion guidance", "reason": "This paper introduces classifier-free guidance, a widely used technique in diffusion models to enhance generation quality and control.  BiGR, while not directly a diffusion model, incorporates the principles of classifier-free guidance in its sampling process using the entropy-ordered sampling method. This method guides the generation process without explicit classifiers, thus improving overall performance and efficiency.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Tianhong Li", "paper_title": "Masked generative encoder to unify representation learning and image synthesis", "reason": "This paper is relevant because it introduces MAGE, a model that attempts to unify representation learning and image generation, similar to BiGR's goal.  Comparing BiGR with MAGE helps understand the relative strengths and weaknesses of different approaches to unify these two tasks.  While both aim for a unified framework, BiGR achieves this through a different approach using binary latent codes and a masked modeling strategy, resulting in a more efficient model.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Tianhong Li", "paper_title": "Autoregressive image generation without vector quantization", "reason": "This paper presents a novel autoregressive image generation method. While BiGR uses a masked modeling approach, this paper is highly relevant because it explores efficient autoregressive models that inspire BiGR's approach to efficient sampling. BiGR's entropy-ordered sampling strategy is directly influenced by such advancements to enhance the model's efficiency and speed.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Ze Wang", "paper_title": "Binary latent diffusion", "reason": "This paper is directly relevant because it introduces binary latent diffusion, which is a core component of the binary transcoder in BiGR.  BiGR adapts and extends this approach to unify both image generation and representation learning within a single framework. This paper's contribution is essential in understanding the mechanism used in BiGR to convert continuous features into binary codes and to achieve efficient training and sampling.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Michael Tschannen", "paper_title": "GIVT: Generative infinite-vocabulary transformers", "reason": "This paper explores a continuous valued tokenizer for image generation, which is closely related to BiGR's use of binary tokenizers. The techniques used in this paper for handling token embeddings and their influence on the generative process are relevant to the design of BiGR's input projection module and the overall generation process. This paper's contribution assists in better understanding and designing BiGR's efficient input representation.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Abhimanyu Dubey", "paper_title": "The llama 3 herd of models", "reason": "This paper introduces a new generation of large language models which are closely related to the architecture of BiGR.  BiGR's use of a transformer-based model as its backbone was directly influenced by the progress and advancement in large language models. This paper's contribution provides valuable insights into the design and training of robust and efficient large language models, knowledge that has significantly influenced the development and performance of BiGR.", "section_number": 3}]}