{"references": [{" publication_date": "2024", "fullname_first_author": "Randall Balestriero", "paper_title": "How learning by reconstruction produces uninformative features for perception", "reason": "This paper is crucial because it directly addresses the core motivation behind BiGR.  It empirically demonstrates that reconstruction-based learning, a common technique in image generation, often fails to produce strong latent representations for perception tasks. This finding directly supports the claim that current image generation models lack strong feature extraction capabilities, thereby establishing the need for a model like BiGR that addresses this limitation.", "section_number": 1}, {" publication_date": "2020", "fullname_first_author": "Jonathan Ho", "paper_title": "Denoising diffusion probabilistic models", "reason": "This foundational paper introduces denoising diffusion probabilistic models (DDPMs), a key technique heavily influencing many current image generation models.  BiGR utilizes similar concepts, building upon the advancements made in DDPMs to achieve high-quality image generation.  Understanding DDPMs is essential for grasping the technical underpinnings of BiGR and its contributions to the field.", "section_number": 1}, {" publication_date": "2021", "fullname_first_author": "Patrick Esser", "paper_title": "Taming transformers for high-resolution image synthesis", "reason": "This work is highly relevant as it explores the use of transformers, a powerful architecture, for image generation, similar to BiGR's approach.  The techniques and insights presented in this paper inform BiGR's design and implementation, making it a crucial foundational work for understanding the context of BiGR.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Robin Rombach", "paper_title": "High-resolution image synthesis with latent diffusion models", "reason": "This paper presents significant advancements in diffusion models, particularly in achieving high-resolution image synthesis.  BiGR's focus on high-quality image generation directly benefits from the progress in this area.  The authors build upon the methodologies introduced in this work and other diffusion-based models, improving the generation quality of the BiGR model.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Peize Sun", "paper_title": "Autoregressive model beats diffusion: Llama for scalable image generation", "reason": "This is a direct comparison paper to BiGR. It presents the state-of-the-art class-conditional image generation model, LlamaGen, which serves as a key baseline for evaluating BiGR's performance. The findings of this paper directly inform the evaluation and justification of BiGR's improvements and contributions to the field.", "section_number": 1}, {" publication_date": "2017", "fullname_first_author": "Aaron Van Den Oord", "paper_title": "Neural discrete representation learning", "reason": "This paper introduces the concept of neural discrete representation learning, which is highly relevant to BiGR's approach of using binary latent codes.  The ideas and techniques from this paper provide a theoretical foundation for BiGR's use of compact binary codes and inform its design choices related to representation learning.", "section_number": 2}, {" publication_date": "2019", "fullname_first_author": "Fatih Cakir", "paper_title": "Hashing with mutual information", "reason": "This research delves into the use of hashing techniques for visual representation. BiGR leverages the advantages of compact binary latent codes, and this work explores the related area of hashing, offering further insights into the effectiveness of binary codes for visual representations.  The use of hashing techniques helps to make the model efficient.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "Kaiming He", "paper_title": "Momentum contrast for unsupervised visual representation learning", "reason": "This highly influential paper introduces a self-supervised learning approach for visual representation learning, which is closely related to BiGR's focus on generative representation capabilities.  The advancements and insights from this method help to establish the context of BiGR's contribution to the field of generative representation learning.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Huiwen Chang", "paper_title": "MaskGIT: Masked generative image transformer", "reason": "This paper introduces the masked generative image transformer (MaskGIT), a model that leverages masked modeling for image generation.  BiGR adopts a similar masked modeling approach but expands on it by using compact binary latent codes to achieve better performance.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Kaiming He", "paper_title": "Masked autoencoders are scalable vision learners", "reason": "This paper introduces masked autoencoders (MAE), a prominent self-supervised method for vision.  BiGR leverages the ideas of masked modeling, adapting the approach to its conditional image generation framework.  The use of masked modeling enables both efficient generative and discriminative tasks.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Tianhong Li", "paper_title": "Masked generative encoder to unify representation learning and image synthesis", "reason": "This paper proposes a masked generative encoder to improve both generation and representation learning, providing a basis for comparison with BiGR.  Both models utilize masked modeling, but BiGR leverages compact binary latent codes to enhance both generation and representation quality, exceeding the performance of MAGE.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Mathilde Caron", "paper_title": "Emerging properties in self-supervised vision transformers", "reason": "This work explores the use of self-supervised learning methods with vision transformers, which is closely related to BiGR\u2019s approach. This provides a broader context for the method and highlights the advances made in self-supervised learning for vision applications.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "Mark Chen", "paper_title": "Generative pretraining from pixels", "reason": "This is a seminal work in generative modeling that introduces a method for generative pre-training from pixels. BiGR builds upon the concepts of generative pre-training, adapting them for conditional generation with improved visual representation capabilities. The advancements of this approach are integrated into the BiGR framework for improved performance.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Junsong Chen", "paper_title": "Pixart-\u03b1: Fast training of diffusion transformer for photorealistic text-to-image synthesis", "reason": "This recent work is highly relevant because it focuses on improving the efficiency and quality of diffusion-based image synthesis. BiGR\u2019s efficiency in image generation is a key selling point, and this paper helps to contextualize BiGR's contribution within the broader landscape of efficient and high-quality image generation.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Ze Wang", "paper_title": "Binary latent diffusion", "reason": "This is a highly relevant paper that introduces the concept of binary latent diffusion for image generation. BiGR directly builds upon this work, adopting the Bernoulli diffusion process for predicting binary codes. The methodologies introduced in this paper are essential components of BiGR's architecture.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Abhimanyu Dubey", "paper_title": "The llama 3 herd of models", "reason": "This paper introduces the Llama language model architecture, which is the foundation for BiGR.  The design choices and underlying architecture of Llama are critically important to understanding BiGR's functionality and performance. The Llama architecture is modified to accommodate the masked token prediction and image generation tasks.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Hugo Touvron", "paper_title": "Llama: Open and efficient foundation language models", "reason": "This paper introduces the Llama language model, which serves as the backbone architecture for BiGR.  The properties and capabilities of Llama are directly relevant to understanding BiGR's design choices and performance. The bidirectional attention modification is introduced for efficient training and inference.", "section_number": 3}, {" publication_date": "2022", "fullname_first_author": "Jonathan Ho", "paper_title": "Classifier-free diffusion guidance", "reason": "This paper introduces classifier-free diffusion guidance, a technique for improving the quality and control of diffusion-based image generation models. BiGR leverages similar ideas of controlling the generative process, albeit through different means. The concepts from classifier-free guidance provide a valuable comparison point when analyzing BiGR's control and quality of image generation.", "section_number": 4}, {" publication_date": "2022", "fullname_first_author": "Hangbo Bao", "paper_title": "BEIT: BERT pre-training of image transformers", "reason": "This paper explores the use of BERT-style pre-training techniques for image transformers.  BiGR shares similarities in leveraging masked modeling, which is inspired by BERT's masked language modeling.   Understanding the concepts and benefits of masked modeling in language models helps to contextualize BiGR's use of this technique for images.", "section_number": 4}]}