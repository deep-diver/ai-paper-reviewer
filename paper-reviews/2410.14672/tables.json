[{"figure_path": "2410.14672/tables/table_7_0.html", "caption": "Table 1: Uniformity comparison. We compare the generative and discriminative performance of our model against LlamaGen (Sun et al., 2024) and three other settings, varying by tokenizers, training objectives, and modeling types.", "description": "Table 1 compares the generative and discriminative performance of BiGR against LlamaGen and other models with varying tokenizers, objectives, and modeling types.", "section": "4.2 UNIFORM PERFORMANCE"}, {"figure_path": "2410.14672/tables/table_7_1.html", "caption": "Table 2: Binary transcoder comparison.", "description": "Table 2 compares the generative and discriminative performance of BiGR using different binary transcoder methods.", "section": "4.3 MODEL ANALYSIS"}, {"figure_path": "2410.14672/tables/table_7_2.html", "caption": "Table 3: Sampling order comparison. We include the autoregressive variant for reference.", "description": "The table compares the generative and discriminative performance of different sampling order strategies for the BiGR model.", "section": "4.3 MODEL ANALYSIS"}, {"figure_path": "2410.14672/tables/table_9_0.html", "caption": "Table 4: Linear-probe evaluation of conditional and unconditional counterparts.", "description": "Table 4 presents a comparison of the linear-probe top-1 accuracy on ImageNet-1k for conditional and unconditional training models.", "section": "4.3 MODEL ANALYSIS"}, {"figure_path": "2410.14672/tables/table_9_1.html", "caption": "Table 1: Uniformity comparison. We compare the generative and discriminative performance of our model against LlamaGen (Sun et al., 2024) and three other settings, varying by tokenizers, training objectives, and modeling types.", "description": "Table 1 compares the generative and discriminative performance of BiGR against LlamaGen and other models with varying tokenizers, training objectives, and modeling types.", "section": "4.2 UNIFORM PERFORMANCE"}, {"figure_path": "2410.14672/tables/table_14_0.html", "caption": "Table 7: The default configuration settings of three models: BiGR-L, BiGR-XL, BiGR-XXL.", "description": "Table 7 shows the default configuration settings for the model architecture, training and inference of BiGR across different model sizes.", "section": "A ADDITIONAL IMPLEMENTATION DETAILS"}, {"figure_path": "2410.14672/tables/table_14_1.html", "caption": "Table 8: Comparison of deterministic and non-deterministic sampling.", "description": "The table compares the performance of deterministic and non-deterministic sampling methods in terms of FID, IS, sFID, precision, and recall.", "section": "4.3 MODEL ANALYSIS"}, {"figure_path": "2410.14672/tables/table_15_0.html", "caption": "Table 9: Model comparison of generative performance on ImageNet-1K. Metrics include Frechet inception distance (FID), inception score (IS), precision (Pre.) and recall (Rec.). All models are tested on 256x256 ImageNet-1K benchmark. The suffix \"-re\" denotes the use of rejection sampling.", "description": "Table 9 compares the generative performance of BiGR against other state-of-the-art models on ImageNet-1K using FID, IS, precision, and recall metrics.", "section": "4.2 UNIFORM PERFORMANCE"}, {"figure_path": "2410.14672/tables/table_16_0.html", "caption": "Table 10: Linear-probe top-1 accuracy on ImageNet-1K. MIM denotes masked image modeling. \u2020: our evaluation results.", "description": "Table 10 compares the linear probe top 1 accuracy on ImageNet-1k of various methods, categorized by contrastive methods, masked image modeling methods and conditional generative methods.", "section": "4.2 UNIFORM PERFORMANCE"}]