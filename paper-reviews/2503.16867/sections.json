[{"heading_title": "T2V Alignment", "details": {"summary": "The paper addresses the crucial challenge of **evaluating the semantic alignment between text prompts and generated videos in Text-to-Video (T2V) generation**. Existing metrics, like CLIPScore, are deemed insufficient as they offer only coarse-grained scores, failing to capture fine-grained alignment details and often diverging from human preference. To overcome these limitations, the paper introduces ETVA, a novel evaluation method centered around fine-grained question generation and answering. This innovative approach hinges on a multi-agent system that parses prompts into semantic scene graphs, enabling the generation of atomic questions. A knowledge-augmented multi-stage reasoning framework is designed for question answering, utilizing an auxiliary LLM to retrieve relevant common-sense knowledge, coupled with a video LLM for answering questions through a multi-stage process. The paper includes a new benchmark specifically for text-to-video alignment evaluation, featuring diverse prompts and atomic questions. Through a comprehensive evaluation of existing T2V models, the research identifies their key capabilities and limitations, paving the way for next-generation T2V generation. ETVA demonstrates a higher correlation with human judgment compared to existing metrics, marking a significant advancement in T2V evaluation."}}, {"heading_title": "ETVA Framework", "details": {"summary": "The ETVA framework is a novel approach designed for evaluating the semantic alignment between text prompts and generated videos, a significant challenge in the field of Text-to-Video (T2V) generation. Existing metrics often fall short, ETVA leverages a multi-agent system to parse prompts into semantic scene graphs and generate atomic questions, enabling fine-grained alignment analysis. A knowledge-augmented multi-stage reasoning framework, incorporates auxiliary LLMs for commonsense knowledge retrieval, further enhancing question answering accuracy by video LLMs. **ETVA's architecture focuses on generating detailed questions and emulating human-like reasoning**, mitigating hallucinations and improving alignment with human preferences. ETVA is a crucial step towards reliable evaluation, addressing existing metrics' limitations."}}, {"heading_title": "Benchmarking T2V", "details": {"summary": "Benchmarking Text-to-Video (T2V) generation is crucial for assessing model capabilities, identifying limitations, and driving progress in the field. Current benchmarks are categorized into general benchmarks evaluating overall performance (**quality, consistency, aesthetics**) and specific benchmarks focusing on particular aspects (**human actions, multi-object composition, physics phenomenon, time-lapse video**). **General benchmarks often lack fine-grained detail**, while **specific benchmarks offer in-depth analysis of narrow aspects**. A comprehensive benchmark should ideally encompass diverse prompts, atomic questions, and evaluation across various semantic categories (**existence, action, material, spatial, number, shape, color, camera, physics, other**). Developing reliable automatic metrics for T2V alignment is essential, as existing metrics often produce coarse-grained scores and fail to align with human preference. Benchmarking requires careful consideration of question generation and answering strategies, along with human annotation for accurate evaluation. Constructing **structured scene graph** for atomic element is important for achieving overall efficiency."}}, {"heading_title": "Atomic Question", "details": {"summary": "**Atomic questions** play a crucial role in enhancing the granularity and accuracy of text-to-video alignment evaluation. By breaking down complex prompts into smaller, more manageable questions, the evaluation focuses on assessing specific details within the generated video, addressing limitations of metrics like CLIPScore that offer coarse-grained scores failing to capture fine-grained nuances. This approach ensures a deeper understanding of the video's content and its alignment with the original text prompt. These **atomic questions** systematically verify various aspects of the video, reducing semantic redundancy, ensuring complete coverage, and generating answerable queries for video LLMs. This strategy aids in overcoming issues such as **video LLM hallucination**, where the model might struggle with complex or ambiguous questions, and allows for a more precise and human-aligned assessment."}}, {"heading_title": "Hallucination", "details": {"summary": "Hallucination in text-to-video generation is a significant issue, where models generate content inconsistent with the input prompt. This paper addresses this challenge by focusing on improving the alignment between the text prompt and the generated video content. The **core problem is that existing metrics often fail to capture fine-grained alignment details**, leading to discrepancies between automated evaluations and human preferences. To mitigate this, the authors introduce a novel evaluation method called ETVA, which **employs a multi-agent system for atomic question generation** and a **knowledge-augmented multi-stage reasoning framework for question answering.** This approach aims to simulate human-like reasoning to better assess the semantic alignment between text and video. The key contribution lies in the fine-grained analysis facilitated by generating detailed questions covering various aspects of the video content, ensuring a more comprehensive evaluation than coarse-grained metrics like CLIPScore. By incorporating commonsense knowledge and multi-stage reasoning, ETVA effectively reduces hallucinations and improves the reliability of text-to-video alignment evaluation. Extensive experiments validate the effectiveness of ETVA, demonstrating superior correlation with human judgment compared to existing evaluation metrics. Furthermore, the paper introduces ETVABench, a comprehensive benchmark for text-to-video alignment evaluation, facilitating systematic comparison and analysis of different T2V models. The study highlights the **limitations of current models in accurately simulating real-world physics and camera movements**, paving the way for future research to address these specific challenges and enhance the overall quality and coherence of generated videos."}}]