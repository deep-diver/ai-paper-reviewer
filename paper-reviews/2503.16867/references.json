{"references": [{"fullname_first_author": "William Peebles", "paper_title": "Scalable diffusion models with transformers", "publication_date": "2022-12-09", "reason": "This paper presents a highly influential architecture and has significantly impacted the field of text-to-video generation."}, {"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-01-01", "reason": "It introduces CLIP, a foundational model for text-image and text-video alignment, and has been broadly applied across various tasks."}, {"fullname_first_author": "Yi Wang", "paper_title": "Internvid: A large-scale video-text dataset for multimodal understanding and generation", "publication_date": "2024-01-01", "reason": "It introduces a substantial video-text dataset that serves as a crucial resource for training and evaluating text-to-video models."}, {"fullname_first_author": "Xuan He", "paper_title": "Videoscore: Building automatic metrics to simulate fine-grained human feedback for video generation", "publication_date": "2024-01-01", "reason": "It introduces an automatic metric for evaluating video generation quality which simulates fine-grained human feedback."}, {"fullname_first_author": "Tim Brooks", "paper_title": "Video generation models as world simulators", "publication_date": "2024-01-01", "reason": "This paper explores the potential of video generation models as world simulators, offering valuable insights into their broader capabilities."}]}