[{"figure_path": "2410.21271/tables/table_6_0.html", "caption": "Table 1 | Perplexity and Commonsense/Math reasoning results of LLaMA2/3 pruned by SparseGPT with different sparsity, with compensation via SVD/EORA of rank 128.", "description": "Table 1 presents the perplexity and commonsense/math reasoning results for LLaMA2/3 models that have been pruned using SparseGPT with varying sparsity levels and then compensated using SVD or EORA, both with a rank of 128.", "section": "4.2.1. Sparse Error Compensation"}, {"figure_path": "2410.21271/tables/table_7_0.html", "caption": "Table 2 | Perplexity and Commonsense/Math reasoning results of LLaMA2/3 quantized by GPTQ with different bit-width, with compensation via SVD/EORA of rank 128.", "description": "Table 2 presents the perplexity and commonsense/math reasoning results of LLaMA2/3 models that have been quantized using GPTQ with different bit-widths, and the results have been compensated using SVD and EORA, both with a rank of 128.", "section": "4.2. \u039c\u0391IN RESULTS"}, {"figure_path": "2410.21271/tables/table_7_1.html", "caption": "Table 3 | Perplexity and Commonsense/Math reasoning results of LLaMA2/3 models pruned using SparseGPT and quantized with GPTQ, with compensation via SVD/EORA of rank 128.", "description": "Table 3 presents the perplexity and commonsense/math reasoning results of LLaMA2/3 models that have been pruned using SparseGPT and quantized using GPTQ, with compensation performed using SVD and EORA, both with a rank of 128.", "section": "4.2.3. Sparse & Quantization Error Compensation"}, {"figure_path": "2410.21271/tables/table_8_0.html", "caption": "Table 4 | Comparision between SVD and EoRA of different rank on compensating LLaMA2/3 models pruned to 2:4 sparsity by SparseGPT on Perplexity and Commonsense/Math reasoning tasks.", "description": "Table 4 presents the results of comparing SVD and EoRA with different ranks on compensating LLaMA2/3 models that have been pruned to 2:4 sparsity using SparseGPT, evaluating their performance on perplexity and commonsense/math reasoning tasks.", "section": "4.3. Compensation With Different Rank"}, {"figure_path": "2410.21271/tables/table_9_0.html", "caption": "Table 5 Fine-tune the compressed LLaMA3-8B models of various compression settings using different initialization of the low-rank matrices for Commonsense/Math reasoning tasks.", "description": "Table 5 presents the fine-tuned results of compressed LLaMA3-8B models with different compression settings and LoRA initialization methods on commonsense and math reasoning tasks.", "section": "4.4. Fine-tuning Compressed Models with EoRA"}, {"figure_path": "2410.21271/tables/table_9_1.html", "caption": "Table 6 | Ablation study on the effect of using different proportions of the dataset for fine-tuning 2:4 pruned LLaMA3-8B models with varying low-rank matrix initializations on Commonsense/Math reasoning tasks.", "description": "Table 6 presents the ablation study results showing the effect of using different proportions of training datasets on fine-tuning the 2:4 pruned LLaMA3-8B models with different low-rank matrix initializations for commonsense and math reasoning tasks.", "section": "4.4.1. Ablation: Fine-tuning with different numbers of training data"}, {"figure_path": "2410.21271/tables/table_10_0.html", "caption": "Table 7 | Accuracy and the Model Size of quantizing EoRA of rank {128,512} to 4/3-bit on compensating LLaMA3-8B of {2:4 sparisity, 4/3-bit}.", "description": "Table 7 presents the accuracy and model size of quantized EoRA with different ranks and bit-widths when compensating for different compression settings of LLaMA3-8B.", "section": "4.5. Quantizing EoRA with Efficiency Evaluation"}]