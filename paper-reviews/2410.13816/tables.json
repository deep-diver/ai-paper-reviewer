[{"figure_path": "2410.13816/tables/table_8_0.html", "caption": "Table 1: (Real-world performance) V-GPS consistently improves the success rates of Octo across the board, achieving an 82.8% improvement on average. This demonstrates that using our value function to re-rank the actions can enhance the generalist policy.", "description": "Table 1 presents the real-world performance improvement of V-GPS across six manipulation tasks using the Octo-small-1.5 policy on a WidowX robot.", "section": "6.2 What Kind of Failure Modes of the Generalist Policy Does V-GPS Address?"}, {"figure_path": "2410.13816/tables/table_8_1.html", "caption": "Table 2: (SIMPLER [11] performance) V-GPS improves the success rates of all five generalist policies across multiple embodiments using the same single value function.", "description": "Table 2 presents the average success rates of five different generalist robotic policies across multiple robot embodiments on twelve SIMPLER tasks, comparing their performance with and without the V-GPS method.", "section": "6.3 Can V-GPS improve various generalist policies across different embodiments?"}, {"figure_path": "2410.13816/tables/table_14_0.html", "caption": "Table 2: (SIMPLER [11] performance) V-GPS improves the success rates of all five generalist policies across multiple embodiments using the same single value function.", "description": "Table 2 presents the average success rates of five different generalist robotic policies on 12 tasks, comparing performance with and without the V-GPS value function.", "section": "6.3 Can V-GPS improve various generalist policies across different embodiments?"}, {"figure_path": "2410.13816/tables/table_15_0.html", "caption": "Table 1: (Real-world performance) V-GPS consistently improves the success rates of Octo across the board, achieving an 82.8% improvement on average. This demonstrates that using our value function to re-rank the actions can enhance the generalist policy.", "description": "Table 1 presents the success rates of the Octo-small-1.5 policy and V-GPS (ours) on six real-world robotic manipulation tasks, showing consistent performance improvement with V-GPS.", "section": "6.2 What Kind of Failure Modes of the Generalist Policy Does V-GPS Address?"}, {"figure_path": "2410.13816/tables/table_16_0.html", "caption": "Table 1: (Real-world performance) V-GPS consistently improves the success rates of Octo across the board, achieving an 82.8% improvement on average. This demonstrates that using our value function to re-rank the actions can enhance the generalist policy.", "description": "Table 1 shows the success rates of the Octo-small-1.5 policy with and without V-GPS across six real-world robotic manipulation tasks.", "section": "6.2 What Kind of Failure Modes of the Generalist Policy Does V-GPS Address?"}, {"figure_path": "2410.13816/tables/table_16_1.html", "caption": "Table 2: (SIMPLER [11] performance) V-GPS improves the success rates of all five generalist policies across multiple embodiments using the same single value function.", "description": "Table 2 presents the success rates of five different generalist policies across multiple robotic platforms and tasks, showing consistent performance improvements when using Value-Guided Policy Steering (V-GPS).", "section": "6.3 Can V-GPS improve various generalist policies across different embodiments?"}, {"figure_path": "2410.13816/tables/table_17_0.html", "caption": "Table 7: (Comparison to fine-tuning generalist policies or training the policy from scratch.) V-GPS is the only method that achieves better performance than the generalist policy.", "description": "Table 7 compares the performance of V-GPS against fine-tuning generalist policies or training from scratch on the same dataset, demonstrating V-GPS's superior performance.", "section": "6.3 Can V-GPS improve various generalist policies across different embodiments?"}, {"figure_path": "2410.13816/tables/table_17_1.html", "caption": "Table 8: (Ablation over the size of datasets.) Even a value function trained on small amounts of data can be effective in guiding generalist policies at test time.", "description": "Table 8 shows the ablation study on the size of datasets used to train the value function, demonstrating that even with reduced data, the value function can effectively improve the success rate of the generalist policies.", "section": "6.3 Can V-GPS improve various generalist policies across different embodiments?"}, {"figure_path": "2410.13816/tables/table_17_2.html", "caption": "Table 1: (Real-world performance) V-GPS consistently improves the success rates of Octo across the board, achieving an 82.8% improvement on average. This demonstrates that using our value function to re-rank the actions can enhance the generalist policy.", "description": "Table 1 presents the real-world performance improvement of the Octo-small-1.5 policy across six tasks using Value-Guided Policy Steering (V-GPS).", "section": "6.2 What Kind of Failure Modes of the Generalist Policy Does V-GPS Address?"}, {"figure_path": "2410.13816/tables/table_17_3.html", "caption": "Table 2: (SIMPLER [11] performance) V-GPS improves the success rates of all five generalist policies across multiple embodiments using the same single value function.", "description": "Table 2 presents the average success rates of five different generalist robotic policies across multiple robot embodiments and tasks within the SIMPLER simulation environment, comparing their performance with and without V-GPS.", "section": "6.3 Can V-GPS improve various generalist policies across different embodiments?"}, {"figure_path": "2410.13816/tables/table_18_0.html", "caption": "Table 11: (Comparisons to the actors of Cal-QL & IQL.) The actors of Cal-QL and IQL consistently achieve a zero success rate. This highlights the benefit of our method, which combines the value function (critic) with the pre-trained generalist policies.", "description": "Table 11 shows that the IQL and Cal-QL actors fail to perform the tasks, highlighting the importance of combining a value function with pre-trained policies.", "section": "I Comparisons to the Actors of Cal-QL & IQL"}, {"figure_path": "2410.13816/tables/table_18_1.html", "caption": "Table 2: (SIMPLER [11] performance) V-GPS improves the success rates of all five generalist policies across multiple embodiments using the same single value function.", "description": "Table 2 presents the average success rates of five different generalist policies across multiple robot embodiments on various tasks within the SIMPLER simulation environment, demonstrating consistent improvement with V-GPS.", "section": "6.3 Can V-GPS improve various generalist policies across different embodiments?"}, {"figure_path": "2410.13816/tables/table_18_2.html", "caption": "Table 1: (Real-world performance) V-GPS consistently improves the success rates of Octo across the board, achieving an 82.8% improvement on average. This demonstrates that using our value function to re-rank the actions can enhance the generalist policy.", "description": "Table 1 presents the success rates of the Octo-small-1.5 policy and the V-GPS method on six real-world robotic manipulation tasks, showing a significant performance improvement by V-GPS.", "section": "6.2 What Kind of Failure Modes of the Generalist Policy Does V-GPS Address?"}]