[{"figure_path": "2410.13816/tables/table_8_0.md", "caption": " (Real-world performance) V-GPS consistently improves the success rates of Octo across the board, achieving an 82.8% improvement on average. This demonstrates that using our value function to re-rank the actions can enhance the generalist policy.", "description": "The table presents the success rates of the Octo-small-1.5 generalist policy and the proposed V-GPS method on six real-world robotic manipulation tasks categorized into three scenes (A, B, and C).  For each task, the table shows the success rate achieved by Octo-small-1.5 alone and the improvement observed when using V-GPS.  The improvement is calculated as the percentage increase in success rate when using V-GPS over Octo-small-1.5. Average success rates are also calculated for each scene and overall.  The table demonstrates a substantial improvement in performance across all tasks when the policy is steered using V-GPS. ", "section": "6.2 What Kind of Failure Modes of the Generalist Policy Does V-GPS Address?"}, {"figure_path": "2410.13816/tables/table_8_1.md", "caption": "(SIMPLER [11] performance) V-GPS improves the success rates of all five generalist policies across multiple embodiments using the same single value function.", "description": "Table 2 presents the results of the SIMPLER experiments.  It shows the success rates of five different generalist robotic policies (Octo-small, Octo-base, Octo-small-1.5, RT1-X, and OpenVLA) on 12 tasks across two robot embodiments (WidowX and Google Robot) with and without V-GPS.  The table is organized to compare each policy's performance before and after V-GPS is applied, highlighting the improvement achieved by incorporating V-GPS. The average success rate is provided for each policy across both embodiments, showcasing the consistent performance boost offered by V-GPS.", "section": "6.3 Can V-GPS improve various generalist policies across different embodiments?"}, {"figure_path": "2410.13816/tables/table_14_0.md", "caption": "Table 3: (V-GPS with IQL) Using an IQL value function for V-GPS is also effective for improving the success rates of all five generalist policies across multiple embodiments.", "description": "Table 3 presents the results of using an IQL value function within the V-GPS framework, applied across various generalist robotic policies (Octo-small, Octo-base, Octo-small-1.5, RT1-X, and OpenVLA) in the SIMPLER simulated environment.  The table shows the success rate for each policy on two tasks involving a WidowX robot and two tasks involving a Google Robot.  The \"+Ours\" column indicates the improvement in success rate achieved by incorporating the IQL value function within V-GPS. The table demonstrates that using IQL for the value function in V-GPS also provides consistent improvement across all five generalist policies.", "section": "6.3 Can V-GPS improve various generalist policies across different embodiments?"}, {"figure_path": "2410.13816/tables/table_15_0.md", "caption": "(Real-world performance) V-GPS consistently improves the success rates of Octo across the board, achieving an 82.8% improvement on average. This demonstrates that using our value function to re-rank the actions can enhance the generalist policy.", "description": "Table 1 presents the success rates of the Octo-small-1.5 policy and the V-GPS approach on six real-world robotic manipulation tasks.  The tasks are grouped into three scenes (Scene A, Scene B, Scene C).  For each task, the table shows the success rate achieved by Octo-small-1.5 alone and the improvement achieved using V-GPS.  The average improvement across all six tasks is also presented.", "section": "6.2 What Kind of Failure Modes of the Generalist Policy Does V-GPS Address?"}, {"figure_path": "2410.13816/tables/table_16_0.md", "caption": "(Real-world performance) V-GPS consistently improves the success rates of Octo across the board, achieving an 82.8% improvement on average. This demonstrates that using our value function to re-rank the actions can enhance the generalist policy.", "description": "The table presents the success rates of the Octo-small-1.5 policy and the V-GPS method on six real-world robotic manipulation tasks across three different scenes. For each task, the table shows the success rate of Octo-small-1.5 alone, the success rate after applying V-GPS, and the percentage improvement achieved by V-GPS. The tasks involve placing objects like green peppers, sweet potatoes, mushrooms, and sushi into containers or on cloths.  The results demonstrate consistent improvement across all tasks, highlighting the effectiveness of V-GPS in enhancing the robustness and precision of generalist robotic policies.", "section": "6.2 What Kind of Failure Modes of the Generalist Policy Does V-GPS Address?"}, {"figure_path": "2410.13816/tables/table_16_1.md", "caption": "Table 1: (Real-world performance) V-GPS consistently improves the success rates of Octo across the board, achieving an 82.8% improvement on average. This demonstrates that using our value function to re-rank the actions can enhance the generalist policy.", "description": "Table 1 presents the results of real-world experiments evaluating the performance of V-GPS on six tasks across three different scenes using the Octo-small-1.5 policy. The table shows the success rate for each task using both the baseline policy (Octo-small-1.5) and V-GPS (Ours), along with the percentage improvement achieved by V-GPS. The tasks involve manipulation involving placing objects on different surfaces, which tests the policy's precision and robustness. V-GPS consistently improves the baseline policy's success rate across all six tasks, with significant improvements observed in Scenes A, B, and C.", "section": "6.2 What Kind of Failure Modes of the Generalist Policy Does V-GPS Address?"}, {"figure_path": "2410.13816/tables/table_17_0.md", "caption": "(Real-world performance) V-GPS consistently improves the success rates of Octo across the board, achieving an 82.8% improvement on average. This demonstrates that using our value function to re-rank the actions can enhance the generalist policy.", "description": "The table presents the success rates of the Octo-small-1.5 generalist robotic policy and the proposed V-GPS method on six real-world robotic manipulation tasks.  These tasks are grouped into three scenes (Scene A, B, and C), each containing two tasks. The table shows the success rate for Octo-small-1.5 alone and then for the same policy when augmented by V-GPS. The final column displays the percentage improvement achieved by V-GPS over Octo-small-1.5 for each task and on average across all tasks within a scene and overall. The results highlight that V-GPS consistently improves the policy's performance across various real-world scenarios.", "section": "6.2 What Kind of Failure Modes of the Generalist Policy Does V-GPS Address?"}, {"figure_path": "2410.13816/tables/table_17_1.md", "caption": "Table 8: (Ablation over the size of datasets.) Even a value function trained on small amounts of data can be effective in guiding generalist policies at test time.", "description": "This ablation study investigates the impact of dataset size on the performance of the V-GPS approach.  Three different dataset sizes are used: the full Bridge dataset (100%), and smaller versions reduced to 50% and 10% of its original size. The success rate is evaluated on the SIMPLER eggplant task for each dataset size.  The results demonstrate that even with a significantly reduced dataset (50%), the model still achieves the same level of improvement over the baseline Octo-small performance. A reduction to 10% of the original dataset does lead to a minor performance reduction, but improvement over the baseline is still observed, highlighting the effectiveness of V-GPS even with limited training data.", "section": "6.1 Experimental Scenarios and Comparisons"}, {"figure_path": "2410.13816/tables/table_17_2.md", "caption": "(Real-world performance) V-GPS consistently improves the success rates of Octo across the board, achieving an 82.8% improvement on average. This demonstrates that using our value function to re-rank the actions can enhance the generalist policy.", "description": "The table presents the success rates of the Octo-small-1.5 policy and the V-GPS method on six real-world robotic manipulation tasks across three different scenes.  For each task, it lists the success rate of the Octo-small-1.5 policy alone and then the success rate achieved after incorporating V-GPS.  The improvement percentage, calculated as ((V-GPS success rate - Octo-small-1.5 success rate) / Octo-small-1.5 success rate) * 100%, is also shown for each task. Finally, an average improvement across all six tasks is provided.", "section": "6.2 What Kind of Failure Modes of the Generalist Policy Does V-GPS Address?"}, {"figure_path": "2410.13816/tables/table_17_3.md", "caption": "(SIMPLER [11] performance) V-GPS improves the success rates of all five generalist policies across multiple embodiments using the same single value function.", "description": "Table 2 presents the success rates of five different generalist policies (Octo-small, Octo-base, Octo-small-1.5, RT1-X, and OpenVLA) across two robot platforms (WidowX and Google Robot)  in the SIMPLER simulation environment.  For each policy, two sets of results are shown: the baseline performance without V-GPS and the performance after applying V-GPS.  The results are averaged across multiple tasks. The table demonstrates that V-GPS consistently improves the success rates of all five policies across both platforms, highlighting its effectiveness in enhancing the performance of various generalist robotic policies.", "section": "6.3 Can V-GPS improve various generalist policies across different embodiments?"}, {"figure_path": "2410.13816/tables/table_18_0.md", "caption": "Table 1: (Real-world performance) V-GPS consistently improves the success rates of Octo across the board, achieving an 82.8% improvement on average. This demonstrates that using our value function to re-rank the actions can enhance the generalist policy.", "description": "Table 1 presents the success rates of the Octo-small-1.5 policy and the V-GPS approach on six real-world robotic manipulation tasks across three different scenes.  For each task, the table shows the success rate of the Octo-small-1.5 policy alone, the success rate when using V-GPS, and the percentage improvement achieved by V-GPS.  The tasks are categorized by scene (A, B, and C), and an average success rate is calculated for each scene and overall. The results demonstrate a substantial improvement in success rates across all tasks when using V-GPS.", "section": "6.2 What Kind of Failure Modes of the Generalist Policy Does V-GPS Address?"}, {"figure_path": "2410.13816/tables/table_18_1.md", "caption": "Table 2: (SIMPLER [11] performance) V-GPS improves the success rates of all five generalist policies across multiple embodiments using the same single value function.", "description": "Table 2 presents the results of the SIMPLER environment experiments.  It shows the success rates of five different generalist robotic policies (Octo-small, Octo-base, Octo-small-1.5, RT1-X, and OpenVLA) across two robot platforms (WidowX and Google Robot) for a total of 12 tasks.  The table compares the baseline performance of each policy to its performance when enhanced by the Value-Guided Policy Steering (V-GPS) method.  Importantly, a single V-GPS value function was used across all policies and embodiments, demonstrating the generality and transferability of the approach.", "section": "6.3 Can V-GPS improve various generalist policies across different embodiments?"}, {"figure_path": "2410.13816/tables/table_18_2.md", "caption": "Table 2: (SIMPLER [11] performance) V-GPS improves the success rates of all five generalist policies across multiple embodiments using the same single value function.", "description": "This table presents the results of the SIMPLER environment evaluation.  It shows the performance of five different generalist robotic policies (Octo-small, Octo-base, Octo-small-1.5, RT1-X, and OpenVLA) across two different robot embodiments (WidowX and Google Robot). The table compares the baseline success rates of each policy on 12 tasks to the success rates achieved after incorporating V-GPS.  Each row represents a task, and the columns show the baseline success rate, the success rate after applying V-GPS, and the percentage improvement. The results demonstrate that V-GPS consistently improves the success rates of all five policies across all tasks and embodiments using a single, pre-trained value function.", "section": "6.3 Can V-GPS improve various generalist policies across different embodiments?"}]