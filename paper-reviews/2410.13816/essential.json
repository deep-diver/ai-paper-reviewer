{"reason": "This paper introduces Value-Guided Policy Steering (V-GPS), a novel method to improve the performance of pre-trained generalist robotic policies.  V-GPS re-ranks actions proposed by a generalist policy using a value function learned via offline reinforcement learning (RL), without requiring fine-tuning or access to the policy's weights.  Experiments on multiple robotic platforms and tasks demonstrate consistent performance improvements.", "takeaways": ["V-GPS enhances generalist robotic policies' precision and robustness without needing to fine-tune or access their weights.", "A single V-GPS value function improves performance across different generalist policies trained on varied datasets.", "V-GPS consistently boosts performance in both simulated and real-world robotic manipulation tasks."], "tldr": "Value-Guided Policy Steering (V-GPS) improves pre-trained generalist robotic policies by re-ranking actions based on a value function learned from offline RL, leading to consistent performance gains across various robots and tasks without the need for fine-tuning."}