{"references": [{" publication_date": "2017", "fullname_first_author": "D. Silver", "paper_title": "Mastering the game of go without human knowledge", "reason": "This paper is foundational in the field of reinforcement learning, demonstrating the potential of deep reinforcement learning agents to achieve superhuman performance in complex games.  Its impact extends beyond game playing, influencing the development of advanced RL techniques used in robotics and other domains.  The paper's success and methodology have inspired many subsequent works, including this one which leverages similar concepts in a different context.", "section_number": 1}, {" publication_date": "2021", "fullname_first_author": "A. Ramesh", "paper_title": "Zero-shot text-to-image generation", "reason": "This paper is highly significant for its advancement of generative models and their ability to produce high-quality images from text descriptions without requiring specific training data for each image.  The concepts of zero-shot learning and large-scale generative models are closely related to this paper's approach, which aims to improve the performance of pre-trained robotic policies without fine-tuning using a general value function.", "section_number": 1}, {" publication_date": "2020", "fullname_first_author": "T. Brown", "paper_title": "Language models are few-shot learners", "reason": "This paper highlights the remarkable capabilities of large language models to perform various tasks with only a few examples, a concept relevant to this paper's approach of guiding robotic policies using a value function trained on a relatively smaller dataset. The method of using a value function to improve performance for language models is an important comparison to our method for robotics.", "section_number": 1}, {" publication_date": "2020", "fullname_first_author": "J. Kaplan", "paper_title": "Scaling laws for neural language models", "reason": "This paper provides critical insights into the relationship between model size, data quantity, and performance in large language models, knowledge that is directly relevant to the field of robotic learning, where the training of increasingly complex models is a key focus.  This paper helps contextualize and understand the scaling aspect of the problem in generalist robotic policies.", "section_number": 1}, {" publication_date": "2021", "fullname_first_author": "R. Bommasani", "paper_title": "On the opportunities and risks of foundation models", "reason": "This paper provides a comprehensive overview of foundation models, their potential, and their associated challenges, which is extremely relevant to the current research on foundation models for robotics. The discussion on the challenges and risks associated with these models directly relates to this paper's approach of enhancing generalist policies without fine-tuning.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "O. X.-E. Collaboration", "paper_title": "Open X-Embodiment: Robotic learning datasets and RT-X models", "reason": "This paper is crucial because it describes the large-scale dataset (OXE) used for pre-training the generalist robotic policies investigated in this work.  The paper introduces the RT-X policy, one of the policies tested in the experimental evaluation section, and thus understanding the data and methods is essential to comprehending this work's impact and results.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "A. Khazatsky", "paper_title": "Droid: A large-scale in-the-wild robot manipulation dataset", "reason": "This paper introduces a large-scale dataset relevant to the field of robotic manipulation, focusing on data gathered in real-world scenarios. The challenges of obtaining quality real-world data and the methods of dealing with its mixed quality directly relates to this paper's work of improving pre-trained generalist policies.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "A. Brohan", "paper_title": "Rt-2: Vision-language-action models transfer web knowledge to robotic control", "reason": "The Rt-2 model is one of the models tested and improved upon in the experimental sections of the paper. Thus, understanding its background and capabilities is important for evaluating the performance improvements reported in the paper.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Octo Model Team", "paper_title": "Octo: An open-source generalist robot policy", "reason": "This paper introduces the Octo policy, which is one of the main policies used for testing in the experimental evaluation section. The paper describes the architecture and training of the Octo policy, providing crucial background information necessary to evaluate the contribution of this paper.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "K. Black", "paper_title": "Zero-shot robotic manipulation with pretrained image-editing diffusion models", "reason": "This paper focuses on the use of diffusion models for zero-shot robotic manipulation, a related area of research that leverages pre-trained models, which is similar to this paper's approach of leveraging pre-trained generalist policies. Understanding this work provides context to evaluate how the current method differs.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "X. Li", "paper_title": "Evaluating real-world robot manipulation policies in simulation", "reason": "This paper introduces SIMPLER, a simulation environment used for evaluation in the current paper. Understanding this simulation environment's characteristics and capabilities is essential for interpreting the results of the experimental evaluations.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "K. Cobbe", "paper_title": "Training verifiers to solve math word problems", "reason": "This work provides an example of using a value function to improve the performance of a large language model, and this study is directly related to this paper's approach, which applies similar concepts to re-ranking actions in robotic policies.  The insight of using sampling-based action selection for improvements is valuable for context.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "A. Hosseini", "paper_title": "V-star: Training verifiers for self-taught reasoners", "reason": "Similar to Cobbe et al. (2021), this work further strengthens the concept of using value function guided reasoning to improve reasoning capabilities in LLMs. Since the paper is drawing a parallel between language models and robotic control, this paper provides further support for that connection.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "S. Han", "paper_title": "Value augmented sampling for language model alignment and personalization", "reason": "This work demonstrates the use of value functions to guide action selection in language models, a technique directly analogous to the proposed V-GPS method. This paper shows the effectiveness of sampling-based methods guided by value functions in related domains.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "H. Chen", "paper_title": "Offline reinforcement learning via high-fidelity generative behavior modeling", "reason": "This paper focuses on offline reinforcement learning (RL) for robotic tasks, a method directly related to the value function training in the proposed V-GPS method.  The use of value functions and offline RL is a key connection between this paper and the proposed method.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "P. Hansen-Estruch", "paper_title": "Idql: Implicit q-learning as an actor-critic method with diffusion policies", "reason": "This work shows the application of implicit Q-learning, one of the offline RL methods that could be used to train the value function for V-GPS.  It provides an alternative algorithm to the one chosen in the paper, demonstrating broader applicability.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "A. Kumar", "paper_title": "Pre-training for robots: Offline rl enables learning new tasks from a handful of trials", "reason": "This paper is highly relevant due to its focus on offline reinforcement learning for robotics, particularly its use of value functions to improve policy performance. It directly addresses the challenge of using offline RL for real-world robotics problems, a key theme of this paper.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "H. Walke", "paper_title": "Bridgedata v2: A dataset for robot learning at scale", "reason": "This paper describes the Bridge V2 dataset, which is a key component of the training data used for the value function in the proposed V-GPS method.  Understanding the details of the dataset is crucial for interpreting the results and the limitations of the method.", "section_number": 6}, {" publication_date": "2024", "fullname_first_author": "M. Kim", "paper_title": "Openvla: An open-source vision-language-action model", "reason": "This paper introduces OpenVLA, one of the generalist robotic policies evaluated in this paper.  OpenVLA is a large vision-language-action model, and understanding its architecture and performance is essential for evaluating the effectiveness of the V-GPS method in improving its performance.", "section_number": 6}]}