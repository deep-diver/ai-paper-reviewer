{"references": [{"fullname_first_author": "Jared Kaplan", "paper_title": "Scaling laws for neural language models", "publication_date": "2020-XX-XX", "reason": "This paper establishes foundational scaling laws for neural language models, providing a theoretical basis for understanding the relationship between model size, training data, and performance, which is crucial for the current work's investigation into optimal scaling laws for VLMs."}, {"fullname_first_author": "Haotian Liu", "paper_title": "LLaVA: Large language and vision assistant", "publication_date": "2023-XX-XX", "reason": "This paper introduces LLaVA, a foundational vision-language model that serves as the basis for many of the experiments and analyses performed in the current work."}, {"fullname_first_author": "Wentong Li", "paper_title": "TokenPacker: Efficient visual projector for multimodal LLMs", "publication_date": "2024-XX-XX", "reason": "This paper introduces TokenPacker, a token compression technique used extensively in this work, providing a critical method for achieving high visual token compression ratios."}, {"fullname_first_author": "Mu Cai", "paper_title": "Matryoshka Multimodal Models", "publication_date": "2024-XX-XX", "reason": "This paper introduces Matryoshka, a competing visual token compression technique, which the authors compare against in their experiments to demonstrate the superiority of their method."}, {"fullname_first_author": "Yuzhang Shang", "paper_title": "LLaVA-PruMerge: Adaptive token reduction for efficient large multimodal models", "publication_date": "2024-XX-XX", "reason": "This paper introduces LLaVA-PruMerge, another competing visual token compression technique, which the authors compare against in their experiments to demonstrate the superiority of their method."}]}