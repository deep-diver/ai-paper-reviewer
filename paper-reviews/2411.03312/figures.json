[{"figure_path": "https://arxiv.org/html/2411.03312/x1.png", "caption": "(a) Scaling laws for VLMs at Q=0\ud835\udc440Q=0italic_Q = 0 (cached text).", "description": "This figure displays the scaling laws for Vision Language Models (VLMs) when the input text query is cached (Q=0). The x-axis represents the inference FLOPs, a measure of computational cost, which is varied by adjusting the number of visual input tokens processed by the model. The y-axis shows the average downstream error, representing the model's performance on downstream tasks.  Different colored lines represent VLMs with different numbers of parameters (LLM sizes), demonstrating how the optimal trade-off between visual tokens and LLM size changes with computational cost.", "section": "3 RESULTS: ESTIMATED SCALING CURVES"}, {"figure_path": "https://arxiv.org/html/2411.03312/x2.png", "caption": "(b) Scaling laws for VLMs at Q=50\ud835\udc4450Q=50italic_Q = 50 (variable text).", "description": "This figure shows the scaling laws for Vision Language Models (VLMs) when the number of text input tokens (Q) is variable and set to 50.  The plot illustrates the relationship between average downstream error (y-axis), inference FLOPs (x-axis), the number of visual tokens (V) processed by the LLM, and the number of LLM parameters (N). Different colors represent different LLM sizes, and the size of the data points reflects the number of visual tokens used. The plot helps to visualize the optimal trade-off between LLM size and the number of visual tokens, which helps to understand the compute optimal behavior in VLMs.  A dotted black line shows the Pareto optimal curve indicating the best performance for a given inference FLOP.", "section": "3 RESULTS: ESTIMATED SCALING CURVES"}, {"figure_path": "https://arxiv.org/html/2411.03312/x3.png", "caption": "Figure 1: Inference optimal scaling laws for VLMs: The number of visual tokens (V\ud835\udc49Vitalic_V) passed to the LLM (after token compression, \u00a7\u00a02.2), along with the LLM parameter count (N\ud835\udc41Nitalic_N), directly determine the inference cost of VLMs (\ud835\udcaa\u2062(N\u2062(Q+V))\ud835\udcaa\ud835\udc41\ud835\udc44\ud835\udc49\\mathcal{O}(N(Q+V))caligraphic_O ( italic_N ( italic_Q + italic_V ) )), where Q\ud835\udc44Qitalic_Q is the text input tokens. Since a VLM\u2019s downstream performance is directly affected by both these factors, it is unclear what the optimal trade-off is for a fixed inference compute.\nIn this work, we try to answer this question with our scaling laws.\nLeft (a): We plot the fitted scaling curves, assuming cached text input tokens (Q=0\ud835\udc440Q=0italic_Q = 0).\nWe observe a surprising trend: for visual reasoning tasks, the compute optimal behavior (dotted black curve) requires using a single visual token with the\nlargest possible language model that can fit under the inference budget. Right (b): Inference optimal behavior under Q=50\ud835\udc4450Q=50italic_Q = 50\nrequires slightly higher number of visual tokens as the LLM already incurs a fixed cost\ndue to the text tokens.", "description": "This figure displays scaling laws for Vision Language Models (VLMs) that illustrate the optimal trade-off between the number of visual tokens and the LLM's parameter count under a fixed inference compute budget.  The left panel (a) shows the scaling laws when text input is cached (Q=0), revealing that for visual reasoning tasks, the optimal performance is achieved with the largest LLM and only one visual token. The right panel (b) demonstrates the scenario with uncached text input (Q=50), where a slightly higher number of visual tokens becomes optimal due to the inherent computational cost of processing the text tokens.", "section": "3 RESULTS: ESTIMATED SCALING CURVES"}, {"figure_path": "https://arxiv.org/html/2411.03312/x4.png", "caption": "Figure 2: Our scaling laws (fitted on VLMs with 0.5-7B LLMs) estimate the performance of a 14B LLM VLM with an error margin of less than 2%.", "description": "Figure 2 shows that the scaling laws derived from experiments using 0.5B to 7B parameter LLMs accurately predict the performance of a significantly larger 14B parameter LLM.  The figure demonstrates the generalizability of the scaling laws across a wide range of model sizes.  The prediction error is less than 2%, indicating a high degree of accuracy and reliability in the established scaling relationship between LLM parameters, number of visual tokens, and downstream performance. This validates the use of the scaling laws for evaluating the performance of larger models without the need for extensive and costly retraining.", "section": "3.3 RESULTS: ESTIMATED SCALING CURVES"}, {"figure_path": "https://arxiv.org/html/2411.03312/x5.png", "caption": "(a) Performance trends and trade-offs of VLMs change when varying the number of input text token Q\ud835\udc44Qitalic_Q.", "description": "This figure demonstrates how the optimal balance between the number of visual tokens and LLM size for VLMs varies depending on the length of the input text query (Q).  As Q increases, the cost of processing text tokens in the VLM increases. Therefore, the impact of adding more visual tokens becomes less significant relative to the impact of increasing LLM size. Initially, with short queries, using a larger LLM with fewer visual tokens is better. But with longer queries, using a smaller LLM with more visual tokens can become more optimal, as the added cost from extra visual tokens is outweighed by the benefits of a larger LLM. This demonstrates the importance of considering the interaction between text and visual tokens when optimizing VLM performance.", "section": "3.3 RESULTS: ESTIMATED SCALING CURVES"}, {"figure_path": "https://arxiv.org/html/2411.03312/x6.png", "caption": "(b) Scaling laws on OCR-like tasks favor visual token count over LLM size; the opposite of visual reasoning.", "description": "The figure demonstrates that for Optical Character Recognition (OCR) tasks, unlike visual reasoning tasks, increasing the number of visual tokens improves performance more significantly than increasing the LLM size.  This contrasts with the findings for visual reasoning tasks, where larger LLMs with fewer visual tokens were optimal.  The plot shows downstream error as a function of inference FLOPs, with different colored lines representing different LLM parameter sizes and point sizes indicating the number of visual tokens. The results suggest that for OCR-like tasks, preserving more visual detail is paramount, even at the cost of using smaller LLMs.", "section": "3.3 Results: Estimated Scaling Curves"}, {"figure_path": "https://arxiv.org/html/2411.03312/x7.png", "caption": "Figure 3: Adjusting input text token count and benchmark family shifts performance trends. Left (a): For visual reasoning tasks, as the number of text tokens Q\ud835\udc44Qitalic_Q increases, the impact of increasing the number of visual tokens V\ud835\udc49Vitalic_V, i.e., reducing compression, becomes more apparent. Intuitively, at enough text tokens, initial increases in visual tokens are only a minor fraction of the overall compute (\u00a7\u00a03.3.2). Right (b): When tasks are changed from visual reasoning to OCR/text-understanding, trends reverse: visual token count should now be prioritized over LLM size (\u00a7\u00a03.3.3).", "description": "Figure 3 explores how the optimal balance between the number of visual tokens and LLM size changes depending on the task and the length of the text input.  The left panel (a) shows that for visual reasoning tasks, increasing the number of text tokens makes the effect of adding more visual tokens less significant, because the text token processing dominates the computational cost. Conversely, for OCR and text understanding tasks (right panel, b), the performance is more strongly affected by the number of visual tokens than the LLM size, reversing the trend observed for visual reasoning.", "section": "3.3 RESULTS: ESTIMATED SCALING CURVES"}, {"figure_path": "https://arxiv.org/html/2411.03312/x8.png", "caption": "Figure 4: \nPerformances of various LLM size and visual token count combinations at similar inference compute. For visual reasoning tasks, at a given fixed inference cost, increasing the LLM size by decreasing the number of visual tokens improves VLM performance. However, for text recognition tasks, decreasing the number of visual tokens is detrimental to performance (\u00a7\u00a03.3.3).", "description": "Figure 4 presents a bar chart comparing the performance of different Vision Language Models (VLMs) across various visual reasoning and text recognition tasks.  The models vary in their size (LLM parameter count) and the number of visual tokens processed.  Importantly, all models are evaluated at approximately the same inference compute cost. The chart shows that for visual reasoning tasks, increasing model size while simultaneously decreasing the number of visual tokens leads to better performance. This supports the finding that for these types of tasks, larger models can leverage smaller sets of well-chosen visual information more effectively. In contrast, for text recognition tasks, reducing the number of visual tokens negatively impacts the model's performance, regardless of model size. This indicates that text recognition relies more heavily on the detail contained within a large number of visual tokens.", "section": "3.3 Results: Estimated Scaling Curves"}, {"figure_path": "https://arxiv.org/html/2411.03312/x9.png", "caption": "Figure 5: \nOur query-based convolutional cross-attention (QueCC, pronounced \u201cquick\u201d) compression technique. User input text tokens are first processed through the LLM backbone to generate text embeddings that are then combined with the visual tokens. Within QueCC, the query-embedded visual tokens are downsampled via convolution. Next, local cross-attention is applied between the downsampled tokens and their respective visual tokens regions. The compressed tokens pass through an MLP before passing into the LLM, alongside input text tokens, for generation (\u00a7\u00a04).", "description": "Figure 5 illustrates the architecture of QueCC (Query-based convolutional cross-attention), a novel token compression technique designed for high compression ratios.  The process begins with user input text tokens, processed by the LLM backbone to produce text embeddings. These embeddings are combined with the original visual tokens.  The core of QueCC then downsamples these query-embedded visual tokens using a convolutional layer, followed by applying local cross-attention between the downsampled tokens and their corresponding visual token regions. Finally, the compressed visual tokens are passed through a Multi-Layer Perceptron (MLP) before being fed into the LLM, alongside the original text tokens, for final generation.", "section": "4 QUERY-BASED TOKEN COMPRESSION"}]