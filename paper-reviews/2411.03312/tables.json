[{"content": "Method | # Token | GQA | MMB | MME | POPE | SQA | TextVQA | VizWiz | VQAv2\n---|---|---|---|---|---|---|---|---|---\nLLaVA-1.5 | 576 | 62.0 | 64.3 | 1510.7 | 85.9 | 66.8 | 58.2 | 50.0 | 78.5\nPruMerge | ~32 | 57.2* | 60.9 | 1350.3 | 76.3 | 68.5 | 56.0 | 45.2* | 72.0\nTokenPacker | 36 | 59.6 | 62.8 | 1440.9* | 83.3* | 71.0* | 53.2* | 50.2 | 75.0*\nMatryoshka Multi. | 36 | 60.3* | 64.8 | \u2013 | 85.5 | \u2013 | \u2013 | 52.8 | \u2013\nMatryoshka Query | 36 | 58.8 | 63.4* | 1416.3 | 81.9 | 66.8 | \u2013 | 51.0* | 73.7\nQueCC (Ours) | 36 | 60.5 | 62.5 | 1442.0 | 84.5* | 70.6* | 53.3* | 50.1 | 75.8\nTokenPacker | 16 | 58.9* | 62.7* | 1378.8* | 83.7* | 68.1* | 52.5* | 50.5* | 74.4*\nMatryoshka Query | 16 | 57.6 | 61.9 | 1408.5 | 80.8 | 67.5 | \u2013 | 49.8* | 71.1\nQueCC | 16 | 59.0 | 62.2* | 1408.0* | 83.4* | 70.7* | 51.3* | 47.7 | 74.5\nTokenPacker | 4 | 56.2* | 61.5* | 1347.6* | 81.7* | 68.5* | 49.2* | 45.7* | 70.5*\nMatryoshka Query | 4 | 53.0 | 56.5 | 1176.1 | 77.6 | 65.1 | \u2013 | 49.4 | 64.1\nQueCC | 4 | 56.5 | 62.1* | 1390.3* | 81.8* | 68.6* | 48.7* | 45.0 | 70.6\nTokenPacker | 1 | 53.4* | 58.7* | 1262.4* | 80.7* | 69.4* | 46.2* | 41.1* | 66.9*\nMatryoshka Multi. | 1 | 52.6 | 59.5 | \u2013 | 78.4 | \u2013 | \u2013 | 49.4 | \u2013\nMatryoshka Query | 2 | 50.8 | 54.4 | 1144.0 | 74.5 | 65.0 | \u2013 | 48.5* | 61.0\nQueCC | 1 | 53.5 | 59.4* | 1269.1* | 81.3* | 69.9* | 46.8* | 44.1 | 67.3", "caption": "Table 1: Comparison of various token compression methods for VLMs at different compression rates. All models use the Vicuna-1.5 7B model as the language backbone. A \u2217 denotes benchmark results for other techniques we evaluated, while best scores are bolded, and second best underlined. Our method outperforms alternatives on almost all benchmarks at extremely high compression regions (visual tokens reduced to 1 or 4) and has strong performance at lower compression rates.", "description": "Table 1 compares different visual token compression methods for Vision Language Models (VLMs) across various compression ratios.  All models utilize the Vicuna-1.5 7B model as their language backbone.  The table shows performance on several benchmark tasks, indicating the accuracy of each method. Results marked with an asterisk (*) represent benchmarks from other studies.  The best scores are in bold, and the second-best scores are underlined.  The authors' method (QueCC) shows superior performance compared to other techniques, particularly at extremely high compression rates (reducing visual tokens to 1 or 4), while still maintaining competitive performance at lower compression levels.", "section": "4 Query-Based Convolutional Cross-Attention (QueCC) Results"}]