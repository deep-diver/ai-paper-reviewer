{"importance": "This paper is crucial because it challenges common assumptions in VLM optimization, **revealing a surprising finding that using fewer visual tokens with larger models is computationally optimal** for visual reasoning tasks. This shifts the focus of research towards extreme token compression, potentially leading to more efficient and cost-effective VLMs for real-world applications.  It also opens new avenues for developing token compression algorithms optimized for high compression ratios, improving VLM deployment.", "summary": "Inference-optimal Vision Language Models (VLMs) need only one visual token but larger models!", "takeaways": ["Compute-optimal VLMs for visual reasoning surprisingly use only one visual token with the largest possible language model.", "The number of visual tokens affects VLM performance much less than the language model's size.", "A new query-based token compression algorithm is proposed to achieve high token compression rates"], "tldr": "Vision Language Models (VLMs) are powerful but computationally expensive due to processing many visual tokens from images.  Current research mostly focuses on modestly reducing token numbers, while the trade-off with model size is unclear. This impacts deployment in real-world applications. \nThis paper investigates the optimal trade-off between model size and the number of visual tokens.  It establishes scaling laws showing that for visual reasoning tasks, surprisingly, using the largest model with a minimal number of visual tokens (often one) leads to the best performance for a given computational budget.  The authors introduce a new query-based token compression algorithm designed for this extreme compression regime. The results demonstrate the need to reconsider token compression strategies and suggest focusing on more significant compression.", "affiliation": "Carnegie Mellon University", "categories": {"main_category": "Multimodal Learning", "sub_category": "Vision-Language Models"}}