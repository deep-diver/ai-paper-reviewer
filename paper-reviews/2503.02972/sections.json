[{"heading_title": "Linguistics+Obfus", "details": {"summary": "**Linguistic obfuscation** is a key strategy to disentangle memorization from true reasoning in LLMs. By applying orthographic and ruleset based transformations to the Problemese data, this approach aims to create novel problem instances that preserve the underlying linguistic structure and reasoning steps. The goal is to prevent LLMs from relying on memorized solutions, forcing them to generalize and apply learned rules to unseen inputs. The success hinges on designing obfuscation techniques that do not inadvertently alter the problem's difficulty or render it unsolvable. Manual annotation and external validation are implemented to ensure that reasoning element is preserved while the new variations still align. Moreover, comparing model performance on original vs. obfuscated versions provides insight into the extent to which models rely on memorization."}}, {"heading_title": "Lingoly Benchmark", "details": {"summary": "While \u201cLingoly Benchmark\u201d isn't explicitly a heading, the paper introduces LINGOLY-TOO, a benchmark designed to evaluate linguistic reasoning in LLMs. **LINGOLY-TOO aims to disentangle memorization from genuine reasoning** by using orthographic templatization to dynamically obfuscate languages, creating unseen linguistic problems. **The benchmark leverages Linguistics Olympiad problems** and generates numerous question variations, preserving reasoning steps while reducing memorization risk. By evaluating LLMs on these obfuscated problems, the paper provides a more robust assessment of reasoning abilities. **This approach addresses the limitations of existing benchmarks** susceptible to data contamination and overestimation of model performance due to prior exposure."}}, {"heading_title": "Tokenisation Imp", "details": {"summary": "The paper explores the impact of tokenization on LLMs' performance in linguistic reasoning, particularly when dealing with obfuscated languages. **Standard tokenization methods might inadvertently assist models** by preserving sub-word units. **Alternative tokenization strategies (dash or character-level) reduce model performance**, indicating that the models rely on token frequencies learned during pre-training. Further research on advanced tokenization techniques could reduce their reliance, and improve generalization."}}, {"heading_title": "Data Exposure", "details": {"summary": "**Data exposure** in LLMs significantly inflates perceived reasoning abilities. If models encounter benchmark data during pre-training, they can memorize solutions rather than truly reasoning.  This **contamination** undermines the validity of benchmarks. Techniques to mitigate this include using low-resource languages, synthetic data generation, and context removal.  The paper's approach, **orthographic obfuscation**, reduces the likelihood of memorization by altering the surface form of the language while preserving the underlying reasoning steps. This allows for a more accurate assessment of genuine reasoning capability. Models performed better with questions in their original orthography, suggesting prior data exposure contributed to their performance. The paper also fine-tuned models on a subset of obfuscated problems as training data and observed improvements on unseen obfuscations which also suggests overfitting of obfuscations used for training. Finally, the paper shows linear regression results that with the inclusion of resourcefulness as a predictor, there are lower effects of data exposure on model performance."}}, {"heading_title": "Human RCT study", "details": {"summary": "While 'Human RCT study' isn't an exact heading, the paper details a human-conducted, randomized controlled trial (RCT) to validate their linguistic obfuscation approach. The core goal was to ensure that obfuscation, used to mitigate LLM memorization, doesn't inadvertently alter the inherent reasoning difficulty of the linguistic problems. The RCT involved human participants solving both original and obfuscated versions of linguistics Olympiad problems. The findings indicate a small performance dip for humans on obfuscated tasks, which is theorized due to unfamiliar orthographies causing a perceived increase in difficulty rather than fundamentally changing the reasoning. This implies that the obfuscation method is valid and sound as it was not intended to be unnecessarily more difficult. The study is very important because in order to validate the obfuscation methods that they created are useful, humans needed to be able to answer them correctly without much difficulty and the results showed this. **The goal is also to see if humans perform comparably on obfuscated problems.**"}}]