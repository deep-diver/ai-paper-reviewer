[{"Alex": "Hey everyone, welcome to the podcast! Today we're diving into a fascinating piece of research that asks: can AI *really* reason, or is it just *really* good at memorizing? We're gonna untangle that knot with linguistic puzzles and some seriously sneaky obfuscation. Joining me is Jamie, ready to pick my brain. Welcome, Jamie!", "Jamie": "Thanks, Alex! Super excited to delve into this. It sounds like a brain-buster!"}, {"Alex": "Alright, so the paper is titled 'LINGOLY-TOO: Disentangling Memorisation from Reasoning with Linguistic Templatisation and Orthographic Obfuscation.' Catchy, right? Basically, it\u2019s about figuring out if Large Language Models, or LLMs, are actually *thinking* when they solve problems, or if they're just spitting back stuff they've already seen during training.", "Jamie": "Okay, so like, are we talking about AI cheating on a test?"}, {"Alex": "Exactly! And the stakes are high, Jamie. If LLMs are just memorizing, then we're overestimating their actual problem-solving skills. This paper offers a framework to tell the difference.", "Jamie": "Hmm, interesting. So, how *do* you tell if an AI is just, like, a really good parrot?"}, {"Alex": "That's where the linguistic puzzles come in. The researchers used Linguistics Olympiad problems \u2013 these are challenges where you have to decipher the grammar of a totally unknown language.", "Jamie": "Ooh, that sounds hard even for a human!"}, {"Alex": "Tell me about it! But here\u2019s the kicker: they didn\u2019t just use the original problems. They *obfuscated* them. They changed the writing system of the made-up language, like, made it even MORE confusing in the new form, but kept the underlying logic the same. So same reasoning required, but looks completely different.", "Jamie": "Wait, so they made the problems harder on purpose?"}, {"Alex": "Precisely! Think of it like this: you know how you can solve a math problem even if it's written with different numbers? Same idea. If the AI can still solve the problem even with the funky new writing, it\u2019s more likely it's actually reasoning, not just remembering a specific pattern. That is the idea of this paper.", "Jamie": "Okay, I get it. So, what kind of fancy moves are used to create the 'funky new writing'?"}, {"Alex": "They used what they call 'orthographic templatisation.' Basically, they built a system to automatically generate endless variations of each question by changing the letters and writing systems of the problem languages. That way, the LLM hasn't ever seen that exact problem before.", "Jamie": "Ah, so it's not just swapping out words, it's like... changing the whole alphabet?"}, {"Alex": "In a way, yes! Imagine replacing all the vowels with symbols or using a completely different script. It keeps the core linguistic structure intact, but it's visually unrecognizable. They call the created benchmark \u201cLINGOLY-TOO\u201d.", "Jamie": "LINGOLY-TOO, it sounds kinda fun. So, they throw these super-mutated problems at the AI. What happened?"}, {"Alex": "Well, that's the interesting part. Even the most advanced LLMs struggled, which implies advanced reasoning is hard. The frontier models, including Claude 3.7 Sonnet, o1-preview and DeepSeek R1, really struggled with these new versions of linguistic problems.", "Jamie": "Ouch. So, the AI's weren't as smart as we thought?"}, {"Alex": "It\u2019s more nuanced than that. The research showed that prior data exposure indeed contributes to overestimating reasoning capabilities, but its not everything. The LLMs exhibited noticeable variance in accuracy across *permutations* of the same problem! That's mind-blowing.", "Jamie": "Permutations? You mean different versions of the *same* problem tripped them up?"}, {"Alex": "Exactly! And get this: they performed better on questions in their *original* orthography! It is an clear indication that, even if the AI sort of knows some reasoning steps, its ability relies much on having the very structure to be already seen and registered before during training.", "Jamie": "So they're, like, cheating but not *really* cheating?"}, {"Alex": "It\u2019s a gray area. It means we have to be *way* more careful about how we evaluate AI. Standard benchmarks might be giving us a false sense of security.", "Jamie": "Umm, so what does this mean for the future of AI research? Are we back to square one?"}, {"Alex": "Not at all! This research gives us a better way to test AI. We need benchmarks like LINGOLY-TOO that are resistant to memorisation.", "Jamie": "Okay, so more tricky tests for the AI. Got it."}, {"Alex": "Absolutely. And it highlights the need to dig deeper into *how* LLMs are generating responses. What's actually going on inside that 'black box'?", "Jamie": "Hmm, so like, reverse-engineering the AI's thought process?"}, {"Alex": "Precisely! Understanding the mechanisms behind response generation is crucial for building truly intelligent and reliable systems.", "Jamie": "This is way more complicated than I thought! But super interesting."}, {"Alex": "It is! And it touches on some really deep questions about what it means to reason, and what we expect from AI.", "Jamie": "Okay, Alex, so like, what's the big takeaway here? What should people remember about this LINGOLY-TOO thing?"}, {"Alex": "The key takeaway is that we can't take AI reasoning at face value. Just because an AI *solves* a problem doesn't mean it *understands* it.", "Jamie": "So, like, don't trust everything you see on the internet? Even from an AI?"}, {"Alex": "Exactly! Always be critical, always question. And remember that AI is a tool, and like any tool, it can be misused or misinterpreted. It's a tricky field.", "Jamie": "Well, that's a little bit scary. Thanks for spelling it all out for me. It makes you wonder."}, {"Alex": "Happy to, Jamie! Now, what are the next steps in the field to make it better in the future?", "Jamie": "It does feel like there's a bit of a crisis to address that we'll want to know."}, {"Alex": "Well, in the field, the goal is to develop new methods to evaluate AI, create new and better datasets that avoid memorisation, and focus our attention on discovering what is reasoning at a high level of the models. But that's a topic for another podcast. This research is a critical reminder: we need robust methods to tease apart genuine reasoning from clever mimicry. Thanks for joining me, Jamie!", "Jamie": "My pleasure, Alex! It's been a mind-blowing conversation."}]