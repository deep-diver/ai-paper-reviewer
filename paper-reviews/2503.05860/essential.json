{"importance": "This paper is important for researchers because it **addresses critical gaps in AI4SE benchmark quality and accessibility**. The BenchFrame and BenchScout tools offers an approach to enhance existing benchmarks and streamline benchmark selection which **ensures more reliable and relevant evaluations of AI models**.", "summary": "This paper reviews AI4SE benchmarks, introduces BenchScout for benchmark discovery, and proposes BenchFrame for benchmark enhancement, demonstrated via HumanEvalNext.", "takeaways": ["Identified and classified 204 AI4SE benchmarks, revealing limitations and gaps.", "Introduced BenchScout, a semantic search tool for AI4SE benchmarks, enhancing usability, effectiveness, and intuitiveness.", "Proposed BenchFrame, a unified method to enhance benchmark quality, demonstrated through HumanEvalNext, improving benchmark reliability."], "tldr": "The integration of AI into Software Engineering (AI4SE) has led to numerous benchmarks for tasks like code generation. However, this surge presents challenges such as scattered benchmark knowledge, difficulty in selecting relevant benchmarks, absence of a uniform standard for benchmark development, and limitations of existing benchmarks. Addressing these issues is critical for accurate evaluation and comparison of AI models in software engineering.\n\nThis paper introduces **BenchScout**, a semantic search tool, to find relevant benchmarks and **BenchFrame**, a unified method to enhance benchmark quality. BenchFrame is applied to the HumanEval benchmark, leading to **HumanEvalNext**, featuring corrected errors, improved language conversion, expanded test coverage, and increased difficulty. The paper then evaluates ten state-of-the-art code language models on HumanEvalNext.", "affiliation": "Delft University of Technology", "categories": {"main_category": "Machine Learning", "sub_category": "Deep Learning"}, "podcast_path": "2503.05860/podcast.wav"}