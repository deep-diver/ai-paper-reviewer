{"references": [{"fullname_first_author": "M. Chen", "paper_title": "Evaluating large language models trained on code", "publication_date": "2021-01-01", "reason": "This paper introduces the HumanEval benchmark, a widely used dataset for evaluating code generation capabilities of large language models."}, {"fullname_first_author": "J. Austin", "paper_title": "Program synthesis with large language models", "publication_date": "2021-01-01", "reason": "This paper introduces the MBPP benchmark, another popular dataset for evaluating code generation capabilities."}, {"fullname_first_author": "P. Yin", "paper_title": "Learning to mine aligned code and natural language pairs from stack overflow", "publication_date": "2018-01-01", "reason": "This paper describes a method for creating a dataset of aligned code and natural language pairs from Stack Overflow, enabling text-to-code models."}, {"fullname_first_author": "X. Hu", "paper_title": "Deep code comment generation", "publication_date": "2018-01-01", "reason": "This paper explores techniques for automatically generating code comments using neural networks, influencing research on code understanding."}, {"fullname_first_author": "T. Yu", "paper_title": "SParC: Cross-Domain Semantic Parsing in Context", "publication_date": "2019-01-01", "reason": "This paper introduces a benchmark for context-dependent semantic parsing of SQL queries in complex, cross-domain settings."}]}