[{"figure_path": "https://arxiv.org/html/2502.17414/x2.png", "caption": "Figure 1: We present\u00a0X-Dancer, a unified transformer-diffusion framework for zero-shot, music-driven human image animation from a single static image, capable of handling diverse body forms and appearances. Our method enables the synthesis of highly expressive and diverse full-body dance motions that are synchronized with music, including detailed movements at the head and hands, which are then seamlessly translated into vivid and lifelike dance videos. Code and model will be available for research purposes.", "description": "Figure 1 showcases X-Dancer's capabilities.  Given a single, static image of a person, X-Dancer generates a full-body dance video synchronized to an input music track.  The system handles diverse body types and appearances, producing highly expressive and realistic movements, including detailed hand and head motions.  The results are seamless, lifelike dance videos. The code and model will be made available for research.", "section": "Abstract"}, {"figure_path": "https://arxiv.org/html/2502.17414/x3.png", "caption": "Figure 2: Overview of\u00a0X-Dancer.\nWe propose a cross-conditional transformer model to autoregressively generate 2D human poses synchronized with input music, followed by a diffusion model that produces high-fidelity videos from a single reference image IR.subscript\ud835\udc3c\ud835\udc45I_{R}.italic_I start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT . First, we develop a multi-part compositional tokenization for 2D poses, encoding and quantizing each body part independently with keypoint confidence. These tokens are then merged into a whole-body, confidence-aware pose using a shared decoder. Next, we train a GPT-based transformer to autoregressively predict future pose tokens with causal attention, conditioned on past poses and aligned music embeddings. For global music style and motion context, we incorporate the entire music sequence and sampled prior poses. With a learnable motion decoder, we generate multi-scale spatial pose guidance upsampled from a learned feature map, incorporating the generated motion tokens within a temporal window (16 frames) using AdaIN. By co-training the motion decoder and temporal modules, our diffusion model is capable of synthesizing temporally smooth and high-fidelity video frames, while maintaining consistent appearance with the reference image with a trained reference net.", "description": "X-Dancer uses a two-stage approach.  First, a transformer model processes music and generates a sequence of 2D human poses, broken down by body part (head, hands, upper body, lower body) for more nuanced control, and accounting for keypoint confidence to handle pose uncertainties.  This model incorporates global musical context and prior poses for better temporal coherence.  Second, a diffusion model uses these poses to animate a single reference image, leveraging a learnable motion decoder and AdaIN for high-quality, temporally smooth video generation that preserves the original image's appearance.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2502.17414/x4.png", "caption": "Figure 3: Qualitative Comparisons. Among all the methods, X-Dancer\u00a0achieves the most expressive and high-fidelity human dance video synthesis, maintaining the highest consistency with both the reference human characteristics and the background scene.", "description": "Figure 3 presents a qualitative comparison of dance video generation results from different methods: X-Dancer, Hallo, Bailando + PoseGuider, and EDGE + PoseGuider.  The figure visually demonstrates that X-Dancer produces the most expressive and high-fidelity dance videos. Importantly, X-Dancer's generated videos maintain a high degree of consistency with the characteristics of the reference image (the person's appearance and body shape) as well as the background of the original scene.", "section": "4. Experiments"}]