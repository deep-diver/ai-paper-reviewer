[{"figure_path": "https://arxiv.org/html/2503.13082/x2.png", "caption": "Figure 1: To enable a human to command a robot using free-form language instructions, our method leverages the world knowledge of Vision-Language Models to interpret instructions and reason about object spatial relationships.\nThis is important when the target object (\u2605) is not directly graspable, requiring the robot to first identify and remove obstructing objects (\u25cf).\nBy optimizing the sequence of actions, our approach ensures efficient task completion.", "description": "This figure demonstrates the process of free-form language-based robotic grasping.  A human provides a natural language instruction to the robot. The robot uses a Vision-Language Model (VLM) to understand the instruction and reason about the spatial relationships between objects in a cluttered bin. If the target object is not directly graspable (indicated by a star), the VLM identifies and removes obstructing objects (indicated by circles) before grasping the target object. The robot optimizes the order of actions to complete the task efficiently.", "section": "I. INTRODUCTION"}, {"figure_path": "https://arxiv.org/html/2503.13082/x3.png", "caption": "Figure 2: FreeGrasp pipeline. (a) The setup considered for the robotic reasoning and grasping task and (b) the proposed pipeline that leverages pre-trained VLMs in a zero-shot manner without additional training.", "description": "Figure 2 illustrates the FreeGrasp system.  (a) shows the physical setup: a robotic arm with a two-finger gripper in front of a bin containing various cluttered objects; an RGB-D camera observes the scene from above. (b) details the FreeGrasp pipeline, a modular process starting with the user providing a free-form language instruction. The system then uses a Vision-Language Model (VLM) in a zero-shot manner (no model training on this specific task) to interpret the instruction and reason about the object locations and spatial relationships. Object localization identifies all visible items, and mark-based visual prompting enhances the VLM's reasoning accuracy.  The VLM then determines which object to grasp first, considering obstructions. Object segmentation isolates the chosen object, grasp estimation finds an appropriate grasp pose, and the robotic arm executes the grasp. This iterative process continues until the target object is successfully grasped. ", "section": "III. OUR METHOD"}, {"figure_path": "https://arxiv.org/html/2503.13082/x4.png", "caption": "Figure 3: Object localization performance with different VLM-based method on MetaGrasNetv2\u00a0[16].", "description": "The figure compares the performance of three different methods for object localization using the MetaGraspNetV2 dataset. The methods are: using GPT-40 and LangSAM, GPT-40, LangSAM, and post-processing, and using Molmo.  The comparison is based on three metrics: Average Precision (AP), Average Recall (AR), and F1 score.  The results show that Molmo outperforms the other two methods, achieving higher scores in AP, AR, and F1. This suggests that Molmo is a more effective method for object localization in cluttered scenes.", "section": "III. OUR METHOD. B. Vision-language grasp reasoning. Object localization"}, {"figure_path": "https://arxiv.org/html/2503.13082/x5.png", "caption": "Figure 4: Examples of FreeGraspData at different task difficulties with three user-provided instructions. \u2605 indicates the target object, and \u25cf indicates the ground-truth objects to pick.", "description": "Figure 4 presents example scenarios from the FreeGraspData dataset, categorized by task difficulty (Easy, Medium, Hard) and the presence or absence of object ambiguity. Each scenario shows a cluttered bin with objects, illustrating different levels of obstruction to reach the target object. Three distinct user-provided instructions are shown for each scenario, demonstrating the variability in natural language descriptions. The target object (\u2605) and the ground-truth sequence of objects to grasp (\u25cf) are marked to indicate the task complexity and demonstrate how the robot should plan its actions.", "section": "IV. FREE-FORM LANGUAGE GRASPING DATASET"}, {"figure_path": "https://arxiv.org/html/2503.13082/x6.png", "caption": "Figure 5: Similarity distribution among the three user-defined instructions used in the FreeGraspData scenarios.", "description": "This figure displays the distribution of similarity scores among the three distinct user-provided instructions for each scenario within the FreeGraspData dataset.  The similarity is measured using three different metrics: GPT score (measuring GPT-40's ability to interpret instructions consistently), embedding score (measuring semantic similarity using Sentence-BERT), and ROUGE-L score (measuring structural similarity).  The data is broken down based on the difficulty level (Easy, Medium, Hard) and the presence/absence of object ambiguity in the scene.  This visualization helps to quantify the variations in human language instructions used in the dataset, and to examine how well a language model (like GPT-40) handles this variation.", "section": "IV. FREE-FORM LANGUAGE GRASPING DATASET"}]