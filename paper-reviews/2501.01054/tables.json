[{"content": "| Method | Llama3-8B | Llama3-70B | GPT-3.5 | GPT-4o-m |\n|---|---|---|---|---|\n| **HumanEval Plus** |  |  |  |  |\n| Vanilla | 53.58 | 73.74 | 67.83 | 82.96 |\n| Grading RM | 62.20<sub> +8.62</sub> | 75.00<sub> +1.26</sub> | 70.12<sub> +2.29</sub> | 83.50<sub> +0.54</sub> |\n| MBR-Exec | 60.30<sub> +6.72</sub> | 75.80<sub> +2.06</sub> | 70.60<sub> +2.77</sub> | 85.20<sub> +2.24</sub> |\n| CodeT | 65.30<sub> +11.72</sub> | 76.20<sub> +2.46</sub> | 73.89<sub> +6.06</sub> | 85.30<sub> +2.34</sub> |\n| MPSC | 59.72<sub> +6.14</sub> | 75.51<sub> +1.77</sub> | 72.76<sub> +4.93</sub> | 84.82<sub> +1.86</sub> |\n| Llama3.1-70B | **72.04**<sub> **+18.46**</sub> | <ins>78.54</ins><sub> +4.80</sub> | **79.76**<sub> **+11.93**</sub> | <ins>85.45</ins><sub> +2.49</sub> |\n| CodeRM-8B | <ins>72.01</ins><sub> +18.43</sub> | **78.69**<sub> **+4.95**</sub> | <ins>78.01</ins><sub> +10.18</sub> | **86.38**<sub> **+3.42**</sub> |\n| **MBPP Plus** |  |  |  |  |\n| Vanilla | 49.20 | 69.33 | 70.53 | 71.59 |\n| Grading RM | 48.40<sub> -0.80</sub> | 70.60<sub> +1.27</sub> | 66.67<sub> -3.86</sub> | 69.00<sub> -2.59</sub> |\n| MBR-Exec | 50.00<sub> +0.80</sub> | 69.80<sub> +0.47</sub> | 70.53<sub> +0.00</sub> | 72.30<sub> +0.71</sub> |\n| CodeT | 59.20<sub> +10.00</sub> | 69.90<sub> +0.57</sub> | 69.92<sub> -0.61</sub> | 73.40<sub> +1.81</sub> |\n| MPSC | 53.32<sub> +4.12</sub> | 70.91<sub> +1.58</sub> | 71.59<sub> +1.06</sub> | 73.20<sub> +1.61</sub> |\n| Llama3.1-70B | <ins>65.26</ins><sub> +16.06</sub> | <ins>71.85</ins><sub> +2.52</sub> | <ins>75.72</ins><sub> +5.19</sub> | <ins>74.96</ins><sub> +3.37</sub> |\n| CodeRM-8B | **66.71**<sub> **+17.51**</sub> | **72.44**<sub> **+3.11**</sub> | **75.96**<sub> **+5.43**</sub> | **75.20**<sub> **+3.61**</sub> |\n| **LiveCodeBench** |  |  |  |  |\n| Vanilla | 11.98 | 25.30 | 20.55 | 34.83 |\n| Grading RM | 13.10<sub> +1.12</sub> | 26.19<sub> +0.89</sub> | 20.83<sub> +0.28</sub> | 36.31<sub> +1.48</sub> |\n| MBR-Exec | 12.04<sub> +0.06</sub> | 25.37<sub> +0.07</sub> | 20.52<sub> -0.03</sub> | 34.83<sub> +0.00</sub> |\n| CodeT | 12.61<sub> +0.63</sub> | 25.89<sub> +0.59</sub> | 20.58<sub> +0.03</sub> | 35.13<sub> +0.30</sub> |\n| MPSC | 11.98<sub> +0.00</sub> | 25.30<sub> +0.00</sub> | 20.55<sub> +0.00</sub> | 34.83<sub> +0.00</sub> |\n| Llama3.1-70B | <ins>13.28</ins><sub> +1.30</sub> | **28.46**<sub> **+3.16**</sub> | **22.80**<sub> **+2.25**</sub> | <ins>38.60</ins><sub> +3.77</sub> |\n| CodeRM-8B | **15.21**<sub> **+3.23**</sub> | <ins>27.73</ins><sub> +2.43</sub> | <ins>21.76</ins><sub> +1.21</sub> | **39.20**<sub> **+4.37**</sub> |", "caption": "Table 1: \nThe main result of our approach and other baselines over three code generation benchmarks.\nGPT-4o-m stands for GPT-4o-mini.\nThe improvements are calculated between methods and vanilla.\nThe top two performances for each dataset and policy model are marked in bold and underlined.", "description": "This table presents a comparison of the performance of various methods for code generation across three different benchmarks: HumanEval Plus, MBPP Plus, and LiveCodeBench.  The methods compared include a baseline (vanilla) and several other approaches (Grading RM, MBR-Exec, CodeT, MPSC, Llama3.1-70B and the authors' proposed CodeRM-8B). Performance is evaluated using four different Language Models (LLMs) as policy models (Llama3-8B, Llama3-70B, GPT-3.5, and GPT-40-mini).  The table shows the percentage of problems solved by each method,  with improvements over the vanilla method calculated and highlighted for the top-performing methods on each dataset.  GPT-40-mini is noted separately from the main GPT-40 model.", "section": "4. Experiments"}, {"content": "| Model | Acc (<mo stretchy=\"false\">\u2191</mo>) | F1 (<mo stretchy=\"false\">\u2191</mo>) | FAR (<mo stretchy=\"false\">\u2193</mo>) | FRR (<mo stretchy=\"false\">\u2193</mo>) |\n|---|---|---|---|---|\n| Llama3.1-8B | 60.02 | 44.97 | 13.66 | 46.13 |\n| Llama3.1-70B | 73.65 | 70.15 | 11.10 | 34.51 |\n| CodeRM-8B (Ours) | 69.64 | 63.63 | 11.17 | 38.55 |\n| Llama3.1-8B | 74.21 | 74.35 | 20.44 | 30.55 |\n| Llama3.1-70B | 78.30 | 78.76 | 17.19 | 25.97 |\n| CodeRM-8B (Ours) | 80.46 | 81.27 | 16.48 | 22.71 |", "caption": "Table 2: The quality of individual unit tests and the combination of multiple unit tests on HumanEval Plus, utilizing Llama3.1-8B as the policy model.\nThe top two performances are highlighted using bold and underlining.", "description": "This table presents a quantitative evaluation of the quality of unit tests generated by different models. It compares the performance of individual unit tests and the combined effect of multiple unit tests (using a majority voting approach) in identifying correct code solutions on the HumanEval Plus benchmark.  The results are specifically shown for Llama3.1-8B as the model generating code solutions (policy model).  The table highlights metrics such as accuracy, F1-score, false acceptance rate (FAR), and false rejection rate (FRR) to illustrate the effectiveness of the unit test generation models.  The top two performing models are highlighted for each metric.", "section": "4.3 Quality of Generated Unit Tests"}, {"content": "| Method | HumanEval+ | MBPP+ |\n|---|---|---|\n| zero-shot | 66.67 | 63.27 |\n| training wo / quality control | 69.71<sub>+3.04</sub> | 64.96<sub>+1.69</sub> |\n| training w / quality control | 71.09<sub>+4.42</sub> | 66.31<sub>+3.04</sub> |", "caption": "Table 3: The effects of synthetic data quality control.", "description": "This table presents the results of an ablation study evaluating the impact of synthetic data quality control on the performance of the CodeRM-8B model.  It shows the effects of applying quality control during the creation of synthetic training data for the unit test generator, comparing metrics (Accuracy, F1-score, False Acceptance Rate (FAR), False Rejection Rate (FRR)) with and without the quality control step.  The comparison is done using the Llama3.1-8B and Llama3.1-70B models, showcasing how quality control enhances the reliability and precision of the generated unit tests.", "section": "3 Towards Efficient and High-Quality Unit Test Scaling"}, {"content": "| Hyperparameters | Value |\n|---|---| \n| Temperature | 0.8 |\n| Top P | 0.95 |\n| Frequency Penalty | 0 |\n| Presence Penalty | 0 |", "caption": "Table 4: The hyperparameters of LLMs for solution and unit test generation.", "description": "This table lists the hyperparameters used for tuning the large language models (LLMs) employed in the paper.  These parameters control aspects of the LLMs' behavior during both code solution generation and unit test generation.  Specifically, it shows the values set for temperature, top-p (nucleus sampling), frequency penalty, and presence penalty.  These parameters influence the randomness and creativity of the model's output, and the values chosen reflect a balance between exploration and exploitation.", "section": "3.1 Unit Test Generator"}]