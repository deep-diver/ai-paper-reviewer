[{"figure_path": "https://arxiv.org/html/2411.04335/x2.png", "caption": "Figure 1: \nGazeGen. (1) User\u2019s View: Overview of the user\u2019s view, setting the context for gaze estimation (input: user\u2019s eye images) and visual editing (inputs: user\u2019s view and predicted gaze point). (2) Real-Time Gaze Estimation: The DFT Gaze Agent (281KB storage) predicts the user\u2019s gaze point (green) aligned with the ground-truth gaze (red). (3) Gaze-Driven Visual Content Generation/Detection: Predicted gaze is used for editing () objects, detecting () objects, or creating animations () based on the user\u2019s focus (). GazeGen sets a new standard for gaze-driven visual content generation, enhancing user experience and positioning users as visual creators.", "description": "Figure 1 illustrates the GazeGen system, showing the user's perspective, real-time gaze estimation, and gaze-driven visual content generation.  The User's View shows the user's visual field and the system's input. Real-time Gaze Estimation displays the DFT Gaze Agent's process of predicting gaze, comparing it to ground truth. Gaze-driven Visual Content Generation/Detection demonstrates how predicted gaze drives actions like object editing, detection, and animation creation.  GazeGen improves user experience by making visual content generation more accessible and intuitive.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2411.04335/x3.png", "caption": "Figure 2: \n\nExtended applications of gaze-driven interaction with GazeGen.\n(1) Real-Time Gaze Estimation: Continuous tracking of eye movements for precise gaze estimation.\n(2) Gaze-Driven Detection: Detecting and identifying objects based on where the user is looking.\n(3) Gaze-Driven Image Editing: Dynamic editing tasks such as Addition (adding objects based on the user\u2019s gaze), Deletion/Replacement (removing or replacing objects based on the user\u2019s gaze), Reposition (move objects by first gazing at the initial position, then the new position), and Material Transfer (change an object\u2019s style or texture by first gazing at a reference object, then applying the style to the target object).\n(4) Gaze-Driven Video Generation: Creating and manipulating video content driven by the user\u2019s gaze.", "description": "Figure 2 illustrates the four main applications enabled by GazeGen's gaze-driven user interaction.  It showcases real-time gaze tracking for precise estimation (1), object detection based on gaze direction (2), dynamic image manipulation through gaze-controlled addition, deletion, replacement, repositioning, and material transfer (3), and finally, gaze-driven video generation and manipulation (4).", "section": "Extended applications of gaze-driven interaction with GazeGen"}, {"figure_path": "https://arxiv.org/html/2411.04335/x4.png", "caption": "Figure 3: \n\nGaze-driven visual content generation. This diagram shows the process starting from the user\u2019s eye, where the gaze estimation agent determines the gaze point. The gaze point is used to get the editing region, which can be toggled to use either a box or a mask. The T2I (Text-to-Image) and T2V (Text-to-Video) modules then generate visual content based on the selected editing region. The On/Off switches indicate whether the box or mask is used for gaze-driven editing.", "description": "This figure illustrates the GazeGen system's workflow for gaze-driven visual content generation.  It begins with the user's eye image, which is processed by a gaze estimation agent to pinpoint the user's gaze. This gaze point then defines an editing region, selectable as either a box or a mask.  The selected region and the user's gaze are then fed into Text-to-Image (T2I) and Text-to-Video (T2V) modules which generate new visual content based on that selected region.  The user can switch between box and mask selection using On/Off toggles, providing flexibility in the editing process.", "section": "3 GazeGen"}, {"figure_path": "https://arxiv.org/html/2411.04335/x5.png", "caption": "Figure 4: \n\nSelf-supervised distillation for a compact model. Using ConvNeXt V2-A (Woo et\u00a0al. 2023) as the teacher network, we create a downsized student network. The first stage of the student model inherits weights from the teacher, while stages 2 to 4 reduce the channel dimensions to one-fourth. Distinct decoders are used to reconstruct both input images and the teacher\u2019s intermediate features. The student processes masked inputs, allowing it to emulate the teacher\u2019s deep understanding of visual data and align with how the teacher perceives and interprets these images. For simplicity, the diagram only illustrates the reconstruction of the teacher\u2019s features to emulate knowledge.", "description": "This figure illustrates the process of self-supervised knowledge distillation used to create a compact gaze estimation model.  A large, complex teacher model (ConvNeXt V2-A) is used to train a smaller, faster student model.  The student model's architecture is simplified by reducing the channel dimensions in later stages.  Importantly, the student model learns to reconstruct both the original input images and the intermediate feature representations from the teacher network. This dual reconstruction process allows the student model to mimic the teacher's understanding of visual data without needing to train on the same large dataset.  The diagram simplifies the visualization by focusing only on the feature reconstruction aspect of the process.", "section": "3 Self-Supervised Compact Model Distillation"}, {"figure_path": "https://arxiv.org/html/2411.04335/extracted/5982601/figs/latency_plot.png", "caption": "Figure 5: \n\nQualitative results on AEA\u00a0dataset. First row: user\u2019s eye. Second row: eye tracking (left) and gaze-driven object detection (right). Predicted gaze (green), ground-truth gaze (red). \nBest viewed in Acrobat Reader; click images to play animations.", "description": "Figure 5 presents qualitative results obtained using the AEA dataset. The first row shows images of a user's eye. The second row displays two key aspects: on the left, real-time eye tracking showing the user's gaze; on the right, objects being detected based on the user's gaze, with the predicted gaze highlighted in green and the ground-truth gaze in red.  For a more dynamic viewing experience, the authors suggest viewing the figure using Acrobat Reader, where clicking on the images will play embedded animations.", "section": "4.4 Qualitative Results"}, {"figure_path": "https://arxiv.org/html/2411.04335/extracted/5982601/figs/limitations/loc1_script2_seq8_rec1_eyetracking_00929.png", "caption": "Figure 6: \n\nQualitative results for gaze-driven image editing. The tasks include: Addition (first row): Adding objects like a lantern, basket, or photo. Deletion/Replacement (second row): Replacing objects with items like a curtain, aquarium, or galaxy. Reposition (third row): Moving objects such as a wall decoration to the upper left corner, books to the lower left corner, or a phone upward. Material Transfer (last row): Changing an object\u2019s style, such as polished wood to the fridge, woven wicker to the washing machine, or polished metal to the chopping board. All edits are based on the user\u2019s gaze.", "description": "Figure 6 presents qualitative results demonstrating gaze-driven image editing capabilities.  The figure showcases four distinct types of image manipulations controlled solely by the user's gaze:  Addition involves adding new objects (like a lantern, basket, or photo) to the scene.  Deletion/Replacement allows for replacing existing objects with entirely different ones (a curtain replacing a window, an aquarium replacing a bookshelf, or a galaxy replacing a painting).  Reposition enables users to move objects within the scene simply by gazing at the desired new location (for example, shifting wall decorations, books, or a phone to a different corner). Lastly, Material Transfer lets users change the texture or material appearance of objects (e.g., applying the texture of polished wood to a fridge, woven wicker to a washing machine, or polished metal to a cutting board).  All actions are directly driven by the user's gaze, providing an intuitive and hands-free method of image editing.", "section": "3.4 Gaze-Driven Visual Content Generation"}, {"figure_path": "https://arxiv.org/html/2411.04335/extracted/5982601/figs/limitations/loc3_script2_seq3_rec2_eyetracking_00018.png", "caption": "Figure 7: \n\nQualitative results for gaze-driven video generation. Objects are replaced based on users\u2019 gaze with animated objects. \nBest viewed in Acrobat Reader; click images to play animations.\nZoom in for a better view.", "description": "Figure 7 showcases the dynamic video generation capabilities of GazeGen.  The system replaces static objects within a scene with animated counterparts, driven entirely by the user's gaze.  Four examples are presented: a river, a starry night sky, a vibrant aquarium, and a tranquil underwater scene. Each image shows how GazeGen interprets the user's gaze and seamlessly integrates animated replacements, illustrating its real-time responsiveness and ability to produce engaging visual content.  The animation effect is best observed using Acrobat Reader.", "section": "3.4 Gaze-Driven Visual Content Generation"}, {"figure_path": "https://arxiv.org/html/2411.04335/extracted/5982601/figs/limitations/failure_gaze_replacement.png", "caption": "Figure 8: \n\nModel latency comparison on Raspberry Pi 4. The figure compares the latency of two gaze estimation models: ConvNeXt V2-A (Teacher) and DFT Gaze (Student). ConvNeXt V2-A shows a latency of 928.84 ms, while DFT Gaze reduces latency to 426.66 ms, demonstrating its efficiency for real-time applications on edge devices.", "description": "This figure showcases the performance comparison of two gaze estimation models, ConvNeXt V2-A and DFT Gaze, on a Raspberry Pi 4.  ConvNeXt V2-A, the larger model, exhibits a latency of 928.84 milliseconds (ms), while DFT Gaze, a smaller model, achieves a significantly reduced latency of 426.66 ms.  The comparison highlights the efficiency of the DFT Gaze model for real-time applications on resource-constrained devices like the Raspberry Pi 4.", "section": "4.3 Gaze Estimation Latency on Edge Device"}]