[{"content": "| Track | Detections |\n|---|---| \n| <img src=\"https://arxiv.org/html/2411.04335/videos/track/l2s4s7r1/0116.png\" width=\"0.45\\textwidth\"> | <img src=\"https://arxiv.org/html/2411.04335/videos/dets/l1s2s6r1/0116.png\" width=\"0.45\\textwidth\"> |", "caption": "Table 1: \n\nComparison of state-of-the-art methods for generalized gaze estimation using within-dataset evaluation. To ensure a fair comparison, we reimplement these methods and apply the same K-means clustering with 15 groups as DFT Gaze during training. We follow the original hyperparameter settings specified in these methods.", "description": "This table compares the performance of several state-of-the-art generalized gaze estimation methods.  The comparison is based on within-dataset evaluation using the AEA and OpenEDS2020 datasets. To ensure a fair comparison, the authors reimplemented the methods, using the same K-means clustering (with 15 groups) and hyperparameter settings as their proposed DFT Gaze method. The table shows the number of parameters, the number of tunable parameters, and the mean angular error (in degrees) achieved by each method on the specified datasets.", "section": "4 Experiments"}, {"content": "| (Eye Tracking) | (Object Detection) |\n|---|---|", "caption": "Table 2: \n\nComparison of state-of-the-art methods for personalized gaze estimation using within-dataset evaluation. To ensure a fair comparison, we reimplement these methods and apply the same K-means clustering with 15 groups as DFT Gaze during training. We follow the original hyperparameter settings specified in these methods. The symbol \u2020\u2020\\dagger\u2020 represents source-free unsupervised domain adaptation (UDA) methods.", "description": "This table compares the performance of several state-of-the-art methods for personalized gaze estimation.  The methods were re-implemented to ensure a fair comparison by using the same K-means clustering (15 groups) and hyperparameters as the DFT Gaze method.  Results are shown for the within-dataset evaluation. The dagger symbol (\u2020) indicates methods that utilize source-free unsupervised domain adaptation (UDA). The table facilitates a quantitative assessment of DFT Gaze's performance relative to existing techniques in personalized gaze estimation.", "section": "4 Experiments"}, {"content": "| Image 1 | Image 2 | Image 3 |\n|---|---|---|\n| https://arxiv.org/html/2411.04335/8videos/animation/l5s4s2r1_00585/0116.png | https://arxiv.org/html/2411.04335/8videos/animation/l5s5s1r1_00283/0116.png | https://arxiv.org/html/2411.04335/8videos/animation/l5s4s2r1_00111/0116.png |", "caption": "Table 3: \n\nGeneralized and personalized gaze Estimation results. The teacher model, ConvNeXt V2-A, with 3.6M parameters, excels in both generalization and personalization, achieving superior performance across all datasets. The student model, DFT Gaze, with only 281K parameters, shows minimal performance drop, maintaining competitive levels in both settings. Despite its compact size, the student model provides robust gaze estimation within a streamlined framework, demonstrating its efficiency and effectiveness.", "description": "This table presents a comparison of the performance of two gaze estimation models: the teacher model (ConvNeXt V2-A) and the student model (DFT Gaze).  It shows the mean angular error (in degrees) for both generalized (trained on a large, general dataset) and personalized (fine-tuned on a small, user-specific dataset) gaze estimation on two benchmark datasets (AEA and OpenEDS2020). The results demonstrate that the significantly smaller student model (DFT Gaze, 281K parameters) achieves comparable accuracy to the larger teacher model (ConvNeXt V2-A, 3.6M parameters) in both generalized and personalized settings, highlighting its efficiency and robustness.", "section": "4 Experiments"}]