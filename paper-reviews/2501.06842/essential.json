{"importance": "This paper is crucial for researchers working with LLMs because it addresses the significant challenge of training instability caused by gradient spikes.  **Its proposed SPAM optimizer offers a practical and effective solution to enhance training stability and resource efficiency**, which is critical for scaling up LLM training.  The findings on gradient spike behavior also provide valuable insights for future research on LLM optimization and architecture design.", "summary": "SPAM optimizer tackles LLM training instability by resetting momentum and clipping gradient spikes, significantly improving stability and efficiency.", "takeaways": ["Gradient spikes during LLM training are prevalent and significantly hinder model performance.", "The proposed SPAM optimizer effectively mitigates gradient spikes by employing momentum reset and spike-aware clipping.", "SPAM demonstrates consistent improvements in training stability and resource efficiency across various LLM sizes and tasks."], "tldr": "Large Language Model (LLM) training suffers from instability due to frequent gradient and loss spikes, leading to inefficient checkpoint recovery and restarts.  These spikes, sometimes 1000x larger than normal, disrupt the learning process and significantly degrade model performance. Existing methods like checkpoint recovery are costly and inefficient. \nThis paper introduces SPAM (Spike-Aware Adam with Momentum Reset), a novel optimizer designed to counter these spikes.  **SPAM uses momentum reset to eliminate harmful spike accumulation and spike-aware gradient clipping to manage their magnitude**.  Extensive experiments across various LLM sizes (60M-1B parameters), tasks (pre-training, fine-tuning, reinforcement learning), and quantization levels showed that SPAM consistently outperforms existing optimizers like Adam and Adafactor, while also enabling memory-efficient training.", "affiliation": "University of Exeter", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}, "podcast_path": "2501.06842/podcast.wav"}