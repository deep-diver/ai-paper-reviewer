[{"content": "| 2.5 **Method** | **Backbone** | **Input** | **ImgNet** | **Params** | **#F** | **Epoch** | **Gym99 5%** | **Gym99 10%** | **Gym288 5%** | **Gym288 10%** | **Diving 5%** | **Diving 10%** |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| 2 MemDPC (ECCV\u201920) [Han, Xie, and Zisserman 2020] | 3D-ResNet-18 | V | \u2717 | 15.4M | 16 | 500 | 10.8 | 24.1 | 14.5 | 21.3 | 54.3 | 62.0 |\n| LTG (CVPR\u201922) [Xiao et al. 2022] | 3D-ResNet-18 | VG | \u2717 | 68.3M | 8 | 180 | 34.3 | 45.8 | 16.2 | 38.7 | 59.8 | 64.3 |\n| SVFormer (CVPR\u201923) [Xing et al. 2023] | ViT-B | V | \u2713 | 121.4M | 8 | 30 | 31.4 | 47.9 | 21.3 | 39.6 | 59.1 | 70.8 |\n| SeFAR-S (Ours) | VIT-S | V | \u2713 | 31.2M | 8 | 30 | 36.7 | 56.3 | 27.8 | 46.9 | 72.2 | 78.4 |\n| SeFAR-B (Ours) | VIT-B | V | \u2713 | 122.1M | 8 | 30 | 39.0 | 56.9 | 28.3 | 48.1 | 72.8 | 80.9 |", "caption": "Table 1: Comparison with state-of-the-art semi-supervised action recognition methods on fine-grained datasets. We employ SeFAR with a sampling combination of {2-2-4}. The primary evaluation metric is top-1 accuracy. In this table, \u201cV\u201d within \u201cInput\u201d denotes RGB video, while \u201cG\u201d represents temporal gradients. \u201cImgNet\u201d indicates the utilization of models pre-trained on ImageNet\u00a0(Russakovsky et\u00a0al. 2015), while \u201c#F\u201d signifies the number of input frames. The labeling rates of the data are indicated by \u201c5%\u201d, \u201c10%\u201d, and \u201c20%\u201d in the datasets. The best results are highlighted in Bold, and the second-best Underlined.", "description": "Table 1 presents a comparison of SeFAR's performance against other state-of-the-art semi-supervised action recognition methods on fine-grained datasets.  The key metric used for comparison is top-1 accuracy.  Different experimental settings are considered, including variations in the input data (RGB video, temporal gradients), the use of ImageNet pre-trained models, the number of input frames processed, and different data labeling rates (5%, 10%, 20%).  SeFAR uses a specific sampling combination of {2-2-4}. The table highlights the best and second-best results for easy identification.", "section": "Experiments"}, {"content": "| Method | UB (10%) | UB (20%) | FX (10%) | FX (20%) | 10m (10%) | 10m (20%) |\n|---|---|---|---|---|---|---|\n| 2.5 Method | **UB** |  | **FX** |  | **10m** |  |\n|  | **10%** | **20%** | **10%** | **20%** | **10%** | **20%** |\n| 2 MemDPC | 20.7 | 19.1 | 13.8 | 15.9 | 65.4 | 71.2 |\n| LTG | 50.5 | 60.5 | 19.6 | 21.6 | 75.2 | 83.5 |\n| SVFormer | 52.9 | 66.8 | 20.1 | 28.8 | 73.8 | 85.9 |\n| SeFAR-S (Ours) | **56.9** | **73.8** | **23.8** | **42.9** | **85.5** | **94.0** |\n| SeFAR-B (Ours) | **58.5** | **75.5** | **27.6** | **44.2** | **87.4** | **94.6** |\n| 2.5  |  |  |  |  |  |  |", "caption": "(a) Results of elements across all events.", "description": "This table presents the performance of different semi-supervised action recognition methods on fine-grained datasets.  It shows the top-1 accuracy achieved by each method across various data subsets (5%, 10%, and 20% labeled data) for three different fine-grained action recognition tasks: Gym99, Gym288 (both gymnastics), and Diving.  The results are broken down into (a) results considering all actions across the entire dataset and (b) results examining elements only within a single event.  It allows for a comparison of performance across different methods under varying data conditions and action complexities.", "section": "Experiment Setup"}, {"content": "| Method | UB-S1 (10%) | UB-S1 (20%) | FX-S1 (10%) | FX-S1 (20%) | 5253B (10%) | 5253B (20%) |\n|---|---|---|---|---|---|---|\n| 2.5 Method | **UB-S1** |  | **FX-S1** |  | **5253B** |  |\n|  | **10%** | **20%** | **10%** | **20%** | **10%** | **20%** |\n| MemDPC | 17.2 | 21.1 | 15.4 | 20.1 | 82.2 | 89.5 |\n| LTG | 21.3 | 29.7 | 14.6 | 19.3 | 64.6 | 76.9 |\n| SVFormer | 28.9 | 47.3 | 18.8 | 22.5 | 86.6 | 90.1 |\n| SeFAR-S (Ours) | **36.6** | **55.3** | **19.2** | **25.5** | **96.4** | **97.3** |\n| SeFAR-B (Ours) | **37.1** | **56.8** | **20.1** | **26.5** | **97.0** | **97.8** |\n| 2.5 |  |  |  |  |  |  |", "caption": "(b) Results of elements within an event.", "description": "This table presents the results of experiments conducted on the elements within a specific event in the FineGym dataset.  It shows the performance of different semi-supervised action recognition methods in terms of Top-1 accuracy, using various labeling rates (10% and 20%). The methods are evaluated on two different event subsets: Uneven Bars (UB) and Floor Exercise (FX).  The table provides a detailed breakdown of the performance within each event, offering a more granular understanding of the methods' accuracy than the overall results across all events.", "section": "Experiments"}, {"content": "| Method | Backbone | Input | ImgNet | #F | Epoch | UCF-101 1% | UCF-101 5% | UCF-101 10% | HMDB-51 40% | HMDB-51 50% |\n|---|---|---|---|---|---|---|---|---|---|---|\n| 2.5 **Method** |  |  |  |  |  | UCF-101 **1%** | UCF-101 **5%** | UCF-101 **10%** | HMDB-51 **40%** | HMDB-51 **50%** |\n|---|---|---|---|---|---|---|---|---|---|---|\n| 2 MT+SD (WACV\u201921) [Jing et al. 2021] | 3D-ResNet-18 | V | \u2717 | 16 | 500 | - | 31.2 | 40.7 | 32.6 | 35.1 |\n| MvPL (ICCV\u201921) [Xiong et al. 2021] | 3D-ResNet-50 | VFG | \u2717 | 8 | 600 | 22.8 | 41.2 | 80.5 | 30.5 | 33.9 |\n| TCLR (CVIU\u201922) [Dave et al. 2022] | 3D-ResNet-18 | V | \u2717 | 16 | 1200 | 26.9 | - | 66.1 | - | - |\n| CMPL (CVPR\u201922) [Xu et al. 2022b] | R50+R50-1/4 | V | \u2717 | 8 | 200 | 25.1 | - | 79.1 | - | - |\n| LTG (CVPR\u201922) [Xiao et al. 2022] | 3D-ResNet-18 | VG | \u2717 | 8 | 180 | - | 44.8 | 62.4 | 46.5 | 48.4 |\n| TimeBalance (CVPR\u201923) [Dave et al. 2023] | 3D-ResNet-50 | V | \u2717 | 8 | 250 | 30.1 | 53.3 | 81.1 | 52.6 | 53.9 |\n| SeFAR (Ours) | VIT-S | V | \u2717 | 8 | 30 | 35.2 | 64.1 | 78.3 | 55.9 | 59.2 |\n| 1.6 FixMatch (NeurlPS\u201920) [Sohn et al. 2020] | SlowFast-R50 | V | \u2713 | 8 | 200 | 16.1 | - | 55.1 | - | - |\n| MemDPC (ECCV\u201920) [Han, Xie, and Zisserman 2020] | 3D-ResNet-18 | V | \u2713 | 16 | 500 | - | - | 44.2 | - | - |\n| ActorCM (CVIU\u201921) [Zou et al. 2023] | R(2+1)D-34 | V | \u2713 | 8 | 360 | - | 45.1 | 53.0 | 35.7 | 39.5 |\n| VideoSSL (WACV\u201921) [Jing et al. 2021] | 3D-ResNet-18 | V | \u2713 | 16 | 500 | - | 32.4 | 42.0 | 32.7 | 36.2 |\n| TACL (TSVT\u201922) [Tong, Tang, and Wang 2023] | 3D-ResNet-50 | V | \u2713 | 16 | 200 | - | 35.6 | 55.6 | 38.7 | 40.2 |\n| L2A (ECCV\u201922) [Gowda et al. 2022] | 3D-ResNet-18 | V | \u2713 | 8 | 400 | - | - | 60.1 | 42.1 | 46.3 |\n| SVFormer-S (CVPR\u201923) [Xing et al. 2023] | ViT-S | V | \u2713 | 8 | 30 | 31.4 | - | 79.1 | 56.2 | 58.2 |\n| SVFormer-B (CVPR\u201923) [Xing et al. 2023] | ViT-B | V | \u2713 | 8 | 30 | 46.1 | - | 84.6 | 59.9 | 64.3 |\n| SeFAR (Ours) | VIT-S | V | \u2713 | 8 | 30 | 46.0 | 73.2 | 84.3 | 58.5 | 62.9 |\n| SeFAR (Ours) | VIT-B | V | \u2713 | 8 | 30 | **50.3** | **77.6** | **87.0** | **61.5** | **65.7** |", "caption": "(c) Results of elements within a set.", "description": "This table presents the performance of different semi-supervised action recognition methods on fine-grained datasets, specifically focusing on the 'element' level within a particular 'set' of actions.  The results are shown for different labeling rates (10% and 20%), illustrating how well each method can distinguish between fine-grained action variations within a constrained set of similar actions. ", "section": "Experiment Results"}, {"content": "| 2.5 **Dual-Ele** | **Mod-Perturb** | **Ada-Reg** | **Gym99** | **Gym288** | **Diving** |\n|---|---|---|---|---|---| \n| 2 \u2717 | \u2717 | \u2717 | 32.6 | 22.7 | 60.4 |\n| \u2713 | \u2717 | \u2717 | 34.8 | 25.4 | 64.6 |\n| \u2713 | \u2713 | \u2717 | 35.9 | 26.6 | 67.4 |\n| \u2713 | \u2713 | \u2713 | **36.7** | **27.8** | **72.2** |\n| 2.5 |  |  |  |  |  |", "caption": "Table 2: Comparison with state-of-the-art semi-supervised action recognition methods on coarse-grained datasets.\u201cV\u201d within \u201cInput\u201d signifies RGB video, \u201cF\u201d indicates optical flow, while \u201cG\u201d denotes temporal gradients.", "description": "Table 2 presents a comparison of the SeFAR model's performance against other state-of-the-art semi-supervised action recognition methods on standard, coarse-grained datasets (UCF101 and HMDB51).  The table details each method's backbone network architecture, the type of input data used (RGB video, optical flow, or temporal gradients), whether ImageNet pre-training was employed, the number of frames processed, the number of training epochs, and the top-1 accuracy achieved at various labeling rates (1%, 5%, 10%, 40%, and 50%). This allows for a comprehensive evaluation of SeFAR's effectiveness compared to existing techniques in semi-supervised learning for action recognition.", "section": "Related Work"}, {"content": "| 2.5 **Perturbation** | **S/O** | **Gym99** | **Gym288** | **Diving** | **G.-New** | **Sth.-Sth.** |\n|---|---|---|---|---|---|---|\n| 2 Spatial-only  |  | 34.2 | 24.4 | 67.9 | 45.6 | 39.4 |\n| Slow (T-Drop) | S | 35.6 | 25.2 | 68.6 | 45.0 | 41.2 |\n| All shuffle | O | 35.2 | 26.3 | 69.0 | 45.5 | 41.9 |\n| Local-shuffle | O | 36.4 | 27.6 | 71.9 | 45.3 | 43.3 |\n| Warping | O | 35.9 | 24.7 | 68.2 | 44.8 | 40.8 |\n| T-Half | O | 36.0 | 24.8 | 68.4 | 44.8 | 42.1 |\n| All reverse | O | 36.3 | 27.3 | 71.2 | 45.9 | 42.7 |\n| Mod-Perturb | O | **36.7** | **27.8** | **72.2** | **46.2** | **44.9** |\n| 2.5  |  |  |  |  |  |  |", "caption": "Table 3: Ablations of different components with SeFAR, where \u2713 means \u201cw/\u201d. To adhere to the principle of consistency regularization in SSL, we employ strong augmentation consistent with SVFormer\u00a0(Xing et\u00a0al. 2023), i.e., temporal warping, once our Mod-Perturb is eliminated.", "description": "This table presents the ablation study of different components in the SeFAR model. It shows the performance of the model on three fine-grained action recognition datasets (Gym99, Gym288, Diving) when removing one component at a time. The components analyzed are: dual-level temporal elements modeling, moderate temporal perturbation (Mod-Perturb), and adaptive regulation (Ada-Reg).  The baseline uses temporal warping as strong augmentation, consistent with SVFormer (another model), but uses different components in SeFAR to analyze each.", "section": "Methodology"}, {"content": "| Visual Encoder | MLLM | Gym-QA-99 | Gym-QA-288 |\n|---|---|---|---| \n| 2.5 **Visual Encoder** | **MLLM** | **Gym-QA-99** | **Gym-QA-288** |\n| 2 CLIP-ViT-L/16 | [https://arxiv.org/html/2501.01245/LLaVA.png](https://arxiv.org/html/2501.01245/LLaVA.png), [https://arxiv.org/html/2501.01245/VideoChat2.png](https://arxiv.org/html/2501.01245/VideoChat2.png) | 37.3 | 41.0 |\n| EVA-CLIP ViT-G/14 | [https://arxiv.org/html/2501.01245/VideoLLaMA.png](https://arxiv.org/html/2501.01245/VideoLLaMA.png), [https://arxiv.org/html/2501.01245/VideoChat.png](https://arxiv.org/html/2501.01245/VideoChat.png) | 43.7 | 44.8 |\n| ViT-L/14 | [https://arxiv.org/html/2501.01245/VideoLLaMA.png](https://arxiv.org/html/2501.01245/VideoLLaMA.png) | 44.3 | 46.0 |\n| SeFAR (Ours) | - | **49.0** | **56.2** |\n| 2.5 |  |  |  |", "caption": "Table 4: Ablation of different temporal augmentations. S and O denote the Speed- and Order-focused.", "description": "This table presents the results of ablation experiments conducted to evaluate the impact of different temporal augmentation techniques on the performance of the SeFAR model.  Specifically, it examines the effects of variations in speed and order of temporal augmentations on three fine-grained action recognition datasets: FineGym (FX), FineGym (10m), and FineDiving (5253B). The results are analyzed to determine the optimal temporal augmentation strategy for enhancing the model's ability to accurately recognize fine-grained actions.", "section": "Methodology"}, {"content": "| Method | FineDiving |  |  |  |  | \n|---|---|---|---|---|---| \n| 2.5 **Method** | **1%** | **3%** | **5%** | **7%** | **10%** | \n| 2 SeFAR w/o Ada-Reg | 61.5 | 64.6 | 67.2 | 69.7 | 73.4 | \n| SeFAR | **66.3** | **69.5** | **72.2** | **74.6** | **78.4** | \n| Increase (%) | 7.8%\u2191 | 7.6%\u2191 | 7.4%\u2191 | 7.0%\u2191 | 6.8%\u2191 | \n| 2.5 |  |  |  |  |  | ", "caption": "Table 5: Ablation of Pre-trained Visual Encoder. We employ Vicuna-7B\u00a0(Chiang et\u00a0al. 2023) as the base LLM, comparing SeFAR\u2019s features with the pre-trained features of commonly used visual encoders in MLLMs further fine-tuned on 5% data (i.e., \n\n\n: LLaVA, \n\n\n: VideoChat2, \n\n\n: VideoLLaMA, \n\n\n: VideoChat, and \n\n\n: VideoLLaVA)", "description": "This table presents an ablation study on the impact of different pre-trained visual encoders on the performance of a multimodal large language model (MLLM) for fine-grained action recognition.  The base MLLM used is Vicuna-7B. The experiment compares the performance when using SeFAR's extracted visual features against features from several commonly used pre-trained visual encoders in MLLMs (LLaVA, VideoChat2, VideoLLaMA, VideoChat, and VideoLLaVA). These visual encoders were further fine-tuned on 5% of the data. The results are evaluated on two fine-grained action recognition tasks, Gym-QA-99 and Gym-QA-288.", "section": "SeFAR Empowers MLLMs"}, {"content": "| 2.5 **Perturbation** | **Speed/Order** | **FX** | **10m** | **UB-S1** | **5253B** |\n|---|---|---|---|---|---| \n| 2 Slow-rate | Speed | 22.4 | 81.2 | 35.6 | 92.8 |\n| T-Drop | Speed | 22.4 | 81.2 | 35.6 | 92.8 |\n| All shuffle | Order | 23.5 | 82.8 | 36.1 | 93.5 |\n| Local-shuffle | Order | 23.0 | 84.1 | 36.5 | 94.9 |\n| Warping | Order | 23.4 | 81.9 | 34.7 | 92.9 |\n| T-Half | Order | 23.3 | 83.0 | 35.3 | 93.4 |\n| All reverse | Order | 23.6 | 83.7 | 35.5 | 95.1 |\n| Mod-Perturb | Order | 23.8 | 85.5 | 36.6 | 96.4 |\n| 2.5 |  |  |  |  |  |", "caption": "Table 6: Ablation of different labeling rates. The first two raw demonstrate our SeFAR w/o and w/ the Adaptive Regulation (Ada-Reg) respectively. The third raw further shows the performance increase rates at different labeling rates.", "description": "This table presents an ablation study on the effect of different data labeling rates on the performance of the SeFAR model. It shows the model's performance with and without the Adaptive Regulation module at various labeling rates (1%, 3%, 5%, 7%, and 10%). The last row quantifies the performance improvement achieved by including the Adaptive Regulation module at each labeling rate.", "section": "Ablation Studies"}, {"content": "| Prediction Times | 1 | 2 | 5 | 10 | 15 | 20 |\n|---|---|---|---|---|---|---|\n| 2.5 Teacher time / Iter. | 29.9 | 68.5 | 75.8 | 160.4 | 260.1 | 361.3 |\n| Total time / Iter. | 982.8 | 991.6 | 1005.1 | 1080.7 | 1220.6 | 1417.6 |\n| Portion (%) | 3.0 | 6.9 | 7.5 | 14.8 | 21.3 | 25.5 |\n| Accuracy (%) | - | 35.3 | 36.2 | 36.7 | 36.8 | 37.0 |", "caption": "Table 7: Deeper comparison of temporal augmentations.", "description": "This table presents a deeper comparison of different temporal augmentation techniques used in the SeFAR model.  It compares the performance of various augmentation strategies ('Slow-rate', 'T-Drop', 'All shuffle', 'Local-shuffle', 'Warping', 'T-Half', 'All reverse', 'Mod-Perturb') across three different subsets of the FineGym dataset (FX, 10m, UB-S1, 5253B). The table shows the Top-1 accuracy achieved by each augmentation method on each subset, categorized by whether speed or order was affected. This allows for analysis of the impact of different augmentation types on the model's ability to correctly identify actions.", "section": "Methodology"}]