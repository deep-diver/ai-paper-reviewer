[{"figure_path": "https://arxiv.org/html/2412.18153/x2.png", "caption": "Figure 1: DepthLab\u00a0 for diverse downstream tasks. Many tasks naturally contain partial depth information, such as (1) 3D Gaussian inpainting, (2) LiDAR depth completion, (3) sparse-view reconstruction with Dust3R, and (4) text-to-scene generation.\nOur model leverages this known information to achieve improved depth estimation, enhancing performance in downstream tasks. We hope to motivate more related tasks to adopt DepthLab.", "description": "DepthLab is a foundational model for depth inpainting that leverages known partial depth information from various sources to improve depth estimation across diverse downstream applications.  The figure showcases four such applications: (1) filling in missing parts of a 3D Gaussian surface; (2) completing incomplete depth data from LiDAR sensors; (3) reconstructing a full 3D scene from sparse views using the DUST3R method; and (4) generating a 3D scene from a text prompt. By incorporating this pre-existing depth information, DepthLab achieves better performance in these tasks than models that lack this capability.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2412.18153/x3.png", "caption": "Figure 2: The training process of DepthLab. First, we apply random masking to the ground truth depth to create the masked depth, followed by interpolation. Both the interpolated masked depth and the original depth undergo random scale normalization before being fed into the encoder. The Reference U-Net extracts RGB features, while the Estimation U-Net takes the noisy depth, masked depth, and encoded mask as input. Layer-by-layer feature fusion allows for finer-grained visual guidance, achieving high-quality depth predictions even in large or complex masked regions.", "description": "DepthLab's training process involves two key components: the Reference U-Net, which extracts RGB features from the input image; and the Estimation U-Net, which processes the depth information.  The training process starts by randomly masking parts of the ground truth depth map and then interpolating the masked areas. Both the original and interpolated depth maps undergo random scale normalization before being fed into their respective encoders.  The Reference and Estimation U-Nets are then combined using a layer-by-layer feature fusion technique. This approach enables DepthLab to leverage both visual and depth cues to generate precise depth maps, especially in complex areas with significant missing data. ", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2412.18153/x4.png", "caption": "Figure 3: Qualitative comparison of various methods on different datasets. In the second column, black represents the known regions, while white indicates the predicted areas.\nNotably, to emphasize the contrast, we reattach the known ground truth depth to the corresponding positions in the right-side visualizations of the depth maps. Other methods exhibit significant geometric inconsistency.", "description": "This figure compares the performance of different depth completion methods on several datasets.  The leftmost column shows the input RGB images. The second column displays the input depth maps, where black indicates known depth values and white represents missing data. The next three columns present the depth maps generated by three different state-of-the-art methods (GeoWizard, DepthAnything V2, and the proposed method). Notably, the known ground truth depth data is re-integrated into the output visualizations of the competing methods to highlight geometric inconsistencies and contrast them against the method proposed in the paper.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2412.18153/x5.png", "caption": "Figure 4: Visualization of gaussian inpainting. By projecting depth directly into three-dimensional space as initial points, natural 3D consistency is maintained, enabling texture editing and object addition. Please zoom in to view more details.", "description": "Figure 4 showcases the results of Gaussian inpainting achieved by DepthLab.  The process starts by directly projecting the depth map into 3D space, using the known depth values as a foundation. This 3D representation ensures the natural consistency of the scene.  The model's ability to maintain this consistency allows for seamless texture editing and the addition of new objects to the scene without introducing artifacts or inconsistencies. Zooming in reveals finer details of the high-quality inpainting results.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2412.18153/x6.png", "caption": "Figure 5: Visualization of 3d scene generation. Left: Depth comparison. \u201dAlign\u201d represents the least-square method and shows clear geometric inconsistencies at boundaries. While LucidDreamer reduces these inconsistencies, it compromises the accuracy of the newly estimated depth.\nIn contrast, our model produces consistent and accurate depth. Right: The improved depth estimation from our model leads to superior 3D scene generation results.", "description": "Figure 5 presents a comparison of 3D scene generation results using different depth estimation methods. The left side shows a depth comparison, highlighting the geometric inconsistencies produced by the least-squares method ('Align') and how LucidDreamer, while improving upon the least-squares approach, still compromises accuracy in newly estimated depth regions. In contrast, the proposed DepthLab model produces consistent and accurate depth estimation. The right side demonstrates the superior 3D scene generation quality achieved using depth estimations from the DepthLab model.  This illustrates DepthLab's ability to produce high-quality 3D reconstructions by accurately inpainting depth data.", "section": "4. Applications"}]