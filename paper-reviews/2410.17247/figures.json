[{"figure_path": "2410.17247/figures/figures_4_0.png", "caption": "Figure 2: Overview of PyramidDrop. We divide the forward pass of the LLM into multiple stages, and drop part of the image tokens at the end of each stage with a pre-defined ratio. The dropping is based on a lightweight attention calculation with a negligible time overhead, and according to this criterion, the LLM accurately selects important image tokens related to instruction. Due to the efficient redundancy reduction strategy, the average sequence length decreases rapidly.", "description": "The figure illustrates the PyramidDrop method, showing how the model divides the forward pass into stages and drops image tokens at the end of each stage based on a lightweight attention mechanism, reducing sequence length and improving efficiency.", "section": "3.2 PYRAMIDDROP"}, {"figure_path": "2410.17247/figures/figures_10_0.png", "caption": "Figure 1: Observatioins about visual redundancy acoross layers. Left: TextVQA performance of LLaVA-1.5 with varying ratio of retained image tokens at different layer. The preserved image tokens are those that receive the highest attention from the text tokens. Right: Visualization of attention map in shallow and deep layers.", "description": "The figure shows that visual redundancy in LLMs increases with depth, with shallow layers being more sensitive to token dropping and deeper layers exhibiting increased redundancy.", "section": "3.1 STUDY OF VISUAL TOKEN REDUNDANCY IN LVLMS"}]