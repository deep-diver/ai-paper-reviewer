[{"figure_path": "2410.17247/figures/figures_4_0.png", "caption": "Figure 2: Overview of PyramidDrop. We divide the forward pass of the LLM into multiple stages, and drop part of the image tokens at the end of each stage with a pre-defined ratio. The dropping is based on a lightweight attention calculation with a negligible time overhead, and according to this criterion, the LLM accurately selects important image tokens related to instruction. Due to the efficient redundancy reduction strategy, the average sequence length decreases rapidly.", "description": "The figure illustrates the PyramidDrop method, showing how the model divides the forward pass into stages and drops image tokens at the end of each stage based on attention weights, reducing the sequence length.", "section": "3.2 PYRAMIDDROP"}, {"figure_path": "2410.17247/figures/figures_10_0.png", "caption": "Figure 5: Visualization of token dropping in LLM of LLaVA -1.5. We compute the attention score of image tokens received from the last instruction token as the ranking criterion, and find LLM accurately retain image tokens according to instruction.", "description": "The figure visualizes how PyramidDrop effectively preserves image tokens related to the instruction, as shown by LLaVA-1.5, accurately retaining relevant tokens for accurate answers.", "section": "3.2 PYRAMIDDROP"}, {"figure_path": "2410.17247/figures/figures_10_1.png", "caption": "Figure 5: Visualization of token dropping in LLM of LLaVA-1.5. We compute the attention score of image tokens received from the last instruction token as the ranking criterion, and find LLM accurately retain image tokens according to instruction.", "description": "The figure visualizes how PyramidDrop effectively preserves image tokens related to the instruction by showing examples of retained image tokens at different layers of the LLaVA-1.5 model.", "section": "3.2 PYRAMIDDROP"}]