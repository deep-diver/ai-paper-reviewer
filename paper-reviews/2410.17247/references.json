{"references": [{" publication_date": "2023", "fullname_first_author": "Hugo Touvron", "paper_title": "Llama: Open and efficient foundation language models", "reason": "This paper introduces LLAMA, a foundational large language model that has significantly impacted the field.  Its open-source nature and efficient design have made it a cornerstone for subsequent research in large language models, including many of the papers cited in the target document.  Many LVLMs are built upon or adapted from LLAMA, making it a highly influential work in the development of large vision-language models.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Wei-Lin Chiang", "paper_title": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality", "reason": "Vicuna is a prominent open-source large language model that has served as a basis for many subsequent vision-language models. Its strong performance and open-source nature have made it highly influential in the field, allowing researchers to build upon its architecture and capabilities.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Wenliang Dai", "paper_title": "Instructblip: Towards general-purpose vision-language models with instruction tuning", "reason": "This paper introduces InstructBLIP, a significant advancement in vision-language models that utilizes instruction tuning.  Instruction tuning has become a highly influential technique in the field, improving the performance and capabilities of many models, including several related to the target paper's work.  InstructBLIP's contributions to instruction tuning have significantly influenced the development of LVLMs and related technologies.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Tri Dao", "paper_title": "Flashattention: Fast and memory-efficient exact attention with io-awareness", "reason": "This paper presents FlashAttention, an efficient attention mechanism that has significantly improved the speed and memory efficiency of large language models.  Its impact is notable because many LLMs, including those used in conjunction with vision-language models, rely on efficient attention mechanisms for performance. The efficiency gains from FlashAttention directly contribute to the advances in large vision-language models.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Haotian Liu", "paper_title": "Llava-next: Improved reasoning, ocr, and world knowledge", "reason": "LLaVA-NeXT is a high-resolution extension of LLaVA, one of the key models used in the experiments and directly related to the work in this paper.  Therefore, this paper is extremely relevant to the context and serves as a crucial foundational model for their comparative analysis and evaluation.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Haotian Liu", "paper_title": "Llava-vl: A large vision-language model", "reason": "LLaVA is a very important and foundational large vision language model that is prominently featured throughout the experiments within this paper. Understanding its strengths and limitations provides crucial context for evaluating the new PyramidDrop method, making this reference critical.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Chaoyou Fu", "paper_title": "Mme: A comprehensive evaluation benchmark for multimodal large language models", "reason": "This paper is crucial because it presents MME, a comprehensive benchmark used to evaluate the performance of the models used in this paper's experiments.  The MME benchmark provides a standardized and widely accepted method for assessing performance, which is important for comparing results against previously established norms and the state of the art.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Yuan Liu", "paper_title": "Mmbench: Is your multi-modal model an all-around player?", "reason": "This paper introduces MMBench, a benchmark directly used to assess the performance of the models in this research. This benchmark provides a structured and widely recognized way to compare model capabilities, allowing for an objective comparison and analysis of results. The benchmark is specifically relevant for multimodal large language models.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Bo Li", "paper_title": "Seed-bench: Benchmarking multimodal llms with generative comprehension", "reason": "This work introduces SEED-Bench, another benchmark dataset used in this paper's experimental evaluation. SEED-Bench provides a comprehensive evaluation framework, helping to measure and compare the performance of different multimodal large language models, making it crucial for understanding the impact of the proposed technique.", "section_number": 4}, {" publication_date": "2017", "fullname_first_author": "Yash Goyal", "paper_title": "Making the v in vqa matter: Elevating the role of image understanding in visual question answering", "reason": "This paper is highly influential because it's one of the foundational datasets for visual question answering (VQA), which has strongly influenced subsequent multimodal models.  It establishes baseline performance levels, provides a common standard for evaluation, and is a widely referenced dataset for assessing vision and language models.", "section_number": 4}, {" publication_date": "2018", "fullname_first_author": "Danna Gurari", "paper_title": "Vizwiz grand challenge: Answering visual questions from blind people", "reason": "VizWiz is a significant dataset for visual question answering and is frequently used as a benchmark for evaluating the performance of vision-language models. Its unique characteristic of including questions posed by visually impaired individuals adds a specific challenge that is relevant for broader applications. The dataset's influence in assessing LVLMs is important for benchmarking.", "section_number": 4}, {" publication_date": "2021", "fullname_first_author": "Minesh Mathew", "paper_title": "Docvqa: A dataset for vqa on document images", "reason": "DocVQA is a benchmark dataset specifically designed for evaluating vision-language models on document images, which are complex and visually dense inputs.  Its use in this paper's evaluations highlights the importance of assessing performance on more challenging, high-resolution input types, which the new PyramidDrop method was also explicitly designed to address.", "section_number": 4}, {" publication_date": "2022", "fullname_first_author": "Ahmed Masry", "paper_title": "Chartqa: A benchmark for question answering about charts with visual and logical reasoning", "reason": "ChartQA is a benchmark dataset focusing on the visual understanding of charts and graphs.  Its inclusion in the experimental evaluation in this paper demonstrates the importance of verifying the proposed model's performance not only on general images but also on specialized visual data that requires reasoning skills.", "section_number": 4}, {" publication_date": "2022", "fullname_first_author": "Minesh Mathew", "paper_title": "Infographicvqa", "reason": "InfographicVQA is a benchmark dataset that focuses on the visual understanding of infographics, a specialized and complex visual format. This makes its inclusion vital because evaluating performance on such data highlights the ability to handle intricate and nuanced visual information in document images, reflecting the capabilities of the proposed method to handle diverse types of visual input.", "section_number": 4}, {" publication_date": "2019", "fullname_first_author": "Amanpreet Singh", "paper_title": "Towards vqa models that can read", "reason": "TextVQA is a widely used and highly influential benchmark dataset in the field of visual question answering.  Its inclusion is critical because it provides a well-established and standardized measure of performance, allowing for a direct comparison with the results of prior work in the field. It is a key benchmark to assess improvements in LVLMs.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Keqin Chen", "paper_title": "Shikra: Unleashing multimodal llm's referential dialogue magic", "reason": "Shikra is a multimodal LLM that showcases improvements in the field of visual question answering. Its inclusion is important because it allows the authors to contextualize and compare the efficiency gains of their proposed method against other state-of-the-art techniques that achieve similar improvements in tasks related to visual understanding.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Kazi Hasan Ibn Arif", "paper_title": "Hired: Attention-guided token dropping for efficient inference of high-resolution vision-language models in resource-constrained environments", "reason": "This paper directly addresses the problem of efficient inference for high-resolution vision-language models by employing token dropping. This makes it particularly relevant for comparison against the proposed method in this paper, which also tackles efficiency in high-resolution scenarios.  It provides a direct point of comparison with similar methods.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Jinze Bai", "paper_title": "Qwen-vl: A frontier large vision-language model with versatile abilities", "reason": "Qwen-VL is a large vision-language model that showcases advanced capabilities in the field. Its inclusion in the related work section provides context for the advancements achieved by the proposed method, showing its relative position within the current state of the art in the field of multimodal models.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Lin Chen", "paper_title": "An image is worth 1/2 tokens after layer 2: Plug-and-play inference acceleration for large vision-language models", "reason": "This paper is directly relevant because it proposes a method that focuses on the efficiency of inference in large vision-language models.  Its methodology is similar in principle to the approach in this paper, which also aims to increase efficiency. Comparing and contrasting both techniques adds significance to the evaluation.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Lin Chen", "paper_title": "Sharegpt4v: Improving large multi-modal models with better captions", "reason": "ShareGPT4V presents an approach to improve large multimodal models with better captions, which has a direct bearing on the performance of large vision-language models. Including this work helps to establish a broader context and compare the relative advantages of the proposed methods with other techniques in the field.", "section_number": 2}]}