{"references": [{" publication_date": "2023", "fullname_first_author": "Hugo Touvron", "paper_title": "Llama: Open and efficient foundation language models", "reason": "This paper introduces LLaMA, a foundational language model that is both open and efficient.  Its open nature has fostered significant advancements in the field, enabling researchers to build upon its architecture and contribute to the rapid progress in LLMs and LVLMs. The efficiency of LLaMA is directly relevant to the focus on improving efficiency in LVLMs, as addressed by PyramidDrop.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Wei-Lin Chiang", "paper_title": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality", "reason": "Vicuna is a significant contribution because it demonstrates the rapid progress and accessibility of large language models.  Its success in replicating GPT-4's capabilities is highly relevant to the broader context of LVLMs and showcases the potential for creating powerful and efficient models.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Wenliang Dai", "paper_title": "Instructblip: Towards general-purpose vision-language models with instruction tuning", "reason": "This paper presents InstructBLIP, a model that pushes the boundaries of vision-language capabilities through instruction tuning. This advancement is highly relevant to the field's efforts in improving performance and efficiency in LVLMs.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Xi Chen", "paper_title": "Pali-x: On scaling up a multilingual vision and language model", "reason": "This paper tackles the challenge of scaling up multilingual vision-language models, a significant area of research that directly relates to the efficiency and scalability challenges addressed by PyramidDrop.  Improving efficiency is critical for scaling up these large models.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Qingqing Cao", "paper_title": "Pumer: Pruning and merging tokens for efficient vision language models", "reason": "This paper directly addresses the problem of efficient vision-language models by focusing on token pruning and merging.  This makes it highly relevant to the work presented in PyramidDrop, which also aims to improve efficiency by reducing the number of tokens.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Suyu Ge", "paper_title": "Model tells you what to discard: Adaptive kv cache compression for llms", "reason": "This paper is highly relevant due to its focus on adaptive KV cache compression for LLMs.  The adaptive nature of the approach is similar in spirit to the adaptive nature of PyramidDrop, which adjusts the number of tokens to be dropped at each stage.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Tri Dao", "paper_title": "Flashattention: Fast and memory-efficient exact attention with io-awareness", "reason": "FlashAttention is a crucial component underlying the efficiency gains in PyramidDrop. The paper's contribution to fast and efficient attention mechanisms directly impacts the efficiency and scalability of large language models, which is central to the goals of PyramidDrop.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Kazi Hasan Ibn Arif", "paper_title": "Hired: Attention-guided token dropping for efficient inference of high-resolution vision-language models in resource-constrained environments", "reason": "This work directly addresses efficient inference of high-resolution vision-language models, a major focus of PyramidDrop.  The attention-guided token dropping strategy is conceptually similar to PyramidDrop's approach, making it a highly relevant related work.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Jinze Bai", "paper_title": "Qwen-vl: A frontier large vision-language model with versatile abilities", "reason": "This paper introduces Qwen-VL, a large vision-language model. Its contribution to the field is highly relevant to the context of PyramidDrop, which aims to improve the efficiency of similar large models.  The model's versatility underlines the importance of efficiency improvements in the LVLMs.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Keqin Chen", "paper_title": "Shikra: Unleashing multimodal llm's referential dialogue magic", "reason": "This paper focuses on multimodal LLMs, directly relevant to the context of PyramidDrop. The work on referential dialogue in multimodal settings showcases the challenges and opportunities in this domain, which are directly relevant to the performance goals of PyramidDrop.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Liang Chen", "paper_title": "An image is worth 1/2 tokens after layer 2: Plug-and-play inference acceleration for large vision-language models", "reason": "This paper directly tackles the challenge of inference acceleration in large vision-language models. The 'plug-and-play' aspect is highly relevant to PyramidDrop's potential use as an add-on for enhancing inference.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Haotian Liu", "paper_title": "Llava-next: Improved reasoning, ocr, and world knowledge", "reason": "This paper presents LLaVA-NeXT, a model used for experimental evaluation of PyramidDrop. LLaVA-NeXT's high-resolution capabilities are specifically relevant to the method, showing that PyramidDrop improves the efficiency of these large models.", "section_number": 4}, {" publication_date": "2017", "fullname_first_author": "Yash Goyal", "paper_title": "Making the v in vqa matter: Elevating the role of image understanding in visual question answering", "reason": "This paper is a foundational contribution to the Visual Question Answering (VQA) field. The focus on image understanding is directly relevant to the efficiency improvements of PyramidDrop, which aims to maximize the information retained from images while reducing redundancy.", "section_number": 4}, {" publication_date": "2018", "fullname_first_author": "Danna Gurari", "paper_title": "Vizwiz grand challenge: Answering visual questions from blind people", "reason": "VizWiz is a significant benchmark dataset for evaluating visual question answering systems that require understanding images with rich visual details. The dataset is relevant to the experiments in PyramidDrop, which focuses on improving performance in high-resolution image understanding.", "section_number": 4}, {" publication_date": "2021", "fullname_first_author": "Minesh Mathew", "paper_title": "Docvqa: A dataset for vqa on document images", "reason": "DocVQA is a benchmark dataset explicitly designed for high-resolution document image understanding.  This is highly relevant to the evaluation of PyramidDrop because the method is aimed at improving efficiency and performance specifically with high-resolution images.", "section_number": 4}, {" publication_date": "2022", "fullname_first_author": "Ahmed Masry", "paper_title": "Chartqa: A benchmark for question answering about charts with visual and logical reasoning", "reason": "ChartQA is a significant benchmark dataset relevant to the experimental evaluation of PyramidDrop, especially in the high-resolution tasks.  This is because this benchmark challenges the models to answer questions involving visual and logical reasoning on charts, a task that demands efficient processing of high-resolution visual data.", "section_number": 4}, {" publication_date": "2022", "fullname_first_author": "Minesh Mathew", "paper_title": "Infographicvqa", "reason": "InfographicVQA is a challenging benchmark dataset that requires fine-grained understanding of complex visual information in infographics. This is highly relevant to the experiments with PyramidDrop because the method is evaluated on various benchmarks, including those with high-resolution visual content.", "section_number": 4}, {" publication_date": "2019", "fullname_first_author": "Amanpreet Singh", "paper_title": "Towards vqa models that can read", "reason": "This paper is fundamental to the VQA field. It focuses on building VQA models that can \"read\", meaning that they effectively process textual information within images. This concept is directly relevant to PyramidDrop's evaluation, as several benchmarks used in the paper's experimental evaluation are related to visual question answering.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Chaoyou Fu", "paper_title": "Mme: A comprehensive evaluation benchmark for multimodal large language models", "reason": "MME is a comprehensive benchmark for evaluating multimodal large language models. Its use in the experimental evaluation of PyramidDrop provides a broad assessment of the model's performance and showcases its versatility across diverse multimodal tasks.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Yuan Liu", "paper_title": "Mmbench: Is your multi-modal model an all-around player?", "reason": "MMBench provides a thorough evaluation of multimodal models, which is highly relevant for assessing the effectiveness of PyramidDrop on various tasks and high-resolution images. The benchmark's focus on diverse capabilities is crucial for verifying the robustness and generalizability of PyramidDrop.", "section_number": 4}]}