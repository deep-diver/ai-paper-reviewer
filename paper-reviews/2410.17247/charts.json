[{"figure_path": "2410.17247/charts/charts_3_0.png", "caption": "Figure 1: Observatioins about visual redundancy acoross layers. Left: TextVQA performance of LLaVA-1.5 with varying ratio of retained image tokens at different layer. The preserved image tokens are those that receive the highest attention from the text tokens. Right: Visualization of attention map in shallow and deep layers.", "description": "The chart displays the TextVQA performance of LLaVA-1.5 with varying ratios of retained image tokens at different layers and visualizes the attention maps in shallow and deep layers to show that visual redundancy increases with the depth of the model.", "section": "3.1 STUDY OF VISUAL TOKEN REDUNDANCY IN LVLMS"}, {"figure_path": "2410.17247/charts/charts_8_0.png", "caption": "Figure 3: We compare the performance of the original LLaVA-1.5 and LLaVA-1.5 trained using PDrop, where we preserve different ratios of image tokens at layer 2, 8, 16, and 24, respectively. The horizontal axis represents the proportion of retained image tokens according to attention score.", "description": "The chart visualizes how the performance of LLaVA-1.5, both original and trained with PyramidDrop, changes at different layers with varying ratios of retained image tokens, based on attention scores.", "section": "4.2 EFFICIENT OF PYRAMID DROP IN TRAINING"}, {"figure_path": "2410.17247/charts/charts_8_1.png", "caption": "Figure 3: We compare the performance of the original LLaVA-1.5 and LLaVA-1.5 trained using PDrop, where we preserve different ratios of image tokens at layer 2, 8, 16, and 24, respectively. The horizontal axis represents the proportion of retained image tokens according to attention score.", "description": "The chart compares the performance of the original LLaVA-1.5 and a version trained with PyramidDrop across different layers and image token retention ratios, showing the effect of the proposed method on model performance.", "section": "4.2 EFFICIENT OF PYRAMID DROP IN TRAINING"}, {"figure_path": "2410.17247/charts/charts_10_0.png", "caption": "Figure 4: The performance of LLaVA-NeXT-7B with different inference acceleration strategies. PDrop (without training) outperforms FastV on DocVQA, ChartQA, and GQA with across various inference cost budgets.", "description": "The chart compares the performance of PyramidDrop and FastV on DocVQA, ChartQA, and GQA across various inference costs (TFLOPs).", "section": "4.3 EFFICIENT OF PYRAMIDDROP IN INFERENCE"}, {"figure_path": "2410.17247/charts/charts_10_1.png", "caption": "Figure 1: Observatioins about visual redundancy acoross layers. Left: TextVQA performance of LLaVA-1.5 with varying ratio of retained image tokens at different layer. The preserved image tokens are those that receive the highest attention from the text tokens. Right: Visualization of attention map in shallow and deep layers.", "description": "The chart visualizes the relationship between the proportion of retained image tokens at different layers of a Large Vision Language Model (LLaVM) and its performance on a TextVQA task, showing that redundancy increases with depth.", "section": "3.1 STUDY OF VISUAL TOKEN REDUNDANCY IN LVLMS"}]