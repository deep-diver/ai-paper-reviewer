[{"figure_path": "2410.17247/charts/charts_3_0.png", "caption": "Figure 1: Observatioins about visual redundancy acoross layers. Left: TextVQA performance of LLaVA-1.5 with varying ratio of retained image tokens at different layer. The preserved image tokens are those that receive the highest attention from the text tokens. Right: Visualization of attention map in shallow and deep layers.", "description": "The chart displays the TextVQA performance of LLaVA-1.5 with varying ratios of retained image tokens at different layers, and visualizes the attention map in shallow and deep layers to show that visual redundancy progressively increases in deeper layers of the model.", "section": "3.1 STUDY OF VISUAL TOKEN REDUNDANCY IN LVLMS"}, {"figure_path": "2410.17247/charts/charts_8_0.png", "caption": "Figure 3: We compare the performance of the original LLaVA-1.5 and LLaVA-1.5 trained using PDrop, where we preserve different ratios of image tokens at layer 2, 8, 16, and 24, respectively. The horizontal axis represents the proportion of retained image tokens according to attention score.", "description": "The chart displays the TextVQA performance of LLaVA-1.5 models (original and trained with PyramidDrop) at different layers with varying ratios of retained image tokens, demonstrating the impact of PyramidDrop on model performance at different depths.", "section": "4.2 EFFICIENT OF PYRAMID DROP IN TRAINING"}, {"figure_path": "2410.17247/charts/charts_8_1.png", "caption": "Figure 3: We compare the performance of the original LLaVA-1.5 and LLaVA-1.5 trained using PDrop, where we preserve different ratios of image tokens at layer 2, 8, 16, and 24, respectively. The horizontal axis represents the proportion of retained image tokens according to attention score.", "description": "The chart compares the performance of the original LLaVA-1.5 and the model trained with PyramidDrop across different layers and varying ratios of retained image tokens, showing that PyramidDrop maintains or improves performance while reducing tokens.", "section": "4.2 EFFICIENT OF PYRAMID DROP IN TRAINING"}, {"figure_path": "2410.17247/charts/charts_10_0.png", "caption": "Figure 4: The performance of LLaVA-NeXT-7B with different inference acceleration strategies. PDrop (without training) outperforms FastV on DocVQA, ChartQA, and GQA with across various inference cost budgets.", "description": "The chart compares the performance of PyramidDrop and FastV inference acceleration strategies across various inference cost budgets (TFLOPs) on three vision-language benchmarks (DocVQA, ChartQA, and GQA).", "section": "4.3 EFFICIENT OF PYRAMIDDROP IN INFERENCE"}, {"figure_path": "2410.17247/charts/charts_10_1.png", "caption": "Figure 1: Observatioins about visual redundancy acoross layers. Left: TextVQA performance of LLaVA-1.5 with varying ratio of retained image tokens at different layer. The preserved image tokens are those that receive the highest attention from the text tokens. Right: Visualization of attention map in shallow and deep layers.", "description": "The chart visualizes the impact of dropping different ratios of image tokens at various layers of a Large Vision Language Model (LLaVM) on TextVQA task performance and attention patterns, revealing that visual redundancy increases with depth.", "section": "3.1 STUDY OF VISUAL TOKEN REDUNDANCY IN LVLMS"}]