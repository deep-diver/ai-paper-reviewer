[{"content": "| Dataset (split) | Repository-Level | Executable Environment | Real task | # Instances (total) | # Instances (train) |\n|---|---|---|---|---|---| \n| CodeFeedback (Zheng et al., 2024b) | \u2717 | \u2717 | \u2713 | 66,383 | 66,383 |\n| APPS (Hendrycks et al., 2021a) | \u2717 | \u2713 | \u2713 | 10,000 | 5,000 |\n| HumanEval (Chen et al., 2021) | \u2717 | \u2713 | \u2713 | 164 | 0 |\n| MBPP (Tao et al., 2024) | \u2717 | \u2713 | \u2713 | 974 | 374 |\n| R2E (Jain et al., 2024) | \u2713 | \u2713 | \u2717 | 246 | 0 |\n| SWE-Bench (train) (Jimenez et al., 2024) | \u2713 | \u2717 | \u2713 | 19,008 | 19,008 |\n| SWE-Gym Raw | \u2713 | \u2717 | \u2713 | 66,894 | 66,894 |\n| SWE-Bench (test) (Jimenez et al., 2024) | \u2713 | \u2713 | \u2713 | 2,294 | 0 |\n| SWE-Gym | \u2713 | \u2713 | \u2713 | 2,438 | 2,438 |", "caption": "Table 1: SWE-Gym is the first publicly-available training environment combining real-world software engineering tasks from GitHub issues with pre-installed dependencies and executable test verification. Repository-level: whether each task is situated in a sophisticated repository;\nExecutable Environment: whether each instance in the resource comes with an executable environment with all relevant dependencies pre-installed;\nReal task: whether the instruction for each instance is collected from human developers.", "description": "SWE-Gym is the first publicly available dataset for training software engineering agents.  This table compares SWE-Gym to other datasets, highlighting key features such as whether tasks originate from sophisticated repositories, include executable environments with pre-installed dependencies, and use instructions written by human developers. It shows SWE-Gym's unique combination of these features, making it superior for training real-world software engineering agents.", "section": "2. SWE-Gym Environment"}, {"content": "Model|Empty Patch (%,\\[downarrow\\])| | |Stuck in Loop (%,\\[downarrow\\])| | |Avg. Turn(s)| |Resolve Rate (%,\\[uparrow\\])| |\n---|---|---|---|---|---|---|---|---|---|---|\nSize|zero-shot|fine-tuned|\u0394|zero-shot|fine-tuned|\u0394|zero-shot|fine-tuned|\u0394|zero-shot|fine-tuned|\u0394|\nSWE-Bench Lite (300 instances)| | | | | | | | | | | | |\n7B|40.3|29.7|-10.7|47.0|31.0|-16.0|20.3|22.2|+1.9|1.0(\u00b11.0)|10.0(\u00b12.4)|+9.0|\n14B|49.7|18.1|-31.6|31.7|27.1|-4.6|23.2|21.4|-1.8|2.7(\u00b11.9)|12.7(\u00b12.3)|+10.0|\n32B|27.0|18.1|-8.9|16.7|18.1|+1.5|15.5|29.3|+13.9|3.0(\u00b11.4)|15.3(\u00b12.5)|+12.3|\nSWE-Bench Verified (500 instances)| | | | | | | | | | | | |\n7B|45.8|33.8|-12.0|39.6|21.0|-18.6|21.9|35.3|+13.4|1.8(\u00b11.1)|10.6(\u00b12.1)|+8.8|\n14B|44.9|14.5|-30.4|32.1|21.3|-10.7|25.5|30.1|+4.6|4.0(\u00b11.6)|16.4(\u00b12.0)|+12.4|\n32B|9.5|13.8|+4.3|29.4|23.8|-5.6|24.6|31.6|+7.0|7.0(\u00b11.3)|20.6(\u00b12.1)|+13.6|", "caption": "Table 3: Model performance (fine-tuned on 491 SWE-Gym-sampled trajectories) on SWE-Bench (Jimenez et\u00a0al., 2024) using OpenHands (Wang et\u00a0al., 2024c) as agent scaffold. We use Qwen-2.5-Coder-Instruct as the base model. We set temperature t=0\ud835\udc610t=0italic_t = 0 for evaluation.", "description": "This table presents the performance of different sized language models (7B, 14B, and 32B parameters) fine-tuned using trajectories from the SWE-Gym dataset.  The models were evaluated on the SWE-Bench Lite and SWE-Bench Verified benchmarks.  The evaluation used the OpenHands agent scaffold and a temperature of 0 during the testing phase.  The table shows the resolve rate, empty patch rate, stuck-in-loop rate, and average number of turns for each model configuration.  These metrics provide insights into the model's ability to successfully complete software engineering tasks and its efficiency during the process.", "section": "3. Training LMs as SWE Agents with SWE-Gym"}, {"content": "| Cap | # Traj | Empty Patch (%,\"\n\u2193) | Resolve Rate (%,\"\n\u2191) |\n|---|---|---|---| \n| 0 (Zero-shot) | 0 | 56.3 | 7.0 |\n| 1 | 36 | 37.3 | 9.0 |\n| 2 | 62 | 29 | 9.7 |\n| 3 | 82 | 43.7 | 7.7 |\n| No Cap (All) | 172 | 30.7 | 9.3 |", "caption": "Table 4: Resolve rate and empty patch rate on SWE-Bench Lite after 7B model trained with with data from different instance capping strategies (Cap) and therefore different number of trajectories (Traj).", "description": "This table shows the results of training a 7B model on the SWE-Bench Lite dataset using different instance capping strategies.  The instance capping limits the number of trajectories sampled per instance, which indirectly affects the number of total training trajectories. The table compares the resolve rate (percentage of successfully resolved tasks) and the empty patch rate (percentage of tasks with no code edits) achieved under each capping strategy.  It demonstrates the impact of data balancing and the trade-off between the quantity and quality of training data on model performance.", "section": "2. SWE-Gym Environment"}, {"content": "| Setting | 7B Model |  | 32B Model |  |\n|---|---|---|---|---|\n| **EP(<span>\\%,\u2193</span>)** | **RR(<span>\\%,\u2191</span>)** | **EP(<span>\\%,\u2193</span>)** | **RR(<span>\\%,\u2191</span>)** |\n| Zero-Shot | 56.3% | 7.0% | 24.3% | 19.0% |\n| Iteration 1 | 29.0% | 9.0% | 18.3% | 19.7% |\n| Iteration 2 | 23.3% | 10.0% | 9.7% | 19.7% |", "caption": "Table 5: Resolve rate (RR) and Empty patch rate (EP) on SWE-Bench Lite with MoatlessTools Scaffold after online rejection sampling fine-tuning, evaluated at temperature t=0\ud835\udc610t=0italic_t = 0. RR shown in highlighted columns.", "description": "This table presents the results of experiments using the MoatlessTools agent scaffold on the SWE-Bench Lite dataset.  The experiments involved online rejection sampling fine-tuning, a technique used to improve model performance. The table shows the Resolve Rate (RR), which represents the percentage of successfully resolved problems, and the Empty Patch Rate (EP), which indicates the percentage of tasks where no code changes were made by the model.  The Resolve Rate is presented in the highlighted columns, and all results are evaluated at a temperature of 0.  The purpose is to demonstrate the effectiveness of the MoatlessTools scaffold combined with online rejection sampling fine-tuning.", "section": "3.3. Self-improvement with Specialized Workflow"}, {"content": "| Model | SWE-Bench | SWE-Bench | Openness | Openness |\n|---|---|---|---|---|\n| Name, Model Size | Lite | Verified | Model | Environment |\n| Ma et al. (2024), 72B | 22.0 | 30.2 | \u2713 | \u2717 |\n| Golubev et al. (2024) Agent and Verifier, 72B | - | 40.6 | \u2717 | \u2713 |\n| Our SWE-Gym Agent and Verifier, 32B | 26.0 | 32.0 | \u2713 | \u2713 |", "caption": "Table 6: Comparison of model performance on SWE-Bench benchmark and if the model weights and environments are publically accessible (openness).", "description": "This table compares the performance of different models on the SWE-Bench benchmark.  It highlights not only the achieved results (on both the Lite and Verified subsets of SWE-Bench) but also the accessibility of the model weights and the environments used for training. This allows for a comparative analysis considering both performance and reproducibility.", "section": "4. Scaling Agent Improvements with SWE-Gym"}, {"content": "|               | Original | Dedup. | Sorted by Random (Dedup.) | Sorted by Random (Dedup.) | Sorted by Repo (Dedup.) | Sorted by Repo (Dedup.) |\n| :------------- | :-------: | :-----: | :-----------------------: | :-----------------------: | :---------------------: | :---------------------: |\n|               |           |         |          First 25%         |         First 50%         |        First 25%        |        First 50%        |\n| getmoto/moto   |    155    |    72   |           12            |           33            |           0           |           46           |\n| Project-MONAI/MONAI |    95    |    53   |           17            |           25            |           53           |           53           |\n| pandas-dev/pandas |    70    |    61   |           14            |           30            |           0           |           0           |\n| python/mypy    |    46    |    27   |            7             |           12            |           0           |           0           |\n| dask/dask      |    45    |    29   |            8             |           17            |           6           |           29           |\n| iterative/dvc  |    36    |    24   |            8             |           12            |           0           |           0           |\n| conan-io/conan |    20    |    12   |            1             |            7             |           12           |           12           |\n| pydantic/pydantic|    11    |     7   |            2             |            4             |           0           |           0           |\n| facebookresearch/hydra |    7     |     5   |            2             |            5             |           0           |           5           |\n| bokeh/bokeh    |     3    |     2   |            1             |            1             |           2           |           2           |\n| modin-project/modin |     3    |     2   |            1             |            1             |           0           |           0           |\n| **Total**      |   491    |   294   |           73            |          147            |           73           |          147           |", "caption": "Table 7: Distribution of success trajectories used in training-time scaling experiments (\u00a74.2). Dedup. denotes that the trajectories are deduplicated by randomly select ONE success trajectory per instance ID; Sorted by random (repo) X% (Dedup.) denotes a subset of trajectories taken from the first X% from dedup. instances that are sorted randomly (by repository name).", "description": "Table 7 details the distribution of successful trajectories used in the training-time scaling experiments of section 4.2.  The table shows how the number of trajectories changes when different sampling methods are applied.  The original data contains all successful trajectories, while the \"Dedup.\" column shows the data after deduplication (one successful trajectory kept per instance ID). \"Sorted by random (repo) X% (Dedup.)\" represents subsets of the deduplicated data, where the first X% of instances are randomly selected (or sorted by repository name). This allows for analysis of how different data sampling strategies (random and repository-based) affect the results of the experiments.", "section": "4. Scaling Agent Improvements with SWE-Gym"}, {"content": "|               | Resolved | Count      | Mean       | Std        | Min        | Max        | 5%         | 10%        | 25%        | 50%        | 75%        | 90%        | 95%        |\n|---------------|----------|------------|------------|------------|------------|------------|------------|------------|------------|------------|------------|------------|------------|\n| Num. of Messages | \u2717        | 5,557.0    | 39.2       | 31.9       | 7.0        | 101.0      | 9.0        | 9.0        | 9.0        | 29.0       | 61.0       | 100.0      | 101.0      |\n|               | \u2713        | 491.0      | 39.9       | 19.9       | 13.0       | 101.0      | 19.0       | 21.0       | 25.0       | 33.0       | 47.5       | 65.0       | 87.0       |\n| Num. of Tokens | \u2717        | 5,557.0    | 17,218.3   | 17,761.6   | 1,615.0    | 167,834.0  | 1,833.0    | 1,907.0    | 2,268.0    | 12,305.0   | 26,434.0   | 41,182.2   | 51,780.6   |\n|               | \u2713        | 491.0      | 18,578.5   | 11,361.4   | 2,560.0    | 81,245.0   | 5,813.0    | 8,357.0    | 11,559.5   | 15,999.0   | 22,040.5   | 31,632.0   | 39,512.5   |", "caption": "Table 8: Statistics of SWE-Gym-sampled trajectories. We use the tokenizer from Qwen-2.5-Coder-Instruct-7B to estimate the number of tokens.", "description": "Table 8 presents a statistical overview of the trajectories sampled from SWE-Gym, which is a dataset used for training software engineering agents and verifiers. The table details the number of messages, and the total number of tokens in each trajectory.  The token count was calculated using the tokenizer from the Qwen-2.5-Coder-Instruct-7B language model.", "section": "2.2 SWE-Gym Lite"}, {"content": "| Agent | Model | Model Size | Training Data | Resolved (%) |\n|---|---|---|---|---|\n| **RAG** | SWE-Llama (Jimenez et al., 2024) | 7B | 10K instances | 1.4 |\n| **RAG** | SWE-Llama (Jimenez et al., 2024) | 13B | 10K instances | 1.2 |\n| **Lingma Agent (Ma et al., 2024)** | Lingma SWE-GPT (v0925) | 7B | 90K PRs from 4K repos | 18.2 |\n| **Lingma Agent (Ma et al., 2024)** | Lingma SWE-GPT (v0925) | 72B | 90K PRs from 4K repos | 28.8 |\n| **OpenHands (Wang et al., 2024c) (Ours)** | fine-tuned Qwen2.5-Coder-Instruct | 32B | 491 agent trajectories from 11 repos | 20.6 |\n| **OpenHands w/ Verifier (Wang et al., 2024c) (Ours)** | fine-tuned Qwen2.5-Coder-Instruct | 32B (Agent & Verifier) | 491 agent trajectories from 11 repos for agent + 1318 x 2 success/failure agent trajectories for verifier | **32.0** |", "caption": "Table 9: Performance comparison with SWE-Bench (Jimenez et\u00a0al., 2024) baselines with publicly accessible weights.\nData source: https://www.swebench.com/, Accessed on Dec 21, 2024.", "description": "This table compares the performance of different software engineering agents on the SWE-Bench benchmark.  It specifically focuses on models with publicly available weights, making the results reproducible. The table lists each agent's model size, the training data used, and the achieved resolution rate on SWE-Bench.  This allows for a direct comparison of different approaches and models in terms of their effectiveness at solving real-world software engineering problems. The data for SWE-Bench is sourced from https://www.swebench.com/ and accessed on December 21, 2024.", "section": "3. Training LMs as SWE Agents with SWE-Gym"}]