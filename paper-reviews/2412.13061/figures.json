[{"figure_path": "https://arxiv.org/html/2412.13061/extracted/6076804/imgs/radar.png", "caption": "Figure 1: Illustration of the quantitative comparison of discrete and continuous tokenization performance across our VidTok model and state-of-the-art methods, evaluated using four metrics: PSNR, SSIM, LPIPS, and FVD. All performance metrics are obtained through experiments conducted under a consistent evaluation protocol to ensure fairness and comparability. Larger chart areas correspond to better performance across all metrics.", "description": "This figure provides a quantitative comparison of VidTok and other state-of-the-art video tokenizers.  The comparison is split into three radar charts, one for discrete tokenization, and two for continuous tokenization (4 channel and 16 channel).  Each radar chart is measured across four metrics (PSNR, SSIM, LPIPS, and FVD).  The area within the radar chart polygon indicates the relative performance across each metric where a larger area denotes better performance.  VidTok appears to generally outperform the other methods.", "section": "4 Experiments"}, {"figure_path": "https://arxiv.org/html/2412.13061/x1.png", "caption": "Figure 2: An overview of video tokenizers.", "description": "This figure presents a high-level overview of the typical architecture of a video tokenizer.  The diagram illustrates the key components and the flow of information during the tokenization and reconstruction process.  A video input is first fed into an encoder, which compresses the video into a compact representation within a latent space.  Within this latent space, a regularizer is often applied to enforce certain properties or constraints, such as sparsity or smoothness.  Finally, a decoder takes the regularized latent representation and reconstructs the original video. Depending on the specific model, the latent space can be either continuous or discrete, and the architecture may operate in a causal or non-causal manner.", "section": "3. VIDTOK"}, {"figure_path": "https://arxiv.org/html/2412.13061/x2.png", "caption": "Figure 3: The improved model architecture. In the context of a causal setting, consider an input with dimensions T\u00d7H\u00d7W=17\u00d7256\u00d7256\ud835\udc47\ud835\udc3b\ud835\udc4a17256256T\\times H\\times W=17\\times 256\\times 256italic_T \u00d7 italic_H \u00d7 italic_W = 17 \u00d7 256 \u00d7 256. Assuming a temporal compression factor of 4444 and a spatial compression factor of 8888, the intermediate latent representation is reduced to dimensions T\u00d7H\u00d7W=5\u00d732\u00d732\ud835\udc47\ud835\udc3b\ud835\udc4a53232T\\times H\\times W=5\\times 32\\times 32italic_T \u00d7 italic_H \u00d7 italic_W = 5 \u00d7 32 \u00d7 32.", "description": "The figure presents the architecture of the video tokenizer, VidTok. It visually illustrates the flow of data through the model, highlighting the components responsible for temporal and spatial compression and decompression. The model takes a video input of size T\u00d7H\u00d7W (Time \u00d7 Height \u00d7 Width) and compresses it into a latent representation of smaller dimensions using an encoder with 2D convolutions and an AlphaBlender operator.  It then reconstructs the original video from the latent representation using a decoder with a similar structure, including 2D convolutions and AlphaBlender. In the context of causal video processing, additional considerations are introduced. The first frame is processed differently to allow the model to function for single images as well.  The diagram also depicts the specific modules used for temporal upsampling and downsampling. The improved model architecture uses a combination of 2D and 3D convolutional layers to reduce computational demands without compromising performance.", "section": "3. VidTok"}, {"figure_path": "https://arxiv.org/html/2412.13061/x3.png", "caption": "Figure 4: Left: Vector Quantization (VQ) employed in Vector Quantised-Variational AutoEncoder (VQ-VAE)\u00a0(Van Den\u00a0Oord et\u00a0al., 2017). Right: Finite Scalar Quantization (FSQ)\u00a0(Mentzer et\u00a0al., 2024) utilized in our model.", "description": "The figure provides a visual comparison of two quantization methods used in the proposed video tokenizer model, VidTok.  Vector Quantization (VQ), shown on the left, maps latent vectors to entries in a learned codebook. This is the method used in VQ-VAE (Vector Quantised-Variational AutoEncoder). Finite Scalar Quantization (FSQ), shown on the right, directly quantizes each scalar value in the latent space to predefined discrete levels and does not require learning a codebook. VidTok uses FSQ.  FSQ enhances training stability and improves codebook utilization.", "section": "3.3 FINITE SCALAR QUANTIZATION"}, {"figure_path": "https://arxiv.org/html/2412.13061/extracted/6076804/imgs/results/full_comp.png", "caption": "Figure 5: Qualitative comparison with the state-of-the-art video tokenizers.", "description": "Qualitative comparison of VidTok with state-of-the-art video tokenizers, showcasing the effectiveness of VidTok in reconstructing video frames with high fidelity. The comparison includes various state-of-the-art models such as OmniTokenizer, Cosmos-DV, CV-VAE, Open-Sora-v1.2, Open-Sora-Plan-v1.2, and CogVideoX, across a range of video content.  The reconstructed frames by each model are juxtaposed with the original frames, allowing a clear assessment of their performance.", "section": "4 Experiments"}, {"figure_path": "https://arxiv.org/html/2412.13061/extracted/6076804/imgs/results/fps8_fps3.png", "caption": "Figure 6: The influence of different sample rates on model performance during training. The second row presents the test results obtained using training data with a sample rate of 8 FPS, while the third row shows the test results using training data with a sample rate of 3 FPS. The results demonstrate that employing training data with reduced frame rates enhances the model\u2019s capacity to effectively capture motion dynamics.", "description": "This figure shows the impact of using different frame rates for the training data on the performance of a video tokenizer model.  The top row displays the ground truth frames. The middle row shows the reconstructed frames when the model was trained with data at 8 frames per second (FPS). The bottom row shows the results when trained with data at 3 FPS. The comparison suggests that training with a lower frame rate (3 FPS) leads to better capturing of motion dynamics, resulting in improved reconstruction, especially noticeable in the fox's legs and tail.  This observation implies that a lower frame rate during training might encourage the model to focus more on the overall motion flow rather than fine-grained details within individual frames.", "section": "4 Experiments"}]