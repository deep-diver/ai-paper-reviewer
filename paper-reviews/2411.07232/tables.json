[{"content": "|               | I-Pix2Pix | Erasedraw | Magicbrush | SDEdit | P2P | Ours |\n| :------------ | :--------: | :--------: | :--------: | :------: | :-: | :-: |\n| Affordance    |   0.276    |   0.341    |   0.418    |   0.397   | 0.474 | 0.828 |", "caption": "Table 1: Comparison of methods based on Affordance score for the Additing Affordance Benchmark.", "description": "This table presents a comparison of different image editing methods based on their performance on the Additing Affordance Benchmark.  The Additing Affordance Benchmark specifically focuses on evaluating the plausibility of object placement in images after adding new objects. The table shows the Affordance scores achieved by several methods, including the authors' proposed method (Ours), highlighting its superior performance in accurately and naturally placing objects within the context of the original image.", "section": "4 Experiments"}, {"content": "| Method | Emu Edit CLIP<sub>dir</sub> | Emu Edit CLIP<sub>out</sub> | Emu Edit CLIP<sub>im</sub> | Emu Edit Inc. | Additing Benchmark CLIP<sub>dir</sub> | Additing Benchmark CLIP<sub>out</sub> | Additing Benchmark CLIP<sub>im</sub> | Additing Benchmark Inc. |\n|---|---|---|---|---|---|---|---|---|\n| InstructPix2Pix | 0.074 | 0.312 | 0.929 | 34% | 0.074 | 0.244 | 0.943 | 55% |\n| Erasedraw | 0.088 | 0.313 | 0.941 | 65% | 0.117 | 0.248 | 0.958 | 76% |\n| Magicbrush | 0.091 | 0.313 | 0.927 | 66% | 0.114 | 0.250 | 0.925 | 86% |\n| SDEdit | \u2014 | \u2014 | \u2014 | \u2014 | 0.091 | 0.248 | 0.955 | 60% |\n| Prompt2Prompt | \u2014 | \u2014 | \u2014 | \u2014 | 0.170 | 0.280 | 0.850 | 97% |\n| **Ours** | **0.101** | **0.322** | **0.929** | **81%** | **0.200** | **0.261** | **0.968** | **93%** |", "caption": "Table 2: CLIP and Inclusion metric results for EmuEdit and Additing Benchmark.", "description": "This table presents a quantitative comparison of different image editing methods on two benchmark datasets: EmuEdit and Additing Benchmark.  For each method, it shows the performance in terms of three CLIP (Contrastive Language-Image Pre-training) scores: CLIPdir (measuring the consistency between image and caption changes), CLIPout (measuring the similarity between the edited image and caption), and CLIPim (measuring the similarity between the edited and original images). Additionally, it includes the 'Inclusion' metric, indicating the percentage of images where the object insertion was successful.  The results highlight the relative strengths and weaknesses of each method across the two benchmarks.", "section": "4 Experiments"}]