{"references": [{"fullname_first_author": "Josh Achiam", "paper_title": "GPT-4 Technical Report", "publication_date": "2023-03-08", "reason": "This paper provides a comprehensive technical report on the GPT-4 model, which is a large language model (LLM) used extensively in the field and directly relevant to the current research on adaptive decoding."}, {"fullname_first_author": "Abhimanyu Dubey", "paper_title": "The LLAMA 3 Herd of Models", "publication_date": "2024-07-21", "reason": "This paper introduces the LLAMA 3 family of models, which are foundational to the current research and serve as the base models in the experiments of the current paper."}, {"fullname_first_author": "Takeshi Kojima", "paper_title": "Large Language Models are Zero-Shot Reasoners", "publication_date": "2022-12-01", "reason": "This paper establishes the capability of LLMs to perform zero-shot reasoning, a key aspect influencing the choice of tasks and evaluation metrics in the current work."}, {"fullname_first_author": "Long Ouyang", "paper_title": "Training Language Models to Follow Instructions with Human Feedback", "publication_date": "2022-12-01", "reason": "This paper details the process of training LLMs using human feedback, a technique highly relevant to the current work's use of preference optimization and reward models."}, {"fullname_first_author": "Rafael Rafailov", "paper_title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model", "publication_date": "2024-12-01", "reason": "This paper introduces a simplified approach to preference optimization, which is directly adopted and extended by the current research for training discrete latent variables like decoding temperature."}]}