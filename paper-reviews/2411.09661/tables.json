[{"content": "| Method | 3-gram-repeats \u2193 | % of non-greedy |\n|---|---|---|\n| Greedy Decoding | 0.36% | 0% |\n| AdaptiveDecoder<sub>tok</sub> | 0.22% | 94% |", "caption": "Table 1: Reducing Repeats using the AdaptiveDecoder. We feed text from Wikitext-2 to the model and ask it to complete it. When completing a text, AdaptiveDecodert\u2062o\u2062ksubscriptAdaptiveDecoder\ud835\udc61\ud835\udc5c\ud835\udc58\\textsc{AdaptiveDecoder}_{tok}AdaptiveDecoder start_POSTSUBSCRIPT italic_t italic_o italic_k end_POSTSUBSCRIPT learns to avoid greedy decoding in order to reduce repeats. In 94% of samples, AdaptiveDecodert\u2062o\u2062ksubscriptAdaptiveDecoder\ud835\udc61\ud835\udc5c\ud835\udc58\\textsc{AdaptiveDecoder}_{tok}AdaptiveDecoder start_POSTSUBSCRIPT italic_t italic_o italic_k end_POSTSUBSCRIPT learns to pick a non-greedy temperature.", "description": "This table presents the results of an experiment designed to evaluate the effectiveness of the AdaptiveDecoder<sub>tok</sub> in reducing n-gram repetitions during text generation.  The experiment involved feeding text from the Wikitext-2 dataset to a language model equipped with the AdaptiveDecoder<sub>tok</sub> and measuring the frequency of n-gram repetitions in the generated text.  The table shows that the AdaptiveDecoder<sub>tok</sub> successfully learned to reduce these repetitions, and it achieved this by selecting non-greedy temperatures (higher temperatures produce more diverse and creative text, reducing the likelihood of repetition) in 94% of the samples.", "section": "4. Experiments"}, {"content": "| Prompt | Predicted \u03c4 |\n|---|---| \n| **Detailed Instructions:** In this task, you are given a country name and you need to return the capital city of the given country. Problem:Guinea-Bissau Solution: | 0.0 |\n| Write a compelling short story about a bitter and intense rivalry between two individuals, where one must have an advantage in terms of their socioeconomic status or physical ability. The story must also incorporate a surprising twist that leads to an unforeseen outcome.  | 1.0 |", "caption": "Table 2: Examples of AdaptiveDecoders\u2062e\u2062qsubscriptAdaptiveDecoder\ud835\udc60\ud835\udc52\ud835\udc5e\\textsc{AdaptiveDecoder}_{seq}AdaptiveDecoder start_POSTSUBSCRIPT italic_s italic_e italic_q end_POSTSUBSCRIPT Predicted Temperatures (\u03c4\ud835\udf0f\\tauitalic_\u03c4) on UltraFeedback.\nHere we show examples of UltraFeedback test prompts where the AdaptiveDecoders\u2062e\u2062qsubscriptAdaptiveDecoder\ud835\udc60\ud835\udc52\ud835\udc5e\\textsc{AdaptiveDecoder}_{seq}AdaptiveDecoder start_POSTSUBSCRIPT italic_s italic_e italic_q end_POSTSUBSCRIPT model predicted \u03c4\u2208{0.0,1.0}\ud835\udf0f0.01.0\\tau\\in\\{0.0,1.0\\}italic_\u03c4 \u2208 { 0.0 , 1.0 }. That is, our model predicts the top prompt requires a factual deterministic response (\u03c4=0.0\ud835\udf0f0.0\\tau=0.0italic_\u03c4 = 0.0), while the bottom prompt requires a creative, stochastic response (\u03c4=1.0\ud835\udf0f1.0\\tau=1.0italic_\u03c4 = 1.0).\nMore examples are shown in Appendix Table\u00a013.", "description": "Table 2 shows examples from the UltraFeedback test set where the model's Adaptive Decoder chose either a very low temperature (0.0, for deterministic, factual responses) or a very high temperature (1.0, for creative, stochastic responses).  This illustrates the model's ability to dynamically select appropriate temperatures based on the task's requirements.  Additional examples are provided in Appendix Table 13.", "section": "4.3. UltraMathStories"}, {"content": "| Decoding Method | Accuracy \u2191 (Majority of N=8 responses) | Accuracy \u2191 (N=1 response) |\n|---|---|---|\n| Best Fixed Temperature | 87.46 | 81.59 |\n| \\textsc{AdaptiveDecoder}_{tok} ($\\tau\\in\\{0.0,0.4,0.8,1.0\\}$) | 87.70 | 80.47 |\n| \\textsc{AdaptiveDecoder}_{tok} ($\\tau\\in\\{0.0,0.4,0.8,1.0,1.2\\}$) | **87.95** | 80.51 |", "caption": "Table 3: AdaptiveDecodert\u2062o\u2062ksubscriptAdaptiveDecoder\ud835\udc61\ud835\udc5c\ud835\udc58\\textsc{AdaptiveDecoder}_{tok}AdaptiveDecoder start_POSTSUBSCRIPT italic_t italic_o italic_k end_POSTSUBSCRIPT for majority voting (8 samples) on the GSM8K dataset. AdaptiveDecodert\u2062o\u2062ksubscriptAdaptiveDecoder\ud835\udc61\ud835\udc5c\ud835\udc58\\textsc{AdaptiveDecoder}_{tok}AdaptiveDecoder start_POSTSUBSCRIPT italic_t italic_o italic_k end_POSTSUBSCRIPT learns to assign appropriate temperatures at different parts of the generation which allows for more accurate sampled reasoning chains which results in a higher accuracy than using a single tuned temperature for the dataset. We also include the accuracy for N=1 response, which underperforms majority voting.", "description": "This table presents the results of using the AdaptiveDecoder<sub>tok</sub> model for majority voting on the GSM8K dataset.  The AdaptiveDecoder<sub>tok</sub> model dynamically adjusts the decoding temperature during generation, leading to more accurate reasoning chains compared to using a single, fixed temperature. The table shows accuracy improvements when using majority voting (averaging results from 8 samples) and highlights the lower accuracy of using a single response, demonstrating the benefits of the AdaptiveDecoder<sub>tok</sub>'s adaptive approach.", "section": "4. Experiments"}, {"content": "| Fixed Temperature | AdaptiveDecoder<sub>seq</sub> | \u03c4=0 | \u03c4=0.6 | \u03c4=1.0 |  |  |  |\n|---|---|---|---|---|---|---|---| \n| **81.59** | **81.59** | 81.59 | 79.15 | 78.32 | <p style='text-align:center'>\u2112<sub>**LPO**</sub><br>(Equation\u00a010)</p> | <p style='text-align:center'>\u2112<sub>**LPO**</sub><br>(Section\u00a03.3)</p> | <p style='text-align:center'>\u2112<sub>**NLL**</sub></p> |", "caption": "Table 4: GSM8K Accuracy comparing different loss functions for training a sequence-leval AdaptiveDecoder (ADs\u2062e\u2062qsubscriptAD\ud835\udc60\ud835\udc52\ud835\udc5e\\textsc{AD}_{seq}AD start_POSTSUBSCRIPT italic_s italic_e italic_q end_POSTSUBSCRIPT). We compare two different \u2112LPOsubscript\u2112LPO\\mathcal{L}_{\\text{LPO{}}}caligraphic_L start_POSTSUBSCRIPT LPO end_POSTSUBSCRIPT loss functions, as outlined in Section\u00a03.3, as well as negative log likelihood loss, \u2112NLLsubscript\u2112NLL\\mathcal{L}_{\\text{NLL}}caligraphic_L start_POSTSUBSCRIPT NLL end_POSTSUBSCRIPT, trained on the chosen responses from the preference pairs.", "description": "This table presents a comparison of different loss functions used to train a sequence-level Adaptive Decoder model on the GSM8K dataset.  The goal is to determine which loss function yields the best accuracy. Three loss functions are compared: two variants of the Latent Preference Optimization (LPO) loss (detailed in section 3.3) and the standard negative log-likelihood (NLL) loss.  The NLL loss is trained only on the chosen responses from the preference pairs, while the LPO loss functions utilize both chosen and rejected responses to learn the optimal parameters for selecting temperatures.  The table shows the accuracy achieved by each loss function on the GSM8K dataset, allowing for a direct comparison of their performance.", "section": "4. Experiments"}, {"content": "|---|---|---|---| \n| **Fixed Temp.** |  | **Temperature Selection** |  | \n|  |  | Greedy (Equation 4) | Sample (Equation 5) | \n| \u03c4=0.0 |  | 53.10 | 52.80 | \n| \u03c4=0.2 |  | 53.35 | 53.15 | \n| \u03c4=0.4 |  | 50.80 | 51.75 | \n| \u03c4=0.6 |  | 52.15 | 52.50 | \n| \u03c4=0.8 |  | 52.78 | 53.65 | \n| \u03c4=1.0 |  | 54.89 | 53.95 |", "caption": "Table 5: AdaptiveDecoder Temperature Selection Methods on UltraFeedback. The AdaptiveDecoder outputs a distribution over temperature values \u03c4\ud835\udf0f\\tauitalic_\u03c4, so we can either sample \u03c4\ud835\udf0f\\tauitalic_\u03c4 from that distribution or greedily select the highest probability \u03c4\ud835\udf0f\\tauitalic_\u03c4.\nHere we show winrates against the fixed temperature decoding in the left column, using the AdaptiveDecoders\u2062e\u2062qsubscriptAdaptiveDecoder\ud835\udc60\ud835\udc52\ud835\udc5e\\textsc{AdaptiveDecoder}_{seq}AdaptiveDecoder start_POSTSUBSCRIPT italic_s italic_e italic_q end_POSTSUBSCRIPT model trained on UltraMathStories (Section\u00a04.3).\nAll the winrates are above 50%, which means the AdaptiveDecoder always outperforms the fixed temperature.\nAlso, we do not observe a significant difference between the two temperature selection methods.", "description": "This table compares two methods for selecting temperatures (a hyperparameter controlling randomness in text generation) within the AdaptiveDecoder model. The AdaptiveDecoder model dynamically chooses the temperature for each token generated, balancing creativity and accuracy.  The first method involves sampling a temperature from a probability distribution. The second method greedily selects the temperature with the highest probability. The table shows the win rates (percentage of correct predictions) for each temperature selection method against fixed temperature methods for UltraFeedback.  UltraFeedback represents a diverse set of tasks requiring varying levels of randomness in the outputs. Results show the AdaptiveDecoder consistently outperforms the fixed temperature baselines across different temperatures, irrespective of the temperature selection method used. ", "section": "4. Experiments"}, {"content": "| Fixed Temp | AdaptiveDecoder<sub>seq</sub><br>Winrate | Fixed Temp<br>Winrate |\n|---|---|---|\n| \u03c4=0.0 | 53.10 | 46.90 |\n| \u03c4=0.2 | 53.35 | 46.65 |\n| \u03c4=0.4 | 50.80 | 49.20 |\n| \u03c4=0.6 | 52.15 | 47.85 |\n| \u03c4=0.8 | 52.78 | 47.22 |\n| \u03c4=1.0 | 54.89 | 45.11 |", "caption": "Table 6: AdaptiveDecoders\u2062e\u2062qsubscriptAdaptiveDecoder\ud835\udc60\ud835\udc52\ud835\udc5e\\textsc{AdaptiveDecoder}_{seq}AdaptiveDecoder start_POSTSUBSCRIPT italic_s italic_e italic_q end_POSTSUBSCRIPT vs Fixed Temperatures Winrates on the UltraFeedback Task.", "description": "This table presents a comparison of the win rates achieved by the ADAPTIVEDECODERseq model against various fixed temperatures on the UltraFeedback task.  The ADAPTIVEDECODERseq model dynamically adjusts the decoding temperature, offering a potential improvement over static temperature approaches. The win rate, a common metric for evaluating model performance, is presented for different fixed temperatures to highlight the impact of temperature on performance.", "section": "4.3. UltraMathStories"}, {"content": "| Fixed Temp | AdaptiveDecoder<sub>seq</sub> Winrate | Fixed Temp Winrate |\n|---|---|---|\n| \u03c4=0.0 | 58.75 | 41.25 |\n| \u03c4=0.2 | 57.25 | 42.75 |\n| \u03c4=0.4 | 57.05 | 42.95 |\n| \u03c4=0.6 | 56.65 | 43.35 |\n| \u03c4=0.8 | 54.55 | 45.45 |\n| \u03c4=1.0 | 52.10 | 47.90 |", "caption": "Table 7: AdaptiveDecoders\u2062e\u2062qsubscriptAdaptiveDecoder\ud835\udc60\ud835\udc52\ud835\udc5e\\textsc{AdaptiveDecoder}_{seq}AdaptiveDecoder start_POSTSUBSCRIPT italic_s italic_e italic_q end_POSTSUBSCRIPT vs Fixed Temperatures Winrates on the Stories Task.", "description": "This table presents the results of comparing the performance of the sequence-level adaptive decoder (ADseq) against fixed-temperature decoding methods on a creative story writing task.  Winrates are calculated by comparing the ADseq model's outputs to outputs generated using various fixed temperatures. Higher winrates indicate superior performance. This comparison helps to demonstrate the effectiveness of the adaptive decoding approach in achieving higher accuracy compared to a single, fixed temperature setting.", "section": "4.3 UltraMathStories"}, {"content": "| Fixed Temp | AdaptiveDecoder<sub>seq</sub> Winrate | Fixed Temp Winrate |\n|---|---|---|\n| \u03c4=0.0 | 50.68 | 49.32 |\n| \u03c4=0.2 | 51.10 | 48.90 |\n| \u03c4=0.4 | 51.14 | 48.86 |\n| \u03c4=0.6 | 51.40 | 48.60 |\n| \u03c4=0.8 | 51.42 | 48.58 |\n| \u03c4=1.0 | 51.82 | 48.18 |", "caption": "Table 8: AdaptiveDecoders\u2062e\u2062qsubscriptAdaptiveDecoder\ud835\udc60\ud835\udc52\ud835\udc5e\\textsc{AdaptiveDecoder}_{seq}AdaptiveDecoder start_POSTSUBSCRIPT italic_s italic_e italic_q end_POSTSUBSCRIPT vs Fixed Temperatures Winrates on the GSM8K Task.", "description": "This table presents the win rates achieved by the ADAPTIVEDECODERseq model, compared to models using fixed temperatures, on the GSM8K (Grade School Math 8K) task.  The GSM8K task involves solving math word problems, and the win rate represents the percentage of problems where the ADAPTIVEDECODERseq model's solution was more accurate than the model using a fixed temperature.  The table helps demonstrate the ADAPTIVEDECODERseq model's ability to adapt its temperature dynamically to improve accuracy on a specific task.", "section": "4.3. UltraMathStories"}, {"content": "| Fixed Temp | AdaptiveDecoder<sub>tok</sub> Winrate | Fixed Temp Winrate |\n|---|---|---|\n| \u03c4=0.0 | 49.60 | 50.40 |\n| \u03c4=0.2 | 50.70 | 49.30 |\n| \u03c4=0.4 | 48.75 | 51.25 |\n| \u03c4=0.6 | 49.60 | 50.40 |\n| \u03c4=0.8 | 49.25 | 50.75 |\n| \u03c4=1.0 | 52.75 | 47.25 |", "caption": "Table 9: AdaptiveDecodert\u2062o\u2062ksubscriptAdaptiveDecoder\ud835\udc61\ud835\udc5c\ud835\udc58\\textsc{AdaptiveDecoder}_{tok}AdaptiveDecoder start_POSTSUBSCRIPT italic_t italic_o italic_k end_POSTSUBSCRIPT vs Fixed Temperatures Winrates on the UltraFeedback Task.", "description": "This table presents a comparison of the win rates achieved by the AdaptiveDecoder<sub>tok</sub> model and those obtained using fixed temperature decoding strategies on the UltraFeedback task.  It shows the performance of AdaptiveDecoder<sub>tok</sub> (a model that dynamically adjusts decoding temperature) against several fixed-temperature baselines (0.0, 0.2, 0.4, 0.6, 0.8, and 1.0).  Each row represents a different fixed temperature, and the win rate is a measure of the model's success relative to the baselines on the UltraFeedback task.", "section": "4. Experiments"}, {"content": "| Fixed Temp | AdaptiveDecoder<sub>tok</sub> Winrate | Fixed Temp Winrate |\n|---|---|---|\n| \u03c4=0.0 | 54.40 | 45.60 |\n| \u03c4=0.2 | 53.40 | 46.60 |\n| \u03c4=0.4 | 54.20 | 45.80 |\n| \u03c4=0.6 | 52.30 | 47.70 |\n| \u03c4=0.8 | 51.10 | 48.90 |\n| \u03c4=1.0 | 47.25 | 52.75 |", "caption": "Table 10: AdaptiveDecodert\u2062o\u2062ksubscriptAdaptiveDecoder\ud835\udc61\ud835\udc5c\ud835\udc58\\textsc{AdaptiveDecoder}_{tok}AdaptiveDecoder start_POSTSUBSCRIPT italic_t italic_o italic_k end_POSTSUBSCRIPT vs Fixed Temperatures Winrates on the Stories Task.", "description": "This table presents a comparison of the win rates achieved by using the token-level adaptive decoder (AdaptiveDecoder<sub>tok</sub>) against those obtained using various fixed temperatures for text generation in a creative story writing task.  The win rate is a measure of the model's success in generating high-quality responses compared to a baseline. The table helps to illustrate the effectiveness of dynamically adjusting the temperature during decoding compared to employing a fixed temperature across all text generations.", "section": "4.3. UltraMathStories"}, {"content": "| Fixed Temp | AdaptiveDecoder<sub>tok</sub><br>Winrate | Fixed Temp<br>Winrate |\n|---|---|---|\n| \\(\\tau=0.0\\) | 49.66 | 50.34 |\n| \\(\\tau=0.2\\) | 50.08 | 49.92 |\n| \\(\\tau=0.4\\) | 50.11 | 49.89 |\n| \\(\\tau=0.6\\) | 50.38 | 49.62 |\n| \\(\\tau=0.8\\) | 50.49 | 49.51 |\n| \\(\\tau=1.0\\) | 51.55 | 48.45 |", "caption": "Table 11: AdaptiveDecodert\u2062o\u2062ksubscriptAdaptiveDecoder\ud835\udc61\ud835\udc5c\ud835\udc58\\textsc{AdaptiveDecoder}_{tok}AdaptiveDecoder start_POSTSUBSCRIPT italic_t italic_o italic_k end_POSTSUBSCRIPT vs Fixed Temperatures Winrates on the GSM8K Task.", "description": "This table presents the win rates achieved by the AdaptiveDecoder<sub>tok</sub> model compared to models using fixed temperatures on the GSM8K math reasoning task.  The AdaptiveDecoder<sub>tok</sub> model dynamically adjusts the decoding temperature at the token level, while the fixed-temperature models use a single temperature throughout the entire decoding process.  Win rate is a metric representing the percentage of times a model's answer to a question was correct compared to another method.", "section": "4. Experiments"}, {"content": "| Fixed Temp | AdaptiveDecoder<sub>tok</sub><br>Constraint Winrate | AdaptiveDecoder<sub>tok</sub><br>ArmoRM Winrate | AdaptiveDecoder<sub>tok</sub><br>Avg Winrate |\n|---|---|---|---|\n| \u03c4=0.0 | 50.95 | 52.55 | 51.75 |\n| \u03c4=0.2 | 53.70 | 49.50 | 51.60 |\n| \u03c4=0.4 | 58.05 | 48.25 | 53.15 |\n| \u03c4=0.6 | 68.05 | 41.05 | 54.55 |\n| \u03c4=0.8 | 77.85 | 36.45 | 57.15 |\n| \u03c4=1.0 | 87.80 | 31.50 | 59.65 |", "caption": "Table 12: AdaptiveDecodert\u2062o\u2062ksubscriptAdaptiveDecoder\ud835\udc61\ud835\udc5c\ud835\udc58\\textsc{AdaptiveDecoder}_{tok}AdaptiveDecoder start_POSTSUBSCRIPT italic_t italic_o italic_k end_POSTSUBSCRIPT Constrained Creative Writing Individual Winrates. Here we show the individual winrates of the AdaptiveDecodert\u2062o\u2062ksubscriptAdaptiveDecoder\ud835\udc61\ud835\udc5c\ud835\udc58\\textsc{AdaptiveDecoder}_{tok}AdaptiveDecoder start_POSTSUBSCRIPT italic_t italic_o italic_k end_POSTSUBSCRIPT for both constraint following and ArmoRM score. The AdaptiveDecodert\u2062o\u2062ksubscriptAdaptiveDecoder\ud835\udc61\ud835\udc5c\ud835\udc58\\textsc{AdaptiveDecoder}_{tok}AdaptiveDecoder start_POSTSUBSCRIPT italic_t italic_o italic_k end_POSTSUBSCRIPT learns to follow the constraint better than all fixed temperatures, but as we compare to higher fixed temperatures, the story winrate goes down because it follows the constraint better.", "description": "This table presents a detailed breakdown of the performance of the AdaptiveDecoder<sub>tok</sub> model on a constrained creative writing task.  It shows the individual win rates for two separate evaluation metrics: constraint satisfaction (how well the model followed the rule of starting each sentence with \"Ab\") and ArmoRM score (a measure of the quality of the generated story).  The results are compared against using various fixed temperatures during decoding.  The AdaptiveDecoder<sub>tok</sub> model demonstrates an ability to balance constraint satisfaction with story quality, outperforming fixed temperature approaches.  However, as fixed temperatures increase, the constraint satisfaction rate improves at the cost of lower story quality, highlighting the model's ability to dynamically adjust temperature during generation.", "section": "4.4. Constrained Creative Writing (ConstrainedStories)"}, {"content": "| Predicted \u03c4=0.0 | \n|---|---| \n| In this task, given a sentence in the English language, your task is to convert it into the Thai language. | \n| Problem:The secondary principals\u2019 association head, Graham Young, said: T\u0308he NCEA system put pressure on schools to accumulate credits - and the easiest way to do that was to encourage students into internally assessed unit standards. | \n| Solution: | \n| You are given a math word problem and you are supposed to apply multiple mathematical operators like addition, subtraction, multiplication, or division on the numbers embedded in the text to answer the following question and then only report the final numerical answer. | \n| Input: Consider Input: debby makes 67 pancakes . she adds blueberries to 20 of them and bananas to 24 of them . the rest are plain . how many plain pancakes are there ? | \n| You have been tasked with arranging a group of travelers, each with different preferences and needs, onto various modes of transportation. There are four modes of transportation available: A, B, C, and D. Each mode has its own unique features and limitations. The travelers and their preferences are as follows: | \n| 1. Alice: Is afraid of flying and prefers to take mode C or D | \n| 2. Bob: Can only travel by mode A due to motion sickness | \n| 3. Charlie: Wants to take mode B because it has the shortest travel time | \n| 4. Dave: Needs to take mode D because he has a lot of luggage | \n| 5. Ellie: Wants to take mode A because she enjoys the scenic route | \n| Your task is to assign each traveler to the mode of transportation that best suits their needs and preferences. Keep in mind that each mode of transportation can only accommodate a certain number of people, and some modes may have already reached their capacity. Can you solve this puzzle and successfully group the travelers onto their preferred modes of transportation? | \n| Predicted \u03c4=1.0 | \n| Write a 70,000 word fantasy novel about a hidden world of magic and mythical creatures. The main character must be a human who discovers this world and becomes involved in a conflict between the magical creatures. The novel should have a fast-paced plot with plenty of action and suspense. The style should be descriptive and immersive, with detailed descriptions of the magical world and its inhabitants. The novel should also explore themes such as the nature of power and the importance of loyalty and friendship. | \n| Write me a 1000 word ghost story in a campfire setting | \n| Write a story about Ego Must, a prominent innovator with technology who leverages his vast wealth to communicate his views. However, despite being exceptionally smart he seems to not understand the basics when it comes to the \u2019us and them\u2019 problem that is at the root of a lot of human conflict. |", "caption": "Table 13: Examples of AdaptiveDecoders\u2062e\u2062qsubscriptAdaptiveDecoder\ud835\udc60\ud835\udc52\ud835\udc5e\\textsc{AdaptiveDecoder}_{seq}AdaptiveDecoder start_POSTSUBSCRIPT italic_s italic_e italic_q end_POSTSUBSCRIPT Predicted Temperatures (\u03c4\ud835\udf0f\\tauitalic_\u03c4) on UltraFeedback.\nHere we show examples of UltraFeedback test prompts where the AdaptiveDecoders\u2062e\u2062qsubscriptAdaptiveDecoder\ud835\udc60\ud835\udc52\ud835\udc5e\\textsc{AdaptiveDecoder}_{seq}AdaptiveDecoder start_POSTSUBSCRIPT italic_s italic_e italic_q end_POSTSUBSCRIPT model predicted \u03c4\u2208{0.0,1.0}\ud835\udf0f0.01.0\\tau\\in\\{0.0,1.0\\}italic_\u03c4 \u2208 { 0.0 , 1.0 }. We can see that the \u03c4=0.0\ud835\udf0f0.0\\tau=0.0italic_\u03c4 = 0.0 prompts require factual, deterministic responses, and the \u03c4=1.0\ud835\udf0f1.0\\tau=1.0italic_\u03c4 = 1.0 prompts require creative, stochastic responses. This shows generalization outside of the GSM8K and Stories subtasks to specific prompts within UltraFeedback.", "description": "Table 13 presents examples from the UltraFeedback test set where the AdaptiveDecoder model predicted temperatures of either 0.0 or 1.0.  The examples demonstrate the model's ability to adapt its decoding strategy:  prompts with a predicted temperature of 0.0 require factual, deterministic answers, while those with a predicted temperature of 1.0 call for creative, stochastic responses.  This showcases the model's capacity to generalize beyond the specific tasks (GSM8K and Stories) used in its initial training.", "section": "4. Experiments"}]