[{"content": "| Method | Reference | Efficiency | Constraint |\n|---|---|---|---| \n| SFT | Free | **High** | Weak |\n| DPO | **Offline** | **High** | Weak |\n| SimPO | Free | **High** | Weak |\n| TDPO | Needed | Low | **Strong / Dense** |\n| FPO(Ours) | **Offline** | **High** | **Strong / Sparse** |", "caption": "Table 1: Specific implementations of Log Probability Difference (LPD), Margin, and Constraint in Equation\u00a010 for DPO, its variants SimPO and TDPO, and the proposed FPO.", "description": "This table details the specific mathematical formulas and parameters used in Direct Preference Optimization (DPO), Simple Preference Optimization (SimPO), Token-Level Direct Preference Optimization (TDPO), and the novel Feature-level Preference Optimization (FPO) method.  It breaks down each method's calculation of the log probability difference (LPD), margin, and constraint terms, illustrating their similarities and differences. The table highlights how FPO incorporates a novel feature-level constraint using sparse autoencoders and an offline reference margin, improving efficiency and stability compared to existing methods.", "section": "2 PRELIMINARY"}, {"content": "| Method |  | LPD |  | Margin |  | Constraint |  | Constraint Type |\n|---|---|---|---|---|---|---|---|---|\n| DPO |  | \\(\\beta\\log\\pi_{\\theta}(y_{w}|x)-\\beta\\log\\pi_{\\theta}(y_{l}|x)\\) |  | \\(\\gamma_{\\text{ref}}\\) |  | 0 |  | - |\n| SimPO |  | \\(\\frac{\\beta}{|y_{w}|}\\log\\pi_{\\theta}(y_{w}|x)-\\frac{\\beta}{|y_{l}|}\\log\\pi_{\\theta}(y_{l}|x)\\) |  | \\(\\gamma\\) (a constant) |  | 0 |  | - |\n| TDPO<sub>i</sub> |  | \\(\\beta\\log\\pi_{\\theta}(y_{w}|x)-\\beta\\log\\pi_{\\theta}(y_{l}|x)\\) |  | \\(\\gamma_{\\text{ref}}\\) |  | \\(\\delta_{\\text{TDPO}_{i}}(x,y_{w},y_{l})\\) |  | KL Divergence |\n| **FPO** |  | \\(\\frac{\\beta}{|y_{w}|}\\log\\pi_{\\theta}(y_{w}|x)-\\frac{\\beta}{|y_{l}|}\\log\\pi_{\\theta}(y_{l}|x))\\) |  | \\(\\gamma_{\\text{ref-LN}}\\) |  | \\(\\delta_{\\text{FPO}}(x,y_{w},y_{l})\\) |  | MSE |", "caption": "Table 2: Performance comparison of different methods for Gemma-2-2B and Gemma-2-9B across various benchmarks (AlpacaEval-2, Arena-Hard, and MT-Bench), compared to Supervised Fine-Tuning (SFT), DPO and variants. Length controlled Winning Rate: WR-L; Winning Rate: WR.", "description": "This table presents a performance comparison of several methods for aligning large language models (LLMs).  It specifically uses the Gemma-2-2B and Gemma-2-9B models, evaluating their performance across three benchmark datasets: AlpacaEval-2, Arena-Hard, and MT-Bench.  The results are compared against a supervised fine-tuning (SFT) baseline and several Direct Preference Optimization (DPO) variants. Key metrics include winning rates (WR), both with and without length control (WR-L), and a delta score indicating the improvement over the baselines. This allows for a comprehensive evaluation of the various methods' effectiveness and efficiency in achieving LLM alignment.", "section": "Experimental Setup"}, {"content": "| Method | Accuracy (%) \u2191 | Diversity (Entropy) \u2191 |\n|---|---|---|\n| DPO | 59.9 | 1.66 |\n| TDPO-1 | 63.2 | 1.65 |\n| TDPO-2 | 64.2 | 1.68 |\n| SimPO | 63.4 | 1.64 |\n| FPO | 64.1 | 1.68 |", "caption": "Table 3: Comparison of FPO and other baseline methods in terms of the trade-off between Alignment (accuracy) and Diversity (entropy) on the UltraFeedback dataset.", "description": "This table presents a comparison of the performance of FPO against other baseline methods. The comparison focuses on two key aspects: alignment (measured by accuracy) and diversity (measured by entropy).  Accuracy represents how well the model aligns with human preferences. Higher accuracy indicates better alignment.  Diversity (entropy) measures the variety of generated responses. Higher entropy indicates more diverse outputs.  The results are evaluated using the UltraFeedback dataset, which is specifically designed to assess instruction-following abilities of LLMs. The table helps illustrate the trade-off between alignment and diversity, showing how FPO balances these two aspects.", "section": "5.1 THE TRADE-OFF BETWEEN CONTROLLABILITY AND EFFICIENCY"}, {"content": "| Model Name | Parameters | Method | SFT | DPO | TDPO-1 | TDPO-2 | SimPO | FPO |\n|---|---|---|---|---|---|---|---|---|\n| Gemma-2-2b | 2B | SFT | - | - | 0.5 |  | - | 0.5 |\n|  |  | DPO | - | - | 0.1 | 0.1 | 2 | 0.1 |\n|  |  | TDPO-1 | 0.5 | - | - | - | 0.5 | - |\n|  |  | TDPO-2 |  | 0.1 | 0.1 | 0.1 | 2 | 0.1 |\n|  |  | SimPO | - | - | - | - | 0.5 | - |\n|  |  | FPO | 0.5 | - | - | - | - | 0.5 |\n|  | learning rate | $5\\times 10^{-7}$ | $5\\times 10^{-7}$ | $5\\times 10^{-7}$ | $5\\times 10^{-7}$ | $5\\times 10^{-7}$ | $5\\times 10^{-7}$ | $5\\times 10^{-7}$ |\n|  | optimizer | Adam | Adam | Adam | Adam | Adam | Adam |\n|  | warmup steps | 150 | 150 | 150 | 150 | 150 | 150 |\n|  | activation checkpoint | True | True | True | True | True | True |\n|  | SAE width | None | None | None | None | None | 16k |\n|  | GPU(s) | 4 * H100 |  |  |  |  |  |  |\n| Gemma-2-9b | 9B | SFT | - | - | 0.5 |  | - | 0.5 |\n|  |  | DPO | - | 0.1 | 0.1 | 0.1 | 2 | 0.1 |\n|  |  | TDPO-1 | 0.5 | - | - | - | 0.5 | - |\n|  |  | TDPO-2 |  | 0.1 | 0.1 | 0.1 | 2 | 0.1 |\n|  |  | SimPO | - | - | - | - | 0.5 | - |\n|  |  | FPO | 0.5 | - | - | - | - | 0.5 |\n|  | learning rate | $5\\times 10^{-7}$ | $5\\times 10^{-7}$ | $5\\times 10^{-7}$ | $5\\times 10^{-7}$ | $5\\times 10^{-7}$ | $5\\times 10^{-7}$ |\n|  | optimizer | RMSprop | RMSprop | RMSprop | RMSprop | RMSprop | RMSprop |\n|  | warmup steps | 150 | 150 | 150 | 150 | 150 | 150 |\n|  | activation checkpoint | True | True | True | True | True | True |\n|  | SAE width | None | None | None | None | None | 16k |\n|  | GPU(s) | 4 * H100 |  |  |  |  |  |  |", "caption": "Table 4: Ablation Study on SAE layer selection, hyperparameters \u03b1\ud835\udefc\\alphaitalic_\u03b1 and stop-gradient operator (Grad. sg. for short). We perform experiments on Gemma-2-2b, with the 25th layer\u2019s residual SAE used to evaluate the effects of varying \u03b1\ud835\udefc\\alphaitalic_\u03b1 and applying a stop-gradient. We search for the best settings considering the trade-off between Alignment (accuracy) and Diversity (entropy).", "description": "This ablation study investigates the impact of different SAE layers, hyperparameters (alpha), and the use of a stop-gradient operator on the performance of the model.  The experiments were conducted using the Gemma-2-2b model, focusing on the 25th layer's residual SAE.  The goal was to find the optimal settings that balance model accuracy (alignment) and the diversity of generated outputs (entropy).", "section": "5.2 ABLATION STUDY"}]