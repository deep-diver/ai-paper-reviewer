[{"figure_path": "https://arxiv.org/html/2411.07618/x1.png", "caption": "Figure 1: Left. The DPO objective loss function and its two main improvement directions: SimPO and TDPO. SimPO focuses on simplifying the reference model, while TDPO concentrates on controlling the alignment process to enhance generation diversity.\nRight. The pipeline of FPO\u00a0consists of sparse autoencoders and the feature-level MSE constraints.", "description": "Figure 1 illustrates the core concept of Direct Preference Optimization (DPO) and two of its main improvements, SimPO and TDPO, alongside the proposed Feature-level Preference Optimization (FPO). The left panel shows the DPO loss function which leverages a reference model to guide the alignment process.  SimPO is depicted as simplifying the DPO process by removing the need for a reference model.  TDPO is shown as focusing on controlling the alignment process at the token-level to improve generation diversity. The right panel details the FPO pipeline which uses sparse autoencoders to generate sparse feature representations that are then used to apply MSE (mean squared error) constraints for efficient and stable alignment.", "section": "1 Introduction"}]