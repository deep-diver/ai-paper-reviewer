[{"content": "|               | Pages | Tokens   | In-Depth |\n|---------------|--------|----------|-----------|\n| DocVQA        | 1.0    | 151.5    | \u2717         |\n| ChartQA       | 1.0    | 236.9    | \u2717         |\n| InfoVQA       | 1.2    | 288.0    | \u2717         |\n| TAT-DQA       | 1.1    | 577.0    | \u2717         |\n| VisualWebBench | 1.0    | 452.4    | \u2717         |\n| PWC           | 12     | 7000     | \u2717         |\n| MP-DocVQA     | 8.3    | 2026.6   | \u2717         |\n| DUDE          | 5.7    | 1831.5   | \u2717         |\n| SlideVQA      | 20.0   | 151.5    | \u2717         |\n| MMLongBench   | 47.5   | 2030.5   | \u2717         |\n| **Ours**      | 210.8  | 120988.0 | \u2713         |", "caption": "Table 1: Benchmark dataset statistics with respect to each domain.", "description": "This table presents a quantitative overview of the M-LongDoc benchmark dataset, broken down by domain (Academic, Product, Financial).  For each domain, it shows the number of documents, the total number of questions, a further breakdown of questions by type (text-based, figure-based, table-based), average number of pages per document, average number of text tokens per document, and the average number of figure and table images per document. This detailed breakdown allows for a comprehensive understanding of the dataset's composition and characteristics across different domains.", "section": "2 M-LONGDOC BENCHMARK"}, {"content": "|       | Academic | Product | Financial | All |\n| :--- | :---: | :---: | :---: | :---: |\n|  | Paper | Manuals | Report |  |\n| Documents | 60 | 60 | 60 | 180 |\n| Questions | 311 | 279 | 261 | 851 |\n| Text-based questions | 95 | 95 | 81 | 271 |\n| Figure-based questions | 114 | 93 | 76 | 283 |\n| Table-based questions | 102 | 91 | 104 | 297 |\n| Average pages per document | 201.2 | 277.8 | 153.4 | 210.8 |\n| Average text tokens per document | 114,129.8 | 109,745.0 | 139,089.3 | 120,988.0 |\n| Average figure images per document | 90.8 | 368.3 | 24.1 | 161.13 |\n| Average table images per document | 34.9 | 96.6 | 83.8 | 71.8 |", "caption": "Table 2: Preliminary study on M-LongDoc for open-source and close-source models. We report the correctness score out of 5 for text-based, figure-based, table-based, and all questions respectively.", "description": "This table presents the results of a preliminary study conducted on the M-LongDoc benchmark, evaluating the performance of both open-source and closed-source models on various question types.  The correctness scores, ranging from 1 to 5, are reported for text-based, figure-based, table-based, and all question types, providing a comprehensive assessment of each model's strengths and weaknesses in handling different modalities within long documents.", "section": "2.4 PRELIMINARY STUDY"}, {"content": "| Model | Text | Figure | Table | All |\n|---|---|---|---|---|\n| Gemini-1.5-pro-002 |  |  |  |  |\n| w/ top k=1 pages | 4.38 | 3.73 | 4.16 | 4.11 |\n| w/ top k=5 pages | 4.60 | 4.31 | 4.54 | 4.49 |\n| w/ top k=10 pages | 4.61 | 4.29 | 4.62 | 4.51 |\n| w/ top k=20 pages | 4.63 | 4.33 | 4.38 | 4.46 |\n| Qwen2-VL-7B-Instruct |  |  |  |  |\n| w/ top k=1 pages | 4.05 | 3.25 | 3.36 | 3.57 |\n| w/ top k=5 pages | 4.17 | 3.67 | 3.46 | 3.78 |\n| w/ top k=10 pages | 4.08 | 3.62 | 3.19 | 3.65 |\n| w/ top k=20 pages | OOM | OOM | OOM | OOM |", "caption": "Table 3: Evaluation of model performance for proprietary and open-source multimodal models. We report the correctness on our benchmark across different document domains and question categories. We bold the highest scores obtained by open-source models.", "description": "This table presents a comparative analysis of various proprietary and open-source multimodal models' performance on a document question answering task.  The evaluation is performed across three different domains (Academic, Product, Finance) and three question categories (Text, Figure, Table), reflecting the diverse nature of the questions and the multimodal documents.  The 'Correctness' score, ranging from 1 to 5, indicates the accuracy and completeness of the model's answers. The highest correctness scores achieved by open-source models are highlighted in bold, facilitating a direct comparison between the performance of these two types of models.", "section": "5 Results"}, {"content": "| Model | Size | Domain:Academic | Domain:Product | Domain:Finance | Question Category:Text | Question Category:Figure | Question Category:Table | Question Category:All |\n|---|---|---|---|---|---|---|---|---|\n| *Proprietary Models* |  |  |  |  |  |  |  |  |\n| GPT-4o | - | 4.56 | 4.38 | 4.51 | 4.55 | 4.38 | 4.53 | 4.49 |\n| Claude 3.5 Sonnet | - | 4.59 | 4.43 | 4.51 | 4.57 | 4.42 | 4.54 | 4.51 |\n| Gemini 1.5 Pro | - | 4.66 | 4.43 | 4.43 | 4.59 | 4.43 | 4.52 | 4.51 |\n| *Open-Source Models* |  |  |  |  |  |  |  |  |\n| LLaVA OneVision | 7B | 3.71 | 3.74 | 3.39 | 4.03 | 3.57 | 3.30 | 3.62 |\n| Qwen2-VL | 7B | 4.03 | 3.88 | 3.56 | 4.08 | 3.83 | 3.62 | 3.84 |\n| Qwen2-VL w/ Retrieval Tuning | 7B | **4.17** | **4.01** | **3.86** | **4.31** | **4.00** | **3.77** | **4.02** |", "caption": "Table 4: Analysis on alternative settings for our benchmark, including removing images from model inputs, and using only the render image of each page as document context, without text extraction.", "description": "This table presents a comparative analysis of the model's performance on the M-LongDoc benchmark under different input configurations. It explores the impact of removing image inputs and using only rendered images (without extracted text) as the document context on the model's ability to answer questions across various categories (text, figure, table). This allows for an assessment of the model's reliance on visual information versus textual information and the effect of different input representation methods on its performance.", "section": "2.4 PRELIMINARY STUDY"}, {"content": "| Model | Question Category |  |  |  |\n|---|---|---|---|---|\n|  | Text | Figure | Table |  |\n| Qwen2-VL | 4.08 | 3.83 | 3.62 |  |\n| w/o Image Inputs | 4.22 | 3.37 | 3.38 |  |\n| w/ Render Page as Inputs | 3.99 | 3.70 | 3.39 |  |", "caption": "Table 5: Retriever performance comparison.", "description": "This table presents a comparison of the performance of four different retrieval methods in retrieving relevant pages for a document question answering task.  The methods compared are BM25, JINA-CLIP, BGE-M3, and ColPali.  Performance is evaluated using Mean Reciprocal Rank (MRR) scores, broken down by question type (Text, Figure, Table) and overall.", "section": "2.4 Preliminary Study"}, {"content": "| Retriever | Text | Figure | Table | All |\n|---|---|---|---|---|\n| BM25 | 56.2 | 31.2 | 42.0 | 43.1 |\n| CLIP | 57.1 | 37.9 | 50.4 | 48.5 |\n| BGE-M3 | 66.4 | 36.4 | 53.6 | 52.1 |\n| ColPali | 68.7 | 67.5 | 65.9 | 67.4 |", "caption": "Table 6: An example of a challenging question from M-LongDoc that requires the model to compare the trends of two charts in a document.", "description": "Table 6 presents a challenging question from the M-LongDoc benchmark dataset.  This question necessitates that the model not only understands the individual charts, but also analyzes and compares the trends displayed within two different charts to formulate a comprehensive answer. The charts visualize the relationship between reference length percentile and the percentage of empty modes, and the relationship between reference sentence length percentile and the probability of empty context. The question demands a nuanced understanding of these relationships and their differences.  The table showcases a realistic and complex scenario from the benchmark, highlighting the challenges posed by multi-modal long documents.", "section": "2 M-LONGDOC BENCHMARK"}, {"content": "| Question | Relevant page (truncated) |\n|---|---| \n| How does the relationship between reference length percentile and the percentage of empty modes differ from the relationship between reference sentence length percentile and the probability of empty context? Explain the key differences in the trends shown by these two graphs. | https://arxiv.org/html/2411.06176/two_charts_understanding_example.png |", "caption": "Table 7: Sample answers generated by Qwen2-VL and Qwen2-VL w/ Retrieval-aware Tuning, respectively.", "description": "This table presents a comparative analysis of the outputs generated by two different models: Qwen2-VL and Qwen2-VL with Retrieval-aware Tuning.  The table showcases how the models respond to a specific question, illustrating their strengths and weaknesses in understanding and processing multimodal data. The comparison highlights the impact of the Retrieval-aware Tuning technique on the model's response accuracy and quality, in terms of how well the generated answer reflects the information presented in the multimodal document.", "section": "5 Results"}]