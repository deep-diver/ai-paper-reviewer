{"references": [{"fullname_first_author": "Minesh Mathew", "paper_title": "DocVQA: A dataset for VQA on document images", "publication_date": "2020-00-00", "reason": "This paper is foundational to the field of visual question answering on document images, and its dataset is frequently used as a benchmark for evaluating models' capabilities in this area."}, {"fullname_first_author": "Yubo Ma", "paper_title": "MMLongBench-doc: Benchmarking long-context document understanding with visualizations", "publication_date": "2024-00-00", "reason": "This paper introduces a benchmark dataset specifically designed for evaluating multimodal models' performance on long and complex documents, directly addressing a key limitation of existing benchmarks and providing a more realistic evaluation setting."}, {"fullname_first_author": "Patrick Lewis", "paper_title": "Retrieval-augmented generation for knowledge-intensive NLP tasks", "publication_date": "2020-00-00", "reason": "This is a seminal paper in the field of retrieval-augmented generation, a technique which the authors use in their framework and is pivotal for addressing the challenges in processing long documents effectively."}, {"fullname_first_author": "Wenhu Chen", "paper_title": "MuRAG: Multimodal retrieval-augmented generator for open question answering over images and text", "publication_date": "2022-12-00", "reason": "This paper proposes a method that is directly relevant to the authors' work on improving the understanding of multimodal long documents, focusing on the combination of retrieval and generation techniques."}, {"fullname_first_author": "Ruochen Zhao", "paper_title": "Auto Arena of LLMs: Automating LLM evaluations with agent peer-battles and committee discussions", "publication_date": "2024-00-00", "reason": "This paper proposes an automated evaluation framework that is crucial for handling the difficulty of assessing open-ended responses, an aspect that is critical for the authors' evaluation framework for multimodal long document understanding."}]}