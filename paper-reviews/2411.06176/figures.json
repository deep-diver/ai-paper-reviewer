[{"figure_path": "https://arxiv.org/html/2411.06176/extracted/5987183/assets/documodal-data-pie.png", "caption": "Figure 1: Data distribution of document topics\nin our M-LongDoc benchmark.", "description": "This figure shows the distribution of document topics within the M-LongDoc benchmark dataset.  It visually represents the proportions of documents belonging to various categories or topics, providing an overview of the dataset's diversity in terms of subject matter. The topics appear to be grouped into broader categories such as \"Academic Papers\", \"Technical Manuals\", and \"Financial Reports,\" each with further subcategories. The size of each slice in the pie chart corresponds to the relative proportion of documents belonging to that specific topic within the entire M-LongDoc dataset.", "section": "2 M-LONGDOC BENCHMARK"}, {"figure_path": "https://arxiv.org/html/2411.06176/x5.png", "caption": "Figure 2: \nComparison of benchmarks along three dimensions: the number of pages per document, the number of tokens per document, and the nature of the responses required. Specifically, we assess whether each benchmark emphasizes in-depth, comprehensive answers or focuses on short or extractive responses.", "description": "Figure 2 compares various question answering benchmarks across three key aspects: the average number of pages per document, the average number of tokens per document, and the type of answer expected.  It highlights whether a benchmark prioritizes detailed, comprehensive answers or is satisfied with shorter, more extractive answers. This helps to illustrate the varying levels of complexity and the types of reasoning skills required by different datasets.", "section": "2 M-LONGDOC BENCHMARK"}, {"figure_path": "https://arxiv.org/html/2411.06176/x6.png", "caption": "Figure 3: Example questions in different multimodal document question answering benchmarks. For illustration, we include content from the relevant page in the original document.\nThe example question from M-LongDoc is more complex than those from other benchmarks, as it requires an explanatory answer rather than an extraction of a short text span. Furthermore, it requires the model to understand the semantics of both image and text.\nPlease note that in our benchmark setting, the model is provided with all page contents from the document, and not only the relevant page.", "description": "Figure 3 compares example questions from various multimodal document question answering benchmarks, including DocVQA and MMLongBench, highlighting the complexity difference.  M-LongDoc questions demand explanatory answers encompassing both image and textual semantics, unlike others requiring simple extractive answers. The figure shows that, in the M-LongDoc benchmark setting, the model is given access to the entire document, not just the relevant page.", "section": "2 M-LONGDOC BENCHMARK"}, {"figure_path": "https://arxiv.org/html/2411.06176/x7.png", "caption": "Figure 4: Overview of our data construction process with question verification stages. For brevity, we shorten the checklist prompts and include the full details in Appendix A.1.", "description": "This figure details the semi-automated pipeline used to generate high-quality, challenging questions for the M-LongDoc benchmark.  It starts with selecting a document page containing a specific content type (text, table, or figure).  Multiple large language models then generate questions based on this page and its surrounding context.  These questions undergo an automated verification process using a checklist to filter out unsuitable questions. Finally, human annotators perform a second verification step to ensure quality and relevance, resulting in a curated set of questions suitable for the benchmark.  The checklist prompts shown are shortened; complete details can be found in Appendix A.1.", "section": "2.2 Question Generation"}, {"figure_path": "https://arxiv.org/html/2411.06176/x8.png", "caption": "Figure 5: Our automated evaluation framework to assess the correctness of open-ended solutions for multimodal question answering. The full evaluation guide is included in Appendix A.3.", "description": "This figure illustrates the automated evaluation process used to assess the quality of open-ended responses generated by models for multimodal question answering tasks.  The process involves multiple evaluation steps, including a thorough review of the provided multimodal document (text, figures, tables), comparison of the model's response to the document's information, and assessment of accuracy, comprehensiveness, and relevance to the question.  Multiple judge models (e.g., large language models) are used to independently score the responses. These individual scores are then aggregated to provide a final, holistic correctness score for each response. The detailed evaluation guide used by the judge models is provided in Appendix A.3 of the paper.", "section": "2.3 AUTOMATED EVALUATION"}]