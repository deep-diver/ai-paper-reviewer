[{"page_end_idx": 1, "page_start_idx": 1, "section_number": 1, "section_title": "INTRODUCTION", "details": {"details": "The transformer architecture has revolutionized natural language processing and deep learning, largely due to its inherent train-time parallelization enabled by the attention mechanism. This allows for massive scaling and has led to the development of state-of-the-art Large Language Models (LLMs). However, LLM inference remains sequential due to its auto-regressive nature, limiting text generation to one token per forward pass and creating a memory bottleneck.  To address this memory limitation, speculative decoding has been proposed, which involves using a smaller, faster language model (draft model) to generate multiple candidate tokens, which are then scored and verified by the larger LLM (target model) in parallel.  This parallel verification is crucial to ensure the final output tokens follow the distribution of the target model.  For speculative decoding to improve efficiency, the combined cost of sampling from the draft model and verifying with the target model must be less than the cost of autoregressive sampling directly from the target model.  This requires that the draft model's distribution closely resembles that of the target model, which is measured by the acceptance rate.", "first_cons": "The inherent sequential nature of autoregressive decoding in LLMs is a significant limitation, restricting inference speed.", "first_pros": "The introduction clearly and concisely explains the context and challenges of LLM inference, highlighting the limitations of autoregressive decoding and the motivation behind speculative decoding.", "keypoints": ["The transformer architecture's success stems from its train-time parallelization (Vaswani et al., 2017), enabling large-scale LLMs but leading to sequential inference.", "LLM inference is sequential due to autoregressive nature, resulting in a memory bottleneck (Shazeer, 2019).", "Speculative decoding uses a smaller draft model to generate candidate tokens, which are then verified by the LLM, improving efficiency if the combined cost is less than direct LLM sampling.", "Successful speculative decoding hinges on the draft model closely approximating the target model's distribution, gauged by the token acceptance rate in the process.."], "second_cons": "The explanation of the memory bottleneck caused by the sequential nature of autoregressive decoding could be more quantitative, providing concrete examples of memory usage.", "second_pros": "The introduction effectively sets the stage for the paper by clearly outlining the problem of slow inference in LLMs and the potential benefits of speculative decoding as a solution. The explanation of the core concept is simple and understandable.", "summary": "This section introduces the challenges of performing inference with Large Language Models (LLMs) due to their autoregressive nature, which results in slow, sequential processing that is memory-bound.  To overcome this, speculative decoding is proposed: this technique uses a smaller, faster draft model to propose multiple tokens that are then verified by the LLM.  The key requirement for this method to be efficient is that the draft model must closely match the distribution of the target model, a factor measured by the acceptance rate during the verification step."}}, {"page_end_idx": 2, "page_start_idx": 2, "section_number": 2, "section_title": "BACKGROUND AND RELATED WORK", "details": {"details": "Autoregressive sampling from LLMs is inherently sequential and memory-bound.  Several approaches have been proposed to accelerate inference, including model compression techniques like quantization and sparsification, which reduce complexity but can degrade decoding quality.  Lossless acceleration methods, particularly speculative decoding, have also emerged. Speculative decoding uses a smaller language model (draft model) to generate multiple candidate tokens, which are then scored by the LLM (target model). This parallel verification, via a sequence of token-level rejection sampling, ensures that the final sequence follows the target model's distribution.  Existing methods, like greedy decoding, can draft and predict multiple tokens by augmenting the base LLM, but LLM text generation often needs sampling with non-zero temperature.  Speculative decoding addresses this by using a smaller draft model for initial sampling, improving overall efficiency if the combined cost is lower than sampling directly from the target model.  More recent work extends speculative decoding to the multi-draft setting, employing multiple draft models to generate candidate token sequences. One such approach, SpecTr, frames the token-level draft selection as an optimal transport problem; others, like SpecInfer, use a token tree structure. An alternative method uses extra decoding heads added to the target model.  Finally, block-level verification methods are explored as another way to improve efficiency.", "first_cons": "The methods described often involve compromises.  For example, model compression techniques that reduce complexity may lead to some degradation in decoding quality.", "first_pros": "The section provides a good overview of existing acceleration techniques for LLMs, including both model compression and speculative decoding approaches.", "keypoints": ["Autoregressive sampling from LLMs is inherently sequential and memory-bound (Shazeer, 2019).", "Model compression (quantization, sparsification) reduces complexity but may degrade quality.", "Speculative decoding uses a smaller draft model and parallel LLM verification to improve efficiency.  This is only beneficial if the combined cost is lower than autoregressive sampling from the target model.", "Multi-draft speculative decoding uses multiple draft models for even greater efficiency. SpecTr (Sun et al., 2024b) uses an optimal transport formulation, while SpecInfer (Miao et al., 2024) uses a token tree.", "Block-level verification is another approach to accelerate inference. Earlier methods like greedy decoding (Stern et al., 2018; Ge et al., 2022) and aggressive decoding (Ge et al., 2022) are also mentioned in contrast to sampling methods with non-zero temperature"], "second_cons": "While the section mentions several techniques, it lacks a direct comparison of their relative performance and efficiency.  The descriptions of individual methods can be brief and lack a focus on underlying differences.", "second_pros": "The background section effectively contextualizes the current research by summarizing a range of prior work aimed at speeding up LLM inference, highlighting various techniques and their limitations. This gives the reader a strong grasp of the problem space and why new approaches are needed.", "summary": "This section reviews existing methods for accelerating large language model (LLM) inference, focusing on model compression and speculative decoding.  Model compression techniques, like quantization and sparsification, offer efficiency gains but can compromise model quality. Speculative decoding, employing a smaller draft model and parallel target model verification, provides a lossless alternative. Recent work extends this to multi-draft decoding, leveraging multiple draft models and different selection strategies (e.g., optimal transport or token tree approaches). The section also mentions block-level verification as another complementary technique to improve efficiency."}}, {"page_end_idx": 3, "page_start_idx": 3, "section_number": 3, "section_title": "TOKEN-LEVEL OPTIMAL DRAFT SELECTION: THEORETICAL ANALYSIS", "details": {"details": "This section delves into the theoretical analysis of token-level optimal draft selection in multi-draft speculative sampling.  It revisits the optimal transport framework introduced in prior work, demonstrating that the optimal acceptance probability can be achieved through a two-step process.  The first step utilizes an importance sampling-type scheme to select an intermediate token from a set of candidate tokens generated by draft models. The second step employs single-draft speculative sampling to generate the final output token. For the specific case of two identical draft models, the authors establish a necessary and sufficient condition for achieving a probability of acceptance equal to 1 and provide an explicit expression for the optimal acceptance probability.  This analysis motivates the development of a new token-level selection scheme based on weighted importance sampling. ", "first_cons": "The theoretical analysis is primarily focused on the case of two identical draft models, limiting the generalizability of the findings to more complex scenarios with multiple or non-identical models.", "first_pros": "The section presents a clear and concise decomposition of the optimal scheme into a two-step process, improving our understanding of the underlying mechanisms.  This two-step process involves importance sampling and single-draft speculative sampling which are relatively straightforward and easy to implement.", "keypoints": ["The optimal token selection scheme can be decomposed into a two-step process: importance sampling followed by single-draft speculative sampling.", "For two identical draft models, a necessary and sufficient condition for acceptance probability to equal one is established.", "An explicit expression for the optimal acceptance probability is provided for the case of two identical draft models.", "A new token-level selection scheme based on weighted importance sampling is proposed."], "second_cons": "While the theoretical analysis provides valuable insights, the practical implications and computational cost of implementing the proposed weighted importance sampling scheme are not fully explored.", "second_pros": "The theoretical analysis offers valuable insights into the optimal token selection problem and motivates the development of improved sampling schemes.  For the case of two identical draft models, the results are particularly insightful providing concrete conditions for optimal performance and an explicit expression for the acceptance probability.", "summary": "This section presents a theoretical analysis of optimal token-level draft selection in multi-draft speculative sampling. It decomposes the optimal scheme into a two-step solution: importance sampling followed by single-draft speculative sampling. For two identical draft models, it provides a necessary and sufficient condition for achieving perfect acceptance probability (1) and an explicit formula for optimal acceptance probability.  This analysis leads to a new token selection scheme using weighted importance sampling."}}, {"page_end_idx": 4, "page_start_idx": 4, "section_number": 3, "section_title": "Optimal Acceptance Probability and Canonical Architecture", "details": {"details": "This section delves into the theoretical analysis of multi-draft speculative sampling, focusing on the optimal token-level selection rule. It introduces a canonical architecture for achieving the optimal acceptance probability. This architecture is a two-step process: first, importance sampling is used to select an intermediate token, and second, single-draft speculative sampling refines the selection using the target model's distribution.  For the specific case of two identical draft models, the authors derive a necessary and sufficient condition for the acceptance probability to reach one, providing an explicit expression for the optimal probability in this scenario. A new class of token-level selection schemes based on weighted importance sampling is also proposed.  These findings provide a deeper understanding of the optimal transport framework for multi-draft speculative sampling, and a new efficient two-step scheme is derived.", "first_cons": "The analysis is primarily focused on the case of two identical draft models. Extending this analysis to more complex scenarios with non-identical or more than two models would enhance the applicability of the theoretical results.", "first_pros": "The introduction of the canonical two-step architecture simplifies the problem of achieving optimal acceptance probability, providing insights into the inherent structure of multi-draft speculative sampling.", "keypoints": ["Optimal acceptance probability can be decomposed into a two-step solution: importance sampling followed by single-draft speculative sampling.", "For two identical draft models, a necessary and sufficient condition for achieving acceptance probability equal to 1 is established.", "An explicit expression for the optimal acceptance probability is provided for the case of two identical draft models.", "A novel token-level selection scheme based on weighted importance sampling is proposed."], "second_cons": "While the proposed weighted importance sampling scheme is computationally more efficient, it is still based on heuristics and not the optimal solution for multiple drafts.", "second_pros": "The theoretical analysis provides valuable insights into the optimal acceptance probability and motivates the development of a new class of efficient token-level selection schemes.  The explicit formula for the optimal probability with two identical draft models provides a valuable benchmark for evaluating the performance of practical schemes.", "summary": "This section presents a theoretical analysis of multi-draft speculative sampling, demonstrating that the optimal acceptance probability can be achieved using a two-step process involving importance sampling and single-draft speculative sampling.  For two identical drafts, a necessary and sufficient condition for the optimal probability to equal one is given, along with an explicit formula for this probability. A novel weighted importance sampling scheme is also proposed."}}, {"page_end_idx": 5, "page_start_idx": 5, "section_number": 3, "section_title": "Optimal Acceptance Probability for the case of K=2 drafts", "details": {"details": "This section delves into the theoretical analysis of multi-draft speculative sampling, specifically focusing on the case with K=2 identical draft models.  The authors derive a necessary and sufficient condition for the acceptance probability to reach its maximum value of 1. This condition, expressed as  \u03a3<sub>x\u2208S</sub> q(x) \u2265 (\u03a3<sub>x\u2208S</sub> p(x))<sup>2</sup> for all subsets S of the vocabulary \u03a9,  reveals a relationship between the target model distribution *q* and the draft model distribution *p*.  Furthermore, they provide an explicit formula for calculating the optimal acceptance probability P*(acc) when the condition is not met. The findings are supported by numerical evaluations that showcase the performance of the optimal scheme against existing baselines, highlighting its improved acceptance probability. The theoretical results motivate the development of a two-step solution for token selection: first, employing importance weighted sampling, and then, single-draft speculative sampling.  The analysis also offers valuable insights into the connections between importance sampling and optimal transport, providing a novel approach to efficient token selection.  The mathematical proofs involved in establishing these results are detailed in the appendix.", "first_cons": "The analysis is primarily focused on the scenario with only two identical draft models (K=2), limiting the generalizability of the findings to more complex scenarios with K > 2.", "first_pros": "The section provides a rigorous theoretical analysis resulting in a necessary and sufficient condition for optimal acceptance probability (P*(acc)=1) when using two identical draft models.", "keypoints": ["A necessary and sufficient condition for optimal acceptance probability (P*(acc) = 1) is established for the K=2 case:  \u03a3<sub>x\u2208S</sub> q(x) \u2265 (\u03a3<sub>x\u2208S</sub> p(x))<sup>2</sup>, where *q* and *p* represent the target and draft distributions respectively.", "An explicit formula for calculating the optimal acceptance probability P*(acc) is provided for the K=2 case.", "A two-step solution for optimal token selection is proposed, involving importance weighted sampling followed by single-draft speculative sampling.", "Numerical evaluations demonstrate consistent improvements of the optimal scheme compared to existing methods.", "The theoretical analysis uses techniques such as Fourier-Motzkin elimination and links importance sampling with optimal transport."], "second_cons": "The provided analytical expressions are limited to the K=2 scenario. While numerical results suggest a pattern that might hold for larger K,  generalization of these formulas to a higher number of drafts would need further investigation.", "second_pros": "The theoretical analysis provides strong mathematical foundations for multi-draft speculative sampling, offering a deeper understanding beyond empirical evaluations. The two-step decomposition into importance weighted sampling and single-draft speculative sampling offers a practical canonical architecture for implementation.", "summary": "This section presents a theoretical analysis of multi-draft speculative sampling, focusing on the case where there are two identical draft models.  It derives a necessary and sufficient condition for achieving the maximum acceptance probability (1), provides an explicit formula for optimal acceptance probability, and proposes a two-step scheme for optimal token selection involving importance weighted sampling followed by single-draft speculative sampling. Numerical results validate the theoretical findings."}}, {"page_end_idx": 6, "page_start_idx": 6, "section_number": 4, "section_title": "FASTER IMPORTANCE WEIGHTED SPECULATIVE SAMPLING", "details": {"details": "This section explores methods to accelerate the importance weighted speculative sampling approach discussed earlier.  The core idea is that in practice, the probability distributions of both the target and draft models tend to be concentrated on a small subset of tokens.  Leveraging this observation, the authors propose two main strategies for improvement:\n\n**1. Truncated Linear Program (LP):** This approach reduces computational cost by fixing some of the \n`w\u1d62\u2c7c` parameters (representing the probability of selecting token `i` given candidate tokens `i` and `j`) to predetermined values based on the sorted probabilities of tokens. A parameter `s` controls how many of the `w\u1d62\u2c7c` parameters are optimized; setting `s` to a smaller value reduces computational burden.  They provide a theoretical guarantee (Equation 14) that bounds the decrease in acceptance probability resulting from this simplification.\n\n**2. Truncated Alphabet:** This method focuses on reducing the size of the alphabet considered.  The algorithm selects a high-probability subset of tokens (`\u03a9\u2080`) from the target distribution and generates an output token from this subset. Tokens outside the high-probability subset are included with probabilities adjusted to maintain the original target distribution. This significantly reduces the number of variables involved, thereby speeding up the process.\n\nBoth approaches offer tradeoffs between computational speed and accuracy.  The authors demonstrate that the accuracy loss is often minimal, especially with well-trained language models, making these faster methods highly practical.", "first_cons": "The theoretical analysis and performance gains presented are limited to the case of K=2 drafts. Generalization to a larger number of drafts is discussed conceptually but lacks the same level of rigorous justification.", "first_pros": "The section introduces two practical optimization strategies (truncated LP and truncated alphabet) to significantly speed up the importance weighted speculative sampling process without causing a substantial loss in accuracy.  The truncated LP method offers a theoretical guarantee on performance degradation.", "keypoints": ["The probability distributions of target and draft models are often concentrated on a small number of tokens.", "Truncated Linear Program (LP) reduces computation by optimizing fewer weights (parameter *s*).", "Truncated Alphabet approach reduces the alphabet size for faster processing.", "Equation 14 provides theoretical guarantee on accuracy loss in Truncated LP approach."], "second_cons": "The heuristic approaches proposed for faster inference (truncated LP and truncated alphabet) lack a comprehensive empirical analysis across various models and datasets.  The presented results are relatively limited.", "second_pros": "The proposed methods are empirically shown to improve the block efficiency and token rates, particularly when the number of drafts (K) increases. The approach is more practical than previous methods due to the reduced computational complexity.", "summary": "This section details two novel methods to accelerate importance weighted speculative sampling: the truncated linear program, which reduces computation by limiting the number of optimized parameters; and the truncated alphabet, which improves efficiency by focusing on the most likely tokens.  Both methods offer practical trade-offs between computational cost and accuracy loss, with theoretical guarantees and empirical improvements demonstrated."}}, {"page_end_idx": 10, "page_start_idx": 8, "section_number": 5, "section_title": "EXPERIMENTAL RESULTS", "details": {"details": "The experimental results section evaluates the performance of the proposed importance sampling (IS) scheme for multi-draft speculative sampling against baselines like SpecTr and SpecInfer across three datasets (Dolly, XSum, WMT).  Experiments were conducted using the OPT model with variations in the draft model temperature and the number of draft models, both with identical and non-identical draft model configurations.  The results consistently show improvements in block efficiency and token rate for the IS scheme, particularly when draft model temperatures vary.  Numerical evaluations of acceptance probability demonstrate its effectiveness, particularly for the case of two draft models where an analytical expression for the optimal acceptance probability is derived and verified experimentally.  Faster implementations using truncated LP and truncated alphabet schemes are also investigated, demonstrating a trade-off between computational cost and performance.  Finally, ROUGE-L and BLEU scores for XSum and WMT datasets are reported to confirm the overall improvement in generation quality.", "first_cons": "The improvements achieved in block efficiency by the IS scheme are small when employing top-k sampling, particularly when the baseline schemes' acceptance probabilities are already close to the theoretical limit.", "first_pros": "The proposed IS scheme consistently outperforms the SpecTr and SpecInfer baselines in block efficiency and token rate across all three datasets, exhibiting significant improvements, especially when varying draft model temperatures.", "keypoints": ["The IS scheme consistently outperforms SpecTr and SpecInfer in block efficiency and token rate across three datasets (Dolly, XSum, WMT).", "For identical draft models, the improvement in token rate is significant (e.g., up to 30% improvement in some cases), especially when draft model temperatures differ.", "For non-identical draft models, the IS scheme maintains the consistent improvement, while SpecInfer's performance gains are negligible when draft model temperatures vary.", "The truncated LP and truncated alphabet schemes provide faster implementations, but there is a trade-off between speed and performance.  Choosing the right truncation parameters (s=5 for LP, \u03a9o=40 for alphabet) is critical.", "The acceptance probability consistently increases with more drafts (e.g. 0.5009 for K=2 to 0.6962 for K=8 on the Dolly task), showing the efficiency of the multi-draft setup.", "ROUGE-L and BLEU scores are also provided to support the quantitative comparison on generation quality."], "second_cons": "The analytical expression for optimal acceptance probability is only derived and experimentally verified for the case of K=2 identical drafts, limiting its generalizability to other scenarios.", "second_pros": "The study presents a novel two-step canonical architecture for multi-draft speculative sampling, decomposing the optimal scheme into importance sampling followed by single-draft speculative sampling.  This provides a theoretical foundation for the effectiveness of the approach.", "summary": "Experiments on three datasets show that a novel importance sampling scheme for multi-draft speculative decoding consistently outperforms existing methods in block efficiency and token rate.  This improvement is especially pronounced when the draft model temperatures are varied.  Faster versions using truncated LP and alphabet methods are also explored, showing a trade-off between speed and performance.  The study verifies the findings with ROUGE-L and BLEU scores, and further provides an analytical expression for optimal acceptance probability for two identical drafts."}}, {"page_end_idx": 10, "page_start_idx": 8, "section_number": 5, "section_title": "IDENTICAL DRAFT MODELS", "details": {"details": "This section presents experimental results comparing the performance of the proposed Importance Sampling (IS) scheme with three other multi-draft speculative sampling schemes (SpecTr, SpecInfer, and single-draft speculative decoding) when identical draft models are used.  The experiments are conducted using three datasets (Dolly, XSum, and WMT) and the OPT model.  The temperature of the target model is fixed at 1.0, while the temperature of the draft models is varied from 1.2 to 2.4. The results show that the IS scheme consistently outperforms other multi-draft schemes in terms of block efficiency (the average number of accepted tokens per use of the draft model) and token rate improvement over the baseline single-draft scheme.  The block efficiency of IS consistently improves as the draft model temperature increases, unlike other schemes that show minimal or no improvement or even negative impact at higher draft temperatures. For the Dolly dataset and two draft models, the proposed scheme achieves a block efficiency of 2.13, significantly outperforming SpecTr (1.76) and SpecInfer (1.77).  Additional experiments (Appendix H) further evaluate various hyperparameters, including different numbers of draft models and top-k sampling, also with consistent superiority of the proposed IS method.", "first_cons": "The improvement gains from the proposed IS scheme diminish as the baseline schemes already achieve acceptance probabilities close to the theoretical limit, as observed when using top-k sampling.", "first_pros": "The proposed Importance Sampling (IS) scheme consistently outperforms other multi-draft schemes in block efficiency and token rate, achieving significant improvements across various datasets and temperatures. For example, on the Dolly dataset, the IS scheme achieves a block efficiency of 2.13 compared to 1.76 for SpecTr and 1.77 for SpecInfer when using two draft models.", "keypoints": ["The IS scheme consistently outperforms SpecTr and SpecInfer in block efficiency and token rate across three datasets (Dolly, XSum, WMT).", "Block efficiency consistently improves with increasing draft model temperature for IS, unlike other schemes showing little to no improvement or negative effects.", "On the Dolly dataset, the IS scheme achieves block efficiency of 2.13 with two draft models, while SpecTr and SpecInfer achieve 1.76 and 1.77 respectively.", "Additional experiments demonstrate consistent superiority of the IS method across other hyperparameter settings, including various numbers of drafts and top-k sampling (Appendix H)."], "second_cons": "The study focuses only on identical draft models and does not extend the analysis to scenarios with non-identical models which could limit generalizability.", "second_pros": "The theoretical analysis of the optimal acceptance probability for multi-draft settings and the introduction of the novel IS scheme provide valuable insights and advances in efficient LLM inference. The comprehensive experiments across multiple datasets and temperature variations demonstrate the practical efficacy and robustness of the proposed method.", "summary": "Experiments using the OPT model on three datasets demonstrate that a novel Importance Sampling (IS) scheme for multi-draft speculative decoding consistently outperforms existing methods (SpecTr, SpecInfer, single-draft) in terms of block efficiency and token rate, especially when draft model temperature increases.  The proposed method achieves a block efficiency of 2.13 for the Dolly dataset using two identical draft models, significantly better than the other methods.  Further experiments in the Appendix corroborate these findings across different hyperparameter settings."}}, {"page_end_idx": 10, "page_start_idx": 9, "section_number": 5, "section_title": "NON-IDENTICAL DRAFT MODELS", "details": {"details": "This section explores the performance of multi-draft speculative sampling when the two draft models utilize the same weights but employ different temperatures for token generation.  The authors compare their proposed Importance Sampling (IS) scheme against the SpecInfer scheme, a previously proposed method for handling non-identical draft distributions.  Experiments across three datasets (Dolly, XSum, WMT) demonstrate that the IS scheme consistently outperforms SpecInfer in block efficiency and token rates.  Specifically, while SpecInfer shows negligible improvement in block efficiency as the second draft model's temperature increases, the IS scheme achieves consistent gains. This behavior is visualized in Figure 4.  The superior performance of the IS scheme is attributed to its ability to effectively leverage information from both draft models, unlike SpecInfer.  The study also includes observations on the effect of altering the temperature of one draft model on the token rate and block efficiency, further highlighting the IS scheme's advantages.", "first_cons": "The analysis focuses only on the scenario with two draft models; the generalizability to multiple drafts (K>2) is not rigorously addressed. Although a method is proposed for extension, a comprehensive evaluation with multiple drafts is lacking.", "first_pros": "The experimental results provide strong empirical support for the superiority of the Importance Sampling scheme over SpecInfer, particularly in scenarios where the draft models have non-identical temperature settings.  The consistent improvements in block efficiency and token rates across various datasets strongly validate the proposed approach.", "keypoints": ["The IS scheme consistently outperforms SpecInfer in terms of block efficiency and token rate across three datasets (Dolly, XSum, WMT).", "SpecInfer shows negligible improvement in block efficiency when the temperature of the second draft model is increased, while IS scheme demonstrates consistent gains.", "The results are visualized in Figure 4, which clearly illustrates the superior performance of IS scheme.", "The analysis focuses on the case with two draft models and doesn't thoroughly address the extension to more than two draft models (K>2)."], "second_cons": "The impact of using different parameters for vocabulary and LP truncation on the performance of the IS scheme is investigated but only to a limited extent, thus leaving some aspects unexplored.", "second_pros": "The study provides valuable insights into how the performance of the Importance Sampling scheme is affected by changes in the temperature of the draft models. This nuanced analysis highlights the scheme's robustness and adaptability.", "summary": "This section investigates multi-draft speculative sampling with non-identical draft models (different temperatures but same weights). The Importance Sampling (IS) scheme consistently outperforms SpecInfer across three datasets, showing consistent improvements in block efficiency and token rates, especially when the second draft's temperature increases, unlike SpecInfer which shows negligible gains. Figure 4 visually represents this significant performance difference.  The section touches upon extending the method to more than two drafts, but lacks a thorough evaluation."}}]