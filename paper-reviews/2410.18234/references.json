{"references": [{" publication_date": "2017", "fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "reason": "This paper introduced the transformer architecture, a fundamental innovation in deep learning that significantly improved the efficiency of natural language processing and paved the way for the development of large language models.  Its introduction of the attention mechanism, allowing for parallelization during training, is crucial to the advancements in LLMs and the very foundation of the speculative decoding techniques studied in this paper.", "section_number": 1}, {" publication_date": "1997", "fullname_first_author": "Sepp Hochreiter", "paper_title": "Long short-term memory", "reason": "This foundational paper introduced the LSTM architecture, a significant precursor to transformers and crucial for understanding the limitations of recurrent neural networks in sequence modeling.  The LSTM's struggles with parallelization and long-range dependencies highlight the advancements achieved by the later transformer architecture and contextualizes the motivations behind speculative decoding techniques that attempt to address the sequentiality and computational cost issues of LLMs.", "section_number": 1}, {" publication_date": "2014", "fullname_first_author": "Junyoung Chung", "paper_title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "reason": "This paper provided an empirical comparison of gated recurrent neural networks (GRUs), which also are sequence-based models but were considered to be computationally more efficient compared to LSTMs. This paper further highlights the advantages of the transformer architecture in sequence modeling, contrasting the parallel nature of transformers with the inherent sequentiality of GRUs and thus providing additional context for the proposed method's improvements.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Josh Achiam", "paper_title": "Gpt-4 technical report", "reason": "This paper introduced GPT-4, one of the most advanced LLMs to date, highlighting the impressive performance and capabilities of current large language models.  Its existence and capabilities demonstrate the rapid progress in the field and emphasize the need for efficient methods to handle the computational costs associated with the inference of such models, motivating the development and research in speculative decoding.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Hugo Touvron", "paper_title": "Llama 2: Open foundation and fine-tuned chat models", "reason": "This paper details Llama 2, another state-of-the-art large language model, demonstrating the continued advancement in LLM capabilities and thus highlighting the importance of finding efficient inference techniques to handle their computationally intensive nature. The paper adds to the context provided by the GPT-4 technical report, showing that improvements in model size continue to result in improvements in performance but also increased computational costs.", "section_number": 1}, {" publication_date": "2020", "fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "reason": "This seminal work demonstrated the remarkable few-shot learning capabilities of large language models, underscoring the potential of LLMs across a wide range of tasks and applications and demonstrating the potential need to accelerate the inference speed to handle the growing scale of models and their use in more computationally intensive applications.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Aakanksha Chowdhery", "paper_title": "Palm: Scaling language modeling with pathways", "reason": "This paper showcases the scaling capabilities and performance of large language models, particularly focusing on improvements using techniques like pathway language modeling. This work adds to the context established by the previous LLM related papers and further underscores the need to develop and improve computationally efficient inference methods to handle the demands of ever-larger and more powerful models.", "section_number": 1}, {" publication_date": "2019", "fullname_first_author": "Noam Shazeer", "paper_title": "Fast transformer decoding: One write-head is all you need", "reason": "This paper directly addresses the memory bottleneck in autoregressive decoding of transformers, providing a solution that directly inspires the design of speculative decoding methods.  The proposed approach in this paper focuses on efficiency improvements in LLM inference, laying groundwork for speculative decoding approaches by highlighting the performance limitations of standard methods, thus providing a direct basis for the improvement shown by speculative decoding methods.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Charlie Chen", "paper_title": "Accelerating large language model decoding with speculative sampling", "reason": "This paper introduced speculative decoding, a novel technique that forms the basis of this paper's core methodology. The approach aims to overcome the limitations of sequential autoregressive decoding by parallelizing the verification process and offers a foundation for the improvements proposed in this research on multi-draft speculative sampling.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Yaniv Leviathan", "paper_title": "Fast inference from transformers via speculative decoding", "reason": "This paper further develops and refines the speculative decoding technique focusing on its practical aspects and efficiency improvements. It adds to the work on speculative decoding providing additional context and rationale for the importance of speculative decoding in addressing the efficiency limitations of LLM inference and providing an additional basis for this paper's proposed multi-draft speculative decoding method.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Ziteng Sun", "paper_title": "Spectr: Fast speculative decoding via optimal transport", "reason": "This paper presents SpecTr, a multi-draft extension to speculative decoding that forms a critical baseline and comparison method for this paper's proposed improvements. SpecTr represents a significant advancement in speculative decoding and demonstrates the potential of multi-draft approaches in improving efficiency. The use of optimal transport framework provides an additional theoretical foundation which is expanded upon in this paper.", "section_number": 1}, {" publication_date": "2018", "fullname_first_author": "Mitchell Stern", "paper_title": "Blockwise parallel decoding for deep autoregressive models", "reason": "This paper is another significant early contribution to the field of speculative decoding focusing on improving efficiency. While the approach is different from the speculative decoding explored in the current research, it nevertheless provides a further contextualization for the broader efficiency problems in LLM inference that have motivated the development of speculative decoding and multi-draft speculative decoding, thereby providing a more comprehensive background for the proposed multi-draft speculative decoding method.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Sebastian Jaszczur", "paper_title": "Sparse is enough in scaling transformers", "reason": "This work explores model compression techniques, specifically sparsification, to improve the efficiency of large language models.  The paper provides an additional basis for understanding the various strategies used for addressing the limitations of autoregressive decoding and thereby highlighting the need for improvement using approaches like speculative decoding methods which are explored in more detail in this paper.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Elias Frantar", "paper_title": "Gptq: Accurate post-training quantization for generative pre-trained transformers", "reason": "This work focuses on model compression through quantization, another approach used to reduce the computational cost of LLMs.  The paper provides further context and support for the research into more computationally efficient inference methods like speculative decoding and highlights the importance of considering and improving efficiency at all levels of LLM processing.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Elias Frantar", "paper_title": "SparseGPT: Massive language models can be accurately pruned in one-shot", "reason": "This paper explores an alternative model compression method, specifically one-shot pruning, focusing on improving efficiency by reducing model size. It further expands upon the context and rationale behind the research on speculative decoding and provides an additional comparative perspective showing that different approaches exist for solving the efficiency challenges in LLM inference, highlighting the motivation behind speculative decoding and the potential advantages of multi-draft speculative decoding.", "section_number": 2}, {" publication_date": "2019", "fullname_first_author": "Noam Shazeer", "paper_title": "Fast transformer decoding: One write-head is all you need", "reason": "This paper provides additional context and support for the research behind speculative decoding methods in this paper.  This paper directly addresses the computational challenges in autoregressive decoding, highlighting the need for more efficient methods, and serves as a further motivation for research into speculative decoding techniques, which attempt to address these efficiency limitations.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Xupeng Miao", "paper_title": "SpecInfer: Accelerating large language model serving with tree-based speculative inference and verification", "reason": "This paper introduces SpecInfer, a competitive multi-draft speculative decoding method, providing a key comparative baseline for this paper's research. SpecInfer demonstrates the use of multiple drafts to improve decoding efficiency but introduces a different selection method thereby providing a basis for further comparison and potential optimization with the methods developed in this research.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Ziteng Sun", "paper_title": "SpecTr: Fast speculative decoding via optimal transport", "reason": "This paper introduces SpecTr, a novel multi-draft speculative decoding method which forms a key comparative baseline for this paper's proposed method.  SpecTr shows the viability of multi-draft approaches and its focus on optimizing the probability of accepting at least one token directly aligns with the research questions in this paper and thereby helps provide a more robust and comprehensive comparative analysis.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Wonseok Jeon", "paper_title": "Recursive speculative decoding: Accelerating llm inference via sampling without replacement", "reason": "This paper introduces another approach to speculative decoding, which is distinct from the methods used in this research but which helps to further highlight the broader importance of speculative decoding in addressing the efficiency challenges in LLM inference and thus serves as an additional contextualization for the research presented in this paper.", "section_number": 2}, {" publication_date": "2010", "fullname_first_author": "Surya T Tokdar", "paper_title": "Importance sampling: a review", "reason": "This paper provides a comprehensive review of importance sampling, a core technique used in the theoretical analysis of the proposed multi-draft speculative sampling method.  The paper forms a strong theoretical basis for understanding the importance sampling step in the proposed two-step framework and helps provide additional context and support for the theoretical analysis.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Yongchao Zhou", "paper_title": "DistillSpec: Improving speculative decoding via knowledge distillation", "reason": "This work further explores speculative decoding, focusing on the use of knowledge distillation to improve its performance.  The paper helps to contextualize the development of speculative decoding and its ongoing refinement, highlighting the challenges and ongoing innovations in this crucial area of LLM inference acceleration and thus provides additional support for the importance of speculative decoding methods in solving efficiency issues.", "section_number": 3}, {" publication_date": "2022", "fullname_first_author": "George B Dantzig", "paper_title": "Fourier-motzkin elimination and its dual", "reason": "This work describes the Fourier-Motzkin elimination technique, a key method used in the theoretical analysis of the optimal acceptance probability in the case of two identical draft models. The technique is central to the mathematical proofs and analysis of this research and plays a key role in establishing the optimality and validity of the proposed approach.", "section_number": 3}, {" publication_date": "1964", "fullname_first_author": "NV Chernikova", "paper_title": "Algorithm for finding a general formula for the non-negative solutions of system of linear equations", "reason": "This paper provides a foundational algorithm used in the theoretical analysis of the optimal acceptance probability in multi-draft speculative sampling. The algorithm is a crucial component of the theoretical analysis and provides a key tool for understanding and establishing the conditions for the optimal acceptance probability to equal 1, and thereby provides support for the claims presented in this paper.", "section_number": 3}, {" publication_date": "1995", "fullname_first_author": "Komei Fukuda", "paper_title": "Double description method revisited", "reason": "This paper presents the double description method, a fundamental tool in polyhedral cone theory that is used in this research for analyzing the connection between Theorem 2 and polyhedral cone representation. This method is used in this research to provide an alternative approach for verifying Theorem 2 for arbitrary alphabet sizes and provides an additional mathematical technique and support for the mathematical analysis presented in this paper.", "section_number": 3}, {" publication_date": "2015", "fullname_first_author": "Geoffrey Hinton", "paper_title": "Distilling the knowledge in a neural network", "reason": "This paper introduces knowledge distillation, a technique that has been applied to improve the performance of draft models in speculative decoding.  It provides additional context to the broader field of model optimization and compression relevant to speculative decoding methods.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Clara Meister", "paper_title": "Locally typical sampling", "reason": "This paper introduces locally typical sampling, a technique related to the importance weighted sampling scheme proposed in this work. It provides additional support for the use of importance sampling in speculative decoding and enhances the understanding of the underlying mathematical principles of weighted importance sampling used in the proposed methods.", "section_number": 4}, {" publication_date": "2022", "fullname_first_author": "Susan Zhang", "paper_title": "Opt: Open pre-trained transformer language models", "reason": "This paper introduces the OPT model, which is the backbone of this paper's empirical results.  The model is extensively used in all experiments, making the paper crucial for understanding the experimental setup and interpreting the results.", "section_number": 5}, {" publication_date": "2018", "fullname_first_author": "Shashi Narayan", "paper_title": "Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization", "reason": "This paper introduces the XSum dataset, one of the datasets used in the experimental evaluation.  The dataset is crucial for evaluating the performance of the proposed methods in a real-world setting. The use of this dataset adds to the validity and generalizability of the experimental results, showing that the proposed methods generalize to various datasets and types of problems.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Conover Mike", "paper_title": "Free dolly: Introducing the world's first open and commercially viable Instruction-Tuned LLM", "reason": "This paper introduces the Databricks-Dolly-15k dataset, one of the datasets used for evaluation in this paper. The paper provides crucial details for the experimental results and highlights the importance of using a large and diverse range of datasets when evaluating the proposed approaches to show that the proposed approaches generalized across different datasets and types of tasks.", "section_number": 5}, {" publication_date": "2018", "fullname_first_author": "Ond\u0159ej Bojar", "paper_title": "Findings of the 2018 conference on machine translation (WMT18)", "reason": "This paper introduces the WMT18 dataset, one of the datasets used in this research for evaluation.  The dataset allows for a more comprehensive and robust evaluation of the proposed methods and ensures that the generalization and validity of the proposed method is demonstrated across a wide range of datasets and types of tasks.", "section_number": 5}, {" publication_date": "2004", "fullname_first_author": "Chin-Yew Lin", "paper_title": "Rouge: A package for automatic evaluation of summaries", "reason": "This paper introduces the ROUGE metric, a key evaluation metric used in this research to assess the quality of text generated by the different speculative decoding methods. The use of this metric provides additional quantitative metrics to demonstrate the overall performance improvement due to this research and provides a more robust and comprehensive evaluation of the results.", "section_number": 5}, {" publication_date": "2002", "fullname_first_author": "Kishore Papineni", "paper_title": "Bleu: A method for automatic evaluation of machine translation", "reason": "This paper introduces the BLEU metric, another key evaluation metric used in this research.  Similar to ROUGE-L, BLEU is used to quantitatively assess the quality of generated text, thereby providing additional validation and support for the claims of improved performance due to this research.", "section_number": 5}]}