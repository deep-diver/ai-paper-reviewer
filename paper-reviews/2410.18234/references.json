{"references": [{" publication_date": "2017", "fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "reason": "This paper introduced the transformer architecture, a fundamental breakthrough in deep learning that underpins the success of large language models (LLMs).  Its train-time parallelization, enabled by the attention mechanism, is a key factor in the scalability of LLMs, making it a cornerstone of modern NLP and the foundation for the work presented in this paper which aims to improve inference of this architecture.", "section_number": 1}, {" publication_date": "2019", "fullname_first_author": "Noam Shazeer", "paper_title": "Fast transformer decoding: One write-head is all you need", "reason": "This paper highlights the memory bottleneck inherent in the autoregressive decoding process of LLMs, motivating the research on alternative decoding strategies such as speculative decoding to improve efficiency.  It directly addresses one of the central challenges discussed in the introduction of the current paper.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Charlie Chen", "paper_title": "Accelerating large language model decoding with speculative sampling", "reason": "This paper introduced the core concept of speculative decoding, which is a central theme of the current paper. The ideas of using a smaller draft model to generate candidate tokens and then verifying them in parallel with the target model are foundational for the current work's multi-draft extension.", "section_number": 1}, {" publication_date": "1997", "fullname_first_author": "Sepp Hochreiter", "paper_title": "Long short-term memory", "reason": "This is a seminal paper in recurrent neural networks, providing background context to the evolution of language models. By contrasting recurrent architectures with transformers, it sets the stage for understanding the advantages of the transformer architecture that's central to the current paper's focus on improving LLM inference.", "section_number": 1}, {" publication_date": "2014", "fullname_first_author": "Junyoung Chung", "paper_title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "reason": "This work further contributes to the background on recurrent neural networks, providing a more recent comparison point to the transformer architecture. It provides a deeper understanding of the transition in architectures leading to transformers and the focus on improving their efficiency in the current paper.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Yaniv Leviathan", "paper_title": "Fast inference from transformers via speculative decoding", "reason": "This paper explores speculative decoding methods, presenting a significant advancement in speeding up LLM inference and introducing key concepts like token-level rejection sampling that form the base for the current research on multi-draft speculative decoding.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Elias Frantar", "paper_title": "SparseGPT: Massive language models can be accurately pruned in one-shot", "reason": "This paper presents a method for model compression which is an alternative approach to accelerating LLM inference, thus forming a crucial part of the overall background on prior methods of improving LLM inference.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Elias Frantar", "paper_title": "GPTQ: Accurate post-training quantization for generative pre-trained transformers", "reason": "This paper delves into a specific model compression technique, post-training quantization, which is a major approach to improve LLM inference. It highlights a technique compared to speculative decoding to accelerate inference.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Yelysei Bondarenko", "paper_title": "Quantizable transformers: Removing outliers by helping attention heads do nothing", "reason": "This paper contributes to the understanding of model compression techniques for transformers, providing another comparative method to speculative decoding which is the main focus of this paper.", "section_number": 2}, {" publication_date": "2018", "fullname_first_author": "Mitchell Stern", "paper_title": "Blockwise parallel decoding for deep autoregressive models", "reason": "This work introduces another method for accelerating LLM inference through parallelization, offering a different approach than speculative decoding, and providing broader context to approaches explored for accelerating LLM inference.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Tao Ge", "paper_title": "Lossless acceleration for seq2seq generation with aggressive decoding", "reason": "This paper explores aggressive decoding techniques, another approach to enhance LLM inference that's different from speculative decoding. The discussion of aggressive decoding methods enhances the context by showing several approaches of efficient LLM inference.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Ziteng Sun", "paper_title": "SpecTr: Fast speculative decoding via optimal transport", "reason": "This paper is directly relevant as it introduces the SpecTr model, which is a key baseline for comparison in the current paper's experiments on multi-draft speculative sampling.  The comparison with SpecTr's performance allows for direct evaluation of the proposed method's improvements.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Xupeng Miao", "paper_title": "SpecInfer: Accelerating large language model serving with tree-based speculative inference and verification", "reason": "This paper presents the SpecInfer method, a key baseline used for comparison in the current paper's experiments. The comparison with SpecInfer allows a direct evaluation of the proposed method's improvements in multi-draft speculative sampling.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Tianle Cai", "paper_title": "Medusa: Simple llm inference acceleration framework with multiple decoding heads", "reason": "This paper introduces another novel technique for accelerating LLM inference which is different from speculative decoding. The method uses multiple decoding heads to increase the efficiency of inference. The comparison of the different methods of improving LLM inference provides a valuable background and context.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Ziteng Sun", "paper_title": "Optimal block-level draft verification for accelerating speculative decoding", "reason": "This work is closely related as it focuses on block-level verification, a complementary approach to token-level verification used in the current paper. Understanding block-level verification methods enhances the context and allows for a more complete understanding of the space of methods of enhancing LLM inference.", "section_number": 2}, {" publication_date": "2015", "fullname_first_author": "Geoffrey Hinton", "paper_title": "Distilling the knowledge in a neural network", "reason": "This paper discusses knowledge distillation, a technique relevant to training draft models for speculative decoding.  Understanding how to effectively train and transfer knowledge to smaller models (draft models) is essential for successful speculative decoding.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Sebastian Jaszczur", "paper_title": "Sparse is enough in scaling transformers", "reason": "This paper explores model compression techniques, specifically focusing on sparsity, and forms part of the broader context of existing methods to enhance LLM inference, often compared against speculative decoding methods in this paper.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Wonseok Jeon", "paper_title": "Recursive speculative decoding: Accelerating llm inference via sampling without replacement", "reason": "This paper explores a novel variation of speculative decoding, offering a comparative approach.  Understanding different speculative decoding techniques broadens the context for understanding the impact and contribution of the current paper's multi-draft improvements.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Clara Meister", "paper_title": "Locally typical sampling", "reason": "This paper introduces a technique for sampling from probability distributions that is relevant to the importance sampling discussed in the current paper.  Understanding various efficient sampling techniques enhances the background and context of this paper.", "section_number": 4}, {" publication_date": "2022", "fullname_first_author": "Yongchao Zhou", "paper_title": "DistillSpec: Improving speculative decoding via knowledge distillation", "reason": "This work is highly relevant as it directly investigates the use of knowledge distillation to improve speculative decoding.  This is a significant advancement in the context of the current research on enhancing the draft model's performance in speculative decoding, addressing challenges in model alignment, and improving acceptance rates.", "section_number": 2}]}