{"references": [{" publication_date": "2017", "fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "reason": "This paper introduced the transformer architecture, which is fundamental to modern large language models (LLMs) and is the foundation upon which the current research builds.  The transformer's train-time parallelization, enabled by the attention mechanism, is directly relevant to the memory-bound limitations of autoregressive decoding discussed in the introduction and motivates the need for faster inference techniques like speculative decoding.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Charlie Chen", "paper_title": "Accelerating large language model decoding with speculative sampling", "reason": "This paper introduced the concept of speculative decoding, a key technique for accelerating LLM inference by using a smaller draft model to generate multiple candidate tokens for parallel verification by a larger target model.  The introduction clearly highlights speculative decoding as a critical approach, and this paper is its seminal work, establishing its foundational concepts and methods.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Yaniv Leviathan", "paper_title": "Fast inference from transformers via speculative decoding", "reason": "This paper is another seminal work on speculative decoding, focusing on accelerating LLM inference using parallel verification of candidate tokens generated by a smaller draft model. The introduction explicitly references this paper along with Chen et al. (2023), highlighting the significance of this technique in the context of memory-bound autoregressive decoding in LLMs.", "section_number": 1}, {" publication_date": "2019", "fullname_first_author": "Noam Shazeer", "paper_title": "Fast transformer decoding: One write-head is all you need", "reason": "This work directly addresses the memory-bound nature of autoregressive decoding in LLMs, a central theme of the introduction. The paper provides solutions to mitigate the high memory requirements of autoregressive generation in transformers, which directly motivates the research presented in the paper focusing on efficient decoding approaches.", "section_number": 1}, {" publication_date": "2017", "fullname_first_author": "Sepp Hochreiter", "paper_title": "Long short-term memory", "reason": "This paper introduced the Long Short-Term Memory (LSTM) architecture, a prominent recurrent neural network (RNN) used before the advent of transformers. Comparing and contrasting the limitations of RNNs (like LSTMs) versus transformers helps contextualize the improvements in parallelization achieved by the attention mechanism in transformers, a central theme in the introduction.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Ziteng Sun", "paper_title": "SpecTr: Fast speculative decoding via optimal transport", "reason": "This paper introduces the SpecTr algorithm, a multi-draft speculative decoding method that is directly compared to the proposed algorithm in the experimental section. Understanding SpecTr's approach and its limitations in terms of computational cost provides a direct benchmark for evaluating the performance and efficiency of the proposed method.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Xupeng Miao", "paper_title": "SpecInfer: Accelerating large language model serving with tree-based speculative inference and verification", "reason": "SpecInfer is another state-of-the-art multi-draft speculative decoding method that serves as a direct baseline for comparison in the experiments.  Understanding SpecInfer's approach provides a crucial context for evaluating the performance gains and contributions of the proposed algorithm in the experimental section.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Wonseok Jeon", "paper_title": "Recursive speculative decoding: Accelerating llm inference via sampling without replacement", "reason": "This paper is relevant because it presents another approach to accelerate LLM inference using speculative decoding. Comparing and contrasting the performance of the proposed algorithm with this and other recent methods shows its performance improvements in different scenarios in the experimental section.", "section_number": 1}, {" publication_date": "2019", "fullname_first_author": "Noam Shazeer", "paper_title": "Fast transformer decoding: One write-head is all you need", "reason": "This paper is significant for providing alternative approaches to alleviate the limitations of autoregressive decoding in LLMs. This work's focus on faster inference techniques directly relates to the context and motivation of the current research, which also aims to accelerate LLM decoding.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Sebastian Jaszczur", "paper_title": "Sparse is enough in scaling transformers", "reason": "This paper introduces model compression techniques, specifically focusing on sparse methods for scaling transformers.  Since model compression is an alternative method for accelerating LLM inference, understanding its performance and limitations helps contextualize the proposed method's advantages and limitations in the background section.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Elias Frantar", "paper_title": "Gptq: Accurate post-training quantization for generative pre-trained transformers", "reason": "This paper explores model compression techniques using quantization, a method to reduce the size of LLMs to speed up the inference.  The background section examines various techniques to accelerate LLM inference, and this paper's focus on quantization provides a key point of comparison and contextualizes the proposed method within the broader landscape of LLM acceleration.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Elias Frantar", "paper_title": "Sparsegpt: Massive language models can be accurately pruned in one-shot", "reason": "This paper presents another model compression technique focused on sparsification, aiming to reduce model size and computational cost for faster inference.  The inclusion of this paper in the background section emphasizes that the current work's contribution focuses on improving inference efficiency while preserving accuracy, in contrast to compression techniques that may trade off accuracy for speed.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Yaniv Leviathan", "paper_title": "Fast inference from transformers via speculative decoding", "reason": "This paper is important because it proposes speculative decoding, which uses a smaller, faster language model (draft model) to generate multiple candidate tokens for parallel verification by the larger LLM (target model). The background section discusses different decoding methods, and this paper is essential for understanding the evolution and improvements in speculative decoding techniques.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Charlie Chen", "paper_title": "Accelerating large language model decoding with speculative sampling", "reason": "This paper is another important work on speculative decoding that serves as a key reference in the background section. The paper introduced the core ideas behind speculative decoding and is a crucial foundation for the work presented in this paper, which focuses on multi-draft extensions to this method.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Ziteng Sun", "paper_title": "SpecTr: Fast speculative decoding via optimal transport", "reason": "This paper introduces SpecTr, a multi-draft speculative decoding method using optimal transport.  The background section discusses the recent trend towards multi-draft speculative decoding, and this paper is directly relevant because it represents a prominent approach in this area, providing a key benchmark against which the proposed method is compared.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Xupeng Miao", "paper_title": "SpecInfer: Accelerating large language model serving with tree-based speculative inference and verification", "reason": "This work presents SpecInfer, a state-of-the-art multi-draft speculative decoding method that efficiently generates candidate sequences for LLM inference.  The background section outlines recent advances in multi-draft speculative decoding, and SpecInfer is a key reference in this section, serving as a direct comparison to the newly proposed method.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Wonseok Jeon", "paper_title": "Recursive speculative decoding: Accelerating llm inference via sampling without replacement", "reason": "This paper explores recursive speculative decoding, which further refines speculative decoding techniques by employing sampling without replacement. The background section highlights that the proposed research is set against the backdrop of improving speculative decoding methods, and the inclusion of this paper underlines the broader context of the work.", "section_number": 2}, {" publication_date": "2010", "fullname_first_author": "Surya T Tokdar", "paper_title": "Importance sampling: a review", "reason": "This paper provides a comprehensive overview of importance sampling, a statistical technique that is fundamental to the two-step architecture proposed in Section 3. The theoretical analysis in Section 3 heavily relies on importance sampling, making this paper a critical reference for understanding the core methodology.", "section_number": 3}, {" publication_date": "2022", "fullname_first_author": "Yongchao Zhou", "paper_title": "Distillspec: Improving speculative decoding via knowledge distillation", "reason": "This paper demonstrates another technique to improve speculative decoding, this time using knowledge distillation.  Knowledge distillation is an alternative method for aligning the distribution of the draft and target models, which is a key challenge addressed by the current research. Understanding this work helps contextualize the different approaches to solve the core challenges in speculative decoding.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Ziteng Sun", "paper_title": "SpecTr: Fast speculative decoding via optimal transport", "reason": "This paper presents SpecTr, a multi-draft extension of speculative decoding, and it directly informs the theoretical analysis in Section 3.  Understanding SpecTr helps establish the context and background for the proposed canonical architecture and its comparison with prior work.", "section_number": 3}]}