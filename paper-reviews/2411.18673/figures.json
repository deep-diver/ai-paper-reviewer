[{"figure_path": "https://arxiv.org/html/2411.18673/x1.png", "caption": "Figure 1: Camera-controlled video generation. Our method enables precise camera controllability in pre-trained video diffusion transformers, allowing joint conditioning of text and camera sequences. We synthesize the same scene with two different camera trajectories as input.\nThe inset images visualize the cameras for the videos in the corresponding columns. The left camera sequence consists of a rotation to the right, while the right camera visualizes a zoom-in, up, and zoom-out trajectory.", "description": "This figure demonstrates the capability of the proposed method to precisely control camera movement during video generation using pre-trained video diffusion transformers.  Two videos of the same scene are shown, each generated with a different camera trajectory. The left video shows a camera rotating to the right, while the right video shows a camera zooming in, moving upward, and then zooming out.  The inset images offer a visual representation of the camera's trajectory for each video.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2411.18673/x2.png", "caption": "Figure 2: VDiT-CCmodel with ControlNet\u00a0[187, 71] camera conditioning built on top of\u00a0VDiT.\nVideo synthesis is performed by large 4,096-dimensional DiT-XL blocks of the frozen VDiT\u00a0backbone, while VDiT-CC\u00a0only processes and injects the camera information through lightweight 128-dimensional DiT-XS blocks (FC stands for fully-connected layers); see Section\u00a03.2 for details.", "description": "The figure illustrates the architecture of the VDiT-CC model, which incorporates camera control into a pre-trained video diffusion transformer (VDiT). The VDiT backbone, consisting of large 4,096-dimensional DiT-XL blocks, is responsible for video synthesis and remains frozen during training.  A lightweight camera conditioning module, VDiT-CC, is built on top of the VDiT. VDiT-CC uses smaller, 128-dimensional DiT-XS blocks to process and inject camera information into the main VDiT network, thereby controlling camera pose without modifying the VDiT's primary video generation capabilities.  Fully-connected layers (FC) are utilized within these DiT-XS blocks.  Details regarding this architecture and training process are further described in Section 3.2 of the paper.", "section": "3 Method"}, {"figure_path": "https://arxiv.org/html/2411.18673/x3.png", "caption": "Figure 3: \nComparing motion spectral volumes for scenes with different motion types.\nVideos with camera motion (purple) exhibit stronger overall motion than the videos with scene motion (orange), especially for the low-frequency range, suggesting that the motion induced by camera transitions is heavily biased towards low-frequency components.", "description": "The figure visualizes the frequency spectrum of motion in videos generated by a video diffusion model. Three types of videos are compared: those with only scene motion, only camera motion, and both scene and camera motion. The results show that camera motion predominantly affects the low-frequency components of the motion spectrum, while scene motion affects both high and low frequencies. This finding suggests that camera movements in videos are characterized by smooth, low-frequency motion, which can inform the design of training and inference strategies for camera control in video generation models.", "section": "3.3 How is camera motion modeled by diffusion?"}, {"figure_path": "https://arxiv.org/html/2411.18673/x4.png", "caption": "(a) A generated video at different diffusion timesteps. The camera has already been decided by the model even at t=0.9\ud835\udc610.9t=0.9italic_t = 0.9 (first 10% of the denoising process) and does not change after that.", "description": "The figure visualizes a video generated by a diffusion model at different stages of the denoising process.  Specifically, it showcases that the camera pose is determined very early in the process (at t=0.9, representing the first 10% of denoising), and remains consistent throughout the rest of the generation. This observation suggests that the model makes crucial camera-related decisions early on, highlighting the influence of low-frequency components in camera motion.", "section": "3.3. How is camera motion modeled by diffusion?"}, {"figure_path": "https://arxiv.org/html/2411.18673/x5.png", "caption": "(b) Motion spectral volumes of VDiT\u2019s generated videos for different diffusion timesteps (left) and their ratio w.r.t. the motion spectral volume at t=0\ud835\udc610t=0italic_t = 0 (i.e., a fully denoised video).", "description": "This figure visualizes the spectral properties of camera motion during video generation using a video diffusion transformer (VDIT). The left panel shows motion spectral volumes (MSVs) for different diffusion timesteps, revealing the distribution of motion energy across frequencies.  The right panel displays the ratio of MSVs at each timestep to the MSV at the final timestep (t=0), highlighting when different motion components appear during the generation process. This analysis helps determine the optimal timing for injecting camera information during video synthesis, ultimately improving generation quality and camera control accuracy.", "section": "3.3. How is camera motion modeled by diffusion?"}, {"figure_path": "https://arxiv.org/html/2411.18673/x6.png", "caption": "Figure 4: \nHow camera motion is modeled by diffusion?\nAs visualized in Figure\u00a04(a) and Figure\u00a03, the motion induced by camera transitions is a low-frequency type of motion.\nWe observe that a video DiT creates low-frequency motion very early in the denoising trajectory: Figure\u00a04(b) (left) shows that even at t=0.96\ud835\udc610.96t{=}0.96italic_t = 0.96 (first \u22484absent4{\\approx}{4}\u2248 4% of the steps), the low-frequency motion components have already been created, while high frequency ones do not fully unveil even till t=0.5\ud835\udc610.5t{=}0.5italic_t = 0.5.\nWe found that controlling the camera pose later in the denoising trajectory is not only unnecessary but detrimental to both scene motion and overall visual quality.", "description": "This figure analyzes how camera motion is represented in a video diffusion model.  Figure 4(a) shows a generated video at different diffusion timesteps, demonstrating that camera motion is largely determined very early in the process. Figure 4(b) provides a spectral analysis of motion, showing that camera motion primarily affects low frequencies and is mostly complete by t=0.96, whereas high-frequency details related to scene motion continue to develop later (till t=0.5). The authors conclude that controlling camera pose later in the diffusion process is counterproductive and harms both scene motion and overall image quality.", "section": "3.3 How is camera motion modeled by diffusion?"}, {"figure_path": "https://arxiv.org/html/2411.18673/x7.png", "caption": "Figure 5: \nVideo DiT is secretly a camera pose estimator.\nWe perform linear probing of camera poses in each of VDiT\u00a0blocks for various noise levels and observe that video DiT performs pose estimation under the hood.\nIts middle blocks carry the most accurate information about the camera locations and orientations, which indicates that the camera signal emerges in the early layers to help the middle and late blocks render other visual features aligned with the viewpoint.", "description": "This figure visualizes the results of a linear probing experiment to determine whether a pre-trained Video Diffusion Transformer (VDiT) implicitly learns camera pose information.  The experiment uses the RealEstate10K dataset and tests various noise levels in the VDiT's internal representations. The results show that camera pose information is surprisingly well-encoded within the VDiT's architecture.  The middle layers of the model contain the most accurate camera information, suggesting that camera signals are processed early in the network to guide later layers in generating visually consistent outputs aligned with the camera viewpoint.", "section": "3.4 What does VDiT know about camera pose?"}, {"figure_path": "https://arxiv.org/html/2411.18673/extracted/6029846/images/latents-pca.png", "caption": "Figure 6: \nRealEstate10k\u00a0[198] videos (upper 2 rows) contain diverse camera trajectories, but are strongly biased towards static scenes.\nTo mitigate this bias and also increase the concepts diversity, we curate 20202020K videos with stationary cameras, but dynamic content (lower 2 rows).\nSuch dataset is easy to construct, and surprisingly effective. Section\u00a04.3 shows that integrating it into our training allows to improve visual quality on out-of-distribution prompts by 17%percent1717\\%17 %.", "description": "The figure shows a comparison of two video datasets used for training a video generation model. The top two rows display examples from the RealEstate10k dataset, which contains diverse camera movements but is biased towards static scenes.  The bottom two rows show examples from a curated dataset of 20k videos featuring dynamic scenes filmed with stationary cameras.  This curated dataset was created to address the bias in RealEstate10k and improve the model's ability to generate dynamic scenes.  Including this dataset in the training process led to a 17% improvement in visual quality when generating videos with out-of-distribution prompts (prompts unseen during training).", "section": "3.5 Mitigating training data limitations"}]