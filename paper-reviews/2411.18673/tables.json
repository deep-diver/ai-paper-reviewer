[{"content": "| Dataset | Image                                                                                                |\n|---|---| \n| RealEstate10K | [https://arxiv.org/html/2411.18673/extracted/6029846/images/getty-static/re1.png](https://arxiv.org/html/2411.18673/extracted/6029846/images/getty-static/re1.png)<br>[https://arxiv.org/html/2411.18673/extracted/6029846/images/getty-static/re2.png](https://arxiv.org/html/2411.18673/extracted/6029846/images/getty-static/re2.png) |\n| Our curated data | [https://arxiv.org/html/2411.18673/extracted/6029846/images/getty-static/ge2.png](https://arxiv.org/html/2411.18673/extracted/6029846/images/getty-static/ge2.png)<br>[https://arxiv.org/html/2411.18673/extracted/6029846/images/getty-static/ge4.png](https://arxiv.org/html/2411.18673/extracted/6029846/images/getty-static/ge4.png) |", "caption": "Table 1: User study. We compare our approach to the original VD3D (FIT) and reimplemented VD3D (DiT) on top of our base model. We conduct a user study where participants indicate their preference based on camera aligntment (CA), motion quality (MQ), text alignment (TA), visual quality (VQ), and overall preference\u00a0(Overall).", "description": "This user study compares the proposed AC3D model against the original VD3D model (trained using a UNet-based architecture) and a re-implementation of VD3D on top of the AC3D's base model (a video diffusion transformer).  Human participants assessed the generated videos based on five criteria: Camera Alignment (how well the generated camera trajectory matched a reference video), Motion Quality (naturalness and fluidity of movement), Text Alignment (how well the video content reflected the text prompt), Visual Quality (overall image and video quality), and Overall Preference (an overall preference rating).  The results show the performance differences across various metrics, highlighting AC3D's strengths.", "section": "4. Experiments"}, {"content": "| Image     |\n| ----------- |\n| https://arxiv.org/html/2411.18673/re1.png |\n| https://arxiv.org/html/2411.18673/re2.png |", "caption": "Table 2: Quantitative evaluation. We evaluate all the models using camera pose and visual quality metrics based on unseen camera trajectories. We compute translation and rotation errors based on the estimated camera poses from generations using ParticleSfM [191]. We evaluate both in-distribution with RealEstate10K\u00a0[198] and out-of-distribtion with MSR-VTT\u00a0[161].", "description": "Table 2 presents a quantitative comparison of different video generation models' performance in terms of camera control and visual quality.  The models are evaluated using metrics based on camera pose and visual fidelity, with the evaluation performed on both in-distribution (RealEstate10K) and out-of-distribution (MSR-VTT) datasets.  Camera pose accuracy is assessed using translation and rotation errors calculated via ParticleSfM. Visual quality metrics include FID, FVD, and CLIP scores.", "section": "4. Experiments"}]