[{"figure_path": "2410.16198/charts/charts_3_0.png", "caption": "Figure 3: The distribution of word counts for CoT and direct answer.", "description": "The chart displays the distribution of word counts for chain-of-thought (CoT) answers and direct answers.", "section": "3 METHOD"}, {"figure_path": "2410.16198/charts/charts_9_0.png", "caption": "Figure 6: The figures illustrate the performance of the DPO model as a verifier on ChartQA, A-OKVQA, and MathVista. Compared to the model trained with RLAIF-V, the model trained on our reasoning data pairs consistently shows improvement in both best-of-N selection and weighted voting.", "description": "The chart displays the performance of three different re-ranking methods (weighted voting with DPO, majority voting, and best-of-N with DPO) on three datasets (ChartQA, A-OKVQA, and MathVista) as the number of candidate answers increases.", "section": "5 RL EXPERIMENTS FOR ENHANCED CHAIN-OF-THOUGHT REASONING"}, {"figure_path": "2410.16198/charts/charts_9_1.png", "caption": "Figure 6: The figures illustrate the performance of the DPO model as a verifier on ChartQA, A-OKVQA, and MathVista. Compared to the model trained with RLAIF-V, the model trained on our reasoning data pairs consistently shows improvement in both best-of-N selection and weighted voting.", "description": "The chart displays the performance of the DPO model as a verifier across three datasets (ChartQA, A-OKVQA, and MathVista) using three re-ranking strategies (weighted voting with DPO, majority voting, and best-of-N with DPO), showing improved performance with the model trained on reasoning data pairs compared to the one trained with RLAIF-V.", "section": "5 RL EXPERIMENTS FOR ENHANCED CHAIN-OF-THOUGHT REASONING"}, {"figure_path": "2410.16198/charts/charts_10_0.png", "caption": "Figure C.1: Randomly sampled examples from LLAVA-NEXT-8B with temperature=1.0 for a test case in ChartQA reveal that the model struggles to effectively follow the CoT prompt. In Sample 1, the model refuses to answer the question. In Samples 2-4, the model generates an answer first, followed by an explanation. In the final sample, the model produces a description instead of reasoning through the question, without providing an answer.", "description": "The chart displays examples of the LLAVA-Next-8B model's inability to follow chain-of-thought reasoning prompts, demonstrating inconsistent responses ranging from refusal to answer to providing answers before reasoning.", "section": "C BASELINE EVALUATION"}, {"figure_path": "2410.16198/charts/charts_25_0.png", "caption": "Figure 3: The distribution of word counts for CoT and direct answer.", "description": "The chart displays the distribution of word counts in chain-of-thought (CoT) answers and direct answers.", "section": "3 METHOD"}, {"figure_path": "2410.16198/charts/charts_27_0.png", "caption": "Figure C.4: Randomly sampled examples from LLAVA-NEXT-FORMAT with a temperature setting of 1.0, evaluated on the same test case in ChartQA, show that after training on 450 format-aligned data, the model is able to follow the CoT prompt by verbalizing the thought process and providing a short answer.", "description": "The chart displays a bar graph showing the long-term price index of various food commodities from 1850 to 2015, measured relative to real prices in 1900, with each bar representing a different food item.", "section": "C BASELINE EVALUATION"}]