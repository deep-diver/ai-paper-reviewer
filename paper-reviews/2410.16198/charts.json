[{"figure_path": "2410.16198/charts/charts_3_0.png", "caption": "Figure 3: The distribution of word counts for CoT and direct answer.", "description": "The chart displays the distribution of word counts in chain-of-thought (CoT) answers and direct answers.", "section": "3 METHOD"}, {"figure_path": "2410.16198/charts/charts_9_0.png", "caption": "Figure 6: The figures illustrate the performance of the DPO model as a verifier on ChartQA, A-OKVQA, and MathVista. Compared to the model trained with RLAIF-V, the model trained on our reasoning data pairs consistently shows improvement in both best-of-N selection and weighted voting.", "description": "The chart displays the performance of the DPO model as a verifier on three datasets (ChartQA, A-OKVQA, and MathVista) using three re-ranking methods (weighted voting with DPO, majority voting, and best-of-N with DPO) across different numbers of candidate answers.", "section": "5 RL EXPERIMENTS FOR ENHANCED CHAIN-OF-THOUGHT REASONING"}, {"figure_path": "2410.16198/charts/charts_9_1.png", "caption": "Figure 6: The figures illustrate the performance of the DPO model as a verifier on ChartQA, A-OKVQA, and MathVista. Compared to the model trained with RLAIF-V, the model trained on our reasoning data pairs consistently shows improvement in both best-of-N selection and weighted voting.", "description": "The chart displays the performance of the DPO model as a verifier for three different datasets (ChartQA, A-OKVQA, and MathVista), comparing its performance with and without RLAIF-V training.", "section": "5 RL EXPERIMENTS FOR ENHANCED CHAIN-OF-THOUGHT REASONING"}, {"figure_path": "2410.16198/charts/charts_10_0.png", "caption": "Figure C.1: Randomly sampled examples from LLAVA-NEXT-8B with temperature=1.0 for a test case in ChartQA reveal that the model struggles to effectively follow the CoT prompt. In Sample 1, the model refuses to answer the question. In Samples 2-4, the model generates an answer first, followed by an explanation. In the final sample, the model produces a description instead of reasoning through the question, without providing an answer.", "description": "The chart displays examples of the LLAVA-Next-8B model's responses to a question about counting food items in a bar chart, demonstrating its inconsistent handling of a chain-of-thought (CoT) prompt.", "section": "C BASELINE EVALUATION"}, {"figure_path": "2410.16198/charts/charts_25_0.png", "caption": "Figure 3: The distribution of word counts for CoT and direct answer.", "description": "The chart displays the distribution of word counts in chain-of-thought (CoT) answers and direct answers.", "section": "3 METHOD"}, {"figure_path": "2410.16198/charts/charts_27_0.png", "caption": "Figure C.4: Randomly sampled examples from LLAVA-NEXT-FORMAT with a temperature setting of 1.0, evaluated on the same test case in ChartQA, show that after training on 450 format-aligned data, the model is able to follow the CoT prompt by verbalizing the thought process and providing a short answer.", "description": "The chart displays a bar graph showing the long-term price index of various food commodities from 1850 to 2015, with each bar representing a different food item and its length proportional to the price index value.", "section": "C BASELINE EVALUATION"}]