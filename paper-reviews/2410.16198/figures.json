[{"figure_path": "2410.16198/figures/figures_2_0.png", "caption": "Figure 1: The upper figure questions whether training exclusively on direct-answer datasets can effectively teach CoT prediction. In the lower figure, generating CoT for prediction provides the additional benefit of reasoning alignment, allowing the model to improve by leveraging self-generated data.", "description": "The figure illustrates the difference between training a VLM on only short answers versus training it on both short answers and detailed reasoning chains (chain-of-thought).", "section": "1 INTRODUCTION"}, {"figure_path": "2410.16198/figures/figures_3_0.png", "caption": "Figure 2: Workflow diagram showing: a) the use of GPT-40 to generate rationale given short annotations; b) SFT of open-source VLM for CoT reasoning; c) Build preference dataset for reinforcement learning with DPO to enhance reasoning.", "description": "The figure illustrates the three-stage pipeline for improving vision language model chain-of-thought reasoning: rationale distillation, supervised fine-tuning, and reinforcement learning.", "section": "3 METHOD"}, {"figure_path": "2410.16198/figures/figures_4_0.png", "caption": "Figure 1: The upper figure questions whether training exclusively on direct-answer datasets can effectively teach CoT prediction. In the lower figure, generating CoT for prediction provides the additional benefit of reasoning alignment, allowing the model to improve by leveraging self-generated data.", "description": "This figure illustrates the difference between training a vision language model (VLM) exclusively on direct answers versus incorporating chain-of-thought (CoT) reasoning, highlighting the benefits of CoT for improved reasoning alignment and self-generated data.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.16198/figures/figures_5_0.png", "caption": "Figure 5: The upper section displays the data sources used for the SFT experiments, while the lower section illustrates the data composition for model training.", "description": "This figure illustrates the data sources and composition used in supervised fine-tuning experiments for the chain-of-thought reasoning model.", "section": "4 SFT EXPERIMENTS FOR CHAIN-OF-THOUGHT LEARNING"}, {"figure_path": "2410.16198/figures/figures_10_0.png", "caption": "Figure 1: The upper figure questions whether training exclusively on direct-answer datasets can effectively teach CoT prediction. In the lower figure, generating CoT for prediction provides the additional benefit of reasoning alignment, allowing the model to improve by leveraging self-generated data.", "description": "The figure illustrates the difference between training a vision language model exclusively on direct answers versus generating chain-of-thought reasoning for prediction, highlighting the benefits of the latter approach for reasoning alignment and model improvement.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.16198/figures/figures_10_1.png", "caption": "Figure 1: The upper figure questions whether training exclusively on direct-answer datasets can effectively teach CoT prediction. In the lower figure, generating CoT for prediction provides the additional benefit of reasoning alignment, allowing the model to improve by leveraging self-generated data.", "description": "The figure illustrates the difference between training a vision language model exclusively on short answers versus incorporating chain-of-thought reasoning and its impact on model performance.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.16198/figures/figures_16_0.png", "caption": "Figure A.3: An example from the A-OKVQA dataset highlights cases where the annotated answer does not match the GPT-40-generated answer. In these cases, the GPT-40 answers are correct, while the annotations contain labeling errors. In the left figure, the sign reads \"dentist\" (correctly identified by GPT-40), and the answer should relate to 'teeth,' not \u2018heart' as in the annotation. In the right figure, the fridge contains beer, but the annotation incorrectly labels it as 'water.' Consequently, we filter out instances where the GPT-40-generated answer does not match the annotated answers.", "description": "The figure shows two examples from the A-OKVQA dataset where the provided annotations are incorrect, highlighting the need for filtering mismatched annotations during data distillation.", "section": "A SHAREGPT-40-REASONING DATA FOR VLM COT REASONING"}, {"figure_path": "2410.16198/figures/figures_16_1.png", "caption": "Figure A.3: An example from the A-OKVQA dataset highlights cases where the annotated answer does not match the GPT-40-generated answer. In these cases, the GPT-40 answers are correct, while the annotations contain labeling errors. In the left figure, the sign reads \"dentist\" (correctly identified by GPT-40), and the answer should relate to 'teeth,' not \u2018heart' as in the annotation. In the right figure, the fridge contains beer, but the annotation incorrectly labels it as 'water.' Consequently, we filter out instances where the GPT-40-generated answer does not match the annotated answers.", "description": "The figure shows two examples from the A-OKVQA dataset where the GPT-40 generated answers are correct but differ from the annotated answers due to errors in the annotations.", "section": "A SHAREGPT-40-REASONING DATA FOR VLM COT REASONING"}]