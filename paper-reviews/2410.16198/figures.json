[{"figure_path": "2410.16198/figures/figures_2_0.png", "caption": "Figure 1: The upper figure questions whether training exclusively on direct-answer datasets can effectively teach CoT prediction. In the lower figure, generating CoT for prediction provides the additional benefit of reasoning alignment, allowing the model to improve by leveraging self-generated data.", "description": "This figure illustrates the difference between training a vision language model exclusively on short answers versus incorporating chain-of-thought reasoning for improved accuracy and alignment.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.16198/figures/figures_3_0.png", "caption": "Figure 2: Workflow diagram showing: a) the use of GPT-40 to generate rationale given short annotations; b) SFT of open-source VLM for CoT reasoning; c) Build preference dataset for reinforcement learning with DPO to enhance reasoning.", "description": "The figure illustrates the three-stage pipeline for improving VLM chain-of-thought reasoning: rationale distillation from GPT-40, supervised fine-tuning with chain-of-thought data, and reinforcement learning using direct preference optimization.", "section": "3 METHOD"}, {"figure_path": "2410.16198/figures/figures_4_0.png", "caption": "Figure 1: The upper figure questions whether training exclusively on direct-answer datasets can effectively teach CoT prediction. In the lower figure, generating CoT for prediction provides the additional benefit of reasoning alignment, allowing the model to improve by leveraging self-generated data.", "description": "This figure illustrates the difference between training a Vision Language Model (VLM) exclusively on direct answers versus generating chain-of-thought (CoT) reasoning for prediction, highlighting the benefits of reasoning alignment using self-generated data.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.16198/figures/figures_5_0.png", "caption": "Figure 5: The upper section displays the data sources used for the SFT experiments, while the lower section illustrates the data composition for model training.", "description": "The figure shows the data sources and composition used in the supervised fine-tuning (SFT) experiments for training the vision-language models.", "section": "SFT Experiments for Chain-of-Thought Learning"}, {"figure_path": "2410.16198/figures/figures_10_0.png", "caption": "Figure 1: The upper figure questions whether training exclusively on direct-answer datasets can effectively teach CoT prediction. In the lower figure, generating CoT for prediction provides the additional benefit of reasoning alignment, allowing the model to improve by leveraging self-generated data.", "description": "The figure illustrates the difference between training a vision language model exclusively on direct answers versus generating chain-of-thought (CoT) reasoning for prediction, highlighting the benefits of CoT for reasoning alignment and improved model performance.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.16198/figures/figures_10_1.png", "caption": "Figure 1: The upper figure questions whether training exclusively on direct-answer datasets can effectively teach CoT prediction. In the lower figure, generating CoT for prediction provides the additional benefit of reasoning alignment, allowing the model to improve by leveraging self-generated data.", "description": "The figure illustrates the difference between training a vision language model exclusively on short answers versus training it with chain-of-thought reasoning, highlighting the benefits of the latter for reasoning alignment and improved performance.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.16198/figures/figures_16_0.png", "caption": "Figure A.3: An example from the A-OKVQA dataset highlights cases where the annotated answer does not match the GPT-40-generated answer. In these cases, the GPT-40 answers are correct, while the annotations contain labeling errors. In the left figure, the sign reads \"dentist\" (correctly identified by GPT-40), and the answer should relate to 'teeth,' not \u2018heart' as in the annotation. In the right figure, the fridge contains beer, but the annotation incorrectly labels it as 'water.' Consequently, we filter out instances where the GPT-40-generated answer does not match the annotated answers.", "description": "The figure shows two examples from the A-OKVQA dataset where the GPT-40 generated answers are correct but the annotations contain errors, highlighting the need for filtering mismatched annotations during data distillation.", "section": "A SHAREGPT-40-REASONING DATA FOR VLM COT REASONING"}, {"figure_path": "2410.16198/figures/figures_16_1.png", "caption": "Figure A.3: An example from the A-OKVQA dataset highlights cases where the annotated answer does not match the GPT-40-generated answer. In these cases, the GPT-40 answers are correct, while the annotations contain labeling errors. In the left figure, the sign reads \"dentist\" (correctly identified by GPT-40), and the answer should relate to \u2018teeth,\u2019 not \u2018heart\u2019 as in the annotation. In the right figure, the fridge contains beer, but the annotation incorrectly labels it as \u2018water.\u2019 Consequently, we filter out instances where the GPT-40-generated answer does not match the annotated answers.", "description": "The figure shows two examples from the A-OKVQA dataset illustrating annotation errors in which GPT-40 generated correct answers, while the provided annotations contained errors.", "section": "A SHAREGPT-40-REASONING DATA FOR VLM COT REASONING"}]