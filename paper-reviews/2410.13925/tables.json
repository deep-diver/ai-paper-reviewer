[{"figure_path": "2410.13925/tables/table_4_0.html", "caption": "TABLE I: Details of FiTv2 model architecture. We follow our original FiT to set the base model and XL model for FiTv2. We also scale up our FiTv2 to 3 billion parameters as our largest model.", "description": "Table I details the architecture of three FiTv2 models (base, XL, and 3B parameter), specifying the number of layers, hidden size, number of heads, and computational cost (GFLOPs and parameters) for each.", "section": "V. EXPERIMENTS"}, {"figure_path": "2410.13925/tables/table_8_0.html", "caption": "TABLE I: Details of FiTv2 model architecture. We follow our original FiT to set the base model and XL model for FiTv2. We also scale up our FiTv2 to 3 billion parameters as our largest model.", "description": "Table I details the architecture of different FiTv2 models, including the number of layers, hidden size, heads, parameters, and GFLOPs.", "section": "V. EXPERIMENTS"}, {"figure_path": "2410.13925/tables/table_9_0.html", "caption": "TABLE II: Ablation results from FiT-B/2 to FiTv2-B/2 without using classifier-free guidance. We train the models to 2000k steps to assess stability. A X indicates that the training process breaks down before reaching this evaluation point.", "description": "Table II presents an ablation study comparing different configurations of FiTv2-B/2 model, showing the impact of various design choices on FID scores and training stability.", "section": "V. Experiments"}, {"figure_path": "2410.13925/tables/table_9_1.html", "caption": "TABLE II: Ablation results from FiT-B/2 to FiTv2-B/2 without using classifier-free guidance. We train the models to 2000k steps to assess stability. A X indicates that the training process breaks down before reaching this evaluation point.", "description": "Table II presents ablation study results comparing different configurations of FiT and FiTv2 models, showing the impact of various design choices on FID score and training stability.", "section": "V. Experiments"}, {"figure_path": "2410.13925/tables/table_9_2.html", "caption": "TABLE IV: Benchmarking class-conditional image generation with in-distribution resolution on ImageNet dataset. \"-G\" denotes the results with classifier-free guidance. *: Flag-DiT-3B and Large-DiT-3B actually have 4.23 billion parameters, where 3B means the parameters of all transformer blocks. \u2020: MDT-G adpots an improved classifier-free guidance strategy.", "description": "Table IV presents a comparison of class-conditional image generation performance metrics (FID, sFID, IS, Precision, Recall) across various state-of-the-art models on ImageNet, focusing on in-distribution resolutions (256x256, 160x320, 128x384).", "section": "V. EXPERIMENTS"}, {"figure_path": "2410.13925/tables/table_10_0.html", "caption": "TABLE V: Benchmarking class-conditional image generation with out-of-distribution resolution on ImageNet dataset. *: FiTv2 adopts VisionNTK and attention scale for resolution extrapolation. Our FiTv2 model achieves state-of-the-art performance across all the resolutions and aspect ratios, demonstrating a strong extrapolation capability.", "description": "Table V presents a comparison of class-conditional image generation models' performance on out-of-distribution resolutions of ImageNet, highlighting FiTv2's state-of-the-art performance and strong extrapolation capabilities.", "section": "V. Experiments"}, {"figure_path": "2410.13925/tables/table_10_1.html", "caption": "TABLE VI: Benchmarking class-conditional image generation with high-resolution image generation on ImageNet dataset. Our FiTv2 can directly generates images with different aspect ratios with stable and state-of-the-art performance.", "description": "Table VI presents a benchmark comparing FiTv2's performance against other state-of-the-art models on class-conditional image generation tasks using various high-resolution images and aspect ratios.", "section": "V. EXPERIMENTS"}]