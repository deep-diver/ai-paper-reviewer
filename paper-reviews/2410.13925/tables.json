[{"figure_path": "2410.13925/tables/table_4_0.html", "caption": "TABLE I: Details of FiTv2 model architecture. We follow our original FiT to set the base model and XL model for FiTv2. We also scale up our FiTv2 to 3 billion parameters as our largets model.", "description": "Table I details the architecture of various FiTv2 models, including their number of layers, hidden size, number of heads, total parameters, and GFLOPS.", "section": "V. EXPERIMENTS"}, {"figure_path": "2410.13925/tables/table_8_0.html", "caption": "TABLE I: Details of FiTv2 model architecture. We follow our original FiT to set the base model and XL model for FiTv2. We also scale up our FiTv2 to 3 billion parameters as our largest model.", "description": "Table I details the architecture of different FiTv2 models, including the number of layers, hidden size, heads, parameters, and GFLOPs.", "section": "V. EXPERIMENTS"}, {"figure_path": "2410.13925/tables/table_9_0.html", "caption": "TABLE II: Ablation results from FiT-B/2 to FiTv2-B/2 without using classifier-free guidance. We train the models to 2000k steps to assess stability. A X indicates that the training process breaks down before reaching this evaluation point.", "description": "Table II shows ablation study results comparing different configurations of FiT and FiTv2 models, highlighting the impact of various design choices on model performance and training stability.", "section": "V. Experiments"}, {"figure_path": "2410.13925/tables/table_9_1.html", "caption": "TABLE II: Ablation results from FiT-B/2 to FiTv2-B/2 without using classifier-free guidance. We train the models to 2000k steps to assess stability. A X indicates that the training process breaks down before reaching this evaluation point.", "description": "Table II presents ablation study results on FiTv2-B/2 model variants, comparing different components against each other regarding FID scores at various training steps and evaluating training stability.", "section": "V. Experiments"}, {"figure_path": "2410.13925/tables/table_9_2.html", "caption": "TABLE IV: Benchmarking class-conditional image generation with in-distribution resolution on ImageNet dataset. \u201c-G\u201d denotes the results with classifier-free guidance. *: Flag-DiT-3B and Large-DiT-3B actually have 4.23 billion parameters, where 3B means the parameters of all transformer blocks. \u2020: MDT-G adpots an improved classifier-free guidance strategy: wt = (1 \u2212 cos \u03c0(t/tmax))w/2, where w = 3.8 is the maximum guidance scale and s = 4 is the controlling factor.", "description": "Table IV presents a comparison of various class-conditional image generation models on ImageNet using in-distribution resolutions, evaluating their performance using FID, sFID, IS, Precision, and Recall.", "section": "V. EXPERIMENTS"}, {"figure_path": "2410.13925/tables/table_10_0.html", "caption": "TABLE V: Benchmarking class-conditional image generation with out-of-distribution resolution on ImageNet dataset. *: FiTv2 adopts VisionNTK and attention scale for resolution extrapolation. Our FiTv2 model achieves state-of-the-art performance across all the resolutions and aspect ratios, demonstrating a strong extrapolation capability.", "description": "Table V presents a comparison of different models' performance on class-conditional image generation tasks using out-of-distribution resolutions on the ImageNet dataset, highlighting FiTv2's superior performance and extrapolation capabilities.", "section": "V. EXPERIMENTS"}, {"figure_path": "2410.13925/tables/table_10_1.html", "caption": "TABLE VI: Benchmarking class-conditional image generation with high-resolution image generation on ImageNet dataset. Our FiTv2 can directly generates images with different aspect ratios with stable and state-of-the-art performance.", "description": "Table VI presents a comparison of FiTv2 against several state-of-the-art models on image generation tasks with high resolutions, showcasing FiTv2's superior performance across different aspect ratios.", "section": "V. Experiments"}]