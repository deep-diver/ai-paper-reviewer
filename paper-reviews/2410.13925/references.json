{"references": [{" publication_date": "2023", "fullname_first_author": "W. Peebles", "paper_title": "Scalable diffusion models with transformers", "reason": "This paper is highly relevant because it is directly cited in the introduction as a model that FiTv2 improves upon and compares against.  It establishes the state-of-the-art in diffusion models with transformers before FiTv2's introduction, serving as a crucial benchmark for evaluating FiTv2's performance and contributions.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Z. Lu", "paper_title": "Fit: Flexible vision transformer for diffusion model", "reason": "This paper is crucial as it introduces FiT, the predecessor of FiTv2, that this paper improves upon.  Understanding FiT's architecture and limitations is essential to appreciating FiTv2's novel contributions and advancements.  It sets the stage for the improvements presented in FiTv2.", "section_number": 1}, {" publication_date": "2020", "fullname_first_author": "J. Ho", "paper_title": "Denoising diffusion probabilistic models", "reason": "This is a foundational paper in the field of diffusion models, providing a theoretical basis for many subsequent advancements, including DDPMs which FiTv2 improves upon.  Its influence on the development of FiTv2 is implicitly acknowledged through FiTv2's improved training strategy that addresses limitations of the original diffusion models.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "Y. Song", "paper_title": "Score-based generative modeling through stochastic differential equations", "reason": "This paper significantly contributed to the theoretical understanding and advancement of score-based generative models. This work is fundamental as it lays a strong theoretical foundation for other diffusion models, and the work is referenced in section II of the paper.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "R. Rombach", "paper_title": "High-resolution image synthesis with latent diffusion models", "reason": "This paper is a key advancement in high-resolution image synthesis, a problem FiTv2 directly tackles.  By improving upon the state-of-the-art in high-resolution image generation, FiTv2 builds upon and surpasses the contributions of this paper.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "P. Dhariwal", "paper_title": "Diffusion models beat gans on image synthesis", "reason": "This paper marks a significant milestone in image synthesis by demonstrating the superiority of diffusion models over Generative Adversarial Networks (GANs). This milestone in image synthesis heavily influences the context for FiTv2 and is directly referenced.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "J. Song", "paper_title": "Denoising diffusion implicit models", "reason": "This paper presents DDIM, a crucial advancement over the original DDPMs.  The improvement in sampling efficiency, achieved through implicit modeling, is directly relevant to the discussion in the related works section and is a direct comparison point for the methods used in FiTv2.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "X. Liu", "paper_title": "Flow straight and fast: Learning to generate and transfer data with rectified flow", "reason": "This paper introduces rectified flow, a key component of FiTv2's architecture that addresses the limitations of traditional diffusion methods.  Understanding the benefits and mechanisms of rectified flow is essential to fully comprehending FiTv2's design choices and improvements.", "section_number": 3}, {" publication_date": "2017", "fullname_first_author": "A. Vaswani", "paper_title": "Attention is all you need", "reason": "This seminal paper introduced the Transformer architecture, a fundamental building block of many modern deep learning models, including FiTv2.  The use of the transformer architecture forms a foundational aspect for understanding the FiTv2 model.", "section_number": 3}, {" publication_date": "2020", "fullname_first_author": "J. Su", "paper_title": "Roformer: Enhanced transformer with rotary position embedding", "reason": "This paper introduced Rotary Positional Embedding (RoPE), a critical component of FiTv2's architecture.  RoPE is discussed in detail in Section III and is directly incorporated into the FiTv2 architecture, making this reference highly important.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "E. J. Hu", "paper_title": "Lora: Low-rank adaptation of large language models", "reason": "This paper introduces LoRA (Low-Rank Adaptation), an essential technique that is incorporated into FiTv2's design as AdaLN-LORA. AdaLN-LORA is a crucial component for improving efficiency, therefore understanding this approach is essential for understanding the design and performance improvement of FiTv2.", "section_number": 4}, {" publication_date": "2020", "fullname_first_author": "J. Su", "paper_title": "Revisiting attention scale operation from the invariance of entropy", "reason": "This paper is essential for understanding the attention scale method used in the FiTv2 model for resolution extrapolation. It details the theoretical basis and implications of this method which FiTv2 utilizes and further improves upon for high-resolution image generation.", "section_number": 7}, {" publication_date": "2023", "fullname_first_author": "S. Chen", "paper_title": "Extending context window of large language models via positional interpolation", "reason": "This paper introduces a crucial technique that is used in the FiTv2 model for resolution extrapolation, namely Positional Interpolation (PI). The paper is directly cited and is important in understanding how FiTv2 addresses challenges in generating images at resolutions beyond its training data.", "section_number": 7}, {" publication_date": "2022", "fullname_first_author": "P. Esser", "paper_title": "Scaling rectified flow transformers for high-resolution image synthesis", "reason": "This paper presents a state-of-the-art model that utilizes rectified flows for high-resolution image generation. FiTv2 builds upon and improves upon the techniques and results presented in this paper by using similar techniques but achieving even better results.", "section_number": 7}, {" publication_date": "2023", "fullname_first_author": "B. Peng", "paper_title": "Yarn: Efficient context window extension of large language models", "reason": "This paper is highly relevant because it introduces YaRN, another method used in the FiTv2 model for resolution extrapolation. The paper contributes to the techniques used in FiTv2 to handle images with various resolutions and therefore improves the understanding of how FiTv2 works.", "section_number": 7}, {" publication_date": "2023", "fullname_first_author": "M. Dehghani", "paper_title": "Scaling vision transformers to 22 billion parameters", "reason": "This paper focuses on scaling vision transformers to a large number of parameters, a feat that FiTv2 also addresses. The results and methodologies discussed in this paper help FiTv2 understand the challenges involved and better approach the scaling process.", "section_number": 7}, {" publication_date": "2023", "fullname_first_author": "A. Radford", "paper_title": "Learning transferable visual models from natural language supervision", "reason": "This paper is fundamental for the text-to-image generation experiments conducted for FiTv2 because it introduces CLIP (Contrastive Language\u2013Image Pre-training), the model used for encoding text and image features for the text-to-image generation.", "section_number": 6}, {" publication_date": "2023", "fullname_first_author": "D. Podell", "paper_title": "Sdxl: Improving latent diffusion models for high-resolution image synthesis", "reason": "This is crucial for the text-to-image task and high-resolution evaluation of FiTv2 as it introduces SDXL-VAE, a powerful encoder-decoder model used in FiTv2 for image encoding. Its improved resolution capability directly supports FiTv2's high-resolution generation capacity.", "section_number": 6}, {" publication_date": "2022", "fullname_first_author": "C. Saharia", "paper_title": "Photorealistic text-to-image diffusion models with deep language understanding", "reason": "This paper is important because it represents a state-of-the-art in photorealistic text-to-image generation, a problem FiTv2 also directly addresses. FiTv2's text-to-image results are compared to this work's results to establish the model's contributions.", "section_number": 6}, {" publication_date": "2021", "fullname_first_author": "S. Changpinyo", "paper_title": "Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts", "reason": "This paper introduces CC12M, a dataset utilized for the text-to-image generation experiments in FiTv2. The dataset is key to FiTv2's evaluation in this specific task; understanding its characteristics is crucial to evaluating FiTv2's performance in text-to-image generation.", "section_number": 6}]}