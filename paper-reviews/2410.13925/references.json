{"references": [{" publication_date": "2009", "fullname_first_author": "J. Deng", "paper_title": "Imagenet: A large-scale hierarchical image database", "reason": "This paper introduces the ImageNet dataset, a crucial resource used extensively in image generation research, including the training of FiTv2. The ImageNet dataset's scale and diversity are essential for benchmarking and evaluating the performance of image generation models. Its impact on the field is undeniable, making it a highly important reference.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "W. Peebles", "paper_title": "Scalable diffusion models with transformers", "reason": "This paper is highly relevant as it introduces the Diffusion Transformer (DiT) family of models, which FiTv2 builds upon and improves. FiTv2 explicitly addresses the limitations of DiT models in handling arbitrary resolutions, positioning itself as an advancement within this established line of research.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Z. Lu", "paper_title": "Fit: Flexible vision transformer for diffusion model", "reason": "This paper is of paramount importance because it introduces the original FiT model, upon which FiTv2 is based.  FiTv2 is explicitly presented as an improved and enhanced version of FiT, inheriting its core concepts and addressing its limitations. Therefore, understanding FiT is fundamental to understanding FiTv2.", "section_number": 1}, {" publication_date": "2020", "fullname_first_author": "J. Ho", "paper_title": "Denoising diffusion probabilistic models", "reason": "This paper is a seminal work in the field of diffusion models, introducing a fundamental approach that has significantly impacted image generation research. DDPMs, as they are known, serve as a key foundation for many subsequent developments, including the diffusion models used in FiTv2.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "Y. Song", "paper_title": "Score-based generative modeling through stochastic differential equations", "reason": "This paper introduces score-based generative models, providing a theoretical framework and practical methods for generating images. Score-based generative models are closely related to diffusion models and are significant to the development of FiTv2.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "J. Song", "paper_title": "Denoising diffusion implicit models", "reason": "This work introduces DDIMs, an improvement over traditional DDPMs, enabling faster sampling.  DDIM is a crucial building block for various generative models, including some of the models used for comparison in the FiTv2 paper, thereby directly influencing the design and evaluation of FiTv2.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "R. Rombach", "paper_title": "High-resolution image synthesis with latent diffusion models", "reason": "This paper is highly relevant because it introduces LDMs, a class of diffusion models capable of generating high-resolution images.  FiTv2 aims to improve upon the limitations of existing models, implicitly including the high-resolution image generation capabilities of LDMs and related models.", "section_number": 2}, {" publication_date": "2018", "fullname_first_author": "R. T. Chen", "paper_title": "Neural ordinary differential equations", "reason": "This paper introduces Neural ODEs, a significant advancement in deep learning techniques.  The concepts and techniques related to Neural ODEs are directly relevant to the understanding of rectified flows used in FiTv2, as rectified flows are also formulated as ODEs and leverage Neural ODE techniques.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "J. Su", "paper_title": "Roformer: Enhanced transformer with rotary position embedding", "reason": "This paper introduces Rotary Positional Embeddings (RoPE), a key technique for positional encoding in transformers, which directly impacts FiTv2's design. FiTv2 uses 2-D RoPE, building upon the foundation laid by this paper's introduction of the 1-D version and its associated benefits in handling variable-length sequences.", "section_number": 2}, {" publication_date": "2017", "fullname_first_author": "A. Vaswani", "paper_title": "Attention is all you need", "reason": "This foundational paper introduces the Transformer architecture, a fundamental building block of many modern deep learning models.  The concepts and techniques from this paper are directly relevant to understanding FiTv2's transformer-based architecture and its innovations.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "E. J. Hu", "paper_title": "Lora: Low-rank adaptation of large language models", "reason": "This paper introduces LoRA, a low-rank adaptation technique which FiTv2 incorporates in its AdaLN-LORA module.  This is a key aspect of FiTv2's efficiency, allowing parameter-efficient fine-tuning and reducing the model's parameter count and computational costs.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "X. Liu", "paper_title": "Flow straight and fast: Learning to generate and transfer data with rectified flow", "reason": "This paper introduces rectified flows, a core component of FiTv2's improved training strategy. Rectified flows provide a significant improvement over traditional diffusion methods due to their increased sampling efficiency and convergence speed, making this work highly influential on the development of FiTv2.", "section_number": 3}, {" publication_date": "2022", "fullname_first_author": "R. Rombach", "paper_title": "High-resolution image synthesis with latent diffusion models", "reason": "This paper provides the foundation for high-resolution image synthesis methods and is critical to understanding the context for FiTv2's performance in high-resolution image generation.  The methods and results presented directly relate to the challenges and opportunities addressed by FiTv2.", "section_number": 3}, {" publication_date": "2020", "fullname_first_author": "M. Dehghani", "paper_title": "Scaling vision transformers to 22 billion parameters", "reason": "This paper demonstrates the effectiveness of scaling up Vision Transformers, directly informing FiTv2's scaling strategy. The results and insights from scaling up Vision Transformers are highly relevant to FiTv2's own scalability analysis, which investigates the efficiency improvements seen with larger models.", "section_number": 5}, {" publication_date": "2021", "fullname_first_author": "A. Radford", "paper_title": "Learning transferable visual models from natural language supervision", "reason": "This paper introduces the CLIP model, which FiTv2 uses for text encoding in its text-to-image generation experiments. CLIP is a fundamental building block for text-to-image models, and its capabilities and limitations are directly relevant to FiTv2's performance in this task.", "section_number": 6}, {" publication_date": "2023", "fullname_first_author": "N. Ma", "paper_title": "Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers", "reason": "This paper introduces SiT, a strong baseline model that FiTv2 is compared to throughout the paper.  The comparison to SiT is crucial for establishing FiTv2's performance gains and demonstrating its superiority in terms of efficiency and capabilities.", "section_number": 7}, {" publication_date": "2023", "fullname_first_author": "P. Esser", "paper_title": "Scaling rectified flow transformers for high-resolution image synthesis", "reason": "This paper explores scaling up rectified flow transformers, a technique directly relevant to FiTv2.  The results and findings from this work are important for evaluating and interpreting FiTv2's performance in high-resolution image generation and for understanding the impact of using rectified flows.", "section_number": 7}, {" publication_date": "2023", "fullname_first_author": "C. Saharia", "paper_title": "Photorealistic text-to-image diffusion models with deep language understanding", "reason": "This paper introduces a state-of-the-art text-to-image diffusion model, providing a strong benchmark for FiTv2's text-to-image generation capabilities.  The results and findings from this work are highly relevant to the comparison of FiTv2's text-to-image capabilities against existing models, enabling a direct and meaningful assessment of its contributions.", "section_number": 8}, {" publication_date": "2021", "fullname_first_author": "A. Brock", "paper_title": "Large scale gan training for high fidelity natural image synthesis", "reason": "This paper presents the BigGAN model, a significant advancement in image generation. BigGAN is a crucial baseline model used for performance comparisons and helps determine the relative strengths and capabilities of FiTv2 when compared to other high-fidelity image generation models.", "section_number": 7}, {" publication_date": "2022", "fullname_first_author": "A. Sauer", "paper_title": "Stylegan-xl: Scaling stylegan to large diverse datasets", "reason": "This paper introduces StyleGAN-XL, a high-resolution generative model.  The use of StyleGAN-XL as a baseline model for FiTv2's evaluations provides a strong benchmark to assess the relative performance and quality of FiTv2 in generating high-resolution images.", "section_number": 7}]}