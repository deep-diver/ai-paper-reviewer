[{"figure_path": "https://arxiv.org/html/2502.09601/x1.png", "caption": "Figure 1: The reasoning model, after the length-compressible CoT tuning, can generate reasoning paths from long to short, leveraging LoRA as a \u2018Valve\u2019. We show one example from our constructed dataset MixChain.", "description": "This figure illustrates the core concept of CoT-Valve, a novel method for controlling the length of reasoning chains generated by a large language model.  Before CoT-Valve tuning, the model produces lengthy reasoning paths, particularly for relatively straightforward tasks.  CoT-Valve leverages the LoRA technique (Low-Rank Adaptation) to introduce a parameter adjustment that acts like a 'valve', allowing the model to generate reasoning chains of varying length depending on the task's complexity.  The figure demonstrates this by showing a single question's reasoning path generated at different lengths (long, medium, short) using the same model. The example is taken from the MixChain dataset, a new dataset specifically constructed to train and evaluate CoT-Valve's length-control capabilities.  The different lengths of reasoning paths showcase the ability of the model to compress its reasoning process for simpler questions, thereby improving efficiency without significantly sacrificing accuracy.", "section": "3 Method"}, {"figure_path": "https://arxiv.org/html/2502.09601/x2.png", "caption": "Figure 2: Illustration of CoT-Valve. In Stage 1, we first determine \u0394\u2062\u03b8\u0394\ud835\udf03\\Delta\\thetaroman_\u0394 italic_\u03b8 from distilling or post-training. Then, the trained \u0394\u2062\u03b8\u0394\ud835\udf03\\Delta\\thetaroman_\u0394 italic_\u03b8 is utilized to construct the MixChain dataset. Using this dataset, we can then apply two enhanced training methods to achieve more precise control over reasoning paths, or to shorten the reasoning paths as needed.", "description": "This figure illustrates the CoT-Valve method, which dynamically controls the length of reasoning chains generated by a language model. It involves three stages. Stage 1 determines a parameter (\u0394\u03b8) that governs the length of the chain, obtained through model distillation or post-training.  Stage 2 uses this parameter to generate a dataset (MixChain) with varying lengths of reasoning chains for the same questions. Stage 3 applies enhanced training methods, CoT-Valve++ and CoT-Valve+P, on the MixChain dataset to improve control and compression of reasoning paths.", "section": "3 Method"}, {"figure_path": "https://arxiv.org/html/2502.09601/x3.png", "caption": "(a) GSM8K, QwQ-32B-Preview", "description": "The figure shows the relationship between the number of tokens and accuracy achieved by different methods on the GSM8K dataset using the QwQ-32B-Preview model.  It compares the performance of the baseline (prompt), CoT-Valve, CoT-Valve with extrapolation, and CoT-Valve++. The x-axis represents the number of tokens used in the reasoning process, and the y-axis represents the accuracy achieved. This visualization helps illustrate the impact of CoT-Valve on controlling reasoning chain length and its effect on model performance.", "section": "4 Experiments"}, {"figure_path": "https://arxiv.org/html/2502.09601/x4.png", "caption": "(b) GSM8K, Llama-3.2-1B-Instruct", "description": "This figure shows the token length and accuracy results for the GSM8K dataset using the Llama-3.2-1B-Instruct model.  It compares different methods, including prompt-based control and the CoT-Valve methods (with and without extrapolation) and shows how the token count and accuracy vary based on the approach.  The x-axis represents the number of tokens in the generated response, and the y-axis represents the accuracy.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.09601/x5.png", "caption": "(c) AIME, Qwen2.5-32B-I w/ LIMO", "description": "The figure shows the relationship between the number of tokens and accuracy for the AIME dataset using the Qwen-2.5-32B instruction-tuned model with LIMO.  It compares several methods including prompt-based methods, CoT-Valve, and its enhanced variants.  The x-axis represents the number of tokens used in the reasoning chain, and the y-axis represents the accuracy of the model's predictions.  The graph illustrates the performance trade-off between reasoning chain length and accuracy for different methods.  The enhanced CoT-Valve methods aim for better controllability and compressibility of the reasoning chain while maintaining accuracy.", "section": "4 Experiments"}]