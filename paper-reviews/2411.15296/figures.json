[{"figure_path": "https://arxiv.org/html/2411.15296/extracted/6018486/figs/timeline.jpg", "caption": "Figure 1: Time line of existing MLLM benchmarks. The center shows the number of benchmarks born at each time.", "description": "This figure displays a timeline of multimodal large language model (MLLM) benchmark development.  The x-axis represents time, showing the periods when new benchmarks were created. The y-axis implicitly represents the number of new benchmarks introduced during each time period, with the height of the bubbles in the center corresponding to this count. This allows for a visual understanding of the evolution and growth of the MLLM evaluation field over time.", "section": "3 BENCHMARK CATEGORIES"}, {"figure_path": "https://arxiv.org/html/2411.15296/extracted/6018486/figs/arch.jpg", "caption": "Figure 2: Typical MLLM architecture. Tokenizer and De-Tokenizer are used for the processing of text, as the standard flow of LLM. With respect to other modalities, specialized encoders and connectors are often required to convert them into tokens, as well pre-trained generators\u00a0[10, 11] to enable multimodal generation capabilities. There are also methods that employ purely discrete modeling to achieve both understanding and generation\u00a0[12].", "description": "This figure illustrates the typical architecture of a Multimodal Large Language Model (MLLM).  It shows three main modules: a modality encoder, a large language model (LLM), and a connector. The modality encoder processes input from various modalities (text, image, audio, video), converting it into a format suitable for the LLM. The connector aligns the processed multimodal features, combining them with text embeddings to create a unified representation. Finally, the LLM processes this multimodal representation to generate a response in natural language.  The figure also notes that for text, tokenizers and de-tokenizers are used for encoding and decoding.  Other modalities often require specialized encoders and connectors to create tokenized representations.  Finally, the figure notes that some MLLMs use pre-trained generators or purely discrete modeling techniques to enhance multimodal capabilities.", "section": "2.1 Architecture of MLLM"}, {"figure_path": "https://arxiv.org/html/2411.15296/extracted/6018486/figs/train.jpg", "caption": "Figure 3: Illustration of three training stages of MLLMs. In the first stage, image-caption pairs are usually used for the modality alignment. In the second stage, the model is tuned on various QA pairs to make it capable of following instrctions. The third stage is responsible for making the model conform to human preferences.", "description": "The figure illustrates the three stages involved in training Multimodal Large Language Models (MLLMs): pre-training, instruction tuning, and alignment tuning.  Pre-training uses image-caption pairs to align different modalities and integrate multimodal knowledge. Instruction tuning fine-tunes the model using various question-answer pairs to improve its ability to follow instructions. Finally, alignment tuning refines the model to better align with human preferences by incorporating feedback on the quality of responses. This process ensures the model is capable of generating helpful, informative, and accurate outputs in response to multimodal input.", "section": "2 BACKGROUND"}, {"figure_path": "https://arxiv.org/html/2411.15296/extracted/6018486/figs/cases.jpg", "caption": "Figure 4: Categories of MLLM benchmarks.", "description": "This figure presents a hierarchical taxonomy of Multimodal Large Language Model (MLLM) benchmarks, categorized by their evaluation focus.  The top-level categories include Foundational Capability (evaluating basic perception and reasoning skills), Model Self-Analysis (exploring model biases, safety, and hallucination), and Extended Applications (assessing performance in specific domains such as code generation, medical image analysis, and autonomous driving). Each category is further broken down into subcategories, with specific benchmarks listed under each. This detailed breakdown provides a comprehensive overview of the landscape of MLLM evaluation.", "section": "3 BENCHMARK CATEGORIES"}, {"figure_path": "https://arxiv.org/html/2411.15296/x1.png", "caption": "Figure 5: Examples of different MLLM evaluation tasks. The answer can be Open-Ended, Yes-or-No, or Multi-Choice.", "description": "This figure showcases a variety of multimodal large language model (MLLM) evaluation tasks, categorized by their foundational capability, model self-analysis, and extended applications.  Each task example demonstrates different types of questions and answer formats, highlighting the diverse challenges and aspects assessed in MLLM evaluation. These include open-ended questions requiring descriptive answers, yes/no questions with binary answers, and multi-choice questions offering several options.  The visual examples paired with questions further underscore the multimodal nature of the tasks and how LLMs must integrate visual and textual information for successful responses.", "section": "3 Benchmark Categories"}]