[{"figure_path": "https://arxiv.org/html/2412.11919/x1.png", "caption": "Figure 1: \nComparison of retrieval-augmented generation frameworks. (a) Traditional RAG uses a dense retriever for document matching, while (b) generative RAG relies on constrained DocID generation. Both require feeding retrieved document text into the LLM for answer generation. (c) Our RetroLLM unifies retrieval and generation in a single auto-regressive decoding process, leveraging FM-Index constraints to retrieve fine-grained evidence.", "description": "This figure compares different retrieval-augmented generation (RAG) frameworks. Traditional RAG uses a separate retriever to fetch documents, while generative RAG generates document IDs. Both feed retrieved content to an LLM for answer generation. Our RetroLLM combines retrieval and generation into a single process, using FM-Index constraints to retrieve evidence directly within the LLM's decoding process.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2412.11919/x2.png", "caption": "(a) Sequence Relevance", "description": "This figure presents an empirical study on the false pruning problem in constrained evidence generation, comparing the relevance scores of generated sequences under corpus-level and document-level FM-Index constraints with labeled evidence sequences. It aims to demonstrate how restricting FM-Index constraints to a curated subset of relevant documents (document-level) effectively mitigates false pruning compared to using corpus-level constraints.", "section": "Empirical Study"}, {"figure_path": "https://arxiv.org/html/2412.11919/x3.png", "caption": "(b) Overall Accuracy", "description": "This figure presents a bar chart comparing the overall accuracies of constrained evidence generation using a corpus-level FM-Index versus a document-level FM-Index across various beam sizes (1, 5, 10). The results show that constraining the search space to a curated subset of relevant documents (document-level FM-Index) significantly improves the overall accuracy of evidence generation compared to using the entire corpus (corpus-level FM-Index), especially for smaller beam sizes. The experiment is used for mitigating the false pruning problem.", "section": "2 Empirical Study"}, {"figure_path": "https://arxiv.org/html/2412.11919/x4.png", "caption": "Figure 2: Empirical Study on false pruning problem in constrained evidence generation, comparing corpus-level and document-level FM-Index approaches.", "description": "This figure presents an empirical study on the false pruning problem in constrained evidence generation. It compares two approaches: using a corpus-level FM-Index and a document-level FM-Index.  Figure 2(a) shows how the relevance score between the query and generated evidence prefix changes over the first 30 generated tokens, comparing corpus-level constraints, document-level constraints, and labeled data.  A higher relevance score indicates better alignment with the query. Figure 2(b) compares the overall accuracy of evidence generation using different beam sizes for both corpus-level and document-level constraints.  The findings demonstrate that using a document-level FM-Index significantly mitigates false pruning, as the relevance scores remain higher and overall accuracy improves with increasing beam size. Conversely, using a corpus-level FM-Index leads to a sharp drop in relevance and lower accuracy, highlighting the severity of the false pruning issue when using broader constraints.", "section": "2.2 Empirical Study"}, {"figure_path": "https://arxiv.org/html/2412.11919/x5.png", "caption": "Figure 3: \nOverview of the RetroLLM Framework, which retrieves fine-grained evidence through a hierarchical, forward-looking FM-Index constrained generation process. During generation, the model autonomously determines whether to generate additional evidence or provide the final answer, based on the sufficiency of the current context.", "description": "RetroLLM unifies retrieval and generation within a single decoding process. It uses a hierarchical, forward-looking FM-Index constraint approach.  First, RetroLLM generates clues constrained by a global FM-Index to identify relevant documents. Then, it generates evidence constrained by document-specific FM-Indexes, guided by forward-looking relevance scores based on future windows within candidate documents. Finally, after sufficient evidence is generated, it produces the final answer.", "section": "3 RetroLLM: Retrieval in Generation"}, {"figure_path": "https://arxiv.org/html/2412.11919/x6.png", "caption": "(a) Parameters vs. Accuracy", "description": "This figure, located in the \"Experimental Results\" section (specifically section 4.4), illustrates the impact of different base language models (LLMs) and their parameter sizes on the accuracy of RetroLLM.  Three LLM series \u2013 Mistral, Llama3, and Qwen2.5 \u2013 are evaluated with parameter sizes ranging from 1 billion to 14 billion. The x-axis represents the LLM's parameter size, while the y-axis represents the accuracy achieved on a suite of five open-domain question answering datasets. The plot helps visualize how model scale affects performance, showing a general trend of increasing accuracy with larger parameter sizes.", "section": "4 Experimental Settings"}, {"figure_path": "https://arxiv.org/html/2412.11919/x7.png", "caption": "(b) Parameters vs. F1", "description": "This line graph demonstrates the change in F1 score as the number of parameters in the base language model changes. The number of parameters is presented along the x-axis and F1 score along the y-axis.", "section": "4.4 Experimental Results"}, {"figure_path": "https://arxiv.org/html/2412.11919/x8.png", "caption": "Figure 4: Impact of performance with different base LLMs, reporting average performance on five datasets.", "description": "This figure analyzes the effect of different base Large Language Models (LLMs) and their parameter sizes on the performance of RetroLLM.  It uses three LLM families (Mistral, Llama3, and Qwen2.5) with sizes varying from 1B to 14B parameters and reports average performance metrics (accuracy and F1 score) across five datasets (NQ, TriviaQA, HotpotQA, PopQA, 2WikiMultiHop). Subfigure (a) shows the trend of accuracy as the parameter size increases, and subfigure (b) depicts the same trend for the F1 score.  This allows for a comparison of model performance and scaling trends across different LLMs and their sizes when used within the RetroLLM framework.", "section": "4. Experimental Results"}]