[{"content": "| Benchmark | Venue | Long Video Included | User-Centric | Scalable | Open-Ended | Automated |\n|---|---|---|---|---|---|---|\n| **MVBench** [35] | CVPR 24 | \u2717 | \u2717 | \u2717 | \u2717 | \u2713 |\n| **MLVU** [78] | Arxiv 24 | \u2713 | \u2717 | \u2717 | \u2717 | \u2713 |\n| **LVBench** [58] | Arxiv 24 | \u2713 | \u2717 | \u2717 | \u2717 | \u2713 |\n| **VideoMME** [20] | Arxiv 24 | \u2713 | \u2717 | \u2717 | \u2717 | \u2713 |\n| **LongVideoBench** [59] | NeurIPS 24 | \u2713 | \u2717 | \u2717 | \u2717 | \u2713 |\n| **WildVision Video Arena** [45] | NeurIPS 24 | ? | \u2713 | \u2717 | \u2713 | \u2717 |\n| **VideoAutoArena (Ours)** | - | \u2713 | \u2713 | \u2713 | \u2713 | \u2713 |", "caption": "Table 1: Comparison of recent popular benchmarks for video analysis. WildVision video data are not yet publicly available.", "description": "Table 1 compares several recent popular benchmarks for video analysis, highlighting key differences in their features.  These features include whether the benchmark uses videos of long duration, whether the evaluation is user-centric (focuses on real-world user needs and questions), scalability of the evaluation (ability to handle a large number of models and videos), whether the benchmark uses open-ended questions (allowing for more complex and nuanced responses) rather than multiple choice questions, and finally, whether the evaluation process is automated.", "section": "2. Related Work"}, {"content": "| Models | Size | ELO | Win Rates | <p style='text-align:center'> (8s, 15s) </p> | <p style='text-align:center'> (15s, 60s) </p> | <p style='text-align:center'> (180s, 600s) </p> | <p style='text-align:center'> (900s, 3600s) </p> |\n|---|---|---|---|---|---|---|---| \n| <p style='text-align:center'> *Proprietary Models*</p>  |  |  |  |  |  |  |  |\n| <img src=\"https://arxiv.org/html/2411.13281/extracted/6012238/emoji/OpenAI.png\" width=\"2\" height=\"2\"> GPT-4o | - | **1505.69** | **89.19** | **1447.86** | **1449.59** | **1575.34** | **1552.23** |\n| <img src=\"https://arxiv.org/html/2411.13281/extracted/6012238/emoji/OpenAI.png\" width=\"2\" height=\"2\"> GPT-4o-mini | - | 1323.25 | 76.90 | 1293.27 | 1343.28 | 1327.75 | 1349.29 |\n| <img src=\"https://arxiv.org/html/2411.13281/extracted/6012238/emoji/google.png\" width=\"2\" height=\"2\"> Gemini-1.5-Pro | - | 1187.01 | 65.11 | 1247.65 | 1171.82 | 1263.58 | 1291.64 |\n| <img src=\"https://arxiv.org/html/2411.13281/extracted/6012238/emoji/google.png\" width=\"2\" height=\"2\"> Gemini-1.5-Flash | - | 1149.52 | 62.07 | 1081.58 | 1131.27 | 1140.07 | 1260.36 |\n| <p style='text-align:center'> *Open-Source Models*</p> |  |  |  |  |  |  |  |\n| <img src=\"https://arxiv.org/html/2411.13281/extracted/6012238/emoji/rhymes.jpg\" width=\"2\" height=\"2\"> Aria | 8 \u00d7 3.5B | **1119.99** | **59.54** | **1147.45** | **1273.77** | **1110.67** | **1111.40** |\n| <img src=\"https://arxiv.org/html/2411.13281/extracted/6012238/emoji/Qwen.png\" width=\"2\" height=\"2\"> Qwen2-VL | 72B | 886.52 | 35.61 | 985.46 | 928.23 | 829.65 | 826.56 |\n| <img src=\"https://arxiv.org/html/2411.13281/extracted/6012238/emoji/Qwen.png\" width=\"2\" height=\"2\"> Qwen2-VL | 7B | 875.56 | 34.90 | 969.28 | 859.33 | 850.30 | 829.21 |\n| <img src=\"https://arxiv.org/html/2411.13281/extracted/6012238/emoji/llava-next.png\" width=\"3\" height=\"3\"> LLaVA-Video | 72B | 836.62 | 30.25 | 796.90 | 850.12 | 827.88 | 782.55 |\n| <img src=\"https://arxiv.org/html/2411.13281/extracted/6012238/emoji/llava-next.png\" width=\"3\" height=\"3\"> LLaVA-Video | 7B | 765.61 | 23.52 | 672.35 | 736.14 | 759.15 | 721.78 |\n| <img src=\"https://arxiv.org/html/2411.13281/extracted/6012238/emoji/llava-next.png\" width=\"3\" height=\"3\"> LLaVA-OneVision | 72B | 763.71 | 23.11 | 731.50 | 710.64 | 759.29 | 741.80 |\n| <img src=\"https://arxiv.org/html/2411.13281/extracted/6012238/emoji/llava-next.png\" width=\"3\" height=\"3\"> LLaVA-OneVision | 7B | 586.52 | 9.86 | 626.70 | 545.82 | 556.31 | 533.18 |", "caption": "Table 2: Our VideoAutoArena Leaderboard. We show the overall ELO ratings and win rates within four different video lengths.", "description": "This table presents the results of the VideoAutoArena benchmark, which automatically evaluates large multimodal models (LMMs) in video analysis.  It shows the overall ELO rating for each of the 11 models tested, reflecting their relative performance across multiple video lengths.  Win rates are also provided for four distinct video duration ranges (8-15s, 15-60s, 180-600s, 900-3600s).  The ELO ratings represent a continuous comparison across multiple models and video lengths, facilitating a dynamic and fair assessment of LMM video analysis capabilities.", "section": "3. VideoAutoArena"}, {"content": "| Models | vs. Sel. | vs. Rej. | Avg. |\n|---|---|---|---|\n| ![OpenAI](https://arxiv.org/html/2411.13281/extracted/6012238/emoji/OpenAI.png) **GPT-4o** | **70.98** | **94.12** | **82.55** |\n| ![OpenAI](https://arxiv.org/html/2411.13281/extracted/6012238/emoji/OpenAI.png) **GPT-4o-mini** | 49.80 | 92.16 | 70.98 |\n| ![google](https://arxiv.org/html/2411.13281/extracted/6012238/emoji/google.png) **Gemini-1.5-Pro** | 28.24 | 82.74 | 55.49 |\n| ![google](https://arxiv.org/html/2411.13281/extracted/6012238/emoji/google.png) **Gemini-1.5-Flash** | 27.25 | 81.96 | 54.61 |\n| ![rhymes](https://arxiv.org/html/2411.13281/extracted/6012238/emoji/rhymes.png) **Aria** | **19.80** | **76.86** | **48.33** |\n| ![Qwen](https://arxiv.org/html/2411.13281/extracted/6012238/emoji/Qwen.png) **Qwen2-VL-72B** | 13.92 | 64.71 | 39.32 |\n| ![Qwen](https://arxiv.org/html/2411.13281/extracted/6012238/emoji/Qwen.png) **Qwen2-VL-7B** | 11.96 | 60.00 | 35.98 |\n| ![llava-next](https://arxiv.org/html/2411.13281/extracted/6012238/emoji/llava-next.png) **LLaVA-Video-72B** | 7.45 | 56.08 | 31.77 |\n| ![llava-next](https://arxiv.org/html/2411.13281/extracted/6012238/emoji/llava-next.png) **LLaVA-OneVision-72B** | 4.12 | 52.16 | 28.14 |\n| ![llava-next](https://arxiv.org/html/2411.13281/extracted/6012238/emoji/llava-next.png) **LLaVA-Video-7B** | 5.29 | 46.67 | 25.98 |\n| ![llava-next](https://arxiv.org/html/2411.13281/extracted/6012238/emoji/llava-next.png) **LLaVA-OneVision-7B** | 3.53 | 30.98 | 17.26 |", "caption": "Table 3: LMMs compete against human selected or rejected answers in our VideoAutoBench.", "description": "This table presents the results of a benchmark called VideoAutoBench.  VideoAutoBench uses human annotations from a subset of battles (model comparisons) in VideoAutoArena to create a gold standard.  GPT-40, a large language model, is then used as an automatic judge to compare the model's answers against these human-selected answers (correct responses) and human-rejected answers (incorrect responses). The table shows how well each model performs compared to these human judgments, providing a streamlined and cost-effective evaluation method for large multimodal models (LMMs) in video analysis.", "section": "4. VideoAutoBench"}]