{"importance": "This paper is crucial for researchers in video analysis and multimodal learning. It introduces **VideoAutoArena**, a novel and scalable automated evaluation framework addressing the limitations of existing benchmarks.  This significantly reduces the high cost and time associated with human annotation, and opens new avenues for research on LMM evaluation.  The **fault-driven evolution strategy** enhances evaluation rigor, while **VideoAutoBench** provides a streamlined alternative for quick assessments.", "summary": "VideoAutoArena automates large multimodal model (LMM) evaluation using simulated users, offering a cost-effective and scalable solution compared to traditional human annotation.", "takeaways": ["VideoAutoArena automates LMM evaluation in video analysis using simulated users.", "A novel fault-driven evolution strategy enhances evaluation rigor by progressively increasing question complexity.", "VideoAutoBench offers a streamlined benchmark for faster and more accessible evaluation."], "tldr": "Current large multimodal model (LMM) evaluation methods for video analysis rely heavily on traditional benchmarks using multiple-choice questions.  These methods often fail to capture the nuances of real-world user interaction and are expensive and time-consuming. This paper addresses these issues by proposing a new evaluation method.\nThe proposed method, VideoAutoArena, uses Large Language Models (LLMs) to simulate human users, generating open-ended and adaptive questions. It incorporates an automated judging system and a fault-driven evolution strategy to enhance scalability and rigor. The results demonstrate that VideoAutoArena effectively differentiates among state-of-the-art LMMs and aligns well with human judgment, offering a cost-effective and scalable evaluation framework.", "affiliation": "Hong Kong Baptist University", "categories": {"main_category": "Multimodal Learning", "sub_category": "Vision-Language Models"}, "podcast_path": "2411.13281/podcast.wav"}