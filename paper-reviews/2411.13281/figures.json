[{"figure_path": "https://arxiv.org/html/2411.13281/x2.png", "caption": "Figure 1: An overview of our VideoAutoArena, where we leverage LMMs for user simulation to automatically evaluate LMMs in video analysis, offering an efficient alternative to costly and time-consuming human annotations, distinct from platforms like LMSYS Chatbot Arena\u00a0[14] and WildVision Arena\u00a0[45]. In this figure, we showcase 4 sampled frames from a Singapore travel vlog video.", "description": "This figure illustrates the VideoAutoArena, a novel automated benchmark for evaluating large multimodal models (LMMs) in video analysis.  Unlike traditional methods relying on human annotation, VideoAutoArena uses LMMs to simulate user behavior, generating open-ended, adaptive questions to assess LMM performance. The system includes a peer battle mechanism where two LMMs answer the same question and an automated judging system to determine the better response. The figure showcases four sample frames from a Singapore travel vlog video used in the benchmark, highlighting the type of video content analyzed.", "section": "3. VideoAutoArena"}, {"figure_path": "https://arxiv.org/html/2411.13281/x3.png", "caption": "Figure 2: Video statistics by category and duration.", "description": "This figure shows two pie charts visualizing the distribution of videos used in the VideoAutoArena benchmark.  The left chart presents the video categories, indicating the proportion of videos in each category (Movie, Life Vlogs, Geography, History, News Programs, Art, STEM, Computer Science, Cooking Recipes, and Travel Guides). The right chart displays the video duration distribution, showing the percentage of videos falling within four duration ranges: (8s, 15s], (15s, 60s], (180s, 600s], and (900s, 3600s].  The total number of videos used (2881) is indicated in both charts.", "section": "3. VideoAutoArena"}, {"figure_path": "https://arxiv.org/html/2411.13281/x4.png", "caption": "Figure 3: Examples of synthesized personas with three levels of relevance and corresponding synthesized questions. We also compare the style of our questions with those in popular long-video benchmarks, including LongVideoBench and VideoMME.", "description": "This figure showcases examples of user personas synthesized by VideoAutoArena, categorized by their relevance to the video content (highly related, moderately related, and unrelated). For each persona, a corresponding question is generated to exemplify how user simulation produces open-ended questions for video understanding tasks.  The questions are designed to assess the models' video analysis abilities from a user-centric perspective. The figure also contrasts the question styles of VideoAutoArena with those of existing benchmarks (LongVideoBench and VideoMME), highlighting how VideoAutoArena's approach better reflects realistic user inquiries.", "section": "3. VideoAutoArena"}, {"figure_path": "https://arxiv.org/html/2411.13281/x5.png", "caption": "(a) Visualization of persona distribution.", "description": "This visualization uses t-SNE to reduce the dimensionality of persona vectors, derived from a sentence embedding model encoding persona descriptions. It compares the distribution of automatically generated personas from VideoAutoArena with those from the PersonaHub dataset.  This allows for a visual comparison of the diversity and representativeness of the user personas generated by VideoAutoArena relative to a well-established, publicly available persona dataset.", "section": "3. VideoAutoArena"}, {"figure_path": "https://arxiv.org/html/2411.13281/x6.png", "caption": "(b) Humans preference ranking.", "description": "This figure presents a bar chart comparing the ranking of questions from VideoAutoArena, VideoMME, and LongVideoBench, based on human preference.  The chart shows the percentage of times each benchmark's questions were ranked first, second, or third by human evaluators.  This indicates how well each benchmark's question style mirrors real-world user queries in video analysis.", "section": "3. VideoAutoArena"}, {"figure_path": "https://arxiv.org/html/2411.13281/x7.png", "caption": "Figure 4: Our user simulation offers diverse personas and more effectively mirrors real-world users\u2019 question styles.", "description": "This figure visualizes the distribution of personas generated by VideoAutoArena and compares it to the distribution of personas from PersonaHub.  The left-hand panel (4a) uses t-SNE to project the high-dimensional persona embeddings into a 2D space, showing a wider spread of our generated personas compared to PersonaHub. The right-hand panel (4b) shows a bar chart comparing the ranking of questions across humans, based on whether the question style best reflects real-world user questions. VideoAutoArena outperforms VideoMME and LongVideoBench, indicating its questions more accurately simulate real user queries.", "section": "3. VideoAutoArena"}, {"figure_path": "https://arxiv.org/html/2411.13281/x8.png", "caption": "Figure 5: Our fault-driven evolution strategy generates increasingly challenging questions for video analysis.", "description": "This figure demonstrates how the fault-driven evolution strategy in VideoAutoArena progressively increases the difficulty of questions posed to large multimodal models (LMMs).  It shows that by iteratively analyzing model responses and identifying weaknesses, the system generates increasingly complex and nuanced questions designed to push the models' video analysis capabilities. The graph likely displays metrics (e.g., question complexity scores or model performance) over a series of evolving questions, illustrating the improvement in question difficulty achieved by the fault-driven evolution strategy.", "section": "3.3. Peer Battles and Fault-Driven Evolution"}, {"figure_path": "https://arxiv.org/html/2411.13281/x9.png", "caption": "Figure 6: Evaluate the accuracy of various judging methods using human annotations as the gold standard. In the Vote (Top N) method, the top N models are used to cast votes.", "description": "This figure displays the accuracy of different judging methods for evaluating large multimodal models (LMMs) in video analysis.  The accuracy of each method is measured against human annotations, which serve as the gold standard.  Specifically, it compares the accuracy of using a single SOTA LMM (GPT-40) as a judge against using a voting system based on the top N performing LMMs. The voting system uses the top 2, 3, and 4 models' judgments to arrive at a final decision. This demonstrates the effectiveness of utilizing a single, high-performing model versus a consensus-based approach for automating the judging process in a scalable and efficient video analysis benchmark.", "section": "3.4 Judging and Ranking"}, {"figure_path": "https://arxiv.org/html/2411.13281/x10.png", "caption": "Figure 7: ELO ratings for models competing on questions before and after applying fault-aware evolution.", "description": "This figure displays the ELO ratings of eleven large multimodal models (LMMs) before and after applying a fault-driven evolution strategy.  The fault-driven evolution progressively increases the complexity of the questions asked to the models, testing their capabilities more rigorously. The comparison allows for an assessment of how well each model adapts to more challenging video analysis scenarios.", "section": "3. VideoAutoArena"}, {"figure_path": "https://arxiv.org/html/2411.13281/x11.png", "caption": "Figure 8: We evaluate the performance of various models based on four different judging standards.", "description": "This figure displays a bar chart visualizing the performance of eleven large multimodal models (LMMs) across four evaluation metrics: Instruction Following, Accuracy, Relevance, and Helpfulness.  Each bar represents an LMM's score on a specific metric, allowing for a comparison of model strengths and weaknesses across various aspects of video understanding.  The chart offers insights into how different models perform on user-centric evaluation standards, highlighting the importance of assessing LMMs beyond traditional accuracy metrics.", "section": "3.4 Judging and Ranking"}, {"figure_path": "https://arxiv.org/html/2411.13281/x12.png", "caption": "Figure 9: Example of a battle between Aria and LLaVa-Video-72B. Red highlights key content, while green highlights important details mentioned only by Aria.", "description": "This figure showcases a comparison between two large multimodal models (LLMs), Aria and LLaVa-Video-72B, in a head-to-head comparison on a video analysis task.  The models were asked the same question about the video. The responses are shown side-by-side. Key information correctly identified by both models is highlighted in red.  Information correctly mentioned only by Aria, demonstrating its superior performance on this specific task, is highlighted in green.  This example illustrates the type of detailed comparison used in the VideoAutoArena benchmark to automatically evaluate different LLMs on their ability to understand and respond to video analysis queries.", "section": "3. VideoAutoArena"}, {"figure_path": "https://arxiv.org/html/2411.13281/x13.png", "caption": "Figure 10: The prompt for video content-constrained persona generation.", "description": "This figure shows the prompt used in the VideoAutoArena framework for generating user personas based on video content.  The prompt instructs the language model to generate three personas: one with a background highly relevant to the video, one with a moderately relevant background, and one with an unrelated background. For each persona, a short paragraph description is requested to simulate real users' diverse backgrounds and motivations for seeking video analysis assistance.", "section": "3. VideoAutoArena"}, {"figure_path": "https://arxiv.org/html/2411.13281/x14.png", "caption": "Figure 11: The prompt for persona-constrained video question asking.", "description": "This figure shows the prompt used to instruct a large language model (LLM) to generate questions for video analysis. The prompt simulates a real user by providing a persona (a description of a user's background, interests, etc.) and then asks the LLM to create a question about a video that would align with that persona. The prompt emphasizes generating a high-quality, realistic question that a real user would ask, rather than a question designed purely for testing the LLM's capabilities.  The prompt also includes instructions for the LLM to provide an ideal response to the question it generated, which helps to evaluate the LLM's overall video understanding capabilities.", "section": "3. VideoAutoArena"}, {"figure_path": "https://arxiv.org/html/2411.13281/x15.png", "caption": "Figure 12: The prompt for our fault-driven evolution generates new questions based on the responses from the two models.", "description": "This figure shows the prompt used in the VideoAutoArena framework to generate new questions for model evaluation.  The process is iterative and designed to increase the complexity of the questions. The prompt instructs the agent to analyze responses from two models, identify their faults and weaknesses, and generate a new, more challenging question targeting those weaknesses.  The goal is to create a progressively more difficult evaluation by focusing on model shortcomings in previous rounds. The new question should still align with the user's persona, but focus on areas where the previous models showed flaws in their understanding of the video.", "section": "3.3 Peer Battles and Fault-Driven Evolution"}, {"figure_path": "https://arxiv.org/html/2411.13281/x16.png", "caption": "Figure 13: The prompt for our automatic judging.", "description": "This figure displays the prompt used by the researchers for their automatic judging process in VideoAutoArena.  The prompt instructs the judge (an LLM) to evaluate two model responses to a video-related question based on four criteria: Instruction Following, Accuracy, Relevance, and Helpfulness.  The judge must analyze each response for these criteria and then provide an overall judgment: Model A wins, Model B wins, Tie (both good), or Tie (both bad). The detailed criteria for each of the four dimensions are also provided.", "section": "3.4 Judging and Ranking"}, {"figure_path": "https://arxiv.org/html/2411.13281/x17.png", "caption": "Figure 14: The prompt for question complexity evaluation.", "description": "This figure shows the prompt used to automatically evaluate the complexity of questions generated for VideoAutoArena.  The prompt presents two questions to the evaluator, who must rate each question across four criteria (Instruction Following, Accuracy, Relevance, Helpfulness) on a scale of 1-5 (1 being easiest, 5 being hardest).  The evaluator then provides an overall difficulty score for each question.  This process helps to ensure that the questions used in VideoAutoArena are progressively challenging and suitable for evaluating the capabilities of large multimodal models (LMMs).", "section": "3. VideoAutoArena"}, {"figure_path": "https://arxiv.org/html/2411.13281/x18.png", "caption": "Figure 15: Examples of our user simulation include five videos from diverse domains: Movies, Computer Science, Life Vlogs, Art, and News Programs. To save space, we only showcase 4 frames of each video.", "description": "This figure showcases examples of user simulations generated by VideoAutoArena for five diverse videos, representing different domains: Movies, Computer Science, Life Vlogs, Art, and News Programs.  For each video, VideoAutoArena simulates a user persona, generating corresponding questions to assess LMMs\u2019 video understanding capabilities.  The figure displays four representative frames from each video to illustrate the variety of content used in the benchmark and to highlight the diverse contexts within which user simulations are generated.", "section": "3. VideoAutoArena"}, {"figure_path": "https://arxiv.org/html/2411.13281/x19.png", "caption": "Figure 16: Example of the battle between Aria and GPT-4o.", "description": "This figure shows a battle between two large multimodal models (LMMs), Aria and GPT-40, in the VideoAutoArena benchmark.  A question is posed regarding the PC-DAN (Point Cloud Deep Affinity Network) method for 3D multi-object tracking, specifically how it uses point clouds and its advantages in autonomous vehicles.  The responses from Aria and GPT-40 are presented, highlighting differences in their level of detail, technical accuracy, and relevance to the user's background.  A judging section follows, evaluating each response based on four criteria: Instruction Following, Accuracy, Relevance, and Helpfulness.  The final judgement indicates which model is superior based on this evaluation.", "section": "3. VideoAutoArena"}, {"figure_path": "https://arxiv.org/html/2411.13281/x20.png", "caption": "Figure 17: Example of the battle between GPT-4o-mini and LLaVa-Video-72B.", "description": "This figure shows a battle between two large multimodal models (LMMs): GPT-40-mini and LLaVa-Video-72B, in the VideoAutoArena benchmark.  Both models receive the same video and a question from a simulated user persona (an art teacher seeking lesson plan ideas).  The models' responses are evaluated based on instruction following, accuracy, relevance, and helpfulness, with GPT-40-mini ultimately deemed the better response.  The figure illustrates the automated evaluation process in VideoAutoArena, showing how the benchmark generates comparable responses and compares their quality using automated metrics. The comparative results show that GPT-40-mini produced a more detailed and helpful response for teaching purposes.", "section": "3. VideoAutoArena"}, {"figure_path": "https://arxiv.org/html/2411.13281/x21.png", "caption": "Figure 18: Example of the battle between Qwen2-VL-72B and LLaVa-Video-7B.", "description": "This figure shows a side-by-side comparison of the responses generated by Qwen2-VL-72B and LLaVa-Video-7B to the same question.  The question is posed within the context of a video about foraging for herbs and making tea, connecting it to folklore and personal stories. The figure highlights how each model addresses the question, allowing for a qualitative assessment of their strengths and weaknesses in terms of instruction following, accuracy, relevance, and helpfulness in this specific context.  The automatic judging section determines which model's answer is better, based on predefined criteria. ", "section": "3. VideoAutoArena"}, {"figure_path": "https://arxiv.org/html/2411.13281/x22.png", "caption": "Figure 19: Example of the battle between Aria and Qwen2-VL-72B.", "description": "This figure showcases a comparison of responses generated by Aria and Qwen2-VL-72B to a user's question about a historical artifact called the \"Mantuan Roundel.\"  The user, described in a persona, asks about the significance of the materials and techniques used in the artifact, and how they reflect Renaissance artistic practices.  Both models attempt to answer, but the figure highlights that Aria provides a more detailed and accurate response, including specific details about the materials and techniques (gilding, silvering) and connecting them to specific themes in Renaissance art. The automatic judging system in the VideoAutoArena framework favors Aria's response as more helpful and relevant.", "section": "3. VideoAutoArena"}]