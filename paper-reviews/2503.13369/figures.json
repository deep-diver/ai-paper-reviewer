[{"figure_path": "https://arxiv.org/html/2503.13369/extracted/6287606/figures/visual_abstract_bk.png", "caption": "Figure 1: The key benefit of utilizing sighted user feedback lies in their assessments, which are based on solid visual grounding. The compiled assessments prove an effective training substance for steering VLMs towards more accessible descriptions. Dataset use and the subsequent validation are described in Sec.\u00a04. A complete list of use cases is provided in Appendix\u00a0A.", "description": "This figure illustrates the process of creating the SIGHTATION dataset.  Sighted users, both general participants and educators, provide feedback on diagram descriptions generated by vision-language models (VLMs). This feedback is crucial because it leverages the sighted users' solid visual understanding, which is then used to guide the VLMs. This process creates a dataset aligned with the preferences of blind and low-vision (BLV) users, making the generated descriptions more accessible for them. Details of the dataset usage and validation process are explained in Section 4, with a comprehensive list of use cases detailed in Appendix A.", "section": "Dataset Construction"}, {"figure_path": "https://arxiv.org/html/2503.13369/extracted/6287606/figures/dimensions_assignment.png", "caption": "Figure 2: The qualities assessed by their respective groups.", "description": "This figure shows the different qualities assessed by three groups of annotators involved in evaluating diagram descriptions.  The three groups were: sighted general participants, sighted educators, and blind or low-vision (BLV) educators. Each group focused on assessing a different subset of qualities, reflecting their respective backgrounds and experiences. This is crucial because it shows how different perspectives are incorporated into the evaluation of the data.", "section": "3 The SIGHTATION Dataset"}, {"figure_path": "https://arxiv.org/html/2503.13369/extracted/6287606/figures/together.png", "caption": "Figure 3: Tuning VLMs on Sightation enhanced various qualities of the diagram descriptions, evaluated by BLV educators, and shown here as normalized ratings averaged in each aspect. The capability of the dataset is most strongly pronounced with the 2B variant, shown above. Full results across 4 models and 22 metrics are reported in Tables\u00a0E.1, \u00a0E.1, \u00a011, and \u00a012.", "description": "The radar chart visualizes the effect size of fine-tuning and guided generation on diagram descriptions generated by various vision-language models (VLMs).  The evaluation is based on assessments by blind and low-vision (BLV) educators across several dimensions: succinctness, diversity, usefulness in different question formats (summary, multiple-choice, open-ended), and interpretability (Nature).  The 2B model shows the most significant improvement.  More detailed results are available in the supplementary material.", "section": "Performance Analysis"}, {"figure_path": "https://arxiv.org/html/2503.13369/extracted/6287606/figures/interleaved_evaluation.png", "caption": "Table 2: Combined recipe effect size on each aspect, measured with BLV assessment.", "description": "This table presents the effect size of using a combined approach (fine-tuning and guided generation) on various aspects of diagram descriptions, as assessed by blind and low-vision (BLV) users.  Effect size is measured using Cohen's d, indicating the standardized difference in means between the experimental condition (combined approach) and a baseline.  Higher values of Cohen's d represent a larger impact of the combined approach on that specific aspect.", "section": "Performance Analysis"}, {"figure_path": "https://arxiv.org/html/2503.13369/x1.png", "caption": "Table 3: Fine tuning effect size on each aspect, measured with BLV assessment.", "description": "This table presents the effect size of fine-tuning various models on the SIGHTATION dataset, specifically focusing on how well the generated descriptions meet the needs and preferences of blind and low-vision (BLV) users.  Effect size is measured using Cohen's d, which quantifies the difference in mean ratings between fine-tuned and baseline models, normalized by the pooled standard deviation. Each row represents a different aspect of the descriptions that were assessed (e.g., succinctness, diversity, usefulness in different contexts), with separate values for the 2B and 7B models.  Larger values indicate stronger effects of fine-tuning.", "section": "Performance Analysis"}, {"figure_path": "https://arxiv.org/html/2503.13369/extracted/6287606/figures/wins_average.png", "caption": "Table 4: Guided generation effect size on each aspect, measured with BLV assessment.", "description": "This table presents the effect size of applying guided generation to the model's outputs, specifically focusing on how well the resulting descriptions align with the preferences of blind and low-vision (BLV) users.  The effect size is measured for each assessment aspect: succinctness, diversity, usefulness (as summary, multiple-choice questions, and open-ended questions), and the overall nature (how interpretive vs. factual).  Larger effect sizes indicate that guided generation has a more substantial impact on that particular aspect.  The table separately shows results for the 2B and 7B models, indicating any differences in the model's response to the treatment.", "section": "Performance Analysis"}, {"figure_path": "https://arxiv.org/html/2503.13369/extracted/6287606/figures/best_sentence_length_word.png", "caption": "Figure 4: Percentage distribution of the quality of question-answer pairs in AI2D and SightationVQA", "description": "This bar chart compares the quality distribution of question-answer pairs from the AI2D dataset and the SIGHTATIONVQA dataset.  The x-axis represents quality levels, ranging from \"very poor\" to \"excellent.\" The y-axis shows the percentage of question-answer pairs falling into each quality level.  The chart visually demonstrates that SIGHTATIONVQA has a significantly higher percentage of question-answer pairs rated as \"excellent\" compared to AI2D.", "section": "A.1 SIGHTATIONVQA"}, {"figure_path": "https://arxiv.org/html/2503.13369/extracted/6287606/figures/spider_retrieval_blip2coco_vs_blip2ours.png", "caption": "Figure 5: Less can be more for BLV users. Our approach streamlines details to highlight the core information while emphasizing key details to increase information density and maximize information efficiency per unit length.", "description": "This figure demonstrates how streamlining diagram descriptions for blind and low-vision (BLV) users can improve information density and efficiency.  The example shows two descriptions of the same diagram. The first is a longer, more detailed description, typical of those generated by sighted individuals. The second is a shorter, more concise description designed specifically for BLV users, highlighting only the core information and key details.", "section": "Dataset Construction"}]