{"importance": "This paper introduces a **dynamic evaluation system** and an **open data repository** to address the challenges of LLM code generation. It offers valuable resources and tools for researchers to evaluate models and advance code generation.", "summary": "CodeArena: Collective evaluation for LLM code generation.", "takeaways": ["Introduces a collective evaluation system for unbiased LLM code generation assessment.", "Provides a public repository of solutions and test cases for LLM code generation.", "Offers automation-ready APIs for seamless LLM code generation integration."], "tldr": "Existing methods to assess code generation by Large Language Models (LLMs) face issues such as **benchmark contamination**, **data loss**, and **limited accessibility**. The subjective problem difficulty assessments made by human curators also impact the accuracy of model evaluations. These shortcomings hinder the **efficient and accurate assessment** of LLM coding capabilities.\n\nTo counter these issues, **CodeArena** was introduced as an **online evaluation framework**. The platform mitigates score biases using a collective evaluation system, dynamically adjusting model scores based on overall performance.  The system includes automated APIs and ensures open access to both submitted solutions and test cases, streamlining the evaluation workflow. This setup enables a fairer, more accessible, and efficient way to evaluate and improve code generation.", "affiliation": "Nanyang Technological University", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}, "podcast_path": "2503.01295/podcast.wav"}