[{"content": "|       | Micro-F1       | Macro-F1       |\n|-------|-----------------|-----------------|\n| Hr    | 0.714 \u00b1 0.002   | 0.721 \u00b1 0.001   |\n| Ca    | 0.703 \u00b1 0.002   | 0.702 \u00b1 0.001   |\n| Sl    | 0.741 \u00b1 0.000   | 0.748 \u00b1 0.001   |\n| El    | 0.730 \u00b1 0.009   | 0.738 \u00b1 0.009   |\n| Entire Test Set | 0.722 \u00b1 0.002   | 0.731 \u00b1 0.003   |", "caption": "TABLE I: GPT-4o performance on each language in the human-labeled test set in micro-F1 and macro-F1 scores, averaged across three prediction iterations, with standard deviation included.", "description": "This table presents the performance of the GPT-40 model on a human-labeled test set, broken down by language.  The evaluation metrics used are micro-F1 and macro-F1 scores.  The results are averages across three prediction iterations, with standard deviations provided to show variability.", "section": "III. LLM-BASED DATASET DEVELOPMENT"}, {"content": "| Annotators | Krippendorff\u2019s alpha |\n|---|---| \n| 1st ann & 2nd ann | 0.728 |\n| 1st ann & GPT-4o | 0.693 |\n| 2nd ann & GPT-4o | 0.752 |", "caption": "TABLE II: Pair-wise inter-annotator agreement in terms of the nominal Krippendorff\u2019s alpha.", "description": "This table presents the pairwise Krippendorff's alpha values, a measure of inter-rater reliability, for the agreement between different annotators on the news topic classification task.  It compares the agreement between two human annotators, the agreement between the first human annotator and the GPT-40 model, and the agreement between the second human annotator and the GPT-40 model.", "section": "IV. MANUAL ANNOTATION OF TEST DATA"}, {"content": "| Annotator | Krippendorff\u2019s alpha |\n|---|---| \n| GPT-4o | 0.934 |\n| Human annotator (1st ann) | 0.796 |", "caption": "TABLE III: Intra-annotator agreement in terms of the nominal Krippendorff\u2019s alpha for the human annotator (1st annotator) and the LLM teacher model.", "description": "This table presents the intra-annotator agreement scores, calculated using Krippendorff's alpha, for both a human annotator and the GPT-40 language model. Intra-annotator agreement measures the consistency of annotations provided by the same annotator on two separate occasions. The table shows the Krippendorff's alpha coefficient for the human annotator's two annotation rounds and the GPT-40 model's two annotation rounds.  A higher Krippendorff's alpha indicates greater consistency in annotations.", "section": "IV. MANUAL ANNOTATION OF TEST DATA"}, {"content": "|               | Hr Test             | Ca Test            | Sl Test            | El Test            | Entire Test Set     |\n| :------------ | :------------------ | :------------------ | :------------------ | :------------------ | :------------------ |\n| Hr Model      | 0.701 \u00b1 0.017       | **0.672** \u00b1 0.005   | 0.733 \u00b1 0.014       | **0.739** \u00b1 0.011   | 0.716 \u00b1 0.009       |\n| Ca Model      | 0.706 \u00b1 0.012       | **0.671** \u00b1 0.008   | 0.728 \u00b1 0.005       | 0.715 \u00b1 0.014       | 0.710 \u00b1 0.007       |\n| Sl Model      | **0.711** \u00b1 0.007   | 0.660 \u00b1 0.016       | **0.736** \u00b1 0.014   | 0.721 \u00b1 0.013       | 0.713 \u00b1 0.005       |\n| El Model      | 0.674 \u00b1 0.004       | 0.662 \u00b1 0.012       | 0.716 \u00b1 0.011       | **0.706** \u00b1 0.013   | 0.695 \u00b1 0.006       |\n| Multilingual  | 0.707 \u00b1 0.011       | 0.656 \u00b1 0.024       | **0.741** \u00b1 0.007   | 0.729 \u00b1 0.004       | 0.714 \u00b1 0.009       |\n| GPT-4o        | 0.721 \u00b1 0.001       | 0.702 \u00b1 0.001       | 0.748 \u00b1 0.001       | 0.738 \u00b1 0.009       | 0.731 \u00b1 0.003       |", "caption": "TABLE IV: Performance in macro-F1 scores of monolingual and multilingual models trained on 5,000 instances and evaluated on each language in the test split, and on the entire test split. The results of the monolingual training and evaluation are highlighted in gray color. The highest score for each language-specific test set is highlighted in bold. GPT-4o performance is added as the upper limit, as the models were trained on its predictions. The reported scores represent the average results across three iterations of training and evaluation, with the standard deviation provided.", "description": "This table presents the macro-F1 scores achieved by monolingual and multilingual XLM-RoBERTa models trained on 5,000 instances of data.  The models were evaluated on four different languages (Croatian, Catalan, Slovenian, Greek) individually and on the entire combined test set.  Monolingual model results are shaded gray to distinguish them from the multilingual model.  The highest score for each language is bolded. The zero-shot GPT-40 performance is also included as a baseline, representing the upper limit of achievable performance since the GPT-40 model's predictions are used to create the training data.  Standard deviations are provided to reflect the variability across three training/evaluation iterations.", "section": "V. STUDENT MODEL FINE-TUNING EXPERIMENTS"}, {"content": "| Label | Description |\n|---|---| \n| arts, culture, entertainment and media | News about cinema, dance, fashion, hairstyle, jewellery, festivals, literature, music, theatre, TV shows, painting, photography, woodworking, art exhibitions, libraries and museums, language, cultural heritage, news media, radio and television, social media, influencers, and disinformation. |\n| conflict, war and peace | News about terrorism, wars, wars victims, cyber warfare, civil unrest (demonstrations, riots, rebellions), peace talks and other peace activities. |\n| crime, law and justice | News about committed crime and illegal activities, the system of courts, law and law enforcement (e.g., judges, lawyers, trials, punishments of offenders). |\n| disaster, accident and emergency incident | Man-made or natural events resulting in injuries, death or damage, e.g., explosions, transport accidents, famine, drowning, natural disasters, emergency planning and response. |\n| economy, business and finance | News about companies, products and services, any kind of industries, national economy, international trading, banks, (crypto)currency, business and trade societies, economic trends and indicators (inflation, employment statistics, GDP, mortgages, \u2026), international economic institutions, utilities (electricity, heating, waste management, water supply). |\n| education | All aspects of furthering knowledge, formally or informally, including news about schools, curricula, grading, remote learning, teachers and students. |\n| environment | News about climate change, energy saving, sustainability, pollution, population growth, natural resources, forests, mountains, bodies of water, ecosystem, animals, flowers and plants. |\n| health | News about diseases, injuries, mental health problems, health treatments, diets, vaccines, drugs, government health care, hospitals, medical staff, health insurance. |\n| human interest | News about life and behaviour of royalty and celebrities, news about obtaining awards, ceremonies (graduation, wedding, funeral, celebration of launching something), birthdays and anniversaries, and news about silly or stupid human errors. |\n| labour | News about employment, employment legislation, employees and employers, commuting, parental leave, volunteering, wages, social security, labour market, retirement, unemployment, unions. |\n| lifestyle and leisure | News about hobbies, clubs and societies, games, lottery, enthusiasm about food or drinks, car/motorcycle lovers, public holidays, leisure venues (amusement parks, cafes, bars, restaurants, etc.), exercise and fitness, outdoor recreational activities (e.g., fishing, hunting), travel and tourism, mental well-being, parties, maintaining and decorating house and garden. |\n| politics | News about local, regional, national and international exercise of power, including news about election, fundamental rights, government, non-governmental organisations, political crises, non-violent international relations, public employees, government policies. |\n| religion | News about religions, cults, religious conflicts, relations between religion and government, churches, religious holidays and festivals, religious leaders and rituals, and religious texts. |\n| science and technology | News about natural sciences and social sciences, mathematics, technology and engineering, scientific institutions, scientific research, scientific publications and innovation. |\n| society | News about social interactions (e.g., networking), demographic analyses, population census, discrimination, efforts for inclusion and equity, emigration and immigration, communities of people and minorities (LGBTQ, older people, children, indigenous people, etc.), homelessness, poverty, societal problems (addictions, bullying), ethical issues (suicide, euthanasia, sexual behaviour) and social services and charity, relationships (dating, divorce, marriage), family (family planning, adoption, abortion, contraception, pregnancy, parenting). |\n| sport | News about sports that can be executed in competitions, e.g., basketball, football, swimming, athletics, chess, dog racing, diving, golf, gymnastics, martial arts, climbing, etc.; sport achievements, sport events, sport organisation, sport venues (stadiums, gymnasiums, \u2026), referees, coaches, sport clubs, drug use in sport. |\n| weather | News about weather forecasts, weather phenomena and weather warning. |", "caption": "TABLE V: IPTC Media Topic labels and their descriptions, which have been constructed by the authors of this paper.", "description": "This table lists the 17 top-level IPTC Media Topic labels and their descriptions as defined and used by the authors in their study.  Each label is provided with a detailed explanation to clarify its meaning and scope for accurate and consistent annotation.  This is especially important as the study uses automatic annotation methods that rely on the preciseness of the definitions.", "section": "APPENDIX A ANNOTATION GUIDELINES"}, {"content": "| Training Data Size | Epoch Number |\n|---|---| \n| 20,000 | 3 |\n| 15,000 | 5 |\n| 10,000 | 9 |\n| 5,000 | 10 |\n| 2,500 | 22 |\n| 1,000 | 24 |", "caption": "TABLE VI: Number of epochs used for fine-tuning the XLM-RoBERTa model, depending on the size of the training data (number of instances).", "description": "This table shows the optimal number of training epochs for the XLM-RoBERTa model when fine-tuning it for news topic classification using different amounts of training data. The number of epochs is adjusted based on the size of the training dataset to achieve optimal performance.  Smaller datasets require more epochs for convergence.", "section": "V. Student Model Fine-tuning Experiments"}]