{"importance": "This paper introduces **f-distill**, a novel framework to enhance one-step diffusion models by minimizing f-divergences, improving image generation quality and diversity. The novel weighting scheme and comprehensive analysis provide insights and opportunities for future research in generative modeling and diffusion-based techniques. The experiments show new SOTA results in ImageNet64 and MS-COCO.", "summary": "f-distill: One-step diffusion models through f-divergence minimization, outperforming reverse-KL with better mode coverage and lower variance.", "takeaways": ["f-distill framework generalizes distribution matching with f-divergences, offering trade-offs in mode coverage and training variance.", "The gradient of f-divergence is derived as the product of score differences and a density ratio-based weighting function.", "Empirical results show that f-distill outperforms variational score distillation, achieving state-of-the-art one-step generation performance with Jensen-Shannon divergence."], "tldr": "Sampling in diffusion models is slow, limiting real-world use. Recent methods distill multi-step models into one-step generators, matching distributions with reverse Kullback-Leibler (KL) divergence, which is mode-seeking. This can ignore diverse modes. This paper tackles this by introducing a novel f-divergence minimization framework, called f-distill. It covers divergences with different trade-offs in mode coverage and training variance. \n\nThe method derives f-divergence gradients between teacher and student distributions, expressed as score difference and a weighting function based on density ratio. It naturally emphasizes samples with higher teacher density. f-distill outperforms variational score distillation across image tasks using forward-KL and Jensen-Shannon divergences. Jensen-Shannon divergence achieves SOTA one-step generation on ImageNet64 and MS-COCO.", "affiliation": "NVIDIA", "categories": {"main_category": "Machine Learning", "sub_category": "Deep Learning"}, "podcast_path": "2502.15681/podcast.wav"}