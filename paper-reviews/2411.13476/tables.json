[{"content": "| Long-context Continuous Training |  |  |\n|---|---|---|\n| Data | UpSampledMix / SlimPajama128K/ SlimPajama64K |  |\n|  | UpSampledMix-128K: | 58% CC, 20% C4, 7% GitHub, 6% ArXiv, 5% Books, 4% Wiki, 2% StackExchange |\n|  | SlimPajama-128K: | 53% CC, 27% C4, 5% GitHub, 5% ArXiv, 4% Books, 3% Wiki, 3% StackExchange |\n|  | SlimPajama-64K: | 54% CC, 25% C4, 5% ArXiv, 5% GitHub, 4% Books, 3% Wiki, 3% StackExchange |\n| Model | Initialization: | Llama-2-7B / Llama-3-8B / Qwen-1.5-1.8B / Mistral-7B-v0.3 |\n|  | RoPE: | 16K: 1\u00d710\u2076, 64K: 5\u00d710\u2076, 128K: 1\u00d710\u2077 |\n|  | Attention: | Full attention/ Intra-doc attention / Intra-doc attention with Reset |\n|  |  | AnchorAttention / AnchorAttention with Tag |\n| Optim. | AdamW (weight decay = 0.1, \u03b2\u2081=0.9, \u03b2\u2082=0.95) |  |\n|  | LR: | 2e-5  Steps: 2000 steps |\n|  | Batch size: | 8 (0.5M token for 64K, 1M tokens for 128K) |", "caption": "Table 1: The training Configuration.", "description": "This table details the configurations used for training the long-context continuous models.  It includes information on the datasets used (UpSampledMix, SlimPajama-128K, and SlimPajama-64K), specifying their composition from different sources (Common Crawl, C4, GitHub, ArXiv, Books, Wikipedia, and StackExchange).  The table also lists the model initializations, model architectures (LLaMA-2-7B, LLaMA-3-8B, Qwen-1.5-1.8B, and Mistral-7B-v0.3), types of attention mechanisms employed (Full Attention, Intra-doc Attention, Intra-doc Attention with Reset, Anchor Attention, and AnchorAttention with Tag), the optimizer (AdamW) used, learning rate, batch size, and number of steps taken in the training process.", "section": "4 Long-Context Extension Protocol"}, {"content": "| Attention Mechanism | 128K | 64K | 32K | 16K | 8K | 4K |\n|---|---|---|---|---|---|---|\n| **SlimPajama-64K** |  |  |  |  |  |  |\n| Full Attention | \\setminus | 66.40 | 71.78 | 77.63 | 83.86 | 89.84 |\n| Intra-Doc Attention | \\setminus | 69.97 | 74.70 | 79.15 | 83.50 | 89.62 |\n| + Reset | \\setminus | 70.03 | 74.18 | 80.27 | 84.51 | 89.52 |\n| + Interleaved Chunks | \\setminus | 60.59 | 66.52 | 71.70 | 79.70 | 84.71 |\n| **AnchorAttention** | \\setminus | 73.25 | 75.97 | 82.91 | 85.48 | 90.69 |\n| + Tag | \\setminus | 73.88 | 74.21 | 82.46 | 85.13 | 89.93 |\n| + Interleaved Chunks | \\setminus | 66.77 | 69.73 | 77.81 | 85.35 | 89.31 |\n| **SlimPajama-128K** |  |  |  |  |  |  |\n| Full Attention | 62.75 | 70.56 | 71.38 | 81.65 | 83.61 | 88.85 |\n| Intra-Doc Attention | 64.31 | 70.87 | 72.07 | 82.60 | 84.11 | 88.98 |\n| + Reset | 65.75 | 73.34 | 73.30 | 82.82 | 84.43 | 90.01 |\n| + Interleaved Chunks | 53.74 | 61.08 | 65.51 | 75.25 | 80.59 | 82.71 |\n| **AnchorAttention** | 66.15 | 77.69 | 74.28 | 83.67 | 86.41 | 90.60 |\n| + Tag | 65.46 | 74.67 | 75.77 | 83.07 | 84.07 | 89.09 |\n| **UpSampledMix-128K** |  |  |  |  |  |  |\n| Full Attention | 63.70 | 71.45 | 72.69 | 82.57 | 84.55 | 90.08 |\n| Intra-Doc Attention | 63.96 | 74.52 | 76.53 | 82.46 | 86.61 | 90.35 |\n| + Reset | 64.10 | 74.55 | 77.73 | 82.82 | 87.16 | 89.98 |\n| **AnchorAttention** | 65.24 | 76.11 | 79.51 | 86.54 | 87.43 | 90.44 |\n| + Tag | 66.85 | 73.52 | 77.18 | 81.62 | 84.90 | 89.01 |", "caption": "Table 5: Results on 64K and 128K Tokens Datasets. Highest scores across all methods are shown in boldface. Within the Intra-Doc Attention category, the higher scores are underlined. AnchorAttention and its variants, outperforming other methods, are highlighted with a background color.", "description": "This table presents the results of experiments evaluating different attention mechanisms on datasets with 64K and 128K tokens.  The goal was to compare the performance of full attention, intra-document attention (with and without position ID reset), and the proposed AnchorAttention (with and without domain tagging and interleaved chunks).  The metrics used to assess performance aren't explicitly stated in the caption but are presumably related to long-context understanding, as indicated by the dataset sizes.  The table highlights the best-performing method in each scenario.  Bold text indicates the overall best performance for each row, while underlined text denotes the best performance within the 'Intra-Document Attention' category. The AnchorAttention methods and variants, due to their superior performance, are highlighted with a shaded background.", "section": "5.1 AnchorAttention Performance on RULER"}, {"content": "| Attention Mechanism | 128K | 64K | 32K | 16K | 8K | 4K |\n|---|---|---|---|---|---|---|\n| **LLaMA-3-8B** |||| ||| |\n| Full Attention | 34.02 | 61.80 | 72.09 | 79.99 | 82.43 | 83.68 |\n| AnchorAttention | **51.49** | **70.99** | 83.06 | 86.90 | 88.09 | 88.72 |\n| + Tag | 49.67 | 70.37 | **84.14** | **87.13** | **88.36** | **88.97** |\n| **Mistral-7B-v0.3** |||| ||| |\n| Full Attention | 45.64 | 49.05 | 54.49 | 64.06 | 69.99 | 72.80 |\n| AnchorAttention | 47.46 | **61.26** | **68.53** | **73.47** | **76.06** | **78.94** |\n| + Tag | **49.61** | 56.80 | 64.13 | 69.47 | 74.65 | 77.34 |\n| **Qwen-1.5-1.8B** |||| ||| |\n| Full Attention | 33.56 | 41.77 | 47.01 | 56.15 | 61.33 | 67.26 |\n| AnchorAttention | 34.32 | **44.31** | 48.63 | 56.90 | **62.62** | **68.61** |\n| + Tag | **35.84** | 43.91 | **50.70** | **57.39** | 61.96 | 67.41 |", "caption": "Table 6: Attention Mechanism Performance Across Different Models and Token Sizes", "description": "This table presents the performance of different attention mechanisms (Full Attention, AnchorAttention, and AnchorAttention with domain tags) across various model architectures (LLaMA-3-8B, Mistral-7B-v0.3, and Qwen-1.5-1.8B) and different context lengths (4K, 8K, 16K, 32K, 64K, and 128K tokens).  It showcases the impact of AnchorAttention in enhancing long-context performance across diverse models and sequence lengths. The results are reported as scores, likely representing a metric measuring the model's ability to correctly perform tasks given a long context.", "section": "5.3 Cross-Model Evaluation of AnchorAttention for Long-Context Performance"}, {"content": "| Attention Mechanism | LongBench ICL | HellaSwag | MMLU |\n|---|---|---|---|\n| **LLaMA-2-7B** | 6.22 | 71.39 | 46.66 |\n| *SlimPajama-64K* |  |  |  |\n| Full Attention | 62.51 | 68.50 | 33.93 |\n| Intra-Doc Attention | 62.79 | **71.01** | 36.94 |\n|  + *Reset* | 63.76 | 70.12 | 37.92 |\n| AnchorAttention | 65.38 | 70.78 | 40.32 |\n|  + *Tag* | **66.02** | 69.10 | **40.67** |\n| *SlimPajama-128K* |  |  |  |\n| Full Attention | 50.72 | 69.46 | 37.93 |\n| Intra-Doc Attention | 51.22 | 69.93 | 39.49 |\n|  + *Reset* | 50.07 | 69.88 | 37.42 |\n| AnchorAttention | 51.85 | **70.51** | 41.63 |\n|  + *Tag* | **51.89** | 70.37 | **42.85** |\n| *UpSampledMix-128K* |  |  |  |\n| Full Attention | 48.96 | 67.64 | 40.58 |\n| Intra-Doc Attention | 49.51 | 70.86 | 41.27 |\n|  + *Reset* | 50.18 | **70.97** | 40.79 |\n| AnchorAttention | 50.17 | 70.11 | 41.15 |\n|  + *Tag* | **50.70** | 68.97 | **42.03** |", "caption": "Table 7: Results on LongBench ICL, HellaSwag, and MMLU datasets.", "description": "This table presents the performance of different attention mechanisms (Full Attention, Intra-Document Attention with and without Position ID reset, and Anchor Attention with and without domain tagging) on three benchmark datasets: LongBench ICL (In-context learning), HellaSwag (commonsense reasoning), and MMLU (multi-task language understanding).  It shows the performance of models trained on different datasets (SlimPajama-64K, SlimPajama-128K, and UpSampledMix-128K) to assess the effectiveness of the proposed AnchorAttention method in various contexts.", "section": "4.2 Controllable Study with Meaningful Evaluations"}, {"content": "|               | Zigzag-Ring (EasyContext) | Our Impl. (AnchorContext) |\n| ------------- | ------------- | ------------- |\n| Full Attn     | 0.75          | 0             |\n| AnchorAttn    | -             | 0             |", "caption": "Table 8: Our distributed computation achieves zero logits difference over 32K sequence length.", "description": "This table presents the results of a numerical accuracy experiment comparing three different methods for distributed training of long-context models. The methods are: 1. FlashAttention2 (baseline, no distributed training), 2. Zigzag-Ring attention (from EasyContext implementation), and 3. AnchorContext (the authors' proposed method using sequence parallelism with DeepSpeed-Ulysses). The experiment measured the difference in attention logits (model outputs) when processing the same 32K-length sequence on 8 A100 GPUs for each method.  The table shows that the authors' method (AnchorContext) achieved zero difference in logits, demonstrating superior numerical stability compared to the other methods.", "section": "5.5 Infrastructure and Engineering"}, {"content": "|                     | Mixture Ratio | C4      | Arxiv   | Github  | StackExchange | CommonCrawl | Wikipedia | Books   |\n| :------------------ | :------------- | :------- | :------- | :------- | :------------- | :----------- | :-------- | :------- |\n| **128K**  (Rotated)|                 |           |           |           |               |             |           |           |\n| _Up-sampled Data Mixture_ |                 |           |           |           |               |             |           |           |\n|                     | Mixture Ratio | 52.34%   | 1.01%    | 3.68%   | 4.56%          | 33.40%       | 4.79%     | 0.21%    |\n|                     | Token Ratio   | 19.53%   | 5.86%    | 6.61%   | 1.64%          | 58.14%       | 3.51%     | 4.69%    |\n|                     |                 |           |           |           |               |             |           |           |\n| _Original SlimPajama_ |                 |           |           |           |               |             |           |           |\n| **128K** (Rotated) | Mixture Ratio | 55.32%   | 0.30%    | 3.65%   | 5.06%          | 31.01%       | 4.59%     | 0.06%    |\n|                     | Token Ratio   | 26.50%   | 4.64%    | 5.05%   | 3.18%          | 53.42%       | 3.34%     | 3.88%    |\n| **64K**  (Rotated) | Mixture Ratio | 55.05%   | 0.40%    | 3.66%   | 4.97%          | 31.23%       | 4.58%     | 0.10%    |\n|                     | Token Ratio   | 25.43%   | 5.22%    | 5.05%   | 2.95%          | 54.24%       | 3.24%     | 3.86%   |", "caption": "Table 9: Domain and Token Distributions", "description": "This table presents a detailed breakdown of the data distribution across various domains within the training datasets. It compares the original SlimPajama dataset with the upsampled versions used in the experiments, highlighting the mixture ratio (percentage of total sequences from each domain) and the token ratio (percentage of total tokens from each domain). The domains covered are: C4, ArXiv, GitHub, StackExchange, Common Crawl, Wikipedia, and Books.  By examining these ratios, we can understand how the dataset composition varies between the original and upsampled versions, allowing for a better understanding of the impact of data composition on model performance.", "section": "Long-Context Extension Protocol"}, {"content": "| Model | NIAH Single 1 | NIAH Single 2 | NIAH Single 3 | NIAH Multikey 1 | NIAH Multikey 2 | NIAH Multikey 3 | NIAH Multivalue | NIAH Multiquery | VT | CWE | FWE | QA 1 | QA 2 |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| **Llama2 7B** | 100.0 | 100.0 | 99.8 | 97.2 | 87.8 | 44.0 | 99.1 | 99.35 | 59.0 | 24.46 | 91.73 | 61.2 | 43.0 |\n| **+ Chat** | 95.2 | 100.0 | 99.8 | 93.2 | 90.0 | 70.2 | 95.8 | 98.7 | 88.4 | 34.26 | 85.93 | 64.8 | 39.4 |\n| **+ Yarn 64K** | 73.0 | 24.4 | 8.0 | 18.0 | 5.8 | 0.8 | 5.9 | 6.35 | 54.2 | 18.16 | 57.8 | 38.6 | 27.6 |\n| **+ Chat + Yarn 64K** | 67.4 | 48.8 | 32.4 | 30.2 | 16.4 | 4.8 | 48.0 | 34.75 | 54.16 | 43.48 | 82.07 | 41.2 | 25.0 |", "caption": "Table 10: Results of different models across various tasks on 4,00040004,0004 , 000 context length.", "description": "This table presents the performance of different large language models (LLMs) on various tasks within a 4,000-token context window.  The models include the base LLaMA-2-7B model and variations incorporating chat capabilities and different positional encoding methods (Yarn).  Performance is evaluated across several task types, including those focusing on common word extraction (CWE), filtering words (FWE), question answering (QA), and the identification of needles within a haystack (NIAH).  The results demonstrate how different model architectures and enhancements affect performance across various tasks with a restricted context length.", "section": "5.1 AnchorAttention Performance on RULER"}, {"content": "|           | 4,000          | 4,096          |\n|------------|-----------------|-----------------|\n| **LLaMA-2-7B** | 24.46          | 76.8           |", "caption": "Table 11: Performance of LLaMA-2-7B on Common Word Extraction (CWE) with different context lengths.", "description": "This table presents the performance of the LLaMA-2-7B language model on the Common Word Extraction (CWE) task within the RULER benchmark, comparing its accuracy at two different context lengths: 4,000 and 4,096 tokens.  The results illustrate how a slight change in context length significantly impacts the model's performance on this specific task, demonstrating the sensitivity of CWE to variations in the context window size.", "section": "4.2 Controllable Study with Meaningful Evaluations"}, {"content": "|               | Code Completion | ICL | Multi-Doc QA | Single-Doc QA | Summarization | Synthetic |\n| :------------ | :---------------: | :-: | :------------: | :------------: | :------------: | :--------: |\n| *SlimPajama-64K* |                 |     |              |              |              |          |\n| Full Attention |      60.52       |62.51|      9.68      |     17.34      |     16.09      |    2.87    |\n| Cross-Doc Attention |     62.95       |62.79|      9.51      |     16.82      |     16.73      |    2.94    |\n| - reset       |      62.76       |63.76|      9.30      |     16.40      |     14.61      |    3.74    |\n| AnchorAttention |      62.04       |65.38|      9.72      |     18.60      |     17.56      |    4.24    |\n| - tag         |      63.53       |66.02|      9.51      |     18.28      |     15.30      |    5.24    |\n| *SlimPajama-128K* |                 |     |              |              |              |          |\n| Full Attention |      54.17       |50.72|      6.36      |     16.43      |     13.30      |    2.04    |\n| Cross-Doc Attention |     54.59       |51.22|      6.42      |     15.59      |     13.92      |    3.63    |\n| - reset       |      52.51       |50.07|      6.30      |     16.64      |     14.45      |    4.18    |\n| AnchorAttention |      54.14       |51.85|      6.32      |     17.74      |     12.67      |    3.89    |\n| - tag         |      55.81       |51.89|      5.93      |     17.67      |     12.43      |    3.41    |\n| *UpSampledMix-128K* |                 |     |              |              |              |          |\n| Full Attention |      53.13       |48.96|      6.12      |     14.66      |     12.77      |    4.13    |\n| Cross-Doc Attention |     54.16       |49.51|      5.72      |     14.62      |     14.38      |    2.57    |\n| - reset       |      54.29       |50.18|      5.57      |     14.30      |     15.23      |    2.55    |\n| AnchorAttention |      53.90       |50.17|      6.30      |     18.29      |     13.78      |    6.13    |\n| - tag         |      55.13       |49.70|      5.65      |     16.90      |     15.53      |    4.20    |", "caption": "Table 12: Performance Metrics across Different Attention Mechanisms and Datasets.", "description": "This table presents a comprehensive comparison of performance metrics across various attention mechanisms and datasets used in the paper.  It shows the results on the Longbench benchmark, broken down by specific sub-tasks (Code Completion ICL, Multi-Doc QA, Single-Doc QA, Summarization, and Synthetic). The datasets compared are SlimPajama-64K, SlimPajama-128K, and UpSampledMix-128K.  For each dataset and task, the table displays performance scores for different attention methods: Full Attention, Cross-Document Attention, Cross-Document Attention with Position ID Reset, Anchor Attention, and Anchor Attention with Domain Tags. This allows for a detailed analysis of how different attention strategies impact performance across various tasks and dataset configurations.", "section": "5.2 What Works and Doesn't in AnchorAttention: Domain Tagging and Interleaved Chunks"}]