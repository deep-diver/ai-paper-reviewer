[{"figure_path": "https://arxiv.org/html/2411.13476/x1.png", "caption": "Figure 1: Effects of positional shifts on attention computations under different settings.\nLeft: Attention difference D\ud835\udc37Ditalic_D (Eq.\u00a04) plotted against varying positional shift \u03941subscript\u03941\\Delta_{1}roman_\u0394 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT (with \u03942=16subscript\u0394216\\Delta_{2}=16roman_\u0394 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 16 fixed). Pretrained models under BFloat16 (blue line) exhibit significant discrepancies compared to Float32 (yellow line) and random initialization (green line), indicating that the relative positional encoding property of RoPE is broken under BFloat16 and that pretraining amplifies this effect.\nMiddle: Per-token attention differences between \u03941=0subscript\u039410\\Delta_{1}=0roman_\u0394 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 0 and \u03942=16subscript\u0394216\\Delta_{2}=16roman_\u0394 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 16, highlighting the first token accounts for most of the attention difference observed. Right: Attention logit difference (Eq.\u00a05) for the first token as sequence length increases, showing increased discrepancies with longer sequences.", "description": "Figure 1 illustrates the impact of positional shifts on attention mechanisms, specifically focusing on the effects of using BFloat16 precision with Rotary Positional Embeddings (RoPE). The left panel displays the attention difference (D) against varying positional shifts (\u03941), holding \u03942 constant at 16.  It highlights a significant discrepancy between pretrained models using BFloat16 (blue) versus Float32 (yellow) and randomly initialized models (green), demonstrating that BFloat16 breaks RoPE's relative positional encoding, and that this effect is amplified by pretraining. The middle panel shows per-token attention differences between \u03941=0 and \u03942=16, revealing the first token's disproportionate contribution to the observed discrepancies.  The right panel illustrates the attention logit difference for the first token as sequence length increases, indicating that the discrepancies become more pronounced with longer sequences.", "section": "Discrepancies in RoPE's Relative Positional Encoding"}, {"figure_path": "https://arxiv.org/html/2411.13476/x2.png", "caption": "Figure 2: Illustrations of different attention paradigms. Left: Standard intra-document attention. Middle: Our improved version, intra-document attention with position ID reset per document. Right: AnchorAttention incorporating a shared anchor token, \ud835\udc9c\ud835\udc9c\\mathscr{A}script_A.", "description": "Figure 2 illustrates three different attention mechanisms used in long-context training.  The left panel shows standard intra-document attention, where each token attends only to tokens within the same document. The middle panel depicts an improved version of intra-document attention where the positional IDs are reset at the beginning of each document to handle the numerical issues caused by BFloat16 and RoPE. The right panel shows AnchorAttention, a novel method proposed in the paper. In AnchorAttention, the first token of each document serves as a shared anchor token (denoted as \\mathscr{A}) with a fixed position ID, making it visible to all documents within the context window while avoiding unnecessary attention computations between different documents.  This approach maintains semantic coherence and mitigates the numerical instability caused by BFloat16 precision issues.", "section": "3 AnchorAttention"}, {"figure_path": "https://arxiv.org/html/2411.13476/x3.png", "caption": "Figure 3: Resetting position IDs improves performance, contradicting theoretical predictions of RoPE.", "description": "Figure 3 presents the results of an experiment comparing two methods of handling positional IDs in long-context training with BFloat16 precision.  The first method assigns continuous IDs from the beginning of the sequence, while the second resets the ID at the start of each document. The figure shows that resetting positional IDs consistently improves performance, particularly on the RULER benchmark, as the context length increases. This contradicts the theoretical expectations of RoPE, which suggests that relative positional encoding should be maintained regardless of constant positional shifts.  The improved performance with resetting IDs implies there is a deviation in RoPE's relative positional encoding when BFloat16 is used, especially in long sequences.", "section": "3.2 Does the Discrepancy in RoPE under BFloat16 Impact Long-context Performance?"}, {"figure_path": "https://arxiv.org/html/2411.13476/x4.png", "caption": "Figure 4: RULER performance varies during long-context training, we recommend reporting the averaged RULER performance rather than just the final training step.\nPPL remains unchanged after the first several steps, failing to reflect improvements in long-context ability.", "description": "Figure 4 illustrates the fluctuations in RULER (long-context understanding benchmark) performance and perplexity (PPL) scores throughout the long-context training process.  While perplexity shows little change after the initial training steps, the RULER scores demonstrate variability.  This highlights that using only the final training step's RULER score can be misleading, and that an average of RULER scores over multiple checkpoints is recommended for a more accurate representation of model progress in long-context understanding.", "section": "4.2 Controllable Study with Meaningful Evaluations"}, {"figure_path": "https://arxiv.org/html/2411.13476/x5.png", "caption": "Figure 5: Illustrations of domain tagging and interleaved chunks.\nLeft: AnchorAttention with domain tagging, where \ud835\udcaf1subscript\ud835\udcaf1\\mathscr{T}_{1}script_T start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT denotes the domain of document d1subscriptd1\\textbf{{d}}_{1}d start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT.\nMiddle: Intra-document attention with interleaved chunks; documents are split into shuffled, interleaved chunks, preserving the original order within each document.\nRight: AnchorAttention with interleaved chunks.", "description": "Figure 5 illustrates three different attention mechanisms applied to long document sequences. The left panel shows AnchorAttention with domain tagging, where each document is prepended with a tag indicating its source domain (e.g., 'Wikipedia' or 'StackExchange'). This tag is masked during loss calculation, allowing the model to learn domain-specific information while preventing conflicts. The middle panel depicts intra-document attention with interleaved chunks. Here, documents are divided into smaller chunks, these chunks are shuffled randomly while keeping the order within each document intact, creating a mixed sequence. This technique aims to improve long-context learning by exposing the model to various combinations of information segments. The right panel presents AnchorAttention with interleaved chunks. This combines the strategies from the left and middle panels to address both the issue of document domain bias and the long-context challenge of handling long sequences in a single pass.", "section": "What Works and Doesn't in AnchorAttention: Domain Tagging and Interleaved Chunks"}, {"figure_path": "https://arxiv.org/html/2411.13476/x6.png", "caption": "Figure 6: Estimated training time required to process 1 billion tokens at various context lengths using different attention mechanisms. Our AnchorAttention reduce more than 50%percent5050\\%50 % of time needed by Full Attention.", "description": "This figure compares the estimated training time needed to process 1 billion tokens at various context lengths using different attention mechanisms: Full Attention and AnchorAttention.  The results show that AnchorAttention significantly reduces training time compared to Full Attention.  The reduction is more than 50% across all context lengths tested.", "section": "5 Experimental Results"}, {"figure_path": "https://arxiv.org/html/2411.13476/x7.png", "caption": "Figure 7: \nVisualization of attention score differences under BFloat16 for individual samples.", "description": "Figure 7 visualizes the attention score differences observed when using BFloat16 precision for individual samples.  It shows how attention computations deviate from the expected results based on the relative positional properties of Rotary Positional Embedding (RoPE). The plots depict the attention difference (calculated using Equation 4) for multiple samples, revealing the consistency of this deviation.  The left plot varies positional shift \u0394\u2081 while keeping \u0394\u2082 fixed at 16, demonstrating the impact of shift on attention scores. The right plot reverses this, keeping \u0394\u2081 fixed at 0 and varying \u0394\u2082, showing the impact of the second shift parameter.  The figure highlights the discrepancy between the attention computations under BFloat16 and the expected results if the computations were done under higher precision. This visual evidence helps support the paper's claim that the limited precision of BFloat16 leads to deviations in RoPE's relative positional encoding, especially in long sequences.", "section": "Discrepancies in RoPE's Relative Positional Encoding"}, {"figure_path": "https://arxiv.org/html/2411.13476/x8.png", "caption": "Figure 8: Training Data Sequence Length Distribution", "description": "This figure displays the distribution of training data sequence lengths for both the original and upsampled versions of the SlimPajama dataset.  It shows four histograms: one each for the original 64K and 128K token sequences, and one each for the 64K and 128K upsampled sequences.  The histograms visualize the frequency with which different lengths of sequences appear in the dataset. By comparing the original and upsampled distributions, one can observe the effects of upsampling on the distribution of sequence lengths.  Specifically, it highlights the increased proportion of longer sequences in the upsampled data compared to the original dataset. This is because the upsampling method aims to increase the number of longer sequences to better train the model to handle long contexts.", "section": "Long-Context Extension Protocol"}]