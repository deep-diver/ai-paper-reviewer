{"references": [{"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-01", "reason": "This paper introduced the Transformer architecture, the foundation upon which many modern large language models are built, including those studied in this paper."}, {"fullname_first_author": "Jianlin Su", "paper_title": "Roformer: Enhanced transformer with rotary position embedding", "publication_date": "2021-04-01", "reason": "This paper introduced Rotary Position Embedding (RoPE), a crucial positional encoding technique for long-context training that is the focus of the current research."}, {"fullname_first_author": "Howard Yen", "paper_title": "Helmet: How to evaluate long-context language models effectively and thoroughly", "publication_date": "2024-10-01", "reason": "This paper provides a comprehensive benchmark for evaluating the long-context capabilities of LLMs, crucial for assessing the effectiveness of the proposed improvements."}, {"fullname_first_author": "Cheng-Ping Hsieh", "paper_title": "Ruler: What's the real context size of your long-context language models?", "publication_date": "2024-04-01", "reason": "This paper introduces the RULER benchmark, used extensively in this paper to evaluate long-context performance, providing a crucial metric for assessing the efficacy of proposed methods."}, {"fullname_first_author": "Yao Fu", "paper_title": "Data engineering for scaling language models to 128k context", "publication_date": "2024-02-01", "reason": "This paper explores long-context training strategies for LLMs, which directly relates to the challenges addressed by the AnchorAttention method proposed in the current research."}]}