[{"figure_path": "https://arxiv.org/html/2501.02260/x1.png", "caption": "Figure 1: MagicFace takes in the AU changes based on the input portrait and edit the portrait to exhibit different expressions. The edited image respects the AU condition and preserve identity, pose, background as well as other facial details.", "description": "Figure 1 showcases the capabilities of the MagicFace model in editing facial expressions.  It demonstrates how the model takes an input portrait and a set of Action Unit (AU) changes.  These AUs represent specific changes to facial muscle movements, allowing for fine-grained control over the expression. The model then produces an edited image that accurately reflects the specified AU changes while maintaining the identity, pose, background, and fine details of the original portrait.  This highlights MagicFace's ability to create realistic and controlled facial expression edits.", "section": "IV. EXPERIMENTS"}, {"figure_path": "https://arxiv.org/html/2501.02260/x2.png", "caption": "Figure 2: A display showcasing various action units and their corresponding intensity scales. Only a set of commonly used AUs are displayed here. For a complete collection of AUs with descriptions see [15].", "description": "Figure 2 shows a chart illustrating different facial action units (AUs) and their intensity levels.  Each AU represents a specific muscle movement in the face, and the intensity scale (A-E) indicates the strength of that movement, ranging from minimal (A) to maximal (E). The figure only displays some commonly used AUs; for a complete list and detailed descriptions, please refer to reference [15] in the paper.", "section": "II. RELATED WORKS"}, {"figure_path": "https://arxiv.org/html/2501.02260/x3.png", "caption": "Figure 3: Overview of MagicFace. During training, a pair of images with the same identity but different pose, background as well as expressions is used respectively as the identity image and the target. AU variations are computed by an estimator and then sent into the denoising UNet as AU condition. Pose and background of the target is parsed into an image condition independently, dealed with an Attribute Controller and then inputted to the denoising UNet. ID encoder takes in the encoded identity image to edit for target AUs, where features in each transformer blocks are merged into the corresponding ones of the denoising UNet via self-attention. During inference, the conditional image will be parsed from the identity image.", "description": "This figure illustrates the MagicFace architecture and workflow.  During training, it uses pairs of images of the same person but with varying poses, backgrounds, and expressions. One image serves as the identity reference, and the other as the target for expression editing.  An AU (Action Unit) estimator calculates the differences in AU intensities between the identity and target images. These AU variations are fed, along with processed pose and background information (from an Attribute Controller), into a denoising UNet. A dedicated ID (Identity) encoder processes the identity image, using self-attention to integrate its features into the UNet, maintaining identity consistency.  At inference time, the model parses pose and background from a single input image, enabling expression editing while preserving identity and other attributes.", "section": "III. METHOD"}, {"figure_path": "https://arxiv.org/html/2501.02260/x4.png", "caption": "Figure 4: Qualitative comparison for facial expression editing. Other methods exhibit shortcomings in preserving the identity, pose or background, and they are unable to support continuous control over the intensity of generated expressions, whereas our method excels in maintaining exceptional detail features and meanwhile allows flexible, fine-grained control to the expression intensity. Please zoom in for more detailed observation.", "description": "Figure 4 presents a qualitative comparison of various facial expression editing methods.  The results highlight the superior performance of the proposed MagicFace model. While other methods struggle to maintain consistent identity, pose, and background details across different expression levels, MagicFace excels in preserving these details while allowing for highly precise and flexible control over the intensity of generated expressions.  The figure encourages closer inspection to fully appreciate the fine details of the generated expressions.", "section": "IV. EXPERIMENTS"}, {"figure_path": "https://arxiv.org/html/2501.02260/x5.png", "caption": "Figure 5: Results of using laboratory dataset for training. In lab setting, the model cannot generalize to image from natural settings (the second row).", "description": "This figure compares the performance of a facial expression editing model trained on a laboratory dataset versus a model trained on a more diverse, real-world dataset. The first row shows results from images belonging to the laboratory dataset.  The second row shows the model's performance on images from natural settings. The comparison highlights the model's limited ability to generalize when trained solely on controlled, lab-based data.  The lack of generalization to real-world scenarios is a key finding of the study, emphasizing the limitations of using solely controlled datasets for training.", "section": "IV. EXPERIMENTS"}, {"figure_path": "https://arxiv.org/html/2501.02260/x6.png", "caption": "Figure 6: Ablation study for different model architectures. Zoom in to view facial details.", "description": "Figure 6 presents an ablation study comparing MagicFace's performance against alternative model architectures.  The study explores four variations: using ControlNet alone with the addition of the Attribute Controller, training only the denoising UNet portion, replacing the identity encoder with a simple convolutional layer, and substituting a CLIP image encoder for the identity encoder.  The results, shown as a series of facial image edits for different AU (Action Unit) combinations for each model architecture, visually demonstrate the impact of each design choice on the ability to maintain identity, accuracy of AU expression, background consistency and pose. This ablation helps to highlight the contribution of each component in MagicFace to achieving high-fidelity facial expression editing.", "section": "IV. EXPERIMENTS"}, {"figure_path": "https://arxiv.org/html/2501.02260/x7.png", "caption": "Figure 7: Demonstration of out-of-domain testing for extreme expressions as well as unseen styles. Some AUs are out of the range [0, 5]. The left column displays the editing results of real person photos with extreme expressions and the right column displays that of the cartoon characters.", "description": "Figure 7 demonstrates the model's ability to generalize to out-of-distribution data.  The left side shows results of editing real human faces with extreme facial expressions (AU values beyond the typical 0-5 range). The right side shows the same capability applied to cartoon characters, which are visually distinct from the training data. This showcases the model's robustness and adaptability beyond the scope of its training set.", "section": "IV. EXPERIMENTS"}, {"figure_path": "https://arxiv.org/html/2501.02260/x8.png", "caption": "Figure 8: Qualitative comparison of images edited by MagicFace trained without/with AU dropout and using different values of the guidance scale \u03b1\ud835\udefc\\alphaitalic_\u03b1. The AU variation to edit is AU4[-6]. Please zoom in for more details.", "description": "Figure 8 displays a comparison of facial images edited using the MagicFace model.  The experiment investigates the effect of AU dropout (a technique to randomly drop action unit information during training) and the guidance scale (\u03b1) on the quality of generated images. The target AU (Action Unit) modified in all examples is AU4, with a decrease in intensity (-6). The top row shows the input image, while subsequent rows present the results obtained with the AU dropout disabled and different values of \u03b1 (ranging from 0.5 to 13.5). The images demonstrate that using AU dropout and adjusting the value of \u03b1 allows for a finer level of control over the resulting facial expression and overall image quality.  The best quality results are produced with a guidance scale \u03b1 around 3.0.  It shows how different parameter settings affect the image synthesis results and facial expressions.", "section": "IV. Experiments"}, {"figure_path": "https://arxiv.org/html/2501.02260/x9.png", "caption": "Figure 9: Visualization of self-attention maps (lst row) and cross-attention maps (2nd row) from the denoising UNet. Please zoom in to observe more details.", "description": "Figure 9 visualizes the attention weights within the self-attention and cross-attention mechanisms of the denoising UNet used in MagicFace.  The top row displays self-attention maps, illustrating how the model focuses on different parts of the input image at various stages of the denoising process. The bottom row shows cross-attention maps, highlighting the model's attention to the AU variation condition (input AU values) and its interaction with image features.  The figure emphasizes the dynamic shift of attention from a more holistic view to a more focused attention on facial regions related to the specified AU changes as the denoising progresses. Zooming in reveals more detailed changes in attention weight distributions.", "section": "IV. Experiments"}]