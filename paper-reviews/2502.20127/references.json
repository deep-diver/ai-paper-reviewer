{"references": [{"fullname_first_author": "Hugo Touvron", "paper_title": "Llama: Open and efficient foundation language models", "publication_date": "2023-02-24", "reason": "Llama is a foundational open-source large language model, essential as a base model for fine-tuning and comparison in issue-resolving tasks."}, {"fullname_first_author": "John Schulman", "paper_title": "Proximal policy optimization algorithms", "publication_date": "2017-07-20", "reason": "PPO is a widely used reinforcement learning algorithm for fine-tuning language models, especially for tasks like issue resolving."}, {"fullname_first_author": "Jason Wei", "paper_title": "Chain-of-thought prompting elicits reasoning in large language models", "publication_date": "2022-01-01", "reason": "Chain-of-thought prompting is a method to elicit better reasoning from LLMs that is used to improve their performance on complex tasks."}, {"fullname_first_author": "Daya Guo", "paper_title": "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning", "publication_date": "2025-01-28", "reason": "DeepSeek-R1 explores rule-based reinforcement learning to enhance model performance in complex tasks, similar to the approach in this paper."}, {"fullname_first_author": "Carlos E Jimenez", "paper_title": "SWE-bench: Can language models resolve real-world github issues?", "publication_date": "2024-01-01", "reason": "SWE-bench is a crucial benchmark for evaluating language models' ability to resolve real-world software issues, providing a standardized testbed for the proposed approach."}]}