{"references": [{"fullname_first_author": "OpenAI", "paper_title": "GPT-4 Technical Report", "publication_date": "2023", "reason": "GPT-4 is a leading large language model, and this report details its capabilities, which are relevant to the advancements in code generation discussed in the paper."}, {"fullname_first_author": "Jason Wei", "paper_title": "Finetuned Language Models Are Zero-Shot Learners", "publication_date": "2021", "reason": "This paper introduced the concept of instruction tuning, a key technique used in the paper's approach to improving code generation models."}, {"fullname_first_author": "Qihao Zhu", "paper_title": "Deepseek-coder-v2: Breaking the barrier of closed-source models in code intelligence", "publication_date": "2024", "reason": "DeepSeek-coder is a strong baseline model for code generation used in the paper's experiments, and this paper details its architecture and performance."}, {"fullname_first_author": "Yuxiang Wei", "paper_title": "Magicoder: Empowering code generation with oss-instruct", "publication_date": "2024", "reason": "This paper introduces Magicoder, another important baseline model for code generation, whose performance is compared against the proposed method."}, {"fullname_first_author": "Zhaojian Yu", "paper_title": "Wavecoder: Widespread and versatile enhanced instruction tuning with refined data generation", "publication_date": "2023", "reason": "This paper presents Wavecoder, a method for enhancing instruction tuning that is similar to the approach proposed in the paper, which makes it an important comparative work."}]}