[{"figure_path": "https://arxiv.org/html/2501.00192/x1.png", "caption": "(a) Challenge 1: Image safety judgment based on subjective rules is a difficult task. Even humans struggle to determine whether this image is suitable for public viewing or not. The MLLM model used here is GPT-4o\u00a0(gpt, ).", "description": "The figure demonstrates the challenges of using large language models (LLMs) for image safety assessment.  Specifically, it highlights the subjectivity of safety rules, even for human judges.  The example shows an image where it is difficult to determine whether the content is suitable for public viewing, underscoring the complexity of automated content moderation with LLMs and the potential for disagreements even among humans. The LLM used for this example was GPT-4.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2501.00192/x2.png", "caption": "(b) Challenge 2: Current MLLMs struggle to reason with complex, lengthy safety rules. The rule applies to imminent death scenarios, this image clearly does not depict one. The model used here is LLaVA-OneVision-Qwen2-72b-ov-chat\u00a0(Li et\u00a0al., 2024).", "description": "This figure demonstrates a challenge in using large language models (LLMs) for image safety assessment.  A complex safety rule, focusing on images depicting imminent death, is presented to the LLaVA-OneVision-Qwen2-72b-ov-chat model. Despite the image clearly not showing a scene of imminent death, the LLM struggles to correctly assess the image's safety, highlighting difficulties in applying nuanced and detailed rules.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2501.00192/x3.png", "caption": "(c) Challenge 3: MLLMs have inherent biases. Despite the absence of a throat slit, the MLLM predicts a rule violation due to its bias, linking blood on the ground, foreleg, and feet to a throat slit. Model here is InternVL2-8B-AWQ\u00a0(Chen et\u00a0al., 2023).", "description": "This figure demonstrates the inherent biases in large language models (LLMs) when used for image safety assessment.  Even when an image doesn't explicitly show a throat slit (violating a specific safety rule), the LLM (InternVL2-8B-AWQ) might incorrectly flag it as unsafe. This is because the model identifies bloodstains on the ground, leg, and feet, associating them with a throat slit due to existing biases in its training data.  This highlights a challenge in directly using LLMs for safety assessments without addressing their inherent biases.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2501.00192/x4.png", "caption": "Figure 1: Examples showing the challenges for simply querying pre-trained MLLMs for zero-shot image safety judgment.", "description": "This figure illustrates three key challenges in using pre-trained, large multimodal language models (MLLMs) for zero-shot image safety assessment.  The task is to determine if an image violates predefined safety rules without fine-tuning the MLLM.  The challenges shown are: 1) Subjectivity of safety rules; humans disagree on the appropriateness of an image. 2) Difficulty of MLLMs reasoning with complex, multi-clause rules; the model struggles to correctly apply a detailed safety policy. 3) Inherent biases in MLLMs; the model incorrectly labels a safe image due to bias.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2501.00192/x5.png", "caption": "Figure 2: Example of the preconditions extracted from the rule.", "description": "This figure illustrates how complex safety rules are broken down into simpler, more manageable \"precondition chains.\"  The example shows a rule prohibiting images depicting people or animals with bloody injuries indicating imminent death.  This rule is decomposed into three preconditions: 1) people or animals are visible; 2) visible bloody injuries exist; and 3) injuries appear to cause imminent death. Each precondition is evaluated separately, creating a more straightforward process for the MLLM to assess image safety.", "section": "3.3 Precondition Extraction"}, {"figure_path": "https://arxiv.org/html/2501.00192/x6.png", "caption": "Figure 3: Process of calculating token based score. The precondition is considered satisfied if the score is larger than a threshold.", "description": "This figure illustrates the process of calculating a token-based score to determine whether a precondition is met.  The process involves querying a pre-trained Large Language Model (LLM) with a Yes/No question about the precondition's fulfillment, given an image. The LLM provides probabilities for both \"Yes\" and \"No\" responses.  A precondition is considered satisfied if the probability of \"Yes\" divided by the sum of the probabilities of \"Yes\" and \"No\" exceeds a predefined threshold.", "section": "3.4 Debiased Token Probability based Judgment"}, {"figure_path": "https://arxiv.org/html/2501.00192/x7.png", "caption": "Figure 4: \nApproach for mitigating the bias from the non-centric content in the image.\nWe compare the token probability based score (see Figure\u00a03) of the original image and the image with centric region removed, and consider the image satisfy the precondition if the difference of the score is large enough.", "description": "This figure illustrates a method to reduce bias stemming from non-centric image regions.  The approach involves comparing the token probability-based scores (as described in Figure 3) for two versions of the image: the original and one with the central region removed.  A significant difference in scores between the two versions indicates that the bias from the non-centric region is substantial and that the image satisfies the precondition.", "section": "3.4 Debiased Token Probability based Judgment"}, {"figure_path": "https://arxiv.org/html/2501.00192/x8.png", "caption": "Figure 5: Process of cascaded reasoning-based judgment.", "description": "This figure illustrates the cascaded reasoning process used when the initial token probability-based judgment lacks confidence.  It shows how the model first attempts a chain-of-thought reasoning process to determine whether a precondition is met. If this process isn't conclusive enough, it then proceeds to ask the model for a concise summary of its reasoning and prediction in JSON format for better accuracy.", "section": "3.5 Reasoning-based Judgment"}, {"figure_path": "https://arxiv.org/html/2501.00192/x9.png", "caption": "(a) Score distribution of InternVL2-76B on the precondition \u201cThe visible, bloody injuries indicate imminent death based on the severity of the injury\u201d.", "description": "This figure shows the distribution of scores generated by the InternVL2-76B language model for a specific precondition: \"The visible, bloody injuries indicate imminent death based on the severity of the injury.\"  The x-axis represents the score, ranging from 0 to 1, indicating the model's confidence that the precondition is met. The y-axis shows the percentage of instances receiving each score. The figure likely illustrates the model's tendency to assign certain scores more frequently than others, possibly due to biases in the training data or the inherent ambiguity of the precondition itself.  It helps demonstrate the challenges of directly using large language models for image safety assessment without additional processing or mitigation strategies.", "section": "4.3 Effectiveness of Different Components"}, {"figure_path": "https://arxiv.org/html/2501.00192/x10.png", "caption": "(b) Score distribution of InternVL2-26B-AWQ on the precondition \u201cLegs of people in swimwear or underwear are spread apart by an angle exceeding 90 degrees\u201d.", "description": "This figure shows the distribution of scores generated by the InternVL2-26B-AWQ language model when evaluating whether a precondition is met. The precondition in question is: \"Legs of people in swimwear or underwear are spread apart by an angle exceeding 90 degrees\".  The x-axis represents the score, ranging from 0 to 1, where a higher score indicates a higher probability that the precondition is met. The y-axis represents the percentage of preconditions that received a particular score. The chart visualizes the distribution of scores, allowing for an analysis of the model's confidence in determining whether the given precondition is satisfied for different images.  This helps assess the model's performance and identify potential areas for improvement.", "section": "4.3 Effectiveness of Different Components"}, {"figure_path": "https://arxiv.org/html/2501.00192/x11.png", "caption": "(c) Score distribution of Qwen2-VL-72B-Instruct on the precondition \u201cThe visible, bloody injuries indicate imminent death based on the severity of the injury\u201d.", "description": "This figure shows the distribution of scores generated by the Qwen2-VL-72B-Instruct model when evaluating the precondition \"The visible, bloody injuries indicate imminent death based on the severity of the injury.\"  The x-axis represents the score, ranging from 0 to 1, where higher scores indicate a greater likelihood that the precondition is met. The y-axis shows the percentage of images that received a particular score.  The distribution is broken down into two categories: 'Satisfied' (precondition is met) and 'Not Satisfied' (precondition is not met). The figure helps illustrate how well the model can objectively assess the precondition based solely on the visual information in the images.", "section": "4.3 Effectiveness of Different Components"}, {"figure_path": "https://arxiv.org/html/2501.00192/x12.png", "caption": "Figure 6: Score distributions across different models under different preconditions. We show the score distributions for queries containing images with ground-truth label \u201cSatisfied the precondition\u201d and \u201cNot Satisfied the precondition\u201d. Additionally, we illustrate the precondition scores without incorporating image tokens, i.e., \u2133\u2062(None,\ud835\udc84)\u2133None\ud835\udc84\\mathcal{M}(\\text{None},\\bm{c})caligraphic_M ( None , bold_italic_c ) in section\u00a03.4.", "description": "Figure 6 presents a comparative analysis of score distributions across three different large language models (LLMs) under varying preconditions.  For each LLM, two sets of scores are shown: one for images that actually satisfy the precondition, and another for images that do not.  Crucially, the figure also displays scores calculated *without* incorporating image tokens into the model's input. This 'no image' condition helps highlight the impact of image features on the model's output and the magnitude of any biases or language priors influencing the score. Each distribution is displayed as a histogram, showing the frequency of different score values.  The x-axis represents the LLM's score (from 0 to 1), and the y-axis represents the frequency or percentage of instances with a particular score.", "section": "3.4 Debiased Token Probability based Judgment"}, {"figure_path": "https://arxiv.org/html/2501.00192/x13.png", "caption": "(a) Recall for ground truth rules.", "description": "This figure shows the recall of the relevance scanning module at different cosine similarity thresholds.  The relevance scanning module pre-filters rules, removing those deemed irrelevant to the image before further processing. The graph plots the recall (proportion of actually relevant rules correctly identified) against the cosine similarity threshold used for filtering. A higher threshold indicates stricter filtering, potentially missing more relevant rules but improving efficiency. The results demonstrate that the module effectively identifies most relevant rules while significantly reducing the number of rules that need subsequent processing.", "section": "3.2 Relevance Scanning"}, {"figure_path": "https://arxiv.org/html/2501.00192/x14.png", "caption": "(b) Fraction of remaining rules.", "description": "The figure shows the fraction of safety rules remaining after applying a relevance scanning module to filter out rules irrelevant to the input image.  The x-axis represents the cosine similarity threshold used for filtering, and the y-axis shows the percentage of rules retained after filtering.  The graph illustrates the effectiveness of the module in reducing the number of rules processed while maintaining a high recall for ground-truth violated rules.  A lower percentage of remaining rules indicates higher efficiency, as fewer rules need to be evaluated by the main model.", "section": "3.2 Relevance Scanning"}, {"figure_path": "https://arxiv.org/html/2501.00192/x15.png", "caption": "Figure 7: Detailed performance of Relevance Scanning module (see Section\u00a03.2) with CLIP\u00a0(Radford et\u00a0al., 2021) on OS Bench. This module effectively filters out a significant proportion of irrelevant rules for the inspected images, while successfully retaining most of the ground-truth violated rules for forwarding to the next phase.", "description": "This figure demonstrates the effectiveness of the Relevance Scanning module (Section 3.2) in filtering out irrelevant rules before processing images for safety assessment.  Using CLIP (Radford et al., 2021), the module achieves high recall (successfully keeping most of the ground-truth violated rules) while significantly reducing the number of rules to be processed in the next stage. The graph shows the trade-off between recall and the proportion of rules remaining after filtering, demonstrating the module's ability to efficiently pre-process the rules.", "section": "3.2 Relevance Scanning"}, {"figure_path": "https://arxiv.org/html/2501.00192/x16.png", "caption": "Figure 8: Distribution of score differences calculated using our image-level debiasing approach (see Figure\u00a04).", "description": "This figure visualizes the distribution of score differences obtained by comparing the token probability scores of whole images and their corresponding centric-region-removed versions.  The image-level debiasing approach, detailed in Figure 4, aims to mitigate bias stemming from non-centric image content.  The distribution reveals how effectively this approach distinguishes between situations where the presence of non-centric content significantly influences the score (large difference) versus those where it has minimal impact (small difference). The x-axis represents the calculated score difference, and the y-axis shows the percentage of images falling within each score difference range.", "section": "4.3 Effectiveness of Different Components"}, {"figure_path": "https://arxiv.org/html/2501.00192/x17.png", "caption": "Figure 9: Prompt for measuring rule objectivenessb based on the template in Zheng et\u00a0al. (2024).", "description": "This figure shows the prompt template used to assess the objectivity of the safety guidelines.  The prompt instructs an evaluator to act as an impartial judge, providing a short explanation of the guideline's objectivity before rating its objectivity on a scale of 1 to 10 (10 being the most objective). This helps ensure that the rules used for image safety judgment are clear, unbiased, and easily understood by the MLLM.", "section": "A.1 Details for Constitution Objectification"}, {"figure_path": "https://arxiv.org/html/2501.00192/extracted/6103213/fig/llava-onevision-qwen2-72b-ov-chat_policy.png", "caption": "Figure 10: Detailed process for precondition extraction and central object word extraction.", "description": "This figure illustrates the two-stage process used to prepare data for the image safety judgment task.  The first stage involves extracting preconditions from safety rules using a large language model (LLM). The provided examples show how complex safety rules are broken down into simpler, more manageable preconditions, making them easier for the MLLM to process in the subsequent judgment stage. The second stage focuses on extracting central object words from preconditions. This involves using the LLM again to identify the key objects or parts of objects described in the preconditions, ensuring efficient processing during image analysis.  This helps to focus the model's attention on the most relevant aspects of the image for safety determination, improving accuracy and efficiency.", "section": "3 Method"}, {"figure_path": "https://arxiv.org/html/2501.00192/extracted/6103213/fig/llava-onevision-qwen2-72b-ov-chat_precondition.png", "caption": "(a) Recall for ground truth rules.", "description": "This figure shows the recall of the relevance scanning module at different cosine similarity thresholds.  The relevance scanning module filters out irrelevant rules before passing relevant ones to the MLLM for processing. A higher recall indicates that the module successfully retains most of the ground-truth violated rules, ensuring that important safety rules are not missed. The x-axis represents the cosine similarity threshold, and the y-axis represents the recall of ground-truth violated rules.", "section": "3.2 Relevance Scanning"}]