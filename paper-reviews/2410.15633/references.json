{"references": [{" publication_date": "2023", "fullname_first_author": "Anthropic", "paper_title": "Introducing claude 2.1", "reason": "This paper introduces Claude 2.1, a large language model used in the LongAlign dataset synthesis, which is a critical component of the study's experimental setup.  Understanding its capabilities is crucial for interpreting the results and evaluating the proposed GATEAU framework's impact on LLM performance.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Yushi Bai", "paper_title": "Longbench: A bilingual, multitask benchmark for long context understanding", "reason": "LongBench is a core benchmark used to evaluate the performance of the models trained using GATEAU. The evaluation results on LongBench provide substantial evidence of GATEAU's effectiveness in improving long context understanding and instruction-following capabilities.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Yushi Bai", "paper_title": "Longalign: A recipe for long context alignment of large language models", "reason": "This paper introduces LongAlign, the primary dataset used in the experiments to train and evaluate LLMs.  LongAlign's synthesis method and characteristics are fundamental to the research, as the GATEAU framework is specifically designed to improve the quality of data selection within LongAlign.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Yihan Cao", "paper_title": "Instruction mining: Instruction data selection for tuning large language models", "reason": "This paper explores instruction mining techniques for data selection, which is related to the core focus of GATEAU.  Comparing GATEAU's approach with the methods described in this paper offers valuable insights into the relative strengths and weaknesses of different data selection strategies in the context of long-context alignment.", "section_number": 2}, {" publication_date": "2023a", "fullname_first_author": "Lichang Chen", "paper_title": "Alpagasus: Training a better alpaca with fewer data", "reason": "This paper focuses on efficient data selection for fine-tuning LLMs, a problem directly relevant to the work presented.  Comparison with Alpagasus demonstrates GATEAU's distinct approach to address long-context alignment challenges and contributes to a broader understanding of data selection strategies for LLMs.", "section_number": 2}, {" publication_date": "2024a", "fullname_first_author": "Longze Chen", "paper_title": "Long context is not long at all: A prospector of long-dependency data for large language models", "reason": "This paper is highly relevant as it directly addresses the challenges of modeling long-range dependencies in long contexts, a core challenge tackled by GATEAU. Understanding the approaches and findings in this paper helps clarify the innovation and contribution of GATEAU.", "section_number": 2}, {" publication_date": "2023b", "fullname_first_author": "Shouyuan Chen", "paper_title": "Extending context window of large language models via positional interpolation", "reason": "This paper addresses the expansion of context windows in LLMs, which is a related but distinct challenge from long-context alignment. Comparing this approach with GATEAU provides context on how the focus on data selection and long-range dependency modeling differentiates GATEAU's approach from other methods aiming to improve long context handling.", "section_number": 1}, {" publication_date": "2024b", "fullname_first_author": "Yukang Chen", "paper_title": "LongloRA: Efficient fine-tuning of long-context large language models", "reason": "This work explores efficient fine-tuning techniques for long-context LLMs.  It serves as a relevant comparison point to GATEAU's approach, highlighting the distinct focus of GATEAU on data selection and long-range dependency modeling, rather than on the efficiency of fine-tuning itself.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Wei-Lin Chiang", "paper_title": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality", "reason": "Vicuna, being an open-source chatbot model, offers a comparative perspective to the LLMs evaluated using GATEAU.  Understanding Vicuna's performance helps contextualize the results obtained using GATEAU and the significance of improvements in instruction-following and long-context understanding.", "section_number": 4}, {" publication_date": "2022", "fullname_first_author": "Zhengxiao Du", "paper_title": "Glm: General language model pretraining with autoregressive blank infilling", "reason": "This paper details GLM, a general language model.  Understanding the architecture and training of GLM helps contextualize the evaluation of GATEAU's approach in improving long-context understanding, as the framework operates within the landscape of general language models.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Yuan Ge", "paper_title": "Clustering and ranking: Diversity-preserved instruction selection through expert-aligned quality estimation", "reason": "This paper introduces a data selection method, CaR, which is used as a baseline for comparison in the GATEAU experiments.  This comparison is crucial for establishing the relative merits and improvements introduced by GATEAU.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Gkamradt", "paper_title": "Llmtest_needleinahaystack", "reason": "This paper introduces the 'Needle in a Haystack' test, which is included as one of the evaluation metrics in the experimental setup. This test directly assesses the long-context understanding capability of the LLMs, providing critical insights into the effectiveness of GATEAU in improving this crucial aspect of LLM performance.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Chi Han", "paper_title": "LM-infinite: Zero-shot extreme length generalization for large language models", "reason": "This paper is relevant because it explores zero-shot generalization in large language models for long texts. Comparing it to GATEAU's approach helps clarify the contributions and limitations of different approaches to handling long contexts, particularly emphasizing the difference between enhancing model capabilities and focusing on superior dataset creation.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Cheng-Yu Hsieh", "paper_title": "Found in the middle: Calibrating positional attention bias improves long context utilization", "reason": "This paper discusses improvements in long-context utilization by addressing positional attention bias.  This provides a complementary perspective to GATEAU's approach, which focuses on data selection to improve alignment rather than directly modifying attention mechanisms within the model architecture.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Dacheng Li", "paper_title": "How long can context length of open-source LLMs truly promise?", "reason": "This paper discusses context length limits in open-source LLMs, which is relevant to the study because it highlights limitations in existing long-context approaches. The contrast with GATEAU's focus on high-quality data selection through HMG and CAM is crucial for understanding its contribution to the field.", "section_number": 2}, {" publication_date": "2024a", "fullname_first_author": "Dongyuan Li", "paper_title": "A survey on deep active learning: Recent advances and new frontiers", "reason": "This paper, focusing on active learning, is relevant as a conceptual foundation for the data selection methods within the GATEAU framework. Active learning strategies are often applied in selecting the most informative samples for training; GATEAU applies this concept to the specific domain of long-context alignment, showcasing its innovative approach.", "section_number": 2}, {" publication_date": "2024b", "fullname_first_author": "Ming Li", "paper_title": "From quantity to quality: Boosting LLM performance with self-guided data selection for instruction tuning", "reason": "This paper explores the impact of data quality in instruction tuning, a crucial aspect directly related to GATEAU's core contribution. Comparing GATEAU's data selection strategy with the self-guided method helps analyze the effectiveness of the proposed approach and the relative strengths of different data selection methods.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Yunshui Li", "paper_title": "One shot learning as instruction data prospector for large language models", "reason": "This paper explores instruction data selection strategies in the context of one-shot learning, offering a comparative approach to data selection for LLM fine-tuning. This comparison helps demonstrate GATEAU's specific focus on addressing the unique challenges of long-context alignment through HMG and CAM.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Wei Liu", "paper_title": "What makes good data for alignment?: A comprehensive study of automatic data selection in instruction tuning", "reason": "This paper focuses on what makes good data for instruction tuning and the evaluation of various methods for automated data selection.  The findings and comparison methods presented offer valuable context for evaluating the effectiveness and novelty of GATEAU's data selection strategy.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Arka Pal", "paper_title": "Giraffe: Adventures in expanding context lengths in llms", "reason": "This paper explores expanding context lengths in LLMs, a related but distinct area from long-context alignment.  The insights from Giraffe help to understand the broader landscape of improving long-context capabilities and the complementary nature of GATEAU's approach to improve data quality and model alignment through effective data selection.", "section_number": 1}]}