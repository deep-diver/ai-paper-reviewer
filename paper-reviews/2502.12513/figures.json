[{"figure_path": "https://arxiv.org/html/2502.12513/x1.png", "caption": "Figure 1: Multimodal interleaved documents are unsuitable for vision-language representation learning. We construct distinct image-text pairs from such documents via retrieval and generation.", "description": "This figure illustrates the challenge of using multimodal interleaved documents for vision-language representation learning.  These documents contain images and text intermixed without explicit pairings, making it difficult to directly use them for training models that learn relationships between images and text. The figure shows a proposed solution involving a pipeline. First, a data extraction step is performed to isolate image and text content. A hierarchical retrieval method then links each image with semantically relevant text segments. Finally, to improve the quality and amount of training data, an image semantic augmented generation module produces synthetic texts, ensuring that each image has multiple associated texts. This process generates distinct image-text pairs suitable for training vision-language models.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2502.12513/x2.png", "caption": "Figure 2: The Real-World Data Extraction pipeline to extract high-quality images and texts from interleaved image-text documents.", "description": "This figure illustrates the Real-World Data Extraction pipeline, a multi-stage process designed to extract high-quality images and text from a large corpus of interleaved image-text documents.  The pipeline begins with the extraction of images and text segments from the original documents.  Then, it proceeds through several filtering steps to remove low-quality, redundant, and semantically irrelevant data. Image filtering includes steps based on aspect ratio, resolution, and perceptual/semantic redundancy.  Sentence filtering similarly includes criteria related to length, complexity, the presence of URLs or emojis, and information entropy to ensure high-quality, meaningful text.  The result of this pipeline is a refined dataset comprising high-quality images and text ready for further use in training vision-language models.", "section": "3 RealSyn Dataset"}, {"figure_path": "https://arxiv.org/html/2502.12513/x3.png", "caption": "Figure 3: The architecture of our proposed framework, which constructs distinct image-text pairs from real-world data extracted from interleaved documents via retrieval and generation.", "description": "This figure illustrates the framework used to generate image-text pairs from real-world, interleaved documents.  The process begins with extracting images and texts from the documents, followed by text semantic clustering to group similar texts. A hierarchical retrieval method then efficiently associates images with semantically relevant texts from various clusters. Finally, an image semantic augmented generation module produces synthetic text captions, enhancing image understanding. This entire framework produces high-quality image-text pairs suitable for vision-language model training.", "section": "3 RealSyn Dataset"}, {"figure_path": "https://arxiv.org/html/2502.12513/x4.png", "caption": "Figure 4: A T-SNE\u00a0Van\u00a0der Maaten and Hinton (2008) projection of LDA\u00a0Blei et\u00a0al. (2003) topic cluster from a randomly selected 1M samples from RealSyn.", "description": "This figure visualizes the distribution of topics within the RealSyn dataset using t-SNE dimensionality reduction on LDA topic modeling results.  A random subset of 1 million samples from the RealSyn dataset was used for this analysis. The visualization helps understand the diversity of topics covered in the dataset. Each point represents a sample, and the clusters show groupings of semantically similar samples.", "section": "Analysis"}, {"figure_path": "https://arxiv.org/html/2502.12513/extracted/6212411/Figures/data_richness/diversity_analysis.png", "caption": "(a) Richness assessment comparison", "description": "This figure compares the richness of various datasets in terms of image-text similarity and token distribution.  It shows that the RealSyn dataset, especially when including both realistic and synthetic texts, has a richer textual context and more diverse vocabulary than datasets solely based on realistic web data, thereby enriching its potential for vision-language representation learning.", "section": "3 RealSyn Dataset"}, {"figure_path": "https://arxiv.org/html/2502.12513/extracted/6212411/Figures/model_scaling/model_scaling_combined.png", "caption": "(b) Diversity assessment comparison", "description": "This figure compares the diversity of different datasets by showing the number of unique entities present in their captions.  A higher number of unique entities indicates greater diversity in the data.  The figure visually demonstrates that the RealSyn dataset has substantially more diverse captions compared to the LAION and YFCC datasets, as represented by the significantly higher count of unique tokens present in its captions. This diversity is beneficial for training robust and generalizable vision-language models.", "section": "3.3 Semantic Balance Sampling"}, {"figure_path": "https://arxiv.org/html/2502.12513/extracted/6212411/Figures/image_captioning/coco2017_name.png", "caption": "Figure 5: The richness assessment and diversity assessment on different datasets. RealSyn-R1: the most relevant retrieved realistic text. RealSyn-S1: the semantic augmented synthetic text based on RealSyn-R1.", "description": "Figure 5 presents a comparison of the richness and diversity of four datasets: YFCC15M, LAION, RealSyn-R1 (using only the most relevant realistic text retrieved for each image), and RealSyn-S1 (incorporating both the most relevant realistic text and its semantically augmented synthetic counterpart).  The richness is assessed by calculating the cosine similarity between image and text embeddings, and the distribution of token counts in the text captions.  The diversity is analyzed using the number of unique entities mentioned in the captions.  The figure shows that RealSyn, especially when using both realistic and synthetic texts, exhibits higher richness and diversity compared to the other datasets.", "section": "3.3 Semantic Balance Sampling"}, {"figure_path": "https://arxiv.org/html/2502.12513/extracted/6212411/Figures/image_captioning/flickr30K_name.png", "caption": "Figure 6: Model scaling capability. We compare the models pre-trained on LAION30M and RealSyn30M.", "description": "Figure 6 presents a comparison of model scaling capabilities between two large-scale image-text datasets: LAION30M and RealSyn30M.  It demonstrates how performance varies across different model sizes (ViT-B/32, ViT-B/16, ViT-L/14) when pre-trained on each dataset. The results are shown across three key metrics: linear probe performance on downstream tasks, zero-shot transfer learning performance, and robustness. This visualization allows assessing which dataset is more effective at improving model scaling and performance on downstream tasks.", "section": "4 Experiments and Results"}, {"figure_path": "https://arxiv.org/html/2502.12513/extracted/6212411/Figures/concept_balance/Image_concept_balance_resize.png", "caption": "Figure 7: Image captioning comparisons on COCO2017 and Flickr30k. B4, MT., RL. and Cd. represent the metric of BLEU, METEOR, ROUGE-L, and Cider.", "description": "Figure 7 presents a comparison of image captioning performance on the COCO2017 and Flickr30k datasets.  The results are shown for different models, with performance evaluated using four metrics: BLEU (B4), METEOR (MT.), ROUGE-L (RL.), and CIDEr (Cd.).  This allows for a comprehensive assessment of caption quality across various models and datasets.", "section": "4.2 Main Results"}, {"figure_path": "https://arxiv.org/html/2502.12513/x5.png", "caption": "Figure 8: Clustering distribution of 15M data obtained from random sampling and semantic balanced sampling.", "description": "Figure 8 presents a comparative analysis of data distribution obtained from two sampling methods: random sampling and semantic balanced sampling.  The x-axis represents the cluster number, and the y-axis denotes the count of samples within each cluster.  Two distinct curves illustrate the distribution of 15 million data points from each sampling technique. The plot visually demonstrates how semantic balanced sampling, in contrast to random sampling, leads to a more uniform and balanced distribution of data across clusters, mitigating potential biases towards certain clusters and improving the model's ability to learn from diverse data.", "section": "Ablation on Semantic Balance Sampling"}, {"figure_path": "https://arxiv.org/html/2502.12513/extracted/6212411/Figures/data_scaling_law.png", "caption": "Figure 9: Visualization of the raw interleaved document, the retrieved realistic text, and synthetic text. Image semantic-related information is highlighted in green.", "description": "Figure 9 visualizes the transformation of an interleaved document into image-text pairs.  It shows the original, unstructured document alongside extracted high-quality images.  The figure then highlights how a hierarchical retrieval system identifies multiple semantically relevant realistic texts associated with each image.  Finally, it displays synthetic texts generated to augment the image's semantic information.  These synthetic texts are created using a large language model and aim to provide fine-grained details that may be missing from the original text. The image semantic-related information in both the realistic and synthetic texts is highlighted in green, demonstrating how this information is preserved and enriched throughout the process.", "section": "3 RealSyn Dataset"}, {"figure_path": "https://arxiv.org/html/2502.12513/x6.png", "caption": "Figure 10: Data Scaling Analysis. Pre-training ViT-B/32 on RealSyn\u00a0in different data scales.", "description": "This figure illustrates the performance scaling of the Vision Transformer (ViT-B/32) model when pre-trained on the RealSyn dataset at various scales.  It visually represents how the model's performance changes as the amount of training data increases. This allows for an evaluation of the dataset's scalability and efficiency, showing whether performance gains plateau or continue to improve with larger datasets.", "section": "Experiments and Results"}, {"figure_path": "https://arxiv.org/html/2502.12513/x7.png", "caption": "Figure 11: Visualization of image-text pairs in our proposed RealSyn\u00a0dataset. Trksubscriptsuperscript\ud835\udc47\ud835\udc58\ud835\udc5fT^{k}_{r}italic_T start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT: the k\ud835\udc58kitalic_k-th retrieved realistic text. Tsksubscriptsuperscript\ud835\udc47\ud835\udc58\ud835\udc60T^{k}_{s}italic_T start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT: the image semantic augmented synthetic text for Trksubscriptsuperscript\ud835\udc47\ud835\udc58\ud835\udc5fT^{k}_{r}italic_T start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT. Image semantic-related information is highlighted in green.", "description": "This figure visualizes examples from the RealSyn dataset, showcasing the integration of realistic and synthetic image-text pairs.  Each row displays a raw image, followed by multiple realistic text descriptions (T<sup>k</sup><sub>r</sub>) retrieved from the original interleaved documents.  These are then complemented by corresponding synthetic texts (T<sup>k</sup><sub>s</sub>) generated to enhance fine-grained semantic details using the model and the original realistic texts. The generated synthetic texts aim to improve the representation learning by adding semantic richness and diversity. Green highlights indicate portions of the synthetic text directly related to image semantics.", "section": "3 RealSyn Dataset"}]