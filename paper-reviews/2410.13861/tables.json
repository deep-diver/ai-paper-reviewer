[{"figure_path": "2410.13861/tables/table_7_0.html", "caption": "Table 1: Image decoding evaluation using image encoder and decoder on the ImageNet validation set. PSNR and LPIPS measure the difference between reconstructed and ground truth images. PSNRd and LPIPSd measure the difference between two separate reconstructions of the same image, reflecting decoding diversity.", "description": "Table 1 presents an evaluation of image decoding performance across different models and feature granularities, using PSNR and LPIPS to measure reconstruction quality and PSNRd and LPIPSd to assess reconstruction diversity.", "section": "4.2 MULTI-GRANULAR VISUAL DECODING"}, {"figure_path": "2410.13861/tables/table_8_0.html", "caption": "Table 2: Diverse text-to-image generation evaluation on MSCOCO 30K validation set. CLIP-I and CLIP-T measure the similarity between generated images and ground truth images or prompts. LPIPSd quantifies the difference between two images generated from the same prompt, reflecting generation diversity. 5-scale Max denotes selecting the image with the highest score among the 5 outputs and computes the average maximum value.", "description": "Table 2 presents a quantitative comparison of diverse text-to-image generation performance across various models, evaluating consistency (CLIP-I, CLIP-T) and diversity (LPIPSd) of generated images.", "section": "4.3 DIVERSE TEXT-TO-IMAGE GENERATION"}, {"figure_path": "2410.13861/tables/table_9_0.html", "caption": "Table 3: Image editing evaluation on Emu-edit test benchmark (Sheynin et al., 2024). 5-scale Max denotes selecting the image with the highest score among the 5 outputs and computes the average maximum value.", "description": "Table 3 presents a quantitative evaluation of PUMA's image editing capabilities against other state-of-the-art models using CLIP-I, CLIP-T, and DINO scores, indicating its performance relative to existing methods.", "section": "4.4 IMAGE EDITING"}, {"figure_path": "2410.13861/tables/table_10_0.html", "caption": "Table 4: Evaluation on multimodal understanding benchmarks. PUMA utilizes CLIP-Large encoder with 224 \u00d7 224 input. Und. and Gen. denote \u201cunderstanding\u201d and \u201cgeneration\u201d, respectively.", "description": "Table 4 presents the quantitative comparison of PUMA against other state-of-the-art models on several multimodal understanding benchmarks.", "section": "4 Experiments"}, {"figure_path": "2410.13861/tables/table_13_0.html", "caption": "Table 1: Image decoding evaluation using image encoder and decoder on the ImageNet validation set. PSNR and LPIPS measure the difference between reconstructed and ground truth images. PSNRd and LPIPSd measure the difference between two separate reconstructions of the same image, reflecting decoding diversity.", "description": "Table 1 presents the quantitative results of image decoding evaluation using PSNR, LPIPS, PSNRd and LPIPSd on the ImageNet validation set, comparing different models and their decoding diversity.", "section": "4.2 MULTI-GRANULAR VISUAL DECODING"}, {"figure_path": "2410.13861/tables/table_17_0.html", "caption": "Table 5: Ablation of different visual token input on image understanding. The experiments are conducted on LLaVA-v1.5 setting with CLIP-Large-224 visual encoder.", "description": "Table 5 shows the ablation study of different visual token inputs on image understanding performance using LLaVA-v1.5 setting with CLIP-Large-224 visual encoder.", "section": "4.7 ABLATION"}, {"figure_path": "2410.13861/tables/table_18_0.html", "caption": "Table 6: CLIP-I and CLIP-T scores on MSCOCO 30K validation set with different feature scales.", "description": "This table presents CLIP-I and CLIP-T scores on the MSCOCO 30K validation set, comparing the performance of PUMA's text-to-image generation across five different feature scales (f4 to fo).", "section": "4.3 DIVERSE TEXT-TO-IMAGE GENERATION"}]