[{"figure_path": "2410.13861/tables/table_7_0.html", "caption": "Table 1: Image decoding evaluation using image encoder and decoder on the ImageNet validation set. PSNR and LPIPS measure the difference between reconstructed and ground truth images. PSNRd and LPIPSd measure the difference between two separate reconstructions of the same image, reflecting decoding diversity.", "description": "Table 1 presents an evaluation of image decoding performance using various models, measured by PSNR, LPIPS, PSNRd, and LPIPSd on the ImageNet validation set.", "section": "4.2 MULTI-GRANULAR VISUAL DECODING"}, {"figure_path": "2410.13861/tables/table_8_0.html", "caption": "Table 2: Diverse text-to-image generation evaluation on MSCOCO 30K validation set. CLIP-I and CLIP-T measure the similarity between generated images and ground truth images or prompts. LPIPSd quantifies the difference between two images generated from the same prompt, reflecting generation diversity. 5-scale Max denotes selecting the image with the highest score among the 5 outputs and computes the average maximum value.", "description": "Table 2 presents a quantitative evaluation of diverse text-to-image generation performance on the MSCOCO 30K validation set, measuring the similarity between generated and ground truth images and prompts, as well as the diversity of generated images.", "section": "4.3 DIVERSE TEXT-TO-IMAGE GENERATION"}, {"figure_path": "2410.13861/tables/table_9_0.html", "caption": "Table 3: Image editing evaluation on Emu-edit test benchmark (Sheynin et al., 2024). 5-scale Max denotes selecting the image with the highest score among the 5 outputs and computes the average maximum value.", "description": "Table 3 presents a quantitative evaluation of image editing performance using CLIP-I, CLIP-T, and DINO scores, comparing PUMA's performance to several existing methods.", "section": "4.4 Image Editing"}, {"figure_path": "2410.13861/tables/table_10_0.html", "caption": "Table 4: Evaluation on multimodal understanding benchmarks. PUMA utilizes CLIP-Large encoder with 224 \u00d7 224 input. Und. and Gen. denote \u201cunderstanding\u201d and \u201cgeneration\u201d, respectively.", "description": "Table 4 presents a comparison of PUMA's performance on several multimodal understanding benchmarks against other state-of-the-art models, highlighting its capabilities in both understanding and generation tasks.", "section": "4 EXPERIMENTS"}, {"figure_path": "2410.13861/tables/table_13_0.html", "caption": "Table 1: Image decoding evaluation using image encoder and decoder on the ImageNet validation set. PSNR and LPIPS measure the difference between reconstructed and ground truth images. PSNRd and LPIPSd measure the difference between two separate reconstructions of the same image, reflecting decoding diversity.", "description": "Table 1 presents quantitative metrics evaluating the performance of different models on ImageNet image reconstruction task, assessing both reconstruction accuracy (PSNR, LPIPS) and diversity (PSNRd, LPIPSd).", "section": "4.2 MULTI-GRANULAR VISUAL DECODING"}, {"figure_path": "2410.13861/tables/table_17_0.html", "caption": "Table 5: Ablation of different visual token input on image understanding. The experiments are conducted on LLaVA-v1.5 setting with CLIP-Large-224 visual encoder.", "description": "Table 5 shows the ablation study of different visual token inputs on image understanding performance using various metrics.", "section": "4.7 ABLATION"}, {"figure_path": "2410.13861/tables/table_18_0.html", "caption": "Table 6: CLIP-I and CLIP-T scores on MSCOCO 30K validation set with different feature scales.", "description": "Table 6 shows the CLIP-I and CLIP-T scores on the MSCOCO 30K validation set for different feature scales used in text-to-image generation, indicating the impact of feature granularity on image quality and consistency.", "section": "4.3 DIVERSE TEXT-TO-IMAGE GENERATION"}]