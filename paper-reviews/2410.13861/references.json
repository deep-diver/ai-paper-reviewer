{"references": [{" publication_date": "2021", "fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "reason": "This paper introduces CLIP, a model that learns visual representations from natural language supervision. CLIP's ability to associate images and text makes it a crucial foundation for multimodal learning tasks such as those addressed by PUMA, which utilizes CLIP for visual feature extraction.  The success and influence of CLIP in the field significantly elevates its importance within this work.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Christoph Schuhmann", "paper_title": "Laion-5b: An open large-scale dataset for training next generation image-text models", "reason": "Laion-5B is one of the largest datasets used in PUMA's training.  The sheer scale and breadth of its multimodal data (images and text) significantly contribute to PUMA's ability to process and generate diverse visual content effectively. Its size and openness have significantly impacted the field of MLLM development.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Dustin Podell", "paper_title": "SDXL: Improving latent diffusion models for high-resolution image synthesis", "reason": "SDXL is a high-quality diffusion model chosen for the image decoders in PUMA.  The selection of SDXL highlights the significance of high-resolution image generation capabilities and the importance of pre-trained models as a foundation for advanced MLLM visual components. Its powerful image generation capabilities are central to the proposed method.", "section_number": 3}, {" publication_date": "2020", "fullname_first_author": "Leo Gao", "paper_title": "The pile: An 800gb dataset of diverse text for language modeling", "reason": "The Pile dataset contributes to PUMA's comprehensive pretraining phase.  The diversity and scale of The Pile, containing a vast collection of textual data, greatly enhance the language modeling capabilities of the PUMA MLLM and allow it to better integrate with visual features.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Yuying Ge", "paper_title": "SEED-X: Multimodal models with unified multi-granularity comprehension and generation", "reason": "SEED-X is a related work that attempts to address the multi-granularity challenge of multimodal tasks using conditional image input to the decoder. While not a direct solution as PUMA, it demonstrates the recognition of the problem that PUMA directly addresses and improves upon.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Runpei Dong", "paper_title": "DreamLLM: Synergistic multimodal comprehension and creation", "reason": "DreamLLM is mentioned as a relevant prior work in the visual generation space of MLLMs.  It addresses some of the challenges related to visual generation, specifically the end-to-end approach which is important to contrast with the training strategy of PUMA.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Yuying Ge", "paper_title": "SEED-data-edit technical report: A hybrid dataset for instructional image editing", "reason": "This dataset is key to the instruction-tuning phase of PUMA\u2019s development.  Focusing specifically on image editing, this dataset significantly contributes to PUMA's proficiency in precise image manipulation. The data's specialized nature strengthens PUMA's ability to perform image editing tasks.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Jinze Bai", "paper_title": "Qwen-VL: A frontier large vision-language model with versatile abilities", "reason": "Qwen-VL is a strong competitor and benchmark for multimodal understanding. Its inclusion as a comparison model in PUMA highlights the performance bar in the field and shows where PUMA stands amongst the top models. The broad capabilities of Qwen-VL place it as a major reference point.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Chaoyou Fu", "paper_title": "MME: A comprehensive evaluation benchmark for multimodal large language models", "reason": "MME is a comprehensive evaluation benchmark for multimodal large language models. Its significance to PUMA's evaluation lies in providing a standardized assessment across various aspects of image understanding and generation, which helps establish PUMA's position within the field.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Can Qin", "paper_title": "Unicontrol: A unified diffusion model for controllable visual generation in the wild", "reason": "Unicontrol is mentioned as a relevant prior work in the field of unified diffusion models for visual generation. Its relevance to PUMA is in how both address the controllable generation aspect of visual content creation, and by comparing PUMA's approach to the Unicontrol method, PUMA highlights its advancement in this area.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Bo Li", "paper_title": "LLaVA-onevision: Easy visual task transfer", "reason": "LLaVA-OneVision is a benchmark dataset used for the evaluation of PUMA\u2019s image understanding capabilities.  It provides a diverse range of vision-language tasks, and its usage validates PUMA's performance against other multimodal models in various vision-language tasks.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Haotian Liu", "paper_title": "LLaVA: Open and efficient foundation language models", "reason": "LLaVA is a key reference point in multimodal understanding.  Its strong performance in multimodal understanding establishes a comparison point for PUMA's evaluation. Comparing to LLaVA shows PUMA's performance in various tasks, especially in visual content generation.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Wenliang Dai", "paper_title": "Instructblip: Towards general-purpose vision-language models with instruction tuning", "reason": "InstructBLIP is mentioned for its focus on instruction tuning and general-purpose vision-language models.   The inclusion of InstructBLIP as a related work reinforces the significance of instruction tuning for enhancing the versatility of MLLMs in visual tasks.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Shelly Sheynin", "paper_title": "EMU-edit: Precise image editing via recognition and generation tasks", "reason": "EMU-Edit provides the benchmark dataset used for evaluating PUMA's image editing capabilities. The performance on this benchmark is critical in showcasing PUMA's ability to perform fine-grained image manipulations, and its inclusion demonstrates the state-of-the-art in image editing within the field of MLLMs.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Yuying Ge", "paper_title": "Seed-llama: Making llama see and draw with seed tokenizer", "reason": "SEED-LLaMA is a pioneering work in the area of unified visual generation in MLLMs, and its comparison to PUMA highlights the advancements made in multi-granular visual representation. By comparing to SEED-LLaMA, PUMA showcases its improvement in both generation quality and handling various granularity demands of different image generation tasks.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Quan Sun", "paper_title": "Emu: Multimodal models with unified multi-granularity comprehension and generation", "reason": "Emu is a key related work that addresses the multimodal understanding and generation tasks with the focus on visual aspects.  Comparing Emu and PUMA highlights the strengths of PUMA's multi-granularity approach in balancing diversity and controllability in image generation tasks.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Shengqiong Wu", "paper_title": "Next-GPT: Any-to-any multimodal llm", "reason": "Next-GPT is a strong competitor model that focuses on any-to-any multimodal capabilities.   Its inclusion highlights the trend toward increasingly versatile MLLMs and provides a relevant benchmark for comparison, showing PUMA's strengths and limitations within the broader context of multimodal model development.", "section_number": 4}, {" publication_date": "2019", "fullname_first_author": "Anand Mishra", "paper_title": "Ocr-vqa: Visual question answering by reading text in images", "reason": "OCR-VQA is a dataset used for the evaluation of PUMA's image understanding capabilities. This dataset is important because its inclusion in the evaluation showcases PUMA's capacity to perform well in complex vision-language tasks that require text extraction and understanding from images. The dataset is particularly useful in showcasing the model's ability to incorporate visual and textual information.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Keyu Tian", "paper_title": "Visual autoregressive modeling: Scalable image generation via next-scale prediction", "reason": "This paper presents VAR, a model that utilizes hierarchical autoregressive with discrete tokens for image generation. This is an important related work because it tackles image generation by using an autoregressive model, similar to PUMA.  This is a significant comparison in the architecture space and highlights some of PUMA\u2019s architectural choices, such as using diffusion based decoders instead of discrete tokens.", "section_number": 2}, {" publication_date": "2018", "fullname_first_author": "Richard Zhang", "paper_title": "The unreasonable effectiveness of deep features as a perceptual metric", "reason": "This paper introduces LPIPS, a perceptual image quality metric used in evaluating PUMA\u2019s performance. The selection of LPIPS is crucial for evaluating image generation quality, as the perceptual similarity is a more effective and meaningful comparison in assessing image quality than traditional metrics such as PSNR.", "section_number": 4}]}