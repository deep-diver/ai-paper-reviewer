{"references": [{" publication_date": "2023", "fullname_first_author": "Hugo Touvron", "paper_title": "LLaMA: Open and efficient foundation language models", "reason": "LLaMA serves as the foundational language model for PUMA, providing the text processing capabilities. Its efficiency and open nature are crucial for the feasibility and reproducibility of PUMA's research.", "section_number": 4}, {" publication_date": "2021", "fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "reason": "CLIP is the backbone of PUMA's multi-granular visual feature extraction.  Its ability to learn transferable visual representations from natural language supervision is fundamental to PUMA's multi-modal approach. ", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Dustin Podell", "paper_title": "SDXL: Improving latent diffusion models for high-resolution image synthesis", "reason": "SDXL provides the pre-trained diffusion model foundation upon which the PUMA image decoders are built.  Fine-tuning these pre-trained models allows efficient and high-quality image generation and reconstruction at various scales within PUMA.", "section_number": 3}, {" publication_date": "2022", "fullname_first_author": "Christoph Schuhmann", "paper_title": "Laion-5B: An open large-scale dataset for training next generation image-text models", "reason": "Laion-5B is a crucial dataset for pre-training PUMA. Its large scale and open nature provide a broad foundation for learning multimodal representations.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Laura Jannes Burger", "paper_title": "Laion: Image data, AI, and dispossession", "reason": "Laion-Aesthetics, a dataset crucial for training PUMA's aesthetics generation capabilities, originates from this work.  Its focus on aesthetics is vital for improving image quality.", "section_number": 3}, {" publication_date": "2024a", "fullname_first_author": "Keqiang Sun", "paper_title": "JourneyDB: A benchmark for generative image understanding", "reason": "JourneyDB is used for evaluating PUMA's high-quality text-to-image generation capabilities.  It provides a standardized and rigorous benchmark for comparing image quality and diversity.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Yuying Ge", "paper_title": "SEED-LLaMA: Making LLAMA see and draw with SEED tokenizer", "reason": "SEED-LLaMA's approach to incorporating image generation into LLMs is relevant, highlighting the state-of-the-art before PUMA's proposed advancements.", "section_number": 2}, {" publication_date": "2024b", "fullname_first_author": "Yuying Ge", "paper_title": "SEED-X: Multimodal models with unified multi-granularity comprehension and generation", "reason": "SEED-X addresses the multi-granularity issue, but indirectly.  It highlights the importance of this problem and the need for more unified approaches like PUMA's.", "section_number": 2}, {" publication_date": "2024a", "fullname_first_author": "Yuying Ge", "paper_title": "SEED-data-edit technical report: A hybrid dataset for instructional image editing", "reason": "This paper provides valuable insights into training datasets for image editing and manipulation tasks which is a significant contribution to PUMA.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Jinze Bai", "paper_title": "Qwen-VL: A frontier large vision-language model with versatile abilities", "reason": "Qwen-VL is compared against PUMA in the experiments, thus highlighting the current state-of-the-art.  Its evaluation provides context for understanding PUMA's overall performance.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Jing Yu Koh", "paper_title": "Generating images with multimodal language models", "reason": "This work is closely related in its attempt to unify image generation and multimodal understanding with LLMs. It is important because it represents prior research and shows the relevance of PUMA's improvements.", "section_number": 2}, {" publication_date": "2024a", "fullname_first_author": "Bo Li", "paper_title": "LLaVA-OneVision: Easy visual task transfer", "reason": "LLaVA-OneVision serves as one of the datasets for evaluating PUMA's image understanding capabilities.  Its wide range of tasks provides a holistic perspective on PUMA's multimodal understanding performance.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Runpei Dong", "paper_title": "DreamLLM: Synergistic multimodal comprehension and creation", "reason": "DreamLLM represents an important attempt to create a unified framework for multimodal understanding and generation.  The authors compare their approach to PUMA.", "section_number": 2}, {" publication_date": "2024a", "fullname_first_author": "Shelly Sheynin", "paper_title": "EMU-Edit: Precise image editing via recognition and generation tasks", "reason": "EMU-Edit provides a benchmark for image editing tasks. Comparing PUMA's performance to EMU-Edit is essential for establishing PUMA's efficacy in fine-grained image manipulation.", "section_number": 4}, {" publication_date": "2022", "fullname_first_author": "Aditya Ramesh", "paper_title": "Hierarchical text-conditional image generation with CLIP latents", "reason": "This paper explores the application of diffusion models to image generation, a technique also utilized by PUMA's decoders.  Understanding the latest advances in diffusion models helps evaluate PUMA's effectiveness in image generation.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Chaoyou Fu", "paper_title": "MMBench: A comprehensive evaluation benchmark for multimodal large language models", "reason": "MMBench serves as one of the benchmarks used to test PUMA's capabilities in multimodal understanding tasks. Its comprehensiveness makes it a valuable tool for comparing various LLMs.", "section_number": 4}, {" publication_date": "2024b", "fullname_first_author": "Quan Sun", "paper_title": "Generative multimodal models are in-context learners", "reason": "This paper highlights the recent advancements in generative multimodal models and their ability to learn in-context. It's important because it emphasizes the ongoing research and development in this field and positions PUMA in its context. ", "section_number": 2}, {" publication_date": "2018", "fullname_first_author": "Richard Zhang", "paper_title": "The unreasonable effectiveness of deep features as a perceptual metric", "reason": "LPIPS, a key metric used in evaluating image quality in the experiments, originates from this work. Understanding the basis for this metric is crucial for interpreting the experimental results.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Zhiliang Peng", "paper_title": "Kosmos-2: Grounding multimodal large language models to the world", "reason": "Kosmos-2's approach is related to PUMA's goal of grounding multimodal models in the real world.  It establishes the wider context of research efforts aimed at creating more robust and versatile multimodal LLMs.", "section_number": 4}, {" publication_date": "2024b", "fullname_first_author": "Yanwei Li", "paper_title": "LLaVA-OneVision: Easy visual task transfer", "reason": "LLaVA's unified multimodal approach serves as a baseline and point of comparison for PUMA.  This allows for evaluating PUMA's relative strengths and weaknesses in comparison to existing state-of-the-art models.", "section_number": 2}]}