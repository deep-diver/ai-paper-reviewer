[{"figure_path": "https://arxiv.org/html/2501.10799/x1.png", "caption": "Figure 1: \\method Training Process.\nGiven a dataset of math problems (left), a language model (LLM) produces both reasoning steps and a final answer.\nEach intermediate reasoning step is evaluated by a process reward model (Process RM), and the final answer is assessed by an outcome reward model (Outcome RM).\nThe binary feedback signals from both levels (outcome-level correctness cosuperscript\ud835\udc50\ud835\udc5cc^{o}italic_c start_POSTSUPERSCRIPT italic_o end_POSTSUPERSCRIPT and stepwise correctness chssubscriptsuperscript\ud835\udc50\ud835\udc60\u210ec^{s}_{h}italic_c start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT) are recorded together with the input (x)\ud835\udc65(x)( italic_x ) and the model\u2019s response (y)\ud835\udc66(y)( italic_y ) \u00a72.1.\nThese signals are then used to compute the \\method loss, guiding the LLM to not only produce correct final answers but also maintain coherent and correct reasoning steps \u00a72.3.\nThrough multiple iterations of this training process \u00a72.4, the model progressively improves both its stepwise reasoning and final answer accuracy.", "description": "The figure illustrates the training process of the STEP-KTO method.  A math problem is input to a large language model (LLM), which generates a series of reasoning steps and a final answer.  The intermediate steps and the final answer are then evaluated by separate reward models: a Process Reward Model (PRM) and an Outcome Reward Model (ORM), respectively.  Each model provides binary feedback (correct or incorrect). This binary feedback, along with the input and the LLM's output, is used to compute a loss function that guides the LLM's training. The iterative process refines the model's ability to produce both correct answers and coherent reasoning steps.", "section": "2 Methodology"}]