[{"content": "| Technique | Mean Avg. Pearson Corr. (mA\u03c1) - Pascal VOC | Mean Avg. Pearson Corr. (mA\u03c1) - COCO | Avg. Runtime/image (s) - Pascal VOC | Avg. Runtime/image (s) - COCO | Model Type |\n|---|---|---|---|---|---| \n| Depth Anything | 0.273 | 0.125 | 0.020 | 0.029 | Depth Prediction |\n| DPT-Large | 0.283 | 0.129 | 0.046 | 0.050 | Depth Prediction |\n| Itti-Koch Model | 0.280 | 0.130 | 0.030 | 0.065 | Saliency Prediction |\n| DeepGaze IIE | 0.459 | 0.170 | 0.042 | 0.084 | Saliency Prediction |\n| Average | 0.324 | 0.139 | 0.035 | 0.057 | N/A |", "caption": "TABLE I: Evaluation results of various Depth and Saliency Prediction techniques on the Pascal VOC and COCO datasets with the respective metrics and their performance. The best-performing results are denoted in bold.", "description": "Table I presents a comprehensive comparison of four different visual prediction models (two depth prediction models and two saliency prediction models) evaluated on two benchmark object detection datasets, Pascal VOC and COCO.  The evaluation metrics include the Mean Average Pearson Correlation (mAp), reflecting the overall correlation between the model predictions and ground truth, and the average runtime per image in seconds. The table clearly shows the superior performance of saliency prediction models compared to depth prediction models, particularly on the Pascal VOC dataset.  The best-performing results for each dataset and model are highlighted in bold, allowing for a direct comparison of model efficacy.", "section": "V. EVALUATION"}, {"content": "| Category Name | giraffe | airplane | parking meter | elephant | horse | stop sign | zebra | bear | bed |\n|---|---|---|---|---|---|---|---|---|---| \n| Avg. Corr. per Class | 0.326 | 0.310 | 0.304 | 0.301 | 0.291 | 0.288 | 0.280 | 0.279 | 0.259 |\n| Category Name | cake | motorcycle | teddy bear | hot dog | person | fire hydrant | sandwich | cat | pizza |\n|---|---|---|---|---|---|---|---|---|---| \n| Avg. Corr. per Class | 0.256 | 0.254 | 0.243 | 0.243 | 0.241 | 0.237 | 0.232 | 0.230 | 0.228 |\n| Category Name | cow | dining table | suitcase | dog | donut | sheep | banana | bird | snowboard |\n|---|---|---|---|---|---|---|---|---|---| \n| Avg. Corr. per Class | 0.224 | 0.221 | 0.221 | 0.206 | 0.203 | 0.196 | 0.184 | 0.153 | 0.122 |\n| Category Name | laptop | orange | keyboard | apple | surfboard | bus | umbrella | train | kite |\n|---|---|---|---|---|---|---|---|---|---| \n| Avg. Corr. per Class | 0.121 | 0.120 | 0.119 | 0.118 | 0.117 | 0.110 | 0.110 | 0.105 | 0.103 |\n| Category Name | broccoli | couch | vase | bowl | skateboard | boat | tennis racket | scissors | frisbee |\n|---|---|---|---|---|---|---|---|---|---| \n| Avg. Corr. per Class | 0.099 | 0.097 | 0.095 | 0.092 | 0.089 | 0.086 | 0.083 | 0.079 | 0.077 |\n| Category Name | remote | carrot | bench | tie | traffic light | cell phone | bicycle | skis | toothbrush |\n|---|---|---|---|---|---|---|---|---|---| \n| Avg. Corr. per Class | 0.076 | 0.075 | 0.074 | 0.074 | 0.073 | 0.071 | 0.070 | 0.070 | 0.070 |\n| Category Name | hair drier | backpack | baseball glove | sink | truck | mouse | handbag | clock | toilet |\n|---|---|---|---|---|---|---|---|---|---| \n| Avg. Corr. per Class | 0.065 | 0.064 | 0.061 | 0.060 | 0.054 | 0.054 | 0.053 | 0.043 | 0.042 |\n| Category Name | baseball bat | refrigerator | sports ball | knife | oven | potted plant | chair | fork | cup |\n|---|---|---|---|---|---|---|---|---|---| \n| Avg. Corr. per Class | 0.037 | 0.033 | 0.020 | 0.016 | 0.016 | 0.016 | 0.013 | 0.012 | 0.012 |\n| Category Name | book | spoon | microwave | bottle | wine glass | car | toaster | tv | mA\u03c1 |\n|---|---|---|---|---|---|---|---|---|---| \n| Avg. Corr. per Class | 0.011 | 0.010 | 0.001 | -0.000 | -0.004 | -0.010 | -0.016 | -0.034 | 0.125 |", "caption": "TABLE II: Average Pearson Correlation Results per Class for Different Categories in the COCO Dataset using Depth Anything Model.", "description": "This table presents a detailed breakdown of the average Pearson correlation coefficients calculated for each object category within the COCO dataset.  The correlations are computed using the Depth Anything model's depth predictions compared to the ground truth depth. The table displays each category's average correlation, providing a granular view of the model's performance across different object types.  This allows for analysis of which object classes are better or worse predicted using the model. The final column shows the mean average precision (mAP) across all categories.", "section": "V. EVALUATION"}]