[{"figure_path": "https://arxiv.org/html/2411.19189/x2.png", "caption": "Figure 1: \nThe RollingDepth model takes an unconstrained video and reconstructs a corresponding depth video. Unlike methods that rely on video diffusion models, it extends a single-image monodepth estimator such that it can process short snippets. To account for temporal context, snippets with varying frame rate are sampled from the video, processed, and reassembled through a global alignment algorithm to obtain long, temporally coherent depth videos. Depth is colour-coded from near\u00a0\u00a0far.", "description": "The RollingDepth model processes an unconstrained video to generate a corresponding depth video.  Instead of using video diffusion models, it enhances a single-image depth estimator to handle short video segments (snippets).  To maintain temporal consistency, snippets with varied frame rates are extracted, processed individually, and then precisely combined using a global alignment algorithm, resulting in a long, temporally coherent depth video.  The color-coding of the depth map ranges from near (darker colors) to far (lighter colors).", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2411.19189/x7.png", "caption": "Figure 2: Overview of the RollingDepth Inference Pipeline. Given a video sequence \ud835\udc31\ud835\udc31\\mathbf{x}bold_x (with\u00a0\u00a0is ithsuperscript\ud835\udc56thi^{\\text{th}}italic_i start_POSTSUPERSCRIPT th end_POSTSUPERSCRIPT frame), we construct NTsubscript\ud835\udc41\ud835\udc47N_{T}italic_N start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT overlapping snippets using a dilated rolling kernel with varying dilation rates, and perform 1-step inference to obtain initial depth snippets (\u00a0\u00a0). Next, depth co-alignment optimizes NTsubscript\ud835\udc41\ud835\udc47N_{T}italic_N start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT pairs of scale and shift values to achieve globally consistent depth throughout the full video. An optional refinement step further enhances details by applying additional, snippet-based denoising steps.", "description": "The RollingDepth model processes video data by first dividing it into overlapping short segments (snippets) using a rolling window.  The snippets are processed using a single-image depth estimation model to get initial depth estimates.  These are then aligned to achieve temporal consistency across the entire video by adjusting scale and shift values. Finally, an optional step refines the results with further denoising.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2411.19189/x8.png", "caption": "Figure 3: Depth Refinement encodes the co-aligned depth video into latent space, contaminates it with a moderate amount of noise, then denoises it with a series of reverse diffusion steps with decreasing snippet dilation rate. After each step, overlapping latents are averaged to propagate information between snippets.", "description": "The figure illustrates the depth refinement process in the RollingDepth model.  A co-aligned depth video is first encoded into a latent space. Then, a moderate level of noise is added. The denoising process follows, using a series of reverse diffusion steps with progressively decreasing snippet dilation rates. This approach ensures that information is shared and refined across overlapping snippets throughout the video, leading to a smoother, more coherent final depth map.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2411.19189/x9.png", "caption": "Figure 4: Qualitative comparison between different methods. RollingDepth excels at preserving fine-grained details (cf. the chandelier in the first sample and the tripod in the third sample) and recovering accurate scene layout (cf. the far plane in the second sample).", "description": "This figure showcases a qualitative comparison of depth estimation results produced by various methods, including RollingDepth, DepthCrafter, ChronoDepth, and Marigold.  The images highlight RollingDepth's ability to preserve fine details in the scene, such as the intricate details of a chandelier and a tripod, which other methods often fail to capture accurately. RollingDepth also more accurately reconstructs the depth of distant parts of the scene, as demonstrated in the accurate representation of the far plane in one of the example scenes. These results highlight the improved accuracy of RollingDepth compared to existing methods.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2411.19189/x10.png", "caption": "Figure 5: AbsRel error over time: The line plot (left) shows the depth error at every individual frame, end-of-line numbers are the average error across the video. The images (right) display error maps (low\u00a0\u00a0high) for two specific frames. RollingDepth\u00a0achieves the lowest error overall, competing methods recover scene layout less faithfully and tend to be biased towards the foreground or the background.", "description": "Figure 5 presents a comparative analysis of depth estimation error over time across different methods.  The left side shows line plots illustrating the absolute relative error (AbsRel) for each frame of example video sequences. The end-of-line numerical values represent the average AbsRel across the entire video. The right side provides visual error maps for two specific frames from these videos, using a color scale to represent the magnitude of error (low to high). This visualization helps to show the spatial distribution of errors and which areas each model struggles with.  The results demonstrate that RollingDepth consistently achieves the lowest overall error, while competing methods show less accurate scene reconstruction and a tendency to favor either foreground or background elements.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2411.19189/extracted/6032098/image/PO_simple.jpg", "caption": "Figure 6: Qualitative comparison of depth predictions (near\u00a0\u00a0far) from in-the-wild videos. To graphically show temporal consistency, we display temporal profiles (red box) for a fixed column (marked with a red line). RollingDepth picks up subtle details like accessories and wrinkled cloth, and mitigates spurious depth discontinuities (cf. background in temporal profile of the first sample) in time.", "description": "Figure 6 presents a qualitative comparison of depth maps generated by different methods for real-world video sequences.  The figure visually demonstrates the temporal consistency of depth estimations, which is crucial for high-quality video depth maps. Each sequence shows a temporal profile (in a red box) that tracks a single column's depth values over time. This highlights the stability and accuracy of RollingDepth in capturing fine details, like wrinkles in clothing and accessories, while minimizing artifacts (such as abrupt changes in background depth).  The figure contrasts RollingDepth's performance with other methods, revealing RollingDepth's superiority in producing smooth, temporally consistent depth estimations.", "section": "4.3 Comparison with Other Methods"}, {"figure_path": "https://arxiv.org/html/2411.19189/extracted/6032098/image/PO_smoke.jpg", "caption": "Figure S1: Examples of PointOdyssey toy scenes (left) and scenes with smoke (right).", "description": "This figure showcases examples from the PointOdyssey dataset that present challenges for depth estimation models.  The left side displays simplistic, toy-like scenes which are not representative of real-world scenarios and thus could lead to overfitting or biased model training. The right side shows scenes containing smoke, an element which tends to cause ambiguity in depth perception for computer vision systems, potentially leading to inconsistent depth map predictions.", "section": "A.4 Baseline Methods"}, {"figure_path": "https://arxiv.org/html/2411.19189/x11.png", "caption": "Figure S2: Examples of PointOdyssey samples that challenge video models. In the cases above, the (inverse) depth range varies significantly across frames. The arrows highlight situations where video models yield distorted depth maps.\nIn the first two rows, this occurs in regions where the depth deviates significantly from the surrounding scene.\nIn the last row, the depth predictions get drawn towards the near plane to match the object close to the camera, biasing the depth in the far field.", "description": "Figure S2 showcases examples from the PointOdyssey dataset where video depth estimation models struggle. These examples highlight scenarios with rapidly changing inverse depth ranges across frames.  The arrows pinpoint areas where these models produce inaccurate depth maps. In the top two rows, inaccuracies arise in regions exhibiting significant depth differences from their surroundings.  The bottom row illustrates a scenario where the model prioritizes matching the depth of a nearby object, thereby distorting the depth values of faraway elements.", "section": "B.1 Failure cases of video models on Point Odyssey"}, {"figure_path": "https://arxiv.org/html/2411.19189/x12.png", "caption": "Figure S3: The two samples on the left show incorrect depth predictions in the cloudy sky.\nThe two samples on the right show inconsistencies between different frames of the same video, where the depth at the glass windows fluctuates between the solid and transparent states.", "description": "Figure S3 showcases examples where RollingDepth struggles. The left side displays inaccurate depth estimations in cloudy skies, a common challenge for depth prediction models. The right side highlights inconsistencies in depth estimations across frames of the same video, specifically concerning areas with glass windows. These inconsistencies manifest as fluctuating depth readings, sometimes appearing solid and at other times transparent, which can be attributed to issues related to the transparency and reflections of glass surfaces.", "section": "B.2. Failure Cases of RollingDepth"}]