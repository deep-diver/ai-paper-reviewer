[{"figure_path": "https://arxiv.org/html/2503.18908/extracted/6306213/assets/nvlogo2.png", "caption": "Figure 1: An overview of our FFN Fusion approach. Step 1111: We apply Puzzle to partially remove FFN layers and remove entire attention layers. Step 2222: We fuse consecutive FFN layers into a single wide FFN layer.", "description": "This figure illustrates the FFN Fusion technique used to optimize large language models.  The process begins by using the Puzzle algorithm (Step 1) to remove some feed-forward network (FFN) layers and all attention layers from the model. The remaining FFN layers are often found in consecutive sequences.  In Step 2, these sequences of FFN layers are 'fused' together into a single, wider FFN layer. This fusion allows for parallel processing of these layers, significantly increasing computational efficiency during inference without significantly impacting accuracy.", "section": "FFN Fusion"}, {"figure_path": "https://arxiv.org/html/2503.18908/x1.png", "caption": "Figure 2: \nBlock-wise dependency heatmap for Ultra-PreFusion (log-scale).\nEach coordinate (i,j)\ud835\udc56\ud835\udc57(i,j)( italic_i , italic_j ) encodes how much block\u00a0j\ud835\udc57jitalic_j depends on block\u00a0i\ud835\udc56iitalic_i, measured by the cosine distance between hj\u2062(\ud835\udc7f)superscript\u210e\ud835\udc57\ud835\udc7fh^{j}({\\bm{X}})italic_h start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT ( bold_italic_X ) and h~ij\u2062(\ud835\udc7f)subscriptsuperscript~\u210e\ud835\udc57\ud835\udc56\ud835\udc7f\\tilde{h}^{j}_{i}({\\bm{X}})over~ start_ARG italic_h end_ARG start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( bold_italic_X ).\nDarker blue hues indicate weaker dependencies.\nThe attention-removed region (dotted box) shows consistently lower values, suggesting greater potential for parallelization.\nDarker red hues indicate stronger dependencies. Further analysis of this Figure can be found in Appendix E.", "description": "This figure is a heatmap visualizing the dependencies between blocks in the Ultra-PreFusion model. Each cell (i,j) represents the cosine similarity between the contribution of block j to the model's output when block i is present and when block i is removed. Darker blue indicates low dependency (good for parallelization), and darker red indicates high dependency (bad for parallelization). The dotted box highlights a region where attention layers have been removed, showing low inter-block dependencies, thus indicating a good candidate for FFN fusion.", "section": "Pairwise block dependency"}, {"figure_path": "https://arxiv.org/html/2503.18908/x2.png", "caption": "Figure 3: Comparison of Ultra-253B-Base before and after applying an additional longer continual pretraining.", "description": "This figure compares the performance of the Ultra-253B-Base model before and after an additional, longer continual pre-training (CPT) phase.  It shows benchmark results across several tasks (MMLU, MATH500, HumanEval, and RULER 128K) to illustrate the impact of the extended training on model accuracy. The bars represent the performance scores achieved on each benchmark for both the pre-CPT and post-CPT versions of the Ultra-253B-Base model, allowing for a direct visual comparison of improvements.", "section": "Efficiency Improvements"}, {"figure_path": "https://arxiv.org/html/2503.18908/x3.png", "caption": "Figure 4: Accuracy vs. latency performance of Ultra-253B-Base. Latency is measured on a single NVIDIA H100 node with tensor parallel (TP) 8, running in FP8. The red line represents the efficient frontier, highlighting models with the best accuracy-to-throughput tradeoff. Accuracy = (MT-Bench\u00d710+MMLU+MMLU-Pro+Arena Hard+HumanEval)/5MT-Bench10MMLUMMLU-ProArena HardHumanEval5(\\text{MT-Bench}\\times 10+\\text{MMLU}+\\text{MMLU-Pro}+\\text{Arena Hard}+\\text{%\nHumanEval})/5( MT-Bench \u00d7 10 + MMLU + MMLU-Pro + Arena Hard + HumanEval ) / 5.", "description": "Figure 4 presents a performance comparison of the Ultra-253B-Base language model against other models.  The x-axis represents the model's inference latency (measured in tokens per second) on a single NVIDIA H100 GPU with tensor parallelism (TP8) using FP8 precision.  The y-axis shows the model's accuracy, which is a composite score calculated as the average of several benchmark metrics. The benchmarks include MT-Bench (weighted 10x), MMLU, MMLU-Pro, Arena Hard, and HumanEval.  The red line illustrates the efficient frontier, which highlights the models that best balance accuracy and speed.  Points above the red line represent models that achieve higher accuracy for a given latency, indicating superior performance.", "section": "4 Producing Large-Scale Models with FFN Fusion"}, {"figure_path": "https://arxiv.org/html/2503.18908/x4.png", "caption": "Figure 5: Accuracy vs. Latency for FFN Removal vs. Fusion.", "description": "This figure compares the accuracy and latency trade-offs between removing FFN layers and fusing them using FFN Fusion.  The x-axis represents latency (in milliseconds), and the y-axis represents accuracy (%).  Multiple lines are shown, each representing a different strategy: (1) a baseline (no FFN removal or fusion), (2) removal of 15 FFN layers, (3) removal of 20 FFN layers, (4) applying FFN Fusion in a single step, and (5) applying FFN Fusion followed by knowledge distillation (KD). The results demonstrate that FFN Fusion achieves significantly better accuracy at comparable latencies compared to simply removing FFN layers. Knowledge distillation further improves the performance of the fusion method.", "section": "5.2 Removing FFNs vs. FFN Fusion"}, {"figure_path": "https://arxiv.org/html/2503.18908/x5.png", "caption": "Figure 6: Per-layer metrics. Upper row is the cosine distance between f\u2062(\ud835\udc7f)\ud835\udc53\ud835\udc7ff({\\bm{X}})italic_f ( bold_italic_X ) and \ud835\udc7f\ud835\udc7f{\\bm{X}}bold_italic_X for the (a) The 49B model and (b) Ultra-253B-Base model. Bottom row represents the ratio between h\u2062(\ud835\udc7f)\u210e\ud835\udc7fh({\\bm{X}})italic_h ( bold_italic_X ) and \ud835\udc7f\ud835\udc7f{\\bm{X}}bold_italic_X for the (c) The 49B model and (d) Ultra-253B-Base model.", "description": "Figure 6 presents a per-layer analysis of two key metrics, comparing the 49B and 253B models.  The top row shows the cosine distance between the input (X) and the output (f(X)) of each layer.  A lower cosine distance suggests that the layer's output is very similar to its input, implying less processing and transformation is occurring. The bottom row visualizes the ratio of h(X) (the block's contribution to X) to X itself.  A lower ratio indicates that the layer has minimal effect on the input's direction, suggesting a low dependency on preceding layers. This analysis helps to identify regions within the model that are suitable for parallelization, as observed by minimal impact from fusing FFN layers.", "section": "6 Block Parallelization"}, {"figure_path": "https://arxiv.org/html/2503.18908/x6.png", "caption": "(a)", "description": "This figure shows the cosine distance between the output of each layer and its input for the 49B parameter model (left) and the Ultra-253B-Base model (right).  The cosine distance is a measure of the similarity between two vectors; a lower cosine distance indicates higher similarity.  The plot shows that in certain regions of the model (primarily those with fused FFN layers), the cosine distance between the input and output is relatively small. This suggests that the FFN layers in these regions have a limited effect on the input's direction, allowing them to be fused without significantly impacting the model's accuracy.  The low cosine distance in these regions is a key observation supporting the FFN Fusion optimization technique.", "section": "5.4 Fusion Explainability"}, {"figure_path": "https://arxiv.org/html/2503.18908/x7.png", "caption": "(b)", "description": "This figure shows the heatmap of the block-wise dependency for the Ultra-PreFusion model (before applying FFN fusion). The heatmap visualizes the dependency between blocks using cosine distance. Darker blue colors represent lower dependency (meaning blocks are relatively independent), while darker red colors indicate higher dependency (meaning blocks are strongly dependent). The figure highlights a region where multiple consecutive FFN layers exhibit low interdependencies, suggesting a significant potential for parallelization using FFN fusion. The dark blue region (marked by a dashed square) indicates such a region, while the redder regions indicate higher dependencies making parallelization more challenging or potentially less effective.", "section": "4 Producing Large-Scale Models with FFN Fusion"}, {"figure_path": "https://arxiv.org/html/2503.18908/x8.png", "caption": "Figure 7: (a) Block-wise Dependency Heatmap of the 49B model (log-scale). Darker blue hues indicate weaker dependencies, darker red hues indicate stronger dependencies. (b) \ud835\udc74maxsubscript\ud835\udc74max{\\bm{M}}_{\\text{max}}bold_italic_M start_POSTSUBSCRIPT max end_POSTSUBSCRIPT and \ud835\udc74sumsubscript\ud835\udc74sum{\\bm{M}}_{\\text{sum}}bold_italic_M start_POSTSUBSCRIPT sum end_POSTSUBSCRIPT values for 4-Block Sequences of the 49B model. Lower values indicating more promising candidates for parallelization.", "description": "Figure 7 visualizes the dependencies between blocks in a 49B parameter language model.  Panel (a) is a heatmap showing pairwise dependencies; darker blue indicates weaker dependence, while darker red shows stronger dependence.  Panel (b) displays two metrics (Mmax and Msum) calculated for sequences of four consecutive blocks, which quantify the maximum and sum of dependencies within those sequences, respectively. Lower values of Mmax and Msum suggest greater potential for parallelization.", "section": "6 Block Parallelization"}, {"figure_path": "https://arxiv.org/html/2503.18908/x9.png", "caption": "Figure 8: A visualization of FFN Fusion applied to SwiGLU. Two FFNs (left) are fused into a single FFN (right).", "description": "Figure 8 illustrates the FFN Fusion technique applied to the SwiGLU activation function. The left side shows two separate SwiGLU layers, each with its own weight matrices (W1, W2, W3).  The input to the second layer depends on the output of the first. FFN Fusion combines these two layers into a single, wider SwiGLU layer (right). The new layer has combined weight matrices which achieve the same result as the two sequential layers but allows for parallel processing. The figure visually demonstrates the effect of reducing sequential computations and improving efficiency by merging FFN layers.", "section": "3 FFN Fusion"}, {"figure_path": "https://arxiv.org/html/2503.18908/x10.png", "caption": "(a) Ultra-PreFusion Attention Layout", "description": "This figure shows the layout of attention layers in the Ultra-PreFusion model before applying FFN Fusion.  The x-axis represents the layer index, and the y-axis indicates the type of layer.  The figure visually represents the sequence and distribution of attention layers and other layers within the model's architecture.  This helps to illustrate the pattern of attention layers that are removed or fused during model optimization.", "section": "5 Additional Empirical Studies"}, {"figure_path": "https://arxiv.org/html/2503.18908/x11.png", "caption": "(b) Ultra-PreFusion FFN Layout", "description": "This figure shows the layout of feed-forward networks (FFNs) in the Ultra-PreFusion model. It illustrates the arrangement of FFN layers, their widths (number of neurons), and how they are distributed throughout the model's architecture.  The x-axis represents the layer index, and the y-axis represents the FFN width (or the number of neurons in each FFN layer).  The plot visually depicts the changes in the width of the FFN layers across the various layers of the model. This visualization helps to understand the model's structure and how the computational complexity varies across different layers.", "section": "C Ultra-253B-Base and Puzzle-49B Architecture Overview"}, {"figure_path": "https://arxiv.org/html/2503.18908/x12.png", "caption": "(c) Puzzle-49B Attention Layout", "description": "This figure is a visualization of the attention layers layout in the Puzzle-49B model architecture. It shows the arrangement and quantity of attention layers across the model's layers, illustrating how the Puzzle algorithm has pruned or removed certain layers. The x-axis represents the layer index, while the y-axis shows the number of attention layers present at each layer. The figure helps in understanding the sparsity introduced by the Puzzle optimization technique, specifically in the context of the 49B parameter model.", "section": "C Ultra-253B-Base and Puzzle-49B Architecture Overview"}, {"figure_path": "https://arxiv.org/html/2503.18908/x13.png", "caption": "(d) Puzzle-49B FFN Layout", "description": "This figure is a visualization of the feed-forward network (FFN) layer layout for the Puzzle-49B model.  It shows the number of FFN layers across the model's depth, and visually depicts the varying widths of these FFN layers as a result of the Puzzle algorithm's optimization. The x-axis represents the layer index and the y-axis represents the width (scaling multiplier) of the FFN layer.  This helps illustrate the effect of Puzzle in making FFN layer widths vary across the layers, and highlights sequences of consecutive FFN layers resulting from the pruning of attention layers.", "section": "C Ultra-253B-Base and Puzzle-49B Architecture Overview"}, {"figure_path": "https://arxiv.org/html/2503.18908/x14.png", "caption": "Figure 9: \nA 2x2 overview of Ultra-253B-Base (top row) and Puzzle-49B (bottom row).\nSubfigures\u00a0(9(a)) and (9(b)) illustrate the attention and FFN configurations, respectively, for the 253B model.\nSubfigures\u00a0(9(c)) and (9(d)) show the corresponding layouts for the 49B model.\nBoth architectures feature variable FFN widths and regions where attention has been removed, although at different scales.", "description": "This figure provides a detailed comparison of the architectural layouts of two large language models: Ultra-253B-Base and Puzzle-49B.  The top row displays the architecture of the Ultra-253B-Base model, showing the arrangement of attention and feed-forward network (FFN) layers.  The bottom row shows the corresponding architecture for the Puzzle-49B model.  Subfigures (a) and (b) specifically detail the attention and FFN layers, respectively, within the Ultra-253B-Base model. Subfigures (c) and (d) present the same information for the Puzzle-49B model.  A key takeaway is that both models exhibit variable FFN widths and sections where attention layers have been removed, although the scale of these variations differs between the two models.", "section": "Ultra-253B-Base and Puzzle-49B Architecture Overview"}, {"figure_path": "https://arxiv.org/html/2503.18908/x15.png", "caption": "Figure 10: FFN Fusion helps reduce latency by increasing GPU utilization and by reducing syncs", "description": "This figure illustrates how FFN Fusion improves latency.  The left side shows a traditional approach with sequential FFN layers (FFN1 and FFN2). Each FFN layer requires synchronization (represented by the 'Sync' label), leading to longer execution times due to the need for communication between GPUs. In contrast, the right side depicts the FFN Fusion method, where FFN1 and FFN2 are merged into a single, wider layer. This reduces the number of synchronization points and increases GPU utilization, resulting in faster inference latency.", "section": "D MoE Inference Performance"}]