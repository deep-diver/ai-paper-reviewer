{"references": [{"fullname_first_author": "Vaswani", "paper_title": "Attention is all you need", "publication_date": "2023-01-01", "reason": "This paper introduces the Transformer architecture, which is the foundation for many modern LLMs and is therefore fundamental to this work."}, {"fullname_first_author": "Wolf", "paper_title": "Huggingface's transformers: State-of-the-art natural language processing", "publication_date": "2020-01-01", "reason": "This paper introduces the Transformers library, which is used for implementing and experimenting with Transformer models."}, {"fullname_first_author": "Radford", "paper_title": "Language models are unsupervised multitask learners", "publication_date": "2019-01-01", "reason": "This paper marks the transition from academic curiosities to tools that are reshaping industries."}, {"fullname_first_author": "Shazeer", "paper_title": "Glu variants improve transformer", "publication_date": "2020-01-01", "reason": "This paper introduces the SwiGLU module used in the FFN layer."}, {"fullname_first_author": "Bercovich", "paper_title": "Puzzle: Distillation-based nas for inference-optimized llms", "publication_date": "2024-01-01", "reason": "This paper introduces the Puzzle framework, a neural architecture search framework that optimizes a trained LLM for inference efficiency. "}]}