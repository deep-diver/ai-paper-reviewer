[{"figure_path": "https://arxiv.org/html/2411.06481/x1.png", "caption": "Figure 1: The figure illustrates that previous extended motion generation methods often struggle with directional instructions, leading to incorrect motions. In contrast, our proposed KMM, with enhanced text-motion alignment, effectively improves the model\u2019s understanding of text queries, resulting in more accurate motion generation.", "description": "This figure demonstrates the limitations of existing extended motion generation methods in handling directional instructions within text prompts.  The top row shows examples of how previous models (PriorMDM, FlowMDM, TEACH) incorrectly interpret directional instructions like \"raise left arm\" or \"kick right leg,\" resulting in inaccurate or opposite movements. The bottom row shows the improved accuracy and correctness of the proposed KMM model under the same conditions.  KMM's enhanced text-motion alignment allows the model to better understand and respond correctly to these directions.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2411.06481/x2.png", "caption": "Figure 2: The figure demonstrates our novel method from three different perspectives: (a) illustrates the key frame masking strategy based on local density and minimum distance to higher density calculation. (b) showcases the overall architecture of the masked bidirectional Mamba. (c) demonstrates the text-to-motion alignment, highlighting the process before and after alignment.", "description": "This figure provides a detailed breakdown of the KMM method, showing its three key components: (a) Key Frame Mask Modeling, which uses local density and minimum distance calculations to strategically mask key frames, enhancing the model's focus on crucial actions; (b) the overall architecture of the masked bidirectional Mamba, illustrating how the masking strategy is integrated into the model's structure; and (c) Text-Motion Alignment, demonstrating the contrastive learning approach that enhances the model's ability to align text and motion data, improving the accuracy and relevance of generated motions.", "section": "Methodology"}, {"figure_path": "https://arxiv.org/html/2411.06481/x3.png", "caption": "Figure 3: The figure shows the user study interface where 50 participants evaluated motion sequences generated by TEACH, PriorMDM, FlowMDM, and KMM, focusing on text-motion alignment, robustness, diversity, and usability. The text prompt are randomly extracted and combined from the HumanML3D (Guo et\u00a0al. 2022) and BABEL (Punnakkal et\u00a0al. 2021) test set.", "description": "This figure depicts the user interface of a study involving 50 participants who assessed motion sequences generated by four different methods: TEACH, PriorMDM, FlowMDM, and the proposed KMM method.  The participants evaluated the generated motions based on four criteria:  text-motion alignment (how well the motion matched the text description), robustness (how realistic and natural the motion appeared), diversity (how varied and interesting the motions were), and usability (how suitable the motions would be for real-world applications, such as in video games or animation).  The text prompts used to generate the motion sequences were randomly selected and combined from the HumanML3D (Guo et al., 2022) and BABEL (Punnakkal et al., 2021) datasets, ensuring a variety of motion types and descriptions.", "section": "User Study"}, {"figure_path": "https://arxiv.org/html/2411.06481/x4.png", "caption": "Figure 4: The figure demonstrates a qualitative comparison between the previous state-of-the-art method in extended motion generation and our KMM. The qualitative results show that our method significantly outperforms others in handling complex text queries and generating more accurate corresponding motions.", "description": "Figure 5 presents a qualitative comparison of extended motion generation results between KMM and three state-of-the-art methods (TEACH, PriorMDM, and FlowMDM).  Three example text prompts of varying complexity are used as input. For each prompt, the generated motion sequences from each method are displayed. The visualization clearly demonstrates KMM's superior performance in accurately interpreting complex instructions and producing more realistic and nuanced motions compared to the other methods.", "section": "Qualitative Comparison"}, {"figure_path": "https://arxiv.org/html/2411.06481/x5.png", "caption": "Figure 5: The figure presents some qualitative visualization results of our proposed KMM model. The text prompts are sourced and combined from HumanML3D (Guo et\u00a0al. 2022) and BABEL (Punnakkal et\u00a0al. 2021). The number within the brackets indicates our ability to condition the generated motion on a specific length, dynamically producing motion of the desired duration. The visualizations showcase KMM\u2019s superior performance in generating robust and diverse motions that align closely with lengthy and complex text queries.", "description": "This figure showcases qualitative results from the KMM model, demonstrating its ability to generate diverse and robust motions from complex, lengthy text prompts.  The prompts are sourced from the HumanML3D and BABEL datasets.  The numbers in parentheses after each prompt indicate the length of the generated motion sequence (in frames), highlighting the model's ability to produce motions of specified durations. The visualizations highlight KMM's superior performance against other state-of-the-art methods in accurately and dynamically generating human motion that precisely aligns with the input text instructions.", "section": "Experiments"}]