[{"figure_path": "https://arxiv.org/html/2411.04989/extracted/5985497/imgs/semantic_correspondence_labeled.jpg", "caption": "Figure 1: Image-to-video generation based on self-guided trajectory control.\nGiven a set of bounding boxes with associated trajectories, we achieve object and camera motion control in image-to-video generation by leveraging the knowledge present in a pre-trained image-to-video diffusion model. Our method is self-guided, offering zero-shot trajectory control without fine-tuning or relying on external knowledge.", "description": "This figure illustrates the image-to-video generation process using self-guided trajectory control.  Input consists of an image and a set of bounding boxes, each with an associated trajectory indicating the desired movement of the object within that box. The model, leveraging a pre-trained image-to-video diffusion model, generates a video where objects and potentially the camera move according to the specified trajectories. This method is unique in its self-guided nature, achieving zero-shot trajectory control without any need for additional fine-tuning or external data.", "section": "1 INTRODUCTION"}, {"figure_path": "https://arxiv.org/html/2411.04989/x1.png", "caption": "Figure 2: Semantic correspondences in video diffusion models.\nWe analyze feature maps in the image-to-video diffusion model SVD\u00a0(Blattmann et\u00a0al., 2023a) for three generated video sequences (row 1).\nWe use PCA to visualize the features at diffusion timestep 30 (out of 50) at the output of an upsampling block (row 2), a self-attention layer (row 3), and the same self-attention layer after our alignment procedure (row 4). Although output feature maps of upsampling blocks in image diffusion models are known to encode semantic information \u00a0(Tang et\u00a0al., 2023), we only observe weak semantic correspondences across frames in SVD.\nThus, we focus on the self-attention layer and modify it to produce feature maps that are semantically aligned across frames.", "description": "This figure visualizes the semantic alignment of feature maps in the Stable Video Diffusion (SVD) model across different layers and frames.  Three example video sequences are shown (Row 1). PCA is used to visualize features extracted at timestep 30 of 50 from the upsampling block (Row 2), the self-attention layer (Row 3), and the modified self-attention layer (Row 4) using the authors' alignment method. The visualization shows that while upsampling blocks in image diffusion models typically exhibit strong semantic alignment, this is weak in SVD across frames. Therefore, the authors focus on modifying the self-attention layers to improve cross-frame semantic alignment.", "section": "3.2 EXTRACTING SEMANTIC VIDEO LAYOUT"}, {"figure_path": "https://arxiv.org/html/2411.04989/extracted/5985497/imgs/figure_baseline_labeled.jpg", "caption": "Figure 3: Overview of the controllable image-to-video generation framework.\nTo control trajectories of scene elements, we optimize the latent \ud835\udc9btsubscript\ud835\udc9b\ud835\udc61\\bm{z}_{t}bold_italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT at specific denoising timesteps t\ud835\udc61titalic_t of a pre-trained video diffusion model.\nFirst, we extract semantically aligned feature maps from the denoising U-Net to estimate the video layout.\nNext, we enforce cross-frame feature similarity along the bounding box trajectory to drive the motion of each region.\nTo preserve the visual quality of the generated video, a frequency-based post-processing method is applied to retain high-frequency noise of the original latent \ud835\udc9btsubscript\ud835\udc9b\ud835\udc61\\bm{z}_{t}bold_italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT. The updated latent \ud835\udc9b~tsubscript~\ud835\udc9b\ud835\udc61\\tilde{\\bm{z}}_{t}over~ start_ARG bold_italic_z end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT\nis input to the next denoising step.", "description": "This figure illustrates the SG-I2V framework for controllable image-to-video generation.  It details the process of manipulating a pre-trained video diffusion model to control object motion within a video generated from a single input image. The process begins by extracting semantically aligned feature maps from the model's U-Net to understand the video's layout. These feature maps are then used to guide the optimization of the latent representation (z_t) at key denoising steps, ensuring consistency in object movement along predefined trajectories. Finally, a post-processing step refines the resulting video's visual quality by selectively preserving high-frequency components of the original latent representation, leading to a more natural-looking and artifact-free video. The updated latent representation (~z_t) is then used in the next denoising step of the video generation process.", "section": "3 METHOD"}, {"figure_path": "https://arxiv.org/html/2411.04989/x2.png", "caption": "Figure 4: Qualitative comparison with supervised baselines.\nWe observe that DragNUWA tends to distort objects rather than move them, and DragAnything is weak at part-level control as it is designed for entity-level control.\nIn contrast, our method can generate videos with natural motion for diverse object and camera trajectories.\nPlease see our project page for more comparisons.", "description": "Figure 4 presents a qualitative comparison of video generation results between the proposed SG-I2V model and existing supervised baselines (DragNUWA and DragAnything).  The comparison highlights the differences in how each method handles object motion. DragNUWA is shown to distort objects instead of smoothly moving them, while DragAnything struggles with fine-grained, part-level control because it is primarily designed for controlling entire objects.  In contrast, SG-I2V is demonstrated to produce videos with natural object and camera movements across a variety of scenarios.", "section": "4 Experiments"}, {"figure_path": "https://arxiv.org/html/2411.04989/x3.png", "caption": "Figure 5: Performance across U-Net feature maps used to compute loss in Eq.\u00a01.\nFor all metrics, lower values are better.\nTemporal and spatial refer to the temporal and spatial self-attention layers.\nWe find that features extracted from self-attention layers generally perform better than those from upsampling blocks and temporal attention layers.\nIn addition, using the feature maps of our modified self-attention layer achieves the best results, since they are semantically aligned across frames.\nCorresponding qualitative visuals are presented in Fig.\u00a013 and our project page.", "description": "Figure 5 investigates the performance of using different feature maps from the U-Net architecture of the Stable Video Diffusion model for computing the loss function in Equation 1.  The goal is to find which feature maps are most effective for controlling object trajectories in image-to-video generation.  The figure shows three metrics (lower values are better): FID (visual quality), FVD (motion consistency), and ObjMC (motion accuracy).  Results reveal that self-attention layers generally outperform upsampling blocks and temporal attention layers.  The modified self-attention layer, which ensures semantic alignment across frames, yields the best results across all three metrics.  Qualitative examples corresponding to this analysis are provided in Figure 13 and the project's webpage.", "section": "3 METHOD"}, {"figure_path": "https://arxiv.org/html/2411.04989/extracted/5985497/imgs/figure_fft_labeled.jpg", "caption": "Figure 6: Performance across U-Net layers used to extract feature maps. Lower is better for all metrics.\nBottom, mid, and top indicate the three resolution levels in the U-Net\u2019s upsampling path, each containing three self-attention layers numbered 1, 2, and 3.\nfor example \u201cM2-3\u201d means applying the loss to features from both mid-resolution layers 2 and 3.\nWe observe that mid-resolution feature maps perform best for trajectory guidance.\nIn addition, using features from both M2 and M3 leads to the best result.\nSee our project page for visualizations.", "description": "This figure analyzes the performance of using feature maps from different layers of the U-Net in a video diffusion model for trajectory control in image-to-video generation.  The U-Net's upsampling path has three resolution levels (bottom, mid, top), each with three self-attention layers (1, 2, 3). The experiment tests using features from various combinations of these layers (e.g., 'M2-3' means using layers 2 and 3 from the mid-resolution level).  The results show that mid-resolution feature maps are best for trajectory guidance, with the best performance achieved by combining layers 2 and 3 of the mid-resolution level.  The figure uses FID, FVD, and ObjMC to measure performance; lower scores are better.  For detailed visualizations, refer to the project webpage.", "section": "4 Experiments"}, {"figure_path": "https://arxiv.org/html/2411.04989/x4.png", "caption": "Figure 7: Effect of high-frequency preservation in post-processing.\nVideos without post-processing tend to demonstrate oversmoothing and have artifacts.\nIn contrast, our post-processing technique retains videos with sharp details and eliminates most of the artifacts.\nSee our project page for more examples.", "description": "This figure demonstrates the effectiveness of the high-frequency preservation post-processing step used in the SG-I2V framework.  The left column shows video frames generated without the post-processing step, exhibiting noticeable oversmoothing and artifacts. In contrast, the right column shows video frames generated with the post-processing step. These frames retain sharp details and significantly reduce artifacts, demonstrating a clear improvement in visual quality.", "section": "3 METHOD"}, {"figure_path": "https://arxiv.org/html/2411.04989/x5.png", "caption": "Figure 8: Study of the cut-off frequency in post-processing.\nLower is better for all metrics.\nThe value \u03b3\ud835\udefe\\gammaitalic_\u03b3 indicates the cut-off frequency.\nFully keeping the optimized latent (\u03b3=1\ud835\udefe1\\gamma=1italic_\u03b3 = 1) results in degraded video quality, as shown by high FID and FVD values.\nOn the other hand, replacing too many frequency components diminishes motion control, as indicated by the increasing ObjMC.", "description": "Figure 8 shows the impact of the cut-off frequency (\u03b3) used in a post-processing step designed to enhance the quality of generated videos.  The graph displays FID, FVD, and ObjMC values as a function of \u03b3.  A \u03b3 value of 1 represents no filtering (fully keeping the optimized latent), which leads to high FID and FVD (indicating poor visual quality).  As \u03b3 decreases, less of the high-frequency components are preserved; while this improves visual quality, it also negatively affects motion control (increased ObjMC).  The optimal \u03b3 balances these competing factors for best overall video quality.", "section": "3.4 High-Frequency Preserved Post-Processing"}, {"figure_path": "https://arxiv.org/html/2411.04989/x6.png", "caption": "Figure 9: Ablation on optimization learning rates. Larger learning rates lead to video quality degradation (i.e., higher FID and FVD), while smaller learning rates result in lower motion fidelity (i.e., higher ObjMC). We choose the learning rate considering this tradeoff.", "description": "This ablation study investigates the impact of different learning rates on the optimization process for controllable image-to-video generation.  The results show a trade-off between visual quality (measured by FID and FVD) and motion fidelity (measured by ObjMC).  Higher learning rates improve motion fidelity at the cost of visual quality, indicated by increased FID and FVD scores.  Conversely, lower learning rates prioritize visual quality, leading to better FID and FVD scores but reduced motion fidelity (higher ObjMC). The optimal learning rate is selected to balance these competing factors for the best overall performance.", "section": "4 Experiments"}, {"figure_path": "https://arxiv.org/html/2411.04989/extracted/5985497/imgs/correspondence_full_labeled.jpg", "caption": "Figure 10: Effect of optimizing latent at individual denoising timesteps.\nFor all metrics, lower values are better.\nHere, we optimize Eq.\u00a01 on a single denoising timestep (t=50\ud835\udc6150t=50italic_t = 50 corresponds to standard Gaussian noise), and we find middle timesteps (e.g. t=30\ud835\udc6130t=30italic_t = 30) achieve the best motion fidelity while maintaining visual quality.\nMore results on optimizing the latent at multiple timesteps can be found in Fig.\u00a016.\nSee Fig.\u00a015 and our project page for qualitative comparisons.", "description": "This figure analyzes the impact of optimizing the latent representation at different denoising timesteps during video generation.  It evaluates the trade-off between visual quality and motion fidelity by optimizing the latent at various steps in the denoising process, using a single timestep at a time. The results show that optimizing at intermediate timesteps (around t=30) yields the best balance, producing high-fidelity motion without compromising visual quality.  The figure presents quantitative results (FID, FVD, ObjMC) for different timesteps. Additional results involving optimization over multiple timesteps are provided in Figure 16. Qualitative comparisons are available in Figure 15 and on the project website.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2411.04989/extracted/5985497/imgs/correspondence_full_feature_labeled.jpg", "caption": "Figure 11: Semantic correspondences in video diffusion models across timesteps.\nOutput feature maps of upsampling blocks have limited semantic correspondences across frames.\nIn contrast, our modified self-attention layers produce semantically aligned feature maps across all the timesteps.", "description": "This figure visualizes feature maps from different layers of a video diffusion model at various diffusion timesteps.  The top rows show the outputs from upsampling blocks, which lack consistent semantic relationships across frames (meaning the features representing the same object don't consistently align across different frames in the video sequence). The bottom rows depict the output from the authors' modified self-attention layers.  These layers show strong semantic correspondence, meaning features related to a given object remain consistently aligned throughout the video sequence.", "section": "3.2 EXTRACTING SEMANTIC VIDEO LAYOUT"}, {"figure_path": "https://arxiv.org/html/2411.04989/extracted/5985497/imgs/figure_fet_full_labeled.jpg", "caption": "Figure 12: Semantic correspondences of different features in video diffusion models.\nWe find features from self-attention layers to be more semantically aligned than that of temporal attention layers and upsampling layers, while our modified self-attention layer produces the most aligned results due to its explicit formulation to attend to the first frame.", "description": "This figure visualizes the semantic alignment of features extracted from different layers of a video diffusion model.  The analysis compares features from upsampling blocks, standard self-attention layers, and temporal attention layers. PCA is used to visualize these features across different frames of a generated video. The results show that features from self-attention layers exhibit stronger semantic alignment than those from upsampling blocks and temporal attention layers. Notably, a modified self-attention layer, where the key and value tokens are replaced with those from the first frame, demonstrates significantly improved semantic alignment across frames. This improved alignment is attributed to the explicit modification of the self-attention mechanism to attend to the first frame.", "section": "3.2 EXTRACTING SEMANTIC VIDEO LAYOUT"}, {"figure_path": "https://arxiv.org/html/2411.04989/extracted/5985497/imgs/figure_layer_labeled.jpg", "caption": "Figure 13: Ablation on U-Net feature maps.\nApplying loss on feature maps extracted from original self/temporal-attention layers or upsampling blocks fails to follow the trajectory due to the semantic misalignment across frames.\nIn contrast, performing optimization with our modified self-attention layers can produce videos consistent with the input trajectory, indicating the importance of using semantically aligned feature maps.\nPlease see our project page for more qualitative results.", "description": "This ablation study compares the effectiveness of using different feature maps from the U-Net in a video diffusion model for trajectory control in image-to-video generation.  Using features from the original self-attention, temporal-attention layers, or upsampling blocks resulted in poor trajectory following due to a lack of semantic alignment across video frames.  Only when using the modified self-attention features (as described in the paper) did the model generate videos that accurately followed the specified object trajectories. This highlights the importance of using semantically aligned features for successful trajectory control. For a visual comparison of the results, refer to the project website.", "section": "3.2 EXTRACTING SEMANTIC VIDEO LAYOUT"}, {"figure_path": "https://arxiv.org/html/2411.04989/extracted/5985497/imgs/figure_time_labeled.jpg", "caption": "Figure 14: Ablation on U-Net layer to extract feature maps. Consistent with the quantitative results in Fig.\u00a06, feature maps extracted from the middle resolution level are most useful for trajectory guidance. Optimizing on other feature maps may generate unrealistic videos with low motion fidelity.", "description": "This ablation study investigates the impact of using feature maps from different U-Net layers in the optimization process for trajectory control in video generation.  The results show that feature maps from the middle resolution level of the U-Net's upsampling path are most effective for guiding the generation of realistic videos with accurate object motion. Using feature maps from other layers (bottom or top resolution levels) leads to videos that are less realistic and exhibit noticeably poor motion fidelity.", "section": "4.4 ABLATION STUDIES"}, {"figure_path": "https://arxiv.org/html/2411.04989/x7.png", "caption": "Figure 15: Visual comparison of different denoising timesteps.\nHere we show the last frame of the generated video. Optimizing latent at later denoising process leads to severe artifacts.", "description": "This figure shows a visual comparison of videos generated by optimizing latent representations at different denoising timesteps during the image-to-video generation process. Each column represents a different starting timestep for optimization (50, 40, 30, 20, 10), with 50 being the noisiest. The last frame of each generated video is displayed. The results demonstrate that optimizing the latent at later timesteps (i.e., those closer to the original image) leads to a degradation in the quality of the generated video and the introduction of severe artifacts. Conversely, optimizing earlier in the process results in visually cleaner frames.", "section": "4.4 ABLATION STUDIES"}]