[{"figure_path": "https://arxiv.org/html/2503.04369/x1.png", "caption": "Figure 1: Proportions of translations exhibiting translationese errors. All LLMs adopt direct translation prompts, with the exception of GPT-3.5 and GPT-4, which incorporate supplementary prompts to facilitate more natural translations. Both \u201cSpecified\u201d and \u201cPolishing\u201d prompts have identical requirements; however, the \u2018Polishing\u2019 prompt specifically instructs LLMs to refine their generated translations.", "description": "This figure shows the percentage of translations containing translationese errors produced by various large language models (LLMs).  All LLMs were initially tested using a standard translation prompt.  However, GPT-3.5 and GPT-4 were also tested with two additional prompts: a \"Specified\" prompt that encouraged natural language, and a \"Polishing\" prompt that asked the models to refine their translations. The \"Polishing\" prompt led to significantly fewer translationese errors across both GPT-3.5 and GPT-4. The figure visually compares the results across all LLMs and prompt types, illustrating the effectiveness of the refinement process.", "section": "Translationese in LLM Translation"}, {"figure_path": "https://arxiv.org/html/2503.04369/x2.png", "caption": "Figure 2: Correlation between the human-annotated translation span ratio (TSR) and LLM-generated perplexity.", "description": "This figure displays the correlation between the translation span ratio (TSR), representing the proportion of a translation exhibiting translationese errors as determined by human annotators, and the perplexity scores generated by LLMs for the same translations.  Higher TSR values indicate more unnatural translations, while higher perplexity values generally suggest less fluent and natural language. The correlation is shown separately for English-to-Chinese and German-to-English translations, providing insights into the relationship between human judgment of naturalness and LLM-predicted perplexity as a measure of translation quality.", "section": "Tracing Translationese in Supervised Data"}, {"figure_path": "https://arxiv.org/html/2503.04369/x3.png", "caption": "Figure 3: \nProportions of supervised training instances exhibiting different levels of translationese errors (TSR).", "description": "This figure shows the percentage of training examples in supervised fine-tuning datasets that contain varying degrees of translationese errors, as measured by the Translationese Span Ratio (TSR).  A higher TSR indicates a greater proportion of the text exhibiting translationese. The figure visually represents the prevalence of translationese in the training data for both English-to-Chinese and German-to-English translation tasks.", "section": "Tracing Translationese in Supervised Training Data"}, {"figure_path": "https://arxiv.org/html/2503.04369/x4.png", "caption": "Figure 4: \nComparison of naturalness between inference-time (Post-Polishing) and training-time polishing (Polished).", "description": "Figure 4 presents a bar chart comparing the naturalness of translations generated using two different approaches: inference-time polishing (post-polishing) and training-time polishing (polished).  The chart visualizes the perplexity scores achieved by Llama-3.1-8B and Qwen-2.5-7B models trained with the two methods for both English-to-Chinese and German-to-English translations. Lower perplexity indicates better translation naturalness. By comparing the perplexity for both approaches, the figure illustrates the effectiveness of incorporating polishing during the training phase versus only during inference on improving the translation quality and reducing translationese.", "section": "Mitigating Translationese from Supervised Training"}, {"figure_path": "https://arxiv.org/html/2503.04369/x5.png", "caption": "Figure 5: Translation naturalness and quality w.r.t. filtered training samples.", "description": "This figure displays the impact of filtering out the least natural training samples on the quality and naturalness of machine translation.  The x-axis represents the proportion of filtered training instances, while the y-axis shows two metrics: perplexity (lower is better, indicating more natural language) and COMET score (higher is better, indicating better overall translation quality). The chart shows that filtering a moderate amount of the least natural training data improves both perplexity and COMET scores. However, excessive filtering negatively affects both metrics.", "section": "5.4 Filtering Unnatural Training Instances"}, {"figure_path": "https://arxiv.org/html/2503.04369/x6.png", "caption": "Figure 6: Annotation platform demonstration (English-Chinese and German-English).", "description": "This figure shows screenshots of the annotation platform used in the study. The platform, built using Label Studio, allows expert translators to annotate text spans exhibiting translationese errors in machine-translated documents.  The examples shown are for English-Chinese and German-English translations.  The interface displays the original text, machine translation, and tools for highlighting specific segments and categorizing the type of translationese error (unnatural sentence flow or unnatural phrase flow).", "section": "3 Translationese in LLM Translation"}]