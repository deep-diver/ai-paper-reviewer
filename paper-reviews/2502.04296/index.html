<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><title>Learning Real-World Action-Video Dynamics with Heterogeneous Masked Autoregression &#183; HF Daily Paper Reviews by AI</title>
<meta name=title content="Learning Real-World Action-Video Dynamics with Heterogeneous Masked Autoregression &#183; HF Daily Paper Reviews by AI"><meta name=description content="HMA: a novel approach for generating high-quality robotic videos 15x faster, enabling real-time policy evaluation and data augmentation for scaling robot learning."><meta name=keywords content="AI Applications,Robotics,üè¢ MIT,"><link rel=canonical href=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04296/><link type=text/css rel=stylesheet href=/ai-paper-reviewer/css/main.bundle.min.595affd4445a931ea6d6e3a5a3c709930fa52a60be10b21c6f81fdb8fecaacea33aacedf80cdc88be45f189be14ed4ce53ea74a1e1406fad9cbf90c5ed409173.css integrity="sha512-WVr/1ERakx6m1uOlo8cJkw+lKmC+ELIcb4H9uP7KrOozqs7fgM3Ii+RfGJvhTtTOU+p0oeFAb62cv5DF7UCRcw=="><script type=text/javascript src=/ai-paper-reviewer/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script><script defer type=text/javascript id=script-bundle src=/ai-paper-reviewer/js/main.bundle.min.efbf3b6b987689fffaf2d7b73173d2690c0279a04d444b0537a77d7f4ff6e6d493445400cb0cf56bc0f0f123e19f15394e63cae34e67f069bd013dd5c73df56e.js integrity="sha512-7787a5h2if/68te3MXPSaQwCeaBNREsFN6d9f0/25tSTRFQAywz1a8Dw8SPhnxU5TmPK405n8Gm9AT3Vxz31bg==" data-copy data-copied></script><script src=/ai-paper-reviewer/lib/zoom/zoom.min.37d2094687372da3f7343a221a470f6b8806f7891aa46a5a03966af7f0ebd38b9fe536cb154e6ad28f006d184b294525a7c4054b6bbb4be62d8b453b42db99bd.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ=="></script><link rel=apple-touch-icon sizes=180x180 href=/ai-paper-reviewer/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/ai-paper-reviewer/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/ai-paper-reviewer/favicon-16x16.png><link rel=manifest href=/ai-paper-reviewer/site.webmanifest><meta property="og:url" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04296/"><meta property="og:site_name" content="HF Daily Paper Reviews by AI"><meta property="og:title" content="Learning Real-World Action-Video Dynamics with Heterogeneous Masked Autoregression"><meta property="og:description" content="HMA: a novel approach for generating high-quality robotic videos 15x faster, enabling real-time policy evaluation and data augmentation for scaling robot learning."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="paper-reviews"><meta property="article:published_time" content="2025-02-06T00:00:00+00:00"><meta property="article:modified_time" content="2025-02-06T00:00:00+00:00"><meta property="article:tag" content="AI Applications"><meta property="article:tag" content="Robotics"><meta property="article:tag" content="üè¢ MIT"><meta property="og:image" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04296/cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04296/cover.png"><meta name=twitter:title content="Learning Real-World Action-Video Dynamics with Heterogeneous Masked Autoregression"><meta name=twitter:description content="HMA: a novel approach for generating high-quality robotic videos 15x faster, enabling real-time policy evaluation and data augmentation for scaling robot learning."><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"Paper Reviews by AI","name":"Learning Real-World Action-Video Dynamics with Heterogeneous Masked Autoregression","headline":"Learning Real-World Action-Video Dynamics with Heterogeneous Masked Autoregression","abstract":"HMA: a novel approach for generating high-quality robotic videos 15x faster, enabling real-time policy evaluation and data augmentation for scaling robot learning.","inLanguage":"en","url":"https:\/\/deep-diver.github.io\/ai-paper-reviewer\/paper-reviews\/2502.04296\/","author":{"@type":"Person","name":"Hugging Face Daily Papers"},"copyrightYear":"2025","dateCreated":"2025-02-06T00:00:00\u002b00:00","datePublished":"2025-02-06T00:00:00\u002b00:00","dateModified":"2025-02-06T00:00:00\u002b00:00","keywords":["AI Applications","Robotics","üè¢ MIT"],"mainEntityOfPage":"true","wordCount":"3466"}]</script><meta name=author content="Hugging Face Daily Papers"><link href=https://github.com/deep-diver/paper-reviewer/ rel=me><link href=https://twitter.com/algo_diver/ rel=me><script src=/ai-paper-reviewer/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj+KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script><script defer src=/ai-paper-reviewer/lib/typeit/typeit.umd.1b3200cb448f5cd1f548f2781452643d3511a43584b377b82c03a58055da4fdb7bc8f6c6c2ce846480c7677ff25bfd0d75f15823c09443ab18e0fd2cad792587.js integrity="sha512-GzIAy0SPXNH1SPJ4FFJkPTURpDWEs3e4LAOlgFXaT9t7yPbGws6EZIDHZ3/yW/0NdfFYI8CUQ6sY4P0srXklhw=="></script><script defer src=/ai-paper-reviewer/lib/packery/packery.pkgd.min.js integrity></script><script type=text/javascript src=/ai-paper-reviewer/js/shortcodes/gallery.min.9b4cb28f931ed922c26fb9b2510c2debb370f6a63305050c2af81740b2919883715e24efbbdf3a081496718ec751df3a72729d4d0bc71d6071297563a97ce1ee.js integrity="sha512-m0yyj5Me2SLCb7myUQwt67Nw9qYzBQUMKvgXQLKRmINxXiTvu986CBSWcY7HUd86cnKdTQvHHWBxKXVjqXzh7g=="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-KX0S6Q55Y7"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-KX0S6Q55Y7")</script><meta name=theme-color><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-app.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-firestore.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-auth.js></script><script>const firebaseConfig={apiKey:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",authDomain:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",projectId:"neurips2024-f3065",storageBucket:"neurips2024-f3065.firebasestorage.app",messagingSenderId:"982475958898",appId:"1:982475958898:web:2147e5d7753d6ac091f0eb",measurementId:"G-YQ46HXQ9JS"};var app=firebase.initializeApp(firebaseConfig),db=firebase.firestore(),auth=firebase.auth()</script></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a></div><div class=min-h-[148px]></div><div class="fixed inset-x-0 pl-[24px] pr-[24px]" style=z-index:100><div id=menu-blur class="absolute opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl shadow-2xl"></div><div class="relative max-w-[64rem] ml-auto mr-auto"><div style=padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start space-x-3"><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/ai-paper-reviewer/ class="text-base font-medium text-gray-500 hover:text-gray-900">HF Daily Paper Reviews by AI</a></nav><nav class="hidden md:flex items-center space-x-5 md:ml-12 h-12"><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title="About This Project">About</p></a><a href=/ai-paper-reviewer/2025-02-12/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title=2025-02-12s>2025-02-12</p></a><a href=/ai-paper-reviewer/2025-02-13/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title=2025-02-13s>2025-02-13</p></a><a href=/ai-paper-reviewer/2025-02-14/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title=2025-02-14s>2025-02-14</p></a><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title="Paper Reviews by AI">Archive</p></a><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title=Tags>Tags</p></a><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></span><p class="text-base font-medium" title></p></a><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span><p class="text-base font-medium" title></p></a><button id=search-button aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button><div class="ltr:mr-14 rtl:ml-14 flex items-center"><button id=appearance-switcher aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></nav><div class="flex md:hidden items-center space-x-5 md:ml-12 h-12"><span></span>
<button id=search-button-mobile aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></button>
<button id=appearance-switcher-mobile aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400" style=margin-right:5px><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div><div class="-my-2 -mr-2 md:hidden"><label id=menu-button class=block><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper style=padding-top:5px class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li id=menu-close-button><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title="About This Project">About</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-02-12/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title=2025-02-12s>2025-02-12</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-02-13/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title=2025-02-13s>2025-02-13</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-02-14/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title=2025-02-14s>2025-02-14</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title="Paper Reviews by AI">Archive</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title=Tags>Tags</p></a></li><li class=mt-1><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li><li class=mt-1><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li></ul></div></label></div></div></div></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("menu-blur");n.style.opacity=t/300})</script><div class="relative flex flex-col grow"><main id=main-content class=grow><article><div id=hero class="h-[150px] md:h-[200px]"></div><div class="fixed inset-x-0 top-0 h-[800px] single_hero_background nozoom" style=background-image:url(/ai-paper-reviewer/paper-reviews/2502.04296/cover_hu_f7b1da986eb8d8a5.png)><div class="absolute inset-0 bg-gradient-to-t from-neutral dark:from-neutral-800 to-transparent mix-blend-normal"></div><div class="absolute inset-0 opacity-60 bg-gradient-to-t from-neutral dark:from-neutral-800 to-neutral-100 dark:to-neutral-800 mix-blend-normal"></div></div><div id=background-blur class="fixed opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl"></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("background-blur");n.style.opacity=t/300})</script><header id=single_header class="mt-5 max-w-prose"><ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden"><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/>HF Daily Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class=inline><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/>Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/2502.04296/>Learning Real-World Action-Video Dynamics with Heterogeneous Masked Autoregression</a><span class="px-1 text-primary-500">/</span></li></ol><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">Learning Real-World Action-Video Dynamics with Heterogeneous Masked Autoregression</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime=2025-02-06T00:00:00+00:00>6 February 2025</time><span class="px-2 text-primary-500">&#183;</span><span>3466 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">17 mins</span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=views_paper-reviews/2502.04296/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=views>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 576 512"><path fill="currentcolor" d="M288 32c-80.8.0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7.0 24.6C17.3 304 48.6 356 95.4 399.4 142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1 3.3-7.9 3.3-16.7.0-24.6-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144 64.5-144 144-144 144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64-11.5.0-22.3-3-31.6-8.4-.2 2.8-.4 5.5-.4 8.4.0 53 43 96 96 96s96-43 96-96-43-96-96-96c-2.8.0-5.6.1-8.4.4 5.3 9.3 8.4 20.1 8.4 31.6z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=likes_paper-reviews/2502.04296/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=likes>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<button id=button_likes class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400" onclick=process_article()>
<span id=button_likes_heart style=display:none class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span><span id=button_likes_emtpty_heart class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M244 84l11.1 12 12-11.98C300.6 51.37 347 36.51 392.6 44.1 461.5 55.58 512 115.2 512 185.1V190.9c0 41.5-17.2 81.2-47.6 109.5L283.7 469.1c-7.5 7-17.4 10.9-27.7 10.9S235.8 476.1 228.3 469.1L47.59 300.4C17.23 272.1.0 232.4.0 190.9V185.1c0-69.9 50.52-129.52 119.4-141 44.7-7.59 92 7.27 124.6 39.9C243.1 84 244 84.01 244 84zm11.1 79.9-45-46.8c-21.7-20.82-52.5-30.7-82.8-25.66C81.55 99.07 48 138.7 48 185.1V190.9c0 28.2 11.71 55.2 32.34 74.4L256 429.3l175.7-164c20.6-19.2 32.3-46.2 32.3-74.4V185.1c0-46.4-33.6-86.03-79.3-93.66C354.4 86.4 323.6 96.28 301.9 117.1l-46.8 46.8z"/></svg>
</span></span><span id=button_likes_text>&nbsp;Like</span></button></span></div><div class="flex flex-row flex-wrap items-center"><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/ai-generated/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">AI Generated
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/-daily-papers/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">ü§ó Daily Papers
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/ai-applications/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">AI Applications
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/robotics/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Robotics
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/-mit/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">üè¢ MIT</span></span></span></div></div><div class="flex author"><img class="!mt-0 !mb-0 h-24 w-24 rounded-full ltr:mr-4 rtl:ml-4" width=96 height=96 alt="Hugging Face Daily Papers" src=/ai-paper-reviewer/img/avatar_hu_97e7d424fadd1c26.png><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">Author</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">Hugging Face Daily Papers</div><div class="text-sm text-neutral-700 dark:text-neutral-400">I am AI, and I review papers on HF Daily Papers</div><div class="text-2xl sm:text-lg"><div class="flex flex-wrap text-neutral-400 dark:text-neutral-500"><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://github.com/deep-diver/paper-reviewer/ target=_blank aria-label=Github rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://twitter.com/algo_diver/ target=_blank aria-label=Twitter rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span></a></div></div></div></div><div class=mb-5></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first lg:ml-auto px-0 lg:order-last ltr:lg:pl-8 rtl:lg:pr-8"><div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-[140px]"><details open id=TOCView class="toc-right mt-0 overflow-y-scroll overscroll-contain scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600 rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 hidden lg:block"><summary class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="min-w-[220px] py-2 border-dotted ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#hma-a-novel-approach>HMA: A Novel Approach</a></li><li><a href=#action-heterogeneity>Action Heterogeneity</a></li><li><a href=#masked-autoregression>Masked Autoregression</a></li><li><a href=#scaling-behaviors>Scaling Behaviors</a></li><li><a href=#future-of-hma>Future of HMA</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><details class="toc-inside mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 lg:hidden"><summary class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#hma-a-novel-approach>HMA: A Novel Approach</a></li><li><a href=#action-heterogeneity>Action Heterogeneity</a></li><li><a href=#masked-autoregression>Masked Autoregression</a></li><li><a href=#scaling-behaviors>Scaling Behaviors</a></li><li><a href=#future-of-hma>Future of HMA</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><script>var margin=200,marginError=50;(function(){var t=$(window),e=$("#TOCView"),s=e.height();function n(){var n=t.height()-margin;s>=n?(e.css("overflow-y","scroll"),e.css("max-height",n+marginError+"px")):(e.css("overflow-y","hidden"),e.css("max-height","9999999px"))}t.on("resize",n),$(document).ready(n)})()</script></div></div><div class="min-w-0 min-h-0 max-w-fit"><div class="article-content max-w-prose mb-20"><br><div class="flex flex-row flex-wrap items-center space-x-2"><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 48 48" fill="none"><rect width="48" height="48" fill="#fff" fill-opacity=".01"/><path d="M18 43V22c0-3.3137 2.6863-6 6-6s6 2.6863 6 6V43" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M12 40V22c0-6.6274 5.3726-12 12-12s12 5.3726 12 12V40" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M6 35V22C6 12.0589 14.0589 4 24 4s18 8.0589 18 18V35" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 44V31" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 24.625v-2.75" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/></svg>
</span></span><span>2502.04296</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg fill="#000" height="800" width="800" id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 511.999 511.999"><g><g><path d="M421.578 190.264l-99.847-99.847c-2.439-2.439-6.391-2.439-8.829.0L82.824 320.495c-2.439 2.439-2.439 6.392.0 8.829l99.847 99.847c2.439 2.439 6.391 2.439 8.829.0l230.078-230.078C424.017 196.655 424.017 192.703 421.578 190.264z"/></g></g><g><g><path d="M506.511 87.672 424.323 5.484c-7.308-7.31-19.175-7.315-26.488.0L348.219 55.1c-2.439 2.439-2.439 6.391.0 8.829l99.847 99.847c2.439 2.437 6.391 2.437 8.829.0l49.616-49.616C513.826 106.847 513.826 94.987 506.511 87.672z"/></g></g><g><g><path d="M508.133 491.11c-1.054-9.556-9.489-16.599-19.104-16.599H111.633l36.058-15.163c4.088-1.719 5.131-7.034 1.994-10.17l-86.854-86.854c-3.137-3.135-8.451-2.094-10.17 1.994C52.224 365.359 2.052 484.66 1.627 485.707c-5.815 13.208 4.855 27.01 18.107 26.263H489.52C500.566 511.97 509.379 502.408 508.133 491.11z"/></g></g></svg>
</span></span><span>Lirui Wang et el.</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span>ü§ó 2025-02-07</span></span></span></div></div><p><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://arxiv.org/abs/2502.04296 target=_self role=button>‚Üó arXiv
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://huggingface.co/papers/2502.04296 target=_self role=button>‚Üó Hugging Face</a></p><audio controls><source src=https://ai-paper-reviewer.com/2502.04296/podcast.wav type=audio/wav>Your browser does not support the audio element.</audio><h3 class="relative group">TL;DR<div id=tldr class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#tldr aria-label=Anchor>#</a></span></h3><div class="lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl"><p>Scaling robot learning is hindered by the need for massive high-quality datasets and real-time, high-fidelity evaluation. Current generative models often struggle with computational efficiency or fail to handle the diverse settings of real-world robotics. Furthermore, existing methods for video generation can be computationally expensive, making real-time applications challenging.</p><p>The paper introduces Heterogeneous Masked Autoregression (HMA), a new approach that tackles these issues. <strong>HMA uses heterogeneous pre-training</strong> from diverse robotic embodiments, domains, and tasks to learn a generalizable action-video dynamic model. <strong>Masked autoregression</strong> enables efficient video prediction, significantly improving speed and visual fidelity. Experiments demonstrate that HMA achieves better visual fidelity and controllability than state-of-the-art models with a <strong>15x speed improvement</strong>, making real-time applications in robotics possible. HMA can generate synthetic data and serve as a video simulator for efficient policy evaluation.</p></div><h4 class="relative group">Key Takeaways<div id=key-takeaways class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#key-takeaways aria-label=Anchor>#</a></span></h4><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-8a60d2ed211c787a789e716552e09ca7></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-8a60d2ed211c787a789e716552e09ca7",{strings:[" HMA uses masked autoregression for efficient and high-fidelity video generation. "],speed:10,lifeLike:!0,startDelay:0,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-1a9537d19a708896f4100823f2b6603c></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-1a9537d19a708896f4100823f2b6603c",{strings:[" HMA handles action heterogeneity across various robotic platforms through heterogeneous pre-training. "],speed:10,lifeLike:!0,startDelay:1e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-218854279367620f5adf89d364eb58b9></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-218854279367620f5adf89d364eb58b9",{strings:[" HMA achieves 15x faster inference speed than previous methods, enabling real-time simulation and evaluation. "],speed:10,lifeLike:!0,startDelay:2e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><h4 class="relative group">Why does it matter?<div id=why-does-it-matter class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#why-does-it-matter aria-label=Anchor>#</a></span></h4><p>This paper is important because it addresses a critical bottleneck in robotics: the need for massive, high-quality datasets and real-time evaluation. The proposed method, <strong>Heterogeneous Masked Autoregression (HMA)</strong>, offers a novel and efficient solution by leveraging masked autoregression and heterogeneous pre-training. This opens new avenues for developing more general and efficient robotic video generation models, which has far-reaching implications for advancing robotics research. The improved efficiency, <strong>15x faster than previous methods</strong>, makes real-time applications feasible.</p><hr><h4 class="relative group">Visual Insights<div id=visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#visual-insights aria-label=Anchor>#</a></span></h4><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.04296/x1.png alt></figure></p><blockquote><p>üîº The figure illustrates the architecture and workflow of the Heterogeneous Masked Autoregression (HMA) model. HMA leverages a diverse dataset of over 3 million video trajectories from 40 different robot embodiments to learn a comprehensive action-video dynamics model. The pre-training phase employs masked autoregression to predict the next set of tokens (visual and action), effectively capturing the complex interactions between robot actions and resulting video observations. Following pre-training, this versatile model finds applications in various robotics tasks: generating realistic video simulations, evaluating robot policies (by simulating the consequences of different actions), creating synthetic training data, and even acting as a direct imitation policy.</p><details><summary>read the caption</summary>Figure 1: Action-Video Dynamics Model from Heterogeneous Robot Interactions. HMA utilizes heterogeneous datasets comprising over 3 million trajectories (videos) from 40 distinct embodiments to pre-train a full dynamics model with next-set-of-token predictions using masked autoregression. After pre-training, the resulting action-video dynamics model is versatile, supporting applications such as video simulation, policy evaluation, synthetic data generation, and direct adoption as an imitation policy.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_centering ltx_align_middle" id=S5.T1.1><tr class=ltx_tr id=S5.T1.1.1><td class="ltx_td ltx_align_left" id=S5.T1.1.1.2 style="padding:.8pt 5.5pt"><span class="ltx_text ltx_font_bold" id=S5.T1.1.1.2.1 style=font-size:80%>Model</span></td><td class="ltx_td ltx_align_center" id=S5.T1.1.1.3 style="padding:.8pt 5.5pt"><span class="ltx_text ltx_font_bold" id=S5.T1.1.1.3.1 style=font-size:80%>Method</span></td><td class="ltx_td ltx_align_center" id=S5.T1.1.1.4 style="padding:.8pt 5.5pt"><span class="ltx_text ltx_font_bold" id=S5.T1.1.1.4.1 style=font-size:80%>Parameters</span><span class=ltx_text id=S5.T1.1.1.4.2 style=font-size:80%> (M)</span></td><td class="ltx_td ltx_align_left" id=S5.T1.1.1.1 style="padding:.8pt 5.5pt"><span class="ltx_text ltx_font_bold" id=S5.T1.1.1.1.1 style=font-size:80%>FPS<math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T1.1.1.1.1.m1.1"><semantics id="S5.T1.1.1.1.1.m1.1a"><mo id="S5.T1.1.1.1.1.m1.1.1" stretchy="false" xref="S5.T1.1.1.1.1.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S5.T1.1.1.1.1.m1.1b"><ci id="S5.T1.1.1.1.1.m1.1.1.cmml" xref="S5.T1.1.1.1.1.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T1.1.1.1.1.m1.1d">‚Üë</annotation></semantics></math></span></td></tr><tr class=ltx_tr id=S5.T1.1.2><td class="ltx_td ltx_align_left ltx_border_t" id=S5.T1.1.2.1 style="padding:.8pt 5.5pt"><span class=ltx_text id=S5.T1.1.2.1.1 style=font-size:80%>IRASim-XL</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T1.1.2.2 style="padding:.8pt 5.5pt"><span class=ltx_text id=S5.T1.1.2.2.1 style=font-size:80%>DiT</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T1.1.2.3 style="padding:.8pt 5.5pt"><span class=ltx_text id=S5.T1.1.2.3.1 style=font-size:80%>679</span></td><td class="ltx_td ltx_align_left ltx_border_t" id=S5.T1.1.2.4 style="padding:.8pt 5.5pt"><span class=ltx_text id=S5.T1.1.2.4.1 style=font-size:80%>0.28</span></td></tr><tr class=ltx_tr id=S5.T1.1.3><td class="ltx_td ltx_align_left" id=S5.T1.1.3.1 style="padding:.8pt 5.5pt"><span class=ltx_text id=S5.T1.1.3.1.1 style=font-size:80%>IRASim-XL, amortized</span></td><td class="ltx_td ltx_align_center" id=S5.T1.1.3.2 style="padding:.8pt 5.5pt"><span class=ltx_text id=S5.T1.1.3.2.1 style=font-size:80%>DiT</span></td><td class="ltx_td ltx_align_center" id=S5.T1.1.3.3 style="padding:.8pt 5.5pt"><span class=ltx_text id=S5.T1.1.3.3.1 style=font-size:80%>679</span></td><td class="ltx_td ltx_align_left" id=S5.T1.1.3.4 style="padding:.8pt 5.5pt"><span class=ltx_text id=S5.T1.1.3.4.1 style=font-size:80%>0.58</span></td></tr><tr class=ltx_tr id=S5.T1.1.4><td class="ltx_td ltx_align_left ltx_border_t" id=S5.T1.1.4.1 style="padding:.8pt 5.5pt"><span class="ltx_text ltx_font_typewriter" id=S5.T1.1.4.1.1 style=font-size:80%>HMA</span><span class=ltx_text id=S5.T1.1.4.1.2 style=font-size:80%>-Base</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T1.1.4.2 style="padding:.8pt 5.5pt"><span class=ltx_text id=S5.T1.1.4.2.1 style=font-size:80%>MaskGIT</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T1.1.4.3 style="padding:.8pt 5.5pt"><span class=ltx_text id=S5.T1.1.4.3.1 style=font-size:80%>44</span></td><td class="ltx_td ltx_align_left ltx_border_t" id=S5.T1.1.4.4 style="padding:.8pt 5.5pt"><span class=ltx_text id=S5.T1.1.4.4.1 style=font-size:80%>22.72</span></td></tr><tr class=ltx_tr id=S5.T1.1.5><td class="ltx_td ltx_align_left" id=S5.T1.1.5.1 style="padding:.8pt 5.5pt"><span class="ltx_text ltx_font_typewriter" id=S5.T1.1.5.1.1 style=font-size:80%>HMA</span><span class=ltx_text id=S5.T1.1.5.1.2 style=font-size:80%>-XL</span></td><td class="ltx_td ltx_align_center" id=S5.T1.1.5.2 style="padding:.8pt 5.5pt"><span class=ltx_text id=S5.T1.1.5.2.1 style=font-size:80%>MaskGIT</span></td><td class="ltx_td ltx_align_center" id=S5.T1.1.5.3 style="padding:.8pt 5.5pt"><span class=ltx_text id=S5.T1.1.5.3.1 style=font-size:80%>679</span></td><td class="ltx_td ltx_align_left" id=S5.T1.1.5.4 style="padding:.8pt 5.5pt"><span class=ltx_text id=S5.T1.1.5.4.1 style=font-size:80%>4.38</span></td></tr><tr class=ltx_tr id=S5.T1.1.6><td class="ltx_td ltx_align_left" id=S5.T1.1.6.1 style="padding:.8pt 5.5pt"><span class="ltx_text ltx_font_typewriter" id=S5.T1.1.6.1.1 style=font-size:80%>HMA</span><span class=ltx_text id=S5.T1.1.6.1.2 style=font-size:80%>-Base</span></td><td class="ltx_td ltx_align_center" id=S5.T1.1.6.2 style="padding:.8pt 5.5pt"><span class=ltx_text id=S5.T1.1.6.2.1 style=font-size:80%>MAR</span></td><td class="ltx_td ltx_align_center" id=S5.T1.1.6.3 style="padding:.8pt 5.5pt"><span class=ltx_text id=S5.T1.1.6.3.1 style=font-size:80%>96</span></td><td class="ltx_td ltx_align_left" id=S5.T1.1.6.4 style="padding:.8pt 5.5pt"><span class=ltx_text id=S5.T1.1.6.4.1 style=font-size:80%>4.44</span></td></tr><tr class=ltx_tr id=S5.T1.1.7><td class="ltx_td ltx_align_left ltx_border_b" id=S5.T1.1.7.1 style="padding:.8pt 5.5pt"><span class="ltx_text ltx_font_typewriter" id=S5.T1.1.7.1.1 style=font-size:80%>HMA</span><span class=ltx_text id=S5.T1.1.7.1.2 style=font-size:80%>-XL</span></td><td class="ltx_td ltx_align_center ltx_border_b" id=S5.T1.1.7.2 style="padding:.8pt 5.5pt"><span class=ltx_text id=S5.T1.1.7.2.1 style=font-size:80%>MAR</span></td><td class="ltx_td ltx_align_center ltx_border_b" id=S5.T1.1.7.3 style="padding:.8pt 5.5pt"><span class=ltx_text id=S5.T1.1.7.3.1 style=font-size:80%>741</span></td><td class="ltx_td ltx_align_left ltx_border_b" id=S5.T1.1.7.4 style="padding:.8pt 5.5pt"><span class=ltx_text id=S5.T1.1.7.4.1 style=font-size:80%>2.01</span></td></tr></table></table></figure><blockquote><p>üîº This table compares the inference speed of different models, highlighting the significant speed advantage of the proposed HMA model. It breaks down the inference speed (frames per second, FPS) for various model sizes (Base and XL) across different methods (MaskGIT and MAR). The table shows that HMA is substantially faster than a comparable model from the literature (IRASim-XL), particularly when using the MaskGIT method. The differences in speed are attributed to HMA&rsquo;s architecture, which avoids repeated passes through the Transformer during generation, unlike diffusion-based models. The table also notes the parameter counts for each model and that all results were obtained using the same hardware configuration (RTX-4080 GPU).</p><details><summary>read the caption</summary>Table 1: Inference Speed. We measure the per-frame inference speed across 16 frames for various model sizes. The Base model has a model size of around 30M and the XL model has a similar model size as IRASim-XL. The models all use 32-block transformers where the base model has dimensions 256 and the XL models have dimensions 768. Our fastest model of the same size is more than 15√ó\times√ó faster than [60] because HMA does not pass through the full Transformer multiple times (with diffusion modeling) to generate each frame. MAR incurs more parameters than MaskGIT [8] because of the diffusion heads [27]. The amortized result for [60] comes from averaging over multiple frames. The speeds are all measured on the same hardware setup with RTX-4080 GPU.</details></blockquote><h3 class="relative group">In-depth insights<div id=in-depth-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#in-depth-insights aria-label=Anchor>#</a></span></h3><h4 class="relative group">HMA: A Novel Approach<div id=hma-a-novel-approach class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#hma-a-novel-approach aria-label=Anchor>#</a></span></h4><p>HMA, as a novel approach, presents a significant advancement in robotic video modeling. Its core innovation lies in <strong>heterogeneous masked autoregression</strong>, enabling the model to learn from diverse robotic datasets despite differences in action spaces and frequencies. This addresses a major bottleneck in scaling robot learning by generating high-fidelity, real-time video simulations from heterogeneous data. The use of masked autoregression offers a significant speed advantage over diffusion-based methods, making real-time interaction feasible. <strong>Post-training applications</strong> are particularly noteworthy, demonstrating HMA&rsquo;s versatility in policy evaluation, data generation, and direct policy implementation. While challenges remain in perfect controllability and handling extreme complexities, <strong>HMA&rsquo;s scalability and efficiency</strong> showcase its potential to transform robot learning and simulation.</p><h4 class="relative group">Action Heterogeneity<div id=action-heterogeneity class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#action-heterogeneity aria-label=Anchor>#</a></span></h4><p>The concept of &lsquo;Action Heterogeneity&rsquo; in robotics highlights the <strong>diverse nature of actions</strong> across different robots. Robots vary significantly in their physical capabilities, control mechanisms, and operational domains. This diversity translates into <strong>heterogeneous action spaces</strong>, meaning robots may have different numbers of degrees of freedom, varying action frequencies, and use different control signals. Addressing this heterogeneity is crucial for building general-purpose world models and policies. <strong>A unified framework</strong> is needed to represent and handle the diverse action spaces, enabling the learning of models capable of simulating and generating actions for various robotic platforms. This involves integrating action information into a shared latent space, allowing for effective transfer learning and seamless interaction across embodiments. <strong>Modularized architectures</strong> are key, such as using separate encoder and decoder modules per robot while sharing a central processing unit that generalizes the dynamics. This approach maximizes adaptability while minimizing training data and computational overhead for each new robot.</p><h4 class="relative group">Masked Autoregression<div id=masked-autoregression class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#masked-autoregression aria-label=Anchor>#</a></span></h4><p>Masked autoregression, as applied in this research, is a powerful technique for modeling action-video dynamics. <strong>Its core strength lies in efficiently handling the complexities of sequential data</strong>, such as those found in robotics videos where actions and observations are interlinked. By masking parts of the input sequence during training and predicting the masked portions, the model learns to capture temporal dependencies and generate high-quality predictions. This approach is <strong>particularly well-suited for real-time applications</strong>, where computational efficiency is paramount. Furthermore, the use of masked autoregression allows for the integration of heterogeneous data, handling variations in action spaces and frequencies from different robots. This leads to more robust and generalizable models. <strong>The combination of masked autoregression with heterogeneous data greatly enhances the ability to build efficient and versatile robotic video simulators</strong>, supporting a wide range of applications like policy evaluation and synthetic data generation.</p><h4 class="relative group">Scaling Behaviors<div id=scaling-behaviors class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#scaling-behaviors aria-label=Anchor>#</a></span></h4><p>The section on &ldquo;Scaling Behaviors&rdquo; in the research paper is crucial for assessing the generalizability and practical applicability of the proposed Heterogeneous Masked Autoregression (HMA) model. It investigates how the model&rsquo;s performance changes across various dimensions: <strong>the number of datasets (embodiments), the size of the datasets (number of trajectories), and the model&rsquo;s complexity (number of parameters).</strong> Positive scaling trends across these dimensions indicate that HMA is robust and capable of handling increasingly diverse and large-scale data. The consistent performance gains as the number of datasets and trajectories increase highlight <strong>the model&rsquo;s ability to learn generalizable representations from heterogeneous data</strong>. Furthermore, the improvements with increased model size suggest that scaling up the model&rsquo;s capacity can further enhance its performance, potentially achieving even higher fidelity and controllability. These scaling experiments demonstrate not just the performance of HMA on a particular dataset, but also its potential for real-world applications where data diversity and scale are significant factors. <strong>The results provide strong evidence that the HMA model is not overfitting to specific datasets</strong>, but rather learning robust and transferable representations of action-video dynamics.</p><h4 class="relative group">Future of HMA<div id=future-of-hma class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#future-of-hma aria-label=Anchor>#</a></span></h4><p>The future of Heterogeneous Masked Autoregression (HMA) appears bright, given its demonstrated success in handling action-video dynamics. <strong>Real-time performance</strong> is a significant achievement, opening doors for direct use in robotics, eliminating the bottleneck of slow simulation. Further research should focus on improving the model&rsquo;s <strong>controllability</strong>, particularly addressing limitations observed with limited training data and complex tasks. <strong>Scaling to even larger datasets</strong> and exploring diverse robotic applications remains key. Addressing limitations in modeling deformable objects and complex physics interactions would broaden the system&rsquo;s applicability. Integrating HMA with advanced planning algorithms and exploring its potential for generating high-fidelity synthetic data for training complex robotic policies are also promising directions. Finally, investigating the use of soft tokens beyond visual generation could significantly enhance the system‚Äôs overall capabilities and efficiency. <strong>Addressing these research opportunities would solidify HMA&rsquo;s position as a leading technology in robotics simulation and learning.</strong></p><h3 class="relative group">More visual insights<div id=more-visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#more-visual-insights aria-label=Anchor>#</a></span></h3><details><summary>More on figures</summary><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.04296/x2.png alt></figure></p><blockquote><p>üîº Figure 2 illustrates the versatility of masked autoregression in modeling robot dynamics. It shows how a single framework can address various robotics problems, such as policy learning (predicting future actions given past observations and actions), forward dynamics (predicting future observations given past observations and actions), passive dynamics (predicting future observations given only past observations), and full dynamics (jointly predicting future observations and actions). This unified approach allows the model to handle different tasks and scenarios in robotics through a common architecture.</p><details><summary>read the caption</summary>Figure 2: Dynamics Model. Masked autoregression in the dynamics model generalizes multiple problem settings including policy learning, forward and passive dynamics, and full dynamics.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.04296/x3.png alt></figure></p><blockquote><p>üîº The figure illustrates the architecture of the Heterogeneous Masked Autoregression (HMA) model. The model processes both video and action sequences from various robotic embodiments. Each embodiment has a dedicated &lsquo;stem&rsquo; (action encoder) and &lsquo;head&rsquo; (action decoder), which map the embodiment-specific action data into a shared latent space. The core of the model is a spatial-temporal transformer (&rsquo;trunk&rsquo;) that processes the shared latent representations to predict both future video frames and actions. The spatial attention mechanism operates bi-directionally on masked and unmasked tokens for both video and action, while the temporal attention mechanism is causal (only considering past information). The modular design allows for easy adaptation to new embodiments by simply training new stem and head components without modifying the trunk.</p><details><summary>read the caption</summary>Figure 3: Network Architecture. The HMA model architecture maps low-level video and action sequences across different embodiments into a shared latent space. For actions, embodiment projectors are activated based on the training sample. The spatial-temporal Transformer produces the output video and action tokens for future frames.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.04296/x4.png alt></figure></p><blockquote><p>üîº This figure demonstrates the capability of the Heterogeneous Masked Autoregression (HMA) model to generate high-quality and diverse videos. The model is trained on a dataset containing videos from various robotic embodiments and heterogeneous action spaces. The left three columns showcase videos that are visually realistic and consistent with typical real-world interactions. In contrast, the right three columns display videos with more unexpected or creative actions, reflecting the model&rsquo;s capacity to produce diverse results. Each group of three images represents consecutive frames from a single video sequence generated by the model.</p><details><summary>read the caption</summary>Figure 4: Pre-trained Video Model Generation. We show that a single unified HMA model can generate realistic (left 3 columns) and diverse (right 3 columns) videos across multiple embodiment datasets with heterogeneous action spaces. Each group shows three generated frames from a single sequence.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.04296/x5.png alt></figure></p><blockquote><p>üîº Figure 5 presents ablation studies on the Heterogeneous Masked Autoregression (HMA) model, specifically examining the effects of different pre-training settings and architectures on video generation performance. Part (a) compares the performance of action-conditioned models (forward and full dynamics) against passive video models, showing that incorporating action information significantly improves both visual fidelity (measured by perplexity) and controllability (measured by APSNR). Part (b) analyzes various action-conditioning architectures within the HMA framework, including modulation, token concatenation, feature addition, and token cross-attention, to determine the optimal design for balancing performance and efficiency. The best performing model configuration is highlighted in purple.</p><details><summary>read the caption</summary>Figure 5: Ablation on Pre-training Settings and Architecture. Under the pre-training setting with VQ tokens, we ablate the video generation performance (visual fidelity measured by perplexity and controllability measured by controllability). (a) We find action-conditioned models outperform passive video models. (b) We compare different action conditioning architectures in the masked autoregression framework. The purple color denotes the best model that we use by default.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.04296/x6.png alt></figure></p><blockquote><p>üîº This figure displays the results of experiments evaluating the scalability of the Heterogeneous Masked Autoregression (HMA) model. Three separate experiments are shown, each exploring a different aspect of scalability: 1. <strong>Scaling the Number of Datasets:</strong> This shows how HMA performs as the number of different robotic datasets used for training increases. More datasets mean more diverse robot embodiments, actions, and environments are included in the training, representing a greater degree of heterogeneity. 2. <strong>Scaling the Number of Trajectories:</strong> This assesses HMA&rsquo;s performance as the total number of training video trajectories increases. More trajectories offer more data to train the model and potentially lead to better performance. 3. <strong>Scaling Model Size:</strong> This experiment examines how HMA behaves as the size and complexity of the model itself increase (number of parameters). For each experiment, two key metrics are shown. Perplexity measures the fidelity of the generated videos (lower perplexity indicates higher fidelity), and ŒîŒî PSNR (Delta PSNR) measures the controllability of the generated videos (lower ŒîŒî PSNR suggests better controllability, indicating that the model&rsquo;s output is more consistent and less affected by random noise). The results in the plots demonstrate that the performance of the HMA model improves with increased dataset diversity, more training data, and larger model size, thus showcasing its strong scaling properties.</p><details><summary>read the caption</summary>Figure 6: Experiments on Scaling Behaviors of HMA. We observe positive trends in the scaling performance of heterogeneous video models across axes including the number of datasets, number of trajectories, and model sizes. The evaluation metrics on fidelity (perplexity) and controllability (ŒîŒî\Deltaroman_ŒîPSNR) are averaged across validation datasets.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.04296/x7.png alt></figure></p><blockquote><p>üîº This figure compares the visual quality of videos generated using different methods: VQ tokens (vector-quantized tokens, a discrete representation) and soft tokens (continuous representation), both within a masked autoregressive framework. The comparison highlights that while diffusion-based models using soft tokens take longer to train (slower convergence), they produce significantly better visual results. This superior quality is evident both qualitatively (by visual inspection) and quantitatively (as measured by the Peak Signal-to-Noise Ratio, or PSNR). The improved visual fidelity from using soft tokens comes at the cost of increased training time.</p><details><summary>read the caption</summary>Figure 7: Qualitative Comparisons Between Tokenizers and Models. Despite longer convergence time, diffusion-based methods (Eq.¬†3) on soft tokens generate better visual quality than on VQ tokens (Eq.¬†2), qualitatively and measured by PSNR.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.04296/x8.png alt></figure></p><blockquote><p>üîº Figure 8 showcases the model&rsquo;s capacity for generating physically realistic video sequences based on user-provided actions. The top row demonstrates the model&rsquo;s handling of object permanence, a scenario where objects continue to exist even when not directly visible. The bottom row depicts a block-pushing interaction, a more complex task requiring understanding of physics and object manipulation. Importantly, both scenarios extend significantly beyond the model&rsquo;s training horizon (over 100 frames), and involve actions unseen during training, highlighting the model&rsquo;s generalization capabilities.</p><details><summary>read the caption</summary>Figure 8: Video Controllability. HMA can follow user action inputs to generate physically plausible object permanence (top row) and block pushing interactions (bottom row). These video predictions are both at out-of-distribution settings and at a much longer horizon than training (over 100 frames).</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.04296/x9.png alt></figure></p><blockquote><p>üîº Figure 9 demonstrates the application of the Heterogeneous Masked Autoregression (HMA) model for policy evaluation. Unlike traditional simulators which primarily focus on successful policy executions, HMA learns from both successful and failed examples of robot actions and their corresponding video sequences. This allows HMA to provide more realistic and comprehensive evaluations of policies, assessing their performance across a wider range of scenarios. The figure highlights that HMA&rsquo;s autoregressive prediction extends far beyond its training timeframe; at inference, it can predict 10 times longer sequences than those seen during training, further enhancing its utility for evaluating policy performance in complex, long-horizon robotic tasks.</p><details><summary>read the caption</summary>Figure 9: Policy Evaluation with HMA. By learning the action-video dynamics over both successful and failed examples, HMA can be used to evaluate policies, similar to a traditional simulator [46]. The autoregressive horizon at inference time is 10 times more than the training time horizon.</details></blockquote></details><details><summary>More on tables</summary><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_centering ltx_align_middle" id=S5.T2.7><tr class=ltx_tr id=S5.T2.7.7><td class="ltx_td ltx_border_r" id=S5.T2.7.7.8 style="padding:.8pt 4pt"></td><td class="ltx_td ltx_align_center" id=S5.T2.1.1.1 style="padding:.8pt 4pt"><span class="ltx_text ltx_font_bold" id=S5.T2.1.1.1.1 style=font-size:80%>PSNR<math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T2.1.1.1.1.m1.1"><semantics id="S5.T2.1.1.1.1.m1.1a"><mo id="S5.T2.1.1.1.1.m1.1.1" stretchy="false" xref="S5.T2.1.1.1.1.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S5.T2.1.1.1.1.m1.1b"><ci id="S5.T2.1.1.1.1.m1.1.1.cmml" xref="S5.T2.1.1.1.1.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T2.1.1.1.1.m1.1d">‚Üë</annotation></semantics></math></span></td><td class="ltx_td ltx_align_center" id=S5.T2.2.2.2 style="padding:.8pt 4pt"><span class="ltx_text ltx_font_bold" id=S5.T2.2.2.2.1 style=font-size:80%>SSIM<math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T2.2.2.2.1.m1.1"><semantics id="S5.T2.2.2.2.1.m1.1a"><mo id="S5.T2.2.2.2.1.m1.1.1" stretchy="false" xref="S5.T2.2.2.2.1.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S5.T2.2.2.2.1.m1.1b"><ci id="S5.T2.2.2.2.1.m1.1.1.cmml" xref="S5.T2.2.2.2.1.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.2.2.2.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T2.2.2.2.1.m1.1d">‚Üë</annotation></semantics></math></span></td><td class="ltx_td ltx_align_center" id=S5.T2.4.4.4 style="padding:.8pt 4pt"><math alttext="\Delta" class="ltx_Math" display="inline" id="S5.T2.3.3.3.m1.1"><semantics id="S5.T2.3.3.3.m1.1a"><mi id="S5.T2.3.3.3.m1.1.1" mathsize="80%" mathvariant="normal" xref="S5.T2.3.3.3.m1.1.1.cmml">Œî</mi><annotation-xml encoding="MathML-Content" id="S5.T2.3.3.3.m1.1b"><ci id="S5.T2.3.3.3.m1.1.1.cmml" xref="S5.T2.3.3.3.m1.1.1">Œî</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.3.3.3.m1.1c">\Delta</annotation><annotation encoding="application/x-llamapun" id="S5.T2.3.3.3.m1.1d">roman_Œî</annotation></semantics></math><span class="ltx_text ltx_font_bold" id=S5.T2.4.4.4.1 style=font-size:80%>PSNR<math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T2.4.4.4.1.m1.1"><semantics id="S5.T2.4.4.4.1.m1.1a"><mo id="S5.T2.4.4.4.1.m1.1.1" stretchy="false" xref="S5.T2.4.4.4.1.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S5.T2.4.4.4.1.m1.1b"><ci id="S5.T2.4.4.4.1.m1.1.1.cmml" xref="S5.T2.4.4.4.1.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.4.4.4.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T2.4.4.4.1.m1.1d">‚Üë</annotation></semantics></math></span></td><td class="ltx_td ltx_align_center" id=S5.T2.5.5.5 style="padding:.8pt 4pt"><span class="ltx_text ltx_font_bold" id=S5.T2.5.5.5.1 style=font-size:80%>LPIPS<math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T2.5.5.5.1.m1.1"><semantics id="S5.T2.5.5.5.1.m1.1a"><mo id="S5.T2.5.5.5.1.m1.1.1" stretchy="false" xref="S5.T2.5.5.5.1.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S5.T2.5.5.5.1.m1.1b"><ci id="S5.T2.5.5.5.1.m1.1.1.cmml" xref="S5.T2.5.5.5.1.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.5.5.5.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T2.5.5.5.1.m1.1d">‚Üì</annotation></semantics></math></span></td><td class="ltx_td ltx_align_center" id=S5.T2.6.6.6 style="padding:.8pt 4pt"><span class="ltx_text ltx_font_bold" id=S5.T2.6.6.6.1 style=font-size:80%>FID<math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T2.6.6.6.1.m1.1"><semantics id="S5.T2.6.6.6.1.m1.1a"><mo id="S5.T2.6.6.6.1.m1.1.1" stretchy="false" xref="S5.T2.6.6.6.1.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S5.T2.6.6.6.1.m1.1b"><ci id="S5.T2.6.6.6.1.m1.1.1.cmml" xref="S5.T2.6.6.6.1.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.6.6.6.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T2.6.6.6.1.m1.1d">‚Üì</annotation></semantics></math></span></td><td class="ltx_td ltx_align_center" id=S5.T2.7.7.7 style="padding:.8pt 4pt"><span class="ltx_text ltx_font_bold" id=S5.T2.7.7.7.1 style=font-size:80%>FVD<math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T2.7.7.7.1.m1.1"><semantics id="S5.T2.7.7.7.1.m1.1a"><mo id="S5.T2.7.7.7.1.m1.1.1" stretchy="false" xref="S5.T2.7.7.7.1.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S5.T2.7.7.7.1.m1.1b"><ci id="S5.T2.7.7.7.1.m1.1.1.cmml" xref="S5.T2.7.7.7.1.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.7.7.7.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T2.7.7.7.1.m1.1d">‚Üì</annotation></semantics></math></span></td></tr><tr class=ltx_tr id=S5.T2.7.8><td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id=S5.T2.7.8.1 style="padding:.8pt 4pt"><span class=ltx_text id=S5.T2.7.8.1.1 style=font-size:80%>IRASim</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T2.7.8.2 style="padding:.8pt 4pt"><span class=ltx_text id=S5.T2.7.8.2.1 style=font-size:80%>25.41</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T2.7.8.3 style="padding:.8pt 4pt"><span class=ltx_text id=S5.T2.7.8.3.1 style=font-size:80%>0.82</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T2.7.8.4 style="padding:.8pt 4pt"><span class=ltx_text id=S5.T2.7.8.4.1 style=font-size:80%>5.78</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T2.7.8.5 style="padding:.8pt 4pt"><span class=ltx_text id=S5.T2.7.8.5.1 style=font-size:80%>0.08</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T2.7.8.6 style="padding:.8pt 4pt"><span class=ltx_text id=S5.T2.7.8.6.1 style=font-size:80%>23.22</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T2.7.8.7 style="padding:.8pt 4pt"><span class=ltx_text id=S5.T2.7.8.7.1 style=font-size:80%>152.20</span></td></tr><tr class=ltx_tr id=S5.T2.7.9><td class="ltx_td ltx_align_left ltx_border_r" id=S5.T2.7.9.1 style="padding:.8pt 4pt"><span class="ltx_text ltx_font_typewriter" id=S5.T2.7.9.1.1 style=font-size:80%>HMA</span></td><td class="ltx_td ltx_align_center" id=S5.T2.7.9.2 style="padding:.8pt 4pt"><span class=ltx_text id=S5.T2.7.9.2.1 style=font-size:80%>28.19</span></td><td class="ltx_td ltx_align_center" id=S5.T2.7.9.3 style="padding:.8pt 4pt"><span class=ltx_text id=S5.T2.7.9.3.1 style=font-size:80%>0.83</span></td><td class="ltx_td ltx_align_center" id=S5.T2.7.9.4 style="padding:.8pt 4pt"><span class=ltx_text id=S5.T2.7.9.4.1 style=font-size:80%>6.06</span></td><td class="ltx_td ltx_align_center" id=S5.T2.7.9.5 style="padding:.8pt 4pt"><span class=ltx_text id=S5.T2.7.9.5.1 style=font-size:80%>0.07</span></td><td class="ltx_td ltx_align_center" id=S5.T2.7.9.6 style="padding:.8pt 4pt"><span class=ltx_text id=S5.T2.7.9.6.1 style=font-size:80%>33.56</span></td><td class="ltx_td ltx_align_center" id=S5.T2.7.9.7 style="padding:.8pt 4pt"><span class=ltx_text id=S5.T2.7.9.7.1 style=font-size:80%>111.52</span></td></tr></table></table></figure><blockquote><p>üîº This table compares the performance of the proposed Heterogeneous Masked Autoregression (HMA) model with IRASim, a state-of-the-art model for interactive robot simulation. The comparison focuses on the Language Table benchmark [31], evaluating visual fidelity (PSNR, SSIM, LPIPS, FID, FVD), controllability (ŒîPSNR), inference speed (FPS), and model size (Parameters). It demonstrates that the HMA model achieves superior visual quality and controllability while being significantly faster and requiring fewer parameters than IRASim. The results are based on 200 held-out trajectories.</p><details><summary>read the caption</summary>Table 2: Comparison with IRASim. In Language Table Benchmark [31], we show that a pre-trained HMA-based model (diffusion) is able to achieve better visual qualities and controllability than IRASim while maintaining faster speed and requiring less compute. The results are computed over 200 held-out trajectories.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_centering ltx_align_middle" id=S5.T3.6><tr class=ltx_tr id=S5.T3.5.5><td class="ltx_td ltx_border_r" id=S5.T3.5.5.6 style="padding:.8pt 7pt"></td><td class="ltx_td ltx_align_center" id=S5.T3.1.1.1 style="padding:.8pt 7pt"><span class="ltx_text ltx_font_bold" id=S5.T3.1.1.1.1 style=font-size:80%>PSNR<math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T3.1.1.1.1.m1.1"><semantics id="S5.T3.1.1.1.1.m1.1a"><mo id="S5.T3.1.1.1.1.m1.1.1" stretchy="false" xref="S5.T3.1.1.1.1.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S5.T3.1.1.1.1.m1.1b"><ci id="S5.T3.1.1.1.1.m1.1.1.cmml" xref="S5.T3.1.1.1.1.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T3.1.1.1.1.m1.1d">‚Üë</annotation></semantics></math></span></td><td class="ltx_td ltx_align_center" id=S5.T3.2.2.2 style="padding:.8pt 7pt"><span class="ltx_text ltx_font_bold" id=S5.T3.2.2.2.1 style=font-size:80%>Perplexity<math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T3.2.2.2.1.m1.1"><semantics id="S5.T3.2.2.2.1.m1.1a"><mo id="S5.T3.2.2.2.1.m1.1.1" stretchy="false" xref="S5.T3.2.2.2.1.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S5.T3.2.2.2.1.m1.1b"><ci id="S5.T3.2.2.2.1.m1.1.1.cmml" xref="S5.T3.2.2.2.1.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.2.2.2.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T3.2.2.2.1.m1.1d">‚Üì</annotation></semantics></math></span></td><td class="ltx_td ltx_align_center" id=S5.T3.4.4.4 style="padding:.8pt 7pt"><math alttext="\Delta" class="ltx_Math" display="inline" id="S5.T3.3.3.3.m1.1"><semantics id="S5.T3.3.3.3.m1.1a"><mi id="S5.T3.3.3.3.m1.1.1" mathsize="80%" mathvariant="normal" xref="S5.T3.3.3.3.m1.1.1.cmml">Œî</mi><annotation-xml encoding="MathML-Content" id="S5.T3.3.3.3.m1.1b"><ci id="S5.T3.3.3.3.m1.1.1.cmml" xref="S5.T3.3.3.3.m1.1.1">Œî</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.3.3.3.m1.1c">\Delta</annotation><annotation encoding="application/x-llamapun" id="S5.T3.3.3.3.m1.1d">roman_Œî</annotation></semantics></math><span class="ltx_text ltx_font_bold" id=S5.T3.4.4.4.1 style=font-size:80%> PSNR<math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T3.4.4.4.1.m1.1"><semantics id="S5.T3.4.4.4.1.m1.1a"><mo id="S5.T3.4.4.4.1.m1.1.1" stretchy="false" xref="S5.T3.4.4.4.1.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S5.T3.4.4.4.1.m1.1b"><ci id="S5.T3.4.4.4.1.m1.1.1.cmml" xref="S5.T3.4.4.4.1.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.4.4.4.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T3.4.4.4.1.m1.1d">‚Üë</annotation></semantics></math></span></td><td class="ltx_td ltx_align_center" id=S5.T3.5.5.5 style="padding:.8pt 7pt"><span class="ltx_text ltx_font_bold" id=S5.T3.5.5.5.1 style=font-size:80%>LPIPS<math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T3.5.5.5.1.m1.1"><semantics id="S5.T3.5.5.5.1.m1.1a"><mo id="S5.T3.5.5.5.1.m1.1.1" stretchy="false" xref="S5.T3.5.5.5.1.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S5.T3.5.5.5.1.m1.1b"><ci id="S5.T3.5.5.5.1.m1.1.1.cmml" xref="S5.T3.5.5.5.1.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.5.5.5.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T3.5.5.5.1.m1.1d">‚Üì</annotation></semantics></math></span></td></tr><tr class=ltx_tr id=S5.T3.6.7><td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id=S5.T3.6.7.1 style="padding:.8pt 7pt"><span class="ltx_text ltx_font_typewriter" id=S5.T3.6.7.1.1 style=font-size:80%>HMA</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T3.6.7.2 style="padding:.8pt 7pt"><span class=ltx_text id=S5.T3.6.7.2.1 style=font-size:80%>21.01</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T3.6.7.3 style="padding:.8pt 7pt"><span class=ltx_text id=S5.T3.6.7.3.1 style=font-size:80%>305.87</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T3.6.7.4 style="padding:.8pt 7pt"><span class=ltx_text id=S5.T3.6.7.4.1 style=font-size:80%>0.01</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T3.6.7.5 style="padding:.8pt 7pt"><span class=ltx_text id=S5.T3.6.7.5.1 style=font-size:80%>0.19</span></td></tr><tr class=ltx_tr id=S5.T3.6.6><td class="ltx_td ltx_align_left ltx_border_r" id=S5.T3.6.6.1 style="padding:.8pt 7pt"><span class="ltx_text ltx_font_typewriter" id=S5.T3.6.6.1.1 style=font-size:80%>HMA</span><span class=ltx_text id=S5.T3.6.6.1.2 style=font-size:80%> </span><sup class=ltx_sup id=S5.T3.6.6.1.3><span class="ltx_text ltx_font_italic" id=S5.T3.6.6.1.3.1 style=font-size:80%>+</span></sup></td><td class="ltx_td ltx_align_center" id=S5.T3.6.6.2 style="padding:.8pt 7pt"><span class=ltx_text id=S5.T3.6.6.2.1 style=font-size:80%>22.04</span></td><td class="ltx_td ltx_align_center" id=S5.T3.6.6.3 style="padding:.8pt 7pt"><span class=ltx_text id=S5.T3.6.6.3.1 style=font-size:80%>189.83</span></td><td class="ltx_td ltx_align_center" id=S5.T3.6.6.4 style="padding:.8pt 7pt"><span class=ltx_text id=S5.T3.6.6.4.1 style=font-size:80%>0.06</span></td><td class="ltx_td ltx_align_center" id=S5.T3.6.6.5 style="padding:.8pt 7pt"><span class=ltx_text id=S5.T3.6.6.5.1 style=font-size:80%>0.17</span></td></tr></table></table></figure><blockquote><p>üîº This table presents the results of fine-tuning the Heterogeneous Masked Autoregression (HMA) model on real-world data. It compares the performance of two approaches: one where the model is fine-tuned using pre-trained weights (HMA+) and another where it&rsquo;s trained from scratch (HMA). The experiment uses a discrete loss function as the baseline for evaluation. The metrics presented likely show improvements in visual fidelity (PSNR, Perplexity), and controllability (ŒîPSNR, LPIPS) achieved by fine-tuning versus training from scratch.</p><details><summary>read the caption</summary>Table 3: Real World Finetuning. HMA + denotes finetuned model based on pre-trained checkpoints while HMA trains from scratch on the finetuning data. This experiment uses the discrete loss baseline.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_centering ltx_align_middle" id=S5.T4.7><tr class=ltx_tr id=S5.T4.6.6><td class="ltx_td ltx_border_r" id=S5.T4.6.6.7 style="padding:.8pt 7pt"></td><td class="ltx_td ltx_align_center" id=S5.T4.1.1.1 style="padding:.8pt 7pt"><span class="ltx_text ltx_font_bold" id=S5.T4.1.1.1.1 style=font-size:80%>PSNR<math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T4.1.1.1.1.m1.1"><semantics id="S5.T4.1.1.1.1.m1.1a"><mo id="S5.T4.1.1.1.1.m1.1.1" stretchy="false" xref="S5.T4.1.1.1.1.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S5.T4.1.1.1.1.m1.1b"><ci id="S5.T4.1.1.1.1.m1.1.1.cmml" xref="S5.T4.1.1.1.1.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T4.1.1.1.1.m1.1d">‚Üë</annotation></semantics></math></span></td><td class="ltx_td ltx_align_center" id=S5.T4.2.2.2 style="padding:.8pt 7pt"><span class="ltx_text ltx_font_bold" id=S5.T4.2.2.2.1 style=font-size:80%>Perplexity<math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T4.2.2.2.1.m1.1"><semantics id="S5.T4.2.2.2.1.m1.1a"><mo id="S5.T4.2.2.2.1.m1.1.1" stretchy="false" xref="S5.T4.2.2.2.1.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S5.T4.2.2.2.1.m1.1b"><ci id="S5.T4.2.2.2.1.m1.1.1.cmml" xref="S5.T4.2.2.2.1.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.2.2.2.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T4.2.2.2.1.m1.1d">‚Üì</annotation></semantics></math></span></td><td class="ltx_td ltx_align_center" id=S5.T4.4.4.4 style="padding:.8pt 7pt"><span class="ltx_text ltx_font_bold" id=S5.T4.4.4.4.2 style=font-size:80%>PSNR<sup class=ltx_sup id=S5.T4.4.4.4.2.1><span class="ltx_text ltx_font_medium" id=S5.T4.4.4.4.2.1.1>‚àó</span></sup><math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T4.4.4.4.2.m2.1"><semantics id="S5.T4.4.4.4.2.m2.1a"><mo id="S5.T4.4.4.4.2.m2.1.1" stretchy="false" xref="S5.T4.4.4.4.2.m2.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S5.T4.4.4.4.2.m2.1b"><ci id="S5.T4.4.4.4.2.m2.1.1.cmml" xref="S5.T4.4.4.4.2.m2.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.4.4.4.2.m2.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T4.4.4.4.2.m2.1d">‚Üë</annotation></semantics></math></span></td><td class="ltx_td ltx_align_center" id=S5.T4.6.6.6 style="padding:.8pt 7pt"><span class="ltx_text ltx_font_bold" id=S5.T4.6.6.6.2 style=font-size:80%>Perplexity<sup class=ltx_sup id=S5.T4.6.6.6.2.1><span class="ltx_text ltx_font_medium" id=S5.T4.6.6.6.2.1.1>‚àó</span></sup><math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T4.6.6.6.2.m2.1"><semantics id="S5.T4.6.6.6.2.m2.1a"><mo id="S5.T4.6.6.6.2.m2.1.1" stretchy="false" xref="S5.T4.6.6.6.2.m2.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S5.T4.6.6.6.2.m2.1b"><ci id="S5.T4.6.6.6.2.m2.1.1.cmml" xref="S5.T4.6.6.6.2.m2.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.6.6.6.2.m2.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T4.6.6.6.2.m2.1d">‚Üì</annotation></semantics></math></span></td></tr><tr class=ltx_tr id=S5.T4.7.8><td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id=S5.T4.7.8.1 style="padding:.8pt 7pt"><span class="ltx_text ltx_font_typewriter" id=S5.T4.7.8.1.1 style=font-size:80%>HMA</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T4.7.8.2 style="padding:.8pt 7pt"><span class=ltx_text id=S5.T4.7.8.2.1 style=font-size:80%>24.17</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T4.7.8.3 style="padding:.8pt 7pt"><span class=ltx_text id=S5.T4.7.8.3.1 style=font-size:80%>20.69</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T4.7.8.4 style="padding:.8pt 7pt"><span class=ltx_text id=S5.T4.7.8.4.1 style=font-size:80%>19.19</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T4.7.8.5 style="padding:.8pt 7pt"><span class=ltx_text id=S5.T4.7.8.5.1 style=font-size:80%>1193.70</span></td></tr><tr class=ltx_tr id=S5.T4.7.7><td class="ltx_td ltx_align_left ltx_border_r" id=S5.T4.7.7.1 style="padding:.8pt 7pt"><span class="ltx_text ltx_font_typewriter" id=S5.T4.7.7.1.1 style=font-size:80%>HMA</span><span class=ltx_text id=S5.T4.7.7.1.2 style=font-size:80%> </span><sup class=ltx_sup id=S5.T4.7.7.1.3><span class="ltx_text ltx_font_italic" id=S5.T4.7.7.1.3.1 style=font-size:80%>+</span></sup></td><td class="ltx_td ltx_align_center" id=S5.T4.7.7.2 style="padding:.8pt 7pt"><span class=ltx_text id=S5.T4.7.7.2.1 style=font-size:80%>25.11</span></td><td class="ltx_td ltx_align_center" id=S5.T4.7.7.3 style="padding:.8pt 7pt"><span class=ltx_text id=S5.T4.7.7.3.1 style=font-size:80%>11.82</span></td><td class="ltx_td ltx_align_center" id=S5.T4.7.7.4 style="padding:.8pt 7pt"><span class=ltx_text id=S5.T4.7.7.4.1 style=font-size:80%>20.20</span></td><td class="ltx_td ltx_align_center" id=S5.T4.7.7.5 style="padding:.8pt 7pt"><span class=ltx_text id=S5.T4.7.7.5.1 style=font-size:80%>103.01</span></td></tr></table></table></figure><blockquote><p>üîº This table presents a comparison of the performance of two models in a simulation transfer learning scenario. The first model, referred to as &lsquo;HMA&rsquo;, is trained from scratch using a combination of cross-entropy and diffusion loss functions. The second model, denoted as &lsquo;HMA+&rsquo;, leverages pre-trained weights as a starting point before further fine-tuning with the same loss functions. The results highlight the impact of transfer learning, showing how using pre-trained weights (HMA+) can improve performance, particularly in terms of PSNR (Peak Signal-to-Noise Ratio), a measure of image quality, and perplexity, which reflects how well the model predicts data. Lower perplexity values indicate better predictive performance. The metrics PSNR* and Perplexity*, which reflect sensitivity to small changes in actions, also demonstrates the effect of using pre-trained weights.</p><details><summary>read the caption</summary>Table 4: Simulation Transfer Learning. We show that pre-trained HMA can help with fine-tuning using cross-entropy losses and diffusion losses jointly. where HMA + denotes the finetuned model based on pre-trained checkpoints.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_centering ltx_align_middle" id=S5.T5.2><tr class=ltx_tr id=S5.T5.2.1><td class="ltx_td ltx_align_left ltx_border_r" id=S5.T5.2.1.1 style="padding:.8pt 8pt"><span class="ltx_text ltx_font_bold" id=S5.T5.2.1.1.1 style=font-size:80%>Policy Evaluator</span></td><td class="ltx_td ltx_align_center" id=S5.T5.2.1.2 style="padding:.8pt 8pt"><span class=ltx_text id=S5.T5.2.1.2.1 style=font-size:80%>1</span></td><td class="ltx_td ltx_align_center" id=S5.T5.2.1.3 style="padding:.8pt 8pt"><span class=ltx_text id=S5.T5.2.1.3.1 style=font-size:80%>2</span></td><td class="ltx_td ltx_align_center" id=S5.T5.2.1.4 style="padding:.8pt 8pt"><span class=ltx_text id=S5.T5.2.1.4.1 style=font-size:80%>3</span></td><td class="ltx_td ltx_align_center" id=S5.T5.2.1.5 style="padding:.8pt 8pt"><span class=ltx_text id=S5.T5.2.1.5.1 style=font-size:80%>4</span></td></tr><tr class=ltx_tr id=S5.T5.2.2><td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id=S5.T5.2.2.1 style="padding:.8pt 8pt"><span class=ltx_text id=S5.T5.2.2.1.1 style=font-size:80%>Ground Truth Simulator</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T5.2.2.2 style="padding:.8pt 8pt"><span class=ltx_text id=S5.T5.2.2.2.1 style=font-size:80%>0.38</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T5.2.2.3 style="padding:.8pt 8pt"><span class=ltx_text id=S5.T5.2.2.3.1 style=font-size:80%>0.52</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T5.2.2.4 style="padding:.8pt 8pt"><span class=ltx_text id=S5.T5.2.2.4.1 style=font-size:80%>0.70</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T5.2.2.5 style="padding:.8pt 8pt"><span class=ltx_text id=S5.T5.2.2.5.1 style=font-size:80%>1.00</span></td></tr><tr class=ltx_tr id=S5.T5.2.3><td class="ltx_td ltx_align_left ltx_border_r" id=S5.T5.2.3.1 style="padding:.8pt 8pt"><span class="ltx_text ltx_font_typewriter" id=S5.T5.2.3.1.1 style=font-size:80%>HMA</span><span class=ltx_text id=S5.T5.2.3.1.2 style=font-size:80%> Simulator</span></td><td class="ltx_td ltx_align_center" id=S5.T5.2.3.2 style="padding:.8pt 8pt"><span class=ltx_text id=S5.T5.2.3.2.1 style=font-size:80%>0.43</span></td><td class="ltx_td ltx_align_center" id=S5.T5.2.3.3 style="padding:.8pt 8pt"><span class=ltx_text id=S5.T5.2.3.3.1 style=font-size:80%>0.56</span></td><td class="ltx_td ltx_align_center" id=S5.T5.2.3.4 style="padding:.8pt 8pt"><span class=ltx_text id=S5.T5.2.3.4.1 style=font-size:80%>0.66</span></td><td class="ltx_td ltx_align_center" id=S5.T5.2.3.5 style="padding:.8pt 8pt"><span class=ltx_text id=S5.T5.2.3.5.1 style=font-size:80%>0.73</span></td></tr></table></table></figure><blockquote><p>üîº This table presents the results of evaluating four different robot control policies using both a ground truth simulator and a learned simulator (HMA). The policies were trained with varying numbers of iterations, allowing for an assessment of how performance correlates across simulators. The high Pearson correlation of 0.95 indicates a strong positive relationship between policy performance as measured by both simulators, suggesting that the learned simulator accurately reflects performance in the real world.</p><details><summary>read the caption</summary>Table 5: Policy Evaluation Results Across 4 Different Policies. We observed positive correlations of the evaluation results for 4 different policies bewteen the ground truth and learned simulators. The Pearson ratio between evaluations is 0.95.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_centering ltx_align_middle" id=S5.T6.2><tr class=ltx_tr id=S5.T6.2.1><td class="ltx_td ltx_border_r" id=S5.T6.2.1.1 style="padding:.8pt 4pt"></td><td class="ltx_td ltx_align_center" id=S5.T6.2.1.2 style="padding:.8pt 4pt"><span class=ltx_text id=S5.T6.2.1.2.1 style=font-size:80%>+0</span></td><td class="ltx_td ltx_align_center" id=S5.T6.2.1.3 style="padding:.8pt 4pt"><span class=ltx_text id=S5.T6.2.1.3.1 style=font-size:80%>+10</span></td><td class="ltx_td ltx_align_center" id=S5.T6.2.1.4 style="padding:.8pt 4pt"><span class=ltx_text id=S5.T6.2.1.4.1 style=font-size:80%>+50</span></td><td class="ltx_td ltx_align_center" id=S5.T6.2.1.5 style="padding:.8pt 4pt"><span class=ltx_text id=S5.T6.2.1.5.1 style=font-size:80%>+90</span></td><td class="ltx_td ltx_align_center" id=S5.T6.2.1.6 style="padding:.8pt 4pt"><span class=ltx_text id=S5.T6.2.1.6.1 style=font-size:80%>original</span></td></tr><tr class=ltx_tr id=S5.T6.2.2><td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id=S5.T6.2.2.1 style="padding:.8pt 4pt"><span class=ltx_text id=S5.T6.2.2.1.1 style=font-size:80%>Success in </span><cite class="ltx_cite ltx_citemacro_cite"><span class=ltx_text id=S5.T6.2.2.1.2.1 style=font-size:80%>[</span><a class=ltx_ref href=https://arxiv.org/html/2502.04296v1#bib.bib32 title><span class=ltx_text style=font-size:90%>32</span></a><span class=ltx_text id=S5.T6.2.2.1.3.2 style=font-size:80%>]</span></cite></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T6.2.2.2 style="padding:.8pt 4pt"><span class=ltx_text id=S5.T6.2.2.2.1 style=font-size:80%>82%</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T6.2.2.3 style="padding:.8pt 4pt"><span class=ltx_text id=S5.T6.2.2.3.1 style=font-size:80%>90%</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T6.2.2.4 style="padding:.8pt 4pt"><span class=ltx_text id=S5.T6.2.2.4.1 style=font-size:80%>96%</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T6.2.2.5 style="padding:.8pt 4pt"><span class=ltx_text id=S5.T6.2.2.5.1 style=font-size:80%>100%</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T6.2.2.6 style="padding:.8pt 4pt"><span class=ltx_text id=S5.T6.2.2.6.1 style=font-size:80%>100%</span></td></tr><tr class=ltx_tr id=S5.T6.2.3><td class="ltx_td ltx_align_left ltx_border_r" id=S5.T6.2.3.1 style="padding:.8pt 4pt"><span class=ltx_text id=S5.T6.2.3.1.1 style=font-size:80%>Validation Loss in </span><cite class="ltx_cite ltx_citemacro_cite"><span class=ltx_text id=S5.T6.2.3.1.2.1 style=font-size:80%>[</span><a class=ltx_ref href=https://arxiv.org/html/2502.04296v1#bib.bib31 title><span class=ltx_text style=font-size:90%>31</span></a><span class=ltx_text id=S5.T6.2.3.1.3.2 style=font-size:80%>]</span></cite></td><td class="ltx_td ltx_align_center" id=S5.T6.2.3.2 style="padding:.8pt 4pt"><span class=ltx_text id=S5.T6.2.3.2.1 style=font-size:80%>1.72</span></td><td class="ltx_td ltx_align_center" id=S5.T6.2.3.3 style="padding:.8pt 4pt"><span class=ltx_text id=S5.T6.2.3.3.1 style=font-size:80%>1.16</span></td><td class="ltx_td ltx_align_center" id=S5.T6.2.3.4 style="padding:.8pt 4pt"><span class=ltx_text id=S5.T6.2.3.4.1 style=font-size:80%>1.09</span></td><td class="ltx_td ltx_align_center" id=S5.T6.2.3.5 style="padding:.8pt 4pt"><span class=ltx_text id=S5.T6.2.3.5.1 style=font-size:80%>0.88</span></td><td class="ltx_td ltx_align_center" id=S5.T6.2.3.6 style="padding:.8pt 4pt"><span class=ltx_text id=S5.T6.2.3.6.1 style=font-size:80%>0.87</span></td></tr></table></table></figure><blockquote><p>üîº This table investigates the impact of synthetic data generated by the HMA model on policy learning. It evaluates how adding varying amounts of synthetic data (from 10% to 90% ) to a small subset of real-world data (10 out of 100 real trajectories) affects policy performance. The experiment uses two benchmarks: Robomimic (success rates are reported) and Language Table (validation losses are reported). The results show how well the HMA-generated data supplements the real data for training effective policies.</p><details><summary>read the caption</summary>Table 6: Synthetic Data for Policy Learning. We evaluate the quality of generated synthetic data by adding different numbers of generated video trajectories in [32] and [31], from 10 to 100, to a fixed subset (10 trajectories) of the original data (100 trajectories). We then conduct policy training and evaluation and report the Robomimic success rates (top row) and Language Table validation losses (bottom row).</details></blockquote></details><h3 class="relative group">Full paper<div id=full-paper class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#full-paper aria-label=Anchor>#</a></span></h3><div id=gallery-8710ad5c1e48d82774f2246694e6bccc class=gallery><img src=https://ai-paper-reviewer.com/2502.04296/1.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.04296/2.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.04296/3.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.04296/4.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.04296/5.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.04296/6.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.04296/7.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.04296/8.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.04296/9.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.04296/10.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.04296/11.png class="grid-w50 md:grid-w33 xl:grid-w25"></div></div><section class="flex flex-row flex-wrap justify-center pt-4 text-xl"><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04296/&amp;title=Learning%20Real-World%20Action-Video%20Dynamics%20with%20Heterogeneous%20Masked%20Autoregression" title="Share on LinkedIn" aria-label="Share on LinkedIn"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://twitter.com/intent/tweet/?url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04296/&amp;text=Learning%20Real-World%20Action-Video%20Dynamics%20with%20Heterogeneous%20Masked%20Autoregression" title="Tweet on Twitter" aria-label="Tweet on Twitter"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="mailto:?body=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04296/&amp;subject=Learning%20Real-World%20Action-Video%20Dynamics%20with%20Heterogeneous%20Masked%20Autoregression" title="Send via email" aria-label="Send via email"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1-27.64 140.9 68.65 266.2 199.1 285.1 19.01 2.888 36.17-12.26 36.17-31.49l1e-4-.6631c0-15.74-11.44-28.88-26.84-31.24-84.35-12.98-149.2-86.13-149.2-174.2.0-102.9 88.61-185.5 193.4-175.4 91.54 8.869 158.6 91.25 158.6 183.2v16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98.0036c-7.299.0-13.2 4.992-15.12 11.68-24.85-12.15-54.24-16.38-86.06-5.106-38.75 13.73-68.12 48.91-73.72 89.64-9.483 69.01 43.81 128 110.9 128 26.44.0 50.43-9.544 69.59-24.88 24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3c0-149.2-133.9-265.632-287.3-235.57zM239.1 304.3c-26.47.0-48-21.56-48-48.05s21.53-48.05 48-48.05 48 21.56 48 48.05-20.6 48.05-48 48.05z"/></svg></span></a></section></div><script>var oid="views_paper-reviews/2502.04296/index.md",oid_likes="likes_paper-reviews/2502.04296/index.md"</script><script type=text/javascript src=/ai-paper-reviewer/js/page.min.0860cf4e04fa2d72cc33ddba263083464d48f67de06114529043cb4623319efed4f484fd7f1730df5abea0e2da6f3538855634081d02f2d6e920b956f063e823.js integrity="sha512-CGDPTgT6LXLMM926JjCDRk1I9n3gYRRSkEPLRiMxnv7U9IT9fxcw31q+oOLabzU4hVY0CB0C8tbpILlW8GPoIw=="></script></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="flex group mr-3" href=/ai-paper-reviewer/paper-reviews/2502.04520/><span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&larr;</span>
<span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Linear Correlation in LM's Compositional Generalization and Hallucination</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2025-02-06T00:00:00+00:00>6 February 2025</time>
</span></span></a></span><span><a class="flex text-right group ml-3" href=/ai-paper-reviewer/paper-reviews/2502.04507/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Fast Video Generation with Sliding Tile Attention</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2025-02-06T00:00:00+00:00>6 February 2025</time>
</span></span><span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&larr;</span></a></span></div></div><div class=pt-3><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class=pt-3><script src=https://utteranc.es/client.js repo=pmnxis/pmnxis.github.io issue-term=pathname label=Comment theme=dark-blue crossorigin=anonymous async></script></div></div></footer></article><div id=top-scroller class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer id=site-footer class="py-10 print:hidden"><nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400"><ul class="flex flex-col list-none sm:flex-row"><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=/ai-paper-reviewer/tags/ title=Tags>Tags</a></li><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=https://deep-diver.github.io/neurips2024/ title>NeurIPS2024</a></li></ul></nav><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2025
Hugging Face Daily Papers</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a></p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/ai-paper-reviewer/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh+sCQ0E53ghYrxgYqw+0GCRyIEpA=="></script></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://deep-diver.github.io/ai-paper-reviewer/ style=z-index:500><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body><script data-name=BMC-Widget data-cfasync=false src=https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js data-id=chansung data-description="Support me on Buy me a coffee!" data-message data-color=#FFDD00 data-position=Left data-x_margin=18 data-y_margin=18></script></html>