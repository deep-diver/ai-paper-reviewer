[{"figure_path": "https://arxiv.org/html/2502.04296/x1.png", "caption": "Figure 1: \nAction-Video Dynamics Model from Heterogeneous Robot Interactions. HMA utilizes heterogeneous datasets comprising over 3 million trajectories (videos) from 40 distinct embodiments to pre-train a full dynamics model with next-set-of-token predictions using masked autoregression. After pre-training, the resulting action-video dynamics model is versatile, supporting applications such as video simulation, policy evaluation, synthetic data generation, and direct adoption as an imitation policy.", "description": "The figure illustrates the architecture and workflow of the Heterogeneous Masked Autoregression (HMA) model.  HMA leverages a diverse dataset of over 3 million video trajectories from 40 different robot embodiments to learn a comprehensive action-video dynamics model. The pre-training phase employs masked autoregression to predict the next set of tokens (visual and action), effectively capturing the complex interactions between robot actions and resulting video observations.  Following pre-training, this versatile model finds applications in various robotics tasks:  generating realistic video simulations, evaluating robot policies (by simulating the consequences of different actions), creating synthetic training data, and even acting as a direct imitation policy.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2502.04296/x2.png", "caption": "Figure 2: Dynamics Model. Masked autoregression in the dynamics model generalizes multiple problem settings including policy learning, forward and passive dynamics, and full dynamics.", "description": "Figure 2 illustrates the versatility of masked autoregression in modeling robot dynamics. It shows how a single framework can address various robotics problems, such as policy learning (predicting future actions given past observations and actions), forward dynamics (predicting future observations given past observations and actions), passive dynamics (predicting future observations given only past observations), and full dynamics (jointly predicting future observations and actions). This unified approach allows the model to handle different tasks and scenarios in robotics through a common architecture.", "section": "3. Heterogeneous Masked Autoregression"}, {"figure_path": "https://arxiv.org/html/2502.04296/x3.png", "caption": "Figure 3: Network Architecture. The HMA model architecture maps low-level video and action sequences across different embodiments into a shared latent space. For actions, embodiment projectors are activated based on the training sample. The spatial-temporal Transformer produces the output video and action tokens for future frames.", "description": "The figure illustrates the architecture of the Heterogeneous Masked Autoregression (HMA) model.  The model processes both video and action sequences from various robotic embodiments.  Each embodiment has a dedicated 'stem' (action encoder) and 'head' (action decoder), which map the embodiment-specific action data into a shared latent space. The core of the model is a spatial-temporal transformer ('trunk') that processes the shared latent representations to predict both future video frames and actions.  The spatial attention mechanism operates bi-directionally on masked and unmasked tokens for both video and action, while the temporal attention mechanism is causal (only considering past information). The modular design allows for easy adaptation to new embodiments by simply training new stem and head components without modifying the trunk.", "section": "3. Heterogeneous Masked Autoregression"}, {"figure_path": "https://arxiv.org/html/2502.04296/x4.png", "caption": "Figure 4: Pre-trained Video Model Generation. We show that a single unified HMA model can generate realistic (left 3 columns) and diverse (right 3 columns) videos across multiple embodiment datasets with heterogeneous action spaces. Each group shows three generated frames from a single sequence.", "description": "This figure demonstrates the capability of the Heterogeneous Masked Autoregression (HMA) model to generate high-quality and diverse videos.  The model is trained on a dataset containing videos from various robotic embodiments and heterogeneous action spaces. The left three columns showcase videos that are visually realistic and consistent with typical real-world interactions. In contrast, the right three columns display videos with more unexpected or creative actions, reflecting the model's capacity to produce diverse results. Each group of three images represents consecutive frames from a single video sequence generated by the model.", "section": "4. Pre-Training Experiments"}, {"figure_path": "https://arxiv.org/html/2502.04296/x5.png", "caption": "Figure 5: Ablation on Pre-training Settings and Architecture. Under the pre-training setting with VQ tokens, we ablate the video generation performance (visual fidelity measured by perplexity and controllability measured by controllability). (a) We find action-conditioned models outperform passive video models. (b) We compare different action conditioning architectures in the masked autoregression framework. The purple color denotes the best model that we use by default.", "description": "Figure 5 presents ablation studies on the Heterogeneous Masked Autoregression (HMA) model, specifically examining the effects of different pre-training settings and architectures on video generation performance. Part (a) compares the performance of action-conditioned models (forward and full dynamics) against passive video models, showing that incorporating action information significantly improves both visual fidelity (measured by perplexity) and controllability (measured by APSNR).  Part (b) analyzes various action-conditioning architectures within the HMA framework, including modulation, token concatenation, feature addition, and token cross-attention, to determine the optimal design for balancing performance and efficiency. The best performing model configuration is highlighted in purple.", "section": "3. Heterogeneous Masked Autoregression"}, {"figure_path": "https://arxiv.org/html/2502.04296/x6.png", "caption": "Figure 6: Experiments on Scaling Behaviors of HMA. We observe positive trends in the scaling performance of heterogeneous video models across axes including the number of datasets, number of trajectories, and model sizes. The evaluation metrics on fidelity (perplexity) and controllability (\u0394\u0394\\Deltaroman_\u0394PSNR) are averaged across validation datasets.", "description": "This figure displays the results of experiments evaluating the scalability of the Heterogeneous Masked Autoregression (HMA) model.  Three separate experiments are shown, each exploring a different aspect of scalability:\n\n1. **Scaling the Number of Datasets:** This shows how HMA performs as the number of different robotic datasets used for training increases.  More datasets mean more diverse robot embodiments, actions, and environments are included in the training, representing a greater degree of heterogeneity.\n2. **Scaling the Number of Trajectories:** This assesses HMA's performance as the total number of training video trajectories increases.  More trajectories offer more data to train the model and potentially lead to better performance.\n3. **Scaling Model Size:** This experiment examines how HMA behaves as the size and complexity of the model itself increase (number of parameters).\n\nFor each experiment, two key metrics are shown. Perplexity measures the fidelity of the generated videos (lower perplexity indicates higher fidelity), and \u0394\u0394\nPSNR (Delta PSNR) measures the controllability of the generated videos (lower \u0394\u0394\nPSNR suggests better controllability, indicating that the model's output is more consistent and less affected by random noise).  The results in the plots demonstrate that the performance of the HMA model improves with increased dataset diversity, more training data, and larger model size, thus showcasing its strong scaling properties.", "section": "4. Pre-Training Experiments"}, {"figure_path": "https://arxiv.org/html/2502.04296/x7.png", "caption": "Figure 7: Qualitative Comparisons Between Tokenizers and Models. Despite longer convergence time, diffusion-based methods (Eq.\u00a03) on soft tokens generate better visual quality than on VQ tokens (Eq.\u00a02), qualitatively and measured by PSNR.", "description": "This figure compares the visual quality of videos generated using different methods: VQ tokens (vector-quantized tokens, a discrete representation) and soft tokens (continuous representation), both within a masked autoregressive framework.  The comparison highlights that while diffusion-based models using soft tokens take longer to train (slower convergence), they produce significantly better visual results. This superior quality is evident both qualitatively (by visual inspection) and quantitatively (as measured by the Peak Signal-to-Noise Ratio, or PSNR). The improved visual fidelity from using soft tokens comes at the cost of increased training time.", "section": "5.1 Toward Real-Time Simulation with HMA"}, {"figure_path": "https://arxiv.org/html/2502.04296/x8.png", "caption": "Figure 8: Video Controllability. HMA can follow user action inputs to generate physically plausible object permanence (top row) and block pushing interactions (bottom row). These video predictions are both at out-of-distribution settings and at a much longer horizon than training (over 100 frames).", "description": "Figure 8 showcases the model's capacity for generating physically realistic video sequences based on user-provided actions.  The top row demonstrates the model's handling of object permanence, a scenario where objects continue to exist even when not directly visible.  The bottom row depicts a block-pushing interaction, a more complex task requiring understanding of physics and object manipulation.  Importantly, both scenarios extend significantly beyond the model's training horizon (over 100 frames), and involve actions unseen during training, highlighting the model's generalization capabilities.", "section": "5. Post-Training Applications"}, {"figure_path": "https://arxiv.org/html/2502.04296/x9.png", "caption": "Figure 9: Policy Evaluation with HMA. By learning the action-video dynamics over both successful and failed examples, HMA can be used to evaluate policies, similar to a traditional simulator [46]. The autoregressive horizon at inference time is 10 times more than the training time horizon.", "description": "Figure 9 demonstrates the application of the Heterogeneous Masked Autoregression (HMA) model for policy evaluation. Unlike traditional simulators which primarily focus on successful policy executions, HMA learns from both successful and failed examples of robot actions and their corresponding video sequences.  This allows HMA to provide more realistic and comprehensive evaluations of policies, assessing their performance across a wider range of scenarios. The figure highlights that HMA's autoregressive prediction extends far beyond its training timeframe; at inference, it can predict 10 times longer sequences than those seen during training, further enhancing its utility for evaluating policy performance in complex, long-horizon robotic tasks.", "section": "5. Post-Training Applications"}]