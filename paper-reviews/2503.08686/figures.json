[{"figure_path": "https://arxiv.org/html/2503.08686/x2.png", "caption": "Figure 1: \nComprehensive comparison between OmniMamba and other unified understanding and generation models.\n(a) Our OmniMamba is trained on only 2M image-text pairs, which is 1000 times less than Show-o.\n(b) With such limited data for training, our OmniMamba significantly outperforms Show-o across a wide range of benchmarks and achieves competitive performance with JanusFlow. Black metrics are for the multimodal understanding benchmark, while the blue metric is for the visual generation task.\n(c)-(d) We compare the speed and memory of OmniMamba with other unified models on the same single NVIDIA 4090 GPU. OmniMamba demonstrates up to a 119.2\u00d7\\times\u00d7 speedup and 63% GPU memory reduction for long-sequence generation.", "description": "Figure 1 presents a comprehensive comparison of OmniMamba against other state-of-the-art unified multimodal understanding and generation models.  Panel (a) highlights OmniMamba's data efficiency, trained on a mere 2 million image-text pairs\u20141000 times less data than Show-o. Panel (b) showcases OmniMamba's superior performance on various benchmarks, surpassing Show-o and achieving comparable results to JanusFlow despite its significantly smaller training dataset. Black metrics represent multimodal understanding, while blue represents visual generation tasks. Panels (c) and (d) demonstrate OmniMamba's speed and memory efficiency on a single NVIDIA A40 GPU, achieving up to a 119.2x speedup and a 63% memory reduction during long-sequence generation.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2503.08686/x3.png", "caption": "Figure 2: Architecture of the proposed OmniMamba \u201cMMU\u201d refers to multimodal understanding, while \u201cT2I\u201d refers to text-to-image generation. OmniMamba employs a next-token prediction paradigm for both multimodal understanding and visual generation tasks. To address the distinct requirements of each task\u2014semantic information extraction for multimodal understanding and high-fidelity image compression for visual generation, we utilize separate encoders and heads. Furthermore, we purpose decoupled vocabularies to guide modality-specific generation and task-specific LoRA for parameter-efficient adaptation.", "description": "OmniMamba processes text and image data using a unified next-token prediction approach.  Separate encoders and heads are used for multimodal understanding (MMU) and text-to-image (T2I) generation to handle their unique requirements: semantic extraction for MMU and high-fidelity image compression for T2I.  Decoupled vocabularies guide modality-specific generation, and task-specific Low-Rank Adaptation (LoRA) enhances parameter efficiency.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.08686/x4.png", "caption": "Figure 3: The Mamba-2 block with task-specific LoRA. It is worth noting that while the Mamba-2 Block in the Mamba-2 paper has two input projectors, the actual code implementation separates the feature dimensions from a single projector output. For simplicity, we depict only one input projector in our illustration. Our task-specific LoRA is applied to this entire input projector.", "description": "This figure illustrates a Mamba-2 block, a core component of the OmniMamba model, enhanced with task-specific Low-Rank Adaptation (LoRA).  The original Mamba-2 architecture uses two input projectors, but the actual implementation merges feature dimensions from a single projector. This figure simplifies the diagram by showing only one input projector for clarity.  Crucially, the task-specific LoRA modules are applied to this entire input projection, enabling efficient task-specific adjustments within the Mamba-2 block. This is key to the model's ability to handle multimodal understanding and visual generation tasks efficiently.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.08686/x5.png", "caption": "Figure 4: Training strategy of OmniMamba. The trainable components are indicated by a flame symbol, while the frozen ones are represented by snowflakes. The dashed arrows indicate that this route is temporarily dropped and does not participate in model training.", "description": "This figure details OmniMamba's three-stage training process.  Stage 1 involves separate pre-training for multimodal understanding (MMU) and text-to-image (T2I) generation.  MMU pre-training focuses on aligning visual and textual representations, while T2I pre-training optimizes visual generation.  Note that only specific components (indicated by flame symbols) are trained in this stage, while others (snowflakes) remain frozen to facilitate efficient learning.  In Stage 2,  unified fine-tuning integrates both MMU and T2I tasks, enabling synergistic learning.  The dashed arrows represent the temporary disabling of certain paths during specific tasks.  Stage 3 is a continuation of Stage 2, further refining model capabilities across both modalities. The figure uses visual cues such as flames and snowflakes to effectively communicate which parts of the model are being updated during each training phase.", "section": "3.5 Decoupled Training Strategy"}, {"figure_path": "https://arxiv.org/html/2503.08686/x6.png", "caption": "Figure 5: Qualitative results of OmniMamba on multimodal understanding and visual generation.", "description": "Figure 5 showcases example outputs from OmniMamba, demonstrating its capabilities in both multimodal understanding and visual generation.  The top row presents prompts (user requests) for image description and image generation tasks.  The bottom row displays OmniMamba's corresponding responses.  The multimodal understanding examples illustrate the model's ability to accurately and comprehensively describe images. The visual generation examples demonstrate the model's capacity to produce images aligned with the given prompts.", "section": "4.4 Qualitative Results"}]