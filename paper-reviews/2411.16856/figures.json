[{"figure_path": "https://arxiv.org/html/2411.16856/x2.png", "caption": "Figure 1: Our method, SAR3D, proposes a comprehensive framework for 3D generation and understanding via autoregressive modeling. For (a) 3D generation, given a single image or text prompt, SAR3D generates multi-scale 3D objects in an autoregressive manner. For (b) 3D understanding, SAR3D-LLM can interpret a 3D model and provide a detailed description.", "description": "This figure illustrates the SAR3D framework, which is composed of two main parts: 3D generation and 3D understanding.  The 3D generation part (a) shows that SAR3D takes either a single image or a text prompt as input and generates a multi-scale 3D object using an autoregressive approach. The 3D understanding part (b) demonstrates that SAR3D-LLM, a large language model fine-tuned on 3D data, can interpret an existing 3D model and provide a detailed description of it.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2411.16856/x3.png", "caption": "Figure 2: Overview of Multi-scale VQVAE. Given a 3D model, we leverage multi-view RGB-D(epth) renderings and Pl\u00fccker embeddings as the input to our multi-view encoder \u2130\u2130\\mathcal{E}caligraphic_E. The encoder predicts a continuous feature map that is then quantized by the multi-scale quantizer \ud835\udcac\ud835\udcac\\mathcal{Q}caligraphic_Q, giving R=(r1,r2,\u2026,rK)\ud835\udc45subscript\ud835\udc5f1subscript\ud835\udc5f2\u2026subscript\ud835\udc5f\ud835\udc3eR=(r_{1},r_{2},\\dots,r_{K})italic_R = ( italic_r start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_r start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , \u2026 , italic_r start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT ) of latent tri-plane features. Each code of different scales share the same codebook. The triplane decoder then converts the quantized latent triplane features into the triplane representation through a plane-wise manner. The predicted triplane is multi-view supervised with the ground truth image, depth, and normal.", "description": "This figure illustrates the architecture of a multi-scale 3D Vector Quantized Variational Autoencoder (VQVAE).  A 3D model is first rendered into multiple views, each view providing an RGB image, a depth map, and a Pl\u00fccker coordinate representation of the camera pose. These multi-view renderings are concatenated and fed into a multi-view encoder. The encoder outputs a continuous feature map. This feature map is then quantized into a sequence of multi-scale tokens (R = (r1, r2, ..., rK)) by a multi-scale quantizer. Importantly, all scales share the same codebook, enabling efficient encoding.  These tokens represent the latent tri-plane features. Finally, a tri-plane decoder reconstructs the tri-plane representation from the quantized tokens using a plane-wise approach. The quality of this reconstruction is assessed by comparing the predicted tri-plane to the ground truth images, depth maps, and surface normals from the original 3D model.", "section": "4.1. Multi-scale 3D VQVAE"}, {"figure_path": "https://arxiv.org/html/2411.16856/x4.png", "caption": "Figure 3: Overview of 3D Generation and 3D Understanding. Given a 3D model, our 3D VQVAE encodes it into multi-scale discrete tokens for both 3D generation and understanding. In (a) 3D Generation, text or a single image is encoded by CLIPTT{}_{\\text{T}}start_FLOATSUBSCRIPT T end_FLOATSUBSCRIPT or DINOv2, and the encoded condition features are integrated into the decoder-only transformer via cross attention. The transformer then causally predicts each scale of the latent triplane. In (b) 3D Understanding, truncated 3D tokens are first processed with an MLP projector. The large language model receives a multimodal sequence of text and 3D tokens and generates a detailed caption describing the input 3D model.", "description": "Figure 3 illustrates SAR3D's two main functionalities: 3D generation and 3D understanding.  A 3D VQVAE encodes a given 3D model into a multi-scale sequence of discrete tokens.  In the 3D generation process (a), either text or a single image is encoded using CLIP or DINOv2. These encoded features are then fed into a decoder-only transformer via cross-attention, which autoregressively predicts each scale of the latent triplane, ultimately generating a 3D model.  In the 3D understanding process (b), a truncated version of the 3D token sequence (excluding the final scales) is processed through an MLP projector. This refined token sequence is combined with text input and given to a large language model, which generates a detailed caption describing the input 3D model.", "section": "4. Method"}, {"figure_path": "https://arxiv.org/html/2411.16856/x5.png", "caption": "Figure 4: Qualitative Comparison of Image-conditioned 3D Generation. Here, we compare with the state-of-the-art 3D generative models under different categories. As visualized here, our method achieves superior 3D consistency across views and generates intact objects without distortion. For more comparisons with other methods, please refer to the Supp Mat.", "description": "Figure 4 presents a qualitative comparison of image-conditioned 3D object generation, showcasing the results of various state-of-the-art methods.  The comparison focuses on three aspects: 3D consistency across multiple views (meaning how well the 3D model looks from different angles), generation of intact objects (meaning if the objects are complete and not partially generated), and overall quality.  The figure visually demonstrates that the proposed method (SAR3D) excels in producing highly consistent 3D models with complete objects from different viewpoints. Additional comparisons and detailed analyses are available in the supplementary materials.", "section": "5.1. Single Image to 3D"}, {"figure_path": "https://arxiv.org/html/2411.16856/x6.png", "caption": "Figure 5: More results of image and text conditioned 3D generation of SAR3D.", "description": "Figure 5 showcases more examples of 3D objects generated by the SAR3D model, demonstrating its capabilities in both image-conditioned and text-conditioned generation.  The image-conditioned examples show how the model produces detailed 3D models from a single input image. The text-conditioned examples illustrate the model's ability to generate 3D models according to textual descriptions, showcasing variations in object shape, texture, and details based on the given text prompts.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2411.16856/x7.png", "caption": "Figure 6: Comparison of Text-conditioned 3D Generation. We present text-conditioned 3D objects generated by SAR3D, displaying two views of each sample.Compared to baseline methods, our approach consistently yields better quality regarding geometry,\ntexture, and text-3D alignment.", "description": "Figure 6 showcases a comparison of text-to-3D object generation results.  SAR3D, the proposed method, is compared against several baseline approaches.  The figure displays two viewpoints of each 3D model generated from text prompts.  The comparison highlights SAR3D's superior performance in generating higher-quality 3D models with improved geometry, texture, and better alignment between the generated object and the original text description.  This demonstrates the efficacy of SAR3D in translating textual descriptions into accurate 3D representations.", "section": "5.2. Text to 3D"}, {"figure_path": "https://arxiv.org/html/2411.16856/x8.png", "caption": "Figure 7: Simultaneous 3D Generation and Captioning. Given a single image or text, SAR3D-LLM can generate both a 3D model and a descriptive caption for the model.", "description": "This figure demonstrates the capabilities of the SAR3D-LLM model to perform simultaneous 3D object generation and captioning. Given either a single image or a text description as input, the model generates a corresponding 3D model and produces a detailed caption that accurately describes the generated model's characteristics, including details such as shape, color, components, and spatial relationships between parts.", "section": "5.3 3D Captioning"}, {"figure_path": "https://arxiv.org/html/2411.16856/x9.png", "caption": "Figure 8: 3D Object Captioning. Given a 3D model, SAR3D-LLM can generate captions that include both category and details.", "description": "Figure 8 showcases the capabilities of SAR3D-LLM, the 3D object understanding component of the SAR3D framework, in generating detailed captions for input 3D models.  It demonstrates that SAR3D-LLM doesn't just identify the object's category (e.g., 'chair', 'house'), but also describes specific details such as shape, color, components, and even spatial relationships between parts of the object. This is a key aspect of the paper's claim for improved 3D understanding, moving beyond simple object classification.", "section": "5.3 3D Captioning"}, {"figure_path": "https://arxiv.org/html/2411.16856/x10.png", "caption": "Figure S1: Transformer Blocks in Our 3D Generation Transformer. The CLIP text encoder (CLIPT) or the DINOv2 image encoder processes text and image embeddings, respectively. The pooled tokens are passed through an MLP to compute the scale and shift parameters for the multi-head self-attention and feedforward network (FFN) modules. Additionally, feature vectors are incorporated into multi-head cross-attention blocks to enable cross-modal attention.", "description": "This figure details the transformer blocks used in the 3D generation process of the SAR3D model.  It shows two distinct transformer blocks: one for text-based conditioning and one for image-based conditioning.  Both blocks begin by processing input embeddings (text from CLIP and image from DINOv2). These embeddings are then passed through a multi-layer perceptron (MLP) to calculate scale and shift parameters for the subsequent self-attention and feed-forward network (FFN) layers.  Critically, both blocks incorporate cross-attention mechanisms to allow interaction between text/image features and the 3D latent representations.", "section": "4. Method"}, {"figure_path": "https://arxiv.org/html/2411.16856/x11.png", "caption": "Figure S2: Additional 3D Captioning Results. Our method generates detailed descriptions based on the input of 8 scales of latent tri-plane tokens.", "description": "Figure S2 presents supplementary results demonstrating the capabilities of SAR3D-LLM for 3D object captioning.  It showcases a variety of 3D models, each accompanied by a detailed caption generated by the model.  The captions go beyond simple object category labeling to include descriptive details about shape, texture, material, and even implied function or context.  This figure highlights the ability of SAR3D to generate comprehensive and nuanced descriptions based on the multi-scale tri-plane tokens extracted from the 3D object, demonstrating a more in-depth understanding than is typically achieved in other models.", "section": "5.3 3D Captioning"}]