{"references": [{" publication_date": "2022", "fullname_first_author": "Josh Achiam", "paper_title": "Gpt-4 technical report", "reason": "This paper is highly relevant because it provides the technical details and evaluation results for GPT-4, a large language model that is closely related to the models investigated in the study. Its comprehensive evaluation metrics and findings on model performance and capabilities provide a valuable benchmark and context for understanding the current state-of-the-art in LLMs and their challenges.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Mohammad Gheshlaghi Azar", "paper_title": "A general theoretical paradigm to understand learning from human preferences", "reason": "This paper presents a general theoretical framework for understanding learning from human preferences, which is the core concept behind reinforcement learning from human feedback (RLHF). This is highly relevant as RLHF plays a central role in the paper's focus on LLMs' overconfidence issues caused by reward model biases.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Yuntao Bai", "paper_title": "Training a helpful and harmless assistant with reinforcement learning from human feedback", "reason": "This study delves into the training of a helpful and harmless assistant through reinforcement learning from human feedback (RLHF), focusing on alignment issues and safety. It's directly relevant as RLHF is the training method at the heart of the LLMs' overconfidence problem examined in the current study, offering insights into the training and potential biases.", "section_number": 1}, {" publication_date": "2021", "fullname_first_author": "Karl Cobbe", "paper_title": "Training verifiers to solve math word problems", "reason": "This paper introduces the GSM8K dataset, which is one of the key datasets used in the experimental evaluation section. This dataset's relevance in the study is significant, as it's used as a benchmark to evaluate the effectiveness of the proposed methods in improving the calibration of LLMs.", "section_number": 4}, {" publication_date": "2017", "fullname_first_author": "Chuan Guo", "paper_title": "On calibration of modern neural networks", "reason": "This paper introduces the concept and techniques for calibrating modern neural networks, which is highly relevant as the study focuses on improving the calibration of large language models (LLMs). It provides essential background and context for the proposed calibration methods.", "section_number": 4}, {" publication_date": "2016", "fullname_first_author": "Dan Hendrycks", "paper_title": "A baseline for detecting misclassified and out-of-distribution examples in neural networks", "reason": "This paper provides a baseline for detecting misclassified and out-of-distribution examples in neural networks.  This is highly relevant to the evaluation of LLMs as it lays the groundwork for assessing performance using Expected Calibration Error (ECE) in the experiments.", "section_number": 4}, {" publication_date": "2020", "fullname_first_author": "Dan Hendrycks", "paper_title": "Measuring massive multitask language understanding", "reason": "This paper introduces the Massive Multitask Language Understanding (MMLU) benchmark, which is used in the experiments.  The importance of this benchmark stems from the fact that the MMLU benchmark is a comprehensive evaluation of LLM performance across a variety of tasks.", "section_number": 4}, {" publication_date": "2017", "fullname_first_author": "John Schulman", "paper_title": "Proximal policy optimization algorithms", "reason": "Proximal Policy Optimization (PPO) is a widely used reinforcement learning algorithm that underpins the training of many LLMs, including those in this study. Understanding PPO's inner workings is critical to appreciating the novel methods proposed, as both build upon and modify the core of the PPO training technique.", "section_number": 3}, {" publication_date": "2022", "fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "reason": "This paper lays the groundwork for Reinforcement Learning from Human Feedback (RLHF), a crucial training method for the LLMs examined in the study.  The authors' method for training LLMs is relevant to the paper's central theme of improving the calibration of LLMs trained using RLHF.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Rafael Rafailov", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "reason": "This paper is crucial because it proposes Direct Preference Optimization (DPO), an alternative approach to reward modeling in RLHF.  The study extends its proposed calibration method to DPO, highlighting the broader applicability and effectiveness of the technique beyond standard RLHF approaches.", "section_number": 5}, {" publication_date": "2019", "fullname_first_author": "Alon Talmor", "paper_title": "CommonsenseQA: A question answering challenge targeting commonsense knowledge", "reason": "The paper introduces the CommonsenseQA dataset, which is used extensively in the initial experiments section of the current study to illustrate and analyze the phenomenon of overconfidence in RLHF-LLMs.  The significance of this dataset lies in its use for demonstrating the core problem that the proposed solutions address.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Katherine Tian", "paper_title": "Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback", "reason": "This paper directly addresses the issue of overconfidence in language models, particularly focusing on techniques for improving calibration. The ideas in this paper are directly relevant to the current paper's central topic of calibrating LLMs, providing a comparative framework and a basis for improving upon existing methods.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Miao Xiong", "paper_title": "Uncertainty quantification with pre-trained language models: A large-scale empirical analysis", "reason": "This paper is a significant contribution as it focuses on methods for quantifying uncertainty with pre-trained language models.  It's relevant to the current study as both examine the issue of uncertainty in LLM output, providing a foundation for the investigation of overconfidence in RLHF LLMs.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Tianyang Xu", "paper_title": "Sayself: Teaching llms to express confidence with self-reflective rationales", "reason": "This paper directly addresses LLM calibration by focusing on methods for enabling LLMs to express confidence more accurately. The approach of explicitly eliciting confidence is comparable to the verbalized confidence technique in the current study, making it important to the understanding and development of LLM calibration methods.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Bianca Zadrozny", "paper_title": "Obtaining calibrated probability estimates from decision trees and naive bayesian classifiers", "reason": "This paper addresses the crucial problem of obtaining calibrated probability estimates from decision trees and naive Bayesian classifiers. This work is important due to its focus on probability calibration, a technique that is highly relevant to the study of LLMs' calibration, providing background information on established techniques for probability calibration.", "section_number": 4}, {" publication_date": "2020", "fullname_first_author": "Jize Zhang", "paper_title": "Mix-n-match: Ensemble and compositional methods for uncertainty calibration in deep learning", "reason": "This paper addresses the challenge of uncertainty calibration in deep learning, which is relevant to the current study's focus on LLMs and calibration techniques.  The proposed 'Mix-n-Match' methods provide alternative calibration strategies which are important for comparison and contrast to the methods being developed in the current study.", "section_number": 4}, {" publication_date": "2022", "fullname_first_author": "Jason Wei", "paper_title": "Chain-of-thought prompting elicits reasoning in large language models", "reason": "This paper introduces the concept of chain-of-thought prompting, a technique used in the experiments to elicit more detailed reasoning from the LLMs. Chain-of-thought prompting plays a role in the experimental methodology and results, making this paper an important reference for understanding the experimental design and interpretation.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Zhilin Wang", "paper_title": "Judging llm-as-a-judge with mt-bench and chatbot arena", "reason": "This paper is highly relevant to the study due to the fact that it introduces MT-Bench and Arena-Hard, two important datasets used in the evaluation of instruction following capabilities of LLMs after implementing the calibration techniques.  The relevance of these benchmarks comes from their assessment of LLMs' capabilities in various aspects, providing a more comprehensive evaluation of the impact of the proposed methods.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Rafael Rafailov", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "reason": "This paper is highly relevant to the study's extension of reward calibration methods to Direct Preference Optimization (DPO).  Understanding DPO is crucial for evaluating the proposed method's applicability and effectiveness beyond standard RLHF approaches.", "section_number": 5}]}