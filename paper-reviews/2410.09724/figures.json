[{"figure_path": "2410.09724/figures/figures_2_0.png", "caption": "Figure 1: (Top): Illustration of verbalized confidence generation. An LLM incorrectly answers a question with high confidence. (Bottom): Comparison between reward scores from a vanilla-trained reward model Llama-3-8b-rm-mixture and our calibrated reward model Llama-3-8b-crm. The vanilla model shows bias towards high confidence though the answer is incorrect. Our calibrated reward model correctly assigns a higher reward to the low confidence for the incorrect answer.", "description": "The figure illustrates the bias of vanilla reward models towards high-confidence responses regardless of accuracy and how a calibrated reward model corrects this bias.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.09724/figures/figures_5_0.png", "caption": "Figure 5: Framework for PPO-C.", "description": "The figure illustrates the framework of PPO-C, showing how it adjusts the reward score based on the model's expressed verbalized confidence and a moving average of past rewards.", "section": "3 CALIBRATED REWARD MODELING AND CALCULATION"}]