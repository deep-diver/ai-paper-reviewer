[{"figure_path": "2410.09724/charts/charts_3_0.png", "caption": "Figure 2: Confidence distributions and corresponding accuracy of two models on CommonsenseQA before and after RLHF. Darker color means more samples fall in that confidence bin. The red dashed line indicates perfect calibration.", "description": "The chart displays the confidence distributions and corresponding accuracy of two models (Llama3-8B and Tulu-2-7B) on the CommonsenseQA dataset before and after RLHF training, illustrating the overconfidence phenomenon in RLHF models.", "section": "2.1 RLHF-LLMS EXHIBIT OVERCONFIDENCE IN THEIR VERBALIZED CONFIDENCE"}, {"figure_path": "2410.09724/charts/charts_7_0.png", "caption": "Figure 20: Comparison of preference distributions between the calibrated reward model Llama-3-8b-crm and the pre-calibrated version Llama-3-8b-rm-mixture on two modes: CHOSEN_WITH_CONF and REJECTED_WITH_CONF.", "description": "The chart compares preference distributions for calibrated versus pre-calibrated reward models across conditions where responses were chosen or rejected, and confidence scores were high or low.", "section": "E.3 CALIBRATED REWARD MODELS"}, {"figure_path": "2410.09724/charts/charts_25_0.png", "caption": "Figure 2: Confidence distributions and corresponding accuracy of two models on CommonsenseQA before and after RLHF. Darker color means more samples fall in that confidence bin. The red dashed line indicates perfect calibration.", "description": "The chart displays the confidence distributions and corresponding accuracy of two models (Llama-3-8B and Tulu-2-7B) on the CommonsenseQA dataset before and after RLHF training, illustrating the overconfidence phenomenon in RLHF-LLMs.", "section": "2.1 RLHF-LLMS EXHIBIT OVERCONFIDENCE IN THEIR VERBALIZED CONFIDENCE"}, {"figure_path": "2410.09724/charts/charts_25_1.png", "caption": "Figure 2: Confidence distributions and corresponding accuracy of two models on CommonsenseQA before and after RLHF. Darker color means more samples fall in that confidence bin. The red dashed line indicates perfect calibration.", "description": "Figure 2 presents the confidence distributions and corresponding accuracy of two models (Llama-3-8B and Tulu-2-7B) on CommonsenseQA, comparing their performance before and after RLHF training.", "section": "2.1 RLHF-LLMS EXHIBIT OVERCONFIDENCE IN THEIR VERBALIZED CONFIDENCE"}, {"figure_path": "2410.09724/charts/charts_25_2.png", "caption": "Figure 14: Confidence distributions of two models on Professional Knowledge before and after RLHF.", "description": "The chart displays the accuracy within confidence bins for two language models (Llama-3-8b and Tulu-2) on a professional knowledge dataset, before and after reinforcement learning from human feedback (RLHF).", "section": "2.1 RLHF-LLMS EXHIBIT OVERCONFIDENCE IN THEIR VERBALIZED CONFIDENCE"}, {"figure_path": "2410.09724/charts/charts_25_3.png", "caption": "Figure 2: Confidence distributions and corresponding accuracy of two models on CommonsenseQA before and after RLHF. Darker color means more samples fall in that confidence bin. The red dashed line indicates perfect calibration.", "description": "The chart displays the confidence distributions and corresponding accuracy of two models (Llama-3-8B and Tulu-2-7B) on CommonsenseQA before and after RLHF, showing the overconfidence phenomenon in RLHF models.", "section": "2.1 RLHF-LLMS EXHIBIT OVERCONFIDENCE IN THEIR VERBALIZED CONFIDENCE"}, {"figure_path": "2410.09724/charts/charts_25_4.png", "caption": "Figure 14: Confidence distributions of two models on Professional Knowledge before and after RLHF.", "description": "The chart displays the accuracy within bins versus confidence for two models (Llama-3-8b and Tulu-2) on the Professional Knowledge dataset, before and after Reinforcement Learning from Human Feedback (RLHF).", "section": "2.1 RLHF-LLMS EXHIBIT OVERCONFIDENCE IN THEIR VERBALIZED CONFIDENCE"}, {"figure_path": "2410.09724/charts/charts_25_5.png", "caption": "Figure 2: Confidence distributions and corresponding accuracy of two models on CommonsenseQA before and after RLHF. Darker color means more samples fall in that confidence bin. The red dashed line indicates perfect calibration.", "description": "The chart displays the confidence distributions and corresponding accuracy of two models (Llama3-8B and Tulu-2-7B) on the CommonsenseQA dataset before and after RLHF training, illustrating the overconfidence phenomenon in RLHF-LLMs.", "section": "2.1 RLHF-LLMS EXHIBIT OVERCONFIDENCE IN THEIR VERBALIZED CONFIDENCE"}, {"figure_path": "2410.09724/charts/charts_26_0.png", "caption": "Figure 2: Confidence distributions and corresponding accuracy of two models on CommonsenseQA before and after RLHF. Darker color means more samples fall in that confidence bin. The red dashed line indicates perfect calibration.", "description": "The chart displays the confidence distributions and accuracy of two models (Llama-3-8B and Tulu-2-7B) on the CommonsenseQA dataset before and after Reinforcement Learning from Human Feedback (RLHF), showing the overconfidence phenomenon in RLHF models.", "section": "2.1 RLHF-LLMS EXHIBIT OVERCONFIDENCE IN THEIR VERBALIZED CONFIDENCE"}, {"figure_path": "2410.09724/charts/charts_26_1.png", "caption": "Figure 2: Confidence distributions and corresponding accuracy of two models on CommonsenseQA before and after RLHF. Darker color means more samples fall in that confidence bin. The red dashed line indicates perfect calibration.", "description": "The chart displays the confidence distributions and corresponding accuracy of two models (Llama3-8B and Tulu-2-7B) on the CommonsenseQA dataset before and after RLHF training, illustrating the overconfidence phenomenon in RLHF-LLMs.", "section": "2.1 RLHF-LLMs EXHIBIT OVERCONFIDENCE IN THEIR VERBALIZED CONFIDENCE"}, {"figure_path": "2410.09724/charts/charts_27_0.png", "caption": "Figure 3: Preference distributions for ArmoRM-Llama3-8B-v0.1, a reward model for PPO training (left) and Tulu-2-DPO-7B, a DPO model (right) on the modified RewardBench dataset across four modes. From top to bottom: CONFIDENCE_REVERSED, CHOSEN_WITH_CONF, REJECTED_WITH_CONF, ANSWER_ONLY. Red bar indicates the preference for a rejected or high-confidence response, and blue bar indicates the preference for a chosen or low-confidence response.", "description": "The chart displays the preference distributions of two reward models (ArmoRM-Llama3-8B-v0.1 and Tulu-2-DPO-7B) across four different conditions showing the models' bias toward high-confidence responses regardless of the response's correctness.", "section": "2 EXPLORING SYSTEMATIC BIASES AND OVERCONFIDENCE IN RLHF-LLMs"}, {"figure_path": "2410.09724/charts/charts_27_1.png", "caption": "Figure 3: Preference distributions for ArmoRM-Llama3-8B-v0.1, a reward model for PPO training (left) and Tulu-2-DPO-7B, a DPO model (right) on the modified RewardBench dataset across four modes. From top to bottom: CONFIDENCE_REVERSED, CHOSEN_WITH_CONF, REJECTED_WITH_CONF, ANSWER_ONLY. Red bar indicates the preference for a rejected or high-confidence response, and blue bar indicates the preference for a chosen or low-confidence response.", "description": "The chart compares preference distributions for vanilla and calibrated reward models on the modified RewardBench dataset across four different modes, showing a bias towards high-confidence responses in the vanilla model.", "section": "2 EXPLORING SYSTEMATIC BIASES AND OVERCONFIDENCE IN RLHF-LLMs"}, {"figure_path": "2410.09724/charts/charts_27_2.png", "caption": "Figure 3: Preference distributions for ArmoRM-Llama3-8B-v0.1, a reward model for PPO training (left) and Tulu-2-DPO-7B, a DPO model (right) on the modified RewardBench dataset across four modes. From top to bottom: CONFIDENCE_REVERSED, CHOSEN_WITH_CONF, REJECTED_WITH_CONF, ANSWER_ONLY. Red bar indicates the preference for a rejected or high-confidence response, and blue bar indicates the preference for a chosen or low-confidence response.", "description": "The chart compares the preference distributions of two reward models (ArmoRM-Llama3-8B-v0.1 and Tulu-2-DPO-7B) across four different scenarios, showing a bias towards high confidence responses.", "section": "2 EXPLORING SYSTEMATIC BIASES AND OVERCONFIDENCE IN RLHF-LLMs"}, {"figure_path": "2410.09724/charts/charts_27_3.png", "caption": "Figure 3: Preference distributions for ArmoRM-Llama3-8B-v0.1, a reward model for PPO training (left) and Tulu-2-DPO-7B, a DPO model (right) on the modified RewardBench dataset across four modes. From top to bottom: CONFIDENCE_REVERSED, CHOSEN_WITH_CONF, REJECTED_WITH_CONF, ANSWER_ONLY. Red bar indicates the preference for a rejected or high-confidence response, and blue bar indicates the preference for a chosen or low-confidence response.", "description": "The chart compares preference distributions for vanilla and calibrated reward models across four scenarios, showing a bias towards high confidence responses in vanilla models.", "section": "2 EXPLORING SYSTEMATIC BIASES AND OVERCONFIDENCE IN RLHF-LLMs"}, {"figure_path": "2410.09724/charts/charts_28_0.png", "caption": "Figure 3: Preference distributions for ArmoRM-Llama3-8B-v0.1, a reward model for PPO training (left) and Tulu-2-DPO-7B, a DPO model (right) on the modified RewardBench dataset across four modes. From top to bottom: CONFIDENCE_REVERSED, CHOSEN_WITH_CONF, REJECTED_WITH_CONF, ANSWER_ONLY. Red bar indicates the preference for a rejected or high-confidence response, and blue bar indicates the preference for a chosen or low-confidence response.", "description": "The chart displays preference distributions for two reward models (ArmoRM-Llama3-8B-v0.1 and Tulu-2-DPO-7B) across four conditions of the modified RewardBench dataset, showing reward model biases toward high confidence responses regardless of correctness.", "section": "2 EXPLORING SYSTEMATIC BIASES AND OVERCONFIDENCE IN RLHF-LLMs"}, {"figure_path": "2410.09724/charts/charts_28_1.png", "caption": "Figure 3: Preference distributions for ArmoRM-Llama3-8B-v0.1, a reward model for PPO training (left) and Tulu-2-DPO-7B, a DPO model (right) on the modified RewardBench dataset across four modes. From top to bottom: CONFIDENCE_REVERSED, CHOSEN_WITH_CONF, REJECTED_WITH_CONF, ANSWER_ONLY. Red bar indicates the preference for a rejected or high-confidence response, and blue bar indicates the preference for a chosen or low-confidence response.", "description": "The chart compares the preference distributions of two reward models (ArmoRM-Llama3-8B-v0.1 and Tulu-2-DPO-7B) across four different conditions, revealing a bias towards high-confidence responses.", "section": "2 EXPLORING SYSTEMATIC BIASES AND OVERCONFIDENCE IN RLHF-LLMS"}, {"figure_path": "2410.09724/charts/charts_28_2.png", "caption": "Figure 3: Preference distributions for ArmoRM-Llama3-8B-v0.1, a reward model for PPO training (left) and Tulu-2-DPO-7B, a DPO model (right) on the modified RewardBench dataset across four modes. From top to bottom: CONFIDENCE_REVERSED, CHOSEN_WITH_CONF, REJECTED_WITH_CONF, ANSWER_ONLY. Red bar indicates the preference for a rejected or high-confidence response, and blue bar indicates the preference for a chosen or low-confidence response.", "description": "The chart compares the preference distributions of two reward models (ArmoRM-Llama3-8B-v0.1 and Tulu-2-DPO-7B) across four different scenarios, showing a bias towards high-confidence responses regardless of correctness.", "section": "2 EXPLORING SYSTEMATIC BIASES AND OVERCONFIDENCE IN RLHF-LLMs"}, {"figure_path": "2410.09724/charts/charts_29_0.png", "caption": "Figure 3: Preference distributions for ArmoRM-Llama3-8B-v0.1, a reward model for PPO training (left) and Tulu-2-DPO-7B, a DPO model (right) on the modified RewardBench dataset across four modes. From top to bottom: CONFIDENCE_REVERSED, CHOSEN_WITH_CONF, REJECTED_WITH_CONF, ANSWER_ONLY. Red bar indicates the preference for a rejected or high-confidence response, and blue bar indicates the preference for a chosen or low-confidence response.", "description": "The chart compares the preference distributions of vanilla and calibrated reward models across different scenarios, revealing a bias toward high-confidence responses in vanilla models.", "section": "2 EXPLORING SYSTEMATIC BIASES AND OVERCONFIDENCE IN RLHF-LLMs"}, {"figure_path": "2410.09724/charts/charts_29_1.png", "caption": "Figure 3: Preference distributions for ArmoRM-Llama3-8B-v0.1, a reward model for PPO training (left) and Tulu-2-DPO-7B, a DPO model (right) on the modified RewardBench dataset across four modes. From top to bottom: CONFIDENCE_REVERSED, CHOSEN_WITH_CONF, REJECTED_WITH_CONF, ANSWER_ONLY. Red bar indicates the preference for a rejected or high-confidence response, and blue bar indicates the preference for a chosen or low-confidence response.", "description": "The chart compares the preference distributions of vanilla and calibrated reward models on the modified RewardBench dataset across four different scenarios.", "section": "2 EXPLORING SYSTEMATIC BIASES AND OVERCONFIDENCE IN RLHF-LLMS"}, {"figure_path": "2410.09724/charts/charts_29_2.png", "caption": "Figure 3: Preference distributions for ArmoRM-Llama3-8B-v0.1, a reward model for PPO training (left) and Tulu-2-DPO-7B, a DPO model (right) on the modified RewardBench dataset across four modes. From top to bottom: CONFIDENCE_REVERSED, CHOSEN_WITH_CONF, REJECTED_WITH_CONF, ANSWER_ONLY. Red bar indicates the preference for a rejected or high-confidence response, and blue bar indicates the preference for a chosen or low-confidence response.", "description": "The chart compares the preference distributions of two reward models (ArmoRM-Llama3-8B-v0.1 and Tulu-2-DPO-7B) across four different experimental conditions, revealing their biases toward high-confidence responses.", "section": "2 EXPLORING SYSTEMATIC BIASES AND OVERCONFIDENCE IN RLHF-LLMS"}, {"figure_path": "2410.09724/charts/charts_30_0.png", "caption": "Figure 20: Comparison of preference distributions between the calibrated reward model Llama-3-8b-crm and the pre-calibrated version Llama-3-8b-rm-mixture on two modes: CHOSEN_WITH_CONF and REJECTED_WITH_CONF.", "description": "The chart compares the preference distributions for chosen and rejected responses with high and low confidence scores between a calibrated and pre-calibrated reward model.", "section": "E.3 CALIBRATED REWARD MODELS"}, {"figure_path": "2410.09724/charts/charts_30_1.png", "caption": "Figure 21: Comparison of preference distributions between the calibrated reward model Mistral-7B-crm and the pre-calibrated version Mistral-7B-RM on two modes: CHOSEN_WITH_CONF and REJECTED_WITH_CONF.", "description": "The chart compares the preference distributions of calibrated and pre-calibrated reward models for chosen and rejected responses with high and low confidence scores.", "section": "E.3 CALIBRATED REWARD MODELS"}, {"figure_path": "2410.09724/charts/charts_30_2.png", "caption": "Figure 20: Comparison of preference distributions between the calibrated reward model Llama-3-8b-crm and the pre-calibrated version Llama-3-8b-rm-mixture on two modes: CHOSEN_WITH_CONF and REJECTED_WITH_CONF.", "description": "The chart compares preference distributions for calibrated versus pre-calibrated reward models across \"chosen with confidence\" and \"rejected with confidence\" response categories.", "section": "E.3 CALIBRATED REWARD MODELS"}]