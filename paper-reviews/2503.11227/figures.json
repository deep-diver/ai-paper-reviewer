[{"figure_path": "https://arxiv.org/html/2503.11227/extracted/6285883/figures/example.jpg", "caption": "Figure 1: An illustration of several triples and graphs. The left half shows a generalized knowledge graph. The right half includes specific examples of triples from KG, EKG, CKG and demonstrates their progressive relationship.", "description": "Figure 1 illustrates the concept of a Generalized Knowledge Graph (GKG) and its constituent parts: Knowledge Graphs (KGs), Event Knowledge Graphs (EKGs), and Commonsense Knowledge Graphs (CKGs). The left side shows a visual representation of the GKG, highlighting its interconnected nature. The right side provides concrete examples of triples (subject, predicate, object) from each graph type, demonstrating how they build upon one another in terms of complexity and scope. KGs represent facts about entities and their relationships. EKGs extend this by modeling events and their temporal relationships. CKGs further generalize these concepts, focusing on abstract commonsense relationships.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2503.11227/extracted/6285883/figures/data_dis.png", "caption": "Figure 2: The illustration of the data distribution for all GKG sub-tasks.", "description": "Figure 2 shows the distribution of the datasets used for training and evaluating the Generalized Knowledge Graph (GKG) construction task. It visually represents the various sub-tasks involved in constructing three types of graphs: Knowledge Graphs (KGs), Event Knowledge Graphs (EKGs), and Commonsense Knowledge Graphs (CKGs). Each sub-task is associated with one or more datasets, indicating the data sources utilized for that specific task. This figure provides a comprehensive overview of the data utilized for training and testing in the study, highlighting the diverse range of tasks and datasets involved in the GKG construction process.", "section": "2 Methodology"}, {"figure_path": "https://arxiv.org/html/2503.11227/extracted/6285883/figures/structure3.png", "caption": "Figure 3: Three-stage curriculum learning tuning framework of GKG-LLM. The upper part represents the GKG dataset \ud835\udc9fGsubscript\ud835\udc9f\ud835\udc3a\\mathcal{D}_{G}caligraphic_D start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT, consisting of the unified datasets. The lower part shows the three stages of GKG training: the KG empowerment stage using the KG datasets to build foundational skills, the EKG enhancement stage using the EKG datasets to enhance specific capabilities, and the CKG generalization stage using the CKG datasets and the counter task dataset to achieve generalization of the GKG-LLM capabilities. The thick arrows between the stages represent the delivery of model parameters from base model to each version of GKG-LLM.", "description": "This figure illustrates the three-stage curriculum learning framework used to train the GKG-LLM model.  The top section shows the unified GKG dataset (\ud835\udc9fG) comprising data from KG, EKG, and CKG sub-tasks, as well as a counter task dataset. The bottom section details the three training stages: 1) KG Empowerment: foundational skills are built using KG datasets; 2) EKG Enhancement: specific capabilities are enhanced using EKG datasets; and 3) CKG Generalization: generalization is achieved using CKG and counter task datasets. Thick arrows indicate the transfer of model parameters between stages, starting from a base model to create the G-Micro, G-Mid, and finally, the GKG-LLM model.", "section": "Methodology"}, {"figure_path": "https://arxiv.org/html/2503.11227/extracted/6285883/figures/2.png", "caption": "Figure 4: Results of different fine-tuning orders. \u201cK-E-C\u201d means the fine-tuning order is KG, EKG and CKG. The following sets of experiments are similar to this one.", "description": "This figure displays the average performance across all tasks for three different fine-tuning orders: K-E-C (KG, EKG, then CKG), K-C-E (KG, CKG, then EKG), E-K-C (EKG, KG, then CKG), E-C-K (EKG, CKG, then KG), C-K-E (CKG, KG, then EKG), and C-E-K (CKG, EKG, then KG).  The results show varying performance across the different sequences, demonstrating the impact of the chosen order on overall model performance.  The K-E-C sequence used in the main study demonstrates the best overall performance, supporting the study's methodology and indicating that a progressive training approach, starting with simpler knowledge graphs and progressing to more complex ones, yields the best results. ", "section": "3.3 Exploration of Three Stages"}, {"figure_path": "https://arxiv.org/html/2503.11227/extracted/6285883/figures/1.png", "caption": "Figure 5: Fine-tuning with a single type of graph and performance of different intermediate version in the GKG-LLM.", "description": "Figure 5 displays the performance comparison between using a single type of graph for fine-tuning and the three-stage fine-tuning approach of GKG-LLM.  The graph shows performance across KG, EKG, and CKG sub-tasks. It demonstrates that the three-stage curriculum learning process (KG empowerment, EKG enhancement, CKG generalization) progressively improves the model's capability across all three graph types, significantly outperforming the single-graph fine-tuning method.", "section": "3.3 Exploration of Three Stages"}, {"figure_path": "https://arxiv.org/html/2503.11227/extracted/6285883/figures/datascaling1.png", "caption": "Figure 6: Results of training with different proportions of complete data.", "description": "This figure displays the impact of varying training dataset sizes on the model's performance.  The x-axis represents the percentage of the complete dataset used for training (10%, 20%, 40%, 60%, 80%, and 100%). The y-axis shows the average F1 score achieved across all tasks.  The figure demonstrates the relationship between the amount of training data and the model's performance across three types of knowledge graph sub-tasks (KG, EKG, CKG), and an overall GKG average.  It helps illustrate the point of diminishing returns in terms of performance gain as training data increases.", "section": "4.3 Analysis on Different Data Scaling"}, {"figure_path": "https://arxiv.org/html/2503.11227/extracted/6285883/figures/OOD.png", "caption": "Figure 7: The average performance on OOD datasets, consisting TCR, Causal-TB and R8 datasets.", "description": "Figure 7 presents the average performance of various models on out-of-distribution (OOD) datasets.  The OOD datasets used are TCR, Causal-TB, and R8, each representing a distinct and challenging test scenario.  The figure displays the average F1 score (or other relevant metric) achieved by each model on these three datasets, allowing for a direct comparison of their robustness and generalization capabilities in handling unseen data.", "section": "3.2 Main Results"}, {"figure_path": "https://arxiv.org/html/2503.11227/extracted/6285883/figures/dataFormat2.jpg", "caption": "Figure 8: An example from the WIKEVENTS dataset. It consists of five fields I\u2062D\ud835\udc3c\ud835\udc37IDitalic_I italic_D, instruction sisubscript\ud835\udc60\ud835\udc56s_{i}italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, few-shot f\u2062s\ud835\udc53\ud835\udc60fsitalic_f italic_s / zero-shot z\u2062s\ud835\udc67\ud835\udc60zsitalic_z italic_s , input xisubscript\ud835\udc65\ud835\udc56x_{i}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, and output yisubscript\ud835\udc66\ud835\udc56y_{i}italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT.", "description": "Figure 8 shows an example data entry from the WIKIEVENTS dataset, illustrating the standardized format used throughout the paper.  Each data entry consists of five key fields:  'ID' uniquely identifies the specific data point; 'instruction' provides the task instructions given to the model; 'few-shot/zero-shot' indicates whether few-shot learning examples were provided; 'input' contains the input data given to the model for processing; and 'output' shows the expected output or ground truth. This standardized format enables the unified processing of diverse sub-tasks within the generalized knowledge graph (GKG) construction framework.", "section": "2.2 Data Collection and Preparation"}, {"figure_path": "https://arxiv.org/html/2503.11227/extracted/6285883/figures/stageGeneralization2.png", "caption": "Figure 9: Comparison of Results by different settings and GKG-LLM.", "description": "Figure 9 displays a bar chart comparing the performance of three different model training strategies against the performance of the GKG-LLM model. The three strategies represent progressively more complex training approaches.  The chart shows the average F1 scores achieved on three different types of knowledge graph sub-tasks (KG, EKG, CKG) for each model.  This helps visualize how a three-stage curriculum learning approach enhances the model's ability to handle various tasks compared to single-stage training methods.", "section": "3.3 Exploration of Three Stages"}, {"figure_path": "https://arxiv.org/html/2503.11227/extracted/6285883/figures/hyperparameters1.png", "caption": "Figure 10: Heatmap of Scores for Different \u03b7Asubscript\ud835\udf02\ud835\udc34\\eta_{A}italic_\u03b7 start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPT and \u03b7Bsubscript\ud835\udf02\ud835\udc35\\eta_{B}italic_\u03b7 start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT Values for our training strategy.", "description": "This heatmap visualizes the performance of the GKG-LLM model under different hyperparameter settings for the LoRA+ fine-tuning technique.  The x-axis represents the value of \u03b7B (eta_B), and the y-axis represents the value of \u03b7A (eta_A). Each cell in the heatmap shows the model's performance (likely F1-score or accuracy) on the GKG construction task under the corresponding \u03b7A and \u03b7B values. The color intensity represents the performance level, with darker colors indicating better performance. This figure is crucial for determining the optimal hyperparameter combination to maximize the model's effectiveness.", "section": "D Exploration of LoRA+ Hyperparameter"}]