[{"figure_path": "2410.17243/figures/figures_2_0.png", "caption": "Figure 2: (a) Vanilla implementation of contrastive loss gathers features to all devices to calculate all similarity simultaneously, where the similarity with squared complexity are repeatedly stored in all devices, causing huge memory costs for loss calculation when batch size increases. (b) Our Inf-CL significant decreases the memory cost by serial and distributed tile-wise computation.", "description": "The figure compares the vanilla implementation of contrastive loss with the proposed Inf-CL method, highlighting how Inf-CL reduces memory costs by using a tile-wise computation strategy.", "section": "2.2 VANILLA IMPLEMENTATION OF CONTRASTIVE LOSS"}, {"figure_path": "2410.17243/figures/figures_2_1.png", "caption": "Figure 2: (a) Vanilla implementation of contrastive loss gathers features to all devices to calculate all similarity simultaneously, where the similarity with squared complexity are repeatedly stored in all devices, causing huge memory costs for loss calculation when batch size increases. (b) Our Inf-CL significant decreases the memory cost by serial and distributed tile-wise computation.", "description": "The figure illustrates the difference in memory usage and computational flow between the vanilla implementation of contrastive loss and the proposed Inf-CL method, highlighting the memory efficiency gains achieved by Inf-CL.", "section": "2.2 VANILLA IMPLEMENTATION OF CONTRASTIVE LOSS"}, {"figure_path": "2410.17243/figures/figures_4_0.png", "caption": "Figure 1: GPU memory usage comparison between Inf-CL and previous methods (CLIP, Open-CLIP). The dashed line marks the common GPU memory limit. Memory costs exceeding the bottleneck of 80G A800 are estimated by curve fitting. Left: With 8\u00d7A800, CLIP and OpenCLIP's memory consumption increases quadratically, while Inf-CL achieves linear growth, reducing memory costs by 78\u00d7 at a batch size of 256k. Right: At a batch size of 1024k, even with 128 GPUs, previous methods exceed memory limits, whereas Inf-CL reduces memory demand by 281\u00d7.", "description": "The figure shows a comparison of GPU memory usage for contrastive loss training between Inf-CL and other methods (CLIP and OpenCLIP) across different batch sizes and numbers of GPUs.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.17243/figures/figures_5_0.png", "caption": "Figure 3: Multi-level tiling strategy. Top: for cross-GPU tiling, each GPU is assigned with multiple rows. The computation and the column-wise communication are performed asynchronously to reduce the cost. Bottom: for in-GPU tiling, the calculations in each GPU are further divided into tiles and the row-wise calculation is distributed to multiple CUDA cores. The accumulative operations of each row are merged into one kernel for reducing I/O times between SRAM and HBM.", "description": "This figure illustrates the multi-level tiling strategy used in Inf-CL to reduce memory consumption and optimize performance by distributing computations across multiple GPUs and CUDA cores.", "section": "3.2 MULTI-LEVEL TILING"}]