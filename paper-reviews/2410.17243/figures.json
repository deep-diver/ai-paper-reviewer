[{"figure_path": "2410.17243/figures/figures_2_0.png", "caption": "Figure 2: (a) Vanilla implementation of contrastive loss gathers features to all devices to calculate all similarity simultaneously, where the similarity with squared complexity are repeatedly stored in all devices, causing huge memory costs for loss calculation when batch size increases. (b) Our Inf-CL significant decreases the memory cost by serial and distributed tile-wise computation.", "description": "The figure compares the vanilla implementation of contrastive loss with the proposed Inf-CL method, highlighting the differences in memory usage and computation.", "section": "2.2 VANILLA IMPLEMENTATION OF CONTRASTIVE LOSS"}, {"figure_path": "2410.17243/figures/figures_2_1.png", "caption": "Figure 2: (a) Vanilla implementation of contrastive loss gathers features to all devices to calculate all similarity simultaneously, where the similarity with squared complexity are repeatedly stored in all devices, causing huge memory costs for loss calculation when batch size increases. (b) Our Inf-CL significant decreases the memory cost by serial and distributed tile-wise computation.", "description": "The figure compares the vanilla implementation of contrastive loss with the proposed Inf-CL method, highlighting how Inf-CL reduces memory costs through tile-wise computation.", "section": "2.2 VANILLA IMPLEMENTATION OF CONTRASTIVE LOSS"}, {"figure_path": "2410.17243/figures/figures_4_0.png", "caption": "Figure 1: GPU memory usage comparison between Inf-CL and previous methods (CLIP, Open-CLIP). The dashed line marks the common GPU memory limit. Memory costs exceeding the bottleneck of 80G A800 are estimated by curve fitting. Left: With 8\u00d7A800, CLIP and OpenCLIP's memory consumption increases quadratically, while Inf-CL achieves linear growth, reducing memory costs by 78\u00d7 at a batch size of 256k. Right: At a batch size of 1024k, even with 128 GPUs, previous methods exceed memory limits, whereas Inf-CL reduces memory demand by 281\u00d7.", "description": "The figure compares the GPU memory usage of Inf-CL with CLIP and OpenCLIP, showing that Inf-CL significantly reduces memory consumption with increasing batch size.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.17243/figures/figures_5_0.png", "caption": "Figure 3: Multi-level tiling strategy. Top: for cross-GPU tiling, each GPU is assigned with multiple rows. The computation and the column-wise communication are performed asynchronously to reduce the cost. Bottom: for in-GPU tiling, the calculations in each GPU are further divided into tiles and the row-wise calculation is distributed to multiple CUDA cores. The accumulative operations of each row are merged into one kernel for reducing I/O times between SRAM and HBM.", "description": "This figure illustrates the multi-level tiling strategy used in Inf-CL to reduce memory consumption during contrastive loss calculations by distributing computations across multiple GPUs and CUDA cores.", "section": "3.2 MULTI-LEVEL TILING"}]