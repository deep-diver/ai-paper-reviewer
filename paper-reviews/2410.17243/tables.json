[{"figure_path": "2410.17243/tables/table_6_1.html", "caption": "Table 1: Training Memory Cost Across Different Hardware and Batch Sizes. Experiments utilize Data Parallelism with Automatic Mixed Precision for efficient distributed training. The baselines include the Vanilla loss (CLIP) and Local loss (OpenCLIP). To minimize memory consumption, Gradient Cache is adopted, with an accumulation batch size of 128. * indicates the use of the data offload strategy, which reduces memory usage by transferring only a small data batch from CPU to GPU during each accumulation step. X denotes cases where the baseline exceeds the hardware memory limit for a given batch size, making training infeasible. Memory cost is evaluated using the ViT-L/14 architecture and the AdamW optimizer.", "description": "Table 1 shows the peak memory usage of CLIP, OpenCLIP, and Inf-CL with varying batch sizes and different numbers of GPUs, highlighting Inf-CL's significantly reduced memory consumption.", "section": "4 EXPERIMENTS"}, {"figure_path": "2410.17243/tables/table_7_0.html", "caption": "Table 1: Training Memory Cost Across Different Hardware and Batch Sizes. Experiments utilize Data Parallelism with Automatic Mixed Precision for efficient distributed training. The baselines include the Vanilla loss (CLIP) and Local loss (OpenCLIP). To minimize memory consumption, Gradient Cache is adopted, with an accumulation batch size of 128. * indicates the use of the data offload strategy, which reduces memory usage by transferring only a small data batch from CPU to GPU during each accumulation step. X denotes cases where the baseline exceeds the hardware memory limit for a given batch size, making training infeasible. Memory cost is evaluated using the ViT-L/14 architecture and the AdamW optimizer.", "description": "Table 1 shows the peak memory usage during training for CLIP, OpenCLIP, and Inf-CL with varying batch sizes and hardware configurations.", "section": "4 EXPERIMENTS"}, {"figure_path": "2410.17243/tables/table_8_0.html", "caption": "Table 1: Training Memory Cost Across Different Hardware and Batch Sizes. Experiments utilize Data Parallelism with Automatic Mixed Precision for efficient distributed training. The baselines include the Vanilla loss (CLIP) and Local loss (OpenCLIP). To minimize memory consumption, Gradient Cache is adopted, with an accumulation batch size of 128. * indicates the use of the data offload strategy, which reduces memory usage by transferring only a small data batch from CPU to GPU during each accumulation step. X denotes cases where the baseline exceeds the hardware memory limit for a given batch size, making training infeasible. Memory cost is evaluated using the ViT-L/14 architecture and the AdamW optimizer.", "description": "Table 1 shows the peak memory usage of CLIP, OpenCLIP and Inf-CL with different batch sizes and hardware configurations.", "section": "4 EXPERIMENTS"}, {"figure_path": "2410.17243/tables/table_9_0.html", "caption": "Table 3: Performance Verification. The training strategies is consistent with Table 2. We choose ViT-B/16 as the model architecture and adopt LiT strategy like Table 4. We evaluate zero-shot top-1 classification accuracy on several data sets, e.g., ImageNet-Validation Deng et al. (2009), ImageNet-v2 (Recht et al., 2019), ObjectNet (Barbu et al., 2019) and ImageNet-OOD (Hendrycks et al., 2021). We also evaluate zero-shot image-text top-1 retrieval accuracy on MSCOCO (Chen et al., 2015).", "description": "Table 3 presents a comparison of the performance of different methods on various image classification and image-text retrieval tasks, showing the impact of batch size on the Inf-CL method.", "section": "4.3 Performance Analysis"}, {"figure_path": "2410.17243/tables/table_9_1.html", "caption": "Table 1: Training Memory Cost Across Different Hardware and Batch Sizes. Experiments utilize Data Parallelism with Automatic Mixed Precision for efficient distributed training. The baselines include the Vanilla loss (CLIP) and Local loss (OpenCLIP). To minimize memory consumption, Gradient Cache is adopted, with an accumulation batch size of 128. * indicates the use of the data offload strategy, which reduces memory usage by transferring only a small data batch from CPU to GPU during each accumulation step. X denotes cases where the baseline exceeds the hardware memory limit for a given batch size, making training infeasible. Memory cost is evaluated using the ViT-L/14 architecture and the AdamW optimizer.", "description": "Table 1 shows the peak memory usage during training of CLIP, OpenCLIP, and Inf-CL with varying batch sizes on 8 and 32 A800 GPUs.", "section": "4 EXPERIMENTS"}, {"figure_path": "2410.17243/tables/table_14_0.html", "caption": "Table 1: Training Memory Cost Across Different Hardware and Batch Sizes. Experiments utilize Data Parallelism with Automatic Mixed Precision for efficient distributed training. The baselines include the Vanilla loss (CLIP) and Local loss (OpenCLIP). To minimize memory consumption, Gradient Cache is adopted, with an accumulation batch size of 128. * indicates the use of the data offload strategy, which reduces memory usage by transferring only a small data batch from CPU to GPU during each accumulation step. X denotes cases where the baseline exceeds the hardware memory limit for a given batch size, making training infeasible. Memory cost is evaluated using the ViT-L/14 architecture and the AdamW optimizer.", "description": "Table 1 shows the peak memory usage of CLIP, OpenCLIP, and Inf-CL with different batch sizes and hardware configurations.", "section": "4 EXPERIMENTS"}, {"figure_path": "2410.17243/tables/table_14_1.html", "caption": "Table 1: Training Memory Cost Across Different Hardware and Batch Sizes. Experiments utilize Data Parallelism with Automatic Mixed Precision for efficient distributed training. The baselines include the Vanilla loss (CLIP) and Local loss (OpenCLIP). To minimize memory consumption, Gradient Cache is adopted, with an accumulation batch size of 128. * indicates the use of the data offload strategy, which reduces memory usage by transferring only a small data batch from CPU to GPU during each accumulation step. X denotes cases where the baseline exceeds the hardware memory limit for a given batch size, making training infeasible. Memory cost is evaluated using the ViT-L/14 architecture and the AdamW optimizer.", "description": "This table compares the peak memory usage (in GB) of CLIP, OpenCLIP, and Inf-CL for various batch sizes and hardware configurations during contrastive learning.", "section": "4 EXPERIMENTS"}]