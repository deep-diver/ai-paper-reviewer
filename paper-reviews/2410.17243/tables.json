[{"figure_path": "2410.17243/tables/table_7_0.html", "caption": "Table 1: Training Memory Cost Across Different Hardware and Batch Sizes. Experiments utilize Data Parallelism with Automatic Mixed Precision for efficient distributed training. The baselines include the Vanilla loss (CLIP) and Local loss (OpenCLIP). To minimize memory consumption, Gradient Cache is adopted, with an accumulation batch size of 128. * indicates the use of the data offload strategy, which reduces memory usage by transferring only a small data batch from CPU to GPU during each accumulation step. X denotes cases where the baseline exceeds the hardware memory limit for a given batch size, making training infeasible. Memory cost is evaluated using the ViT-L/14 architecture and the AdamW optimizer.", "description": "The table shows the peak memory usage (GB) of different contrastive loss methods under varying batch sizes and hardware configurations.", "section": "4 EXPERIMENTS"}, {"figure_path": "2410.17243/tables/table_8_0.html", "caption": "Table 1: Training Memory Cost Across Different Hardware and Batch Sizes. Experiments utilize Data Parallelism with Automatic Mixed Precision for efficient distributed training. The baselines include the Vanilla loss (CLIP) and Local loss (OpenCLIP). To minimize memory consumption, Gradient Cache is adopted, with an accumulation batch size of 128. * indicates the use of the data offload strategy, which reduces memory usage by transferring only a small data batch from CPU to GPU during each accumulation step. X denotes cases where the baseline exceeds the hardware memory limit for a given batch size, making training infeasible. Memory cost is evaluated using the ViT-L/14 architecture and the AdamW optimizer.", "description": "Table 1 shows the peak memory usage of CLIP, OpenCLIP, and Inf-CL under various batch sizes and hardware configurations, highlighting the memory efficiency of Inf-CL.", "section": "4 EXPERIMENTS"}, {"figure_path": "2410.17243/tables/table_9_0.html", "caption": "Table 3: Performance Verification. The training strategies is consistent with Table 2. We choose ViT-B/16 as the model architecture and adopt LiT strategy like Table 4. We evaluate zero-shot top-1 classification accuracy on several data sets, e.g., ImageNet-Validation Deng et al. (2009), ImageNet-v2 (Recht et al., 2019), ObjectNet (Barbu et al., 2019) and ImageNet-OOD (Hendrycks et al., 2021). We also evaluate zero-shot image-text top-1 retrieval accuracy on MSCOCO (Chen et al., 2015).", "description": "Table 3 presents a performance comparison of different methods on various datasets, showing the impact of batch size and the Inf-CL method on the zero-shot classification accuracy and image-text retrieval performance.", "section": "4.3 PERFORMANCE ANALYSIS"}, {"figure_path": "2410.17243/tables/table_9_1.html", "caption": "Table 1: Training Memory Cost Across Different Hardware and Batch Sizes. Experiments utilize Data Parallelism with Automatic Mixed Precision for efficient distributed training. The baselines include the Vanilla loss (CLIP) and Local loss (OpenCLIP). To minimize memory consumption, Gradient Cache is adopted, with an accumulation batch size of 128. * indicates the use of the data offload strategy, which reduces memory usage by transferring only a small data batch from CPU to GPU during each accumulation step. X denotes cases where the baseline exceeds the hardware memory limit for a given batch size, making training infeasible. Memory cost is evaluated using the ViT-L/14 architecture and the AdamW optimizer.", "description": "Table 1 shows the peak GPU memory cost for training with different batch sizes using various methods (CLIP, OpenCLIP, and Inf-CL) and hardware configurations (8 and 32 GPUs).", "section": "4 EXPERIMENTS"}, {"figure_path": "2410.17243/tables/table_14_0.html", "caption": "Table 1: Training Memory Cost Across Different Hardware and Batch Sizes. Experiments utilize Data Parallelism with Automatic Mixed Precision for efficient distributed training. The baselines include the Vanilla loss (CLIP) and Local loss (OpenCLIP). To minimize memory consumption, Gradient Cache is adopted, with an accumulation batch size of 128. * indicates the use of the data offload strategy, which reduces memory usage by transferring only a small data batch from CPU to GPU during each accumulation step. X denotes cases where the baseline exceeds the hardware memory limit for a given batch size, making training infeasible. Memory cost is evaluated using the ViT-L/14 architecture and the AdamW optimizer.", "description": "Table 1 shows the peak memory cost of contrastive loss calculations for different models, batch sizes, and numbers of GPUs, comparing the proposed Inf-CL method with existing baselines.", "section": "4 EXPERIMENTS"}]