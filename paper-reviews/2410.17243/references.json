{"references": [{" publication_date": "2021", "fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "reason": "This paper introduces CLIP, a foundational model for contrastive learning of visual and textual representations, highlighting the importance of large batch sizes for improved performance.  Its widespread impact and influence on subsequent research in the field make it a highly significant reference.", "section_number": 1}, {" publication_date": "2020", "fullname_first_author": "Ting Chen", "paper_title": "A simple framework for contrastive learning of visual representations", "reason": "This highly influential paper introduces a simple yet effective framework for contrastive learning, significantly impacting the field of self-supervised representation learning.  Its clarity, simplicity and effectiveness make it a crucial reference for understanding the core principles of contrastive learning.", "section_number": 1}, {" publication_date": "2020", "fullname_first_author": "Kaiming He", "paper_title": "Momentum contrast for unsupervised visual representation learning", "reason": "This seminal paper presents MoCo, a highly influential method for contrastive learning that significantly improves performance in self-supervised visual representation learning.  Its innovative approach to memory efficiency makes it crucial to the discussion of scaling contrastive learning.", "section_number": 1}, {" publication_date": "2018", "fullname_first_author": "Aaron van den Oord", "paper_title": "Representation learning with contrastive predictive coding", "reason": "This paper introduces contrastive predictive coding, a fundamental technique in representation learning that lays the groundwork for many subsequent approaches, including contrastive learning. Its theoretical contributions are highly relevant to understanding the underlying principles of contrastive learning.", "section_number": 1}, {" publication_date": "2021", "fullname_first_author": "Gabriel Ilharco", "paper_title": "Openclip", "reason": "This paper introduces OpenCLIP, a memory-efficient implementation of CLIP, addressing some of the memory limitations associated with large-scale contrastive learning.  The solutions and analysis provided are highly relevant to this paper.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Yihao Chen", "paper_title": "Disco-CLIP: A distributed contrastive loss for memory efficient clip training", "reason": "This work presents DisCo-CLIP, another memory-efficient variant of CLIP which uses distributed training to reduce memory demands.  The proposed methods and their effectiveness are highly relevant to the context of this paper.", "section_number": 1}, {" publication_date": "2020", "fullname_first_author": "Jared Kaplan", "paper_title": "Scaling laws for neural language models", "reason": "This paper explores scaling laws in large language models, providing valuable context for understanding the challenges and opportunities in scaling up the training of large, complex models.  The findings are indirectly relevant to this work, offering insights into the computational aspects of scaling.", "section_number": 1}, {" publication_date": "2017", "fullname_first_author": "Elad Hoffer", "paper_title": "Train longer, generalize better: closing the generalization gap in large batch training of neural networks", "reason": "This paper explores the effects of large batch size training on generalization, a critical issue in deep learning. Its analysis of the impact of large batch size and potential solutions is relevant to this paper, particularly in the context of the performance section.", "section_number": 1}, {" publication_date": "2021", "fullname_first_author": "Chao Jia", "paper_title": "Scaling up visual and vision-language representation learning with noisy text supervision", "reason": "This paper focuses on scaling visual and vision-language representation learning, which is highly relevant to the context of this work.  It addresses the challenges in training large-scale models and provides insights into the effectiveness of different training strategies.", "section_number": 1}, {" publication_date": "2021", "fullname_first_author": "Luyu Gao", "paper_title": "Scaling deep contrastive learning batch size under memory limited setup", "reason": "This paper directly addresses the memory limitations in scaling contrastive learning.  The proposed methods and their effectiveness are highly relevant to this paper, and their limitations highlight the contribution and significance of this work.", "section_number": 1}, {" publication_date": "2019", "fullname_first_author": "Nikunj Saunshi", "paper_title": "A theoretical analysis of contrastive unsupervised representation learning", "reason": "This paper provides a theoretical analysis of contrastive unsupervised representation learning, providing valuable context for understanding the underlying principles and limitations of contrastive learning.  The theoretical insights are highly relevant to the context of this work.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Changyou Chen", "paper_title": "Why do we need large batch sizes in contrastive learning? A gradient-bias perspective", "reason": "This paper examines the reasons behind the need for large batch sizes in contrastive learning, providing valuable theoretical and practical insights into this crucial aspect.  The analysis of the interplay between batch size, gradient bias, and model performance is directly relevant to this work.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Xiaohua Zhai", "paper_title": "Scaling vision transformers", "reason": "This paper introduces insights into the scaling properties of vision transformers, which are relevant to the context of large model training for contrastive learning. It examines the challenges in training large vision transformers and suggests strategies for efficient training. ", "section_number": 1}, {" publication_date": "2017", "fullname_first_author": "Paulius Micikevicius", "paper_title": "Mixed precision training", "reason": "This paper introduces mixed precision training, a widely adopted technique for training large deep learning models efficiently by using both single and half precision arithmetic.  The ability to reduce memory demands and increase computational efficiency makes this a valuable reference.", "section_number": 4}, {" publication_date": "2021", "fullname_first_author": "Hieu Pham", "paper_title": "Combined scaling for open-vocabulary image classification", "reason": "This paper examines scaling strategies in open-vocabulary image classification, which is highly relevant to this paper.  The strategies and evaluation methods utilized provide valuable insights into the challenges and potential solutions associated with scaling large model training.", "section_number": 4}, {" publication_date": "2020", "fullname_first_author": "Shen Li", "paper_title": "Pytorch distributed: Experiences on accelerating data parallel training", "reason": "This paper discusses data parallel training in PyTorch, a crucial component in the implementation of the proposed Inf-CL method.  The analysis of different techniques for distributed training provides valuable insights into the challenges and potential solutions for large-scale training.", "section_number": 4}, {" publication_date": "2022", "fullname_first_author": "Hao Liu", "paper_title": "Ring attention with blockwise transformers for near-infinite context", "reason": "This paper introduces the concept of Ring Attention, a technique relevant to optimizing memory and computation in large transformer models. The approach to managing memory and computation at scale is relevant to addressing memory issues in contrastive learning.", "section_number": 4}, {" publication_date": "2018", "fullname_first_author": "Andrei Barbu", "paper_title": "ObjectNet: A large-scale bias-controlled dataset for pushing the limits of object recognition models", "reason": "This paper presents ObjectNet, a large-scale dataset that presents challenges to typical object recognition models. The nature of this dataset and the challenges it poses is relevant to this work because the authors aim to train large-scale models.", "section_number": 4}, {" publication_date": "2021", "fullname_first_author": "Christoph Schuhmann", "paper_title": "Laion-400m: Open dataset of clip-filtered 400 million image-text pairs", "reason": "This paper introduces a large-scale image-text dataset, Laion-400M, which serves as the basis for many of the experiments in this paper. The size and nature of the dataset are significant in demonstrating the ability to train very large models.", "section_number": 4}, {" publication_date": "2020", "fullname_first_author": "Ting Chen", "paper_title": "Big self-supervised models are strong semi-supervised learners", "reason": "This work demonstrates the benefits of large self-supervised models and how they impact semi-supervised learning. This is relevant as the authors employ a large model in their experiments.", "section_number": 4}]}