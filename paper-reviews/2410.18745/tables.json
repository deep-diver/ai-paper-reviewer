[{"figure_path": "2410.18745/tables/table_8_0.html", "caption": "Table 1: Needle-in-a-haystack (4 needles) results of 7 base models across various methods (columns reordered from smallest to largest average) where Ltrain means the size of the training context window. All the models were tested using their training length. The number of test cases is 500.", "description": "Table 1 shows the performance comparison of seven base models across various methods on the Needle-in-a-Haystack task, highlighting the impact of STRING on improving effective context length.", "section": "4.2 MAIN RESULTS OF STRING"}, {"figure_path": "2410.18745/tables/table_9_0.html", "caption": "Table 2: Performance of various models and methods on RULER with a tested at a sequence length of 128K. The RULER benchmark consists of 13 tasks (500 test cases for each task) categorized into Needle-in-a-Haystack (NIAH), Variable Tracing (VT), Aggregation, and Question Answering (QA). We report the average scores for each category as well as the overall average across all 13 tasks. Effective denotes the actual effective sequence length as defined in RULER, indicating whether the model surpasses the performance of Llama2 (Touvron et al., 2023b), and Claimed represents the sequence length reported by the model.", "description": "Table 2 presents the performance comparison of various models and methods on the RULER benchmark, focusing on the effective context length achieved at a sequence length of 128K.", "section": "4.2 MAIN RESULTS OF STRING"}, {"figure_path": "2410.18745/tables/table_9_1.html", "caption": "Table 3: Comparison of STRING with three leading commercial long-context models on InfiniteBench. Each model is evaluated using a maximum context length of 128K.", "description": "Table 3 compares the performance of STRING against three leading commercial models and the original RoPE on the InfiniteBench benchmark, using a maximum context length of 128K.", "section": "4.2 MAIN RESULTS OF STRING"}, {"figure_path": "2410.18745/tables/table_19_0.html", "caption": "Table 1: Needle-in-a-haystack (4 needles) results of 7 base models across various methods (columns reordered from smallest to largest average) where Ltrain means the size of the training context window. All the models were tested using their training length. The number of test cases is 500.", "description": "Table 1 presents the Needle-in-a-Haystack results of seven base models across various methods, showing the impact of different training context window sizes and methods on performance.", "section": "4.2 MAIN RESULTS OF STRING"}, {"figure_path": "2410.18745/tables/table_20_0.html", "caption": "Table 1: Needle-in-a-haystack (4 needles) results of 7 base models across various methods (columns reordered from smallest to largest average) where  means the size of the training context window. All the models were tested using their training length. The number of test cases is 500.", "description": "Table 1 presents the Needle-in-a-Haystack results of seven base models across various methods, showing the impact of different methods on the models' performance within their original training context length.", "section": "4.2 MAIN RESULTS OF STRING"}, {"figure_path": "2410.18745/tables/table_22_0.html", "caption": "Table 2: Performance of various models and methods on RULER with a tested at a sequence length of 128K. The RULER benchmark consists of 13 tasks (500 test cases for each task) categorized into Needle-in-a-Haystack (NIAH), Variable Tracing (VT), Aggregation, and Question Answering (QA). We report the average scores for each category as well as the overall average across all 13 tasks. Effective denotes the actual effective sequence length as defined in RULER, indicating whether the model surpasses the performance of Llama2 (Touvron et al., 2023b), and Claimed represents the sequence length reported by the model.", "description": "Table 2 presents the performance comparison of various models and methods on the RULER benchmark, focusing on effective context length and overall performance across different task categories.", "section": "4.2 MAIN RESULTS OF STRING"}]