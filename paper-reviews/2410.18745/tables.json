[{"figure_path": "2410.18745/tables/table_8_0.html", "caption": "Table 1: Needle-in-a-haystack (4 needles) results of 7 base models across various methods (columns reordered from smallest to largest average) where Ltrain means the size of the training context window. All the models were tested using their training length. The number of test cases is 500.", "description": "Table 1 presents the Needle-in-a-Haystack (4 needles) results of 7 base models across various methods, showing the impact of different methods on improving the effective context length.", "section": "4.2 MAIN RESULTS OF STRING"}, {"figure_path": "2410.18745/tables/table_9_0.html", "caption": "Table 2: Performance of various models and methods on RULER with a tested at a sequence length of 128K. The RULER benchmark consists of 13 tasks (500 test cases for each task) categorized into Needle-in-a-Haystack (NIAH), Variable Tracing (VT), Aggregation, and Question Answering (QA). We report the average scores for each category as well as the overall average across all 13 tasks. Effective denotes the actual effective sequence length as defined in RULER, indicating whether the model surpasses the performance of Llama2 (Touvron et al., 2023b), and Claimed represents the sequence length reported by the model.", "description": "Table 2 presents the performance comparison of various models and methods on the RULER benchmark, showing their effective sequence lengths and scores on different tasks.", "section": "4.2 MAIN RESULTS OF STRING"}, {"figure_path": "2410.18745/tables/table_9_1.html", "caption": "Table 3: Comparison of STRING with three leading commercial long-context models on InfiniteBench. Each model is evaluated using a maximum context length of 128K.", "description": "Table 3 compares the performance of STRING against three leading commercial long-context models and Llama3.1 8B and 70B on InfiniteBench, showcasing STRING's performance improvement.", "section": "4.2 MAIN RESULTS OF STRING"}, {"figure_path": "2410.18745/tables/table_19_0.html", "caption": "Table 1: Needle-in-a-haystack (4 needles) results of 7 base models across various methods (columns reordered from smallest to largest average) where  means the size of the training context window. All the models were tested using their training length. The number of test cases is 500.", "description": "Table 1 presents the Needle-in-a-Haystack results of seven base models across various methods, showing the impact of different training context window sizes on the performance.", "section": "4.2 MAIN RESULTS OF STRING"}, {"figure_path": "2410.18745/tables/table_20_0.html", "caption": "Table 1: Needle-in-a-haystack (4 needles) results of 7 base models across various methods (columns reordered from smallest to largest average) where Ltrain means the size of the training context window. All the models were tested using their training length. The number of test cases is 500.", "description": "Table 1 presents the Needle-in-a-Haystack task results for seven base models across different methods, showing the impact of training context window size on model performance.", "section": "4.2 MAIN RESULTS OF STRING"}, {"figure_path": "2410.18745/tables/table_22_0.html", "caption": "Table 1: Needle-in-a-haystack (4 needles) results of 7 base models across various methods (columns reordered from smallest to largest average) where Ltrain means the size of the training context window. All the models were tested using their training length. The number of test cases is 500.", "description": "Table 1 presents the Needle-in-a-Haystack results of seven base models across various methods, showing the impact of different methods on effective context length.", "section": "4.2 MAIN RESULTS OF STRING"}]