{"references": [{" publication_date": "2023", "fullname_first_author": "Anthropic", "paper_title": "Introducing 100K Context Windows", "reason": "This paper is highly relevant to the introduction because it provides a real-world example of a significant advancement in LLM context window size. The claim of 100K context window directly supports the introduction's observation that recent advancements have made it feasible to train LLMs with exceptionally long context windows. It thus adds credibility and context to the initial problem statement.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Llama Team", "paper_title": "The llama 3 herd of models", "reason": "This paper is crucial to the introduction as it exemplifies the core problem highlighted in the introduction: the discrepancy between the trained context length and effective context length. Llama3.1, mentioned prominently in the introduction, is detailed in this paper, emphasizing the significant gap between the claimed 128K tokens context length and the actual effective context length observed in practice.  This paper provides concrete evidence supporting the research problem's significance and relevance.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Hsieh", "paper_title": "Ruler: What's the real context size of your long-context language models?", "reason": "This paper is essential to the introduction as it quantifies the discrepancy between claimed and effective context length in various LLMs, providing strong empirical support for the central research question. The introduction uses RULER's findings to emphasize how open-source LLMs often have an effective context length less than 50% of their training length, making it a critical piece of evidence for motivating the research problem.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Su", "paper_title": "Roformer: Enhanced transformer with rotary position embedding", "reason": "This paper is very important to Section 2 because it introduces Rotary Position Embedding (RoPE), a key positional encoding method used in many LLMs, whose limitations in handling long-range dependencies are analyzed in this section. Understanding RoPE is essential to understanding the left-skewed position frequency distribution that the authors identify as a core limitation of LLMs in effectively utilizing long contexts.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Cerebras", "paper_title": "Slimpajama: A 627b token, cleaned and deduplicated version of redpajama", "reason": "This paper describes SlimPajama, a widely used pretraining corpus that is instrumental to Section 2's analysis of the left-skewed position frequency distribution. The authors use SlimPajama to illustrate their observations about the unequal frequency distribution of relative positional indices, providing empirical evidence that supports their claim.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "Bao", "paper_title": "Unilmv2: Pseudo-masked language models for unified language model pre-training", "reason": "This paper is relevant to Section 2.1 as it discusses various positional embedding methods, including relative positional encodings, which are compared and contrasted with Rotary Position Embeddings (RoPE) in the context of the left-skewed positional frequency distribution. The paper establishes the broader context for understanding the positional embedding techniques employed by LLMs and their potential impact on effective context utilization.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Su", "paper_title": "Roformer: Enhanced transformer with rotary position embedding", "reason": "This paper is very important to Section 2.2, as it introduces the key concept of Relative Position Matrix and provides the formula for calculating the position frequency f(i). This information is directly used by the authors to explain the left-skewed position frequency and its role in limiting the effective context length of LLMs, making the paper critical for the core argument of this section.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "gkamradt", "paper_title": "Llmtest_needleinahaystack: Doing simple retrieval from llm models", "reason": "This paper introduces the Needle-in-a-Haystack task, which is the core evaluation method used in Section 3 to probe the relationship between position frequency and effective context length in LLMs. The choice of this task and its detailed description are directly linked to the experimental design and the analysis of the results presented in Section 3.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Llama Team", "paper_title": "The llama 3 herd of models", "reason": "This paper provides crucial data on the Llama 3 model, which is used in Section 3 for empirical analysis.  The paper contains information on Llama 3's training details, including context window size and token count. This data is essential for validating and comparing the experimental results of the study, making it a crucial reference for the section.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Dao", "paper_title": "Flashattention-2: Faster attention with better parallelism and work partitioning", "reason": "This paper is highly important to Section 4 as it introduces FlashAttention, the efficient attention mechanism that is used for the implementation of STRING.  The authors' integration of STRING with FlashAttention is a key element of their proposed solution, making this paper's contribution to computational efficiency a vital element in supporting STRING's claims.", "section_number": 4}, {" publication_date": "2020", "fullname_first_author": "Beltagy", "paper_title": "Longformer: The long-document transformer", "reason": "This paper is relevant to Section 4 because it discusses sliding window attention, a technique that is incorporated into the STRING algorithm. The combination of sliding window attention with shifted self-attention is a key component of STRING's efficiency and effectiveness, making this paper relevant to understanding the implementation and workings of the proposed method.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Hsieh", "paper_title": "Ruler: What's the real context size of your long-context language models?", "reason": "This paper is directly referenced in Section 4.2 as one of the key benchmarks used to evaluate the effectiveness of STRING. RULER is a comprehensive benchmark specifically designed for evaluating long-context capabilities of LLMs, making it a highly relevant and important reference for the study.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Zhang", "paper_title": "\u221ebench: Extending long context evaluation beyond 100k tokens", "reason": "This paper introduces InfiniteBench, another important benchmark used in Section 4.2 to evaluate STRING.  InfiniteBench provides a diverse set of long-context tasks, allowing for a more comprehensive evaluation of STRING's performance. The inclusion of InfiniteBench results adds weight to the overall findings and demonstrates the broader applicability of STRING.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Bai", "paper_title": "Qwen technical report", "reason": "This paper is important for Section 4.2 because it provides details about Qwen2, a large-scale language model to which STRING is applied and compared against other models, including GPT-4-128K. The performance comparison strengthens the claim that STRING improves the effective context length, particularly when compared to large and commercially developed models.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "OpenAI", "paper_title": "Gpt-4 technical report", "reason": "This paper is significant for Section 4.2 because it provides performance data for GPT-4-128K, a leading commercial LLM.  STRING's performance is directly compared against GPT-4-128K on several benchmarks, showing that STRING can achieve comparable or superior results in certain cases. This comparison is crucial for demonstrating the efficacy of STRING against commercially available state-of-the-art LLMs.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Anthropic", "paper_title": "Introducing 100K Context Windows", "reason": "This paper is used in Section 4.2 to compare STRING's performance against Claude 2, a commercially developed LLM.  The direct comparison adds further evidence that STRING can achieve better or comparable performance to leading commercial models without the need for additional training, underlining the efficiency and effectiveness of the proposed method.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Zhang", "paper_title": "Ruler: What's the real context size of your long-context language models?", "reason": "This paper is very important to Section 4.2 as it provides background information on the RULER benchmark, which is used to evaluate the performance of STRING.  The description of RULER's tasks and evaluation metrics provides context for understanding the experimental setup and the interpretation of results.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Zhang", "paper_title": "\u221ebench: Extending long context evaluation beyond 100k tokens", "reason": "This paper is highly relevant to Section 4.2 because it provides essential information about InfiniteBench, the benchmark used to evaluate STRING's performance and compare it with commercially available models.  The description of InfiniteBench's tasks, metrics, and the performance results obtained from STRING provide crucial context for assessing the contributions of this work.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Peng", "paper_title": "Yarn: Efficient context window extension of large language models", "reason": "This paper is important to Section 4.2 as it is one of the baseline methods compared against STRING.  The inclusion of YaRN allows for a direct comparison of performance and helps to establish the relative improvement achieved by STRING.  The results demonstrate STRING's superior performance and justify its novel approach.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Jiang", "paper_title": "Minference 1.0: Accelerating pre-filling for long-context llms via dynamic sparse attention", "reason": "This paper is relevant to Section 5 because it discusses efficient architectures for handling long contexts, a topic directly related to the broader research context of this paper. The focus on improving efficiency in long-context processing aligns with the paper's overall theme and provides context for understanding the various approaches taken to address the challenges of processing long texts in LLMs.", "section_number": 5}]}