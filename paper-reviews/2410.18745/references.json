{"references": [{" publication_date": "2023", "fullname_first_author": "Tri Dao", "paper_title": "Flashattention-2: Faster attention with better parallelism and work partitioning", "reason": "This paper introduces FlashAttention-2, a significant improvement over the original FlashAttention. Its faster speed and improved parallelism are crucial for handling the computationally intensive long-context attention calculations that are central to the STRING method's efficiency.  The improvements offered by FlashAttention-2 are directly relevant to STRING's practical applicability and scalability.", "section_number": 4}, {" publication_date": "2022", "fullname_first_author": "Tri Dao", "paper_title": "Flashattention: Fast and memory-efficient exact attention with io-awareness", "reason": "This is a foundational paper that underpins the efficiency of STRING.  FlashAttention's speed and memory efficiency are essential for making the STRING method practical for large LLMs with long context windows. As STRING directly builds upon FlashAttention's framework, the improvements from this paper are essential to the overall performance gains of the proposed approach.", "section_number": 4}, {" publication_date": "2020", "fullname_first_author": "Iz Beltagy", "paper_title": "Longformer: The long-document transformer", "reason": "Longformer introduces architectural innovations for handling long sequences that are relevant to the underlying challenges tackled by STRING.  Though not directly used in STRING, Longformer's approach to efficient long-sequence processing provides a relevant point of comparison and highlights the ongoing quest for improving long-context performance within the field of LLMs.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Llama Team", "paper_title": "The llama 3 herd of models", "reason": "This paper introduces the Llama 3 series of models which are used in the experiments in this paper. The Llama 3 models serve as the primary test subjects for the proposed STRING method, and their performance characteristics directly impact the evaluation and validation of STRING's effectiveness. The study heavily relies on the Llama 3 models to test the approach.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Jinze Bai", "paper_title": "Qwen technical report", "reason": "This paper introduces the Qwen language model, another key model used in the experimental validation and comparison of the STRING method. The inclusion of Qwen provides a broader perspective on the capabilities and limitations of STRING across different LLMs. Like Llama 3, Qwen forms an important benchmark against which STRING's effectiveness is assessed.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Cheng-Ping Hsieh", "paper_title": "Ruler: What's the real context size of your long-context language models?", "reason": "The RULER benchmark, introduced in this paper, is a crucial component of the experimental evaluation of STRING. It provides a standardized and widely accepted method for measuring the effective context length of LLMs, making it essential for validating the improvements achieved by STRING. The experimental results using the RULER benchmark are critical for demonstrating the effectiveness of STRING.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Guangxuan Xiao", "paper_title": "Infllm: Unveiling the intrinsic capacity of Ilms for understanding extremely long sequences with training-free memory", "reason": "This paper presents InfLLM, an approach to handling long contexts which is relevant to the problem tackled by STRING.  While STRING uses a different technique, comparing their performances highlights the different methodologies available and the trade-offs inherent in each strategy. InfLLM offers a valuable point of comparison to contextualize the contributions of STRING.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Hugo Touvron", "paper_title": "Llama: Open and efficient foundation language models", "reason": "This paper introduces the Llama model family, which serves as a foundational model for some of the experiments conducted in the paper. The Llama models are used to assess STRING's effectiveness and demonstrate its applicability to widely used open-source models. The findings on Llama models are essential for the paper's claims about the generality of STRING's approach.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Tri Dao", "paper_title": "Mamba: Linear-time sequence modeling with selective state spaces", "reason": "Mamba offers an alternative architectural approach to handling long sequences, providing a contrasting perspective to the approach taken by STRING. Comparing the performance characteristics of Mamba and STRING helps demonstrate the unique capabilities and potential advantages of the STRING approach in efficiently processing long sequences.", "section_number": 5}, {" publication_date": "2020", "fullname_first_author": "Jared Kaplan", "paper_title": "Scaling laws for neural language models", "reason": "This paper establishes fundamental scaling laws for neural language models, providing a theoretical foundation for understanding the relationship between model size, dataset size, and performance. These scaling laws directly inform the context of the research presented in this paper, providing a framework to contextualize the challenges faced in efficiently processing long sequences, making it a key reference for explaining the motivations behind this research.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Chenxin An", "paper_title": "L-eval: Instituting standardized evaluation for long context language models", "reason": "This paper introduces L-Eval, a standardized evaluation benchmark for assessing performance on long-context language models.  While not directly used in the experiments of this paper, L-Eval represents important work in the field and provides a crucial context for understanding the challenges and requirements for evaluating effective context length in LLMs. The need for such standardization underlines the significance of the problem being addressed by the proposed STRING method.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Wenhan Xiong", "paper_title": "Effective long-context scaling of foundation models", "reason": "This paper addresses the challenging problem of scaling LLMs to effectively handle long contexts.  Its findings and approaches are directly relevant to the central theme of this paper\u2014improving the practical utilization of long contexts in LLMs\u2014making it an important reference for understanding the state-of-the-art and the challenges in this research area.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Tianyu Fu", "paper_title": "Data engineering for scaling language models to 128k context", "reason": "This paper explores data engineering techniques for training LLMs with longer contexts, providing crucial background information for this research.  The methods and insights discussed are directly relevant to the challenges in obtaining sufficient high-quality training data for very long contexts. The paper's findings provide important context for understanding the limitations of directly increasing context lengths and provide justification for exploring alternative approaches, like STRING.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Anthropic", "paper_title": "Introducing 100K Context Windows", "reason": "This paper reports on the development of LLMs with an extended context window size of 100K tokens.  It highlights the growing trend of extending context window lengths in LLMs. It serves as a relevant reference for demonstrating the progress in LLMs and underlining the need for addressing the gap between theoretical and actual effective context length\u2014the main focus of this paper. The information is used in the introduction to justify the importance of the addressed problem.", "section_number": 1}, {" publication_date": "2017", "fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "reason": "This seminal paper introduced the Transformer architecture, which is the foundation for most modern LLMs. The Transformer architecture is fundamental to the field and provides a crucial context for understanding the complexities involved in processing long sequences, making it an essential reference for research in LLMs.  Its significance forms the basis for much of the subsequent research in the field.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Xinrong Zhang", "paper_title": "\u221ebench: Extending long context evaluation beyond 100k tokens", "reason": "This paper introduces the InfiniteBench benchmark, another important evaluation metric for measuring the effective context length of LLMs.  InfiniteBench complements RULER, providing a more comprehensive and diverse evaluation methodology, crucial for establishing the robustness and generality of the STRING method's effectiveness across different evaluation tasks.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Meizhi Zhong", "paper_title": "Understanding the rope extensions of long-context llms: An attention perspective", "reason": "This paper analyzes the behavior of Rotary Position Embeddings (RoPE), a commonly used positional encoding technique in LLMs, in long-context scenarios. Its analysis is directly relevant to the challenges addressed by STRING as RoPE is the positional encoding scheme used in most of the models where STRING is applied and tested.  The insights provided help contextualize the limitations of RoPE and highlight the need for approaches like STRING to improve long-context performance.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "gkamradt", "paper_title": "llmtest_needleinahaystack: Doing simple retrieval from llm models", "reason": "This work provides the Needle-in-a-Haystack benchmark that is used in the experiments. The benchmark is used to directly evaluate and assess the performance gains achieved by the proposed STRING method. Its use provides clear and established methodology for evaluating the practical effectiveness of STRING's proposed technique.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Peitian Zhang", "paper_title": "Soaring from 4k to 400k: Extending llm's context with activation beacon", "reason": "This work explores methods for extending the context window size of LLMs, providing a relevant comparison to the approach taken by STRING. The activation beacon method offers an alternative approach to address the limitations of standard positional encodings, and comparing it with STRING helps highlight the specific advantages and disadvantages of each methodology in addressing the problem of underutilized long contexts.", "section_number": 5}, {" publication_date": "2022", "fullname_first_author": "Jianlin Su", "paper_title": "Roformer: Enhanced transformer with rotary position embedding", "reason": "This work introduces Roformer, an enhanced Transformer architecture incorporating Rotary Position Embeddings (RoPE), which are directly related to the positional encoding methods used in the tested models.  Understanding the mechanisms and limitations of RoPE is essential for interpreting the results of the experiments and evaluating the effectiveness of STRING in addressing issues related to positional encoding. This paper is therefore essential context for understanding the foundations of the proposed work.", "section_number": 2}]}