{"references": [{" publication_date": "2023", "fullname_first_author": "Anthropic", "paper_title": "Introducing 100K Context Windows", "reason": "This paper is highly relevant as it directly addresses the advancements in context window sizes for LLMs. The claim of 100K context windows showcases the significant progress in this area, providing a benchmark against which to measure the performance and limitations of other models discussed in the paper.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Llama Team", "paper_title": "The llama 3 herd of models", "reason": "This paper introduces Llama 3.1, a model with a significantly increased context length (128K tokens), which is crucial to the paper's central theme of examining the discrepancy between claimed and actual effective context lengths.  The details of Llama 3.1's development and the performance results provide a valuable context for the research.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Bai", "paper_title": "Qwen technical report", "reason": "This technical report details the design and architecture of Qwen, a large language model with a substantial context window. This report is essential as it provides a comparative model to study the relationship between context window and effective context usage, a critical point of the main research question.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Su", "paper_title": "Roformer: Enhanced transformer with rotary position embedding", "reason": "This paper introduces Rotary Position Embedding (RoPE), a crucial component in many LLMs and directly related to the study's focus on the limitations of effective context length in models that use ROPE.  Understanding RoPE's mechanics and its implications is fundamental to analyzing the undertraining of long-distance positions.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Peng", "paper_title": "Yarn: Efficient context window extension of large language models", "reason": "This paper introduces a method to extend context windows in LLMs, therefore it's important to the paper as a counterpoint; the paper explores a way to extend context, but also highlights limitations in previous work. The limitations demonstrated in this paper serve as a major motivation for the present research.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "An", "paper_title": "L-eval: Instituting standardized evaluation for long context language models", "reason": "This paper is highly relevant because it proposes a standardized evaluation framework for LLMs. The methodology and the benchmarks highlighted are crucial for evaluating the claims made in other studies and validating the effectiveness of STRING.  Their work provides a standardized approach for measuring the effective context length, making this paper a key benchmark for other studies.", "section_number": 1}, {" publication_date": "2020", "fullname_first_author": "Beltagy", "paper_title": "Longformer: The long-document transformer", "reason": "This paper introduces the Longformer architecture, a model designed to efficiently handle long sequences. Understanding the design choices and the performance limitations of the Longformer in handling extremely long sequences is highly relevant to the study's exploration of effective context length.", "section_number": 1}, {" publication_date": "2020", "fullname_first_author": "Bao", "paper_title": "Unilmv2: Pseudo-masked language models for unified language model pre-training", "reason": "This paper describes a method for pre-training language models that addresses the challenge of modeling long sequences. The approach and results are particularly relevant as a comparison point for understanding the impact of different training methodologies on the effectiveness of context utilization in LLMs.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Press", "paper_title": "Train short, test long: Attention with linear biases enables input length extrapolation", "reason": "This paper is highly relevant as it directly addresses one of the core challenges of the paper: the discrepancy between the claimed and actual effective context lengths. The method and results presented offer a comparison point to understand the trade-offs between different approaches to handle long sequences.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Zhang", "paper_title": "Qwen technical report", "reason": "This technical report provides details about the Qwen model's architecture, training data, and evaluation results.  Analyzing this information gives a valuable benchmark against which to compare the effectiveness of the proposed STRING method and to demonstrate its advantages in improving the utilization of long contexts.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Cerebras", "paper_title": "SlimPajama: A 627b token, cleaned and deduplicated version of redpajama", "reason": "This paper is essential due to the study's use of the SlimPajama dataset for its experimental analysis of position frequency distribution. Understanding the properties of SlimPajama is crucial to interpret the results of the experiments and to assess the generalizability of the findings.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "gkamradt", "paper_title": "Llmtest_needleinahaystack: Doing simple retrieval from Ilm models", "reason": "This work provides the Needle-in-a-Haystack task, a crucial component of the paper's experimental evaluation of the proposed STRING method and other baseline models. Understanding the details of the task and its design is essential for interpreting the results.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Llama Team", "paper_title": "The llama 3 herd of models", "reason": "This paper describes the training process and the performance metrics for the Llama 3 model. The information is vital for providing context, understanding the design choices made during model development, and evaluating the effectiveness of the proposed approach.", "section_number": 3}, {" publication_date": "2022", "fullname_first_author": "Dao", "paper_title": "Flashattention-2: Faster attention with better parallelism and work partitioning", "reason": "This paper details the FlashAttention-2 algorithm, a crucial component of the STRING implementation. The efficiency and effectiveness of FlashAttention-2 are essential to the proposed method, and understanding this algorithm is crucial for evaluating the results.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Hsieh", "paper_title": "Ruler: What's the real context size of your long-context language models?", "reason": "This paper introduces the RULER benchmark, which is one of the key benchmarks used to evaluate the performance of the proposed STRING method and other baseline models. Understanding the design of this benchmark and the evaluation metrics is essential for interpreting the results.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Zhang", "paper_title": "InfiniteBench", "reason": "This paper introduces the InfiniteBench benchmark, a critical component of the paper's evaluation of the proposed STRING method.  The benchmark tests the models on a wide range of tasks and is important for demonstrating the generality and robustness of the approach.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "OpenAI", "paper_title": "Gpt-4 technical report", "reason": "This paper introduces GPT-4 and its performance on various benchmarks, which is crucial for setting a strong baseline and providing a context for comparing the performance of the proposed STRING method against leading commercial models.  The GPT-4 results serve as a benchmark for evaluating the improvements achieved by STRING.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Jiang", "paper_title": "Minference 1.0: Accelerating pre-filling for long-context Ilms via dynamic sparse attention", "reason": "This paper is highly relevant because it explores techniques to optimize inference for long-context LLMs.  Understanding the challenges faced in long-context inference and the strategies proposed in this paper is crucial to evaluating the effectiveness of STRING and its impact on model efficiency.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Fu", "paper_title": "Data engineering for scaling language models to 128k context", "reason": "This paper focuses on data engineering techniques for improving the performance of LLMs. Understanding the data engineering challenges and the strategies proposed in this paper provide a broader context for evaluating the effectiveness of STRING, as the method doesn't require specialized data.", "section_number": 5}]}