{"reason": "Shifted Rotray position embeddING (STRING) significantly improves the effective context length of large language models by addressing the left-skewed frequency distribution of relative positions, achieving state-of-the-art results for open-source LLMs.", "summary": "Boosting LLMs' long-context abilities: STRING shifts trained positions to enhance distant information gathering, significantly outperforming existing methods.", "takeaways": ["STRING dramatically improves the performance of LLMs on long-context benchmarks without additional training.", "The left-skewed frequency distribution of relative positions in LLMs hinders their ability to utilize longer contexts effectively.", "STRING achieves state-of-the-art results for open-source LLMs, even surpassing some commercial models in performance."], "tldr": "Large Language Models (LLMs) are limited by their effective context length, often falling short of their training capacity. This paper identifies the cause: a left-skewed frequency distribution of relative positions used during training.  To address this, the researchers propose STRING, a training-free method that shifts well-trained position embeddings to overwrite less effective ones. STRING significantly improves performance across several LLMs on long-context benchmarks, establishing new state-of-the-art results for open-source models.  Even without additional training, STRING enhances the ability of LLMs to effectively use longer contexts, bringing them closer to their theoretical potential. This is achieved through a clever manipulation of the position embedding matrix to leverage frequently-used positions for representing longer distances, thus compensating for the under-training of less frequent, long-distance positions."}