{"reason": "This paper investigates why the effective context length of large language models (LLMs) is often much shorter than their training length.  The authors find that this is due to a left-skewed distribution of relative positions during pretraining, meaning the model is undertrained on long-range dependencies.  They propose STRING, a training-free method that shifts well-trained position embeddings to improve performance on long-context tasks, achieving state-of-the-art results for open-source LLMs.", "summary": "Boosting LLMs' long-context performance, STRING, a training-free method, shifts position embeddings to overcome undertraining on long-range dependencies, achieving state-of-the-art results.", "takeaways": ["The effective context length of LLMs is limited by a left-skewed position frequency distribution during pretraining, hindering long-range dependency modeling.", "STRING, a training-free method, significantly improves LLM performance on long-context tasks by shifting position embeddings to utilize frequently trained positions.", "STRING establishes new state-of-the-art results for open-source LLMs on popular long-context benchmarks, even surpassing some commercial models."], "tldr": "Large Language Models (LLMs) often struggle to use their full context window effectively, performing far worse on long-range tasks than expected based on their training data. This paper identifies the problem: the frequency of different relative positional encodings is highly skewed towards shorter distances during training. This means the models haven't seen enough examples of long-range relationships and thus can't handle them well. To fix this without needing to retrain, the researchers propose STRING, a simple method that re-assigns position encodings at inference time, effectively overwriting undertrained ones with well-trained ones. This results in significant performance improvements on several benchmark tests for open-source LLMs, even exceeding the performance of some commercial models.  The findings highlight the critical role of position encodings in LLM performance and offer a practical, training-free solution to improve long-context understanding."}