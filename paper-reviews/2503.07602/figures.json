[{"figure_path": "https://arxiv.org/html/2503.07602/x2.png", "caption": "Figure 1: \nRelational video customization results of DreamRelation. Given a few exemplar videos, our method can customize specific relations and generalize them to novel domains, where animals mimic human interactions.", "description": "This figure showcases the results of the DreamRelation model.  Four examples are displayed, each demonstrating a different human-like interaction between two animals (hugging, punching, cheering, shaking hands).  The key takeaway is that given a few example videos of a specific relationship, DreamRelation can not only generate new videos exhibiting that same relationship but also extrapolate the relationship to different animal pairs and settings, showcasing the model's ability to customize and generalize relational interactions.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2503.07602/x3.png", "caption": "Figure 2: (a) General Video DiT models like Mochi\u00a0[69] often struggle to generate unconventional or counter-intuitive interactions, even with detailed descriptions.\n(b) Our method can customize a specific relation to generate videos on new subjects.", "description": "Figure 2 presents a comparison of video generation results between a state-of-the-art text-to-video model (Mochi) and the proposed DreamRelation model.  Both models received the same prompt: \"A bear is hugging with a tiger.\" (a) shows Mochi's output, which demonstrates a failure to generate the unconventional interaction of a bear and tiger hugging; instead, it produced a more typical or expected interaction.  (b) illustrates DreamRelation's result, showcasing the model's ability to generate the specific, uncommon relation requested, adapting it even to new subjects not seen during training. This highlights DreamRelation's superior capacity for relational video customization.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.07602/x4.png", "caption": "Figure 3: Averaged value feature across all layers and frames in Mochi. We identify that the relations encompass intricate spatial arrangements, layout variations, and nuanced temporal dynamics, presenting challenges in relational video customization.", "description": "The figure visualizes the average of value feature maps across all layers and frames of the Mochi model.  The value feature is a key component in the attention mechanism, representing information about the objects and their relationships. By averaging these features, the figure highlights the consistent patterns that the Mochi model detects during relation processing. The analysis reveals that relations involve complex interactions in terms of spatial arrangement, positional layout changes, and intricate temporal dynamics. These combined factors present significant challenges when developing methods for relational video customization.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.07602/x5.png", "caption": "Figure 4: Overall framework of DreamRelation.\nOur method decomposes relational video customization into two concurrent processes.\n(1) In Relational Decoupling Learning, Relation LoRAs in relation LoRA triplet capture relational information, while Subject LoRAs focus on subject appearances. This decoupling process is guided by hybrid mask training strategy based on their corresponding masks.\n(2) In Relational Dynamics Enhancement, the proposed space-time relational contrastive loss pulls relational dynamics features (anchor and positive features) from pairwise differences closer, while pushing them away from appearance features (negative features) of single-frame outputs.\nDuring inference, subject LoRAs are excluded to prevent introducing undesired appearances and enhance generalization.", "description": "DreamRelation processes relational video customization in two stages: Relational Decoupling Learning and Relational Dynamics Enhancement.  The first stage uses Relation LoRAs and Subject LoRAs within a 'relation LoRA triplet' to separate relational information from subject appearances. A hybrid mask training strategy ensures the LoRAs focus on the correct aspects. The second stage uses a space-time relational contrastive loss. This loss function brings together relational dynamics features from different frames of similar videos and pushes away unrelated appearance features from single frames. During inference, Subject LoRAs are omitted to improve generalization and reduce unwanted appearance influence.", "section": "3. DreamRelation"}, {"figure_path": "https://arxiv.org/html/2503.07602/x6.png", "caption": "Figure 5: Features and subspace similarity analysis of MM-DiT. (a) Value features across different videos encapsulate rich appearance information, and relational information often intertwines with these appearance cues. Meanwhile, query and key features exhibit similar patterns that differ from those of value features.\n(b) We perform singular value decomposition on the query, key, and value matrices of each MM-DiT block and compute the similarity of the subspaces spanned by their top-k left singular vectors, indicating query and key matrices share more common information while remaining independent of the value matrix.", "description": "Figure 5 analyzes the query, key, and value features within the Multimodal Diffusion Transformer (MM-DiT) model's attention mechanism. Part (a) visualizes these features across various video examples, demonstrating that value features capture rich appearance information, often intertwined with relational information, while query and key features display similar patterns distinct from value features. Part (b) quantifies this observation by performing singular value decomposition on the query, key, and value matrices of each MM-DiT block. It then calculates the subspace similarity between these matrices, revealing that query and key matrices share substantial common information, yet remain independent of the value matrix.", "section": "3. DreamRelation"}, {"figure_path": "https://arxiv.org/html/2503.07602/x7.png", "caption": "Figure 6: Qualitative comparison results. Our method outperforms all baselines in precisely capturing the intended relation and mitigating appearance and background leakage.", "description": "Figure 6 presents a qualitative comparison of DreamRelation against several baseline methods for relational video customization.  The figure showcases example videos generated by each method for two different relations: 'shaking hands' and 'high-fiving'.  By comparing the results, we observe that DreamRelation excels at accurately representing the intended relationship between the subjects, while effectively reducing unwanted appearance artifacts and background distractions present in the videos generated by other methods. This visual comparison highlights DreamRelation's superiority in capturing the nuanced details of the specified relations, showcasing the effectiveness of its relational decoupling and dynamics enhancement strategies.", "section": "4. Experiment"}, {"figure_path": "https://arxiv.org/html/2503.07602/x8.png", "caption": "Figure 7: (a) Our method focuses on the desired relational region. (b) Our method is most preferred by users across all aspects.", "description": "Figure 7 presents a two-part analysis of the DreamRelation model's performance. (a) shows attention maps, visualizations that highlight where the model focuses its attention.  The visualization demonstrates that DreamRelation concentrates its attention on the relevant areas depicting the specified relationship between subjects, rather than being distracted by irrelevant details or background elements. (b) shows the results of a user study comparing DreamRelation to other methods.  The bar chart indicates that users overwhelmingly favored DreamRelation across all evaluation criteria (Relation Alignment, Text Alignment, and Overall Quality).  This shows that not only does the model focus on the correct areas but also generates videos of superior quality and alignment with user expectations.", "section": "4. Experiment"}, {"figure_path": "https://arxiv.org/html/2503.07602/x9.png", "caption": "Figure 8: Qualitative ablation study on each component.", "description": "This ablation study visually demonstrates the individual contributions of each component of the DreamRelation model.  It shows the results of removing, one at a time, the hybrid mask training strategy (HMT), the space-time relational contrastive loss (RCL), the Relation LoRAs, the Subject LoRAs, and the FFN LoRAs.  By comparing these results to the complete model, the impact of each component on the model's performance can be assessed.  Each row shows the performance of a modified model on several evaluation metrics.", "section": "4.3 Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2503.07602/x10.png", "caption": "Figure 9: More qualitative results of DreamRelation (1/2). Please zoom in for a better view.", "description": "This figure displays several example results from the DreamRelation model, showcasing its ability to generate videos depicting various relational interactions between animals. Each row presents a specific relation (e.g., cheering, high-five, hugging, etc.) along with corresponding example videos generated by the model. The videos show different animals performing actions that represent human-like interactions, highlighting the model's capacity to generalize and apply relations across novel domains and animal species. The caption encourages viewers to zoom in for better detail visualization.", "section": "3. DreamRelation"}]