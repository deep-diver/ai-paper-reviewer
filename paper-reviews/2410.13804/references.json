{"references": [{" publication_date": "2020", "fullname_first_author": "Cuong Nguyen", "paper_title": "LEEP: A new measure to evaluate transferability of learned representations", "reason": "This paper proposes LEEP, a novel method for estimating task transferability which is closely related to the proposed ICT method in this paper.  While LEEP uses linear probing, which still requires training, it's an important advancement over previous methods that rely on computationally expensive techniques. Understanding its strengths and weaknesses helps position the novel ICT method within the existing literature.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Jason Wei", "paper_title": "Finetuned language models are zero-shot learners", "reason": "This paper is highly relevant because it's a seminal work in in-context learning (ICL), a critical concept underlying the proposed ICT method. The findings in this paper on the zero-shot capabilities of fine-tuned language models directly influenced the development of the efficient and training-free ICT method presented in this paper.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Yupeng Chang", "paper_title": "A survey on evaluation of large language models", "reason": "This survey paper offers a comprehensive overview of existing LLM evaluation methods and highlights the current challenges and open issues in this area, specifically the high cost of evaluating LLMs, which directly motivates this research. This paper is important because it shows the landscape and challenges in evaluating LLMs.", "section_number": 1}, {" publication_date": "2021", "fullname_first_author": "Dan Hendrycks", "paper_title": "Measuring massive multitask language understanding", "reason": "This paper introduced the MMLU benchmark, which serves as the primary dataset for evaluating the proposed benchmark reduction method. MMLU is an influential large language model benchmark. This paper is crucial because it's a fundamental dataset used for the experimental evaluation.", "section_number": 5}, {" publication_date": "2021", "fullname_first_author": "Jason Wei", "paper_title": "Finetuned language models are zero-shot learners", "reason": "This paper is important because it is a foundational paper in the field of in-context learning (ICL), which forms the basis of the proposed ICT method.  Its discussion of zero-shot learning capabilities in language models directly supports the efficiency and scalability of the proposed approach.", "section_number": 3}, {" publication_date": "2020", "fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "reason": "This paper significantly influenced the development of the ICT method, as it introduced the concept of in-context learning (ICL) and demonstrated its effectiveness in improving the performance of large language models on various tasks without requiring explicit model training. The introduction of in-context learning is critical to this paper.", "section_number": 3}, {" publication_date": "2020", "fullname_first_author": "Cuong Nguyen", "paper_title": "LEEP: A new measure to evaluate transferability of learned representations", "reason": "This paper introduced LEEP, which is an important precursor to the proposed ICT method. Although LEEP employs linear probing, which is more resource-intensive than in-context learning, its focus on efficiently measuring transferability and providing a scalable solution is directly relevant to this research.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Albert Q. Jiang", "paper_title": "Mistral 7b", "reason": "This paper introduces a new language model, Mistral-7b-v0.3, which is one of the key LLMs used to evaluate the effectiveness of the proposed benchmark reduction method. The inclusion of this specific model, as one of the models that are evaluated on the benchmark, demonstrates the wide adoption of the paper's method.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Hugo Touvron", "paper_title": "Llama 2: Open foundation and fine-tuned chat models", "reason": "This paper introduces Llama-2-13B and Llama-2-7B, which are LLMs used in this research for estimating ICT. As key models used in experiments, it is an important aspect of this paper.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Marah Abdin", "paper_title": "Phi-3 technical report: A highly capable language model locally on your phone", "reason": "This paper introduces Phi-2 and Phi-3, two language models used in the experiments to evaluate the proposed benchmark reduction technique.  The models are tested on the original and reduced benchmarks, and their performance helps validate the effectiveness of the proposed approach.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Marco Bellagente", "paper_title": "Stable lm 2 1.6 b technical report", "reason": "This paper introduces StableLM-2-1.6B, which is used in this research as one of the LLMs for evaluating the proposed benchmark reduction approach. The experimental results from this model help to validate the effectiveness and generalizability of the proposed approach.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Yupeng Chang", "paper_title": "A survey on evaluation of large language models", "reason": "This survey paper provides a comprehensive overview of the challenges and limitations of existing LLM evaluation methods, providing important context and background information for the current research in efficient benchmark reduction. The survey paper's insights motivate this research.", "section_number": 1}, {" publication_date": "1977", "fullname_first_author": "Gerard Cornuejols", "paper_title": "On the uncapacitated location problem", "reason": "This is a foundational paper on facility location problems, which forms the basis of the proposed BENTO method for benchmark task reduction. Its work on submodular optimization techniques is directly relevant to and underpins the efficiency of BENTO.", "section_number": 4}, {" publication_date": "1978", "fullname_first_author": "G. L. Nemhauser", "paper_title": "An analysis of approximations for maximizing submodular set functions-i", "reason": "This paper presents theoretical results on the approximability of submodular functions using greedy algorithms, which is directly relevant to the proposed BENTO method.  The work on greedy approximation algorithms is essential to efficiently solving the facility location problem used in BENTO.", "section_number": 4}, {" publication_date": "2003", "fullname_first_author": "Mikhail Belkin", "paper_title": "Laplacian eigenmaps for dimensionality reduction and data representation", "reason": "This paper introduces Laplacian Eigenmaps (LE), which is a crucial component of the proposed BENTO-le method. LE is used to enhance the clustering structure in the task transferability matrix, improving the robustness and performance of the benchmark reduction process.", "section_number": 4}, {" publication_date": "2009", "fullname_first_author": "Stephen Robertson", "paper_title": "The probabilistic relevance framework: Bm25 and beyond", "reason": "This paper introduces BM25, a classic text similarity measure, used as a baseline method in this research for benchmark task reduction. This is an important baseline in comparing the proposed method.", "section_number": 5}, {" publication_date": "2021", "fullname_first_author": "Dan Hendrycks", "paper_title": "Aligning AI with shared human values", "reason": "This paper introduced the AGIEval benchmark, which is one of the datasets used for evaluating the proposed benchmark reduction approach. The inclusion of this dataset shows the generality of the paper's method.", "section_number": 5}, {" publication_date": "2022", "fullname_first_author": "Mirac Suzgun", "paper_title": "Challenging big-bench tasks and whether chain-of-thought can solve them", "reason": "This paper introduced the Big-Bench Hard benchmark, another dataset for evaluating the proposed benchmark reduction approach.  Its inclusion demonstrates the generality of this approach across diverse benchmark datasets.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Wanjun Zhong", "paper_title": "Agieval: A human-centric benchmark for evaluating foundation models", "reason": "This paper introduced the AGIEval benchmark, which is used in the experiments to evaluate the proposed benchmark reduction technique.  The results on this benchmark demonstrate the generality of the proposed approach.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Timothy R. McIntosh", "paper_title": "Inadequacies of large language model benchmarks in the era of generative artificial intelligence", "reason": "This paper critiques the limitations of existing LLM benchmarks, which serves as a strong motivation for the development of more efficient and effective evaluation methods like the one proposed in this paper. The paper shows the inadequacy of existing benchmarks.", "section_number": 1}]}