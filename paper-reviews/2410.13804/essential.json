{"importance": "This paper is highly relevant to researchers working on large language model (LLM) evaluation and benchmarking.  It offers a novel, efficient method for reducing the size of benchmark datasets without sacrificing evaluation accuracy.  This is crucial for managing the high computational cost associated with evaluating LLMs and for making benchmarking more accessible to researchers with limited resources. The findings open avenues for optimizing LLM evaluation strategies and inform future research in efficient and cost-effective LLM evaluation.", "summary": "BENTO efficiently reduces LLM benchmark size by 95% using in-context transferability, achieving 97% evaluation accuracy, saving computational costs without compromising quality.", "takeaways": ["In-context transferability (ICT) effectively measures task similarity for LLM benchmark reduction.", "BENTO, a facility location-based method, selects a representative subset of tasks for efficient LLM evaluation.", "Benchmark reduction using ICT and BENTO achieves significant cost savings without accuracy loss."], "tldr": "Evaluating large language models (LLMs) is expensive due to the large number of tasks in benchmark datasets. This paper introduces BENTO, a novel method that efficiently reduces the number of tasks needed for accurate LLM evaluation. BENTO leverages in-context learning to estimate task transferability, identifying highly representative tasks. By optimizing a facility location function, BENTO selects a minimal subset of tasks that maintain evaluation accuracy, significantly reducing computational costs. Experiments on MMLU and FLAN benchmarks show that BENTO reduces tasks to 5% while inducing only a <4% difference in evaluation accuracy compared to using the full benchmark.  This approach is training-free, gradient-free, and highly efficient, offering a practical solution for researchers and developers working with LLMs."}