[{"page_end_idx": 2, "page_start_idx": 1, "section_number": 1, "section_title": "INTRODUCTION", "details": {"details": "The introduction section highlights the critical need for efficient evaluation of large language models (LLMs).  Current LLM benchmarks, while aiming for comprehensiveness, often include tens to hundreds of tasks, leading to high evaluation costs due to the expense of sequential decoding in autoregressive LLMs.  This cost significantly impacts the development process. The core issue is the substantial cost associated with evaluating LLMs on large-scale benchmarks.  The paper proposes to address this by investigating the feasibility of reducing the number of tasks without sacrificing evaluation quality, focusing on the concept of task transferability\u2014the idea that skills learned in one task can positively influence performance on another.  Existing approaches to estimating transferability are deemed computationally prohibitive for LLMs due to their reliance on model finetuning or Fisher information. This paper aims to develop a cost-efficient and training-free approach using in-context learning (ICL) to measure transferability, offering a solution that\u2019s both practical and scalable for LLMs.", "first_cons": "The introduction section lacks concrete examples of existing benchmark reduction methods, making it difficult for the reader to fully grasp the challenges and limitations of current approaches before the proposed method is introduced.  It mentions that existing methods rely on computationally expensive techniques, but doesn't provide specific examples or details.", "first_pros": "The introduction effectively establishes the significance and timeliness of the research problem. The high cost of evaluating LLMs on large benchmarks is clearly articulated, creating a compelling motivation for seeking more efficient evaluation methods. The problem statement is well-defined and focused, setting the stage for the proposed solution.", "keypoints": ["High cost of evaluating LLMs (tens to hundreds of tasks in current benchmarks, sequential decoding in autoregressive LLMs leads to high cost and severe overhead).", "Focus on task transferability and relevance as key factors in efficient benchmark reduction.", "Existing transferability estimation methods are computationally expensive (relying on model finetuning or Fisher information, making them unsuitable for LLMs).", "Proposed method aims for cost-efficient and training-free task transferability estimation using in-context learning (ICL).", "Goal: Reduce the number of tasks in LLM benchmarks to optimize evaluation efficiency without sacrificing quality, potentially reducing tasks to only 5% while keeping evaluation accuracy above 97%"], "second_cons": "The introduction doesn't explicitly state the specific contributions or the novelty of the proposed method compared to existing work in a concise manner. This makes it slightly harder for the reader to immediately understand the key advancements the paper offers.", "second_pros": "The introduction is well-written and easy to follow.  The language is clear, concise, and the flow of ideas is logical. The problem is clearly articulated, and the motivation for the research is well-justified. The introduction sets a solid foundation for the subsequent sections of the paper.", "summary": "This paper addresses the high cost of evaluating large language models (LLMs) by proposing a novel method for efficiently reducing the number of tasks in LLM benchmarks.  Current benchmarks, with tens to hundreds of tasks, are expensive to evaluate due to the nature of autoregressive LLMs. This research investigates task transferability, aiming to identify representative subsets of tasks using in-context learning (ICL), which is a training-free and gradient-free approach, allowing for a significant reduction in evaluation costs while maintaining high accuracy. The authors aim to reduce tasks to 5% of the original benchmark with only a <4% difference in evaluation accuracy."}}, {"page_end_idx": 3, "page_start_idx": 2, "section_number": 2, "section_title": "RELATED WORK", "details": {"details": "This section, \"Related Work,\" reviews existing research on task transferability and benchmark reduction for large language models (LLMs).  Regarding task transferability, previous methods often relied on computationally expensive techniques like Bayesian optimization, information theory, or model fine-tuning, making them unsuitable for large-scale LLM evaluation.  In contrast, the paper highlights the potential of in-context learning (ICL) as a cost-effective and training-free alternative for estimating transferability.  Concerning benchmark reduction, the paper categorizes existing methods into two approaches: task selection and example selection.  Task selection methods often lack efficiency or accuracy, while example selection methods are often limited in their ability to substantially reduce the size of the benchmark. The paper emphasizes that its proposed approach provides a more efficient and accurate alternative to these existing methods.", "first_cons": "The review of existing work on task transferability and benchmark reduction is not exhaustive and might overlook some relevant approaches.", "first_pros": "The section clearly identifies the limitations of previous methods, highlighting the need for a more efficient approach to evaluating LLMs.", "keypoints": ["Existing transferability estimation methods are computationally expensive, relying on model finetuning or Fisher information.", "In-context learning (ICL) is proposed as a training-free and cost-efficient alternative for estimating task transferability.", "Benchmark reduction methods are categorized into task selection and example selection; both have limitations in efficiency and accuracy.", "The paper aims to provide a more efficient and accurate method than existing approaches."], "second_cons": "The discussion lacks a detailed comparison of the proposed approach with all the existing benchmark reduction techniques mentioned.", "second_pros": "The section effectively positions the paper's contribution within the existing literature, clearly demonstrating its novelty and advantage.", "summary": "This section reviews prior work on efficiently estimating task transferability and reducing the size of LLM benchmarks.  It critiques existing approaches, highlighting their limitations in computational cost and accuracy. The authors point out the high cost and inefficiency associated with existing methods that employ model finetuning or Fisher information for measuring transferability and existing methods that do not substantially reduce benchmark size. They then introduce their proposed method as a cost-effective and accurate alternative using in-context learning for transferability and a novel facility location function for task selection within the benchmark."}}, {"page_end_idx": 5, "page_start_idx": 3, "section_number": 3, "section_title": "TASK TRANSFERABILITY ANALYSIS BY IN-CONTEXT LEARNING", "details": {"details": "This section introduces a novel approach to analyze task transferability in large language model (LLM) benchmarks using in-context learning (ICL).  Instead of the computationally expensive fine-tuning methods, the authors propose *in-context transferability (ICT)*, which measures the transferability between two tasks by using the examples from one task as context when evaluating the performance on the other.  This approach allows for efficient computation of the task transferability matrix for a large number of tasks.  The resulting matrix is visualized as a graph to show the relationships between the tasks. Spectral clustering is then applied to group similar tasks based on their transferability.  This clustering reveals meaningful relationships between the tasks, showcasing the effectiveness of ICT.  The section ends by highlighting the efficiency of ICT and the potential for using this approach for benchmark reduction.", "first_cons": "The reliance on ICL for transferability estimation might not capture the full complexity of task relationships, especially those requiring fine-tuned model adaptation.", "first_pros": "The ICT method is highly efficient and does not require model training, making it scalable for large benchmarks.", "keypoints": ["In-context transferability (ICT) is introduced as a training-free method to measure the transferability between tasks in an LLM benchmark.", "ICT uses in-context learning (ICL) to estimate transferability by using examples from one task as context for evaluating the performance on another task.", "Applying spectral clustering on the resulting transferability matrix reveals clusters of benchmark tasks with high intra-cluster transferability.", "The visualization of the transferability matrix as a graph provides insights into the relationships between tasks in the benchmark."], "second_cons": "The effectiveness of spectral clustering might be affected by the choice of hyperparameters and the specific clustering algorithm used.  The selection of the best hyperparameters might not generalize across different benchmarks.", "second_pros": "The ICT method efficiently reveals valuable insights into the structure of LLM benchmarks by clustering similar tasks together, showing that the benchmark tasks are not randomly distributed but have a \u2018sparse\u2019 topology, implying substantial redundancy.", "summary": "This section introduces a novel, efficient method, in-context transferability (ICT), for analyzing task relationships within LLM benchmarks.  It leverages in-context learning to compute a transferability matrix, visualized as a graph to reveal inherent task clusters. This graph shows that many tasks are highly related, providing a basis for efficient benchmark reduction by identifying redundant tasks.  The approach is training-free and uses spectral clustering to highlight the most representative subset of tasks."}}, {"page_end_idx": 5, "page_start_idx": 4, "section_number": 4, "section_title": "BENCHMARK-TASK REDUCTION (BENTO) BY FACILITY LOCATION", "details": {"details": "This section introduces BENTO, a benchmark task reduction method that leverages facility location (FL) to select a representative subset of tasks from a larger benchmark.  The core idea is to identify a small set of tasks that accurately reflects the overall performance of various LLMs across the entire benchmark. BENTO uses a similarity matrix derived from the in-context transferability (ICT) scores between tasks.  The ICT is computed using in-context learning (ICL), a training-free and gradient-free method.  The facility location problem aims to maximize the similarity between each task in the original benchmark and its nearest neighbor within the selected subset.  This optimization problem is submodular, so a greedy algorithm can efficiently find a near-optimal solution. The authors present two variations of BENTO: BENTO-sim, using a similarity matrix directly from ICT, and BENTO-le, employing Laplacian Eigenmaps (LE) on the ICT matrix to enhance the clustering structure and robustness. The method's efficiency is highlighted by its ability to reduce the number of tasks in a benchmark to as little as 5% while maintaining a high level of accuracy in evaluating LLMs (97% accuracy is achieved on average), outperforming random selection and other baselines.", "first_cons": "The method's reliance on in-context learning and similarity measures might be sensitive to the quality and representativeness of the exemplars and questions used to estimate task transferability.  Using different exemplars or changing the evaluation metric may yield different results and may not scale well with extremely large and diverse benchmarks.", "first_pros": "BENTO is highly efficient and scalable. It is training-free and gradient-free, making it significantly faster and more cost-effective than existing methods that require model finetuning or computationally intensive techniques. It achieves remarkable task reduction (as low as 5%) with minimal impact on the accuracy of LLM evaluations (inducing only <4% difference).", "keypoints": ["BENTO reduces the number of tasks in a benchmark to as little as 5% while maintaining high accuracy (97% on average).", "It uses a facility location function, a submodular optimization problem, to select the most representative task subset, making the algorithm efficient and effective.", "It leverages in-context transferability (ICT) estimated via in-context learning (ICL), eliminating the need for model training or gradient computations.", "Two variants, BENTO-sim and BENTO-le, are presented. BENTO-le, using Laplacian Eigenmaps (LE), shows better performance with fewer tasks."], "second_cons": "The choice of hyperparameters, such as the number of exemplars (L), random seeds (M), and the constant (c) in the similarity matrix computation might need further fine-tuning and sensitivity analysis to ensure robustness and optimal performance across diverse benchmarks and models.", "second_pros": "The method offers two variants (BENTO-sim and BENTO-le), allowing flexibility in choosing the best approach depending on the benchmark's characteristics and desired performance. It provides a principled and mathematically grounded approach to benchmark reduction, rather than relying on heuristics or arbitrary selection methods.", "summary": "BENTO is a novel, efficient benchmark reduction method for evaluating large language models (LLMs). It uses in-context learning to estimate task transferability and then applies a facility location function to select a small, representative subset of tasks from the benchmark. BENTO outperforms existing methods, reducing tasks by up to 95% while maintaining 97% evaluation accuracy on average, making LLM evaluation significantly more efficient and cost-effective."}}, {"page_end_idx": 8, "page_start_idx": 6, "section_number": 5, "section_title": "EXPERIMENT", "details": {"details": "This section details the experimental setup and results of evaluating the proposed BENTO method and baselines for benchmark task reduction on two datasets: MMLU and FLAN.  The experiments focus on measuring the Normalized Root Mean Square Error (NRMSE) for different numbers of selected tasks (k).  The authors compare BENTO against several baselines, including random task selection, a GPT-4-based selection, and methods using BM25 similarity scores. The results show that BENTO consistently outperforms the baselines, achieving high accuracy with a significantly reduced number of tasks (e.g., only 5% of the original tasks).  BENTO leverages in-context learning to estimate task transferability, enabling the efficient selection of representative tasks.  Ablation studies analyze the impact of hyperparameters and similarity metrics on the performance.  The detailed performance of different models on both the original and reduced benchmarks is also reported.", "first_cons": "The study focuses primarily on two benchmark datasets, MMLU and FLAN.  The generalizability of the approach to other, potentially more diverse benchmarks, might be limited and require further investigation.", "first_pros": "The BENTO method demonstrates strong performance, achieving a less than 4% error rate compared to using the full benchmark while significantly reducing the number of tasks required (e.g. 97% accuracy with only 3 tasks out of 57 on MMLU). This translates to substantial efficiency gains in LLM evaluation.", "keypoints": ["BENTO consistently outperforms baselines across both MMLU and FLAN datasets, achieving a remarkably high average accuracy with only 5% of the original tasks.", "The NRMSE (Normalized Root Mean Square Error) is used as the primary evaluation metric, quantifying the relative error between the reduced and full benchmark evaluations.", "The study includes ablation studies to analyze the impact of different hyperparameters (e.g. the parameter c in the similarity matrix calculation) and alternative similarity metrics on the performance.", "On MMLU, BENTO achieves 97% evaluation accuracy on average with only 3 out of 57 tasks;  on FLAN, it achieves around 96% accuracy with only 5% of the tasks. "], "second_cons": "While the ablation study explores hyperparameter tuning and alternative similarity metrics, more extensive investigation on these aspects could strengthen the robustness and generalizability of the reported findings.", "second_pros": "The method is training-free, gradient-free, and highly efficient, relying only on in-context learning, thus making it computationally less expensive compared to other benchmark reduction methods.", "summary": "This experiment section evaluates the proposed BENTO method for benchmark task reduction on MMLU and FLAN datasets.  The results show that BENTO significantly outperforms various baselines by achieving high evaluation accuracy (e.g., >95%) while reducing the number of tasks to only 5% of the original benchmark.  Ablation studies are conducted to analyze the effects of different hyperparameters and similarity metrics.  The method's efficiency and effectiveness are highlighted, demonstrating substantial improvement in evaluating LLMs with significantly reduced computational cost."}}, {"page_end_idx": 9, "page_start_idx": 8, "section_number": 7, "section_title": "LIMITATIONS", "details": {"details": "This section acknowledges a crucial limitation of the proposed benchmark reduction method: smaller benchmarks, while efficient, might be less diverse and more vulnerable to adversarial attacks.  This inherent trade-off between efficiency and comprehensiveness is highlighted. The authors recognize this limitation and suggest it represents a fundamental tension in the evaluation process between speed and the completeness of the metrics used. They don't offer solutions in this section, but the acknowledgment itself is important for responsible application of the method.", "first_cons": "Reduced benchmarks are inherently less diverse and thus potentially more vulnerable to adversarial attacks.", "first_pros": "The authors acknowledge a key limitation of their method, demonstrating transparency and responsible research.", "keypoints": ["Smaller benchmarks, while efficient, may lack diversity and be vulnerable to adversarial attacks.", "A fundamental trade-off exists between the evaluation process's efficiency and the comprehensiveness of the metrics."], "second_cons": "The section only points out the limitation without providing concrete solutions or mitigation strategies.", "second_pros": "The acknowledgment of this limitation encourages further research to address the issue of bias and robustness in smaller benchmarks.", "summary": "The limitations section frankly addresses the inherent trade-off between efficiency and diversity when using reduced benchmarks. Smaller benchmarks, while computationally advantageous, risk decreased diversity and increased vulnerability to adversarial attacks. This limitation highlights the need for careful consideration when applying the proposed benchmark reduction technique."}}]