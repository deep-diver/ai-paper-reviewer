[{"figure_path": "2410.13804/tables/table_2_0.html", "caption": "Table 1: Evaluation of LLMs on two benchmarks and their BENTO-reduced versions using the same prompts and random seeds\u00b2. Previously reported results\u00b3are available in Appendix G.", "description": "The table presents the performance of several LLMs on two benchmarks (MMLU and BBH) and their reduced versions using BENTO, showing the consistency of evaluation results with a reduced number of tasks.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.13804/tables/table_7_0.html", "caption": "Table 2: NRMSE on MMLU (lower the better) when selecting k tasks for evaluation. Each number is averaged over 9 different models. The standard deviation can be found in Appendix C.", "description": "Table 2 presents the normalized root mean squared error (NRMSE) on the MMLU benchmark for different task reduction methods and varying numbers of selected tasks.", "section": "5.1 MAIN RESULTS"}, {"figure_path": "2410.13804/tables/table_7_1.html", "caption": "Table 3: NRMSE on FLAN (lower the better). k is the number of selected tasks. Each number is averaged over 6 different models. The standard deviation can be found in Appendix C.", "description": "Table 3 presents the normalized root mean square error (NRMSE) on the FLAN benchmark for different task reduction methods and varying numbers of selected tasks (k).", "section": "5.1 MAIN RESULTS"}, {"figure_path": "2410.13804/tables/table_8_0.html", "caption": "Table 4: Ablation study of similarity metrics: we compare the best NRMSE on different datasets achieved by different metrics: \u201ccos\u201d- cosine similarity, and \u201ccheby\u201d\u2013 Chebyshev similarity.", "description": "Table 4 presents an ablation study comparing the best NRMSE achieved by different similarity metrics (cosine, Chebyshev, and Euclidean) across three datasets (MMLU, AGIEval Eng, and Big Bench Hard) for the BENTO benchmark reduction method.", "section": "5.2 ABLATION STUDY"}, {"figure_path": "2410.13804/tables/table_9_0.html", "caption": "Table 5: Example selection with and without BENTO (-sim) on MMLU. \u201cRandom\u201d refers to random selection of examples. \"Random+BENTO\" applies \"Random\u201d at first to reduce the examples per task to 5% and then selects a subset of tasks by BENTO. It shows that BENTO can further improve example selection and outperforms example selection only. For example, \"Random+BENTO\" with 2.0% remaining data achieves a lower NRMSE than \"Random\" with 5.0% remaining data; \u201cRandom+BENTO\u201d with 0.7% remaining data achieves the same NRMSE as \"Random\" with 2.0% remaining data.", "description": "Table 5 presents a comparison of example selection strategies (random vs. random combined with BENTO) demonstrating that BENTO improves NRMSE, even with fewer examples.", "section": "5 EXPERIMENT"}, {"figure_path": "2410.13804/tables/table_13_0.html", "caption": "Table 1: Evaluation of LLMs on two benchmarks and their BENTO-reduced versions using the same prompts and random seeds\u00b2. Previously reported results\u00b3are available in Appendix G.", "description": "The table presents the performance of various LLMs on the full MMLU and BBH benchmarks, and their reduced versions using the BENTO method.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.13804/tables/table_14_0.html", "caption": "Table 2: NRMSE on MMLU (lower the better) when selecting k tasks for evaluation. Each number is averaged over 9 different models. The standard deviation can be found in Appendix C.", "description": "Table 2 presents the normalized root mean square error (NRMSE) on the MMLU benchmark for different methods of task reduction, varying the number of selected tasks (k) and averaged over nine language models.", "section": "5.1 Main Results"}, {"figure_path": "2410.13804/tables/table_14_1.html", "caption": "Table 2: NRMSE on MMLU (lower the better) when selecting k tasks for evaluation. Each number is averaged over 9 different models. The standard deviation can be found in Appendix C.", "description": "Table 2 presents the normalized root mean square error (NRMSE) on the MMLU benchmark for different benchmark reduction methods when selecting varying numbers of tasks.", "section": "5.1 MAIN RESULTS"}, {"figure_path": "2410.13804/tables/table_15_0.html", "caption": "Table 8: Performance of different models on all tasks and selected tasks of MMLU.", "description": "Table 8 presents the performance of nine different LLMs on all tasks and a selected subset of tasks from the MMLU benchmark, showcasing the consistency of model performance between the full benchmark and the reduced benchmark.", "section": "5.1 MAIN RESULTS"}, {"figure_path": "2410.13804/tables/table_15_1.html", "caption": "Table 9: NRMSE on AGIEval English (lower the better). k is the number of selected tasks. Each number is averaged over 4 different models.", "description": "Table 9 presents the normalized root mean square error (NRMSE) on the AGIEval English benchmark for different task reduction methods, varying the number of selected tasks (k).", "section": "5.1 MAIN RESULTS"}, {"figure_path": "2410.13804/tables/table_15_2.html", "caption": "Table 10: NRMSE on Big Bench Hard (lower the better). k is the number of selected tasks. Each number is averaged over 8 different models.", "description": "The table presents the normalized root mean squared error (NRMSE) on the Big Bench Hard benchmark for different task reduction methods, varying the number of selected tasks (k).", "section": "5.1 Main Results"}, {"figure_path": "2410.13804/tables/table_16_0.html", "caption": "Table 11: Comparison of full benchmark performance and reduced benchmark performance. The numbers outside brackets are measured by ourselves and the numbers inside are reported by previous works. The difference may come from different prompts / quantization.", "description": "Table 11 compares the performance of various LLMs on full benchmarks and their BENTO-reduced versions, showing consistent results despite significant task reduction.", "section": "5 EXPERIMENT"}]