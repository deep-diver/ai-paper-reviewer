{"references": [{"fullname_first_author": "Jared Kaplan", "paper_title": "Scaling laws for neural language models", "publication_date": "2020-01-08", "reason": "This paper established scaling laws for neural language models, which are fundamental to understanding the relationship between model size, training data, and performance, informing the design and optimization of LLMs."}, {"fullname_first_author": "Edward J. Hu", "paper_title": "Lora: Low-rank adaptation of large language models", "publication_date": "2022-04-25", "reason": "This paper introduced LoRA, a low-rank adaptation technique for efficiently fine-tuning large language models, addressing the computational challenges of training large LLMs."}, {"fullname_first_author": "Mikhail Belkin", "paper_title": "Reconciling modern machine-learning practice and the classical bias-variance trade-off", "publication_date": "2019-06-15", "reason": "This paper offered insights into the theoretical aspects of overparameterization in machine learning, which is closely related to the efficiency gains through architecture optimization described in the current work."}, {"fullname_first_author": "Zeyuan Allen-Zhu", "paper_title": "A convergence theory for deep learning via over-parameterization", "publication_date": "2019-06-09", "reason": "This paper investigated convergence in deep learning through overparameterization, a key concept related to the efficiency gains resulting from architecture optimization in large language models."}, {"fullname_first_author": "Geoffrey E. Hinton", "paper_title": "Distilling the knowledge in a neural network", "publication_date": "2015-03-02", "reason": "This paper introduced knowledge distillation, a crucial technique for model compression and efficient transfer learning, which is fundamental to the Puzzle framework's approach to optimizing LLMs."}]}