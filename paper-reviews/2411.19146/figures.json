[{"figure_path": "https://arxiv.org/html/2411.19146/x1.png", "caption": "Figure 1: An overview of the three stages of our Puzzle framework.", "description": "The Puzzle framework consists of three stages.  In Stage 1, \"Crafting the Puzzle Pieces,\" block-wise local distillation is used to train alternative sub-block replacements for each layer of the parent LLM in parallel. Each alternative replacement is scored based on its quality and inference cost.  This creates a library of scored blocks, like puzzle pieces. Stage 2, \"Assembling the Puzzle Architecture,\" utilizes mixed-integer programming (MIP) to assemble an optimized heterogeneous architecture by selecting the best-scoring blocks from the library, while satisfying constraints such as latency, memory usage, and throughput.  Finally, Stage 3, \"Uptraining,\" involves training the assembled architecture using global knowledge distillation to improve inter-block compatibility and performance.", "section": "2 Search Space"}, {"figure_path": "https://arxiv.org/html/2411.19146/x2.png", "caption": "Figure 2: Blockwise local distillation (BLD): each block is trained in parallel and independently.", "description": "The figure illustrates the Blockwise Local Distillation (BLD) process, a key component of the Puzzle framework.  Instead of distilling an entire model at once, BLD trains individual blocks (typically transformer layers) of the child model independently and in parallel. Each child block is trained to mimic its corresponding parent block using the parent's activations as input.  This is done separately for each block, allowing for parallelization across multiple GPUs and resulting in significant computational savings. The individual trained blocks are then assembled to form the complete, optimized child model. The loss function used is the mean squared error (MSE) between the parent block's output and the child block's output.", "section": "3 Blockwise Local Distillation"}, {"figure_path": "https://arxiv.org/html/2411.19146/x3.png", "caption": "Figure 3: Coupled BLD requires training |\ud835\udc9ci|\u00d7|\u2131i|subscript\ud835\udc9c\ud835\udc56subscript\u2131\ud835\udc56|\\mathcal{A}_{i}|\\times|\\mathcal{F}_{i}|| caligraphic_A start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | \u00d7 | caligraphic_F start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | variants per transformer layer, while decoupled BLD requires only |\ud835\udc9ci|+|\u2131i|subscript\ud835\udc9c\ud835\udc56subscript\u2131\ud835\udc56|\\mathcal{A}_{i}|+|\\mathcal{F}_{i}|| caligraphic_A start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | + | caligraphic_F start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | variants per layer, significantly speeding up library construction.", "description": "The figure illustrates the computational cost difference between coupled and decoupled Blockwise Local Distillation (BLD) methods in training sub-blocks for Neural Architecture Search (NAS). Coupled BLD trains all combinations of attention sub-block variants (|Ai|) and FFN sub-block variants (|Fi|) for each layer, resulting in |Ai| * |Fi| training instances per layer. In contrast, decoupled BLD trains each sub-block type separately, resulting in |Ai| + |Fi| training instances per layer. This significantly reduces the overall training cost, accelerating the construction of the block library required for the NAS process.", "section": "3 Blockwise Local Distillation"}, {"figure_path": "https://arxiv.org/html/2411.19146/x4.png", "caption": "Figure 4: Preference of human annotators in a blind test comparison. Results indicate comparable performance between Llama-3.1-70B-Instruct and Nemotron-51B.", "description": "Human annotators conducted a blind test comparing the performance of Llama-3.1-70B-Instruct and Nemotron-51B across various tasks.  The results show that there is no statistically significant difference in perceived quality between the two models, indicating that Nemotron-51B, despite having significantly fewer parameters, maintains performance comparable to its larger counterpart.", "section": "Main Results"}, {"figure_path": "https://arxiv.org/html/2411.19146/", "caption": "Figure 5: Accuracy vs. Throughput performance of Nemotron-51B compared to state-of-the-art models. Throughput is measured on NVIDIA H100 GPUs with the optimal TP setting per model, all running in FP8 on a \u201ctext generation\u201d scenario (see Table\u00a03). The red line represents the efficient frontier, highlighting models with the best accuracy-to-throughput tradeoff. Accuracy=(MT-Bench \u00d7\\times\u00d710 + MMLU) / 2", "description": "Figure 5 illustrates the performance of Nemotron-51B against other state-of-the-art large language models.  The x-axis represents the throughput (tokens processed per second) achieved on NVIDIA H100 GPUs using FP8 precision and optimal tensor parallelism for each model during text generation tasks (as defined in Table 3).  The y-axis shows the accuracy, calculated as the average of MT-Bench scores multiplied by 10 and MMLU scores, divided by 2. The red line indicates the Pareto frontier; points on or above the line represent models with optimal accuracy-to-throughput ratios. Nemotron-51B is shown to achieve high accuracy with good throughput, making it efficient for real-world deployment.", "section": "Main Results"}, {"figure_path": "https://arxiv.org/html/2411.19146/x6.png", "caption": "Figure 6: The estimated runtime of the attention and FFN subblocks of Nemotron-51B, relative to the original subblocks of Llama-3.1-70B-Instruct.", "description": "Figure 6 shows a comparison of the runtime of attention and feed-forward network (FFN) sub-blocks in Nemotron-51B, a smaller model created by the Puzzle framework, compared to the original Llama-3.1-70B-Instruct model.  The graph plots the estimated runtime of each sub-block type (attention and FFN) across the layers of Nemotron-51B, expressed as a fraction of the original runtime in Llama-3.1-70B-Instruct.  This allows visualization of where the Puzzle framework made significant optimizations (shown in green), leading to reduced computation time in Nemotron-51B.  The graph also highlights that certain layers retained their original computational complexity, indicating that these sections of the model were deemed critical for maintaining model performance.", "section": "In-Depth Analysis and Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2411.19146/x7.png", "caption": "Figure 7: Accuracy vs. Throughput performance of Llama-3.1-8B-Instruct child derivatives, some constructed using LM loss as replace-1-block scores, and some constructed using KL divergence as replace-1-block scores. LM loss aims to capture the general quality degradation induced by changing a specific parent block, while KL divergence aims to capture the distance from the parent model induced by this change. KL divergence results in better architecture choices. Accuracy = (MT-Bench \u00d7\\times\u00d710 + MMLU) / 2. Throughput is estimated via the sum of measured block runtimes on a single NVIDIA RTX 4090 GPU.", "description": "Figure 7 compares different methods for scoring alternative blocks during neural architecture search (NAS) for LLMs.  It shows the accuracy and throughput of Llama-3.1-8B-Instruct models optimized using two different scoring metrics: LM loss and KL divergence.  LM loss measures the overall quality decrease when replacing a block, while KL divergence measures the difference between the original and modified models' outputs.  The results indicate that using KL divergence for scoring leads to better model architectures (higher accuracy) compared to using LM loss.  The accuracy is calculated as the average of MT-Bench and MMLU scores, weighted 10:1, and throughput is estimated based on the sum of block runtimes on a single NVIDIA RTX 4090 GPU.", "section": "8.1.4 Impact of Different Block Scoring Metrics"}]