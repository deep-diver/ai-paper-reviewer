[{"content": "| $\nmathcal{L}_{\ntext{LM}}$ (1) | $\nmathcal{L}_{\ntext{cosine}}$ (2) | $\nmathcal{L}_{\ntext{KLD}}$ (3) | **MMLU** | **MT-Bench** | **Average** | **Validation $\nmathcal{L}_{\ntext{KLD}}$** |\n|---|---|---|---|---|---|---|\n| \u2717 | \u2717 | \u2717 | 78.39 | 8.67 | 82.55 | 0.19 |\n| \u2713 | \u2717 | \u2717 | 78.55 | 7.71 | 77.83 | 0.31<sup>\u2217</sup> |\n| \u2713 | \u2717 | \u2713 | 79.26 | 8.85 | 83.88 | 0.14 |\n| \u2717 | \u2717 | \u2713 | 79.33 | 8.68 | 83.07 | 0.10 |\n| \u2713 | \u2713 | \u2717 | 79.04 | 7.80 | 78.52 | 0.30<sup>\u2217</sup> |\n| \u2717 | \u2713 | \u2717 | 79.40 | 8.74 | 83.40 | 0.16 |\n| \u2713 | \u2713 | \u2713 | 79.45 | 8.66 | 83.03 | 0.14 |\n| \u2717 | \u2713 | \u2713 | 79.61 | 8.87 | 84.16 | 0.11 |\n| Llama-3.1-70B-Instruct (parent) |  |  | 81.66 | 8.93 | 85.48 | 0.00 |\n| Nemotron-51B-Instruct (child)<sup>\u2020</sup> |  |  | 80.20 | 8.99 | 85.10 | 0.08 |", "caption": "Table 1: Ablation study for different combinations of LM loss, block (hidden activations) loss, and logits KLD loss. All models (Nemotron-51B, derived from Llama-3.1-70B-Instruct) were trained for \u223c5\u2062Bsimilar-toabsent5\ud835\udc35\\sim 5B\u223c 5 italic_B tokens. First row did not undergo uptraining. Adjacent rows with the same color differ only in the \u2112LMsubscript\u2112LM\\mathcal{L}_{\\text{LM}}caligraphic_L start_POSTSUBSCRIPT LM end_POSTSUBSCRIPT component. \u2217During the KD process for this combination, the validation \u2112KLDsubscript\u2112KLD\\mathcal{L}_{\\text{KLD}}caligraphic_L start_POSTSUBSCRIPT KLD end_POSTSUBSCRIPT consistently increased. \u2020Trained for 45B tokens using \u2112GKDsubscript\u2112GKD\\mathcal{L}_{\\text{GKD}}caligraphic_L start_POSTSUBSCRIPT GKD end_POSTSUBSCRIPT defined in Equation\u00a0(4).", "description": "This table presents an ablation study evaluating different loss function combinations during the knowledge distillation (KD) process for fine-tuning the Nemotron-51B model.  It investigates the impact of including the language modeling loss (\u2112LM), hidden state cosine similarity loss (\u2112cosine), and Kullback-Leibler divergence loss (\u2112KLD). Each row represents a different combination of these losses used during the 5 billion token training phase. The results show the impact of each loss component on downstream task performance (MMLU, MT-Bench), and overall validation \u2112KLD. The table highlights that the combination of cosine similarity loss and KL-divergence loss resulted in the best performance, while including the LM loss negatively impacts performance in most scenarios. Note that one model was additionally trained for 45B tokens using the best-performing loss combination.", "section": "8 In-Depth Analysis and Ablation Studies"}, {"content": "| Benchmark | Llama-3.1-70B-Instruct | Nemotron-51B | Accuracy Preserved (%) |\n|---|---|---|---| \n| Winogrande [43] | 85.08 | 84.53 | 99.35 |\n| ARC Challenge [11] | 70.39 | 69.20 | 98.30 |\n| MMLU [19] | 81.66 | 80.20 | 98.21 |\n| HellaSwag [52] | 86.44 | 85.58 | 99.01 |\n| GSM8K [12] | 92.04 | 91.43 | 99.34 |\n| TruthfulQA [31] | 59.86 | 58.63 | 97.94 |\n| XLSum English [17] | 33.86 | 31.61 | 93.36 |\n| MMLU Chat* | 81.76 | 80.58 | 98.55 |\n| GSM8K Chat* | 81.58 | 81.88 | 100.37 |\n| Instruct HumanEval (n=20) [10]** | 75.85 | 73.84 | 97.35 |\n| MT-Bench [53] | 8.93 | 8.99 | 100.67 |", "caption": "Table 2: Accuracy comparison of Nemotron-51B with Llama-3.1-70B-Instruct across several benchmarks. Accuracy preserved is the ratio of child to parent accuracy. *Chat prompt as defined in Adler et\u00a0al. [2]. **version by\nCodeParrot.", "description": "This table compares the performance of the smaller, optimized model Nemotron-51B against its larger parent model Llama-3.1-70B-Instruct across multiple established language model benchmarks.  For each benchmark, it shows the accuracy achieved by both models and calculates the percentage of the parent model's accuracy that the smaller model retains (Accuracy Preserved).  This illustrates the extent to which Nemotron-51B maintains performance while being significantly more efficient.  The benchmarks include a variety of tasks assessing different capabilities of the language model, allowing for a more thorough comparison.  Notes are provided to clarify specific variations of certain benchmarks.", "section": "Main Results"}, {"content": "| Scenario | Input/Output | Nemotron-51B (TP#) | Llama-3.1-70B-Instruct (TP#) | Speedup |\n|---|---|---|---|---|\n| Chatbot | 128/128 | 5478 (TP1) | 2645 (TP1) | 2.07 |\n| Text Generation | 128/1024 | 6472 (TP1) | 2975 (TP4) / 1274 (TP1) | 2.17 / 5.08 |\n| Long Text Generation | 128/2048 | 4910 (TP2) | 2786 (TP4) | 1.76 |\n| Inference-time compute | 128/4096 | 3855 (TP2) | 1828 (TP4) | 2.11 |\n| Summarization/RAG | 2048/128 | 653 (TP1) | 339 (TP4) / 301 (TP1) | 1.92 / 2.17 |\n| Stress Test | 2048/2048 | 2622 (TP2) | 1336 (TP4) | 1.96 |", "caption": "Table 3: Throughput comparison of Nemotron-51B and Llama-3.1-70B-Instruct across various scenarios. Throughput is measured in tokens per second per GPU (NVIDIA H100). TP# indicates the number of GPUs used in tensor parallelism. Note: Results were obtained on NVIDIA H100 SXM GPUs with FP8 quantization for weights, activations and KV cache using TensorRT-LLM. Optimal tensor parallelism was used for each model. Input/output sequence lengths indicate the prefill (input) and decode (output) operations performed by the LLM.", "description": "This table compares the inference throughput of the Nemotron-51B model and its parent model, Llama-3.1-70B-Instruct, across different scenarios.  Throughput is measured in tokens per second per NVIDIA H100 GPU.  The number of GPUs used for tensor parallelism is also shown.  The experiments used FP8 quantization for weights, activations, and KV cache with the TensorRT-LLM inference engine.  Optimal tensor parallelism was used for both models.  The input/output sequence lengths show the lengths used for the prefill (input) and decoding (output) steps of each LLM.", "section": "Main Results"}, {"content": "| Context Length | Parent Average Score | Child Average Score | Accuracy Preserved (%) |\n|---|---|---|---| \n| 1024 | 99.81 | 99.78 | **99.90** |\n| 2048 | 97.97 | 97.32 | **99.34** |\n| 4096 | 93.38 | 92.12 | **98.65** |\n| 8192 | 92.68 | 90.63 | **97.79** |\n| 16384 | 91.08 | 87.65 | **96.23** |", "caption": "Table 4: Performance comparison of Llama-3.1-70B-Instruct (parent) and Nemotron-51B (child) on the RULER benchmark for context lengths up to 16K tokens. Accuracy preserved is the ratio of the Nemotron average to the Llama average, expressed as a percentage.", "description": "This table presents a detailed comparison of the performance of Llama-3.1-70B-Instruct (the original, larger model) and Nemotron-51B (the optimized, smaller model) on the RULER benchmark.  The RULER benchmark tests various reasoning abilities of language models across different context lengths (the amount of text the model processes at once). The table shows the average scores achieved by both models at context lengths of 1024, 2048, 4096, 8192, and 16384 tokens.  A key metric is \"Accuracy Preserved,\" which indicates the percentage of the original model's accuracy retained by Nemotron-51B at each context length. This illustrates how well the optimization process maintained performance while reducing model size and computational needs.  It helps to understand the tradeoff between model size and accuracy in different context lengths.", "section": "Main Results"}, {"content": "| Model | Throughput* | Accuracy |\n|---|---|---|\n| Ours (child) | 5856 | 73.98 |\n| Llama-3.2-3B-Instruct | 5737 | 70.36 |\n| Llama-3.1-8B-Instruct (parent) | 3385 | 76.40 |", "caption": "Table 5: Accuracy and throughput of our high-throughput child derivative of Llama-3.1-8B-Instruct, which achieves equivalent throughput to Llama-3.2-3B-Instruct and far better accuracy. Throughput is estimated via the sum of measured block runtimes on a single NVIDIA RTX 4090 GPU, measured with an input-output sequence length of 1024 tokens each, the scenario for which this model was optimized. Accuracy = (MT-Bench \u00d7\\times\u00d710 + MMLU) / 2.", "description": "Table 5 presents a comparison of the performance of three language models: a new high-throughput model derived from Llama-3.1-8B-Instruct using the Puzzle framework, Llama-3.2-3B-Instruct, and the original Llama-3.1-8B-Instruct.  The comparison focuses on throughput (tokens processed per second) and accuracy. Throughput is measured using a single NVIDIA RTX 4090 GPU, processing input and output sequences of 1024 tokens each. Accuracy is calculated as the average of two benchmarks: MT-Bench multiplied by 10, and MMLU, divided by 2.  The table highlights that the new Puzzle-derived model achieves a throughput comparable to Llama-3.2-3B-Instruct while demonstrating significantly higher accuracy.", "section": "Main Results"}, {"content": "| Model | Throughput* | Accuracy |\n|---|---|---|\n| Puzzle with Coupled BLD | 5856 | 73.98 |\n| Puzzle with Decouple BLD | 5834 | 72.92 |\n| Llama-3.2-3B-Instruct | 5737 | 70.36 |", "caption": "Table 6: The effect of coupled BLD vs decoupled BLD on high-throughput child derivatives of Llama-3.1-8B-Instruct. We found a relevant subspace of the search space using a decoupled BLD Puzzle, then trained coupled BLD on this subspace and ran a separate Puzzle, leading to additional improvement. Throughput is estimated via the sum of measured block runtimes on a single NVIDIA RTX 4090 GPU. Accuracy = (MT-Bench \u00d7\\times\u00d710 + MMLU) / 2.", "description": "This table compares the performance of two different approaches to building a library of blocks for training a smaller, more efficient language model (LLM) from a larger one.  The first approach uses \"decoupled BLD,\" which trains the blocks independently to save computational resources. The second then uses \"coupled BLD,\" training combinations of blocks together after having identified a smaller, more promising search space using the decoupled method. The table shows that the two-stage approach (decoupled followed by coupled BLD) leads to better performance (higher accuracy) compared to using only the decoupled approach, showcasing the effectiveness of this combined strategy for efficient LLM optimization.  Throughput is calculated for a single NVIDIA RTX 4090 GPU.  Accuracy is a combined score from the MT-Bench and MMLU benchmarks.", "section": "8.1 Block Library Construction Ablation Studies"}, {"content": "| Model | MT-Bench | MMLU | MMLU-STEM |\n|---|---|---|---|\n| Gutenberg-Trained | 7.98 | 74.84 | 64.5 |\n| DistillationMix-Trained | 8.61 | 78.39 | 70.35 |", "caption": "Table 7: Benchmark results on Llama-3.1-70B-Instruct derivatives obtained from Puzzle without uptraining applied with different datasets.", "description": "This table presents the results of applying the Puzzle framework to create LLM derivatives from the Llama-3.1-70B-Instruct model, without the final global knowledge distillation (GKD) uptraining step.  It compares the performance of models trained on two different datasets: the diverse Distillation Mix and the more limited Project Gutenberg dataset.  The performance metrics include the MT-Bench and MMLU scores to evaluate the models' capabilities across various tasks and domains. The purpose is to assess how well the Puzzle framework preserves model performance with different training data.", "section": "8.1 Block Library Construction Ablation Studies"}, {"content": "| BLD Token Budget | MT-Bench | MMLU |\n|---|---|---|\n| 1.0B Tokens | 8.98 | 78.54 |\n| 0.5B Tokens | 8.86 | 78.44 |\n| 0.25B Tokens | 8.51 | 78.27 |", "caption": "Table 8: Performance comparison of Puzzle-optimized architectures trained with varying BLD token budgets. Metrics include MT-Bench and MMLU scores.", "description": "This table presents a comparison of the performance of LLMs whose architectures were optimized using the Puzzle framework.  The models were trained with varying amounts of training data during the Blockwise Local Distillation (BLD) phase. The comparison focuses on two key metrics: MT-Bench scores and MMLU scores. This allows assessment of how much the amount of training data in the BLD stage impacts downstream performance in terms of these two metrics.", "section": "8.1 Block Library Construction Ablation Studies"}, {"content": "| Model | Throughput* | Half-MMLU Accuracy (Test Set) |\n|---|---|---|\n| Puzzle: scored with Half-MMLU accuracy (train set) | 5818 | 66.24 |\n| Puzzle: scored with KL divergence | 5834 | 64.94 |\n| Llama-3.2-3B-Instruct | 5737 | 60.06 |", "caption": "Table 9: The effect of task-oriented block scoring on high-throughput child derivatives of Llama-3.1-8B-Instruct. We split the tasks in MMLU into two equal-sized sets and use one of them for block quality scoring and the other for evaluation, showing that even with the same library of trained blocks, block selection can be customized to build architectures that fit a desired target task. Throughput is estimated via the sum of measured block runtimes on a single NVIDIA RTX 4090 GPU.", "description": "This table presents an ablation study on the effect of using different block scoring metrics within the Puzzle framework.  Specifically, it investigates how using a subset of the MMLU benchmark for scoring blocks (Half-MMLU), rather than a general metric like KL divergence, impacts the resulting child model's performance.  The experiment used a reduced search space to manage computational costs.  The table shows the throughput (estimated via summed block runtimes on an NVIDIA RTX 4090 GPU) and accuracy (measured using the test set from the split MMLU benchmark) of different models to illustrate how tailoring block scoring to specific downstream tasks can influence the final model's strengths and weaknesses.", "section": "8.1.4 Impact of Different Block Scoring Metrics"}, {"content": "| Model | MMLU | Throughput (tokens/sec) |\n|---|---|---|\n| Puzzle (No-op only) | 75.40 | 5604.18 |\n| Puzzle (Full search space) | 78.39 | 5500.25 |", "caption": "Table 10: Comparison of pre-uptraining Nemotron-51B (derived using the full search space) and a no-op-only variant.", "description": "This table compares the performance of two versions of the Nemotron-51B model. One version was created using the full search space of the Puzzle framework's architecture search, while the other was constrained to only use 'no-op' operations (meaning no alterations made to the original model's architecture). The comparison focuses on the MMLU score, which measures accuracy across diverse tasks, and the throughput (in tokens per second), indicating inference speed.  This demonstrates the impact of search space size on model accuracy and efficiency.", "section": "8.1.5 Effects of Limited Search Space Diversity"}, {"content": "| Optimization Method | MMLU | Throughput (tokens/sec) |\n|---|---|---|\n| Greedy Algorithm | 70.74 | 5500.30 |\n| MIP | 78.39 | 5500.25 |", "caption": "Table 11: Comparison of the budget-constrained greedy algorithm and MIP as search algorithms for Puzzle. Results are shown for pre-uptraining Nemotron-51B under identical throughput constraints.", "description": "This table compares the performance of two different search algorithms used in the Puzzle framework for optimizing the Llama-3.1-70B-Instruct model: a greedy algorithm and a Mixed Integer Programming (MIP) approach. Both algorithms aim to find the best architecture that meets specific throughput requirements (tokens per second) while maintaining accuracy.  The table shows the MMLU (Massive Multitask Language Understanding) score and the achieved throughput (tokens/sec) for each algorithm applied to the Nemotron-51B model *before* the final global knowledge distillation (GKD) uptraining step. This allows for a direct comparison of the algorithms' effectiveness in finding a suitable architecture based solely on their search capabilities.", "section": "8.2 Search Algorithm Ablation Studies"}, {"content": "| Optimization Method | MMLU | Throughput (tokens/sec) |\n|---|---|---|\n| Maximizing Parameters | 23.12 | 5727.08 |\n| pre-uptraining Nemotron-51B | 78.39 | 5500.25 |", "caption": "Table 12: Comparison of maximizing parameter count with Puzzle\u2019s MIP-based optimization as search algorithms. Results are shown for pre-uptraining Nemotron-51B under identical throughput constraints.", "description": "This table compares the performance of two different search algorithms used in the Puzzle framework for optimizing the architecture of the Nemotron-51B language model before the final uptraining stage. One algorithm uses a simple heuristic of maximizing the number of parameters, while the other utilizes Puzzle's mixed-integer programming (MIP) approach. Both algorithms operate under identical throughput constraints to ensure a fair comparison. The table presents the MMLU (Massive Multitask Language Understanding) scores and throughput achieved by both algorithms, highlighting the superior performance of the MIP-based optimization method.", "section": "8.2 Search Algorithm Ablation Studies"}, {"content": "| Model Name | GKD Uptraining | MMLU | MT-Bench | Average |\n|---|---|---|---|---|\n| Llama-3.1-70B-Instruct (parent) | - | 81.66 | 8.93 | 85.48 |\n|  | \u2717 | 78.39 | 8.67 | 82.55 |\n| Nemotron-51B-Instruct (child) | \u2713 | 80.20 | 8.99 | 85.10 |\n| Llama-3.1-8B-Instruct (parent) | - | 69.40 | 8.34 | 76.40 |\n|  | \u2717 | 65.25 | 7.29 | 69.06 |\n| Child derivative of Llama-3.1-8B-Instruct (child) | \u2713 | 65.46 | 8.25 | 73.98 |", "caption": "Table 13: Impact of global knowledge distillation uptraining on MMLU and MT-Bench benchmark scores for child models derived from Llama-3.1-70B-Instruct and Llama-3.1-8B-Instruct.", "description": "This table presents the results of an ablation study evaluating the impact of global knowledge distillation (GKD) uptraining on the performance of smaller language models derived from two larger models: Llama-3.1-70B-Instruct and Llama-3.1-8B-Instruct.  The table shows the MMLU and MT-Bench benchmark scores for each model *with* and *without* the GKD uptraining step, allowing for a direct comparison of performance gains achieved through this final training stage.", "section": "8 In-Depth Analysis and Ablation Studies"}, {"content": "| Context Length | qa_hotpotqa | qa_squad | common_words_extraction* | variable_tracking_1_4 | variable_tracking_2_2 | freq_words_extraction_2 | freq_words_extraction_3.5 | Average | Accuracy Preserved (%) |\n|---|---|---|---|---|---|---|---|---|---| \n| **Parent** |  |  |  |  |  |  |  |  |  |\n| 1024 | N/A | N/A | 100.00 | 100.00 | 100.00 | 99.40 | 99.67 | 99.81 | - |\n| 2048 | N/A | 88.40 | 100.00 | 100.00 | 100.00 | 99.53 | 99.87 | 97.97 | - |\n| 4096 | 67.60 | 87.40 | 100.00 | 100.00 | 99.87 | 99.00 | 99.80 | 93.38 | - |\n| 8192 | 67.80 | 83.80 | 99.96 | 100.00 | 99.40 | 97.87 | 99.93 | 92.68 | - |\n| 16384 | 63.20 | 82.00 | 98.86 | 100.00 | 96.87 | 96.67 | 99.93 | 91.08 | - |\n| 32768 | 61.60 | 77.20 | 93.48 | 100.00 | 97.93 | 95.53 | 100.00 | 89.39 | - |\n| 65536 | 55.4 | 72.60 | 26.16 | 99.96 | 97.93 | 94.53 | 99.87 | 78.06 | - |\n| 131072 | 33.65 | 49.04 | 2.37 | 56.85 | 36.33 | 78.61 | 85.71 | 48.94 | - |\n| **Child** |  |  |  |  |  |  |  |  |  |\n| 1024 | N/A | N/A | 99.98 | 100.00 | 100.00 | 99.40 | 99.53 | 99.78 | **99.9** |\n| 2048 | N/A | 86.20 | 99.86 | 99.96 | 99.67 | 98.40 | 99.80 | 97.32 | **99.34** |\n| 4096 | 63.40 | 85.00 | 99.92 | 100.00 | 98.93 | 97.73 | 99.87 | 92.12 | **98.65** |\n| 8192 | 58.20 | 80.80 | 99.34 | 100.00 | 99.60 | 96.67 | 99.80 | 90.63 | **97.79** |\n| 16384 | 53.40 | 75.60 | 93.50 | 99.72 | 96.80 | 94.73 | 99.80 | 87.65 | **96.23** |\n| 32768 | 45.60 | 70.60 | 51.92 | 98.28 | 93.67 | 90.27 | 99.47 | 78.54 | **87.86** |\n| 65536 | 7.4 | 15.20 | 2.28 | 3.48 | 7.87 | 36.93 | 8.67 | 11.6 | **14.86** |\n| 131072 | 3.80 | 3.20 | 0.10 | 0.00 | 0.00 | 2.07 | 0.00 | 1.31 | **2.67** |", "caption": "Table 14: Full performance comparison of the parent (Llama-3.1-70B-Instruct) and child (Nemotron-51B) models on a subset of the RULER benchmark across all context lengths. Accuracy preserved is the ratio of the child score to the parent score, expressed as a percentage. The names of the benchmarks refer to their implementation and settings used in the official github repository.\n*Varies depending on context length", "description": "This table presents a comprehensive comparison of the performance of the Llama-3.1-70B-Instruct model (parent model) and its distilled version, Nemotron-51B (child model), across various tasks and context lengths within the RULER benchmark.  It showcases how well Nemotron-51B, a smaller and more efficient model, retains the accuracy of its larger counterpart. For each task and context length, the table shows the parent model's accuracy, the child model's accuracy, and the percentage of the parent's accuracy that is preserved by the child model.  This allows for a direct comparison of accuracy and showcases the effectiveness of the distillation technique. The asterisk (*) indicates variations in certain metrics that depend on the specific context length used. The benchmark names are linked to their specific implementations and settings in the official Github repository.", "section": "B RULER Benchmark Performance Tables"}]