{"references": [{"fullname_first_author": "Yuntao Bai", "paper_title": "Training a helpful and harmless assistant with reinforcement learning from human feedback", "publication_date": "2022-04-05", "reason": "This paper is foundational for using reinforcement learning from human feedback, a core method also used in this paper for text-to-video model alignment."}, {"fullname_first_author": "Kevin Black", "paper_title": "Training diffusion models with reinforcement learning", "publication_date": "2023-05-13", "reason": "This paper details training diffusion models, the basis of many text-to-video models, using reinforcement learning, a key technique also utilized here."}, {"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-00-00", "reason": "This is a highly influential paper introducing a method for learning visual models from natural language, fundamental to the approach in this text-to-video work."}, {"fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-00-00", "reason": "This paper is highly relevant due to its focus on training language models using human feedback, a core technique applied in this paper's reward model training."}, {"fullname_first_author": "Ji Lin", "paper_title": "VILA: On pre-training for visual language models", "publication_date": "2024-00-00", "reason": "This paper introduces VILA, a visual-language model used in this paper's LIFT-CRITIC, a crucial component for learning the reward function."}]}