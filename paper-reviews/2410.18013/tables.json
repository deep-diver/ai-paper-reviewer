[{"figure_path": "2410.18013/tables/table_7_0.html", "caption": "Table 1: Quantitative Results on GenEval. RankDPO improves results on most categories, notably \"two objects\", \"counting\", and \"color attribution\" for SDXL and SD3-Medium.", "description": "Table 1 presents a quantitative comparison of different models' performance on the GenEval benchmark, highlighting the improvements achieved by RankDPO on several key categories.", "section": "4.1 Comparison Results"}, {"figure_path": "2410.18013/tables/table_7_1.html", "caption": "Table 2: Quantitative Results on T2I-CompBench. RankDPO provides consistent improvements on all categories for both SDXL and SD3-Medium.", "description": "Table 2 presents a quantitative comparison of the performance of SDXL and SD3-Medium models, before and after applying RankDPO, across various attributes on the T2I-CompBench benchmark.", "section": "4.1 Comparison Results"}, {"figure_path": "2410.18013/tables/table_8_0.html", "caption": "Table 3: Quantitative results on DPG-Bench. DSG (Cho et al., 2024) and VQAScore (Lin et al., 2024) measure prompt following using VQA models while Q-Align (Wu et al., 2024a) measures visual quality using multimodal LLMs.", "description": "Table 3 presents a quantitative comparison of different models' performance on the DPG-Bench benchmark, evaluating both prompt alignment and visual quality using various metrics.", "section": "4.1 Comparison Results"}, {"figure_path": "2410.18013/tables/table_8_1.html", "caption": "Table 4: Effect of the preference labelling and data quality on the final model.", "description": "Table 4 shows the impact of different preference labeling methods and data quality on the final model's performance, measured by prompt alignment and visual quality.", "section": "4.2 ABLATION ANALYSIS"}, {"figure_path": "2410.18013/tables/table_8_2.html", "caption": "Table 3: Quantitative results on DPG-Bench. DSG (Cho et al., 2024) and VQAScore (Lin et al., 2024) measure prompt following using VQA models while Q-Align (Wu et al., 2024a) measures visual quality using multimodal LLMs.", "description": "Table 3 presents a quantitative comparison of different models' performance on the DPG-Bench benchmark, evaluating both prompt alignment and visual quality.", "section": "4.1 Comparison Results"}, {"figure_path": "2410.18013/tables/table_9_0.html", "caption": "Table 1: Quantitative Results on GenEval. RankDPO improves results on most categories, notably \"two objects\", \"counting\", and \"color attribution\" for SDXL and SD3-Medium.", "description": "Table 1 presents a quantitative comparison of different models' performance on the GenEval benchmark, highlighting the improvements achieved by RankDPO, especially in categories involving multiple objects, counting, and color attribution.", "section": "4.1 Comparison Results"}, {"figure_path": "2410.18013/tables/table_11_0.html", "caption": "Table 1: Quantitative Results on GenEval. RankDPO improves results on most categories, notably \"two objects\", \"counting\", and \"color attribution\" for SDXL and SD3-Medium.", "description": "Table 1 presents a quantitative comparison of different models' performance on the GenEval benchmark, highlighting the improvements achieved by the proposed RankDPO method.", "section": "4.1 Comparison Results"}, {"figure_path": "2410.18013/tables/table_15_0.html", "caption": "Table 6: Comparison of T2I-Compbench Dataset with DPG-Bench, including model attributes, training time, and inference time increases.", "description": "Table 6 compares the performance of different models on T2I-Compbench and DPG-bench datasets, showing training time, training data size, and inference time.", "section": "4.1 Comparison Results"}, {"figure_path": "2410.18013/tables/table_15_1.html", "caption": "Table 7: Comparing features of our proposal against baselines that aim to improve T2I model quality post-training. ELLA* also replaces the CLIP text-encoders with T5-XL text-encoder and a 470M parameter adapter applied at each timestep, thereby increasing the inference cost.", "description": "Table 7 compares the proposed method against other baselines in terms of training images, GPU days, inference cost, and DPG-Bench score, highlighting its efficiency and effectiveness.", "section": "4.1 Comparison Results"}, {"figure_path": "2410.18013/tables/table_16_0.html", "caption": "Table 1: Quantitative Results on GenEval. RankDPO improves results on most categories, notably \"two objects\", \"counting\", and \"color attribution\" for SDXL and SD3-Medium.", "description": "Table 1 presents a quantitative comparison of the performance of different models on the GenEval benchmark, highlighting the improvements achieved by RankDPO on several key categories.", "section": "4.1 Comparison Results"}, {"figure_path": "2410.18013/tables/table_18_0.html", "caption": "Table 1: Quantitative Results on GenEval. RankDPO improves results on most categories, notably \"two objects\", \"counting\", and \"color attribution\" for SDXL and SD3-Medium.", "description": "Table 1 presents a quantitative comparison of the performance of different models on the GenEval benchmark, highlighting the improvements achieved by RankDPO in various categories.", "section": "4.1 Comparison Results"}]