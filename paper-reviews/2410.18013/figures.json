[{"figure_path": "2410.18013/figures/figures_5_0.png", "caption": "Figure 2: Overview of our two novel components: (A) Syn-Pic and (B) RankDPO. Left illustrates the pipeline to generate a synthetically ranked preference dataset. It starts by collecting prompts and generating images using the same prompt for different T2I models. Next, we calculate the overall preference score using Reward models (e.g., PickScore, ImageReward). Finally, we rank these images in the decreasing order of preference scores. Right: Given true preference rankings for generated images per prompt, we first obtain predicted ranking by current model checkpoint using scores si (see Eq. 5). In this instance, although the predicted ranking is inverse of the true rankings, the ranks (1, 4) obtains a larger penalty than the ranks (2, 3). This penalty is added to our ranking loss through DCG weights (see Eq. 6). Thus, by optimizing \u03b8 with Ranking Loss (see Eq. 7), the updated model addresses the incorrect rankings (1,4). This procedure is repeated over the training process, where the rankings induced by the model aligns with the labelled preferences.", "description": "The figure shows a schematic overview of the two main components of the proposed method: (A) a synthetically labeled preference dataset generation process (Syn-Pic) and (B) a ranking-based preference optimization method (RankDPO).", "section": "3 Method"}, {"figure_path": "2410.18013/figures/figures_9_0.png", "caption": "Figure 4: Comparison among different preference optimization methods and RankDPO for SDXL. The results illustrate that we generate images with better prompt alignment and aesthetic quality.", "description": "The figure shows a qualitative comparison of images generated by different preference optimization methods for the same set of prompts, highlighting the superior prompt following and image quality achieved by the proposed RankDPO method.", "section": "4 Experiments"}, {"figure_path": "2410.18013/figures/figures_17_0.png", "caption": "Figure 1: Our approach, trained on a synthetic preference dataset with a ranking objective in the preference optimization, improves prompt following and visual quality for SDXL (Podell et al., 2023) and SD3-Medium (Esser et al., 2024), without requiring any manual annotations.", "description": "The figure shows a qualitative comparison of image generation results from different models (SDXL and SD3) with and without the proposed approach, highlighting improvements in prompt following and visual quality.", "section": "Introduction"}, {"figure_path": "2410.18013/figures/figures_19_0.png", "caption": "Figure 1: Our approach, trained on a synthetic preference dataset with a ranking objective in the preference optimization, improves prompt following and visual quality for SDXL (Podell et al., 2023) and SD3-Medium (Esser et al., 2024), without requiring any manual annotations.", "description": "The figure shows a comparison of text-to-image generation results using different models (SDXL and SD3) with and without the proposed method, illustrating improved prompt following and visual quality.", "section": "Introduction"}, {"figure_path": "2410.18013/figures/figures_19_1.png", "caption": "Figure 1: Our approach, trained on a synthetic preference dataset with a ranking objective in the preference optimization, improves prompt following and visual quality for SDXL (Podell et al., 2023) and SD3-Medium (Esser et al., 2024), without requiring any manual annotations.", "description": "The figure shows image generation results from SDXL and SD3-Medium models before and after applying the proposed approach, highlighting improved prompt following and visual quality.", "section": "Introduction"}]