[{"figure_path": "2410.18013/figures/figures_5_0.png", "caption": "Figure 2: Overview of our two novel components: (A) Syn-Pic and (B) RankDPO. Left illustrates the pipeline to generate a synthetically ranked preference dataset. It starts by collecting prompts and generating images using the same prompt for different T2I models. Next, we calculate the overall preference score using Reward models (e.g., PickScore, ImageReward). Finally, we rank these images in the decreasing order of preference scores. Right: Given true preference rankings for generated images per prompt, we first obtain predicted ranking by current model checkpoint using scores si (see Eq. 5). In this instance, although the predicted ranking is inverse of the true rankings, the ranks (1, 4) obtains a larger penalty than the ranks (2, 3). This penalty is added to our ranking loss through DCG weights (see Eq. 6). Thus, by optimizing 0 with Ranking Loss (see Eq. 7), the updated model addresses the incorrect rankings (1,4). This procedure is repeated over the training process, where the rankings induced by the model aligns with the labelled preferences.", "description": "This figure illustrates the pipeline of generating a synthetically ranked preference dataset (Syn-Pic) and the ranking-based preference optimization (RankDPO) method.", "section": "3 METHOD"}, {"figure_path": "2410.18013/figures/figures_9_0.png", "caption": "Figure 4: Comparison among different preference optimization methods and RankDPO for SDXL. The results illustrate that we generate images with better prompt alignment and aesthetic quality.", "description": "Figure 4 shows a qualitative comparison of images generated by different preference optimization methods for SDXL, highlighting improved prompt alignment and aesthetic quality with RankDPO.", "section": "4 Experiments"}, {"figure_path": "2410.18013/figures/figures_17_0.png", "caption": "Figure 1: Our approach, trained on a synthetic preference dataset with a ranking objective in the preference optimization, improves prompt following and visual quality for SDXL (Podell et al., 2023) and SD3-Medium (Esser et al., 2024), without requiring any manual annotations.", "description": "The figure shows a qualitative comparison of text-to-image generation results from different models (SDXL and SD3) before and after applying the proposed ranked preference optimization method.", "section": "Introduction"}, {"figure_path": "2410.18013/figures/figures_19_0.png", "caption": "Figure 1: Our approach, trained on a synthetic preference dataset with a ranking objective in the preference optimization, improves prompt following and visual quality for SDXL (Podell et al., 2023) and SD3-Medium (Esser et al., 2024), without requiring any manual annotations.", "description": "The figure shows a qualitative comparison of text-to-image generation results using different methods (SDXL, SD3, and the proposed approach) for various prompts, highlighting improved prompt following and visual quality with the proposed method.", "section": "Introduction"}, {"figure_path": "2410.18013/figures/figures_19_1.png", "caption": "Figure 1: Our approach, trained on a synthetic preference dataset with a ranking objective in the preference optimization, improves prompt following and visual quality for SDXL (Podell et al., 2023) and SD3-Medium (Esser et al., 2024), without requiring any manual annotations.", "description": "The figure shows image generation results from SDXL and SD3-Medium models before and after applying the proposed method, demonstrating improved prompt following and visual quality.", "section": "1 Introduction"}]