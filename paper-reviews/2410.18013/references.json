{"references": [{" publication_date": "2023", "fullname_first_author": "Yuval Kirstain", "paper_title": "Pick-a-pic: An open dataset of user preferences for text-to-image generation", "reason": "This paper is crucial because it provides the preference dataset used for training and evaluation of the proposed method.  The dataset is a key element of the work, as it enables the training of the model and provides ground truth for evaluation. The cost comparison between the traditional method of collecting human-annotated datasets and the method presented in this paper is also discussed in the introduction.  Therefore, understanding this dataset is essential for fully grasping the paper's overall contribution.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Rafael Rafailov", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "reason": "This paper introduces Direct Preference Optimization (DPO), a core method in the paper being analyzed.  It fundamentally underpins the approach of using preferences directly for model optimization rather than relying on intermediate reward models, which is a central theme. The paper's theoretical foundations are highly relevant to the current work and inform its methodology.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Bram Wallace", "paper_title": "Diffusion model alignment using direct preference optimization", "reason": "This work directly addresses the challenges of aligning diffusion models using preference optimization. It proposes a tractable alternative to the standard DPO objective, directly applicable to diffusion models. The comparative results in this paper are important for context and demonstrate existing approaches, to which the proposed solution can be compared and contrasted.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Jaemin Cho", "paper_title": "Davidsonian scene graph: Improving reliability in fine-grained evaluation for text-image generation", "reason": "This paper introduces a framework for evaluating text-to-image models, which is relevant to the evaluation part of the current work. The proposed method enhances evaluation through a more fine-grained analysis that focuses on the specific aspects of the generated images, such as object detection, color attribution, etc.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Kaiyi Huang", "paper_title": "T2i-compbench: A comprehensive benchmark for open-world compositional text-to-image generation", "reason": "This paper provides a comprehensive benchmark dataset for evaluating text-to-image generation models. This is crucial for evaluating the proposed method\u2019s performance, which directly uses this benchmark as a metric.  The benchmark covers various aspects of the quality and alignment of generated images with given prompts.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Dhruba Ghosh", "paper_title": "Geneval: An object-focused framework for evaluating text-to-image alignment", "reason": "This paper describes another benchmark for evaluating text-to-image models.  The results on GenEval in the paper being reviewed are significant for showing improvement over existing methods, establishing the relative performance and benefits of the proposed approach.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Jiwoo Hong", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "reason": "This work is directly compared with in the experimental section of the paper being analyzed, showcasing the improvement in terms of prompt alignment achieved by the proposed method. It establishes a baseline against which the improvements of the proposed technique can be measured.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Christopher Burges", "paper_title": "Learning to rank with nonsmooth cost functions", "reason": "This paper is foundational to the proposed RankDPO algorithm which leverages ideas from the learning-to-rank literature. The work's methodology for ranking-based optimization informs the development of the proposed RankDPO method, providing theoretical backing and justification for its design.", "section_number": 3}, {" publication_date": "2018", "fullname_first_author": "Xuanhui Wang", "paper_title": "The lambdaloss framework for ranking metric optimization", "reason": "This paper provides an alternative loss function, which is used to improve performance in RankDPO.  LambdaLoss is adopted because it handles the ranking loss effectively. The use of LambdaLoss is crucial for the overall design of the RankDPO algorithm, impacting its ability to handle ranked data effectively.", "section_number": 3}, {" publication_date": "1952", "fullname_first_author": "Ralph Allan Bradley", "paper_title": "Rank analysis of incomplete block designs: I. the method of paired comparisons", "reason": "This paper introduces the Bradley-Terry model, a foundational concept used in the formulation of the DPO objective function.  The Bradley-Terry model is a key component of DPO which underpins this paper's optimization strategy.  The current paper builds upon this foundation and adapts it to handle ranked preferences instead of just pairwise comparisons.", "section_number": 3}, {" publication_date": "2022", "fullname_first_author": "Robin Rombach", "paper_title": "High-resolution image synthesis with latent diffusion models", "reason": "This paper introduces Stable Diffusion, a key model used in this work.  As a foundational model for text-to-image generation, understanding its architecture and capabilities is essential for understanding the context and impact of the proposed improvements. It also contextualizes the dataset creation process, where images are generated from various models.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Patrick Esser", "paper_title": "Scaling rectified flow transformers for high-resolution image synthesis", "reason": "This paper introduces the SD3-Medium model, another key model used for experiments and comparisons.  Understanding this model's strengths and weaknesses is important in the context of the evaluation, as it is compared against SDXL and serves as another baseline for assessing the proposed method.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Dustin Podell", "paper_title": "SDXL: Improving latent diffusion models for high-resolution image synthesis", "reason": "This paper introduces the SDXL model, a key model used in this work. As a foundational model for text-to-image generation, understanding its architecture and capabilities is essential for understanding the context and impact of the proposed improvements.  It also contextualizes the dataset creation process where images are generated from various models.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Junsong Chen", "paper_title": "Pixart-sigma: Weak-to-strong training of diffusion transformer for 4k text-to-image generation", "reason": "This paper introduces Pixart-\u03a3, one of the models used in this paper for generating the initial images for comparison. The generation of the synthetic dataset relies on producing outputs from multiple models, and PixArt-\u03a3 is a key model whose outputs are part of the dataset used for preference optimization.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Paul F Christiano", "paper_title": "Deep reinforcement learning from human preferences", "reason": "This paper discusses Reinforcement Learning from Human Feedback (RLHF), a key technique that is directly related to DPO, which is used in this paper. Understanding the techniques and challenges of RLHF provides the essential background for appreciating the contributions of the proposed methodology in using DPO with synthetic data.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Xiaoshi Wu", "paper_title": "Human preference score v2: A solid benchmark for evaluating human preferences of text-to-image generation", "reason": "This work contributes to the evaluation and benchmarking aspect of the study.  The results of the current work on HPSv2.1 provide further context and support the effectiveness of the approach by demonstrating that using multiple reward models increases the reliability and quality of the synthetic dataset.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Jiwoo Hong", "paper_title": "Training a helpful and harmless assistant with reinforcement learning from human feedback", "reason": "This paper discusses reinforcement learning from human feedback, which provides the background for the alignment aspect of the proposed method.  The direct connection between RLHF and DPO highlights the importance of understanding the alignment aspects of the problem that are solved by both RLHF and DPO in their respective contexts.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Jiwoo Hong", "paper_title": "Aligning text-to-image models using human feedback", "reason": "This paper provides insights into the process of aligning text-to-image models using human feedback, a key aspect of the research. The paper\u2019s contribution is relevant to the goals of this work and allows for comparison and evaluation of the techniques used.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Haoning Wu", "paper_title": "Q-align: Teaching Imms for visual scoring via discrete text-defined levels", "reason": "This paper is relevant because it is used as a benchmark to evaluate the visual quality aspect of the proposed method in the experimental section of the current paper. The use of Q-Align as a metric emphasizes the importance of visual quality as a key factor in assessing the effectiveness of the proposed preference optimization strategy.", "section_number": 4}]}