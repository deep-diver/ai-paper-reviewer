{"references": [{" publication_date": "2023", "fullname_first_author": "Yuval Kirstain", "paper_title": "Pick-a-Pic: An open dataset of user preferences for text-to-image generation", "reason": "This paper is highly relevant because it is cited as a costly dataset used for training preference optimization models for text-to-image generation.  The authors' work directly addresses the high cost and rapid obsolescence of human-annotated preference datasets by proposing a synthetically labeled preference dataset, enabling direct comparison with and improvement over a well-known and expensive dataset. The proposed dataset is a key part of their novel approach to scalable preference optimization.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Christoph Schuhmann", "paper_title": "LAION-5B: An open large-scale dataset for training next generation image-text models", "reason": "This paper is crucial because it represents one of the largest open-source image-text datasets available for training large models. The scale and availability of LAION-5B provide a powerful benchmark for evaluating the cost-effectiveness and generalizability of the authors' methods. The use of synthetic data for training is directly compared to this large real-world dataset.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Robin Rombach", "paper_title": "High-resolution image synthesis with latent diffusion models", "reason": "This paper is highly relevant to the work due to its introduction of high-resolution image synthesis using latent diffusion models. This provides context for the authors' efforts to improve upon high-resolution image generation using DPO (Direct Preference Optimization). The improvements that the proposed method achieves in visual quality are measured against this highly influential model.", "section_number": 1}, {" publication_date": "2017", "fullname_first_author": "Paul F Christiano", "paper_title": "Deep reinforcement learning from human preferences", "reason": "This is a foundational paper that introduced deep reinforcement learning (RLHF) from human preferences.  The paper is foundational because it demonstrates how to align complex models with human preferences. The authors' method is directly compared with and improves upon existing methods grounded in RLHF.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Rafael Rafailov", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "reason": "This paper introduced the Direct Preference Optimization (DPO) method, a key concept for the authors' work. The use of DPO serves as a core methodological basis for the proposed approach, and therefore understanding the original work is essential to evaluating the contributions made by the proposed method.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Bram Wallace", "paper_title": "Diffusion model alignment using direct preference optimization", "reason": "This paper is significant as it proposes a similar approach, applying direct preference optimization to diffusion models.  However, it does not address the scalability concerns of the human-annotated data, which the authors' synthetic approach aims to resolve. Hence, this paper provides a direct comparison for the novelty of the presented approach.", "section_number": 2}, {" publication_date": "1952", "fullname_first_author": "Ralph Allan Bradley", "paper_title": "Rank analysis of incomplete block designs: I. the method of paired comparisons", "reason": "This paper is a seminal work in preference learning which introduces the Bradley-Terry model. The proposed method is built upon the Bradley-Terry model, which is a fundamental model for pairwise preference ranking.  Understanding this foundational work provides context for the authors' modifications and advancements.", "section_number": 3}, {" publication_date": "2018", "fullname_first_author": "Xuanhui Wang", "paper_title": "The lambdaloss framework for ranking metric optimization", "reason": "This paper is significant because it introduces the LambdaLoss function, which is adapted and used in the RankDPO objective. This provides a strong methodological basis for the authors' ranking-based preference optimization algorithm, showing a connection to existing work on learning to rank.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "James Betker", "paper_title": "Improving image generation with better captions. OpenAI Technical Report", "reason": "This paper demonstrates improved image generation using better captions, providing relevant background on the capabilities of current text-to-image models. This sets a benchmark against which the authors' approach is compared, as the improvements in visual quality and prompt following are demonstrated against this model's state-of-the-art performance.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Dustin Podell", "paper_title": "SDXL: Improving latent diffusion models for high-resolution image synthesis", "reason": "This paper is a direct benchmark against which the authors' work is compared because it introduces SDXL, a state-of-the-art text-to-image generation model. The authors test their proposed approach on SDXL, demonstrating its ability to enhance an already powerful model. This shows the effectiveness of their approach even when applied to existing high-performing models.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Patrick Esser", "paper_title": "Scaling rectified flow transformers for high-resolution image synthesis", "reason": "This paper is another significant benchmark as it introduces an alternative high-resolution text-to-image model that the authors evaluate.  The method's performance on the SD3-Medium model is used to test the generalizability of their findings beyond the SDXL model. The improvements obtained are validated against an existing method, which shows the benefit of using the approach across multiple models.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Jaemin Cho", "paper_title": "Davidsonian scene graph: Improving reliability in fine-grained evaluation for text-image generation", "reason": "This paper introduces a more reliable method for evaluating text-to-image generation. Since the authors evaluate their results using the Davidsonian Scene Graph framework, this provides a key methodological context. The improvement in scores obtained demonstrates an advantage using the proposed method, highlighting an improvement in alignment with this advanced evaluation approach.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Kaiyi Huang", "paper_title": "T2I-Compbench: A comprehensive benchmark for open-world compositional text-to-image generation", "reason": "This paper is a crucial benchmark dataset that is used for evaluation in the paper. T2I-Compbench provides a standard for evaluating compositional capabilities of text-to-image models. The results obtained on this benchmark demonstrate the efficacy of the authors' methods in enhancing compositional generation capabilities. The strong performance gains on this benchmark are key to the paper's conclusions.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Dhruba Ghosh", "paper_title": "Geneval: An object-focused framework for evaluating text-to-image alignment", "reason": "This paper presents GenEval, a benchmark dataset the authors use to evaluate their model. GenEval is specifically designed for evaluating object-centric aspects of text-to-image alignment, complementing other benchmarks focused on broader aspects of image quality. The results of the experiment on GenEval demonstrate the efficacy of the method in improving object-focused aspects of image generation.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Haoning Wu", "paper_title": "Q-Align: Teaching LLMs for visual scoring via discrete text-defined levels", "reason": "This paper introduces Q-Align, a method that is used for assessing the visual quality of the images generated. Since visual quality is one of the key aspects of text-to-image generation, this paper provides a critical evaluation metric. The improved visual quality achieved using the proposed method is validated against the Q-Align scores.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Jiwoo Hong", "paper_title": "Aligning text-to-image models using human feedback", "reason": "This paper is closely related because it explores methods for aligning text-to-image models using human feedback, similar to the authors' goal.  While this work also uses human feedback, the authors' approach using synthetic data provides a significantly cheaper and scalable alternative for achieving similar performance improvements.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Jiwoo Hong", "paper_title": "Margin-aware preference optimization for aligning diffusion models without reference", "reason": "This work is closely related to the proposed work as it also focuses on preference optimization for diffusion models, showcasing existing techniques in the domain. While this method achieves strong results, it lacks the scalability and cost-effectiveness offered by the synthetically labeled preference dataset approach presented by the authors.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Ying Fan", "paper_title": "Reinforcement learning for fine-tuning text-to-image diffusion models", "reason": "This paper is relevant because it discusses a common approach to improving text-to-image generation that utilizes reinforcement learning.  The paper highlights the limitations of reward-based methods, such as \"reward hacking,\" that the authors' work aims to address. By presenting an alternative, more robust, and scalable method, this paper contributes significantly to the field.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Fei Deng", "paper_title": "PRDP: Proximal reward difference prediction for large-scale reward finetuning of diffusion models", "reason": "This paper is highly related because it also deals with optimizing diffusion models using rewards but does not directly address the issue of high human labeling costs. This paper is used as a comparison against the authors' proposed method, which directly targets the scalability issue by eliminating the need for human annotation.", "section_number": 4}]}