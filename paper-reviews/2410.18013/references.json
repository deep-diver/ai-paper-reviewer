{"references": [{" publication_date": "2023", "fullname_first_author": "Yuval Kirstain", "paper_title": "Pick-a-Pic: An open dataset of user preferences for text-to-image generation", "reason": "This paper is highly relevant because it introduces the Pick-a-Pic dataset, which is frequently referenced in the paper as a benchmark for existing methods.  The paper's discussion of the dataset's cost (nearly $50,000) directly supports the paper's motivation to develop a cost-effective alternative for preference data collection.  The detailed description of the dataset helps in understanding the scale and nature of preference datasets typically used in this field, setting the context for the proposed Syn-Pic dataset.", "section_number": 1}, {" publication_date": "2017", "fullname_first_author": "Paul F Christiano", "paper_title": "Deep reinforcement learning from human preferences", "reason": "This paper is foundational to the field of preference learning, particularly reinforcement learning from human feedback (RLHF), a key concept related to the paper's approach.  The methods described in this paper are closely related to the core idea of aligning models using human feedback, even though the application domain is different (LLMs vs T2I models). The core methodology helps to establish a crucial background for the paper's approach.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Rafael Rafailov", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "reason": "This paper introduces Direct Preference Optimization (DPO), a core technique that the proposed method builds upon.  The paper is crucial because it lays the theoretical groundwork for using preferences directly in optimization, instead of relying on reward models. The paper's insights directly influence the proposed RankDPO method.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Bram Wallace", "paper_title": "Diffusion model alignment using direct preference optimization", "reason": "This is a highly relevant paper because it applies DPO techniques to image generation, which is very close to the problem being addressed in the current paper.  Their application of DPO to diffusion models is directly relevant and provides a reference point to compare the paper's improvements.  This paper also discusses the drawbacks of using human-generated preference data, reinforcing the paper's argument for a synthetic approach.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Robin Rombach", "paper_title": "High-resolution image synthesis with latent diffusion models", "reason": "This paper introduces Stable Diffusion, a foundational text-to-image model used as a basis in the experiments section.  The paper's contributions to generating high-quality images are directly relevant because the paper builds on and extends this technology by improving the alignment of generated images with human preferences.  Understanding the model's architecture and capabilities is fundamental to interpreting the results.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Patrick Esser", "paper_title": "Scaling rectified flow transformers for high-resolution image synthesis", "reason": "This paper introduces SD3, another foundational model used in the experiments.  As with Stable Diffusion, it provides a relevant baseline against which the paper compares its results.  SD3's architectural choices and resulting image quality are both contextually relevant to the paper's investigation into improving model alignment.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Dustin Podell", "paper_title": "SDXL: Improving latent diffusion models for high-resolution image synthesis", "reason": "This paper introduces SDXL, a key model used extensively in the experimental evaluation section. Understanding SDXL's capabilities and limitations is crucial to interpreting the paper's results.  The paper highlights the performance gains achieved using RankDPO on SDXL compared to the baseline model, demonstrating the efficacy of the proposed method on a state-of-the-art model.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Jaemin Cho", "paper_title": "Davidsonian scene graph: Improving reliability in fine-grained evaluation for text-image generation", "reason": "This paper is highly relevant because it introduces a scene graph based evaluation method (Davidsonian Scene Graph) utilized in the evaluation of the proposed method on the DPG-Bench dataset. The understanding of this benchmark is crucial for interpreting the paper's results regarding prompt alignment and visual quality.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Kaiyi Huang", "paper_title": "T2I-Compbench: A comprehensive benchmark for open-world compositional text-to-image generation", "reason": "This paper introduces the T2I-Compbench dataset that the proposed method is evaluated on, establishing a strong context for understanding the experimental results.  The specific metrics used to assess performance on this benchmark are crucial for evaluating the effectiveness of the proposed model and directly support the claims and conclusions made in the paper.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Haoning Wu", "paper_title": "Q-Align: Teaching LLMs for visual scoring via discrete text-defined levels", "reason": "This paper is relevant as it introduces the Q-Align metric, a key metric used for evaluating the visual quality of generated images in the experiments.  The use of Q-Align helps to provide a more nuanced evaluation of the generated images by considering both text alignment and aesthetic quality, thereby providing a more comprehensive assessment of the proposed method.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Jiwoo Hong", "paper_title": "Training a helpful and harmless assistant with reinforcement learning from human feedback", "reason": "This paper is relevant as it focuses on aligning language models, providing a method for collecting preferences and training a reward model. This provides context and shows that similar approaches can also be successful, thus improving trust in the proposed methodology.", "section_number": 2}, {" publication_date": "2018", "fullname_first_author": "Xuanhui Wang", "paper_title": "The lambdaloss framework for ranking metric optimization", "reason": "This paper is highly relevant to the proposed method because it introduces LambdaLoss, a loss function used in learning to rank.  The choice of LambdaLoss and its properties are explicitly discussed in the method section and the paper's contribution to ranking optimization methods directly contributes to the paper's methodology.", "section_number": 3}, {" publication_date": "1952", "fullname_first_author": "Ralph Allan Bradley", "paper_title": "Rank analysis of incomplete block designs: I. the method of paired comparisons", "reason": "This paper is foundational as it introduces the Bradley-Terry model, a fundamental concept in pairwise preference learning which is closely related to the paper's proposed approach.  The use of the Bradley-Terry model and its properties in the context of pairwise preference learning forms a basis for understanding the proposed method's modifications and extensions.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Dongzhi Jiang", "paper_title": "Comat: Aligning text-to-image diffusion model with image-to-text concept matching", "reason": "This is a relevant paper because it also addresses the problem of aligning text-to-image models with human preferences, although using a different technique.  Comparing and contrasting the two methods, particularly their strengths and limitations, allows for a broader evaluation and understanding of the proposed approach within the existing literature.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "Jonathan Ho", "paper_title": "Denoising diffusion probabilistic models", "reason": "This is a foundational paper in diffusion models, providing a theoretical framework for understanding how diffusion models work. Since the proposed work leverages diffusion models for image generation, understanding the underlying principles is crucial for assessing the improvements made by the proposed method.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Junsong Chen", "paper_title": "Pixart-sigma: Weak-to-strong training of diffusion transformer for 4k text-to-image generation", "reason": "This paper is relevant because it provides another state-of-the-art text-to-image model, Pixart-\u03a3, which is used as one of the model for preference dataset generation in this paper.  The architecture and quality of images generated by Pixart-\u03a3 are factors that affect the resulting preference dataset, thus impacting the final performance of the proposed method.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Fei Deng", "paper_title": "PRDP: Proximal reward difference prediction for large-scale reward finetuning of diffusion models", "reason": "This paper is relevant because it introduces a method for large-scale reward finetuning in diffusion models, which is computationally expensive, and the current paper aims to avoid this problem by introducing a more cost-effective solution. The comparison of the two methods demonstrates that the proposed method is superior in terms of cost and computational resources while still showing improved performance.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Ying Fan", "paper_title": "Reinforcement learning for fine-tuning text-to-image diffusion models", "reason": "This paper provides another relevant method using reinforcement learning for fine-tuning text-to-image diffusion models. The paper demonstrates that reinforcement learning is used as a means to improve alignment.  This method also suffers from the same drawbacks of computational expense which motivates the current paper to explore an alternative approach.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Kevin Black", "paper_title": "Training diffusion models with reinforcement learning", "reason": "This paper introduces a method for training diffusion models with reinforcement learning, which directly relates to the paper's focus on aligning models with human feedback. The core concept of using reinforcement learning to improve model alignment is relevant, though the computational cost associated with this approach is a significant drawback compared to the paper's proposed method.", "section_number": 2}]}