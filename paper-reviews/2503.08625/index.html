<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><title>SegAgent: Exploring Pixel Understanding Capabilities in MLLMs by Imitating Human Annotator Trajectories &#183; HF Daily Paper Reviews by AI</title>
<meta name=title content="SegAgent: Exploring Pixel Understanding Capabilities in MLLMs by Imitating Human Annotator Trajectories &#183; HF Daily Paper Reviews by AI"><meta name=description content="SegAgent: Improves MLLMs' pixel understanding by mimicking human annotation, enabling mask refinement without altering output space."><meta name=keywords content="Multimodal Learning,Vision-Language Models,üè¢ Zhejiang University,China,"><link rel=canonical href=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.08625/><link type=text/css rel=stylesheet href=/ai-paper-reviewer/css/main.bundle.min.595affd4445a931ea6d6e3a5a3c709930fa52a60be10b21c6f81fdb8fecaacea33aacedf80cdc88be45f189be14ed4ce53ea74a1e1406fad9cbf90c5ed409173.css integrity="sha512-WVr/1ERakx6m1uOlo8cJkw+lKmC+ELIcb4H9uP7KrOozqs7fgM3Ii+RfGJvhTtTOU+p0oeFAb62cv5DF7UCRcw=="><script type=text/javascript src=/ai-paper-reviewer/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script><script defer type=text/javascript id=script-bundle src=/ai-paper-reviewer/js/main.bundle.min.efbf3b6b987689fffaf2d7b73173d2690c0279a04d444b0537a77d7f4ff6e6d493445400cb0cf56bc0f0f123e19f15394e63cae34e67f069bd013dd5c73df56e.js integrity="sha512-7787a5h2if/68te3MXPSaQwCeaBNREsFN6d9f0/25tSTRFQAywz1a8Dw8SPhnxU5TmPK405n8Gm9AT3Vxz31bg==" data-copy data-copied></script><script src=/ai-paper-reviewer/lib/zoom/zoom.min.37d2094687372da3f7343a221a470f6b8806f7891aa46a5a03966af7f0ebd38b9fe536cb154e6ad28f006d184b294525a7c4054b6bbb4be62d8b453b42db99bd.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ=="></script><link rel=apple-touch-icon sizes=180x180 href=/ai-paper-reviewer/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/ai-paper-reviewer/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/ai-paper-reviewer/favicon-16x16.png><link rel=manifest href=/ai-paper-reviewer/site.webmanifest><meta property="og:url" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.08625/"><meta property="og:site_name" content="HF Daily Paper Reviews by AI"><meta property="og:title" content="SegAgent: Exploring Pixel Understanding Capabilities in MLLMs by Imitating Human Annotator Trajectories"><meta property="og:description" content="SegAgent: Improves MLLMs‚Äô pixel understanding by mimicking human annotation, enabling mask refinement without altering output space."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="paper-reviews"><meta property="article:published_time" content="2025-03-11T00:00:00+00:00"><meta property="article:modified_time" content="2025-03-11T00:00:00+00:00"><meta property="article:tag" content="Multimodal Learning"><meta property="article:tag" content="Vision-Language Models"><meta property="article:tag" content="üè¢ Zhejiang University, China"><meta property="og:image" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.08625/cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.08625/cover.png"><meta name=twitter:title content="SegAgent: Exploring Pixel Understanding Capabilities in MLLMs by Imitating Human Annotator Trajectories"><meta name=twitter:description content="SegAgent: Improves MLLMs‚Äô pixel understanding by mimicking human annotation, enabling mask refinement without altering output space."><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"Paper Reviews by AI","name":"SegAgent: Exploring Pixel Understanding Capabilities in MLLMs by Imitating Human Annotator Trajectories","headline":"SegAgent: Exploring Pixel Understanding Capabilities in MLLMs by Imitating Human Annotator Trajectories","abstract":"SegAgent: Improves MLLMs\u0026rsquo; pixel understanding by mimicking human annotation, enabling mask refinement without altering output space.","inLanguage":"en","url":"https:\/\/deep-diver.github.io\/ai-paper-reviewer\/paper-reviews\/2503.08625\/","author":{"@type":"Person","name":"Hugging Face Daily Papers"},"copyrightYear":"2025","dateCreated":"2025-03-11T00:00:00\u002b00:00","datePublished":"2025-03-11T00:00:00\u002b00:00","dateModified":"2025-03-11T00:00:00\u002b00:00","keywords":["Multimodal Learning","Vision-Language Models","üè¢ Zhejiang University, China"],"mainEntityOfPage":"true","wordCount":"2632"}]</script><meta name=author content="Hugging Face Daily Papers"><link href=https://github.com/deep-diver/paper-reviewer/ rel=me><link href=https://twitter.com/algo_diver/ rel=me><script src=/ai-paper-reviewer/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj+KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script><script defer src=/ai-paper-reviewer/lib/typeit/typeit.umd.1b3200cb448f5cd1f548f2781452643d3511a43584b377b82c03a58055da4fdb7bc8f6c6c2ce846480c7677ff25bfd0d75f15823c09443ab18e0fd2cad792587.js integrity="sha512-GzIAy0SPXNH1SPJ4FFJkPTURpDWEs3e4LAOlgFXaT9t7yPbGws6EZIDHZ3/yW/0NdfFYI8CUQ6sY4P0srXklhw=="></script><script defer src=/ai-paper-reviewer/lib/packery/packery.pkgd.min.js integrity></script><script type=text/javascript src=/ai-paper-reviewer/js/shortcodes/gallery.min.9b4cb28f931ed922c26fb9b2510c2debb370f6a63305050c2af81740b2919883715e24efbbdf3a081496718ec751df3a72729d4d0bc71d6071297563a97ce1ee.js integrity="sha512-m0yyj5Me2SLCb7myUQwt67Nw9qYzBQUMKvgXQLKRmINxXiTvu986CBSWcY7HUd86cnKdTQvHHWBxKXVjqXzh7g=="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-KX0S6Q55Y7"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-KX0S6Q55Y7")</script><meta name=theme-color><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-app.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-firestore.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-auth.js></script><script>const firebaseConfig={apiKey:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",authDomain:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",projectId:"neurips2024-f3065",storageBucket:"neurips2024-f3065.firebasestorage.app",messagingSenderId:"982475958898",appId:"1:982475958898:web:2147e5d7753d6ac091f0eb",measurementId:"G-YQ46HXQ9JS"};var app=firebase.initializeApp(firebaseConfig),db=firebase.firestore(),auth=firebase.auth()</script></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a></div><div class=min-h-[148px]></div><div class="fixed inset-x-0 pl-[24px] pr-[24px]" style=z-index:100><div id=menu-blur class="absolute opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl shadow-2xl"></div><div class="relative max-w-[64rem] ml-auto mr-auto"><div style=padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start space-x-3"><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/ai-paper-reviewer/ class="text-base font-medium text-gray-500 hover:text-gray-900">HF Daily Paper Reviews by AI</a></nav><nav class="hidden md:flex items-center space-x-5 md:ml-12 h-12"><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>About</p></a><a href=/ai-paper-reviewer/2025-03-24/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>2025-03-24</p></a><a href=/ai-paper-reviewer/2025-03-25/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>2025-03-25</p></a><a href=/ai-paper-reviewer/2025-03-26/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>2025-03-26</p></a><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Archive</p></a><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Tags</p></a><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></span><p class="text-base font-medium" title></p></a><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span><p class="text-base font-medium" title></p></a><button id=search-button aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button><div class="ltr:mr-14 rtl:ml-14 flex items-center"><button id=appearance-switcher aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></nav><div class="flex md:hidden items-center space-x-5 md:ml-12 h-12"><span></span>
<button id=search-button-mobile aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></button>
<button id=appearance-switcher-mobile aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400" style=margin-right:5px><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div><div class="-my-2 -mr-2 md:hidden"><label id=menu-button class=block><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper style=padding-top:5px class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li id=menu-close-button><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>About</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-03-24/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>2025-03-24</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-03-25/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>2025-03-25</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-03-26/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>2025-03-26</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Archive</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Tags</p></a></li><li class=mt-1><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li><li class=mt-1><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li></ul></div></label></div></div></div></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("menu-blur");n.style.opacity=t/300})</script><div class="relative flex flex-col grow"><main id=main-content class=grow><article><div id=hero class="h-[150px] md:h-[200px]"></div><div class="fixed inset-x-0 top-0 h-[800px] single_hero_background nozoom" style=background-image:url(/ai-paper-reviewer/paper-reviews/2503.08625/cover_hu16692001106218624102.png)><div class="absolute inset-0 bg-gradient-to-t from-neutral dark:from-neutral-800 to-transparent mix-blend-normal"></div><div class="absolute inset-0 opacity-60 bg-gradient-to-t from-neutral dark:from-neutral-800 to-neutral-100 dark:to-neutral-800 mix-blend-normal"></div></div><div id=background-blur class="fixed opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl"></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("background-blur");n.style.opacity=t/300})</script><header id=single_header class="mt-5 max-w-prose"><ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden"><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/>HF Daily Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class=inline><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/>Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/2503.08625/>SegAgent: Exploring Pixel Understanding Capabilities in MLLMs by Imitating Human Annotator Trajectories</a><span class="px-1 text-primary-500">/</span></li></ol><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">SegAgent: Exploring Pixel Understanding Capabilities in MLLMs by Imitating Human Annotator Trajectories</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime=2025-03-11T00:00:00+00:00>11 March 2025</time><span class="px-2 text-primary-500">&#183;</span><span>2632 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">13 mins</span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=views_paper-reviews/2503.08625/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=views>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 576 512"><path fill="currentcolor" d="M288 32c-80.8.0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7.0 24.6C17.3 304 48.6 356 95.4 399.4 142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1 3.3-7.9 3.3-16.7.0-24.6-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144 64.5-144 144-144 144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64-11.5.0-22.3-3-31.6-8.4-.2 2.8-.4 5.5-.4 8.4.0 53 43 96 96 96s96-43 96-96-43-96-96-96c-2.8.0-5.6.1-8.4.4 5.3 9.3 8.4 20.1 8.4 31.6z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=likes_paper-reviews/2503.08625/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=likes>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<button id=button_likes class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400" onclick=process_article()>
<span id=button_likes_heart style=display:none class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span><span id=button_likes_emtpty_heart class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M244 84l11.1 12 12-11.98C300.6 51.37 347 36.51 392.6 44.1 461.5 55.58 512 115.2 512 185.1V190.9c0 41.5-17.2 81.2-47.6 109.5L283.7 469.1c-7.5 7-17.4 10.9-27.7 10.9S235.8 476.1 228.3 469.1L47.59 300.4C17.23 272.1.0 232.4.0 190.9V185.1c0-69.9 50.52-129.52 119.4-141 44.7-7.59 92 7.27 124.6 39.9C243.1 84 244 84.01 244 84zm11.1 79.9-45-46.8c-21.7-20.82-52.5-30.7-82.8-25.66C81.55 99.07 48 138.7 48 185.1V190.9c0 28.2 11.71 55.2 32.34 74.4L256 429.3l175.7-164c20.6-19.2 32.3-46.2 32.3-74.4V185.1c0-46.4-33.6-86.03-79.3-93.66C354.4 86.4 323.6 96.28 301.9 117.1l-46.8 46.8z"/></svg>
</span></span><span id=button_likes_text>&nbsp;Like</span></button></span></div><div class="flex flex-row flex-wrap items-center"><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/ai-generated/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">AI Generated
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/-daily-papers/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">ü§ó Daily Papers
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/multimodal-learning/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Multimodal Learning
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/vision-language-models/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Vision-Language Models
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/-zhejiang-university-china/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">üè¢ Zhejiang University, China</span></span></span></div></div><div class="flex author"><img class="!mt-0 !mb-0 h-24 w-24 rounded-full ltr:mr-4 rtl:ml-4" width=96 height=96 alt="Hugging Face Daily Papers" src=/ai-paper-reviewer/img/avatar_hu1570846118988919414.png><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">Author</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">Hugging Face Daily Papers</div><div class="text-sm text-neutral-700 dark:text-neutral-400">I am AI, and I review papers on HF Daily Papers</div><div class="text-2xl sm:text-lg"><div class="flex flex-wrap text-neutral-400 dark:text-neutral-500"><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://github.com/deep-diver/paper-reviewer/ target=_blank aria-label=Github rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://twitter.com/algo_diver/ target=_blank aria-label=Twitter rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span></a></div></div></div></div><div class=mb-5></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first lg:ml-auto px-0 lg:order-last ltr:lg:pl-8 rtl:lg:pr-8"><div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-[140px]"><details open id=TOCView class="toc-right mt-0 overflow-y-scroll overscroll-contain scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600 rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 hidden lg:block"><summary class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="min-w-[220px] py-2 border-dotted ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#hlmat-a-new-task>HLMAT: A New Task</a></li><li><a href=#segagent-details>SegAgent Details</a></li><li><a href=#iterative-refinement>Iterative Refinement</a></li><li><a href=#hres-dataset>HRES Dataset</a></li><li><a href=#future-mllm-agent>Future MLLM Agent</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><details class="toc-inside mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 lg:hidden"><summary class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#hlmat-a-new-task>HLMAT: A New Task</a></li><li><a href=#segagent-details>SegAgent Details</a></li><li><a href=#iterative-refinement>Iterative Refinement</a></li><li><a href=#hres-dataset>HRES Dataset</a></li><li><a href=#future-mllm-agent>Future MLLM Agent</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><script>var margin=200,marginError=50;(function(){var t=$(window),e=$("#TOCView"),s=e.height();function n(){var n=t.height()-margin;s>=n?(e.css("overflow-y","scroll"),e.css("max-height",n+marginError+"px")):(e.css("overflow-y","hidden"),e.css("max-height","9999999px"))}t.on("resize",n),$(document).ready(n)})()</script></div></div><div class="min-w-0 min-h-0 max-w-fit"><div class="article-content max-w-prose mb-20"><br><div class="flex flex-row flex-wrap items-center space-x-2"><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 48 48" fill="none"><rect width="48" height="48" fill="#fff" fill-opacity=".01"/><path d="M18 43V22c0-3.3137 2.6863-6 6-6s6 2.6863 6 6V43" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M12 40V22c0-6.6274 5.3726-12 12-12s12 5.3726 12 12V40" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M6 35V22C6 12.0589 14.0589 4 24 4s18 8.0589 18 18V35" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 44V31" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 24.625v-2.75" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/></svg>
</span></span><span>2503.08625</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg fill="#000" height="800" width="800" id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 511.999 511.999"><g><g><path d="M421.578 190.264l-99.847-99.847c-2.439-2.439-6.391-2.439-8.829.0L82.824 320.495c-2.439 2.439-2.439 6.392.0 8.829l99.847 99.847c2.439 2.439 6.391 2.439 8.829.0l230.078-230.078C424.017 196.655 424.017 192.703 421.578 190.264z"/></g></g><g><g><path d="M506.511 87.672 424.323 5.484c-7.308-7.31-19.175-7.315-26.488.0L348.219 55.1c-2.439 2.439-2.439 6.391.0 8.829l99.847 99.847c2.439 2.437 6.391 2.437 8.829.0l49.616-49.616C513.826 106.847 513.826 94.987 506.511 87.672z"/></g></g><g><g><path d="M508.133 491.11c-1.054-9.556-9.489-16.599-19.104-16.599H111.633l36.058-15.163c4.088-1.719 5.131-7.034 1.994-10.17l-86.854-86.854c-3.137-3.135-8.451-2.094-10.17 1.994C52.224 365.359 2.052 484.66 1.627 485.707c-5.815 13.208 4.855 27.01 18.107 26.263H489.52C500.566 511.97 509.379 502.408 508.133 491.11z"/></g></g></svg>
</span></span><span>Muzhi Zhu et el.</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span>ü§ó 2025-03-12</span></span></span></div></div><p><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://arxiv.org/abs/2503.08625 target=_self role=button>‚Üó arXiv
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://huggingface.co/papers/2503.08625 target=_self role=button>‚Üó Hugging Face</a></p><audio controls><source src=https://ai-paper-reviewer.com/2503.08625/podcast.wav type=audio/wav>Your browser does not support the audio element.</audio><h3 class="relative group">TL;DR<div id=tldr class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#tldr aria-label=Anchor>#</a></span></h3><div class="lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl"><p><strong>Multimodal Large Language Models (MLLMs)</strong> struggle with pixel-level comprehension, limiting their applicability despite advances in image understanding. Current evaluation methods are too coarse for assessing fine-grained understanding, while existing segmentation methods disrupt the MLLM&rsquo;s text output space by relying on implicit tokens or external pixel decoders. This paper aims to address these limitations.</p><p>To solve this, the paper introduces the <strong>Human-Like Mask Annotation Task (HLMAT)</strong>, where MLLMs mimic human annotators using interactive segmentation tools, modeling the segmentation task as a multi-step Markov Decision Process. It introduces <strong>SegAgent</strong>, a model fine-tuned on human-like annotation trajectories, achieving SOTA performance and supporting tasks like mask refinement and annotation filtering. Techniques like StaR and PRM guided tree search further enhance the model.</p></div><h4 class="relative group">Key Takeaways<div id=key-takeaways class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#key-takeaways aria-label=Anchor>#</a></span></h4><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-b322f7b57f1fe67fa1e102fdb111786e></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-b322f7b57f1fe67fa1e102fdb111786e",{strings:[" HLMAT: A new task for evaluating fine-grained pixel understanding in MLLMs by modeling human annotation trajectories. "],speed:10,lifeLike:!0,startDelay:0,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-146f9cbee1b60d6e57d4cf649edfcf5e></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-146f9cbee1b60d6e57d4cf649edfcf5e",{strings:[" SegAgent: A fine-tuned MLLM that achieves competitive segmentation performance and supports mask refinement and annotation filtering. "],speed:10,lifeLike:!0,startDelay:1e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-17734abff54e9a39ac4b6e583f7149ee></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-17734abff54e9a39ac4b6e583f7149ee",{strings:[" StaR+ and PRM-guided tree search: Effective techniques for enhancing model robustness in complex segmentation tasks. "],speed:10,lifeLike:!0,startDelay:2e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><h4 class="relative group">Why does it matter?<div id=why-does-it-matter class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#why-does-it-matter aria-label=Anchor>#</a></span></h4><p>This work is important as it introduces a new method for evaluating and enhancing pixel-level understanding in MLLMs, a crucial step towards more capable and versatile AI systems. The SegAgent framework and HLMAT task open new avenues for research in visual reasoning and decision-making for MLLMs, with potential applications.</p><hr><h4 class="relative group">Visual Insights<div id=visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#visual-insights aria-label=Anchor>#</a></span></h4><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.08625/extracted/6271622/framework.png alt></figure></p><blockquote><p>üîº SegAgent is a model that imitates human annotators using interactive segmentation tools. This figure illustrates the SegAgent framework. An image and a text prompt are input into the MLLM (multimodal large language model), which then iteratively generates text-based coordinates representing clicks (positive or negative) for a segmentation tool. The segmentation tool updates a mask based on the clicks. The process continues until a satisfactory mask is produced. The lower part of the figure shows a sequence of iterations, with each step visualizing both the current action (click coordinates) and the resulting mask. The goal is to evaluate the MLLM&rsquo;s pixel-level comprehension by assessing its ability to generate high-quality masks through this iterative process.</p><details><summary>read the caption</summary>Figure 1: The overall framework of SegAgent. The image below shows a complete set of trajectories. We visualize current action atsubscriptùëéùë°a_{t}italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT and the resulting mask Mt+1subscriptùëÄùë°1M_{t+1}italic_M start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT in one image.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id=S4.T1.4.1><tbody class=ltx_tbody><tr class=ltx_tr id=S4.T1.4.1.1.1><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id=S4.T1.4.1.1.1.1 rowspan=2><span class="ltx_text ltx_font_bold" id=S4.T1.4.1.1.1.1.1>Method</span></th><td class="ltx_td ltx_align_center ltx_border_tt" id=S4.T1.4.1.1.1.2 rowspan=2><span class="ltx_text ltx_font_bold" id=S4.T1.4.1.1.1.2.1>Venue</span></td><td class="ltx_td ltx_align_center ltx_border_tt" colspan=3 id=S4.T1.4.1.1.1.3><span class="ltx_text ltx_font_bold" id=S4.T1.4.1.1.1.3.1>refCOCO</span></td><td class="ltx_td ltx_align_center ltx_border_tt" colspan=3 id=S4.T1.4.1.1.1.4><span class="ltx_text ltx_font_bold" id=S4.T1.4.1.1.1.4.1>refCOCO+</span></td><td class="ltx_td ltx_align_center ltx_border_tt" colspan=2 id=S4.T1.4.1.1.1.5><span class="ltx_text ltx_font_bold" id=S4.T1.4.1.1.1.5.1>refCOCOg</span></td></tr><tr class=ltx_tr id=S4.T1.4.1.2.2><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T1.4.1.2.2.1>val</td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T1.4.1.2.2.2>testA</td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T1.4.1.2.2.3>testB</td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T1.4.1.2.2.4>val</td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T1.4.1.2.2.5>testA</td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T1.4.1.2.2.6>testB</td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T1.4.1.2.2.7>val(U)</td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T1.4.1.2.2.8>test(U)</td></tr><tr class=ltx_tr id=S4.T1.4.1.3.3><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id=S4.T1.4.1.3.3.1><em class="ltx_emph ltx_font_italic" id=S4.T1.4.1.3.3.1.1>traditional methods</em></th><td class="ltx_td ltx_border_t" id=S4.T1.4.1.3.3.2></td><td class="ltx_td ltx_border_t" id=S4.T1.4.1.3.3.3></td><td class="ltx_td ltx_border_t" id=S4.T1.4.1.3.3.4></td><td class="ltx_td ltx_border_t" id=S4.T1.4.1.3.3.5></td><td class="ltx_td ltx_border_t" id=S4.T1.4.1.3.3.6></td><td class="ltx_td ltx_border_t" id=S4.T1.4.1.3.3.7></td><td class="ltx_td ltx_border_t" id=S4.T1.4.1.3.3.8></td><td class="ltx_td ltx_border_t" id=S4.T1.4.1.3.3.9></td><td class="ltx_td ltx_border_t" id=S4.T1.4.1.3.3.10></td></tr><tr class=ltx_tr id=S4.T1.4.1.4.4><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T1.4.1.4.4.1>MAttNet¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.08625v1#bib.bib67 title><span class=ltx_text style=font-size:90%>67</span></a>]</cite></th><td class="ltx_td ltx_align_center" id=S4.T1.4.1.4.4.2>CVPR‚Äô18</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.4.4.3>56.51</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.4.4.4>62.37</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.4.4.5>51.70</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.4.4.6>46.67</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.4.4.7>52.39</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.4.4.8>40.08</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.4.4.9>47.64</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.4.4.10>48.61</td></tr><tr class=ltx_tr id=S4.T1.4.1.5.5><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T1.4.1.5.5.1>LAVT¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.08625v1#bib.bib59 title><span class=ltx_text style=font-size:90%>59</span></a>]</cite></th><td class="ltx_td ltx_align_center" id=S4.T1.4.1.5.5.2>CVPR‚Äô22</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.5.5.3>72.7</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.5.5.4>75.8</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.5.5.5>68.8</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.5.5.6>62.1</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.5.5.7>68.4</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.5.5.8>55.1</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.5.5.9>61.2</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.5.5.10>62.1</td></tr><tr class=ltx_tr id=S4.T1.4.1.6.6><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T1.4.1.6.6.1>CRIS¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.08625v1#bib.bib54 title><span class=ltx_text style=font-size:90%>54</span></a>]</cite></th><td class="ltx_td ltx_align_center" id=S4.T1.4.1.6.6.2>CVPR‚Äô22</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.6.6.3>70.5</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.6.6.4>73.2</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.6.6.5>66.1</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.6.6.6>65.3</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.6.6.7>68.1</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.6.6.8>53.7</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.6.6.9>59.9</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.6.6.10>60.4</td></tr><tr class=ltx_tr id=S4.T1.4.1.7.7><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T1.4.1.7.7.1>PolyFormer-L¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.08625v1#bib.bib29 title><span class=ltx_text style=font-size:90%>29</span></a>]</cite></th><td class="ltx_td ltx_align_center" id=S4.T1.4.1.7.7.2>CVPR‚Äô23</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.7.7.3>76.94</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.7.7.4>78.49</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.7.7.5>74.83</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.7.7.6>72.15</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.7.7.7>75.71</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.7.7.8>66.73</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.7.7.9>71.15</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.7.7.10>71.17</td></tr><tr class=ltx_tr id=S4.T1.4.1.8.8><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T1.4.1.8.8.1>X-Decoder¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.08625v1#bib.bib81 title><span class=ltx_text style=font-size:90%>81</span></a>]</cite></th><td class="ltx_td ltx_align_center" id=S4.T1.4.1.8.8.2>CVPR‚Äô23</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.8.8.3>-</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.8.8.4>-</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.8.8.5>-</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.8.8.6>-</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.8.8.7>-</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.8.8.8>-</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.8.8.9>64.6</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.8.8.10>-</td></tr><tr class=ltx_tr id=S4.T1.4.1.9.9><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T1.4.1.9.9.1>SEEM¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.08625v1#bib.bib82 title><span class=ltx_text style=font-size:90%>82</span></a>]</cite></th><td class="ltx_td ltx_align_center" id=S4.T1.4.1.9.9.2>NeurIPS‚Äô23</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.9.9.3>-</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.9.9.4>-</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.9.9.5>-</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.9.9.6>-</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.9.9.7>-</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.9.9.8>-</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.9.9.9>65.7</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.9.9.10>-</td></tr><tr class=ltx_tr id=S4.T1.4.1.10.10><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id=S4.T1.4.1.10.10.1><em class="ltx_emph ltx_font_italic" id=S4.T1.4.1.10.10.1.1>LLM based methods</em></th><td class="ltx_td ltx_border_t" id=S4.T1.4.1.10.10.2></td><td class="ltx_td ltx_border_t" id=S4.T1.4.1.10.10.3></td><td class="ltx_td ltx_border_t" id=S4.T1.4.1.10.10.4></td><td class="ltx_td ltx_border_t" id=S4.T1.4.1.10.10.5></td><td class="ltx_td ltx_border_t" id=S4.T1.4.1.10.10.6></td><td class="ltx_td ltx_border_t" id=S4.T1.4.1.10.10.7></td><td class="ltx_td ltx_border_t" id=S4.T1.4.1.10.10.8></td><td class="ltx_td ltx_border_t" id=S4.T1.4.1.10.10.9></td><td class="ltx_td ltx_border_t" id=S4.T1.4.1.10.10.10></td></tr><tr class=ltx_tr id=S4.T1.4.1.11.11><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T1.4.1.11.11.1><span class=ltx_text id=S4.T1.4.1.11.11.1.1 style=color:#000>LISA(SAM)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.08625v1#bib.bib19 title><span class=ltx_text style=font-size:90%>19</span></a>]</cite></span></th><td class="ltx_td ltx_align_center" id=S4.T1.4.1.11.11.2>CVPR‚Äô24</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.11.11.3>74.9</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.11.11.4>79.1</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.11.11.5>72.3</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.11.11.6>65.1</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.11.11.7>70.8</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.11.11.8>58.1</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.11.11.9>67.9</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.11.11.10>70.6</td></tr><tr class=ltx_tr id=S4.T1.4.1.12.12><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T1.4.1.12.12.1><span class=ltx_text id=S4.T1.4.1.12.12.1.1 style=color:#000>PixelLM¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.08625v1#bib.bib46 title><span class=ltx_text style=font-size:90%>46</span></a>]</cite></span></th><td class="ltx_td ltx_align_center" id=S4.T1.4.1.12.12.2>CVPR‚Äô24</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.12.12.3>73.0</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.12.12.4>76.5</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.12.12.5>68.2</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.12.12.6>66.3</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.12.12.7>71.7</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.12.12.8>58.3</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.12.12.9>69.3</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.12.12.10>70.5</td></tr><tr class=ltx_tr id=S4.T1.4.1.13.13><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T1.4.1.13.13.1><span class=ltx_text id=S4.T1.4.1.13.13.1.1 style=color:#000>PerceptionGPT¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.08625v1#bib.bib42 title><span class=ltx_text style=font-size:90%>42</span></a>]</cite></span></th><td class="ltx_td ltx_align_center" id=S4.T1.4.1.13.13.2>CVPR‚Äô24</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.13.13.3>75.1</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.13.13.4>78.6</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.13.13.5>71.7</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.13.13.6>68.5</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.13.13.7>73.9</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.13.13.8>61.3</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.13.13.9>70.3</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.13.13.10>71.7</td></tr><tr class=ltx_tr id=S4.T1.4.1.14.14><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T1.4.1.14.14.1><span class=ltx_text id=S4.T1.4.1.14.14.1.1 style=color:#000>GSVA(SAM)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.08625v1#bib.bib56 title><span class=ltx_text style=font-size:90%>56</span></a>]</cite></span></th><td class="ltx_td ltx_align_center" id=S4.T1.4.1.14.14.2>CVPR‚Äô24</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.14.14.3>77.2</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.14.14.4>78.9</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.14.14.5>73.5</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.14.14.6>65.9</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.14.14.7>69.6</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.14.14.8>59.8</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.14.14.9>72.7</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.14.14.10>73.3</td></tr><tr class=ltx_tr id=S4.T1.4.1.15.15><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T1.4.1.15.15.1>SAM4MLLM(Qwen)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.08625v1#bib.bib10 title><span class=ltx_text style=font-size:90%>10</span></a>]</cite></th><td class="ltx_td ltx_align_center" id=S4.T1.4.1.15.15.2>ECCV‚Äô24</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.15.15.3>77.1</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.15.15.4>80.9</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.15.15.5>72.5</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.15.15.6>71.5</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.15.15.7>76.8</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.15.15.8>64.7</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.15.15.9>74.5</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.15.15.10>75.2</td></tr><tr class=ltx_tr id=S4.T1.4.1.16.16><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T1.4.1.16.16.1>Qwen box¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.08625v1#bib.bib2 title><span class=ltx_text style=font-size:90%>2</span></a>]</cite> + SAM</th><td class="ltx_td ltx_align_center" id=S4.T1.4.1.16.16.2>-</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.16.16.3>71.79</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.16.16.4>75.20</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.16.16.5>67.29</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.16.16.6>65.39</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.16.16.7>71.62</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.16.16.8>59.12</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.16.16.9>66.93</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.16.16.10>68.94</td></tr><tr class=ltx_tr id=S4.T1.4.1.17.17><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id=S4.T1.4.1.17.17.1><em class="ltx_emph ltx_font_italic" id=S4.T1.4.1.17.17.1.1>Our methods</em></th><td class="ltx_td ltx_border_t" id=S4.T1.4.1.17.17.2></td><td class="ltx_td ltx_border_t" id=S4.T1.4.1.17.17.3></td><td class="ltx_td ltx_border_t" id=S4.T1.4.1.17.17.4></td><td class="ltx_td ltx_border_t" id=S4.T1.4.1.17.17.5></td><td class="ltx_td ltx_border_t" id=S4.T1.4.1.17.17.6></td><td class="ltx_td ltx_border_t" id=S4.T1.4.1.17.17.7></td><td class="ltx_td ltx_border_t" id=S4.T1.4.1.17.17.8></td><td class="ltx_td ltx_border_t" id=S4.T1.4.1.17.17.9></td><td class="ltx_td ltx_border_t" id=S4.T1.4.1.17.17.10></td></tr><tr class=ltx_tr id=S4.T1.4.1.18.18><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T1.4.1.18.18.1>SegAgent-LLaVA+SAM</th><td class="ltx_td ltx_align_center ltx_border_bb" id=S4.T1.4.1.18.18.2 rowspan=4><span class=ltx_text id=S4.T1.4.1.18.18.2.1>this work</span></td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.18.18.3>79.20</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.18.18.4>81.44</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.18.18.5>75.72</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.18.18.6>71.53</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.18.18.7>76.68</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.18.18.8>65.44</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.18.18.9>74.80</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.18.18.10>74.90</td></tr><tr class=ltx_tr id=S4.T1.4.1.19.19><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T1.4.1.19.19.1>SegAgent-Qwen+SAM</th><td class="ltx_td ltx_align_center" id=S4.T1.4.1.19.19.2>78.01</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.19.19.3>80.34</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.19.19.4>74.98</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.19.19.5>70.86</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.19.19.6>75.52</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.19.19.7>65.75</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.19.19.8>74.49</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.19.19.9>74.62</td></tr><tr class=ltx_tr id=S4.T1.4.1.20.20><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T1.4.1.20.20.1>SegAgent-LLaVA+SClick</th><td class="ltx_td ltx_align_center" id=S4.T1.4.1.20.20.2>77.81</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.20.20.3>80.03</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.20.20.4>74.12</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.20.20.5>66.73</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.20.20.6>71.16</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.20.20.7>59.89</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.20.20.8>70.45</td><td class="ltx_td ltx_align_center" id=S4.T1.4.1.20.20.9>71.25</td></tr><tr class=ltx_tr id=S4.T1.4.1.21.21><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id=S4.T1.4.1.21.21.1>SegAgent-Qwen+SClick</th><td class="ltx_td ltx_align_center ltx_border_bb" id=S4.T1.4.1.21.21.2>79.69</td><td class="ltx_td ltx_align_center ltx_border_bb" id=S4.T1.4.1.21.21.3>81.35</td><td class="ltx_td ltx_align_center ltx_border_bb" id=S4.T1.4.1.21.21.4>76.57</td><td class="ltx_td ltx_align_center ltx_border_bb" id=S4.T1.4.1.21.21.5>72.49</td><td class="ltx_td ltx_align_center ltx_border_bb" id=S4.T1.4.1.21.21.6>75.80</td><td class="ltx_td ltx_align_center ltx_border_bb" id=S4.T1.4.1.21.21.7>66.89</td><td class="ltx_td ltx_align_center ltx_border_bb" id=S4.T1.4.1.21.21.8>75.11</td><td class="ltx_td ltx_align_center ltx_border_bb" id=S4.T1.4.1.21.21.9>75.20</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents a quantitative comparison of different methods&rsquo; performance on the Referring Expression Segmentation (RES) dataset. It specifically contrasts traditional computer vision methods with more recent approaches leveraging Large Language Models (LLMs). The table highlights the performance (measured likely by Intersection over Union or IoU) of each method across three subsets of the RES dataset: refCOCO, refCOCO+, and refCOCOg. The table also distinguishes whether a method uses the Segment Anything Model (SAM) or the SimpleClick interactive segmentation model, showcasing the impact of these different tools on overall performance.</p><details><summary>read the caption</summary>Table 1: Comparison of methods on RES dataset. We indicate which models use SAM. ‚ÄúSClick‚Äù denotes the use of SimpleClick as the interactive segmentation model.</details></blockquote><h3 class="relative group">In-depth insights<div id=in-depth-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#in-depth-insights aria-label=Anchor>#</a></span></h3><h4 class="relative group">HLMAT: A New Task<div id=hlmat-a-new-task class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#hlmat-a-new-task aria-label=Anchor>#</a></span></h4><p>While &lsquo;HLMAT: A New Task&rsquo; isn&rsquo;t directly from the text, it suggests a novel task-centric approach, likely focusing on <strong>Human-Like Mask Annotation</strong>. This highlights a shift towards <strong>more realistic simulation of human annotators&rsquo; behaviors</strong> in image segmentation. The task likely demands <strong>fine-grained pixel understanding</strong> from MLLMs. It introduces a <strong>vision-centric, multi-step decision-making process</strong>, moving beyond simple image understanding towards interactive, iterative annotation. This approach allows MLLMs to learn from human-like annotation trajectories using interactive segmentation tools. The core idea would be to mimic the steps a human would take during segmentation, using iterative refinement and feedback. <strong>HLMAT promises a more nuanced and comprehensive method for evaluating and advancing MLLMs&rsquo; visual reasoning skills</strong>, fostering more accurate and flexible AI systems.</p><h4 class="relative group">SegAgent Details<div id=segagent-details class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#segagent-details aria-label=Anchor>#</a></span></h4><p>The paper likely delves into the specifics of the SegAgent model, a system designed to perform image segmentation by imitating human annotators. <strong>Model architecture details</strong> such as the vision encoder and LLM are probably covered, highlighting key components like ConvNeXt-L CLIP or Qwen-VL-Chat. Hyperparameter settings, including learning rates, batch sizes, and training epochs, would be specified to allow for reproducibility. A crucial aspect is the <strong>prompt engineering</strong>, detailing how instructions are formatted for the LLM to guide its segmentation process. Further discussion of the architecture could show details about the VIT structure utilized. The coordinate formats employed would be discussed, and <strong>the rationale behind the specific choices</strong> could be highlighted.</p><h4 class="relative group">Iterative Refinement<div id=iterative-refinement class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#iterative-refinement aria-label=Anchor>#</a></span></h4><p><strong>Iterative refinement</strong> is a crucial aspect of many computer vision tasks, as it allows models to progressively improve their predictions by repeatedly refining an initial estimate. In segmentation, this could involve starting with a coarse mask and then iteratively adjusting its boundaries based on local image features and contextual information. This approach is particularly useful for handling complex shapes and ambiguous regions, where a single-pass prediction might be insufficient. The success of iterative refinement depends on several factors, including the quality of the initial estimate, the effectiveness of the refinement mechanism, and the stopping criterion. A well-designed iterative refinement strategy can lead to significant improvements in segmentation accuracy and robustness.</p><h4 class="relative group">HRES Dataset<div id=hres-dataset class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#hres-dataset aria-label=Anchor>#</a></span></h4><p>Based on the paper, the <strong>High-quality Referring Expression Segmentation (HRES) dataset</strong> is a novel contribution designed to address the limitations of existing datasets like RefCOCO, which lack the complexity and annotation quality needed for multi-step reasoning in MLLMs. The authors note that RefCOCO&rsquo;s masks often contain noise and don&rsquo;t require extensive iterative refinement. HRES utilizes subsets from <strong>HQSeg-44K</strong>, specifically <strong>DIS5K</strong> and <strong>ThinObject5K</strong>, which offer more detailed and precise annotations of complex objects. DIS5K focuses on high-resolution images with binary segmentation masks, while ThinObject5K targets objects with thin structures like insect legs. This is crucial for <strong>evaluating MLLMs&rsquo; fine-grained pixel understanding</strong> and ability to refine masks over multiple steps, ultimately enabling more robust and reliable performance in challenging segmentation scenarios. This dataset is key because it enables a focus on visual understanding, and more robust evaluation of MLLMs.</p><h4 class="relative group">Future MLLM Agent<div id=future-mllm-agent class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#future-mllm-agent aria-label=Anchor>#</a></span></h4><p>If we envision &ldquo;Future MLLM Agents,&rdquo; several key directions emerge from this paper&rsquo;s context. The core idea of enabling <strong>fine-grained pixel understanding</strong> in MLLMs opens up a path toward more capable agents. The approach of <strong>imitating human annotator trajectories</strong> is interesting because it leverages the existing interaction data. These agents would likely perform complex tasks demanding visual reasoning and precise manipulation. <strong>HLMAT</strong> acts as a way to explore and advance the MLLMs&rsquo; visual capabilities. The framework allows for a more direct assessment of the MLLMs&rsquo; ability to process and understand visual information at the pixel level. Additionally, the integration of techniques like <strong>StaR+</strong> and <strong>tree search with PRM</strong> hints at more robust and reliable agents that can overcome noisy or ambiguous environments.</p><h3 class="relative group">More visual insights<div id=more-visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#more-visual-insights aria-label=Anchor>#</a></span></h3><details><summary>More on figures</summary><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.08625/extracted/6271622/trace.png alt></figure></p><blockquote><p>üîº This figure visualizes a generated trajectory from an automated algorithm that simulates human-like mask annotation. Each image shows the mask at a particular iteration, along with the current action (a positive or negative click) represented by a point on the image. Iteration 0 begins with an empty mask, and subsequent iterations show how the mask is refined based on the sequential actions. The trajectory generation is based on the ground truth (GT) mask; however, the GT mask contains noise, resulting in the last two actions (Iteration 3,4) being meaningless and not contributing to the overall mask refinement. The visualization helps illustrate the process of iterative mask creation using simulated human actions.</p><details><summary>read the caption</summary>Figure 2: An example of generated trajectory. We visualize current action atsubscriptùëéùë°a_{t}italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT and the resulting mask Mt+1subscriptùëÄùë°1M_{t+1}italic_M start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT in one image. Due to the noise from GT Mask, the action for Iteration 3,4 is meaningless</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.08625/extracted/6271622/noc.png alt></figure></p><blockquote><p>üîº The bar chart visualizes the complexity of three datasets: DIS5K, ThinObject5K, and refcoco. The complexity is measured by the average number of clicks needed to achieve 80% and 85% Intersection over Union (IoU) during interactive image segmentation. The chart clearly shows that DIS5K and ThinObject5K require significantly more clicks than refcoco, indicating a higher complexity level in the former two datasets. This difference is due to the inherent characteristics of the datasets, such as the presence of more intricate object boundaries and finer-grained details in DIS5K and ThinObject5K compared to refcoco. Therefore, the datasets DIS5K and ThinObject5K are more challenging and appropriate for evaluating the pixel-level understanding capabilities of a model.</p><details><summary>read the caption</summary>Figure 3: Comparison of dataset complexity.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.08625/extracted/6271622/hres.png alt></figure></p><blockquote><p>üîº The bar chart compares the performance of different strategies for image segmentation on the HRES dataset. The baseline strategy uses a model fine-tuned with supervised fine-tuning (SFT) and a fixed-step greedy decoding method during testing. The chart then shows performance improvements achieved by incorporating policy improvement methods (StaR+), process reward modeling (PRM), and the addition of tree search. The improvements are shown for both the DIS and ThinObject subsets of the HRES dataset, illustrating the effectiveness of each method in enhancing segmentation accuracy in complex scenarios.</p><details><summary>read the caption</summary>Figure 4: Comparison of different strategies on the HRES dataset.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.08625/extracted/6271622/tree2.png alt></figure></p><blockquote><p>üîº This figure illustrates the process of PRM-guided tree search used in SegAgent for interactive image segmentation. The model begins with an initial mask and image. At each step, it generates multiple candidate actions (positive or negative clicks). It uses a PRM (Process Reward Model) to predict the reward (measured by IoU) for each action, selecting the action with the highest predicted reward. The chosen action is then input to the interactive segmentation tool to update the mask. This iterative process repeats until a satisfactory mask is produced or a predetermined number of steps is reached. The visualization shows the progression of the mask at each step and the corresponding predicted rewards.</p><details><summary>read the caption</summary>Figure 5: An illustrative example of PRM-guided tree search. The model predicts the reward at each step and selects the action with the highest reward to generate the next mask.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.08625/x1.png alt></figure></p><blockquote><p>üîº Figure 6 shows a detailed example of the text prompt used to guide the model during the mask refinement process in the interactive segmentation task. The prompt provides clear instructions on how to refine a mask using three actions: adding a positive point (to expand the mask), adding a negative point (to shrink the mask), and generating additional information (optional). The red text section is where user-specific inputs, such as the object description, are included to ensure the model&rsquo;s understanding of the task. This prompt design is crucial for enabling the model to learn the human-like annotation process.</p><details><summary>read the caption</summary>Figure 6: The prompt provides detailed instructions for refining a segmentation mask with three possible actions: adding a positive point, adding a negative point. The red part indicates user-specific input, such as object descriptions.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.08625/x2.png alt></figure></p><blockquote><p>üîº This figure visually compares the quality and complexity of annotations across three datasets: ThinObject5k-TE, DIS5K, and RefCOCO. Each row displays example images from one dataset, with their corresponding segmentation masks overlaid in green. The differences in mask detail and accuracy highlight how the complexity of the segmentation task varies between datasets. This is important for evaluating the performance of the proposed method, as more complex and accurate datasets better assess the fine-grained pixel-level understanding capabilities of the model.</p><details><summary>read the caption</summary>Figure 7: Examples of Images and Annotations from Various Datasets. The figure showcases representative samples from three datasets: ThinObject5k-TE, DIS5K, and RefCOCO. Each row represents a dataset, with images and corresponding annotations highlighting different objects and scenes. The annotations (green overlays) demonstrate the varying levels of detail and complexity across datasets.</details></blockquote></details><details><summary>More on tables</summary><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id=A1.T2.6><thead class=ltx_thead><tr class=ltx_tr id=A1.T2.6.1.1><th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id=A1.T2.6.1.1.1 style="padding:-.4pt 3pt"><span class=ltx_text id=A1.T2.6.1.1.1.1 style=font-size:80%># Steps</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=A1.T2.6.1.1.2 style="padding:-.4pt 3pt"><span class=ltx_text id=A1.T2.6.1.1.2.1 style=font-size:80%>1</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=A1.T2.6.1.1.3 style="padding:-.4pt 3pt"><span class=ltx_text id=A1.T2.6.1.1.3.1 style=font-size:80%>3</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=A1.T2.6.1.1.4 style="padding:-.4pt 3pt"><span class=ltx_text id=A1.T2.6.1.1.4.1 style=font-size:80%>5</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=A1.T2.6.1.1.5 style="padding:-.4pt 3pt"><span class=ltx_text id=A1.T2.6.1.1.5.1 style=font-size:80%>7</span></th></tr></thead><tbody class=ltx_tbody><tr class=ltx_tr id=A1.T2.6.2.1><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id=A1.T2.6.2.1.1 style="padding:-.4pt 3pt"><span class=ltx_text id=A1.T2.6.2.1.1.1 style=font-size:80%>w/o PRM</span></th><td class="ltx_td ltx_align_center ltx_border_t" id=A1.T2.6.2.1.2 style="padding:-.4pt 3pt"><span class=ltx_text id=A1.T2.6.2.1.2.1 style=font-size:80%>71.53</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=A1.T2.6.2.1.3 style="padding:-.4pt 3pt"><span class=ltx_text id=A1.T2.6.2.1.3.1 style=font-size:80%>73.67</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=A1.T2.6.2.1.4 style="padding:-.4pt 3pt"><span class=ltx_text id=A1.T2.6.2.1.4.1 style=font-size:80%>73.88</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=A1.T2.6.2.1.5 style="padding:-.4pt 3pt"><span class=ltx_text id=A1.T2.6.2.1.5.1 style=font-size:80%>68.22</span></td></tr><tr class=ltx_tr id=A1.T2.6.3.2><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id=A1.T2.6.3.2.1 style="padding:-.4pt 3pt"><span class=ltx_text id=A1.T2.6.3.2.1.1 style=font-size:80%>w/ PRM</span></th><td class="ltx_td ltx_align_center ltx_border_bb" id=A1.T2.6.3.2.2 style="padding:-.4pt 3pt"><span class=ltx_text id=A1.T2.6.3.2.2.1 style=font-size:80%>71.53</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=A1.T2.6.3.2.3 style="padding:-.4pt 3pt"><span class=ltx_text id=A1.T2.6.3.2.3.1 style=font-size:80%>72.98</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=A1.T2.6.3.2.4 style="padding:-.4pt 3pt"><span class=ltx_text id=A1.T2.6.3.2.4.1 style=font-size:80%>75.21</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=A1.T2.6.3.2.5 style="padding:-.4pt 3pt"><span class=ltx_text id=A1.T2.6.3.2.5.1 style=font-size:80%>75.43</span></td></tr></tbody></table></table></figure><blockquote><p>üîº This table compares the model&rsquo;s performance with and without the process reward model (PRM) across different numbers of steps in a multi-step segmentation task. It demonstrates the impact of PRM on mitigating error accumulation in longer sequences.</p><details><summary>read the caption</summary>Table 2: Performance comparison under different steps.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id=A5.T3.4.4><thead class=ltx_thead><tr class=ltx_tr id=A5.T3.4.4.4><th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id=A5.T3.4.4.4.5><span class="ltx_text ltx_font_bold" id=A5.T3.4.4.4.5.1 style=font-size:144%>Model</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=A5.T3.1.1.1.1><span class="ltx_text ltx_font_bold" id=A5.T3.1.1.1.1.1 style=font-size:144%>MAE</span><span class=ltx_text id=A5.T3.1.1.1.1.2 style=font-size:144%></span><math alttext="\downarrow" class="ltx_Math" display="inline" id="A5.T3.1.1.1.1.m1.1"><semantics id="A5.T3.1.1.1.1.m1.1a"><mo id="A5.T3.1.1.1.1.m1.1.1" mathsize="144%" stretchy="false" xref="A5.T3.1.1.1.1.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="A5.T3.1.1.1.1.m1.1b"><ci id="A5.T3.1.1.1.1.m1.1.1.cmml" xref="A5.T3.1.1.1.1.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.T3.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="A5.T3.1.1.1.1.m1.1d">‚Üì</annotation></semantics></math></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=A5.T3.2.2.2.2><span class="ltx_text ltx_font_bold" id=A5.T3.2.2.2.2.1 style=font-size:144%>MSE</span><span class=ltx_text id=A5.T3.2.2.2.2.2 style=font-size:144%></span><math alttext="\downarrow" class="ltx_Math" display="inline" id="A5.T3.2.2.2.2.m1.1"><semantics id="A5.T3.2.2.2.2.m1.1a"><mo id="A5.T3.2.2.2.2.m1.1.1" mathsize="144%" stretchy="false" xref="A5.T3.2.2.2.2.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="A5.T3.2.2.2.2.m1.1b"><ci id="A5.T3.2.2.2.2.m1.1.1.cmml" xref="A5.T3.2.2.2.2.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.T3.2.2.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="A5.T3.2.2.2.2.m1.1d">‚Üì</annotation></semantics></math></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=A5.T3.3.3.3.3><span class="ltx_text ltx_font_bold" id=A5.T3.3.3.3.3.1 style=font-size:144%>Pearson</span><span class=ltx_text id=A5.T3.3.3.3.3.2 style=font-size:144%></span><math alttext="\uparrow" class="ltx_Math" display="inline" id="A5.T3.3.3.3.3.m1.1"><semantics id="A5.T3.3.3.3.3.m1.1a"><mo id="A5.T3.3.3.3.3.m1.1.1" mathsize="144%" stretchy="false" xref="A5.T3.3.3.3.3.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="A5.T3.3.3.3.3.m1.1b"><ci id="A5.T3.3.3.3.3.m1.1.1.cmml" xref="A5.T3.3.3.3.3.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.T3.3.3.3.3.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A5.T3.3.3.3.3.m1.1d">‚Üë</annotation></semantics></math></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=A5.T3.4.4.4.4><span class="ltx_text ltx_font_bold" id=A5.T3.4.4.4.4.1 style=font-size:144%>Spearman</span><span class=ltx_text id=A5.T3.4.4.4.4.2 style=font-size:144%></span><math alttext="\uparrow" class="ltx_Math" display="inline" id="A5.T3.4.4.4.4.m1.1"><semantics id="A5.T3.4.4.4.4.m1.1a"><mo id="A5.T3.4.4.4.4.m1.1.1" mathsize="144%" stretchy="false" xref="A5.T3.4.4.4.4.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="A5.T3.4.4.4.4.m1.1b"><ci id="A5.T3.4.4.4.4.m1.1.1.cmml" xref="A5.T3.4.4.4.4.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.T3.4.4.4.4.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A5.T3.4.4.4.4.m1.1d">‚Üë</annotation></semantics></math></th></tr></thead><tbody class=ltx_tbody><tr class=ltx_tr id=A5.T3.4.4.5.1><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id=A5.T3.4.4.5.1.1><span class=ltx_text id=A5.T3.4.4.5.1.1.1 style=font-size:144%>SegAgent-Qwen</span></th><td class="ltx_td ltx_align_center ltx_border_t" id=A5.T3.4.4.5.1.2><span class=ltx_text id=A5.T3.4.4.5.1.2.1 style=font-size:144%>6.88</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=A5.T3.4.4.5.1.3><span class=ltx_text id=A5.T3.4.4.5.1.3.1 style=font-size:144%>193.98</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=A5.T3.4.4.5.1.4><span class=ltx_text id=A5.T3.4.4.5.1.4.1 style=font-size:144%>0.90</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=A5.T3.4.4.5.1.5><span class=ltx_text id=A5.T3.4.4.5.1.5.1 style=font-size:144%>0.87</span></td></tr><tr class=ltx_tr id=A5.T3.4.4.6.2><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id=A5.T3.4.4.6.2.1><span class=ltx_text id=A5.T3.4.4.6.2.1.1 style=font-size:144%>SegAgent-LLaVA</span></th><td class="ltx_td ltx_align_center ltx_border_bb" id=A5.T3.4.4.6.2.2><span class=ltx_text id=A5.T3.4.4.6.2.2.1 style=font-size:144%>5.58</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=A5.T3.4.4.6.2.3><span class=ltx_text id=A5.T3.4.4.6.2.3.1 style=font-size:144%>175.35</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=A5.T3.4.4.6.2.4><span class=ltx_text id=A5.T3.4.4.6.2.4.1 style=font-size:144%>0.91</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=A5.T3.4.4.6.2.5><span class=ltx_text id=A5.T3.4.4.6.2.5.1 style=font-size:144%>0.90</span></td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents a quantitative evaluation of SegAgent&rsquo;s ability to filter annotations. It assesses the model&rsquo;s performance on a regression task where it predicts the Intersection over Union (IoU) between a generated mask and the ground truth mask. Lower Mean Absolute Error (MAE) and Mean Squared Error (MSE) scores indicate higher accuracy in predicting the IoU. Conversely, higher Pearson and Spearman correlation coefficients signify stronger agreement between the model&rsquo;s IoU predictions and the actual ground truth IoU values.</p><details><summary>read the caption</summary>Table 3: Evaluation of SegAgent‚Äôs Annotation Filtering Ability. Lower MAE and MSE indicate better accuracy, while higher Pearson and Spearman correlation coefficients reflect stronger agreement with ground truth IoU.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id=A5.T4.6><tbody class=ltx_tbody><tr class=ltx_tr id=A5.T4.6.1.1><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id=A5.T4.6.1.1.1><span class="ltx_text ltx_font_bold" id=A5.T4.6.1.1.1.1 style=font-size:144%>Mask Color</span></th><td class="ltx_td ltx_align_center ltx_border_tt" id=A5.T4.6.1.1.2><span class="ltx_text ltx_font_bold" id=A5.T4.6.1.1.2.1 style=font-size:144%>Green</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=A5.T4.6.1.1.3><span class="ltx_text ltx_font_bold" id=A5.T4.6.1.1.3.1 style=font-size:144%>Blue</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=A5.T4.6.1.1.4><span class="ltx_text ltx_font_bold" id=A5.T4.6.1.1.4.1 style=font-size:144%>Red</span></td></tr><tr class=ltx_tr id=A5.T4.6.2.2><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id=A5.T4.6.2.2.1><span class="ltx_text ltx_font_bold" id=A5.T4.6.2.2.1.1 style=font-size:144%>mIoU</span></th><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id=A5.T4.6.2.2.2><span class=ltx_text id=A5.T4.6.2.2.2.1 style=font-size:144%>0.749</span></td><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id=A5.T4.6.2.2.3><span class=ltx_text id=A5.T4.6.2.2.3.1 style=font-size:144%>0.750</span></td><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id=A5.T4.6.2.2.4><span class=ltx_text id=A5.T4.6.2.2.4.1 style=font-size:144%>0.749</span></td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents the results of an experiment evaluating the impact of different mask colors (green, blue, and red) on the performance of the SegAgent model in image segmentation tasks. The mean Intersection over Union (mIoU) metric was used to measure the performance, with nearly identical results obtained for all three colors. This demonstrates that the model&rsquo;s segmentation capabilities are robust to variations in mask color.</p><details><summary>read the caption</summary>Table 4: Evaluation of Mask Color on Segmentation Performance.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id=A5.T5.6><thead class=ltx_thead><tr class=ltx_tr id=A5.T5.6.1.1><th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id=A5.T5.6.1.1.1><span class="ltx_text ltx_font_bold" id=A5.T5.6.1.1.1.1 style=font-size:144%>Initial Action</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=A5.T5.6.1.1.2><span class="ltx_text ltx_font_bold" id=A5.T5.6.1.1.2.1 style=font-size:144%>NO box</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=A5.T5.6.1.1.3><span class="ltx_text ltx_font_bold" id=A5.T5.6.1.1.3.1 style=font-size:144%>Qwen Box</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=A5.T5.6.1.1.4><span class="ltx_text ltx_font_bold" id=A5.T5.6.1.1.4.1 style=font-size:144%>Self Box</span></th></tr></thead><tbody class=ltx_tbody><tr class=ltx_tr id=A5.T5.6.2.1><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id=A5.T5.6.2.1.1><span class="ltx_text ltx_font_bold" id=A5.T5.6.2.1.1.1 style=font-size:144%>refcoco(val)</span></th><td class="ltx_td ltx_align_center ltx_border_t" id=A5.T5.6.2.1.2><span class=ltx_text id=A5.T5.6.2.1.2.1 style=font-size:144%>77.81</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=A5.T5.6.2.1.3><span class=ltx_text id=A5.T5.6.2.1.3.1 style=font-size:144%>78.01</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=A5.T5.6.2.1.4><span class=ltx_text id=A5.T5.6.2.1.4.1 style=font-size:144%>77.85</span></td></tr><tr class=ltx_tr id=A5.T5.6.3.2><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=A5.T5.6.3.2.1><span class="ltx_text ltx_font_bold" id=A5.T5.6.3.2.1.1 style=font-size:144%>refcoco+(val)</span></th><td class="ltx_td ltx_align_center" id=A5.T5.6.3.2.2><span class=ltx_text id=A5.T5.6.3.2.2.1 style=font-size:144%>70.88</span></td><td class="ltx_td ltx_align_center" id=A5.T5.6.3.2.3><span class=ltx_text id=A5.T5.6.3.2.3.1 style=font-size:144%>70.86</span></td><td class="ltx_td ltx_align_center" id=A5.T5.6.3.2.4><span class=ltx_text id=A5.T5.6.3.2.4.1 style=font-size:144%>70.50</span></td></tr><tr class=ltx_tr id=A5.T5.6.4.3><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id=A5.T5.6.4.3.1><span class="ltx_text ltx_font_bold" id=A5.T5.6.4.3.1.1 style=font-size:144%>refcocog(test)</span></th><td class="ltx_td ltx_align_center ltx_border_bb" id=A5.T5.6.4.3.2><span class=ltx_text id=A5.T5.6.4.3.2.1 style=font-size:144%>73.13</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=A5.T5.6.4.3.3><span class=ltx_text id=A5.T5.6.4.3.3.1 style=font-size:144%>74.62</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=A5.T5.6.4.3.4><span class=ltx_text id=A5.T5.6.4.3.4.1 style=font-size:144%>74.33</span></td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents a comparison of SegAgent&rsquo;s performance using different initial actions for segmentation. The initial action refers to the first action taken by the model when initiating a segmentation task. Three different initial actions were tested: no initial box (only clicks provided), a bounding box predicted by the Qwen-VL-chat model, and a bounding box predicted by the SegAgent itself. The performance is evaluated across three different subsets of the referring expression segmentation (RES) dataset (refcoco, refcoco+, refcocog) using the validation or test set for each. The results show the impact of the initial action selection on the mIoU (mean Intersection over Union) score, providing insights into the model&rsquo;s robustness to the initial action.</p><details><summary>read the caption</summary>Table 5: Evaluation of Different Initial Actions on SegAgent Performance.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id=A5.T6.6><tbody class=ltx_tbody><tr class=ltx_tr id=A5.T6.6.1.1><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id=A5.T6.6.1.1.1><span class="ltx_text ltx_font_bold" id=A5.T6.6.1.1.1.1 style=font-size:144%>Coordinate Format</span></th><td class="ltx_td ltx_align_center ltx_border_tt" id=A5.T6.6.1.1.2><span class="ltx_text ltx_font_bold" id=A5.T6.6.1.1.2.1 style=font-size:144%>[0, 1)</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=A5.T6.6.1.1.3><span class="ltx_text ltx_font_bold" id=A5.T6.6.1.1.3.1 style=font-size:144%>[0, 1000)</span></td></tr><tr class=ltx_tr id=A5.T6.6.2.2><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id=A5.T6.6.2.2.1><span class="ltx_text ltx_font_bold" id=A5.T6.6.2.2.1.1 style=font-size:144%>mIoU</span></th><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id=A5.T6.6.2.2.2><span class=ltx_text id=A5.T6.6.2.2.2.1 style=font-size:144%>0.749</span></td><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id=A5.T6.6.2.2.3><span class=ltx_text id=A5.T6.6.2.2.3.1 style=font-size:144%>0.747</span></td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents the results of an experiment comparing the performance of the SegAgent model using two different coordinate formats for representing the location of segmentation points: decimals in the range [0, 1) and integers in the range [0, 1000). The mIoU (mean Intersection over Union) metric is used to evaluate the segmentation performance for each format. The purpose of the experiment is to assess the model&rsquo;s robustness to the choice of coordinate representation.</p><details><summary>read the caption</summary>Table 6: Evaluation of Coordinate Format on Segmentation Performance.</details></blockquote></details><h3 class="relative group">Full paper<div id=full-paper class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#full-paper aria-label=Anchor>#</a></span></h3><div id=gallery-d83f2a4f8c4e653cb48ec8ed4a091af4 class=gallery><img src=https://ai-paper-reviewer.com/2503.08625/1.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.08625/2.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.08625/3.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.08625/4.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.08625/5.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.08625/6.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.08625/7.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.08625/8.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.08625/9.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.08625/10.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.08625/11.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.08625/12.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.08625/13.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.08625/14.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.08625/15.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.08625/16.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.08625/17.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.08625/18.png class="grid-w50 md:grid-w33 xl:grid-w25"></div></div><section class="flex flex-row flex-wrap justify-center pt-4 text-xl"><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.08625/&amp;title=SegAgent:%20Exploring%20Pixel%20Understanding%20Capabilities%20in%20MLLMs%20by%20Imitating%20Human%20Annotator%20Trajectories" title="Share on LinkedIn" aria-label="Share on LinkedIn"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://twitter.com/intent/tweet/?url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.08625/&amp;text=SegAgent:%20Exploring%20Pixel%20Understanding%20Capabilities%20in%20MLLMs%20by%20Imitating%20Human%20Annotator%20Trajectories" title="Tweet on Twitter" aria-label="Tweet on Twitter"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="mailto:?body=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.08625/&amp;subject=SegAgent:%20Exploring%20Pixel%20Understanding%20Capabilities%20in%20MLLMs%20by%20Imitating%20Human%20Annotator%20Trajectories" title="Send via email" aria-label="Send via email"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1-27.64 140.9 68.65 266.2 199.1 285.1 19.01 2.888 36.17-12.26 36.17-31.49l1e-4-.6631c0-15.74-11.44-28.88-26.84-31.24-84.35-12.98-149.2-86.13-149.2-174.2.0-102.9 88.61-185.5 193.4-175.4 91.54 8.869 158.6 91.25 158.6 183.2v16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98.0036c-7.299.0-13.2 4.992-15.12 11.68-24.85-12.15-54.24-16.38-86.06-5.106-38.75 13.73-68.12 48.91-73.72 89.64-9.483 69.01 43.81 128 110.9 128 26.44.0 50.43-9.544 69.59-24.88 24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3c0-149.2-133.9-265.632-287.3-235.57zM239.1 304.3c-26.47.0-48-21.56-48-48.05s21.53-48.05 48-48.05 48 21.56 48 48.05-20.6 48.05-48 48.05z"/></svg></span></a></section></div><script>var oid="views_paper-reviews/2503.08625/index.md",oid_likes="likes_paper-reviews/2503.08625/index.md"</script><script type=text/javascript src=/ai-paper-reviewer/js/page.min.0860cf4e04fa2d72cc33ddba263083464d48f67de06114529043cb4623319efed4f484fd7f1730df5abea0e2da6f3538855634081d02f2d6e920b956f063e823.js integrity="sha512-CGDPTgT6LXLMM926JjCDRk1I9n3gYRRSkEPLRiMxnv7U9IT9fxcw31q+oOLabzU4hVY0CB0C8tbpILlW8GPoIw=="></script></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="flex group mr-3" href=/ai-paper-reviewer/paper-reviews/2503.08605/><span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&larr;</span>
<span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Tuning-Free Multi-Event Long Video Generation via Synchronized Coupled Sampling</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2025-03-11T00:00:00+00:00>11 March 2025</time>
</span></span></a></span><span><a class="flex text-right group ml-3" href=/ai-paper-reviewer/paper-reviews/2503.08507/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Referring to Any Person</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2025-03-11T00:00:00+00:00>11 March 2025</time>
</span></span><span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&larr;</span></a></span></div></div><div class=pt-3><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class=pt-3><script src=https://utteranc.es/client.js repo=pmnxis/pmnxis.github.io issue-term=pathname label=Comment theme=dark-blue crossorigin=anonymous async></script></div></div></footer></article><div id=top-scroller class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer id=site-footer class="py-10 print:hidden"><nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400"><ul class="flex flex-col list-none sm:flex-row"><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=/ai-paper-reviewer/tags/ title>Tags</a></li><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=https://deep-diver.github.io/neurips2024/ title>NeurIPS2024</a></li></ul></nav><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2025
Hugging Face Daily Papers</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a></p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/ai-paper-reviewer/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh+sCQ0E53ghYrxgYqw+0GCRyIEpA=="></script></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://deep-diver.github.io/ai-paper-reviewer/ style=z-index:500><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body><script data-name=BMC-Widget data-cfasync=false src=https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js data-id=chansung data-description="Support me on Buy me a coffee!" data-message data-color=#FFDD00 data-position=Left data-x_margin=18 data-y_margin=18></script></html>