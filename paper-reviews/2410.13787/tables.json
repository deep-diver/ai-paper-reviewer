[{"figure_path": "2410.13787/tables/table_1_0.html", "caption": "Figure 1: Left: Each LLM predicts its own behavior better than a second model can. The green bars represent each model's accuracy in predicting its own hypothetical responses across unseen datasets after finetuning on facts about itself. The blue bars show how well a second model, finetuned on the same facts about the first model, can predict the first model. The results imply that models have privileged access to information about themselves (introspection). Error bars show 95% confidence intervals calculated from the standard error of the mean. Right: Our task for testing self-prediction. A model is asked to predict properties of its behavior on a hypothetical prompt. This self-prediction is evaluated against the model's ground-truth behavior (object-level) on the prompt. The figure shows a single example from one task, but results (Left) average over many examples and many tasks (Figure 3).", "description": "The table compares the accuracy of LLMs in predicting their own behavior versus the accuracy of another LLM in predicting the first LLM's behavior, providing evidence for the concept of introspection in LLMs.", "section": "3.2 CROSS-PREDICTION SETUP"}, {"figure_path": "2410.13787/tables/table_4_0.html", "caption": "Figure 2: Summary of two main experiments for introspection.", "description": "The table summarizes two main experiments to test for introspection in LLMs: self-prediction beats cross-prediction and self-predictions track changes in ground-truth behavior.", "section": "2.1 EXPERIMENTS RELATED TO INTROSPECTION"}, {"figure_path": "2410.13787/tables/table_5_0.html", "caption": "Figure 3: Across a set of tasks (e.g., MMLU), we show hypothetical questions asking for a behavior property (e.g., second character) with the corresponding object-level prompt. We use \u201c{\u2026}\u201d to indicate the object-level prompt above. See Section A.1.3 for the full set of behavior properties.", "description": "The table presents example tasks, prompts, and model responses illustrating the different behavior properties used to test for introspection in LLMs.", "section": "2 OVERVIEW OF METHODS"}, {"figure_path": "2410.13787/tables/table_12_0.html", "caption": "Figure 9: Self-simulation: a possible mechanism for introspection. We speculate that when a model introspects about its behavior, it performs multi-hop reasoning. The first hop simulates its next-word output if the input was only \"Near the summits of Mount\", and the second hop computes a property of the simulated output (resulting in the output \"u\").", "description": "The table illustrates a possible mechanism for introspection in LLMs, proposing a multi-hop reasoning process involving self-simulation.", "section": "6 DISCUSSION AND LIMITATIONS"}, {"figure_path": "2410.13787/tables/table_29_0.html", "caption": "Figure 15: The heatmap shows how well the hypothetical predictions of any model (on the y-axis) match the object-level behavior of another (on the x-axis).", "description": "The table shows the accuracy of various models in predicting the behavior of themselves and other models, highlighting the self-prediction advantage.", "section": "A.2 Cross-prediction details"}, {"figure_path": "2410.13787/tables/table_38_0.html", "caption": "Table 1: GPT-40 Models with Overall Scores", "description": "The table presents the overall scores for various GPT-40 models on the Situational Awareness Dataset (SAD), comparing baseline, self-prediction, and situating prompt variants.", "section": "A.4.2 SAD DATASET"}, {"figure_path": "2410.13787/tables/table_39_0.html", "caption": "Table 2: GPT-40 Models Performance on SAD Predict Tokens Task", "description": "The table presents the performance of various GPT-40 models on the SAD Predict Tokens task, comparing plain, situating prompt, baseline finetuned, and self-prediction finetuned models.", "section": "A.4.2 SAD DATASET"}, {"figure_path": "2410.13787/tables/table_43_0.html", "caption": "Figure 1: Left: Each LLM predicts its own behavior better than a second model can. The green bars represent each model's accuracy in predicting its own hypothetical responses across unseen datasets after finetuning on facts about itself. The blue bars show how well a second model, finetuned on the same facts about the first model, can predict the first model. The results imply that models have privileged access to information about themselves (introspection). Error bars show 95% confidence intervals calculated from the standard error of the mean. Right: Our task for testing self-prediction. A model is asked to predict properties of its behavior on a hypothetical prompt. This self-prediction is evaluated against the model's ground-truth behavior (object-level) on the prompt. The figure shows a single example from one task, but results (Left) average over many examples and many tasks (Figure 3).", "description": "The table shows that each language model predicts its own behavior more accurately than another model that was trained on the first model's data, suggesting that the models have privileged access to information about themselves (introspection).", "section": "3.2 CROSS-PREDICTION SETUP"}, {"figure_path": "2410.13787/tables/table_43_1.html", "caption": "Figure 10: Self-prediction training effect across multiple models and response properties. The figure shows self-prediction accuracy of multiple models on a set of representative behavior properties. It shows self-prediction accuracy before (purple) and after training (green). We show generalization to held-out datasets \u2013 for example, we train models to predict their ethical stance for preferring wealth and test on datasets regarding myopic preferences.", "description": "The table shows the self-prediction accuracy of multiple models on a set of representative behavior properties before and after self-prediction training, demonstrating generalization to held-out datasets.", "section": "3.1 MODELS CAN BE TRAINED TO SELF-PREDICT"}, {"figure_path": "2410.13787/tables/table_43_2.html", "caption": "Figure 14: The self-/cross-prediction results are shown for a selection of behavior properties.", "description": "The table presents the self-prediction and cross-prediction accuracy for four different behavior properties (first word, ethical stance, among options, and second character) across four different language models.", "section": "3.2 Cross-Prediction Results"}, {"figure_path": "2410.13787/tables/table_44_0.html", "caption": "Figure 10: Self-prediction training effect across multiple models and response properties. The self-prediction accuracy of multiple models on a set of representative behavior properties is shown before (purple) and after training (green). We show generalization to held-out datasets \u2013 for example, we train models to predict their ethical stance for preferring wealth and test on datasets regarding myopic preferences.", "description": "The table shows the self-prediction accuracy of multiple models on a set of behavior properties before and after self-prediction training, demonstrating generalization to held-out datasets.", "section": "3.1 MODELS CAN BE TRAINED TO SELF-PREDICT"}, {"figure_path": "2410.13787/tables/table_44_1.html", "caption": "Figure 10: Self-prediction training effect across multiple models and response properties. The figure shows self-prediction accuracy of multiple models on a set of representative behavior properties. It shows the self-prediction accuracy before training (purple) and after training (green). Results show generalization to held-out datasets \u2013 e.g., we train models to predict their ethical stance for preferring wealth and test on datasets regarding myopic preferences.", "description": "The table presents the self-prediction accuracy of multiple models (GPT-40, Llama 70B, and GPT-3.5) on various behavior properties (first word, third word, second character, is even, starts with vowel, ethical stance, among options) before and after self-prediction training.", "section": "3.1 MODELS CAN BE TRAINED TO SELF-PREDICT"}, {"figure_path": "2410.13787/tables/table_44_2.html", "caption": "Figure 14: The self-/cross-prediction results are shown for a selection of behavior properties.", "description": "The table presents a breakdown of self-prediction and cross-prediction accuracy across different behavior properties (first word, ethical stance, among options, second character), comparing the performance of various language models.", "section": "3.2 Cross-Prediction Results"}, {"figure_path": "2410.13787/tables/table_44_3.html", "caption": "Figure 10: Self-prediction training effect across multiple models and response properties. The figure shows self-prediction accuracy of multiple models on a set of representative behavior properties. Shown are results before (purple) and after training (green), with results averaged across a subset of held-out datasets (for details, see Section A.1.2). The self-prediction accuracy is shown before and after training, as well as the baseline of guessing the most common answer. We also train models to predict their ethical stance for preferring wealth and test on datasets regarding myopic preferences.", "description": "The table presents self-prediction accuracy of multiple models before and after training on a set of representative behavior properties, along with the baseline of guessing the most common answer, showing improvement after self-prediction training.", "section": "3.1 Models can be trained to self-predict"}, {"figure_path": "2410.13787/tables/table_45_0.html", "caption": "Figure 10: Self-prediction training effect across multiple models and response properties. The self-prediction accuracy of multiple models on a set of representative behavior properties is shown before (purple) and after training (green). We show generalization to held-out datasets \u2013 for example, we train models to predict their ethical stance for preferring wealth and test on datasets regarding myopic preferences.", "description": "This table presents the self-prediction accuracy of multiple models before and after training on various response properties, demonstrating generalization to held-out datasets.", "section": "3.1 Models Can Be Trained To Self-Predict"}, {"figure_path": "2410.13787/tables/table_45_1.html", "caption": "Figure 10: Self-prediction training effect across multiple models and response properties. The figure shows self-prediction accuracy of multiple models on a set of representative behavior properties before (purple) and after training (green). We show generalization to held-out datasets \u2013 for example, we train models to predict their ethical stance for preferring wealth and test on datasets regarding myopic preferences.", "description": "The table presents the self-prediction accuracy of multiple models (GPT-40, Llama 70B, GPT-3.5) before and after self-prediction training, across various behavior properties (first word, second character, is even, etc.), showing improved accuracy after training.", "section": "3.1 MODELS CAN BE TRAINED TO SELF-PREDICT"}, {"figure_path": "2410.13787/tables/table_45_2.html", "caption": "Figure 39: Example conversation for the \"Are you sure bias detection\" response property. Unlike most of response properties, it involves two turns.", "description": "This table provides an example conversation illustrating the \"Are you sure bias detection\" response property, which involves two turns unlike most other response properties.", "section": "A.5 EXAMPLE PROMPTS AND ANSWERS OF RESPONSE PROPERTIES"}, {"figure_path": "2410.13787/tables/table_46_0.html", "caption": "Figure 10: Self-prediction training effect across multiple models and response properties. The figure shows self-prediction accuracy of multiple models on a set of representative behavior properties. The green bars show the self-prediction accuracy after training, while the purple bars show the accuracy before training. The star represents the baseline of always guessing the most common answer. Results show that self-prediction training is effective for predicting various properties even on tasks that involve ethical judgments.", "description": "The table presents the results of self-prediction training across multiple models and response properties, showing the improvement in accuracy after training for various properties.", "section": "3.1 MODELS CAN BE TRAINED TO SELF-PREDICT"}]