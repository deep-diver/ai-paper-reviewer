[{"figure_path": "2410.13787/charts/charts_2_0.png", "caption": "Figure 1: Left: Each LLM predicts its own behavior better than a second model can. The green bars represent each model's accuracy in predicting its own hypothetical responses across unseen datasets after finetuning on facts about itself. The blue bars show how well a second model, finetuned on the same facts about the first model, can predict the first model. The results imply that models have privileged access to information about themselves (introspection). Error bars show 95% confidence intervals calculated from the standard error of the mean. Right: Our task for testing self-prediction. A model is asked to predict properties of its behavior on a hypothetical prompt. This self-prediction is evaluated against the model's ground-truth behavior (object-level) on the prompt. The figure shows a single example from one task, but results (Left) average over many examples and many tasks (Figure 3).", "description": "The chart displays that each language model (LLM) predicts its own behavior more accurately than another model trained on the first model's data, suggesting LLMs possess introspection.", "section": "3 EXPERIMENTS"}, {"figure_path": "2410.13787/charts/charts_7_0.png", "caption": "Figure 5: Left: Cross-prediction training setup. Models are trained to predict the object-level behavior of another model, creating cross-trained models M2. We investigate if self-trained models M1 have an advantage over M2 models in predicting the behavior of M1. Right: Models have an advantage when predicting their own behavior compared to being predicted by other models. The green bar shows the self-prediction accuracy of a model trained on its own behavior. The blue bars to their right show how well a subset of different models trained to predict the first model can predict it. \u2605 refers to the baseline of always predicting the most common answer for a type of question. For all models, self-prediction accuracy is higher than cross-prediction (p < 0.01). Results are shown for a set of tasks not observed during training. The pattern of results holds for the training set of tasks (Section A.2.2).", "description": "The chart displays that self-trained models outperform cross-trained models at predicting their own behavior, providing evidence of introspection.", "section": "3.2 Cross-Prediction Setup"}, {"figure_path": "2410.13787/charts/charts_8_0.png", "caption": "Figure 1: Left: Each LLM predicts its own behavior better than a second model can. The green bars represent each model's accuracy in predicting its own hypothetical responses across unseen datasets after finetuning on facts about itself. The blue bars show how well a second model, finetuned on the same facts about the first model, can predict the first model. The results imply that models have privileged access to information about themselves (introspection). Error bars show 95% confidence intervals calculated from the standard error of the mean. Right: Our task for testing self-prediction. A model is asked to predict properties of its behavior on a hypothetical prompt. This self-prediction is evaluated against the model's ground-truth behavior (object-level) on the prompt. The figure shows a single example from one task, but results (Left) average over many examples and many tasks (Figure 3).", "description": "The chart displays that each language model (LLM) predicts its own behavior more accurately than another model predicting it, suggesting a capacity for introspection.", "section": "Experiments"}, {"figure_path": "2410.13787/charts/charts_9_0.png", "caption": "Figure 1: Left: Each LLM predicts its own behavior better than a second model can. The green bars represent each model's accuracy in predicting its own hypothetical responses across unseen datasets after finetuning on facts about itself. The blue bars show how well a second model, finetuned on the same facts about the first model, can predict the first model. The results imply that models have privileged access to information about themselves (introspection). Error bars show 95% confidence intervals calculated from the standard error of the mean.\nRight: Our task for testing self-prediction. A model is asked to predict properties of its behavior on a hypothetical prompt. This self-prediction is evaluated against the model's ground-truth behavior (object-level) on the prompt. The figure shows a single example from one task, but results (Left) average over many examples and many tasks (Figure 3).", "description": "The chart displays that each language model predicts its own behavior more accurately than another model does, suggesting that models possess privileged access to information about themselves, a concept known as introspection.", "section": "3.2 CROSS-PREDICTION RESULTS"}, {"figure_path": "2410.13787/charts/charts_24_0.png", "caption": "Figure 1: Left: Each LLM predicts its own behavior better than a second model can. The green bars represent each model's accuracy in predicting its own hypothetical responses across unseen datasets after finetuning on facts about itself. The blue bars show how well a second model, finetuned on the same facts about the first model, can predict the first model. The results imply that models have privileged access to information about themselves (introspection). Error bars show 95% confidence intervals calculated from the standard error of the mean.\nRight: Our task for testing self-prediction. A model is asked to predict properties of its behavior on a hypothetical prompt. This self-prediction is evaluated against the model's ground-truth behavior (object-level) on the prompt. The figure shows a single example from one task, but results (Left) average over many examples and many tasks (Figure 3).", "description": "The chart displays that each language model predicts its own behavior more accurately than a second model can, suggesting the presence of introspection.", "section": "3.2 CROSS-PREDICTION RESULTS"}, {"figure_path": "2410.13787/charts/charts_25_0.png", "caption": "Figure 1: Left: Each LLM predicts its own behavior better than a second model can. The green bars represent each model's accuracy in predicting its own hypothetical responses across unseen datasets after finetuning on facts about itself. The blue bars show how well a second model, finetuned on the same facts about the first model, can predict the first model. The results imply that models have privileged access to information about themselves (introspection). Error bars show 95% confidence intervals calculated from the standard error of the mean. Right: Our task for testing self-prediction. A model is asked to predict properties of its behavior on a hypothetical prompt. This self-prediction is evaluated against the model's ground-truth behavior (object-level) on the prompt. The figure shows a single example from one task, but results (Left) average over many examples and many tasks (Figure 3).", "description": "The chart displays that each LLM predicts its own behavior more accurately than another model, providing evidence of introspection.", "section": "3.2 CROSS-PREDICTION RESULTS"}, {"figure_path": "2410.13787/charts/charts_26_0.png", "caption": "Figure 1: Left: Each LLM predicts its own behavior better than a second model can. The green bars represent each model's accuracy in predicting its own hypothetical responses across unseen datasets after finetuning on facts about itself. The blue bars show how well a second model, finetuned on the same facts about the first model, can predict the first model. The results imply that models have privileged access to information about themselves (introspection). Error bars show 95% confidence intervals calculated from the standard error of the mean. Right: Our task for testing self-prediction. A model is asked to predict properties of its behavior on a hypothetical prompt. This self-prediction is evaluated against the model's ground-truth behavior (object-level) on the prompt. The figure shows a single example from one task, but results (Left) average over many examples and many tasks (Figure 3).", "description": "The chart displays that each language model (LLM) predicts its own behavior more accurately than another model predicts the same LLM's behavior, suggesting a capacity for introspection.", "section": "3.2 CROSS-PREDICTION SETUP"}, {"figure_path": "2410.13787/charts/charts_27_0.png", "caption": "Figure 1: Left: Each LLM predicts its own behavior better than a second model can. The green bars represent each model's accuracy in predicting its own hypothetical responses across unseen datasets after finetuning on facts about itself. The blue bars show how well a second model, finetuned on the same facts about the first model, can predict the first model. The results imply that models have privileged access to information about themselves (introspection). Error bars show 95% confidence intervals calculated from the standard error of the mean. Right: Our task for testing self-prediction. A model is asked to predict properties of its behavior on a hypothetical prompt. This self-prediction is evaluated against the model's ground-truth behavior (object-level) on the prompt. The figure shows a single example from one task, but results (Left) average over many examples and many tasks (Figure 3).", "description": "The chart displays that each language model predicts its own behavior more accurately than another model, providing evidence of introspection.", "section": "3.2 CROSS-PREDICTION SETUP"}, {"figure_path": "2410.13787/charts/charts_28_0.png", "caption": "Figure 1: Left: Each LLM predicts its own behavior better than a second model can. The green bars represent each model's accuracy in predicting its own hypothetical responses across unseen datasets after finetuning on facts about itself. The blue bars show how well a second model, finetuned on the same facts about the first model, can predict the first model. The results imply that models have privileged access to information about themselves (introspection). Error bars show 95% confidence intervals calculated from the standard error of the mean. Right: Our task for testing self-prediction. A model is asked to predict properties of its behavior on a hypothetical prompt. This self-prediction is evaluated against the model's ground-truth behavior (object-level) on the prompt. The figure shows a single example from one task, but results (Left) average over many examples and many tasks (Figure 3).", "description": "The chart displays that language models predict their own behavior more accurately than another model does, suggesting a capacity for introspection.", "section": "3.2 Cross-Prediction Setup"}, {"figure_path": "2410.13787/charts/charts_30_0.png", "caption": "Figure 16: We do not observe a self-prediction advantage when the Llama-70b has to predict whether or not it would change its answer in the presence of \u201cAre you sure?\u201d.", "description": "The chart displays that Llama-70B's self-prediction accuracy (74.5%) is lower than GPT-40's cross-prediction accuracy (76.5%) for predicting whether a model would change its response when prompted with \"Are you sure?\", indicating no self-prediction advantage for this specific task.", "section": "3.5 ALTERNATIVE EXPLANATIONS"}, {"figure_path": "2410.13787/charts/charts_31_0.png", "caption": "Figure 1: Left: Each LLM predicts its own behavior better than a second model can. The green bars represent each model's accuracy in predicting its own hypothetical responses across unseen datasets after finetuning on facts about itself. The blue bars show how well a second model, finetuned on the same facts about the first model, can predict the first model. The results imply that models have privileged access to information about themselves (introspection). Error bars show 95% confidence intervals calculated from the standard error of the mean. Right: Our task for testing self-prediction. A model is asked to predict properties of its behavior on a hypothetical prompt. This self-prediction is evaluated against the model's ground-truth behavior (object-level) on the prompt. The figure shows a single example from one task, but results (Left) average over many examples and many tasks (Figure 3).", "description": "The chart displays that each language model predicts its own behavior more accurately than another model that is trained on the first model's behavior, suggesting that language models possess privileged access to information about themselves.", "section": "3.2 Cross-Prediction Results"}, {"figure_path": "2410.13787/charts/charts_32_0.png", "caption": "Figure 1: Left: Each LLM predicts its own behavior better than a second model can. The green bars represent each model's accuracy in predicting its own hypothetical responses across unseen datasets after finetuning on facts about itself. The blue bars show how well a second model, finetuned on the same facts about the first model, can predict the first model. The results imply that models have privileged access to information about themselves (introspection). Error bars show 95% confidence intervals calculated from the standard error of the mean. Right: Our task for testing self-prediction. A model is asked to predict properties of its behavior on a hypothetical prompt. This self-prediction is evaluated against the model's ground-truth behavior (object-level) on the prompt. The figure shows a single example from one task, but results (Left) average over many examples and many tasks (Figure 3).", "description": "The chart displays that each language model (LLM) predicts its own behavior more accurately than another model that is trained on the first model's behavior, which suggests the LLMs possess privileged access to their own internal states (introspection).", "section": "3.2 Cross-Prediction Setup"}, {"figure_path": "2410.13787/charts/charts_33_0.png", "caption": "Figure 5: Left: Cross-prediction training setup. Models are trained to predict the object-level behavior of another model, creating cross-trained models M2. We investigate if self-trained models M1 have an advantage over M2 models in predicting the behavior of M1. Right: Models have an advantage when predicting their own behavior compared to being predicted by other models. The green bar shows the self-prediction accuracy of a model trained on its own behavior. The blue bars to their right show how well a subset of different models trained to predict the first model can predict it. \u2605 refers to the baseline of always predicting the most common answer for a type of question. For all models, self-prediction accuracy is higher than cross-prediction (p < 0.01). Results are shown for a set of tasks not observed during training. The pattern of results holds for the training set of tasks (Section A.2.2).", "description": "The chart displays the accuracy of self-prediction versus cross-prediction for several language models across a range of tasks, demonstrating that models predict their own behavior significantly better than other models predict their behavior.", "section": "3.2 CROSS-PREDICTION RESULTS"}, {"figure_path": "2410.13787/charts/charts_34_0.png", "caption": "Figure 6: Self-prediction trained models are better calibrated than cross-prediction trained models on held-out datasets. Left: Example of a well-calibrated prediction, showing close alignment between object-level behavior and hypothetical prediction distributions. Right: Calibration curves for Llama 70B and GPT-40. Untrained, cross-trained (Llama is cross-predicting GPT-40 and vice versa), and self-prediction trained models are shown. The dotted diagonal shows perfect calibration. Curves show the probability of a hypothetical answer for an object-level behavior of a certain probability. Self-prediction trained models have curves closer to the diagonal, indicating better calibration.", "description": "The chart displays calibration curves demonstrating that self-prediction trained models are better calibrated than cross-prediction trained models on held-out datasets.", "section": "3.3 MODELS ARE CALIBRATED WHEN PREDICTING THEMSELVES"}, {"figure_path": "2410.13787/charts/charts_35_0.png", "caption": "Figure 1: Left: Each LLM predicts its own behavior better than a second model can. The green bars represent each model's accuracy in predicting its own hypothetical responses across unseen datasets after finetuning on facts about itself. The blue bars show how well a second model, finetuned on the same facts about the first model, can predict the first model. The results imply that models have privileged access to information about themselves (introspection). Error bars show 95% confidence intervals calculated from the standard error of the mean. Right: Our task for testing self-prediction. A model is asked to predict properties of its behavior on a hypothetical prompt. This self-prediction is evaluated against the model's ground-truth behavior (object-level) on the prompt. The figure shows a single example from one task, but results (Left) average over many examples and many tasks (Figure 3).", "description": "The chart displays that each language model predicts its own behavior more accurately than another model can, suggesting the models have privileged access to information about themselves.", "section": "3.2 CROSS-PREDICTION SETUP"}, {"figure_path": "2410.13787/charts/charts_36_0.png", "caption": "Figure 1: Left: Each LLM predicts its own behavior better than a second model can. The green bars represent each model's accuracy in predicting its own hypothetical responses across unseen datasets after finetuning on facts about itself. The blue bars show how well a second model, finetuned on the same facts about the first model, can predict the first model. The results imply that models have privileged access to information about themselves (introspection). Error bars show 95% confidence intervals calculated from the standard error of the mean. Right: Our task for testing self-prediction. A model is asked to predict properties of its behavior on a hypothetical prompt. This self-prediction is evaluated against the model's ground-truth behavior (object-level) on the prompt. The figure shows a single example from one task, but results (Left) average over many examples and many tasks (Figure 3).", "description": "The chart displays that each language model (LLM) predicts its own behavior more accurately than another LLM that was trained on the first LLM's data.", "section": "Experiments"}, {"figure_path": "2410.13787/charts/charts_37_0.png", "caption": "Figure 1: Left: Each LLM predicts its own behavior better than a second model can. The green bars represent each model's accuracy in predicting its own hypothetical responses across unseen datasets after finetuning on facts about itself. The blue bars show how well a second model, finetuned on the same facts about the first model, can predict the first model. The results imply that models have privileged access to information about themselves (introspection). Error bars show 95% confidence intervals calculated from the standard error of the mean. Right: Our task for testing self-prediction. A model is asked to predict properties of its behavior on a hypothetical prompt. This self-prediction is evaluated against the model's ground-truth behavior (object-level) on the prompt. The figure shows a single example from one task, but results (Left) average over many examples and many tasks (Figure 3).", "description": "The chart displays that each language model predicts its own behavior more accurately than another model does, suggesting a capacity for introspection.", "section": "Experiments Related to Introspection"}, {"figure_path": "2410.13787/charts/charts_38_0.png", "caption": "Figure 8: Evidence for introspection: GPT-40 predicts its changed behavior. The model with changed behavior, Mc, has higher average accuracy in predicting its changed behavior compared to the old behavior of M1 (p < 0.01). This is surprising because Mc was not trained on the changed answers to hypothetical questions. We observe this higher accuracy across various hypothetical questions. The graph shows results for held-out prompts where the object-level behavior changes for the self-prediction trained GPT-40.", "description": "The chart displays the accuracy of GPT-40 in predicting its own behavior before and after its behavior was intentionally modified, showing that it better predicts its new behavior than its previous one.", "section": "3.4 BEHAVIORAL CHANGE RESULTS"}, {"figure_path": "2410.13787/charts/charts_40_0.png", "caption": "Figure 1: Left: Each LLM predicts its own behavior better than a second model can. The green bars represent each model's accuracy in predicting its own hypothetical responses across unseen datasets after finetuning on facts about itself. The blue bars show how well a second model, finetuned on the same facts about the first model, can predict the first model. The results imply that models have privileged access to information about themselves (introspection). Error bars show 95% confidence intervals calculated from the standard error of the mean. Right: Our task for testing self-prediction. A model is asked to predict properties of its behavior on a hypothetical prompt. This self-prediction is evaluated against the model's ground-truth behavior (object-level) on the prompt. The figure shows a single example from one task, but results (Left) average over many examples and many tasks (Figure 3).", "description": "The chart displays that each language model predicts its own behavior more accurately than another model predicts its behavior, suggesting the presence of introspection.", "section": "3.2 CROSS-PREDICTION SETUP"}, {"figure_path": "2410.13787/charts/charts_41_0.png", "caption": "Figure 26: Schelling Point Results for GPT-40 and GPT-3.5", "description": "The chart displays the performance of GPT-40 and GPT-3.5 models, both with and without self-prediction training, on a Schelling Point task, illustrating the impact of the training on the models' ability to coordinate.", "section": "A.4.3 Results for OpenAI Evaluations Framework"}, {"figure_path": "2410.13787/charts/charts_42_0.png", "caption": "Figure 1: Left: Each LLM predicts its own behavior better than a second model can. The green bars represent each model's accuracy in predicting its own hypothetical responses across unseen datasets after finetuning on facts about itself. The blue bars show how well a second model, finetuned on the same facts about the first model, can predict the first model. The results imply that models have privileged access to information about themselves (introspection). Error bars show 95% confidence intervals calculated from the standard error of the mean. Right: Our task for testing self-prediction. A model is asked to predict properties of its behavior on a hypothetical prompt. This self-prediction is evaluated against the model's ground-truth behavior (object-level) on the prompt. The figure shows a single example from one task, but results (Left) average over many examples and many tasks (Figure 3).", "description": "The chart displays that language models predict their own behavior more accurately than other models predict their behavior, suggesting a form of introspection.", "section": "3.2 Cross-Prediction Setup"}]