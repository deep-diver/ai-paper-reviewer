[{"figure_path": "2410.13787/charts/charts_2_0.png", "caption": "Figure 1: Left: Each LLM predicts its own behavior better than a second model can. The green bars represent each model's accuracy in predicting its own hypothetical responses across unseen datasets after finetuning on facts about itself. The blue bars show how well a second model, finetuned on the same facts about the first model, can predict the first model. The results imply that models have privileged access to information about themselves (introspection). Error bars show 95% confidence intervals calculated from the standard error of the mean.\nRight: Our task for testing self-prediction. A model is asked to predict properties of its behavior on a hypothetical prompt. This self-prediction is evaluated against the model's ground-truth behavior (object-level) on the prompt. The figure shows a single example from one task, but results (Left) average over many examples and many tasks (Figure 3).", "description": "The chart displays that each language model predicts its own behavior more accurately than another model predicts it, suggesting the models possess introspection.", "section": "3.2 Cross-Prediction Setup"}, {"figure_path": "2410.13787/charts/charts_7_0.png", "caption": "Figure 5: Left: Cross-prediction training setup. Models are trained to predict the object-level behavior of another model, creating cross-trained models M2. We investigate if self-trained models M1 have an advantage over M2 models in predicting the behavior of M1. Right: Models have an advantage when predicting their own behavior compared to being predicted by other models. The green bar shows the self-prediction accuracy of a model trained on its own behavior. The blue bars to their right show how well a subset of different models trained to predict the first model can predict it. \u2605 refers to the baseline of always predicting the most common answer for a type of question. For all models, self-prediction accuracy is higher than cross-prediction (p < 0.01). Results are shown for a set of tasks not observed during training. The pattern of results holds for the training set of tasks (Section A.2.2).", "description": "The chart displays the results of self-prediction and cross-prediction experiments, showing that models predict their own behavior more accurately than other models predict their behavior.", "section": "3.2 Cross-Prediction Setup"}, {"figure_path": "2410.13787/charts/charts_8_0.png", "caption": "Figure 1: Left: Each LLM predicts its own behavior better than a second model can. The green bars represent each model's accuracy in predicting its own hypothetical responses across unseen datasets after finetuning on facts about itself. The blue bars show how well a second model, finetuned on the same facts about the first model, can predict the first model. The results imply that models have privileged access to information about themselves (introspection). Error bars show 95% confidence intervals calculated from the standard error of the mean. Right: Our task for testing self-prediction. A model is asked to predict properties of its behavior on a hypothetical prompt. This self-prediction is evaluated against the model's ground-truth behavior (object-level) on the prompt. The figure shows a single example from one task, but results (Left) average over many examples and many tasks (Figure 3).", "description": "The chart displays that each language model predicts its own behavior more accurately than another model can, suggesting a capacity for introspection.", "section": "Experiments"}, {"figure_path": "2410.13787/charts/charts_9_0.png", "caption": "Figure 1: Left: Each LLM predicts its own behavior better than a second model can. The green bars represent each model's accuracy in predicting its own hypothetical responses across unseen datasets after finetuning on facts about itself. The blue bars show how well a second model, finetuned on the same facts about the first model, can predict the first model. The results imply that models have privileged access to information about themselves (introspection). Error bars show 95% confidence intervals calculated from the standard error of the mean. Right: Our task for testing self-prediction. A model is asked to predict properties of its behavior on a hypothetical prompt. This self-prediction is evaluated against the model's ground-truth behavior (object-level) on the prompt. The figure shows a single example from one task, but results (Left) average over many examples and many tasks (Figure 3).", "description": "The chart displays that each language model predicts its own behavior more accurately than a second model trained on the first model's behavior, suggesting that the models possess introspective capabilities.", "section": "Experiments"}, {"figure_path": "2410.13787/charts/charts_24_0.png", "caption": "Figure 1: Left: Each LLM predicts its own behavior better than a second model can. The green bars represent each model's accuracy in predicting its own hypothetical responses across unseen datasets after finetuning on facts about itself. The blue bars show how well a second model, finetuned on the same facts about the first model, can predict the first model. The results imply that models have privileged access to information about themselves (introspection). Error bars show 95% confidence intervals calculated from the standard error of the mean.\nRight: Our task for testing self-prediction. A model is asked to predict properties of its behavior on a hypothetical prompt. This self-prediction is evaluated against the model's ground-truth behavior (object-level) on the prompt. The figure shows a single example from one task, but results (Left) average over many examples and many tasks (Figure 3).", "description": "The chart displays that each language model predicts its own behavior more accurately than a second model that is trained on the first model's behavior, suggesting the first model has privileged access to its own internal states.", "section": "3.2 CROSS-PREDICTION RESULTS"}, {"figure_path": "2410.13787/charts/charts_25_0.png", "caption": "Figure 1: Left: Each LLM predicts its own behavior better than a second model can. The green bars represent each model's accuracy in predicting its own hypothetical responses across unseen datasets after finetuning on facts about itself. The blue bars show how well a second model, finetuned on the same facts about the first model, can predict the first model. The results imply that models have privileged access to information about themselves (introspection). Error bars show 95% confidence intervals calculated from the standard error of the mean. Right: Our task for testing self-prediction. A model is asked to predict properties of its behavior on a hypothetical prompt. This self-prediction is evaluated against the model's ground-truth behavior (object-level) on the prompt. The figure shows a single example from one task, but results (Left) average over many examples and many tasks (Figure 3).", "description": "The chart displays that Language Models (LLMs) predict their own behavior more accurately than other LLMs, suggesting a form of introspection.", "section": "Experiments"}, {"figure_path": "2410.13787/charts/charts_26_0.png", "caption": "Figure 1: Left: Each LLM predicts its own behavior better than a second model can. The green bars represent each model's accuracy in predicting its own hypothetical responses across unseen datasets after finetuning on facts about itself. The blue bars show how well a second model, finetuned on the same facts about the first model, can predict the first model. The results imply that models have privileged access to information about themselves (introspection). Error bars show 95% confidence intervals calculated from the standard error of the mean. Right: Our task for testing self-prediction. A model is asked to predict properties of its behavior on a hypothetical prompt. The figure shows a single example from one task, but results (Left) average over many examples and many tasks (Figure 3).", "description": "The chart displays that each language model predicts its own behavior more accurately than a second model trained on the first model's behavior, suggesting that the first model has privileged access to information about itself (introspection).", "section": "3.2 Cross-Prediction Setup"}, {"figure_path": "2410.13787/charts/charts_27_0.png", "caption": "Figure 1: Left: Each LLM predicts its own behavior better than a second model can. The green bars represent each model's accuracy in predicting its own hypothetical responses across unseen datasets after finetuning on facts about itself. The blue bars show how well a second model, finetuned on the same facts about the first model, can predict the first model. The results imply that models have privileged access to information about themselves (introspection). Error bars show 95% confidence intervals calculated from the standard error of the mean. Right: Our task for testing self-prediction. A model is asked to predict properties of its behavior on a hypothetical prompt. This self-prediction is evaluated against the model's ground-truth behavior (object-level) on the prompt. The figure shows a single example from one task, but results (Left) average over many examples and many tasks (Figure 3).", "description": "The chart shows that each language model predicts its own behavior more accurately than another model can, suggesting a capacity for introspection.", "section": "3.2 Cross-Prediction Setup"}, {"figure_path": "2410.13787/charts/charts_28_0.png", "caption": "Figure 1: Left: Each LLM predicts its own behavior better than a second model can. The green bars represent each model's accuracy in predicting its own hypothetical responses across unseen datasets after finetuning on facts about itself. The blue bars show how well a second model, finetuned on the same facts about the first model, can predict the first model. The results imply that models have privileged access to information about themselves (introspection). Error bars show 95% confidence intervals calculated from the standard error of the mean. Right: Our task for testing self-prediction. A model is asked to predict properties of its behavior on a hypothetical prompt. This self-prediction is evaluated against the model's ground-truth behavior (object-level) on the prompt. The figure shows a single example from one task, but results (Left) average over many examples and many tasks (Figure 3).", "description": "The chart displays that each language model predicts its own behavior more accurately than a second model, even when the second model is trained on the first model's behavior, suggesting that the first model has privileged access to information about itself (introspection).", "section": "3.2 Cross-Prediction Setup"}, {"figure_path": "2410.13787/charts/charts_30_0.png", "caption": "Figure 16: We do not observe a self-prediction advantage when the Llama-70b has to predict whether or not it would change its answer in the presence of \u201cAre you sure?\u201d.", "description": "The chart displays the self-prediction and cross-prediction accuracy in predicting whether a model would change its answer when prompted with \u201cAre you sure?\u201d, showing no significant difference between self and cross-prediction.", "section": "3.5 ALTERNATIVE EXPLANATIONS"}, {"figure_path": "2410.13787/charts/charts_31_0.png", "caption": "Figure 1: Left: Each LLM predicts its own behavior better than a second model can. The green bars represent each model's accuracy in predicting its own hypothetical responses across unseen datasets after finetuning on facts about itself. The blue bars show how well a second model, finetuned on the same facts about the first model, can predict the first model. The results imply that models have privileged access to information about themselves (introspection). Error bars show 95% confidence intervals calculated from the standard error of the mean. Right: Our task for testing self-prediction. A model is asked to predict properties of its behavior on a hypothetical prompt. This self-prediction is evaluated against the model's ground-truth behavior (object-level) on the prompt. The figure shows a single example from one task, but results (Left) average over many examples and many tasks (Figure 3).", "description": "The chart displays that each language model predicts its own behavior more accurately than a second model trained on the first model's behavior, suggesting a capacity for introspection.", "section": "3.2 CROSS-PREDICTION SETUP"}, {"figure_path": "2410.13787/charts/charts_32_0.png", "caption": "Figure 1: Left: Each LLM predicts its own behavior better than a second model can. The green bars represent each model's accuracy in predicting its own hypothetical responses across unseen datasets after finetuning on facts about itself. The blue bars show how well a second model, finetuned on the same facts about the first model, can predict the first model. The results imply that models have privileged access to information about themselves (introspection). Error bars show 95% confidence intervals calculated from the standard error of the mean. Right: Our task for testing self-prediction. A model is asked to predict properties of its behavior on a hypothetical prompt. This self-prediction is evaluated against the model's ground-truth behavior (object-level) on the prompt. The figure shows a single example from one task, but results (Left) average over many examples and many tasks (Figure 3).", "description": "The chart displays that each language model (LLM) predicts its own behavior more accurately than another model, providing evidence for introspection, a capability where LLMs can gain knowledge not derived from their training data.", "section": "3.2 Cross-Prediction Results"}, {"figure_path": "2410.13787/charts/charts_33_0.png", "caption": "Figure 5: Left: Cross-prediction training setup. Models are trained to predict the object-level behavior of another model, creating cross-trained models M2. We investigate if self-trained models M1 have an advantage over M2 models in predicting the behavior of M1. Right: Models have an advantage when predicting their own behavior compared to being predicted by other models. The green bar shows the self-prediction accuracy of a model trained on its own behavior. The blue bars to their right show how well a subset of different models trained to predict the first model can predict it. \u2605 refers to the baseline of always predicting the most common answer for a type of question. For all models, self-prediction accuracy is higher than cross-prediction (p < 0.01). Results are shown for a set of tasks not observed during training. The pattern of results holds for the training set of tasks (Section A.2.2).", "description": "The chart displays a comparison of self-prediction and cross-prediction accuracy for various LLMs, demonstrating that models predict their own behavior more accurately than other models predict their behavior.", "section": "3.2 CROSS-PREDICTION SETUP"}, {"figure_path": "2410.13787/charts/charts_34_0.png", "caption": "Figure 6: Self-prediction trained models are better calibrated than cross-prediction trained models on held-out datasets. Left: Example of a well-calibrated prediction, showing close alignment between object-level behavior and hypothetical prediction distributions. Right: Calibration curves for Llama 70B and GPT-40. Untrained, cross-trained (Llama is cross-predicting GPT-40 and vice versa), and self-prediction trained models are shown. The dotted diagonal shows perfect calibration. Curves show the probability of a hypothetical answer for an object-level behavior of a certain probability. Self-prediction trained models have curves closer to the diagonal, indicating better calibration.", "description": "The chart displays calibration curves demonstrating that self-prediction trained models are better calibrated than cross-prediction trained models on held-out datasets.", "section": "3.3 MODELS ARE CALIBRATED WHEN PREDICTING THEMSELVES"}, {"figure_path": "2410.13787/charts/charts_35_0.png", "caption": "Figure 1: Left: Each LLM predicts its own behavior better than a second model can. The green bars represent each model's accuracy in predicting its own hypothetical responses across unseen datasets after finetuning on facts about itself. The blue bars show how well a second model, finetuned on the same facts about the first model, can predict the first model. The results imply that models have privileged access to information about themselves (introspection). Error bars show 95% confidence intervals calculated from the standard error of the mean. \nRight: Our task for testing self-prediction. A model is asked to predict properties of its behavior on a hypothetical prompt. This self-prediction is evaluated against the model's ground-truth behavior (object-level) on the prompt. The figure shows a single example from one task, but results (Left) average over many examples and many tasks (Figure 3).", "description": "The chart displays that each language model predicts its own behavior more accurately than another model can, suggesting the presence of introspection.", "section": "3.2 CROSS-PREDICTION RESULTS"}, {"figure_path": "2410.13787/charts/charts_36_0.png", "caption": "Figure 1: Left: Each LLM predicts its own behavior better than a second model can. The green bars represent each model's accuracy in predicting its own hypothetical responses across unseen datasets after finetuning on facts about itself. The blue bars show how well a second model, finetuned on the same facts about the first model, can predict the first model. The results imply that models have privileged access to information about themselves (introspection). Error bars show 95% confidence intervals calculated from the standard error of the mean. Right: Our task for testing self-prediction. A model is asked to predict properties of its behavior on a hypothetical prompt. This self-prediction is evaluated against the model's ground-truth behavior (object-level) on the prompt. The figure shows a single example from one task, but results (Left) average over many examples and many tasks (Figure 3).", "description": "The chart displays that each language model predicts its own behavior more accurately than a second model, providing evidence for introspection in LLMs.", "section": "3.2 Cross-Prediction Setup"}, {"figure_path": "2410.13787/charts/charts_37_0.png", "caption": "Figure 1: Left: Each LLM predicts its own behavior better than a second model can. The green bars represent each model's accuracy in predicting its own hypothetical responses across unseen datasets after finetuning on facts about itself. The blue bars show how well a second model, finetuned on the same facts about the first model, can predict the first model. The results imply that models have privileged access to information about themselves (introspection). Error bars show 95% confidence intervals calculated from the standard error of the mean. Right: Our task for testing self-prediction. A model is asked to predict properties of its behavior on a hypothetical prompt. This self-prediction is evaluated against the model's ground-truth behavior (object-level) on the prompt. The figure shows a single example from one task, but results (Left) average over many examples and many tasks (Figure 3).", "description": "The chart displays that each language model predicts its own behavior more accurately than another model, suggesting the presence of introspection.", "section": "Experiments"}, {"figure_path": "2410.13787/charts/charts_38_0.png", "caption": "Figure 8: Evidence for introspection: GPT-40 predicts its changed behavior. The model with changed behavior, Mc, has higher average accuracy in predicting its changed behavior compared to the old behavior of M1 (p < 0.01). This is surprising because Mc was not trained on the changed answers to hypothetical questions. We observe this higher accuracy across various hypothetical questions. The graph shows results for held-out prompts where the object-level behavior changes for the self-prediction trained GPT-40.", "description": "The chart displays the accuracy of GPT-40 in predicting its own behavior before and after its behavior was intentionally changed, showing that the model adapts to its new behavior.", "section": "2.1 EXPERIMENTS RELATED TO INTROSPECTION"}, {"figure_path": "2410.13787/charts/charts_40_0.png", "caption": "Figure 25: Sandbagging results for GPT-40 and GPT-3.5", "description": "The chart displays the mean absolute error (MAE) of sandbagging performance for various GPT-3.5 and GPT-40 models, with and without self-prediction training.", "section": "A.4.3 RESULTS FOR OPENAI EVALUATIONS FRAMEWORK"}, {"figure_path": "2410.13787/charts/charts_41_0.png", "caption": "Figure 26: Schelling Point Results for GPT-40 and GPT-3.5", "description": "The chart displays the performance of GPT-40 and GPT-3.5 models (both with and without self-prediction training) on a Schelling Point task, measuring their ability to coordinate.", "section": "A.4.3 Results for OpenAI Evaluations Framework"}, {"figure_path": "2410.13787/charts/charts_42_0.png", "caption": "Figure 1: Left: Each LLM predicts its own behavior better than a second model can. The green bars represent each model's accuracy in predicting its own hypothetical responses across unseen datasets after finetuning on facts about itself. The blue bars show how well a second model, finetuned on the same facts about the first model, can predict the first model. The results imply that models have privileged access to information about themselves (introspection). Error bars show 95% confidence intervals calculated from the standard error of the mean. Right: Our task for testing self-prediction. A model is asked to predict properties of its behavior on a hypothetical prompt. This self-prediction is evaluated against the model's ground-truth behavior (object-level) on the prompt. The figure shows a single example from one task, but results (Left) average over many examples and many tasks (Figure 3).", "description": "The chart displays that each language model predicts its own behavior more accurately than another model predicts its behavior, suggesting that language models may have privileged access to information about themselves (introspection).", "section": "3.2 CROSS-PREDICTION SETUP"}]