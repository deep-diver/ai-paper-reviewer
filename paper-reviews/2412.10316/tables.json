[{"content": "| Editing Model | Plug-and-Play | Flexible-Scale | Multi-turn Interactive | Instruction Editing |\n|---|---|---|---|---|\n| Prompt2Prompt [8] | \u2713 | \u2713 |  |  |\n| MasaCtrl [9] | \u2713 | \u2713 |  |  |\n| MagicQuill [17] | \u2713 | \u2713 | \u2713 |  |\n| InstructPix2Pix [13] |  |  |  | \u2713 |\n| GenArtist [25] | \u2713 |  |  | \u2713 |\n| *BrushEdit* | \u2713 | \u2713 | \u2713 | \u2713 |\n| Inpainting Model | Plug-and-Play | Flexible-Scale | Content-Aware | Shape-Aware |\n| Blended Diffusion [26, 27] | \u2713 |  |  |  |\n| SmartBrush [28] |  |  |  | \u2713 |\n| SD Inpainting [5] |  |  | \u2713 | \u2713 |\n| PowerPaint [29] |  |  | \u2713 | \u2713 |\n| HD-Painter [30] |  |  | \u2713 | \u2713 |\n| ReplaceAnything [31] |  |  | \u2713 | \u2713 |\n| Imagen [32] |  |  | \u2713 | \u2713 |\n| ControlNet-Inpainting [33] | \u2713 | \u2713 | \u2713 |  |\n| *BrushEdit* | \u2713 | \u2713 | \u2713 | \u2713 |", "caption": "TABLE I: Comparison of BrushEdit\u00a0with Previous Image Editing/Inpainting Methods. Note that we only list commonly used text-guided diffusion methods in this table.", "description": "This table compares BrushEdit with other image editing and inpaining methods, focusing on key features like flexible content scaling, multi-turn interactivity, instruction-based editing, and handling of various mask shapes. The comparison highlights BrushEdit's unique ability to handle all these aspects in a single, unified framework.", "section": "III. PRELIMINARIES AND MOTIVATION"}, {"content": "| Inverse | Editing | PSNR (\u2191) | LPIPS\u00d710\u00b3 (\u2193) | MSE\u00d710\u2074 (\u2193) | SSIM\u00d710\u00b2 (\u2191) | CLIP Similariy (\u2191) |\n|---|---|---|---|---|---|---| \n| **DDIM** | **P2P** | 17.87 | 208.80 | 219.88 | 71.14 | 22.44 |\n| **PnP** | **P2P** | 27.22 | 54.55 | 32.86 | 84.76 | 22.10 |\n| **DDIM** | **MasaCtrl** | 22.17 | 106.62 | 86.97 | 79.67 | 21.16 |\n| **PnP** | **MasaCtrl** | 22.64 | 87.94 | 81.09 | 81.33 | 21.35 |\n| **DDIM** | **P2P-Zero** | 20.44 | 172.22 | 144.12 | 74.67 | 20.54 |\n| **PnP** | **P2P-Zero** | 21.53 | 138.98 | 127.32 | 77.05 | 21.05 |\n| **DDIM** | **PnP** | 22.28 | 113.46 | 83.64 | 79.05 | **22.55** |\n| **PnP** | **PnP** | 22.46 | 106.06 | 80.45 | 79.68 | **22.62** |\n| *BrushEdit* |  | **32.16** | **17.22** | **8.43** | **97.08** | 22.44 |", "caption": "TABLE II: Comparison of BrushEdit\u00a0with various editing methods in PnpBench. For editing methods Prompt-to-Prompt (P2P)[8], MasaCtrl[9], Pix2Pix-Zero (P2P-Zero)[9], and Plug-and-Play (PnP)[66], we evaluate two inversion techniques, DDIM Inversion (DDIM)[2] and PnP Inversion (PnP)[11], to establish stronger baselines. Red stands for the best result, Blue stands for the second best result.", "description": "This table presents a quantitative comparison of BrushEdit with various other image editing methods using the PnPBench dataset. The comparison includes inversion-based methods like Prompt-to-Prompt, MasaCtrl, Pix2Pix-Zero, and Plug-and-Play, each evaluated with two inversion techniques (DDIM and PnP). The metrics used for evaluation are PSNR, LPIPS, MSE, SSIM, and CLIP Similarity.  The best results are highlighted in red, and the second-best results are highlighted in blue, demonstrating BrushEdit's superior performance in preserving unedited regions and ensuring accurate text alignment in edited areas.", "section": "V. Experiments"}, {"content": "| Methods | *BrushEdit* | NP | EF | AIDI | EDICT | NT | Style Diffusion |\n|---|---|---|---|---|---|---|---| \n| Inference Time (s) | **3.57** | **18.22** | 19.10 | 35.41 | 35.48 | 148.48 | 382.98 |", "caption": "TABLE III: Comparison of inference time between our inpainting-based BrushEdit\u00a0 and other inversion-based methods, including Negative-Prompt Inversion (NP), Edit Friendly Inversion (EF), AIDI[98], EDICT, Null-Text Inversion (NT), and Style Diffusion added with Prompt-to-Prompt. BrushEdit\u00a0achieves better editing results with far less inference time than all inversion-based methods.", "description": "This table compares the inference time of BrushEdit with other inversion-based editing methods on image editing tasks. It demonstrates that BrushEdit achieves comparable or better results while using significantly less inference time. Specifically, BrushEdit takes 3.57 seconds per image edit, while other methods range from 18.22 to 382.98 seconds.", "section": "V. Experiments"}, {"content": "|                       | Inside-inpainting |                       |       |           |           |           | Outside-inpainting |                       |       |           |           |           |           |\n| :-------------------- | :--------------- | :-------------------- | :---- | :-------- | :-------- | :-------- | :---------------- | :-------------------- | :---- | :-------- | :-------- | :-------- | :-------- |\n| **Metrics**           | **Models**        | **PSNR\u2191**           | **MSE\u00d710\u00b3\u2193** | **LPIPS\u00d710\u00b3\u2193** | **SSIM\u2191** | **CLIP Sim\u2191** | **Metrics**         | **Models**        | **PSNR\u2191**           | **MSE\u00d710\u00b3\u2193** | **LPIPS\u00d710\u00b3\u2193** | **SSIM\u2191** | **CLIP Sim\u2191** |\n|                       | **BLD (1)**       | 21.33               | 9.76          | 49.26         | 74.58     | 26.15       |                       | **BLD (1)**       | 15.85               | 35.86          | 21.40         | 77.40     | 26.73       |\n|                       | **SDI (2)**       | 21.52               | 13.87         | 48.39         | 89.07     | 26.17       |                       | **SDI (2)**       | 18.04               | 19.87         | 15.13         | 91.42     | 27.21       |\n|                       | **HDP (3)**       | 22.61               | 9.95          | 43.50         | 89.03     | 26.37       |                       | **HDP (3)**       | 18.03               | 22.99         | 15.22         | 90.48     | 26.96       |\n|                       | **PP (4)**        | 21.43               | 32.73         | 48.43         | 86.39     | **26.48**      |                       | **PP (4)**        | 18.04               | 31.78         | 15.13         | 90.11     | 26.72       |\n|                       | **CNI (5)**       | 12.39               | 78.78         | 243.62        | 65.25     | 26.47       |                       | **CNI (5)**       | 11.91               | 83.03         | 58.16         | 66.80     | **27.29**      |\n|                       | **CNI* (5)**      | 22.73               | 24.58         | 43.49         | 91.53     | 26.22       |                       | **CNI* (5)**      | 17.50               | 37.72         | 19.95         | 94.87     | 26.92       |\n|                       | **BrushNet-Seg*** | **31.94**            | **0.80**     | **18.67**    | **96.55** | 26.39       |                       | **BrushNet-Seg*** | **27.82**            | **2.25**     | **4.63**     | **98.95** | **27.22**      |\n|                       | *BrushEdit***     | **31.98**            | **0.79**     | 18.92        | **96.68** | 26.24       |                       | *BrushEdit***     | 27.65               | 2.30          | 4.90          | **98.97** | 27.29       |", "caption": "TABLE IV: Quantitative comparisons between BrushEdit\u00a0and other diffusion-based inpainting models in BrushBench: Blended Latent Diffusion (BLD)[27], Stable Diffusion Inpainting (SDI)[5], HD-Painter (HDP)[30], PowerPaint (PP)[29], ControlNet-Inpainting (CNI)[33], and our previous Segmentation-based BrushNet-Seg[22]. The table shows metrics on background fidelity and text alignment (Text Align) for both inside- and outside-inpainting. All models use Stable Diffusion V1.5 as the base model. Red indicates the best result, while Blue indicates the second-best result.", "description": "Comparison of BrushEdit and other diffusion-based inpainting models on BrushBench for inside- and outside-inpainting tasks, evaluating background fidelity and text alignment using Stable Diffusion v1.5. Metrics include PSNR, MSE, LPIPS, SSIM, and CLIP Similarity. Higher values indicate better performance for PSNR, SSIM, CLIP Similarity. Lower values indicate better performance for MSE, LPIPS.", "section": "V. Experiments"}, {"content": "| Metrics | Masked Background Fidelity | Text Align | CLIP Sim | \n|---|---|---|---|---| \n| Models | PSNR | MSE x 10^3 | LPIPS x 10^3 | SSIM x 10^3 |\n|---|---|---|---|---|---| \n| **BLD**[27] | 20.89 | 10.93 | 31.90 | 85.09 | 28.62 |\n| **SDI**[5] | 23.25 | 6.94 | 24.30 | 90.13 | 28.00 |\n| **HDP**[30] | 23.07 | **6.70** | 24.32 | 92.56 | 28.34 |\n| **PP**[29] | 23.34 | 20.12 | 24.12 | 91.49 | 27.80 |\n| **CNI**[33] | 12.71 | 69.42 | 159.71 | 79.16 | 28.16 |\n| **CNI***[33] | 22.61 | 35.93 | 26.14 | 94.05 | 27.74 |\n| **BrushNet-Ran*** | **33.66** | **0.63** | **10.12** | **98.13** | **28.87** |\n| ***BrushEdit*** | **32.97** | **0.70** | **7.24** | **98.60** | **29.62** |", "caption": "TABLE V: Quantitative comparisons among BrushEdit\u00a0and other diffusion-based inpainting models, Random-mask-based BrushNet-Ran in EditBench. A detailed explanation of compared methods and metrics can be found in the caption of Tab.\u00a0IV. Red stands for the best result, Blue stands for the second best result.", "description": "This table presents a quantitative comparison of various image inpainting models using the EditBench dataset with random masks. The comparison includes Blended Latent Diffusion (BLD), Stable Diffusion Inpainting (SDI), HD-Painter (HDP), PowerPaint (PP), ControlNet-Inpainting (CNI), BrushNet-Ran, and the proposed method BrushEdit. The table evaluates the models based on background fidelity metrics (PSNR, MSE, LPIPS, SSIM) and text alignment using CLIP Similarity. The results demonstrate that BrushEdit significantly improves the preservation of masked regions, outperforming other methods, thanks to its novel dual-branch framework and combined mask training strategy.", "section": "V. Experiments"}, {"content": "| Metrics | Image Quality | | | Masked Region Preservation | | | Text Align | CLIP Sim |\n|---|---|---|---|---|---|---|---|---| \n| Model | IR\u00d710\u2191 | HPS\u00d710\u00b2\u2191 | AS\u2191 | PSNR\u2191 | MSE\u00d710\u00b2\u2193 | LPIPS\u00d710\u00b3\u2193 | | |\n|---|---|---|---|---|---|---|---|---| \n| SDI | 11.00 | 27.53 | 6.53 | 19.78 | 16.87 | 31.76 | 26.69 | |\n| w/o fine-tune | 11.59 | 27.71 | 6.59 | 19.86 | 16.09 | 31.68 | 26.91 | |\n| w/ fine-tune | **11.63** | **27.73** | **6.60** | **20.13** | **15.84** | **31.57** | **26.93** | |", "caption": "TABLE VI: Ablation on dual-branch design. Stable Diffusion Inpainting (SDI) use single-branch design, where the entire UNet is fine-tuned. We conducted an ablation analysis by training a dual-branch model with two variations: one with the base UNet fine-tuned, and another with the base UNet forzened. Results demonstrate the superior performance achieved by adopting the dual-branch design. Red is the best result.", "description": "This table presents an ablation study on BrushEdit's dual-branch vs. single-branch design for image inpainting. Stable Diffusion Inpainting (SDI) uses a single-branch design where the entire UNet is fine-tuned.  Two dual-branch models were evaluated: one with the base UNet fine-tuned and one with the base UNet frozen.  Results show superior performance with the dual-branch design, with freezing the base UNet providing the best balance of performance and model flexibility.", "section": "V. Experiments / H. Ablation Study"}, {"content": "| Metrics | Image Quality | Masked Region Preservation | Text Align |\n|---|---|---|---|---|---|\n| Enc | Mask | Attn | UNet | Blend | IR \u00d710\u2191 | HPS \u00d710\u00b2\u2191 | AS\u2191 | PSNR\u2191 | MSE \u00d710\u00b2\u2193 | LPIPS \u00d710\u00b3\u2193 | CLIP Sim\u2191 |\n|---|---|---|---|---|---|---|---|---|---|---|---|\n| Conv | w/ | w/o | full | w/o | 11.05 | 26.23 | 6.55 | 14.89 | 37.23 | 64.54 | 26.76 |\n| VAE | w/o | w/o | full | w/o | 11.55 | 27.70 | 6.57 | 17.96 | 26.38 | 49.33 | 26.87 |\n| VAE | w/ | w/ | full | w/o | 11.25 | 27.62 | 6.56 | 18.69 | 19.44 | 34.28 | 26.63 |\n| Conv | w/ | w/ | CN | w/o | 9.58 | 26.85 | 6.47 | 12.15 | 80.91 | 150.89 | 26.88 |\n| VAE | w/ | w/ | CN | w/o | 10.53 | 27.42 | 6.59 | 18.28 | 24.36 | 41.63 | 26.89 |\n| VAE | w/ | w/o | CN | w/o | 11.42 | 27.69 | 6.58 | 18.49 | 24.09 | 36.33 | 26.86 |\n| VAE | w/ | w/o | half | w/o | 11.47 | 27.70 | 6.57 | 19.01 | 23.77 | 33.57 | 26.87 |\n| VAE | w/ | w/o | full | w/o | 11.59 | 27.71 | **6.59** | 19.86 | 16.09 | 31.68 | **26.91** |\n| VAE | w/ | w/o | full | paste | 11.72 | 27.93 | 6.58 | - | - | - | 26.80 |\n| VAE | w/ | w/o | full | blur | **11.76** | **27.94** | 6.58 | **29.88** | **1.53** | **11.65** | 26.81 |", "caption": "TABLE VII: Ablation on model architecture.\nWe ablate on the following components: the image encoder (Enc), selected from a random initialized convolution (Conv) and a VAE; the inclusion of mask in input (Mask), chosen from adding (w/) and not adding (w/o); the presence of cross-attention layers (Attn), chosen from adding (w/) and not adding (w/o); the type of UNet feature addition (UNet), selected from adding the full UNet feature (full), adding half of the UNet feature (half), and adding the feature like ControlNet (CN); and finally, the blending operation (Blend), chosen from not adding (w/o), direct pasting (paste), and blurred blending (blur). Red is the best result.", "description": "This table presents the ablation study results for different model architectures on image inpainting tasks. It investigates the impact of varying components such as the image encoder (using a randomly initialized convolutional layer or a VAE), mask inclusion, the presence of cross-attention layers, the method of UNet feature addition (full, half, or like ControlNet), and blending operation (none, direct pasting, or blurred blending).", "section": "H. Ablation Study"}]