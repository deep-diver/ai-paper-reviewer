[{"content": "| Model Size | # of STIV Blocks | Hidden Dim | # of Attn Heads |\n|---|---|---|---|\n| XL (600M) | 28 | 1,152 | 18 |\n| XXL (1.5B) | 38 | 1,536 | 24 |\n| M (8.7B) | 46 | 3,072 | 48 |", "caption": "Table 1: Model Configurations", "description": "This table presents the configurations of different STIV (Scalable Text and Image Conditioned Video Generation) models used in the paper's experiments.  The configurations include model size (in number of parameters), the number of STIV blocks, the hidden dimension size within the blocks, and the number of attention heads. These specifications are crucial for understanding the computational resources and performance characteristics associated with each model variant.", "section": "3. Recipe Study for STIV"}, {"content": "| Model | COCO FID\u2193 | COCO PICK\u2191 | COCO CLIP\u2191 | Gen Eval\u2191 | DSG Eval\u2191 | HPSv2 Eval\u2191 | Image Reward\u2191 |\n|---|---|---|---|---|---|---|---| \n| Baseline | 26.17 | 20.91 | 32.03 | 0.358 | 0.571 | 26.33 | -0.25 |\n| + QK norm | 25.60 | 20.92 | 32.08 | 0.372 | 0.574 | 26.32 | -0.22 |\n| + Sandwich norm | 25.76 | 20.97 | 32.13 | 0.366 | 0.577 | 26.32 | -0.23 |\n| + Cond. norm | 25.58 | 21.05 | 32.27 | 0.393 | 0.583 | 26.43 | -0.22 |\n| + LR to 2E-4 | 26.35 | 21.03 | 32.28 | 0.379 | 0.586 | 26.40 | -0.12 |\n| + Flow | 24.96 | 21.45 | 32.90 | 0.457 | 0.639 | 26.95 | 0.15 |\n| + Renorm | 21.16 | 21.46 | 32.93 | 0.471 | 0.668 | 27.27 | 0.32 |\n| + AdaFactor | 20.26 | 21.47 | 32.97 | 0.474 | 0.661 | 27.26 | 0.32 |\n| + MaskDiT | 23.85 | 21.51 | 33.07 | 0.499 | 0.663 | 27.28 | 0.30 |\n| + Shared AdaLN | 22.83 | 21.44 | 33.12 | 0.496 | 0.658 | 27.27 | 0.24 |\n| + Micro cond. | 20.02 | 21.50 | 33.09 | 0.498 | 0.673 | 27.27 | 0.41 |\n| + RoPE | 18.40 | 21.46 | 33.11 | 0.502 | 0.680 | 27.26 | 0.48 |\n| + Internal VAE | 19.57 | 21.79 | 33.26 | 0.492 | 0.668 | 27.26 | 0.52 |\n| + Internal CLIP | **17.97** | 21.89 | 33.62 | **0.607** | 0.717 | 27.40 | 0.65 |\n| + Synth. captions | 18.04 | **22.10** | **33.65** | **0.685** | **0.751** | **27.65** | **0.81** |", "caption": "Table 2: Text-to-image model ablation studies.", "description": "This table presents the results of an ablation study on the text-to-image model. It shows how different design choices and training techniques affect the model's performance, as measured by various metrics.  Each row represents a different model variation, building upon the previous one with an additional modification.  The metrics provide a comprehensive evaluation of the generated images, assessing different aspects like image quality, alignment with the prompt, and efficiency of generation.", "section": "3.2. Ablation Studies for Key Changes on T2I"}, {"content": "| Module | VBench |  |  | \n|---|---|---|---|\n| <span class=\"ltx_rule\" style=\"width:100%;height:1.0pt;background:black;display:inline-block;\">\u00a0</span><span class=\"ltx_text\" style=\"font-size:90%;\">Base model</span> | 80.19 | 70.51 | 78.25 |\n| <span class=\"ltx_rule\" style=\"width:100%;height:1.0pt;background:black;display:inline-block;\">\u00a0</span><span class=\"ltx_text\" style=\"font-size:90%;\">w/ temp. patch=1</span> | **80.92** | 71.69 | **79.07** |\n| <span class=\"ltx_rule\" style=\"width:100%;height:1.0pt;background:black;display:inline-block;\">\u00a0</span><span class=\"ltx_text\" style=\"font-size:90%;\">w/ temp. patch=4</span> | 79.72 | 69.15 | 77.61 |\n| <span class=\"ltx_rule\" style=\"width:100%;height:1.0pt;background:black;display:inline-block;\">\u00a0</span><span class=\"ltx_text\" style=\"font-size:90%;\">w/ causal temp._atten</span> | 74.59 | **73.13** | 74.30 |\n| <span class=\"ltx_rule\" style=\"width:100%;height:1.0pt;background:black;display:inline-block;\">\u00a0</span><span class=\"ltx_text\" style=\"font-size:90%;\">+ temp. scale_shift_gate</span> | 80.32 | 68.94 | 78.04 |\n| <span class=\"ltx_rule\" style=\"width:100%;height:1.0pt;background:black;display:inline-block;\">\u00a0</span><span class=\"ltx_text\" style=\"font-size:90%;\">+ temp. mask</span> | 77.58 | 65.95 | 75.25 |\n| <span class=\"ltx_rule\" style=\"width:100%;height:1.0pt;background:black;display:inline-block;\">\u00a0</span><span class=\"ltx_text\" style=\"font-size:90%;\">- spatial mask</span> | 80.57 | 70.31 | 78.52 |\n|  |  |  |  |", "caption": "Table 3: Ablation Study Results for Different Model Components for Text-Image-To-Video\u00a0(TI2V) task on VBench-I2V.", "description": "This table presents the ablation study results for different model components used in the Text-Image-to-Video (TI2V) task, specifically focusing on the VBench-I2V evaluation metric. It showcases the impact of various components on different aspects of video generation quality, such as subject consistency, background consistency, temporal flickering, motion smoothness, dynamic degree, aesthetic quality, image quality, and overall scores.  By comparing different model configurations, the table provides insights into which components are most critical for achieving high performance in TI2V video generation.", "section": "3.2. Ablation Studies for Key Changes on T2I"}, {"content": "| Init. | MSRVTT \u2193 | VBench Quality \u2191 | VBench Semantic \u2191 | VBench Total \u2191 |\n|---|---|---|---|---|\n| <span class=\"ltx_rule\" style=\"width:100%;height:1.0pt;background:black;display:inline-block;\">\u00a0</span> Init. | 417.98 | 80.27 | 67.84 | 77.78 |\n| <span class=\"ltx_rule\" style=\"width:100%;height:1.0pt;background:black;display:inline-block;\">\u00a0</span> Scratch | 415.63 | 80.28 | 71.29 | 78.49 |\n| <span class=\"ltx_rule\" style=\"width:100%;height:1.0pt;background:black;display:inline-block;\">\u00a0</span> T2V-256 | **401.83** | 79.77 | 71.58 | 78.13 |\n| <span class=\"ltx_rule\" style=\"width:100%;height:1.0pt;background:black;display:inline-block;\">\u00a0</span> T2I-512 | 405.14 | **80.45** | **72.37** | **78.83** |\n| <span class=\"ltx_rule\" style=\"width:100%;height:1.0pt;background:black;display:inline-block;\">\u00a0</span> Both |  |  |  |  |", "caption": "Table 4: Comparison of T2V, STIV and STIV with JIT-CFG on VBench and VBench-I2V I2V Score, Quality, Total scores.", "description": "This table presents a quantitative comparison of three different video generation models: Text-to-Video (T2V), Text-Image-to-Video (STIV), and STIV enhanced with Joint Image-Text Classifier-Free Guidance (JIT-CFG). The models are evaluated using two distinct benchmarks: VBench and VBench-I2V.  For each model and benchmark, the table provides three key metrics: Quality, I2V (Image-to-Video, specific to the VBench-I2V benchmark), and Total score. The Quality score assesses the overall quality of the generated video, while the I2V score specifically evaluates how well the generated video aligns with the provided input image (relevant only for the STIV models using VBench-I2V). The Total score represents a weighted average combining Quality and I2V scores (where applicable). This comparison facilitates the understanding of how the integration of image conditioning and the JIT-CFG technique impact the performance of video generation models across various quality aspects.", "section": "3. Recipe Study for STIV"}, {"content": "| Init. | MSRVTT | VBench (Quality \u2191) | VBench (Semantic \u2191) | VBench (Total \u2191) |\n|---|---|---|---|---|\n| <span class=\"ltx_rule\" style=\"width:100%;height:1.0pt;background:black;display:inline-block;\">\u00a0</span> <span class=\"ltx_text\" style=\"font-size:90%;\">Init.</span> | MSRVTT | FVD \u2193 | Quality \u2191 | Semantic \u2191 | Total \u2191 |\n| <span class=\"ltx_rule\" style=\"width:100%;height:1.0pt;background:black;display:inline-block;\">\u00a0</span> <span class=\"ltx_text\" style=\"font-size:90%;\">T2I</span> | 549.13 | 78.71 | 65.69 | 76.10 |\n| <span class=\"ltx_text\" style=\"font-size:90%;\">T2V (inter.)</span> | 407.86 | 79.56 | 65.42 | 76.73 |\n| <span class=\"ltx_text\" style=\"font-size:90%;\">T2V (extra.)</span> | **397.90** | 79.18 | 64.63 | 76.27 |\n| <span class=\"ltx_text\" style=\"font-size:90%;\">T2V 2x (inter.)</span> | 401.94 | **79.59** | **66.24** | **76.92** |\n| <span class=\"ltx_rule\" style=\"width:100%;height:1.0pt;background:black;display:inline-block;\">\u00a0</span> |  |  |  |  |", "caption": "Table 5: Effect of JIT-CFG on motion-related scores.", "description": "This table presents the impact of using Joint Image-Text Classifier-Free Guidance (JIT-CFG) on the motion quality of videos generated by the STIV model.  The metrics shown assess various aspects of motion, such as dynamic degree, temporal smoothness, and background consistency. By comparing the scores with and without JIT-CFG, the table demonstrates the method's effectiveness in improving the realism and coherence of the generated video's motion.", "section": "3.4 Ablation Studies on Key Designs for TI2V"}, {"content": "| Models | SubjCons | BgCons | TempSmooth | MotDeg | DynQual | AesthQual | ImgSubj | I2VSubj | I2VBg | I2VMot | CamScores | I2VAvgScores | AvgScores |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| **CA** | 82.2 | 92.8 | 95.7 | 96.3 | **42.4** | 48.8 | 65.5 | 88.9 | 90.9 | 26.9 | 68.2 | 73.0 |\n| CA + FFL | 84.5 | 95.6 | 96.1 | 96.7 | 29.7 | 48.7 | 64.7 | 91.5 | 94.7 | 17.6 | 67.2 | 72.0 |\n| CA + LP | 95.2 | **98.7** | **97.4** | 98.1 | 22.2 | 57.3 | **66.8** | 96.9 | 97.3 | 22.7 | 72.3 | 75.3 |\n| FR | 94.5 | 98.3 | 96.6 | 97.8 | 36.6 | **58.0** | 66.1 | 96.8 | 97.1 | **31.5** | **75.8** | **77.3** |\n| FR + CA | 95.1 | 98.6 | 97.0 | 98.1 | 35.4 | 58.0 | 66.2 | 96.9 | 97.3 | 28.8 | 74.4 | 77.1 |\n| FR + CA + LP | **95.3** | 98.5 | 97.3 | **98.2** | 22.4 | 57.3 | 66.3 | **97.0** | **97.4** | 25.8 | 73.4 | 75.6 |\n| FR + CA + LP + FFL | 95.2 | 98.7 | 97.4 | 98.1 | 22.2 | 57.3 | 66.8 | 96.9 | 97.3 | 22.7 | 72.3 | 75.3 |", "caption": "Table 6: Results for different model initialization on VBench-I2V.", "description": "This table presents the ablation study results of different model initialization methods on the VBench-I2V benchmark. It compares the performance metrics of TI2V models initialized from different starting points: training from scratch, initializing from a pre-trained T2I model, and initializing from a pre-trained T2V model. The metrics evaluated include subjective scores such as Subject Consistency, Background Consistency, Temporal Flickering, Motion Smoothness, Dynamic Degree, Aesthetic Quality, and Image Quality, and objective scores such as overall I2V score, which is computed as the average of I2V Subject, I2V Background, and I2V Camera Motion.", "section": "3.2. Ablation Studies for Key Changes on T2I"}, {"content": "| Model | VBench-T2V | VBench-I2V |\n|---|---|---|\n| **Q \u2191** | **S \u2191** | **T \u2191** | **I \u2191** | **Q \u2191** | **T \u2191** |\n| T2V-M-512 | 82.2 | 77.0 | **81.2** | / | / | / |\n| STIV-M-512 | 74.6 | 31.9 | 66.1 | **98.0** | 82.1 | **90.1** |\n| STIV-M-512-JIT | 82.3 | **74.1** | 80.7 | 97.6 | 81.9 | 89.8 |\n| STIV-M-512-JIT-TUP | **83.0** | 73.1 | 81.0 | 97.2 | **82.3** | 89.7 |", "caption": "Table 7: Compare Panda-30M and Panda-10M (high-quality) using XL T2V model.", "description": "This table presents a comparison of the performance of a Text-to-Video (T2V) model trained on two different datasets: Panda-30M and Panda-10M.  Panda-10M is a curated subset of Panda-30M, focusing on higher-quality videos to enhance model performance. The table shows the results of evaluating the T2V model's performance on these two datasets using three metrics: FVD (Fr\u00e9chet Video Distance), measuring the quality of generated videos, and two VBench scores for quality and semantic relevance. By comparing results from Panda-30M and Panda-10M, the table demonstrates the impact of dataset quality on the effectiveness of training a T2V model. The XL T2V model was used in this comparison.", "section": "3.5. Video Data Engine"}, {"content": "| Model | Dynamic Degree | Motion Smoothness | Temporal Consistency | Background Flickering |\n|---|---|---|---|---|\n| STIV-M-512 | 10.2 | 99.6 | 99.3 | 99.1 |\n| STIV-M-512-JIT | 24.0 | 99.1 | 98.6 | 98.6 |", "caption": "Table 8: Compare different captions using XL T2V model. DSG-Video metrics are calculated from 100 random captions.", "description": "This table compares the performance of two different captioning methods for training a text-to-video (T2V) model using the XL model variant.  The methods are frame-based captioning followed by large language model (LLM) summarization (FCapLLM), and direct video captioning (VCap). The evaluation metrics used include the total number of objects described in the captions, the number of objects identified as hallucinated using the DSG-Video metric, and the FVD (Fr\u00e9chet Video Distance) and VBench scores reflecting the overall quality of the videos generated using these captions.  100 randomly selected captions were used for the DSG-Video evaluation.", "section": "3.5. Video Data Engine"}, {"content": "| Initialization | Subj | Bg | Temp | Mot | Dyn | Aesth | Img | I2V | I2V | Cam | Avg |\n|---|---|---|---|---|---|---|---|---|---|---|---| \n| **T2V** | 94.1 | 98.2 | 96.5 | 97.7 | **37.1** | **57.9** | 65.5 | **96.6** | 96.9 | **38.0** | **77.9** |\n| **T2I** | **94.5** | **98.7** | **96.9** | **97.9** | 36.5 | 57.4 | **66.1** | 96.6 | **97.3** | 29.8 | 77.2 |", "caption": "Table 9: Performance comparison of T2V variants with open-sourced and close-sourced models on VBench.", "description": "This table presents a quantitative comparison of the performance of various Text-to-Video (T2V) models on the VBench benchmark. It includes both open-source and closed-source models, allowing for a comprehensive evaluation of the state-of-the-art in T2V. The table specifically compares different variants of the authors' proposed T2V model (denoted as \"Ours\") across different scales (XL, XXL, M) and with fine-tuning on high-quality data (SFT) and temporal upsampling (+TUP). This detailed comparison facilitates a thorough analysis of the impact of model scaling, fine-tuning strategies, and architectural choices on the overall quality and semantic alignment of generated videos.", "section": "3. Results"}, {"content": "| Data | MSRVTT | VBench FVD \u2193 | VBench Quality \u2191 | VBench Semantic \u2191 | VBench Total \u2191 |\n|---|---|---|---|---|---|\n| <span class=\"ltx_rule\" style=\"width:100%;height:1.0pt;background:black;display:inline-block;\">\u00a0</span><br>Panda-30M | 770.9 | 80.4 | **73.6** | 65.6 |\n| <span class=\"ltx_rule\" style=\"width:100%;height:1.0pt;background:black;display:inline-block;\">\u00a0</span><br>Panda-10M | **759.2** | **80.8** | 73.4 | **66.2** |", "caption": "Table 10: Performance comparison of STIV-TI2V variants with open-sourced and close-sourced models on VBench-I2V.", "description": "This table presents a quantitative comparison of the performance of various Text-Image-to-Video (TI2V) models, including the proposed STIV model and its variants, against state-of-the-art open-source and closed-source models. The evaluation is conducted using the VBench-I2V benchmark, a comprehensive evaluation metric specifically designed for TI2V models, focusing on image-video alignment aspects.  The table shows the performance scores of different models across multiple metrics within VBench-I2V, facilitating a direct performance comparison and highlighting the effectiveness of the proposed STIV model and its design choices.", "section": "3. Recipe Study for STIV"}, {"content": "| Caption | Total Object | DSG-Video<sub>i</sub>(\u2193) | DSG-Video<sub>s</sub>(\u2193) | MSRVTT FVD (\u2193) | VBench (\u2191) |\n|---|---|---|---|---|---| \n| FCapLLM | 1249 | 6.4 | 24.0 | 808.1 | 64.2 |\n| VCap | 1911 | 5.3 | 15.0 | 770.9 | 65.6 |", "caption": "Table 11: Detailed Evaluation Results for Text-To-Video Generation Models.", "description": "This table presents a detailed quantitative analysis of various text-to-video generation models.  It compares performance across multiple metrics, providing a comprehensive evaluation.  These metrics cover various aspects of video quality, including temporal consistency (e.g., subject consistency, background consistency, temporal flickering, motion smoothness, dynamic degree), image quality (e.g., aesthetic quality, imaging quality), semantic alignment (e.g., object class, multiple objects, human action), and overall video quality. The models are compared based on their performance scores for each of these individual criteria.", "section": "3.2 Ablation Studies for Key Changes on T2I"}, {"content": "| Model | Quality \u2191 | Semantic \u2191 | Total \u2191 |\n|---|---|---|---|\n| Open Sourced Models ||||\n| OpenSora V1.2 [74] | 81.4 | 73.4 | 79.8 |\n| AnimateDiff-V2 [26] | 82.9 | 69.8 | 80.3 |\n| VideoCrafter-2.0 [7] | 82.2 | 73.4 | 80.4 |\n| T2V-Turbo [38] | 82.2 | 74.5 | 80.6 |\n| CogVideoX-2B [65] | 82.2 | 75.8 | 80.9 |\n| Allegro [75] | 83.1 | 73.0 | 81.1 |\n| CogVideoX-5B [65] | 82.8 | 77.0 | 81.6 |\n| LaVie-2 [60] | 83.2 | 75.7 | 81.8 |\n| Close Sourced Models ||||\n| Gen-2 [51] | 82.5 | 73.0 | 80.6 |\n| PIKA [44] | 82.9 | 71.8 | 80.7 |\n| EMU3 [24] | 84.1 | 68.4 | 81.0 |\n| KLING [34] | 83.4 | 75.7 | 81.9 |\n| Gen-3 [52] | 84.1 | 75.2 | 82.3 |\n| Ours ||||\n| XL | 80.7 | 72.5 | 79.1 |\n| XXL | 81.2 | 72.7 | 79.5 |\n| M | 82.1 | 74.8 | 80.6 |\n| M-512 | 82.2 | 77.0 | 81.2 |\n| M-512 SFT | 83.9 | 78.3 | 82.8 |\n| M-512 SFT + TUP | 84.2 | 78.5 | **83.1** |\n| M-512 UnmaskSFT | 83.7 | **79.5** | 82.9 |\n| M-512 UnmaskSFT + TUP | **84.4** | 77.2 | 83.0 |", "caption": "Table 12: Detailed Evaluation Results for Text-Image-To-Video Generation Models.", "description": "This table presents a detailed breakdown of the performance of various text-to-image-to-video generation models.  It assesses performance across several key metrics, offering a granular view of each model's strengths and weaknesses in generating high-quality videos from both text and image inputs.  Metrics include various aspects of video quality (both temporal and image quality), the alignment of generated videos with the input text and image conditions, and an overall consistency score. The table also provides a comparison with various state-of-the-art models, allowing for a direct assessment of the relative performance of the models evaluated in the study.  Averages are also computed across different dimensions to provide a holistic evaluation.", "section": "3.2 Ablation Studies for Key Changes on T2I"}, {"content": "| Model | Quality \u2191 | I2V \u2191 | Total \u2191 |\n|---|---|---|---|\n| VideoCrafter-I2V [6] | 81.3 | 89.0 | 85.1 |\n| Consistent-I2V [49] | 78.9 | 94.8 | 86.8 |\n| DynamicCrafter-256 [62] | 80.2 | 96.6 | 88.4 |\n| SEINE-512 [11] | 80.6 | 96.3 | 88.4 |\n| I2VGen-XL [70] | 81.2 | 95.8 | 88.5 |\n| DynamicCrafter-512 [62] | 81.6 | 96.6 | 89.1 |\n| Animate-Anything [14] | 81.2 | 98.3 | 89.8 |\n| SVD [2] | 82.8 | 96.9 | 89.9 |\n| STIV-XL | 79.1 | 95.7 | 87.4 |\n| STIV-M | 78.8 | 96.3 | 87.6 |\n| STIV-M-512 | 82.1 | 98.0 | 90.1 |\n| STIV-M-512-JIT | 81.9 | 97.6 | 89.8 |", "caption": "Table 13: A breakdown of FLOPs for training high resolution T2V models. Unit 1021superscript102110^{21}10 start_POSTSUPERSCRIPT 21 end_POSTSUPERSCRIPT.", "description": "This table details the computational cost (measured in FLOPs, or floating-point operations) associated with training high-resolution Text-to-Video (T2V) models using different initialization strategies.  It breaks down the FLOPs across four training stages, and the total FLOPs are provided for each approach. The unit of measurement for FLOPs is 10<sup>21</sup>.", "section": "3.2. Ablation Studies for Key Changes on T2I"}, {"content": "| Model | MSRVTT (FVD \u2193) | MovieGen (FVD \u2193) |\n|---|---|---|\n| **T2V** | 536.2 | 347.2 |\n| **STIV-V2V** | **183.7** | **186.3** |", "caption": "Table 14: A breakdown of FLOPs for training high frame count T2V models. Unit: 1021superscript102110^{21}10 start_POSTSUPERSCRIPT 21 end_POSTSUPERSCRIPT.", "description": "This table details the computational cost (floating point operations, or FLOPs) associated with training high-frame-count text-to-video (T2V) models using different initialization methods.  It breaks down the FLOPs across four training stages for several approaches.  The unit for FLOPs is 10<sup>21</sup>.", "section": "3.2. Ablation Studies for Key Changes on T2I"}, {"content": "| Model | use text | MSRVTT FID \u2193 | MSRVTT FVD \u2193 |\n|---|---|---|---| \n| STIV-TUP | No | 2.2 | 6.3 |\n| STIV-TUP | Yes | 2.0 | 5.9 |", "caption": "Table 15: Detailed VBench metrics of different model initialization methods for higher resolution T2V model training.", "description": "This table presents a detailed comparison of various model initialization methods for training higher-resolution text-to-video (T2V) models.  It shows how different initialization strategies impact various aspects of the generated videos as measured by VBench metrics.  The metrics cover video quality (temporal consistency, motion smoothness, etc.), video-text alignment (semantics, styles), and overall quality.  By comparing different initialization approaches (from scratch, from a lower-resolution T2V model, from a T2I model, and jointly from both T2I and T2V), the table allows for a thorough assessment of the impact of initialization on the final model's performance.", "section": "3.2. Ablation Studies for Key Changes on T2I"}, {"content": "| Model | PSNR \u2191 | SSIM \u2191 | LPIPS \u2193 |\n|---|---|---|---|\n| Zero123++ [55] | 21.200 | 0.723 | **0.143** |\n| STIV-TI2V-XL | **21.643** | **0.724** | 0.156 |", "caption": "Table 16: Detailed VBench metrics of different model initialization methods for higher frame count T2V model training.", "description": "This table presents a detailed breakdown of the performance metrics from the VBench benchmark for several variations of a text-to-video (T2V) model.  These variations differ in how the model is initialized, specifically focusing on how pre-trained models of different resolutions and frame counts are leveraged to initialize the main higher-frame-count T2V model. The goal is to analyze the effectiveness of different initialization strategies on the final model's performance in terms of visual quality, semantic alignment, and overall consistency.  The metrics evaluated include various aspects of video quality and semantic alignment with the prompt.", "section": "3.2 Ablation Studies for Key Designs for T2V"}, {"content": "Model|Subject|Back.|Temporal|Motion|Dynamic|Aesthetic|Imaging|Object|Multiple|Human\n---|---|---|---|---|---|---|---|---|---|---\nCogVideoX-5B [65]|96.2|96.5|98.7|96.9|80.0|62.0|62.9|85.2|62.1|99.4\nCogVideoX-2B [65]|96.8|96.6|98.9|97.7|59.9|60.8|61.7|83.4|62.6|98.0\nAllegro [75]|96.3|96.7|99.0|98.8|55.0|63.7|63.6|87.5|59.9|91.4\nAnimateDiff-V2 [26]|95.3|97.7|98.8|97.8|40.8|67.2|70.1|90.9|36.9|92.6\nOpenSora V1.2 [74]|96.8|97.6|99.5|98.5|42.4|56.9|63.3|82.2|51.8|91.2\nT2V-Turbo [38]|96.3|97.0|97.5|97.3|49.2|63.0|72.5|94.0|54.7|95.2\nVideoCrafter-2.0 [7]|96.9|98.2|98.4|97.7|42.5|63.1|67.2|92.6|40.7|95.0\nLaVie-2 [60]|97.9|98.5|98.8|98.4|31.1|67.6|70.4|97.5|64.9|96.4\nLaVIE [60]|91.4|97.5|98.3|96.4|49.7|54.9|61.9|91.8|33.3|96.8\nModelScope [59]|89.9|95.3|98.3|95.8|66.4|52.1|58.6|82.2|39.0|92.4\nVideoCrafter [6]|86.2|92.9|97.6|91.8|89.7|44.4|57.2|87.3|25.9|93.0\nCogVideo [30]|92.2|95.4|97.6|96.5|42.2|38.2|41.0|73.4|18.1|78.2\nPIKA [44]|96.9|97.4|99.7|99.5|47.5|62.4|61.9|88.7|43.1|86.2\nGen-3 [52]|97.1|96.6|98.6|99.2|60.1|63.3|66.8|87.8|53.6|96.4\nGen-2 [51]|97.6|97.6|99.6|99.6|18.9|67.0|67.4|90.9|55.5|89.2\nKLING [34]|98.3|97.6|99.3|99.4|46.9|61.2|65.6|87.2|68.1|93.4\nEMU3 [24]|95.3|97.7|98.6|98.9|79.3|59.6|62.6|86.2|44.6|77.7\nXL|96.0|98.5|98.4|96.5|62.5|56.3|59.3|91.5|41.3|98.0\nXXL|97.5|98.9|99.1|98.2|48.6|56.2|59.7|91.1|49.1|99.0\nM-256|96.0|98.5|98.6|97.2|68.1|57.0|60.8|88.8|62.1|98.0\nM-512|95.9|96.9|98.8|98.0|59.7|60.6|62.5|85.9|72.4|96.0\nM-512-SFT|96.7|97.4|98.7|98.3|70.8|61.7|63.9|88.1|67.7|97.0\nM-512-SFT+TUP|94.8|95.9|98.7|99.2|70.8|63.7|65.0|88.9|70.3|95.0\nM-512-UnMSFT|94.3|96.9|98.8|96.7|77.8|61.4|68.6|90.0|72.3|97.0\nM-512-UnMSFT+TUP|95.2|95.8|98.8|99.2|70.8|63.6|65.9|90.0|69.8|94.0\nModel|Color|Spatial|Scene|App.|Temp.|Overall|Quality|Semantic|Total|Averaged\n---|---|---|---|---|---|---|---|---|---|---\nCogVideoX-5B [65]|82.8|66.4|53.2|24.9|25.4|27.6|82.8|77.0|81.6|70.0\nCogVideoX-2B [65]|79.4|69.9|51.1|24.8|24.4|26.7|82.2|75.8|80.9|68.3\nAllegro [75]|82.8|67.2|46.7|20.5|24.4|26.4|83.1|73.0|81.1|67.5\nAnimateDiff-V2 [26]|87.5|34.6|50.2|22.4|26.0|27.0|82.9|69.8|80.3|64.7\nOpenSora V1.2 [74]|90.1|68.6|42.4|24.0|24.5|26.9|81.4|73.4|79.8|66.0\nT2V-Turbo [38]|89.9|38.7|55.6|24.4|25.5|28.2|82.6|74.8|81.0|67.4\nVideoCrafter-2.0 [7]|92.9|35.9|55.3|25.1|25.8|28.2|82.2|73.4|80.4|66.0\nLaVie-2 [60]|91.7|38.7|49.6|25.1|25.2|27.4|83.2|75.8|81.8|67.6\nLaVIE [60]|86.4|34.1|52.7|23.6|25.9|26.4|78.8|70.3|77.1|63.8\nModelScope [59]|81.7|33.7|39.3|23.4|25.4|25.7|78.1|66.5|75.8|62.4\nVideoCrafter [6]|78.8|36.7|43.4|21.6|25.4|25.2|81.6|72.2|79.7|62.3\nCogVideo [30]|79.6|18.2|28.2|22.0|7.8|7.7|72.1|46.8|67.0|52.3\nPIKA [44]|90.6|61.0|49.8|22.3|24.2|25.9|82.9|71.8|80.7|66.1\nGen-3 [52]|80.9|65.1|54.6|24.3|24.7|26.7|84.1|75.2|82.3|68.5\nGen-2 [51]|89.5|66.9|48.9|19.3|24.1|26.2|82.5|73.0|80.6|66.1\nKLING [34]|89.9|73.0|50.9|19.6|24.2|26.4|83.4|75.7|81.9|68.8\nEMU3 [24]|88.3|68.7|37.1|20.9|23.3|24.8|84.1|68.4|81.0|66.7\nXL|86.4|42.4|54.4|22.4|26.3|27.8|80.7|72.5|79.1|66.1\nXXL|90.8|45.1|45.5|22.1|26.1|27.4|81.2|72.7|79.5|65.9\nM-256|83.6|44.5|54.7|22.5|26.6|28.4|82.7|74.8|80.6|67.9\nM-512|91.2|51.0|53.6|23.9|25.8|27.8|82.2|77.0|81.2|68.8\nM-512-SFT|93.7|58.0|52.8|24.6|26.2|28.5|83.9|78.3|82.8|70.3\nM-512-SFT+TUP|94.7|50.6|57.3|24.5|26.7|28.6|84.2|78.5|83.1|70.3\nM-512-UnMSFT|92.0|59.8|53.1|24.8|26.7|28.8|83.7|79.5|82.9|71.2\nM-512-UnMSFT+TUP|87.7|46.9|57.1|24.5|26.6|28.5|84.4|77.2|83.0|69.7", "caption": "Table 17: Performance of Class-to-Video Generation on UCF-101.", "description": "This table presents a quantitative evaluation of class-to-video generation models on the UCF-101 dataset.  It compares several models, including the proposed STIV model, across two key metrics: Inception Score (IS), measuring the quality and diversity of generated videos, and Fr\u00e9chet Video Distance (FVD), assessing the realism of the generated videos by comparing their distribution to the distribution of real videos.  Higher IS values and lower FVD values indicate better performance.  The table also includes results for the STIV model with different ablations such as adding spatial or temporal masks, indicating how these changes affect model performance.", "section": "3.5 Video Data Engine"}]