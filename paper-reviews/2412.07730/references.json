{"references": [{"fullname_first_author": "Patrick Esser", "paper_title": "Scaling rectified flow transformers for high-resolution image synthesis", "publication_date": "2024-00-00", "reason": "This paper is highly relevant due to its focus on improving the stability and efficiency of training large-scale video generation models, a key challenge addressed in the main paper."}, {"fullname_first_author": "Gedas Bertasius", "paper_title": "Is space-time attention all you need for video understanding?", "publication_date": "2021-00-00", "reason": "This paper introduces a novel attention mechanism for video understanding that is highly relevant to the main paper's architecture."}, {"fullname_first_author": "Andreas Blattmann", "paper_title": "Stable video diffusion: scaling latent video diffusion models to large datasets", "publication_date": "2023-11-15", "reason": "This paper focuses on scaling latent video diffusion models, a crucial aspect of the main paper's work, and its findings directly inform the design choices made in the main study."}, {"fullname_first_author": "Junsong Chen", "paper_title": "Pixart-a: Fast training of diffusion transformer for photorealistic text-to-image synthesis", "publication_date": "2023-10-00", "reason": "This paper introduces a framework that serves as the basis for the STIV model architecture, providing a solid foundation for the work described in the main paper."}, {"fullname_first_author": "William Peebles", "paper_title": "Scalable diffusion models with transformers", "publication_date": "2023-00-00", "reason": "This paper is highly relevant due to its focus on scaling diffusion models, a central problem in video generation addressed by the main study, and its use of transformers to improve efficiency."}]}