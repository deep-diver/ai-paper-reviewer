[{"figure_path": "https://arxiv.org/html/2412.07730/extracted/6048756/other_figure/quality_vs_semantic.png", "caption": "Figure 1: Performance comparison of our Text-to-Video model against both open-source and closed-source state-of-the-art models on VBench\u00a0[31].", "description": "This figure displays a performance comparison chart, illustrating the results of several text-to-video (T2V) models on the VBench benchmark [31]. It compares the performance of the authors' proposed model against state-of-the-art models, both open-source and closed-source.  The chart likely uses VBench's metrics for evaluating video generation quality, which may include measures of semantic accuracy and visual fidelity, allowing for a comparison of both the quality and accuracy of video generation. The visualization helps to understand the relative strengths and weaknesses of different T2V models and shows how the authors' model performs compared to existing solutions.", "section": "3. Recipe Study for STIV"}, {"figure_path": "https://arxiv.org/html/2412.07730/x1.png", "caption": "Figure 2: Text-to-Video and Text-Image-to-Video generation samples by T2V and STIV models. The text prompts and first frame image conditions are borrowed from Sora\u2019s demos\u00a0[42] and MovieGenBench\u00a0[46].", "description": "This figure showcases the video generation capabilities of both the Text-to-Video (T2V) model and the STIV model (which incorporates both text and image conditions).  The top two rows present examples generated using only text prompts as input to the T2V model.  The bottom two rows demonstrate examples where both text and a first frame image (as an image condition) were given as input to the STIV model.  The text prompts and initial image frames are taken directly from existing benchmark datasets (Sora [42] and MovieGenBench [46]) to allow for a direct comparison against these previously published models.", "section": "2. Basics for STIV"}, {"figure_path": "https://arxiv.org/html/2412.07730/x2.png", "caption": "Figure 3: We replace the first frame of the noised video latents with the ground truth latent and randomly drop out the image condition. We use cross attention to incorporate the text embedding, and use QK-norm in multi-head attention, the sandwich-norm in both attention and feedforward, and stateless layernorm after singleton conditions to stabilize the training.", "description": "This figure illustrates the architecture of the STIV block, a key component of the STIV model.  The process begins with frame replacement, where the first noisy frame of the video latent is swapped with the corresponding ground truth frame from the image condition.  To further enhance the model's robustness, the image condition is randomly dropped out during training.  Text embeddings are incorporated through cross-attention. The architecture then uses QK-norm within the multi-head attention mechanism and sandwich-norm in both attention and feedforward layers.  Finally, stateless layernorm is applied after the singleton conditions to enhance training stability. This carefully designed architecture allows STIV to efficiently and effectively handle both text-to-video (T2V) and text-image-to-video (TI2V) tasks.", "section": "2. Basics for STIV"}, {"figure_path": "https://arxiv.org/html/2412.07730/x3.png", "caption": "Figure 4: Progressive training pipeline of the STIV model. The T2I model is first trained to initialize the T2V model, which then initializes the STIV model at both low and high resolutions. Notably, the high-res T2V model is initialized using both the high-res T2I model and the low-res T2V model.", "description": "This figure illustrates the progressive training approach used to develop the STIV model.  The process begins by training a text-to-image (T2I) model at a low resolution. This pre-trained T2I model is then used to initialize a text-to-video (T2V) model, also at low resolution.  The low-resolution T2V model is subsequently used, along with a newly trained high-resolution T2I model, to initialize a high-resolution T2V model. Finally, this high-resolution T2V model is used to initialize the final STIV model (text-image-to-video) at both low and high resolutions. This progressive training strategy leverages knowledge learned at each stage to improve the efficiency and performance of subsequent models.", "section": "3. Recipe Study for STIV"}, {"figure_path": "https://arxiv.org/html/2412.07730/x4.png", "caption": "Figure 5: Ablation study of the STIV model, from the base T2I model to the temporally-aware T2V model, and finally to the image-conditioned TI2V model.", "description": "This figure details an ablation study comparing three model variations. It starts with a base Text-to-Image (T2I) model, then progresses to a temporally-aware Text-to-Video (T2V) model, and culminates in an Image-conditioned Text-to-Video (TI2V) model. Each step builds upon the previous one, adding complexity and capabilities.  The purpose is to demonstrate the incremental impact of adding temporal awareness and image conditioning on the performance and functionality of the video generation model. ", "section": "3. Recipe Study for STIV"}, {"figure_path": "https://arxiv.org/html/2412.07730/x5.png", "caption": "(a) Ablation Study of T2V model design using T2V-XL. The base model uses temporal path size 2, non-causal temporal attention, spatial masking ratio 0.5, and no temporal masking.", "description": "This ablation study investigates the impact of various design choices on the performance of a text-to-video (T2V) model.  The baseline model uses a temporal patch size of 2, non-causal temporal attention (meaning the model processes the temporal dimension sequentially, not bidirectionally), a spatial masking ratio of 0.5 (half of the spatial tokens are masked during training), and no temporal masking. The study systematically varies these design choices (temporal patch size, temporal attention causality, spatial masking ratio, and temporal masking) to determine how each impacts model performance.  Results are likely presented as metrics reflecting the quality of generated videos.", "section": "3.2. Ablation Studies for Key Changes on T2I"}, {"figure_path": "https://arxiv.org/html/2412.07730/x6.png", "caption": "(b) Different model initialization for T2V-XL-512.", "description": "This figure shows a comparison of different initialization methods for training a text-to-video (T2V) model.  The model used is a large T2V model (T2V-XL) with a resolution of 512x512. Different initialization strategies are compared based on their performance on the VBench metric, which measures several factors of video generation quality, including semantic alignment and quality scores.", "section": "3.2 Ablation Studies for Key Changes on T2I"}, {"figure_path": "https://arxiv.org/html/2412.07730/x8.png", "caption": "(c) Different initialization for T2V-XL 40 frames.", "description": "This figure shows the ablation study on different initialization methods for training T2V-XL models with 40 frames.  It compares the performance of various initialization strategies, including training from scratch, initializing from a lower-resolution T2V model (20 frames), initializing from a T2I model, and a combined initialization using both a low-resolution T2V model and a high-resolution T2I model. The results are likely presented in terms of metrics relevant to video generation quality.", "section": "3.2. Ablation Studies for Key Changes on T2I"}, {"figure_path": "https://arxiv.org/html/2412.07730/extracted/6048756/figures/category_distribution_bar_v3.png", "caption": "Figure 6: Ablation studies of key designs for T2V.", "description": "This figure presents ablation studies that analyze the impact of various design choices on the performance of the text-to-video (T2V) model.  Specifically, it examines how different configurations affect VBench metrics (Quality, Semantic, and Total). The subplots (a), (b), and (c) showcase results for different aspects: (a) analyzes the effects of model design choices (temporal patch size, attention type, spatial masking); (b) investigates how different initialization methods for the T2V model (training from scratch, initializing from a lower resolution T2V model, initializing from a T2I model, a combination of T2V and T2I initialization) affect VBench metrics; and (c) shows the effects of different model initialization approaches when training with a higher number of frames (40 frames).  The results help determine optimal model architecture and training strategies for high-quality video generation.", "section": "3. Recipe Study for STIV"}]