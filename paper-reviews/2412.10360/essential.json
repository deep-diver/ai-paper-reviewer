{"importance": "**This paper is crucial for researchers** working with large multimodal models (LMMs) applied to video. It provides **valuable insights** into effective design and training strategies, significantly impacting efficiency and performance in video understanding tasks. The introduction of **Scaling Consistency** simplifies experimentation, while **Apollo** and **ApolloBench** establish new benchmarks and facilitate further progress. This research opens new avenues by highlighting data composition and video sampling.  It empowers researchers to develop better video-LMMs.", "summary": "Apollo LMMs achieve SOTA on video understanding tasks by exploring and optimizing the design and training of video-LMMs.", "takeaways": ["Frames-per-second sampling during training significantly outperforms uniform frame sampling for video-LMMs.", "SigLIP is the most effective single vision encoder for video-LMMs, and combining it with InternVideo2 further enhances performance.", "Including a moderate amount of text data (around 10-14%) and maintaining a slight video-heavy mix in training data composition leads to optimal model performance for video-LMMs"], "tldr": "Large Multimodal Models (LMMs) are revolutionizing how we understand and interact with video data. However, **designing and training effective video-LMMs** is computationally demanding, leading to **suboptimal** design decisions. Existing video understanding benchmarks lack rigorous analysis, and **evaluating** on them is computationally expensive and time-consuming. Limited open research restricts progress in video-LMMs due to their complexity and high computational cost.  This necessitates a comprehensive study into effective model design and training strategies.\n\nThis research explores numerous video-specific design choices for video-LMMs, like **video sampling, architectures, data composition, and training schedules**.  A key finding is **Scaling Consistency**, where design decisions on smaller models and datasets transfer to larger ones, **reducing** computational costs.  The authors introduce **Apollo**, a family of LMMs demonstrating state-of-the-art performance on multiple video understanding benchmarks across different model sizes. They also curate **ApolloBench**, an efficient benchmark suite that reduces evaluation time while improving quality. The research **uncovers** critical factors driving performance, such as the **superiority** of fps video sampling and specific vision encoder combinations, providing **actionable insights** for the research community.", "affiliation": "Meta GenAI", "categories": {"main_category": "Multimodal Learning", "sub_category": "Vision-Language Models"}, "podcast_path": "2412.10360/podcast.wav"}