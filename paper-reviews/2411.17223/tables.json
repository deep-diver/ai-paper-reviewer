[{"content": "|                   | Method                    | CLIP-T\u2191 | CLIP-I\u2191 | DINO\u2191 | FID\u2193 | CLIP-T\u2191 | CLIP-I\u2191 | DINO\u2191 | FID\u2193 |\n|-------------------|-----------------------------|---------|---------|-------|------|---------|---------|-------|------|\n| **Large-scale Training** | Paint-by-Example [39]       | 0.238   | 0.541   | 0.582 | 44.6 | -       | -       | -     | -    |\n|                   | MimicBrush [5]              | 0.256   | 0.614   | 0.631 | 47.0 | -       | -       | -     | -    |\n|                   | AnyDoor [6]                 | 0.284   | 0.688   | 0.711 | 53.7 | -       | -       | -     | -    |\n|                   | IP-Adapter [40]             | 0.268   | 0.633   | 0.633 | 45.3 | 0.236   | 0.611   | 0.629 | 47.5 |\n|                   | LAR-Gen [26]                | 0.269   | 0.662   | 0.676 | 48.7 | 0.257   | 0.656   | 0.668 | 49.6 |\n| **Tuning-free**   | TIGIC [18]                  | 0.262   | 0.533   | 0.584 | 45.2 | 0.236   | 0.479   | 0.548 | 47.3 |\n| **Few-shot Fine-tuning** | DreamBooth\u2020                | 0.248   | 0.609   | 0.604 | 40.8 | 0.235   | 0.567   | 0.595 | 42.0 |\n|                   | DreamBooth [33]             | 0.253   | 0.644   | 0.628 | **40.5** | 0.235   | 0.601   | 0.612 | **41.7** |\n|                   | DreamMix\u2020                   | 0.284   | 0.713   | 0.685 | 43.4 | 0.284   | 0.659   | 0.672 | 44.2 |\n|                   | DreamMix                     | **0.285** | **0.728** | **0.712** | 42.9 | **0.289** | **0.674** | **0.695** | 43.9 |\n|                   | Real Image                  | 0.315   | 0.883   | 0.865 | 35.9 | -       | -       | -     | -    |", "caption": "Table 1: Quantitative comparison of different methods on Identity Preservation and Attribute Editing. The \u201c-\u201d symbol indicates that the method don\u2019t support text-driven editing. We divide the compared methods into three types based on their training protocol: large-scale training, tuning-free, and few-shot finetuning. \u201c\u2020\u2020\\dagger\u2020\u201d indicates that only one image is used in model finetuning for each subject.", "description": "Table 1 presents a quantitative comparison of various image inpainting methods across two key aspects: identity preservation and attribute editing.  The methods are categorized into three groups based on their training approach: large-scale training, tuning-free methods, and few-shot fine-tuning methods.  The table uses several metrics to evaluate performance in both tasks, including CLIP-T (text similarity), CLIP-I (image similarity), DINO (a visual similarity metric), and FID (Fr\u00e9chet Inception Distance, measuring the visual quality of generated images). A dash (-) indicates methods that lack text-driven editing capabilities. The double dagger symbol (\u2020\u2020) highlights methods that used only one image per subject during model fine-tuning, suggesting a potentially limited capacity for generalization.", "section": "4. Experiments"}, {"content": "| Method | Identity Preservation \u2191 | Attribute Editing \u2191 |\n|---|---|---|\n| IP-Adapter [40] | 8% | 7% |\n| MimicBrush [5] | 12% | - |\n| AnyDoor [6] | 10% | - |\n| TIGIC [18] | 2% | 4% |\n| LAR-Gen [26] | 13% | 15% |\n| DreamMix | 55% | 74% |", "caption": "Table 2: \nUser study results for Identity Preservation and Attribute Editing tasks, showing user preference percentages for each method. DreamMix outperforms other methods significantly in both Identity Preservation (55%) and Attribute Editing (74%), indicating strong user preference.", "description": "This table presents the results of a user study comparing different methods for image inpainting, focusing on two key aspects: identity preservation (how well the method maintains the original object's identity) and attribute editing (how well the method allows modifications of the object's attributes).  The study involved 100 participants, each evaluating 30 different image editing results across both tasks.  The percentage of participants who preferred each method for each task is shown.  DreamMix significantly outperforms all other compared methods in both identity preservation (55% preference) and attribute editing (74% preference), demonstrating its superior performance according to user perception.", "section": "4. Experiments"}, {"content": "| Method | CLIP-T \u2191 | CLIP-I \u2191 | DINO \u2191 |\n|---|---|---|---| \n| **Identity Preservation** | Baseline | 0.253 | 0.644 | 0.628 |\n| +DIF | 0.291 | 0.723 | 0.717 |\n| ++TAS | 0.290 | 0.728 | 0.715 |\n| +++ADM | 0.285 | 0.728 | 0.712 |\n| **Attribute Editing** | Baseline | 0.235 | 0.601 | 0.612 |\n| +DIF | 0.265 | 0.696 | 0.708 |\n| ++TAS | 0.275 | 0.685 | 0.700 |\n| +++ADM | 0.289 | 0.674 | 0.695 |", "caption": "Table 3: Quantitative comparison of our ablation methods. The Baseline method indicates the base inpainting model\u00a0[20] combined with DreamBooth\u2019s finetuning strategy\u00a0[33].", "description": "This table presents a quantitative comparison of different model variations, assessing their performance on identity preservation and attribute editing tasks.  The baseline model used for comparison is the Fooocus inpainting model [20] fine-tuned with DreamBooth's [33] strategy.  The table shows the impact of adding three key components of the proposed DreamMix model: the disentangled inpainting framework (DIF), textual attribute substitution (TAS), and attribute decoupling mechanism (ADM). Results are measured using CLIP-T, CLIP-I, and DINO metrics.", "section": "4. Experiments"}]