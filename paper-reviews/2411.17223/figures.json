[{"figure_path": "https://arxiv.org/html/2411.17223/x2.png", "caption": "Figure 1: DreamMix on various subject-driven image customization tasks. (a) Identity Preservation: DreamMix precisely inserts a target object into any scene, achieving high-fidelity and harmonized composting results. (b) Attribute Editing: DreamMix allows users to modify object attributes such as color, texture, and shape or add accessories based on textual instructions. (c) Small Object Inpainting: DreamMix effectively performs small object insertion and editing while preserving fine-grained details and visual harmony.", "description": "Figure 1 showcases DreamMix's capabilities in subject-driven image customization across three scenarios. (a) Identity Preservation: DreamMix seamlessly integrates a target object into diverse backgrounds, maintaining object fidelity and visual harmony. (b) Attribute Editing: Users can manipulate object attributes (color, texture, shape) or add accessories through textual commands. (c) Small Object Inpainting: DreamMix effectively handles insertions and edits of small objects, preserving fine details and overall visual coherence.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2411.17223/x3.png", "caption": "Figure 2: Overview of DreamMix. During finetuning, we use the source data {\ud835\udc99s,\ud835\udc91s}subscript\ud835\udc99\ud835\udc60subscript\ud835\udc91\ud835\udc60\\{\\boldsymbol{x}_{s},\\boldsymbol{p}_{s}\\}{ bold_italic_x start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT , bold_italic_p start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT } along with regular data {\ud835\udc99r,\ud835\udc91r}subscript\ud835\udc99\ud835\udc5fsubscript\ud835\udc91\ud835\udc5f\\{\\boldsymbol{x}_{r},\\boldsymbol{p}_{r}\\}{ bold_italic_x start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT , bold_italic_p start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT } constructed via an attribute decoupling mechanism (Sec.\u00a03.3), to enable pretrained Text-to-Image (T2I) inpainting models to efficiently adapt to subject generation. At testing, we employ a disentangled inpainting framework (Sec.\u00a03.2), which divides the denoising process into two stages: Local Content Generation (LCG) and Global Context Harmonization (GCH). Additionally, we propose a textual attribute substitution module (Sec.\u00a03.4) to generate a decomposed text embedding to enhance the editability of our method during testing.", "description": "Figure 2 illustrates the architecture and workflow of DreamMix, a subject-driven image inpainting model.  During training, DreamMix uses two types of data: source data ({x<sub>s</sub>, p<sub>s</sub>}), which includes subject images (x<sub>s</sub>) and their corresponding text descriptions (p<sub>s</sub>); and regular data ({x<sub>r</sub>, p<sub>r</sub>}), which is generated using an attribute decoupling mechanism to enhance model generalization.  This combination of data allows the pre-trained Text-to-Image (T2I) inpainting model to efficiently adapt for subject generation. At the inference stage, DreamMix leverages a disentangled framework to split the inpainting process into two stages: Local Content Generation (LCG) that focuses on precise object insertion and Global Context Harmonization (GCH) responsible for visual coherence of the output.  A textual attribute substitution module enhances the accuracy of attribute editing by generating a decomposed text embedding for the model's input.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2411.17223/x4.png", "caption": "Figure 3: Pipeline of Attribute Decoupling Mechanism (ADM). We obtain the attribute word list using a VLM agent\u00a0[24] and create regular data with more diverse text formats and image contents.", "description": "Figure 3 details the Attribute Decoupling Mechanism (ADM), a key part of the DreamMix model.  ADM addresses the challenge of overfitting in subject-driven image inpainting by augmenting the training data.  The process begins with feeding subject images into a Vision-Language Model (VLM). The VLM analyzes the images and generates a list of attribute words (e.g., color, shape, material) describing the subject. These attributes are then combined with the original subject descriptions to create more diverse text prompts. Finally, new images are generated using these prompts, resulting in 'regular data' that is used to augment the original subject data and prevent overfitting during model training. This expanded and diversified dataset helps DreamMix achieve better generalization and enhanced editability.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2411.17223/x5.png", "caption": "Figure 4: Visual comparison between different methods. From left to right are input image and subject, visual results of our DreamMix, IP-Adapter\u00a0[40], DreamBooth\u00a0[33], TIGIC\u00a0[18], AnyDoor\u00a0[6], and LAR-Gen\u00a0[26].", "description": "Figure 4 presents a visual comparison of various image inpainting methods applied to four diverse examples. Each row showcases a different image inpainting task, beginning with the input image and its corresponding subject on the far left. The subsequent columns display the results generated by six different techniques: DreamMix (the authors' method), IP-Adapter, DreamBooth, TIGIC, AnyDoor, and LAR-Gen. This visual comparison allows for a direct qualitative assessment of each method's performance in terms of both identity preservation and attribute editing capabilities, highlighting the relative strengths and weaknesses of each approach in handling diverse subject-driven image inpainting challenges.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2411.17223/x6.png", "caption": "Figure 5: Effect of different values of \u03bb\ud835\udf06\\lambdaitalic_\u03bb in disentangled inpainting framework. \u03bb=1\ud835\udf061\\lambda=1italic_\u03bb = 1 means only GCH stage is performed while \u03bb=0\ud835\udf060\\lambda=0italic_\u03bb = 0 means only LCG stage is used. \u03bb\ud835\udf06\\lambdaitalic_\u03bb is set to 0.7 in our experiments.", "description": "This figure demonstrates the impact of the lambda (\u03bb) hyperparameter on the disentangled inpainting framework.  The framework consists of two stages: Local Content Generation (LCG) and Global Context Harmonization (GCH).  Lambda controls the balance between these two stages.  \u03bb = 1 means only the GCH stage is active, while \u03bb = 0 means only the LCG stage is used. The experiments in the paper used \u03bb = 0.7. The figure visually shows the results of varying \u03bb, illustrating how the balance affects the quality and harmony of the inpainted image.", "section": "3. Method"}]