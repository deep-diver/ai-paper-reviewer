[{"figure_path": "https://arxiv.org/html/2503.20756/extracted/6213327/figures/first.png", "caption": "Figure 1: Direct application of LMMs in Autonomous Driving Systems faces several challenges, including the misunderstanding of traffic knowledge, the complex and varied road conditions, and the diverse states of vehicle. Knowledge Editing that enables efficient, continuous, and precise updates to knowledge can effectively address these challenges.", "description": "This figure illustrates the challenges of directly applying Large Multimodal Models (LMMs) to Autonomous Driving Systems (ADS).  Three major issues are highlighted: 1) LMMs may misunderstand traffic rules and signage, leading to incorrect actions. 2) Real-world driving conditions are highly variable and complex, exceeding the scope of typical training data; therefore LMMs may not perform well in all situations. 3) Vehicles exhibit diverse states of motion, making accurate prediction difficult. The figure proposes Knowledge Editing as a solution to address these challenges by allowing for targeted, efficient, continuous, and precise updates to the model's knowledge base without needing a complete re-training.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2503.20756/extracted/6213327/figures/category_static.png", "caption": "Figure 2: The statistics of scenario types for ADS-Edit.", "description": "This figure shows a breakdown of the different scenario types included in the ADS-Edit dataset.  The three main scenario types are: Perception (evaluating basic visual perception), Understanding (assessing comprehension of autonomous driving knowledge), and Decision Making (testing the ability to make informed driving decisions).  The numbers represent the quantity of data samples belonging to each scenario type.", "section": "3 Bechmark Construction"}, {"figure_path": "https://arxiv.org/html/2503.20756/extracted/6213327/figures/process.png", "caption": "Figure 3: The overview of ADS-Edit construction pipeline.", "description": "This figure illustrates the process of constructing the ADS-Edit dataset, which is a multimodal knowledge editing dataset specifically designed for autonomous driving systems.  It begins by selecting three autonomous driving datasets as raw data sources: LingoQA, DriveLM, and CODA-LM.  These datasets contain various types of visual data including videos, multi-view images, and single images, along with associated questions and answers. The raw data undergoes preprocessing steps, primarily condensing answers using the Deepseek-v3 model to improve editing performance and simplify evaluation. The processed data is then split into three subsets for reliability, generality, and locality evaluation.  Each subset has additional steps to create targeted data that focuses on a particular evaluation aspect, such as rephrasing questions for generality testing.  Finally, quality control is implemented through manual verification to ensure data accuracy.", "section": "3 Bechmark Construction"}, {"figure_path": "https://arxiv.org/html/2503.20756/extracted/6213327/figures/seq_qwen2.jpg", "caption": "Figure 4: Lifelong Editing results of Qwen2-VL. \u00d7 indicates that Prompt triggers an Out-of-Memory (OOM) error at 750 and 1000 editing iterations.", "description": "This figure displays the results of lifelong knowledge editing experiments using the Qwen2-VL language model.  Four different knowledge editing methods (Prompt, AdaLora, GRACE, and WISE) were tested. The y-axis shows the performance metrics (Reliability, Generality, and Locality), and the x-axis represents the number of editing iterations. The figure shows how these metrics evolve as the model receives more and more edits, highlighting the effect of repeated knowledge updates.  The 'x' symbol indicates that the Prompt method caused an out-of-memory error at 750 and 1000 iterations. This suggests that some methods are more computationally expensive than others for this type of task and model.", "section": "4.2 Main Results"}, {"figure_path": "https://arxiv.org/html/2503.20756/extracted/6213327/figures/category_2.jpg", "caption": "Figure 5: \nThe average generality metric of single editing across different scenarios.", "description": "This figure displays the average generality scores achieved by different knowledge editing methods across three autonomous driving scenarios: perception, understanding, and decision-making.  Generality measures how well a model generalizes its learned knowledge to new, similar situations after knowledge editing. The graph helps assess the effectiveness and robustness of each knowledge editing technique across various complexities of driving tasks.", "section": "4.2 Main Results"}, {"figure_path": "https://arxiv.org/html/2503.20756/extracted/6213327/figures/filetype_2.jpg", "caption": "Figure 6: \nThe average generality metric of single editing across different data types.", "description": "This figure shows the average generality scores achieved by different knowledge editing methods across three data types: video, multi-view images, and single images. Generality refers to how well a model generalizes its learned knowledge to new, unseen examples.  The results illustrate the relative effectiveness of knowledge editing techniques on different data modalities in the context of autonomous driving.", "section": "4.2 Main Results"}, {"figure_path": "https://arxiv.org/html/2503.20756/extracted/6213327/figures/case_ppt.png", "caption": "Figure 7: Cases analysis of editing LLaVA-OneVision with WISE.", "description": "This figure provides a qualitative analysis of the WISE (Wang et al., 2024b) knowledge editing method applied to the LLaVA-OneVision model. It showcases three example scenarios highlighting successful knowledge editing in autonomous driving. The top row illustrates the editing of autonomous driving knowledge, where the model successfully incorporates the user's instruction to adjust driving based on visibility conditions. The second and third rows show how WISE edits facts (material of umbrella) and context-dependent knowledge (location of keyboard shortcut), successfully preserving the pre-existing knowledge in these separate domains. However, it also highlights how WISE struggles to maintain local knowledge while updating other, related knowledge.", "section": "4.2 Main Results"}, {"figure_path": "https://arxiv.org/html/2503.20756/extracted/6213327/figures/seq_llava.jpg", "caption": "Figure 8: Lifelong Editing results of LLaVA-OneVision. \u00d7 indicates that Prompt triggers an Out-of-Memory (OOM) error at 750 and 1000 editing iterations.", "description": "Figure 8 presents the results of lifelong knowledge editing on the LLaVA-OneVision model.  Lifelong editing involves sequentially updating the model's knowledge multiple times and evaluating its performance after each update. The figure displays the model's performance on various metrics (Reliability, Generality, and Locality) across different numbers of editing iterations (1, 250, 500, 750, and 1000). The 'Prompt' method, a simple approach to knowledge editing, encountered Out-of-Memory (OOM) errors at 750 and 1000 iterations, indicating its limitation in handling numerous edits.  The other methods (AdaLora, GRACE, and WISE) show varying degrees of success in maintaining good performance across the metrics, highlighting their robustness to sequential updates.", "section": "4.2 Main Results"}, {"figure_path": "https://arxiv.org/html/2503.20756/extracted/6213327/figures/video_case.png", "caption": "Figure 9: A video data case of ADS-Edit benchmark.", "description": "This figure shows an example from the ADS-Edit benchmark dataset, specifically illustrating a video data case.  It displays multiple modalities of data including a video clip, an original question about obstacle recognition,  a rephrased version of the question, the ground truth answer, and even examples of unrelated questions and answers used to assess locality. The inclusion of these elements highlights the multimodal and multifaceted nature of the ADS-Edit dataset designed for knowledge editing tasks in autonomous driving.", "section": "3.2 Data Collection"}, {"figure_path": "https://arxiv.org/html/2503.20756/extracted/6213327/figures/multi_case.png", "caption": "Figure 10: A multi-views image data case of ADS-Edit benchmark.", "description": "This figure shows an example from the ADS-Edit benchmark dataset, specifically showcasing a multi-view image scenario.  It includes the original question, a rephrased version, the ground truth answer, and the answers generated by the model. The image data consists of multiple views of the same scene, to test the model's ability to handle such data.  It also includes examples of unrelated knowledge queries and answers to evaluate the model's locality, meaning its ability to update knowledge without affecting unrelated information.", "section": "3 Bechmark Construction"}, {"figure_path": "https://arxiv.org/html/2503.20756/extracted/6213327/figures/single_case.png", "caption": "Figure 11: A single image data case of ADS-Edit benchmark.", "description": "This figure shows a sample from the ADS-Edit benchmark dataset, specifically demonstrating a single image data point.  The example includes the data type, image type, source dataset, original question, rephrased question, answer, image, rephrased image, original ground truth, rephrased ground truth, and locality data (unrelated question and answer). This illustrates the structure and components of the multimodal knowledge editing dataset used for evaluating various model capabilities in autonomous driving scenarios.", "section": "3 Bechmark Construction"}]