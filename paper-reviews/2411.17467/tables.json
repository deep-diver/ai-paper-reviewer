[{"content": "| Methods | ModelNet40 | OBJ-BG | OBJ-ONLY | PB-T50-RS |\n|---|---|---|---|---|\n| PointNet [20] | 89.2 | 73.3 | 79.2 | 68.0 |\n| SpiderCNN [36] | 92.4 | 77.1 | 79.5 | 73.7 |\n| PointNet++ [21] | 90.7 | 82.3 | 84.3 | 77.9 |\n| DGCNN [28] | 92.9 | 86.1 | 85.5 | 78.5 |\n| PointCNN [13] | \u2013 | 86.1 | 85.5 | 78.5 |\n| PTv1 [44] | 93.7 | \u2013 | \u2013 | \u2013 |\n| PTv2 [30] | 94.2 | \u2013 | \u2013 | \u2013 |\n| OcCo [27] | 92.1 | 84.85 | 85.54 | 78.79 |\n| Point-BERT [42] | 93.2 | 87.43 | 88.12 | 83.07 |\n| Scratch [42] | 91.4 | 79.86 | 80.55 | 77.24 |\n| Point-MAE-SN [19] | **93.8** | 90.02 | 88.29 | 85.18 |\n| Point-MAE-Zero | 93.0 | **90.36** | **88.64** | **85.46** |", "caption": "Table 1: Object Classification. We evaluate the object classification performance on ModelNet40 and three variants of ScanObjectNN. Classification accuracy (%) is reported (higher is better). Top: Performance of existing methods with various neural network architectures and pretraining strategies. Bottom: Comparison with our baseline methods.", "description": "This table presents a comparison of object classification accuracy across different methods on the ModelNet40 benchmark dataset and three variations of ScanObjectNN.  It shows the performance of various state-of-the-art methods, highlighting the neural network architecture and pre-training strategies used.  The bottom section contrasts these existing methods against the performance of the models presented in the paper (Point-MAE-SN and Point-MAE-Zero).  Higher accuracy percentages indicate better performance.", "section": "4.1 Object Classification"}, {"content": "| Methods | 5w/10s | 5w/20s | 10w/10s | 10w/20s |\n|---|---|---|---|---|\n| DGCNN-rand [28] | 31.6 \u00b1 2.8 | 40.8 \u00b1 4.6 | 19.9 \u00b1 2.1 | 16.9 \u00b1 1.5 |\n| DGCNN-OcCo [28] | 90.6 \u00b1 2.8 | 92.5 \u00b1 1.9 | 82.9 \u00b1 1.3 | 86.5 \u00b1 2.2 |\n| Transformer-OcCo [27] | 94.0 \u00b1 3.6 | 95.9 \u00b1 2.3 | 89.4 \u00b1 5.1 | 92.4 \u00b1 4.6 |\n| Point-BERT [42] | 94.6 \u00b1 3.1 | 96.3 \u00b1 2.7 | 91.0 \u00b1 5.4 | 92.7 \u00b1 5.1 |\n| Scratch [42] | 87.8 \u00b1 5.2 | 93.3 \u00b1 4.3 | 84.6 \u00b1 5.5 | 89.4 \u00b1 6.3 |\n| Point-MAE-SN [19] | **96.3 \u00b1 2.5** | **97.8 \u00b1 1.8** | **92.6 \u00b1 4.1** | **95.0 \u00b1 3.0** |\n| Point-MAE-Zero | 95.4 \u00b1 2.5 | 97.7 \u00b1 1.6 | 91.3 \u00b1 5.1 | 95.0 \u00b1 3.5 |", "caption": "Table 2: Few-shot classification on ModelNet40. We evaluate performance on four n\ud835\udc5bnitalic_n-way, m\ud835\udc5amitalic_m-shot configurations. For example, 5w/10s denotes a 5-way, 10-shot classification task. The table reports the mean classification accuracy (%) and standard deviation across 10 independent runs for each configuration. Top: Results from existing methods for comparison. Bottom: Comparison with our baseline methods.", "description": "This table presents the results of a few-shot learning experiment on the ModelNet40 dataset.  Few-shot learning is a machine learning technique where a model is trained on a small number of examples for each class. The experiment evaluated the performance of different models under four different configurations, each defined by the number of classes (n-way) and the number of samples per class (m-shot).  The table shows the mean classification accuracy and standard deviation across ten independent runs for each configuration. The results are compared against several existing methods and also show the performance of training the model from scratch. This allows assessment of the benefit gained through self-supervised pre-training.", "section": "4. Experiments"}, {"content": "| Methods | mIoU<sub>I</sub> | aero | bag | cap | car | chair | earphone | guitar | knife |\n|---|---|---|---|---|---|---|---|---|---| \n| PointNet [20] | 83.7 | 83.4 | 78.7 | 82.5 | 74.9 | 89.6 | 73.0 | 91.5 | 85.9 |\n| PointNet++ [21] | 85.1 | 82.4 | 79.0 | 87.7 | 77.3 | 90.8 | 71.8 | 91.0 | 85.9 |\n| DGCNN [28] | 85.2 | 84.0 | 83.4 | 86.7 | 77.8 | 90.6 | 74.7 | 91.2 | 87.5 |\n| OcCo [27] | 85.1 | 83.3 | 85.2 | 88.3 | 79.9 | 90.7 | 74.1 | 91.9 | 87.6 |\n| Point-BERT [42] | 85.6 | 84.3 | 84.8 | 88.0 | 79.8 | 91.0 | 81.7 | 91.6 | 87.9 |\n| Scratch [42] | 85.1 | 82.9 | **85.4** | 87.7 | 78.8 | 90.5 | **80.8** | 91.1 | **87.7** |\n| Point-MAE-SN [19] | **86.1** | 84.3 | 85.0 | 88.3 | 80.5 | 91.3 | 78.5 | **92.1** | 87.4 |\n| Point-MAE-Zero | **86.1** | **85.0** | 84.2 | **88.9** | **81.5** | **91.6** | 76.9 | **92.1** | 87.6 |", "caption": "Table 3: Part Segmentation Results. We report the mean Intersection over Union (IoU) across all instances (mIoUI) and the IoU (%) for each category on the ShapeNetPart benchmark (higher values indicate better performance).", "description": "This table presents the results of a 3D part segmentation task.  The evaluation is performed on the ShapeNetPart benchmark dataset.  The key metric used is the Intersection over Union (IoU), which measures the overlap between predicted and ground truth part segmentations. The table shows the mean IoU across all instances (mIoU1) and the IoU for each individual object category in the ShapeNetPart dataset. Higher IoU values indicate better performance of the model in accurately segmenting the parts of the 3D shapes.", "section": "4.2 Part Segmentation"}, {"content": "| Methods | lamp | laptop | motor | mug | pistol | rocket | skateboard | table |\n|---|---|---|---|---|---|---|---|---|\n| PointNet [20] | 80.8 | 95.3 | 65.2 | 93.0 | 81.2 | 57.9 | 72.8 | 80.6 |\n| PointNet++ [21] | 83.7 | 95.3 | 71.6 | 94.1 | 81.3 | 58.7 | 76.4 | 82.6 |\n| DGCNN [28] | 82.8 | 95.7 | 66.3 | 94.9 | 81.1 | 63.5 | 74.5 | 82.6 |\n| OcCo [27] | 84.7 | 95.4 | 75.5 | 94.4 | 84.1 | 63.1 | 75.7 | 80.8 |\n| Point-BERT [42] | 85.2 | 95.6 | 75.6 | 94.7 | 84.3 | 63.4 | 76.3 | 81.5 |\n| Scratch [42] | 85.3 | 95.6 | 73.9 | **94.9** | 83.5 | 61.2 | 74.9 | 80.6 |\n| Point-MAE-SN [19] | **86.1** | **96.1** | 75.2 | 94.6 | 84.7 | 63.5 | 77.1 | **82.4** |\n| Point-MAE-Zero | 86.0 | 96.0 | **77.8** | 94.8 | **85.3** | **64.7** | **77.3** | 81.4 |", "caption": "Table 4: Masked Point Cloud Completion. The table reports the L2subscript\ud835\udc3f2L_{2}italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT Chamfer distance (lower is better) between predicted masked points and ground truth on the test set of ShapeNet and procedurally synthesized 3D shapes. With Guidance: center points of masked patches are added to mask tokens in the pretrained decoder, guiding masked point prediction during Point-MAE training. Without Guidance: no information from masked patches is available during training.", "description": "Table 4 presents the L2 Chamfer distance results for masked point cloud completion.  Lower values indicate better performance. The results are shown for both ShapeNet (a dataset of human-designed 3D shapes) and procedurally generated 3D shapes.  Two scenarios are considered: \"With Guidance,\" where the center points of the masked patches are used to help the model during reconstruction, and \"Without Guidance,\" where no such information is provided. This comparison highlights the model's ability to reconstruct masked points in different contexts.", "section": "4.3. Masked Point Cloud Completion"}, {"content": "| Methods | With Guidance |  | Without Guidance |  | \n|---|---|---|---|---| \n|  | ShapeNet | Synthetic | ShapeNet | Synthetic | \n| Point-MAE-SN [19] | **0.015** | **0.024** | **0.024** | 0.039 | \n| Point-MAE-Zero | 0.016 | **0.024** | 0.026 | **0.037** | ", "caption": "Table 5: Object Classification. Classification accuracy (%) on ModelNet40 and three ScanObjectNN variants under the revised evaluation setup (Higher is better). Note Scratch* indicates baseline method\u2019s results we obtained.", "description": "This table presents the results of object classification experiments using different methods.  The accuracy of shape classification is evaluated on the ModelNet40 dataset and three variants of the ScanObjectNN dataset (OBJ-BG, OBJ-ONLY, and PB-T50-RS). The methods compared include training from scratch, Point-MAE pretrained on ShapeNet (Point-MAE-SN), and Point-MAE trained on procedurally generated data (Point-MAE-Zero).  Higher accuracy percentages indicate better performance. The results are based on a revised evaluation setup, enhancing the rigor of the comparison.  The \"Scratch*\" row denotes the results obtained from the training from scratch baseline.", "section": "4. Experiments"}, {"content": "| Methods | Pre-train Loss | Downstream Accuracy |\n|---|---|---|\n| Scratch | \u2013 | 77.24 |\n| Point-MAE-SN | 2.62 | 85.18 |\n| Point-MAE-Zero |  |  |\n| (a) Single Primitive | 3.17 | 83.93 |\n| (b) Multiple Primitives | 4.10 | 84.52 |\n| (c) Complex Primitives | 4.43 | 84.73 |\n| (d) Shape Augmentation | 5.28 | 85.46 |", "caption": "Table 6: Few-shot classification on ModelNet40. We evaluate performance on four n\ud835\udc5bnitalic_n-way, m\ud835\udc5amitalic_m-shot configurations. For example, 5w/10s denotes a 5-way, 10-shot classification task. The table reports the mean classification accuracy (%) and standard deviation across 10 independent runs for each configuration. Top: Results from existing methods for comparison. Bottom: Comparison with our baseline methods. Note Scratch* indicates baseline method\u2019s results we obtained.", "description": "This table presents the results of a few-shot learning experiment on the ModelNet40 dataset.  Few-shot learning is a machine learning technique where the model is trained on a small number of examples per class. The experiment evaluates the model's performance under different scenarios, using different numbers of classes (n-way) and examples per class (m-shot). The table shows the mean accuracy and standard deviation across 10 independent runs for each configuration, comparing the performance of different pre-training methods (Point-MAE-SN, Point-MAE-Zero) against a baseline (Scratch*).  The results highlight the effect of different pre-training strategies on few-shot learning performance.", "section": "4. Experiments"}, {"content": "| Methods | ModelNet40 | OBJ-BG | OBJ-ONLY | PB-T50-RS |\n|---|---|---|---|---|\n| Scratch [42] | 91.4 | 79.86 | 80.55 | 77.24 |\n| Scratch* | 93.4 | 87.44 | 82.03 | 81.99 |\n| Point-MAE-SN [19] | **93.8** | 90.02 | 88.29 | 85.18 |\n| Point-MAE-Zero | 93.0 | **90.36** | **88.64** | **85.46** |", "caption": "Table 7: Part Segmentation Results. We report the mean Intersection over Union (IoU) across all instances (mIoUI) and the IoU (%) for each category on the ShapeNetPart benchmark (higher values indicate better performance). Note Scratch* indicates baseline method\u2019s results we obtained.", "description": "This table presents the results of 3D part segmentation on the ShapeNetPart benchmark dataset.  It compares the performance of different methods, including those trained from scratch and those using pre-trained models (Point-MAE-SN and Point-MAE-Zero). The metrics used are the mean Intersection over Union (mIoU) across all instances and the IoU for each individual category. Higher values indicate better performance.  A baseline result obtained from training a model from scratch is also included (indicated by Scratch*). The table allows for a comparison of the effectiveness of different approaches to 3D part segmentation.", "section": "4.2 Part Segmentation"}, {"content": "| Methods | 5w/10s | 5w/20s | 10w/10s | 10w/20s |\n|---|---|---|---|---|\n| Scratch [42] | 87.8 \u00b1 5.2 | 93.3 \u00b1 4.3 | 84.6 \u00b1 5.5 | 89.4 \u00b1 6.3 |\n| Scratch* | 92.7 \u00b1 3.5 | 95.5 \u00b1 3.1 | 88.5 \u00b1 5.1 | 92.0 \u00b1 4.5 |\n| Point-MAE-SN [19] | **96.3 \u00b1 2.5** | **97.8 \u00b1 1.8** | **92.6 \u00b1 4.1** | **95.0 \u00b1 3.0** |\n| Point-MAE-Zero | 95.4 \u00b1 2.5 | 97.7 \u00b1 1.6 | 91.3 \u00b1 5.1 | 95.0 \u00b1 3.5 |", "caption": "Table 8: Object Classification. Classification accuracy (%) on ModelNet40 and three ScanObjectNN variants under the revised evaluation setup. The original test set was split to create a new test set and validation set, and all models were re-evaluated using the updated splits. (Higher is better). Note Scratch* indicates baseline method\u2019s results we obtained and we do not apply voting in these experiments.", "description": "This table presents the results of object classification experiments using four different methods: a training-from-scratch baseline (Scratch*), Point-MAE pretrained on ShapeNet (Point-MAE-SN), Point-MAE-Zero, and another training-from-scratch baseline.  The evaluation was performed on the ModelNet40 dataset and three variants of the ScanObjectNN dataset (OBJ-BG, OBJ-ONLY, and PB-T50-RS).  To enhance the rigor of the evaluation, the original test set was divided into new test and validation sets. The reported accuracies reflect the performance of each method on the new test set, without the voting strategy used in the main paper. Higher accuracy values indicate better performance.", "section": "4. Experiments"}, {"content": "| Methods | mIoU<sub>I</sub> | aero | bag | cap | car | chair | earphone | guitar | knife |\n|---|---|---|---|---|---|---|---|---|---| \n| Scratch [42] | 85.1 | 82.9 | **85.4** | 87.7 | 78.8 | 90.5 | **80.8** | 91.1 | **87.7** |\n| Scratch* | 84.0 | 84.3 | 83.1 | **89.1** | 80.6 | 91.2 | 74.5 | **92.1** | 87.3 |\n| Point-MAE-SN [19] | **86.1** | 84.3 | 85.0 | 88.3 | 80.5 | 91.3 | 78.5 | **92.1** | 87.4 |\n| Point-MAE-Zero | **86.1** | **85.0** | 84.2 | 88.9 | **81.5** | **91.6** | 76.9 | **92.1** | 87.6 |", "caption": "Table 9: Object Classification. Classification accuracy (%) on ModelNet40 and three ScanObjectNN variants under the revised evaluation setup (Higher is better). Note Scratch* indicates baseline method\u2019s results we obtained.", "description": "This table presents the results of object classification using linear probing.  Linear probing is a method where only a single linear layer is trained on top of a pre-trained model (Point-MAE-SN and Point-MAE-Zero), while the weights of the pre-trained model are frozen.  The results are compared against a baseline of training from scratch. The evaluation was performed using a revised dataset split, different from the one reported in the main paper, to ensure more rigorous evaluation. The accuracy is measured on ModelNet40 and three variants of ScanObjectNN (OBJ-BG, OBJ-ONLY, and PB-T50-RS). Higher accuracy indicates better performance.", "section": "C. Linear Probing"}]