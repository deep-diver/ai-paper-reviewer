[{"figure_path": "https://arxiv.org/html/2411.17467/x1.png", "caption": "Figure 1: Point-MAE-Zero. (a) Our synthetic 3D point clouds are generated by sampling, compositing, and augmenting simple primitives with procedural 3D programs\u00a0[34]. (b) We use Point-MAE\u00a0[42] as our pretraining framework to learn 3D representation from synthetic 3D shapes, dubbed Point-MAE-Zero where \u201cZero\u201d underscores that we do not use any human-made 3D shapes. (c) We evaluate Point-MAE-Zero in various 3D shape understanding tasks.", "description": "Figure 1 illustrates the Point-MAE-Zero framework.  (a) shows the process of generating synthetic 3D point clouds using procedural programs: simple primitives are sampled, combined, and augmented. (b) details the self-supervised learning approach where Point-MAE is used to learn 3D representations from these synthetic shapes, a model referred to as Point-MAE-Zero (the \"Zero\" indicates no use of human-created 3D data). (c) shows the downstream tasks used to evaluate the effectiveness of the learned representations.", "section": "3. Learning from Procedural 3D Programs"}, {"figure_path": "https://arxiv.org/html/2411.17467/x2.png", "caption": "Figure 2: Masked Point Cloud Completion. This figure visualizes shape completion results with Point-MAE-SN and Point-MAE-Zero on the test split of ShapeNet and procedurally synthesized 3D shapes.\nLeft: Ground truth 3D point clouds and masked inputs with a 60% mask ratio.\nMiddle: Shape completion results using the centers of masked input patches as guidance, following the training setup of Point-MAE\u00a0[19].\nRight: Point cloud reconstructions without any guidance points.\nThe L2subscript\ud835\udc3f2L_{2}italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT Chamfer distance (lower is better) between the predicted 3D point clouds and the ground truth is displayed below each reconstruction.", "description": "Figure 2 demonstrates the performance of Point-MAE-SN and Point-MAE-Zero models on masked point cloud completion.  It uses ShapeNet and procedurally generated 3D shapes. The left column shows the ground truth point clouds and masked versions (60% masked). The middle column shows reconstruction results using guidance points (centers of masked patches), mirroring the Point-MAE training methodology. The right column displays reconstruction results without using any guidance points.  The Chamfer distance, a metric measuring the difference between the reconstructed and ground truth point clouds, is shown beneath each reconstruction.", "section": "3.2 Point-MAE-Zero"}, {"figure_path": "https://arxiv.org/html/2411.17467/x4.png", "caption": "Figure 3: Impact of 3D Shape Complexity on Performance.\nLeft: Examples of procedurally generated 3D shapes with increasing complexity, used for pretraining. Textures are shown for illustration purposes only; in practice, only the surface points are used.\nRight: Comparison of pretraining masked point reconstruction loss (Eqn.\u00a01)\u00a0[19] and downstream classification accuracy on the ScanObjectNN dataset\u00a0[25]. Each row in Point-MAE-Zero represents an incrementally compounded effect of increasing shape complexity and augmentation, with the highest accuracy achieved using shape augmentation.", "description": "Figure 3 demonstrates how increasing the complexity of procedurally generated 3D shapes used for pretraining affects model performance.  The left panel shows examples of shapes, starting with simple primitives and progressing to more complex shapes created through combinations and augmentations.  The right panel presents a quantitative comparison, showing that while increasing complexity initially increases the pretraining loss (masked point cloud reconstruction loss, as defined in Equation 1 of the referenced Point-MAE paper), it ultimately leads to higher downstream classification accuracy on the ScanObjectNN dataset.  Each row in the 'Point-MAE-Zero' section represents a step-wise increase in shape complexity and the application of augmentations, culminating in the highest accuracy when augmentations are included.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2411.17467/x5.png", "caption": "Figure 4: Impact of pretraining dataset size. We report the classification accuracy (%) on the PB-T50-RS subset of ScanObjectNN\u00a0[25] as a function of the pretraining dataset size.", "description": "This figure illustrates the effect of varying the size of the training dataset on the performance of the Point-MAE-Zero model. The model's classification accuracy on the PB-T50-RS subset of the ScanObjectNN dataset is evaluated using different numbers of synthetic 3D shapes in the pretraining dataset. The graph shows how the accuracy changes as a function of the dataset size, indicating the relationship between dataset size and model performance.", "section": "4.4. Analysis"}, {"figure_path": "https://arxiv.org/html/2411.17467/x6.png", "caption": "Figure 5: Learning curves in downstream tasks. We present validation accuracy (top row) and training curves (bottom row) in object classification tasks on ScanObjectNN (left column) and ModelNet40 (right column).", "description": "This figure displays the learning curves for two object classification tasks: ScanObjectNN and ModelNet40.  The top row shows the validation accuracy over training epochs, while the bottom row presents the corresponding training loss.  The plots illustrate the training progress for three different methods: training from scratch, using Point-MAE pretrained on ShapeNet, and using Point-MAE pretrained on synthetic data generated by procedural 3D programs.  This comparison highlights the impact of different pretraining strategies on model performance and convergence speed for both datasets.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2411.17467/x7.png", "caption": "Figure 6: t-SNE visualization of 3D shape representations. (a) displays the distribution of representations from transformer encoders with different initialization strategies: randomly initialized (Scratch), pre-trained with Point-MAE on ShapeNet (Point-MAE-SN), and pre-trained on procedurally generated 3D shapes (Point-MAE-Zero). (b) shows the distribution of shape representations after fine-tuning on the target tasks, which include object classification on ModelNet40 (top row) and ScanObjectNN (bottom row). Each point represents a 3D shape while the color denotes the semantic categories.", "description": "This figure visualizes the distribution of 3D shape representations learned using different methods.  Panel (a) compares the representations before fine-tuning, showing the results from a randomly initialized model, a model pre-trained on ShapeNet (Point-MAE-SN), and a model pre-trained on procedurally generated 3D shapes (Point-MAE-Zero). Panel (b) shows how these representations change after fine-tuning on two object classification tasks: ModelNet40 (top) and ScanObjectNN (bottom).  Each point in the t-SNE plots represents a single 3D shape, and the color of the point indicates its semantic category.  This visualization helps to understand how different training methods affect the learned representations and how these representations evolve during fine-tuning.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2411.17467/x8.png", "caption": "Figure 7: Masked Point Cloud Completion. This figure visualizes shape completion results with Point-MAE-SN and Point-MAE-Zero on the test split of ShapeNet and procedurally synthesized 3D shapes.\nLeft: Ground truth 3D point clouds and masked inputs with a 60% mask ratio.\nMiddle: Shape completion results using the centers of masked input patches as guidance, following the training setup of Point-MAE\u00a0[19].\nRight: Point cloud reconstructions without any guidance points.\nThe L2subscript\ud835\udc3f2L_{2}italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT Chamfer distance (lower is better) between the predicted 3D point clouds and the ground truth is displayed below each reconstruction.", "description": "Figure 7 demonstrates the performance of Point-MAE-SN and Point-MAE-Zero on masked point cloud completion.  The figure shows three columns for each of several ShapeNet and synthetic 3D shapes: the ground truth complete shape; the shape with 60% of its points masked (masked input); and the completed shape generated by each model. The middle column uses the center points of masked patches as guidance during reconstruction, mirroring the Point-MAE training process. The right-most column shows reconstruction without any guidance.  The L2 Chamfer distance between the generated and ground truth shapes quantifies the reconstruction error and is provided below each reconstruction.", "section": "4.3 Masked Point Cloud Completion"}]