{"importance": "This paper introduces **TokenSwift**, a novel framework designed to accelerate ultra-long sequence generation in LLMs. TokenSwift demonstrates superior acceleration performance and effectively mitigates issues related to repetitive content, which is essential for future research.", "summary": "TokenSwift: Accelerate LLM ultra-long sequence generation up to 100K tokens with >3x speedup and lossless accuracy!", "takeaways": ["TokenSwift is the first framework to accelerate ultra-long sequence generation up to 100K tokens with lossless accuracy.", "TokenSwift achieves over 3x speedup compared to autoregressive generation across various models.", "TokenSwift enhances diversity in ultra-long sequence generation while increasing speedup as the generation length increases."], "tldr": "Generating ultra-long sequences with large language models is crucial, yet time-intensive. Existing methods like speculative decoding are insufficient for sequences up to 100K tokens due to limitations such as frequent model reloading, dynamic key-value management, and repetitive generation. Addressing this bottleneck is essential for unleashing LLMs' full potential in real-world applications.\n\nTo solve this, the study introduces **TokenSwift**, a new framework that accelerates the generation of ultra-long sequences while maintaining model quality. TokenSwift uses n-gram retrieval and dynamic KV cache updates to achieve >3x speedup across different model scales and architectures, which alleviates model reloading, KV management and output repetition issues. The approach helps save hours of time.", "affiliation": "NLCo Lab, BIGAI", "categories": {"main_category": "Natural Language Processing", "sub_category": "Text Generation"}, "podcast_path": "2502.18890/podcast.wav"}