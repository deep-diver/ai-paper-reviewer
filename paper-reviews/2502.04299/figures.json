[{"figure_path": "https://arxiv.org/html/2502.04299/x1.png", "caption": "Figure 1: MotionCanvas offers comprehensive motion controls to animate a static image (the \u201cInputs\u201d column) with various types of camera movements and object motions. Note the different camera movements across columns and object motions across rows. Please use Adobe Acrobat Reader for video playback.", "description": "This figure demonstrates the core functionality of MotionCanvas, showcasing its ability to generate diverse video animations from a single static image. The 'Inputs' column displays the initial static image, which serves as the foundation for all generated videos. Each row represents a unique type of object motion, while each column shows a different camera movement.  By systematically varying both object and camera motions, the figure visually highlights the comprehensive control MotionCanvas offers over the image-to-video generation process.  To view the animations, please use Adobe Acrobat Reader.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2502.04299/x2.png", "caption": "Figure 2: Overview of MotionCanvas. Given an input image and high-level scene-space motion intent, MotionCanvas decomposes and translates the motion (camera and object motion with their timing) into screen space by leveraging the depth-based synthesis and hierarchical transformation with the Motion Signal Translation module. These screen-space motion signals are subsequently passed to a video generation model to produce the final cinematic shots.", "description": "MotionCanvas processes user-specified scene-space motion into screen-space representations for video generation.  It takes an input image and high-level motion intentions (camera movements and object manipulations with timing).  The Motion Signal Translation module converts these scene-space motions to screen-space signals using depth-based synthesis and hierarchical transformations. These screen-space signals then guide a video generation model, creating the final cinematic video.", "section": "3. MotionCanvas"}, {"figure_path": "https://arxiv.org/html/2502.04299/x3.png", "caption": "Figure 3: Illustration of our motion-conditioned video generation model. The input image and bbox color frames are tokenized via a 3D-VAE encoder and then summed. The resultant tokens are concatenated with other conditional tokens, and fed into the DiT-based video generation model.", "description": "This figure illustrates the architecture of the motion-conditioned video generation model used in MotionCanvas.  The process begins with an input image and bounding boxes (bboxes) representing objects. These are encoded using a 3D Variational Autoencoder (VAE). The encoded bboxes are combined with other conditional information (e.g., point trajectories, text prompts) and then fed into a Diffusion Transformer (DiT)-based video generation model. The DiT model processes this combined information to generate the final video output.", "section": "3. Motion-conditioned Video Generation"}, {"figure_path": "https://arxiv.org/html/2502.04299/x4.png", "caption": "Figure 4: Shot design generated by our MotionCanvas under various types of joint camera and object motion controls.", "description": "This figure shows examples of video shots generated by MotionCanvas, demonstrating its ability to control both camera and object movements simultaneously.  Each row represents a different type of object motion (e.g., static objects, objects moving along a track, etc.) applied to the same input image. Each column illustrates a different camera motion technique (e.g., static camera, dolly zoom, orbit, etc.). This figure highlights the versatility and controllability of the MotionCanvas system in creating diverse cinematic shots from a single still image.", "section": "4. Applications"}, {"figure_path": "https://arxiv.org/html/2502.04299/x5.png", "caption": "Figure 5: Long videos with the same complex sequences of camera motion while different object motion controls in each case generated by our MotionCanvas.", "description": "This figure showcases the ability of MotionCanvas to generate long videos with complex and consistent camera movements, while simultaneously demonstrating control over distinct object motions within each video.  Multiple videos are shown, all sharing identical camera trajectories but exhibiting varied object movements, highlighting the model's capacity for independent control of camera and object animation.", "section": "4. Applications"}, {"figure_path": "https://arxiv.org/html/2502.04299/x6.png", "caption": "Figure 6: Generated videos with diverse and fine-grained local motion controls (upper), and in coordination with camera motion control (bottom).", "description": "This figure showcases the capabilities of MotionCanvas in generating videos with intricate and nuanced control over object motion. The top row displays examples of videos featuring diverse and fine-grained local object motions, demonstrating the system's ability to precisely manipulate individual object parts. The bottom row presents videos where these local object movements are seamlessly integrated with camera motion controls, highlighting the system's capacity to coordinate complex scene-space actions for cinematic shot design.", "section": "4. Applications"}, {"figure_path": "https://arxiv.org/html/2502.04299/x7.png", "caption": "Figure 7: Results when our method is applied for: (upper) motion transfer, and (bottom) video editing for changing objects, adding and removing objects.", "description": "This figure demonstrates the versatility of MotionCanvas in video editing applications. The top row showcases motion transfer, where the motion from a source video is seamlessly applied to a different image. The bottom row exemplifies the ability to modify video content by adding, removing, or changing objects within the scene.", "section": "4. Applications"}, {"figure_path": "https://arxiv.org/html/2502.04299/x8.png", "caption": "Figure 8: Camera motion control comparison. Compared to existing baselines, our method performs better at following the intended camera motion, especially for complex shot type such as the \u201cDolly-Zoom\u201d effect (second example).", "description": "This figure demonstrates a comparison of camera motion control between MotionCanvas and two existing baselines (MotionCtrl and CameraCtrl) across different shot designs.  The results show that MotionCanvas is more effective at accurately reproducing intended camera movements. The comparison is particularly striking in the second example, a complex 'Dolly-Zoom' shot, where MotionCanvas achieves significantly better accuracy.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.04299/x9.png", "caption": "Figure 9: Visual comparison of the resulatant videos from DragAnything, MOFA-Video, TrackDiffusion, Ourscoordcoord{}_{\\text{coord}}start_FLOATSUBSCRIPT coord end_FLOATSUBSCRIPT, and our MotionCanvas.", "description": "Figure 9 presents a visual comparison of video generation results from five different methods: DragAnything, MOFA-Video, TrackDiffusion, Ourscoord, and MotionCanvas.  Each method was given the same input image and task of animating a person performing a front flip. The comparison highlights the differences in motion quality, smoothness, and adherence to the intended motion parameters between various approaches.  It showcases how MotionCanvas improves upon existing methods by generating more natural and realistic animations with better control and accuracy in object and camera movement.", "section": "5. Experiments"}]