{"references": [{"fullname_first_author": "Daya Guo", "paper_title": "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning", "publication_date": "2025-01-01", "reason": "This paper introduces DeepSeek-R1, which serves as a primary motivation and a baseline model for comparison in the current study because it demonstrates complex reasoning abilities achievable through reinforcement learning in LLMs."}, {"fullname_first_author": "Liunian Harold Li", "paper_title": "Grounded language-image pre-training", "publication_date": "2022-01-01", "reason": "This paper is significant because it details how to integrate text-aligned image encoders with LLMs, a foundational component for vision-language models (LVLMs) that the current study builds upon."}, {"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-01-01", "reason": "This paper is important as it describes how to learn visual models from natural language supervision, which is relevant for creating the initial reasoning structure in the SFT stage of the current study."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Llama 2: Open foundation and fine-tuned chat models", "publication_date": "2023-07-01", "reason": "This paper presents LLaMA 2, which forms the basis for the base model (Qwen2.5-VL-7B) used in the current study, making it important for understanding the initial capabilities and architecture before enhancements."}, {"fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-01-01", "reason": "This paper is significant because it details training language models using human feedback, which is conceptually related to using reinforcement learning (RL) to refine models, as explored in the current study."}]}