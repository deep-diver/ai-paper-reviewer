[{"figure_path": "https://arxiv.org/html/2503.17352/x1.png", "caption": "Figure 1: Performance (accuracy) of OpenVLThinker-7B on multi-modal reasoning benchmarks.", "description": "This bar chart displays the accuracy of the OpenVLThinker-7B model on three multi-modal reasoning benchmarks: MathVista, MathVerse, and MathVision.  The accuracy is presented as a percentage, and separate bars are shown for OpenVLThinker-7B, Qwen2.5-VL-7B, and GPT-4 (reported).  The chart visually compares the performance of OpenVLThinker-7B against existing models on these challenging tasks, highlighting its relative strengths.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.17352/x2.png", "caption": "Figure 2: Demonstration of the training process toward OpenVLThinker-7B. We obtain the SFT data for iteration 1 via text-based R1 model that only receives the question and the generated image caption. Subsequently, we apply SFT and GRPO iteratively to leverage new reasoning data from the previous iteration and achieve self-improvement. We also evolve the data sources to progressively include more challenging questions over iterations.", "description": "This figure illustrates the iterative training process for OpenVLThinker-7B, a vision-language model.  The process starts with a base model and a text-based reasoning model (DeepSeek-R1).  An image is input, and a captioning model generates a caption.  The caption and the original question are then fed to the DeepSeek-R1 model, which produces reasoning steps and an answer. This data (image, question, reasoning, answer) constitutes the initial Supervised Fine-Tuning (SFT) dataset.  Iteratively, the model is fine-tuned using this SFT dataset and then further improved with reinforcement learning (GRPO).  The improved model from each iteration then generates refined SFT data for the next round. This iterative approach, combining SFT and GRPO, results in improved reasoning ability,  along with progressively more challenging questions to further enhance the model's capabilities.", "section": "4. OpenVLThinker: Iterative Self-improvement via SFT and RL"}, {"figure_path": "https://arxiv.org/html/2503.17352/x3.png", "caption": "Figure 3: An example reasoning trace of OpenVLThinker. It shows desirable reasoning behavior including self-reflection and self-verification, such as \u201cI double-check my work\u201d. Corresponding image and question are shown in Figure\u00a09.", "description": "This figure displays an example of OpenVLThinker's reasoning process. The model is presented with a question about an image (the image and question are shown in Figure 9).  The model demonstrates a chain of thought, showing its step-by-step reasoning. Notably, the model also exhibits self-reflection, verifying its work and correcting itself when necessary.  This example illustrates the model's ability to perform complex reasoning tasks and its capacity for self-correction and verification.", "section": "4. OpenVLThinker: Iterative Self-improvement via SFT and RL"}, {"figure_path": "https://arxiv.org/html/2503.17352/x4.png", "caption": "Figure 4: Iterative performance improvement of our model on MathVista. We note that SFT-Iter(i) is always fine-tuned from the base model Qwen2.5-VL-7B, with its training data generated from GRPO-Iter(i-1). GRPO-Iter(i) is obtained by applying GRPO to train SFT-Iter(i).", "description": "This figure displays the iterative performance improvement of the OpenVLThinker model on the MathVista benchmark.  The process involves alternating between supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO) reinforcement learning.  Each SFT iteration (SFT-Iter(i)) fine-tunes the base Qwen2.5-VL-7B model using data generated by the previous GRPO iteration (GRPO-Iter(i-1)).  Each GRPO iteration trains the model from the previous SFT iteration. This iterative approach demonstrates consistent performance gains over multiple rounds.", "section": "4. OpenVLThinker: Iterative Self-improvement via SFT and RL"}, {"figure_path": "https://arxiv.org/html/2503.17352/x5.png", "caption": "(a) Average response length", "description": "The figure shows two line graphs, (a) and (b), illustrating the training progress of the model using GRPO at iteration 1.  Graph (a) displays the average response length over training steps, while graph (b) shows the test accuracy on a held-out validation set. Both graphs indicate the model's performance improvement during training, with the test accuracy rising alongside an increase in average response length.", "section": "4.4. Iterative Refinements"}, {"figure_path": "https://arxiv.org/html/2503.17352/x6.png", "caption": "(b) Test score", "description": "This figure shows the test accuracy during the training process of the GRPO (Group Relative Policy Optimization) at iteration 1.  The x-axis represents the training step, and the y-axis shows the test accuracy on a held-out validation set. The graph illustrates the model's performance improvement over training steps.  The accuracy is observed on a held-out validation set from the same distribution as the training data.", "section": "4.4. Iterative Refinements"}, {"figure_path": "https://arxiv.org/html/2503.17352/x7.png", "caption": "Figure 5: Training progress with GRPO at iteration 1. We report the test accuracy on a held-out validation set from the same distribution as the training data.", "description": "This figure displays the training progress of the model using Group Relative Policy Optimization (GRPO) during the first iteration.  The left subplot shows the average response length generated by the model over training steps. The right subplot shows the test accuracy on a held-out validation dataset, which has the same distribution as the training dataset. The figure demonstrates how the model's performance and response length evolve during the training process.", "section": "4. OpenVLThinker: Iterative Self-improvement via SFT and RL"}, {"figure_path": "https://arxiv.org/html/2503.17352/x8.png", "caption": "(a) Average response length", "description": "The figure shows two subplots. Subplot (a) presents a line graph illustrating the average response length during the training process of the GRPO (Group Relative Policy Optimization) at iteration 1.  The x-axis represents the training step, while the y-axis indicates the average response length. The graph shows fluctuations in the average response length over the training steps. Subplot (b) displays a line graph depicting the test accuracy on a held-out validation set during the same GRPO training process at iteration 1. The x-axis represents the training step, and the y-axis shows the test accuracy. This subplot demonstrates the test accuracy's behavior over the course of training.  Together, the two subplots provide insight into the relationship between response length and model performance during GRPO training.", "section": "4.4. Iterative Refinements"}, {"figure_path": "https://arxiv.org/html/2503.17352/extracted/6298557/figures/image.jpg", "caption": "(b) Test score", "description": "The figure shows the test accuracy during the training process of the model using GRPO at iteration 1. The x-axis represents the training step, and the y-axis represents the test accuracy on a held-out validation set. The accuracy gradually improves over the training steps, indicating that the GRPO optimization process is effective in improving the model's performance.  The graph demonstrates that as the average length of model responses increases during the GRPO optimization process, the test accuracy also increases.", "section": "4.4. Iterative Refinements"}, {"figure_path": "https://arxiv.org/html/2503.17352/extracted/6298557/figures/example_question.jpg", "caption": "Figure 6: Training progress with GRPO at iteration 2. We report the test accuracy on a held-out validation set from the same distribution as the training data.", "description": "This figure displays the training progress of the model using Group Relative Policy Optimization (GRPO) at iteration 2.  The left subplot shows the average response length throughout the training process, indicating the length of the model's reasoning steps. The right subplot presents the test accuracy, measured on a held-out validation set that mirrors the distribution of the training data, allowing for a robust evaluation of the model's generalization capabilities. The trendlines illustrate the interplay between the length of the reasoning process and the overall model accuracy, revealing the effectiveness of the GRPO training strategy.", "section": "4.4. Iterative Refinements"}, {"figure_path": "https://arxiv.org/html/2503.17352/x9.png", "caption": "Figure 7: Pass@k accuracy of different reasoning models based on captions generated with different vision-language LLMs.", "description": "This figure displays the performance (Pass@k accuracy) of different reasoning models on a visual question answering (VQA) task. The performance is evaluated based on captions generated by two different vision-language large language models (LLMs): LLaVA-v1.6-34B and GPT-40.  The x-axis represents the number of reasoning paths sampled (k), while the y-axis shows the accuracy. Different colored lines represent different reasoning models, comparing standard models against those incorporating R1-style reasoning. The results illustrate the impact of caption quality from different vision-language models on the effectiveness of various reasoning approaches in a VQA context.", "section": "A. Additional Empirical Study"}, {"figure_path": "https://arxiv.org/html/2503.17352/x10.png", "caption": "Figure 8: The image to the visual reasoning task that we showed in section\u00a04.2", "description": "Figure 8 shows the image used in a visual reasoning task presented in Section 4.2 of the paper. The image contains a simple line segment drawn on a ruler. The task for the model is to determine the length of the line, demonstrating the model's ability to understand simple measurements using visual information.", "section": "4. OpenVLThinker: Iterative Self-improvement via SFT and RL"}]