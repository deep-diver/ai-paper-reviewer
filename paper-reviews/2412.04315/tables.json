[{"content": "| Name | # Para | BS | n<sub>layer</sub> | d | d<sub>ffn</sub> | d<sub>head</sub> | n<sub>head</sub> | n<sub>kv</sub> |\n|---|---|---|---|---|---|---|---|---|\n| 0.005B | 5,247,232 | 32 | 8 | 256 | 640 | 64 | 4 | 1 |\n| 0.03B | 31,470,080 | 32 | 12 | 512 | 1,280 | 64 | 8 | 2 |\n| 0.1B | 106,196,736 | 64 | 18 | 768 | 1,920 | 64 | 12 | 3 |\n| 0.2B | 245,416,960 | 128 | 24 | 1,024 | 2,560 | 64 | 16 | 2 |\n| 0.4B | 476,852,480 | 256 | 30 | 1,280 | 3,200 | 64 | 20 | 2 |\n| 0.8B | 828,225,024 | 512 | 36 | 1,536 | 3,840 | 64 | 24 | 3 |", "caption": "Table 1: The detailed hyper-parameters of small models trained for loss estimation.", "description": "This table details the hyperparameters used in training the smaller language models that were used to estimate the loss function in the paper's experiments.  These smaller models served as a basis for calculating the effective parameter size of larger LLMs, a key part of the paper's capacity density metric.  The hyperparameters listed include the number of parameters, batch size, number of layers, dimension of the hidden layer, inner dimension of the feedforward network, number of attention heads, and other model details.", "section": "3.1 Evaluation Settings"}]