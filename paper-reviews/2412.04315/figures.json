[{"figure_path": "https://arxiv.org/html/2412.04315/x1.png", "caption": "Figure 1: The estimated capacity density of open-source base LLMs.", "description": "This figure shows the estimated capacity density of various open-source base Large Language Models (LLMs) released since early 2023. Capacity density is a new metric introduced in the paper to evaluate the training quality of LLMs by considering both effectiveness and efficiency. It's calculated as the ratio of a model's effective parameter size (the minimum number of parameters needed to achieve the same performance as the model) to its actual parameter size. The figure plots the capacity density against the release date of each LLM, revealing an exponential growth trend which the authors call the \"Densing Law\". This trend indicates a doubling of maximum capacity density roughly every 3.3 months.  Different LLMs are represented by different colored circles, with their sizes possibly corresponding to parameter size or another relevant model characteristic. A trendline is fitted to the data points to visually demonstrate the exponential growth.", "section": "Highlights"}, {"figure_path": "https://arxiv.org/html/2412.04315/x2.png", "caption": "(a) Loss Estimation", "description": "This figure shows the results of the loss estimation step in the two-step estimation approach used to calculate LLM density.  The left panel (a) displays the relationship between the language model loss and the pre-training compute (represented as 6ND),  which is fitted using a power-law function (L = aN^\u03b1 + bD^-\u03b2). Separate curves are shown for different downstream tasks (MMLU, BBH, MATH, HumanEval, MBPP, and the average across all tasks). The x-axis represents the pre-training compute and the y-axis shows the language modeling loss.  Each curve depicts how the language model loss changes with increasing compute for the specific task.", "section": "2.2 Loss Estimation"}, {"figure_path": "https://arxiv.org/html/2412.04315/x3.png", "caption": "(b) Performance Estimation", "description": "Figure 2(b) displays the results of the performance estimation step in a two-step process for predicting downstream task performance.  The first step uses scaling laws to estimate language modeling loss, and the second step uses this loss to predict actual performance.  The plot shows the fitted curves for multiple downstream tasks (MMLU, BBH, MATH, HumanEval, MBPP, and their average).  Triangles represent the performance of larger models (those with more than 4 billion parameters) that were used for prediction, demonstrating the effectiveness of the two-step estimation approach for predicting the performance of significantly larger models based on smaller ones.", "section": "2.3 Performance Estimation"}, {"figure_path": "https://arxiv.org/html/2412.04315/x4.png", "caption": "Figure 2: The results for loss estimation and performance estimation. Here, the lines are fitted curves. X-axis in (a) refers to the pre-training compute, which is approximated by Compute=6\u2062N\u2062DCompute6\ud835\udc41\ud835\udc37\\text{Compute}=6NDCompute = 6 italic_N italic_D. Triangles in (b) are larger models for prediction.", "description": "Figure 2 presents the results of a two-step estimation process used to predict the performance of downstream tasks based on the pre-training loss.  Panel (a) shows the loss estimation, plotting the relationship between pre-training computational cost (approximated by Compute = 6ND, where N is the number of parameters and D is the number of training tokens) and the pre-training loss for several smaller models. The lines represent fitted curves for this relationship. Panel (b) displays the performance estimation, showing the relationship between pre-training loss and downstream task performance.  The lines are fitted curves based on the smaller models. The triangles represent larger models used for prediction, demonstrating the effectiveness of the model in predicting performance based on pre-training loss. These plots showcase the process of using scaling laws to estimate the effective parameter size and the density of LLMs.", "section": "2. Loss Estimation"}, {"figure_path": "https://arxiv.org/html/2412.04315/x5.png", "caption": "Figure 3: Prices of LLMs that can outperform GPT-3.5. The line connects the cheapest models.", "description": "This figure shows the price trend of Large Language Models (LLMs) that surpass the performance of GPT-3.5. The y-axis represents the price per million tokens, and the x-axis denotes the release date.  The data points represent various models, and a line is drawn to connect the lowest price point at each time interval.  This visually demonstrates how the cost of accessing high-performance LLMs has decreased over time. ", "section": "3.4 Corollaries of Densing Law"}, {"figure_path": "https://arxiv.org/html/2412.04315/x6.png", "caption": "Figure 4: Density evaluated using MMLU. Two trend lines represent the growth of LLMs\u2019 density before and after the release of ChatGPT.", "description": "Figure 4 presents the capacity density of various LLMs evaluated using the MMLU benchmark.  The x-axis represents the release date of the models, and the y-axis shows the capacity density.  Two trend lines are included: one showing the density growth rate before the release of ChatGPT, and another showing the growth rate after its release. This visualization demonstrates that the growth of LLM capacity density accelerated significantly after ChatGPT's release, highlighting the impact of this model on subsequent LLM development.", "section": "3 Density Evolution"}]