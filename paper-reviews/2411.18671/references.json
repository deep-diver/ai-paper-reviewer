{"references": [{"fullname_first_author": "Nicolas Carion", "paper_title": "End-to-End Object Detection with Transformers", "publication_date": "2020-08-23", "reason": "This paper introduces the DETR framework, which is foundational to TAPTRv3's architecture and approach to point tracking."}, {"fullname_first_author": "Hongyang Li", "paper_title": "TAPTRv2: Attention-based Position Update Improves Tracking Any Point", "publication_date": "2024-11-27", "reason": "As a direct predecessor to TAPTRv3, this paper builds the core framework and concepts upon which TAPTRv3 is developed and improved."}, {"fullname_first_author": "Carl Doersch", "paper_title": "TAP-Vid: A Benchmark for Tracking Any Point in a Video", "publication_date": "2022-12-01", "reason": "This paper introduces the TAP-Vid benchmark, providing the primary datasets used for evaluating TAPTRv3 and comparing it to other methods."}, {"fullname_first_author": "Zachary Teed", "paper_title": "RAFT: Recurrent All-Pairs Field Transforms for Optical Flow", "publication_date": "2020-08-23", "reason": "RAFT is a significant influence on many point tracking methods, including TAPTRv3, providing advancements in optical flow estimation that improve upon previous cost volume approaches."}, {"fullname_first_author": "Alexey Dosovitskiy", "paper_title": "FlowNet: Learning Optical Flow with Convolutional Networks", "publication_date": "2015-12-07", "reason": "This paper is a seminal work in learning optical flow using convolutional networks, influencing many later approaches including those for point tracking."}]}