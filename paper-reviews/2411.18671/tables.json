[{"content": "| Method | Training Data | Kinetics AJ | Kinetics OA | RGB-Stacking AJ | RGB-Stacking OA | RoboTAP AJ | RoboTAP OA | DAVIS AJ | DAVIS OA |\n|---|---|---|---|---|---|---|---|---|---|---|\n| PIPs [12] | FT\u2020 | 31.7 | 53.7 | 72.9 | \u2013 | 59.1 | \u2013 | \u2013 | 42.2 | 64.8 | 77.7 |\n| PIPs++ [54] | PO\u2020 | \u2013 | 63.5 | \u2013 | \u2013 | 58.5 | \u2013 | \u2013 | 63.0 | \u2013 | 69.1 | \u2013 |\n| TAP-Net [6] | Kub24 | 38.5 | 54.4 | 80.6 | 53.5 | 68.1 | 86.3 | 45.1 | 62.1 | 82.9 | 33.0 | 48.6 | 78.8 |\n| TAPIR [7] | Kub24 | 49.6 | 64.2 | 85.0 | 55.5 | 69.7 | 88.0 | 59.6 | 73.4 | 87.0 | 56.2 | 70.0 | 86.5 |\n| CoTracker [20] | Kub24 | 49.6 | 64.3 | 83.3 | 67.4 | 78.9 | 85.2 | 58.6 | 70.6 | 87.0 | 61.8 | 76.1 | 88.3 |\n| TAPTR [26] | Kub24 | 49.0 | 64.4 | 85.2 | 60.8 | 76.2 | 87.0 | 60.1 | 75.3 | 86.9 | 63.0 | 76.1 | 91.1 |\n| TAPTRv2 [25] | Kub24 | 49.7 | 64.2 | 85.7 | 53.4 | 70.5 | 81.2 | 60.9 | 74.6 | 87.7 | 63.5 | 75.9 | 91.4 |\n| LocoTrack [5] | Kub24 | 52.9 | 66.8 | 85.3 | 69.7 | 83.2 | 89.5 | 62.3 | 76.2 | 87.1 | 63.0 | 75.3 | 87.2 |\n| BootsTAPIR [8] | Kub24+15M | 54.6 | 68.4 | 86.5 | 70.8 | 83.0 | 89.9 | 64.9 | 80.1 | 86.3 | 61.4 | 73.6 | 88.7 |\n| CoTracker3 (online) [19] | Kub64 | 54.1 | 66.6 | 87.1 | 71.1 | 81.9 | 90.3 | 60.8 | 73.7 | 87.1 | 64.5 | 76.7 | 89.7 |\n| CoTracker3 (online) [19] | Kub64+15K | 55.8 | 68.5 | 88.3 | 71.7 | 83.6 | 91.1 | 66.4 | 78.8 | 90.8 | 63.8 | 76.3 | 90.2 |\n| TAPTRv3 (Ours) | Kub24 | 54.5 | 67.5 | 88.2 | 73.0 | 86.2 | 90.0 | 64.6 | 77.2 | 90.1 | 63.2 | 76.7 | 91.0 |", "caption": "Table 1: Comparison of TAPTRv3 with prior methods. Note that, CoTracker3\u2020 is a concurrent work. CoTracker3\u2020 and BootsTAPIR\u2020 both introduced extra data for training. Specifically, CoTracker3\u2020 re-rendered synthetic Kubric videos with length of 64 frames and additionally incorporated 15K real videos. In contrast, BootsTAPIR\u2020 trained on an additional 15M real videos.\nTAPTRv3 obtains state-of-the-art performance on most datasets and remains competitive with methods trained on extra internal data.\nTraining data: (Kub24) Kubric\u00a0[11] with 24 frames per video, (Kub64) Kubric with 64 frames per video, (PO) PointOdyssey\u00a0[54], (FT) FlyingThings++\u00a0[29].", "description": "This table compares the performance of TAPTRv3 against other point tracking methods on four benchmark datasets: Kinetics, RGB-Stacking, RoboTAP, and DAVIS.  It highlights TAPTRv3's state-of-the-art performance, even when compared to methods that used significantly more training data (CoTracker3 and BootsTAPIR used 15K and 15M additional real-world videos respectively, compared to TAPTRv3's 11K synthetic training videos). The table also details the training data used by each method, specifying whether Kubric videos (24 frames or 64 frames per video), PointOdyssey, or FlyingThings++ were used.", "section": "4. Experiments"}, {"content": "| Row | LTA | Vis. Aware | No Window | No Inv. Sup. | CCA | AJ | OA |\n|---|---|---|---|---|---|---|---| \n| 1 | \u2717 | \u2717 | \u2717 | \u2717 | \u2717 | 46.0 | 62.8 | 83.8 |\n| 2 | \u2713 | \u2717 | \u2717 | \u2717 | \u2717 | 48.9 | 63.0 | 85.7 |\n| 3 | \u2713 | \u2713 | \u2717 | \u2717 | \u2717 | 50.1 | 63.1 | 85.9 |\n| 4 | \u2713 | \u2713 | \u2713 | \u2717 | \u2717 | 51.1 | 63.3 | 85.8 |\n| 5 | \u2713 | \u2713 | \u2713 | \u2713 | \u2717 | 51.9 | 64.8 | 86.9 |\n| 6 | \u2713 | \u2713 | \u2713 | \u2713 | \u2713 | **52.9** | **65.8** | **87.2** |", "caption": "Table 5: Ablation on decoder input positions.", "description": "This table presents an ablation study on the impact of different decoder input positions on the model's performance. It compares the model's Average Jaccard (AJ), Average Occlusion Accuracy (<d_avg>), and Overall Accuracy (OA) when using different input positions for the decoder. These positions include predictions from the previous frame and the results from global matching when a scene cut is detected.  The results help determine the optimal input strategy for robust point tracking.", "section": "4.4 Ablation Studies and Analysis"}, {"content": "| Patch-level Similarity | AJ | &lt;\u03b4<sup>x</sup><sub>avg</sub> | OA |\n|---|---|---|---| \n| Element-wise | 52.5 | 65.1 | 85.7 |\n| Every two point | **52.9** | **65.8** | **87.2** |", "caption": "Table 6: Ablation on global matching trigger. Whether to employ the global matching positions when detecting scene cuts. \u201cKin.\u201d is short for TAP-Vid-Kinetics, and \u201cS.C.\u201d indicates scene cuts.", "description": "This table presents an ablation study on the impact of the global matching trigger mechanism on the TAPTRv3 model's performance. It compares the model's performance on the TAP-Vid-Kinetics dataset with and without employing the global matching positions when scene cuts are detected. The results are presented in terms of Average Jaccard (AJ),  < \u03b4\u03b1\u03c5\u03b1 (average precision at thresholds of 1, 2, 4, 8, and 16 pixels), and overall accuracy (OA). A comparison is made for the entire dataset and specifically on subsets containing scene cuts. This helps to determine the effectiveness of triggering global matching only when necessary, avoiding unnecessary computation and potential negative impacts in videos without scene changes.", "section": "4. Experiments"}, {"content": "| Update Methods | AJ | < \n \u03b4 x avg  \n | OA |\n|---|---|---|---|\n| VLTA | 52.6 | 65.3 | 86.8 |\n| MLP | 52.5 | 65.7 | **87.3** |\n| No Updates | **52.9** | **65.8** | 87.2 |", "caption": "Table 7: Ablation on number of context features N2superscript\ud835\udc412N^{2}italic_N start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT.", "description": "This table presents the results of an ablation study on the impact of varying the number of context features used in the Context-aware Cross-Attention (CCA) module of the TAPTRv3 model.  The number of context features is determined by N\u00b2 where N is the grid size used for sampling the features.  The table shows the Average Jaccard (AJ), the percentage of correctly classified occluded points (<\u03b4_avg), and the overall Occlusion Accuracy (OA) for different values of N (N = 1, 3, 5). It demonstrates the effect of the number of context features on the model's performance.", "section": "4.4 Ablation Studies and Analysis"}, {"content": "| Input Positions of Decoder | AJ | &lt;\u03b4xavg | OA |\n|---|---|---|---|\n| Global Matching Calculation | 51.0 | 63.3 | 84.1 |\n| Previous Frame\u2019s Prediction | **52.9** | **65.8** | **87.2** |", "caption": "Table 8: Ablation on memory size of VLTA.", "description": "This table presents the ablation study results on the memory size used in the Visibility-aware Long-Temporal Attention (VLTA) module.  The study tests different memory sizes (12, 24, 48 frames, and all past frames) to determine the impact of memory length on the model's performance.  The results show that increasing the memory size improves the model's accuracy in tracking, as measured by Average Jaccard (AJ), occlusion accuracy (OA), and the average precision of visible points' position (<\u03b4_avg). This demonstrates the effectiveness of VLTA in capturing long-range temporal context for robust point tracking.", "section": "4.4 Ablation Studies and Analysis"}, {"content": "| Input Positions of Decoder | Dataset | AJ | <$\"\\delta^{x}_{avg}$\"< | OA |\n|---|---|---|---|---|\n| Previous Frame\u2019s Prediction | Kin. | 54.5 | 67.5 | 88.2 |\n| Global Matching if S.C. | Kin. | **54.6** | **67.8** | **88.4** |\n| Previous Frame\u2019s Prediction | Kin. w/ S.C. | 54.4 | 66.9 | 86.6 |\n| Global Matching if S.C. | Kin. w/ S.C. | **55.1** | **68.1** | **87.4** |", "caption": "Table 9: Ablation on calculation of similarity map", "description": "This table presents an ablation study on how the similarity map for global matching is calculated in TAPTRv3.  Two methods are compared: using point-level features and using spatial context features. The results show a small improvement when using spatial context features, suggesting that incorporating context information improves the global matching accuracy slightly.  However, given the low computational cost and the infrequency of global matching (only triggered by scene cuts), the authors conclude that including spatial context features is beneficial despite the modest performance gain.", "section": "4. Experiments"}]