[{"figure_path": "https://arxiv.org/html/2412.04146/x2.png", "caption": "Figure 1: Customizable virtual dressing results of our AnyDressing. Reliability: AnyDressing is well-suited for a variety of scenes and complex garments. Compatibility: AnyDressing is compatible with LoRA\u00a0[15] and plugins such as ControlNet\u00a0[55] and FaceID\u00a0[54].", "description": "Figure 1 showcases the versatility and compatibility of the AnyDressing model.  Multiple rows demonstrate successful virtual dressing results across various scenes (realistic, stylized, complex garments), highlighting the model's reliability in handling diverse contexts. The bottom row specifically demonstrates the model's seamless integration with popular extensions like LoRA, ControlNet, and FaceID, enhancing its capabilities and adaptability.", "section": "Abstract"}, {"figure_path": "https://arxiv.org/html/2412.04146/x3.png", "caption": "Figure 2: \nOverview of AnyDressing. Given N\ud835\udc41Nitalic_N target garments, AnyDressing customizes a character dressed in multiple target garments. The GarmentsNet leverages the Garment-Specific Feature Extractor (GFE) module to extract detailed features from multiple garments. The DressingNet integrates these features for virtual dressing using a Dressing-Attention (DA) module and an Instance-Level Garment Localization Learning mechanism. Moreover, the Garment-Enhanced Texture Learning (GTL) strategy further enhances texture details.", "description": "This figure illustrates the AnyDressing pipeline, which takes N garments as input and generates an image of a character wearing those garments.  The process begins with the GarmentsNet, which uses a Garment-Specific Feature Extractor (GFE) to analyze the individual garments. This information is then passed to the DressingNet, which uses a Dressing-Attention (DA) mechanism and an Instance-Level Garment Localization Learning strategy to integrate the garment features into the final image generation process. Finally, a Garment-Enhanced Texture Learning (GTL) process is used to refine details and generate a high-quality image.  The diagram visually details the flow of information through these different modules.", "section": "4. Methodology"}, {"figure_path": "https://arxiv.org/html/2412.04146/x4.png", "caption": "Figure 3: Qualitative comparisons with state-of-the-art methods. Please zoom in for more details.", "description": "This figure provides a qualitative comparison of AnyDressing with several state-of-the-art virtual dressing models. It showcases the results of both single-garment and multi-garment dressing tasks, highlighting AnyDressing's ability to maintain clothing style and texture consistency, while accurately reflecting the text prompt and avoiding background contamination. The superior performance of AnyDressing is evident in the detailed results, especially when handling multiple garments.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.04146/x5.png", "caption": "Figure 4:  Examples of plug-in results of AnyDressing.", "description": "This figure showcases AnyDressing's versatility and compatibility with various community plugins. It presents several example images generated using AnyDressing, each integrated with a different plugin (e.g., ControlNet, IP-Adapter-FaceID, LoRA).  This demonstrates AnyDressing's ability to enhance and customize image generation by incorporating additional controls and features beyond the standard text prompts, resulting in varied and highly controlled image outputs.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.04146/x6.png", "caption": "Figure 5: Ablation results on GFE and IGL modules.", "description": "This figure shows the ablation study on the Garment-Specific Feature Extractor (GFE) and Instance-Level Garment Localization (IGL) modules. The ablation is conducted on the base model, the model with GFE, and the model with both GFE and IGL. The results demonstrate that GFE reduces garment confusion and improves garment consistency, while IGL enhances the fidelity to the text prompts and minimizes background contamination.", "section": "4.2 DressingNet"}, {"figure_path": "https://arxiv.org/html/2412.04146/x7.png", "caption": "Figure 6: Ablation results on GTL module.", "description": "This figure shows the ablation study on the Garment-Enhanced Texture Learning (GTL) module. The GTL module aims to improve the fine-grained texture details in the generated images by incorporating perceptual and high-frequency losses. The figure visually compares the results with and without the GTL module, demonstrating that GTL enhances the texture quality of the generated garments.", "section": "4.3. Garment-Enhanced Texture Learning"}, {"figure_path": "https://arxiv.org/html/2412.04146/x8.png", "caption": "Figure 7: \nExamples of the training dataset I.", "description": "This figure displays examples of the training dataset used in the AnyDressing model.  Each example shows a triplet of images: a model image (a person wearing clothing), an in-shop image of the upper garment, and an image of the lower garment extracted from the model image using a human parsing model. The dataset is designed to train the model to handle multiple clothing items simultaneously, while also accommodating variations in style and texture. The consistent use of a model image ensures that the model focuses on garment-centric details, rather than the broader context of the image.", "section": "5.1 Setup"}, {"figure_path": "https://arxiv.org/html/2412.04146/x9.png", "caption": "Figure 8: \nScreenshot of user study.", "description": "This figure shows a screenshot of the online survey used in the user study.  The survey presents five generated images from different methods for a given reference garment and a text prompt. Participants are asked to rate each image based on four criteria: how well it matches the text prompt, how consistent the clothing textures are with the reference garment, the overall image quality, and a final comprehensive judgment combining all three factors.", "section": "5.1 Setup"}, {"figure_path": "https://arxiv.org/html/2412.04146/x10.png", "caption": "Figure 9: \nExamples of the training dataset II.", "description": "Figure 9 shows examples of the second training dataset used in the AnyDressing model.  This dataset expands on the first by including a hat as an additional garment, resulting in image triplets containing a model image, an in-shop upper garment, a cropped lower garment, and a cropped hat image. Similarly, triplets are generated from lower garment data, resulting in images with a cropped upper garment, an in-shop lower garment, and a cropped hat. This extension demonstrates the ability of the model to handle additional garments.", "section": "7.2 Datasets"}, {"figure_path": "https://arxiv.org/html/2412.04146/x11.png", "caption": "Figure 10: \nQualitative results of more combinations of clothing items.", "description": "This figure displays various example images generated by AnyDressing, showcasing its ability to handle multiple clothing items simultaneously. Each row presents results for different combinations of garments, illustrating the model's versatility in generating images with diverse clothing styles and combinations, all while adhering to the textual prompt given.  The model successfully integrates multiple clothing items without causing significant garment confusion or misalignment.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.04146/x12.png", "caption": "Figure 11: \nMore ablation results on GFE and IGL modules.", "description": "This figure shows the ablation study results of the Garment-Specific Feature Extractor (GFE) module and the Instance-Level Garment Localization (IGL) learning mechanism.  It compares the results of the base model against models incorporating GFE and then IGL. The images illustrate that the GFE module significantly reduces garment confusion and improves garment consistency. Adding the IGL further improves fidelity to the text prompts and minimizes background contamination by ensuring that each garment's attention focuses on the correct area.", "section": "4.2.2 Instance-Level Garment Localization Learning"}, {"figure_path": "https://arxiv.org/html/2412.04146/x13.png", "caption": "Figure 12: \nMore qualitative comparisons I.", "description": "This figure displays qualitative comparison results between AnyDressing and four other state-of-the-art virtual try-on methods for single-garment scenarios. Each row represents a different method, while each column shows the results for a different text prompt and reference garment. The figure allows for a visual comparison of the generated images, enabling an assessment of each method's ability to faithfully reproduce the garment's textures and styles while adhering to the text description.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.04146/x14.png", "caption": "Figure 13: \nMore qualitative comparisons II.", "description": "Figure 13 presents more qualitative comparisons between AnyDressing and other state-of-the-art virtual dressing methods.  It shows the results of generating images of characters wearing various combinations of clothing items based on text prompts. The figure highlights the differences in clothing style, texture consistency, and overall image quality between AnyDressing and competing methods.", "section": "More Comparisons"}, {"figure_path": "https://arxiv.org/html/2412.04146/x15.png", "caption": "Figure 14: \nMore qualitative results I.", "description": "This figure displays more qualitative results of AnyDressing, showcasing the model's ability to generate diverse and high-quality images across a range of scenarios and garment combinations.  Various combinations of clothing items, along with personalized text prompts, demonstrate the model's ability to maintain consistency in clothing style, texture, and overall image quality while adhering to the specified prompts.", "section": "More Visual Results"}, {"figure_path": "https://arxiv.org/html/2412.04146/x16.png", "caption": "Figure 15: \nMore qualitative results II.", "description": "Figure 15 showcases more qualitative results of the AnyDressing model.  The figure displays multiple images generated by the model, each showcasing a man dressed in different outfits and poses. The clothing styles are diverse, ranging from casual sportswear to more formal attire. The backgrounds are also varied, depicting settings like a park, city street, office building, and beach. Each set of images uses the same set of garments but with different poses and backgrounds, demonstrating the versatility and control offered by the AnyDressing model in generating varied depictions while maintaining the integrity of the chosen clothing items.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.04146/x17.png", "caption": "Figure 16: \nMore qualitative results III.", "description": "Figure 16 shows additional qualitative results from the AnyDressing model.  It presents diverse examples of virtual dressing results generated by the model with various clothing items in various styles and scenarios, showcasing the model's ability to handle different combinations of clothing items and produce coherent, high-quality results.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.04146/x18.png", "caption": "Figure 17: \nMore results of combining ControlNet\u00a0[55] and FaceID\u00a0[54].", "description": "This figure showcases the versatility of AnyDressing by demonstrating its seamless integration with ControlNet [55] and FaceID [54].  It displays diverse results generated using AnyDressing, incorporating pose control from ControlNet and identity preservation from FaceID, resulting in a wide range of realistic and stylized virtual dressing outputs.", "section": "More Applications"}]