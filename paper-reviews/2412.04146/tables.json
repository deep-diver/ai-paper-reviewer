[{"content": "| Method | Single Grament | Single Grament | Single Grament | Single Grament | Single Grament | Single Grament | Multiple Graments | Multiple Graments | Multiple Graments |\n|---|---|---|---|---|---|---|---|---|---| \n| [5] VITON-HD | CLIP-T \u2191 | CLIP-I \u2191 | CLIP-AS \u2191 | CLIP-T \u2191 | CLIP-I \u2191 | CLIP-AS \u2191 | CLIP-T \u2191 | CLIP-I* \u2191 | CLIP-AS \u2191 |\n|  |  |  |  |  |  |  |  |  |  |\n| [54] IP-Adapter | 0.268 | 0.644 | 5.674 | 0.272 | 0.632 | 5.678 | 0.277 | 0.523 | 5.795 |\n| [47] StableGarment | 0.285 | 0.583 | 5.781 | 0.281 | 0.587 | 5.648 | 0.284 | 0.556 | 5.735 |\n| [4] MagicClothing | 0.288 | 0.640 | 5.703 | 0.298 | 0.619 | 5.784 | 0.266 | 0.583 | 5.540 |\n| [40] IMAGDressing | 0.202 | 0.734 | 5.077 | 0.230 | 0.684 | 5.133 | 0.242 | 0.614 | 5.291 |\n| **Ours** | 0.289 | 0.741 | 5.881 | 0.296 | 0.710 | 5.931 | 0.296 | 0.734 | 5.874 |", "caption": "Table 1:  Quantitative comparisons with baseline methods for both single-garment and multi-garment evaluation.", "description": "Table 1 presents a quantitative comparison of the proposed AnyDressing model against several baseline methods for virtual clothing generation. The evaluation is performed on two scenarios: single-garment dressing and multi-garment dressing.  For each scenario and method, three metrics are reported: CLIP-T (measures text consistency between the generated image and the text prompt), CLIP-I (measures the consistency of generated clothing textures with the reference images), and CLIP-AS (overall aesthetic quality assessment).  This provides a comprehensive comparison of performance across different models and demonstrates AnyDressing's capabilities in handling both single and multiple garment virtual dressing tasks.", "section": "5. Experiments"}, {"content": "| Method | Texture Consistency \u2191 | Align with Prompt \u2191 | Image Quality \u2191 | Comprehensive Evaluation \u2191 |\n|---|---|---|---|---|\n| IP-Adapter [54] | 0.45% | 6.65% | 11.95% | 2.20% |\n| StableGarment [47] | 1.60% | 4.85% | 2.65% | 2.05% |\n| MagicClothing [4] | 2.05% | 9.00% | 9.70% | 3.75% |\n| IMAGDressing [40] | 2.10% | 2.50% | 3.90% | 1.70% |\n| **Ours** | **93.80%** | **77.00%** | **71.80%** | **90.30%** |", "caption": "Table 2: User study with baseline methods.", "description": "This table presents the results of a user study comparing AnyDressing to several baseline methods for multi-garment virtual dressing.  Users were shown sets of images generated by each method for various prompts and asked to rate them across several aspects: consistency with the reference garments, alignment with the prompt, overall image quality, and a comprehensive evaluation combining these aspects.  The results show a significant preference for AnyDressing across all evaluation criteria.", "section": "5. Experiments"}, {"content": "| Texture | Consistency \u2191 |\n|---|---|", "caption": "Table 3:  Ablation study of AnyDressing.", "description": "This table presents the results of an ablation study conducted on the AnyDressing model. It shows the impact of removing or adding different components of the model on its performance, as measured by CLIP-T, CLIP-I*, and CLIP-AS scores.  The components tested are: Garment-Specific Feature Extractor (GFE), Instance-Level Garment Localization (IGL), and Garment-Enhanced Texture Learning (GTL). The table helps to understand the contribution of each module to the overall performance of the model and identify which modules are most crucial for achieving good results.", "section": "5.4 Ablation Studies"}]