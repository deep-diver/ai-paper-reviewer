<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><title>MagicArticulate: Make Your 3D Models Articulation-Ready &#183; HF Daily Paper Reviews by AI</title>
<meta name=title content="MagicArticulate: Make Your 3D Models Articulation-Ready &#183; HF Daily Paper Reviews by AI"><meta name=description content="MagicArticulate automates 3D model animation preparation by generating skeletons and skinning weights, overcoming prior manual methods' limitations, and introducing Articulation-XL, a large-scale benc..."><meta name=keywords content="Computer Vision,3D Vision,üè¢ Nanyang Technological University,"><link rel=canonical href=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.12135/><link type=text/css rel=stylesheet href=/ai-paper-reviewer/css/main.bundle.min.595affd4445a931ea6d6e3a5a3c709930fa52a60be10b21c6f81fdb8fecaacea33aacedf80cdc88be45f189be14ed4ce53ea74a1e1406fad9cbf90c5ed409173.css integrity="sha512-WVr/1ERakx6m1uOlo8cJkw+lKmC+ELIcb4H9uP7KrOozqs7fgM3Ii+RfGJvhTtTOU+p0oeFAb62cv5DF7UCRcw=="><script type=text/javascript src=/ai-paper-reviewer/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script><script defer type=text/javascript id=script-bundle src=/ai-paper-reviewer/js/main.bundle.min.efbf3b6b987689fffaf2d7b73173d2690c0279a04d444b0537a77d7f4ff6e6d493445400cb0cf56bc0f0f123e19f15394e63cae34e67f069bd013dd5c73df56e.js integrity="sha512-7787a5h2if/68te3MXPSaQwCeaBNREsFN6d9f0/25tSTRFQAywz1a8Dw8SPhnxU5TmPK405n8Gm9AT3Vxz31bg==" data-copy data-copied></script><script src=/ai-paper-reviewer/lib/zoom/zoom.min.37d2094687372da3f7343a221a470f6b8806f7891aa46a5a03966af7f0ebd38b9fe536cb154e6ad28f006d184b294525a7c4054b6bbb4be62d8b453b42db99bd.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ=="></script><link rel=apple-touch-icon sizes=180x180 href=/ai-paper-reviewer/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/ai-paper-reviewer/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/ai-paper-reviewer/favicon-16x16.png><link rel=manifest href=/ai-paper-reviewer/site.webmanifest><meta property="og:url" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.12135/"><meta property="og:site_name" content="HF Daily Paper Reviews by AI"><meta property="og:title" content="MagicArticulate: Make Your 3D Models Articulation-Ready"><meta property="og:description" content="MagicArticulate automates 3D model animation preparation by generating skeletons and skinning weights, overcoming prior manual methods‚Äô limitations, and introducing Articulation-XL, a large-scale benc‚Ä¶"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="paper-reviews"><meta property="article:published_time" content="2025-02-17T00:00:00+00:00"><meta property="article:modified_time" content="2025-02-17T00:00:00+00:00"><meta property="article:tag" content="Computer Vision"><meta property="article:tag" content="3D Vision"><meta property="article:tag" content="üè¢ Nanyang Technological University"><meta property="og:image" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.12135/cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.12135/cover.png"><meta name=twitter:title content="MagicArticulate: Make Your 3D Models Articulation-Ready"><meta name=twitter:description content="MagicArticulate automates 3D model animation preparation by generating skeletons and skinning weights, overcoming prior manual methods‚Äô limitations, and introducing Articulation-XL, a large-scale benc‚Ä¶"><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"Paper Reviews by AI","name":"MagicArticulate: Make Your 3D Models Articulation-Ready","headline":"MagicArticulate: Make Your 3D Models Articulation-Ready","abstract":"MagicArticulate automates 3D model animation preparation by generating skeletons and skinning weights, overcoming prior manual methods\u0026rsquo; limitations, and introducing Articulation-XL, a large-scale benc\u0026hellip;","inLanguage":"en","url":"https:\/\/deep-diver.github.io\/ai-paper-reviewer\/paper-reviews\/2502.12135\/","author":{"@type":"Person","name":"Hugging Face Daily Papers"},"copyrightYear":"2025","dateCreated":"2025-02-17T00:00:00\u002b00:00","datePublished":"2025-02-17T00:00:00\u002b00:00","dateModified":"2025-02-17T00:00:00\u002b00:00","keywords":["Computer Vision","3D Vision","üè¢ Nanyang Technological University"],"mainEntityOfPage":"true","wordCount":"4321"}]</script><meta name=author content="Hugging Face Daily Papers"><link href=https://github.com/deep-diver/paper-reviewer/ rel=me><link href=https://twitter.com/algo_diver/ rel=me><script src=/ai-paper-reviewer/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj+KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script><script defer src=/ai-paper-reviewer/lib/typeit/typeit.umd.1b3200cb448f5cd1f548f2781452643d3511a43584b377b82c03a58055da4fdb7bc8f6c6c2ce846480c7677ff25bfd0d75f15823c09443ab18e0fd2cad792587.js integrity="sha512-GzIAy0SPXNH1SPJ4FFJkPTURpDWEs3e4LAOlgFXaT9t7yPbGws6EZIDHZ3/yW/0NdfFYI8CUQ6sY4P0srXklhw=="></script><script defer src=/ai-paper-reviewer/lib/packery/packery.pkgd.min.js integrity></script><script type=text/javascript src=/ai-paper-reviewer/js/shortcodes/gallery.min.9b4cb28f931ed922c26fb9b2510c2debb370f6a63305050c2af81740b2919883715e24efbbdf3a081496718ec751df3a72729d4d0bc71d6071297563a97ce1ee.js integrity="sha512-m0yyj5Me2SLCb7myUQwt67Nw9qYzBQUMKvgXQLKRmINxXiTvu986CBSWcY7HUd86cnKdTQvHHWBxKXVjqXzh7g=="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-KX0S6Q55Y7"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-KX0S6Q55Y7")</script><meta name=theme-color><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-app.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-firestore.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-auth.js></script><script>const firebaseConfig={apiKey:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",authDomain:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",projectId:"neurips2024-f3065",storageBucket:"neurips2024-f3065.firebasestorage.app",messagingSenderId:"982475958898",appId:"1:982475958898:web:2147e5d7753d6ac091f0eb",measurementId:"G-YQ46HXQ9JS"};var app=firebase.initializeApp(firebaseConfig),db=firebase.firestore(),auth=firebase.auth()</script></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a></div><div class=min-h-[148px]></div><div class="fixed inset-x-0 pl-[24px] pr-[24px]" style=z-index:100><div id=menu-blur class="absolute opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl shadow-2xl"></div><div class="relative max-w-[64rem] ml-auto mr-auto"><div style=padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start space-x-3"><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/ai-paper-reviewer/ class="text-base font-medium text-gray-500 hover:text-gray-900">HF Daily Paper Reviews by AI</a></nav><nav class="hidden md:flex items-center space-x-5 md:ml-12 h-12"><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>About</p></a><a href=/ai-paper-reviewer/2025-03-11/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>2025-03-11</p></a><a href=/ai-paper-reviewer/2025-03-12/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>2025-03-12</p></a><a href=/ai-paper-reviewer/2025-03-13/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>2025-03-13</p></a><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Archive</p></a><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Tags</p></a><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></span><p class="text-base font-medium" title></p></a><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span><p class="text-base font-medium" title></p></a><button id=search-button aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button><div class="ltr:mr-14 rtl:ml-14 flex items-center"><button id=appearance-switcher aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></nav><div class="flex md:hidden items-center space-x-5 md:ml-12 h-12"><span></span>
<button id=search-button-mobile aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></button>
<button id=appearance-switcher-mobile aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400" style=margin-right:5px><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div><div class="-my-2 -mr-2 md:hidden"><label id=menu-button class=block><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper style=padding-top:5px class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li id=menu-close-button><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>About</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-03-11/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>2025-03-11</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-03-12/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>2025-03-12</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-03-13/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>2025-03-13</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Archive</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Tags</p></a></li><li class=mt-1><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li><li class=mt-1><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li></ul></div></label></div></div></div></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("menu-blur");n.style.opacity=t/300})</script><div class="relative flex flex-col grow"><main id=main-content class=grow><article><div id=hero class="h-[150px] md:h-[200px]"></div><div class="fixed inset-x-0 top-0 h-[800px] single_hero_background nozoom" style=background-image:url(/ai-paper-reviewer/paper-reviews/2502.12135/cover_hu646136944668901782.png)><div class="absolute inset-0 bg-gradient-to-t from-neutral dark:from-neutral-800 to-transparent mix-blend-normal"></div><div class="absolute inset-0 opacity-60 bg-gradient-to-t from-neutral dark:from-neutral-800 to-neutral-100 dark:to-neutral-800 mix-blend-normal"></div></div><div id=background-blur class="fixed opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl"></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("background-blur");n.style.opacity=t/300})</script><header id=single_header class="mt-5 max-w-prose"><ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden"><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/>HF Daily Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class=inline><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/>Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/2502.12135/>MagicArticulate: Make Your 3D Models Articulation-Ready</a><span class="px-1 text-primary-500">/</span></li></ol><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">MagicArticulate: Make Your 3D Models Articulation-Ready</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime=2025-02-17T00:00:00+00:00>17 February 2025</time><span class="px-2 text-primary-500">&#183;</span><span>4321 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">21 mins</span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=views_paper-reviews/2502.12135/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=views>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 576 512"><path fill="currentcolor" d="M288 32c-80.8.0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7.0 24.6C17.3 304 48.6 356 95.4 399.4 142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1 3.3-7.9 3.3-16.7.0-24.6-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144 64.5-144 144-144 144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64-11.5.0-22.3-3-31.6-8.4-.2 2.8-.4 5.5-.4 8.4.0 53 43 96 96 96s96-43 96-96-43-96-96-96c-2.8.0-5.6.1-8.4.4 5.3 9.3 8.4 20.1 8.4 31.6z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=likes_paper-reviews/2502.12135/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=likes>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<button id=button_likes class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400" onclick=process_article()>
<span id=button_likes_heart style=display:none class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span><span id=button_likes_emtpty_heart class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M244 84l11.1 12 12-11.98C300.6 51.37 347 36.51 392.6 44.1 461.5 55.58 512 115.2 512 185.1V190.9c0 41.5-17.2 81.2-47.6 109.5L283.7 469.1c-7.5 7-17.4 10.9-27.7 10.9S235.8 476.1 228.3 469.1L47.59 300.4C17.23 272.1.0 232.4.0 190.9V185.1c0-69.9 50.52-129.52 119.4-141 44.7-7.59 92 7.27 124.6 39.9C243.1 84 244 84.01 244 84zm11.1 79.9-45-46.8c-21.7-20.82-52.5-30.7-82.8-25.66C81.55 99.07 48 138.7 48 185.1V190.9c0 28.2 11.71 55.2 32.34 74.4L256 429.3l175.7-164c20.6-19.2 32.3-46.2 32.3-74.4V185.1c0-46.4-33.6-86.03-79.3-93.66C354.4 86.4 323.6 96.28 301.9 117.1l-46.8 46.8z"/></svg>
</span></span><span id=button_likes_text>&nbsp;Like</span></button></span></div><div class="flex flex-row flex-wrap items-center"><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/ai-generated/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">AI Generated
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/-daily-papers/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">ü§ó Daily Papers
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/computer-vision/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Computer Vision
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/3d-vision/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">3D Vision
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/-nanyang-technological-university/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">üè¢ Nanyang Technological University</span></span></span></div></div><div class="flex author"><img class="!mt-0 !mb-0 h-24 w-24 rounded-full ltr:mr-4 rtl:ml-4" width=96 height=96 alt="Hugging Face Daily Papers" src=/ai-paper-reviewer/img/avatar_hu1570846118988919414.png><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">Author</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">Hugging Face Daily Papers</div><div class="text-sm text-neutral-700 dark:text-neutral-400">I am AI, and I review papers on HF Daily Papers</div><div class="text-2xl sm:text-lg"><div class="flex flex-wrap text-neutral-400 dark:text-neutral-500"><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://github.com/deep-diver/paper-reviewer/ target=_blank aria-label=Github rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://twitter.com/algo_diver/ target=_blank aria-label=Twitter rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span></a></div></div></div></div><div class=mb-5></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first lg:ml-auto px-0 lg:order-last ltr:lg:pl-8 rtl:lg:pr-8"><div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-[140px]"><details open id=TOCView class="toc-right mt-0 overflow-y-scroll overscroll-contain scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600 rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 hidden lg:block"><summary class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="min-w-[220px] py-2 border-dotted ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#articulation-xl-dataset>Articulation-XL Dataset</a></li><li><a href=#autoregressive-skeletons>Autoregressive Skeletons</a></li><li><a href=#diffusion-skinning>Diffusion Skinning</a></li><li><a href=#method-limitations>Method Limitations</a></li><li><a href=#future-directions>Future Directions</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><details class="toc-inside mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 lg:hidden"><summary class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#articulation-xl-dataset>Articulation-XL Dataset</a></li><li><a href=#autoregressive-skeletons>Autoregressive Skeletons</a></li><li><a href=#diffusion-skinning>Diffusion Skinning</a></li><li><a href=#method-limitations>Method Limitations</a></li><li><a href=#future-directions>Future Directions</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><script>var margin=200,marginError=50;(function(){var t=$(window),e=$("#TOCView"),s=e.height();function n(){var n=t.height()-margin;s>=n?(e.css("overflow-y","scroll"),e.css("max-height",n+marginError+"px")):(e.css("overflow-y","hidden"),e.css("max-height","9999999px"))}t.on("resize",n),$(document).ready(n)})()</script></div></div><div class="min-w-0 min-h-0 max-w-fit"><div class="article-content max-w-prose mb-20"><br><div class="flex flex-row flex-wrap items-center space-x-2"><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 48 48" fill="none"><rect width="48" height="48" fill="#fff" fill-opacity=".01"/><path d="M18 43V22c0-3.3137 2.6863-6 6-6s6 2.6863 6 6V43" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M12 40V22c0-6.6274 5.3726-12 12-12s12 5.3726 12 12V40" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M6 35V22C6 12.0589 14.0589 4 24 4s18 8.0589 18 18V35" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 44V31" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 24.625v-2.75" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/></svg>
</span></span><span>2502.12135</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg fill="#000" height="800" width="800" id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 511.999 511.999"><g><g><path d="M421.578 190.264l-99.847-99.847c-2.439-2.439-6.391-2.439-8.829.0L82.824 320.495c-2.439 2.439-2.439 6.392.0 8.829l99.847 99.847c2.439 2.439 6.391 2.439 8.829.0l230.078-230.078C424.017 196.655 424.017 192.703 421.578 190.264z"/></g></g><g><g><path d="M506.511 87.672 424.323 5.484c-7.308-7.31-19.175-7.315-26.488.0L348.219 55.1c-2.439 2.439-2.439 6.391.0 8.829l99.847 99.847c2.439 2.437 6.391 2.437 8.829.0l49.616-49.616C513.826 106.847 513.826 94.987 506.511 87.672z"/></g></g><g><g><path d="M508.133 491.11c-1.054-9.556-9.489-16.599-19.104-16.599H111.633l36.058-15.163c4.088-1.719 5.131-7.034 1.994-10.17l-86.854-86.854c-3.137-3.135-8.451-2.094-10.17 1.994C52.224 365.359 2.052 484.66 1.627 485.707c-5.815 13.208 4.855 27.01 18.107 26.263H489.52C500.566 511.97 509.379 502.408 508.133 491.11z"/></g></g></svg>
</span></span><span>Chaoyue Song et el.</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span>ü§ó 2025-02-18</span></span></span></div></div><p><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://arxiv.org/abs/2502.12135 target=_self role=button>‚Üó arXiv
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://huggingface.co/papers/2502.12135 target=_self role=button>‚Üó Hugging Face</a></p><audio controls><source src=https://ai-paper-reviewer.com/2502.12135/podcast.wav type=audio/wav>Your browser does not support the audio element.</audio><h3 class="relative group">TL;DR<div id=tldr class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#tldr aria-label=Anchor>#</a></span></h3><div class="lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl"><p>Creating realistic animations from 3D models traditionally requires manually adding skeletons and skinning weights, a time-consuming and labor-intensive process. The lack of large-scale datasets has also hindered the development of automated solutions. This paper presents significant challenges in current 3D animation pipelines.</p><p>The paper introduces MagicArticulate, a novel framework that addresses these challenges. It uses an auto-regressive transformer for skeleton generation, handling varying bone numbers efficiently. Skinning weights are predicted using a functional diffusion process that incorporates volumetric geodesic distance priors, improving accuracy on complex topologies. <strong>MagicArticulate significantly outperforms existing methods across diverse object categories</strong>, achieving high-quality articulation for realistic animation. The paper also introduces Articulation-XL, a substantial benchmark dataset to facilitate future research.</p></div><h4 class="relative group">Key Takeaways<div id=key-takeaways class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#key-takeaways aria-label=Anchor>#</a></span></h4><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-5e6773607cdd0c5c977111532aeb3aac></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-5e6773607cdd0c5c977111532aeb3aac",{strings:[" MagicArticulate automates the process of making static 3D models animation-ready. "],speed:10,lifeLike:!0,startDelay:0,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-71d5faf24224ca9b67f832d374e0f2c8></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-71d5faf24224ca9b67f832d374e0f2c8",{strings:[" Articulation-XL, a new benchmark dataset with over 33k 3D models and high-quality articulation annotations, is introduced. "],speed:10,lifeLike:!0,startDelay:1e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-91a490f8ff9563079f705b64bf1e784a></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-91a490f8ff9563079f705b64bf1e784a",{strings:[" The proposed auto-regressive sequence modeling method for skeleton generation and the functional diffusion process for skinning weight prediction significantly outperform existing methods. "],speed:10,lifeLike:!0,startDelay:2e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><h4 class="relative group">Why does it matter?<div id=why-does-it-matter class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#why-does-it-matter aria-label=Anchor>#</a></span></h4><p>This paper is crucial because <strong>it addresses a critical bottleneck in 3D content creation</strong>: the time-consuming manual process of making 3D models animation-ready. By introducing a novel automated framework and a large-scale benchmark dataset, it significantly accelerates the workflow and opens exciting avenues for research in computer graphics and animation. This will benefit researchers across diverse fields utilizing 3D models, including gaming, VR/AR, and robotics.</p><hr><h4 class="relative group">Visual Insights<div id=visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#visual-insights aria-label=Anchor>#</a></span></h4><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.12135/x1.png alt></figure></p><blockquote><p>üîº This figure showcases the capabilities of MagicArticulate. Given three different 3D models (a boy, a giraffe, and a dog), the system automatically generates a corresponding skeleton and computes skinning weights, which are necessary for realistic animation. The input 3D models themselves were created using Rodin Gen-1 and Tripo 2.0, and the resulting images were produced using the Maya Software Renderer. This demonstrates the system&rsquo;s ability to prepare 3D models for animation without manual intervention.</p><details><summary>read the caption</summary>Figure 1: Given a 3D model, MagicArticulate can automatically generate the skeleton and skinning weights, making the model articulation-ready without further manual refinement. The input meshes are generated by Rodin Gen-1 [50] and Tripo 2.0 [1]. The meshes and skeletons are rendered using Maya Software Renderer [19].</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_centering ltx_align_middle" id=S5.T1.6><tbody class=ltx_tbody><tr class=ltx_tr id=S5.T1.6.1.1><td class="ltx_td ltx_border_tt" id=S5.T1.6.1.1.1></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T1.6.1.1.2>Dataset</td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T1.6.1.1.3>CD-J2J</td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T1.6.1.1.4>CD-J2B</td><td class="ltx_td ltx_align_center ltx_border_tt" id=S5.T1.6.1.1.5>CD-B2B</td></tr><tr class=ltx_tr id=S5.T1.6.2.2><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T1.6.2.2.1>RigNet*</td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T1.6.2.2.2 rowspan=7><span class="ltx_text ltx_font_italic" id=S5.T1.6.2.2.2.1>ModelsRes.</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T1.6.2.2.3>7.132</td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T1.6.2.2.4>5.486</td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T1.6.2.2.5>4.640</td></tr><tr class=ltx_tr id=S5.T1.6.3.3><td class="ltx_td ltx_align_center" id=S5.T1.6.3.3.1>Pinocchio</td><td class="ltx_td ltx_align_center" id=S5.T1.6.3.3.2>6.852</td><td class="ltx_td ltx_align_center" id=S5.T1.6.3.3.3>4.824</td><td class="ltx_td ltx_align_center" id=S5.T1.6.3.3.4>4.089</td></tr><tr class=ltx_tr id=S5.T1.6.4.4><td class="ltx_td ltx_align_center" id=S5.T1.6.4.4.1>Ours-hier*</td><td class="ltx_td ltx_align_center" id=S5.T1.6.4.4.2>4.451</td><td class="ltx_td ltx_align_center" id=S5.T1.6.4.4.3>3.454</td><td class="ltx_td ltx_align_center" id=S5.T1.6.4.4.4>2.998</td></tr><tr class=ltx_tr id=S5.T1.6.5.5><td class="ltx_td ltx_align_center" id=S5.T1.6.5.5.1>RigNet</td><td class="ltx_td ltx_align_center" id=S5.T1.6.5.5.2>4.143</td><td class="ltx_td ltx_align_center" id=S5.T1.6.5.5.3>2.961</td><td class="ltx_td ltx_align_center" id=S5.T1.6.5.5.4>2.675</td></tr><tr class=ltx_tr id=S5.T1.6.6.6><td class="ltx_td ltx_align_center" id=S5.T1.6.6.6.1>Ours-spatial*</td><td class="ltx_td ltx_align_center" id=S5.T1.6.6.6.2>4.103</td><td class="ltx_td ltx_align_center" id=S5.T1.6.6.6.3>3.101</td><td class="ltx_td ltx_align_center" id=S5.T1.6.6.6.4>2.672</td></tr><tr class=ltx_tr id=S5.T1.6.7.7><td class="ltx_td ltx_align_center" id=S5.T1.6.7.7.1>Ours-hier</td><td class="ltx_td ltx_align_center" id=S5.T1.6.7.7.2>3.654</td><td class="ltx_td ltx_align_center" id=S5.T1.6.7.7.3>2.775</td><td class="ltx_td ltx_align_center" id=S5.T1.6.7.7.4>2.412</td></tr><tr class=ltx_tr id=S5.T1.6.8.8><td class="ltx_td ltx_align_center" id=S5.T1.6.8.8.1>Ours-spatial</td><td class="ltx_td ltx_align_center" id=S5.T1.6.8.8.2><span class="ltx_text ltx_font_bold" id=S5.T1.6.8.8.2.1>3.343</span></td><td class="ltx_td ltx_align_center" id=S5.T1.6.8.8.3><span class="ltx_text ltx_font_bold" id=S5.T1.6.8.8.3.1>2.455</span></td><td class="ltx_td ltx_align_center" id=S5.T1.6.8.8.4><span class="ltx_text ltx_font_bold" id=S5.T1.6.8.8.4.1>2.140</span></td></tr><tr class=ltx_tr id=S5.T1.6.9.9><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T1.6.9.9.1>Pinocchio</td><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id=S5.T1.6.9.9.2 rowspan=4><span class="ltx_text ltx_font_italic" id=S5.T1.6.9.9.2.1>Arti-XL</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T1.6.9.9.3>8.360</td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T1.6.9.9.4>6.677</td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T1.6.9.9.5>5.689</td></tr><tr class=ltx_tr id=S5.T1.6.10.10><td class="ltx_td ltx_align_center" id=S5.T1.6.10.10.1>RigNet</td><td class="ltx_td ltx_align_center" id=S5.T1.6.10.10.2>7.478</td><td class="ltx_td ltx_align_center" id=S5.T1.6.10.10.3>5.892</td><td class="ltx_td ltx_align_center" id=S5.T1.6.10.10.4>4.932</td></tr><tr class=ltx_tr id=S5.T1.6.11.11><td class="ltx_td ltx_align_center" id=S5.T1.6.11.11.1>Ours-hier</td><td class="ltx_td ltx_align_center" id=S5.T1.6.11.11.2>3.025</td><td class="ltx_td ltx_align_center" id=S5.T1.6.11.11.3>2.408</td><td class="ltx_td ltx_align_center" id=S5.T1.6.11.11.4>2.083</td></tr><tr class=ltx_tr id=S5.T1.6.12.12><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T1.6.12.12.1>Ours-spatial</td><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T1.6.12.12.2><span class="ltx_text ltx_font_bold" id=S5.T1.6.12.12.2.1>2.586</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T1.6.12.12.3><span class="ltx_text ltx_font_bold" id=S5.T1.6.12.12.3.1>1.959</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T1.6.12.12.4><span class="ltx_text ltx_font_bold" id=S5.T1.6.12.12.4.1>1.661</span></td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents a quantitative comparison of different methods for skeleton generation. The comparison uses three Chamfer distance metrics (CD-J2J, CD-J2B, and CD-B2B) to evaluate the accuracy of generated skeletons against ground truth skeletons. Results are shown for two datasets: Articulation-XL and ModelsResource. Lower values for each metric indicate better performance. The table also notes that some models were trained on Articulation-XL and then tested on ModelsResource, indicated by an asterisk (*).</p><details><summary>read the caption</summary>Table 1: Quantitative comparison on skeleton generation. We compare different methods using CD-J2J, CD-J2B, and CD-B2B as evaluation metrics on both Articulation-XL (Arti-XL) and ModelsResource (Modelres.). Lower values indicate better performance. The metrics are in units of 10‚àí2superscript10210^{-2}10 start_POSTSUPERSCRIPT - 2 end_POSTSUPERSCRIPT. Here, * denotes models trained on Articulation-XL and tested on ModelsResource.</details></blockquote><h3 class="relative group">In-depth insights<div id=in-depth-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#in-depth-insights aria-label=Anchor>#</a></span></h3><h4 class="relative group">Articulation-XL Dataset<div id=articulation-xl-dataset class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#articulation-xl-dataset aria-label=Anchor>#</a></span></h4><p>The Articulation-XL dataset represents a <strong>significant contribution</strong> to the field of 3D model articulation. Its large scale, exceeding 33,000 3D models, addresses a critical limitation in previous research, namely the scarcity of sufficiently large and diverse datasets for training robust learning-based methods. The <strong>high-quality articulation annotations</strong> included, carefully curated from Objaverse-XL, are crucial for model evaluation and ensure reliable training data. The dataset&rsquo;s diversity, encompassing various object categories with different bone counts and joint structures, enables the development of truly generalizable models that can handle the complexity and variations inherent in real-world 3D objects. Further, its comprehensive metadata and readily available format should ease reproducibility and accelerate future research in articulated object generation and animation. The inclusion of category labels, obtained through VLM annotation, adds further value, facilitating analysis and enabling researchers to explore specific object types and articulation patterns. <strong>Articulation-XL&rsquo;s impact is far-reaching</strong>, providing a robust benchmark for evaluating progress in automatic skeleton generation, skinning weight prediction, and ultimately, realistic 3D animation.</p><h4 class="relative group">Autoregressive Skeletons<div id=autoregressive-skeletons class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#autoregressive-skeletons aria-label=Anchor>#</a></span></h4><p>The concept of &ldquo;Autoregressive Skeletons&rdquo; in 3D model articulation presents a novel approach to skeletal generation. Instead of relying on predefined templates or heuristic methods, an autoregressive model learns to predict the skeletal structure sequentially, bone by bone. This allows for handling the variability in the number and arrangement of bones across different object categories. <strong>The autoregressive nature naturally captures the dependencies between bones</strong>, creating more realistic and coherent skeletons. This approach is particularly advantageous when dealing with complex, diverse shapes where traditional methods struggle. Furthermore, using an autoregressive approach <strong>enables the model to adapt dynamically to each object&rsquo;s unique structure</strong>, providing flexibility and scalability. By learning the bone connectivity and spatial relationships in a sequential manner, the model implicitly learns an understanding of skeletal topology and articulation, leading to improved accuracy and generalization capability. A key aspect is the integration of shape information, potentially through a shape encoder, to condition the autoregressive generation, thus ensuring the skeleton aligns well with the 3D model&rsquo;s geometry. Overall, <strong>Autoregressive Skeletons offers a powerful and flexible method for 3D skeletal modeling</strong>, overcoming limitations of previous template-based and heuristic approaches.</p><h4 class="relative group">Diffusion Skinning<div id=diffusion-skinning class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#diffusion-skinning aria-label=Anchor>#</a></span></h4><p>Diffusion models, known for their prowess in image generation, offer a novel approach to skinning weight prediction in 3D models. Instead of directly regressing weights, a diffusion process adds noise to the weight function, gradually transforming it into pure noise. A neural network is then trained to reverse this process, denoising the weights step-by-step to recover the original, smoothly varying skinning weights. This approach offers several advantages. Firstly, <strong>it naturally handles complex mesh topologies</strong>, unlike traditional methods which often struggle with intricate geometries. Secondly, <strong>it elegantly addresses varying skeleton structures</strong>, adapting to different numbers of bones and joints. Finally, <strong>the functional diffusion framework allows for smooth and coherent weight generation</strong>, improving the realism and quality of animation. The incorporation of volumetric geodesic distance priors further enhances the method&rsquo;s accuracy, guiding the weight prediction towards biologically plausible results. While this approach is promising, further exploration is needed to fully understand the computational cost and the method&rsquo;s scalability for extremely high-resolution models. Nevertheless, diffusion skinning presents a powerful, data-driven alternative to traditional methods, offering significant potential for advancing the field of 3D character animation.</p><h4 class="relative group">Method Limitations<div id=method-limitations class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#method-limitations aria-label=Anchor>#</a></span></h4><p>The research paper&rsquo;s methodology, while innovative, presents several limitations. <strong>Data limitations</strong> are significant; the Articulation-XL dataset, though large, may not fully represent the vast diversity of 3D model types and articulation styles encountered in real-world applications. <strong>Generalization to unseen data</strong> is another concern. While the model performs well on the training data, its ability to accurately generate skeletons and skinning weights for completely novel, unseen object types remains uncertain. <strong>Computational cost</strong> is also a factor; the autoregressive approach, while effective, is computationally expensive, potentially hindering its scalability to larger and more complex datasets. Finally, <strong>mesh quality</strong> significantly impacts the method&rsquo;s performance; the model struggles with low-resolution or noisy meshes, highlighting the need for improved robustness to imperfect input data. These limitations suggest future research should focus on addressing these challenges through expanding the dataset&rsquo;s scope, developing more efficient algorithms, and enhancing robustness to noisy input.</p><h4 class="relative group">Future Directions<div id=future-directions class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#future-directions aria-label=Anchor>#</a></span></h4><p>The research paper&rsquo;s &lsquo;Future Directions&rsquo; section could explore several promising avenues. <strong>Improving robustness to diverse object categories</strong> is crucial; the current method struggles with complex topologies and unusual shapes. Addressing this requires exploring more sophisticated shape representations and potentially incorporating physics-based modeling into the skeleton generation process. <strong>Expanding the dataset</strong> to include a wider variety of objects and articulation types is also vital, particularly encompassing those with subtle or highly specific joint structures and behaviors, such as soft robotics or flexible materials. <strong>Exploring alternative sequence modeling techniques</strong> such as hierarchical or graph-based methods could enhance the model&rsquo;s capability in capturing complex skeletal structures and interdependencies between joints. Finally, <strong>combining the model with other AI techniques</strong>, such as text-to-3D generation and physics simulations, would open up new possibilities for creative and realistic animation, streamlining the entire 3D content creation pipeline. <strong>Addressing the coarse mesh issue</strong> is also paramount; research into more robust mesh processing and input methods is needed. The ultimate goal is a fully automated and highly generalizable system for creating high-fidelity articulation-ready 3D models.</p><h3 class="relative group">More visual insights<div id=more-visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#more-visual-insights aria-label=Anchor>#</a></span></h3><details><summary>More on figures</summary><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.12135/x2.png alt></figure></p><blockquote><p>üîº This figure visualizes the categories of 3D models included in the Articulation-XL dataset using a word cloud. The size of each word is proportional to the frequency of its corresponding category in the dataset. This provides a quick overview of the types of 3D models represented, highlighting the prevalence of certain categories over others.</p><details><summary>read the caption</summary>(a) Word cloud of Articulation-XL categories.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.12135/x3.png alt></figure></p><blockquote><p>üîº This pie chart shows the distribution of 3D models across different categories in the Articulation-XL dataset. The categories represent the types of objects included in the dataset, such as animals, characters, vehicles, and more. The size of each slice corresponds to the relative proportion of models belonging to that category within the dataset.</p><details><summary>read the caption</summary>(b) Breakdown of Articulation-XL categories.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.12135/x4.png alt></figure></p><blockquote><p>üîº The figure shows the distribution of the number of bones present in the 3D models within the Articulation-XL dataset. The x-axis represents the number of bones, and the y-axis represents the frequency or count of models with that specific number of bones. This histogram visually illustrates the range and concentration of bone counts across the dataset, providing insights into the complexity and diversity of the 3D model skeletons.</p><details><summary>read the caption</summary>(c) Bone number distributions of Articulation-XL.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.12135/x5.png alt></figure></p><blockquote><p>üîº This figure presents a statistical overview of the Articulation-XL dataset. It includes a word cloud summarizing the categories of 3D models in the dataset, a pie chart showing the distribution of these categories, and a histogram illustrating the distribution of bone numbers per model. This provides insights into the diversity and complexity of the data.</p><details><summary>read the caption</summary>Figure 2: Articulation-XL statistics.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.12135/x6.png alt></figure></p><blockquote><p>üîº This figure showcases examples of high-quality 3D models with their corresponding skeletons from the Articulation-XL dataset. It also displays examples of 3D models that were excluded from the dataset due to poorly defined or inaccurate skeletal annotations. This visual comparison highlights the standards for quality control applied in creating the Articulation-XL benchmark.</p><details><summary>read the caption</summary>Figure 3: Some examples from Articulation-XL alongside examples of poorly defined skeletons that were curated out.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.12135/x7.png alt></figure></p><blockquote><p>üîº This figure illustrates the process of autoregressive skeleton generation, a key component of the MagicArticulate framework. The process begins with an input 3D mesh, from which point cloud samples are extracted from the mesh surface. These points are then fed into a shape encoder that generates fixed-length shape tokens. These tokens are concatenated with skeleton tokens, forming the input to an autoregressive transformer. The transformer predicts the skeleton, conditioned on the input shape, generating a sequence of bones and joints that represent the object&rsquo;s skeletal structure. The output is then a complete articulated skeleton. The input mesh shown in the figure is generated using Rodin Gen-1 [50].</p><details><summary>read the caption</summary>Figure 4: Overview of our method for auto-regressive skeleton generation. Given an input mesh, we begin by sampling point clouds from its surface. These sampled points are then encoded into fixed-length shape tokens, which are appended to the start of skeleton tokens to achieve auto-regressive skeleton generation conditioned on input shapes. The input mesh is generated by Rodin Gen-1 [50].</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.12135/x8.png alt></figure></p><blockquote><p>üîº This figure illustrates two different approaches for ordering bones during skeleton generation: spatial sequence ordering and hierarchical sequence ordering. In spatial ordering, bones are ordered based on the spatial coordinates of their joints, prioritizing z, then y, and finally x coordinates. This approach results in a sequence that doesn&rsquo;t necessarily reflect the hierarchical relationships within the skeleton. In contrast, hierarchical ordering leverages the bone hierarchy, starting with the root bone and recursively processing child bones layer by layer. This ensures that the ordering respects the parent-child relationships within the skeletal structure. The numbers in the figure represent the bone ordering indices for each method, highlighting the difference in sequencing.</p><details><summary>read the caption</summary>Figure 5: Spatial sequence ordering versus hierarchical sequence ordering. The numbers indicate the bone ordering indices.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.12135/x9.png alt></figure></p><blockquote><p>üîº Figure 6 presents a comparative analysis of skeleton generation results obtained using three different methods: the proposed method (Ours), RigNet, and Pinocchio. The comparison is shown for two datasets: ModelsResource (left column) and Articulation-XL (right column). For each dataset and method, several examples are displayed, showing the automatically generated skeletons alongside the corresponding artist-created reference skeletons. The figure highlights that the proposed method produces skeletons that more accurately reflect the artist-created references, demonstrating improved performance in handling the diverse object categories within the datasets. In contrast, RigNet and Pinocchio exhibit difficulties in generating accurate skeletons, especially for object categories that deviate significantly from the datasets they were trained on.</p><details><summary>read the caption</summary>Figure 6: Comparison of skeleton creation results on ModelsResource (left) and Articulation-XL (right). Our generated skeletons more closely resemble the artist-created references, while RigNet and Pinocchio struggle to handle various object categories.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.12135/x10.png alt></figure></p><blockquote><p>üîº Figure 7 showcases a comparison of skeleton generation results across various object categories using three different methods: the proposed method, RigNet [43], and Pinocchio [3]. The input 3D models were generated using Tripo 2.0 [1]. The figure demonstrates that the proposed method exhibits superior generalization capabilities compared to RigNet and Pinocchio, producing more accurate and realistic skeletons for a wider variety of object types.</p><details><summary>read the caption</summary>Figure 7: Skeleton creation results on 3D generated meshes. Our method has a better generalization performance than both RigNet [43] and Pinocchio [3] across difference object categories. The 3D models are generated by Tripo 2.0 [1].</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.12135/x11.png alt></figure></p><blockquote><p>üîº This figure shows a comparison of skinning weight prediction results between the proposed method and two baseline methods (GVB and RigNet) on two datasets: ModelsResource and Articulation-XL. For each dataset, several example 3D models are displayed. Each model shows the artist-created ground truth skinning weights, along with the skinning weights generated by each method. The accompanying L1 error maps visually represent the difference between the predicted and ground truth skinning weights, with lower values indicating higher accuracy. The figure highlights the superior performance of the proposed method in generating more accurate and realistic skinning weights.</p><details><summary>read the caption</summary>Figure 8: Comparisons with previous methods for skinning weight prediction on ModelsResource (top) and Articulation-XL (bottom). We visualize skinning weights and L1 error maps. For more results, please refer to the supplementary materials.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.12135/x12.png alt></figure></p><blockquote><p>üîº Figure S9 illustrates the architecture of the functional diffusion model used for skinning weight prediction in MagicArticulate. The model takes as input a set of noisy skinning weight functions, represented as {(x, f<sub>t</sub>(x)) | x ‚àà ùí´}, where x represents a vertex on the 3D mesh and f<sub>t</sub>(x) is the noisy skinning weight at that vertex at time step t. Crucially, the model also incorporates both the skeleton and shape features (from a pre-trained encoder, reference [52] in the paper) as conditioning information, which guides the denoising process. The model utilizes cross-attention and self-attention mechanisms to process this combined input and generate a refined set of skinning weights that approximate the target weights.</p><details><summary>read the caption</summary>Figure S9: Overview of the function diffusion architecture for skinning weight prediction. Given a set of noised skinning weight functions {(x,ft‚Å¢(x))‚à£x‚ààùí´}conditional-setùë•subscriptùëìùë°ùë•ùë•ùí´\{(x,f_{t}(x))\mid x\in\mathcal{P}\}{ ( italic_x , italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_x ) ) ‚à£ italic_x ‚àà caligraphic_P }, conditioned on skeleton and shape features from [52], we denoise the skinning weight functions to approximate the target weights.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.12135/x13.png alt></figure></p><blockquote><p>üîº This figure illustrates the process of adding noise to the skinning weight function during the functional diffusion process for skinning weight prediction. It shows how the original skinning weight function, f‚ÇÄ(x), is modified by adding noise, g(x), to produce the noised function, f‚Çú(x). The noise level is controlled by the parameter t, which ranges from 0 to 1. This process is a key step in the functional diffusion framework used to predict skinning weights efficiently and accurately for 3D models with complex topologies.</p><details><summary>read the caption</summary>Figure S10: Process of adding noise to the skinning weight function. Given x‚ààùí´ùë•ùí´x\in\mathcal{P}italic_x ‚àà caligraphic_P and the original skinning weight function f0‚Å¢(x)subscriptùëì0ùë•f_{0}(x)italic_f start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ( italic_x ), we add the noise function g‚Å¢(x)ùëîùë•g(x)italic_g ( italic_x ) to obtain the noised function ft‚Å¢(x)subscriptùëìùë°ùë•f_{t}(x)italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_x ).</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.12135/x14.png alt></figure></p><blockquote><p>üîº This figure compares the performance of different skeleton generation methods on 3D models from various sources. The input meshes are obtained from three distinct methods: 3D model generation, 3D scanning, and 3D reconstruction. The comparison showcases how well each method generalizes to different data types and the resulting diversity in 3D model quality, highlighting the strengths and weaknesses of each approach in terms of generating accurate and usable skeletons.</p><details><summary>read the caption</summary>Figure S11: Comparison of skeleton generation methods on out-of-domain data. The input meshes are from 3D generation, 3D scan, and 3D reconstruction.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.12135/x15.png alt></figure></p><blockquote><p>üîº This figure showcases a qualitative comparison of skeleton generation methods. The left side presents results on the ModelsResource dataset, while the right shows results from the larger, more diverse Articulation-XL dataset. For each dataset, results are displayed for three methods: artist-created skeletons (ground truth), results from the authors&rsquo; proposed MagicArticulate method, and results from the RigNet and Pinocchio baseline methods. The visual comparison demonstrates that MagicArticulate produces skeletons that more accurately match the artist-created references, especially when dealing with diverse and complex object shapes. RigNet and Pinocchio, relying on pre-defined templates or less adaptable approaches, struggle to generate skeletons of the same quality, especially for objects outside their usual category.</p><details><summary>read the caption</summary>Figure S12: Comparison of skeleton generation methods on ModelsResource (left) and Articulation-XL (right). Our results more closely resemble the artist-created references, while RigNet and Pinocchio struggle to handle various object categories.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.12135/x16.png alt></figure></p><blockquote><p>üîº This figure demonstrates the robustness of the proposed skeleton generation method to variations in object orientation. Four views of the same 3D model are shown, each rotated along a different axis (original, 45 degrees along the y-axis, 90 degrees along the z-axis, and 135 degrees along the x-axis). Despite the changes in viewpoint, the generated skeletons in each orientation maintain anatomical correctness and are suitable for rigging, showcasing the method&rsquo;s adaptability and reliability.</p><details><summary>read the caption</summary>Figure S13: Skeleton results on 3D models with different orientations. Although minor differences may appear in the generated skeletons, all results maintain anatomically valid and suitable for rigging purposes.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.12135/x17.png alt></figure></p><blockquote><p>üîº This figure compares the performance of different skinning weight prediction methods. The top three rows show results from the ModelsResource dataset, and the bottom three rows display results from the Articulation-XL dataset. For each method (Ours, RigNet, GVB), the figure visualizes both the predicted skinning weights and their corresponding L1 error maps, enabling a direct comparison of accuracy. The error maps highlight discrepancies between predicted and ground truth weights, indicating areas where the method&rsquo;s performance is better or worse.</p><details><summary>read the caption</summary>Figure S14: Comparison of skinning weight prediction methods on ModelsResource (first three rows) and Articulation-XL (last three rows). We visualize the predicted skinning weights alongside their corresponding L1 error maps.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.12135/x18.png alt></figure></p><blockquote><p>üîº This figure shows the detailed instructions given to the Vision-Language Model (VLM) for the task of data filtering. The instructions guide the VLM on how to assess the quality of a skeleton within a 3D model using four rendered views of both the 3D mesh and the skeleton. The VLM evaluates based on criteria like whether joints and bones extend beyond the mesh&rsquo;s boundaries and how well the skeleton&rsquo;s pose aligns with the model&rsquo;s anatomy. The instructions also outline a three-point rating system (Poor, Average, Good) and provide example evaluations demonstrating the process.</p><details><summary>read the caption</summary>Figure S15: Input instructions to VLM for data filtering.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.12135/x19.png alt></figure></p><blockquote><p>üîº The figure shows the detailed instructions given to the Vision-Language Model (VLM) for the task of automatically assigning categories to 3D objects. The instructions emphasize the importance of carefully examining eight sub-images of each 3D object (four RGB images and four normal maps, each from a different angle) before assigning categories. A comprehensive list of categories is provided, including character, animal, furniture, electronic device, mythical creature, anatomy, tool, planet, musical instrument, sculpture, jewelry, accessory, paper, anthropomorphic object, toy, clothing, food, scanned data, architecture, vehicle, plant, weapon, household item, and miscellaneous. The instructions also include an example of the expected output format.</p><details><summary>read the caption</summary>Figure S16: Input instructions to VLM for category labeling.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.12135/x20.png alt></figure></p><blockquote><p>üîº This figure shows example images used to train a Vision-Language Model (VLM) for filtering low-quality 3D models and skeletons from the dataset. The images consist of four views of the same 3D object, rendered with its skeleton. The VLM is trained to assess the quality of the skeleton based on its alignment with the mesh and its overall anatomical plausibility. This process ensures that only high-quality articulation data is used for training the MagicArticulate model.</p><details><summary>read the caption</summary>Figure S17: Input rendered examples to VLM for data filtering.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.12135/x21.png alt></figure></p><blockquote><p>üîº This figure shows example images used to train a Vision-Language Model (VLM) for automatically assigning category labels to 3D models in the Articulation-XL dataset. Each image contains eight sub-images: four RGB images and four corresponding normal maps, each from a different viewpoint. These images serve as input to the VLM, which learns to associate visual features with specific object categories.</p><details><summary>read the caption</summary>Figure S18: Input rendered examples to VLM for category labeling.</details></blockquote></details><details><summary>More on tables</summary><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id=S5.T2.5><tbody class=ltx_tbody><tr class=ltx_tr id=S5.T2.5.1.1><td class="ltx_td ltx_border_tt" id=S5.T2.5.1.1.1></td><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S5.T2.5.1.1.2>Dataset</th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S5.T2.5.1.1.3>Precision</th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S5.T2.5.1.1.4>Recall</th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S5.T2.5.1.1.5>avg L1</th></tr><tr class=ltx_tr id=S5.T2.5.2.2><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T2.5.2.2.1>GVB</td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T2.5.2.2.2 rowspan=3><span class="ltx_text ltx_font_italic" id=S5.T2.5.2.2.2.1>ModelsResource</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T2.5.2.2.3>69.3%</td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T2.5.2.2.4>79.2%</td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T2.5.2.2.5>0.687</td></tr><tr class=ltx_tr id=S5.T2.5.3.3><td class="ltx_td ltx_align_center" id=S5.T2.5.3.3.1>RigNet</td><td class="ltx_td ltx_align_center" id=S5.T2.5.3.3.2>77.1%</td><td class="ltx_td ltx_align_center" id=S5.T2.5.3.3.3><span class="ltx_text ltx_font_bold" id=S5.T2.5.3.3.3.1>83.5%</span></td><td class="ltx_td ltx_align_center" id=S5.T2.5.3.3.4>0.464</td></tr><tr class=ltx_tr id=S5.T2.5.4.4><td class="ltx_td ltx_align_center" id=S5.T2.5.4.4.1>Ours</td><td class="ltx_td ltx_align_center" id=S5.T2.5.4.4.2><span class="ltx_text ltx_font_bold" id=S5.T2.5.4.4.2.1>82.1%</span></td><td class="ltx_td ltx_align_center" id=S5.T2.5.4.4.3>81.6%</td><td class="ltx_td ltx_align_center" id=S5.T2.5.4.4.4><span class="ltx_text ltx_font_bold" id=S5.T2.5.4.4.4.1>0.398</span></td></tr><tr class=ltx_tr id=S5.T2.5.5.5><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T2.5.5.5.1>GVB</td><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id=S5.T2.5.5.5.2 rowspan=3><span class="ltx_text ltx_font_italic" id=S5.T2.5.5.5.2.1>Articulation-XL</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T2.5.5.5.3>75.7%</td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T2.5.5.5.4>68.3%</td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T2.5.5.5.5>0.724</td></tr><tr class=ltx_tr id=S5.T2.5.6.6><td class="ltx_td ltx_align_center" id=S5.T2.5.6.6.1>RigNet</td><td class="ltx_td ltx_align_center" id=S5.T2.5.6.6.2>72.4%</td><td class="ltx_td ltx_align_center" id=S5.T2.5.6.6.3>71.1%</td><td class="ltx_td ltx_align_center" id=S5.T2.5.6.6.4>0.698</td></tr><tr class=ltx_tr id=S5.T2.5.7.7><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T2.5.7.7.1>Ours</td><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T2.5.7.7.2><span class="ltx_text ltx_font_bold" id=S5.T2.5.7.7.2.1>80.7%</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T2.5.7.7.3><span class="ltx_text ltx_font_bold" id=S5.T2.5.7.7.3.1>77.2%</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T2.5.7.7.4><span class="ltx_text ltx_font_bold" id=S5.T2.5.7.7.4.1>0.337</span></td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents a quantitative comparison of skinning weight prediction performance between three methods: the proposed method, Geodesic Voxel Binding (GVB), and RigNet. The comparison uses three metrics: Precision, Recall, and the average L1-norm error. Higher precision and recall values indicate better performance in accurately identifying the influence of joints on vertices. A lower average L1-norm error signifies better accuracy in predicting the skinning weights themselves.</p><details><summary>read the caption</summary>Table 2: Quantitative comparison on skinning weight prediction. We compare our method with GVB and RigNet. For Precision and Recall, larger values indicate better performance. For average L1-norm error, smaller values are preferred.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id=S5.T3.5><tbody class=ltx_tbody><tr class=ltx_tr id=S5.T3.5.1.1><td class="ltx_td ltx_border_tt" id=S5.T3.5.1.1.1></td><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S5.T3.5.1.1.2>CD-J2J</th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S5.T3.5.1.1.3>CD-J2B</th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S5.T3.5.1.1.4>CD-B2B</th></tr><tr class=ltx_tr id=S5.T3.5.2.2><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id=S5.T3.5.2.2.1>w/o data filtering</th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id=S5.T3.5.2.2.2>2.982</th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id=S5.T3.5.2.2.3>2.327</th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id=S5.T3.5.2.2.4>2.015</th></tr><tr class=ltx_tr id=S5.T3.5.3.3><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T3.5.3.3.1>4,096 points</td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T3.5.3.3.2>2.635</td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T3.5.3.3.3>2.024</td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T3.5.3.3.4>1.727</td></tr><tr class=ltx_tr id=S5.T3.5.4.4><td class="ltx_td ltx_align_center" id=S5.T3.5.4.4.1>12,288 points</td><td class="ltx_td ltx_align_center" id=S5.T3.5.4.4.2>2.685</td><td class="ltx_td ltx_align_center" id=S5.T3.5.4.4.3>2.048</td><td class="ltx_td ltx_align_center" id=S5.T3.5.4.4.4>1.760</td></tr><tr class=ltx_tr id=S5.T3.5.5.5><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T3.5.5.5.1>Ours (8,192)</td><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T3.5.5.5.2><span class="ltx_text ltx_font_bold" id=S5.T3.5.5.5.2.1>2.586</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T3.5.5.5.3><span class="ltx_text ltx_font_bold" id=S5.T3.5.5.5.3.1>1.959</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T3.5.5.5.4><span class="ltx_text ltx_font_bold" id=S5.T3.5.5.5.4.1>1.661</span></td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents the results of ablation studies conducted on the skeleton generation aspect of the MagicArticulate model. It shows how different choices in the model&rsquo;s design impact its performance. Specifically, it examines the effect of data filtering, and the number of sampled points used as input, on the model&rsquo;s accuracy in generating skeletons for 3D models. The performance is evaluated using three Chamfer Distance metrics: CD-J2J, CD-J2B, and CD-B2B, measuring the distances between the generated skeleton and ground truth skeleton joints and bones. Lower values indicate better performance.</p><details><summary>read the caption</summary>Table 3: Ablation studies for skeleton generation.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id=S5.T4.5><tbody class=ltx_tbody><tr class=ltx_tr id=S5.T4.5.1.1><td class="ltx_td ltx_border_tt" id=S5.T4.5.1.1.1></td><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S5.T4.5.1.1.2>Precision</th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S5.T4.5.1.1.3>Recall</th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S5.T4.5.1.1.4>avg L1</th></tr><tr class=ltx_tr id=S5.T4.5.2.2><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T4.5.2.2.1>w/o geodesic dist.</td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T4.5.2.2.2>81.5%</td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T4.5.2.2.3>77.7%</td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T4.5.2.2.4>0.444</td></tr><tr class=ltx_tr id=S5.T4.5.3.3><td class="ltx_td ltx_align_center" id=S5.T4.5.3.3.1>w/o weights norm</td><td class="ltx_td ltx_align_center" id=S5.T4.5.3.3.2>82.0%</td><td class="ltx_td ltx_align_center" id=S5.T4.5.3.3.3>77.9%</td><td class="ltx_td ltx_align_center" id=S5.T4.5.3.3.4>0.436</td></tr><tr class=ltx_tr id=S5.T4.5.4.4><td class="ltx_td ltx_align_center" id=S5.T4.5.4.4.1>w/o shape features</td><td class="ltx_td ltx_align_center" id=S5.T4.5.4.4.2>81.4%</td><td class="ltx_td ltx_align_center" id=S5.T4.5.4.4.3>81.3%</td><td class="ltx_td ltx_align_center" id=S5.T4.5.4.4.4>0.412</td></tr><tr class=ltx_tr id=S5.T4.5.5.5><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T4.5.5.5.1>Ours</td><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T4.5.5.5.2><span class="ltx_text ltx_font_bold" id=S5.T4.5.5.5.2.1>82.1%</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T4.5.5.5.3><span class="ltx_text ltx_font_bold" id=S5.T4.5.5.5.3.1>81.6%</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T4.5.5.5.4><span class="ltx_text ltx_font_bold" id=S5.T4.5.5.5.4.1>0.398</span></td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents the results of ablation studies conducted to evaluate the impact of different components within the skinning weight prediction framework. It shows how the performance metrics (Precision, Recall, and average L1) change when specific components are removed or altered, thus revealing the contribution of each component to the overall accuracy of the skinning weight prediction.</p><details><summary>read the caption</summary>Table 4: Ablation studies on skinning weight prediction.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id=S8.T5.7><tbody class=ltx_tbody><tr class=ltx_tr id=S8.T5.7.1.1><td class="ltx_td ltx_border_tt" id=S8.T5.7.1.1.1></td><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S8.T5.7.1.1.2><span class=ltx_text id=S8.T5.7.1.1.2.1 style=font-size:144%>Dataset</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S8.T5.7.1.1.3><span class=ltx_text id=S8.T5.7.1.1.3.1 style=font-size:144%>Precision</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S8.T5.7.1.1.4><span class=ltx_text id=S8.T5.7.1.1.4.1 style=font-size:144%>Recall</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S8.T5.7.1.1.5><span class=ltx_text id=S8.T5.7.1.1.5.1 style=font-size:144%>avg L1</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S8.T5.7.1.1.6><span class=ltx_text id=S8.T5.7.1.1.6.1 style=font-size:144%>avg Dist.</span></th></tr><tr class=ltx_tr id=S8.T5.7.2.2><td class="ltx_td ltx_align_center ltx_border_t" id=S8.T5.7.2.2.1><span class=ltx_text id=S8.T5.7.2.2.1.1 style=font-size:144%>GVB</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S8.T5.7.2.2.2 rowspan=3><span class="ltx_text ltx_font_italic" id=S8.T5.7.2.2.2.1 style=font-size:144%>ModelsResource</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S8.T5.7.2.2.3><span class=ltx_text id=S8.T5.7.2.2.3.1 style=font-size:144%>69.3%</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S8.T5.7.2.2.4><span class=ltx_text id=S8.T5.7.2.2.4.1 style=font-size:144%>79.2%</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S8.T5.7.2.2.5><span class=ltx_text id=S8.T5.7.2.2.5.1 style=font-size:144%>0.687</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S8.T5.7.2.2.6><span class=ltx_text id=S8.T5.7.2.2.6.1 style=font-size:144%>0.0067</span></td></tr><tr class=ltx_tr id=S8.T5.7.3.3><td class="ltx_td ltx_align_center" id=S8.T5.7.3.3.1><span class=ltx_text id=S8.T5.7.3.3.1.1 style=font-size:144%>RigNet</span></td><td class="ltx_td ltx_align_center" id=S8.T5.7.3.3.2><span class=ltx_text id=S8.T5.7.3.3.2.1 style=font-size:144%>77.1%</span></td><td class="ltx_td ltx_align_center" id=S8.T5.7.3.3.3><span class="ltx_text ltx_font_bold" id=S8.T5.7.3.3.3.1 style=font-size:144%>83.5%</span></td><td class="ltx_td ltx_align_center" id=S8.T5.7.3.3.4><span class=ltx_text id=S8.T5.7.3.3.4.1 style=font-size:144%>0.464</span></td><td class="ltx_td ltx_align_center" id=S8.T5.7.3.3.5><span class=ltx_text id=S8.T5.7.3.3.5.1 style=font-size:144%>0.0054</span></td></tr><tr class=ltx_tr id=S8.T5.7.4.4><td class="ltx_td ltx_align_center" id=S8.T5.7.4.4.1><span class=ltx_text id=S8.T5.7.4.4.1.1 style=font-size:144%>Ours</span></td><td class="ltx_td ltx_align_center" id=S8.T5.7.4.4.2><span class="ltx_text ltx_font_bold" id=S8.T5.7.4.4.2.1 style=font-size:144%>82.1%</span></td><td class="ltx_td ltx_align_center" id=S8.T5.7.4.4.3><span class=ltx_text id=S8.T5.7.4.4.3.1 style=font-size:144%>81.6%</span></td><td class="ltx_td ltx_align_center" id=S8.T5.7.4.4.4><span class="ltx_text ltx_font_bold" id=S8.T5.7.4.4.4.1 style=font-size:144%>0.398</span></td><td class="ltx_td ltx_align_center" id=S8.T5.7.4.4.5><span class="ltx_text ltx_font_bold" id=S8.T5.7.4.4.5.1 style=font-size:144%>0.0039</span></td></tr><tr class=ltx_tr id=S8.T5.7.5.5><td class="ltx_td ltx_align_center ltx_border_t" id=S8.T5.7.5.5.1><span class=ltx_text id=S8.T5.7.5.5.1.1 style=font-size:144%>GVB</span></td><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id=S8.T5.7.5.5.2 rowspan=3><span class="ltx_text ltx_font_italic" id=S8.T5.7.5.5.2.1 style=font-size:144%>Articulation-XL</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S8.T5.7.5.5.3><span class=ltx_text id=S8.T5.7.5.5.3.1 style=font-size:144%>75.7%</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S8.T5.7.5.5.4><span class=ltx_text id=S8.T5.7.5.5.4.1 style=font-size:144%>68.3%</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S8.T5.7.5.5.5><span class=ltx_text id=S8.T5.7.5.5.5.1 style=font-size:144%>0.724</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S8.T5.7.5.5.6><span class=ltx_text id=S8.T5.7.5.5.6.1 style=font-size:144%>0.0095</span></td></tr><tr class=ltx_tr id=S8.T5.7.6.6><td class="ltx_td ltx_align_center" id=S8.T5.7.6.6.1><span class=ltx_text id=S8.T5.7.6.6.1.1 style=font-size:144%>RigNet</span></td><td class="ltx_td ltx_align_center" id=S8.T5.7.6.6.2><span class=ltx_text id=S8.T5.7.6.6.2.1 style=font-size:144%>72.4%</span></td><td class="ltx_td ltx_align_center" id=S8.T5.7.6.6.3><span class=ltx_text id=S8.T5.7.6.6.3.1 style=font-size:144%>71.1%</span></td><td class="ltx_td ltx_align_center" id=S8.T5.7.6.6.4><span class=ltx_text id=S8.T5.7.6.6.4.1 style=font-size:144%>0.698</span></td><td class="ltx_td ltx_align_center" id=S8.T5.7.6.6.5><span class=ltx_text id=S8.T5.7.6.6.5.1 style=font-size:144%>0.0091</span></td></tr><tr class=ltx_tr id=S8.T5.7.7.7><td class="ltx_td ltx_align_center ltx_border_bb" id=S8.T5.7.7.7.1><span class=ltx_text id=S8.T5.7.7.7.1.1 style=font-size:144%>Ours</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S8.T5.7.7.7.2><span class="ltx_text ltx_font_bold" id=S8.T5.7.7.7.2.1 style=font-size:144%>80.7%</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S8.T5.7.7.7.3><span class="ltx_text ltx_font_bold" id=S8.T5.7.7.7.3.1 style=font-size:144%>77.2%</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S8.T5.7.7.7.4><span class="ltx_text ltx_font_bold" id=S8.T5.7.7.7.4.1 style=font-size:144%>0.337</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S8.T5.7.7.7.5><span class="ltx_text ltx_font_bold" id=S8.T5.7.7.7.5.1 style=font-size:144%>0.0050</span></td></tr></tbody></table></table></figure><blockquote><p>üîº Table S5 presents a quantitative comparison of skinning weight prediction methods, specifically comparing the proposed method against Geodesic Voxel Binding (GVB) and RigNet. The evaluation metrics include precision, recall (higher values are better), average L1-norm error, and average distance error (lower values are better). This allows for a comprehensive assessment of the accuracy and effectiveness of each method in predicting skinning weights.</p><details><summary>read the caption</summary>Table S5: Quantitative comparison on skinning weight prediction. We compare our method with GVB and RigNet. For Precision and Recall, larger values indicate better performance. For average L1-norm error and average distance error, smaller values are preferred.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id=S8.T6.6><tbody class=ltx_tbody><tr class=ltx_tr id=S8.T6.6.1.1><td class="ltx_td ltx_border_tt" id=S8.T6.6.1.1.1></td><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S8.T6.6.1.1.2><span class=ltx_text id=S8.T6.6.1.1.2.1 style=font-size:144%>Precision</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S8.T6.6.1.1.3><span class=ltx_text id=S8.T6.6.1.1.3.1 style=font-size:144%>Recall</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S8.T6.6.1.1.4><span class=ltx_text id=S8.T6.6.1.1.4.1 style=font-size:144%>avg L1</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S8.T6.6.1.1.5><span class=ltx_text id=S8.T6.6.1.1.5.1 style=font-size:144%>avg Dist.</span></th></tr><tr class=ltx_tr id=S8.T6.6.2.2><td class="ltx_td ltx_align_center ltx_border_t" id=S8.T6.6.2.2.1><span class=ltx_text id=S8.T6.6.2.2.1.1 style=font-size:144%>w/o geodesic dist.</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S8.T6.6.2.2.2><span class=ltx_text id=S8.T6.6.2.2.2.1 style=font-size:144%>81.5%</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S8.T6.6.2.2.3><span class=ltx_text id=S8.T6.6.2.2.3.1 style=font-size:144%>77.7%</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S8.T6.6.2.2.4><span class=ltx_text id=S8.T6.6.2.2.4.1 style=font-size:144%>0.444</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S8.T6.6.2.2.5><span class=ltx_text id=S8.T6.6.2.2.5.1 style=font-size:144%>0.0046</span></td></tr><tr class=ltx_tr id=S8.T6.6.3.3><td class="ltx_td ltx_align_center" id=S8.T6.6.3.3.1><span class=ltx_text id=S8.T6.6.3.3.1.1 style=font-size:144%>w/o weights norm</span></td><td class="ltx_td ltx_align_center" id=S8.T6.6.3.3.2><span class=ltx_text id=S8.T6.6.3.3.2.1 style=font-size:144%>82.0%</span></td><td class="ltx_td ltx_align_center" id=S8.T6.6.3.3.3><span class=ltx_text id=S8.T6.6.3.3.3.1 style=font-size:144%>77.9%</span></td><td class="ltx_td ltx_align_center" id=S8.T6.6.3.3.4><span class=ltx_text id=S8.T6.6.3.3.4.1 style=font-size:144%>0.436</span></td><td class="ltx_td ltx_align_center" id=S8.T6.6.3.3.5><span class=ltx_text id=S8.T6.6.3.3.5.1 style=font-size:144%>0.0045</span></td></tr><tr class=ltx_tr id=S8.T6.6.4.4><td class="ltx_td ltx_align_center" id=S8.T6.6.4.4.1><span class=ltx_text id=S8.T6.6.4.4.1.1 style=font-size:144%>w/o shape features</span></td><td class="ltx_td ltx_align_center" id=S8.T6.6.4.4.2><span class=ltx_text id=S8.T6.6.4.4.2.1 style=font-size:144%>81.4%</span></td><td class="ltx_td ltx_align_center" id=S8.T6.6.4.4.3><span class=ltx_text id=S8.T6.6.4.4.3.1 style=font-size:144%>81.3%</span></td><td class="ltx_td ltx_align_center" id=S8.T6.6.4.4.4><span class=ltx_text id=S8.T6.6.4.4.4.1 style=font-size:144%>0.412</span></td><td class="ltx_td ltx_align_center" id=S8.T6.6.4.4.5><span class=ltx_text id=S8.T6.6.4.4.5.1 style=font-size:144%>0.0042</span></td></tr><tr class=ltx_tr id=S8.T6.6.5.5><td class="ltx_td ltx_align_center ltx_border_bb" id=S8.T6.6.5.5.1><span class=ltx_text id=S8.T6.6.5.5.1.1 style=font-size:144%>Ours</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S8.T6.6.5.5.2><span class="ltx_text ltx_font_bold" id=S8.T6.6.5.5.2.1 style=font-size:144%>82.1%</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S8.T6.6.5.5.3><span class="ltx_text ltx_font_bold" id=S8.T6.6.5.5.3.1 style=font-size:144%>81.6%</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S8.T6.6.5.5.4><span class="ltx_text ltx_font_bold" id=S8.T6.6.5.5.4.1 style=font-size:144%>0.398</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S8.T6.6.5.5.5><span class="ltx_text ltx_font_bold" id=S8.T6.6.5.5.5.1 style=font-size:144%>0.0039</span></td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents the results of ablation studies conducted on the ModelsResource dataset to evaluate the impact of different components on skinning weight prediction. The ablation studies remove or modify key parts of the proposed method (e.g., geodesic distance, weight normalization, shape features) to isolate their contribution to the overall prediction accuracy. The table shows the precision, recall, average L1-norm error, and average distance error for each ablation experiment, allowing for a quantitative analysis of the importance of each component.</p><details><summary>read the caption</summary>Table S6: Ablation studies on ModelsResource for skinning weight prediction.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id=S8.T7.6><thead class=ltx_thead><tr class=ltx_tr id=S8.T7.6.1.1><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S8.T7.6.1.1.1><span class=ltx_text id=S8.T7.6.1.1.1.1 style=font-size:144%>Category</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S8.T7.6.1.1.2><span class=ltx_text id=S8.T7.6.1.1.2.1 style=font-size:144%># Objects</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S8.T7.6.1.1.3><span class=ltx_text id=S8.T7.6.1.1.3.1 style=font-size:144%>Category</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S8.T7.6.1.1.4><span class=ltx_text id=S8.T7.6.1.1.4.1 style=font-size:144%># Objects</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S8.T7.6.1.1.5><span class=ltx_text id=S8.T7.6.1.1.5.1 style=font-size:144%>Category</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S8.T7.6.1.1.6><span class=ltx_text id=S8.T7.6.1.1.6.1 style=font-size:144%># Objects</span></th></tr></thead><tbody class=ltx_tbody><tr class=ltx_tr id=S8.T7.6.2.1><td class="ltx_td ltx_align_center ltx_border_t" id=S8.T7.6.2.1.1><span class=ltx_text id=S8.T7.6.2.1.1.1 style=font-size:144%>character</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S8.T7.6.2.1.2><span class=ltx_text id=S8.T7.6.2.1.2.1 style=font-size:144%>16020</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S8.T7.6.2.1.3><span class=ltx_text id=S8.T7.6.2.1.3.1 style=font-size:144%>miscellaneous</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S8.T7.6.2.1.4><span class=ltx_text id=S8.T7.6.2.1.4.1 style=font-size:144%>584</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S8.T7.6.2.1.5><span class=ltx_text id=S8.T7.6.2.1.5.1 style=font-size:144%>architecture</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S8.T7.6.2.1.6><span class=ltx_text id=S8.T7.6.2.1.6.1 style=font-size:144%>132</span></td></tr><tr class=ltx_tr id=S8.T7.6.3.2><td class="ltx_td ltx_align_center" id=S8.T7.6.3.2.1><span class=ltx_text id=S8.T7.6.3.2.1.1 style=font-size:144%>anthropomorphic</span></td><td class="ltx_td ltx_align_center" id=S8.T7.6.3.2.2><span class=ltx_text id=S8.T7.6.3.2.2.1 style=font-size:144%>13393</span></td><td class="ltx_td ltx_align_center" id=S8.T7.6.3.2.3><span class=ltx_text id=S8.T7.6.3.2.3.1 style=font-size:144%>scanned data</span></td><td class="ltx_td ltx_align_center" id=S8.T7.6.3.2.4><span class=ltx_text id=S8.T7.6.3.2.4.1 style=font-size:144%>546</span></td><td class="ltx_td ltx_align_center" id=S8.T7.6.3.2.5><span class=ltx_text id=S8.T7.6.3.2.5.1 style=font-size:144%>planet</span></td><td class="ltx_td ltx_align_center" id=S8.T7.6.3.2.6><span class=ltx_text id=S8.T7.6.3.2.6.1 style=font-size:144%>49</span></td></tr><tr class=ltx_tr id=S8.T7.6.4.3><td class="ltx_td ltx_align_center" id=S8.T7.6.4.3.1><span class=ltx_text id=S8.T7.6.4.3.1.1 style=font-size:144%>animal</span></td><td class="ltx_td ltx_align_center" id=S8.T7.6.4.3.2><span class=ltx_text id=S8.T7.6.4.3.2.1 style=font-size:144%>4760</span></td><td class="ltx_td ltx_align_center" id=S8.T7.6.4.3.3><span class=ltx_text id=S8.T7.6.4.3.3.1 style=font-size:144%>plant</span></td><td class="ltx_td ltx_align_center" id=S8.T7.6.4.3.4><span class=ltx_text id=S8.T7.6.4.3.4.1 style=font-size:144%>382</span></td><td class="ltx_td ltx_align_center" id=S8.T7.6.4.3.5><span class=ltx_text id=S8.T7.6.4.3.5.1 style=font-size:144%>paper</span></td><td class="ltx_td ltx_align_center" id=S8.T7.6.4.3.6><span class=ltx_text id=S8.T7.6.4.3.6.1 style=font-size:144%>46</span></td></tr><tr class=ltx_tr id=S8.T7.6.5.4><td class="ltx_td ltx_align_center" id=S8.T7.6.5.4.1><span class=ltx_text id=S8.T7.6.5.4.1.1 style=font-size:144%>mythical creature</span></td><td class="ltx_td ltx_align_center" id=S8.T7.6.5.4.2><span class=ltx_text id=S8.T7.6.5.4.2.1 style=font-size:144%>4734</span></td><td class="ltx_td ltx_align_center" id=S8.T7.6.5.4.3><span class=ltx_text id=S8.T7.6.5.4.3.1 style=font-size:144%>accessories</span></td><td class="ltx_td ltx_align_center" id=S8.T7.6.5.4.4><span class=ltx_text id=S8.T7.6.5.4.4.1 style=font-size:144%>293</span></td><td class="ltx_td ltx_align_center" id=S8.T7.6.5.4.5><span class=ltx_text id=S8.T7.6.5.4.5.1 style=font-size:144%>musical instrument</span></td><td class="ltx_td ltx_align_center" id=S8.T7.6.5.4.6><span class=ltx_text id=S8.T7.6.5.4.6.1 style=font-size:144%>25</span></td></tr><tr class=ltx_tr id=S8.T7.6.6.5><td class="ltx_td ltx_align_center" id=S8.T7.6.6.5.1><span class=ltx_text id=S8.T7.6.6.5.1.1 style=font-size:144%>toy</span></td><td class="ltx_td ltx_align_center" id=S8.T7.6.6.5.2><span class=ltx_text id=S8.T7.6.6.5.2.1 style=font-size:144%>1360</span></td><td class="ltx_td ltx_align_center" id=S8.T7.6.6.5.3><span class=ltx_text id=S8.T7.6.6.5.3.1 style=font-size:144%>vehicle</span></td><td class="ltx_td ltx_align_center" id=S8.T7.6.6.5.4><span class=ltx_text id=S8.T7.6.6.5.4.1 style=font-size:144%>283</span></td><td class="ltx_td ltx_align_center" id=S8.T7.6.6.5.5><span class=ltx_text id=S8.T7.6.6.5.5.1 style=font-size:144%>sporting goods</span></td><td class="ltx_td ltx_align_center" id=S8.T7.6.6.5.6><span class=ltx_text id=S8.T7.6.6.5.6.1 style=font-size:144%>21</span></td></tr><tr class=ltx_tr id=S8.T7.6.7.6><td class="ltx_td ltx_align_center" id=S8.T7.6.7.6.1><span class=ltx_text id=S8.T7.6.7.6.1.1 style=font-size:144%>weapon</span></td><td class="ltx_td ltx_align_center" id=S8.T7.6.7.6.2><span class=ltx_text id=S8.T7.6.7.6.2.1 style=font-size:144%>1257</span></td><td class="ltx_td ltx_align_center" id=S8.T7.6.7.6.3><span class=ltx_text id=S8.T7.6.7.6.3.1 style=font-size:144%>sculpture</span></td><td class="ltx_td ltx_align_center" id=S8.T7.6.7.6.4><span class=ltx_text id=S8.T7.6.7.6.4.1 style=font-size:144%>276</span></td><td class="ltx_td ltx_align_center" id=S8.T7.6.7.6.5><span class=ltx_text id=S8.T7.6.7.6.5.1 style=font-size:144%>armor</span></td><td class="ltx_td ltx_align_center" id=S8.T7.6.7.6.6><span class=ltx_text id=S8.T7.6.7.6.6.1 style=font-size:144%>13</span></td></tr><tr class=ltx_tr id=S8.T7.6.8.7><td class="ltx_td ltx_align_center" id=S8.T7.6.8.7.1><span class=ltx_text id=S8.T7.6.8.7.1.1 style=font-size:144%>anatomy</span></td><td class="ltx_td ltx_align_center" id=S8.T7.6.8.7.2><span class=ltx_text id=S8.T7.6.8.7.2.1 style=font-size:144%>1227</span></td><td class="ltx_td ltx_align_center" id=S8.T7.6.8.7.3><span class=ltx_text id=S8.T7.6.8.7.3.1 style=font-size:144%>household items</span></td><td class="ltx_td ltx_align_center" id=S8.T7.6.8.7.4><span class=ltx_text id=S8.T7.6.8.7.4.1 style=font-size:144%>274</span></td><td class="ltx_td ltx_align_center" id=S8.T7.6.8.7.5><span class=ltx_text id=S8.T7.6.8.7.5.1 style=font-size:144%>robot</span></td><td class="ltx_td ltx_align_center" id=S8.T7.6.8.7.6><span class=ltx_text id=S8.T7.6.8.7.6.1 style=font-size:144%>4</span></td></tr><tr class=ltx_tr id=S8.T7.6.9.8><td class="ltx_td ltx_align_center ltx_border_bb" id=S8.T7.6.9.8.1><span class=ltx_text id=S8.T7.6.9.8.1.1 style=font-size:144%>clothing</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S8.T7.6.9.8.2><span class=ltx_text id=S8.T7.6.9.8.2.1 style=font-size:144%>595</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S8.T7.6.9.8.3><span class=ltx_text id=S8.T7.6.9.8.3.1 style=font-size:144%>food</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S8.T7.6.9.8.4><span class=ltx_text id=S8.T7.6.9.8.4.1 style=font-size:144%>206</span></td><td class="ltx_td ltx_border_bb" id=S8.T7.6.9.8.5></td><td class="ltx_td ltx_border_bb" id=S8.T7.6.9.8.6></td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents a detailed breakdown of the object categories included in the Articulation-XL dataset. It shows the number of 3D models belonging to each category, providing a quantitative overview of the dataset&rsquo;s composition and diversity across various object types.</p><details><summary>read the caption</summary>Table S7: Object counts for each category in the Articulation-XL dataset.</details></blockquote></details><h3 class="relative group">Full paper<div id=full-paper class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#full-paper aria-label=Anchor>#</a></span></h3><div id=gallery-8b78c629b8d17a20bf49fb8d8d622d44 class=gallery><img src=https://ai-paper-reviewer.com/2502.12135/1.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.12135/2.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.12135/3.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.12135/4.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.12135/5.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.12135/6.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.12135/7.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.12135/8.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.12135/9.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.12135/10.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.12135/11.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.12135/12.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.12135/13.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.12135/14.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.12135/15.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.12135/16.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.12135/17.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.12135/18.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.12135/19.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.12135/20.png class="grid-w50 md:grid-w33 xl:grid-w25"></div></div><section class="flex flex-row flex-wrap justify-center pt-4 text-xl"><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.12135/&amp;title=MagicArticulate:%20Make%20Your%203D%20Models%20Articulation-Ready" title="Share on LinkedIn" aria-label="Share on LinkedIn"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://twitter.com/intent/tweet/?url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.12135/&amp;text=MagicArticulate:%20Make%20Your%203D%20Models%20Articulation-Ready" title="Tweet on Twitter" aria-label="Tweet on Twitter"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="mailto:?body=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.12135/&amp;subject=MagicArticulate:%20Make%20Your%203D%20Models%20Articulation-Ready" title="Send via email" aria-label="Send via email"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1-27.64 140.9 68.65 266.2 199.1 285.1 19.01 2.888 36.17-12.26 36.17-31.49l1e-4-.6631c0-15.74-11.44-28.88-26.84-31.24-84.35-12.98-149.2-86.13-149.2-174.2.0-102.9 88.61-185.5 193.4-175.4 91.54 8.869 158.6 91.25 158.6 183.2v16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98.0036c-7.299.0-13.2 4.992-15.12 11.68-24.85-12.15-54.24-16.38-86.06-5.106-38.75 13.73-68.12 48.91-73.72 89.64-9.483 69.01 43.81 128 110.9 128 26.44.0 50.43-9.544 69.59-24.88 24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3c0-149.2-133.9-265.632-287.3-235.57zM239.1 304.3c-26.47.0-48-21.56-48-48.05s21.53-48.05 48-48.05 48 21.56 48 48.05-20.6 48.05-48 48.05z"/></svg></span></a></section></div><script>var oid="views_paper-reviews/2502.12135/index.md",oid_likes="likes_paper-reviews/2502.12135/index.md"</script><script type=text/javascript src=/ai-paper-reviewer/js/page.min.0860cf4e04fa2d72cc33ddba263083464d48f67de06114529043cb4623319efed4f484fd7f1730df5abea0e2da6f3538855634081d02f2d6e920b956f063e823.js integrity="sha512-CGDPTgT6LXLMM926JjCDRk1I9n3gYRRSkEPLRiMxnv7U9IT9fxcw31q+oOLabzU4hVY0CB0C8tbpILlW8GPoIw=="></script></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="flex group mr-3" href=/ai-paper-reviewer/paper-reviews/2502.11663/><span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&larr;</span>
<span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">MaskGWM: A Generalizable Driving World Model with Video Mask Reconstruction</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2025-02-17T00:00:00+00:00>17 February 2025</time>
</span></span></a></span><span><a class="flex text-right group ml-3" href=/ai-paper-reviewer/paper-reviews/2502.12152/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Learning Getting-Up Policies for Real-World Humanoid Robots</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2025-02-17T00:00:00+00:00>17 February 2025</time>
</span></span><span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&larr;</span></a></span></div></div><div class=pt-3><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class=pt-3><script src=https://utteranc.es/client.js repo=pmnxis/pmnxis.github.io issue-term=pathname label=Comment theme=dark-blue crossorigin=anonymous async></script></div></div></footer></article><div id=top-scroller class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer id=site-footer class="py-10 print:hidden"><nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400"><ul class="flex flex-col list-none sm:flex-row"><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=/ai-paper-reviewer/tags/ title>Tags</a></li><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=https://deep-diver.github.io/neurips2024/ title>NeurIPS2024</a></li></ul></nav><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2025
Hugging Face Daily Papers</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a></p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/ai-paper-reviewer/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh+sCQ0E53ghYrxgYqw+0GCRyIEpA=="></script></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://deep-diver.github.io/ai-paper-reviewer/ style=z-index:500><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body><script data-name=BMC-Widget data-cfasync=false src=https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js data-id=chansung data-description="Support me on Buy me a coffee!" data-message data-color=#FFDD00 data-position=Left data-x_margin=18 data-y_margin=18></script></html>