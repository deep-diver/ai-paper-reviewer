[{"figure_path": "https://arxiv.org/html/2502.12135/x1.png", "caption": "Figure 1: Given a 3D model, MagicArticulate can automatically generate the skeleton and skinning weights, making the model articulation-ready without further manual refinement. The input meshes are generated by Rodin Gen-1 [50] and Tripo 2.0 [1]. The meshes and skeletons are rendered using Maya Software Renderer [19].", "description": "This figure showcases the capabilities of MagicArticulate.  Given three different 3D models (a boy, a giraffe, and a dog), the system automatically generates a corresponding skeleton and computes skinning weights, which are necessary for realistic animation. The input 3D models themselves were created using Rodin Gen-1 and Tripo 2.0, and the resulting images were produced using the Maya Software Renderer.  This demonstrates the system's ability to prepare 3D models for animation without manual intervention.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2502.12135/x2.png", "caption": "(a) Word cloud of Articulation-XL categories.", "description": "This figure visualizes the categories of 3D models included in the Articulation-XL dataset using a word cloud. The size of each word is proportional to the frequency of its corresponding category in the dataset.  This provides a quick overview of the types of 3D models represented, highlighting the prevalence of certain categories over others.", "section": "3. Articulation-XL"}, {"figure_path": "https://arxiv.org/html/2502.12135/x3.png", "caption": "(b) Breakdown of Articulation-XL categories.", "description": "This pie chart shows the distribution of 3D models across different categories in the Articulation-XL dataset.  The categories represent the types of objects included in the dataset, such as animals, characters, vehicles, and more. The size of each slice corresponds to the relative proportion of models belonging to that category within the dataset.", "section": "3. Articulation-XL"}, {"figure_path": "https://arxiv.org/html/2502.12135/x4.png", "caption": "(c) Bone number distributions of Articulation-XL.", "description": "The figure shows the distribution of the number of bones present in the 3D models within the Articulation-XL dataset.  The x-axis represents the number of bones, and the y-axis represents the frequency or count of models with that specific number of bones. This histogram visually illustrates the range and concentration of bone counts across the dataset, providing insights into the complexity and diversity of the 3D model skeletons.", "section": "3. Articulation-XL"}, {"figure_path": "https://arxiv.org/html/2502.12135/x5.png", "caption": "Figure 2: Articulation-XL statistics.", "description": "This figure presents a statistical overview of the Articulation-XL dataset. It includes a word cloud summarizing the categories of 3D models in the dataset, a pie chart showing the distribution of these categories, and a histogram illustrating the distribution of bone numbers per model. This provides insights into the diversity and complexity of the data.", "section": "3. Articulation-XL"}, {"figure_path": "https://arxiv.org/html/2502.12135/x6.png", "caption": "Figure 3: Some examples from Articulation-XL alongside examples of poorly defined skeletons that were curated out.", "description": "This figure showcases examples of high-quality 3D models with their corresponding skeletons from the Articulation-XL dataset.  It also displays examples of 3D models that were excluded from the dataset due to poorly defined or inaccurate skeletal annotations. This visual comparison highlights the standards for quality control applied in creating the Articulation-XL benchmark.", "section": "3. Articulation-XL"}, {"figure_path": "https://arxiv.org/html/2502.12135/x7.png", "caption": "Figure 4: Overview of our method for auto-regressive skeleton generation. Given an input mesh, we begin by sampling point clouds from its surface. These sampled points are then encoded into fixed-length shape tokens, which are appended to the start of skeleton tokens to achieve auto-regressive skeleton generation conditioned on input shapes. The input mesh is generated by Rodin Gen-1 [50].", "description": "This figure illustrates the process of autoregressive skeleton generation, a key component of the MagicArticulate framework.  The process begins with an input 3D mesh, from which point cloud samples are extracted from the mesh surface. These points are then fed into a shape encoder that generates fixed-length shape tokens. These tokens are concatenated with skeleton tokens, forming the input to an autoregressive transformer.  The transformer predicts the skeleton, conditioned on the input shape, generating a sequence of bones and joints that represent the object's skeletal structure. The output is then a complete articulated skeleton. The input mesh shown in the figure is generated using Rodin Gen-1 [50].", "section": "4.1 Auto-regressive skeleton generation"}, {"figure_path": "https://arxiv.org/html/2502.12135/x8.png", "caption": "Figure 5: Spatial sequence ordering versus hierarchical sequence ordering. The numbers indicate the bone ordering indices.", "description": "This figure illustrates two different approaches for ordering bones during skeleton generation: spatial sequence ordering and hierarchical sequence ordering.  In spatial ordering, bones are ordered based on the spatial coordinates of their joints, prioritizing z, then y, and finally x coordinates.  This approach results in a sequence that doesn't necessarily reflect the hierarchical relationships within the skeleton.  In contrast, hierarchical ordering leverages the bone hierarchy, starting with the root bone and recursively processing child bones layer by layer. This ensures that the ordering respects the parent-child relationships within the skeletal structure.  The numbers in the figure represent the bone ordering indices for each method, highlighting the difference in sequencing.", "section": "4.1 Auto-regressive skeleton generation"}, {"figure_path": "https://arxiv.org/html/2502.12135/x9.png", "caption": "Figure 6: Comparison of skeleton creation results on ModelsResource (left) and Articulation-XL (right). Our generated skeletons more closely resemble the artist-created references, while RigNet and Pinocchio struggle to handle various object categories.", "description": "Figure 6 presents a comparative analysis of skeleton generation results obtained using three different methods: the proposed method (Ours), RigNet, and Pinocchio.  The comparison is shown for two datasets: ModelsResource (left column) and Articulation-XL (right column). For each dataset and method, several examples are displayed, showing the automatically generated skeletons alongside the corresponding artist-created reference skeletons. The figure highlights that the proposed method produces skeletons that more accurately reflect the artist-created references, demonstrating improved performance in handling the diverse object categories within the datasets. In contrast, RigNet and Pinocchio exhibit difficulties in generating accurate skeletons, especially for object categories that deviate significantly from the datasets they were trained on.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.12135/x10.png", "caption": "Figure 7: Skeleton creation results on 3D generated meshes. Our method has a better generalization performance than both RigNet [43] and Pinocchio [3] across difference object categories. The 3D models are generated by Tripo 2.0 [1].", "description": "Figure 7 showcases a comparison of skeleton generation results across various object categories using three different methods: the proposed method, RigNet [43], and Pinocchio [3].  The input 3D models were generated using Tripo 2.0 [1]. The figure demonstrates that the proposed method exhibits superior generalization capabilities compared to RigNet and Pinocchio, producing more accurate and realistic skeletons for a wider variety of object types.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.12135/x11.png", "caption": "Figure 8: Comparisons with previous methods for skinning weight prediction on ModelsResource (top) and Articulation-XL (bottom). We visualize skinning weights and L1 error maps. For more results, please refer to the supplementary materials.", "description": "This figure shows a comparison of skinning weight prediction results between the proposed method and two baseline methods (GVB and RigNet) on two datasets: ModelsResource and Articulation-XL.  For each dataset, several example 3D models are displayed. Each model shows the artist-created ground truth skinning weights, along with the skinning weights generated by each method. The accompanying L1 error maps visually represent the difference between the predicted and ground truth skinning weights, with lower values indicating higher accuracy. The figure highlights the superior performance of the proposed method in generating more accurate and realistic skinning weights.", "section": "5.3. Skinning weight prediction results"}, {"figure_path": "https://arxiv.org/html/2502.12135/x12.png", "caption": "Figure S9: Overview of the function diffusion architecture for skinning weight prediction. Given a set of noised skinning weight functions {(x,ft\u2062(x))\u2223x\u2208\ud835\udcab}conditional-set\ud835\udc65subscript\ud835\udc53\ud835\udc61\ud835\udc65\ud835\udc65\ud835\udcab\\{(x,f_{t}(x))\\mid x\\in\\mathcal{P}\\}{ ( italic_x , italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_x ) ) \u2223 italic_x \u2208 caligraphic_P }, conditioned on skeleton and shape features from [52], we denoise the skinning weight functions to approximate the target weights.", "description": "Figure S9 illustrates the architecture of the functional diffusion model used for skinning weight prediction in MagicArticulate.  The model takes as input a set of noisy skinning weight functions, represented as {(x, f<sub>t</sub>(x)) | x \u2208 \ud835\udcab}, where x represents a vertex on the 3D mesh and f<sub>t</sub>(x) is the noisy skinning weight at that vertex at time step t.  Crucially, the model also incorporates both the skeleton and shape features (from a pre-trained encoder, reference [52] in the paper) as conditioning information, which guides the denoising process. The model utilizes cross-attention and self-attention mechanisms to process this combined input and generate a refined set of skinning weights that approximate the target weights.", "section": "4.2. Skinning weight prediction"}, {"figure_path": "https://arxiv.org/html/2502.12135/x13.png", "caption": "Figure S10: Process of adding noise to the skinning weight function. Given x\u2208\ud835\udcab\ud835\udc65\ud835\udcabx\\in\\mathcal{P}italic_x \u2208 caligraphic_P and the original skinning weight function f0\u2062(x)subscript\ud835\udc530\ud835\udc65f_{0}(x)italic_f start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ( italic_x ), we add the noise function g\u2062(x)\ud835\udc54\ud835\udc65g(x)italic_g ( italic_x ) to obtain the noised function ft\u2062(x)subscript\ud835\udc53\ud835\udc61\ud835\udc65f_{t}(x)italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_x ).", "description": "This figure illustrates the process of adding noise to the skinning weight function during the functional diffusion process for skinning weight prediction.  It shows how the original skinning weight function, f\u2080(x), is modified by adding noise, g(x), to produce the noised function, f\u209c(x). The noise level is controlled by the parameter t, which ranges from 0 to 1. This process is a key step in the functional diffusion framework used to predict skinning weights efficiently and accurately for 3D models with complex topologies.", "section": "4.2. Skinning weight prediction"}, {"figure_path": "https://arxiv.org/html/2502.12135/x14.png", "caption": "Figure S11: Comparison of skeleton generation methods on out-of-domain data. The input meshes are from 3D generation, 3D scan, and 3D reconstruction.", "description": "This figure compares the performance of different skeleton generation methods on 3D models from various sources.  The input meshes are obtained from three distinct methods: 3D model generation, 3D scanning, and 3D reconstruction. The comparison showcases how well each method generalizes to different data types and the resulting diversity in 3D model quality, highlighting the strengths and weaknesses of each approach in terms of generating accurate and usable skeletons.", "section": "8. Additional experimental results"}, {"figure_path": "https://arxiv.org/html/2502.12135/x15.png", "caption": "Figure S12: Comparison of skeleton generation methods on ModelsResource (left) and Articulation-XL (right). Our results more closely resemble the artist-created references, while RigNet and Pinocchio struggle to handle various object categories.", "description": "This figure showcases a qualitative comparison of skeleton generation methods.  The left side presents results on the ModelsResource dataset, while the right shows results from the larger, more diverse Articulation-XL dataset.  For each dataset, results are displayed for three methods:  artist-created skeletons (ground truth), results from the authors' proposed MagicArticulate method, and results from the RigNet and Pinocchio baseline methods. The visual comparison demonstrates that MagicArticulate produces skeletons that more accurately match the artist-created references, especially when dealing with diverse and complex object shapes. RigNet and Pinocchio, relying on pre-defined templates or less adaptable approaches, struggle to generate skeletons of the same quality, especially for objects outside their usual category.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.12135/x16.png", "caption": "Figure S13: Skeleton results on 3D models with different orientations. Although minor differences may appear in the generated skeletons, all results maintain anatomically valid and suitable for rigging purposes.", "description": "This figure demonstrates the robustness of the proposed skeleton generation method to variations in object orientation.  Four views of the same 3D model are shown, each rotated along a different axis (original, 45 degrees along the y-axis, 90 degrees along the z-axis, and 135 degrees along the x-axis). Despite the changes in viewpoint, the generated skeletons in each orientation maintain anatomical correctness and are suitable for rigging, showcasing the method's adaptability and reliability.", "section": "8. Additional experimental results"}, {"figure_path": "https://arxiv.org/html/2502.12135/x17.png", "caption": "Figure S14: Comparison of skinning weight prediction methods on ModelsResource (first three rows) and Articulation-XL (last three rows). We visualize the predicted skinning weights alongside their corresponding L1 error maps.", "description": "This figure compares the performance of different skinning weight prediction methods.  The top three rows show results from the ModelsResource dataset, and the bottom three rows display results from the Articulation-XL dataset. For each method (Ours, RigNet, GVB), the figure visualizes both the predicted skinning weights and their corresponding L1 error maps, enabling a direct comparison of accuracy. The error maps highlight discrepancies between predicted and ground truth weights, indicating areas where the method's performance is better or worse.", "section": "5.3. Skinning weight prediction results"}, {"figure_path": "https://arxiv.org/html/2502.12135/x18.png", "caption": "Figure S15: Input instructions to VLM for data filtering.", "description": "This figure shows the detailed instructions given to the Vision-Language Model (VLM) for the task of data filtering.  The instructions guide the VLM on how to assess the quality of a skeleton within a 3D model using four rendered views of both the 3D mesh and the skeleton. The VLM evaluates based on criteria like whether joints and bones extend beyond the mesh's boundaries and how well the skeleton's pose aligns with the model's anatomy.  The instructions also outline a three-point rating system (Poor, Average, Good) and provide example evaluations demonstrating the process.", "section": "5.1. Implementation details"}, {"figure_path": "https://arxiv.org/html/2502.12135/x19.png", "caption": "Figure S16: Input instructions to VLM for category labeling.", "description": "The figure shows the detailed instructions given to the Vision-Language Model (VLM) for the task of automatically assigning categories to 3D objects.  The instructions emphasize the importance of carefully examining eight sub-images of each 3D object (four RGB images and four normal maps, each from a different angle) before assigning categories.  A comprehensive list of categories is provided, including character, animal, furniture, electronic device, mythical creature, anatomy, tool, planet, musical instrument, sculpture, jewelry, accessory, paper, anthropomorphic object, toy, clothing, food, scanned data, architecture, vehicle, plant, weapon, household item, and miscellaneous.  The instructions also include an example of the expected output format.", "section": "9. Category annotation"}, {"figure_path": "https://arxiv.org/html/2502.12135/x20.png", "caption": "Figure S17: Input rendered examples to VLM for data filtering.", "description": "This figure shows example images used to train a Vision-Language Model (VLM) for filtering low-quality 3D models and skeletons from the dataset. The images consist of four views of the same 3D object, rendered with its skeleton.  The VLM is trained to assess the quality of the skeleton based on its alignment with the mesh and its overall anatomical plausibility. This process ensures that only high-quality articulation data is used for training the MagicArticulate model.", "section": "7. More details of MagicArticulate"}, {"figure_path": "https://arxiv.org/html/2502.12135/x21.png", "caption": "Figure S18: Input rendered examples to VLM for category labeling.", "description": "This figure shows example images used to train a Vision-Language Model (VLM) for automatically assigning category labels to 3D models in the Articulation-XL dataset.  Each image contains eight sub-images: four RGB images and four corresponding normal maps, each from a different viewpoint. These images serve as input to the VLM, which learns to associate visual features with specific object categories.", "section": "9.3. Category annotation"}]