[{"content": "|                   | SQuAD R@5 | SQuAD M@5 | NQ R@5 | NQ M@5 | TriviaQA R@5 | TriviaQA M@5 | PAQ R@5 | PAQ M@5 | RIR R@5 | RIR M@5 |\n|-------------------|------------|------------|---------|---------|---------------|---------------|----------|----------|----------|----------|\n| _Pre-trained retrieval model_ |            |            |         |         |               |               |          |          |          |          |\n| SBERT             |   0.812    |   0.667    |  0.754  |  0.576  |     0.677     |     0.413     |  0.808   |  0.701   |  0.376   |  0.297   |\n| E5                |   0.803    |   0.674    |  0.760  |  0.581  |     0.645     |     0.390     |  0.816   |  0.716   |  0.484   |  0.396   |\n| BGE                |   0.829    |   0.701    |  0.674  |  0.502  |     0.690     |     0.422     |  0.752   |  0.647   |  0.451   |  0.367   |\n| GTE                |   0.866    |   0.744    |  0.767  |  0.587  |     0.726     |     0.443     |  0.836   |  0.736   |  0.528   |  0.435   |\n| _Retrained retrieval model_ |            |            |         |         |               |               |          |          |          |          |\n| SBERT<SUB>RT</SUB> |   0.742    |   0.585    |  0.739  |  0.550  |     0.577     |     0.342     |  0.859   |  0.742   |  0.739   |  0.631   |\n| BGE<SUB>RT</SUB>  |   0.841    |   0.701    |  0.751  |  0.553  |     0.640     |     0.384     |  0.901   |  0.802   |  0.953   |  0.881   |\n| GeAR               |   0.883    |   0.762    |  0.747  |  0.567  |     0.660     |     0.398     |  0.940   |  0.855   |  0.961   |  0.903   |\n| GeAR<SUB>w/o\u2112<SUB>LM</SUB></SUB> |   0.889    |   0.776    |  0.755  |  0.565  |     0.660     |     0.399     |  0.955   |  0.877   |  0.963   |  0.907   |", "caption": "Table 1: Comparison of documents retrieval performance on different datasets, where R@k stands for Recall@k, M@k stands for MAP@k.", "description": "This table presents a comparison of document retrieval performance across several datasets.  The performance is measured using two metrics: Recall@k (R@k) and Mean Average Precision@k (MAP@k). Recall@k indicates the proportion of relevant documents retrieved within the top k results, while MAP@k averages the precision across all ranks.  Multiple retrieval models are compared, allowing for a quantitative assessment of their relative effectiveness in retrieving relevant documents from different types of data.", "section": "4.2 Overall performance"}, {"content": "| | SQuAD |  | NQ |  | TriviaQA |  | PAQ |  | RIR |  |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|  | R@1 | M@1 | R@1 | M@1 | R@1 | M@1 | R@1 | M@1 | R@3 | M@3 |\n| Pre-trained retrieval model |  |  |  |  |  |  |  |  |  |  |\n| SBERT | 0.739 | 0.800 | 0.558 | 0.652 | 0.359 | 0.583 | 0.498 | 0.561 | 0.891 | 0.874 |\n| E5 | 0.783 | 0.847 | 0.590 | 0.683 | 0.379 | 0.613 | 0.573 | 0.640 | 0.891 | 0.878 |\n| BGE | 0.768 | 0.830 | 0.570 | 0.663 | 0.362 | 0.589 | 0.565 | 0.630 | 0.894 | 0.881 |\n| GTE | 0.758 | 0.820 | 0.548 | 0.639 | 0.352 | 0.572 | 0.525 | 0.590 | 0.895 | 0.886 |\n| Retrained retrieval model |  |  |  |  |  |  |  |  |  |  |\n| SBERT<sub>RT</sub> | 0.516 | 0.568 | 0.445 | 0.523 | 0.281 | 0.472 | 0.363 | 0.418 | 0.899 | 0.991 |\n| BGE<sub>RT</sub> | 0.455 | 0.538 | 0.601 | 0.656 | 0.288 | 0.475 | 0.409 | 0.466 | 0.897 | 0.888 |\n| GeAR | 0.810 | 0.874 | 0.765 | 0.871 | 0.515 | 0.808 | 0.885 | 0.965 | 0.954 | 0.897 |", "caption": "Table 2: Comparison of units localization performance on different datasets, where R@k stands for Recall@k, M@k stands for MAP@k.", "description": "This table presents a comparison of the performance of different models on the task of units localization.  Units localization refers to the ability of a model to identify the specific sections of a document that are most relevant to a given query. The table shows the Recall@k and Mean Average Precision@k (MAP@k) scores for several models across various datasets.  Recall@k represents the proportion of relevant units correctly retrieved within the top k results, while MAP@k is a more sophisticated metric that considers both the precision and recall of the retrieved units. Higher scores in both metrics indicate better performance on the units localization task.", "section": "4.2 Overall performance"}, {"content": "| SQuAD |  | NQ |  | TriviaQA |  | PAQ |  | RIR |  |\n|---|---|---|---|---|---|---|---|---|---|---|\n| EM | F1 | EM | F1 | EM | F1 | EM | F1 | Rouge-1 | Rouge-L |\n| 61.2 | 65.3 | 66.1 | 61.0 | 47.4 | 60.0 | 88.1 | 92.4 | 87.4 | 87.1 |", "caption": "Table 3: Generation performance of GeAR\u00a0on different tasks.", "description": "This table presents a quantitative evaluation of the GeAR model's text generation capabilities across various tasks.  It shows the performance metrics (Exact Match (EM) and F1 scores for question answering tasks, and ROUGE-1 and ROUGE-L scores for relevant information retrieval tasks) achieved by GeAR on different benchmark datasets (SQUAD, NQ, TriviaQA, PAQ, and RIR). This demonstrates the model's ability to generate high-quality text relevant to the input query across multiple scenarios and datasets.", "section": "4 Experiments"}, {"content": "| Hyperparameter | Assignment |\n|---|---| \n| Computing Infrastructure | 16 MI200-64GB GPUs |\n| Number of epochs | 10 |\n| Batch size per GPU | 48 / 16 |\n| Maximum sequence length | 512 |\n| Optimizer | AdamW |\n| AdamW epsilon | 1e-8 |\n| AdamW beta weights | 0.9, 0.999 |\n| Learning rate scheduler | Cosine lr schedule |\n| Initialization learning rate | 1e-5 |\n| Minimum learning rate | 1e-6 |\n| Weight decay | 0.05 |\n| Warmup steps | 1000 |\n| Warmup learning rate | 1e-6 |", "caption": "Table 4: Hyperparameter settings", "description": "This table details the hyperparameters used in training the GeAR model.  It lists settings for various aspects of the training process, including the computing infrastructure (number of GPUs and memory), training epochs, batch size, optimizer used (AdamW), learning rate schedule and settings, weight decay, and warmup steps. These settings are crucial for replicating the experiments presented in the paper.", "section": "3.4 Training Objectives"}, {"content": "| Scenario | Data Number |\n|---|---| \n| QAR | 30,000,000 |\n| RIR | 5,676,877 |", "caption": "Table 5: Training data statistics.", "description": "This table shows the number of data samples used for training the GeAR model in two different retrieval scenarios: Question Answer Retrieval (QAR) and Relevant Information Retrieval (RIR).  The QAR scenario used 30,000,000 data samples, while the RIR scenario utilized 5,676,877 samples.", "section": "3.2 Data construction"}, {"content": "| Scenario | Dataset | Documents Number | Queries Number |\n|---|---|---|---| \n| QA | Squad | 20,239 | 5,928 |\n|  | NQ | 64,501 | 2,889 |\n|  | TriviaQA | 104,160 | 14,000 |\n|  | PAQ | 932,601 | 20,000 |\n| RIR | RIR | 2,315,413 | 145,562 |", "caption": "Table 6: The evaluation data statistics for the document retrieval task.", "description": "This table presents the evaluation datasets used for the document retrieval task.  It shows the number of documents and queries in each dataset used for testing the performance of various document retrieval models. Datasets include SQUAD, NQ, TriviaQA, and PAQ, providing a comprehensive evaluation across different question-answering scenarios.", "section": "4 Experiments"}, {"content": "| Scenario | Dataset | Data Number |\n|---|---|---|\n| QA | Squad | 5,928 |\n|  | NQ | 2,889 |\n|  | TriviaQA | 14,000 |\n|  | PAQ | 20,000 |\n| RIR | RIR | 10,000 |", "caption": "Table 7: The evaluation data statistics for the units localization and information generation tasks.", "description": "This table presents the number of data samples used for evaluating the units localization and information generation tasks.  It breaks down the counts by dataset (SQUAD, NQ, TriviaQA, PAQ, and RIR) and scenario (QA and RIR).  These numbers reflect the size of the test sets used to assess the model's performance in these specific aspects. The 'QA' scenario refers to Question Answering Retrieval and 'RIR' denotes Relevant Information Retrieval.", "section": "4. Experiments"}]