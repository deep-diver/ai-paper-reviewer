{"references": [{"fullname_first_author": "Tomas Mikolov", "paper_title": "Distributed representations of words and phrases and their compositionality", "publication_date": "2013", "reason": "This paper introduced Word2Vec, a foundational model for word embeddings that significantly impacted the field of natural language processing and is frequently cited in embedding-based retrieval research."}, {"fullname_first_author": "Jeffrey Pennington", "paper_title": "Glove: Global vectors for word representation", "publication_date": "2014", "reason": "GloVe is another highly influential method for generating word embeddings, offering an alternative to Word2Vec and contributing substantially to the development of embedding-based approaches."}, {"fullname_first_author": "Jacob Devlin", "paper_title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "publication_date": "2019", "reason": "BERT is a highly impactful transformer-based language model, frequently used as a foundation for advanced contextual embeddings and bi-encoder retrieval systems."}, {"fullname_first_author": "Nils Reimers", "paper_title": "Sentence-BERT: Sentence embeddings using Siamese BERT-networks", "publication_date": "2019", "reason": "Sentence-BERT is a widely adopted adaptation of BERT specifically designed for generating sentence embeddings, making it highly relevant to the document retrieval tasks discussed in the paper."}, {"fullname_first_author": "Vladimir Karpukhin", "paper_title": "Dense passage retrieval for open-domain question answering", "publication_date": "2020", "reason": "This paper introduced a significant advancement in passage retrieval by using dense vector representations, directly addressing the core challenge tackled in the discussed GeAR model."}]}