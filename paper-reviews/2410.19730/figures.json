[{"figure_path": "2410.19730/figures/figures_1_0.png", "caption": "Figure 1: Experimental results on average counting accuracy based on different tokenization choices, using GPT-40 mini. Our approach treats the model as a black-box, manipulating BPE tokenizers to function differently through carefully engineered string formats.", "description": "The figure shows that using character-level tokenization instead of BPE tokenization significantly improves the accuracy of GPT-40 mini in counting tasks.", "section": "Abstract"}, {"figure_path": "2410.19730/figures/figures_4_0.png", "caption": "Figure 2: Illustration of inductive counting as performed by humans, RNNs, and LLMs with CoT, respectively.", "description": "The figure illustrates the inductive counting process performed by humans, recurrent neural networks (RNNs), and large language models (LLMs) using chain-of-thought (CoT).", "section": "3 CoT + Ideal Assumption = Complete Counting Ability"}, {"figure_path": "2410.19730/figures/figures_4_1.png", "caption": "Figure 2: Illustration of inductive counting as performed by humans, RNNs, and LLMs with CoT, respectively.", "description": "The figure illustrates the inductive counting process performed by humans, recurrent neural networks (RNNs), and large language models (LLMs) using Chain of Thought (CoT).", "section": "3 CoT + Ideal Assumption = Complete Counting Ability"}, {"figure_path": "2410.19730/figures/figures_8_0.png", "caption": "Figure 5: Pairwise comparison of counting accuracy for different letters in strings. The left plot shows the distribution of accuracy for a and b in ab strings, with each dot representing the average accuracy for a in a given CoT case (e.g., spaced-string in the [10,20] range), connected to the corresponding accuracy for b in the same setting. The right plot illustrates a similar case for e and z in ez strings. Note: The y-axis limit exceeds [0,1] as the distribution is calculated based on variance and mean, with larger variance pushing the upper bound of the confidence interval beyond the maximum value.", "description": "Figure 5 shows a pairwise comparison of counting accuracy for different letter pairs (a,b and e,z) across various string tokenization methods and CoT conditions.", "section": "Main Results"}, {"figure_path": "2410.19730/figures/figures_14_0.png", "caption": "Figure 3: Four types of string formatting used for counting tasks to manipulate tokenization in LLMs. Examples in the figure are tokenized using the GPT-40 tokenizer. Each string-token type is labeled as (a), (b), (c), and (d) in the diagram. Note that changing the format does not alter the fundamental nature or difficulty of the counting task.", "description": "The figure shows four different ways of formatting strings for counting tasks, demonstrating how different tokenization methods impact the ability of large language models to count.", "section": "4 Tokenization as a Black Box Model"}]