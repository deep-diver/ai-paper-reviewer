{"importance": "This paper is crucial for researchers working with large language models (LLMs) because it reveals the significant impact of tokenization on LLM reasoning abilities, particularly in tasks requiring complex computations like counting.  The findings challenge existing assumptions about LLM capabilities and highlight the need for careful consideration of tokenization strategies during both model development and application.  It opens new avenues for research on improving LLM reasoning and potentially broadens the applicability of LLMs to more complex tasks.", "summary": "LLM counting abilities are surprisingly sensitive to tokenization;  carefully crafted tokenization strategies significantly improve accuracy, bridging the gap between theory and practice.", "takeaways": ["Tokenization significantly impacts the counting ability of LLMs, even when using advanced reasoning techniques like Chain of Thought (CoT).", "Specific tokenization methods, such as separating items into individual tokens, can dramatically improve LLM counting performance.", "The frequency of letters in a token can affect counting accuracy, with less frequent letters showing greater accuracy."], "tldr": "This research explores how the choice of tokenization significantly affects the ability of Large Language Models (LLMs) to perform counting tasks.  While LLMs, built on Transformers, have theoretical limitations in performing inductive reasoning tasks like counting, the use of Chain of Thought (CoT) prompting has shown improvement. However, this study reveals a previously overlooked factor: tokenization.  Using different methods for breaking down input text into tokens (e.g., byte-pair encoding), the researchers demonstrate that models' accuracy varies greatly, sometimes achieving near-perfect results with a well-chosen tokenization scheme, and sometimes dropping to near-chance levels of accuracy.  The analysis highlights that the implicit tokenization choices significantly undermine a model's ability, even if the model is theoretically capable of the task.  Experiments on several models, including GPT-4, showcase these variations. The key finding is that proper, item-separated tokenization (where each item to be counted becomes a separate token) is crucial for accurate counting, unlike the commonly used byte-pair encoding which groups characters together. Furthermore, experiments indicate that token frequency correlates with accuracy, where less frequent letters yield better performance than frequent ones in counting tasks. This work provides valuable insights for improving LLM design and application by highlighting the importance of tokenization strategies, especially in tasks involving inductive reasoning."}