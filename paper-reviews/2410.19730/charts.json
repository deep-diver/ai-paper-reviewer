[{"figure_path": "2410.19730/charts/charts_7_0.png", "caption": "Figure 4: Distribution of shift from correct count to GPT-calculated count, for each type of string-token fomrat (a), (b), (c) and (d) in order. The statsiticas show the results for letter a at length range [30, 40], as this range the error rate is high. We only calculate the shift when error is made, as correct counting instance does not have any shift.", "description": "The chart displays the distribution of errors (shifts from the correct count) in GPT's counting performance for different tokenization methods, revealing a bias towards undercounting with pure BPE tokenization.", "section": "5.3 Error Shifts Reveal Mistakes in Counting with BPE Tokenization"}, {"figure_path": "2410.19730/charts/charts_9_0.png", "caption": "Figure 6: Counting accuracy (Orange) with respect to target letter frequency (Blue) in Human Natural Language.", "description": "The chart displays a positive correlation between letter frequency and counting error rate in human language.", "section": "5.4 Different Tokens Have Varying Sensitivity in Counting Tasks"}]