{"references": [{" publication_date": "2023", "fullname_first_author": "Josh Achiam", "paper_title": "GPT-4 technical report", "reason": "This paper is foundational as it provides a technical report on GPT-4, one of the LLMs used in the main study's experiments.  Understanding GPT-4's architecture and capabilities is crucial for interpreting the results and assessing the generalizability of the findings.  The report's details on model architecture and training data are relevant to the study's analysis of tokenization effects and CoT reasoning.", "section_number": 5}, {" publication_date": "2020", "fullname_first_author": "Joshua Ackerman", "paper_title": "A survey of neural networks and formal languages", "reason": "This paper provides a comprehensive survey of neural networks and formal languages, which is highly relevant to the theoretical foundation of the main study.  It offers a crucial theoretical background on the computational power of different neural network architectures, particularly RNNs and transformers.  Understanding the theoretical limitations of these architectures is essential to interpret the findings and limitations of the main study.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Zeyuan Allen-Zhu", "paper_title": "Physics of language models: Part 3.1, knowledge storage and extraction", "reason": "This work is important as it examines the knowledge storage and extraction mechanisms of language models.  This is pertinent to the main study, which explores how tokenization affects counting abilities by impacting a model's ability to access and process relevant information. The paper provides a theoretical foundation for understanding how information is represented and accessed within LLMs, a key factor influencing their performance on counting tasks.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Jinze Bai", "paper_title": "Qwen technical report", "reason": "This paper is significant because it provides a technical report on Qwen, a large language model that could serve as a benchmark to test the proposed method's applicability on other LLMs. The Qwen model is a relevant comparison point for GPT-4, allowing for generalization of findings across multiple models.", "section_number": 5}, {" publication_date": "2002", "fullname_first_author": "George S Boolos", "paper_title": "Computability and logic", "reason": "This is a classic text in computability theory, providing a foundational understanding of the theoretical limits of computation. The study's theoretical analysis draws heavily from computability theory to explain the limitations of Transformers in performing counting tasks, making this a crucial reference for establishing the background and justifying the research questions.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Yingshan Chang", "paper_title": "Language models need inductive biases to count inductively", "reason": "This paper is highly relevant to the central theme of the main study, as it directly addresses the challenges of LLMs in inductive counting. The main study builds on this work by focusing on the impact of tokenization strategies.  Understanding the challenges of inductive counting is crucial to interpreting the results and limitations of the main study.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "David Chiang", "paper_title": "Overcoming a theoretical limitation of self-attention", "reason": "This paper addresses the theoretical limitations of self-attention mechanisms in transformers, a critical architectural aspect relevant to the main study.  The main study's focus on the counting ability of LLMs is directly linked to the theoretical limitations of the transformer architecture, making this paper highly relevant.", "section_number": 3}, {" publication_date": "1964", "fullname_first_author": "Nicolaas Govert De Bruijn", "paper_title": "Polya's theory of counting", "reason": "This seminal work in combinatorics, Polya's theory of counting, is fundamental to understanding the mathematical underpinnings of counting problems.  The main study uses this theory to establish the theoretical basis for inductive counting, further emphasizing the challenges and complexities involved in applying LLMs to such tasks.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Gr\u00e9goire Del\u00e9tang", "paper_title": "Neural networks and the chomsky hierarchy", "reason": "This is a seminal paper in the field, establishing the connection between the computational power of neural networks and the Chomsky hierarchy.  This is vital for understanding the theoretical limits of Transformers in performing counting tasks, a key theme in the main study.", "section_number": 2}, {" publication_date": "2018", "fullname_first_author": "Jacob Devlin", "paper_title": "Bert: Pre-training of deep bidirectional transformers for language understanding", "reason": "This paper is important as it introduced BERT, a prominent transformer-based language model.  The main study investigates the limitations of transformers, and understanding the architecture and capabilities of a widely used transformer model like BERT is crucial for interpreting and evaluating the results.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "Angela Fan", "paper_title": "Addressing some limitations of transformers with feedback memory", "reason": "This paper provides insights into the limitations of transformers and proposes solutions involving feedback memory. This is relevant to the main study, which discusses the limitations of transformers in inductive counting tasks and explores the use of CoT to overcome these limitations. The paper provides a potential avenue for enhancing transformer-based models for counting, offering a valuable perspective on improving the limitations of LLMs in counting tasks.", "section_number": 3}, {" publication_date": "1968", "fullname_first_author": "Patrick C Fischer", "paper_title": "Counter machines and counter languages", "reason": "This foundational paper in theoretical computer science explores the computational power of counter machines, which are directly relevant to understanding the theoretical limitations of Transformers.  The main study uses the concept of counter machines to illustrate the sequential nature of counting and compare it to the parallel processing nature of Transformers, making this paper essential for the theoretical foundation of the main study.", "section_number": 1}, {" publication_date": "2014", "fullname_first_author": "Karol Gregor", "paper_title": "Deep autoregressive networks", "reason": "This paper is relevant as it presents deep autoregressive networks, a type of neural network architecture. The main study uses transformers, which are related to autoregressive models. Understanding the characteristics and limitations of different neural network architectures helps contextualize the study's focus on the limitations of transformers in inductive counting tasks.", "section_number": 2}, {" publication_date": "1995", "fullname_first_author": "Mark Jerrum", "paper_title": "The computational complexity of counting", "reason": "This is a seminal work in computational complexity, crucial for establishing the theoretical difficulty of counting problems.  The main study is built on theoretical foundations of counting complexity and the limitations of constant-depth models. This paper provides context and establishes the theoretical underpinnings of the study\u2019s focus on the computational hardness of counting.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Zhi Jin", "paper_title": "Contranovo: A contrastive learning approach to enhance de novo peptide sequencing", "reason": "This paper, while not directly related to the main topic, is included as it highlights the potential of other neural network approaches to solve complex tasks. The main study focuses on the limitations of transformers.  This contrast provides a wider perspective on neural network capabilities, helping contextualize the challenges and limitations of transformers in addressing complex reasoning tasks such as counting.", "section_number": 2}, {" publication_date": "2015", "fullname_first_author": "Yann LeCun", "paper_title": "Deep learning", "reason": "This review paper provides a comprehensive overview of deep learning, a field that is fundamental to the main study. Deep learning techniques are the basis for the LLMs used in the experiments. Understanding deep learning is vital for interpreting the results and limitations of the main study's analysis of LLMs' performance on counting tasks.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Zhiyuan Li", "paper_title": "Chain of thought empowers transformers to solve inherently serial problems", "reason": "This paper directly addresses the limitations of transformers in solving serial problems, which is highly relevant to the main study's focus on counting.  Counting is inherently a serial process.  Understanding how CoT enhances the ability of transformers to solve serial problems is crucial for interpreting the results and limitations of the main study.", "section_number": 3}, {" publication_date": "2022", "fullname_first_author": "Puyuan Liu", "paper_title": "A character-level length-control algorithm for non-autoregressive sentence summarization", "reason": "This paper is relevant to the main study as it discusses the challenges of handling variable-length sequences in neural networks.  The main study deals with counting tasks that inherently involve variable-length sequences, thus understanding the challenges of handling sequence lengths is crucial.", "section_number": 2}, {" publication_date": "1999", "fullname_first_author": "Paul Rodriguez", "paper_title": "A recurrent neural network that learns to count", "reason": "This paper demonstrates that RNNs can learn to count.  This is important because the study contrasts the counting abilities of RNNs with those of Transformers, and this serves as a key comparison point.  Understanding the performance of RNNs provides context and emphasizes the significant limitations of transformers in performing counting tasks.", "section_number": 2}]}