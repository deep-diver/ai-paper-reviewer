[{"figure_path": "https://arxiv.org/html/2411.04752/extracted/5984900/cmir_gpt.png", "caption": "Figure 1: An overview of the GPT-3.5 Turbo architecture.", "description": "This figure illustrates the architecture of the GPT-3.5 Turbo model, highlighting the key components involved in processing text input and generating output. It shows the flow of information from tokenization and embedding to attention mechanisms (transformer architecture), feedforward neural networks, and finally, output generation through a softmax layer.  The layered structure of the model, including multiple decoder blocks stacked together to achieve a deeper understanding of the input sequence, is also visualized.  The diagram shows the different stages of processing:  tokenization, embedding, positional encoding, attention mechanisms, feedforward neural networks, and output generation via a softmax layer.", "section": "5. Methodology"}, {"figure_path": "https://arxiv.org/html/2411.04752/extracted/5984900/cmir_gpt1.png", "caption": "Figure 2: Overview diagram of the methodology followed for GPT-3.5 Turbo.", "description": "This figure illustrates the process flow of the proposed methodology which uses GPT-3.5 Turbo.  The process starts with providing a prompt containing both the query and document to the model.  The model then processes this input through several steps including tokenization, embedding, positional encoding, attention mechanisms using a transformer architecture, and feedforward neural networks before generating a relevance score. This relevance score is then fed into a mathematical model that accounts for the sequential relationships between documents to determine the final relevance of the document to the given query. The output of the model indicates whether the document is relevant or not based on the calculated probability.", "section": "5. Methodology"}]