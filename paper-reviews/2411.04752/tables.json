[{"content": "| MAP Score | ndcg Score | p@5 Score | p@10 Score | Team Name | Submission File | Rank |\n|---|---|---|---|---|---|---|\n| 0.701773 | 0.797937 | 0.793333 | 0.766667 | TextTitans | submit_cmir | 5 |\n| 0.701773 | 0.797937 | 0.793333 | 0.766667 | TextTitans | submit_cmir_1 | 4 |\n| 0.701773 | 0.797937 | 0.793333 | 0.766667 | TextTitans | submit_cmir_2 | 3 |\n| 0.701773 | 0.797937 | 0.793333 | 0.766667 | TextTitans | submit_cmir_3 | 2 |\n| 0.703734 | 0.799196 | 0.793333 | 0.766667 | TextTitans | submit_cmir_4 | 1 |", "caption": "Table 1: A Comparison of MAP, NDCG, P@5, and P@10 Scores for the TextTitans Team.", "description": "Table 1 presents the evaluation metrics for five different submissions from the team named \"TextTitans\" for a code-mixed information retrieval task.  The metrics used include Mean Average Precision (MAP), Normalized Discounted Cumulative Gain (NDCG), Precision at 5 (P@5), and Precision at 10 (P@10).  These metrics assess the ranking quality of the retrieved documents.  The table shows consistent performance across the first four submissions, with a slight improvement observed in the fifth submission, indicating minor gains in retrieval accuracy.  The identical P@5 and P@10 scores across all submissions suggest consistent top-k retrieval performance.", "section": "6. Results"}]