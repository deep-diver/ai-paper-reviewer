[{"page_end_idx": 1, "page_start_idx": 1, "section_number": 1, "section_title": "Introduction", "details": {"details": "Large language models (LLMs) have demonstrated remarkable capabilities in memorizing factual knowledge and performing knowledge-intensive tasks. However, the knowledge stored within their parameters (parametric knowledge) can be inaccurate or outdated.  Retrieval and tool-augmented approaches have been explored to supplement LLMs with external knowledge (contextual knowledge), but these methods can lead to undesired behavior when contextual knowledge conflicts with the model's internal knowledge. This phenomenon, termed as context-memory knowledge conflicts, results in the LLM potentially relying on inaccurate information, leading to incorrect outputs. Existing methods to address knowledge conflicts involve fine-tuning or prompting based strategies.  These methods often require additional interactions with the model, resulting in high latency and hindering practical applications. This paper focuses on using representation engineering to directly address this problem, aiming for a more efficient and transparent method for steering knowledge usage in LLMs during inference.", "first_cons": "Existing methods to resolve knowledge conflicts in LLMs often involve additional interactions with the model, causing high latency and hindering practical applications.", "first_pros": "Representation engineering offers an efficient and transparent framework for controlling the behavior of LLMs.", "keypoints": ["LLMs store factual knowledge in their parameters (parametric knowledge), but this knowledge can be inaccurate or outdated.", "Contextual knowledge (external information) can conflict with parametric knowledge, leading to context-memory knowledge conflicts.", "Context-memory knowledge conflicts can lead to LLMs using outdated or incorrect information and producing incorrect outputs.", "Existing methods for resolving knowledge conflicts are often inefficient and involve additional interactions with the model, leading to high latency.", "This paper proposes to use representation engineering methods to directly steer the usage of parametric and contextual knowledge in LLMs during inference time, focusing on efficiency and transparency."], "second_cons": "Existing representation engineering methods fail to effectively steer knowledge usage, potentially because they directly modify polysemantic dense vectors that overlap with many independent semantic features.", "second_pros": "The paper proposes a novel training-free representation engineering method (SPARE) using sparse auto-encoders (SAEs) to control knowledge selection behaviors in LLMs.", "summary": "Large language models (LLMs) excel at knowledge-intensive tasks but suffer from context-memory knowledge conflicts, where internal knowledge clashes with provided context. This leads to unreliable outputs. Existing solutions are often slow, requiring additional interactions.  This paper introduces representation engineering as a more efficient approach to resolve these conflicts, focusing on inference-time manipulation to improve accuracy and reduce latency."}}, {"page_end_idx": 2, "page_start_idx": 2, "section_number": 2, "section_title": "Background", "details": {"details": "This section establishes the background for understanding the problem of knowledge conflicts in LLMs and introduces sparse autoencoders (SAEs). The problem is set up using open-domain question-answering (ODQA) tasks with instances represented as (Q, EM, M, Ec, C), where Q is the question, EM supports memorised knowledge, Ec conflicts with it, M is the memorised answer, and C is the contextual answer.  The core issue is that LLMs may rely on inaccurate information due to conflicts between memorised and contextual knowledge. Previous research highlights LLMs' preference for contextual knowledge, even when inaccurate, but the authors argue that LLMs should also utilize accurate memorised knowledge. Sparse autoencoders (SAEs) are introduced as a technique to interpret the complex representations of LLMs by decomposing them into monosemantic features, which allows more precise control over model activations.\n\nThe use of ODQA tasks with knowledge conflicts is a critical aspect of the experimental setup.  The (Q, EM, M, Ec, C) representation provides a structured way to quantify knowledge conflicts. The authors point to the fact that LLMs often prioritize contextual information, even if incorrect, and the need to give LLMs the ability to balance this preference and potentially use the accurate information from their parameters is the main point of the paper.  The introduction of SAEs as a means for precise activation editing and resolving the issues related to polysemantic dense vectors opens the way for an innovative solution in the rest of the paper.\n\nIn essence, the background lays the groundwork for the core research question: how can we steer an LLM's selection between contradictory sources of information? It sets the stage for the introduction of their proposed method, SPARE, which aims to resolve knowledge conflicts in a way that is both efficient and effective, compared to approaches that rely on extensive retraining or additional interactions.", "first_cons": "The background section is somewhat limited in its exploration of existing approaches to handling knowledge conflicts in LLMs.  While it acknowledges the challenges and prior research, it could have provided a more comprehensive overview of competing methods and their limitations.", "first_pros": "The clear definition of the ODQA task with knowledge conflicts, represented as (Q, EM, M, Ec, C), is very helpful for understanding the problem the research addresses.  This structured representation simplifies the problem and allows for easier evaluation of results.", "keypoints": ["Open-domain question-answering (ODQA) tasks are used to investigate knowledge conflicts in LLMs.", "Each ODQA instance is represented as (Q, EM, M, Ec, C), highlighting the conflict between memorised (EM, M) and contextual (Ec, C) knowledge.", "LLMs tend to prefer contextual knowledge, even if incorrect, over accurate memorised knowledge.", "Sparse Auto-Encoders (SAEs) are introduced as a way to interpret complex LLM representations and enable precise activation editing.", "SAEs decompose polysemantic dense vectors into monosemantic features, aiding in fine-grained control of LLM behavior."], "second_cons": "The description of sparse autoencoders (SAEs) is relatively brief. While sufficient for a basic understanding, a more detailed explanation of how SAEs achieve the decomposition of polysemantic activations into monosemantic features might help readers unfamiliar with this technique.", "second_pros": "The section effectively highlights the critical need for LLMs to balance their use of contextual and parametric knowledge, addressing a significant limitation of current LLMs.", "summary": "This background section sets the stage for a study on knowledge conflicts in LLMs by defining the problem using open-domain question answering (ODQA) tasks and the representation of these tasks as (Q, EM, M, Ec, C).  It also emphasizes the tendency of LLMs to prioritize contextual information even when incorrect and introduces sparse autoencoders (SAEs) as a solution for interpreting LLM representations and enabling fine-grained control over their behavior.  The section highlights the challenges of existing LLM knowledge management and paves the way for introducing a novel solution that balances the usage of contextual and memorised knowledge."}}, {"page_end_idx": 3, "page_start_idx": 3, "section_number": 3, "section_title": "Detection of Knowledge Conflicts", "details": {"details": "This section investigates the detectability of knowledge conflicts within LLMs during the generation process.  The authors hypothesize that signals indicating knowledge conflicts are present in the internal activations of the model and aim to pinpoint their location. They employ a linear probing method to classify whether a given activation comes from an instance with a knowledge conflict or a coherent instance.  The method uses logistic regression models trained on the residual stream of LLMs, specifically focusing on the activations at the last position of the sequence (thought to encode information for predicting the answer's first token). Experiments are performed on Llama2-7B and Gemma2-9B, testing various activation types (hidden states, MLP, and Self-Attention).  The results show that probing accuracy increases from the first layer to mid-layers, peaking before decreasing in later layers; this suggests that the signal of knowledge conflict is registered in mid-layers.  The analysis shows that mid-layers show the strongest signal for detecting conflict, with accuracy decreasing in later layers, and that the method can reliably detect knowledge conflict using the activations.", "first_cons": "The linear probing method, while effective in detecting signals, might not fully capture the complexity of knowledge conflicts.  The focus solely on the residual stream's last position could potentially overlook other relevant signals in different layers or positions.", "first_pros": "The experimental design is clear and well-executed.  The use of linear probing with logistic regression is a straightforward yet effective method for investigating the presence of signals in the network activations.", "keypoints": ["Probing accuracy increases from the first layer to mid-layers before decreasing in later layers, indicating that mid-layers are the optimal location for detecting knowledge conflicts.", "The linear probing method achieves high AUROC (Area Under the Receiver Operating Characteristic curve) scores on both Llama2-7B and Gemma2-9B, demonstrating its effectiveness in detecting signals of knowledge conflicts.", "Different activation types (hidden states, MLP, and Self-Attention) are tested, offering a comprehensive assessment of where knowledge conflict signals might be found within the model's structure."], "second_cons": "The study only uses two LLMs, and further investigation is needed to evaluate the generalizability of the findings to other LLMs and tasks. Although they mention that the probing accuracy in later layers decreases, the method doesn't explicitly explain the reason behind it, which would further enhance the insights.", "second_pros": "The study provides a quantitative assessment of the detectability of knowledge conflicts within LLMs. The results clearly show the optimal layers for signal detection, and different activation types are systematically compared, enriching the overall understanding.", "summary": "This section investigates the detection of knowledge conflicts in LLMs by analyzing their internal activations.  A linear probing method, applied to the residual stream of LLMs, demonstrates a clear increase in accuracy for detecting knowledge conflict signals from the initial layers to mid-layers, before a decrease in accuracy in later layers. This implies that mid-layers are the most informative location for capturing signals of knowledge conflicts within LLMs, highlighting a clear relationship between internal representation and knowledge conflict during the generation process. The study utilizes two different LLMs to demonstrate the consistency of the observations."}}, {"page_end_idx": 4, "page_start_idx": 4, "section_number": 4, "section_title": "Resolving Knowledge Conflicts by Representation Engineering", "details": {"details": "This section introduces SPARE, a novel method for resolving knowledge conflicts in LLMs using sparse auto-encoders (SAEs).  SPARE operates in three steps. First, it identifies SAE activations that correlate with different knowledge selection behaviors (contextual vs. parametric). This is achieved by collecting activations that lead to choices favoring either contextual or parametric knowledge, computing the average SAE activation vectors for each group (Zc and Zm), and calculating the mutual information between each SAE activation and the answer choice (contextual or parametric).  Activations with high mutual information are deemed influential. Second, SPARE extracts functional features that control the knowledge selection by selecting top-k activations with highest mutual information I(Zi;Y) and then constructing two orthogonal SAE activations, Zc and Zm, which correlate with specific knowledge selection behaviors. Third, SPARE applies these features to edit the internal activations of the LLM at inference time to steer the knowledge selection behavior.  The editing involves a process of adding desirable activations (from Zm for parametric knowledge or Zc for contextual knowledge) and subtracting undesirable ones (opposite).  This controlled editing modifies the hidden state at the last position of the input sequence, influencing the model's subsequent generation. The method avoids directly modifying activations to prevent information loss, ensuring a controlled and effective alteration.", "first_cons": "The method relies on pre-trained SAEs, which may not be available for all LLMs, limiting its general applicability.", "first_pros": "SPARE is a training-free method, meaning it doesn't require additional training of the LLM, making it efficient and practical.", "keypoints": ["SPARE uses Sparse Auto-Encoders (SAEs) to decompose complex LLM activations into interpretable features.", "It identifies less than 0.05% SAE activations that control knowledge selection behavior (Gemma2-9B).", "It achieves improvements of +10% over existing representation engineering methods and +15% over contrastive decoding methods.", "The method works by editing internal activations at inference time, avoiding the need for additional training."], "second_cons": "The effectiveness of SPARE might be limited to specific tasks (open-domain question-answering) and types of knowledge conflicts (context-memory).", "second_pros": "SPARE provides a fine-grained and precise control over knowledge selection, surpassing other methods in selectively favoring contextual or parametric knowledge.", "summary": "SPARE is a training-free representation engineering method that leverages pre-trained sparse auto-encoders to effectively control knowledge selection in LLMs during inference.  It achieves this by identifying key SAE activations associated with specific knowledge selection behaviors, extracting functional features, and using these features to edit the internal activations of the model to steer the choice between contextual and parametric knowledge. Experiments show significant improvements over existing methods in open-domain question-answering tasks."}}, {"page_end_idx": 5, "page_start_idx": 5, "section_number": 5, "section_title": "Experimental Results", "details": {"details": "The experimental results section evaluates the performance of the proposed SPARE method and compares it with several baselines on two open-domain question-answering datasets with knowledge conflicts: NQSwap and Macnoise.  Two LLMs, Llama3-8B and Gemma2-9B, were used for evaluation. The metrics used for evaluation are Exact Match (EM) for overall performance, EMC (accuracy of steering the usage of contextual knowledge), and EMM (accuracy of steering the usage of parametric knowledge). The results show that SPARE significantly outperforms existing representation engineering methods (+10%) and contrastive decoding methods (+15%), achieving the best performance in steering knowledge selection behaviors under knowledge conflicts.  Further analysis investigates the capability of SPARE in changing knowledge selection behaviors, maintaining original behavior, and the impact of various components of the method. An ablation study demonstrates the effectiveness of all components of SPARE.  The analysis also explores the effectiveness of SPARE when applied to different layers of the model, revealing that mid-layers are most effective for controlling knowledge selection behavior.", "first_cons": "The study focuses on open-domain question-answering tasks with knowledge conflicts, limiting the generalizability of the findings to other tasks or types of conflicts.", "first_pros": "SPARE significantly outperforms state-of-the-art representation engineering methods (+10%) and contrastive decoding methods (+15%).", "keypoints": ["SPARE significantly outperforms existing methods, achieving +10% improvement over representation engineering methods and +15% over contrastive decoding methods.", "Mid-layers of the LLMs are identified as the most effective for controlling knowledge selection behavior.", "Analysis shows SPARE effectively changes knowledge selection behavior and maintains original behavior while minimizing negative impact.", "Ablation study validates all components of the SPARE method are crucial for performance."], "second_cons": "The method relies on pre-trained sparse auto-encoders (SAEs), which may not be available for all LLMs, limiting its applicability.", "second_pros": "SPARE is a training-free method, offering efficient and transparent control over LLM behavior at inference time.", "summary": "The experimental results demonstrate that the proposed SPARE method significantly outperforms existing representation engineering and contrastive decoding methods in controlling knowledge selection behaviors of LLMs in open-domain question answering tasks with knowledge conflicts, particularly showing a +10% improvement over representation engineering methods and a +15% improvement over contrastive decoding methods.  Analysis reveals that mid-layers are most effective for control, and an ablation study verifies the importance of all SPARE components.  However, the approach's reliance on pre-trained SAEs and its focus on specific tasks limit the generalizability of these findings."}}, {"page_end_idx": 7, "page_start_idx": 7, "section_number": 6, "section_title": "Analysis and Discussion", "details": {"details": "This section delves into a multi-faceted analysis of the model's performance in controlling knowledge selection, focusing on the effectiveness of its layer choices, the impact of interventions, and an ablation study. The layer choice analysis reveals that the model achieves optimal control in the middle layers (around the 15th layer for Llama2-7B and Llama3-8B, and between the 23rd and 31st layers for Gemma2-9B), aligning with the idea that these layers hold crucial task-specific functional information.  The intervention analysis showcases the model's capability to significantly alter the model's behaviour while maintaining high accuracy, surpassing other comparable methods.  A detailed ablation study is also provided, which highlights the model's reliance on both the removal of negative features and the addition of positive ones.  The detailed analysis of residual streams via probing and skewness measurement provides further insights into the model's internal functioning during knowledge conflicts.", "first_cons": "The analysis primarily focuses on the ability to control the selection of either contextual or parametric knowledge, potentially overlooking the need for more nuanced decision-making regarding which knowledge source to prioritize in complex situations.  The real-world applicability may be limited due to the reliance on pre-trained SAEs and the relatively narrow range of tasks considered.", "first_pros": "The multi-faceted analysis provides a comprehensive evaluation of the model's performance, going beyond simple accuracy metrics by examining the model's capability in steering towards either contextual or parametric knowledge, and maintaining accuracy. The ablation study systematically evaluates the contribution of different components, improving the model's understandability and providing a better understanding of its strengths and limitations.", "keypoints": ["Optimal control is achieved in the middle layers of the model (around 15th for Llama2-7B and Llama3-8B, and between 23rd and 31st for Gemma2-9B).", "SPARE significantly improves the model's ability to control the use of either contextual or parametric knowledge, surpassing other methods by +10% for representation engineering and +15% for contrastive decoding methods.", "The ablation study demonstrates the importance of both removing undesired features and adding desired ones for effective control; removing only decreases accuracy to near zero, while adding only provides little to no improvement.", "Analysis of residual stream reveals distinct skewness patterns when selecting contextual knowledge versus parametric knowledge, highlighting the impact of the model's internal state on knowledge selection."], "second_cons": "The study's reliance on specific datasets and models limits the generalizability of its findings. Further research is needed to explore how the model performs on a broader range of datasets and tasks to confirm its robustness and wider applicability.", "second_pros": "The analysis combines quantitative and qualitative evaluations of the model's performance, offering a more in-depth understanding of its behavior. The combination of probing accuracy, skewness pattern analysis, and ablation study provides a strong basis for understanding how the model operates internally and how its design choices influence the ability to effectively control knowledge selection.", "summary": "This section presents a detailed analysis of the effectiveness of a representation engineering method in controlling knowledge selection in LLMs.  The analysis focuses on the optimal layer choice for intervention, the model's ability to steer towards either contextual or parametric knowledge while maintaining accuracy, and an ablation study highlighting the importance of both adding positive and removing negative features. The analysis of residual streams through probing and skewness measurements provides additional insights into the model's internal workings during knowledge conflicts.  The findings suggest that the model effectively controls knowledge selection but may have limited generalizability due to its reliance on specific datasets, models, and pre-trained sparse autoencoders."}}, {"page_end_idx": 8, "page_start_idx": 8, "section_number": 7, "section_title": "Related Works", "details": {"details": "This section, \"Related Works,\" primarily discusses two prominent approaches to enhancing Large Language Model (LLM) understanding: Representation Engineering and Mechanistic Interpretability.  Representation Engineering, the main focus, offers a higher-level framework for understanding and modifying LLM behavior by directly manipulating their representations.  This contrasts with Mechanistic Interpretability, which attempts to understand LLMs by analyzing lower-level mechanisms like individual neurons and circuits. The section notes that while mechanistic interpretability excels at explaining simpler mechanisms, it struggles with more complex LLM phenomena.  Additionally, the section touches on the concept of \"Knowledge Conflicts\" in LLMs\u2014discrepancies between contextual and parametric knowledge\u2014and the use of Sparse Auto-Encoders (SAEs) as a tool to analyze LLM internal representations.  The role of SAEs in decomposing complex activations into simpler, more interpretable features is highlighted, particularly in relation to their use in the proposed SPARE method.", "first_cons": "The discussion of Mechanistic Interpretability feels somewhat underdeveloped, providing limited detail and contrast to Representation Engineering beyond stating its limitations in handling complex LLM behavior.", "first_pros": "The section clearly contrasts Representation Engineering with Mechanistic Interpretability, highlighting the strengths and limitations of each approach. This provides valuable context for understanding the benefits of focusing on representation engineering for improved LLM control.", "keypoints": ["Representation Engineering offers a higher-level, more effective approach to understanding and modifying complex LLM behaviors compared to Mechanistic Interpretability.", "Sparse Auto-Encoders (SAEs) are highlighted as a key tool for decomposing complex LLM activations into more interpretable features.  The use of SAEs is not only relevant to understanding but also directly contributes to the effectiveness of the SPARE method (mentioned earlier in the paper).", "The section acknowledges the challenges of mechanistic interpretability in handling complex phenomena.  This helps to justify the chosen approach of representation engineering.", "The concept of \"Knowledge Conflicts\" in LLMs\u2014discrepancies between contextual and parametric knowledge\u2014is introduced as a significant challenge addressed by the proposed methods."], "second_cons": "The section lacks specific examples of how Knowledge Conflicts manifest in real-world scenarios and the consequences of these conflicts. More concrete examples would enhance understanding.", "second_pros": "The section effectively positions the paper's work within the broader research landscape of LLM understanding and control, creating a strong foundation for the presented methodology.", "summary": "This section of the paper explores related work in understanding and controlling large language models (LLMs), focusing on the contrast between representation engineering and mechanistic interpretability. It highlights the advantages of representation engineering for addressing complex LLM behaviors and introduces the concept of knowledge conflicts within LLMs, setting the stage for the paper's proposed method, SPARE. The section also underscores the role of sparse autoencoders (SAEs) as tools for interpreting and manipulating LLM internal representations."}}, {"page_end_idx": 9, "page_start_idx": 9, "section_number": 8, "section_title": "Conclusions", "details": {"details": "The conclusion section summarizes the research on context-memory knowledge conflicts in LLMs.  The researchers found that these conflicts can be detected by analyzing the residual stream of the model.  They then introduced SPARE, a training-free representation engineering method leveraging pre-trained sparse auto-encoders (SAEs). SPARE effectively steers the knowledge selection behaviors of LLMs, improving accuracy in open-domain question-answering tasks by +10% over existing representation engineering methods and +15% over contrastive decoding methods.  While effective, SPARE's reliance on pre-trained SAEs and its focus on specific ODQA tasks limit generalizability. Future work should explore broader applications and address these limitations.  The analysis also includes multi-perspective controlling analysis that measures how effectively SPARE changes behavior, the potential negative impact of interventions, and the impact of different layer editing choices.  It also examines the effects of editing hidden states rather than SAE activations directly, noting reconstruction losses and the need for non-negative activations. The study highlights the mid-layers of LLMs as particularly influential for knowledge selection, a finding supported by prior research.  Ablation studies demonstrate the importance of both adding and removing features to effectively steer model behavior, avoiding unintended consequences of editing.", "first_cons": "SPARE's reliance on pre-trained SAEs limits its applicability to models without readily available SAEs.", "first_pros": "SPARE improves accuracy in open-domain question-answering tasks by +10% compared to existing representation engineering methods and by +15% compared to contrastive decoding methods.", "keypoints": ["SPARE, a training-free method, improves accuracy by +10% over representation engineering methods and +15% over contrastive decoding.", "Mid-layers of LLMs are most influential for knowledge selection.", "Editing hidden states directly is less effective than editing SAE activations.", "Ablation studies show the importance of both adding and removing features to control knowledge selection, avoiding unintended side effects of editing"], "second_cons": "The study focuses on specific ODQA tasks with context-memory knowledge conflicts, limiting its generalizability to other tasks or conflict types.", "second_pros": "The study provides a multi-perspective analysis of SPARE's controlling capabilities, including impact of intervention and ablation studies examining the effectiveness of different editing strategies.", "summary": "This research concludes that context-memory knowledge conflicts in large language models (LLMs) can be effectively addressed using SPARE, a novel training-free representation engineering method. SPARE leverages pre-trained sparse autoencoders to accurately control knowledge selection, significantly outperforming existing methods in open-domain question-answering tasks.  However, limitations exist in its reliance on pre-trained SAEs and its focus on specific ODQA tasks."}}]