{"references": [{" publication_date": "2019", "fullname_first_author": "Fabio Petroni", "paper_title": "Language Models as Knowledge Bases?", "reason": "This paper is foundational because it investigates the potential of language models to serve as knowledge bases, a key aspect relevant to understanding and addressing knowledge conflicts. It lays the groundwork for exploring how knowledge is stored and accessed within LLMs, and how this can lead to inaccuracies and conflicts.", "section_number": 1}, {" publication_date": "2020", "fullname_first_author": "Tom B Brown", "paper_title": "Language Models are Few-Shot Learners", "reason": "This is a highly influential paper in the field of LLMs, establishing the concept of few-shot learning and showcasing the remarkable ability of LLMs to perform various tasks with minimal explicit training. This is directly relevant to the current work, as it highlights the inherent capabilities of LLMs while simultaneously suggesting potential limitations in knowledge accuracy and consistency.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Hugo Touvron", "paper_title": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "reason": "This paper introduces Llama 2, a significant advancement in the field of LLMs that is used in the current study.  The authors of this work use Llama 2 to evaluate the proposed method, making it a crucial reference in demonstrating the practical applicability of the proposed method to a state-of-the-art LLM.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Albert Q Jiang", "paper_title": "Mistral 7B", "reason": "This paper introduces Mistral 7B, another significant LLM. The authors of this paper use Mistral 7B to evaluate the proposed method, making it a crucial reference in demonstrating the practical applicability of the proposed method to a state-of-the-art LLM.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Rohan Anil", "paper_title": "Gemini: A Family of Highly Capable Multimodal Models", "reason": "This paper introduces Gemini, a large multimodal model, demonstrating further advancements in LLM capabilities and the expanding scope of applications.  Mentioning Gemini in the related work section provides broader context to the ongoing development and challenges in the field of LLMs, underscoring the significance of addressing knowledge conflicts.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Rongwu Xu", "paper_title": "Knowledge Conflicts for LLMs: A Survey", "reason": "This paper serves as a comprehensive overview of knowledge conflicts in LLMs, which is the primary focus of the current study. This survey provides a broad contextual background on the types of knowledge conflicts, their impact, and existing approaches. This makes it an essential reference for understanding the specific problem addressed in the current research.", "section_number": 1}, {" publication_date": "2021", "fullname_first_author": "Shayne Longpre", "paper_title": "Entity-based Knowledge Conflicts in Question Answering", "reason": "This paper is highly relevant as it introduces the concept of knowledge conflicts in the context of question answering.  The study explicitly addresses the problem of conflicts between an LLM's internal knowledge and contextual information, directly connecting to the core issue addressed by the current research.  The datasets and methodology introduced in this paper are also used in the current research, making it an important reference.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Leo Gao", "paper_title": "Scaling and Evaluating Sparse Autoencoders", "reason": "This paper introduces a method of scaling and evaluating sparse autoencoders. The current research builds on this by proposing a method that uses pre-trained sparse autoencoders to steer the knowledge selection behavior of LLMs.  Understanding how to scale and evaluate these models is crucial for their effective application in the context of the current research.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Trenton Bricken", "paper_title": "Towards Monosemanticity: Decomposing Language Models with Dictionary Learning", "reason": "This paper proposes a method for decomposing language models into monosemantic features. The approach in this study is closely related, as the authors also decompose complex LLM representations into more easily interpretable features using sparse autoencoders.  This makes it an important reference to contextualize and support the technical approach taken in the current research.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Robert Huben", "paper_title": "Sparse Autoencoders Find Highly Interpretable Features in Language Models", "reason": "This paper is highly relevant because it explores the use of sparse autoencoders for interpreting language model representations. It directly supports the methodology used in the current research, which also leverages sparse autoencoders to identify and manipulate specific features within the LLM's activations to steer its behavior. This shared approach makes it a highly relevant reference.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Adly Templeton", "paper_title": "Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet", "reason": "This paper is highly relevant as it demonstrates the potential of using sparse autoencoders for extracting interpretable features from LLMs. The current research also leverages sparse autoencoders to identify and manipulate the activations that control the knowledge selection behavior of LLMs.  This makes the paper a crucial reference in justifying the choice of method and establishing its potential.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Roee Hendel", "paper_title": "In-context learning creates task vectors", "reason": "This paper introduces the concept of task vectors, which are representations that capture the essence of a specific task.  The current research draws inspiration from this work by hypothesizing that a combination of SAE activations might represent functional features, similar to task vectors, that control knowledge selection. The concept of task vectors provides a strong theoretical foundation for the approach taken in the current research.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Nicholas L. Turner", "paper_title": "Towards Monosemanticity: Decomposing Language Models with Dictionary Learning", "reason": "This paper is directly relevant because it explores the use of dictionary learning for decomposing language model representations into more interpretable features.  This relates to the current research's use of sparse autoencoders for the same purpose of achieving finer control over LLM behavior. The shared methodological approach makes this a significant reference.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Yifu Qiu", "paper_title": "Spectral Editing of Activations for Large Language Model Alignment", "reason": "This work is highly relevant as it also uses representation engineering to control the behavior of LLMs. The current research directly compares its performance with this method on the same datasets and tasks, highlighting the relative advantages of the proposed approach. The comparison is essential for demonstrating the effectiveness of the proposed method.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Yung-Sung Chuang", "paper_title": "DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models", "reason": "This paper introduces a contrastive decoding method for improving the factuality of LLM outputs.  The current research directly compares its method with this approach, demonstrating the superior performance of the proposed method in steering knowledge selection behavior.  This comparison is crucial for highlighting the advancements made in the current research.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Weijia Shi", "paper_title": "Trusting Your Evidence: Hallucinate Less with Context-Aware Decoding", "reason": "This paper is highly relevant as it proposes a contrastive decoding method for reducing hallucinations in LLM outputs. The current research directly compares its method with this approach, demonstrating superior performance in steering knowledge selection behavior.  This comparison is essential for validating the effectiveness of the proposed method and highlighting its improvements.", "section_number": 5}, {" publication_date": "2020", "fullname_first_author": "Tom B Brown", "paper_title": "Language Models are Few-Shot Learners", "reason": "This paper is a highly influential work that introduces the concept of in-context learning, demonstrating the ability of LLMs to perform tasks with minimal explicit training. The current research utilizes in-context learning as a baseline and compares its performance with the proposed method, highlighting the superior capabilities of the new method in resolving knowledge conflicts.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Abhimanyu Dubey", "paper_title": "The Llama 3 Herd of Models", "reason": "This paper introduces Llama 3, a significant advancement in the field of LLMs used in the current study. The authors of this work use Llama 3 to evaluate the proposed method, making it a crucial reference for demonstrating the practical applicability of the proposed method to a state-of-the-art LLM.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Morgane Rivi\u00e8re", "paper_title": "Gemma 2: Improving Open Language Models at a Practical Size", "reason": "This paper introduces Gemma 2, another significant LLM. The authors of this work use Gemma 2 to evaluate the proposed method, making it a crucial reference for demonstrating the practical applicability of the proposed method to a state-of-the-art LLM.  The use of pre-trained SAEs from GemmaScope in this paper is also particularly relevant to the current research.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Timo Schick", "paper_title": "Toolformer: Language Models Can Teach Themselves to Use Tools", "reason": "This paper explores the use of tools to augment LLMs, providing a broader context for the research on knowledge conflicts and retrieval-based methods.  The use of external tools, such as knowledge bases, is directly relevant to the discussion of knowledge conflicts and the approaches to addressing them, making it a pertinent reference.", "section_number": 5}]}