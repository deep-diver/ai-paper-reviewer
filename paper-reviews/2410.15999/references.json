{"references": [{" publication_date": "2019", "fullname_first_author": "Fabio Petroni", "paper_title": "Language models as knowledge bases?", "reason": "This paper is foundational in establishing the capability of Language Models to function as knowledge bases, a core concept that this paper builds upon and addresses the limitations of. It's the earliest work cited that directly supports the paper's main premise about LLMs' knowledge storage capacity.", "section_number": 1}, {" publication_date": "2020", "fullname_first_author": "Tom B Brown", "paper_title": "Language models are few-shot learners", "reason": "This is a highly influential paper that established the impressive few-shot learning abilities of large language models, setting the stage for further research into their knowledge-intensive tasks and the potential for conflict resolution. The work is foundational to this paper's exploration of how these models use both parametric and contextual knowledge.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Hugo Touvron", "paper_title": "Llama 2: Open foundation and fine-tuned chat models", "reason": "This paper introduces Llama 2, one of the LLMs used in the experiments.  The model's architecture and capabilities are directly relevant to the experimental results, making this a highly important reference for validating the proposed method on a widely used and accessible LLM.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Rohan Anil", "paper_title": "Gemini: a family of highly capable multimodal models", "reason": "This paper introduces the Gemini family of models, highlighting its multimodal capabilities and high performance, which is a crucial aspect of this paper's aim to deal with conflicts in knowledge.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Albert Q Jiang", "paper_title": "Mistral 7b", "reason": "This paper introduces Mistral-7B, another LLM used in the experimental evaluation.  Its capabilities and architecture are directly relevant to the paper's findings and contribute significantly to the reliability and reproducibility of the results.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Abhimanyu Dubey", "paper_title": "The llama 3 herd of models", "reason": "This work introduces Llama 3, a significant LLM used in the experiments.  Its architecture, capabilities, and performance on knowledge tasks are directly relevant to the experimental validation of the proposed method.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Morgane Rivi\u00e8re", "paper_title": "Gemma 2: Improving open language models at a practical size", "reason": "This paper introduces Gemma 2, a key LLM used in the experimental evaluation, and provides insights into its architecture and training.  Gemma 2's specific capabilities and performance are directly relevant to the validity and reliability of the experiments.  The pre-trained SAEs for this model also significantly contribute to the paper's contribution.", "section_number": 5}, {" publication_date": "2020", "fullname_first_author": "Vladimir Karpukhin", "paper_title": "Dense passage retrieval for open-domain question answering", "reason": "This paper is highly relevant because it discusses retrieval-augmented generation, a key technique used to address knowledge conflicts in LLMs.  The techniques described in this work are directly related to the problem that this paper attempts to solve, making it a fundamental reference.", "section_number": 1}, {" publication_date": "2020", "fullname_first_author": "Patrick Lewis", "paper_title": "Retrieval-augmented generation for knowledge-intensive nlp tasks", "reason": "This paper is highly relevant because it discusses retrieval-augmented generation for knowledge-intensive NLP tasks, a crucial technique in handling knowledge conflicts in LLMs. The approach described here is directly connected to the problem and solution this paper presents, making it a very important reference.", "section_number": 1}, {" publication_date": "2021", "fullname_first_author": "Shayne Longpre", "paper_title": "Entity-based knowledge conflicts in question answering", "reason": "This paper introduces NQSwap, one of the benchmark datasets used in the evaluation.  The dataset's characteristics (knowledge conflicts) are directly relevant to the paper's core contribution, making the reference essential for understanding the context and validation of the results.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Hung-Ting Chen", "paper_title": "Rich knowledge sources bring complex knowledge conflicts: Recalibrating models to reflect conflicting evidence", "reason": "This paper directly addresses the issue of knowledge conflicts, offering detailed analysis and categorization of such conflicts. This is directly relevant to the core problem this paper investigates, enhancing its significance.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Yung-Sung Chuang", "paper_title": "Decoding by contrasting layers improves factuality in large language models", "reason": "This paper introduces the DoLa (Decoding by Contrasting Layers) method, a baseline used in comparing performance with the proposed SPARE method.  Including this in the reference section is essential for ensuring a fair and rigorous comparison in the evaluation of the proposed method.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Weijia Shi", "paper_title": "Trusting your evidence: Hallucinate less with context-aware decoding", "reason": "This paper introduces the CAD (Context-Aware Decoding) method, another baseline in the experimental comparison with SPARE.  This makes it a critical reference for evaluating the proposed method's performance against existing state-of-the-art approaches.", "section_number": 5}, {" publication_date": "2020", "fullname_first_author": "Tom B Brown", "paper_title": "Language models are few-shot learners", "reason": "This paper is highly relevant due to its introduction of in-context learning, a crucial technique in handling knowledge conflicts in LLMs. The ideas introduced in this paper are a significant baseline and a point of comparison for the proposed method.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Leo Gao", "paper_title": "Scaling and evaluating sparse autoencoders", "reason": "This paper is crucial because it introduces and evaluates sparse autoencoders (SAEs), a key component of the proposed method. The insights and techniques presented in this work are directly relevant to the design and evaluation of the proposed SPARE method.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Trenton Bricken", "paper_title": "Towards monosemanticity: Decomposing language models with dictionary learning", "reason": "This paper explores monosemanticity in LLMs which is a very relevant concept to the present work and is used for justification of using SAEs in the proposed method.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Roee Hendel", "paper_title": "In-context learning creates task vectors", "reason": "This paper is highly relevant to this paper due to its introduction of Task Vectors, which is the foundation of one of the baselines used in the experimental comparison and analysis of this work.  It provides a key point of reference for understanding and evaluating the effectiveness of the proposed method.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Yifu Qiu", "paper_title": "Spectral editing of activations for large language model alignment", "reason": "This paper introduces SEA (Spectral Editing of Activations), a prominent baseline method in representation engineering used for comparison in this paper's experimental evaluation.  Understanding SEA and its limitations is essential for a complete and fair analysis of the proposed method.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Nicholas L Turner", "paper_title": "Activation addition: Steering language models without optimization", "reason": "This paper introduces the ActAdd method which is an important baseline used for comparison in the experimental evaluation of the proposed SPARE method. ActAdd is used as a baseline in the paper's experimental section and understanding its approach is crucial for a rigorous comparison.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Alex Mallen", "paper_title": "When not to trust language models: Investigating effectiveness of parametric and non-parametric memories", "reason": "This paper is highly relevant as it directly addresses the issue of knowledge conflicts and how LLMs can be unreliable due to parametric knowledge contradictions.  It directly informs the problem definition and motivates the need for the proposed solution.", "section_number": 1}]}