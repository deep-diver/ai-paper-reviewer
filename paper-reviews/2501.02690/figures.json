[{"figure_path": "https://arxiv.org/html/2501.02690/x2.png", "caption": "Figure 1: \nGS-DiT generates multi-camera shooting videos by bringing pseudo 4D Gaussian fields to video diffusion transformers.", "description": "This figure showcases the GS-DiT framework's ability to generate videos from multiple camera angles.  The framework leverages pseudo 4D Gaussian fields, which provide a compact representation of the 4D spatiotemporal information within the video.  GS-DiT uses these fields to guide a video diffusion transformer, resulting in the generation of multi-camera videos that maintain visual consistency and dynamic content throughout the different viewpoints.", "section": "Abstract"}, {"figure_path": "https://arxiv.org/html/2501.02690/x3.png", "caption": "Figure 2: An overview of GS-DiT. In the training stage, we build a pseudo 4D Gaussian field from an input video via dense 3D point tracking. Our GS-DiT learns to generate the original video guided by the video rendered from the pseudo 4D Gaussian field. In the inference stage, we can build the pseudo 4D Gaussian via dense 3D point tracking, directly lifting the depth map, or optimizing a 4D Gaussian field. Editing and rendering the Gaussian field with scheduled camera intrinsic and extrinsic bring various cinematic effects.", "description": "This figure illustrates the GS-DiT framework.  The training process begins with dense 3D point tracking of an input video to construct a pseudo 4D Gaussian field.  This field is then rendered into a video, which serves as guidance for a pre-trained video diffusion transformer (DiT) that's fine-tuned to generate the original input video.  During inference, the pseudo 4D Gaussian field can be created using 3D point tracking, depth map lifting, or 4D Gaussian field optimization.  Manipulating the Gaussian field parameters (camera intrinsics and extrinsics) allows for cinematic effects like multi-camera shooting and dolly zoom.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2501.02690/x4.png", "caption": "Figure 3: The neural network architecture of GS-DiT. GS-DiT generates video conditioned on the video rendered from our pseudo 4D Gaussian field.", "description": "The figure illustrates the architecture of the GS-DiT model.  It shows how a pre-trained video diffusion transformer (DiT) is fine-tuned using videos rendered from a pseudo 4D Gaussian field.  The pseudo 4D Gaussian field is constructed using 3D point tracking from the input video. The fine-tuned DiT then generates videos conditioned on this rendered video, allowing for advanced 4D control over the generated video content.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2501.02690/x5.png", "caption": "Figure 4: Qualitative comparison of synchronized video generation with camera control. (a)(b) are the first and last frame of the input video. (c) is the camera trajectory visualization. (d)(e)(f) are the last frame of the video generated by our GS-DiT, GCD, and MonST3R.", "description": "Figure 4 presents a qualitative comparison of three different video generation methods in terms of their ability to generate synchronized videos with camera control.  The input video's first and last frames are shown in (a) and (b). Panel (c) visualizes the camera trajectory used for the generation. The last frames generated by GS-DiT (the authors' proposed method), GCD, and MonST3R are displayed in (d), (e), and (f) respectively, allowing for a visual comparison of the results of each method.", "section": "4.2 4D Video Control"}, {"figure_path": "https://arxiv.org/html/2501.02690/x6.png", "caption": "Figure 5: Video generation with 4D control. (a) presents the dolly zoom effects and (b) rotates the body of the fan.", "description": "This figure demonstrates the capabilities of GS-DiT in controlling video generation with four-dimensional (4D) control.  Subfigure (a) shows the effect of a dolly zoom, a cinematic technique that involves simultaneous zooming and camera movement to create a dynamic perspective shift. Subfigure (b) illustrates object manipulation, specifically rotating the blades of a fan, showcasing the model's ability to modify individual elements within a video scene, all within the 4D control framework of GS-DiT.", "section": "4.2. 4D Video Control"}, {"figure_path": "https://arxiv.org/html/2501.02690/x7.png", "caption": "Figure A1: Comparison with video inpainting. (a) is the input frame. (b) is the video frame generated by our GS-DiT. (c) and (d) are the video frames generated by Inpainting-A and Inpainting-B.", "description": "This figure compares the video inpainting results of GS-DiT against two baseline methods (Inpainting-A and Inpainting-B). It shows how GS-DiT successfully fills in missing parts of the video frame, while the baseline methods struggle to produce coherent results. The input frame (a) is compared with the output of GS-DiT (b), Inpainting-A (c), and Inpainting-B (d), highlighting GS-DiT's superiority in restoring the integrity of the video.", "section": "Appendix"}, {"figure_path": "https://arxiv.org/html/2501.02690/x8.png", "caption": "Figure A2: The structure of M\u2062o\u2062t\u2062i\u2062o\u2062n\u2062E\u2062n\u2062cf\u2062l\u2062o\u2062w\ud835\udc40\ud835\udc5c\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc38\ud835\udc5bsubscript\ud835\udc50\ud835\udc53\ud835\udc59\ud835\udc5c\ud835\udc64{MotionEnc_{flow}}italic_M italic_o italic_t italic_i italic_o italic_n italic_E italic_n italic_c start_POSTSUBSCRIPT italic_f italic_l italic_o italic_w end_POSTSUBSCRIPT.", "description": "Figure A2 shows the architecture of the MotionEnc_flow module used in the D3D-PT (Dense 3D Point Tracking) method.  The input consists of the cropped correlation information from the previous iteration (Corr(x_t,i-1)), the flow from the previous iteration (f_t,i-1), and the visibility from the previous iteration (v_t,i-1). These inputs are processed through a series of convolutional layers (Conv1x1, Conv7x7, Conv3x3) to extract relevant motion features. The output is the motion feature (m_t,i), which is then fed into the ConvGRU_flow module for further processing.", "section": "Appendix A. More Details about D3D-PT"}, {"figure_path": "https://arxiv.org/html/2501.02690/x9.png", "caption": "Figure A3: The structure of F\u2062l\u2062o\u2062w\u2062H\u2062e\u2062a\u2062d\ud835\udc39\ud835\udc59\ud835\udc5c\ud835\udc64\ud835\udc3b\ud835\udc52\ud835\udc4e\ud835\udc51{FlowHead}italic_F italic_l italic_o italic_w italic_H italic_e italic_a italic_d.", "description": "Figure A3 illustrates the architecture of the FlowHead component within the D3D-PT (Dense 3D Point Tracking) module.  FlowHead takes as input the output of a convolutional recurrent neural network (ConvGRU), processes it through several convolutional layers (Conv3x3), and ultimately outputs a refined flow field (\u0394ft,i), representing the update to the point tracks.  The use of convolutional layers allows for spatial context to be considered in refining the flow estimations. The structure depicts a feed-forward network design, enhancing the precision of the point tracking by leveraging spatial information.", "section": "A. More Details about D3D-PT"}, {"figure_path": "https://arxiv.org/html/2501.02690/x10.png", "caption": "Figure A4: The structure of V\u2062i\u2062s\u2062H\u2062e\u2062a\u2062d\ud835\udc49\ud835\udc56\ud835\udc60\ud835\udc3b\ud835\udc52\ud835\udc4e\ud835\udc51{VisHead}italic_V italic_i italic_s italic_H italic_e italic_a italic_d.", "description": "Figure A4 shows the architecture of the VisHead component within the D3D-PT (Dense 3D Point Tracking) module.  VisHead predicts the visibility of 3D points, indicating whether they are visible or occluded in a given frame.  The figure details the convolutional layers used to process input features and produce the visibility prediction. The network uses several convolutional layers (Conv3x3) of varying filter sizes (e.g., 128, 256) to extract relevant features.  The final layer uses a 1x1 convolution to output the visibility prediction.  The structure is part of the iterative refinement process where points' 2D locations, depths, and visibility are progressively refined.", "section": "Appendix A. More Details about D3D-PT"}]