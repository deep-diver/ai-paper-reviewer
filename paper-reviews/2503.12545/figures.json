[{"figure_path": "https://arxiv.org/html/2503.12545/x2.png", "caption": "Figure 1: Example of an image of Joe Biden speaking at the White House. Before unlearning (a) MLLMs have the ability to generate responses related to various visual concepts (Identify and Event). The goal of Machine Unlearning (MU) for MLLMs is to selectively forget specific concepts within the model. When the unlearning target is Identity (b), the model mistakenly identifies Joe Biden as a different person. When the unlearning target is Event (c), the model misinterprets the speech as a concert.", "description": "Figure 1 illustrates the concept of machine unlearning (MU) in multimodal large language models (MLLMs) using an example image of Joe Biden speaking at the White House.  Panel (a) shows that before unlearning, the MLLM correctly identifies both the person (Joe Biden) and the event (speaking at the White House).  The goal of MU is to selectively remove specific information from the model without retraining. Panel (b) demonstrates the result when the unlearning target is the \"Identity\" of Joe Biden; the model incorrectly identifies him as someone else. Panel (c) shows the outcome when the unlearning target is the \"Event\"; the model misinterprets the event as a concert. This figure highlights the challenge of MU in MLLMs, where removing specific information can unintentionally affect related concepts.", "section": "Abstract"}, {"figure_path": "https://arxiv.org/html/2503.12545/x6.png", "caption": "Figure 2: Comparison between previous MU benchmarks for MLLMs and our PEBench.", "description": "This figure compares PEBench with two other multimodal machine unlearning (MU) benchmarks for large language models (LLMs): MMUBench and CLEAR.  MMUBench uses real-world entities and images, while CLEAR uses synthetic data.  PEBench, in contrast, utilizes synthetic data to avoid data leakage issues and enables a fairer comparison of MU methods. The figure highlights that existing benchmarks focus on discrete entities, whereas PEBench expands the scope to encompass both identities and event scenes (broader visual concepts) commonly found together within images. This allows for a more comprehensive and realistic evaluation of MU in MLLMs.", "section": "3. PEBench"}, {"figure_path": "https://arxiv.org/html/2503.12545/x7.png", "caption": "Figure 3: Overview of the PEBench framework, illustrating the data curation and evaluation processes.", "description": "Figure 3 provides a detailed overview of the PEBench framework, illustrating the complete data curation and evaluation pipeline. The framework consists of two main stages. The first stage focuses on data curation: generating text descriptions for diverse person-event pairs using GPT-4 and generating corresponding images to ensure consistency and coupling in visual concepts. The second stage is the evaluation pipeline which involves splitting the dataset, training the goal model and the finetuned model, and finally evaluating their performance to assess the effectiveness of the unlearning methods using metrics like Efficacy, Generality, Scope, and more. ", "section": "3. PEBench"}]