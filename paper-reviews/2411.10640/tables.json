[{"content": "| Type | Public (M) | In-House (M) | In-House / Public |\n|---|---|---|---|\n| **Pure Text** | 2.2 | 64.7 | 29.4 |\n| **Caption** | 10.0 | 306.3 | 30.6 |\n| **VQA** | 20.3 | 44.4 | 2.2 |\n| **OCR** | 23.3 | 173.9 | 7.5 |\n| **Total** | **55.8** | **589.3** | **10.6** |", "caption": "Table 1: Detailed statistics of the fine-tuning dataset. Summary of dataset types, counts (in millions), and in-house/public ratios for each category used in fine-tuning.", "description": "This table presents a detailed breakdown of the dataset used for the fine-tuning stage of the BlueLM-V-3B model. It includes the number of data points (in millions) for each category (Pure Text, Caption, VQA, OCR) and specifies whether the data originates from publicly available sources or internal (in-house) collections. The in-house/public ratio is also provided for each category, offering insights into the composition of the training data.", "section": "4. Training Data"}, {"content": "| Language Model | Vision Model | Params | Method | VQAv2<sub>val</sub> | TextVQA<sub>val</sub> | DocVQA<sub>val</sub> | OCRBench | ChartQA<sub>test</sub> |\n|---|---|---|---|---|---|---|---|---|\n| **MiniCPM-2B** [39] | **SigLIP-400M** [141] | 3B | InternVL 1.5 | 70.5 | 46.9 | 26.2 | 327 | 15.7 |\n|  |  |  | LLaVA-NeXT | 70.1 | 44.2 | 24.3 | 324 | 14.8 |\n|  |  |  | Ours | **71.8** | **49.4** | **27.3** | **343** | **16.9** |\n| **BlueLM-3B** | **SigLIP-400M** [141] | 3B | InternVL 1.5 | 78.3 | 52.7 | 28.7 | 338 | 16.8 |\n|  |  |  | LLaVA-NeXT | 77.7 | 51.4 | 29.6 | 351 | 16.4 |\n|  |  |  | Ours | **79.5** | **56.2** | **31.3** | **360** | **17.5** |\n| Ours (fully-trained) |  |  |  | 82.7 | 78.4 | 86.6 | 829 | 80.4 |", "caption": "Table 2: Comparison results of different dynamic resolution methods. We compare the performance of models trained using different dynamic resolution methods. We use the LLaVA\u00a0[69] 558k dataset for pre-training, and the LLaVA 665k dataset for fine-tuning. To better demonstrate our improvements, we conduct experiments on both our in-house BlueLM-3B language model and the open-sourced MiniCPM-2B language model, which have similar parameter counts (2.7B). Our dynamic image processing method achieves the best performance. \u2020We also provide the results of the fully trained BlueLM-V-3B model for reference.", "description": "This table compares the performance of different dynamic image resolution methods used in training multimodal large language models (MLLMs).  The comparison uses two models with similar parameter counts: the in-house BlueLM-3B and the open-source MiniCPM-2B (both around 2.7B parameters).  The LLaVA dataset (558k for pre-training and 665k for fine-tuning) was used for training.  The table highlights the superior performance of the proposed dynamic image processing method in BlueLM-V-3B, and also includes results for a fully trained BlueLM-V-3B model for additional context.", "section": "3.1 Basic Network Components"}, {"content": "| Model | Params | Avg. | MMBench | MMStar | MMMU | MathVista | HallusionBench | AI2D | OCRBench | MMVet |\n|---|---|---|---|---|---|---|---|---|---|---|\n| **Qwen2-VL** [125] | 8B | **67** | 81 | 60.7 | **53.7** | 61.4 | **50.4** | 83 | 843 | **61.8** |\n| **MiniCPM-V-2.6** [134] | 8B | 65.2 | 78 | 57.5 | 49.8 | 60.6 | 48.1 | 82.1 | **852** | 60 |\n| **InternVL2** [22] | 8B | 64.1 | 79.4 | 61.5 | 51.2 | 58.3 | 45 | 83.6 | 794 | 54.3 |\n| **POINTS-Qwen2.5** [74] | 8.3B | 62.5 | 78 | 60.9 | 51.4 | **63** | 45.6 | 81.2 | 717 | 47.9 |\n| **BlueLM-V (Ours)** | **3B** | 66.1 | **82.7** | **62.3** | 45.1 | 60.8 | 48 | **85.3** | 829 | **61.8** |", "caption": "Table 3: OpenCompass benchmark. Comparison results on the OpenCompass benchmark for models with parameter sizes less than or equal to 10B. BlueLM-V-3B achieves state-of-the-art performance on 4 out of 8 tasks, with an average performance ranking of second.", "description": "This table presents a comparison of BlueLM-V-3B's performance against other large language models (LLMs) on the OpenCompass benchmark.  OpenCompass is a comprehensive evaluation suite for foundation models, assessing performance across a range of tasks. The table focuses on models with 10 billion parameters or fewer.  BlueLM-V-3B, despite having only 3 billion parameters, achieves state-of-the-art results on four out of the eight tasks evaluated and ranks second overall.", "section": "5. Experiments"}, {"content": "| Model | Params | TextVQA<sub>val</sub> | DocVQA<sub>test</sub> | MTVQA |\n|---|---|---|---|---|\n| **Phi-3-Vision** [2] | 4.2B | 72.4 | 84.6 | 13.9 |\n| **MiniCPM-V-2** [134] | 2.8B | 73.2 | 71.9 | 9.3 |\n| **InternVL2** [22] | 4B | 74.7 | 89.2 | 15.5 |\n| **Qwen2-VL** [125] | 2B | 79.9 | 90.1 | 20.7 |\n| **BlueLM-V (Ours)** | 3B | 78.4 | 87.8 | **32.7** |", "caption": "Table 4: Text-centric/OCR benchmarks. Comparison on text-centric/OCR benchmarks shows that BlueLM-V-3B achieves performance comparable to SOTA MLLMs with similar parameter sizes, while significantly enhancing multilingual capability. \u2020We evaluate TextVQA and MTVQA on VLMEvalKit\u00a0[29] for a fair comparison. OCRBench has been included in OpenCompass.", "description": "Table 4 presents a comparison of BlueLM-V-3B's performance on text-centric and OCR benchmarks against other state-of-the-art (SOTA) Multimodal Large Language Models (MLLMs).  The focus is on models with similar parameter sizes. The results show BlueLM-V-3B achieves comparable performance to these SOTA models but with a significant advantage in multilingual capabilities.  To ensure fairness, TextVQA and MTVQA evaluations used the VLMEvalKit [29].  Note that OCRBench results are included within the OpenCompass benchmark.", "section": "5. Experiments"}, {"content": "| Model Name | Params | Processor | Solution | Image Processing | LLM Prefilling | Throughput |\n|---|---|---|---|---|---|---|\n| MiniCPM-V 2.5 [134] | 8B | MediaTek Dimensity 9300 | CPU (llama.cpp) \u2639 | 4.0s | 13.9s | 4.9 token/s |\n| BlueLM-V-3B (Ours) | 3B | MediaTek Dimensity 9300 | NPU \u263a | 2.53s (0.47+2.06) | 2.7s | 24.4 token/s |", "caption": "Table 5: Deployment efficiency comparison with MiniCPM-V. MiniCPM-V deploys an 8B model on the CPU, leading to longer image processing latency, LLM prefilling latency, and lower throughput. \u2020MiniCPM-V calculates encoding latency by including both model loading time and encoding time. In our setting, we need 0.47s to simultaneously load the ViT and LLM once during system initialization.", "description": "This table compares the deployment efficiency of BlueLM-V-3B with MiniCPM-V.  MiniCPM-V uses an 8B parameter model and runs on the CPU, resulting in significantly slower image processing, LLM pre-filling (preparing the language model for generation), and overall throughput (tokens generated per second) compared to BlueLM-V-3B.  The difference is attributed to MiniCPM-V's CPU deployment and the inclusion of model loading time in its latency calculation.  BlueLM-V-3B's superior efficiency stems from its smaller 3B parameter model and use of the NPU (Neural Processing Unit). The 0.47s load time for BlueLM-V-3B accounts for the simultaneous loading of both Vision Transformer (ViT) and Language Model (LLM) components at the start of the system initialization.", "section": "5.3 Deployment Efficiency Evaluation"}, {"content": "| Task | Dataset |\n|---|---| \n| **Text-only** | ALLaVA [14], ScienceQA [79], Orca-Math [90], OpenOrca [63], MetaMathQA [137], WizardLM [130], MathInstruct [117] |\n| **Caption** | TextCaps [104], Screen2Words [122], VizWiz [36], Laion [99], COCO [20], LLaVA [71], ALLaVA [14], SVIT [146], SA1B [51], VSR [66], Chart2Text [48], MultiMath [94], ArXivCap [59], COYO [11] |\n| **OCR** | Wukong [32], HierText [76], TextOCR [108], WildReceipt [111], DocILE [105], SVRD [139], DocLayNet [95], XFUND [131], COCO-Text [121], SROIE [42], FUNSD [44], CORD [92], Paper2Fig100k [98], Docmatix [53], LAION-2B-OCR [65], SynthDoG [50], WebSight [54], DeepForm [112], Kleister [110], TabFact [19] |\n| **VQA** | LVIS-Instruct4V [124], CLEVR [45], TallyQA [3], LNQA [96], Geo170K [102], ALLaVA [14], DocVQA [84], ChartQA [83], ArxivQA [59], GEOS [100], PMC-VQA [144], KVQA [101], Geometry3K [77], MapQA [13], PlotQA [88], ViQuAE [55], VQA-RAD [52], ST-VQA [9], TextVQA [106], LLaVAR [145], SIBR [133], MMC-Inst [68], IconQA [78], GQA [43], SciGraphQA [60], LRV-Instruction [67], DVQA [46], InfographicVQA [85], FigureQA [47], WikiTableQuestions [93], TAT-DQA [147], VisualMRC [113], ScienceQA [79], OCR-VQA [89], WebSRC [21], PathVQA [37], UniGeo [15], ScreenQA [38], VizWiz [35], SVIT [146], CogVLM [126], FM-IQA [30], VQAv2 [31], OK-VQA [82], EST-VQA [127], VisDial [27], Shikra [16], Super-CLEVR [61], LLaVA [69], IDK [12], AlfWorld [103], M-HalDetect [34], Cambrian7M [116], LLaVA-OneVision [56], mPLUG-DocOwl [135], UReader [136] |", "caption": "Table 6: Training data. This table presents the open-source datasets used in the fine-tuning stage, corresponding with the categories and data volume in Tab.\u00a01 of the main text.", "description": "Table 6 lists the open-source datasets used in the fine-tuning stage of the BlueLM-V-3B model training.  It details the datasets used for each task category (Text-only, Caption, OCR, and VQA), showing the specific datasets contributing to each category.  The table connects these datasets to the data volumes reported in Table 1 of the paper, providing context for the scale of the fine-tuning data used. Note that some datasets may be used in multiple categories.", "section": "4. Training Recipe"}, {"content": "| Configuration | Stage 1 |\n|---|---| \n| **LLM Sequence Length** | 4096 |\n| **Dynamic Resolution** | None (384\u00d7384) |\n| **Optimizer** | AdamW |\n| **Optimizer Hyperparams** | \u03b2\u2081=0.9, \u03b2\u2082=0.98, \u03f5=10\u207b\u2076 |\n| **Peak LR** | 10\u207b\u00b3 |\n| **LR Schedule** | Cosine Decay |\n| **Weight Decay** | 0.05 |\n| **Training Steps** | 3.434k |\n| **Warm-up Steps** | 34 |\n| **Global Batch Size** | 720 |\n| **Gradient Accumulation** | 1 |\n| **Numerical Precision** | bfloat16 |", "caption": "Table 7: Hyper-parameters. Hyper-parameters for the pre-training stage (stage 1).", "description": "This table details the hyperparameters used during the pre-training phase (stage 1) of the BlueLM-V-3B model.  It includes settings for the optimizer (AdamW), learning rate schedule (cosine decay), weight decay, training steps, warm-up steps, batch size, gradient accumulation, and numerical precision.  These settings are crucial for controlling the training process and achieving optimal model performance.", "section": "3. BlueLM-V-3B"}, {"content": "| Configuration | Stage 2 |\n|---|---| \n| **LLM Sequence Length** | 4096 |\n| **Dynamic Resolution** | Up to 16 patches (1536x1536) |\n| **Optimizer** | AdamW |\n| **Optimizer Hyperparams** | \u03b2\u2081=0.9, \u03b2\u2082=0.98, \u03f5=10\u207b\u2076 |\n| **Peak LR** | 10\u207b\u2074 |\n| **LR Schedule** | Cosine Decay |\n| **Weight Decay** | 0.05 |\n| **ViT Layer-wise LR Decay** | 0.9 |\n| **Training Steps** | 131k |\n| **Warm-up Steps** | 1310 |\n| **Global Batch Size** | 5760 |\n| **Gradient Accumulation** | 8 |\n| **Numerical Precision** | bfloat16 |", "caption": "Table 8: Hyper-parameters. Hyper-parameters for the fine-tuning stage (stage 2).", "description": "This table details the hyperparameters used during the fine-tuning stage of the BlueLM-V-3B model training.  It includes settings for the optimizer (AdamW), learning rate schedule (cosine decay), weight decay, batch size, and other relevant parameters. Note that because of upsampling for some smaller datasets, the total number of training steps and the batch size might exceed the total volume of data.", "section": "3. BlueLM-V-3B"}]