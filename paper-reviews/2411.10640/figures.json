[{"figure_path": "https://arxiv.org/html/2411.10640/x1.png", "caption": "Figure 1: Comparison with mainstream MLLMs. We compare the performance of several mainstream MLLMs with a parameter count similar to that of BlueLM-V-3B across multiple benchmarks. BlueLM-V-3B leads in the majority of datasets.", "description": "This figure presents a comparison of BlueLM-V-3B's performance against other mainstream multimodal large language models (MLLMs) on various benchmark datasets.  The models selected for comparison have a similar parameter count to BlueLM-V-3B, ensuring a fair comparison based on model size. The benchmarks assess the MLLMs' capabilities across a variety of tasks, and the radar chart visually represents the performance of each model on each benchmark.  The key takeaway is that BlueLM-V-3B outperforms most of the other models in a majority of the benchmark datasets.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2411.10640/x2.png", "caption": "Figure 2: Model architecture of BlueLM-V-3B. The architecture of BlueLM-V-3B follows the classical LLaVA approach. We integrate a dynamic resolution processing module (as in LLaVA-NeXT\u00a0[70] and InternVL 1.5\u00a0[22]) to enhance model capabilities and apply token downsampling to reduce deployment complexity.", "description": "BlueLM-V-3B's architecture is a modified version of the LLaVA approach.  It consists of an image encoder (SigLIP-400M), an MLP projector, and a language model (BlueLM-2.7B). To handle high-resolution images efficiently, a dynamic resolution processing module is included, similar to those used in LLaVA-NeXT and InternVL 1.5.  A token downsampler is added to reduce the number of tokens, which improves efficiency for mobile devices. The diagram shows how images and text are processed and passed to the language model for response generation.", "section": "3. BlueLM-V-3B"}, {"figure_path": "https://arxiv.org/html/2411.10640/x3.png", "caption": "Figure 3: Existing methods overly enlarge images. (A) For LLaVA-NeXT, an image with resolution 394\u00d7\\times\u00d7390 selects a 2:2 aspect ratio and is resized and padded to 768\u00d7\\times\u00d7768 (4\u00d7\\times\u00d7 area enlargement). (B) For InternVL 1.5, an image with resolution 380\u00d7\\times\u00d776 chooses a 5:1 aspect ratio and is directly resized to 1920\u00d7\\times\u00d7384 (25\u00d7\\times\u00d7 area enlargement). BlueLM-V-3B, in contrast, selects a 1:1 aspect ratio for both resolutions, resulting in the minimum number of image tokens after ViT encoding, which can facilitate both model training and deployment.", "description": "This figure compares the image processing approaches of three different Multimodal Large Language Models (MLLMs): LLaVA-NeXT, InternVL 1.5, and the authors' proposed BlueLM-V-3B.  It highlights how each model handles high-resolution images. LLaVA-NeXT and InternVL 1.5 both utilize dynamic resolution schemes but tend to significantly enlarge images, leading to a larger number of image tokens after processing by the Vision Transformer (ViT).  LLaVA-NeXT increases the image area by 4 times, while InternVL 1.5 increases it by 25 times.  In contrast, BlueLM-V-3B uses a fixed 1:1 aspect ratio, minimizing image enlargement and resulting in the fewest image tokens. This optimized approach leads to more efficient model training and deployment on mobile devices.", "section": "3.2 Dynamic Image Resolution"}, {"figure_path": "https://arxiv.org/html/2411.10640/x4.png", "caption": "Figure 4: Batched image encoding on NPU. We design a parallel processing scheme for image patches on the NPU. The figure illustrates the case of 4 patches being processed in parallel.", "description": "This figure illustrates the parallel processing of image patches on the Neural Processing Unit (NPU) of a mobile device, a key optimization in BlueLM-V-3B.  The image shows four patches being processed concurrently using the batched image encoding approach, significantly improving processing speed. This contrasts with sequential processing of patches, which would be much slower. The system utilizes a pipeline to take advantage of the NPU's capabilities and minimize latency.", "section": "3.2 Dynamic Image Resolution"}, {"figure_path": "https://arxiv.org/html/2411.10640/x5.png", "caption": "Figure 5: Pipeline parallelism in image encoding. We design a pipeline parallelism scheme for image encoding. The Conv2D layer in the vision embedding module of SigLIP (on the CPU) and the vision transformer blocks (on the NPU) for different image patches run parallel to improve inference speed. This image illustrates the pipeline parallelism scheme combined with batched image patch encoding.", "description": "This figure illustrates how pipeline parallelism and batched image encoding are used in BlueLM-V-3B to speed up image processing.  The process begins with multiple image patches from a single image, which are encoded in parallel using the Conv2D layer of SigLIP on the CPU.  These intermediate results then feed into the Vision Transformer blocks on the NPU for further parallel processing, significantly shortening the overall inference time.", "section": "3.2 Dynamic Image Resolution"}, {"figure_path": "https://arxiv.org/html/2411.10640/x6.png", "caption": "Figure 6: Overall framework of deploying BlueLM-V-3B. We decouple ViT image processing from user instruction (text or audio) handling to enhance overall efficiency. The text responses by LLM can be further converted on the fly to audio responses.", "description": "The figure illustrates the overall framework of the BlueLM-V-3B deployment.  It highlights a key efficiency improvement: decoupling the image processing (handled by the ViT) from user input processing (text or audio instructions). This allows parallel processing, where image encoding happens concurrently with the handling of user instructions. Once the image encoding is finished, the user instruction is submitted to the LLM for response generation.  For added user-friendliness, the generated text responses can be converted into audio responses in real-time.", "section": "3. BlueLM-V-3B"}, {"figure_path": "https://arxiv.org/html/2411.10640/x7.png", "caption": "Figure 7: ViT inference time for 2:4 resolution aspect ratio. We experiment with 1, 2, 4, and 6 image patches per batch on the NPU, using a 2:4 resolution aspect ratio (comprising one global patch and 8 local patches). Overall, processing 4 patches per batch delivers the fastest performance.", "description": "This figure shows the inference time of the Vision Transformer (ViT) model when processing image patches with a 2:4 aspect ratio.  The experiment varies the number of image patches processed per batch on the Neural Processing Unit (NPU): 1, 2, 4, and 6. Each batch consists of a global patch and 8 local patches. The results show that processing 4 patches per batch achieves the fastest inference time, indicating an optimal balance between parallelization and computational overhead.", "section": "3.2 Dynamic Image Resolution"}, {"figure_path": "https://arxiv.org/html/2411.10640/x8.png", "caption": "Figure 8: Latency and output speed comparison. We compare the latency and output generation speed with processing different numbers of input tokens in parallel. t{x\ud835\udc65xitalic_x}/t1 implies processing x\ud835\udc65xitalic_x input tokens in parallel. The output token is fixed to 1 per trunk as the LLM can only generate one token for each forward process.", "description": "Figure 8 illustrates the trade-off between latency and throughput when processing various numbers of input tokens concurrently in the BlueLM-V-3B model.  The x-axis represents the number of tokens processed in parallel (t{x}/t1 denotes processing x tokens in parallel), while the y-axis shows both latency (in seconds) and output speed (in tokens per second).  The figure highlights that increasing the number of parallel tokens initially reduces latency and increases throughput, but beyond a certain point, this trend reverses likely due to the limitations of NPU resources.  The output token count remains fixed at one per forward pass, independent of the number of parallel tokens processed, reflecting the autoregressive nature of the LLM. This emphasizes the efficiency optimization achieved in BlueLM-V-3B.", "section": "3.3 Token Downsampler"}, {"figure_path": "https://arxiv.org/html/2411.10640/x9.png", "caption": "Figure 9: Case study. (A) LLaVA-NeXT chooses resolution 384\u00d7\\times\u00d7768 for an image with the original size of 380\u00d7\\times\u00d7393. (B) InternVL 1.5 chooses resolution 1920\u00d7\\times\u00d7384 for an image with the original size of 500\u00d7\\times\u00d7102.", "description": "This figure demonstrates the exaggerated image resolution in existing methods. Panel (A) shows that LLaVA-NeXT chooses a resolution of 384x768 for an image originally sized 380x393, significantly increasing the image size. Panel (B) illustrates InternVL 1.5 selecting a resolution of 1920x384 for an image initially sized 500x102, further highlighting the issue of excessive enlargement. This excessive enlargement increases the number of image tokens, hindering efficient deployment on mobile devices. ", "section": "3.2 Dynamic Image Resolution"}]