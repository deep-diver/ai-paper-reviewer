[{"figure_path": "https://arxiv.org/html/2412.06578/x2.png", "caption": "Figure 1: MoViE is a fast video editing model, capable of generating 12121212 frames per second on a mobile phone. It requires significantly fewer floating point operations (FLOPs) to edit a single video frame, making it computationally more efficient than competing methods.", "description": "Figure 1 demonstrates the speed and efficiency of MoViE, a novel mobile video editing model.  It showcases the model's ability to generate 12 frames per second on a mobile phone, significantly outperforming existing methods in terms of floating-point operations (FLOPs) per frame.  This highlights MoViE's computational efficiency and suitability for on-device video editing.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2412.06578/x3.png", "caption": "Figure 2: Multimodal Guidance Distillation Overview: Standard classifier-free guidance inference pipeline (left) with two input conditionings (image and text) requires three inference runs per diffusion step. Our distilled pipeline (right) incorporates guidance scales sIsubscript\ud835\udc60\ud835\udc3cs_{I}italic_s start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT and sTsubscript\ud835\udc60\ud835\udc47s_{T}italic_s start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT into UNet and only performs one inference run.", "description": "Figure 2 illustrates the computational efficiency improvement achieved through multimodal guidance distillation. The standard classifier-free guidance pipeline (left) uses two input conditionings (image and text), requiring three inference runs per diffusion step due to separate calculations for text-only, image-only, and combined image and text conditionings.  In contrast, the distilled pipeline (right) integrates guidance scales (sI and sT) directly into the UNet, thereby requiring only one inference run per diffusion step.", "section": "3. Multimodal Guidance Distillation"}, {"figure_path": "https://arxiv.org/html/2412.06578/x4.png", "caption": "Figure 3: Adversarial Distillation: We distill a multi-step teacher into a single step student using adversarial losses. Unlike existing adversarial distillation approaches [51, 67] that forego guidance flexibility for faster sampling, we preserve guidance strength property during adversarial training by providing the synthetic latent xtsubscript\ud835\udc65\ud835\udc61x_{t}italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT from teacher\u2019s denoising process and conditioning the student on the corresponding guidance scales.", "description": "This figure illustrates the adversarial distillation process used to train a single-step video editing model.  A multi-step teacher model is used to generate a sequence of increasingly less noisy video frames (denoising process). At each step, synthetic noisy frames are created and fed to the student model along with guidance signals (controlling image and text edits). The discriminator distinguishes between real (from the teacher) and synthetic (from the student) frames, training the student to generate realistic edits. Importantly, guidance scales (controlling the strength of image and text guidance) are preserved in this process, avoiding loss of control over editing strength that is seen in other adversarial distillation methods.", "section": "3.4 Adversarial Step Distillation"}, {"figure_path": "https://arxiv.org/html/2412.06578/x5.png", "caption": "Figure 4: MoViE at text guidance [4.0,8.0,12.0]4.08.012.0[4.0,8.0,12.0][ 4.0 , 8.0 , 12.0 ] and image guidance [1.25,1.75]1.251.75[1.25,1.75][ 1.25 , 1.75 ]. Our adversarial training maintains guidance scales, allowing us to control edit strength during inference. (Prompt: In Van Gogh Style)", "description": "This figure showcases the results of MoViE, a mobile video editing model, demonstrating its ability to control the intensity of edits during inference.  Three different text guidance scales (4.0, 8.0, and 12.0) are tested in conjunction with an image guidance scale of 1.75.  Higher numbers indicate stronger adherence to the edit instructions. The example uses the prompt \"In Van Gogh Style\" to transform a portrait image, illustrating how MoViE can apply stylistic edits with varying degrees of intensity based on adjustable parameters.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.06578/x6.png", "caption": "Figure 5: CLIP metrics for InstructPix2Pix, Mobile-Pix2pix, Multi-modal Guidance (MMG) Mobile-Pix2pix and MoViE. As shown in the graphs, proposed optimizations improve the efficiency greatly with minimum quality drop.", "description": "Figure 5 presents a comparison of CLIP image and text similarity metrics across four different models: InstructPix2Pix, Mobile-Pix2Pix (the base model with architectural optimizations), Mobile-Pix2Pix with Multi-modal Guidance (MMG), and MoViE (the final model incorporating all optimizations).  The x-axis represents the computational cost (end-to-end TFLOPs per frame), while the y-axis shows the CLIP similarity scores. The plot demonstrates that the proposed optimizations in Mobile-Pix2Pix, MMG, and MoViE significantly reduce computational cost without a substantial decrease in image quality, as indicated by the similarity scores remaining relatively high.  MoViE achieves the best balance of high image quality and very low computational cost.", "section": "4. Results"}, {"figure_path": "https://arxiv.org/html/2412.06578/x7.png", "caption": "Figure 6: Qualitative results of MoViE on DAVIS. Our method can handle complex global edits as well as perform more nuanced attribute editing while requiring very few computational resources. Please refer to the Appendix for video results.", "description": "Figure 6 showcases the qualitative results obtained using the MoViE model on the DAVIS dataset.  The images demonstrate the model's capability to perform both extensive global edits (e.g., transforming the entire scene) and subtle attribute edits (e.g., altering specific features of an object).  A key aspect highlighted is that MoViE achieves these editing results with minimal computational resources, unlike many other comparable video editing models.  For a comprehensive visualization of the edits, including video demonstrations, the reader is directed to the Appendix.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.06578/x8.png", "caption": "Figure 7: CLIP metrics for different autoencoder configurations. A substational FLOPs reduction can be achieved by incorporating TAESD, with minimal drop in editing performance.", "description": "This figure displays the impact of utilizing different autoencoder configurations on the performance of the video editing model. Specifically, it compares the standard Variational Autoencoder (VAE) against the Tiny Autoencoder for Stable Diffusion (TAESD). The results indicate that incorporating TAESD leads to a substantial reduction in FLOPs (floating point operations), representing a significant improvement in computational efficiency, with minimal negative impact on the overall editing performance as measured by CLIP (Contrastive Language\u2013Image Pre-training) metrics.", "section": "3.2 Mobile-Pix2Pix"}, {"figure_path": "https://arxiv.org/html/2412.06578/x9.png", "caption": "Figure 8: Qualitative comparison of our method to the base model. The efficiency is greatly improved whereas quality is not compromised both for style transfer and attribute edits. Please refer to the Appendix for video results.", "description": "This figure shows a qualitative comparison between the results of the proposed MoViE model and its base model.  The images demonstrate that MoViE achieves comparable or better results in terms of visual quality for both style transfer and attribute editing tasks while significantly improving efficiency.  For a detailed video comparison, please refer to the Appendix of the paper.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.06578/x10.png", "caption": "Figure 9: Qualitative comparison of our method MoViE to other SOTA video editing algorithms. We evaluate on two challenging editing scenarios. MoViE produces good quality edits yet far outperforms other competing methods in terms of efficiency. Please refer to the Appendix for video results.", "description": "This figure presents a qualitative comparison between MoViE and other state-of-the-art video editing algorithms.  Two challenging editing scenarios are used to evaluate the algorithms.  The results showcase that MoViE achieves high-quality edits while significantly outperforming other methods in terms of computational efficiency.  For detailed video results, the reader is referred to the Appendix.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.06578/x11.png", "caption": "Figure 10: Human Evaluation results comparing MoViE to TokenFlow[11], InsV2V[7], and Rerender-A-Video[63].", "description": "This figure presents the results of a human evaluation comparing the video editing quality of MoViE against three state-of-the-art methods: TokenFlow, InsV2V, and Rerender-A-Video.  The evaluation involved participants comparing video pairs, choosing which video was better or if both were equal.  The results visually show the percentage of times MoViE was rated as better, equal or worse than each comparison method. This provides a qualitative measure of MoViE's performance relative to other advanced video editing techniques.", "section": "4. Experiments"}]