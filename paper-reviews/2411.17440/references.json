{"references": [{"fullname_first_author": "Aditya Ramesh", "paper_title": "Zero-shot text-to-image generation", "publication_date": "2021-07-19", "reason": "This paper is foundational for text-to-image generation, providing the base model that many other text-to-video models are built upon."}, {"fullname_first_author": "Jonathan Ho", "paper_title": "Denoising diffusion probabilistic models", "publication_date": "2020-12-01", "reason": "This work introduces the foundation of diffusion models, which are crucial to the identity preserving text-to-video models discussed in the paper."}, {"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-19", "reason": "CLIP, introduced in this paper, is a critical component for bridging the gap between text and image, enabling the model to understand both text prompts and reference images."}, {"fullname_first_author": "Tim Brooks", "paper_title": "Video generation models as world simulators", "publication_date": "2024-00-00", "reason": "This paper introduces a model, which can simulate physical world, showing the potential of the Diffusion Transformer based model."}, {"fullname_first_author": "Andreas Blattmann", "paper_title": "Stable video diffusion: Scaling latent video diffusion models to large datasets", "publication_date": "2023-11-21", "reason": "This paper addresses challenges in scaling video diffusion models, offering crucial techniques for generating high-quality videos, which is also essential for IPT2V."}]}