[{"figure_path": "https://arxiv.org/html/2411.17440/x1.png", "caption": "Figure 2: Overview of the proposed ConsisID. Based on Findings of DiT, low-frequency facial information is embedded into the shallow layers, while high-frequency information is incorporated into the vision tokens within the attention blocks. The ID-preserving Recipe is applied to ease training and improve generalization. The cross face, DropToken and Dropout are executed based on probability.", "description": "ConsisID processes low-frequency facial features (like overall shape and proportions) through a global facial extractor, embedding them in the initial layers of the DiT model.  High-frequency details (like fine wrinkles and texture) are handled by a local facial extractor, which injects them directly into the attention blocks of the DiT. This frequency-aware approach maintains identity consistency while enhancing the model's ability to generate detailed and realistic videos.  Furthermore, the ID-preserving Recipe, along with probabilistic application of cross-face, DropToken, and Dropout techniques, facilitates easier training and improved generalization.", "section": "3.2 ConsisID: Keep Your Identity Consistent"}, {"figure_path": "https://arxiv.org/html/2411.17440/x2.png", "caption": "Figure 1: Examples of identity-preserving video generation (IPT2V) by our ConsisID. Given a reference image, our method can generate realistic and personalized human-centered videos while preserving identity. Red indicates that attributes in long instructions.", "description": "This figure showcases the capability of the ConsisID model in generating identity-preserving videos.  Starting with a single reference image of a person, the model generates short video clips based on text prompts.  The generated videos maintain the identity of the person in the reference image, regardless of the actions or environment described in the text prompt. This demonstrates the model's ability to create realistic, personalized video content while faithfully representing the subject's appearance.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2411.17440/x3.png", "caption": "Figure 3: (a - e) Fourier spectrum of different id signal injection. The center area represents low frequencies and the surrounding area represents high frequencies. (f) Relative log amplitudes of Fourier transformed generated videos. A larger response value indicates a higher inclusion of frequency information. (a - f) verify the effect of our frequency decomposition.", "description": "Figure 3 visualizes the impact of different identity signal injection methods on the frequency spectrum of generated videos. Subfigures (a) through (e) show the Fourier transforms of videos generated using various combinations of high and low-frequency identity signals. The central area of each spectrum corresponds to low frequencies, while the surrounding area represents high frequencies. The intensity of the spectral components indicates the strength of the signal's contribution at each frequency. This allows for a visual comparison of how different injection techniques affect the frequency content of the generated videos. Subfigure (f) displays the relative log amplitudes of the Fourier transforms of generated videos across the various methods. Higher values suggest a stronger presence of frequency components, directly corresponding to the amount of frequency information incorporated. The figure confirms the effectiveness of the frequency decomposition method by demonstrating its capacity to introduce both high and low-frequency details.", "section": "3.2 ConsisID: Keep Your Identity Consistent"}, {"figure_path": "https://arxiv.org/html/2411.17440/x4.png", "caption": "Figure 4: User Study between ConsisID and state-of-the-art methods. ConsisID is preferred by voters in all dimensions.", "description": "A user study comparing ConsisID to other state-of-the-art methods for identity-preserving text-to-video generation.  The chart displays the percentage of user preference across five dimensions: Identity Preservation, Text Relevance, Visual Quality, Motion Amplitude, and overall user preference. ConsisID demonstrates higher preference in all five aspects when compared to the other methods.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2411.17440/x5.png", "caption": "Figure 5: Qualitative analysis between ConsisID and ID-Animator [15]. ID-Animator can only generate videos of the face region, and the identity Preservation is poor (e.g., shape, texture). Additionally, it cannot generate specified content according to the text prompt (e.g., action, decoration, background). ConsisID achieves advantages in identity preservation, visual quality, motion amplitude, and text relevance. Moreover, our ConsistID can generate more frames rather than ID-Animator (49 480\u00d7\\times\u00d7720p frames v.s. 16 512\u00d7\\times\u00d7512p frames).", "description": "Figure 5 presents a qualitative comparison of video generation results between ConsisID and ID-Animator, highlighting ConsisID's superior performance.  ID-Animator is limited to generating only face-region videos with poor identity preservation (noticeable shape and texture inconsistencies). It also fails to incorporate specified content from text prompts (actions, decorations, backgrounds). In contrast, ConsisID demonstrates significantly improved identity preservation, visual quality, motion expressiveness, and relevance to the text prompt.  Furthermore, ConsisID generates a considerably larger number of frames (49 frames at 480x720p resolution) compared to ID-Animator (16 frames at 512x512p resolution).", "section": "4.2. Qualitative Analysis"}, {"figure_path": "https://arxiv.org/html/2411.17440/x6.png", "caption": "Figure 6: Effect of Different Components via Qualitative Analysis. Removing any component may result in the loss of high- or low-frequency facial information, or hinder the ability to modify video content based on the text prompt.", "description": "This figure demonstrates the importance of each component in the ConsisID model for identity-preserving text-to-video generation.  By removing one component at a time (Global Facial Extractor, Local Facial Extractor, Coarse-to-Fine Training, Dynamic Mask Loss, or Dynamic Cross-face Loss), the resulting videos show either a loss of high or low-frequency facial details, or an inability to generate videos that accurately reflect the text prompt.  This highlights how each component contributes to both the accurate representation of identity and the effective incorporation of textual instructions into the generated video.", "section": "3.2 ConsisID: Keep Your Identity Consistent"}, {"figure_path": "https://arxiv.org/html/2411.17440/x7.png", "caption": "Figure 7: Effect of Different Control Signal Injection Way via Qualitative Analysis. Only (c), which injects both high & low-freq face signals into the suitable location, performs best.", "description": "Figure 7 presents a qualitative comparison of different approaches for injecting control signals (high and low-frequency face signals) into the diffusion transformer model for identity-preserving text-to-video generation.  The results demonstrate that injecting both high and low-frequency signals at appropriate locations within the model (method c) yields superior performance compared to other methods.  Methods that used only high or only low-frequency signals, or injected the signals into inappropriate locations, resulted in artifacts, poor identity preservation, or an inability to generate high-quality videos.", "section": "3.2. ConsisID: Keep Your Identity Consistent"}, {"figure_path": "https://arxiv.org/html/2411.17440/x8.png", "caption": "Figure 8: Effect of the Inversion Steps t\ud835\udc61titalic_t. Overall quality does not improve consistently as t\ud835\udc61titalic_t increases, but first improves and then declines. This may be because the early steps are dominated by low frequency, whereas the later steps are dominated by high frequency.", "description": "Figure 8 illustrates the impact of varying the number of inversion steps (t) during video generation.  The experiment shows that image quality does not monotonically improve with increasing t. Initially, quality improves as low-frequency components are emphasized, resulting in well-defined structures. However, at higher values of t, high-frequency details become dominant, causing a decline in overall quality, potentially due to noise amplification.", "section": "4.7 Ablation on the Number of Inversion Steps"}]