{"importance": "This paper is crucial for researchers working on reinforcement learning for large language models (LLMs).  It addresses the significant challenge of efficiently training LLMs with dense rewards, a problem hindering progress in complex reasoning tasks. **PRIME's novel approach, using implicit rewards and online updates, offers a scalable and cost-effective solution**, opening new avenues for improving LLM reasoning capabilities.", "summary": "PRIME (Process Reinforcement through IMplicit rEwards) revolutionizes LLM training by efficiently using implicit process rewards from online policy rollouts and outcome labels, significantly boosting reasoning capabilities and outperforming existing methods.", "takeaways": ["PRIME uses implicit process rewards, eliminating the need for expensive process-level labels during training.", "PRIME demonstrates significant improvement in various LLM reasoning benchmarks, outperforming existing models, achieving 15.1% improvement on average and 20%+ on key benchmarks.", "The online updating mechanism of PRIME prevents reward hacking and enables efficient use of limited training data, achieving 2.5x sample efficiency gain compared to using outcome rewards only"], "tldr": "Current methods for training large language models (LLMs) using reinforcement learning (RL) often rely on sparse outcome-level rewards, leading to issues like training inefficiency and difficulty in assigning credit for intermediate steps. Dense rewards, providing feedback at each step, can address these issues. However, obtaining high-quality dense rewards is costly and prone to reward hacking. This paper introduces PRIME, a novel method that addresses these challenges. \nPRIME leverages implicit process rewards derived from policy rollouts and outcome labels. This innovative technique avoids the expensive process of labeling each step and mitigates reward hacking. Experiments show PRIME significantly enhances performance on benchmark reasoning tasks, outperforming existing models and requiring substantially less training data.  **The results show that PRIME offers a scalable and efficient alternative** for training LLMs with dense rewards.", "affiliation": "Tsinghua University", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}, "podcast_path": "2502.01456/podcast.wav"}