{"references": [{"fullname_first_author": "Josh Achiam", "paper_title": "GPT-4 Technical Report", "publication_date": "2023-03-01", "reason": "This paper is considered very important because it is the technical report for the highly influential GPT-4 model, which sets a benchmark for large language models."}, {"fullname_first_author": "Zhe Chen", "paper_title": "InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks", "publication_date": "2024-12-05", "reason": "This paper is one of the very important references as it provides the foundation for scaling up vision foundation models and aligning them for generic visual-linguistic tasks."}, {"fullname_first_author": "Tsung-Yi Lin", "paper_title": "Microsoft COCO: Common Objects in Context", "publication_date": "2014-09-05", "reason": "This paper introduces the Microsoft COCO dataset, which is a widely-used benchmark for object detection, segmentation, and captioning, making it a crucial resource for evaluating vision-language models."}, {"fullname_first_author": "Rafael Rafailov", "paper_title": "Direct Preference Optimization: Your Language Model Is Secretly a Reward Model", "publication_date": "2023-05-24", "reason": "This paper introduces Direct Preference Optimization (DPO), an algorithm that improves upon Reinforcement Learning from Human Feedback (RLHF) by directly optimizing language models based on preference data, leading to more efficient and stable training."}, {"fullname_first_author": "Yufei Zhan", "paper_title": "Griffon-G: Bridging Vision-Language and Vision-Centric Tasks via Large Multimodal Models", "publication_date": "2024-10-25", "reason": "This paper is vital to Vision-R1, as it is the project which Vision-R1 extends upon to improve object localization capabilities in large language models."}]}