[{"figure_path": "https://arxiv.org/html/2403.12959/x1.png", "caption": "Figure 1: WHAC synergizes human-camera (camera-frame SMPL-X estimation), camera-world (visual odometry), and human-world (our proposed MotionVelocimeter) modeling for constructing world-grounded human and camera trajectories.", "description": "This figure illustrates the WHAC framework, which integrates three key components to estimate world-grounded human and camera trajectories.  The first component is camera-frame SMPL-X estimation, which provides initial estimates of human pose and shape in the camera's coordinate system.  This is combined with visual odometry (VO), which estimates the camera's trajectory in the world coordinate system, providing information about camera movement. Finally, the human-world component, the MotionVelocimeter, analyzes human movements to infer velocity and thus scale information, refining both camera and human trajectory estimates. The synergy of these three components allows WHAC to accurately estimate both camera and human trajectories with correct scale in the world.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2403.12959/x2.png", "caption": "Figure 2: Overview of WHAC. SMPL-X estimator extracts camera-frame SMPL-X with dummy depth, which is recovered in Sec.\u00a03.2. The scaleless camera trajectory estimated by VO is then used to canonicalize the human trajectory to estimate its velocity and thus scale in Sec.\u00a03.3. A camera trajectory is then derived for alignment and scale recovery, which subsequently updates the human trajectory in Sec.\u00a03.4.", "description": "The figure illustrates the WHAC framework's workflow.  It starts with an SMPL-X estimator that outputs camera-frame SMPL-X data with initially unknown depth.  The depth is then recovered (Section 3.2). Simultaneously, visual odometry (VO) provides a scaleless camera trajectory. This trajectory is used to canonicalize (standardize) the human motion data, allowing for the estimation of human velocity and subsequently, scale (Section 3.3). A more refined camera trajectory is then derived, incorporating the scale information (Section 3.4). This refined camera trajectory is then used to further improve the accuracy of the human trajectory.", "section": "3 Methodology"}, {"figure_path": "https://arxiv.org/html/2403.12959/x3.png", "caption": "Figure 3: a) Human trajectories H\ud835\udc3bHitalic_H derived from camera trajectories C\ud835\udc36Citalic_C of different scales can be vastly different in both shape and direction, despite that the same camera-frame human root depth dtsubscript\ud835\udc51\ud835\udc61d_{t}italic_d start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT and translations thcsubscriptsuperscript\ud835\udc61\ud835\udc50\u210et^{c}_{h}italic_t start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT are used. b) Different pairs of focal length f\ud835\udc53fitalic_f and tzsubscript\ud835\udc61\ud835\udc67t_{z}italic_t start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT can correspond to the same image.", "description": "Figure 3 demonstrates the ambiguity in estimating 3D human trajectories from monocular video.  Panel (a) shows that different camera trajectories (at different scales) will produce vastly different human trajectories, even if the camera-frame human root depth and translation are kept consistent. This highlights the challenge of scale estimation. Panel (b) illustrates that the same image could result from different combinations of focal length and human root depth, further emphasizing the ill-posed nature of the problem.", "section": "3 Methodology"}, {"figure_path": "https://arxiv.org/html/2403.12959/x4.png", "caption": "Figure 4: Visualization of WHAC-A-Mole sample sequences, animated with a) AMASS, b-c) DLP-MoCap, and d-e) DD100. In each sample, the first row depicts the overview (note the camera trajectory shown in bright rays), and the second and the third rows show the camera view and overlaid SMPL-X annotations.", "description": "Figure 4 visualizes examples from the WHAC-A-Mole dataset.  Each example shows three rows: the top row displays an overview of the scene with the camera trajectory highlighted, the second row shows a camera view from that trajectory, and the bottom row presents the same camera view with SMPL-X body annotations overlaid. The dataset's motion sequences are from three sources: AMASS (a), DLP-MoCap (b-c), and DD100 (d-e).", "section": "4 WHAC-A-Mole Dataset"}, {"figure_path": "https://arxiv.org/html/2403.12959/x5.png", "caption": "Figure 5: Visualization on in-the-wild hard cases. WHAC leverages human-camera-scene collaboration to resolve cases where motion prior alone would fail: a) Skateboarding and b) Treadmill. c) WHAC can also handle fast cases.", "description": "Figure 5 presents challenging scenarios where motion estimation alone might fail.  It showcases WHAC's ability to leverage information from human motion, camera movement, and scene context for improved accuracy.  Specifically: (a) shows a skateboarding example, where the human may appear stationary in the camera frame, but is actually moving in the world; (b) shows a treadmill example where the human is moving, but the root translation in the world frame is minimal; and (c) demonstrates WHAC handling a fast-moving scene from a real-world video, again highlighting its ability to combine multiple sources of information.", "section": "5.7 Visualization"}, {"figure_path": "https://arxiv.org/html/2403.12959/x6.png", "caption": "Figure 6: Visualization of world space results on the EMDB dataset. a1) and b1) depict camera trajectories, while a2) and b2) illustrate human trajectories. Notably, in sequence b, the human is descending stairs, and WHAC effectively captures the global trajectory, indicating a downward direction besides recovering the absolute trajectory scale in the world space. The grid size in the plots is 2m.", "description": "This figure visualizes the results of the proposed WHAC method on the EMDB dataset, focusing on world-space trajectory estimation.  The top row (a1 and b1) shows camera trajectories, while the bottom row (a2 and b2) shows corresponding human trajectories.  The plots clearly demonstrate WHAC's ability to accurately recover both camera and human movements, including scale, in the 3D world coordinate system. The example in sequence 'b' highlights this capability, accurately capturing the downward motion of a human descending stairs.  The grid lines in the plots represent a 2-meter spacing.", "section": "5.7 Visualization"}, {"figure_path": "https://arxiv.org/html/2403.12959/x7.png", "caption": "Figure 7: Visualization of camera space results on WHAC-A-Mole dataset. Each sample comprises two rows: the first row displays the original input frames from the sequence, while the second row overlays the SMPL-X results. This visualization showcases WHAC\u2019s performance on challenging scenes, including sequences with severe occlusions, intricate human interactions, and dynamic dancing poses.", "description": "Figure 7 visualizes the results of WHAC (World-grounded Humans and Cameras) on the WHAC-A-Mole dataset, demonstrating its ability to estimate human poses and shapes accurately in challenging scenarios. Each sample in the figure shows two rows: the top row displays the original video frames, while the bottom row overlays the estimated SMPL-X model on top of the video frames. The figure showcases several challenging cases: scenes with severe occlusions, intricate human interactions, and dynamic poses such as dancing, illustrating the robustness and accuracy of the WHAC method.", "section": "5.7 Visualization"}, {"figure_path": "https://arxiv.org/html/2403.12959/x8.png", "caption": "Figure 8: Illustration of MotionVelocimeter module. The inputs are canonicalized 3D joints regressed from SMPL-X meshes, and the outputs are root velocities in the canonical space.", "description": "The MotionVelocimeter module takes as input 3D joints from SMPL-X meshes that have been canonicalized (aligned to a standard pose).  It processes these joints to output root velocities, also in the canonical space. This means the module focuses on the speed and direction of the human's root movement, relative to the starting point of the sequence.  The use of canonicalized data simplifies the task and makes the velocity estimation more robust.", "section": "C Motion Velocimeter"}]