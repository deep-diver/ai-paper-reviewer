[{"content": "| Method | Dataset | FID \u2193 | FVD \u2193 | Rerr \u2193 | Terr \u2193 | LPIPS \u2193 | PSNR \u2191 | SSIM \u2191 |\n|---|---|---|---|---|---|---|---|---| \n| MotionCtrl [47] | RealEstate10K | 22.58 | 229.34 | 0.231 | 0.794 | 0.296 | 14.68 | 0.402 |\n| VD3D [1] | RealEstate10K | 21.40 | 187.55 | 0.053 | 0.126 | 0.227 | 17.26 | 0.514 |\n| ViewCrafter [55] | RealEstate10K | 20.89 | 203.71 | 0.054 | 0.152 | 0.212 | 18.91 | 0.501 |\n| **Ours** | RealEstate10K | **16.16** | **153.48** | **0.046** | **0.093** | **0.206** | **19.71** | **0.557** |\n| MotionCtrl [47] | DL3DV | 25.58 | 248.77 | 0.467 | 1.114 | 0.309 | 14.35 | 0.385 |\n| VD3D [1] | DL3DV | 22.70 | 232.97 | 0.094 | 0.237 | 0.259 | 16.28 | 0.487 |\n| ViewCrafter [55] | DL3DV | 20.55 | 210.62 | 0.092 | 0.243 | 0.237 | 17.10 | 0.519 |\n| **Ours** | DL3DV | **17.74** | **169.34** | **0.061** | **0.130** | **0.218** | **17.56** | **0.543** |\n| MotionCtrl [47] | Tanks and Temples | 30.17 | 289.62 | 0.834 | 1.501 | 0.312 | 14.58 | 0.386 |\n| VD3D [1] | Tanks and Temples | 24.33 | 244.18 | 0.117 | 0.292 | 0.284 | 15.35 | 0.467 |\n| ViewCrafter [55] | Tanks and Temples | 22.41 | 230.56 | 0.125 | 0.306 | 0.245 | 16.20 | 0.506 |\n| **Ours** | Tanks and Temples | **19.46** | **189.32** | **0.094** | **0.172** | **0.221** | **16.87** | **0.529** |\n| Lora-branch | Ablations on *RE10K* | 19.02 | 212.74 | 0.102 | 0.157 | - | - | - |\n| Ctrl-branch | Ablations on *RE10K* | 18.75 | 205.45 | 0.058 | 0.104 | - | - | - |\n| **Dual-branch** | Ablations on *RE10K* | **17.22** | **183.54** | **0.052** | **0.095** | - | - | - |", "caption": "Table 1: Quantitative comparison to the prior arts in camera-guided video generation on RealEstate10K, DL3DV, and Tanks and Temples dataset. We report the performance for visual quality (FID and FVD), camera-guidance precision (Rerrsubscript\ud835\udc45errR_{\\text{err}}italic_R start_POSTSUBSCRIPT err end_POSTSUBSCRIPT and Terrsubscript\ud835\udc47errT_{\\text{err}}italic_T start_POSTSUBSCRIPT err end_POSTSUBSCRIPT), and visual similarity (LPIPS, PSNR, and SSIM).", "description": "This table presents a quantitative comparison of various camera-guided video generation models, including MotionCtrl, VD3D, ViewCrafter, and the proposed method. The evaluation is conducted on three benchmark datasets: RealEstate10K, DL3DV, and Tanks and Temples.  The metrics used for comparison encompass visual quality (FID and FVD), camera guidance precision (Rotation Error and Translation Error), and visual similarity (LPIPS, PSNR, and SSIM). A lower value is better for FID, FVD, LPIPS, rotation error, and translation error. A higher value is better for PSNR and SSIM.", "section": "4.1. Comparison of Camera-Guided Video Generation"}, {"content": "| Method | RealEstate10K | | | DL3DV | | | Tanks-and-Temples | | |\n|---|---|---|---|---|---|---|---|---|---|---|---| \n| *Metrics* | LPIPS \u2193 | PSNR \u2191 | SSIM \u2191 | LPIPS \u2193 | PSNR \u2191 | SSIM \u2191 | LPIPS \u2193 | PSNR \u2191 | SSIM \u2191 |\n|---|---|---|---|---|---|---|---|---|---|---| \n| ZeroNVS [38] | 0.448 | 13.01 | 0.378 | 0.465 | 13.35 | 0.339 | 0.470 | 12.94 | 0.325 |\n| ViewCrafter [55] | 0.341 | 16.84 | 0.514 | 0.352 | 15.53 | 0.525 | 0.384 | 14.93 | 0.483 |\n| **Ours** | **0.292** | **17.15** | **0.550** | **0.325** | **16.64** | **0.574** | **0.344** | **15.90** | **0.510** |\n| Ablation-LaLRM | | | | | | | | | |\n| RGB-14 | 0.137 | 21.39 | 0.751 | 0.205 | 18.76 | 0.696 | 0.221 | 19.70 | 0.605 |\n| RGB-49 | 0.126 | 25.06 | 0.830 | 0.196 | 20.94 | 0.733 | 0.192 | 20.54 | 0.687 |\n| **Latent-based** | **0.122** | **27.10** | **0.864** | **0.159** | **23.25** | **0.786** | **0.170** | **22.66** | **0.743** |", "caption": "Table 2: Quantitative comparison on various benchmark datasets for 3D scene novel view synthesis with single view condition.", "description": "This table presents a quantitative comparison of different methods for 3D scene novel view synthesis from a single image on various benchmark datasets, including RealEstate10K, DL3DV, and Tanks-and-Temples.  The metrics used for evaluation include LPIPS, PSNR, and SSIM, which measure the quality and similarity of the rendered views compared to ground truth views. The table shows the performance of ZeroNVS, ViewCrafter, the proposed method, and an ablation study of the proposed method.", "section": "4.2. Comparison of 3D Scene Generation"}, {"content": "| Expression | Specification | Explanation | \n|---|---|---| \n| **Expression** | **Specification** | **Explanation** |\n|---|---|---| \n| *commonly used* |  |  |\n| $x$ | $x \\in R^{T\\times H\\times W\\times 3}$ | source video clip |\n| $s$ | - | stride to sample clip $x$ from source video |\n| $z$ | $z \\in R^{t\\times h\\times w\\times c}$ | video latent embedded from $x$ |\n| $\\mathcal{E}$ | - | encoder from 3D-VAE |\n| $r_s$ | $r_s = \\frac{H}{h} = \\frac{W}{w}$ | spatial compression rate |\n| $r_t$ | $r_t = \\frac{T}{t}$ | temporal compression rate |\n| $p$ | $p \\in R^{T\\times H\\times W\\times 6}$ | Pl\u00fccker embedding of cameras of video clip $x$ |\n| *diffusion used* |  |  |\n| $\\tau$ | - | diffusion time step |\n| $\\alpha_{\\tau}, \\sigma_{\\tau}$ | - | diffusion noise scheduler parameters |\n| $z_{\\tau}$ | - | noisy video latent |\n| $D_{\\theta}$ | - | diffusion model parameterized by $\\theta$ |\n| $o_v$ | $o_v \\in R^{N_v\\times d_v}$ | visual tokens as a sequence in diffusion model |\n| $o_{\\mathrm{ctrl}}$, $o_{\\mathrm{lora}}$ | $o_{\\mathrm{ctrl}}$, $o_{\\mathrm{lora}} \\in R^{N_v\\times d_v}$ | camera tokens as a sequence in diffusion model |\n| $N$ | - | number of transformer blocks in ControlNet branch |\n| *reconstruction used* |  |  |\n| $p_l$ | - | spatial patch size applied to $z$ in LaLRM |\n| $o_l$ | $o_l \\in R^{N_l\\times d_l}$ | visual latent tokens as a sequence in LaLRM |\n| $N_l$ | $N_l = t\\cdot\\frac{h}{p_l}\\cdot\\frac{w}{p_l}$ | number of visual latent tokens in LaLRM |\n| $o_{\\mathrm{p}}$ | $o_{\\mathrm{p}} \\in R^{N_l\\times d_l}$ | camera tokens as a sequence in LaLRM |\n| $V$ | - | number of supervision views in LaLRM |\n| $G$ | $G \\in R^{(T\\cdot H\\cdot W)\\times 12}$ | Gaussian feature map in LaLRM |", "caption": "Table A1: Overview of the notations used in the paper.", "description": "This table provides a comprehensive list of notations used throughout the paper, along with their corresponding specifications and explanations.  It serves as a quick reference for readers to understand the meaning of symbols and variables used in equations and figures.", "section": "A. More Analysis on Controllable Video Generation"}, {"content": "| Architecture | FID \u2193 | FVD \u2193 | R<sub>err</sub> \u2193 | T<sub>err</sub> \u2193 |\n|---|---|---|---|---| \n| Lora-branch | 19.02 | 212.74 | 0.102 | 0.157 |\n| Ctrl-branch | 18.75 | 205.45 | 0.058 | 0.104 |\n| Dual-branch | **17.22** | **183.54** | **0.052** | **0.095** |\n| Dual w/o LoraModule | 17.84 | 195.07 | 0.062 | 0.101 |\n| *Ctrl-branch* only | | | | |\n| w/o weight copy | 18.92 | 206.75 | 0.065 | 0.108 |\n| block-1 | 19.90 | 214.66 | 0.114 | 0.162 |\n| blocks-10 | 19.15 | 210.74 | 0.075 | 0.126 |\n| blocks-30 | 20.15 | 221.61 | 0.056 | 0.105 |", "caption": "Table A2: Analysis on architecture designs in camera-guided video generation model. We report the performance for visual quality (FID and FVD) and pose control precision (Rerrsubscript\ud835\udc45errR_{\\text{err}}italic_R start_POSTSUBSCRIPT err end_POSTSUBSCRIPT and Terrsubscript\ud835\udc47errT_{\\text{err}}italic_T start_POSTSUBSCRIPT err end_POSTSUBSCRIPT) from models trained on RealEstate10K dataset. The first section of the table is adopted from Tab.1 in the main paper.", "description": "This table evaluates different architecture designs for a camera-guided video generation model, focusing on visual quality (FID and FVD) and camera control precision (rotation and translation errors).  The models are trained on the RealEstate10K dataset. The top section of this table replicates data from Table 1 in the main paper, providing a baseline comparison to other state-of-the-art methods.", "section": "A. More Analysis on Controllable Video Generation"}, {"content": "| Method | RealEstate10K | | | DL3DV | | | Tanks-and-Temples | | |\n|---|---|---|---|---|---|---|---|---|---|---|\n| *Metrics* | LPIPS \u2193 | PSNR \u2191 | SSIM \u2191 | LPIPS \u2193 | PSNR \u2191 | SSIM \u2191 | LPIPS \u2193 | PSNR \u2191 | SSIM \u2191 |\n| LaLRM\u2013 | 0.295 | 17.06 | 0.538 | 0.343 | 16.62 | 0.570 | 0.359 | 15.85 | 0.502 |\n| **LaLRM** | **0.292** | **17.15** | **0.550** | **0.325** | **16.64** | **0.574** | **0.344** | **15.90** | **0.510** |", "caption": "Table A3: Analysis on involving in-the-wild dataset to fine-tune LaLRM. We report the performance on various benchmark datasets for novel view synthesis of 3D scenes, which are built from single view condition.", "description": "This table evaluates the effect of fine-tuning the Latent Large Reconstruction Model (LaLRM) with an in-the-wild dataset on its ability to perform 3D scene novel view synthesis from single-view images. The evaluation uses three benchmark datasets: RealEstate10K, DL3DV, and Tanks-and-Temples. The metrics used are LPIPS, PSNR, and SSIM, which measure the quality and similarity of rendered images compared to ground truth views.  Two versions of the model are evaluated: LaLRM- (trained without the in-the-wild dataset), and LaLRM (trained with the in-the-wild dataset), demonstrating the effectiveness of including in-the-wild data during fine-tuning.", "section": "B. More Analysis on 3D Reconstruction"}]