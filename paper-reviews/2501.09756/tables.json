[{"content": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"id24.16\">\n<tr class=\"ltx_tr\" id=\"id16.8.8\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"id9.1.1.1\" style=\"padding-bottom:5.0pt;\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_img_square\" height=\"114\" id=\"id9.1.1.1.g1\" src=\"extracted/6136146/figures/extra_results/teaser_row_11/col_00.jpg\" width=\"114\"/></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"id10.2.2.2\" style=\"padding-bottom:5.0pt;\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_img_portrait\" height=\"114\" id=\"id10.2.2.2.g1\" src=\"extracted/6136146/figures/extra_results/teaser_row_11/col_03.jpg\" width=\"76\"/></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"id11.3.3.3\" style=\"padding-bottom:5.0pt;\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_img_portrait\" height=\"114\" id=\"id11.3.3.3.g1\" src=\"extracted/6136146/figures/extra_results/teaser_row_11/col_06.jpg\" width=\"76\"/></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"id12.4.4.4\" style=\"padding-bottom:5.0pt;\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_img_portrait\" height=\"114\" id=\"id12.4.4.4.g1\" src=\"extracted/6136146/figures/extra_results/teaser_row_11/col_09.jpg\" width=\"76\"/></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"id13.5.5.5\" style=\"padding-bottom:5.0pt;\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_img_square\" height=\"114\" id=\"id13.5.5.5.g1\" src=\"extracted/6136146/figures/extra_results/teaser_row_10/col_00.jpg\" width=\"114\"/></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"id14.6.6.6\" style=\"padding-bottom:5.0pt;\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_img_portrait\" height=\"114\" id=\"id14.6.6.6.g1\" src=\"extracted/6136146/figures/extra_results/teaser_row_10/col_02.jpg\" width=\"76\"/></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"id15.7.7.7\" style=\"padding-bottom:5.0pt;\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_img_portrait\" height=\"114\" id=\"id15.7.7.7.g1\" src=\"extracted/6136146/figures/extra_results/teaser_row_10/col_05.jpg\" width=\"76\"/></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"id16.8.8.8\" style=\"padding-bottom:5.0pt;\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_img_portrait\" height=\"114\" id=\"id16.8.8.8.g1\" src=\"extracted/6136146/figures/extra_results/teaser_row_10/col_08.jpg\" width=\"76\"/></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"id24.16.16\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"id17.9.9.1\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_img_square\" height=\"114\" id=\"id17.9.9.1.g1\" src=\"extracted/6136146/figures/extra_figures/extra_teaser_row_06/col_00.jpg\" width=\"114\"/></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"id18.10.10.2\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_img_portrait\" height=\"114\" id=\"id18.10.10.2.g1\" src=\"extracted/6136146/figures/extra_figures/extra_teaser_row_06/col_01.jpg\" width=\"76\"/></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"id19.11.11.3\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_img_portrait\" height=\"114\" id=\"id19.11.11.3.g1\" src=\"extracted/6136146/figures/extra_figures/extra_teaser_row_06/col_03.jpg\" width=\"76\"/></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"id20.12.12.4\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_img_portrait\" height=\"114\" id=\"id20.12.12.4.g1\" src=\"extracted/6136146/figures/extra_figures/extra_teaser_row_06/col_09.jpg\" width=\"76\"/></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"id21.13.13.5\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_img_square\" height=\"114\" id=\"id21.13.13.5.g1\" src=\"extracted/6136146/figures/teaser_figure/teaser_row_22/col_00.jpg\" width=\"114\"/></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"id22.14.14.6\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_img_portrait\" height=\"114\" id=\"id22.14.14.6.g1\" src=\"extracted/6136146/figures/toy_figures_3/toy_figure_row_00/col_08.jpg\" width=\"76\"/></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"id23.15.15.7\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_img_portrait\" height=\"114\" id=\"id23.15.15.7.g1\" src=\"extracted/6136146/figures/toy_figures_3/toy_figure_row_00/col_02.jpg\" width=\"76\"/></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"id24.16.16.8\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_img_portrait\" height=\"114\" id=\"id24.16.16.8.g1\" src=\"extracted/6136146/figures/toy_figures_3/toy_figure_row_00/col_09.jpg\" width=\"76\"/></td>\n</tr>\n</table>", "caption": "Table 1: Comparisons: We compare against baselines on a held-out set of our synthetic dataset and data rendered through a Light Stage. While trained only on synthetic data, our model performs comparably to SwitchLight, a commercial relighting method trained with Light Stage data.", "description": "This table presents a quantitative comparison of the proposed SynthLight model against several state-of-the-art baselines for portrait relighting.  The comparison is performed using two distinct test sets: a held-out subset of the synthetic dataset used for training and a separate dataset of images rendered using a Light Stage.  Performance is measured using standard image quality metrics (LPIPS, SSIM, PSNR) and a face-recognition metric (FaceNet), assessing both the image fidelity and preservation of identity.  Remarkably, despite SynthLight being trained exclusively on synthetic data, its performance on both datasets is comparable to or better than that of SwitchLight, a commercially available relighting model trained on Light Stage data. This showcases the effectiveness of SynthLight in learning robust relighting capabilities from synthetic data.", "section": "4. Experiments"}, {"content": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S3.F2.4\">\n<tr class=\"ltx_tr\" id=\"S3.F2.4.4\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.F2.1.1.1\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_img_square\" height=\"76\" id=\"S3.F2.1.1.1.g1\" src=\"extracted/6136146/figures/data_figure/male/r8_HD_Male_031.obj_sample_0003_world_0004_rotate_0000_subject_image_01.jpg\" width=\"76\"/></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\" id=\"S3.F2.2.2.2\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_img_square\" height=\"76\" id=\"S3.F2.2.2.2.g1\" src=\"extracted/6136146/figures/data_figure/male/r8_HD_Male_031.obj_sample_0003_world_0008_rotate_0001_subject_image_01.jpg\" width=\"76\"/></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.F2.3.3.3\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_img_square\" height=\"76\" id=\"S3.F2.3.3.3.g1\" src=\"extracted/6136146/figures/data_figure/female/r8_SD_Female_021.obj_sample_0003_world_0007_rotate_0004_subject_image_01.jpg\" width=\"76\"/></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S3.F2.4.4.4\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_img_square\" height=\"76\" id=\"S3.F2.4.4.4.g1\" src=\"extracted/6136146/figures/data_figure/female/r8_SD_Female_021.obj_sample_0003_world_0009_rotate_0006_subject_image_01.jpg\" width=\"76\"/></td>\n</tr>\n</table>", "caption": "Table 2: User Study: Preference rates indicate how often our method was preferred over baselines. For example, a rate of 0.92 under Lighting means our method was preferred 92% of the time over IC-Light. Based on 482 responses from 20 participants, our method consistently outperforms baselines in lighting, image quality, and subject identity, since all preference rates exceed 0.5. This highlights superior image quality over relighting methods [22, 19] and better lighting over harmonization methods [59].", "description": "This table presents the results of a user study comparing SynthLight's performance against other state-of-the-art methods for portrait relighting.  Twenty participants provided 482 responses in total.  Each comparison used a pairwise forced-choice format, asking participants to indicate which method (SynthLight or a baseline) they preferred based on specific criteria: lighting, overall image quality, and preservation of subject identity.  The results show that SynthLight consistently outperforms baselines across all three criteria, achieving preference rates above 50% in each case.  This indicates that SynthLight produces superior image quality compared to relighting methods ([22], [19]), and better lighting compared to harmonization methods ([59]).", "section": "4. Experiments"}, {"content": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S3.F6.sf1.7\">\n<tr class=\"ltx_tr\" id=\"S3.F6.sf1.7.7\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.F6.sf1.1.1.1\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_img_square\" height=\"122\" id=\"S3.F6.sf1.1.1.1.g1\" src=\"extracted/6136146/figures/big_figure/row_1/input_portrait.jpg\" width=\"122\"/></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S3.F6.sf1.2.2.2\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_img_portrait\" height=\"122\" id=\"S3.F6.sf1.2.2.2.g1\" src=\"extracted/6136146/figures/big_figure/row_1/output_1.jpg\" width=\"81\"/></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S3.F6.sf1.3.3.3\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_img_portrait\" height=\"122\" id=\"S3.F6.sf1.3.3.3.g1\" src=\"extracted/6136146/figures/big_figure/row_1/inset_output_2.jpg\" width=\"81\"/></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\" id=\"S3.F6.sf1.4.4.4\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_img_portrait\" height=\"122\" id=\"S3.F6.sf1.4.4.4.g1\" src=\"extracted/6136146/figures/big_figure/row_1/output_3.jpg\" width=\"81\"/></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\" id=\"S3.F6.sf1.5.5.5\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_img_square\" height=\"122\" id=\"S3.F6.sf1.5.5.5.g1\" src=\"extracted/6136146/figures/big_figure/row_2/input_portrait.jpg\" width=\"122\"/></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S3.F6.sf1.6.6.6\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_img_portrait\" height=\"122\" id=\"S3.F6.sf1.6.6.6.g1\" src=\"extracted/6136146/figures/big_figure/row_2/output_1.jpg\" width=\"81\"/></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\" id=\"S3.F6.sf1.7.7.7\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_img_portrait\" height=\"122\" id=\"S3.F6.sf1.7.7.7.g1\" src=\"extracted/6136146/figures/big_figure/row_2/output_4.jpg\" width=\"81\"/></td>\n</tr>\n</table>", "caption": "Table 3: Ablations highlight the contributions of each component i.e. Multi-Task training and Inference-time Adaptation (Sec.\u00a03.3 and Sec.\u00a03.4 respectively). Adding Light Stage data during training improves performance on Light Stage Test set, and qualitatively improves details but brings lighting biases (See Fig.\u00a09).", "description": "This table presents an ablation study evaluating the impact of different components on the SynthLight model's performance.  The study compares several model variations: a base model without any additional training strategies or inference-time adaptations; a model incorporating multi-task training (combining relighting and text-to-image tasks); a model with inference-time adaptation (using classifier-free guidance); the full SynthLight model incorporating both multi-task training and inference-time adaptation; and finally, a model trained with both synthetic and Light Stage data.  The results are assessed using both quantitative metrics (LPIPS, SSIM, PSNR, and FaceNet) on synthetic and Light Stage test datasets and qualitative observations regarding the level of detail and lighting accuracy.  The results show that the addition of Light Stage data improves quantitative results on the Light Stage dataset but can introduce lighting biases (as shown in Figure 9 in the paper).", "section": "4. Experiments"}, {"content": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S3.F6.sf2.7\">\n<tr class=\"ltx_tr\" id=\"S3.F6.sf2.7.7\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.F6.sf2.1.1.1\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_img_square\" height=\"122\" id=\"S3.F6.sf2.1.1.1.g1\" src=\"extracted/6136146/figures/big_figure/row_3/input_portrait_right.jpg\" width=\"122\"/></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\" id=\"S3.F6.sf2.2.2.2\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_img_portrait\" height=\"122\" id=\"S3.F6.sf2.2.2.2.g1\" src=\"extracted/6136146/figures/big_figure/row_3/output_right_1.jpg\" width=\"81\"/></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\" id=\"S3.F6.sf2.3.3.3\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_img_portrait\" height=\"122\" id=\"S3.F6.sf2.3.3.3.g1\" src=\"extracted/6136146/figures/big_figure/row_3/output_right_2.jpg\" width=\"81\"/></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S3.F6.sf2.4.4.4\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_img_square\" height=\"122\" id=\"S3.F6.sf2.4.4.4.g1\" src=\"extracted/6136146/figures/big_figure/row_6/col_00.jpg\" width=\"122\"/></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\" id=\"S3.F6.sf2.5.5.5\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_img_portrait\" height=\"122\" id=\"S3.F6.sf2.5.5.5.g1\" src=\"extracted/6136146/figures/big_figure/row_6/inset_col_11.jpg\" width=\"81\"/></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\" id=\"S3.F6.sf2.6.6.6\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_img_portrait\" height=\"122\" id=\"S3.F6.sf2.6.6.6.g1\" src=\"extracted/6136146/figures/big_figure/row_6/col_04.jpg\" width=\"81\"/></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\" id=\"S3.F6.sf2.7.7.7\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_img_portrait\" height=\"122\" id=\"S3.F6.sf2.7.7.7.g1\" src=\"extracted/6136146/figures/big_figure/row_6/col_09.jpg\" width=\"81\"/></td>\n</tr>\n</table>", "caption": "Table 4: Ablating initial checkpoint: We evaluate our method, initialized with IC-Light, against initialization with SD 1.5. All tables in both main paper and supplementary, including non-inference specific ablations, are generated with classifier-free guidance parameters, \u03bbT=2subscript\ud835\udf06\ud835\udc472\\lambda_{T}=2italic_\u03bb start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT = 2, \u03bbI=3subscript\ud835\udf06\ud835\udc3c3\\lambda_{I}=3italic_\u03bb start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT = 3. See main paper for detailed descriptions of them.", "description": "This table compares the performance of the SynthLight model when initialized with two different pre-trained models: IC-Light and Stable Diffusion 1.5.  The comparison focuses on the results obtained from both synthetic and Light Stage test datasets.  The metrics used include LPIPS (lower is better), SSIM (higher is better), PSNR (higher is better), and FaceNet (lower is better).  The experiment uses classifier-free guidance with parameters \u03bbT=2 and \u03bbI=3 during both training and inference.  The results show that while initializing with IC-Light provides slightly better performance, both initializations lead to good overall results, demonstrating the model's robustness.", "section": "4. Experiments"}]