{"importance": "This paper is important for researchers because it provides **a comprehensive analysis of Whisper models, quantizatization methods, and experimental evaluations**. By understanding the practical applications, researchers can better optimize model size, latency, and accuracy of ASR systems, and **find deployment opportunities. This information is crucial for developing real-time applications**, and improving accessibility.", "summary": "Quantization optimizes OpenAI's Whisper models, balancing model size, speed, and accuracy for diverse applications.", "takeaways": ["Quantization reduces Whisper model size by up to 45% without sacrificing accuracy.", "Quantization can decrease Whisper model latency by 19%, improving processing speed.", "The study offers insights into selecting the optimal Whisper model and quantization method."], "tldr": "Automated Speech Recognition (ASR) models like OpenAI's Whisper have become essential for many applications, including translation and live transcription. However, issues arise due to inaccuracies in transcription along with increased latency and high computational demands. The researchers analyzed Whisper and two of its variants, focusing on their capabilities. Prior research explored methods to enhance Whisper's performance, but the impact of quantization on model size and latency needed further exploring.\n\nThis study addresses the gap by **evaluating the capabilities of Whisper and its variants** along with defining quantization techniques for Whisper models. In addition, they examined performance of model in terms of word error rate, processing speed, and latency. The research summarizes qualitative and quantitative findings from two experimental evaluations, showing how **quantization reduces latency by 19% and model size by 45%**, while also preserving transcription accuracy.", "affiliation": "Independent Researcher", "categories": {"main_category": "Speech and Audio", "sub_category": "Speech Recognition"}, "podcast_path": "2503.09905/podcast.wav"}