[{"content": "| Method | Data Source | Data Annotator | Data Type | Data Scale | Tuning Technique | Tuning Trick | Base LLM |\n|---|---|---|---|---|---|---|---| \n| AttrScore [Yue et al. (2023)] | Manual | Human | QA, NLI, Fact-Checking, Summarization | 63.8K | SFT | - | Multiple LLMs |\n| PandaLM [Wang et al. (2024h)] | Manual | Human | Instruction Following | 300K | SFT | - | Multiple LLMs |\n| AUTO-J [Li et al. (2024e)] | Synthetic | GPT-4 | Real-world Scenarios | 4K | SFT | - | LLaMA-2 |\n| JudgeLM [Zhu et al. (2023)] | Synthetic | GPT-4 | Instruction Following | 100K | SFT | - | Vicuna |\n| Self-Judge [Lee et al. (2024)] | Manual | Human | Preference Learning | 65/57K | SFT | JSFT | LLaMA-2 |\n| X-EVAL [Liu et al. (2024a)] | Manual | Human | Dialogue, Summarization, Data-to-Text | 55K | SFT | Two-Stage Instruction Tuning | Flan-T5 |\n| FLAMe [Vu et al. (2024)] | Manual | Human | Various Tasks | 5M+ | SFT | Multi-task Training | PaLM-2 |\n| InstructScore [Xu et al. (2023)] | Manual& Synthetic | Human& GPT-4 | Various Tasks | 20K | SFT | Meta-Feedback | LLaMA |\n| CritiqueLLM [Ke et al. (2024)] | Manual | Human | Instruction Following, real-world scenarios | 5K | SFT | Prompt Simplify, Swapping Augmentation | ChatGLM3 |\n| Meta-Rewarding [Wu et al. (2024)] | Synthetic | LLaMA-3 | Preference Learning | 20K | Preference Learning | Meta-Rewarding | LLaMA-3 |\n| Self-Taught Evaluator [Wang et al. (2024f)] | Synthetic | Mixtral | Various Tasks | 20K | Preference Learning | Self-Taught | LLaMA-3 |\n| HALU-J [Wang et al. (2024a)] | Synthetic | GPT-4o | Fact Extraction | 2.6K | Preference Learning | DPO | Mistral |\n| OffsetBias [Park et al. (2024)] | Synthetic | GPT-4, Claude3 | Preference Learning | 8.5K | SFT | Debiasing Augmentation | LLaMA-3 |\n| SorryBench [Xie et al. (2024a)] | Synthetic | GPT-4 | Safety | 2.7K | SFT | - | Multiple LLMs |\n| LLaVA-Critic [Xiong et al. (2024b)] | Synthetic | GPT-4o | Preference Learning | 113K | Preference Learning | DPO | LLaVA-v.1.5 |\n| PROME-THEUS2 [Kim et al. (2024)] | Synthetic | GPT-4 | Preference Learning | 300K | SFT | Joint Training, Weight Merging | Mistral |\n| Themis [Hu et al. (2024)] | Manual & Synthetic | Human & GPT-4 | Various Tasks | 67K | Preference Learning | Multi-perspective Consistency Verification, Rating-oriented DPO | LLaMA-3 |", "caption": "Table 1: Overview of tuning methods in LLM-as-a-judge.", "description": "This table provides a comprehensive overview of various methods used to fine-tune Large Language Models (LLMs) for the specific task of acting as a judge. It details the data source used for training (manually labeled or synthetic data), the annotator involved (humans or other LLMs), the type of data, the scale of the dataset, the specific fine-tuning techniques employed, any additional tricks used to improve performance, and the base LLM used as a starting point.  This allows for a detailed comparison of different approaches to training LLMs for judgment tasks.", "section": "4.1 Tuning"}, {"content": "| Method | Data Type | Scale | Reference | Metrics | Purpose |\n|---|---|---|---|---|---| \n| MT-Bench [Zheng et al. (2023)] | Multi-turn Conversation | 80 | Human Expert | Consistency, Bias, Error | General Performance, Position/Verbosity/Self-enhancement Bias |\n| Chatbot Arena [Zheng et al. (2023)] | Single-turn Conversation | 30K | User | Consistency, Bias, Error | General Performance, Position/Verbosity/Self-enhancement Bias |\n| CodeJudge-Eval [Zhao et al. (2024a)] | Code | 457 | Execution System | Accuracy, F1 | General Performance |\n| JUDGE-BENCH [Tan et al. (2024a)] | Various Tasks | 70K | Human | Cohen\u2019s kappa, Correlation | General Performance |\n| SOS-BENCH [Penfever et al. (2024)] | Various Tasks | 152K | Human | Normalized Accuracy | General Performance |\n| LLM-judge-eval [Wei et al. (2024a)] | Summarization, Alignment | 1K | Human | Accuracy, Flipping Noise, Position Bias, Length Bias | General Performance |\n| DHP [Wang et al. (2024g)] | Various Tasks | 400 | Human | Discernment Score | General Performance |\n| EVALBIAS-BENCH [Wang et al. (2024c)] | Alignment | 80 | Human | Accuracy | Various Bias |\n| [Raju et al. (2024)] | Various Tasks | 1.5K | Human | Separability, Agreement, BrierScore | Domain-specific Performance |\n| MLLM-as-a-judge [Chen et al. (2024a)] | Various Tasks | 30K | Human | Human Agreement, Analysis Grading, Hallucination Detection | Multimodal |\n| MM-EVAL [Son et al. (2024b)] | Various Tasks | 5K | Human | Accuracy | Multilingual |\n| KUDGE [Son et al. (2024a)] | Question Answering | 3.3K | Human & GPT-4o | Accuracy, Correlation | Non-English & Challenging |\n| [Murugadoss et al. (2024)] | Various Tasks | - | Human | Correlation | Evaluation Instruction Following |\n| [Thakur et al. (2024)] | Question Answering | 400 | Human | Scott\u2019s \u03c0, Percent Agreement | Vulnerability |\n| JudgeBench [Tan et al. (2024a)] | Various Tasks | 350 | GPT-4o | Accuracy | Challenging |\n| Arena-Hard Auto [Li et al. (2024h)] | Alignment | 500 | GPT-4-Turbo | Separability, Agreement | Challenging |\n| R-Judge [Yuan et al. (2024b)] | Multi-turn Interaction | 569 | Human | F1, Recall, Spec, Effect | Safety |\n| [Shi et al. (2024)] | Alignment | 100K | Human | Repetition Stability, Position Consistency, Preference Fairness | Position Bias |", "caption": "Table 2: Overview of various benchmarks and datasets for LLM-as-a-judge.", "description": "This table provides a comprehensive overview of various benchmarks and datasets used for evaluating LLMs in their role as judges.  It details the methodology, data type, scale, reference source, metrics used, and purpose of each benchmark. The benchmarks are categorized by their specific goals, such as assessing general performance, quantifying bias, or focusing on particular application domains. This allows for a clear understanding of the diverse methods used to evaluate LLM-as-a-judge systems, highlighting the complexities of this emerging research area.", "section": "6 Benchmark: Judging LLM-as-a-judge"}]