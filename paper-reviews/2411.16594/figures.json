[{"figure_path": "https://arxiv.org/html/2411.16594/x1.png", "caption": "Figure 1: Overview of various input and output formats of LLM-as-a-judge.", "description": "This figure illustrates the different input and output types used in LLM-as-a-judge systems.  The input can be a single candidate (point-wise) or multiple candidates (pair-wise or list-wise). The output can be a score for each candidate, a ranking of candidates, or a selection of the best candidates.", "section": "2 Preliminary"}, {"figure_path": "https://arxiv.org/html/2411.16594/x2.png", "caption": "Figure 2: Taxonomy of research in LLM-as-a-judge that consists of judging attribution, methodology and application.", "description": "This figure presents a comprehensive taxonomy of research on using Large Language Models (LLMs) as evaluators.  It breaks down the research into three key aspects:  What is being evaluated (attributes such as helpfulness, harmlessness, etc.), how the evaluation is performed (methodology such as fine-tuning, prompting techniques, etc.), and where this type of LLM evaluation is used (applications in areas like alignment, retrieval, reasoning, etc.).  The taxonomy is presented as a tree diagram, allowing for a detailed exploration of the various sub-categories and their interrelationships within the field of LLM-as-a-judge.", "section": "2 Preliminary"}, {"figure_path": "https://arxiv.org/html/2411.16594/x3.png", "caption": "Figure 3: LLMs are capable of judging various attributes.", "description": "This figure shows the different attributes that Large Language Models (LLMs) can effectively judge.  These attributes are key aspects often evaluated in various natural language processing (NLP) tasks, such as the helpfulness, harmlessness, reliability, relevance, feasibility, and overall quality of generated text or other outputs. The image visually represents the multifaceted evaluation capabilities of LLMs, demonstrating their potential to go beyond simple metrics in assessing the nuanced aspects of text and other outputs.", "section": "3 Attribute"}, {"figure_path": "https://arxiv.org/html/2411.16594/x4.png", "caption": "Figure 4: Overview of prompting strategies for LLM-as-a-judge.", "description": "This figure illustrates six different prompting strategies used in LLM-as-a-judge systems.  Each strategy aims to improve the accuracy and effectiveness of LLMs in performing judgment tasks by addressing specific limitations or biases. The strategies shown are: Swapping Operation, Rule Augmentation, Multi-agent Collaboration, Demonstration, Multi-turn Interaction, and Comparison Acceleration. Each strategy is visually represented with a flowchart to depict the process involved.", "section": "4.2 Prompting"}]