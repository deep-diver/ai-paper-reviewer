[{"figure_path": "https://arxiv.org/html/2502.06155/x1.png", "caption": "Figure 1: We observe the\u00a0Attention Tile\u00a0pattern in 3D DiTs. (a) the attention map can be broken down into smaller repetitive blocks. (b) These blocks can be classified into two types, where attention weights on the diagonal blocks are noticeably larger than on off-diagonal ones. (c) These blocks exhibit locality, where the attention score differences between the first frame and later frames gradually increases. (d) The block structure is stable across different data points, but varies across layers. We randomly select 2 prompts (one landscape and one portrait) and record the important positions in the attention map that accounted for 90% (95%, 99%) of the total. We printed the proportion of stable overlap of important positions across layers.", "description": "Figure 1 illustrates the \"Attention Tile\" pattern observed in 3D Diffusion Transformers (DiTs) for video generation.  Panel (a) shows that the attention maps can be decomposed into repeating blocks. Panel (b) distinguishes between diagonal blocks (stronger attention weights) and off-diagonal blocks (weaker weights). Panel (c) highlights the locality within these blocks, showing that attention decreases as the distance from the first frame increases. Panel (d) demonstrates the consistency of the block pattern across different input videos but with variations across different layers of the DiT.  The analysis involved randomly selecting two video prompts (landscape and portrait orientations) and identifying the most important attention positions (those accounting for 90%, 95%, and 99% of total attention).  The figure then shows the overlap of these key positions across various layers, quantifying the stability of the Attention Tile pattern.", "section": "3. EFFICIENT-VDIT"}, {"figure_path": "https://arxiv.org/html/2502.06155/x2.png", "caption": "Figure 2: \u00a0Efficient-vDiT\u00a0takes in a pre-trained 3D Full Attention video diffusion transformer(DiT), with slow inference speed and high fidelity. It then operates on three stages to greatly accelerate the inference while maintaining the fidelity. In Stage 1, we modify the multi-step consistency distillation framework from\u00a0(Heek et\u00a0al., 2024) to the video domain, which turned a DiT model to a CM model with stable training. In Stage 2,\u00a0Efficient-vDiT\u00a0performs a searching algorithm to find the best sparse attention pattern for each layer. In stage 3,\u00a0Efficient-vDiT\u00a0performs a knowledge distillation procedure to optimize the fidelity of the sparse DiT. At the end,\u00a0Efficient-vDiT\u00a0outputs a DiT with linear attention, high fidelity and fastest inference speed.", "description": "This figure illustrates the Efficient-VDIT framework, which enhances the speed of video generation while maintaining high fidelity.  It starts with a pre-trained 3D full attention Diffusion Transformer (DiT) known for its accuracy but slow inference. The framework then uses a three-stage process. Stage 1 adapts multi-step consistency distillation for video, creating a more efficient CM model. Stage 2 searches for optimal sparse attention patterns in each layer, achieving linearity in time complexity.  Stage 3 uses knowledge distillation to refine the sparse model, resulting in a final DiT that is significantly faster than the original with minimal impact on video quality.", "section": "3. EFFICIENT-VDIT"}, {"figure_path": "https://arxiv.org/html/2502.06155/x3.png", "caption": "Figure 3: Exemplar attention mask (2:6:262:62 : 6). It maintains the attention in the main diagonals and against 2 global reference latent frames. Tile blocks in white are not computed.", "description": "This figure showcases a sample of a sparse attention mask used in the EFFICIENT-VDIT model.  The mask is represented as a grid, highlighting which latent frame connections are maintained during attention computation. The (2:6) notation signifies that each frame attends to 2 global reference frames, aside from its connections along the main diagonal. The white blocks illustrate areas where attention is pruned, reducing computational cost.  This sparse attention pattern is designed to leverage the repetitive 'Attention Tile' structure identified in video data. By selectively pruning less important connections, the model significantly accelerates the attention mechanism while aiming to retain sufficient information for high-quality video generation. ", "section": "3. EFFICIENT-VDIT"}, {"figure_path": "https://arxiv.org/html/2502.06155/x4.png", "caption": "Figure 4: Search results for Open-Sora-Plan v1.2 model (29 frames). We verify that different layers have different sparsity in 3D video DiTs.", "description": "Figure 4 shows the results of an experiment to determine the optimal sparsity level for each layer in a 3D video diffusion transformer (DiT) model, specifically Open-Sora-Plan v1.2, when generating videos with 29 frames.  The experiment aimed to find the appropriate balance between computational efficiency and performance.  The graph displays the sparsity level (vertical axis) for each layer (horizontal axis) after an optimization process. Different layers exhibit varying levels of sparsity, indicating that some layers can tolerate more sparsity than others during inference without significantly impacting the model's performance.  This finding supports the design choice of the EFFICIENT-VDIT framework, which employs layer-wise sparsity search to optimize the trade-off between inference speed and video quality.", "section": "3. EFFICIENT-VDIT"}, {"figure_path": "https://arxiv.org/html/2502.06155/extracted/6191047/figures/prompt_sample/ablation.jpg", "caption": "Figure 5: Qualitative samples of our models. We compare the generation quality between the base model, MLCD model, and after knowledge distillation. Frames shown are equally spaced samples from the generated video. Efficient-vDiT\u00a0is shortened as \u2018E-vdit\u2019 for simplicity. More samples can be found in Appendix F.", "description": "This figure displays a qualitative comparison of video generation results from three different models: the original, a model using multi-step consistency distillation (MLCD), and a model that incorporates both MLCD and knowledge distillation.  The comparison showcases the visual quality of video samples generated by each model. Each row presents a video sample from the same prompt, where the columns represent the generated videos by the three models. This allows viewers to directly compare the visual fidelity, motion clarity, and overall quality achieved through the different model architectures.  Additional examples are provided in Appendix F of the paper.", "section": "4.4. Qualitative result"}, {"figure_path": "https://arxiv.org/html/2502.06155/extracted/6191047/figures/prompt_sample/cogvideo_batch.jpg", "caption": "Figure 6: Qualitative samples of ablation of distillation order. sampled from VBench prompts. We show that both MLCD and\u00a0Efficient-vDiT\u00a0model can simliar quality on these samples. In two consecutive videos, the top shows results from MLCD + CD model followed by KD + MLCD model.", "description": "This figure displays a comparison of video generation results from different models to demonstrate the impact of distillation order on video quality.  The top row of each pair of videos shows results from a model trained using multi-step consistency distillation (MLCD) followed by knowledge distillation (KD). The bottom row shows results from a model trained with the opposite order: KD followed by MLCD.  The videos were generated using prompts from the VBench dataset, a standard benchmark for evaluating video generation models. The purpose is to visually show whether the order of applying these two techniques significantly affects the final video quality and to demonstrate that the order doesn't matter much. ", "section": "4.3 Video Quality benchmark"}, {"figure_path": "https://arxiv.org/html/2502.06155/extracted/6191047/figures/prompt_sample/output_batch_1.jpg", "caption": "Figure 7: Qualitative samples of CogvideoX-5B (Yang et\u00a0al., 2024b) distillation from its sample prompts. We show that our attention distill is capable of MM-DiT model architecture. In two consecutive videos, the top shows results from the base model, followed by the distillation model.", "description": "Figure 7 presents a qualitative comparison of video generation results from the base CogVideoX-5B model and a model that incorporates attention distillation. The figure demonstrates the effectiveness of the proposed attention distillation technique, showing that it can be successfully applied to the MM-DiT architecture of CogVideoX-5B.  The comparison is shown in pairs of videos where the top row displays the results from the original, non-distilled model, and the bottom row shows results from the model trained with attention distillation. This visual comparison illustrates that the attention distillation method preserves the overall quality and details of the generated videos while potentially offering improved efficiency or faster generation.", "section": "4.4. Qualitative result"}, {"figure_path": "https://arxiv.org/html/2502.06155/extracted/6191047/figures/prompt_sample/output_batch_3.jpg", "caption": "Figure 8: Based on Open-Sora\u2019s examples (Zheng et\u00a0al., 2024) , we selected dynamic prompts featuring centralized explosions and radiating energy, demonstrating dramatic transitions from focal points to expansive environmental transformations, emphasizing large-scale motion.", "description": "Figure 8 showcases video generation results from three models: the baseline model, a model using multi-step consistency distillation (MLCD), and the proposed EFFICIENT-VDIT model.  The prompts used are dynamic and involve centralized explosions and radiating energy. The videos generated demonstrate the models' ability to handle large-scale motion and dramatic transitions, transitioning from concentrated focal points to expansive environmental effects.", "section": "4.4. Qualitative result"}]