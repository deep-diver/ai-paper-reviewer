[{"figure_path": "https://arxiv.org/html/2503.10460/extracted/6275403/figures/Light-R1-wenliang.png", "caption": "Figure 1: Overview of training pipeline of Light-R1-32B.", "description": "This figure illustrates the training pipeline used to develop the Light-R1-32B model.  It shows the data processing steps, including data collection from various sources, decontamination to remove duplicates and inconsistencies, and difficulty filtering to select the most challenging problems for training. The model training phase is also depicted, highlighting the curriculum learning strategy with two stages of supervised fine-tuning (SFT) followed by direct preference optimization (DPO). This curriculum approach gradually increases the difficulty of the training data to improve model reasoning capabilities. The figure visually summarizes the complete workflow from raw data to the final Light-R1-32B model.", "section": "3.1 Data Preparation"}, {"figure_path": "https://arxiv.org/html/2503.10460/x1.png", "caption": "Figure 2: RL Learning curves of response length and train-reward, smoothed with Savitzky-Golay filter.", "description": "This figure shows the learning curves of response length and training reward during reinforcement learning.  The curves are smoothed using a Savitzky-Golay filter to reduce noise and highlight trends.  The x-axis represents the training steps, while the y-axes represent the average response length and average training reward. The plot illustrates how both response length and reward increase simultaneously during the training process, indicating successful reinforcement learning.", "section": "4 Light-R1-14B-DS: Successful RL on Already Long-COT Finetuned Models"}]