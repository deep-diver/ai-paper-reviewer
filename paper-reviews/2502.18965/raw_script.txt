[{"Alex": "Hey everyone, welcome to the podcast! Today, we're diving into the wild world of AI-powered recommendations. Forget endless scrolling \u2013 imagine an AI that *knows* exactly what you want before you do! We're unpacking a fascinating paper on something called 'OneRec', which is revolutionizing how we discover content online. I'm Alex, your host, and resident AI geek.", "Jamie": "Wow, sounds intriguing! I'm Jamie, and I'm definitely ready to be amazed. So, OneRec... what exactly *is* it?"}, {"Alex": "Think of it as a super-smart recommendation engine. Most systems have different stages: first, they grab a bunch of possibilities (that's retrieval), then narrow it down (that's ranking). OneRec throws all that out the window! It uses a single, unified AI model to both find *and* rank content. It's like going from a clunky assembly line to a sleek, all-in-one machine.", "Jamie": "Hmm, okay, so it's more efficient. But how does it actually *work*? I mean, what makes it so different?"}, {"Alex": "It's all about generative AI. Instead of just picking from a pre-existing list, OneRec *generates* potential recommendations. Imagine it predicting the videos you\u2019d want to see, almost like writing a story about your interests. This 'story' then becomes your personalized feed.", "Jamie": "Generates recommendations? That sounds incredibly complex! Is it like those AI image generators, but for videos?"}, {"Alex": "That\u2019s a great analogy, Jamie! In a way, yes. It uses similar techniques, but instead of pixels, it\u2019s working with 'semantic tokens' that represent the meaning and content of videos. It predicts sequences of these tokens to build your perfect viewing session.", "Jamie": "Okay, 'semantic tokens' \u2013 got it. So, instead of just saying 'cat video,' it understands what *kind* of cat video I'd be in the mood for. Makes sense!"}, {"Alex": "Exactly! And that\u2019s where the 'preference alignment' part comes in. OneRec doesn't just guess randomly; it learns from your past behavior to understand your specific tastes. This is refined through a process called Iterative Preference Alignment, using something called Direct Preference Optimization, or DPO.", "Jamie": "DPO\u2026 that sounds\u2026 intimidating. What is DPO in simple terms, and how does Iterative Preference Alignment enhance result quality?"}, {"Alex": "DPO is a clever technique to fine-tune the model based on user preferences. Think of it like this: Instead of directly telling the AI what\u2019s good or bad, you show it pairs of recommendations and let it figure out which one you\u2019d prefer. Iterative preference alignment just means going through this process multiple times for further refinement.", "Jamie": "So it's constantly learning and improving based on user behavior in order to fine-tune its recommendations."}, {"Alex": "Precisely. Traditional DPO requires both positive (liked) and negative (disliked) examples simultaneously, which isn\u2019t always possible in a recommendation setting. OneRec overcomes this using a reward model to *simulate* user feedback. This is where the Iterative Preference Alignment comes in, refining those preferences repeatedly.", "Jamie": "A reward model? How does the system design a reward model to simulate user generation and customize the sampling strategy according to the recommendation system\u2019s online learning, since you can't just ask people what they want?"}, {"Alex": "The reward model is trained on historical user interaction data \u2013 what videos users watched, liked, shared, etc. It learns to predict how a user would rate a given set of recommendations. Then, using that predicted score, the system can cleverly pick the best recommendations.", "Jamie": "That's actually brilliant! So it kind of creates its own 'virtual users' to learn from. It's able to take these simulated signals and use them to improve the system without explicitly asking the user what they want!"}, {"Alex": "Yes! The paper describes that OneRec also uses self-generated hard negative sampling to improve the quality of generated results, which contrasts with the Direct Preference Optimization (DPO) strategy used in NLP.", "Jamie": "OK, so it's like the AI is saying, 'I think you'll like this,' and then another part of the AI is saying, 'No, you'd actually *hate* that!' to improve the results"}, {"Alex": "Exactly. It uses the reward model, which is personalized, to evaluate options rather than self-hard rejected samples from the beam search results rather than random sampling.", "Jamie": "That's a fantastic use of AI. To make sure I get this, beam search is used to find a collection of the best possible option based on some metrics and you're using self-rejected samples to make sure it doesn't generate things you DON'T like? That sounds next-level"}, {"Alex": "That's correct! Now, one of the challenges with generative models is scaling them up. Large language models have shown massive improvements with scale, and the same is true for recommendation systems. To improve model capacity without greatly increasing computation, this paper adopts Mixture-of-Experts (MoE).", "Jamie": "MoE\u2026 Another acronym! What does MoE bring to the table, then?"}, {"Alex": "It\u2019s a way to increase the model's capacity without increasing the number of computations proportionally. It means that each input is processed only by a *portion* of the network, so you get the benefit of a larger model without the cost of processing every input through the entire thing.", "Jamie": "Ah, so it's like having a team of specialists, and only calling in the ones you need for a particular task. Makes sense for efficiency!"}, {"Alex": "Precisely! The paper demonstrates impressive results, achieving a 1.6% increase in watch-time on Kuaishou, a huge short-video platform. Considering the scale of Kuaishou, that's a *massive* impact.", "Jamie": "Wow, 1.6% on a platform like Kuaishou? That\u2019s huge! So it's not just theoretically interesting; it's actually making a real-world difference. How did OneRec get deployed, then?"}, {"Alex": "The system processes collected interaction logs as training data, initially adopting the next token prediction objective LNTP to train the seed model. After convergence, we add the DPO loss LDPO for preference alignment. The trained parameters are synchronized to the online inference module and the DPO sampling server for real-time serving and preference-based data selection", "Jamie": "This is getting into the complicated engineering challenges of deploying the model! I'd love to know more about the design decision. In terms of model architecture, OneRec\u2019s transformer-based encoder-decoder model consistent with the T5 architecture?"}, {"Alex": "The model leverage stacked multi-head self-attention and feed-forward layers to process the input sequence. T5 also uses a transformer-based framework consisting of two main components: an encoder for modeling user historical interactions and a decoder for session list generation. One difference is to train a larger model at reasonable economic costs, for the feed-forward neural networks (FNNs) in the decoder, we adopt the MoE architecture.", "Jamie": "So the architecture is more or less the same, but it brings a series of upgrades to further improve the original architecture for the task. Cool."}, {"Alex": "Exactly. Another key focus was on the session-wise generation approach. Instead of predicting the next item in isolation, OneRec generates a *list* of recommendations for an entire session. The point-by-point generation method necessitates hand-craft strategies to ensure coherence and diversity in the generated results. In contrast, the session-wise learning process enables the model to autonomously learn the optimal session structure by feeding it preferred data", "Jamie": "Aha! So, it's understanding the *context* of my viewing session, not just picking videos at random."}, {"Alex": "Right! This allows the model to autonomously learn the optimal session structure by feeding it preferred data. The model can autonomously decide diversity and coherence, which gives the AI more power.", "Jamie": "Speaking of experiments, what kind of experiments were run to evaluate the effectiveness of each module and how were the baselines chosen for comparison?"}, {"Alex": "Well, the authors compared OneRec against state-of-the-art recommendation models like SASRec, BERT4Rec, FDSA, and TIGER. They also experimented with different variations of DPO. To evaluate OneRec, they calculated the mean reward for different target metrics, including session watch time (swt), view probability (vtr), follow probability (wtr) and like probability (ltr).", "Jamie": "It sounds like all modules contributed to positive gains, making it difficult to remove it. How about the future directions of OneRec? "}, {"Alex": "The paper mentions that while OneRec significantly improves user watch time, there are still limitations in interactive indicators, such as likes. The authors suggest that future research should focus on enhancing the end-to-end generative recommendation's capability in multi-objective modeling to provide a better user experience", "Jamie": "Hmm, more optimization is needed. Well, thanks so much for diving into the details of this paper!"}, {"Alex": "My pleasure, Jamie! So, to summarize, OneRec represents a significant step forward in recommendation systems by unifying retrieval and ranking into a single, generative model. The use of DPO and mixture-of-experts allows for efficient and personalized recommendations, paving the way for more engaging and relevant online experiences. The next steps involve improving user experience, such as likes. And that's all the time we have for today. Thanks for tuning in!", "Jamie": ""}]