[{"figure_path": "2410.01968/tables/table_8_0.html", "caption": "Table 1: Results on two selected challenging motions: kick and jump.", "description": "The table presents a comparison of the performance of three algorithms (FLD, SCAE, and BMI) on two challenging motions (kick and jump), showing the time percentage and height achieved for each motion.", "section": "5 Experiments"}, {"figure_path": "2410.01968/tables/table_13_0.html", "caption": "Table 2: Elements of one data point (step) in the dataset", "description": "Table 2 lists the components of a single data point (step) in the dataset, including base position, rotation, linear and angular velocities, projected gravity, and joint positions and velocities.", "section": "A.3.1 Dataset"}, {"figure_path": "2410.01968/tables/table_13_1.html", "caption": "Table 4: Elements of the observation space for robot policy", "description": "Table 4 lists the elements of the observation space used in the robot policy, including their symbols, dimensions, and added noise levels for training.", "section": "A.3.2 State and Action Spaces"}, {"figure_path": "2410.01968/tables/table_14_0.html", "caption": "Table 5: Architecture of the neural networks used in SCAE", "description": "Table 5 details the architecture of the encoder and decoder neural networks used in the self-consistent auto-encoder (SCAE) for the latent dynamics model, specifying layer type, output size, kernel size, normalization, and activation function for each layer.", "section": "A.3.3 SCAE Training"}, {"figure_path": "2410.01968/tables/table_14_1.html", "caption": "Table 6: Hyper-parameters of SCAE training", "description": "The table lists the hyperparameters used for training the self-consistent autoencoder (SCAE) in the latent dynamics model learning stage.", "section": "A.3.3 SCAE Training"}, {"figure_path": "2410.01968/tables/table_14_2.html", "caption": "Table 7: Architecture of the neural networks used in policy training", "description": "Table 7 shows the architecture of the neural networks used for training the robot policy (\u03c0) and value function (V) within the Proximal Policy Optimization (PPO) algorithm.", "section": "A.3.4 Policy Training"}, {"figure_path": "2410.01968/tables/table_15_0.html", "caption": "Table 8: Hyper-parameters of policy training", "description": "Table 8 lists the hyperparameters used for training the robot policy using the proximal policy optimization algorithm.", "section": "A.3.4 Policy Training"}, {"figure_path": "2410.01968/tables/table_15_1.html", "caption": "Table 1: Results on two selected challenging motions: kick and jump.", "description": "The table shows the quantitative results of FLD, SCAE, and BMI on two challenging motions (kick and jump), measured by kicking time, kicking height, and jumping time.", "section": "5.2 Performance of Policy Learning"}, {"figure_path": "2410.01968/tables/table_16_0.html", "caption": "Table 11: Weights of the regularization rewards", "description": "This table shows the weights used for the regularization reward in the BMI framework, which consists of weights for action rate, joint acceleration, and joint torque.", "section": "A.3.5 BMI Training"}, {"figure_path": "2410.01968/tables/table_16_1.html", "caption": "Table 12: Hyper-parameters of BMI fine-tuning", "description": "This table lists the hyperparameters used in the bi-level fine-tuning stage of the proposed method.", "section": "A.3.5 BMI Training"}]