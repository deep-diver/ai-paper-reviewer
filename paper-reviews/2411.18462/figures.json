[{"figure_path": "https://arxiv.org/html/2411.18462/x1.png", "caption": "Figure 1: The \u201cdifficulty\u201d of tokens varies in a sequence, resulting in different numbers of accepted draft tokens at different positions.", "description": "The figure illustrates how the difficulty of generating tokens varies within a sequence.  A draft model is used to predict tokens which are then verified by a more powerful target model.  Some tokens, like common greetings, are easy for the draft model to predict and thus have high acceptance rates (many draft tokens are accepted and verified). Other tokens, such as those requiring complex reasoning or domain-specific knowledge, are much harder for the draft model to accurately predict. This results in a lower acceptance rate, with fewer draft tokens being accepted for these more difficult parts of the sentence.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2411.18462/x4.png", "caption": "Figure 2: The correlation between draft model entropy and draft token acceptance probability (top) and lengths of accepted draft seqeunces (bottom).", "description": "This figure displays the relationship between draft model entropy and two key aspects of speculative decoding: the probability of draft tokens being accepted by the target model, and the lengths of accepted draft sequences. The top row shows that lower draft model entropy correlates with higher acceptance probabilities, while the bottom row reveals that lower entropy also corresponds to longer sequences of accepted draft tokens.  This visualization underscores the variability in drafting difficulty and motivates the need for a dynamic draft length policy that adapts to varying levels of prediction uncertainty.", "section": "Method"}, {"figure_path": "https://arxiv.org/html/2411.18462/x5.png", "caption": "Figure 3: Distribution histograms of the entropy ratio Hq,p/Hqsubscript\ud835\udc3b\ud835\udc5e\ud835\udc5dsubscript\ud835\udc3b\ud835\udc5eH_{q,p}/H_{q}italic_H start_POSTSUBSCRIPT italic_q , italic_p end_POSTSUBSCRIPT / italic_H start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT. For most tokens, this ratio falls into a narrow range, indicating that the cross entropy Hq,psubscript\ud835\udc3b\ud835\udc5e\ud835\udc5dH_{q,p}italic_H start_POSTSUBSCRIPT italic_q , italic_p end_POSTSUBSCRIPT can be approximated by a constant multiplication of the draft entropy Hqsubscript\ud835\udc3b\ud835\udc5eH_{q}italic_H start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT.", "description": "Figure 3 presents histograms showing the distribution of the ratio between the cross-entropy (Hq,p) and the draft entropy (Hq) for three different large language models. The cross-entropy measures the dissimilarity between the probability distributions generated by the draft and target language models.  The draft entropy quantifies the uncertainty in the draft model's predictions. The histograms reveal that for the vast majority of tokens, this ratio remains within a narrow range.  This key observation supports the paper's approximation that the cross-entropy (Hq,p) can be estimated efficiently using only the draft entropy (Hq) multiplied by a constant value. This simplification is crucial for the algorithm\u2019s efficiency.", "section": "2 Method"}, {"figure_path": "https://arxiv.org/html/2411.18462/x8.png", "caption": "Figure 4: Comparison between the actual acceptance probability from Equation\u00a0(2), the acceptance probability lower bound from Equation\u00a0(5), and the estimated lower bound after approximating the cross entropy Hq,psubscript\ud835\udc3b\ud835\udc5e\ud835\udc5dH_{q,p}italic_H start_POSTSUBSCRIPT italic_q , italic_p end_POSTSUBSCRIPT with a constant multiplication of Hqsubscript\ud835\udc3b\ud835\udc5eH_{q}italic_H start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT. Each position on the x-axis corresponds to a token, which has been sorted according to the actual acceptance probability.", "description": "Figure 4 illustrates the relationship between the theoretical acceptance probability of draft tokens and their entropy.  The figure compares three values: the actual acceptance probability calculated using Equation (2) from the paper; a lower bound on this probability derived from Equation (5), which uses the cross-entropy between the draft and target language models; and an estimated lower bound that simplifies Equation (5) by approximating cross-entropy as a constant multiple of draft model entropy. The tokens are ordered on the x-axis according to their actual acceptance probability, enabling a visual comparison of the actual and estimated probabilities.", "section": "2 Method"}, {"figure_path": "https://arxiv.org/html/2411.18462/x12.png", "caption": "Figure 5: The average generated draft length, accepted draft length, and acceptance rate of Qwen2.5 (top) and LLaMA-3 (bottom) on SpecBench. Compared with the two baselines, SVIP\u00a0leads to a shorter draft length and a much higher acceptance rate.", "description": "Figure 5 presents a comparison of the performance of three different draft length policies (Constant, Heuristics, and SVIP) on the SpecBench benchmark using Qwen-2.5 and LLaMA-3 language models.  The top half shows results for Qwen-2.5, while the bottom half shows results for LLaMA-3.  For each model and policy, the figure displays three key metrics: the average generated draft length (the average number of tokens generated by the draft model before verification), the average accepted draft length (the average number of tokens accepted by the target model after verification), and the overall acceptance rate (the percentage of draft tokens accepted by the target model). The figure clearly demonstrates that SVIP consistently leads to shorter generated draft lengths and significantly higher acceptance rates compared to both the Constant and Heuristics baselines, thereby indicating its superior efficiency in speculative decoding.", "section": "3.1 Experiments on SpecBench"}]