[{"content": "| | **CoT** | **Hermes-coder** | **Hermes** |\n|---|---|---|---|\n| **Llama-3.1-70b** | 0% | 5% | 25% |\n| **Llama-3.1-405b** | 5% | 15% | 45% |\n| **GPT-4o** | 25% | 55% | 82.5% |", "caption": "Table I: Success score of different LLMs on power control and energy saving task.", "description": "This table presents the success rates achieved by different Large Language Models (LLMs) on two specific network tasks: power control and energy saving.  It compares three approaches: Chain-of-Thought (CoT), a method where the LLM generates code based on a chain of thought, Hermes-coder (where the code generation part of the Hermes framework is used), and the full Hermes framework. The success rate is defined as the percentage of times the LLM correctly predicts the outcome of the network task.  The table highlights how the performance varies across different LLMs (GPT-40, Llama 3.1-70b, and Llama 3.1-405b) and different methods demonstrating that the full Hermes framework generally performs better, especially with more advanced LLMs.", "section": "IV. EXPERIMENTAL RESULTS"}, {"content": "| Number of expert-designed blocks | 0 | 1 | 2 | 3 | 4 | 5 |\n|---|---|---|---|---|---|---|\n| **Llama-3.1-70b** | 25% | 25% | 30% | 45% | 60% | 75% |\n| **Llama-3.1-405b** | 45% | 50% | 65% | 70% | 75% | 80% |", "caption": "Table II: Performance with varying numbers of expert-designed blocks", "description": "This table presents the success rates achieved by two different sized language models (Llama-3.1-70b and Llama-3.1-405b) when using Hermes with varying numbers of pre-designed, expert-created model blocks integrated into the system.  It shows how the success rate increases as more expert-designed blocks are added, demonstrating the benefit of incorporating existing, reliable model components.", "section": "V. FUTURE AXES OF RESEARCH"}]