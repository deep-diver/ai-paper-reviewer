[{"figure_path": "2410.21252/figures/figures_2_0.png", "caption": "Figure 1: The compromised quality of synthesized SFT data often affects the performance of long-context SFT models, while LongReward utilizes an off-the-shelf LLM to provide reliable rewards for long-context-based model responses, enabling the employment of RL algorithms such as DPO to further enhance models' capacities.", "description": "The figure illustrates how LongReward uses an off-the-shelf LLM to improve the performance of long-context large language models by providing reliable rewards for model responses, which are then used to enhance the models' capacities with reinforcement learning.", "section": "1 Introduction"}, {"figure_path": "2410.21252/figures/figures_4_0.png", "caption": "Figure 2: Illustration of LongReward. LongReward evaluates a long-context-based model response from four dimensions: helpfulness, logicality, faithfulness, and completeness. It assigns a score ranging from 0 to 10 for each dimension, and takes their average as the final reward.", "description": "The figure illustrates the LongReward method, which uses an off-the-shelf LLM to provide rewards for long-context model responses based on four human-valued dimensions.", "section": "3 Methodology"}]