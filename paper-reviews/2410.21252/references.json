{"references": [{" publication_date": "2024", "fullname_first_author": "Anthropic", "paper_title": "Anthropic: Introducing claude 3.5 sonnet", "reason": "This paper is highly relevant as it introduces a significant advancement in long-context LLMs, which is the core topic of the paper.  The introduction section discusses the state-of-the-art advancements in long-context models, making this citation essential for establishing the context and background of the research.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Zeng", "paper_title": "Longalign: A recipe for long context alignment of large language models", "reason": "This paper is crucial because it directly addresses the problem of long-context alignment in LLMs, which is a key focus of the main paper.  The introduction discusses the challenges of aligning LLMs with human preferences and this paper provides relevant background on techniques for improving alignment.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Reid", "paper_title": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context", "reason": "This citation is important as it shows a significant step towards larger context windows in LLMs, which is a relevant advancement in the field. The introduction discusses the challenges of working with extensive context, and this citation showcases the latest improvements in handling long sequences.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Bai", "paper_title": "Longbench: A bilingual, multitask benchmark for long context understanding", "reason": "This paper is highly significant since it introduces a benchmark specifically designed for evaluating long-context capabilities in LLMs. The experimental section evaluates the LongReward model on this benchmark, making this citation crucial for understanding the experimental setup and validating the performance claims.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Xiong", "paper_title": "Effective long-context scaling of foundation models", "reason": "This citation is highly relevant because it focuses on techniques for extending the context window in LLMs. The introduction mentions various approaches to increase the context length, including continual pre-training, and this paper offers details on one such method.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Bai", "paper_title": "Longalign: A recipe for long context alignment of large language models", "reason": "This paper is crucial because it directly addresses the problem of long-context alignment in LLMs, which is a key focus of the main paper.  The introduction discusses the challenges of aligning LLMs with human preferences and this paper provides relevant background on techniques for improving alignment.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Vavekanand", "paper_title": "Llama 3.1: An in-depth analysis of the next-generation large language model", "reason": "This paper is important as it provides details on the Llama-3.1-8B model, one of the LLMs used in the experiments. This information is crucial for understanding the models used, the experimental setup, and the relevance of the results.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Zeng", "paper_title": "Chatglm: A family of large language models from GLM-130B to GLM-4 all tools", "reason": "This paper provides details on the GLM-4-9B model, another key model used in the experiments. Understanding the architecture and characteristics of this model is crucial for interpreting the results obtained in the study.", "section_number": 4}, {" publication_date": "2022", "fullname_first_author": "Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "reason": "This paper is foundational to the topic of reinforcement learning for LLMs, which is central to the methodology section of the main paper. It provides an overview of the use of RL and human feedback for aligning LLMs, and this citation serves to position the research within that context.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Lee", "paper_title": "RLAIF vs. RLHF: scaling reinforcement learning from human feedback with AI feedback", "reason": "This paper expands on the topic of using reinforcement learning to improve LLMs by exploring the use of AI feedback. This is relevant to the methodology section, where LongReward uses an LLM to provide rewards, and this citation provides a related approach.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Tian", "paper_title": "Fine-tuning language models for factuality", "reason": "This paper is relevant because it addresses the challenge of improving the factuality of LLM outputs, a crucial aspect of the LongReward methodology.  The methodology section emphasizes the importance of faithfulness, and this citation describes techniques to improve factuality.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Rafailov", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "reason": "This paper introduces the DPO algorithm, a key component of the methodology. The methodology section explains how LongReward is combined with DPO, making this citation essential for understanding the core approach.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Dubois", "paper_title": "AlpacaFarm: A simulation framework for methods that learn from human feedback", "reason": "This citation is relevant because it discusses a method for obtaining feedback using LLMs, which is similar to the approach used in LongReward. The methodology section explores the use of an LLM as a judge, and this citation presents a related concept.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Yuan", "paper_title": "Self-rewarding language models", "reason": "This citation is related to the use of LLMs for reward generation, which is the core idea of the LongReward method.  The methodology section presents the use of an external LLM for assessment, and this citation offers another similar approach.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Zheng", "paper_title": "Judging LLM-as-a-judge with mt-bench and chatbot arena", "reason": "This paper introduces the LLM-as-judge approach for evaluation, which is highly relevant to the LongReward methodology.  The methodology section describes the use of an LLM to assess model responses, and this citation provides further context on this approach.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Liu", "paper_title": "Lost in the middle: How language models use long contexts", "reason": "This paper discusses limitations in how LLMs use long contexts, which is a relevant topic in the introduction section. The introduction mentions limitations of current long-context LLMs, and this citation provides specific insights into those limitations.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Zhang", "paper_title": "Longcite: Enabling LLMs to generate fine-grained citations in long-context QA", "reason": "This paper is relevant to the introduction as it discusses another aspect of challenges in working with long contexts in LLMs.  The introduction highlights challenges in data quality for SFT models, and this citation offers a specific example of a related limitation.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Hou", "paper_title": "Chatglm-rlhf: Practices of aligning large language models with human feedback", "reason": "This paper is highly relevant to the experimental section, as it discusses the DPO algorithm, which is used in conjunction with LongReward. This paper provides background on the DPO algorithm and its application in LLM training, which directly impacts the experimental methodology and results.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Pang", "paper_title": "Iterative reasoning preference optimization", "reason": "This paper is directly relevant to the methodology and experimental sections because it describes a technique used in DPO training.  The methodology explains how LongReward is combined with DPO, and this citation provides details on a specific aspect of the DPO optimization procedure.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Bai", "paper_title": "Training a helpful and harmless assistant with reinforcement learning from human feedback", "reason": "This paper discusses reinforcement learning methods from human feedback, providing an important context for the methodology section. This citation is relevant because it shows a prior approach that utilizes reinforcement learning for aligning LLMs and provides background for the LongReward approach.", "section_number": 3}]}