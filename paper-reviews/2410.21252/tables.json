[{"figure_path": "2410.21252/tables/table_6_0.html", "caption": "Table 1: Detailed statistics of benchmarks we used for evaluation. \"Avg Len\" refers to the average number of words/characters in the context of English/Chinese instances. \"LC Win Rate\u201d denotes length-controlled Win Rate (Dubois et al., 2024) against GPT-4-turbo.", "description": "Table 1 presents detailed statistics of the benchmarks used for evaluating the models, including the task type, number of data points, average length, language, metric used, and the judge model.", "section": "4 Experiments"}, {"figure_path": "2410.21252/tables/table_6_1.html", "caption": "Table 2: Results of automatic evaluation on long-context benchmarks rated by GPT-40. \"S-Doc QA\", \"M-Doc QA\", and \"Summ\" denote Single-Doc QA, Multi-Doc QA, and Summarization, respectively.", "description": "Table 2 presents the automatic evaluation results on LongBench-Chat and LongBench rated by GPT-40, showing the performance of different models on three long-context tasks.", "section": "4.4 Results on Long-Context Benchmarks"}, {"figure_path": "2410.21252/tables/table_7_0.html", "caption": "Table 3: FactScore of the SFT and LongReward+DPO versions of models on 260 randomly sampled questions from LongBench-Chat and LongBench, taking GPT-40-mini as the judge. \"#Facts\" and \"FactScore\" denote the average number of atomic facts and the ratio of supported facts per response, respectively.", "description": "Table 3 presents the results of evaluating the faithfulness of the SFT and LongReward+DPO models by measuring the number of factual statements and the ratio of supported factual statements in their responses.", "section": "4 Experiments"}, {"figure_path": "2410.21252/tables/table_7_1.html", "caption": "Table 4: Results of human evaluation of LongRe-ward+DPO version of Llama-3.1-8B on LongBench-Chat against the SFT baseline. We report the proportion of wins, ties, and losses of the DPO model on each di-mension.", "description": "Table 4 presents the results of a human evaluation comparing the performance of a Llama-3.1-8B model fine-tuned with DPO and LongReward against a baseline SFT model across four dimensions: helpfulness, logicality, faithfulness, and completeness.", "section": "4.4 Results on Long-Context Benchmarks"}, {"figure_path": "2410.21252/tables/table_7_2.html", "caption": "Table 5: Performance of different models on short-context instruction-following benchmarks.", "description": "Table 5 presents the performance comparison of different models on two short-context instruction-following benchmarks, MT-Bench and AlpacaEval2, showing the impact of LongReward on short instruction following.", "section": "4.5 Results on Short-Context Benchmarks"}, {"figure_path": "2410.21252/tables/table_8_0.html", "caption": "Table 6: Performance of DPO models using different preference datasets, where the short- and long-context preference data are constructed using short reward model trained by Hou et al. (2024) and LongReward, respectively.", "description": "Table 6 shows the performance comparison of DPO models trained using short-context, long-context, and combined short and long-context preference datasets on LongBench and short-context benchmarks.", "section": "4 Experiments"}, {"figure_path": "2410.21252/tables/table_8_1.html", "caption": "Table 7: Alignment of different reward methods with human preference on a set of 464 manually annotated long-context preference pairs, where the queries and responses are from LongBench-Chat and the SFT checkpoint of Llama-3.1-8B, respectively.", "description": "This table shows the accuracy of different reward methods in aligning with human preferences for long-context model responses.", "section": "4.7 Alignment with Human Preference"}]