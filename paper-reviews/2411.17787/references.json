{"references": [{"fullname_first_author": "Keyu Tian", "paper_title": "Visual autoregressive modeling: Scalable image generation via next-scale prediction", "publication_date": "2024-04-05", "reason": "This paper introduces the Visual Autoregressive (VAR) modeling paradigm, the core method this paper improves upon."}, {"fullname_first_author": "Jinze Bai", "paper_title": "Qwen technical report", "publication_date": "2023-09-16", "reason": "This is a major foundational large language model that is frequently cited as a state-of-the-art achievement, showing the importance of large model scalability to the field."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Llama 2: Open foundation and fine-tuned chat models", "publication_date": "2023-07-09", "reason": "This is another frequently cited state-of-the-art large language model, further supporting the importance of scalable large models."}, {"fullname_first_author": "Haopeng Li", "paper_title": "Scalable autoregressive image generation with mamba", "publication_date": "2024-08-12", "reason": "This paper is a recent, important contribution to the field of autoregressive image generation, providing a close comparison point to the current work."}, {"fullname_first_author": "Yao Teng", "paper_title": "Accelerating autoregressive text-to-image generation with training-free speculative jacobi decoding", "publication_date": "2024-10-01", "reason": "This paper offers another recent, related approach to increasing efficiency in autoregressive image generation, giving a useful comparison point."}]}