[{"content": "| Method | #Steps | Speedup\u2191 | Latency\u2193 | Throughput\u2191 | #Param | Memory\u2193 | FID\u2193 | IS\u2191 | Precision\u2191 | Recall\u2191 |\n|---|---|---|---|---|---|---|---|---|---|---|\n| DiT-XL/2 | 50 | 0.2x | 19.20s | 3.33it/s | 675M | 11369MB | 2.26 | 239 | 0.80 | 0.60 |\n| MAR-B | 100 | 0.1x | 29.80s | 2.15it/s | 208M | 8725MB | 2.31 | 282 | 0.82 | 0.57 |\n| AiM-XL | 256 | 0.4x | 9.32s | 6.87it/s | 763M | 20983MB | 2.56 | 257 | 0.82 | 0.57 |\n| LlamaGen-XXL | 384 | <0.1x | 73.97s | 0.87it/s | 1.4B | 42632MB | 2.34 | 254 | 0.80 | 0.59 |\n| VAR-d30 | 10 | 1.0x | 3.62s | 17.71it/s | 2.0B | 39228MB | 1.95 | 301 | 0.81 | 0.59 |\n| VAR-d24 | 10 | 1.7x | 2.07s | 30.92it/s | 1.0B | 25093MB | 2.11 | 311 | 0.82 | 0.59 |\n| VAR-d20 | 10 | 2.8x | 1.29s | 49.62it/s | 600M | 17814MB | 2.61 | 301 | 0.83 | 0.56 |\n| **VAR-CoDe N=9** | 9+1 | 1.2x | 2.97s | 21.54it/s | 2.0+0.3B | 28803MB | 1.94 | 296 | 0.80 | 0.61 |\n| **VAR-CoDe N=8** | 8+2 | 1.7x | 2.11s | 30.33it/s | 2.0+0.3B | 21019MB | 1.98 | 302 | 0.81 | 0.60 |\n| **VAR-CoDe N=7** | 7+3 | 2.3x | 1.60s | 40.00it/s | 2.0+0.3B | 19943MB | 2.11 | 303 | 0.82 | 0.59 |\n| **VAR-CoDe N=6** | 6+4 | 2.9x | 1.27s | 50.39it/s | 2.0+0.3B | 19943MB | 2.27 | 297 | 0.82 | 0.58 |", "caption": "Table 1: Quantitative assessment of the efficiency-quality trade-off across various methods. Inference efficiency is evaluated with a batch size of 64 on NVIDIA L20 GPU, with latency measured excluding VQVAE or VQGAN as it incurs a shared time cost across all methods.", "description": "This table presents a comparison of different image generation methods based on their efficiency and image quality.  Efficiency is measured by speedup (relative to a baseline method), latency (inference time), and throughput (images generated per second). Image quality is assessed using FID (Fr\u00e9chet Inception Distance), Inception Score (IS), precision, and recall. The experiments were conducted with a batch size of 64 on an NVIDIA L20 GPU, and the latency measurements exclude the time taken by VQGAN or VQVAE decoders as these are shared across all methods.", "section": "4.1 Experimental Results"}, {"content": "| Discription | N=6 | N=7 | N=8 | N=9 |\n|---|---|---|---|---|\n| CoDe Training-free | 2.42 | 2.26 | 2.10 | 1.99 |\n| CoDe Fine-tuning | 2.27 | 2.11 | 1.98 | 1.94 |", "caption": "Table 2: Effect of specialized fine-tuning", "description": "This table shows the impact of specialized fine-tuning on the Collaborative Decoding (CoDe) method.  It presents FID scores and speedup factors for different numbers of drafting steps (N) in the CoDe framework.  The \"Training-free\" row indicates the performance without any fine-tuning, showing the performance baseline.  The other rows illustrate the improvements achieved through fine-tuning the model.  This allows for a comparison of the performance trade-offs between training-free operation and the benefits of specialized fine-tuning.", "section": "3.3. Collaborative Decoding"}, {"content": "| Method | Running | KV Cache | Params | Total |\n|---|---|---|---|---|\n| VAR (bs=8) | 314MB | 3595MB | **8089MB** | 12002MB |\n| +CoDe | **284MB** | **1023MB** | 9275MB | **10619MB** |\n| VAR (bs=16) | 615MB | 7191MB | **8089MB** | 15901MB |\n| +CoDe | **557MB** | **2056MB** | 9275MB | **11951MB** |\n| VAR (bs=32) | 1216MB | 14345MB | **8089MB** | 23662MB |\n| +CoDe | **1103MB** | **4083MB** | 9275MB | **14614MB** |\n| VAR (bs=64) | 2420MB | 28707MB | **8089MB** | 39228MB |\n| +CoDe | **2195MB** | **8160MB** | 9275MB | **19943MB** |\n| VAR (bs=128) | OOM(0.48GB) | OOM(57GB) | OOM(0.80GB) | OOM(70GB) |\n| +CoDe | **4380MB** | **16320MB** | 9275MB | **30598MB** |", "caption": "Table 3: Memory usage comparison across different batch sizes", "description": "This table compares the memory usage of the original VAR model and the proposed CoDe model across different batch sizes (8, 16, 32, 64, and 128).  For each batch size, it breaks down the memory consumption into three categories: running memory (memory used during the model's execution), KV cache (memory used for storing key-value pairs in the attention mechanism), and model parameters. The total memory usage is also shown. The table demonstrates how CoDe significantly reduces memory usage compared to the original VAR model, especially for larger batch sizes.  Out of memory (OOM) errors occur for the original VAR at batch size 128, which are avoided by CoDe.", "section": "4. Experimental Results"}, {"content": "Scale|Params|FID \u2193|IS \u2191|Precision \u2191|Recall \u2191\n---|---|---|---|---:|:---:\n2|0.3B|2.23|291|0.8122|0.5895\n2|0.6B|2.13|292|0.8078|0.5947\n2|1.0B|2.04|295|0.8107|0.6027\n2|2.0B|1.95|301|0.8107|0.5945\n3|0.3B|2.35|283|0.8064|0.5864\n3|0.6B|2.21|290|0.8047|0.5967\n3|1.0B|2.09|295|0.8074|0.5940\n3|2.0B|1.95|301|0.8107|0.5945\n4|0.3B|2.27|290|0.8086|0.5953\n4|0.6B|2.18|293|0.8068|0.5924\n4|1.0B|2.13|296|0.8061|0.5983\n4|2.0B|1.95|301|0.8107|0.5945\n5|0.3B|2.17|296|0.8119|0.5936\n5|0.6B|2.13|298|0.8087|0.5948\n5|1.0B|2.10|301|0.8087|0.6025\n5|2.0B|1.95|301|0.8107|0.5945\n6|0.3B|2.09|301|0.8119|0.5984\n6|0.6B|2.05|304|0.8100|0.5976\n6|1.0B|2.05|305|0.8089|0.5999\n6|2.0B|1.95|301|0.8107|0.5945\n7|0.3B|2.09|302|0.8067|0.6010\n7|0.6B|2.05|305|0.5095|0.6061\n7|1.0B|2.04|307|0.8077|0.6008\n7|2.0B|1.95|301|0.8107|0.5945\n8|0.3B|2.08|304|0.8135|0.5978\n8|0.6B|2.04|308|0.8110|0.6024\n8|1.0B|2.02|307|0.8094|0.6038\n8|2.0B|1.95|301|0.8107|0.5945\n9|0.3B|2.02|304|0.8133|0.6059\n9|0.6B|2.01|307|0.8121|0.5948\n9|1.0B|2.00|307|0.8097|0.6011\n9|2.0B|1.95|301|0.8107|0.5945\n10|0.3B|1.99|306|0.8120|0.5978\n10|0.6B|1.97|305|0.8102|0.6053\n10|1.0B|1.98|303|0.8102|0.6053\n10|2.0B|1.95|301|0.8107|0.5945", "caption": "Table 4: Impact of increasing parameters across scales", "description": "This table shows the impact of increasing model parameters at different scales on image generation quality.  The experiment predicts tokens for a single scale using models of varying sizes (0.3B, 0.6B, 1.0B, and 2.0B parameters), while using the largest model (2B) for all other scales.  The results demonstrate that increasing parameters significantly improves quality at smaller scales, but this improvement diminishes as the scale increases; at the final scale, the largest model shows little advantage over the smallest.", "section": "3.2 Key Observations"}, {"content": "| Method | #Steps | Speedup \u2191 | Latency \u2193 | Throughput \u2191 | #Param | Memory \u2193 | MUSIQ \u2191 | CLIPIQA \u2191 | NIQE \u2193 |\n|---|---|---|---|---|---|---|---|---|---| \n| VAR-d30 | 10 | 1.0x | 3.62s | 17.71it/s | 2.0B | 40414MB | 60.72 | 0.6813 | 6.1739 |\n| **VAR-CoDe N=9** | 9+1 | 1.2x | 2.97s | 21.54it/s | 2.0+0.3B | 28803MB | 60.78 | 0.6818 | 6.1024 |\n| **VAR-CoDe N=8** | 8+2 | 1.7x | 2.11s | 30.33it/s | 2.0+0.3B | 21019MB | 60.79 | 0.6812 | 6.0849 |\n| **VAR-CoDe N=7** | 7+3 | 2.3x | 1.60s | 40.00it/s | 2.0+0.3B | 19943MB | 60.82 | 0.6800 | 6.1247 |\n| **VAR-CoDe N=6** | 6+4 | 2.9x | 1.27s | 50.39it/s | 2.0+0.3B | 19943MB | 60.76 | 0.6808 | 6.1490 |", "caption": "Table 5: No reference metrics for additional image quality assessments.", "description": "This table presents a comparison of image quality metrics for different models.  It assesses the performance of the original VAR-d30 model and several variants of CoDe (Collaborative Decoding) with varying numbers of drafting steps (N).  The metrics used are not reference-based (meaning they don't compare to a ground truth image), instead relying on metrics that evaluate the image quality itself. The metrics included are MUSIQ, CLIPIQA, and NIQE, each offering a different perspective on image quality, providing a more comprehensive assessment than FID and IS alone. The table shows how different models trade off efficiency (speedup, throughput) with various image quality aspects.", "section": "4.2. Main Results"}, {"content": "| Configuration | FID \u2193 | IS \u2191 | Precision \u2191 | Recall \u2191 |\n|---|---|---|---|---|\n| **CoDe N=9** | 1.99 | 306 | 0.8120 | 0.5978 |\n| **CoDe N=8** | 2.10 | 308 | 0.8155 | 0.5915 |\n| **CoDe N=7** | 2.25 | 309 | 0.8204 | 0.5781 |\n| **CoDe N=6** | 2.42 | 306 | 0.8283 | 0.5721 |\n| **CoDe N=5** | 2.56 | 303 | 0.8313 | 0.5660 |\n| **CoDe N=4** | 2.75 | 295 | 0.8342 | 0.5427 |\n| **CoDe N=3** | 2.99 | 288 | 0.8410 | 0.5327 |\n| **CoDe N=2** | 3.19 | 283 | 0.8433 | 0.5179 |\n| **CoDe N=1** | 3.39 | 268 | 0.8132 | 0.5382 |", "caption": "Table 6: The training-free performance of CoDe", "description": "This table presents the performance of the Collaborative Decoding (CoDe) method without any additional training.  It shows the FID (Fr\u00e9chet Inception Distance), Inception Score (IS), Precision, and Recall metrics for different numbers of drafting steps (N) in CoDe.  Lower FID scores indicate better image quality. Higher IS, precision, and recall values suggest improved image generation performance.", "section": "3.3. Collaborative Decoding"}]