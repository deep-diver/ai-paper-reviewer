[{"figure_path": "https://arxiv.org/html/2502.06635/x1.png", "caption": "Figure 1: The architecture of Steel-LLM Transformer block", "description": "This figure illustrates the architecture of a Transformer block within the Steel-LLM model.  It shows the flow of information through the self-attention mechanism and the enhanced feed-forward network (FFN). The self-attention module processes input tokens, and the FFN, enhanced with Soft Mixture of Experts (Soft MOE), further transforms the results.  The diagram details the components within the FFN, including MLP layers and the Swish activation function, and highlights the use of Soft MOE for efficient processing of large numbers of tokens.", "section": "3 ARCHITECTURE"}, {"figure_path": "https://arxiv.org/html/2502.06635/x2.png", "caption": "Figure 2: Pretraining Data Distribution", "description": "This figure shows the distribution of the pretraining data across different sources and languages. The data primarily consists of Chinese text (67%), drawn from various sources such as SkyPile (33%), WanJuan (32.3%), and Wikipedia-cn. A smaller portion (21.8%) is English data from sources like WanJuan and Code. A small fraction (1.6%) is Chat Data, and another 11.3% consists of code data from Starcode.", "section": "5 PRETRAINING"}, {"figure_path": "https://arxiv.org/html/2502.06635/x3.png", "caption": "Figure 3: Pre-training loss curve for Steel-LLM", "description": "This figure shows the loss curve during the pre-training phase of the Steel-LLM model. The x-axis represents the number of tokens processed, and the y-axis represents the training loss.  The curve displays a steep initial drop in loss, followed by a gradual decrease as training progresses.  The figure highlights that the first 200,000 training steps used NVIDIA A100-80G GPUs, while the remaining steps used NVIDIA H800-80G GPUs, potentially explaining any noticeable changes in the curve's slope. This visualization provides insight into the training process's stability and efficiency.", "section": "4 Training Framework"}, {"figure_path": "https://arxiv.org/html/2502.06635/x4.png", "caption": "Figure 4: Supervised Fine-tuning loss curve for Steel-LLM", "description": "This figure shows the training loss curve during the supervised fine-tuning stage of the Steel-LLM model. The x-axis represents the training steps, while the y-axis represents the loss value. The curve illustrates how the model's performance improved as the training progressed. The consistent downward trend of the loss curve demonstrates the model's learning and optimization during the fine-tuning phase.  It helps visualize the model's convergence and efficiency of the fine-tuning process.", "section": "6 Post Training"}, {"figure_path": "https://arxiv.org/html/2502.06635/x5.png", "caption": "Figure 5: Direct Preference Optimization loss curve for Steel-LLM", "description": "This figure shows the loss curve during the Direct Preference Optimization (DPO) process for the Steel-LLM model. The x-axis represents the training steps, and the y-axis shows the loss value. The curve illustrates how the loss decreased over the training process, indicating that the model's performance improved as it learned to align with human preferences.  A lower loss indicates a better alignment of the model's output with preferred responses.", "section": "6.3 LEARNING FROM HUMAN PREFERENCES"}]