{"references": [{"fullname_first_author": "Vaswani, A.", "paper_title": "Attention is all you need", "publication_date": "2017-12-01", "reason": "This paper introduced the Transformer architecture, which is fundamental to the large language models (LLMs) discussed in the audited prompt caching study."}, {"fullname_first_author": "Zheng, L.", "paper_title": "SGLang: Efficient execution of structured language model programs", "publication_date": "2024-12-01", "reason": "This paper proposed a prompt caching method relevant to the study's focus on LLM prompt caching optimizations."}, {"fullname_first_author": "Gim, I.", "paper_title": "Prompt cache: Modular attention reuse for low-latency inference", "publication_date": "2024-12-01", "reason": "This paper presented another key prompt caching optimization technique that directly impacts the timing variations investigated in the audit."}, {"fullname_first_author": "Kocher, P.", "paper_title": "Spectre attacks: Exploiting speculative execution", "publication_date": "2019-05-01", "reason": "This work is seminal in the field of side-channel attacks, providing a crucial theoretical foundation for understanding the timing attack methodology used to detect prompt caching in the study."}, {"fullname_first_author": "Bernstein, D. J.", "paper_title": "Cache-timing attacks on AES", "publication_date": "2005-04-14", "reason": "This paper is foundational to the understanding of cache-timing attacks, a key technique employed in this research to assess the vulnerability of prompt caching mechanisms in LLMs."}]}