[{"figure_path": "https://arxiv.org/html/2501.12326/extracted/6146349/figures/case-1-v8.png", "caption": "Figure 1: A demo case of UI-TARS that helps user to find flights.", "description": "This figure showcases a step-by-step demonstration of UI-TARS, a native GUI agent, performing a complex flight search task.  It highlights the agent's capabilities, including understanding natural language instructions, interacting with various GUI elements (e.g., text fields, dropdowns, buttons, calendars), handling unexpected situations (like error messages), employing iterative reasoning, and ultimately achieving the user's goal.  Each step shows screenshots of the GUI alongside the agent's \"thoughts\" and actions. The process reveals UI-TARS's ability to decompose the task into smaller subtasks, employ reflective thinking to adjust its strategy based on the context, and continuously learn from both successes and failures.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2501.12326/extracted/6146349/figures/5-stage.png", "caption": "Figure 2: The evolution path for GUI agents.", "description": "This figure illustrates the evolution of Graphical User Interface (GUI) agents over time, categorized into four stages based on their level of autonomy and reliance on human intervention.  It visually represents the shift from simple rule-based agents to more complex, adaptable models leveraging large language models (LLMs) and machine learning. The figure shows the increase in generalization capabilities and the decrease in human oversight needed as GUI agents evolve.", "section": "2 Evolution Path of GUI Agents"}, {"figure_path": "https://arxiv.org/html/2501.12326/extracted/6146349/figures/core.png", "caption": "Figure 3: An overview of core capabilities and evaluation for GUI agents.", "description": "This figure provides a comprehensive overview of the core capabilities and evaluation metrics for GUI agents.  It illustrates the interconnectedness of four key aspects: perception (how the agent understands the GUI), action (how the agent interacts with the GUI), reasoning (the agent's decision-making process, encompassing both fast, intuitive System 1 thinking and slower, deliberate System 2 thinking), and memory (short-term and long-term storage for information).  Different evaluation methods are also shown, categorized by offline versus online evaluation and further separated into metrics that evaluate each core capability (perception, grounding, and agent capabilities). This visualization helps clarify how these components work together and how their performance is measured.", "section": "2 Evolution Path of GUI Agents"}, {"figure_path": "https://arxiv.org/html/2501.12326/extracted/6146349/figures/model_arc.png", "caption": "Figure 4: Overview of UI-TARS. We illustrate the architecture of the model and its core capabilities.", "description": "Figure 4 provides a detailed illustration of UI-TARS' architecture and its core components. It visually depicts the model's interaction with the environment, showing how it receives user queries, processes observations from screenshots, generates thoughts (reasoning steps), selects actions, and then receives feedback from the environment to further improve its performance.  Key capabilities like perception (enhanced GUI understanding from screenshots), action (standardized across different platforms), System-2 Reasoning (incorporating deliberate reasoning into multi-step decisions), and learning from prior experiences (through online trace bootstrapping and reflection tuning) are integrated within the architecture. This visualization helps to understand the iterative, data-driven nature of UI-TARS and how its components work together to enable effective GUI interaction.", "section": "4 UI-TARS"}, {"figure_path": "https://arxiv.org/html/2501.12326/x1.png", "caption": "Figure 5: Data example of perception and grounding data.", "description": "Figure 5 presents example data used for perception and grounding tasks in the UI-TARS model. It illustrates five key aspects of the model's perception capabilities: (1) Element Description: provides detailed descriptions of GUI elements including type, visual appearance, location, and function. (2) Task Grounding: demonstrates how the model accurately locates and interacts with GUI elements based on provided queries. (3) Question Answering (QA): shows the model's ability to answer questions about GUI elements and their contextual relationships. (4) Set-of-Mark Perception: showcases the model's capacity to locate and identify elements using visual markers for improved accuracy. (5) Dense Captioning: illustrates the model's capability to generate detailed captions describing the entire GUI layout, including spatial relationships and interactions between elements. Each aspect is demonstrated using specific examples from different GUI screenshots, highlighting the model's precise understanding and interaction with various GUI elements.", "section": "4.2 Enhancing GUI Perception"}, {"figure_path": "https://arxiv.org/html/2501.12326/extracted/6146349/figures/thought_pattern.png", "caption": "Table 1: Unified action space for different platforms.", "description": "This table presents a unified action space designed to standardize actions across various platforms (web, mobile, desktop).  Instead of platform-specific actions, it uses a common set of operations such as 'click', 'type', 'scroll', and 'drag'.  Additionally, it includes actions from language agents (like API calls), improving agent versatility.  The actions are categorized into atomic (single operations) and compositional (sequences of actions). This standardized approach enhances the transferability and efficiency of actions across different GUI environments.", "section": "4.3 Unified Action Modeling and Grounding"}, {"figure_path": "https://arxiv.org/html/2501.12326/extracted/6146349/figures/online_bootstrap.png", "caption": "Table 2: Basic statistics for grounding and multi-step action trace data, comparing both our annotated dataset and open-source data across different platforms (web, mobile, and desktop). We report the number of elements (Ele.) and the number of action traces (Trace).", "description": "This table presents a quantitative comparison of the datasets used for training UI-TARS.  It contrasts data gathered from two main sources: an annotated dataset created by the researchers, and a collection of open-source datasets. The comparison highlights differences across various platforms (web, mobile, and desktop) and focuses on two key metrics:\n\n* **Number of elements (Ele.):** Represents the total count of distinct GUI elements identified and annotated within each dataset. This metric reflects the richness and diversity of GUI elements covered in each source.\n* **Number of action traces (Trace):**  Indicates the quantity of sequential interactions logged, capturing the steps involved in completing tasks in the respective datasets. This metric is crucial for evaluating the model's ability to learn and perform multi-step operations.\n\nBy breaking down the dataset characteristics by platform, the table reveals potential variations in data distribution and the complexity of GUI elements present. This information is essential for understanding the strengths and limitations of the training data and assessing its impact on the model's performance across different GUI types.", "section": "4 UI-TARS"}, {"figure_path": "https://arxiv.org/html/2501.12326/x2.png", "caption": "Figure 6: Various reasoning patterns in our augmented thought.", "description": "This figure showcases various reasoning patterns employed by the UI-TARS model. Each example demonstrates a different type of reasoning, such as task decomposition, long-term consistency, milestone recognition, trial and error, and reflection.  The screenshots illustrate how these reasoning processes manifest in the model's 'thoughts', which are intermediate steps generated before executing an action.  These 'thoughts' provide insight into the model's decision-making process, showing how it breaks down complex tasks, maintains consistency across multiple steps, identifies milestones, learns from mistakes, and reflects on its performance.", "section": "4.4 Infusing System-2 Reasoning"}, {"figure_path": "https://arxiv.org/html/2501.12326/x3.png", "caption": "Figure 7: Overview of the online bootstrapping process.", "description": "This figure illustrates the iterative process of online data bootstrapping used to enhance UI-TARS's learning. It starts with a set of initial task instructions. These are then used to generate interaction traces using the current UI-TARS model on virtual machines.  The raw traces go through a multi-step filtering process involving rule-based reward functions, VLM scoring, and human review, producing high-quality filtered traces. These filtered traces are then used to fine-tune the model, creating an improved version.  New instructions are generated, and the process repeats, iteratively refining the data and improving the model's performance. The process also includes a reflection tuning step, where errors identified by the model (or humans) are used to create improved interaction traces, further enhancing the model's ability to learn from mistakes.", "section": "4.5 Learning from Prior Experience in Long-term Memory"}, {"figure_path": "https://arxiv.org/html/2501.12326/x4.png", "caption": "Figure 8: Performance of system-1 (no-thought) and system-2 (with thought) in in-domain (Mind2Web, AndroidControl, GUI Odyssey) and out-of-domain (AndroidWorld) benchmarks.", "description": "Figure 8 presents a comparison of the performance of System-1 (intuition-based) and System-2 (deliberative) reasoning models in both in-domain and out-of-domain GUI benchmarks.  The in-domain benchmarks (Mind2Web, AndroidControl, GUI Odyssey) represent tasks the models were trained on, while the out-of-domain benchmark (AndroidWorld) tests generalization capabilities on unseen tasks.  The graph displays the success rates (or other relevant metrics) for each model across the different benchmarks at various sample sizes (BoN values of 1, 16, and 64).  This allows for an analysis of how the reasoning method (System-1 vs. System-2) and sample size affect performance, revealing the trade-offs between speed (intuition) and accuracy (deliberation).", "section": "5.5 Comparing System 1 and System 2 Reasoning"}, {"figure_path": "https://arxiv.org/html/2501.12326/x5.png", "caption": "Figure 9: Test case on Ubuntu impress scene from UI-TARS. The task is: Make the background color of slide 2 same as the color of the title from slide 1.", "description": "This figure showcases a demonstration of UI-TARS, a native GUI agent, performing a task within the Ubuntu Impress software.  The specific task is to modify the background color of slide 2 to match the color of the title on slide 1. The figure presents a sequence of screenshots illustrating the steps UI-TARS takes to accomplish this, including selecting the relevant slide, accessing color settings, and choosing the appropriate color.  Each screenshot is accompanied by UI-TARS's internal thought process, demonstrating its reasoning and decision-making capabilities throughout the task.", "section": "4 UI-TARS"}, {"figure_path": "https://arxiv.org/html/2501.12326/x6.png", "caption": "Figure 10: Test case on Android from UI-TARS. The task is: Play the song under Mount Fuji.", "description": "This figure showcases a demonstration of UI-TARS's capabilities on an Android device.  The user task is to play a specific song, \"Under Mount Fuji (\u5bcc\u58eb\u5c71\u4e0b)\". The figure visually depicts a step-by-step sequence of UI interactions made by UI-TARS.  Each step is shown with a screenshot, indicating the actions performed, along with the agent's accompanying thought process at that step. The visual progression demonstrates the agent's ability to locate and interact with the relevant UI elements (e.g., the music app icon, search bar) to successfully complete the task, which involves locating the song within the music player app and starting playback.", "section": "4 UI-TARS"}, {"figure_path": "https://arxiv.org/html/2501.12326/x7.png", "caption": "Figure 11: Test case on Ubuntu VSCode scene from UI-TARS. The task is: Please help me install the autoDocstring extension in VS Code.", "description": "This figure showcases a demonstration of UI-TARS, a native GUI agent, performing the task of installing the 'autoDocstring' extension in the VS Code editor.  The figure is a sequence of screenshots capturing UI-TARS's interaction with the VS Code interface, showing the steps taken, including launching VS Code, navigating to the extensions panel, typing the extension name, clicking to install it, handling an error, and finally confirming the successful installation. Each screenshot is accompanied by a description of UI-TARS's reasoning process (its internal 'thoughts' shown as text boxes) explaining the rationale behind each action. This visualization provides insights into how UI-TARS processes visual information, reasons, and interacts with the GUI to achieve a given task, highlighting its functionality as a native agent without reliance on pre-programmed workflows or human intervention.", "section": "A Case Study"}, {"figure_path": "https://arxiv.org/html/2501.12326/x8.png", "caption": "Figure 12: Test case on Windows chrome scene from UI-TARS. The task is: I want to show bookmarks bar by default in chrome.", "description": "This figure showcases a test case demonstrating UI-TARS's ability to interact with a Chrome browser on a Windows system. The goal is to configure Chrome settings to display the bookmarks bar by default.  The figure visually depicts a step-by-step sequence of UI-TARS's actions: launching Chrome, accessing settings through the three-dot menu, navigating to the Appearance settings, and finally, activating the 'Show bookmarks bar' toggle switch. Each step is accompanied by UI-TARS's corresponding 'thought' process, revealing its internal decision-making logic, as well as the precise coordinates of each mouse click action (start_box). This detailed visualization effectively demonstrates UI-TARS's functionality, including its capability to perceive UI elements accurately, execute multi-step sequences of actions, and apply reasoning to complete the task successfully.", "section": "A Case Study"}, {"figure_path": "https://arxiv.org/html/2501.12326/x9.png", "caption": "Figure 13: Dense caption example.", "description": "The figure shows an example of dense captioning from the UI-TARS model.  The input is a screenshot of a webpage showing a TSB Bank advertisement for a cashback offer on a current account.  The output is a detailed description of the webpage's layout and elements, including the navigation bar, the main advertisement, supporting images and text, and several icons linking to other services. The caption provides very detailed information about each element's visual appearance, location, and function, demonstrating the model's ability to understand a complex GUI interface at a granular level.", "section": "4.2 Enhancing GUI Perception"}]