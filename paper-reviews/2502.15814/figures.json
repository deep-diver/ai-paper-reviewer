[{"figure_path": "https://arxiv.org/html/2502.15814/extracted/6217510/media/teaser.png", "caption": "Figure 1: Comparing Topic-StoryCloze performance of different SLMs as a function of training compute. Model size is indicated by the size of the circle.", "description": "This figure compares the Topic-StoryCloze performance of various Speech Language Models (SLMs) against the amount of computational resources (measured in FLOPs) used for training.  The size of each circle in the plot corresponds to the size of the respective SLM, providing a visual representation of the model's scale. The plot shows that even with increased compute, performance gains level off and there are significant differences between SLMs.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2502.15814/extracted/6217510/media/twist_vs_gslm.png", "caption": "Figure 2: Comparing PPL of different models of similar parameter count, with and without TWIST initialisation.", "description": "Figure 2 illustrates the impact of TWIST initialization on the performance of various speech language models (SLMs) with similar parameter counts.  It compares the validation perplexity (PPL), a measure of how well a model predicts unseen data, for models trained both with and without TWIST initialization. The x-axis represents the estimated FLOPs (floating-point operations), reflecting the computational cost of training, while the y-axis shows the validation PPL.  The figure helps to demonstrate whether initializing SLMs with pre-trained text Language Models (like TWIST) improves performance, especially in a computationally constrained setting.", "section": "4.1 Model & Optimisation"}, {"figure_path": "https://arxiv.org/html/2502.15814/extracted/6217510/media/twist_models.png", "caption": "Figure 3: Comparing PPL of different models under TWIST initialisation.", "description": "This figure compares the validation perplexity (PPL) scores of various speech language models (SLMs) of different sizes and architectures when initialized using the TWIST method.  It demonstrates the impact of model size and architecture on the PPL, showcasing the performance differences between models with and without TWIST initialization under a fixed computational budget. The x-axis represents the estimated FLOPs (floating-point operations), which correlates with the model's computational cost. The y-axis shows the validation perplexity. The graph illustrates that TWIST initialization provides benefits in terms of reduced perplexity.", "section": "4.1 Model & Optimisation"}, {"figure_path": "https://arxiv.org/html/2502.15814/extracted/6217510/media/optim_analysis.png", "caption": "Figure 4: Comparing validation PPL of our best model with different optimisers and schedulers.", "description": "This figure compares the validation perplexity (PPL) achieved by the best-performing speech language model (SLM) when trained using different optimization algorithms (AdamW, AdaLomo, AdEMAMix) and learning rate schedules (cosine decay, inverse square root).  The goal is to determine the combination that yields the lowest validation perplexity, indicating optimal model performance within the resource constraints of a single GPU.", "section": "4.1 Model & Optimisation"}, {"figure_path": "https://arxiv.org/html/2502.15814/extracted/6217510/media/dpo_slider.png", "caption": "Figure 5: Analysing the optimal part of the 24 hour compute budget that should be used for DPO, with the rest used for pre-training.", "description": "This figure analyzes the optimal allocation of a 24-hour compute budget between pre-training and Direct Preference Optimization (DPO) for training a speech language model.  It shows how different durations dedicated to DPO affect the model's performance, measured by various metrics.  The results help determine the most efficient balance between pre-training and DPO to maximize model performance within the time constraint.", "section": "4.2 Data"}]