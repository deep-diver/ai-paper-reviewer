{"importance": "This paper is crucial for researchers in AI and deep learning due to its **comprehensive survey** of test-time computing scaling, a rapidly evolving field. Its **framework** for understanding the transition from System-1 to System-2 thinking models opens up **new research avenues**, particularly in the area of multimodal reasoning and efficient scaling strategies for LLMs.  By highlighting the **limitations** of current approaches and proposing future directions, it guides researchers towards more robust and efficient AI systems. The findings are highly relevant to ongoing efforts to improve the reasoning capabilities and generalization of large language models.", "summary": "Unlocking LLM potential: This paper surveys test-time computing, showing how it boosts reasoning abilities by shifting from reactive System-1 to deliberate System-2 thinking, paving the way for more powerful AI.", "takeaways": ["Test-time computing enhances both System-1 (improving robustness) and System-2 (enhancing reasoning) thinking in LLMs.", "The transition from System-1 to System-2 thinking is facilitated by several test-time computing strategies, including parameter updates, input modification, and output calibration.", "Future research should focus on generalizing System-2 models, exploring multimodal reasoning, and developing efficient scaling strategies."], "tldr": "Large language models (LLMs) have shown remarkable progress, yet they still face limitations in robustness and complex reasoning.  This paper explores **test-time computing**, a technique that enhances model performance by increasing computational effort during inference.  Early test-time computing methods focused on adapting System-1 models\u2014those that rely on pattern recognition\u2014to address issues like distribution shifts. However, the paper's focus is on advancing LLMs to exhibit System-2 thinking, which involves more deliberate and complex reasoning processes. \nThe paper organizes its survey according to the shift from System-1 to System-2 thinking. It details various test-time computing techniques for each type of model, including parameter updates, input modification, representation editing, and output calibration for System-1 models. For System-2 models, the paper highlights techniques like repeated sampling, self-correction, and tree search.  The study also identifies and discusses several **challenges** and **future research directions**, such as achieving generalizable System-2 models, efficient scaling strategies, and extending test-time computing to multimodal scenarios.", "affiliation": "Soochow University", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}, "podcast_path": "2501.02497/podcast.wav"}