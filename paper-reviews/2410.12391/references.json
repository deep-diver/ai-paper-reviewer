{"references": [{" publication_date": "2023", "fullname_first_author": "Trenton Bricken", "paper_title": "Towards monosemanticity: Decomposing language models with dictionary learning", "reason": "This paper introduces a novel approach to understanding language model features by treating them as functions assigning values to data points. This approach is central to the current research's methodology and forms the basis for analyzing feature evolution.  The study's use of sparse autoencoders, inspired by this work, enables efficient feature extraction and analysis, making it a crucial foundational reference.", "section_number": 1}, {" publication_date": "2015", "fullname_first_author": "Yixuan Li", "paper_title": "Convergent learning: Do different neural networks learn the same representations?", "reason": "This paper is foundational to the field of feature universality in language models.  It investigates whether different neural networks learn similar representations, a key theme in understanding the generalizability and robustness of language models' underlying features. Its findings on convergent learning directly relate to the current research's investigation of feature persistence across various models.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Bilal Chughtai", "paper_title": "A toy model of universality: Reverse engineering how networks learn group operations", "reason": "This paper contributes to the understanding of feature universality in language models.  Its focus on reverse-engineering how networks learn group operations provides insights into the underlying mechanisms that drive the convergence or divergence of features across different models. This is highly relevant to the current study's exploration of feature evolution.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Hoagy Cunningham", "paper_title": "Sparse autoencoders find highly interpretable features in language models", "reason": "This paper demonstrates the effectiveness of sparse autoencoders in extracting interpretable features from language models, a method that is central to the methodology of the current research. Its success in finding interpretable features is directly relevant to the current study's aim of analyzing and interpreting feature evolution across various models.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Adly Templeton", "paper_title": "Scaling monosemanticity: Extracting interpretable features from claude 3 sonnet", "reason": "This paper shows how to control the output of language models using extracted features and provides insights into the interpretability of these features, directly related to the current study's method of using autoencoders to study feature evolution. Its approach to controlling model output through feature manipulation enhances our understanding of the impact of features on model behavior.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Alex Warstadt", "paper_title": "Call for papers the babylm challenge: Sample-efficient pretraining on a developmentally plausible corpus", "reason": "This paper describes the BabyLM corpus used in the current research's base model.  It explains the methodology for creating this dataset, and it provides background information on its construction and properties.  Understanding the BabyLM corpus is crucial to interpreting the results and appreciating the design choices in the current study.", "section_number": 3}, {" publication_date": "2022", "fullname_first_author": "Denis Kocetkov", "paper_title": "The stack: 3 tb of permissively licensed source code", "reason": "This paper presents The Stack dataset, a significant portion of which (Python subset) was used as training data for the BabyPython model in the current research.  A clear understanding of the data source is critical for interpreting the results. This reference provides essential context and details about the training data composition.", "section_number": 3}, {" publication_date": "2020", "fullname_first_author": "Suchin Gururangan", "paper_title": "Don't stop pretraining: Adapt language models to domains and tasks", "reason": "This paper discusses methods for adapting language models to different domains and tasks, a crucial aspect of the current research's focus on transfer learning. The paper's exploration of fine-tuning techniques is highly relevant to the current study's methodology of fine-tuning language models on new domains (Lua and TinyStories).", "section_number": 3}, {" publication_date": "2022", "fullname_first_author": "Leshem Choshen", "paper_title": "Fusing finetuned models for better pretraining", "reason": "This paper introduces methods for merging fine-tuned language models, a central technique employed in the current research.  The concept of model merging through techniques like spherical linear interpolation is directly relevant to the current study's approach of combining fine-tuned models to create a merged model (LuaStories).", "section_number": 3}, {" publication_date": "2020", "fullname_first_author": "Marco Tulio Ribeiro", "paper_title": "Beyond accuracy: Behavioral testing of NLP models with CheckList", "reason": "This work emphasizes the importance of going beyond accuracy metrics to understand and evaluate the behavior of NLP models. This perspective is relevant to the current research's focus on interpreting model features and understanding how their behavior evolves during transfer learning.  The call for behavioral testing aligns with this study's focus on feature evolution and interpretability.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Ronen Eldan", "paper_title": "Tinystories: How small can language models be and still speak coherent english?", "reason": "This paper introduces the TinyStories dataset, which is used in the current research for fine-tuning.  The understanding of the dataset's properties and its suitability for the research objectives is crucial for interpreting the results and understanding the methodology.", "section_number": 3}, {" publication_date": "2021", "fullname_first_author": "Nelson Elhage", "paper_title": "A mathematical framework for transformer circuits", "reason": "This paper introduces a mathematical framework for understanding the inner workings of transformer networks. It offers a deeper theoretical understanding of the mechanisms underlying language model behavior, providing context for the current research's study of feature dynamics.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Leo Gao", "paper_title": "Scaling and evaluating sparse autoencoders", "reason": "This paper studies sparse autoencoders at a larger scale than the current work, providing insights into the scalability and limitations of this technique.  Understanding the challenges and capabilities of sparse autoencoders at scale provides important context for the current work's use of this method for feature extraction.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Charles Goddard", "paper_title": "Arcee's mergekit: A toolkit for merging large language models", "reason": "This paper introduces a toolkit for merging large language models which provides context and practical insights into model merging, which is a key component of the current research's methodology. Understanding the different methods and challenges in merging models helps to evaluate the current work's choice of SLERP for this purpose.", "section_number": 3}, {" publication_date": "2015", "fullname_first_author": "Diederik Kingma", "paper_title": "Adam: A method for stochastic optimization", "reason": "The Adam optimizer was used in the training of the sparse autoencoders.  Understanding the properties and performance of the Adam optimizer is critical for interpreting the results of the sparse autoencoder training and the quality of the extracted features.", "section_number": 3}, {" publication_date": "2020", "fullname_first_author": "Leo Gao", "paper_title": "The Pile: An 800gb dataset of diverse text for language modeling", "reason": "While not directly used in the current study, this paper provides context regarding the size and diversity of language modeling datasets.  The comparison to this large dataset helps contextualize the limitations of the smaller datasets used in the present research and aids in understanding the generalizability of the findings.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Minyoung Huh", "paper_title": "The platonic representation hypothesis", "reason": "This paper relates to the theoretical underpinnings of feature universality and sheds light on the reasons why models might converge on similar representations. It helps put the empirical findings of the current work into a broader theoretical context. It provides a complementary perspective on the topic of feature convergence and universality.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Mitchell Wortsman", "paper_title": "Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time", "reason": "This paper investigates the impact of model averaging, a related approach to model merging.  It provides context for the current research's choice of model merging through SLERP, as both are techniques that aim to create a model which incorporates the strengths of several individual models.", "section_number": 3}, {" publication_date": "1985", "fullname_first_author": "Ken Shoemake", "paper_title": "Animating rotation with quaternion curves", "reason": "This paper introduces spherical linear interpolation (SLERP), a technique used in the current research for merging fine-tuned models. It provides the mathematical foundation for the merging method and is essential to understanding how the merged model was generated.", "section_number": 3}]}