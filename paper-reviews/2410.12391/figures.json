[{"figure_path": "2410.12391/figures/figures_2_0.png", "caption": "Figure 1: Overview of the experimental design. We start with a base model trained on BabyLM and Python code (1), which is fine-tuned (FT) on two new domains: the Lua programming language (2), and TinyStories (3). The fine-tuned models are merged into a single LuaStories model using spherical linear interpolation (SLERP) interpolation (4). For each of these models, we train a sparse auto-encoder on the MLP activations using the same data distribution as the original model.", "description": "This figure shows the experimental design, starting from a base model trained on BabyLM and Python, then fine-tuned on Lua and TinyStories, and finally merged using SLERP, with sparse autoencoders trained for each model.", "section": "3 Methodology"}, {"figure_path": "2410.12391/figures/figures_4_0.png", "caption": "Figure 3: Visualisation of the feature activation patterns of the universally extracted variable assignment features found in each model. Each token is highlighted according to the feature's activation level, where darker background colour denotes higher level of activation. Additionally, we note the observed activation pattern correlations between each feature.", "description": "Figure 3 visualizes the feature activation patterns of the variable assignment feature across four language models, highlighting the correlation between the models' features.", "section": "4.2 Feature Flow Case Studies"}, {"figure_path": "2410.12391/figures/figures_4_1.png", "caption": "Figure 4: Examples of observed activation patterns of the BabyPython Python exception feature, and the closest matching feature in the Lua model, qualitatively showing insufficient correlation between the two.", "description": "Figure 4 shows examples of observed activation patterns of the BabyPython Python exception feature and its closest match in the Lua model, highlighting insufficient correlation between them.", "section": "4.2 Feature Flow Case Studies"}]