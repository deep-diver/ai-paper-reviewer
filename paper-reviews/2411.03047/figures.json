[{"figure_path": "https://arxiv.org/html/2411.03047/extracted/5979162/images/fig_teaser_1016_v6.png", "caption": "Figure 1. We propose a hierarchical framework to recover different levels of garment details by leveraging the garment shape and deformation priors from the GarVerseLOD dataset. Given a single clothed human image, our approach is capable of generating high-fidelity 3D standalone garment meshes that exhibit realistic deformation and are well-aligned with the input image. Original images courtesy of licensed photos and Stable Diffusion\u00a0(Rombach et\u00a0al., 2022). The images with a gray background are synthesized, while the rest are licensed photos.", "description": "Figure 1 showcases the GarVerseLOD dataset and the hierarchical framework for 3D garment reconstruction.  The framework leverages garment shape and deformation priors learned from the dataset to reconstruct high-fidelity 3D garment meshes from a single image.  The figure demonstrates the system's ability to handle various garment types and poses, producing realistic results that align well with the input images. Some images were sourced from licensed photos, while others were generated using Stable Diffusion. The gray background indicates synthetically generated images.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2411.03047/extracted/5979162/images/fig_dataset_pipeline3.png", "caption": "Figure 2. The pipeline of our novel strategy for constructing a progressive garment dataset with levels of details. (a) Each case shows the reference image and the artist-crafted T-pose coarse garment in Garment Style Database. (b) A example of the reference image and the artist-crafted detail-pair in Local Detail Database. (c) A example of the reference image and the artist-crafted deformation-pair in Garment Deformation Database. (d) To obtain an T-pose garment with geometric details, we first sample a shape MCsubscript\ud835\udc40\ud835\udc36M_{C}italic_M start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT from the Garment Style Database and a \u201cLocal Detail Pair\u201d (LCsubscript\ud835\udc3f\ud835\udc36L_{C}italic_L start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT, LFsubscript\ud835\udc3f\ud835\udc39L_{F}italic_L start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT) from the Local Detail Database. Then we transfer the geometric details depicted by (LCsubscript\ud835\udc3f\ud835\udc36L_{C}italic_L start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT, LFsubscript\ud835\udc3f\ud835\udc39L_{F}italic_L start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT) to MCsubscript\ud835\udc40\ud835\udc36M_{C}italic_M start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT to obtain MLsubscript\ud835\udc40\ud835\udc3fM_{L}italic_M start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT. (e) The deformation depicted by a sampled \u201cGarment Deformation Pair\u201d (DTsubscript\ud835\udc37\ud835\udc47D_{T}italic_D start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT, DFsubscript\ud835\udc37\ud835\udc39D_{F}italic_D start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT) is transferred to MLsubscript\ud835\udc40\ud835\udc3fM_{L}italic_M start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT to obtain the fine garment MDsubscript\ud835\udc40\ud835\udc37M_{D}italic_M start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPT, which contains fine-grained geometric details and complex deformations (Fine Garment Dataset). Original images courtesy of licensed photos.", "description": "This figure illustrates the process of creating a hierarchical garment dataset with levels of detail. It starts with three basic databases: Garment Style Database (containing T-pose coarse garments), Local Detail Database (pairs of T-pose garments with and without fine details), and Garment Deformation Database (pairs of T-pose and deformed garments). These databases are combined to create the Fine Garment Dataset, which contains garments with both fine details and complex deformations.  The process involves sampling shapes and deformations from the basic databases and transferring them to generate progressively more detailed garment models.", "section": "3 DATASET"}, {"figure_path": "https://arxiv.org/html/2411.03047/extracted/5979162/images/fig_dataset_gallery.png", "caption": "Figure 3. Left: Our novel strategy for generating extensive photorealistic paired images. We acquire rendered images of 3D garments with random camera views. These rendered images are processed through Canny-Conditional Stable Diffusion\u00a0(Rombach et\u00a0al., 2022; Mou et\u00a0al., 2023; Zhang et\u00a0al., 2023a) to produce photorealistic images. Right: (a) The garment sampled from Fine Garment Dataset; (b) The synthesized image; (c) The pixel-aligned mask; (d) The normal map rendered using (a); (e) The garment mask rendered by (a); (f) The counterpart T-pose coarse garment of (a). In Sec.\u00a04, (b, f) is used to train the coarse garment estimator, while (b,c,d) is adopted to train the normal estimator. (d, e, a) is utilized to train the fine garment estimator and the geometry-aware boundary predictor. Synthesized images courtesy of Stable Diffusion.", "description": "Figure 3 illustrates the process of generating photorealistic images of garments for training the model.  The left side shows the pipeline:  starting with textureless 3D garment renderings from various viewpoints, these are fed into Canny-Conditional Stable Diffusion to create photorealistic images with diverse appearances. The right side displays example results.  (a) shows a garment from the Fine Garment Dataset, (b) is the generated photorealistic image, (c) its corresponding pixel-aligned mask, (d) the normal map rendered from the 3D garment, (e) the garment mask from the 3D model, and (f) the corresponding T-pose coarse garment. Section 4 of the paper details how these images are used for training different parts of the model: (b, f) trains the coarse garment estimator; (b, c, d) trains the normal estimator; and (d, e, a) trains the fine garment estimator and geometry-aware boundary predictor.  All synthesized images were produced using Stable Diffusion.", "section": "3 DATASET"}, {"figure_path": "https://arxiv.org/html/2411.03047/extracted/5979162/images/fig_method2.png", "caption": "Figure 4. The pipeline of our proposed method. Given an RGB image, our method first estimates the T-pose garment shape G\u2062(\u03b1)\ud835\udc3a\ud835\udefcG({{\\alpha}})italic_G ( italic_\u03b1 ) (Eq.\u00a04) and computes its pose-related deformation MP\u2062(\u03b1,\u03b2,\u03b8)subscript\ud835\udc40\ud835\udc43\ud835\udefc\ud835\udefd\ud835\udf03M_{P}(\\alpha,\\beta,\\theta)italic_M start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT ( italic_\u03b1 , italic_\u03b2 , italic_\u03b8 ) with the help of the predicted SMPL body (Eq.\u00a07, Eq.\u00a010). Then a pixel-aligned network is used to reconstruct implicit fine garment MIsubscript\ud835\udc40\ud835\udc3cM_{I}italic_M start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT and the geometry-aware boundary estimator is adopted to predict the garment boundary. Finally, we register MP\u2062(\u22c5)subscript\ud835\udc40\ud835\udc43\u22c5M_{P}(\\cdot)italic_M start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT ( \u22c5 ) to MIsubscript\ud835\udc40\ud835\udc3cM_{I}italic_M start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT to obtain the final mesh MFsubscript\ud835\udc40\ud835\udc39M_{F}italic_M start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT, which has fine topology and open-boundaries. Images courtesy of Stable Diffusion.", "description": "This figure illustrates the pipeline of the proposed 3D garment reconstruction method. Starting with an RGB image as input, the method first estimates the coarse T-pose garment shape using equation 4.  This shape is then refined by incorporating pose-related deformations calculated using equations 7 and 10, which leverage a predicted SMPL body model. Next, a pixel-aligned network reconstructs an implicit fine garment representation, and a geometry-aware boundary estimator predicts the garment's boundary. Finally, the coarse and fine garment representations are registered to produce a final garment mesh with accurate topology and open boundaries.  The images shown in the figure were generated using Stable Diffusion.", "section": "4 METHOD"}, {"figure_path": "https://arxiv.org/html/2411.03047/extracted/5979162/images/fig_result_gallery_1016_v6.png", "caption": "Figure 5. Result gallery of our method. Each image is followed by the reconstructed garment mesh. As illustrated, our method can effectively reconstruct garments with intricate deformations and fine-grained surface details. To support the modeling of folded structures, such as collars, we assembled a repository of diverse real-world collars that were crafted based on our topologically-consistent garments. A lightweight classification network was trained to select the collar that best matches the given image in terms of appearance\u00a0(Zhu et\u00a0al., 2022). Original images courtesy of licensed photos and Stable Diffusion. The images with a gray background are synthesized, while the rest are licensed photos.", "description": "This figure showcases the results of the proposed 3D garment reconstruction method.  It presents pairs of input images and their corresponding reconstructed 3D garment meshes. The examples demonstrate the method's capability to accurately reconstruct garments with complex shapes and detailed features, even in the presence of significant deformations.  A key improvement is the inclusion of realistic collars, achieved by creating a separate database of various collar types and training a classification network to select the most appropriate collar for each garment based on the input image. This addresses a significant challenge in realistic garment reconstruction by incorporating nuanced details often missing in previous methods. The source of the input images is specified; those with gray backgrounds are synthetically generated, while the rest are from licensed photo sources.", "section": "5 Experiments"}, {"figure_path": "https://arxiv.org/html/2411.03047/extracted/5979162/images/fig_compare_1016_v6.png", "caption": "Figure 6. Qualitative comparison between ours and the state of the arts. For each row, the input image is followed by the results generated by BCNet\u00a0(Jiang et\u00a0al., 2020), ClothWild\u00a0(Moon et\u00a0al., 2022), Deep Fashion3D\u00a0(Zhu et\u00a0al., 2020), ReEF\u00a0(Zhu et\u00a0al., 2022) and our method. Input images courtesy of Stable Diffusion.", "description": "Figure 6 presents a qualitative comparison of five different methods for 3D garment reconstruction from a single image: BCNet, ClothWild, DeepFashion3D, ReEF, and the authors' proposed method. Each row shows an input image followed by the results generated by each of the five methods.  This allows for a visual comparison of the accuracy, detail, and overall quality of the garment reconstructions produced by each approach. The input images were all generated using Stable Diffusion.", "section": "5 Experiments"}, {"figure_path": "https://arxiv.org/html/2411.03047/extracted/5979162/images/fig_ablation_boundary_1016_v6.png", "caption": "Figure 7. Qualitative comparison between our method and the alternative strategy for predicting garment boundary from in-the-wild images. The input image (a) is followed by the boundaries generated by (b) ReEF\u2019s strategy and (c) our geometry-aware estimator. ReEF fails to accurately predict boundaries with complex poses and deformations, leading to discontinuous boundaries. Our geometry-aware boundary prediction outperforms ReEF in reconstructing complex garment boundaries that are well-aligned with the garment shape. Input images courtesy of Stable Diffusion.", "description": "Figure 7 presents a qualitative comparison of garment boundary prediction methods using real-world images.  The figure showcases three columns: (a) the input image, (b) the boundary prediction from the ReEF method, and (c) the boundary prediction from the proposed geometry-aware method. The comparison highlights the superior performance of the proposed method, which accurately reconstructs complex and deformed garment boundaries that closely align with the garment's shape, unlike ReEF's prediction which suffers from inaccuracies and discontinuities, especially in complex poses.", "section": "4.3 Geometry-aware Boundary Prediction"}, {"figure_path": "https://arxiv.org/html/2411.03047/extracted/5979162/images/fig_ablation_data_1016_v6.png", "caption": "Figure 8. Qualitative comparison on different data. The input image (a) is followed by the results generated by networks trained with (b) ReEF\u2019s data and (c) our GarVerseLOD. Input images courtesy of Stable Diffusion.", "description": "This figure compares the results of 3D garment reconstruction using two different datasets: ReEF and GarVerseLOD.  The same input image is used for both models. Column (a) shows the input image. Column (b) presents the reconstruction result obtained by training a model on the ReEF dataset. Column (c) displays the reconstruction result obtained by training a model on the GarVerseLOD dataset. The comparison highlights the impact of different datasets on the accuracy and quality of the garment reconstruction, demonstrating the superior performance of GarVerseLOD. The images are generated using Stable Diffusion.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2411.03047/extracted/5979162/images/fig_ablation_coarse_1016_v6.png", "caption": "Figure 9. Qualitative comparison between our method and the alternative strategy for obtaining coarse garment template. (a) the input image; (b) the template (black part) cropped from SMPL; (c) the registration result using (b); (d) the coarse garment estimated by our coarse garment estimator; and (e) the registration result using (d). Input images courtesy of Stable Diffusion.", "description": "Figure 9 compares different approaches for obtaining a coarse garment template, a crucial step in 3D garment reconstruction.  It shows the results of two methods: (a) Input image: The image serves as the input to the garment reconstruction process. (b) SMPL-cropped template: A template (the black part) is created by directly cropping a section from an SMPL (Skinned Multi-Person Linear Model) body mesh. This method represents a simplified approach where garment information is borrowed from a general human body model. (c) Registration result using (b): The template from (b) is registered (or aligned) to the input image, producing a coarse garment estimate. (d) Coarse garment estimated by our method: The proposed method estimates a coarse garment template.  This method learns garment characteristics directly from data rather than relying on a human body model. (e) Registration result using (d): The template produced by our method is registered to the input image, yielding a coarse garment estimate. The figure demonstrates that using a learned garment estimator (our method) leads to superior registration results compared to simply cropping from a human body model.", "section": "4.1 Coarse Explicit Garment Estimation"}, {"figure_path": "https://arxiv.org/html/2411.03047/extracted/5979162/images/fig_ablation_implicit_udf_1016_v6.png", "caption": "Figure 10. Qualitative comparison on different representation. The input image (a) is followed by the result generated by (b) UDF, (c) registering to (b), (d) occupancy field and (e) registering to (d). Input images courtesy of Stable Diffusion.", "description": "This figure compares the results of using different 3D representations for garment reconstruction: Unsigned Distance Fields (UDF) and occupancy fields.  The input image (a) is shown alongside reconstruction attempts using (b) UDF alone, (c) UDF followed by registration to refine the result, (d) an occupancy field, and (e) the occupancy field with subsequent registration. The comparison highlights the effectiveness of the occupancy field approach, especially when combined with registration for accurate garment reconstruction.  Images were synthesized using Stable Diffusion.", "section": "4.4 3D Garment Shape Registration"}, {"figure_path": "https://arxiv.org/html/2411.03047/extracted/5979162/images/fig_limitation_1016_v6.png", "caption": "Figure 11. Failure cases. Our framework may struggle to reconstruct garments with complex topology, such as those multi-layered structures (a) or featuring slits (b). Images courtesy of licensed photos and Stable Diffusion.", "description": "Figure 11 showcases instances where the proposed garment reconstruction method encounters difficulties.  Panel (a) illustrates a limitation in handling garments with complex, multi-layered structures, such as layered skirts or dresses. The model struggles to accurately capture the individual layers and their interactions. Panel (b) demonstrates challenges in reconstructing garments with slits or openings.  These features present significant topological complexities that the current approach has difficulty resolving. Both examples highlight scenarios where the model's capacity to handle complex garment geometry and topology is limited.", "section": "6 Conclusion"}, {"figure_path": "https://arxiv.org/html/2411.03047/extracted/5979162/supp_images/fig_template_f322.png", "caption": "Figure 12. Predefined templates for each garment category, including (a) dress, (b) skirt, (c) top, (d) pant, and (e) coat.", "description": "This figure shows the five predefined garment templates used as the base for creating the 3D garment models in the GarVerseLOD dataset.  Each template represents a basic, T-pose garment shape for a different clothing category: (a) dress, (b) skirt, (c) top, (d) pants, and (e) coat. These templates serve as a starting point for the artists who then manually add detailed geometry and realistic deformations to create the diverse garment models in the dataset.", "section": "3 DATASET"}, {"figure_path": "https://arxiv.org/html/2411.03047/extracted/5979162/supp_images/fig_deformation_craft.png", "caption": "Figure 13. Given a \u201cCollected Image\u201d, we utilize PyMAF\u00a0(Zhang et\u00a0al., 2021, 2023b) to estimate SMPL body. Eight artists are then tasked with creating \u201cT-pose Garment\u201d shapes by deforming a predefined \u201cTemplate\u201d to match the T-pose body predicted by PyMAF. Then the SMPL\u2019s Linear Blend Skinning (LBS) is extended to the T-pose garment to obtain the \u201cPosed Garment\u201d. Finally, the artists are further instructed to refine the posed garment to get the \u201cCrafted Garment\u201d while ensuring that garment deformations closely match the collected images. \u201cPosed Garment\u201d represent the shape of clothing influenced by human pose, while \u201cCrafted Garment\u201d capture the state of garments affected by various complex factors\u2014not only pose but also other environmental influences, such as garment-environment interactions and external forces like wind.", "description": "The figure illustrates the process of creating high-fidelity 3D garment models. It starts with a real image of a person wearing clothes.  PyMAF is used to estimate the underlying 3D human body pose (SMPL). Eight artists then manually adjust a template garment mesh to match the T-pose of this estimated body, creating the \"T-pose Garment\".  Next, SMPL's Linear Blend Skinning (LBS) is applied to this \"T-pose Garment\" to generate a \"Posed Garment\" which reflects the basic pose-related deformations. Finally, the artists further refine the \"Posed Garment\", resulting in the \"Crafted Garment\", which incorporates more complex deformations that would not be solely caused by pose, such as those resulting from environmental influences or other factors affecting the fabric.  This multi-step process ensures that the final \"Crafted Garment\" models accurately reflect the realistic drape and texture of the clothing.", "section": "3 DATASET"}, {"figure_path": "https://arxiv.org/html/2411.03047/extracted/5979162/supp_images/stitched_image_group_0.jpg", "caption": "Figure 14. More Results on Loose-fitting Garments.", "description": "This figure showcases the results of the proposed method on various loose-fitting garments.  It visually demonstrates the ability of the model to handle complex cloth deformations and generate high-fidelity 3D garment reconstructions from single, in-the-wild images. Each image is paired with its corresponding generated 3D model, highlighting the accuracy and detail of the reconstructions.", "section": "5 Experiments"}, {"figure_path": "https://arxiv.org/html/2411.03047/extracted/5979162/supp_images/stitched_image_group_1.jpg", "caption": "Figure 15. More Results on Loose-fitting Garments.", "description": "This figure shows additional results of the proposed method applied to loose-fitting garments.  It showcases the model's ability to reconstruct a variety of loose garments with different styles and poses, highlighting its generalization capabilities and robustness to various levels of garment deformation.", "section": "5 Experiments"}, {"figure_path": "https://arxiv.org/html/2411.03047/extracted/5979162/supp_images/stitched_image_group_2.jpg", "caption": "Figure 16. More Results on Loose-fitting Garments.", "description": "This figure showcases additional results of 3D garment reconstruction from single images.  It demonstrates the method's ability to handle loose-fitting garments, a challenging scenario due to the increased complexity of garment deformations and the lack of strong visual cues.  The images show a variety of loose-fitting garments (dresses, skirts, etc.) and their corresponding reconstructed 3D models.  The success in reconstructing the shapes and textures of these loose garments highlights the robustness and generalization capability of the proposed method.", "section": "5 Experiments"}, {"figure_path": "https://arxiv.org/html/2411.03047/extracted/5979162/supp_images/stitched_image_group_3.jpg", "caption": "Figure 17. More Results on Loose-fitting Garments.", "description": "This figure showcases additional results of the proposed method on loose-fitting garments. It demonstrates the method's ability to reconstruct various loose-fitting garments with different shapes, poses, and textures, highlighting its generalization capability and robustness in handling various complex garment deformations.  Each image shows an input image followed by its corresponding 3D reconstruction.", "section": "5 Experiments"}, {"figure_path": "https://arxiv.org/html/2411.03047/extracted/5979162/supp_images/LOD_0.png", "caption": "Figure 18. An illustration of our Garment Style Database.", "description": "This figure shows a collection of simplified garment models from the Garment Style Database. Each model represents a basic garment shape (dress, skirt, coat, top, or pants) in a T-pose, lacking detailed textures or intricate folds.  These simplified models serve as foundational templates for generating more complex garments by adding local details and deformations in later stages of the dataset creation process.", "section": "3 DATASET"}, {"figure_path": "https://arxiv.org/html/2411.03047/extracted/5979162/supp_images/LOD_1.png", "caption": "Figure 19. An illustration of our Local Detail Database.", "description": "Figure 19 shows a subset of the Local Detail Database from the GarVerseLOD dataset.  This database contains pairs of T-posed garment models, one with and one without fine-grained geometric details such as wrinkles.  These pairs are used to learn how to transfer realistic local detail from a detailed model onto a simpler, more basic model.  The images illustrate the variety of clothing items and detail levels captured in this part of the dataset.", "section": "3 DATASET"}, {"figure_path": "https://arxiv.org/html/2411.03047/extracted/5979162/supp_images/LOD_2.png", "caption": "Figure 20. An illustration of our Garment Deformation Database.", "description": "Figure 20 visually showcases the Garment Deformation Database, a key component of the GarVerseLOD dataset.  This database contains pairs of T-posed and deformed garment meshes. The T-posed mesh represents the garment in a neutral pose, while the deformed mesh showcases the garment's appearance after undergoing various deformations. These deformations result from a combination of factors like body pose, interactions with the environment, and self-collisions. The paired data within this database are crucial for training the model to learn how different factors influence the garment's shape and overall appearance.", "section": "3 DATASET"}, {"figure_path": "https://arxiv.org/html/2411.03047/extracted/5979162/supp_images/LOD_3.png", "caption": "Figure 21. An illustration of our Fine Garment Dataset.", "description": "Figure 21 visually showcases the \"Fine Garment Dataset,\" a crucial component of the GarVerseLOD dataset.  Unlike the other datasets (Garment Style, Local Detail, and Garment Deformation), this dataset integrates the details from all three, resulting in high-fidelity 3D garment models that capture both global deformations (like those caused by pose) and fine-grained local details (like wrinkles and creases). Each garment model in the dataset presents a complex, realistic representation of clothing.", "section": "3 DATASET"}]