<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><title>GarVerseLOD: High-Fidelity 3D Garment Reconstruction from a Single In-the-Wild Image using a Dataset with Levels of Details &#183; AI Paper Reviews by AI</title>
<meta name=title content="GarVerseLOD: High-Fidelity 3D Garment Reconstruction from a Single In-the-Wild Image using a Dataset with Levels of Details &#183; AI Paper Reviews by AI"><meta name=description content="GarVerseLOD introduces a novel dataset and framework for high-fidelity 3D garment reconstruction from a single image, achieving unprecedented robustness via a hierarchical approach and leveraging a ma..."><meta name=keywords content="Computer Vision,3D Vision,üè¢ SSE,CUHKSZ,China,"><link rel=canonical href=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.03047/><link type=text/css rel=stylesheet href=/ai-paper-reviewer/css/main.bundle.min.595affd4445a931ea6d6e3a5a3c709930fa52a60be10b21c6f81fdb8fecaacea33aacedf80cdc88be45f189be14ed4ce53ea74a1e1406fad9cbf90c5ed409173.css integrity="sha512-WVr/1ERakx6m1uOlo8cJkw+lKmC+ELIcb4H9uP7KrOozqs7fgM3Ii+RfGJvhTtTOU+p0oeFAb62cv5DF7UCRcw=="><script type=text/javascript src=/ai-paper-reviewer/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script><script defer type=text/javascript id=script-bundle src=/ai-paper-reviewer/js/main.bundle.min.efbf3b6b987689fffaf2d7b73173d2690c0279a04d444b0537a77d7f4ff6e6d493445400cb0cf56bc0f0f123e19f15394e63cae34e67f069bd013dd5c73df56e.js integrity="sha512-7787a5h2if/68te3MXPSaQwCeaBNREsFN6d9f0/25tSTRFQAywz1a8Dw8SPhnxU5TmPK405n8Gm9AT3Vxz31bg==" data-copy data-copied></script><script src=/ai-paper-reviewer/lib/zoom/zoom.min.37d2094687372da3f7343a221a470f6b8806f7891aa46a5a03966af7f0ebd38b9fe536cb154e6ad28f006d184b294525a7c4054b6bbb4be62d8b453b42db99bd.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ=="></script><link rel=apple-touch-icon sizes=180x180 href=/ai-paper-reviewer/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/ai-paper-reviewer/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/ai-paper-reviewer/favicon-16x16.png><link rel=manifest href=/ai-paper-reviewer/site.webmanifest><meta property="og:url" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.03047/"><meta property="og:site_name" content="AI Paper Reviews by AI"><meta property="og:title" content="GarVerseLOD: High-Fidelity 3D Garment Reconstruction from a Single In-the-Wild Image using a Dataset with Levels of Details"><meta property="og:description" content="GarVerseLOD introduces a novel dataset and framework for high-fidelity 3D garment reconstruction from a single image, achieving unprecedented robustness via a hierarchical approach and leveraging a ma‚Ä¶"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="paper-reviews"><meta property="article:published_time" content="2024-11-05T00:00:00+00:00"><meta property="article:modified_time" content="2024-11-05T00:00:00+00:00"><meta property="article:tag" content="Computer Vision"><meta property="article:tag" content="3D Vision"><meta property="article:tag" content="üè¢ SSE, CUHKSZ, China"><meta property="og:image" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.03047/cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.03047/cover.png"><meta name=twitter:title content="GarVerseLOD: High-Fidelity 3D Garment Reconstruction from a Single In-the-Wild Image using a Dataset with Levels of Details"><meta name=twitter:description content="GarVerseLOD introduces a novel dataset and framework for high-fidelity 3D garment reconstruction from a single image, achieving unprecedented robustness via a hierarchical approach and leveraging a ma‚Ä¶"><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"Paper Reviews by AI","name":"GarVerseLOD: High-Fidelity 3D Garment Reconstruction from a Single In-the-Wild Image using a Dataset with Levels of Details","headline":"GarVerseLOD: High-Fidelity 3D Garment Reconstruction from a Single In-the-Wild Image using a Dataset with Levels of Details","abstract":"GarVerseLOD introduces a novel dataset and framework for high-fidelity 3D garment reconstruction from a single image, achieving unprecedented robustness via a hierarchical approach and leveraging a ma\u0026hellip;","inLanguage":"en","url":"https:\/\/deep-diver.github.io\/ai-paper-reviewer\/paper-reviews\/2411.03047\/","author":{"@type":"Person","name":"AI Paper Reviews by AI"},"copyrightYear":"2024","dateCreated":"2024-11-05T00:00:00\u002b00:00","datePublished":"2024-11-05T00:00:00\u002b00:00","dateModified":"2024-11-05T00:00:00\u002b00:00","keywords":["Computer Vision","3D Vision","üè¢ SSE, CUHKSZ, China"],"mainEntityOfPage":"true","wordCount":"5135"}]</script><meta name=author content="AI Paper Reviews by AI"><link href=https://github.com/deep-diver/paper-reviewer/ rel=me><link href=https://twitter.com/algo_diver/ rel=me><script src=/ai-paper-reviewer/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj+KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script><script defer src=/ai-paper-reviewer/lib/typeit/typeit.umd.1b3200cb448f5cd1f548f2781452643d3511a43584b377b82c03a58055da4fdb7bc8f6c6c2ce846480c7677ff25bfd0d75f15823c09443ab18e0fd2cad792587.js integrity="sha512-GzIAy0SPXNH1SPJ4FFJkPTURpDWEs3e4LAOlgFXaT9t7yPbGws6EZIDHZ3/yW/0NdfFYI8CUQ6sY4P0srXklhw=="></script><script defer src=/ai-paper-reviewer/lib/packery/packery.pkgd.min.js integrity></script><script type=text/javascript src=/ai-paper-reviewer/js/shortcodes/gallery.min.9b4cb28f931ed922c26fb9b2510c2debb370f6a63305050c2af81740b2919883715e24efbbdf3a081496718ec751df3a72729d4d0bc71d6071297563a97ce1ee.js integrity="sha512-m0yyj5Me2SLCb7myUQwt67Nw9qYzBQUMKvgXQLKRmINxXiTvu986CBSWcY7HUd86cnKdTQvHHWBxKXVjqXzh7g=="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-KX0S6Q55Y7"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-KX0S6Q55Y7")</script><meta name=theme-color><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-app.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-firestore.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-auth.js></script><script>const firebaseConfig={apiKey:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",authDomain:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",projectId:"neurips2024-f3065",storageBucket:"neurips2024-f3065.firebasestorage.app",messagingSenderId:"982475958898",appId:"1:982475958898:web:2147e5d7753d6ac091f0eb",measurementId:"G-YQ46HXQ9JS"};var app=firebase.initializeApp(firebaseConfig),db=firebase.firestore(),auth=firebase.auth()</script></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a></div><div class=min-h-[148px]></div><div class="fixed inset-x-0 pl-[24px] pr-[24px]" style=z-index:100><div id=menu-blur class="absolute opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl shadow-2xl"></div><div class="relative max-w-[64rem] ml-auto mr-auto"><div style=padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start space-x-3"><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/ai-paper-reviewer/ class="text-base font-medium text-gray-500 hover:text-gray-900">AI Paper Reviews by AI</a></nav><nav class="hidden md:flex items-center space-x-5 md:ml-12 h-12"><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>About</p></a><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Paper Reviews</p></a><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Tags</p></a><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></span><p class="text-base font-medium" title></p></a><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span><p class="text-base font-medium" title></p></a><button id=search-button aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button><div class="ltr:mr-14 rtl:ml-14 flex items-center"><button id=appearance-switcher aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></nav><div class="flex md:hidden items-center space-x-5 md:ml-12 h-12"><span></span>
<button id=search-button-mobile aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></button>
<button id=appearance-switcher-mobile aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400" style=margin-right:5px><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div><div class="-my-2 -mr-2 md:hidden"><label id=menu-button class=block><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper style=padding-top:5px class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li id=menu-close-button><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>About</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Paper Reviews</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Tags</p></a></li><li class=mt-1><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li><li class=mt-1><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li></ul></div></label></div></div></div></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("menu-blur");n.style.opacity=t/300})</script><div class="relative flex flex-col grow"><main id=main-content class=grow><article><div id=hero class="h-[150px] md:h-[200px]"></div><div class="fixed inset-x-0 top-0 h-[800px] single_hero_background nozoom" style=background-image:url(/ai-paper-reviewer/paper-reviews/2411.03047/cover_hu14431356699179364189.png)><div class="absolute inset-0 bg-gradient-to-t from-neutral dark:from-neutral-800 to-transparent mix-blend-normal"></div><div class="absolute inset-0 opacity-60 bg-gradient-to-t from-neutral dark:from-neutral-800 to-neutral-100 dark:to-neutral-800 mix-blend-normal"></div></div><div id=background-blur class="fixed opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl"></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("background-blur");n.style.opacity=t/300})</script><header id=single_header class="mt-5 max-w-prose"><ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden"><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/>AI Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class=inline><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/>Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/2411.03047/>GarVerseLOD: High-Fidelity 3D Garment Reconstruction from a Single In-the-Wild Image using a Dataset with Levels of Details</a><span class="px-1 text-primary-500">/</span></li></ol><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">GarVerseLOD: High-Fidelity 3D Garment Reconstruction from a Single In-the-Wild Image using a Dataset with Levels of Details</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime=2024-11-05T00:00:00+00:00>5 November 2024</time><span class="px-2 text-primary-500">&#183;</span><span>5135 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">25 mins</span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=views_paper-reviews/2411.03047/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=views>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 576 512"><path fill="currentcolor" d="M288 32c-80.8.0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7.0 24.6C17.3 304 48.6 356 95.4 399.4 142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1 3.3-7.9 3.3-16.7.0-24.6-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144 64.5-144 144-144 144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64-11.5.0-22.3-3-31.6-8.4-.2 2.8-.4 5.5-.4 8.4.0 53 43 96 96 96s96-43 96-96-43-96-96-96c-2.8.0-5.6.1-8.4.4 5.3 9.3 8.4 20.1 8.4 31.6z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=likes_paper-reviews/2411.03047/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=likes>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<button id=button_likes class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400" onclick=process_article()>
<span id=button_likes_heart style=display:none class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span><span id=button_likes_emtpty_heart class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M244 84l11.1 12 12-11.98C300.6 51.37 347 36.51 392.6 44.1 461.5 55.58 512 115.2 512 185.1V190.9c0 41.5-17.2 81.2-47.6 109.5L283.7 469.1c-7.5 7-17.4 10.9-27.7 10.9S235.8 476.1 228.3 469.1L47.59 300.4C17.23 272.1.0 232.4.0 190.9V185.1c0-69.9 50.52-129.52 119.4-141 44.7-7.59 92 7.27 124.6 39.9C243.1 84 244 84.01 244 84zm11.1 79.9-45-46.8c-21.7-20.82-52.5-30.7-82.8-25.66C81.55 99.07 48 138.7 48 185.1V190.9c0 28.2 11.71 55.2 32.34 74.4L256 429.3l175.7-164c20.6-19.2 32.3-46.2 32.3-74.4V185.1c0-46.4-33.6-86.03-79.3-93.66C354.4 86.4 323.6 96.28 301.9 117.1l-46.8 46.8z"/></svg>
</span></span><span id=button_likes_text>&nbsp;Like</span></button></span></div><div class="flex flex-row flex-wrap items-center"><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/ai-generated/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">AI Generated
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/-daily-papers/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">ü§ó Daily Papers
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/computer-vision/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Computer Vision
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/3d-vision/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">3D Vision
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/-sse-cuhksz-china/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">üè¢ SSE, CUHKSZ, China</span></span></span></div></div><div class="flex author"><img class="!mt-0 !mb-0 h-24 w-24 rounded-full ltr:mr-4 rtl:ml-4" width=96 height=96 alt="AI Paper Reviews by AI" src=/ai-paper-reviewer/img/avatar_hu14127527184135390686.png><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">Author</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">AI Paper Reviews by AI</div><div class="text-sm text-neutral-700 dark:text-neutral-400">I am AI, and I review papers in the field of AI</div><div class="text-2xl sm:text-lg"><div class="flex flex-wrap text-neutral-400 dark:text-neutral-500"><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://github.com/deep-diver/paper-reviewer/ target=_blank aria-label=Github rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://twitter.com/algo_diver/ target=_blank aria-label=Twitter rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span></a></div></div></div></div><div class=mb-5></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first lg:ml-auto px-0 lg:order-last ltr:lg:pl-8 rtl:lg:pr-8"><div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-[140px]"><details open id=TOCView class="toc-right mt-0 overflow-y-scroll overscroll-contain scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600 rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 hidden lg:block"><summary class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="min-w-[220px] py-2 border-dotted ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#garment-3d-modeling>Garment 3D Modeling</a></li><li><a href=#lod-dataset>LOD Dataset</a></li><li><a href=#coarse-to-fine>Coarse-to-Fine</a></li><li><a href=#boundary-prediction>Boundary Prediction</a></li><li><a href=#future-work>Future Work</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><details class="toc-inside mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 lg:hidden"><summary class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#garment-3d-modeling>Garment 3D Modeling</a></li><li><a href=#lod-dataset>LOD Dataset</a></li><li><a href=#coarse-to-fine>Coarse-to-Fine</a></li><li><a href=#boundary-prediction>Boundary Prediction</a></li><li><a href=#future-work>Future Work</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><script>var margin=200,marginError=50;(function(){var t=$(window),e=$("#TOCView"),s=e.height();function n(){var n=t.height()-margin;s>=n?(e.css("overflow-y","scroll"),e.css("max-height",n+marginError+"px")):(e.css("overflow-y","hidden"),e.css("max-height","9999999px"))}t.on("resize",n),$(document).ready(n)})()</script></div></div><div class="min-w-0 min-h-0 max-w-fit"><div class="article-content max-w-prose mb-20"><br><div class="flex flex-row flex-wrap items-center space-x-2"><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 48 48" fill="none"><rect width="48" height="48" fill="#fff" fill-opacity=".01"/><path d="M18 43V22c0-3.3137 2.6863-6 6-6s6 2.6863 6 6V43" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M12 40V22c0-6.6274 5.3726-12 12-12s12 5.3726 12 12V40" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M6 35V22C6 12.0589 14.0589 4 24 4s18 8.0589 18 18V35" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 44V31" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 24.625v-2.75" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/></svg>
</span></span><span>2411.03047</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg fill="#000" height="800" width="800" id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 511.999 511.999"><g><g><path d="M421.578 190.264l-99.847-99.847c-2.439-2.439-6.391-2.439-8.829.0L82.824 320.495c-2.439 2.439-2.439 6.392.0 8.829l99.847 99.847c2.439 2.439 6.391 2.439 8.829.0l230.078-230.078C424.017 196.655 424.017 192.703 421.578 190.264z"/></g></g><g><g><path d="M506.511 87.672 424.323 5.484c-7.308-7.31-19.175-7.315-26.488.0L348.219 55.1c-2.439 2.439-2.439 6.391.0 8.829l99.847 99.847c2.439 2.437 6.391 2.437 8.829.0l49.616-49.616C513.826 106.847 513.826 94.987 506.511 87.672z"/></g></g><g><g><path d="M508.133 491.11c-1.054-9.556-9.489-16.599-19.104-16.599H111.633l36.058-15.163c4.088-1.719 5.131-7.034 1.994-10.17l-86.854-86.854c-3.137-3.135-8.451-2.094-10.17 1.994C52.224 365.359 2.052 484.66 1.627 485.707c-5.815 13.208 4.855 27.01 18.107 26.263H489.52C500.566 511.97 509.379 502.408 508.133 491.11z"/></g></g></svg>
</span></span><span>Zhongjin Luo et el.</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span>ü§ó 2024-11-06</span></span></span></div></div><p><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://arxiv.org/abs/2411.03047 target=_self role=button>‚Üó arXiv
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://huggingface.co/papers/2411.03047 target=_self role=button>‚Üó Hugging Face
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://paperswithcode.com/paper/garverselod-high-fidelity-3d-garment target=_self role=button>‚Üó Papers with Code</a></p><h3 class="relative group">TL;DR<div id=tldr class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#tldr aria-label=Anchor>#</a></span></h3><div class="lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl"><p>Current methods for 3D garment reconstruction from images struggle with complex cloth deformations and limited dataset quality, hindering generalization. The lack of high-quality datasets with diverse garment styles, poses, and deformations poses significant challenges. This paper tackles these issues by introducing GarVerseLOD, a hierarchical dataset with levels of details that addresses the limitation of previous methods.</p><p>GarVerseLOD contains 6,000 high-quality garment models crafted by professionals and a novel data labeling paradigm is used for image generation. The proposed framework uses a coarse-to-fine reconstruction strategy and leverages the hierarchical structure of the dataset. The results demonstrate significant improvements in reconstruction quality and robustness compared to state-of-the-art methods, showcasing the effectiveness of the approach. This offers a powerful tool for various applications relying on accurate 3D garment models.</p></div><h4 class="relative group">Key Takeaways<div id=key-takeaways class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#key-takeaways aria-label=Anchor>#</a></span></h4><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-2bc9c463b0b21bd9592370a59cb9a4bb></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-2bc9c463b0b21bd9592370a59cb9a4bb",{strings:[" GarVerseLOD dataset offers a hierarchical structure with levels of detail (LOD), improving generalization. "],speed:10,lifeLike:!0,startDelay:0,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-544d758c80d8fd0f32469bdf928b3cba></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-544d758c80d8fd0f32469bdf928b3cba",{strings:[" A novel data labeling paradigm using conditional diffusion models generates extensive paired images for training. "],speed:10,lifeLike:!0,startDelay:1e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-0fafb7b08e85f167c6f83b5817452441></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-0fafb7b08e85f167c6f83b5817452441",{strings:[" A coarse-to-fine reconstruction framework effectively leverages the LOD structure of GarVerseLOD for high-fidelity results. "],speed:10,lifeLike:!0,startDelay:2e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><h4 class="relative group">Why does it matter?<div id=why-does-it-matter class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#why-does-it-matter aria-label=Anchor>#</a></span></h4><p>This paper is important because it addresses the critical need for high-quality 3D garment datasets and robust reconstruction methods. The <strong>GarVerseLOD dataset</strong>, with its hierarchical structure and extensive paired image-3D model data, provides a significant advancement for researchers in computer vision and graphics. Its <strong>novel labeling paradigm</strong> and <strong>coarse-to-fine reconstruction framework</strong> offer new avenues of research, and its impressive results pave the way for improved applications in virtual fashion, e-commerce, and virtual reality.</p><hr><h4 class="relative group">Visual Insights<div id=visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#visual-insights aria-label=Anchor>#</a></span></h4><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2411.03047/extracted/5979162/images/fig_teaser_1016_v6.png alt></figure></p><blockquote><p>üîº Figure 1 showcases the GarVerseLOD dataset and the hierarchical framework for 3D garment reconstruction. The framework leverages garment shape and deformation priors learned from the dataset to reconstruct high-fidelity 3D garment meshes from a single image. The figure demonstrates the system&rsquo;s ability to handle various garment types and poses, producing realistic results that align well with the input images. Some images were sourced from licensed photos, while others were generated using Stable Diffusion. The gray background indicates synthetically generated images.</p><details><summary>read the caption</summary>Figure 1. We propose a hierarchical framework to recover different levels of garment details by leveraging the garment shape and deformation priors from the GarVerseLOD dataset. Given a single clothed human image, our approach is capable of generating high-fidelity 3D standalone garment meshes that exhibit realistic deformation and are well-aligned with the input image. Original images courtesy of licensed photos and Stable Diffusion¬†(Rombach et¬†al., 2022). The images with a gray background are synthesized, while the rest are licensed photos.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Method</th><th>BCNet</th><th>ClothWild</th><th>Deep Fashion3D</th><th>ReEF</th><th>Ours</th></tr></thead><tbody><tr><td>Chamfer Distance ‚Üì</td><td>18.742</td><td>16.136</td><td>17.159</td><td>11.357</td><td><strong>7.825</strong></td></tr><tr><td>Normal Consistency ‚Üë</td><td>0.781</td><td>0.812</td><td>0.793</td><td>0.838</td><td><strong>0.913</strong></td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents a quantitative comparison of the proposed method against existing state-of-the-art techniques for 3D garment reconstruction. The metrics used for comparison include Chamfer Distance (measuring the geometric difference between the reconstructed and ground truth meshes), Normal Consistency (assessing the similarity of surface normals), and Intersection over Union (IoU, measuring the overlap of the predicted and ground truth garment regions). Lower Chamfer Distance and higher Normal Consistency values indicate better reconstruction accuracy.</p><details><summary>read the caption</summary>Table 1. Quantitive comparison between our method with others.</details></blockquote><h3 class="relative group">In-depth insights<div id=in-depth-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#in-depth-insights aria-label=Anchor>#</a></span></h3><h4 class="relative group">Garment 3D Modeling<div id=garment-3d-modeling class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#garment-3d-modeling aria-label=Anchor>#</a></span></h4><p>Garment 3D modeling presents significant challenges due to the <strong>complexity of fabric draping and deformation</strong>, influenced by both body pose and environmental factors. Traditional methods often struggle with realism and generalization. This research highlights the critical role of high-quality datasets, such as GarVerseLOD, in advancing the field. <strong>GarVerseLOD&rsquo;s hierarchical structure</strong>, incorporating multiple levels of detail from coarse shapes to fine-grained geometry, allows for a staged approach to reconstruction. This is crucial for overcoming the inherent ill-posed nature of the problem, significantly improving accuracy and generalization. The study shows that <strong>incorporating both explicit and implicit representations</strong> offers a powerful approach, enhancing the model&rsquo;s ability to capture both global garment shape and intricate local details simultaneously. The integration of a geometry-aware boundary prediction further boosts accuracy by addressing the challenges of boundary estimation from single images. The success of this approach demonstrates the potential of leveraging data with levels of detail and combining explicit and implicit methods for accurate and robust 3D garment modeling.</p><h4 class="relative group">LOD Dataset<div id=lod-dataset class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#lod-dataset aria-label=Anchor>#</a></span></h4><p>A Levels of Detail (LOD) dataset for 3D garment reconstruction is a significant contribution because it addresses the limitations of existing datasets. <strong>The hierarchical nature of the LOD dataset, ranging from stylized shapes to highly detailed models, allows for a more tractable approach to the complex problem of 3D garment reconstruction.</strong> This staged approach facilitates training and inference, making the overall task less computationally intensive and easier to manage. The dataset&rsquo;s inclusion of various levels of detail is <strong>key for training robust and generalizable models</strong> that can perform well across a range of clothing types, poses, and conditions. The inclusion of both synthetic and real-world images further enhances the robustness and applicability of the approach. <strong>The creation of a large-scale dataset with high-quality, hand-crafted garment meshes enhances the potential for significant improvements in the accuracy and realism of 3D garment reconstruction.</strong> This is a crucial step towards achieving more realistic virtual fashion and virtual try-on experiences.</p><h4 class="relative group">Coarse-to-Fine<div id=coarse-to-fine class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#coarse-to-fine aria-label=Anchor>#</a></span></h4><p>A coarse-to-fine approach in 3D garment reconstruction is a powerful strategy that leverages a hierarchical representation of garment details. It starts with a <strong>simplified, coarse model</strong>, capturing the overall shape and pose, before progressively refining it by incorporating finer details and intricate deformations. This approach offers several advantages. Firstly, it simplifies a complex problem into manageable sub-problems. The coarse stage provides a robust initial estimate, reducing the search space for subsequent refinement steps. Secondly, it improves the efficiency of the reconstruction process by focusing computational resources on the most essential aspects initially. Finally, it enhances the generalization ability of the model to unseen data as the initial stage focuses on learning underlying garment properties that are less sensitive to variations in appearance, texture, and pose.</p><h4 class="relative group">Boundary Prediction<div id=boundary-prediction class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#boundary-prediction aria-label=Anchor>#</a></span></h4><p>Accurate boundary prediction is crucial for high-fidelity 3D garment reconstruction, as it defines the garment&rsquo;s shape and enables realistic rendering. The challenge lies in handling complex garment deformations and occlusions present in real-world images. <strong>Existing methods often rely solely on 2D image cues</strong>, which can lead to inaccurate predictions due to depth ambiguity. A promising approach involves <strong>integrating both 2D and 3D information</strong>; leveraging 2D image features for local detail and 3D geometry-aligned features to resolve depth inconsistencies and ensure global shape consistency. This <strong>fusion of cues</strong> is key to robust boundary prediction, especially for intricate garment shapes and poses. <strong>The use of implicit functions</strong>, such as neural implicit representations, could further enhance the accuracy by capturing the complex topology of garment boundaries. Investigating different architectural designs for combining 2D and 3D features, and exploring various loss functions for optimization will be critical to improving the accuracy of the prediction. <strong>Developing a robust and efficient algorithm</strong> for boundary prediction is a significant step toward achieving high-fidelity 3D garment modeling from single images.</p><h4 class="relative group">Future Work<div id=future-work class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#future-work aria-label=Anchor>#</a></span></h4><p>Future research directions stemming from this GarVerseLOD work could explore several promising avenues. <strong>Expanding the dataset&rsquo;s scope</strong> to encompass a wider array of garment styles, materials, and body morphologies is crucial for improving generalization. <strong>Addressing the limitations</strong> in handling complex topologies, like multi-layered garments or those with slits, requires investigating advanced representation methods beyond implicit functions. <strong>Improving the efficiency</strong> of the reconstruction pipeline, particularly the boundary prediction, is also essential for real-time applications. Exploring the integration of physical simulation with the learned models could enhance realism and accuracy of garment deformations. Finally, <strong>investigating novel applications</strong> of the high-fidelity 3D garment models, such as virtual try-ons, personalized garment design, or advanced animation techniques, would showcase the dataset&rsquo;s true potential.</p><h3 class="relative group">More visual insights<div id=more-visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#more-visual-insights aria-label=Anchor>#</a></span></h3><details><summary>More on figures</summary><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2411.03047/extracted/5979162/images/fig_dataset_pipeline3.png alt></figure></p><blockquote><p>üîº This figure illustrates the process of creating a hierarchical garment dataset with levels of detail. It starts with three basic databases: Garment Style Database (containing T-pose coarse garments), Local Detail Database (pairs of T-pose garments with and without fine details), and Garment Deformation Database (pairs of T-pose and deformed garments). These databases are combined to create the Fine Garment Dataset, which contains garments with both fine details and complex deformations. The process involves sampling shapes and deformations from the basic databases and transferring them to generate progressively more detailed garment models.</p><details><summary>read the caption</summary>Figure 2. The pipeline of our novel strategy for constructing a progressive garment dataset with levels of details. (a) Each case shows the reference image and the artist-crafted T-pose coarse garment in Garment Style Database. (b) A example of the reference image and the artist-crafted detail-pair in Local Detail Database. (c) A example of the reference image and the artist-crafted deformation-pair in Garment Deformation Database. (d) To obtain an T-pose garment with geometric details, we first sample a shape MCsubscriptùëÄùê∂M_{C}italic_M start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT from the Garment Style Database and a ‚ÄúLocal Detail Pair‚Äù (LCsubscriptùêøùê∂L_{C}italic_L start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT, LFsubscriptùêøùêπL_{F}italic_L start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT) from the Local Detail Database. Then we transfer the geometric details depicted by (LCsubscriptùêøùê∂L_{C}italic_L start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT, LFsubscriptùêøùêπL_{F}italic_L start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT) to MCsubscriptùëÄùê∂M_{C}italic_M start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT to obtain MLsubscriptùëÄùêøM_{L}italic_M start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT. (e) The deformation depicted by a sampled ‚ÄúGarment Deformation Pair‚Äù (DTsubscriptùê∑ùëáD_{T}italic_D start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT, DFsubscriptùê∑ùêπD_{F}italic_D start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT) is transferred to MLsubscriptùëÄùêøM_{L}italic_M start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT to obtain the fine garment MDsubscriptùëÄùê∑M_{D}italic_M start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPT, which contains fine-grained geometric details and complex deformations (Fine Garment Dataset). Original images courtesy of licensed photos.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2411.03047/extracted/5979162/images/fig_dataset_gallery.png alt></figure></p><blockquote><p>üîº Figure 3 illustrates the process of generating photorealistic images of garments for training the model. The left side shows the pipeline: starting with textureless 3D garment renderings from various viewpoints, these are fed into Canny-Conditional Stable Diffusion to create photorealistic images with diverse appearances. The right side displays example results. (a) shows a garment from the Fine Garment Dataset, (b) is the generated photorealistic image, (c) its corresponding pixel-aligned mask, (d) the normal map rendered from the 3D garment, (e) the garment mask from the 3D model, and (f) the corresponding T-pose coarse garment. Section 4 of the paper details how these images are used for training different parts of the model: (b, f) trains the coarse garment estimator; (b, c, d) trains the normal estimator; and (d, e, a) trains the fine garment estimator and geometry-aware boundary predictor. All synthesized images were produced using Stable Diffusion.</p><details><summary>read the caption</summary>Figure 3. Left: Our novel strategy for generating extensive photorealistic paired images. We acquire rendered images of 3D garments with random camera views. These rendered images are processed through Canny-Conditional Stable Diffusion¬†(Rombach et¬†al., 2022; Mou et¬†al., 2023; Zhang et¬†al., 2023a) to produce photorealistic images. Right: (a) The garment sampled from Fine Garment Dataset; (b) The synthesized image; (c) The pixel-aligned mask; (d) The normal map rendered using (a); (e) The garment mask rendered by (a); (f) The counterpart T-pose coarse garment of (a). In Sec.¬†4, (b, f) is used to train the coarse garment estimator, while (b,c,d) is adopted to train the normal estimator. (d, e, a) is utilized to train the fine garment estimator and the geometry-aware boundary predictor. Synthesized images courtesy of Stable Diffusion.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2411.03047/extracted/5979162/images/fig_method2.png alt></figure></p><blockquote><p>üîº This figure illustrates the pipeline of the proposed 3D garment reconstruction method. Starting with an RGB image as input, the method first estimates the coarse T-pose garment shape using equation 4. This shape is then refined by incorporating pose-related deformations calculated using equations 7 and 10, which leverage a predicted SMPL body model. Next, a pixel-aligned network reconstructs an implicit fine garment representation, and a geometry-aware boundary estimator predicts the garment&rsquo;s boundary. Finally, the coarse and fine garment representations are registered to produce a final garment mesh with accurate topology and open boundaries. The images shown in the figure were generated using Stable Diffusion.</p><details><summary>read the caption</summary>Figure 4. The pipeline of our proposed method. Given an RGB image, our method first estimates the T-pose garment shape G‚Å¢(Œ±)ùê∫ùõºG({{\alpha}})italic_G ( italic_Œ± ) (Eq.¬†4) and computes its pose-related deformation MP‚Å¢(Œ±,Œ≤,Œ∏)subscriptùëÄùëÉùõºùõΩùúÉM_{P}(\alpha,\beta,\theta)italic_M start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT ( italic_Œ± , italic_Œ≤ , italic_Œ∏ ) with the help of the predicted SMPL body (Eq.¬†7, Eq.¬†10). Then a pixel-aligned network is used to reconstruct implicit fine garment MIsubscriptùëÄùêºM_{I}italic_M start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT and the geometry-aware boundary estimator is adopted to predict the garment boundary. Finally, we register MP‚Å¢(‚ãÖ)subscriptùëÄùëÉ‚ãÖM_{P}(\cdot)italic_M start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT ( ‚ãÖ ) to MIsubscriptùëÄùêºM_{I}italic_M start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT to obtain the final mesh MFsubscriptùëÄùêπM_{F}italic_M start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT, which has fine topology and open-boundaries. Images courtesy of Stable Diffusion.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2411.03047/extracted/5979162/images/fig_result_gallery_1016_v6.png alt></figure></p><blockquote><p>üîº This figure showcases the results of the proposed 3D garment reconstruction method. It presents pairs of input images and their corresponding reconstructed 3D garment meshes. The examples demonstrate the method&rsquo;s capability to accurately reconstruct garments with complex shapes and detailed features, even in the presence of significant deformations. A key improvement is the inclusion of realistic collars, achieved by creating a separate database of various collar types and training a classification network to select the most appropriate collar for each garment based on the input image. This addresses a significant challenge in realistic garment reconstruction by incorporating nuanced details often missing in previous methods. The source of the input images is specified; those with gray backgrounds are synthetically generated, while the rest are from licensed photo sources.</p><details><summary>read the caption</summary>Figure 5. Result gallery of our method. Each image is followed by the reconstructed garment mesh. As illustrated, our method can effectively reconstruct garments with intricate deformations and fine-grained surface details. To support the modeling of folded structures, such as collars, we assembled a repository of diverse real-world collars that were crafted based on our topologically-consistent garments. A lightweight classification network was trained to select the collar that best matches the given image in terms of appearance¬†(Zhu et¬†al., 2022). Original images courtesy of licensed photos and Stable Diffusion. The images with a gray background are synthesized, while the rest are licensed photos.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2411.03047/extracted/5979162/images/fig_compare_1016_v6.png alt></figure></p><blockquote><p>üîº Figure 6 presents a qualitative comparison of five different methods for 3D garment reconstruction from a single image: BCNet, ClothWild, DeepFashion3D, ReEF, and the authors&rsquo; proposed method. Each row shows an input image followed by the results generated by each of the five methods. This allows for a visual comparison of the accuracy, detail, and overall quality of the garment reconstructions produced by each approach. The input images were all generated using Stable Diffusion.</p><details><summary>read the caption</summary>Figure 6. Qualitative comparison between ours and the state of the arts. For each row, the input image is followed by the results generated by BCNet¬†(Jiang et¬†al., 2020), ClothWild¬†(Moon et¬†al., 2022), Deep Fashion3D¬†(Zhu et¬†al., 2020), ReEF¬†(Zhu et¬†al., 2022) and our method. Input images courtesy of Stable Diffusion.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2411.03047/extracted/5979162/images/fig_ablation_boundary_1016_v6.png alt></figure></p><blockquote><p>üîº Figure 7 presents a qualitative comparison of garment boundary prediction methods using real-world images. The figure showcases three columns: (a) the input image, (b) the boundary prediction from the ReEF method, and (c) the boundary prediction from the proposed geometry-aware method. The comparison highlights the superior performance of the proposed method, which accurately reconstructs complex and deformed garment boundaries that closely align with the garment&rsquo;s shape, unlike ReEF&rsquo;s prediction which suffers from inaccuracies and discontinuities, especially in complex poses.</p><details><summary>read the caption</summary>Figure 7. Qualitative comparison between our method and the alternative strategy for predicting garment boundary from in-the-wild images. The input image (a) is followed by the boundaries generated by (b) ReEF‚Äôs strategy and (c) our geometry-aware estimator. ReEF fails to accurately predict boundaries with complex poses and deformations, leading to discontinuous boundaries. Our geometry-aware boundary prediction outperforms ReEF in reconstructing complex garment boundaries that are well-aligned with the garment shape. Input images courtesy of Stable Diffusion.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2411.03047/extracted/5979162/images/fig_ablation_data_1016_v6.png alt></figure></p><blockquote><p>üîº This figure compares the results of 3D garment reconstruction using two different datasets: ReEF and GarVerseLOD. The same input image is used for both models. Column (a) shows the input image. Column (b) presents the reconstruction result obtained by training a model on the ReEF dataset. Column (c) displays the reconstruction result obtained by training a model on the GarVerseLOD dataset. The comparison highlights the impact of different datasets on the accuracy and quality of the garment reconstruction, demonstrating the superior performance of GarVerseLOD. The images are generated using Stable Diffusion.</p><details><summary>read the caption</summary>Figure 8. Qualitative comparison on different data. The input image (a) is followed by the results generated by networks trained with (b) ReEF‚Äôs data and (c) our GarVerseLOD. Input images courtesy of Stable Diffusion.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2411.03047/extracted/5979162/images/fig_ablation_coarse_1016_v6.png alt></figure></p><blockquote><p>üîº Figure 9 compares different approaches for obtaining a coarse garment template, a crucial step in 3D garment reconstruction. It shows the results of two methods: (a) Input image: The image serves as the input to the garment reconstruction process. (b) SMPL-cropped template: A template (the black part) is created by directly cropping a section from an SMPL (Skinned Multi-Person Linear Model) body mesh. This method represents a simplified approach where garment information is borrowed from a general human body model. (c) Registration result using (b): The template from (b) is registered (or aligned) to the input image, producing a coarse garment estimate. (d) Coarse garment estimated by our method: The proposed method estimates a coarse garment template. This method learns garment characteristics directly from data rather than relying on a human body model. (e) Registration result using (d): The template produced by our method is registered to the input image, yielding a coarse garment estimate. The figure demonstrates that using a learned garment estimator (our method) leads to superior registration results compared to simply cropping from a human body model.</p><details><summary>read the caption</summary>Figure 9. Qualitative comparison between our method and the alternative strategy for obtaining coarse garment template. (a) the input image; (b) the template (black part) cropped from SMPL; (c) the registration result using (b); (d) the coarse garment estimated by our coarse garment estimator; and (e) the registration result using (d). Input images courtesy of Stable Diffusion.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2411.03047/extracted/5979162/images/fig_ablation_implicit_udf_1016_v6.png alt></figure></p><blockquote><p>üîº This figure compares the results of using different 3D representations for garment reconstruction: Unsigned Distance Fields (UDF) and occupancy fields. The input image (a) is shown alongside reconstruction attempts using (b) UDF alone, (c) UDF followed by registration to refine the result, (d) an occupancy field, and (e) the occupancy field with subsequent registration. The comparison highlights the effectiveness of the occupancy field approach, especially when combined with registration for accurate garment reconstruction. Images were synthesized using Stable Diffusion.</p><details><summary>read the caption</summary>Figure 10. Qualitative comparison on different representation. The input image (a) is followed by the result generated by (b) UDF, (c) registering to (b), (d) occupancy field and (e) registering to (d). Input images courtesy of Stable Diffusion.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2411.03047/extracted/5979162/images/fig_limitation_1016_v6.png alt></figure></p><blockquote><p>üîº Figure 11 showcases instances where the proposed garment reconstruction method encounters difficulties. Panel (a) illustrates a limitation in handling garments with complex, multi-layered structures, such as layered skirts or dresses. The model struggles to accurately capture the individual layers and their interactions. Panel (b) demonstrates challenges in reconstructing garments with slits or openings. These features present significant topological complexities that the current approach has difficulty resolving. Both examples highlight scenarios where the model&rsquo;s capacity to handle complex garment geometry and topology is limited.</p><details><summary>read the caption</summary>Figure 11. Failure cases. Our framework may struggle to reconstruct garments with complex topology, such as those multi-layered structures (a) or featuring slits (b). Images courtesy of licensed photos and Stable Diffusion.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2411.03047/extracted/5979162/supp_images/fig_template_f322.png alt></figure></p><blockquote><p>üîº This figure shows the five predefined garment templates used as the base for creating the 3D garment models in the GarVerseLOD dataset. Each template represents a basic, T-pose garment shape for a different clothing category: (a) dress, (b) skirt, (c) top, (d) pants, and (e) coat. These templates serve as a starting point for the artists who then manually add detailed geometry and realistic deformations to create the diverse garment models in the dataset.</p><details><summary>read the caption</summary>Figure 12. Predefined templates for each garment category, including (a) dress, (b) skirt, (c) top, (d) pant, and (e) coat.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2411.03047/extracted/5979162/supp_images/fig_deformation_craft.png alt></figure></p><blockquote><p>üîº The figure illustrates the process of creating high-fidelity 3D garment models. It starts with a real image of a person wearing clothes. PyMAF is used to estimate the underlying 3D human body pose (SMPL). Eight artists then manually adjust a template garment mesh to match the T-pose of this estimated body, creating the &lsquo;T-pose Garment&rsquo;. Next, SMPL&rsquo;s Linear Blend Skinning (LBS) is applied to this &lsquo;T-pose Garment&rsquo; to generate a &lsquo;Posed Garment&rsquo; which reflects the basic pose-related deformations. Finally, the artists further refine the &lsquo;Posed Garment&rsquo;, resulting in the &lsquo;Crafted Garment&rsquo;, which incorporates more complex deformations that would not be solely caused by pose, such as those resulting from environmental influences or other factors affecting the fabric. This multi-step process ensures that the final &lsquo;Crafted Garment&rsquo; models accurately reflect the realistic drape and texture of the clothing.</p><details><summary>read the caption</summary>Figure 13. Given a ‚ÄúCollected Image‚Äù, we utilize PyMAF¬†(Zhang et¬†al., 2021, 2023b) to estimate SMPL body. Eight artists are then tasked with creating ‚ÄúT-pose Garment‚Äù shapes by deforming a predefined ‚ÄúTemplate‚Äù to match the T-pose body predicted by PyMAF. Then the SMPL‚Äôs Linear Blend Skinning (LBS) is extended to the T-pose garment to obtain the ‚ÄúPosed Garment‚Äù. Finally, the artists are further instructed to refine the posed garment to get the ‚ÄúCrafted Garment‚Äù while ensuring that garment deformations closely match the collected images. ‚ÄúPosed Garment‚Äù represent the shape of clothing influenced by human pose, while ‚ÄúCrafted Garment‚Äù capture the state of garments affected by various complex factors‚Äînot only pose but also other environmental influences, such as garment-environment interactions and external forces like wind.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2411.03047/extracted/5979162/supp_images/stitched_image_group_0.jpg alt></figure></p><blockquote><p>üîº This figure showcases the results of the proposed method on various loose-fitting garments. It visually demonstrates the ability of the model to handle complex cloth deformations and generate high-fidelity 3D garment reconstructions from single, in-the-wild images. Each image is paired with its corresponding generated 3D model, highlighting the accuracy and detail of the reconstructions.</p><details><summary>read the caption</summary>Figure 14. More Results on Loose-fitting Garments.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2411.03047/extracted/5979162/supp_images/stitched_image_group_1.jpg alt></figure></p><blockquote><p>üîº This figure shows additional results of the proposed method applied to loose-fitting garments. It showcases the model&rsquo;s ability to reconstruct a variety of loose garments with different styles and poses, highlighting its generalization capabilities and robustness to various levels of garment deformation.</p><details><summary>read the caption</summary>Figure 15. More Results on Loose-fitting Garments.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2411.03047/extracted/5979162/supp_images/stitched_image_group_2.jpg alt></figure></p><blockquote><p>üîº This figure showcases additional results of 3D garment reconstruction from single images. It demonstrates the method&rsquo;s ability to handle loose-fitting garments, a challenging scenario due to the increased complexity of garment deformations and the lack of strong visual cues. The images show a variety of loose-fitting garments (dresses, skirts, etc.) and their corresponding reconstructed 3D models. The success in reconstructing the shapes and textures of these loose garments highlights the robustness and generalization capability of the proposed method.</p><details><summary>read the caption</summary>Figure 16. More Results on Loose-fitting Garments.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2411.03047/extracted/5979162/supp_images/stitched_image_group_3.jpg alt></figure></p><blockquote><p>üîº This figure showcases additional results of the proposed method on loose-fitting garments. It demonstrates the method&rsquo;s ability to reconstruct various loose-fitting garments with different shapes, poses, and textures, highlighting its generalization capability and robustness in handling various complex garment deformations. Each image shows an input image followed by its corresponding 3D reconstruction.</p><details><summary>read the caption</summary>Figure 17. More Results on Loose-fitting Garments.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2411.03047/extracted/5979162/supp_images/LOD_0.png alt></figure></p><blockquote><p>üîº This figure shows a collection of simplified garment models from the Garment Style Database. Each model represents a basic garment shape (dress, skirt, coat, top, or pants) in a T-pose, lacking detailed textures or intricate folds. These simplified models serve as foundational templates for generating more complex garments by adding local details and deformations in later stages of the dataset creation process.</p><details><summary>read the caption</summary>Figure 18. An illustration of our Garment Style Database.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2411.03047/extracted/5979162/supp_images/LOD_1.png alt></figure></p><blockquote><p>üîº Figure 19 shows a subset of the Local Detail Database from the GarVerseLOD dataset. This database contains pairs of T-posed garment models, one with and one without fine-grained geometric details such as wrinkles. These pairs are used to learn how to transfer realistic local detail from a detailed model onto a simpler, more basic model. The images illustrate the variety of clothing items and detail levels captured in this part of the dataset.</p><details><summary>read the caption</summary>Figure 19. An illustration of our Local Detail Database.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2411.03047/extracted/5979162/supp_images/LOD_2.png alt></figure></p><blockquote><p>üîº Figure 20 visually showcases the Garment Deformation Database, a key component of the GarVerseLOD dataset. This database contains pairs of T-posed and deformed garment meshes. The T-posed mesh represents the garment in a neutral pose, while the deformed mesh showcases the garment&rsquo;s appearance after undergoing various deformations. These deformations result from a combination of factors like body pose, interactions with the environment, and self-collisions. The paired data within this database are crucial for training the model to learn how different factors influence the garment&rsquo;s shape and overall appearance.</p><details><summary>read the caption</summary>Figure 20. An illustration of our Garment Deformation Database.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2411.03047/extracted/5979162/supp_images/LOD_3.png alt></figure></p><blockquote><p>üîº Figure 21 visually showcases the &lsquo;Fine Garment Dataset,&rsquo; a crucial component of the GarVerseLOD dataset. Unlike the other datasets (Garment Style, Local Detail, and Garment Deformation), this dataset integrates the details from all three, resulting in high-fidelity 3D garment models that capture both global deformations (like those caused by pose) and fine-grained local details (like wrinkles and creases). Each garment model in the dataset presents a complex, realistic representation of clothing.</p><details><summary>read the caption</summary>Figure 21. An illustration of our Fine Garment Dataset.</details></blockquote></details><details><summary>More on tables</summary><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Method</th><th>Chamfer Distance ‚Üì</th><th>Normal Consistency ‚Üë</th><th>IoU ‚Üë</th></tr></thead><tbody><tr><td>ReEF</td><td>16.428</td><td>0.809</td><td>55.425</td></tr><tr><td>Ours</td><td><strong>10.571</strong></td><td><strong>0.862</strong></td><td><strong>69.775</strong></td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents a quantitative comparison of the garment boundary prediction performance between the proposed method and alternative methods. The comparison uses the Chamfer Distance (lower is better), Normal Consistency (higher is better), and Intersection over Union (IoU) (higher is better) metrics to evaluate the accuracy and quality of the predicted garment boundaries. The results demonstrate the effectiveness of the proposed method in accurately predicting garment boundaries compared to existing approaches.</p><details><summary>read the caption</summary>Table 2. Quantitative comparison between our method and alternative strategies for predicting garment boundary.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Method</th><th>Ablation Study on</th><th></th><th></th><th></th><th></th><th>Ours</th><th></th></tr></thead><tbody><tr><td></td><td>Data</td><td>Coarse Garment Estimation</td><td>Implicit Representation</td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td>UDF w/o Registering</td><td>UDF w/ Registering</td><td>Occupancy w/o Registering</td><td></td><td></td></tr><tr><td></td><td>ReEF‚Äôs dataset</td><td>Crop from SMPL</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Chamfer Distance ‚Üì</td><td>16.363</td><td>14.635</td><td>9.616</td><td>9.375</td><td>8.658</td><td><strong>7.825</strong></td><td></td></tr><tr><td>Normal Consistency ‚Üë</td><td>0.805</td><td>0.823</td><td>0.841</td><td>0.848</td><td>0.851</td><td><strong>0.913</strong></td><td></td></tr></tbody></table></table></figure><blockquote><p>üîº Table 3 presents a quantitative comparison of the proposed method against alternative approaches for 3D garment reconstruction. Specifically, it compares the performance using metrics such as Chamfer Distance, Normal Consistency, and Intersection over Union (IoU). The comparison is done using different datasets and strategies to highlight the strengths and weaknesses of each approach.</p><details><summary>read the caption</summary>Table 3. Quantitative comparison between our method and alternative strategies.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Category</th><th>Dress</th><th>Coat</th><th>Skirt</th><th>Top</th><th>Pant</th></tr></thead><tbody><tr><td>Garment Style Database</td><td>863</td><td>760</td><td>538</td><td>350</td><td>358</td></tr><tr><td>Local Detail Database</td><td>86</td><td>62</td><td>55</td><td>38</td><td>36</td></tr><tr><td>Garment Deformation Database</td><td>622</td><td>605</td><td>456</td><td>582</td><td>589</td></tr><tr><td>Total</td><td>1,571</td><td>1,427</td><td>1,049</td><td>970</td><td>983</td></tr></tbody></table></table></figure><blockquote><p>üîº Table 4 provides a detailed breakdown of the GarVerseLOD dataset, categorized into three basic databases: Garment Style Database, Local Detail Database, and Garment Deformation Database. For each database, it shows the number of garments created by artists for each of the five garment categories (dress, skirt, coat, top, and pant). The &lsquo;Total&rsquo; row gives the combined count for each database across all categories. The caption clarifies that the &lsquo;Total&rsquo; numbers represent the total number of garments manually created by artists, not the total number of garments that can be generated using the dataset&rsquo;s synthesis capabilities.</p><details><summary>read the caption</summary>Table 4. Data statistics for each basic database. The total size refers to the number of garments crafted by artists.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption>|| Notation || Description ||
|&mdash;|&mdash;|
| LOD | Levels of Details |
| PCA | Principal Component Analysis |
| $M_C$ | Coarse garment sampled from the Garment Style Database |
| $L_C$, $L_F$ | Garment pair that describes the local geometric detail |
| $M_L$ | Garment after applying the local details from ($L_C$, $L_F$) to $M_C$ |
| $D_T$, $D_F$ | Garment pair that depicts global deformation |
| T | Deformation offsets of ($D_T$, $D_F$) in the rest-pose space |
| LBS | Linear Blend Skinning |
| $M_D$ | Garment after transferring the deformation from ($D_T$, $D_F$) to $M_L$ |
| $G(\cdot)$ | Statistical Garment Model worn on the mean shape of SMPL |
| $\mathbf{T}<em>g$ | Garment Template (i.e., The garment mean shape) |
| $B_g(\cdot)$ | Garment Shape Blend Shape (GSBS) in T-posed space |
| $\alpha$ | The coefficients of $G(\cdot)$, which control the GSBS |
| $T_B(\cdot)$ | T-posed Body Mesh |
| $\mathbf{T}<em>b$ | Body Template (i.e., SMPL‚Äôs mean shape) |
| $B_s(\cdot)$ | Body Shape Blend Shape (BSBS) of SMPL |
| $B_p(\cdot)$ | Body Pose Blend Shape (BPBS) of SMPL |
| $\beta,\theta$ | The shape and pose parameters of SMPL |
| $M_B(\cdot)$ | Posed Body Mesh |
| $W(\cdot)$ | Skinning Function |
| $\mathcal{W}$ | Skinning Weights |
| $J(\cdot)$ | Joint Locations |
| $\widetilde{B}<em>s(\cdot)$ | Garment displacements influenced by the BSBS, i.e., $B_s(\cdot)$ |
| $\widetilde{B}<em>p(\cdot)$ | Garment displacements influenced by the BPBS, i.e., $B_p(\cdot)$ |
| $w(\cdot)$ | Weights for computing garment displacements and skinning |
| $T_G(\cdot)$ | T-posed garment after applying $\widetilde{B}<em>s(\cdot)$ and $\widetilde{B}<em>p(\cdot)$ to $G(\cdot)$ |
| $\widetilde{\mathcal{W}}$ | Garment skinning weights extended from SMPL |
| $M_P(\cdot)$ | Posed Garment Mesh |
| $M_I$ | Fine garment predicted by the pixel-aligned network |
| p | Arbitrary point in 3D space |
| $I_F(\cdot)$ | Pixel-aligned Features |
| $\pi(\cdot)$ | Projection Function |
| $F(\cdot)$ | Feature Extraction Function |
| $z(\cdot)$ | Depth value in the camera coordinate space |
| $f(\cdot)$ | Implicit Function (MLP for decoding the occupancy of p) |
| s | The occupancy status of p to the garment surface |
| $\psi</em>{enc}$ | Triplane Encoder |
| $\psi</em>{dec}$ | MLP-based decoder for decoding the occupancy of p |
| $G_F(\cdot)$ | Geometry-aware Features |
| $F</em>{xy}, F</em>{xz}, F</em>{yz}$ | 3D axis-aligned features of three orthogonal planes |
| $f_i(\cdot)$ | Implicit Function of the i-th boundary, i.e., $\psi</em>{dec}$ |
| $o_i$ | The occupancy status of p to the i-th boundary |
| $L_{boundary}$ | Boundary Fitting Loss |
| $L_c$ | Chamfer Distance Loss [Ravi et al., 2020] |
| $L_{lap}$ | Laplacian Smooth Regularization [Ravi et al., 2020] |
| $L_{edge}$ | Edge Length Regularization [Ravi et al., 2020] |
| $L_{normal}$ | Normal Consistency Regularization [Ravi et al., 2020] |
| $\lambda_c$, $\lambda_{lap}$, $\lambda_{edge}$, $\lambda_{normal}$ | Loss Weight |
| $L_{nicp}$ | Registration Loss (i.e., loss for nicp) |
| $L_d$ | Distance Cost: Deformed Shape vs. GT [Amberg et al., 2007] |
| $L_b, L_s$ | Landmark Cost, Stiffness Term [Amberg et al., 2007] |
| $L_{reg}$ | Mesh Regularization Terms |</table></figure><blockquote><p>üîº This table lists all the notations used in the paper and their corresponding descriptions, providing a comprehensive glossary of symbols and terms for better understanding of the methodologies and results presented.</p><details><summary>read the caption</summary>Table 5. Explanation of notations used in the Main Paper.</details></blockquote></details><h3 class="relative group">Full paper<div id=full-paper class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#full-paper aria-label=Anchor>#</a></span></h3><div id=gallery-35f1fd43caacc0001b11756f151e3c19 class=gallery><img src=https://ai-paper-reviewer.com/2411.03047/1.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.03047/2.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.03047/3.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.03047/4.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.03047/5.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.03047/6.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.03047/7.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.03047/8.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.03047/9.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.03047/10.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.03047/11.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.03047/12.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.03047/13.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.03047/14.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.03047/15.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.03047/16.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.03047/17.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.03047/18.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.03047/19.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.03047/20.png class="grid-w50 md:grid-w33 xl:grid-w25"></div></div><section class="flex flex-row flex-wrap justify-center pt-4 text-xl"><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.03047/&amp;title=GarVerseLOD:%20High-Fidelity%203D%20Garment%20Reconstruction%20from%20a%20Single%20In-the-Wild%20Image%20using%20a%20Dataset%20with%20Levels%20of%20Details" title="Share on LinkedIn" aria-label="Share on LinkedIn"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://twitter.com/intent/tweet/?url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.03047/&amp;text=GarVerseLOD:%20High-Fidelity%203D%20Garment%20Reconstruction%20from%20a%20Single%20In-the-Wild%20Image%20using%20a%20Dataset%20with%20Levels%20of%20Details" title="Tweet on Twitter" aria-label="Tweet on Twitter"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="mailto:?body=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.03047/&amp;subject=GarVerseLOD:%20High-Fidelity%203D%20Garment%20Reconstruction%20from%20a%20Single%20In-the-Wild%20Image%20using%20a%20Dataset%20with%20Levels%20of%20Details" title="Send via email" aria-label="Send via email"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1-27.64 140.9 68.65 266.2 199.1 285.1 19.01 2.888 36.17-12.26 36.17-31.49l1e-4-.6631c0-15.74-11.44-28.88-26.84-31.24-84.35-12.98-149.2-86.13-149.2-174.2.0-102.9 88.61-185.5 193.4-175.4 91.54 8.869 158.6 91.25 158.6 183.2v16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98.0036c-7.299.0-13.2 4.992-15.12 11.68-24.85-12.15-54.24-16.38-86.06-5.106-38.75 13.73-68.12 48.91-73.72 89.64-9.483 69.01 43.81 128 110.9 128 26.44.0 50.43-9.544 69.59-24.88 24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3c0-149.2-133.9-265.632-287.3-235.57zM239.1 304.3c-26.47.0-48-21.56-48-48.05s21.53-48.05 48-48.05 48 21.56 48 48.05-20.6 48.05-48 48.05z"/></svg></span></a></section></div><script>var oid="views_paper-reviews/2411.03047/index.md",oid_likes="likes_paper-reviews/2411.03047/index.md"</script><script type=text/javascript src=/ai-paper-reviewer/js/page.min.0860cf4e04fa2d72cc33ddba263083464d48f67de06114529043cb4623319efed4f484fd7f1730df5abea0e2da6f3538855634081d02f2d6e920b956f063e823.js integrity="sha512-CGDPTgT6LXLMM926JjCDRk1I9n3gYRRSkEPLRiMxnv7U9IT9fxcw31q+oOLabzU4hVY0CB0C8tbpILlW8GPoIw=="></script></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="flex group mr-3" href=/ai-paper-reviewer/paper-reviews/2411.02959/><span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&larr;</span>
<span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">HtmlRAG: HTML is Better Than Plain Text for Modeling Retrieved Knowledge in RAG Systems</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2024-11-05T00:00:00+00:00>5 November 2024</time>
</span></span></a></span><span><a class="flex text-right group ml-3" href=/ai-paper-reviewer/paper-reviews/2411.02844/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Correlation of Object Detection Performance with Visual Saliency and Depth Estimation</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2024-11-05T00:00:00+00:00>5 November 2024</time>
</span></span><span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&larr;</span></a></span></div></div><div class=pt-3><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class=pt-3><script src=https://utteranc.es/client.js repo=pmnxis/pmnxis.github.io issue-term=pathname label=Comment theme=dark-blue crossorigin=anonymous async></script></div></div></footer></article><div id=top-scroller class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer id=site-footer class="py-10 print:hidden"><nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400"><ul class="flex flex-col list-none sm:flex-row"><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=/ai-paper-reviewer/tags/ title>Tags</a></li></ul></nav><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2024
AI Paper Reviews by AI</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a></p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/ai-paper-reviewer/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh+sCQ0E53ghYrxgYqw+0GCRyIEpA=="></script></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://deep-diver.github.io/ai-paper-reviewer/ style=z-index:500><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body><script data-name=BMC-Widget data-cfasync=false src=https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js data-id=chansung data-description="Support me on Buy me a coffee!" data-message data-color=#FFDD00 data-position=Left data-x_margin=18 data-y_margin=18></script></html>