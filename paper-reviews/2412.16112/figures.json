[{"figure_path": "https://arxiv.org/html/2412.16112/x2.png", "caption": "Figure 1: Ultra-resolution results generated by the linearized FLUX.1-dev model with our approach CLEAR. Resolution is marked on the top-right corner of each result in the format of width\u00d7\\times\u00d7height. Corresponding prompts can be found in the appendix.", "description": "This figure showcases high-resolution images generated using the FLUX.1-dev model, enhanced with the CLEAR method. Each image's dimensions are specified in the top right corner (width x height). The diverse range of images demonstrates the model's ability to generate detailed and visually appealing outputs across various resolutions.  The specific text prompts used to generate each image can be found in the appendix of the paper.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2412.16112/x3.png", "caption": "Figure 2: Comparison of speed and GFLOPS between the proposed linearized DiT and the original FLUX.1-dev. Speed is evaluated by performing 20 denoising steps on a single H100 GPU. FLOPS is calculated with the approximation: 4\u00d7\u2211M\u00d7c4\ud835\udc40\ud835\udc504\\times\\sum M\\times c4 \u00d7 \u2211 italic_M \u00d7 italic_c, where c\ud835\udc50citalic_c is the feature dimension and M\ud835\udc40Mitalic_M denotes the attention masks. log2subscript2\\log_{2}roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT is applied on both vertical axes for better visualization. The raw data are supplemented in the appendix.", "description": "This figure compares the speed and computational cost (GFLOPS) of the proposed linearized Diffusion Transformer (DiT) model with the original FLUX.1-dev model.  The speed is determined by measuring the time it takes to perform 20 denoising steps using a single NVIDIA H100 GPU.  The GFLOPS (floating-point operations per second) calculation is an approximation using the formula 4 * \u03a3M * c, where 'c' is the feature dimension, and 'M' represents the attention masks. The logarithmic scale (log2) is used for both axes to enhance the visualization of the results.  Raw data is available in the paper's appendix.", "section": "Abstract"}, {"figure_path": "https://arxiv.org/html/2412.16112/x4.png", "caption": "Figure 3: Preliminary results of various efficient attention methods on FLUX-1.dev. The prompt is \u201cA small blue plane sitting on top of a field\u201d.", "description": "This figure displays the results of different efficient attention mechanisms applied to the FLUX-1.dev model for image generation.  Each method's output image is shown, resulting from the same prompt: \"A small blue plane sitting on top of a field\". This visualization allows for a qualitative comparison of the image quality and detail produced by each attention mechanism, highlighting the strengths and weaknesses of each approach in the context of pre-trained diffusion transformers.", "section": "3. What are Crucial for Linearizing DiTs?"}, {"figure_path": "https://arxiv.org/html/2412.16112/x5.png", "caption": "Figure 4: Visualization of attention maps by various heads for an intermediate denoising step. Attention in pre-trained DiTs is largely conducted in a local fashion.", "description": "This figure visualizes attention maps generated by different attention heads during an intermediate step in the denoising process of a diffusion model.  Each attention map highlights the relationships between different tokens (representing image patches or text embeddings) within the model's input.  The visualization demonstrates that the attention mechanism in pre-trained diffusion transformers (DiTs) primarily focuses on local relationships, with most significant attention scores concentrated within a small spatial neighborhood of each query token. This observation supports the argument made by the authors that local attention patterns are key to successfully converting pretrained DiTs to linear complexity.", "section": "3.1 What are Crucial for Linearizing DiTs?"}, {"figure_path": "https://arxiv.org/html/2412.16112/x6.png", "caption": "Figure 5: We try perturbing remote and local features respectively through clipping the relative distances required for rotary position embedding. Perturbing remote features has no obvious impact on image quality, whereas altering local features results in significant distortion. The text prompt and the original generation result are consistent with Fig.\u00a03.", "description": "This figure demonstrates the importance of local features for image generation in diffusion transformers.  Two experiments are shown: one where remote features (those far from the query token) are perturbed, and another where local features are perturbed.  Perturbing remote features has minimal effect on the generated image quality.  However, altering local features causes significant distortion, highlighting the crucial role of local feature interactions in preserving image quality. The experiment uses rotary position embedding to manipulate features, and the results are consistent with those presented in Figure 3.", "section": "3.1. What are Crucial for Linearizing DiTs?"}, {"figure_path": "https://arxiv.org/html/2412.16112/x7.png", "caption": "Figure 6: Illustration of the proposed convolution-like linearization strategy for pre-trained DiTs. In each text-image joint attention module, text queries aggregate information from all text and image tokens, while each image token gathers information only from tokens within a local circular window.", "description": "This figure illustrates the CLEAR (Convolution-like Linearization) method for efficient attention in Diffusion Transformers (DiTs).  It shows how text queries in a text-image joint attention module access information from all text and image tokens, whereas image queries only interact with tokens within a localized circular window around them. This localized approach reduces the computational complexity of attention, making the model more efficient, especially for high-resolution images.", "section": "3. Methods"}, {"figure_path": "https://arxiv.org/html/2412.16112/x8.png", "caption": "Figure 7: To enhance multi-GPU parallel inference, each text query aggregates only the key-value tokens from the patch managed by its assigned GPU, then averages the attention results across all GPUs, which also generates high-quality images.", "description": "This figure illustrates a method for enhancing multi-GPU parallel inference in the CLEAR model.  Instead of each GPU processing all image tokens, each text query is only assigned tokens from its corresponding patch (a portion of the total image assigned to that GPU). This reduces communication overhead. After each GPU processes its patch, the attention results are averaged across all GPUs before generating the final image, effectively enabling high-quality image generation with significantly faster computation speed.", "section": "3.4 Multi-GPU Parallel Inference"}, {"figure_path": "https://arxiv.org/html/2412.16112/x9.png", "caption": "Figure 8: Qualitative examples by the linearized FLUX-1.dev models with CLEAR and the original model.", "description": "Figure 8 showcases qualitative comparisons between images generated by the original FLUX-1.dev model and its linearized version using the CLEAR method.  The figure visually demonstrates the effectiveness of CLEAR in preserving image quality and detail while significantly reducing computational cost.  Each image pair shares the same prompt, enabling a direct comparison of the outputs from both models.  This allows for a clear assessment of the impact of CLEAR on the final image quality and visual fidelity.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.16112/x10.png", "caption": "Figure 9: Qualitative examples of using CLEAR with SDEdit\u00a0[40] for high-resolution generation (left), FLUX-1.schnell in a zero-shot manner (middle), and ControlNet\u00a0[69] (right). G.T. and Cond. denote ground-truth and condition images, separately.", "description": "Figure 9 demonstrates the versatility of CLEAR, showcasing its application in three scenarios. The leftmost part illustrates CLEAR's ability to enhance high-resolution image generation when combined with SDEdit [40], a method for upscaling images.  The central section shows CLEAR's zero-shot generalization capabilities, seamlessly integrating with FLUX-1.schnell without any additional training.  Finally, the right side exhibits CLEAR's compatibility with ControlNet [69], a plugin that allows for image-guided generation.  Accompanying each scenario are ground truth (G.T.) and condition images for comparison.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2412.16112/x11.png", "caption": "Figure 10: Fine-tuning on real data results in inferior performance compared to fine-tuning on self-generated synthetic data.", "description": "This figure shows a comparison of the training loss curves for fine-tuning a diffusion model using real data versus synthetic data generated by the model itself.  The graph clearly illustrates that fine-tuning with synthetic data leads to significantly lower training loss and faster convergence compared to training with real data. This indicates that using self-generated synthetic data as training examples is more effective for optimizing and linearizing pre-trained diffusion transformers.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.16112/x12.png", "caption": "Figure 11: Training dynamics of various efficient attention alternatives on FLUX-1.dev.", "description": "This figure shows the training loss curves for several efficient attention mechanisms compared to the baseline FLUX-1.dev model.  It illustrates the convergence speed and overall performance of different attention methods during the fine-tuning process on 10K self-generated samples for 10K iterations. The plot allows for a visual comparison of how effectively each attention mechanism learns to perform image denoising in a diffusion model.", "section": "3. Methods"}, {"figure_path": "https://arxiv.org/html/2412.16112/x13.png", "caption": "Figure 12: The linearized DiTs by CLEAR are compatible with various pipelines dedicated for high-resolution inference. The prompt is shown in Fig.\u00a015.", "description": "Figure 12 demonstrates the compatibility of the CLEAR method with different high-resolution inference pipelines.  It shows examples of images generated using the linearized diffusion transformers produced by CLEAR, and processed with image upscaling techniques like SDEdit and I-Max, demonstrating the effectiveness of CLEAR in generating high-resolution images through various pipelines.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.16112/x14.png", "caption": "Figure 13: Qualitative comparisons on FLUX-1.dev (top) and SD3.5-Large (bottom). The left subplots are results by the original models while the right ones are by the CLEAR linearized models. Prompts are listed in Fig.\u00a016.", "description": "Figure 13 presents a qualitative comparison of image generation results between the original FLUX-1.dev and Stable Diffusion 3.5-Large models and their corresponding versions modified with CLEAR (a proposed linearization technique).  The top row shows results from FLUX-1.dev, while the bottom row displays results from Stable Diffusion 3.5-Large. For each model, the left-hand side shows images generated by the original model, whereas the right-hand side presents images generated by the CLEAR-linearized version.  This visual comparison highlights the similarity in image quality between the original and linearized models, showcasing the effectiveness of CLEAR in maintaining performance while reducing computational complexity.  The prompts used to generate these images are detailed in Figure 16.", "section": "4. Experiments"}]