[{"figure_path": "https://arxiv.org/html/2502.06527/x2.png", "caption": "Figure 1: \nCustomVideoX synthesizes natural motions while preserving the fine-grained object details.", "description": "Figure 1 showcases CustomVideoX's ability to generate videos with natural-looking movements and high-fidelity details.  It displays four examples of video generation.  Each example shows how CustomVideoX accurately replicates the fine details of the specified objects (like a cat, dog, cartoon character, or plush toy) while realistically animating them in dynamic settings.  The resulting videos combine natural-looking movement with the preservation of object details, highlighting the effectiveness of the method.", "section": "Abstract"}, {"figure_path": "https://arxiv.org/html/2502.06527/x3.png", "caption": "Figure 2: The overall pipeline of CustomVideoX. CustomVideoX is capable of producing personalized videos that conform to specified instructions, utilizing provided image objects and textual descriptions.\nIt enhances each video frame by incorporating reference image through 3D Reference Attention mechanism, allowing for dynamic interactions between the reference images and video frames, both temporally and spatially. Moreover, CustomVideoX employs a Time-Aware Attention Bias strategy and an Entity Region-Aware Enhancement module to boost spatial and temporal coherence throughout the denoising process, enabling the model to maintain consistent reference feature capture across frames.", "description": "CustomVideoX uses a three-stage process to generate personalized videos.  First, it takes a reference image and textual description as input. Second, it employs 3D Reference Attention to dynamically integrate reference image features with each video frame, ensuring both spatial and temporal consistency.  Third, Time-Aware Attention Bias and Entity Region-Aware Enhancement modules further refine the process, optimizing the balance between reference image influence and textual guidance throughout the video generation process. This approach leads to videos that adhere to instructions while maintaining high quality and visual consistency.", "section": "4. Custom VideoX"}, {"figure_path": "https://arxiv.org/html/2502.06527/x4.png", "caption": "Figure 3: The time-aware attention bias v.s. fixed attention bias in the sampling process. TAB dynamically regulates the influence of reference features using a parabolic temporal mask, enhancing the consistency of reference images throughout the generation sequence.", "description": "Figure 3 illustrates the comparison between time-aware attention bias and fixed attention bias during the video generation process.  The y-axis represents the attention bias, and the x-axis represents the timestep in the sampling process. The blue line shows the fixed attention bias, which remains constant throughout the process. The orange line represents the time-aware attention bias, which dynamically adjusts the influence of reference features using a parabolic temporal mask.  This parabolic function starts with minimal influence at the beginning and end of the generation sequence, gradually increasing in the middle before decreasing again.  This dynamic modulation ensures a smoother and more consistent integration of reference features throughout the video generation process, improving the overall temporal coherence and visual quality.", "section": "4.2. Time-Aware Attention Bias"}, {"figure_path": "https://arxiv.org/html/2502.06527/x5.png", "caption": "Figure 4: Overview of the Proposed VideoBench Framework. From left to right, panel left illustrates word counting, while panel right provides visual examples demonstrating the application of the framework.", "description": "Figure 4 presents a comprehensive overview of the VideoBench framework, a novel benchmark designed for evaluating the performance of customized video generation models. The figure is divided into two main sections: a word count visualization (left panel) and a visual demonstration (right panel).  The word count panel provides a quantitative analysis of the dataset, indicating the number of distinct objects and associated prompts used to create the benchmark, offering insights into its diversity and coverage. The visual demonstration panel displays example videos generated by various methods, showcasing the application and effectiveness of the VideoBench framework, thus highlighting its role in evaluating the quality, consistency, and overall performance of different customized video generation models.", "section": "5.2. VideoBench Benchmark"}, {"figure_path": "https://arxiv.org/html/2502.06527/x6.png", "caption": "Figure 5: The qualitative results compared to the personalized model (+ I2V model). When compared to several different methods, CustomVideoX clearly demonstrates superior capabilities on concept fidelity and caption semantic consistency", "description": "Figure 5 presents a qualitative comparison of video generation results between CustomVideoX and other methods. It showcases CustomVideoX's superior ability to maintain concept fidelity and semantic consistency with the given captions, even when compared to models that use an image-to-video approach (+ I2V).  The figure visually demonstrates the improved quality and alignment of CustomVideoX's generated videos with textual descriptions, highlighting its advantages in personalized video generation.", "section": "6. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.06527/x7.png", "caption": "Figure 6: Compared with VideBooth. CustomVideoX adopts the optimal solution to effectively preserve the fidelity of image prompts and achieve better visual quality.", "description": "Figure 6 presents a comparison of video generation results between CustomVideoX and VideoBooth, highlighting CustomVideoX's superior ability to maintain the visual fidelity of the input image prompt while producing videos of higher overall quality.  The figure visually demonstrates that CustomVideoX more accurately reflects the details and features of the reference image in the generated video sequence, showcasing an improvement in visual quality over VideoBooth.", "section": "6.2. Quantitative Comparison"}, {"figure_path": "https://arxiv.org/html/2502.06527/x8.png", "caption": "Figure 7: Ablation study visualizations comparing module contributions. Demonstration of the effectiveness of the 3D Reference Attention, TAB, and ERAE modules.", "description": "Figure 7 shows an ablation study comparing the effects of three key modules in the CustomVideoX model: 3D Reference Attention, Time-Aware Attention Bias (TAB), and Entity Region-Aware Enhancement (ERAE).  Each row presents a video generated with different combinations of these modules enabled or disabled. By comparing the visual results across the rows, one can assess the individual contribution of each module to the overall quality and consistency of video generation.  The results demonstrate the improvements to generated video clarity, object consistency, and temporal coherence from each component.", "section": "6.3. Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2502.06527/x9.png", "caption": "Figure 8: The overview of video customization data collection pipeline. When dealing with complex scenarios that contain concepts with\nQwen2.5 and Grounding SAM models. Our data pipeline could still extract precise video quality via video resolution, aesthetic score, motion score, and temporal consistency.", "description": "Figure 8 details the process of curating a high-quality video dataset for training a video generation model.  The process starts by filtering videos based on resolution (requiring 1080p or higher), aesthetic score (above 5.3), and motion score to ensure high quality and dynamic content.  Temporal consistency is also checked, discarding videos with poor coherence.  Next, videos undergo further filtering using Qwen-2.5 (a language model) to identify the main entity and assess background complexity.  Complex backgrounds are excluded, ensuring focus on clear subjects. Finally, Grounding SAM (a segmentation model) is used to extract the main object from the first frame of each selected video to create training data pairs, consisting of the extracted object and the corresponding video segment. This multi-stage filtering process ensures that the final dataset contains only high-quality videos with clear subjects and simple backgrounds suitable for training.", "section": "5. Dataset"}, {"figure_path": "https://arxiv.org/html/2502.06527/x10.png", "caption": "Figure 9: Additional results of subject personalization for video generation on diverse scenarios (1/2).", "description": "This figure displays example videos generated by the CustomVideoX model, showcasing its ability to personalize video generation across a variety of scenes and objects.  The examples demonstrate the model's capability to maintain subject consistency and produce high-quality videos, even with complex scenes and diverse object types, including animals, vehicles, and cartoon characters. Each row shows a reference image and the generated video sequence. The results highlight CustomVideoX's performance in handling challenging video generation tasks.", "section": "6. Experiments"}]