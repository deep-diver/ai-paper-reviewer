{"references": [{" publication_date": "2023", "fullname_first_author": "Lei Huang", "paper_title": "A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions", "reason": "This paper provides a comprehensive overview of the hallucination problem in LLMs, covering its principles, taxonomy, and open challenges.  This foundational understanding of the problem is crucial to the current work, which aims to address the issue of hallucination caused by knowledge inconsistency in LLMs.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Ziwei Ji", "paper_title": "Survey of hallucination in natural language generation", "reason": "This work provides another broad survey of hallucination in LLMs, complementing the Huang et al. survey by offering additional perspectives and insights into the different facets of this problem.  It's a key reference for understanding the context and challenges related to LLM hallucination.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Erik Jones", "paper_title": "Teaching language models to hallucinate less with synthetic tasks", "reason": "This paper is directly related to the current work as it explores the use of synthetic data to improve LLM factuality. Although it focuses on consistency with evidence, it is a key comparison point for the current work which focuses on improving inherent factuality without relying on external evidence.  The authors highlight the differences between their approaches. ", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "I-Chun Chern", "paper_title": "Factool: Factuality detection in generative ai a tool augmented framework for multi-task and multi-domain scenarios", "reason": "This paper is relevant because it explores factuality detection, a complementary approach to directly addressing the problem of hallucination. The work shows how factuality detection can be used to assess and improve the factuality of LLM outputs.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Yung-Sung Chuang", "paper_title": "Dola: Decoding by contrasting layers improves factuality in large language models", "reason": "This paper explores improving factuality through decoding algorithms.  It's a related approach to the current work that tackles the problem of hallucination from the perspective of inference.  The current work offers a contrasting approach which addresses the problem at the training stage.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Shehzaad Dhuliawala", "paper_title": "Chain-of-verification reduces hallucination in large language models", "reason": "This work uses chain-of-verification to reduce hallucinations.  It's a noteworthy approach that complements the current work.  The authors present the differences in their method compared to this work in terms of the disentanglement mechanism.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Shangbin Feng", "paper_title": "Knowledge card: Filling LLMs' knowledge gaps with plug-in specialized language models", "reason": "This paper proposes filling knowledge gaps by adding specialized language models.  It is relevant to the current work as both approaches aim to improve LLM knowledge by augmenting their capabilities, but with different mechanisms.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Luyu Gao", "paper_title": "RARR: Researching and revising what language models say, using language models", "reason": "This paper focuses on improving the reliability of LLM outputs by researching and revising what language models say.  It's relevant to the current work because it also tackles the problem of hallucination by improving the quality of LLM responses.  The authors show the contrast of their approach compared to this work.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Zorik Gekhman", "paper_title": "Does fine-tuning llms on new knowledge encourage hallucinations?", "reason": "This paper is highly relevant as it directly investigates the impact of fine-tuning LLMs on new knowledge and explores how this process can lead to hallucinations. The authors study the knowledge inconsistency problem between pre-training and fine-tuning, which is the direct motivation of the current work. ", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Gaurav Ghosal", "paper_title": "Understanding finetuning for factual knowledge extraction", "reason": "This paper studies fine-tuning for factual knowledge extraction, an approach related to the current work that addresses the challenge of factual accuracy. It's important to understand various strategies that attempt to resolve the challenge of factuality in LLM outputs.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Arnav Gudibande", "paper_title": "The false promise of imitating proprietary language models", "reason": "This work is relevant because it critically examines the limitations and potential issues of simply imitating proprietary language models.  It highlights the importance of developing novel methods for training LLMs to overcome such limitations, directly relating to the proposed method of PREREQ-TUNE.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Himanshu Gupta", "paper_title": "TarGEN: Targeted data generation with large language models", "reason": "This paper is related to the current work's use of synthetic data, but with a different focus. TarGEN explores targeted data generation for specific skills or alignments, offering additional insights into the potential benefits of controlling training data in enhancing LLM capabilities.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Katie Kang", "paper_title": "Unfamiliar finetuning examples control how language models hallucinate", "reason": "This paper directly addresses the issue of hallucination caused by unfamiliar fine-tuning examples.  It's relevant because it helps to explain the root cause of the problem that the current work aims to address.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Simran Kaur", "paper_title": "Instruct-skillmix: A powerful pipeline for llm instruction tuning", "reason": "This paper offers a different approach to instruction tuning in LLMs, highlighting various techniques for enhancing the effectiveness of instruction-based fine-tuning. The work provides a broader context for understanding different strategies for improving LLM performance and factuality.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Nayeon Lee", "paper_title": "Factuality enhanced language models for open-ended text generation", "reason": "This paper directly addresses the challenge of enhancing factuality in language models.  It provides another perspective on methods for improving the factual accuracy of LLM outputs, offering a comparison point for the proposed approach in the current work.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Sheng-Chieh Lin", "paper_title": "FLAME: Factuality-aware alignment for large language models", "reason": "This is a key baseline method compared against PREREQ-TUNE in the experiments.  Understanding FLAME's approach and its performance limitations is crucial for evaluating the novelty and effectiveness of PREREQ-TUNE. ", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Katherine Tian", "paper_title": "Fine-tuning language models for factuality", "reason": "This is another key baseline method compared to PREREQ-TUNE.  The authors meticulously describe their methods for enhancing factual accuracy during fine-tuning, providing a strong benchmark against which to measure the performance of PREREQ-TUNE.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Sewon Min", "paper_title": "FActScore: Fine-grained atomic evaluation of factual precision in long form text generation", "reason": "FActScore is the primary metric used to evaluate the factual accuracy of generated text in the experiments.  A thorough understanding of FActScore is essential to interpret the results and assess the validity of the claims made in the paper.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Yasin Abbasi Yadkori", "paper_title": "Mitigating Ilm hallucinations via conformal abstention", "reason": "This paper explores answer abstention as a technique to mitigate hallucinations, offering a complementary strategy to the approach proposed in this work.  Understanding its strengths and limitations helps to provide a better context for evaluating the novel approach presented in this paper.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Chunting Zhou", "paper_title": "LIMA: Less is more for alignment", "reason": "This paper explores a different approach to LLM alignment, offering valuable insights and a comparison point for the current work. The insights from this paper provide an additional context for understanding alignment strategies and their impact on LLM performance and factuality.", "section_number": 4}]}