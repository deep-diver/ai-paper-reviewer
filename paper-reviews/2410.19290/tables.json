[{"figure_path": "2410.19290/tables/table_7_0.html", "caption": "Table 1: Performance for long-form generation tasks (persons and medical entities) and short QA. *: trained with the same hyperparameters as our method to show the impact of prerequisite learning. \u2020: numbers different from Ghosal et al. (2024) because we process ambiguous questions; see Appendix D.2 for results on the original data. +: lower than the original paper because the original model only generates 2.7 claims.", "description": "Table 1 presents the main results of the proposed method PREREQ-TUNE and several baselines on long-form generation (persons and medical entities) and short QA tasks, showing the accuracy and number of claims generated.", "section": "4.2 Main Results"}, {"figure_path": "2410.19290/tables/table_8_0.html", "caption": "Table 2: QA accuracy on fictitious synthetic test data, which contains unseen questions for the skill LORA Askill. Accuracy is computed for two different answers to the same question (V1 and V2).", "description": "Table 2 shows the QA accuracy on a fictitious synthetic test dataset, comparing the accuracy of two different answers (V1 and V2) to the same question using different configurations of the model.", "section": "4.3 THE KNOWLEDGE GROUNDING OF PREREQ-TUNE"}, {"figure_path": "2410.19290/tables/table_8_1.html", "caption": "Table 3: Performance on fictitious synthetic training data. Memorized Entities measures the percentage of named entities in the fictitious persons' biographies that are memorized.", "description": "Table 3 presents the performance results on fictitious synthetic training data, showing the accuracy and the percentage of memorized entities for different models.", "section": "4.3 THE KNOWLEDGE GROUNDING OF PREREQ-TUNE"}, {"figure_path": "2410.19290/tables/table_15_0.html", "caption": "Table 1: Performance for long-form generation tasks (persons and medical entities) and short QA. *: trained with the same hyperparameters as our method to show the impact of prerequisite learning. \u2020: numbers different from Ghosal et al. (2024) because we process ambiguous questions; see Appendix D.2 for results on the original data. +: lower than the original paper because the original model only generates 2.7 claims.", "description": "Table 1 presents the accuracy, and the number of generated claims for three different tasks (long-form generation of persons' biographies and medical entities' descriptions, and short QA) for six different methods, including the proposed method PREREQ-TUNE.", "section": "4.1 EXPERIMENT SETTINGS"}, {"figure_path": "2410.19290/tables/table_16_0.html", "caption": "Table 1: Performance for long-form generation tasks (persons and medical entities) and short QA.", "description": "Table 1 presents the main results of the proposed PREREQ-TUNE method and several baselines on long-form generation tasks (persons and medical entities) and short QA tasks, showing the accuracy and number of claims generated.", "section": "4.2 Main Results"}, {"figure_path": "2410.19290/tables/table_17_0.html", "caption": "Table 1: Performance for long-form generation tasks (persons and medical entities) and short QA. *: trained with the same hyperparameters as our method to show the impact of prerequisite learning. \u2020: numbers different from Ghosal et al. (2024) because we process ambiguous questions; see Appendix D.2 for results on the original data. +: lower than the original paper because the original model only generates 2.7 claims.", "description": "Table 1 presents the main results of the proposed PREREQ-TUNE method and several baselines on three different tasks (long-form generation for persons and medical entities, and short QA) in terms of accuracy and the number of claims generated.", "section": "4.2 Main Results"}, {"figure_path": "2410.19290/tables/table_18_0.html", "caption": "Table 7: Accuracy on the original PopQA, without data cleaning.", "description": "Table 7 presents the accuracy of four different methods on the original PopQA dataset without data cleaning.", "section": "4.2 Main Results"}, {"figure_path": "2410.19290/tables/table_18_1.html", "caption": "Table 8: Performance of different formats for the knowledge dataset Dknow.", "description": "The table presents the performance of different formats for the knowledge dataset (statement-based, passage-based, and both) on short QA and biography generation tasks.", "section": "4.2 Main Results"}, {"figure_path": "2410.19290/tables/table_19_0.html", "caption": "Table 1: Performance for long-form generation tasks (persons and medical entities) and short QA. *: trained with the same hyperparameters as our method to show the impact of prerequisite learning. \u2020: numbers different from Ghosal et al. (2024) because we process ambiguous questions; see Appendix D.2 for results on the original data. +: lower than the original paper because the original model only generates 2.7 claims.", "description": "Table 1 presents the accuracy and number of claims generated by different methods for three tasks: long-form generation (persons and medical entities) and short QA.", "section": "4.2 Main Results"}]