{"references": [{"fullname_first_author": "Josh Achiam", "paper_title": "GPT-4 technical report", "publication_date": "2023-03-08", "reason": "This paper provides the technical details of GPT-4, a large language model that is used as a comparison point for Cuckoo in evaluating performance."}, {"fullname_first_author": "Monica Agrawal", "paper_title": "Large language models are few-shot clinical information extractors", "publication_date": "2022-12-07", "reason": "This paper explores using LLMs for information extraction tasks, providing context for Cuckoo's approach and highlighting the trend of leveraging LLMs for IE."}, {"fullname_first_author": "Sergei Bogdanov", "paper_title": "NuNER: Entity recognition encoder pre-training via LLm-annotated data", "publication_date": "2024-11-12", "reason": "NuNER is a significant baseline for Cuckoo, representing a state-of-the-art pre-trained IE model using LLM-generated data, providing a strong comparison for evaluating Cuckoo's performance."}, {"fullname_first_author": "Colin Raffel", "paper_title": "Exploring the limits of transfer learning with a unified text-to-text transformer", "publication_date": "2020-07-01", "reason": "C4, the dataset introduced in this paper, is a core component of Cuckoo's pre-training data, highlighting its importance in scaling IE model training."}, {"fullname_first_author": "Simone Tedeschi", "paper_title": "MultiNERD: A multilingual, multi-genre and fine-grained dataset for named entity recognition (and disambiguation)", "publication_date": "2022-07-10", "reason": "MultiNERD is a key comparative model for Cuckoo, representing a prominent pre-trained IE model with a large, manually annotated dataset, allowing for a direct comparison of performance and training approaches."}]}