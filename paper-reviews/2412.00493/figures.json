[{"figure_path": "https://arxiv.org/html/2412.00493/x1.png", "caption": "Figure 1: \nComparison of previous work and our method: (a) Previous 3D LLMs are initialized on MLLMs trained solely on image-text pairs, and learn point cloud or voxel representations via fine-tuning on 3D scenes. The 3D point clouds are reconstructed from RGB-D videos.\n(b) Our method directly utilizes video frames and 3D coordinates as input, where the 3D coordinates are converted from depths through coordinate transformation.\nWe then transfer the ability of video understanding to 3D scene understanding by injecting position information into video representations.", "description": "Figure 1 illustrates the core difference between existing 3D large language models (LLMs) and the proposed Video-3D LLM.  (a) shows the conventional approach:  pre-trained LLMs, trained only on 2D image-text data, are fine-tuned with 3D point cloud or voxel representations derived from RGB-D videos. This indirect method struggles to capture the inherent complexity of 3D scenes. (b) highlights the Video-3D LLM approach:  it directly leverages video frames and their corresponding 3D coordinates (obtained via coordinate transformation from depth data) as input. By integrating positional information directly into the video representation, Video-3D LLM effectively bridges the gap between 2D and 3D understanding, leading to improved performance in 3D scene understanding tasks.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2412.00493/x2.png", "caption": "Figure 2: \nThe overview of the model architecture.\n(a) shows the integration of video sequence and global coordinates for creating position-aware video representations.\n(b) and (c) detail the examples of 3D dense captioning and 3D visual grounding, respectively.\nOur approach can generalize well to other 3D tasks.", "description": "Figure 2 illustrates the Video-3D LLM architecture.  Part (a) shows how video sequences and their corresponding 3D global coordinates are integrated to create position-aware video representations.  This is a key innovation, allowing the model to understand spatial context within 3D scenes. Parts (b) and (c) provide concrete examples of how this architecture is used for two specific 3D scene understanding tasks: 3D dense captioning (generating detailed descriptions of objects) and 3D visual grounding (locating objects based on textual descriptions). The figure highlights the model's versatility, suggesting its applicability to a wide range of other 3D scene understanding tasks.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2412.00493/x3.png", "caption": "Figure 3: \nThe visualization results on ScanRefer.\nThe green/red/blue colors indicate the correct/incorrect/ground truth boxes.", "description": "This figure visualizes the results of the ScanRefer task, a 3D visual grounding benchmark.  It showcases several examples where the model attempts to locate objects based on textual descriptions.  Each example displays three boxes: a green box representing the model's correct prediction, a red box showing an incorrect prediction, and a blue box indicating the ground truth object location. The visualization helps demonstrate the accuracy and limitations of the Video-3D LLM model in locating objects within 3D scenes.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.00493/x4.png", "caption": "Figure 4: \nThe visualization results on Scan2Cap.\nThe input boxes are marked in blue.", "description": "This figure showcases example results from the Scan2Cap task.  It presents several examples where the model generates captions for objects within a 3D scene.  For each example, the ground truth (GT) caption and the model's generated caption are shown alongside the visual input.  The input includes bounding boxes around objects (in blue), illustrating the model's understanding of spatial relations. Comparing the generated and ground truth captions highlights the model's success (or challenges) in accurately describing objects and their contexts within the 3D scene.", "section": "4. Experiments"}]