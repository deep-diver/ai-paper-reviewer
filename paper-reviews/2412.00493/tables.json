[{"content": "| Method | 3D Generalist | ScanRefer Acc@0.25 | ScanRefer Acc@0.5 | ScanRefer F1@0.25 | ScanRefer F1@0.5 | Multi3DRef B-4@0.5 | Multi3DRef C@0.5 | Scan2Cap C | Scan2Cap EM | ScanQA EM | SQA3D EM |\n|---|---|---|---|---|---|---|---|---|---|---|---| \n| **Expert Models** |  |  |  |  |  |  |  |  |  |  |  |\n| ScanRefer [6] |  | 37.3 | 24.3 |  |  |  |  |  |  |  |  |\n| MVT [24] |  | 40.8 | 33.3 |  |  |  |  |  |  |  |  |\n| 3DVG-Trans [54] |  | 45.9 | 34.5 |  |  |  |  |  |  |  |  |\n| ViL3DRel [9] |  | 47.9 | 37.7 |  |  |  |  |  |  |  |  |\n| M3DRef-CLIP [52] |  | 51.9 | 44.7 | 42.8 |  | 38.4 |  |  |  |  |  |\n| Scan2Cap [7] |  |  |  |  |  |  |  | 22.4 | 35.2 |  |  |\n| ScanQA [3] |  |  |  |  |  |  |  |  |  | 64.9 | 21.1 |\n| 3D-VisTA [58] |  | 50.6 | 45.8 |  |  |  |  | 34.0 | 66.9 | 69.6 | 22.4 |\n| **2D LLMs** |  |  |  |  |  |  |  |  |  |  |  |\n| Oryx-34B [35] |  | \u2013 | \u2013 | \u2013 | \u2013 | \u2013 | \u2013 | \u2013 | 72.3 | \u2013 | \u2013 |\n| LLaVA-Video-7B [53] |  | \u2013 | \u2013 | \u2013 | \u2013 | \u2013 | \u2013 | \u2013 | 88.7 | \u2013 | 48.5 |\n| **3D LLMs** |  |  |  |  |  |  |  |  |  |  |  |\n| 3D-LLM(Flamingo) [21] |  | 21.2 | \u2013 | \u2013 | \u2013 | \u2013 | \u2013 | \u2013 | 59.2 | 20.4 | \u2013 |\n| 3D-LLM(BLIP2-flant5) [21] |  | 30.3 | \u2013 | \u2013 | \u2013 | \u2013 | \u2013 | \u2013 | 69.4 | 20.5 | \u2013 |\n| Chat-3D [45] |  | \u2013 | \u2013 | \u2013 | \u2013 | \u2013 | \u2013 | \u2013 | 53.2 | \u2013 |  |\n| Chat-3D v2 [22] | \u2713 | 42.5 | 38.4 | 45.1 | 41.6 | 31.8 | 63.9 | 87.6 |  | 54.7 |\n| LL3DA [11] | \u2713 | \u2013 | \u2013 | \u2013 | \u2013 |  |  | 36.0 | 62.9 | 76.8 | \u2013 |\n| SceneLLM [19] | \u2713 | \u2013 | \u2013 | \u2013 | \u2013 |  |  |  |  | 80.0 | 27.2 |\n| LEO [23] | \u2713 | \u2013 | \u2013 | \u2013 | \u2013 |  |  | 38.2 | 72.4 | 101.4 | 50.0 |\n| Grounded 3D-LLM [14] | \u2713 | 47.9 | 44.1 | 45.2 | 40.6 | 70.6 | 35.5 | 72.7 |  |  |  |\n| PQ3D [59] | \u2713 | 57.0 | 51.2 |  | 50.1 | 36.0 | 80.3 |  |  | 47.1 |\n| ChatScene [22] | \u2713 | 55.5 | 50.2 | 57.1 | 52.4 | 36.3 | 77.1 | 87.7 | 21.6 | 54.6 |\n| LLaVA-3D [57] | \u2713 | 54.1 | 42.4 |  |  | 41.1 | 79.2 | 91.7 | 27.0 | 55.6 |\n| **Video-3D LLM (MC)** | \u2713 | 57.9 | 51.2 | 57.9 | 52.4 | 40.2 | 80.0 | 100.5 | 29.5 | 57.7 |\n| **Video-3D LLM (Uniform)** | \u2713 | **58.1** | **51.7** | **58.0** | **52.7** | **41.3** | **83.8** | **102.1** | **30.1** | **58.6** |", "caption": "Table 1: Overall performance comparison.\n\u201cExpert models\u201d are customized for specific tasks through task-oriented heads.\n\u201c3D Generalist\u201d means the model can perform multiple 3D tasks in a single model.\nLLaVA-Video is assessed in a zero-shot setting.", "description": "Table 1 presents a comprehensive comparison of the proposed Video-3D LLM model's performance against various state-of-the-art methods across five distinct 3D scene understanding benchmarks.  These benchmarks evaluate performance on different tasks, including 3D visual grounding (ScanRefer and Multi3DRefer), 3D dense captioning (Scan2Cap), and 3D question answering (ScanQA and SQA3D). The table highlights the distinction between \"Expert Models,\" which are specifically designed and trained for individual tasks, and \"3D Generalist\" models like Video-3D LLM, capable of handling multiple tasks within a single architecture. The results showcase Video-3D LLM's superior performance compared to other generalist models and its competitive performance against expert models, even in zero-shot scenarios (as seen with LLaVA-Video).", "section": "4. Experiments"}, {"content": "| Frame Number | Sampling Strategy | Inference Time | ScanRefer Acc@0.25 | ScanRefer Acc@0.5 | ScanRefer F1@0.25 | ScanRefer F1@0.5 | Multi3dRefer B-4@0.5 | Multi3dRefer C@0.5 | Scan2Cap C | Scan2Cap EM | ScanQA EM | SQA3D EM |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| **Fixed Frame Number** |  |  |  |  |  |  |  |  |  |  |  |\n| 8 | Uniform | 309ms | 48.93 | 43.50 | 49.80 | 45.40 | 37.34 | 68.82 | 94.98 | 27.57 | 56.77 |\n| 8 | MC |  | 53.47 | 47.41 | 53.55 | 48.54 | 38.77 | 73.08 | 96.37 | 28.00 | 56.97 |\n| 16 | Uniform | 537ms | 55.42 | 49.17 | 54.95 | 49.82 | 39.39 | 76.96 | 99.86 | 28.96 | 57.70 |\n| 16 | MC |  | 56.46 | 50.11 | 56.65 | 51.39 | 39.59 | 76.84 | 100.63 | 29.49 | 57.82 |\n| 32 | Uniform | 1050ms | 58.11 | **51.72** | **58.02** | **52.68** | **41.30** | **83.76** | 102.06 | 30.09 | 58.56 |\n| 32 | MC |  | **58.27** | 51.68 | 57.93 | 52.50 | 40.32 | 81.58 | **102.33** | **30.35** | **59.25** |\n| **Adaptive Frame Number** |  |  |  |  |  |  |  |  |  |  |  |\n| \u224818 | MC* | 527ms | 57.86 | 51.18 | 57.87 | 52.40 | 40.18 | 80.00 | 100.54 | 29.50 | 57.72 |\n| **Previous SOTA** |  |  |  |  |  |  |  |  |  |  |  |\n| LLaVA-3D [57] |  | 433ms | 54.1 | 42.4 | \u2013 | \u2013 | 41.1 | 79.2 | 91.7 | 27.0 | 55.6 |", "caption": "Table 2: Ablation study for the effect of frame sampling strategy. \u201cMC\u201d represents maximum coverage sampling. \u201cMC\u2217\u201d denotes sampling frames until over 95% of the scene\u2019s voxels are covered or a maximum of 32 frames is reached.", "description": "This ablation study analyzes the impact of different frame sampling strategies on the performance of the Video-3D LLM model.  It compares three approaches: using a fixed number of frames sampled uniformly, using a fixed number of frames selected via a maximum coverage strategy, and using an adaptive number of frames determined by the maximum coverage strategy, stopping when a coverage threshold is met. The results are evaluated across several 3D scene understanding benchmarks (ScanRefer, Multi3DRefer, Scan2Cap, ScanQA, and SQA3D) using multiple metrics.  The table shows the impact of each sampling strategy on both accuracy (across various metrics depending on the task) and inference time.", "section": "3.1. Frame Sampling Strategy"}, {"content": "| 3D-PE | Coord. | ScanRefer Acc@0.25 | ScanRefer Acc@0.5 | Scan2Cap C@0.5 | ScanQA EM |\n|---|---|---|---|---|---| \n| **3D-PE** | **Coord.** |  |  |  |  |\n| None | Avg | 57.50 | 50.84 | 31.03 | 30.03 |\n| MLP |  | **59.63** | **52.98** | 76.23 | 29.62 |\n| Sin |  | 58.11 | 51.72 | **83.76** | **30.09** |\n| Sin | Center | 57.53 | 51.06 | 80.88 | 29.39 |\n|  | Min-Max | 58.05 | **51.77** | 82.75 | **30.18** |\n|  | **Avg** |  58.11 | 51.72 | **83.76** | 30.09 |", "caption": "Table 3: Ablation study for the effect of coordinate encoding. \u201cCoord.\u201d means the method for aggregating the coordinates.", "description": "This table presents an ablation study analyzing the impact of different coordinate encoding methods on the model's performance. It investigates various techniques for aggregating 3D coordinates, comparing their effects on key metrics across multiple 3D scene understanding tasks.  The results help determine the optimal approach for incorporating spatial information into the model's video representations.", "section": "3. Method"}, {"content": "| Patch Size | Loss | ScanRefer Acc@0.25 | ScanRefer Acc@0.5 | Multi3DRefer Acc@0.5 | Multi3DRefer Acc@0.5 |\n|---|---|---|---|---|---| \n| 14 | InfoNCE | **56.44** | **50.08** | **56.31** | **51.05** |\n| 27 | InfoNCE | 55.23 | 48.93 | 56.13 | 50.90 |\n| 14 | BCE | 51.63 | 45.82 | 46.07 | 41.47 |", "caption": "Table 4: Ablation study for the effect of visual grounding.\nWe train the model separately on the ScanRefer and Multi3DRefer datasets.", "description": "This table presents an ablation study focusing on the impact of different training strategies for 3D visual grounding.  Instead of training a single model on both ScanRefer and Multi3DRefer datasets simultaneously, this experiment trains separate models for each dataset to isolate and analyze the effects of each training configuration on the final performance metrics. The results help determine the best approach for achieving high accuracy in 3D visual grounding tasks.", "section": "4.3 Ablation Study"}, {"content": "| Data | Data Count | Scan Count | Ques length | Answer Length |\n|---|---|---|---|---|\n| ScanRefer [6] | 36,665 | 562 | 24.9 | \u2013 |\n| Multi3DRefer [52] | 43,838 | 562 | 34.8 | \u2013 |\n| Scan2Cap [7] | 36,665 | 562 | 13.0 | 17.9 |\n| ScanQA [3] | 26,515 | 562 | 13.7 | 2.4 |\n| SQA3D [36] | 79,445 | 518 | 37.8 | 1.1 |", "caption": "Table 5: \nDetailed statistics for training data.\nWe report the average lengths for questions and answers, respectively.", "description": "Table 5 presents a statistical overview of the training datasets used in the paper.  It details the number of data points (scan, question, answer) for each dataset (ScanRefer, Multi3DRefer, Scan2Cap, ScanQA, and SQA3D) and provides the average lengths of the questions and answers in each dataset. This information is crucial for understanding the scale and characteristics of the data used to train the proposed Video-3D LLM model.", "section": "4.1 Experimental Setup"}, {"content": "| Dataset | Data Count | Scan Count | Ques length | Answer Length |\n|---|---|---|---|---|\n| ScanRefer [6](https://arxiv.org/html/2412.00493v1#bib.bib6) (Val) | 9,508 | 141 | 25.0 | \u2013 |\n| Multi3DRefer [52](https://arxiv.org/html/2412.00493v1#bib.bib52) (Val) | 11,120 | 141 | 34.7 | \u2013 |\n| Scan2Cap [7](https://arxiv.org/html/2412.00493v1#bib.bib7) (Val) | 2,068 | 141 | 13.0 | 18.7 |\n| ScanQA [3](https://arxiv.org/html/2412.00493v1#bib.bib3) (Val) | 4,675 | 71 | 13.8 | 2.4 |\n| SQA3D [36](https://arxiv.org/html/2412.00493v1#bib.bib36) (Test) | 3,519 | 67 | 36.3 | 1.1 |", "caption": "Table 6: Detailed statistics for testing data.\nWe report the average lengths for questions and answers, respectively.", "description": "This table presents a statistical overview of the testing data used in the experiments, specifically focusing on the lengths of questions and answers across different datasets. The datasets include ScanRefer, Multi3DRefer, Scan2Cap, ScanQA, and SQA3D. For each dataset, the average length of questions and the average length of answers are provided, giving insights into the scale and complexity of the textual data involved in the 3D scene understanding benchmarks.", "section": "4.1. Experimental Setup"}, {"content": "| Method | What | Is | How | Can | Which | Others | Avg. |\n|---|---|---|---|---|---|---|---| \n| SQA3D [36] | 31.6 | 63.8 | 46.0 | 69.5 | 43.9 | 45.3 | 46.6 |\n| 3D-VisTA [58] | 34.8 | 63.3 | 45.4 | 69.8 | 47.2 | 48.1 | 48.5 |\n| LLaVA-Video [53] | 42.7 | 56.3 | 47.5 | 55.3 | 50.1 | 47.2 | 48.5 |\n| Scene-LLM [19] | 40.9 | 69.1 | 45.0 | 70.8 | 47.2 | 52.3 | 54.2 |\n| LEO [23] | \u2013 | \u2013 | \u2013 | \u2013 | \u2013 | \u2013 | 50.0 |\n| ChatScene [22] | 45.4 | 67.0 | 52.0 | 69.5 | 49.9 | 55.0 | 54.6 |\n| LLaVA-3D [57] | \u2013 | \u2013 | \u2013 | \u2013 | \u2013 | \u2013 | 55.6 |\n| Video-3D LLM (Uniform) | 51.1 | 72.4 | 55.5 | 69.8 | 51.3 | 56.0 | 58.6 |\n| Video-3D LLM (MC) | 50.0 | 70.7 | 57.9 | 69.8 | 50.1 | 55.8 | 57.7 |", "caption": "Table 7: Performance comparison on the test set of SQA3D [36].", "description": "This table presents a detailed comparison of different models' performance on the SQA3D benchmark's test set.  It shows the average exact match accuracy (EM) across different question types (What Is, How Can, Which, Others), providing a comprehensive evaluation of each model's ability to answer questions related to 3D scenes. The models compared include various state-of-the-art and baseline models in 3D scene understanding.", "section": "4.2. Comparison with State-of-the-art Methods"}, {"content": "| Method | C | B-4 | M | R |\n|---|---|---|---|---|\n| Scan2Cap [7] | 39.08 | 23.32 | 21.97 | 44.48 |\n| 3DJCG [4] | 49.48 | 31.03 | 24.22 | 50.80 |\n| D3Net [8] | 62.64 | 35.68 | 25.72 | 53.90 |\n| 3D-VisTA [58] | 66.9 | 34.0 | 27.1 | 54.3 |\n| LL3DA [11] | 65.19 | 36.79 | 25.97 | 55.06 |\n| LEO [23] | 68.4 | 36.9 | 27.7 | 57.8 |\n| ChatScene [22] | 77.19 | 36.34 | 28.01 | 58.12 |\n| LLaVA-3D [57] | 79.21 | 41.12 | **30.21** | **63.41** |\n| Video-3D LLM (Uniform) | **83.77** | **42.43** | 28.87 | 62.34 |\n| Video-3D LLM (MC) | 80.00 | 40.18 | 28.49 | 61.68 |", "caption": "Table 8: Performance comparison on the validation set of Scan2Cap\u00a0[7]. C, B-4, M, R represent CIDEr, BLEU-4, Meteor, Rouge-L, respectively.", "description": "This table presents a detailed comparison of different models' performance on the Scan2Cap benchmark, a task focused on generating detailed captions for objects within 3D scenes.  It compares several state-of-the-art models, highlighting their performance using four common evaluation metrics: CIDEr, BLEU-4, Meteor, and Rouge-L.  These metrics provide a comprehensive evaluation of caption quality by assessing various aspects like coherence, fluency, and semantic similarity to reference captions.", "section": "4.2. Experiments"}, {"content": "| Method | Venue | Unique Acc@0.25 | Unique Acc@0.5 | Multiple Acc@0.25 | Multiple Acc@0.5 | Overall Acc@0.25 | Overall Acc@0.5 |\n|---|---|---|---|---|---|---|---| \n| ScanRefer [6] | ECCV20 | 76.33 | 53.51 | 32.73 | 21.11 | 41.19 | 27.40 |\n| MVT [24] | CVPR22 | 77.67 | 66.45 | 31.92 | 25.26 | 40.80 | 33.26 |\n| 3DVG-Transformer [54] | ICCV21 | 81.93 | 60.64 | 39.30 | 28.42 | 47.57 | 34.67 |\n| ViL3DRel [9] | NeurIPS22 | 81.58 | 68.62 | 40.30 | 30.71 | 47.94 | 37.73 |\n| 3DJCG [4] | CVPR22 | 83.47 | 64.34 | 41.39 | 30.82 | 49.56 | 37.33 |\n| D3Net [8] | ECCV22 | \u2013 | 72.04 | \u2013 | 30.05 | \u2013 | 37.87 |\n| M3DRef-CLIP [52] | ICCV23 | 85.3 | 77.2 | 43.8 | 36.8 | 51.9 | 44.7 |\n| 3D-VisTA [58] | ICCV23 | 81.6 | 75.1 | 43.7 | 39.1 | 50.6 | 45.8 |\n| 3D-LLM (Flamingo) [21] | NeurIPS23 | \u2013 | \u2013 | \u2013 | \u2013 | 21.2 | \u2013 |\n| 3D-LLM (BLIP2-flant5) [21] | NeurIPS23 | \u2013 | \u2013 | \u2013 | \u2013 | 30.3 | \u2013 |\n| Grounded 3D-LLM [14] | ArXiv24 | \u2013 | \u2013 | \u2013 | \u2013 | 47.9 | 44.1 |\n| PQ3D [59] | ECCV24 | 86.7 | 78.3 | 51.5 | 46.2 | 57.0 | 51.2 |\n| ChatScene [22] | NeurIPS24 | 89.59 | 82.49 | 47.78 | 42.90 | 55.52 | 50.23 |\n| LLaVA-3D [57] | ArXiv24 | \u2013 | \u2013 | \u2013 | \u2013 | 54.1 | 42.2 |\n| Video-3D LLM (Uniform) | \u2013 | 87.97 | 78.32 | 50.93 | 45.32 | 58.12 | 51.72 |\n| Video-3D LLM (MC) | \u2013 | 86.61 | 77.02 | 50.95 | 44.96 | 57.87 | 51.18 |", "caption": "Table 9: Performance comparison on the validation set of ScanRefer [6].\n\u201cUnique\u201d and \u201cMultiple\u201d depends on whether there are other objects of the same class as the target object.", "description": "This table presents a quantitative comparison of various models on the ScanRefer benchmark for 3D visual grounding.  It shows the performance of different models, categorized as expert models (designed specifically for ScanRefer), 2D LLMs, and 3D LLMs, along with the proposed Video-3D LLM. Performance is measured using accuracy at two Intersection over Union (IoU) thresholds (Acc@0.25 and Acc@0.5).  The results are further broken down into \"Unique\" and \"Multiple\" scenarios: Unique denotes scenes with only one object of the target class, while Multiple denotes scenes with multiple objects of the target class. This breakdown helps to analyze how well each model generalizes to different levels of visual complexity and object density within 3D scenes.", "section": "4.2. Results"}, {"content": "| Method | ZT w/o D F1 | ZT w/ D F1 | ST w/o D F1@0.25 | ST w/o D F1@0.5 | ST w/ D F1@0.25 | ST w/ D F1@0.5 | MT F1@0.25 | MT F1@0.5 | ALL F1@0.25 | ALL F1@0.5 |\n|---|---|---|---|---|---|---|---|---|---|---|\n| M3DRef-CLIP [52] | 81.8 | 39.4 | 53.5 | 47.8 | 34.6 | 30.6 | 43.6 | 37.9 | 42.8 | 38.4 |\n| D3Net [8] | 81.6 | 32.5 | \u2013 | 38.6 | \u2013 | 23.3 | \u2013 | 35.0 | \u2013 | 32.2 |\n| 3DJCG [4] | 94.1 | 66.9 | \u2013 | 26.0 | \u2013 | 16.7 | \u2013 | 26.2 | \u2013 | 26.6 |\n| Grounded 3D-LLM [14] | \u2013 | \u2013 | \u2013 | \u2013 | \u2013 | \u2013 | \u2013 | \u2013 | 45.2 | 40.6 |\n| PQ3D [59] | 85.4 | 57.7 | \u2013 | 68.5 | \u2013 | 43.6 | \u2013 | 40.9 | \u2013 | 50.1 |\n| ChatScene [22] | 90.3 | 62.6 | 82.9 | 75.9 | 49.1 | 44.5 | 45.7 | 41.1 | 57.1 | 52.4 |\n| Video-3D LLM (Uniform) | 94.7 | 78.5 | 82.6 | 73.4 | 52.1 | 47.2 | 40.8 | 35.7 | 58.0 | 52.7 |\n| Video-3D LLM (MC) | 94.1 | 76.7 | 81.2 | 72.6 | 52.7 | 47.4 | 40.6 | 35.3 | 57.9 | 52.4 |", "caption": "Table 10: Performance comparison on the validation set of Multi3DRefer [52]. ZT: zero-target, ST: single-target, MT: multi-target, D: distractor.", "description": "This table presents a detailed comparison of different models' performance on the Multi3DRefer dataset.  Multi3DRefer is a benchmark for 3D visual grounding, focusing on the task of locating multiple objects in a 3D scene based on textual descriptions. The table breaks down the results based on several key factors: \n\n* **Zero-target (ZT):**  Indicates scenarios where the description doesn't specify the number of target objects to locate.\n* **Single-target (ST):** Indicates scenarios with descriptions explicitly specifying one target object.\n* **Multi-target (MT):** Indicates scenarios with descriptions specifying multiple target objects.\n* **With Distractors (w/D):**  Indicates scenes that contain additional objects that are not the target objects, adding difficulty to the task.\n* **Without Distractors (w/o D):** Indicates scenes without these additional distracting objects.\n\nThe evaluation metric used is the F1-score, which is calculated at an IoU threshold of 0.25 and 0.5. A higher F1-score indicates better performance. By separating the results in this manner, the table allows for a granular analysis of how different models perform under varying complexities of the 3D visual grounding task.", "section": "4.2. Results"}, {"content": "| Method | Venue | EM | B-1 | B-2 | B-3 | B-4 | ROUGE-L | METEOR | CIDEr |\n|---|---|---|---|---|---|---|---|---|---| \n| ScanQA [3] | CVPR22 | 21.05 | 30.24 | 20.40 | 15.11 | 10.08 | 33.33 | 13.14 | 64.86 |\n| 3D-VisTA [58] | ICCV23 | 22.4 | \u2013 | \u2013 | \u2013 | 10.4 | 35.7 | 13.9 | 69.6 |\n| Oryx-34B [35] | ArXiv24 | \u2013 | 38.0 | 24.6 | \u2013 | \u2013 | 37.3 | 15.0 | 72.3 |\n| LLaVA-Video-7B [53] | ArXiv24 | \u2013 | 39.71 | 26.57 | 9.33 | 3.09 | 44.62 | 17.72 | 88.70 |\n| 3D-LLM (Flamingo) [21] | NeurIPS23 | 20.4 | 30.3 | 17.8 | 12.0 | 7.2 | 32.3 | 12.2 | 59.2 |\n| 3D-LLM (BLIP2-flant5) [21] | NeurIPS23 | 20.5 | 39.3 | 25.2 | 18.4 | 12.0 | 35.7 | 14.5 | 69.4 |\n| Chat-3D [45] | ArXiv23 | \u2013 | 29.1 | \u2013 | \u2013 | 6.4 | 28.5 | 11.9 | 53.2 |\n| NaviLLM [55] | CVPR24 | 23.0 | \u2013 | \u2013 | \u2013 | 12.5 | 38.4 | 15.4 | 75.9 |\n| LL3DA [11] | CVPR24 | \u2013 | \u2013 | \u2013 | \u2013 | 13.53 | 37.31 | 15.88 | 76.79 |\n| Scene-LLM [19] | ArXiv24 | 27.2 | 43.6 | 26.8 | 19.1 | 12.0 | 40.0 | 16.6 | 80.0 |\n| LEO [23] | ICML24 | \u2013 | \u2013 | \u2013 | \u2013 | 11.5 | 39.3 | 16.2 | 80.0 |\n| Grounded 3D-LLM [14] | ArXiv24 | \u2013 | \u2013 | \u2013 | \u2013 | 13.4 | \u2013 | \u2013 | 72.7 |\n| ChatScene [22] | NeurIPS24 | 21.62 | 43.20 | 29.06 | 20.57 | 14.31 | 41.56 | 18.00 | 87.70 |\n| LLaVA-3D [57] | arXiv24 | 27.0 | \u2013 | \u2013 | \u2013 | 14.5 | 50.1 | 20.7 | 91.7 |\n| Video-3D LLM (Uniform) | \u2013 | **30.10** | **47.05** | **31.70** | **22.83** | 16.17 | 49.02 | 19.84 | **102.06** |\n| Video-3D LLM (MC) | \u2013 | 29.50 | 46.23 | 31.22 | 22.71 | **16.28** | 48.19 | 19.36 | 100.54 |", "caption": "Table 11: Performance comparison on the validation set of ScanQA [3].\nEM indicates exact match accuracy, and B-1, B-2, B-3, B-4 denote BLEU-1, -2, -3, -4, respectively.", "description": "Table 11 presents a detailed comparison of the performance of various models on the ScanQA benchmark's validation set.  The benchmark assesses a model's ability to answer questions about 3D scenes. The table lists multiple models, including the proposed Video-3D LLM and several state-of-the-art baselines.  For each model, it shows the exact match accuracy (EM) and BLEU scores (B-1, B-2, B-3, B-4), which are common metrics for evaluating the quality of generated text. This allows for a direct comparison of the model's ability to generate accurate and fluent answers to the 3D scene questions.  The inclusion of multiple metrics provides a comprehensive evaluation of the model's performance.", "section": "4.2. Experiments"}]