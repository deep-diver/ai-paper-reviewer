{"importance": "**SocialGPT** significantly advances social relation reasoning by leveraging foundation models, achieving competitive zero-shot results and offering interpretability.  This opens exciting avenues for research in multimodal reasoning and prompt engineering, particularly for tasks involving complex cognitive functions like social understanding.", "summary": "SocialGPT uses vision and language foundation models for zero-shot social relation reasoning, achieving state-of-the-art results with improved interpretability via Greedy Segment Prompt Optimization.", "takeaways": ["**SocialGPT**, a modular framework, combines vision and language models for social relation reasoning.", "**SocialGPT** achieves competitive zero-shot performance without additional model training.", "**GSPO**, a greedy segment prompt optimization, significantly improves LLM performance in SocialGPT."], "tldr": "Current social relation reasoning methods are limited by their generalizability and lack of interpretability.  They typically rely on end-to-end training of dedicated networks on large labeled datasets, which hinders their adaptability to unseen scenarios. This paper proposes a new approach by using a modular framework, incorporating the strengths of both Vision Foundation Models (VFMs) and Large Language Models (LLMs), to improve performance and interpretability.\nThis modular approach translates visual content into a textual social story using VFMs, facilitating reasoning with LLMs. To further enhance performance, the paper introduces Greedy Segment Prompt Optimization (GSPO), an automated prompt optimization method that addresses the challenges of long prompt optimization in LLMs.  The experimental results demonstrate that SocialGPT, combined with GSPO, significantly outperforms existing methods on benchmark datasets, showcasing its effectiveness and versatility.", "affiliation": "Harvard University", "categories": {"main_category": "Multimodal Learning", "sub_category": "Vision-Language Models"}}