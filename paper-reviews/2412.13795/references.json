{"references": [{"fullname_first_author": "Tom B Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-05-28", "reason": "This paper introduces the groundbreaking GPT-3 model, which significantly advanced the field of large language models and is foundational to the research discussed in the current paper."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "LLaMA: Open and Efficient Foundation Language Models", "publication_date": "2023-02-27", "reason": "This work introduces the LLaMA family of LLMs, which is a central focus of analysis and experimentation in the discussed paper."}, {"fullname_first_author": "Ruibin Xiong", "paper_title": "On Layer Normalization in the Transformer Architecture.", "publication_date": "2020-07-13", "reason": "This study provides key theoretical insights into the behavior of layer normalization in transformers, which directly informs the current paper's hypothesis regarding deep layer inefficiency."}, {"fullname_first_author": "Zihang Dai", "paper_title": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context", "publication_date": "2019-01-07", "reason": "This paper introduces important architectural advancements for LLMs, setting the stage for further work on normalization strategies, a key topic of the current paper."}, {"fullname_first_author": "Andrey Gromov", "paper_title": "The unreasonable ineffectiveness of the deeper layers", "publication_date": "2024-03-29", "reason": "This work directly investigates the underutilization of deeper layers in LLMs, making it highly relevant to the current paper's focus on improving their efficiency."}]}