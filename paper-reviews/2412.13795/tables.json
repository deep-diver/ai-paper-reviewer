[{"content": "|  | **LLaMA-71M** | **LLaMA-130M** | **LLaMA-250M** | **LLaMA-1B** |\n|---|---|---|---|---| \n| Training Tokens | 1.1B | 2.2B | 3.9B | 5B |\n| Post-LN | 35.18 | 26.95 | 1409.09 | 1411.54 |\n| DeepNorm | 34.87 | 27.17 | 22.77 | 1410.94 |\n| Pre-LN | 34.77 | 26.78 | 21.92 | 18.65 |\n| Mix-LN | **33.12** | **26.07** | **21.39** | **18.18** |", "caption": "Table 1: Perplexity (\u2193\u2193\\downarrow\u2193) comparison of various normalization methods across various LLaMA sizes.", "description": "This table presents a comparison of perplexity scores achieved by different normalization methods (Post-LN, DeepNorm, Pre-LN, and Mix-LN) when applied to various LLaMA models of different sizes (71M, 130M, 250M, and 1B parameters).  The training was conducted with varying numbers of training tokens depending on the model size. Lower perplexity scores indicate better performance.", "section": "4.1 LLM PRE-TRAINING"}, {"content": "| Method | MMLU | BoolQ | ARC-e | PIQA | Hellaswag | OBQA | Winogrande | Avg. |\n|---|---|---|---|---|---|---|---|---| \n| **LLaMA-250M** | | | | | | | | |\n| Post-LN | 22.95 | 37.83 | 26.94 | 52.72 | 26.17 | 11.60 | 49.56 | 32.54 |\n| DeepNorm | 23.60 | 37.86 | 36.62 | 61.10 | 25.69 | 15.00 | 49.57 | 35.63 |\n| Pre-LN | 24.93 | 38.35 | 40.15 | 63.55 | 26.34 | 16.20 | 49.01 | 36.93 |\n| Mix-LN | **26.53** | **56.12** | **41.68** | **66.34** | **30.16** | **18.00** | **50.56** | **41.34** |\n| **LLaMA-1B** | | | | | | | | |\n| Post-LN | 22.95 | 37.82 | 25.08 | 49.51 | 25.04 | 13.80 | 49.57 | 31.96 |\n| DeepNorm | 23.35 | 37.83 | 27.06 | 52.94 | 26.19 | 11.80 | 49.49 | 32.67 |\n| Pre-LN | 26.54 | **62.20** | 45.70 | 67.79 | 30.96 | 17.40 | 50.51 | 43.01 |\n| Mix-LN | **27.99** | 61.93 | **48.11** | **68.50** | **31.35** | **18.80** | **55.93** | **44.66** |", "caption": "Table 2: Fine-tuning performance (\u2191\u2191\\uparrow\u2191) of LLaMa with various normalizations.", "description": "This table presents the fine-tuning performance of LLaMa models with different normalization methods on eight downstream tasks: MMLU, BoolQ, ARC-e, PIQA, HellaSwag, OBQA, and Winogrande.  It includes results for two LLaMa model sizes (250M and 1B parameters) and four normalization strategies (Post-LN, DeepNorm, Pre-LN, and Mix-LN).  The average performance across all tasks is also provided. Higher scores indicate better performance.", "section": "4.3 SUPERVISED FINE-TUNING"}, {"content": "| Method | Model | Final Reward |\n|---|---|---| \n| Pre-LN | LLaMA-1B | 0.75 |\n| Mix-LN | LLaMA-1B | **1.32** |", "caption": "Table 3: RLHF comparison of final reward (\u2191\u2191\\uparrow\u2191) of Pre-LN and Mix-LN with LLaMA-1B.", "description": "This table compares the final reward achieved by Large Language Models (LLMs) after Reinforcement Learning from Human Feedback (RLHF) training. Specifically, it presents the results for LLaMA-1B models trained with two different layer normalization methods: Pre-LN (Pre-Layer Normalization) and Mix-LN (the proposed method).  The reward is a metric used to assess the quality of the generated text, with higher reward values generally indicating better performance.  This comparison aims to demonstrate the effectiveness of Mix-LN in improving the quality of LLMs fine-tuned with RLHF.", "section": "4.4 REINFORCEMENT LEARNING FROM HUMAN FEEDBACK"}, {"content": "| Model | ViT-Tiny | ViT-Small |\n|---|---|---| \n| Pre-LN | 67.30 | 75.99 |\n| Mix-LN | **67.34** | **76.40** |", "caption": "Table 4: Accuracy (\u2191\u2191\\uparrow\u2191) comparison of Pre-LN and Mix-LN on ViT models.", "description": "This table compares the accuracy of Vision Transformer (ViT) models using two different layer normalization methods: Pre-LN (pre-layer normalization) and Mix-LN (mixed layer normalization). Mix-LN is a novel technique introduced in this paper that combines Pre-LN and Post-LN to improve performance.  The table presents accuracy scores for two ViT model sizes (ViT-Tiny and ViT-Small) when trained on the ImageNet-1K dataset.  The goal is to demonstrate that the benefits of Mix-LN extend beyond language models to other domains like computer vision.", "section": "4.5 EVALUATION WITH VISION TRANSFORMERS"}, {"content": "| Post-LN ratios <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T5.3.1.1.1.m1.1\"><semantics id=\"S5.T5.3.1.1.1.m1.1a\"><mi id=\"S5.T5.3.1.1.1.m1.1.1\" xref=\"S5.T5.3.1.1.1.m1.1.1.cmml\">\u03b1</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.T5.3.1.1.1.m1.1b\"><ci id=\"S5.T5.3.1.1.1.m1.1.1.cmml\" xref=\"S5.T5.3.1.1.1.m1.1.1\">\ud835\udefc</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T5.3.1.1.1.m1.1c\">\\alpha</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.T5.3.1.1.1.m1.1d\">italic_\u03b1</annotation></semantics></math> | Pre-LN | Mix-LN | Mix-LN | Mix-LN | Mix-LN | Mix-LN | Post-LN |\n|---|---|---|---|---|---|---|---|\n| | 0 | 16.7% | 25.0% | 33.0% | 41.7% | 50.0% | 100% |\n| Perplexity | 18.65 | 18.34 | **18.18** | 18.41 | 18.55 | 18.86 | 1434 |", "caption": "Table 5: Perplexity of LLaMA-1B with various Post-LN ratios \u03b1\ud835\udefc\\alphaitalic_\u03b1.", "description": "This table presents the perplexity scores achieved by the LLaMA-1B model when trained with different Post-LN ratios (\u03b1). The \u03b1 parameter controls the proportion of layers in the model that utilize Post-Layer Normalization, while the remaining layers use Pre-Layer Normalization. A ratio of 0 corresponds to using Pre-LN for all layers, whereas a ratio of 1 represents using Post-LN for all layers. Intermediate values create a mix of Pre-LN and Post-LN within the model.  The table compares the performance of these mixed normalization strategies against the baseline performance of pure Pre-LN and Post-LN.  Lower perplexity scores indicate better performance. The purpose of this analysis is to demonstrate the performance benefits of the proposed Mix-LN approach over standard Pre-LN and Post-LN, as well as to find the optimal balance between the two normalization methods.", "section": "5 ANALYSIS AND MORE EVALUATIONS"}, {"content": "| Model | Pre-LN | Admin | Group-LN | Sandwich-LN | Mix-LN |\n|---|---|---|---|---|---| \n| LLaMA-250M | 23.39 | 24.82 | 23.10 | 23.26 | **22.33** |", "caption": "Table 6: Comparison against other normalization methods on LLaMA-250M.", "description": "This table compares the perplexity scores of the LLaMA-250M model when trained with different normalization methods including Pre-LN, Admin, Group-LN, Sandwich-LN and the proposed Mix-LN. The results highlight the effectiveness of Mix-LN in achieving lower perplexity compared to other recent normalization techniques.", "section": "Main Experimental Results"}, {"content": "| Params | Hidden | Intermediate | Heads | Layers | Steps | Data amount | LR | Batch Size | \\[\\]alpha |\n|---|---|---|---|---|---|---|---|---|---|\n| 71M | 512 | 1368 | 8 | 12 | 10K | 1.1B | 1e-3 | 512 | 25% |\n| 130M | 768 | 2048 | 12 | 12 | 20K | 2.2B | 1e-3 | 512 | 25% |\n| 250M | 1024 | 2560 | 16 | 24 | 40K | 3.9B | 1e-3 | 512 | 25% |\n| 1B | 2048 | 5461 | 32 | 24 | 100K | 5.0B | 5e-4 | 512 | 25% |\n| 7B | 4096 | 11008 | 32 | 32 | 13K | 1.7B | 5e-4 | 512 | 6.25% |", "caption": "Table 7: Hyperparameters of LLaMA models used in this paper.", "description": "This table, located in Appendix A.1, details the hyperparameters used for pre-training different sizes of LLaMA models, following configurations from Lialin et al. (2023a) and Zhao et al. (2024). It outlines the model size (in parameters), hidden size, intermediate size, number of heads, number of layers, training steps, data amount, learning rate, batch size, and the Post-LN ratio (\u03b1) used for Mix-LN experiments. The table shows how these hyperparameters vary across different model sizes, from 71M to 7B parameters, and provides context for understanding the experimental setup and results discussed in the paper.  The max sequence length was fixed as 256 with a batch size of 512, totaling 131K tokens per batch. A warmup of the learning rate to 10% of the training steps was used. All models are trained using Adam optimizer with cosine annealing for the learning rate schedule, decaying to 10% of the initial value. Different learning rates are used based on the model sizes: 1e-3 for model sizes under 250M and 5e-4 for the 1B model.", "section": "A DETAILS OF EXPERIMENTS"}, {"content": "| Normalization | Scaled Initialization | Scaled Embed | Perplexity |\n|---|---|---|---| \n| Pre-LN |  |  | 32.18 |\n| Mix-LN |  |  | 29.95 |\n| Pre-LN | \u2713 |  | 30.63 |\n| Mix-LN | \u2713 |  | 29.77 |\n| Pre-LN | \u2713 | \u2713 | 31.28 |\n| Mix-LN | \u2713 | \u2713 | 31.19 |", "caption": "Table 8: Perplexity of LLaMA-130M with various normalization methods with Scaled Initialization and Scaled Embed.", "description": "This table compares the perplexity of a 130M parameter LLaMA language model trained with different normalization methods, combined with Scaled Initialization and Scaled Embed techniques.  Mix-LN consistently shows the lowest perplexity both with and without these additional techniques, demonstrating its compatibility with training stabilizers and ability to improve the overall quality of the model.", "section": "B COMPATIBILITY TO ADVANCED"}]