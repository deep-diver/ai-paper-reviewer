[{"figure_path": "https://arxiv.org/html/2502.05795/x1.png", "caption": "Figure 1: \nLayerwise output variance. This figure compares the output variance across various layers for different setups: (1) Pre-LN; (2) Pre-LN with Scaled Initialization; and (3) LayerNorm Scaling. The experiments are conducted on the LLaM-130M model trained for 10,000 steps. The proposed LayerNorm Scaling effectively controls the variance across layers.", "description": "This figure shows the output variance of different layers in a 130M parameter LLaMA model under three different normalization methods: Pre-Layer Normalization (Pre-LN), Pre-LN with Scaled Initialization, and LayerNorm Scaling.  The x-axis represents the update steps during training (up to 10,000 steps), and the y-axis represents the average output variance across layers.  The plot visually demonstrates that Pre-LN leads to a significant increase in output variance as the layer depth increases, while LayerNorm Scaling effectively mitigates this issue, maintaining relatively stable variance across all layers.", "section": "Mitigating CoD through LayerNorm Scaling"}, {"figure_path": "https://arxiv.org/html/2502.05795/x2.png", "caption": "Figure 2: \nPerformance drop of layer pruning across different LLMs.\n(a) BERT-Large (Post-LN), (b) Mistral-7B (Pre-LN), (c) Qwen-7B (Pre-LN), (d) DeepSeek-7B (Pre-LN), (e) LLaMA2-7B (Pre-LN), and (f) LLaMA2-13B (Pre-LN). The results show that Pre-LN models exhibit significant inefficiency in deeper layers, while Post-LN models maintain strong deep-layer contributions.", "description": "This figure displays the performance drop when layers are pruned from various large language models (LLMs).  It compares six different LLMs: BERT-Large, which uses Post-Layer Normalization (Post-LN), and five others (Mistral-7B, Qwen-7B, DeepSeek-7B, LLaMA2-7B, and LLaMA2-13B) that use Pre-Layer Normalization (Pre-LN). The x-axis represents the layer index, and the y-axis shows the performance drop (\u0394P(l)) in the MMLU or SQUAD benchmark when a layer is removed. The results illustrate that in Pre-LN models, removing deeper layers has minimal impact on overall performance, highlighting their inefficiency; whereas Post-LN models show a greater performance drop when deeper layers are removed.", "section": "2. Empirical Evidence of the Curse of Depth"}, {"figure_path": "https://arxiv.org/html/2502.05795/x3.png", "caption": "Figure 3: Comparison between Pre-LN (a) and LayerNorm Scaling (b). LayerNorm Scaling applies a scaling factor inversely proportional to the square root of the layer index l\ud835\udc59litalic_l, preventing excessive variance growth and stabilizing training dynamics across layers.", "description": "This figure illustrates the core difference between the Pre-Layer Normalization (Pre-LN) and the proposed LayerNorm Scaling methods.  Panel (a) shows the architecture of a standard Transformer block using Pre-LN, where layer normalization is applied before the attention and feed-forward network operations.  This can lead to an exponential increase in the output variance as the depth (layer index,  'l') increases. Panel (b) depicts the LayerNorm Scaling method, which addresses this issue by introducing a scaling factor inversely proportional to the square root of the layer index (1/\u221al). This scaling effectively controls the variance, ensuring that deeper layers contribute more effectively to training and preventing the variance explosion problem associated with Pre-LN.", "section": "4. LayerNorm Scaling"}, {"figure_path": "https://arxiv.org/html/2502.05795/x4.png", "caption": "Figure 4: \nPerformance drop of layer pruning on LLaMA-130M. LayerNorm Scaling enables deep layers to make a meaningful contribution to the model.", "description": "This figure shows the performance drop when pruning layers in the LLaMA-130M model, comparing the standard Pre-Layer Normalization (Pre-LN) with the proposed LayerNorm Scaling method.  The Pre-LN model shows minimal performance degradation when deeper layers are pruned, indicating their limited contribution. In contrast, the LayerNorm Scaling model exhibits significant performance decrease upon pruning of deeper layers, suggesting their crucial role in learning and improved model performance. This demonstrates LayerNorm Scaling successfully mitigates the Curse of Depth and enables deeper layers to participate in the overall performance of the model.", "section": "2. Empirical Evidence of the Curse of Depth"}, {"figure_path": "https://arxiv.org/html/2502.05795/x5.png", "caption": "Figure 5: Training loss of LLaMA-1B with Pre-LN and LayerNorm Scaling.", "description": "This figure displays the training loss curves for a 1B parameter LLaMA model using two different layer normalization techniques: Pre-Layer Normalization (Pre-LN) and the proposed LayerNorm Scaling.  It visually compares the convergence speed and overall loss achieved by each method during the model's training process.  The graph allows for a direct comparison of the effectiveness of the two normalization strategies.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.05795/x6.png", "caption": "Figure 6: \nVariance growth across layers in LLaMA-130M with Pre-LN.\nEach subplot shows the variance at different training stages (1000, 3000, and 6000 epochs).\nIn all cases, the variance follows an exponential growth pattern as depth increases, indicating that deeper layers experience uncontrolled variance amplification regardless of training progress.", "description": "This figure visualizes the variance of layer outputs in a LLaMA-130M model trained with Pre-Layer Normalization (Pre-LN). Three subplots display the variance at different training epochs (1000, 3000, and 6000).  The key observation is that variance remains low in the initial layers but increases exponentially as the layer depth increases, regardless of the training stage. This exponential growth suggests that deeper layers suffer from uncontrolled variance amplification during training, even with Pre-LN, highlighting the need for variance control mechanisms like LayerNorm Scaling.", "section": "2. Empirical Evidence of the Curse of Depth"}]