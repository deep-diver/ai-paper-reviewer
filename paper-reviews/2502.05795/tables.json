[{"content": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S5.T1.1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T1.1.1.1.1\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\" id=\"S5.T1.1.1.1.1.1\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T1.1.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.1.1.1.1.2.1\">LLaMA-130M</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T1.1.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.1.1.1.1.3.1\">LLaMA-250M</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T1.1.1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.1.1.1.1.4.1\">LLaMA-350M</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T1.1.1.1.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.1.1.1.1.5.1\">LLaMA-1B</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.1.1.2.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row\" id=\"S5.T1.1.1.2.2.1\">Training Tokens</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T1.1.1.2.2.2\">2.2B</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T1.1.1.2.2.3\">3.9B</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T1.1.1.2.2.4\">6.0B</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T1.1.1.2.2.5\">8.9B</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T1.1.1.3.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S5.T1.1.1.3.1.1\">Post-LN <cite class=\"ltx_cite ltx_citemacro_citep\">(Ba, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2502.05795v1#bib.bib2\" title=\"\">2016</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T1.1.1.3.1.2\">26.95</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T1.1.1.3.1.3\">1409.79</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T1.1.1.3.1.4\">1368.33</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T1.1.1.3.1.5\">1390.75</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.1.1.4.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T1.1.1.4.2.1\">DeepNorm <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et\u00a0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2502.05795v1#bib.bib39\" title=\"\">2024</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.1.1.4.2.2\">27.17</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.1.1.4.2.3\">22.77</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.1.1.4.2.4\">1362.59</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.1.1.4.2.5\">1409.08</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.1.1.5.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T1.1.1.5.3.1\">Mix-LN <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et\u00a0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2502.05795v1#bib.bib21\" title=\"\">2024b</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.1.1.5.3.2\">26.07</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.1.1.5.3.3\">21.39</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.1.1.5.3.4\">1363.21</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.1.1.5.3.5\">1414.78</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.1.1.6.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T1.1.1.6.4.1\">Pre-LN <cite class=\"ltx_cite ltx_citemacro_citep\">(Baevski and Auli, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2502.05795v1#bib.bib3\" title=\"\">2019</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.1.1.6.4.2\">26.73</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.1.1.6.4.3\">21.92</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.1.1.6.4.4\">19.58</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.1.1.6.4.5\">17.02</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.1.1.7.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\" id=\"S5.T1.1.1.7.5.1\">Pre-LN + LayerNorm Scaling</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T1.1.1.7.5.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.1.1.7.5.2.1\">25.76</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T1.1.1.7.5.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.1.1.7.5.3.1\">20.35</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T1.1.1.7.5.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.1.1.7.5.4.1\">18.20</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T1.1.1.7.5.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.1.1.7.5.5.1\">15.71</span></td>\n</tr>\n</tbody>\n</table>", "caption": "Table 1: Perplexity (\u2193) comparison of various layer normalization methods across various LLaMA sizes.", "description": "This table presents a comparison of perplexity scores achieved by different layer normalization methods across various sizes of LLaMA language models.  Perplexity is a metric indicating how well a model predicts a sample, with lower scores signifying better performance. The table allows for a quantitative assessment of the effectiveness of different normalization techniques (Post-LN, DeepNorm, Mix-LN, Pre-LN, and Pre-LN with LayerNorm Scaling) in training LLMs of varying scale (130M, 250M, 350M, and 1B parameters). The results highlight the impact of the chosen normalization method on the overall model performance and training efficiency.", "section": "5. Experiments"}, {"content": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S5.T2.3.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T2.3.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" id=\"S5.T2.3.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T2.3.1.1.1.1.1\" style=\"font-size:80%;\">Pre-LN</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T2.3.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T2.3.1.1.1.2.1\" style=\"font-size:80%;\">Admin</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T2.3.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T2.3.1.1.1.3.1\" style=\"font-size:80%;\">Group-LN</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T2.3.1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T2.3.1.1.1.4.1\" style=\"font-size:80%;\">Sandwich-LN</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T2.3.1.1.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T2.3.1.1.1.5.1\" style=\"font-size:80%;\">Mix-LN</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T2.3.1.1.1.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T2.3.1.1.1.6.1\" style=\"font-size:80%;\">LayerNorm Scaling</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.3.1.2.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\" id=\"S5.T2.3.1.2.2.1\"><span class=\"ltx_text\" id=\"S5.T2.3.1.2.2.1.1\" style=\"font-size:80%;\">26.73</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T2.3.1.2.2.2\"><span class=\"ltx_text\" id=\"S5.T2.3.1.2.2.2.1\" style=\"font-size:80%;\">27.91</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T2.3.1.2.2.3\"><span class=\"ltx_text\" id=\"S5.T2.3.1.2.2.3.1\" style=\"font-size:80%;\">28.01</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T2.3.1.2.2.4\"><span class=\"ltx_text\" id=\"S5.T2.3.1.2.2.4.1\" style=\"font-size:80%;\">26.51</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T2.3.1.2.2.5\"><span class=\"ltx_text\" id=\"S5.T2.3.1.2.2.5.1\" style=\"font-size:80%;\">26.07</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T2.3.1.2.2.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T2.3.1.2.2.6.1\" style=\"font-size:80%;\">25.76</span></td>\n</tr>\n</tbody>\n</table>", "caption": "Table 2: Comparison against other normalization methods on LLaMA-130M. Perplexity (\u2193) is reported.", "description": "This table presents a comparison of the perplexity scores achieved by different layer normalization methods when training the LLaMA-130M language model.  Lower perplexity indicates better performance. The methods compared include Pre-LN (Pre-Layer Normalization), Admin, Group-LN (Group Layer Normalization), Sandwich-LN, Mix-LN, and LayerNorm Scaling.  This allows for a direct assessment of how these different normalization techniques impact the model's ability to learn and generate text, offering insights into their relative effectiveness.", "section": "5. Experiments"}, {"content": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S5.T3.3.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T3.3.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S5.T3.3.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.3.1.1.1.1.1\">Method</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T3.3.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.3.1.1.1.2.1\">MMLU</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T3.3.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.3.1.1.1.3.1\">BoolQ</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T3.3.1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.3.1.1.1.4.1\">ARC-e</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T3.3.1.1.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.3.1.1.1.5.1\">PIQA</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T3.3.1.1.1.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.3.1.1.1.6.1\">Hellaswag</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T3.3.1.1.1.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.3.1.1.1.7.1\">OBQA</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T3.3.1.1.1.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.3.1.1.1.8.1\">Winogrande</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T3.3.1.1.1.9\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.3.1.1.1.9.1\">Average</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.3.1.2.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" colspan=\"9\" id=\"S5.T3.3.1.2.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.3.1.2.2.1.1\">LLaMA-250M</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T3.3.1.3.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T3.3.1.3.1.1\">Post-LN <cite class=\"ltx_cite ltx_citemacro_citep\">(Ba, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2502.05795v1#bib.bib2\" title=\"\">2016</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.3.1.2\">22.95</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.3.1.3\">37.83</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.3.1.4\">26.94</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.3.1.5\">52.72</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.3.1.6\">26.17</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.3.1.7\">11.60</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.3.1.8\">49.56</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.3.1.9\">32.54</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.3.1.4.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T3.3.1.4.2.1\">DeepNorm <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et\u00a0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2502.05795v1#bib.bib39\" title=\"\">2024</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.4.2.2\">23.60</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.4.2.3\">37.86</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.4.2.4\">36.62</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.4.2.5\">61.10</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.4.2.6\">25.69</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.4.2.7\">15.00</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.4.2.8\">49.57</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.4.2.9\">35.63</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.3.1.5.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T3.3.1.5.3.1\">Mix-LN <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et\u00a0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2502.05795v1#bib.bib21\" title=\"\">2024b</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.5.3.2\">26.53</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.5.3.3\">56.12</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.5.3.4\">41.68</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.5.3.5\">66.34</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.5.3.6\">30.16</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.5.3.7\">18.00</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.5.3.8\">50.56</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.5.3.9\">41.34</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.3.1.6.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T3.3.1.6.4.1\">Pre-LN <cite class=\"ltx_cite ltx_citemacro_citep\">(Baevski and Auli, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2502.05795v1#bib.bib3\" title=\"\">2019</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.6.4.2\">24.93</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.6.4.3\">38.35</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.6.4.4\">40.15</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.6.4.5\">63.55</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.6.4.6\">26.34</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.6.4.7\">16.20</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.6.4.8\">49.01</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.6.4.9\">36.93</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.3.1.7.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S5.T3.3.1.7.5.1\">Pre-LN + LayerNorm Scaling</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.3.1.7.5.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.3.1.7.5.2.1\">27.08</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.3.1.7.5.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.3.1.7.5.3.1\">58.17</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.3.1.7.5.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.3.1.7.5.4.1\">45.24</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.3.1.7.5.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.3.1.7.5.5.1\">67.38</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.3.1.7.5.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.3.1.7.5.6.1\">32.81</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.3.1.7.5.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.3.1.7.5.7.1\">18.80</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.3.1.7.5.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.3.1.7.5.8.1\">52.49</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.3.1.7.5.9\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.3.1.7.5.9.1\">43.14</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.3.1.8.6\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" colspan=\"9\" id=\"S5.T3.3.1.8.6.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.3.1.8.6.1.1\">LLaMA-1B</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.3.1.9.7\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T3.3.1.9.7.1\">Post-LN <cite class=\"ltx_cite ltx_citemacro_citep\">(Ba, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2502.05795v1#bib.bib2\" title=\"\">2016</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.9.7.2\">22.95</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.9.7.3\">37.82</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.9.7.4\">25.08</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.9.7.5\">49.51</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.9.7.6\">25.04</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.9.7.7\">13.80</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.9.7.8\">49.57</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.9.7.9\">31.96</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.3.1.10.8\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T3.3.1.10.8.1\">DeepNorm <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et\u00a0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2502.05795v1#bib.bib39\" title=\"\">2024</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.10.8.2\">23.35</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.10.8.3\">37.83</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.10.8.4\">27.06</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.10.8.5\">52.94</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.10.8.6\">26.19</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.10.8.7\">11.80</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.10.8.8\">49.49</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.10.8.9\">32.67</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.3.1.11.9\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T3.3.1.11.9.1\">Mix-LN <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et\u00a0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2502.05795v1#bib.bib21\" title=\"\">2024b</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.11.9.2\">23.19</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.11.9.3\">37.83</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.11.9.4\">25.08</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.11.9.5\">49.51</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.11.9.6\">25.04</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.11.9.7\">11.80</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.11.9.8\">49.57</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.11.9.9\">31.72</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.3.1.12.10\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T3.3.1.12.10.1\">Pre-LN <cite class=\"ltx_cite ltx_citemacro_citep\">(Baevski and Auli, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2502.05795v1#bib.bib3\" title=\"\">2019</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.12.10.2\">26.54</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.12.10.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.3.1.12.10.3.1\">62.20</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.12.10.4\">45.70</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.12.10.5\">67.79</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.12.10.6\">30.96</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.12.10.7\">17.40</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.12.10.8\">50.51</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.12.10.9\">43.01</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.3.1.13.11\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\" id=\"S5.T3.3.1.13.11.1\">Pre-LN + LayerNorm Scaling</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T3.3.1.13.11.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.3.1.13.11.2.1\">28.69</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T3.3.1.13.11.3\">61.80</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T3.3.1.13.11.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.3.1.13.11.4.1\">48.85</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T3.3.1.13.11.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.3.1.13.11.5.1\">67.92</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T3.3.1.13.11.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.3.1.13.11.6.1\">33.94</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T3.3.1.13.11.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.3.1.13.11.7.1\">18.60</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T3.3.1.13.11.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.3.1.13.11.8.1\">54.30</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T3.3.1.13.11.9\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.3.1.13.11.9.1\">44.87</span></td>\n</tr>\n</tbody>\n</table>", "caption": "Table 3: Fine-tuning performance (\u2191\u2191\\uparrow\u2191) of LLaMA with various normalizations.", "description": "This table presents the results of fine-tuning various LLaMA models (LLaMA-250M and LLaMA-1B) on eight downstream tasks using different layer normalization techniques.  The table shows the performance improvement (measured as a percentage increase) achieved by using LayerNorm Scaling compared to standard Pre-LN (Pre-Layer Normalization), Post-LN (Post-Layer Normalization), DeepNorm, and Mix-LN.  The results demonstrate the effectiveness of LayerNorm Scaling in enhancing fine-tuning performance across different model sizes and tasks.", "section": "5.2 Supervised Fine-tuning"}, {"content": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S5.T4.1.1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T4.1.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S5.T4.1.1.1.1.1\">Perplexity (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.1.1.1.1.1.m1.1\"><semantics id=\"S5.T4.1.1.1.1.1.m1.1a\"><mo id=\"S5.T4.1.1.1.1.1.m1.1.1\" stretchy=\"false\" xref=\"S5.T4.1.1.1.1.1.m1.1.1.cmml\">\u2193</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T4.1.1.1.1.1.m1.1b\"><ci id=\"S5.T4.1.1.1.1.1.m1.1.1.cmml\" xref=\"S5.T4.1.1.1.1.1.m1.1.1\">\u2193</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T4.1.1.1.1.1.m1.1c\">\\downarrow</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.T4.1.1.1.1.1.m1.1d\">\u2193</annotation></semantics></math>)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T4.1.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.1.1.1.1.2.1\">LLaMA-130M</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T4.1.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.1.1.1.1.3.1\">LLaMA-250M</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.1.1.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row\" id=\"S5.T4.1.1.1.2.1.1\">Training Tokens</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T4.1.1.1.2.1.2\">2.2B</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T4.1.1.1.2.1.3\">3.9B</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T4.1.1.1.3.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S5.T4.1.1.1.3.1.1\">Pre-LN</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.1.1.1.3.1.2\">26.73</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.1.1.1.3.1.3\">21.92</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.1.1.4.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T4.1.1.1.4.2.1\">+ LayerScale</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.1.1.1.4.2.2\">27.93</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.1.1.1.4.2.3\">23.45</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.1.1.5.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T4.1.1.1.5.3.1\">+ Scaled Initialization</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.1.1.1.5.3.2\">26.04</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.1.1.1.5.3.3\">20.98</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.1.1.6.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S5.T4.1.1.1.6.4.1\">+ LayerNorm Scaling</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T4.1.1.1.6.4.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.1.1.1.6.4.2.1\">25.76</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T4.1.1.1.6.4.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.1.1.1.6.4.3.1\">20.35</span></td>\n</tr>\n</tbody>\n</table>", "caption": "Table 4: Comparison against other scaling methods.", "description": "This table compares the performance of LayerNorm Scaling against other scaling methods (LayerScale and Scaled Initialization) in terms of perplexity on the LLaMA-130M and LLaMA-250M models.  It highlights the impact of different scaling techniques on model performance during pre-training, showing how LayerNorm Scaling achieves lower perplexity compared to the baselines.", "section": "5. Experiments"}]