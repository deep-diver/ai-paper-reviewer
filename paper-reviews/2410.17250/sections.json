[{"page_end_idx": 1, "page_start_idx": 1, "section_number": 1, "section_title": "Introduction", "details": {"details": "The introduction section sets the stage for the paper by highlighting the growing importance of Large Multimodal Models (LMMs) and the need for robust evaluation benchmarks, particularly in languages beyond English.  It emphasizes the limitations of current benchmarks, which primarily focus on English and lack a comprehensive consideration of cultural context.  The authors introduce JMMMU (Japanese MMMU) as a novel solution to address these shortcomings, positioning it as the first large-scale Japanese benchmark designed for expert-level evaluations of LMMs within a Japanese cultural context.  JMMMU is presented as a benchmark comprising two subsets: a culture-agnostic (CA) subset allowing direct comparison to the existing English MMMU, and a culture-specific (CS) subset reflecting unique aspects of Japanese culture. The introduction concludes by stating the paper's aim to not only improve LMM performance in Japanese but also to establish a model for creating high-standard culturally diverse benchmarks for multilingual LMM development.", "first_cons": "The introduction could benefit from more concrete examples of the limitations of existing LMM evaluation benchmarks.  While it mentions the lack of focus on non-English languages and cultural context, providing specific instances would strengthen the argument for the necessity of JMMMU.", "first_pros": "The introduction effectively highlights the significance of the research by emphasizing the need for culturally sensitive evaluations of LMMs, a crucial area often overlooked in the field. The clear articulation of the problem and the proposed solution makes the paper's objective readily apparent.", "keypoints": ["Current benchmarks predominantly focus on English, neglecting other languages.", "There's a lack of robust benchmarks considering cultural context in LMM evaluation.", "JMMMU is introduced as the first large-scale Japanese benchmark for expert-level tasks.", "JMMMU includes two subsets: culture-agnostic (CA) and culture-specific (CS), enabling comprehensive evaluation.", "The goal is to advance LMM performance in Japanese and set a standard for culturally diverse benchmarks."], "second_cons": "The introduction could be more concise. Some sentences could be combined or shortened to improve readability and maintain the reader's attention.", "second_pros": "The introduction successfully establishes the context, problem, and proposed solution in a clear and logical manner. The introduction of the two subsets of JMMMU is well-explained and easy to understand. This makes the introduction highly effective in capturing the reader's interest and preparing them for the rest of the paper.", "summary": "This paper introduces JMMMU, a new large-scale Japanese benchmark designed to evaluate the performance of Large Multimodal Models (LMMs) on expert-level tasks, addressing the current lack of culturally-aware benchmarks in non-English languages.  JMMMU features two subsets: a culture-agnostic set for direct comparison with existing English benchmarks, and a culture-specific set reflecting the unique aspects of Japanese culture. The goal is to improve LMM performance in Japanese and provide a model for creating high-standard, culturally diverse benchmarks for multilingual LMM development."}}, {"page_end_idx": 3, "page_start_idx": 2, "section_number": 2, "section_title": "Related Work", "details": {"details": "The section \"Related Work\" discusses the existing landscape of Large Multimodal Models (LMMs) and their benchmarks, particularly focusing on the limitations of current benchmarks.  It highlights the scarcity of high-quality, culture-aware benchmarks for languages other than English, especially for tasks requiring expert-level knowledge. The authors point out that existing Japanese LMM benchmarks mainly focus on common sense knowledge, neglecting expert-level reasoning, and often rely on direct translations of English benchmarks, failing to adequately capture Japanese cultural nuances.  These shortcomings impede the development of robust LMMs capable of handling the complexities of different languages and their cultural contexts.  The section sets the stage for the introduction of JMMMU by emphasizing the need for a comprehensive, large-scale, culture-aware benchmark to accurately assess the capabilities of LMMs in Japanese.", "first_cons": "The section's focus on the limitations of existing benchmarks could be perceived as overly critical without offering more constructive suggestions for improvement beyond the introduction of their own benchmark.  A more balanced perspective, acknowledging the contributions of prior work while highlighting its shortcomings, would strengthen the argument.", "first_pros": "The section effectively establishes the need for the proposed JMMMU benchmark by clearly outlining the shortcomings of existing LMM benchmarks, particularly in the area of multilingual and culture-aware evaluation. This creates a strong rationale for the new benchmark.", "keypoints": ["Lack of emphasis on non-English languages in current LMM evaluations, with a focus primarily on English.", "Existing Japanese benchmarks mostly address common sense knowledge, overlooking expert-level reasoning and cultural context.", "Direct translation of English benchmarks often leads to culturally inappropriate questions in Japanese.", "Limited size of existing culture-aware Japanese benchmarks (up to 102 questions), making reliable quantitative evaluation challenging."], "second_cons": "The review of existing benchmarks feels somewhat incomplete. While it mentions several benchmarks, it lacks a detailed comparative analysis of their strengths and weaknesses, which would provide more context for the introduction of JMMMU.  A more thorough literature review would strengthen the paper's claims.", "second_pros": "The section provides a concise yet informative overview of the existing literature on LMMs and their evaluation, highlighting the key issues related to multilingualism and cultural awareness. This serves as a strong foundation for introducing the authors' contribution.", "summary": "This section reviews the existing research on Large Multimodal Models (LMMs) and their benchmarks, revealing a significant gap in multilingual and culture-aware evaluation, especially for languages like Japanese.  Current benchmarks often focus on English and lack the scale and cultural sensitivity needed to comprehensively assess LMM performance in other languages.  The limited availability of high-quality Japanese benchmarks, often relying on direct translations from English or focusing on simple, common-sense tasks, underscores the need for a more sophisticated approach, leading to the introduction of JMMMU as a solution to these limitations."}}, {"page_end_idx": 4, "page_start_idx": 3, "section_number": 3, "section_title": "JMMMU Benchmark", "details": {"details": "The JMMMU benchmark is introduced as the first large-scale Japanese benchmark designed to evaluate Large Multimodal Models (LMMs) on expert-level tasks within a Japanese cultural context.  It comprises 1320 questions and 1118 images, significantly expanding on existing benchmarks by over 10 times.  The benchmark is cleverly structured into two complementary subsets: a Culture-Agnostic (CA) subset with 720 questions (translated from the English MMMU benchmark) to allow for direct comparison with English-language performance, and a Culture-Specific (CS) subset with 600 questions designed specifically to assess cultural understanding within Japanese contexts. This two-pronged approach enables a nuanced evaluation of LMM capabilities, separating language proficiency from cultural understanding.  The evaluation shows that while some models perform well on the CA subset, they often struggle with the CS subset, highlighting limitations in the depth of their understanding of the Japanese language and culture.  The benchmark also assesses the performance of 18 different LMMs, including both open-source and proprietary models, and reveals significant performance gaps, particularly in the CS subset, suggesting ample room for future improvement in both model development and benchmark design.", "first_cons": "The benchmark, while extensive, may not cover the full range of expert-level tasks or cultural nuances present in the Japanese language, and further expansion and refinement could improve its scope and accuracy.", "first_pros": "The JMMMU benchmark is the first large-scale, culturally sensitive benchmark for evaluating Japanese LMMs, addressing a critical gap in multilingual model evaluation.", "keypoints": ["1320 questions and 1118 images in total.", "Two subsets: Culture-Agnostic (CA) subset with 720 questions and Culture-Specific (CS) subset with 600 questions.", "Significant performance difference between CA and CS subsets highlights challenges in cultural understanding.", "Evaluates 18 different LMMs, including open-source and proprietary models.  Significant performance gaps, especially in the CS subset, are identified.", "Over 10 times larger than existing culture-aware Japanese benchmarks"], "second_cons": "The reliance on translation for the CA subset may introduce biases and limitations, as direct translation does not always capture cultural nuances.", "second_pros": "The design of JMMMU, with its CA and CS subsets, allows for a more nuanced and comprehensive evaluation of LMMs, separating language proficiency from cultural understanding.  This is crucial for building more robust and culturally aware models.", "summary": "JMMMU is a novel Japanese benchmark for evaluating large multimodal models that uses a unique two-subset approach: a culture-agnostic subset (translated from English MMMU) and a culture-specific subset (newly created) to comprehensively assess language ability and cultural understanding. Evaluation of 18 LMMs reveals significant performance gaps, highlighting the need for more sophisticated models and culturally sensitive benchmarks."}}, {"page_end_idx": 6, "page_start_idx": 4, "section_number": 4, "section_title": "Experiments", "details": {"details": "The experiment section (Section 4) of the paper evaluates the performance of various Large Multimodal Models (LMMs) on the newly introduced JMMMU benchmark.  The evaluation is divided into two parts: the Culture-Agnostic (CA) subset and the Culture-Specific (CS) subset. The CA subset involves translating existing MMMU questions into Japanese to isolate the impact of language differences on model performance.  The CS subset includes questions specifically designed to assess cultural understanding within the Japanese context. A total of 18 LMMs are evaluated: three proprietary models (GPT-40, Gemini 1.5 Pro, and Claude 3.5 Sonnet) and fifteen open-source models. The results reveal significant performance discrepancies between models and subsets.  Proprietary models generally outperform open-source models, achieving up to 58.6% accuracy compared to 40.5% for open-source models.  The CA subset demonstrates a performance drop for most models when evaluated in Japanese compared to English, indicating a language gap. The CS subset exposes the inadequacy of many models in understanding Japanese cultural nuances, highlighting the limitations of benchmarks relying solely on translated data. The combination of both subsets helps reveal a more holistic picture of model capabilities, identifying models that perform well in one subset but not the other, thereby pointing towards a shallow understanding of the language or culture.", "first_cons": "The study focuses primarily on Japanese language and culture, limiting its generalizability to other languages and cultural contexts.", "first_pros": "The introduction of the JMMMU benchmark offers a significant contribution to the field by providing a large-scale, culturally sensitive evaluation tool for Japanese LMMs, which was previously lacking.", "keypoints": ["A total of 18 LMMs were evaluated, including 3 proprietary and 15 open-source models.", "Proprietary models significantly outperformed open-source models, achieving up to 58.6% accuracy versus 40.5%.", "The Culture-Agnostic (CA) subset showed a performance drop for most models in Japanese compared to English (up to 8.6%), highlighting a language gap.", "The Culture-Specific (CS) subset exposed the models' inadequate understanding of Japanese cultural context, emphasizing the need for culturally sensitive benchmarks.", "Combining CA and CS subsets revealed that some models perform well in one but not the other, indicating a shallow understanding of language or culture without depth in cultural understanding"], "second_cons": "The reliance on existing MMMU questions for the CA subset, although translated, might not fully capture the nuances of the Japanese language and cultural context.", "second_pros": "The comprehensive analysis of results, including a detailed breakdown by model and subset, offers valuable insights into the strengths and weaknesses of current LMMs regarding Japanese language and culture.", "summary": "This experiment section evaluates 18 Large Multimodal Models (LMMs) using a new Japanese benchmark, JMMMU, divided into culture-agnostic (CA) and culture-specific (CS) subsets.  Proprietary models significantly outperformed open-source models on both subsets, with a notable performance drop observed on the CA subset when evaluating Japanese versus English. The CS subset highlighted the shortcomings of models in understanding Japanese cultural context. This comprehensive evaluation emphasizes the importance of culturally diverse benchmarks for a more accurate assessment of LMM capabilities."}}]