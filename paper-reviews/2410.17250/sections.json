[{"page_end_idx": 1, "page_start_idx": 1, "section_number": 1, "section_title": "Introduction", "details": {"details": "The introduction highlights the accelerating research in Large Multimodal Models (LMMs) for non-English languages, emphasizing the need for enhanced user experiences across diverse populations.  It introduces JMMMU, a novel large-scale Japanese benchmark designed to evaluate LMMs on expert-level tasks within a Japanese cultural context. This benchmark addresses the limitations of existing benchmarks which primarily focus on English and lack cultural considerations. JMMMU features two key subsets: a culture-agnostic (CA) subset enabling direct comparison with English counterparts, and a culture-specific (CS) subset reflecting Japanese cultural nuances. The introduction anticipates that evaluating LMMs using JMMMU will reveal performance gaps attributable to both language variation and cultural understanding, ultimately advancing LMM development in Japanese and guiding the creation of culturally diverse benchmarks for multilingual LMMs.", "first_cons": "The introduction could provide more concrete examples of the types of expert-level tasks included in JMMMU to better illustrate its scope and complexity.", "first_pros": "The introduction clearly states the problem, the solution (JMMMU), and the expected impact, creating a strong foundation for the paper.", "keypoints": ["Focus on non-English LMMs and the need for broader user experiences", "Introduction of JMMMU, the first large-scale Japanese benchmark for expert-level LMM evaluation", "Two subsets in JMMMU: culture-agnostic (CA) for direct comparison and culture-specific (CS) for Japanese cultural context", "Anticipation of revealing performance gaps due to both language and cultural factors", "Goal of advancing Japanese LMMs and establishing guidelines for culturally diverse benchmarks"], "second_cons": "While the introduction mentions the limitations of existing benchmarks, it could delve deeper into the specific shortcomings of these benchmarks to strengthen its argument.", "second_pros": "The clear description of JMMMU's two subsets (CA and CS) and their purpose provides a concise yet informative overview of the benchmark's design and functionality.", "summary": "This paper introduces JMMMU, a novel large-scale Japanese benchmark designed to evaluate Large Multimodal Models (LMMs) on expert-level tasks, addressing the limitations of existing English-centric benchmarks that lack cultural sensitivity.  JMMMU features a culture-agnostic (CA) subset for comparison with English benchmarks and a culture-specific (CS) subset tailored to the Japanese context, aiming to reveal performance gaps attributable to both language and cultural understanding, thereby promoting the development of more robust and culturally inclusive multilingual LMMs."}}, {"page_end_idx": 3, "page_start_idx": 2, "section_number": 2, "section_title": "Related Work", "details": {"details": "The section \"Related Work\" discusses the landscape of Large Multimodal Models (LMMs) and their evaluation benchmarks, focusing on the limitations of existing benchmarks, particularly in non-English languages like Japanese.  It highlights that while many LMMs have been developed and show improvement in English-centric tasks, the progress in other languages lags significantly. The authors point out that many existing Japanese LMM benchmarks primarily focus on common sense knowledge, rather than expert-level reasoning, and lack sufficient cultural context.  They further criticize the limited size of these benchmarks, questioning their statistical reliability for comprehensive evaluation.  The section sets the stage for the introduction of JMMMU by emphasizing the need for a large-scale, culturally-aware benchmark for evaluating LMMs in Japanese, a deficiency the authors aim to address with their proposed benchmark.", "first_cons": "The section primarily focuses on the shortcomings of existing benchmarks without offering many concrete examples of these limitations beyond general statements.", "first_pros": "The section effectively highlights the critical need for improved and culturally-sensitive benchmarks for evaluating LMMs in non-English languages, particularly Japanese. It provides a strong rationale for the development of JMMMU.", "keypoints": ["Most current LMM benchmarks focus primarily on English, neglecting the need for evaluation in other languages.", "Existing Japanese LMM benchmarks are limited in size (up to 102 questions) and mostly focus on common sense, not expert-level, knowledge.", "Many existing benchmarks are created by directly translating English benchmarks, lacking cultural context.", "There's a significant challenge in accurately evaluating the capabilities of non-English languages, due to the lack of appropriate benchmarks."], "second_cons": "The discussion lacks a detailed comparative analysis of different existing benchmarks, making it hard to fully grasp the specific strengths and weaknesses of each.", "second_pros": "The section provides a clear and concise overview of the current state-of-the-art in LMM benchmarks and identifies a significant gap in research, especially in the context of non-English languages and cultural sensitivity.  This sets a strong foundation for the introduction of the JMMMU benchmark in the following section.", "summary": "This section reviews the current state of Large Multimodal Models (LMMs) and their evaluation, focusing on the lack of comprehensive and culturally appropriate benchmarks for non-English languages, particularly Japanese.  It critiques existing Japanese benchmarks for their limited scope, size, and cultural sensitivity, setting the stage for the introduction of a new, more comprehensive benchmark to address these shortcomings."}}, {"page_end_idx": 4, "page_start_idx": 3, "section_number": 3, "section_title": "JMMMU Benchmark", "details": {"details": "The JMMMU benchmark is introduced as the first large-scale Japanese benchmark designed to evaluate Large Multimodal Models (LMMs) on expert-level tasks based on the Japanese cultural context.  It addresses limitations of existing benchmarks by including both culture-agnostic (CA) and culture-specific (CS) subsets. The CA subset, containing 720 questions translated from the English MMMU, allows for direct comparison and assessment of language variation effects on LMM performance. The CS subset, comprising 600 newly crafted questions, evaluates the models' understanding of Japanese culture.  The benchmark's creation involved translating existing questions, crafting new culturally relevant questions, and using images primarily sourced from Wikimedia Commons.  The evaluation of 18 open-source and 3 proprietary LMMs revealed significant performance gaps, highlighting the challenges in achieving true multilingual and culture-aware capabilities. Japanese-made LMMs showed relatively better performance on culture-specific questions compared to others that showed similar performance on culture-agnostic tasks.  The study highlights the risk of overestimating LMM performance by relying solely on translation-based evaluations.", "first_cons": "The benchmark focuses only on the Japanese language, limiting its applicability to other cultural contexts.", "first_pros": "JMMMU is the first large-scale Japanese benchmark designed for expert-level evaluation of LMMs, addressing the gap in existing benchmarks.", "keypoints": ["JMMMU is the first large-scale Japanese benchmark for expert-level LMM evaluation.", "It includes 1320 questions (720 culture-agnostic, 600 culture-specific) and 1118 images.", "The CA subset allows for direct comparison with the English MMMU benchmark.", "Evaluation of 18 open-source and 3 proprietary LMMs revealed significant performance gaps, highlighting the challenges in achieving true multilingual and culture-aware capabilities.", "Japanese-made LMMs performed relatively better on culture-specific questions compared to others with similar culture-agnostic performance. This demonstrates the importance of including culture-specific evaluation in benchmark design"], "second_cons": "The study mainly focuses on the performance evaluation and doesn't offer detailed insights into how to improve LMM performance.", "second_pros": "The benchmark is openly available and includes two complementary subsets enabling comprehensive culture-aware evaluations.", "summary": "JMMMU, a novel large-scale Japanese benchmark, evaluates large multimodal models' performance on expert-level tasks considering both culture-agnostic and culture-specific aspects of Japanese language and culture.  By comparing performance across 18 open source and 3 proprietary models on 1320 questions with 1118 images, JMMMU reveals significant performance discrepancies and highlights the importance of cultural understanding for true multilingual capabilities."}}, {"page_end_idx": 6, "page_start_idx": 4, "section_number": 4, "section_title": "Experiments", "details": {"details": "The experiment section evaluates the performance of various Large Multimodal Models (LMMs) on the newly introduced JMMMU benchmark.  The LMMs are categorized into proprietary (GPT-40, Gemini 1.5 Pro, Claude 3.5 Sonnet) and open-source models.  The benchmark itself consists of two subsets: a Culture-Agnostic (CA) subset, which involves translating existing MMMU questions into Japanese, and a Culture-Specific (CS) subset, with newly created questions reflecting Japanese cultural context.  The results show that proprietary models significantly outperform open-source models, achieving up to 58.6% accuracy versus 40.5% for open-source models.  The CA subset reveals a performance drop in many LMMs when evaluated in Japanese compared to English, highlighting language variation challenges.  The CS subset exposes the models' inadequate understanding of Japanese culture.  A key finding is that some models perform well on the CA subset but poorly on the CS subset, indicating a superficial understanding of the language lacking cultural depth.  The experiment also investigates the impact of translating images and text on model performance, showing varied effects across different models.  Finally, error analysis reveals a significant 'Lack of Knowledge' in culture-specific subjects for the top-performing model, highlighting the importance of cultural context in LMM evaluation.", "first_cons": "The experiment focuses primarily on GPT-40 and other proprietary models, with limited analysis and discussion of other open-source models, potentially resulting in a less comprehensive analysis of the LMM landscape.", "first_pros": "The experimental design is rigorous and well-defined, employing a benchmark that includes both culture-agnostic and culture-specific subsets, which allows for more comprehensive and insightful evaluation of LMMs.", "keypoints": ["Proprietary models (GPT-40, Gemini, Claude) significantly outperform open-source models (up to 58.6% vs 40.5%).", "Performance drops (up to 8.6%) observed in the Culture-Agnostic (CA) subset when evaluated in Japanese vs. English, highlighting language variation challenges.", "Culture-Specific (CS) subset reveals inadequate cultural understanding of Japanese context in many LMMs.", "Some models excel in the CA subset but fail in the CS subset, exposing superficial language understanding lacking cultural depth.", "Image and text translation significantly impact performance, with varied results across models."], "second_cons": "The study primarily uses one top-performing proprietary model (GPT-40) for in-depth error analysis, potentially limiting the generalizability of the findings to other models.", "second_pros": "The study provides valuable insights into the challenges of evaluating LMMs in non-English languages, particularly highlighting the need for culture-aware benchmarks and emphasizing the importance of deep cultural understanding in LMM development.", "summary": "This experiment section rigorously evaluates various Large Multimodal Models (LMMs) on a new Japanese benchmark (JMMMU) that includes both culture-agnostic and culture-specific subsets.  Proprietary models substantially outperform open-source models, while the culture-specific tasks expose the limitations of current models in understanding nuanced cultural contexts. The results highlight challenges related to language variation and the necessity of culturally diverse benchmarks for advancing LMM performance in multilingual settings."}}]