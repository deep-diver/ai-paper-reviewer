{"references": [{" publication_date": "2023", "fullname_first_author": "Hugo Touvron", "paper_title": "Llama: Open and efficient foundation language models", "reason": "This paper is highly influential in the field of large language models (LLMs), introducing the LLaMA architecture, which has significantly impacted subsequent research and development in LLMs, including multimodal models.  Its open-source nature has made it a cornerstone for further advancements.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Xiang Yue", "paper_title": "MMMU: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi", "reason": "This paper introduces the MMMU benchmark, which serves as a foundation and inspiration for JMMMU.  The MMMU's comprehensive design and rigorous evaluation methodology provided a solid basis for the development of JMMMU, enabling a direct comparison and cultural adaptation of the benchmark.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Yuichi Inoue", "paper_title": "Heron-bench: A benchmark for evaluating vision language models in Japanese", "reason": "This work is directly related to JMMMU as it is one of the few existing culture-aware benchmarks for evaluating Japanese Vision Language Models.  This existing work is explicitly expanded upon by JMMMU, with JMMMU highlighting Heron-Bench's limitations in size and scale, which it addresses.  The inclusion of Heron-bench in the related works establishes JMMMU within the existing literature and benchmarks for Japanese language.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "SakanaAI", "paper_title": "JA-Multi-Image-VQA", "reason": "This paper is another existing Japanese multimodal benchmark relevant to the scope of the presented work. JMMMU explicitly references it as one of the existing culture-aware benchmarks in Japanese, highlighting its limitations in size and expert-level tasks, which is a problem JMMMU solves.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "SakanaAI", "paper_title": "JA-VG-VQA-500", "reason": "This benchmark serves as a further comparison point for the work presented. The authors explicitly mention it to highlight the lack of expert-level tasks and the limited scope of existing Japanese LMM benchmarks, thereby further justifying their contribution.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "SakanaAI", "paper_title": "JA-VLM-Bench-in-the-wild", "reason": "This paper is another related Japanese benchmark, mentioned in Section 2 to highlight existing limitations of cultural and expert level evaluation in current Japanese benchmark projects.  The comparison justifies the need and contributions of JMMMU by demonstrating its improvement over existing works.", "section_number": 2}, {" publication_date": "2018", "fullname_first_author": "Nobuyuki Shimizu", "paper_title": "Visual question answering dataset for bilingual image understanding: A study of cross-lingual transfer using attention maps", "reason": "This paper provides context and background on the progress of vision-language understanding in Japanese, thereby providing a direct comparison for the proposed research.  Referencing it helps to establish the contribution of the proposed research.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Turing", "paper_title": "LLaVA-bench-in-the-wild", "reason": "This benchmark is highlighted in the related works to illustrate the limitations of existing datasets in the evaluation of multilingual LLMs.  It serves as a supporting argument for the proposed JMMMU benchmark, showcasing the lack of culturally-aware and expert-level Japanese evaluations in the field.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Turing", "paper_title": "LLaVA-bench-ja", "reason": "The authors mention this to support their claim of a lack of comprehensive benchmarks specifically designed for Japanese LMMs, providing a critical comparison to their proposed JMMMU benchmark which fills this gap.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Yuichi Inoue", "paper_title": "Heron-bench: A benchmark for evaluating vision language models in Japanese", "reason": "This benchmark showcases the current state of Japanese vision-language model benchmarks. JMMMU builds upon this work, addressing its shortcomings and providing a much larger, more comprehensive benchmark.", "section_number": 3}, {" publication_date": "2015", "fullname_first_author": "Stanislaw Antol", "paper_title": "VQA: Visual question answering", "reason": "This foundational paper in Visual Question Answering (VQA) provides a baseline for understanding the development of multimodal benchmarks.  Mentioning it allows the authors to position JMMMU within the broader context of VQA research and highlight how it addresses limitations of earlier approaches.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Lichang Chen", "paper_title": "Alpaca-plus: Training a better alpaca with fewer data", "reason": "This is a significant paper in the realm of large language models that has directly influenced the development of several LMMs.  It is particularly relevant because it illustrates the current trends in large language model training that are relevant to the open-source models being evaluated.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Zhe Chen", "paper_title": "InternVL: Scaling up vision foundation models and aligning for generic visual-linguistic tasks", "reason": "This paper is mentioned to showcase the advancements in vision-language models, setting the stage for evaluating the performance of these models on the JMMMU benchmark.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Wenyi Hong", "paper_title": "CogVLM2: Visual language models for image and video understanding", "reason": "This paper introduces the CogVLM2 model, which is evaluated in the experiments.  Highlighting this model and its performance provides a specific comparison point within the overall evaluation results of JMMMU.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Dongfu Jiang", "paper_title": "Mantis: Interleaved multi-image instruction tuning", "reason": "This paper introduces the Mantis model, which is evaluated in the experiments.  It is significant because it is one of the open-source models whose performance on the JMMMU benchmark is analyzed and discussed.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Hugo Lauren\u00e7on", "paper_title": "Building and better understanding vision-language models: insights and future directions", "reason": "This paper offers valuable insights and perspectives on the development and evaluation of vision-language models.  It sets the stage for understanding the challenges and opportunities present in the field and helps provide context for the proposed JMMMU benchmark.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Haotian Liu", "paper_title": "LLaVA-next: Improved reasoning, ocr, and world knowledge", "reason": "This paper details the LLaVA-Next model, which is one of the open-source models evaluated in the experiments.  It demonstrates advancements in LLM capabilities that are relevant to the overall experimental design and results of JMMMU.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Pan Lu", "paper_title": "Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts", "reason": "This paper is referenced to establish the broader context of evaluating multimodal models, particularly focusing on their mathematical reasoning capabilities.  This helps to position the JMMMU benchmark within the broader landscape of multimodal model evaluation and highlight its unique features.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Atsuyuki Miyai", "paper_title": "Unsolvable problem detection: Evaluating trustworthiness of vision language models", "reason": "This paper focuses on the trustworthiness of vision-language models and is closely related to the aims of JMMMU.  Its inclusion in the related works section establishes the relevance of the proposed work and its contribution to the field.", "section_number": 2}]}