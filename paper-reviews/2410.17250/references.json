{"references": [{" publication_date": "2015", "fullname_first_author": "Stanislaw Antol", "paper_title": "VQA: Visual question answering", "reason": "This seminal paper introduced the Visual Question Answering (VQA) task, which is fundamental to the field of multimodal understanding.  It laid the groundwork for numerous subsequent benchmarks and datasets in the area and established the evaluation metrics used extensively in the field. The development of JMMMU builds upon the progress in VQA and aims to expand upon the task by making the VQA tasks culture specific and more challenging.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Lichang Chen", "paper_title": "Alpagasus: Training a better alpaca with fewer data", "reason": "This paper is significant because it introduces Alpaca, a large language model that is trained to be instruction-following.  Many of the benchmarks in this study have adopted the instruction following scheme to improve their performance, directly and indirectly.  The research direction in this paper is related to the instruction-following aspect discussed in section 5 of this paper.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Zhe Chen", "paper_title": "InternVL: Scaling up vision foundation models and aligning for generic visual-linguistic tasks", "reason": "This work is highly relevant as it introduces InternVL, a large-scale vision and language model that pushes the state-of-the-art in multimodal understanding. The performance of InternVL is benchmarked in the study, thus its importance.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Lin Chen", "paper_title": "Sharegpt4v: Improving large multi-modal models with better captions", "reason": "This paper discusses improvements in large multimodal models using better captioning techniques.  As the benchmark in this study involves image captioning, the methods and results in this paper are related and important.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Xingyu Fu", "paper_title": "Blink: Multimodal large language models can see but not perceive", "reason": "This paper is important because it provides a valuable analysis of the limitations of current large multimodal models, which directly relates to the challenges and directions for future research discussed in section 5 of this paper.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "Deepak Gupta", "paper_title": "A unified framework for multilingual and code-mixed visual question answering", "reason": "This work is highly relevant because it presents a framework for multilingual and code-mixed visual question answering (VQA), which is fundamental to the task this paper intends to tackle. It provides the base for multilingual multimodal models which is the focus of the related work and the proposed model.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Wenyi Hong", "paper_title": "Cogvlm2: Visual language models for image and video understanding", "reason": "CogVLM2 is one of the models tested in this study.  It demonstrates a significant advance in the field of multimodal understanding, and evaluating its performance is crucial for understanding the current state-of-the-art and identifying areas for improvement.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Yuichi Inoue", "paper_title": "Heron-bench: A benchmark for evaluating vision language models in japanese", "reason": "This paper introduces Heron-bench, a Japanese benchmark for evaluating vision-language models. Since this work is also proposing a new benchmark, comparing with other existing benchmarks, especially those focusing on Japanese, is important to justify the contribution.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Dongfu Jiang", "paper_title": "Mantis: Interleaved multi-image instruction tuning", "reason": "Mantis is one of the models evaluated in this paper, which showcases improved instruction-following capabilities.  Understanding its performance relative to other models is crucial for evaluating the impact of instruction tuning on multilingual and multimodal understanding.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Hugo Lauren\u00e7on", "paper_title": "Building and better understanding vision-language models: insights and future directions", "reason": "This paper provides valuable insights into the challenges and future directions of vision-language model development.  Since the paper focuses on evaluating new benchmarks and improving existing models, referencing this paper enhances the significance and impact of the study.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Pan Lu", "paper_title": "Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts", "reason": "This paper proposes MathVista, a benchmark for evaluating mathematical reasoning capabilities of foundational models.  The authors also emphasize the importance of  multilingual evaluations, which is directly relevant to the rationale and contributions of the proposed benchmark.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Atsuyuki Miyai", "paper_title": "Unsolvable problem detection: Evaluating trustworthiness of vision language models", "reason": "This paper is important as it tackles the crucial issue of evaluating the trustworthiness of vision-language models.  This directly relates to the broader theme of the paper, which aims to provide a more robust benchmark for evaluating the reliability and trustworthiness of models' performance.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Masoud Monajatipoor", "paper_title": "Metavl: Transferring in-context learning ability from language models to vision-language models", "reason": "This paper focuses on transferring in-context learning abilities from language models to vision-language models. This is highly relevant to the work as the paper evaluates instruction-following capabilities of the models. The methods discussed here would be valuable to compare against and could be used as a future direction.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Fangyu Liu", "paper_title": "Visually grounded reasoning across languages and cultures", "reason": "This paper is highly relevant as it directly addresses the challenge of creating benchmarks for multimodal reasoning across different languages and cultures.  It is foundational work related to the goals and contributions of this paper.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Yuan Liu", "paper_title": "MM-Bench: Is your multi-modal model an all-around player?", "reason": "This paper introduces MM-Bench, a comprehensive benchmark for evaluating multimodal models.  Evaluating the proposed model against this benchmark, which considers various tasks and aspects of multimodal understanding, would significantly enhance the paper's evaluation and comparison.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "David Romero", "paper_title": "Cvqa: Culturally-diverse multilingual visual question answering benchmark", "reason": "This paper is highly relevant because it addresses the need for culturally diverse benchmarks in visual question answering. Since the study focuses on the cultural aspect of multimodal model evaluation, this paper would be a valuable contribution to the discussion of related work.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Nobuyuki Shimizu", "paper_title": "Visual question answering dataset for bilingual image understanding: A study of cross-lingual transfer using attention maps", "reason": "This early work in the field of visual question answering (VQA) is relevant to the current research on the cultural aspects of multimodal model understanding. Its focus on cross-lingual transfer is directly relevant to the approach of this paper.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Hugo Touvron", "paper_title": "Llama: Open and efficient foundation language models", "reason": "LLaMA is the base language model for many of the models evaluated in this paper. Therefore, understanding its characteristics and capabilities is essential for interpreting the results and assessing the performance of the models built upon LLaMA.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Xiang Yue", "paper_title": "MMMU: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi", "reason": "This paper introduces MMMU, a large-scale benchmark for evaluating multimodal models.  It serves as a significant precedent and inspiration for JMMMU, which expands on MMMU's design and focuses specifically on the Japanese language and cultural context.  Understanding MMMU is crucial for appreciating the novelty and contribution of JMMMU.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Ge Zhang", "paper_title": "CMMMU: A chinese massive multi-discipline multimodal understanding benchmark", "reason": "This paper introduces CMMMU, a Chinese counterpart to MMMU.  As JMMMU is the Japanese counterpart to MMMU, comparing the methodologies and results of these culturally-specific benchmarks is crucial for understanding the broader context and impact of this research.", "section_number": 3}]}