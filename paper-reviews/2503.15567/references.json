{"references": [{"fullname_first_author": "Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-01-01", "reason": "This paper is crucial because it introduces the Transformer architecture, which is used in the decoder and is a fundamental component of modern neural networks."}, {"fullname_first_author": "Kingma", "paper_title": "Auto-Encoding Variational Bayes", "publication_date": "2014-01-01", "reason": "This paper is essential as it introduces Variational Autoencoders (VAEs), which are used to compress 3D molecules into a latent space, a key step in the UDM-3D approach."}, {"fullname_first_author": "Ho", "paper_title": "Denoising Diffusion Probabilistic Models", "publication_date": "2020-01-01", "reason": "This paper is fundamental because it introduces denoising diffusion probabilistic models, which are used to generate molecules in the latent space."}, {"fullname_first_author": "Rombach", "paper_title": "High-Resolution Image Synthesis with Latent Diffusion Models", "publication_date": "2022-01-01", "reason": "This paper is significant as it presents Latent Diffusion Models (LDMs), which are used for generative modeling in the compressed latent space, a key step in the UDM-3D approach."}, {"fullname_first_author": "Diao", "paper_title": "Relational Attention: Generalizing Transformers for Graph-Structured Tasks", "publication_date": "2023-01-01", "reason": "This paper is important because it introduces the Relational Transformer (R-Trans), used in the encoder to effectively integrate multi-modal features into token sequences."}]}