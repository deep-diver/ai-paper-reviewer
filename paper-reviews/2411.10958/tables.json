[{"content": "| Method | Smoothing (Q+K) | Llama 3.1 (Lambda) \u2191 | Llama 3.1 (WikiText) \u2193 | CogVideo (vqa-a) \u2191 | CogVideo (vqa-t) \u2191 |\n|---|---|---|---|---|---| \n| **Full-Precision** | - | 81.5% | 6.013 | 77.605 | 75.360 |\n| **INT4 Quantization** | \u2717 | 72.6% | 11.698 | 27.114 | 24.670 |\n|  | \u2713 | **80.8%** | **6.219** | **77.276** | **75.147** |", "caption": "Table 1: End-to-end metrics comparison of different quantization methods, where Q,K are quantized into INT4, while P,V stay in full precision.", "description": "This table presents a comparison of end-to-end performance metrics across various quantization methods.  The focus is on the impact of quantizing the Q and K matrices to 4-bit integers (INT4), while keeping the P and V matrices at full precision.  The metrics used allow for evaluation of the accuracy loss introduced by the quantization process.  The table helps to assess whether quantizing Q and K to INT4 while maintaining P and V at full precision leads to significant performance degradation.", "section": "3.2 Per-warp INT4 Quantization"}, {"content": "| Method | Cos Sim \u2191 | Relative L1 \u2193 | RMSE \u2193 |\n|---|---|---|---|\n| Per-token | 99.45% | 0.0649 | 0.0335 |\n| Per-warp | 99.45% | 0.0648 | 0.0334 |\n| Per-block | 98.03% | 0.1492 | 0.0744 |\n| Per-tensor | 97.15% | 0.1800 | 0.0865 |", "caption": "Table 6: An accuracy example on real tensors of CogvideoX model with or without smoothing V\ud835\udc49Vitalic_V.", "description": "This table presents a comparison of the accuracy of dot product operations using FP22 data type in the CogvideoX model, with and without applying a smoothing technique to matrix V.  It demonstrates the impact of smoothing V on mitigating precision loss inherent in the FP22 accumulator used for the FP8 matrix multiplication. The table visually shows heatmaps to illustrate the data distribution in matrices V and P, and a graph showing the error of FP22 compared to FP32.", "section": "3.4 Smooth V"}, {"content": "| Method | Cos Sim \u2191 | Relative L1 \u2193 | RMSE \u2193 |\n|---|---|---|---|\n| Per-token | 96.76% | 0.1916 | 0.0775 |\n| Per-warp | 96.71% | 0.1956 | 0.0779 |\n| Per-block | 90.68% | 0.3615 | 0.1490 |\n| Per-tensor | 85.85% | 0.4687 | 0.2261 |", "caption": "Table 7: Error of the FP8 Matmul instruction of mma(f8f8f32).", "description": "This table shows the errors in the FP8 matrix multiplication instruction, mma(f32.f8.f8.f32), compared to the results obtained using the FP32 instruction.  It illustrates the precision loss incurred when using the FP8 accumulator in FP8 matrix multiplications. The table displays the accumulated value errors for different precision levels, highlighting the discrepancies between FP8 and FP32 calculations.", "section": "3.4 Smooth V"}, {"content": "| Q,K | \\widetilde{P},V | Cos Sim \u2191 | Relative L1 \u2193 | RMSE \u2193 |\n|---|---|---|---|---|\n| INT4 | INT8 | <span style=\"color:#990000;\">77.05%</span> | <span style=\"color:#990000;\">0.5618</span> | <span style=\"color:#990000;\">0.5044</span> |\n| INT4 | E5M2 | <span style=\"color:#990000;\">99.20%</span> | <span style=\"color:#990000;\">0.0905</span> | <span style=\"color:#990000;\">0.0903</span> |\n| INT4 | **E4M3** | <span style=\"color:#008000;\">**99.44%**</span> | <span style=\"color:#008000;\">**0.0683**</span> | <span style=\"color:#008000;\">**0.0347**</span> |\n| INT4 | **FP16** | **99.45%** | **0.0649** | **0.0335** |", "caption": "Table 8: Two kernel implementations of SageAttention2.", "description": "This table presents two different kernel implementations of the SageAttention2 algorithm.  The key difference lies in the quantization granularity used for the Q and K matrices, and the speed/accuracy trade-off involved.  SageAttn2-4b uses 4-bit quantization per-warp, while SageAttn2-8b uses 8-bit quantization per-warp, for both Q and K.  Both implementations employ FP8 for P and V, with a per-block and per-channel quantization strategy, respectively.", "section": "3 SageAttention-2"}, {"content": "| Q,K | \\widetilde{P},V | Cos Sim \u2191 | Relative L1 \u2193 | RMSE \u2193 |\n|---|---|---|---|---|\n| INT4 | INT8 | 19.52% | 0.9579 | 1.4483 |\n|  | E5M2 | 94.94% | 0.2327 | 0.2361 |\n|  | **E4M3** | **96.70%** | **0.1956** | **0.0779** |\n| **FP16** |  | 96.76% | 0.1916 | 0.0775 |", "caption": "Table 11: End-to-end metrics loss across text, image, and video generation models.", "description": "This table presents a comprehensive evaluation of the end-to-end performance of the proposed SageAttention2 model across various tasks involving text, image, and video generation.  For each model (Llama2, Llama3.1, GLM4, CogvideoX, Open-Sora, Flux, and TIMM), it compares the performance of the full-precision attention mechanism with various quantization methods.  Metrics reported include perplexity (for text), accuracy (for text and image classification), and specific metrics relevant to video generation and image quality (CLIPSim, CLIP-Temp, VQA-a, VQA-t, FScore, FID, sFID, CLIP score, and ImageReward).  It demonstrates the impact of different quantization approaches on the overall model performance.", "section": "4 Experiment"}]