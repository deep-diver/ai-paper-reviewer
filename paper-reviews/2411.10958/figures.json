[{"figure_path": "https://arxiv.org/html/2411.10958/x1.png", "caption": "Figure 1: The left figure shows the kernel speedup on RTX4090 GPU. The right figure shows the end-to-end inference speedup of generating the first token and performance metrics for the needle-in-a-haystack task\u00a0(gkamradt, 2023) with a sequence length of 100K on Llama3.1 on L20 GPU.", "description": "This figure is a composite of two sub-figures. The left sub-figure presents a bar chart illustrating the kernel speedup achieved by SageAttention2 compared to FlashAttention2 on an RTX 4090 GPU.  It showcases the significant performance improvement of SageAttention2. The right sub-figure displays the end-to-end inference speedup and performance metrics for the same models, but this time focusing on the 'needle-in-a-haystack' task using the LLaMA-3-8B model. The task is performed with a sequence length of 100K tokens, providing a comparison of the inference speed across the two attention mechanisms.", "section": "Preliminary"}, {"figure_path": "https://arxiv.org/html/2411.10958/x2.png", "caption": "Figure 2: An example of quantizing Q, K to INT4 from CogvideoX.", "description": "This figure demonstrates the consequences of directly quantizing the query (Q) and key (K) matrices to 4-bit integers (INT4) during the attention mechanism of the CogvideoX model.  Direct quantization without additional techniques leads to significant information loss, resulting in a drastic reduction in the quality of the generated video.  It visually showcases the difference between using a naive INT4 quantization and the proposed SageAttention2 method.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2411.10958/x3.png", "caption": "Figure 3: Workflow of SageAttention2.  1 Smooth Q,K,V.  2 A GEMV to obtain \u0394\u2062S\u0394\ud835\udc46\\Delta Sroman_\u0394 italic_S.  3 Per-warp quantize Q,K and per-channel quantize V.  4 Perform the SageAttention2 kernel.  5 Correct the output.", "description": "This figure illustrates the workflow of the SageAttention2 algorithm, a novel method for accelerating attention mechanisms in deep learning models. The process begins by smoothing the Q, K, and V matrices to improve accuracy (Step 1).  A general matrix-vector multiplication (GEMV) is then performed to obtain \u0394S (Step 2).  Subsequently, the Q and K matrices are quantized using a per-warp approach, while V is quantized per-channel (Step 3). This is followed by execution of the core SageAttention2 kernel (Step 4). Finally, the output is corrected to ensure accuracy (Step 5). This detailed breakdown clarifies each step involved in the algorithm's operation.", "section": "3 SageAttention-2"}, {"figure_path": "https://arxiv.org/html/2411.10958/x4.png", "caption": "Figure 4: Typical examples of tensors\u2019 data distribution in attention.", "description": "This figure visualizes the distribution of data within various tensors used in the attention mechanism.  It showcases examples from different models and highlights the range and distribution of values for the Q, K, V, and S tensors, illustrating how their data characteristics vary across tokens and channels.  This visualization is important to understanding the challenges of quantization, as uneven or extreme value distributions can make effective quantization difficult.", "section": "3 SageAttention-2"}, {"figure_path": "https://arxiv.org/html/2411.10958/x5.png", "caption": "Table 2: Average accuracy across all layers using different quantization granularities.", "description": "This table presents a comparison of the average accuracy achieved across all layers of a model when different quantization granularities are used for the Q and K matrices in the attention mechanism.  It compares the cosine similarity, relative L1 distance, and RMSE across four different quantization methods: per-token, per-warp, per-block, and per-tensor.  The table helps illustrate the trade-off between quantization granularity and accuracy.", "section": "3.2 Per-warp INT4 Quantization"}, {"figure_path": "https://arxiv.org/html/2411.10958/x6.png", "caption": "Table 3: Worst accuracy across all layers using different quantization granularities.", "description": "This table presents the worst-case accuracy metrics across all layers of a model when different quantization granularities are used for the Q and K matrices in the attention mechanism.  The metrics shown are Cosine Similarity (Cos Sim), Relative L1 distance, and Root Mean Squared Error (RMSE). Lower values for Relative L1 and RMSE indicate better accuracy.  The table helps to illustrate the impact of the choice of quantization granularity on the accuracy of the model's attention mechanism.", "section": "3.2 Per-warp INT4 Quantization"}, {"figure_path": "https://arxiv.org/html/2411.10958/x7.png", "caption": "Figure 5: An example of quantized value distribution of Q\ud835\udc44Qitalic_Q before and after smoothing Q\ud835\udc44Qitalic_Q.", "description": "Figure 5 displays histograms illustrating the distribution of quantized values for the Q matrix before and after applying a smoothing technique.  The x-axis represents the quantized values, while the y-axis indicates frequency. The before-smoothing histogram shows a less uniform distribution, concentrated towards the extremes of the quantized range. The after-smoothing histogram demonstrates a more uniform distribution of quantized values, suggesting that smoothing successfully mitigated the effect of outliers and improved the overall quantization accuracy.", "section": "3.3 Smooth Q"}, {"figure_path": "https://arxiv.org/html/2411.10958/x8.png", "caption": "Table 4: Average accuracy using different data types of (P~,V)~\ud835\udc43\ud835\udc49(\\widetilde{P},V)( over~ start_ARG italic_P end_ARG , italic_V ) across all layers of a CogvideoX model, where (Q,K)\ud835\udc44\ud835\udc3e(Q,K)( italic_Q , italic_K ) are smoothed.", "description": "This table presents a comparison of the average accuracy achieved across all layers of the CogvideoX model when using different data types for matrices P and V in the attention mechanism.  The accuracy is measured using various metrics. Notably, matrices Q and K are smoothed before being used in the attention calculations. The different data types explored include INT8, FP16, and INT4 for (P, V) to compare the performance of using various levels of precision for these matrices.  This allows for evaluating the trade-off between computational efficiency and accuracy.", "section": "3.5 Quantization for Q, K, P, V"}, {"figure_path": "https://arxiv.org/html/2411.10958/x9.png", "caption": "Table 5: Worst accuracy using different data types of (P~,V)~\ud835\udc43\ud835\udc49(\\widetilde{P},V)( over~ start_ARG italic_P end_ARG , italic_V ) across all layers of a CogvideoX model, where (Q,K)\ud835\udc44\ud835\udc3e(Q,K)( italic_Q , italic_K ) are smoothed.", "description": "This table presents the worst-case accuracy metrics across all layers of the CogvideoX model when using different data types for matrices P and V in the attention mechanism.  The accuracy is evaluated using several metrics, such as cosine similarity, relative L1 distance, and root mean square error. The Q and K matrices are pre-processed using a smoothing technique to improve accuracy. The different data types tested include INT8, E5M2, INT4, and FP16, allowing for comparison of performance with various quantization methods.", "section": "3.5 Quantization for Q, K, P, V"}, {"figure_path": "https://arxiv.org/html/2411.10958/x10.png", "caption": "Figure 6: An example of dot product precison a row of P~~\ud835\udc43\\widetilde{P}over~ start_ARG italic_P end_ARG and a column of V\ud835\udc49Vitalic_V presented by FP22 data type.", "description": "This figure visualizes the impact of using a 22-bit accumulator (FP22) instead of a 32-bit accumulator (FP32) during the matrix multiplication of P and V in the attention mechanism. It compares the dot product precision of a row from matrix P and a column from matrix V when using FP22.  The heatmaps show the distribution of values before and after applying the smoothing technique to V.  The graph illustrates the error introduced by using FP22 compared to the higher precision FP32.", "section": "3.4 Smooth V"}, {"figure_path": "https://arxiv.org/html/2411.10958/x11.png", "caption": "Figure 7: Mean and standard deviation of c\u2062o\u2062s\u2062s\u2062i\u2062m\u2217(1\u2212L\u20621)\ud835\udc50\ud835\udc5c\ud835\udc60\ud835\udc60\ud835\udc56\ud835\udc5a1\ud835\udc3f1cossim*(1-L1)italic_c italic_o italic_s italic_s italic_i italic_m \u2217 ( 1 - italic_L 1 )) of SageAttn-4b in different layers and timesteps for different inputs in Llama3.1 and CogvideoX.", "description": "Figure 7 shows the performance of the SageAttn-4b model (a 4-bit attention mechanism) across different layers and timesteps of the Llama3.1 and CogvideoX models.  It plots the mean and standard deviation of a combined accuracy metric, calculated as cossim * (1 - L1), which balances cosine similarity (cossim) and relative L1 distance (L1).  Higher values indicate better performance.  The figure aims to illustrate whether the accuracy of SageAttn-4b is consistent across different parts of the network and with different inputs, highlighting potential areas where it may underperform.", "section": "3.6 Adaptive Quantization over Layer and Timestep"}, {"figure_path": "https://arxiv.org/html/2411.10958/x12.png", "caption": "Figure 8: Speed comparison between SageAttention2 and baselines (RTX4090, headdim=64).", "description": "This figure displays a speed comparison of SageAttention2 against several baselines using the RTX4090 GPU with a hidden dimension of 64.  The x-axis represents the sequence length, and the y-axis represents the speed in TOPS (Trillions of Operations Per Second). Different colored bars show the performance for each method: Torch, xformers, FlashAttention2, SageAttention, SageAttention2-8b, and SageAttention2-4b.  The graph visually demonstrates how SageAttention2 achieves faster performance than other approaches, especially at longer sequence lengths.", "section": "4.2 Speed and Accuracy of Kernels"}, {"figure_path": "https://arxiv.org/html/2411.10958/x13.png", "caption": "Figure 9: Speed comparison between SageAttention2 and baselines (RTX4090, headdim=128).", "description": "This figure compares the speed of SageAttention2 with several baselines (Torch, xformers, and FlashAttention2) on an RTX4090 GPU. The experiment is performed with a hidden dimension size of 128 and for both causal and non-causal attention mechanisms.  The x-axis represents the sequence length, while the y-axis shows the speed in TOPS (Tera Operations Per Second). The different lines represent different methods, allowing a direct comparison of their performance across varying sequence lengths. It helps to visualize the efficiency gains of SageAttention2 over existing attention mechanisms.", "section": "3 SageAttention-2"}, {"figure_path": "https://arxiv.org/html/2411.10958/x14.png", "caption": "Figure 10: Speed comparison between SageAttention2 and baselines (RTX4090, headdim=256).", "description": "This figure showcases a performance comparison between SageAttention2 and other baseline methods for attention mechanisms. The comparison is based on the speed (measured in TOPS - Tera Operations Per Second) achieved by each method while processing sequences of varying lengths on an RTX 4090 GPU.  The different settings include causal and non-causal attention, with head dimensions of 256. The graph likely shows SageAttention2's speed advantage over other methods, especially as sequence length increases.", "section": "3 SageAttention-2"}, {"figure_path": "https://arxiv.org/html/2411.10958/x15.png", "caption": "Figure 11: Speed comparison between SageAttention2 and baselines (L20, headdim=64).", "description": "This figure presents a comparison of the inference speed among four different attention mechanisms: SageAttention2 (with 4-bit and 8-bit implementations), FlashAttention2, and xformers.  The comparison is performed on an L20 GPU with a head dimension of 64. The x-axis represents the sequence length, and the y-axis shows the inference speed measured in TOPS (Tera Operations Per Second).  The figure allows for a direct visual assessment of the relative performance gains of SageAttention2 compared to existing state-of-the-art methods across different sequence lengths.  Separate graphs are provided for both causal and non-causal attention.", "section": "3.5 Quantization for Q, K, P, V"}]