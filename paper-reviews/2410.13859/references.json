{"references": [{" publication_date": "2023", "fullname_first_author": "Josh Achiam", "paper_title": "GPT-4 technical report", "reason": "This paper is foundational as it presents a technical report on GPT-4, a highly influential and widely used large language model (LLM).  Its detailed analysis of GPT-4's architecture, performance, and limitations provides crucial context for understanding advancements in LLMs and their extension to multimodal applications, as discussed in the paper on \u03b3-MoD.  The report's insights are relevant to the challenges of computational cost and efficiency improvements in large language models, a central theme of the \u03b3-MoD paper.", "section_number": 1}, {" publication_date": "2024b", "fullname_first_author": "Gen Luo", "paper_title": "Feast your eyes: Mixture-of-resolution adaptation for multimodal large language models", "reason": "This paper is highly relevant to the study's main topic of efficient multimodal large language models (MLLMs).  It directly addresses the computational cost limitations of LLMs and specifically explores the efficiency improvements through mixture-of-resolution adaptation.  The findings and methodology of this paper provide valuable insights and context for the \u03b3-MoD approach to address similar efficiency issues.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "David Raposo", "paper_title": "Mixture-of-depths: Dynamically allocating compute in transformer-based language models", "reason": "This paper introduces the Mixture-of-Depths (MoDs) approach, which is the foundation for the \u03b3-MoD method proposed in the main study.  It provides the theoretical basis and conceptual framework for reducing computation by selectively skipping layers for less important tokens.  The paper's findings and insights into MoDs are crucial for understanding the novelty and improvements of \u03b3-MoD.", "section_number": 1}, {" publication_date": "2024d", "fullname_first_author": "Haotian Liu", "paper_title": "Visual instruction tuning", "reason": "This paper is significant due to its introduction of the visual instruction tuning technique applied in LLMs. The study's approach to adapting MoDs to existing MLLMs is closely related to the advancements in visual instruction tuning and benefits from similar techniques.  The insights gained in this paper help to contextulize the improvements and efficiency gains proposed in the \u03b3-MoD method.", "section_number": 2}, {" publication_date": "2024b", "fullname_first_author": "Gen Luo", "paper_title": "Feast your eyes: Mixture-of-resolution adaptation for multimodal large language models", "reason": "This work directly addresses the efficiency problem in multimodal LLMs, which is the core focus of the main study. The exploration of mixture-of-resolution adaptation, aimed at reducing computational costs, is highly relevant to the \u03b3-MoD's approach.  The results and methodology of this study offer valuable insights and contextual information for the \u03b3-MoD method.", "section_number": 2}, {" publication_date": "2024a", "fullname_first_author": "Bin Lin", "paper_title": "Moe-llava: Mixture of experts for large vision-language models", "reason": "This paper is highly relevant as it explores the use of Mixture-of-Experts (MoEs) in large vision-language models (LLaVMs), a key related approach to the study's main focus on adapting Mixture-of-Depths (MoDs) to MLLMs. Comparing and contrasting MoEs and MoDs, as done in the main paper, helps to highlight the advantages and efficiency improvements of MoDs over MoEs for MLLMs.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "David Raposo", "paper_title": "Mixture-of-depths: Dynamically allocating compute in transformer-based language models", "reason": "This paper directly introduces the core concept of Mixture-of-Depths (MoDs), which is central to the main study's proposed \u03b3-MoD method.  It provides the foundation and theoretical basis for the technique used for efficiently reducing the computational cost in LLMs by dynamically skipping less important computations. The paper's findings and insights are crucial for understanding the novelty and contributions of \u03b3-MoD.", "section_number": 3}, {" publication_date": "2024a", "fullname_first_author": "Bo Li", "paper_title": "Llava-onevision: Easy visual task transfer", "reason": "This paper is important due to its introduction of LLaVA-OneVision, a multimodal model that demonstrates advancements in efficiency and ease of visual task transfer.  The paper provides significant contextual information to the study's work, particularly regarding the efficiency improvements and task adaptation in the context of MLLMs, which are relevant aspects of the \u03b3-MoD method.", "section_number": 5}, {" publication_date": "2024b", "fullname_first_author": "Gen Luo", "paper_title": "Feast your eyes: Mixture-of-resolution adaptation for multimodal large language models", "reason": "This paper is directly related to the main study as it focuses on improving the efficiency of multimodal large language models (MLLMs) using a mixture-of-resolution adaptation technique. The results, methodology, and findings provide valuable insights and contextual background information to the \u03b3-MoD approach.", "section_number": 5}, {" publication_date": "2024a", "fullname_first_author": "Bin Lin", "paper_title": "Moe-llava: Mixture of experts for large vision-language models", "reason": "This paper is significant because it proposes MoE-LLaVA, a related approach focusing on improving the efficiency of LLMs using Mixture-of-Experts (MoEs). This provides a relevant comparison point and context for the \u03b3-MoD's use of MoDs, allowing the paper to highlight the advantages of MoDs over MoEs in certain aspects for improving MLLM efficiency.", "section_number": 5}, {" publication_date": "2024b", "fullname_first_author": "Gen Luo", "paper_title": "LLaVA-HR", "reason": "This paper introduces LLaVA-HR, a state-of-the-art multimodal language model that is used in several comparisons within the main study's experimental evaluation.  The LLaVA-HR model serves as a benchmark and is directly compared to the \u03b3-MoD's performance in terms of efficiency and accuracy, highlighting the improvements achieved through the \u03b3-MoD method.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "David Raposo", "paper_title": "Mixture-of-depths: Dynamically allocating compute in transformer-based language models", "reason": "This paper is highly relevant as it presents the foundational Mixture-of-Depths (MoDs) approach that forms the basis for the \u03b3-MoD method.  It details the theoretical framework, core mechanisms, and potential benefits of MoDs for enhancing the efficiency of language models, which serves as a fundamental basis for the \u03b3-MoD methodology.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Josh Achiam", "paper_title": "GPT-4 technical report", "reason": "This paper is important as it provides the technical details of GPT-4, a widely used LLM.  Its architecture, training data, and limitations serve as a benchmark and reference for comparisons with the \u03b3-MoD's performance and efficiency improvements. The paper offers a crucial comparison point in the context of large language models.", "section_number": 5}, {" publication_date": "2024a", "fullname_first_author": "Bo Li", "paper_title": "Llava-onevision: Easy visual task transfer", "reason": "The LLaVA-OneVision model is compared against \u03b3-MoD within the study's experiments.  The paper is important for providing a relevant baseline for evaluating the efficiency and accuracy of the proposed method. It allows for a direct comparison of the \u03b3-MoD against similar methods and for better understanding of the improvements achieved by \u03b3-MoD.", "section_number": 5}, {" publication_date": "2020", "fullname_first_author": "Mingbao Lin", "paper_title": "Hrank: Filter pruning using high-rank feature map", "reason": "This paper is highly relevant to the study due to its introduction of the HRank method for filter pruning, which is conceptually related to the ARank metric used in \u03b3-MoD.   The HRank method's techniques are relevant to the efficient computation of the ARank metric, which guides the MoD deployment in \u03b3-MoD. The paper thus informs a crucial part of the \u03b3-MoD methodology.", "section_number": 4}, {" publication_date": "2019", "fullname_first_author": "Daniel M Ziegler", "paper_title": "Fine-tuning language models from human preferences", "reason": "This paper provides crucial context to the study's focus on instruction tuning, which is a key component of \u03b3-MoD. The findings and methodology of this paper are important for understanding the impact of human preferences and fine-tuning on language model performance, which directly informs and is relevant to the \u03b3-MoD's training process and methodology.", "section_number": 5}, {" publication_date": "2017", "fullname_first_author": "Yash Goyal", "paper_title": "Making the v in vqa matter: Elevating the role of image understanding in visual question answering", "reason": "This paper is important as it introduces the VQA dataset, a key benchmark used in the experimental evaluation of the \u03b3-MoD method. This dataset provides a widely recognized and standardized evaluation set for visual question answering models, which is directly related to the capabilities and evaluation of MLLMs, like \u03b3-MoD.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Josh Achiam", "paper_title": "GPT-4 technical report", "reason": "This paper serves as a baseline for comparing the performance and capabilities of \u03b3-MoD.  The GPT-4 model is a widely-used LLM and its characteristics help contextualize the improvements proposed by the \u03b3-MoD model in terms of efficiency and performance.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Hugo Touvron", "paper_title": "Llama: Open and efficient foundation language models", "reason": "This paper is important as it introduces the LLaMA model, a significant LLM that serves as a point of comparison for evaluating the \u03b3-MoD method. LLaMA's architecture and efficiency make it a relevant baseline for assessing the performance and efficiency gains of the proposed method. The use of LLaMA in comparisons helps to contextualize and validate the \u03b3-MoD's improvements.", "section_number": 2}]}