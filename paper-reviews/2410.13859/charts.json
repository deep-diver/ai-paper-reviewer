[{"figure_path": "2410.13859/charts/charts_2_0.png", "caption": "Figure 1: Visualization of attention maps in the MLLM and comparison of MoE with MoD. (a) Lower-rank layers often exhibit redundancy in their attention computation. (b) Different from MoE, MoD achieves the computational sparsity from the perspective of \u201cactivated token\u201d, where the computational budget is dynamically allocated to each token.", "description": "The chart visualizes attention maps in a multimodal large language model (MLLM) to illustrate layer redundancy and compares the Mixture-of-Expert (MoE) and Mixture-of-Depth (MoD) approaches for achieving computational sparsity.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.13859/charts/charts_5_0.png", "caption": "Figure 3: Visualization of ARank based on different tasks (left) and sample sizes (right). The horizontal axis represents the layer index of LLaVA-HR. The darker color indicates the larger ARank.", "description": "The chart visualizes the rank of attention maps (ARank) across different layers of the LLaVA-HR model for various tasks and sample sizes.", "section": "4 METHOD"}, {"figure_path": "2410.13859/charts/charts_10_0.png", "caption": "Figure 4: Visualization of routing results for different MoD layers. \u201cQ\u201d, \u201cI\u201d and \u201cA\u201d denote the question, image and response, respectively. The skipped tokens in sub-figure (b) are colored in gray.", "description": "The chart visualizes the routing results of different MoD layers, showing which tokens are skipped (gray) and kept during processing.", "section": "5.3.3 QUALITATIVE ANALYSIS"}]