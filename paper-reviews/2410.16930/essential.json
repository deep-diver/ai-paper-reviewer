{"reason": "This paper introduces Math Neurosurgery, a novel method to isolate and manipulate math-specific parameters in LLMs using only forward passes, improving math performance without affecting general language skills.", "summary": "Math Neurosurgery precisely targets LLMs' math reasoning parameters via forward passes, boosting performance without harming other abilities.", "takeaways": ["Math Neurosurgery isolates math-specific LLM parameters using only forward passes, improving efficiency.", "Pruning these parameters removes math skills while preserving general language abilities.", "Scaling these parameters enhances math performance (4-17% on GSM8K) in various LLMs."], "tldr": "Large Language Models (LLMs) are increasingly used for complex tasks including mathematical reasoning.  However, it's unclear how this ability is encoded within the model's parameters. This research presents \"Math Neurosurgery,\" a novel method to pinpoint and manipulate the specific parts of an LLM responsible for mathematical reasoning.  Unlike previous methods, Math Neurosurgery uses only forward passes (simple calculations without backpropagation) which makes it computationally less expensive.  The researchers show that removing these identified parameters eliminates a model's ability to solve math problems while leaving its general language skills intact. Conversely, slightly increasing the strength of these parameters significantly improves the model's math capabilities, demonstrating a direct link between specific parameters and mathematical reasoning in LLMs.  The method's effectiveness is also showcased with various LLMs of different sizes and training, highlighting its robustness and potential applicability across different LLM architectures. This research provides valuable insights into the inner workings of LLMs and opens up new avenues for improving their performance on specific tasks."}