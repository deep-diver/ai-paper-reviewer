{"references": [{" publication_date": "2021", "fullname_first_author": "Karl Cobbe", "paper_title": "Training Verifiers to Solve Math Word Problems", "reason": "This paper introduces the GSM8K dataset, a benchmark dataset used extensively in the current paper for evaluating mathematical reasoning capabilities of LLMs.  Its use as a key evaluation metric makes this paper highly relevant to the current work's experimental setup and results interpretation.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Janice Ahn", "paper_title": "Large Language Models for Mathematical Reasoning: Progresses and Challenges", "reason": "This review paper provides a comprehensive overview of the field of mathematical reasoning in LLMs, which forms the background and context for the current paper's investigation into isolating math-specific parameters. Its insights into the challenges and progress in this area are crucial for understanding the contributions of this research.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Rishabh Agarwal", "paper_title": "Many-Shot In-Context Learning", "reason": "This work is referenced in the experimental setup for utilizing a subset of the GSM8K test split (200 random samples). The methodology concerning the experimental setup of using subsets of data is directly referenced and is critical to the methodology applied in this work.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Bradley Brown", "paper_title": "Large Language Monkeys: Scaling Inference Compute with Repeated Sampling", "reason": "This work is referenced in the experimental setup for utilizing a subset of the GSM8K test split (200 random samples). The methodology concerning the experimental setup of using subsets of data is directly referenced and is critical to the methodology applied in this work.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Ting-Yun Chang", "paper_title": "Do Localization Methods Actually Localize Memorized Data in LLMs?", "reason": "This paper provides a critical evaluation of existing skill localization methods in LLMs, informing the current research's approach to isolating math-specific parameters.  It directly addresses the challenges of disentangling specific skills from other intertwined abilities, a central concern in the current work.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Damai Dai", "paper_title": "Knowledge Neurons in Pretrained Transformers", "reason": "This paper explores the localization of knowledge within LLMs, offering valuable insights into how LLMs encode information. Its findings are relevant to understanding how math knowledge might be represented in the model parameters, which directly relates to the core investigation in the current research.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Abhishek Panigrahi", "paper_title": "Task-Specific Skill Localization in Fine-tuned Language Models", "reason": "This research investigates task-specific skill localization in LLMs, which directly relates to the central goal of the current study to isolate mathematical reasoning.  The methods and findings of this work inform and contextualize the novel approach proposed in the current paper.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Tianyi Tang", "paper_title": "Language-Specific Neurons: The Key to Multilingual Capabilities in Large Language Models", "reason": "This paper's focus on identifying language-specific neurons in LLMs is directly relevant to the current research's effort in identifying math-specific parameters.  The methods and findings are directly compared and contrasted with the novel approach proposed in the current paper.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Mingjie Sun", "paper_title": "A Simple and Effective Pruning Approach for Large Language Models", "reason": "This paper introduces the Wanda method, a state-of-the-art pruning technique used as a basis for the novel MathNeuro method in the current paper.  The Wanda method forms a critical component of MathNeuro's methodology, making this paper essential to understanding the current research's contributions.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Yun Luo", "paper_title": "An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning", "reason": "This paper investigates catastrophic forgetting in LLMs, which is relevant to the context of isolating math-specific parameters because interventions should ideally not negatively affect the model's abilities in other domains. The findings inform the considerations made in the experimental design to assess the impact of the proposed interventions.", "section_number": 4}, {" publication_date": "2017", "fullname_first_author": "Guokun Lai", "paper_title": "RACE: Large-scale Reading Comprehension Dataset From Examinations", "reason": "This paper introduces the RACE dataset, one of the non-math datasets used in the experiments for measuring general language understanding abilities of the models after pruning or scaling parameters identified by MathNeuro.  Understanding the impact of the interventions on non-math tasks is crucial to evaluating MathNeuro's effectiveness.", "section_number": 4}, {" publication_date": "2021", "fullname_first_author": "Dan Hendrycks", "paper_title": "Measuring Massive Multitask Language Understanding", "reason": "This paper introduces the MMLU dataset, one of the non-math datasets used in the experiments to measure the impact of the interventions on the models' general knowledge.  The results in this dataset are directly compared with those in the math-specific GSM8K dataset to further assess the effectiveness of the proposed approach.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Michael Hanna", "paper_title": "How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model", "reason": "This paper provides insights into how LLMs process mathematical concepts, which is directly relevant to understanding how to identify parameters associated with mathematical reasoning.  The findings contribute to the theoretical understanding of the current work's objective to isolate math-specific parameters.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Daking Rai", "paper_title": "An Investigation of Neuron Activation as a Unified Lens to Explain Chain-of-Thought Eliciting Arithmetic Reasoning of LLMs", "reason": "This paper explores the use of neuron activation patterns to understand arithmetic reasoning in LLMs, which is directly related to the current research's goal of identifying parameters associated with mathematical reasoning abilities.  It provides a complementary perspective on understanding the inner workings of LLMs in mathematical contexts.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Alessandro Stolfo", "paper_title": "A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis", "reason": "This paper offers a mechanistic interpretation of arithmetic reasoning in language models, contributing to the theoretical background for the current research's focus on parameter identification.  Understanding the mechanisms underlying mathematical reasoning in LLMs is critical to the design and evaluation of methods for isolating relevant parameters.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Xavier Suau", "paper_title": "Whispering Experts: Neural Interventions for Toxicity Mitigation in Language Models", "reason": "This research is relevant to the current work due to its focus on targeted neural interventions in LLMs. While targeting toxicity mitigation, the principles and methodologies for such interventions are applicable to the current research's goal of improving mathematical performance through targeted parameter manipulation.  It offers an example of the type of targeted parameter intervention which the current work seeks to achieve for mathematical reasoning.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Yiran Zhao", "paper_title": "How do Large Language Models Handle Multilingualism?", "reason": "This work investigates multilingual capabilities in LLMs, providing insights into how different skills and abilities might be encoded within LLMs. Its findings are relevant to the current work's exploration of how to isolate domain-specific parameters like those related to math reasoning, which could be potentially intertwined with other language-related capabilities.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Alec Radford", "paper_title": "Learning to Generate Reviews and Discovering Sentiment", "reason": "This work is indirectly related to the current research. Its investigation into the internal representation of sentiment contributes to the broader understanding of knowledge representation within LLMs. This knowledge is pertinent to the current work\u2019s investigation into knowledge representation of math capabilities.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Sarkar Snigdha Sarathi Das", "paper_title": "Unified Low-Resource Sequence Labeling by Sample-Aware Dynamic Sparse Finetuning", "reason": "This paper addresses the challenge of fine-tuning large language models efficiently with limited data, which is relevant to the current study's use of single-sample evaluations for data efficiency. The methods employed in this paper are related to the efficient parameter optimization techniques used in the current research.", "section_number": 4}, {" publication_date": "2018", "fullname_first_author": "Anthony Bau", "paper_title": "Identifying and Controlling Important Neurons in Neural Machine Translation", "reason": "This paper provides foundational work related to identifying and manipulating important neurons in neural networks.  The methods and findings are relevant to the current work's objective of identifying and manipulating parameters associated with mathematical reasoning in LLMs. It provides a conceptual basis and precedent for targeting specific parameters for intervention.", "section_number": 2}]}