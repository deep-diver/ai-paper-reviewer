{"references": [{" publication_date": "2021", "fullname_first_author": "Karl Cobbe", "paper_title": "Training Verifiers to Solve Math Word Problems", "reason": "This paper introduces the GSM8K dataset, a benchmark dataset widely used in evaluating the mathematical reasoning capabilities of LLMs.  The GSM8K dataset is central to the experiments conducted in the paper, forming the basis for assessing the effectiveness of MathNeuro in both pruning and scaling scenarios. The quality and popularity of GSM8K makes this reference highly important.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Janice Ahn", "paper_title": "Large Language Models for Mathematical Reasoning: Progresses and Challenges", "reason": "This paper provides a valuable overview of the current state-of-the-art and challenges in LLMs' mathematical reasoning capabilities. It is important because it situates the current research in the broader field of LLM and mathematical reasoning, highlighting the novelty of the approach described in the target paper.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Rishabh Agarwal", "paper_title": "Many-Shot In-Context Learning", "reason": "This paper is relevant due to its discussion of in-context learning, which is a critical factor in evaluating the performance of LLMs on mathematical reasoning tasks.  The study's findings on in-context learning provide a relevant context for interpreting the results of the target paper's experiments.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Bradley Brown", "paper_title": "Large Language Monkeys: Scaling Inference Compute with Repeated Sampling", "reason": "This paper discusses scaling inference compute, which is directly relevant to the experiments evaluating the scalability and performance of LLMs across different sizes (1B to 8B parameters). It provides a relevant contextual understanding of the challenges faced by LLMs when performing mathematical reasoning tasks and supports the need for improved methods.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Ting-Yun Chang", "paper_title": "Do Localization Methods Actually Localize Memorized Data in LLMs?", "reason": "This paper critically examines the accuracy of existing localization methods. This reference provides valuable context by discussing the challenges and limitations of existing parameter identification techniques in LLMs, which justifies the need for the proposed MathNeuro method.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Damai Dai", "paper_title": "Knowledge Neurons in Pretrained Transformers", "reason": "This paper explores how knowledge is encoded in LLMs, providing a theoretical background related to the core concept of the present study: identifying and isolating task-specific parameters. The insights from this paper are critical for understanding the complex interactions between different parameters in large language models.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Abhishek Panigrahi", "paper_title": "Task-Specific Skill Localization in Fine-tuned Language Models", "reason": "This study focuses on task-specific skill localization in fine-tuned LLMs. This work is relevant as it provides a comparison point for MathNeuro's performance, particularly concerning its ability to accurately isolate task-specific parameters without relying on computationally intensive methods.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Tianyi Tang", "paper_title": "Language-Specific Neurons: The Key to Multilingual Capabilities in Large Language Models", "reason": "This paper presents LAPE, one of the methods used for comparison in the target paper.  It introduces an alternative approach to parameter importance analysis, which is crucial for establishing the novelty and effectiveness of the proposed method MathNeuro and showing improvements over previous work.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Mingjie Sun", "paper_title": "A Simple and Effective Pruning Approach for Large Language Models", "reason": "This paper introduces Wanda, one of the baselines in the experimental section of the target paper. Wanda is directly used by MathNeuro for parameter importance calculation, making this reference crucial for understanding MathNeuro's design and methodology. The comparison highlights the improvement provided by MathNeuro over the state-of-the-art method.", "section_number": 4}, {" publication_date": "2017", "fullname_first_author": "Guokun Lai", "paper_title": "RACE: Large-scale Reading Comprehension Dataset From Examinations", "reason": "This paper introduces the RACE dataset used as a benchmark for general language understanding in the experimental section. Comparing the model's performance on this dataset with math performance reveals the specificity of parameter interventions in MathNeuro.", "section_number": 4}, {" publication_date": "2021", "fullname_first_author": "Dan Hendrycks", "paper_title": "Measuring Massive Multitask Language Understanding", "reason": "This paper introduces the MMLU dataset, used to evaluate the model's performance in general language understanding.  Including MMLU allows for assessment of the impact of MathNeuro on general language tasks, demonstrating that the proposed method specifically targets mathematical abilities.", "section_number": 4}, {" publication_date": "2018", "fullname_first_author": "Anthony Bau", "paper_title": "Identifying and Controlling Important Neurons in Neural Machine Translation", "reason": "This paper explores the identification and control of neurons in neural machine translation (NMT), providing a foundational understanding of parameter manipulation techniques in neural networks. Its relevance lies in the foundational aspects of neural network parameter understanding and manipulation.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Michael Hanna", "paper_title": "How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model", "reason": "This paper investigates how GPT-2 performs mathematical computations, which is essential for understanding the mechanisms underlying math skills in LLMs. This provides critical background information and highlights the need for methods to analyze and isolate these skills, as done in the target paper.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Daking Rai", "paper_title": "An Investigation of Neuron Activation as a Unified Lens to Explain Chain-of-Thought Eliciting Arithmetic Reasoning of LLMs", "reason": "This paper explores the relationship between neuron activation and chain-of-thought reasoning for arithmetic tasks in LLMs, contributing to a deeper understanding of how mathematical reasoning is represented within LLMs. This understanding helps justify the need for a novel approach, like MathNeuro, to explicitly isolate math-specific parameters.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Alessandro Stolfo", "paper_title": "A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis", "reason": "This paper uses causal mediation analysis to understand arithmetic reasoning in LLMs, offering a mechanistic perspective on how this ability might be encoded. This is relevant as it provides insight into the complexities of mathematical reasoning in LLMs, supporting the rationale behind the methods proposed in the target paper.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Yun Luo", "paper_title": "An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning", "reason": "This paper examines catastrophic forgetting in LLMs.  The study of catastrophic forgetting helps understand potential challenges in parameter pruning and scaling.  This awareness is crucial for effectively applying MathNeuro and avoiding unintended consequences.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Yuanzhi Li", "paper_title": "Textbooks Are All You Need II: phi-1.5 technical report", "reason": "This paper describes Phi-1.5, one of the LLMs used in the experiments.  Understanding the architecture and training data of this specific LLM is crucial for interpreting the results obtained for Phi-1.5.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Qintong Li", "paper_title": "GSM-Plus: A Comprehensive Benchmark for Evaluating the Robustness of LLMs as Mathematical Problem Solvers", "reason": "This paper proposes GSM-Plus, a benchmark focusing on the robustness of LLMs in solving mathematical problems. While not directly used in the experiments, it offers a valuable contextual perspective, informing the choice of GSM8K as the primary evaluation dataset for assessing the model's mathematical capabilities.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Chen Li", "paper_title": "Common 7B Language Models Already Possess Strong Math Capabilities", "reason": "This paper examines the mathematical capabilities of language models.  It's important for understanding the baseline level of performance that MathNeuro is aiming to improve upon.  It helps to frame the significant improvements observed in the target paper within the larger context of LLM abilities.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Janice Ahn", "paper_title": "Large Language Models for Mathematical Reasoning: Progresses and Challenges", "reason": "This paper discusses advancements and obstacles in using LLMs for mathematical reasoning.  This overview of the existing research highlights the value and originality of the approach presented in the target paper.", "section_number": 1}]}