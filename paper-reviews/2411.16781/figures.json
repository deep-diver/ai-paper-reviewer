[{"figure_path": "https://arxiv.org/html/2411.16781/x2.png", "caption": "Figure 1: UniPose can handle pose comprehension, generation and editing tasks under different instructions within a unified framework.", "description": "This figure illustrates the unified multimodal framework of UniPose.  UniPose is designed to handle three main categories of human pose tasks: comprehension, generation, and editing.  The diagram shows examples of each task type, demonstrating how UniPose utilizes various input modalities (images, text, 3D poses) and outputs corresponding results.  It highlights the framework's ability to seamlessly transition between these task types with different instructions.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2411.16781/x3.png", "caption": "Figure 2: Method overview: UniPose comprises a Pose Tokenizer, Visual Processor and a pose-aware language LLM. Combining Pose Tokens learned by pose tokenizer, Visual Embeddings from visual processor and Text Tokens from text tokenizer, UniPose enables joint modeling of pose comprehension, generation and editing within a unified visual-language backbone.", "description": "UniPose is a unified multimodal framework for human pose that uses three main components: a pose tokenizer to convert 3D poses into discrete tokens, a visual processor to extract relevant features from images, and a pose-aware large language model (LLM) to integrate the information and perform the tasks.  The framework jointly models pose comprehension, generation, and editing by combining pose tokens, visual embeddings, and text tokens within a unified visual-language backbone.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2411.16781/x4.png", "caption": "(a) Pose-Text Alignment Pretraining Stage.", "description": "This figure illustrates the training process of the UniPose model's first stage, called \"Pose-Text Alignment Pretraining\".  In this stage, the model learns to align text and pose modalities by training on several tasks involving these modalities: Pose-to-Text, Pose-Diff, Text-to-Pose, and Pose Editing. The figure showcases the architecture of the model, showing how visual and textual inputs are processed by the model, highlighting the various components and their interactions. The illustration also likely shows the overall workflow of information flow within the model architecture during this specific training phase. ", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2411.16781/x5.png", "caption": "(b) Visual Projector Pretraining Stage.", "description": "This stage of the UniPose training process focuses on aligning image features with the unified text-pose embedding space.  It uses a combination of CLIP and a pose-specific vision transformer to extract visual features. The goal is to enable the large language model (LLM) to effectively understand and generate poses from image inputs, refining the visual representation to better match the textual information.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2411.16781/x6.png", "caption": "(c) Instruction Finetuning Stage.", "description": "This stage fine-tunes the visual processor and LLM using a multitask, multimodal instruction dataset.  The dataset contains 200 instruction templates per task, allowing the model to better understand and follow instructions for various pose-related tasks. This fine-tuning process improves the model's ability to handle a wider range of instructions and perform well on unseen tasks.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2411.16781/x7.png", "caption": "Figure 3: The training paradigm of UniPose.", "description": "This figure illustrates the three-stage training process for UniPose.  The first stage focuses on aligning pose and text modalities using a pose tokenizer and language model. The second stage pre-trains a visual processor to align visual and text embeddings. The third stage fine-tunes the entire model on various pose-related tasks using instructions, enhancing its instruction-following capabilities and unifying its understanding of pose across different modalities.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2411.16781/x8.png", "caption": "Table 5: Comparisons on pose editing task. Reconstruction metrics are reported on PoseFix [13] dataset.", "description": "Table 5 presents the results of the pose editing task.  The experiment evaluates UniPose's performance on the PoseFix dataset, a benchmark that focuses on editing 3D human poses based on textual instructions. Three key metrics assess the accuracy of the pose corrections: Mean Per Joint Position Error (MPJPE), which measures the average error in the position of each joint; PA-MPJPE (Pose-Aware MPJPE), a metric that is less sensitive to global pose differences; and Frechet Inception Distance (FID), which quantifies the similarity between the distribution of generated and ground-truth poses.  Lower values for MPJPE and PA-MPJPE, and a lower FID, indicate better performance.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2411.16781/x10.png", "caption": "Table 6: Ablation study on the components of the visual processor.", "description": "This ablation study investigates the impact of different visual processor components on the overall performance of the UniPose model. It compares the model's performance using different combinations of visual encoders (CLIP-ViT, Pose-ViT), analyzing their individual contributions to tasks such as pose estimation, image-to-text generation, and other relevant metrics. The results highlight the importance of specific encoders or their combination for achieving optimal performance on various downstream tasks.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2411.16781/x11.png", "caption": "(a) Comparison with Qwen-VL [4] and GPT-4o [1] on Image-to-Text task.", "description": "This figure shows a comparison of the image-to-text task performance between UniPose, Qwen-VL, and GPT-4.  It highlights the ability of UniPose to generate more detailed and accurate descriptions of human poses in images compared to the other two models. The examples illustrate UniPose's superior performance in capturing nuanced details about pose and body orientation.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2411.16781/x12.png", "caption": "(b) Comparison with GPT-4o [1] on Image-Diff task.", "description": "This figure shows a qualitative comparison of UniPose and GPT-4's performance on the Image-Diff task. Image-Diff involves generating a textual description of the differences between two human poses shown in a pair of images. The figure highlights the differences in the quality of descriptions generated by UniPose and GPT-4 by presenting an example where UniPose produced a more accurate and detailed description.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2411.16781/x13.png", "caption": "Figure 4: Examples on Image-to-Text and Image-Diff tasks. We mark incorrect captions in red and correct in green. UniPose can accurately perceive a person\u2019s orientation from images.", "description": "Figure 4 presents examples of UniPose's performance on image-based pose comprehension tasks, namely Image-to-Text and Image-Diff.  The Image-to-Text task involves generating a textual description of a human pose from a single image. The Image-Diff task focuses on generating a textual comparison of the differences between two human poses depicted in separate images.  For both tasks, UniPose's outputs are compared to those from other models, with incorrect captions highlighted in red and correct ones in green.  The figure visually demonstrates UniPose's superior ability to accurately capture and describe fine-grained details of human poses, particularly concerning body orientation, as evidenced by its more accurate captions.", "section": "4. Experiments"}]