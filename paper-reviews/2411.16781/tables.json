[{"content": "| Tasks | Pose Comprehension | Pose Comprehension | Pose Comprehension | Pose Comprehension | Pose Generation | Pose Generation | Pose Editing |\n|---|---|---|---|---|---|---|---| \n|  | Pose-to-Text | Image-to-Text | Pose-Diff | Image-Diff | Text-to-Pose | Pose Estimation |  |\n| Input\u2192Output | Pose\u2192Text | Image\u2192Text | Pose Pair\u2192Text | Image Pair\u2192Text | Text\u2192Pose | Image\u2192Pose | Pose&Text\u2192Pose |\n| HMR 2.0 [23] | \u2718 | \u2718 | \u2718 | \u2718 | \u2718 | \u2714 | \u2718 |\n| PoseScript [14] | \u2714 | \u2718 | \u2718 | \u2718 | \u2714 | \u2718 | \u2718 |\n| PoseFix [13] | \u2718 | \u2718 | \u2714 | \u2718 | \u2718 | \u2718 | \u2714 |\n| ChatPose [18] | \u2718 | \u2714 | \u2718 | \u2718 | \u2714 | \u2714 | \u2718 |\n| ChatHuman [41] | \u2718 | \u2714 | \u2718 | \u2718 | \u2714 | \u2714 | \u2718 |\n| UniPose (Ours) | \u2714 | \u2714 | \u2714 | \u2714 | \u2714 | \u2714 | \u2714 |", "caption": "Table 1: Comparison of recent methods across various pose comprehension, generation and editing tasks.", "description": "This table compares several recent methods in human pose estimation, focusing on their capabilities across three key tasks: pose comprehension, pose generation, and pose editing.  For each method, it indicates whether it supports various input modalities (such as images and text) for each task and shows the method's ability to perform different sub-tasks within each category (e.g., image-to-text, text-to-pose).  This allows for a direct comparison of the strengths and weaknesses of different approaches to human pose understanding and manipulation.", "section": "1. Introduction"}, {"content": "| Task | Dataset | Method | R-Precision \u2191 |  |  | Linguistic metrics \u2191 |  |  |  |\n|---|---|---|---|---|---|---|---|---|---|\n| Pose-to-Text | PoseScript [14] | PoseScript [14] | **91.6** | **95.6** | 97.0 | **12.9** | **33.9** | **34.2** |  |\n|  |  | UniPose \u2020 | 18.1 | 30.0 | 39.1 | 10.8 | 30.1 | 29.5 |  |\n|  |  | UniPose | 85.6 | 95.2 | **97.6** | 12.1 | 33.3 | 30.8 |  |\n| Pose-Diff | PoseFix [13] | PoseFix [13] | 64.6 | 77.1 | 83.0 | 12.0 | 33.5 | **36.7** |  |\n|  |  | UniPose \u2020 | 8.4 | 14.6 | 19.2 | 8.5 | 28.2 | 27.3 |  |\n|  |  | UniPose | **67.9** | **81.8** | **88.6** | **13.8** | **33.7** | 31.2 |  |\n| Image-to-Text | ImageScript | LLaVA [43] | 5.7 | 12.0 | 18.9 | 3.2 | 21.8 | 32.9 |  |\n|  |  | Qwen-VL [4] | 8.9 | 15.6 | 19.8 | 1.4 | 15.9 | 21.6 |  |\n|  |  | GPT4V [1] | 17.7 | 24.0 | 32.3 | 7.1 | 29.1 | 34.2 |  |\n|  |  | UniPose \u2020 | 22.4 | 32.8 | 41.2 | **18.2** | **42.4** | **45.2** |  |\n|  |  | UniPose | **24.5** | **35.4** | **43.2** | **18.2** | **42.5** | 44.7 |  |\n| Image-Diff | ImageDiff | GPT4V [1] | 7.3 | 13.5 | 18.8 | 1.3 | 16.1 | 21.8 |  |\n|  |  | UniPose \u2020 | 13.0 | 18.8 | 26.4 | 14.0 | 34.1 | 40.1 |  |\n|  |  | UniPose | **13.5** | **25.0** | **33.8** | **15.9** | **36.5** | **39.6** | ", "caption": "Table 2: Comparisons on pose comprehension tasks. We compare the pose-retrieval precision (R-Precision) and linguistic metrics on various datasets. UniPose \u2020\u2020\\dagger\u2020 represents training UniPose on the single corresponding task.", "description": "This table presents a comparison of different methods' performance on pose comprehension tasks.  It assesses both the accuracy of pose retrieval (R-Precision) and the quality of generated textual descriptions (linguistic metrics such as BLEU, ROUGE-L, and METEOR).  The comparison is done across various datasets (PoseScript, PoseFix, ImageScript, ImageDiff) to show the generalizability of the methods.  The UniPose\u2020 results show performance when the UniPose model is trained only on the specific task being evaluated, highlighting its performance when trained specifically for each task compared to its performance when trained on multiple tasks.", "section": "4. Experiments"}, {"content": "| Method | R<sup>T2P</sup>\u2191 Top-5 | R<sup>T2P</sup>\u2191 Top-10 | R<sup>T2P</sup>\u2191 Top-20 | R<sup>P2T</sup>\u2191 Top-5 | R<sup>P2T</sup>\u2191 Top-10 | R<sup>P2T</sup>\u2191 Top-20 | Pose Reconstruction Metric \u2193 MPJPE | Pose Reconstruction Metric \u2193 PA-MPJPE | Pose Reconstruction Metric \u2193 FID |\n|---|---|---|---|---|---|---|---|---|---|\n| PoseScript [14] | 73.3 | **82.5** | 89.4 | 70.0 | **82.5** | 87.4 | 318.0 | **161.3** | 0.075 |\n| ChatPose [18] | 17.6 | 25.3 | 35.8 | 28.0 | 39.0 | 54.4 | - | - | - |\n| ChatHuman [41] | 41.8 | 52.6 | 65.1 | 42.1 | 52.3 | 66.5 | - | - | - |\n| UniPose \u2020 | 67.5 | 77.6 | 85.5 | 62.8 | 74.8 | 83.6 | 342.7 | 190.0 | 0.046 |\n| UniPose | **73.7** | 82.4 | **89.6** | **70.9** | 80.5 | **89.6** | **308.6** | 171.1 | **0.038** |", "caption": "Table 3: Comparisons on Text-to-Pose generation task. The retrieval and reconstruction metrics are reported on PoseScript [14] dataset.", "description": "This table presents the results of the Text-to-Pose generation task, a key component of the UniPose model.  It compares UniPose's performance against other state-of-the-art methods on the PoseScript dataset. The evaluation metrics include retrieval metrics (Top-5, Top-10, Top-20 for both RT2P and RP2T, reflecting the accuracy of pose retrieval based on text descriptions) and reconstruction metrics (MPJPE, PA-MPJPE, and FID, measuring the difference between generated and ground-truth 3D poses).  The table helps assess UniPose's ability to accurately generate human poses from textual descriptions.", "section": "4. Experiments"}, {"content": "| Method | 3DPW [61] \u2193 |  | H36M [29] \u2193 |  | \n|---|---|---|---|---| \n| MPJPE | PA-MPJPE | MPJPE | PA-MPJPE |  | \n| HMR [33] | 130.0 | 76.7 | 88.0 | 56.8 | \n| PyMAF [75] | 92.8 | 58.9 | 57.7 | 40.5 | \n| SMPLer [70] | 73.7 | 43.4 | 45.2 | **32.4** | \n| HMR2.0 [23] | 70.0 | 44.5 | **44.8** | 33.6 | \n| Zolly [64] | 76.2 | 47.9 | 49.4 | 32.3 | \n| MEGA [20] | **67.5** | **41.0** | - | - | \n| TokenHMR [15] | 71.0 | 44.3 | - | - | \n| ChatPose [18] | 163.6 | 81.9 | 126.0 | 82.4 | \n| UniPose \u2020 | 97.4 | 61.2 | **65.8** | **39.4** | \n| UniPose | **94.7** | **59.1** | 69.2 | 41.8 |", "caption": "Table 4: Comparisons on pose estimation task. Reconstruction metrics are reported on the 3DPW and Human3.6M datasets.", "description": "Table 4 presents a comparison of different methods for the human pose estimation task.  The table shows the mean per-joint position error (MPJPE) and the Procrustes aligned MPJPE (PA-MPJPE), which are reconstruction error metrics, for several methods on two widely used datasets: 3DPW and Human3.6M. Lower values indicate better performance in accurately estimating 3D human poses from images.", "section": "4. Experiments"}, {"content": "| Method | MPJPE \u2193 | PA-MPJPE \u2193 | FID \u2193 |\n|---|---|---|---|\n| PoseFix [13] | 300.2 | 144.1 | 0.019 |\n| UniPose \u2020 | 310.8 | 157.0 | 0.019 |\n| UniPose | **270.3** | **138.9** | **0.015** |", "caption": "Table 7: Ablation study on different attention mechanisms.", "description": "This table presents an ablation study comparing the performance of different attention mechanisms in the UniPose model. Specifically, it contrasts the use of causal attention versus mixed attention (a combination of causal and bidirectional attention).  The study evaluates these approaches across two key tasks: Text-to-Pose (generating a 3D pose from a textual description) and Pose-to-Text (generating a textual description from a 3D pose). The results are assessed using standard metrics for both tasks, including retrieval performance (RT2P and RP2T) and linguistic quality (BLEU-4, ROUGE-L, and METEOR).  The table also includes the inference latency (time taken to perform the task) for each method.  This analysis helps determine which attention mechanism is more effective for each task in UniPose.", "section": "3.2 Pose-aware Vision-Language Model"}, {"content": "| CLIP-ViT | Pose-ViT | Pose Estimation \u2193 | Image-to-Text \u2191 |  |  |  |\n|---|---|---|---|---|---|---|\n|  |  | MPJPE | PA-MPJPE | BLEU-4 | ROUGE-L | METEOR |\n| \u2714 | \u2718 | 193.4 | 86.1 | 11.1 | 30.2 | 33.9 |\n| \u2714 | \u2714 | **96.1** | **58.9** | **13.3** | **31.7** | **35.2** |", "caption": "Table 1: Detailed datasets for training UniPose. The PoseScript dataset provides human annotations (PoseScript-H) and expands its dataset with automated captions (PoseScript-A), as does the PoseFix dataset.", "description": "This table details the datasets used for training the UniPose model.  It breaks down the data used in each training stage: Pose-Text Alignment Pretraining, Visual Projector Pretraining, and Instruction Finetuning.  For each stage, it lists the specific datasets used (PoseScript-A, PoseFix-A, ImageScript-A, ImageDiff-A, PoseEst, PoseScript-H, PoseFix-H, ImageScript-R, ImageDiff-R), along with the number of samples in each.  The PoseScript and PoseFix datasets are further categorized into human-annotated (PoseScript-H and PoseFix-H) and automatically captioned (PoseScript-A and PoseFix-A) subsets.  The table clarifies the composition of the training data to provide a complete view of how the model was trained.", "section": "3.3 Training and Inference Paradigm"}, {"content": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S4.T7.6\">\n<tr class=\"ltx_tr\" id=\"S4.T7.6.7\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" id=\"S4.T7.6.7.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T7.6.7.1.1\" style=\"font-size:80%;\">Attention Type</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"7\" id=\"S4.T7.6.7.2\"><span class=\"ltx_text\" id=\"S4.T7.6.7.2.1\" style=\"font-size:80%;\">Text-to-Pose</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\" id=\"S4.T7.6.7.3\"><span class=\"ltx_text\" id=\"S4.T7.6.7.3.1\" style=\"font-size:80%;\">Pose-to-Text</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T7.6.6\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"3\" id=\"S4.T7.1.1.1\"><math alttext=\"\\mathrm{R}^{\\mathrm{T2P}}\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T7.1.1.1.m1.1\"><semantics id=\"S4.T7.1.1.1.m1.1a\"><mrow id=\"S4.T7.1.1.1.m1.1.1\" xref=\"S4.T7.1.1.1.m1.1.1.cmml\"><msup id=\"S4.T7.1.1.1.m1.1.1.2\" xref=\"S4.T7.1.1.1.m1.1.1.2.cmml\"><mi id=\"S4.T7.1.1.1.m1.1.1.2.2\" mathsize=\"80%\" mathvariant=\"normal\" xref=\"S4.T7.1.1.1.m1.1.1.2.2.cmml\">R</mi><mi id=\"S4.T7.1.1.1.m1.1.1.2.3\" mathsize=\"80%\" xref=\"S4.T7.1.1.1.m1.1.1.2.3.cmml\">T2P</mi></msup><mo id=\"S4.T7.1.1.1.m1.1.1.1\" mathsize=\"80%\" stretchy=\"false\" xref=\"S4.T7.1.1.1.m1.1.1.1.cmml\">\u2191</mo><mi id=\"S4.T7.1.1.1.m1.1.1.3\" xref=\"S4.T7.1.1.1.m1.1.1.3.cmml\"></mi></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.T7.1.1.1.m1.1b\"><apply id=\"S4.T7.1.1.1.m1.1.1.cmml\" xref=\"S4.T7.1.1.1.m1.1.1\"><ci id=\"S4.T7.1.1.1.m1.1.1.1.cmml\" xref=\"S4.T7.1.1.1.m1.1.1.1\">\u2191</ci><apply id=\"S4.T7.1.1.1.m1.1.1.2.cmml\" xref=\"S4.T7.1.1.1.m1.1.1.2\"><csymbol cd=\"ambiguous\" id=\"S4.T7.1.1.1.m1.1.1.2.1.cmml\" xref=\"S4.T7.1.1.1.m1.1.1.2\">superscript</csymbol><ci id=\"S4.T7.1.1.1.m1.1.1.2.2.cmml\" xref=\"S4.T7.1.1.1.m1.1.1.2.2\">R</ci><ci id=\"S4.T7.1.1.1.m1.1.1.2.3.cmml\" xref=\"S4.T7.1.1.1.m1.1.1.2.3\">T2P</ci></apply><csymbol cd=\"latexml\" id=\"S4.T7.1.1.1.m1.1.1.3.cmml\" xref=\"S4.T7.1.1.1.m1.1.1.3\">absent</csymbol></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T7.1.1.1.m1.1c\">\\mathrm{R}^{\\mathrm{T2P}}\\uparrow</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.T7.1.1.1.m1.1d\">roman_R start_POSTSUPERSCRIPT T2P end_POSTSUPERSCRIPT \u2191</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" colspan=\"3\" id=\"S4.T7.2.2.2\"><math alttext=\"\\mathrm{R}^{\\mathrm{P2T}}\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T7.2.2.2.m1.1\"><semantics id=\"S4.T7.2.2.2.m1.1a\"><mrow id=\"S4.T7.2.2.2.m1.1.1\" xref=\"S4.T7.2.2.2.m1.1.1.cmml\"><msup id=\"S4.T7.2.2.2.m1.1.1.2\" xref=\"S4.T7.2.2.2.m1.1.1.2.cmml\"><mi id=\"S4.T7.2.2.2.m1.1.1.2.2\" mathsize=\"80%\" mathvariant=\"normal\" xref=\"S4.T7.2.2.2.m1.1.1.2.2.cmml\">R</mi><mi id=\"S4.T7.2.2.2.m1.1.1.2.3\" mathsize=\"80%\" xref=\"S4.T7.2.2.2.m1.1.1.2.3.cmml\">P2T</mi></msup><mo id=\"S4.T7.2.2.2.m1.1.1.1\" mathsize=\"80%\" stretchy=\"false\" xref=\"S4.T7.2.2.2.m1.1.1.1.cmml\">\u2191</mo><mi id=\"S4.T7.2.2.2.m1.1.1.3\" xref=\"S4.T7.2.2.2.m1.1.1.3.cmml\"></mi></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.T7.2.2.2.m1.1b\"><apply id=\"S4.T7.2.2.2.m1.1.1.cmml\" xref=\"S4.T7.2.2.2.m1.1.1\"><ci id=\"S4.T7.2.2.2.m1.1.1.1.cmml\" xref=\"S4.T7.2.2.2.m1.1.1.1\">\u2191</ci><apply id=\"S4.T7.2.2.2.m1.1.1.2.cmml\" xref=\"S4.T7.2.2.2.m1.1.1.2\"><csymbol cd=\"ambiguous\" id=\"S4.T7.2.2.2.m1.1.1.2.1.cmml\" xref=\"S4.T7.2.2.2.m1.1.1.2\">superscript</csymbol><ci id=\"S4.T7.2.2.2.m1.1.1.2.2.cmml\" xref=\"S4.T7.2.2.2.m1.1.1.2.2\">R</ci><ci id=\"S4.T7.2.2.2.m1.1.1.2.3.cmml\" xref=\"S4.T7.2.2.2.m1.1.1.2.3\">P2T</ci></apply><csymbol cd=\"latexml\" id=\"S4.T7.2.2.2.m1.1.1.3.cmml\" xref=\"S4.T7.2.2.2.m1.1.1.3\">absent</csymbol></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T7.2.2.2.m1.1c\">\\mathrm{R}^{\\mathrm{P2T}}\\uparrow</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.T7.2.2.2.m1.1d\">roman_R start_POSTSUPERSCRIPT P2T end_POSTSUPERSCRIPT \u2191</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T7.3.3.3\">\n<span class=\"ltx_text\" id=\"S4.T7.3.3.3.1\" style=\"font-size:80%;\">Latency (s) </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T7.3.3.3.m1.1\"><semantics id=\"S4.T7.3.3.3.m1.1a\"><mo id=\"S4.T7.3.3.3.m1.1.1\" mathsize=\"80%\" stretchy=\"false\" xref=\"S4.T7.3.3.3.m1.1.1.cmml\">\u2193</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T7.3.3.3.m1.1b\"><ci id=\"S4.T7.3.3.3.m1.1.1.cmml\" xref=\"S4.T7.3.3.3.m1.1.1\">\u2193</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T7.3.3.3.m1.1c\">\\downarrow</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.T7.3.3.3.m1.1d\">\u2193</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T7.4.4.4\">\n<span class=\"ltx_text\" id=\"S4.T7.4.4.4.1\" style=\"font-size:80%;\">BLEU-4 </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T7.4.4.4.m1.1\"><semantics id=\"S4.T7.4.4.4.m1.1a\"><mo id=\"S4.T7.4.4.4.m1.1.1\" mathsize=\"80%\" stretchy=\"false\" xref=\"S4.T7.4.4.4.m1.1.1.cmml\">\u2191</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T7.4.4.4.m1.1b\"><ci id=\"S4.T7.4.4.4.m1.1.1.cmml\" xref=\"S4.T7.4.4.4.m1.1.1\">\u2191</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T7.4.4.4.m1.1c\">\\uparrow</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.T7.4.4.4.m1.1d\">\u2191</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T7.5.5.5\">\n<span class=\"ltx_text\" id=\"S4.T7.5.5.5.1\" style=\"font-size:80%;\">ROUGE-L </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T7.5.5.5.m1.1\"><semantics id=\"S4.T7.5.5.5.m1.1a\"><mo id=\"S4.T7.5.5.5.m1.1.1\" mathsize=\"80%\" stretchy=\"false\" xref=\"S4.T7.5.5.5.m1.1.1.cmml\">\u2191</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T7.5.5.5.m1.1b\"><ci id=\"S4.T7.5.5.5.m1.1.1.cmml\" xref=\"S4.T7.5.5.5.m1.1.1\">\u2191</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T7.5.5.5.m1.1c\">\\uparrow</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.T7.5.5.5.m1.1d\">\u2191</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T7.6.6.6\">\n<span class=\"ltx_text\" id=\"S4.T7.6.6.6.1\" style=\"font-size:80%;\">METEOR </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T7.6.6.6.m1.1\"><semantics id=\"S4.T7.6.6.6.m1.1a\"><mo id=\"S4.T7.6.6.6.m1.1.1\" mathsize=\"80%\" stretchy=\"false\" xref=\"S4.T7.6.6.6.m1.1.1.cmml\">\u2191</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T7.6.6.6.m1.1b\"><ci id=\"S4.T7.6.6.6.m1.1.1.cmml\" xref=\"S4.T7.6.6.6.m1.1.1\">\u2191</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T7.6.6.6.m1.1c\">\\uparrow</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.T7.6.6.6.m1.1d\">\u2191</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T7.6.8\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S4.T7.6.8.1\"><span class=\"ltx_text\" id=\"S4.T7.6.8.1.1\" style=\"font-size:80%;\">Causal Attention</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T7.6.8.2\"><span class=\"ltx_text\" id=\"S4.T7.6.8.2.1\" style=\"font-size:80%;\">9.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T7.6.8.3\"><span class=\"ltx_text\" id=\"S4.T7.6.8.3.1\" style=\"font-size:80%;\">14.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T7.6.8.4\"><span class=\"ltx_text\" id=\"S4.T7.6.8.4.1\" style=\"font-size:80%;\">20.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T7.6.8.5\"><span class=\"ltx_text\" id=\"S4.T7.6.8.5.1\" style=\"font-size:80%;\">9.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T7.6.8.6\"><span class=\"ltx_text\" id=\"S4.T7.6.8.6.1\" style=\"font-size:80%;\">14.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T7.6.8.7\"><span class=\"ltx_text\" id=\"S4.T7.6.8.7.1\" style=\"font-size:80%;\">22.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T7.6.8.8\"><span class=\"ltx_text\" id=\"S4.T7.6.8.8.1\" style=\"font-size:80%;\">2.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T7.6.8.9\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T7.6.8.9.1\" style=\"font-size:80%;\">26.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T7.6.8.10\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T7.6.8.10.1\" style=\"font-size:80%;\">39.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T7.6.8.11\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T7.6.8.11.1\" style=\"font-size:80%;\">38.0</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T7.6.9\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\" id=\"S4.T7.6.9.1\"><span class=\"ltx_text\" id=\"S4.T7.6.9.1.1\" style=\"font-size:80%;\">Mixed Attention</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T7.6.9.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T7.6.9.2.1\" style=\"font-size:80%;\">13.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T7.6.9.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T7.6.9.3.1\" style=\"font-size:80%;\">20.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T7.6.9.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T7.6.9.4.1\" style=\"font-size:80%;\">28.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T7.6.9.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T7.6.9.5.1\" style=\"font-size:80%;\">15.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T7.6.9.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T7.6.9.6.1\" style=\"font-size:80%;\">23.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T7.6.9.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T7.6.9.7.1\" style=\"font-size:80%;\">32.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T7.6.9.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T7.6.9.8.1\" style=\"font-size:80%;\">0.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T7.6.9.9\"><span class=\"ltx_text\" id=\"S4.T7.6.9.9.1\" style=\"font-size:80%;\">25.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T7.6.9.10\"><span class=\"ltx_text\" id=\"S4.T7.6.9.10.1\" style=\"font-size:80%;\">39.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T7.6.9.11\"><span class=\"ltx_text\" id=\"S4.T7.6.9.11.1\" style=\"font-size:80%;\">36.7</span></td>\n</tr>\n</table>", "caption": "Table 2: Examples of instruction templates utilized during the instruction finetuning stage of UniPose training.", "description": "This table lists example instruction templates used in the instruction finetuning stage of UniPose training.  It shows various tasks (Pose Comprehension, Pose Generation, Pose Editing), sub-tasks within each category, example input types (image, text, SMPL pose), and the expected output.  The instructions demonstrate the diverse range of human pose-related queries UniPose can handle.", "section": "3.3 Training and Inference Paradigm"}, {"content": "| Training paradigm | Task | Dataset | Samples |\n|---|---|---|---| \n| Pose-Text Align <br> Pretraining | Pose-to-Text, Pose-Diff, <br> Text-to-Pose, Pose-Edit | PoseScript-A | 70k |\n|  | PoseFix-A | 93k |\n| Visual Projector <br> Pretraining | Image-to-Text, <br> Image-Diff, <br> Pose Estimation | ImageScript-A | 50k |\n|  | ImageDiff-A | 50k |\n|  | PoseEst | 100k |\n| Instruction <br> Finetuning | All tasks | PoseScript-H | 5k |\n|  |  | PoseFix-H | 5k |\n|  |  | ImageScript-R | 6k |\n|  |  | ImageDiff-R | 6k |\n|  |  | PoseEst | 6k |", "caption": "Table 3: \nTraining hyperparameters of UniPose. Image Res denotes the input image resolution of CLIP-ViT and Pose-ViT, and the same as Patch Size.", "description": "This table details the hyperparameters used during the training of the UniPose model.  It specifies settings for three training stages: Pose-Text Alignment Pretraining, Visual Projector Pretraining, and Instruction Finetuning.  For each stage, the table lists the batch size, learning rate, number of epochs, image resolution (which is the same for both CLIP-ViT and Pose-ViT encoders), patch size, warmup epoch number, learning rate schedule type, and optimizer used.  Understanding these hyperparameters is crucial for comprehending the training process and its impact on the UniPose model's performance.", "section": "3. Method"}, {"content": "| Task | Sub-Task | Input | Output |\n|---|---|---|---| \n| Pose <br> Comp | Pose-to-Text | Generate a description of the SMPL pose: &lt;pose&gt;. | &lt;caption&gt; |\n|  |  | Interpret the SMPL pose in &lt;pose&gt; and generate a written description. |  |\n|  | Pose-Diff | Provide a summary of how SMPL pose &lt;pose&gt; differs from &lt;pose&gt;. |  |\n|  |  | Detail any SMPL pose changes seen between &lt;pose&gt; and &lt;pose&gt;. |  |\n|  | Image-to-Text | Describe the pose of the individual in the &lt;image&gt;. |  |\n|  |  | Analyze &lt;image&gt; and describe the posture displayed. |  |\n|  | Image-Diff | Compare &lt;image&gt; and &lt;image&gt;, outline how the person\u2019s posture differs. |  |\n|  |  | Identify how the individual\u2019s pose varies from &lt;image&gt; to &lt;image&gt;. |  |\n| Pose <br> Gen | Pose Estimation | Could you estimate the SMPL pose of the individual in &lt;image&gt;? | &lt;pose&gt; |\n|  |  | Look at the &lt;image&gt; and return the SMPL pose parameters for the figure shown. |  |\n|  | Text-to-Pose | Could you generate the SMPL pose from the description: &lt;caption&gt;? |  |\n|  |  | Using the description &lt;caption&gt;, please create the corresponding SMPL pose. |  |\n|  | Pose Editing | Modify &lt;pose&gt; based on this instruction: &lt;caption&gt;. |  |\n|  |  | Refine &lt;pose&gt; by applying the description provided: &lt;caption&gt;. |  |", "caption": "Table 4: The retrieval results on the PoseScript [14] and PoseFix [13] datasets. We report Top 1 / 5 / 10 RP\u20622\u2062Tsuperscript\ud835\udc45\ud835\udc432\ud835\udc47R^{P2T}italic_R start_POSTSUPERSCRIPT italic_P 2 italic_T end_POSTSUPERSCRIPT and RT\u20622\u2062Psuperscript\ud835\udc45\ud835\udc472\ud835\udc43R^{T2P}italic_R start_POSTSUPERSCRIPT italic_T 2 italic_P end_POSTSUPERSCRIPT,\nalong with the mean recall (mRecall), which is the average of all retrieval recall values.", "description": "Table 4 presents a detailed comparison of retrieval performance on the PoseScript and PoseFix datasets.  It assesses the accuracy of retrieving poses given text descriptions (RP2T) and retrieving text descriptions given poses (RT2P). The evaluation includes top-1, top-5, and top-10 results for both tasks, providing a comprehensive view of retrieval precision at different ranking levels.  Additionally, the table reports the mean recall (mRecall), which averages the recall across all retrieval ranks, offering a single measure of overall retrieval effectiveness.", "section": "4. Experiments"}, {"content": "| Configuration | Pose-Text Align\nPretraining | Visual Projector\nPretraining | Instruction\nFinetuning |\n|---|---|---|---|\n| Batch Size | 24 | 8 | 8 |\n| Learning Rate | 1.5e-4 | 5e-5 | 5e-5 |\n| Epochs | 6 | 2 | 2 |\n| Image Res | 336 \u00d7 336 / 256 \u00d7 256 |  |  | \n| Patch Size | 14 \u00d7 14 / 16 \u00d7 16 |  |  | \n| Warmup Epochs | 0.03 |  |  | \n| LR Schedule | Cosine |  |  | \n| Optimizer | AdamW |  |  |", "caption": "Table 5: Ablation on global orientation noise for the Pose Tokenizer.", "description": "This table presents an ablation study on the effect of adding random noise to the global orientation during the training of the Pose Tokenizer. It shows the impact of this noise on the performance of the tokenizer, measured by MPJPE and PA-MPJPE metrics on the AMASS and MOYO datasets.  The results demonstrate how introducing noise improves the robustness of the tokenizer, particularly on the MOYO dataset.", "section": "B.3 UniPose"}]