[{"figure_path": "2410.17637/tables/table_8_0.html", "caption": "Table 1: Main results on multi-image benchmarks. We compare our MIA-DPO along with other DPO algorithms across five multi-image benchmarks. Our method brings significant performance improvements to both the classic LLaVa-v1.5 and the recent InternLM-XC2.5. In contrast, other single-image DPO methods perform poorly on multi-image benchmarks.", "description": "Table 1 compares the performance of MIA-DPO and other preference optimization methods across five multi-image benchmarks, highlighting MIA-DPO's superior performance on both LLaVa-v1.5 and InternLM-XC2.5.", "section": "4.2 Results on Multi-Images Benchmarks"}, {"figure_path": "2410.17637/tables/table_9_0.html", "caption": "Table 2: Main results on single-image benchmarks. We compare MIA-DPO with other DPO approaches across seven single-image benchmarks. MIA-DPO, which not only enhances multi-image performance but also maintains strong proficiency in single-image tasks.", "description": "Table 2 compares MIA-DPO's performance on seven single-image benchmarks against other direct preference optimization methods, showing its ability to maintain strong single-image performance while improving multi-image results.", "section": "4.3 RESULTS ON SINGLE-IMAGES BENCHMARKS"}, {"figure_path": "2410.17637/tables/table_10_0.html", "caption": "Table 3: Ablation Studies. The top row refers to the LLaVa-v1.5 baseline. We conduct experiments about the impact of without (w/o) and with (w) post-selection techniques and dpo data types.", "description": "Table 3 presents ablation study results comparing MIA-DPO with and without post-selection and different data types, showing the impact of each component on the overall performance.", "section": "4.4 ABLATION STUDIES"}, {"figure_path": "2410.17637/tables/table_16_0.html", "caption": "Table 1: Main results on multi-image benchmarks. We compare our MIA-DPO along with other DPO algorithms across five multi-image benchmarks. Our method brings significant performance improvements to both the classic LLaVa-v1.5 and the recent InternLM-XC2.5. In contrast, other single-image DPO methods perform poorly on multi-image benchmarks.", "description": "Table 1 compares the performance of MIA-DPO and other DPO methods across five multi-image benchmarks, showing MIA-DPO's significant performance improvements on LLaVa-v1.5 and InternLM-XC2.5.", "section": "4.2 RESULTS ON MULTI-IMAGES BENCHMARKS"}, {"figure_path": "2410.17637/tables/table_16_1.html", "caption": "Table 5: Ablation Studies. The top row refers to the LLaVa-v1.5 baseline. We conducted an ablation study using GPT-40-mini for data selection.", "description": "The table compares the performance of using GPT-40-mini for data selection against MIA-DPO across five multi-image benchmarks.", "section": "4.3 Results on Single-Images Benchmarks"}, {"figure_path": "2410.17637/tables/table_17_0.html", "caption": "Table 1: Main results on multi-image benchmarks. We compare our MIA-DPO along with other DPO algorithms across five multi-image benchmarks. Our method brings significant performance improvements to both the classic LLaVa-v1.5 and the recent InternLM-XC2.5. In contrast, other single-image DPO methods perform poorly on multi-image benchmarks.", "description": "Table 1 compares the performance of MIA-DPO and other DPO algorithms across five multi-image benchmarks, showing MIA-DPO's significant performance improvements over existing methods.", "section": "4.2 RESULTS ON MULTI-IMAGES BENCHMARKS"}, {"figure_path": "2410.17637/tables/table_17_1.html", "caption": "Table 1: Main results on multi-image benchmarks. We compare our MIA-DPO along with other DPO algorithms across five multi-image benchmarks. Our method brings significant performance improvements to both the classic LLaVa-v1.5 and the recent InternLM-XC2.5. In contrast, other single-image DPO methods perform poorly on multi-image benchmarks.", "description": "This table compares the performance of MIA-DPO and other direct preference optimization methods across five multi-image benchmarks, showing MIA-DPO's superior performance.", "section": "4.2 RESULTS ON MULTI-IMAGES BENCHMARKS"}, {"figure_path": "2410.17637/tables/table_18_0.html", "caption": "Table 8: DPO Data Statistic. We listed in the table the data volume used for DPO with LLaVa-v1.5 and InternLM-XC2d5, along with the proportion of each type of data.", "description": "The table presents the data volume used for direct preference optimization (DPO) with two large vision-language models, LLaVa-v1.5 and InternLM-XC2.5, categorized by data type (Sequence, Grid Collage, Pic-in-Pic).", "section": "4 Experiments"}]