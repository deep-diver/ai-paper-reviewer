{"references": [{" publication_date": "2023", "fullname_first_author": "Josh Achiam", "paper_title": "GPT-4 technical report", "reason": "This paper is highly relevant as it introduces GPT-4, a state-of-the-art proprietary model that excels in handling multi-image contexts, serving as a benchmark for open-source models like the one presented in the current paper.  Understanding GPT-4's capabilities provides context for the challenges faced by open-source LVLMs and the need for improved multi-image understanding.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "reason": "This is a seminal paper in reinforcement learning from human feedback (RLHF), a crucial technique for aligning language models with human preferences.  The paper's methodology and findings are directly relevant to the preference alignment aspect of LVLMs, informing the approach used in the current research.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Yuntao Bai", "paper_title": "Constitutional AI: Harmlessness from ai feedback", "reason": "This paper introduces Reinforcement Learning from AI Feedback (RLAIF), an alternative to RLHF that uses AI feedback for alignment.  It offers an important comparative approach to RLHF, particularly relevant given the discussion of preference alignment in LVLMs.", "section_number": 1}, {" publication_date": "2023a", "fullname_first_author": "Anas Awadalla", "paper_title": "OpenFlamingo: An open-source framework for training large autoregressive vision-language models", "reason": "This paper is highly relevant because it presents OpenFlamingo, a significant open-source LVLMs.  The paper details its architecture and training process, providing a comparison point for the current work's focus on improving open-source LVLMs' multi-image understanding.", "section_number": 1}, {" publication_date": "2024a", "fullname_first_author": "Anas Awadalla", "paper_title": "MINT-1T: Scaling open-source multimodal data by 10x: A multimodal dataset with one trillion tokens", "reason": "This paper introduces a massive multimodal dataset (MINT-1T), which is highly relevant because it addresses the scarcity of diverse training data, a significant challenge highlighted in the introduction. The scale and diversity of MINT-1T provide context for the data augmentation strategies employed in the proposed method.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Jinze Bai", "paper_title": "Qwen-VL: A frontier large vision-language model with versatile abilities", "reason": "This paper introduces Qwen-VL, a large vision-language model, and its performance serves as a benchmark against which the proposed method's performance can be evaluated and compared.  Qwen-VL's capabilities provide further context for the limitations of current open-source LVLMs and the need for improvement in multi-image understanding.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Quan Sun", "paper_title": "Generative multimodal models are in-context learners", "reason": "This paper focuses on preference alignment in visual domains, using RLHF and DPO. Its approach and results offer a valuable comparison point for the proposed method's techniques and efficiency in visual preference alignment. ", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Rafael Rafailov", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "reason": "This paper introduces Direct Preference Optimization (DPO), a core technique used in the proposed MIA-DPO. Understanding DPO's principles and applications is essential for evaluating the innovation and contribution of MIA-DPO.", "section_number": 3}, {" publication_date": "2017", "fullname_first_author": "John Schulman", "paper_title": "Proximal policy optimization algorithms", "reason": "This is a fundamental paper in reinforcement learning, introducing the Proximal Policy Optimization (PPO) algorithm.  PPO is closely related to DPO, which is used in the proposed method, and understanding PPO's foundation is key to evaluating the related DPO aspects of this research.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Zhiyuan Zhao", "paper_title": "Beyond hallucinations: Enhancing lvlms through hallucination-aware direct preference optimization", "reason": "This paper addresses the issue of hallucinations in LVLMs, which is central to the proposed MIA-DPO's approach. The strategies and solutions presented in this paper provide relevant context for understanding the challenges and contributions in the proposed work.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Yiyang Zhou", "paper_title": "Aligning modalities in vision large language models via preference fine-tuning", "reason": "This paper focuses on preference fine-tuning in vision-language models, a closely related approach to the preference optimization techniques employed in the proposed method. The insights and findings from this research are important for understanding the context and contributions of the proposed work.", "section_number": 2}, {" publication_date": "2024a", "fullname_first_author": "Haotian Liu", "paper_title": "Improved baselines with visual instruction tuning", "reason": "This paper focuses on instruction tuning for visual language models, a related area to the proposed method. Its findings and approach provide valuable context for comparing different methodologies in enhancing the capabilities of LVLMs in handling visual instructions.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Dongfu Jiang", "paper_title": "MANTIS: Interleaved multi-image instruction tuning", "reason": "This paper is highly relevant because it directly addresses the challenge of handling multi-image data in LVLMs, which is the core focus of the proposed method.  The techniques and findings presented in MANTIS offer a direct comparison point for the innovations presented in this research.", "section_number": 2}, {" publication_date": "2024b", "fullname_first_author": "Hugo Lauren\u00e7on", "paper_title": "What matters when building vision-language models?", "reason": "This paper provides a valuable discussion of factors influencing vision-language model performance. Understanding these factors is key for evaluating the effectiveness and impact of the proposed MIA-DPO method.", "section_number": 2}, {" publication_date": "2024b", "fullname_first_author": "Hugo Lauren\u00e7on", "paper_title": "OBELICS: An open web-scale filtered dataset of interleaved image-text documents", "reason": "This paper introduces a large-scale dataset (OBELICS) relevant to the research on multi-image LVLMs. The size and nature of OBELICS provide further context for the challenges highlighted in the introduction, particularly regarding data scarcity and annotation costs.", "section_number": 2}, {" publication_date": "2024a", "fullname_first_author": "Feng Li", "paper_title": "LLaVA-NeXT-Interleave: Tackling multi-image, video, and 3d in large multimodal models", "reason": "This paper focuses on improving the ability of LVLMs to handle multi-image data, a key area addressed by the proposed MIA-DPO. The comparison with LLaVA-NeXT-Interleave provides context for the contributions of the proposed method.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Junnan Li", "paper_title": "BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models", "reason": "This paper discusses pre-training techniques for vision-language models, providing valuable context for the broader context of LVLMs and their development.  Understanding different pre-training methodologies is key to evaluating the impact of the data augmentation strategies employed in the proposed MIA-DPO.", "section_number": 2}, {" publication_date": "2024a", "fullname_first_author": "Haotian Liu", "paper_title": "Visual instruction tuning", "reason": "This paper focuses on visual instruction tuning, a crucial aspect of improving the capabilities of LVLMs in understanding and interpreting visual instructions. It offers insights into different instruction tuning methodologies and their impact on performance, providing a useful comparison point for the proposed MIA-DPO.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Yuan Liu", "paper_title": "MMBench: Is your multi-modal model an all-around player?", "reason": "This paper introduces MMBench, a comprehensive benchmark designed to evaluate multimodal models. Understanding the capabilities and challenges of MMBench provides important context for evaluating the performance of MIA-DPO on multi-modal tasks.", "section_number": 2}, {" publication_date": "2019", "fullname_first_author": "Drew A Hudson", "paper_title": "GQA: A new dataset for real-world visual reasoning and compositional question answering", "reason": "This paper introduces GQA, a dataset focused on visual reasoning. The challenges and characteristics of GQA provide valuable insights into the complexities of multi-image understanding, further contextualizing the contributions of the proposed MIA-DPO.", "section_number": 2}]}