{"references": [{" publication_date": "2023", "fullname_first_author": "Josh Achiam", "paper_title": "GPT-4 technical report", "reason": "This paper is a technical report describing GPT-4, a highly influential large language model that serves as a benchmark in the field. Its capabilities, particularly its multi-image handling prowess, are compared against the open-source models discussed in the paper, providing context for the need for improved open-source alternatives like MIA-DPO.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Anas Awadalla", "paper_title": "OpenFlamingo: An open-source framework for training large autoregressive vision-language models", "reason": "This paper introduces OpenFlamingo, a significant open-source framework for training large vision-language models.  It is directly relevant to the paper's goal of improving open-source LVLMs' multi-image capabilities and is cited as an example of a promising approach that still faces challenges.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Yuntao Bai", "paper_title": "Constitutional AI: Harmlessness from ai feedback", "reason": "This paper is highly relevant to the preference alignment aspect of the target paper.  It introduces Reinforcement Learning from AI Feedback (RLAIF), a key technique in aligning LLMs with human values, which is directly related to the goal of MIA-DPO to improve visual preference alignment.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Lin Chen", "paper_title": "Are we on the right way for evaluating large vision-language models?", "reason": "This paper critically examines existing evaluation methods for large vision-language models and raises concerns about their limitations.  This is relevant to the target paper, which proposes MIA-DPO as an improved method for training and evaluating LVLMs, particularly in the multi-image context.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Dongfu Jiang", "paper_title": "Interleaved multi-image instruction tuning", "reason": "This paper is directly relevant to the context of multi-image LVLMs as it introduces a new approach to instruction tuning for multi-image data. Its relevance stems from the fact that the authors' method also seeks to improve the performance of LVLMs on multi-image tasks, but through preference optimization rather than instruction tuning.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Kunchang Li", "paper_title": "MVBench: A comprehensive multi-modal video understanding benchmark", "reason": "This paper introduces MVBench, a comprehensive benchmark for multi-modal video understanding. The reference highlights the need for robust benchmarks to evaluate the progress of LVLMs, which is directly relevant to the experimental evaluation of MIA-DPO in the target paper.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Bo Li", "paper_title": "LLaVA-NeXT-Interleave: Tackling multi-image, video, and 3d in large multimodal models", "reason": "This paper focuses on handling multi-image, video, and 3D data in large multimodal models. It is relevant because it addresses a similar challenge to MIA-DPO\u2014improving LVLMs' ability to understand complex multi-modal data\u2014but using a different approach, highlighting the ongoing research effort.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "reason": "This paper is highly influential in the field of reinforcement learning from human feedback (RLHF).  The methods described in this paper are directly relevant to the preference alignment aspect of the target paper because MIA-DPO utilizes the DPO approach that's closely related to RLHF.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Quan Sun", "paper_title": "Generative multimodal models are in-context learners", "reason": "This paper is relevant to the preference alignment aspect of the target paper. It discusses reinforcement learning approaches for aligning large multimodal models, which is closely related to the DPO optimization method used in MIA-DPO. This paper's findings highlight the effectiveness of preference alignment in addressing model hallucinations, a key focus of the target paper.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Xiang Yue", "paper_title": "MMMU: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi", "reason": "This paper presents MMMU, a benchmark used in the target paper's experiments. The benchmark's focus on multi-disciplinary multimodal understanding and reasoning directly relates to the goals of MIA-DPO, providing a relevant and important context for evaluating the performance of the proposed method.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Xingyu Fu", "paper_title": "BLINK: Multimodal large language models can see but not perceive", "reason": "This paper introduces the BLINK benchmark, used in the target paper's experimental evaluation.  BLINK's focus on multimodal reasoning and visual perception directly relates to the challenges that MIA-DPO aims to address, making this reference crucial for understanding the experimental setup and results.", "section_number": 4}, {" publication_date": "2018", "fullname_first_author": "Alane Suhr", "paper_title": "A corpus for reasoning about natural language grounded in photographs", "reason": "This paper introduces the NLVR2 benchmark, a key dataset used for evaluating visual reasoning capabilities in the target paper. The dataset's focus on visual reasoning and its use in evaluating MIA-DPO's performance makes this reference highly relevant to the experimental results section.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Lin Chen", "paper_title": "MMStar: A benchmark for evaluating multi-modal performance", "reason": "This paper introduces the MMStar benchmark, which is used for evaluating multi-modal performance in the target paper. This benchmark directly relates to the goals of MIA-DPO, and its inclusion in the experimental evaluation strengthens the study's validity and relevance.", "section_number": 4}, {" publication_date": "2022", "fullname_first_author": "Pan Lu", "paper_title": "GQA: A new dataset for real-world visual reasoning and compositional question answering", "reason": "This paper introduces GQA, a dataset that's indirectly relevant. The focus on real-world visual reasoning and compositional question answering makes it relevant to the overall context of improving LVLMs.  The target paper's methodology is partially inspired by the challenges highlighted in the development of this dataset for visual question answering.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Weihao Yu", "paper_title": "MM-Vet: Evaluating large multimodal models for integrated capabilities", "reason": "This paper is relevant because it introduces the MMVet benchmark, used for evaluating the performance of large multimodal models.  The focus on integrated capabilities, especially in the visual domain, aligns directly with the goals of MIA-DPO.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Rafael Rafailov", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "reason": "This paper introduces the Direct Preference Optimization (DPO) algorithm, which is the core optimization method employed by MIA-DPO.  Understanding DPO is essential for understanding the technical approach taken by the authors, making this a highly important reference.", "section_number": 3}, {" publication_date": "2017", "fullname_first_author": "John Schulman", "paper_title": "Proximal policy optimization algorithms", "reason": "This paper introduces the Proximal Policy Optimization (PPO) algorithm, a reinforcement learning algorithm closely related to DPO (used in MIA-DPO).  Understanding PPO provides crucial context for the optimization techniques used in the target paper's methodology.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Quan Sun", "paper_title": "Generative multimodal models are in-context learners", "reason": "This paper is relevant because it presents a reinforcement learning approach for aligning large multimodal models, which is related to the DPO optimization used in MIA-DPO. The discussion of preference alignment and the challenges in creating high-quality data are crucial to the context of MIA-DPO's methodology.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Zhiyuan Zhao", "paper_title": "Beyond hallucinations: Enhancing lvlms through hallucination-aware direct preference optimization", "reason": "This paper is highly relevant because it also addresses the issue of hallucinations in LVLMs and proposes a method to mitigate them through direct preference optimization.  This method, while not directly related to multi-image contexts, shares the key concept of using preference optimization to address hallucinations.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Pan Zhang", "paper_title": "InternLM-Xcomposer-2.5: A versatile large vision language model supporting long-contextual input and output", "reason": "This paper introduces InternLM-XC2.5, a model used in the experimental evaluation. InternLM-XC2.5 is a key model used in the experiments, and its performance is directly compared to the performance of MIA-DPO.  This makes the reference crucial for assessing the impact and significance of the proposed method.", "section_number": 4}]}