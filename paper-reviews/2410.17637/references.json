{"references": [{" publication_date": "2023", "fullname_first_author": "Josh Achiam", "paper_title": "GPT-4 technical report", "reason": "This paper is foundational as it presents GPT-4, a state-of-the-art proprietary model that excels in multi-image understanding, serving as a benchmark and highlighting the gap open-source LVLMs need to bridge.  Its technical details provide valuable context for understanding the challenges and opportunities in visual language modeling.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Anas Awadalla", "paper_title": "OpenFlamingo: An open-source framework for training large autoregressive vision-language models", "reason": "This paper introduces OpenFlamingo, a significant open-source contribution in the field, providing a framework that enables researchers to train large autoregressive vision-language models. Its accessibility and open-source nature make it crucial for advancing the development of LVLMs, directly impacting the research area discussed in the MIA-DPO paper.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Yuntao Bai", "paper_title": "Constitutional AI: Harmlessness from ai feedback", "reason": "This paper is highly relevant because it introduces Reinforcement Learning from AI Feedback (RLAIF), a key technique for aligning LLMs with human values and reducing hallucinations.  RLAIF is a critical component of many LVLMs, making this paper foundational to the broader discussion on preference alignment and model safety within the LVLMs field.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "reason": "This is a seminal work in the field of reinforcement learning from human feedback (RLHF), a crucial technique for aligning large language models with human preferences.  The techniques and insights from this paper are directly relevant to preference alignment, a core component of the MIA-DPO method.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Anas Awadalla", "paper_title": "MINT-1T: Scaling open-source multimodal data by 10x: A multimodal dataset with one trillion tokens", "reason": "This paper presents a massive multimodal dataset (MINT-1T), which is incredibly important due to its scale and accessibility. The availability of large-scale, open-source datasets is crucial for training and evaluating LVLMs, directly impacting the feasibility and progress in the field discussed in the MIA-DPO paper.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Jinze Bai", "paper_title": "Qwen-VL: A frontier large vision-language model with versatile abilities", "reason": "This paper introduces Qwen-VL, a strong open-source LVLMs model.  Its performance and capabilities in handling visual information provide a valuable comparative benchmark for evaluating the performance of the MIA-DPO method and understanding the current state-of-the-art in open-source LVLMs.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Quan Sun", "paper_title": "Generative multimodal models are in-context learners", "reason": "This paper is significant because it explores the effectiveness of direct preference optimization (DPO) in the visual domain for mitigating hallucinations, directly relating to the core methodology of the MIA-DPO approach.  Understanding the successes and limitations of prior DPO methods in single-image scenarios is crucial for evaluating the improvements made by MIA-DPO in the multi-image setting.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Zhiqing Sun", "paper_title": "Aligning large multimodal models with factually augmented rlhf", "reason": "This paper focuses on aligning large multimodal models with human preferences using a method that combines factual augmentation with RLHF.  This is directly related to MIA-DPO as both address the challenges of visual preference alignment, providing a relevant benchmark and context for the proposed approach.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Rafael Rafailov", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "reason": "This paper introduces Direct Preference Optimization (DPO), the core optimization algorithm used in MIA-DPO. Understanding the principles and workings of DPO is essential for understanding the technical foundation of MIA-DPO and its potential impact on the field.", "section_number": 3}, {" publication_date": "2017", "fullname_first_author": "John Schulman", "paper_title": "Proximal policy optimization algorithms", "reason": "This paper introduces Proximal Policy Optimization (PPO), a widely used reinforcement learning algorithm. While MIA-DPO uses DPO, understanding PPO is essential for placing the choice of DPO in context and appreciating the broader landscape of reinforcement learning algorithms used in training LVLMs.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Abhimanyu Dubey", "paper_title": "The llama 3 herd of models", "reason": "This paper introduces a collection of Llama 3 models, which are relevant because they represent a significant advance in open-source LVLMs. The models' performance and capabilities provide a crucial benchmark for evaluating the effectiveness of MIA-DPO.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Quan Sun", "paper_title": "Generative multimodal models are in-context learners", "reason": "This paper is significant because it explores the effectiveness of direct preference optimization (DPO) in the visual domain for mitigating hallucinations, directly relating to the core methodology of the MIA-DPO approach.  Understanding the successes and limitations of prior DPO methods in single-image scenarios is crucial for evaluating the improvements made by MIA-DPO in the multi-image setting.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Zhiyuan Zhao", "paper_title": "Enhancing lvlms through hallucination-aware direct preference optimization", "reason": "This paper proposes HA-DPO, a method that uses GPT-4 to detect and correct hallucinations.  It's directly relevant as a comparison to MIA-DPO, allowing for an analysis of the relative strengths and limitations of each approach in dealing with hallucinations in LVLMs.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Yiyang Zhou", "paper_title": "Aligning modalities in vision large language models via preference fine-tuning", "reason": "This paper is important as it explores preference fine-tuning in vision-language models, directly addressing the core challenge tackled by MIA-DPO. Comparing the approaches and results allows for a better understanding of the contributions and limitations of each method in improving the alignment of LVLMs' preferences with human preferences.", "section_number": 4}, {" publication_date": "2019", "fullname_first_author": "Drew A Hudson", "paper_title": "GQA: A new dataset for real-world visual reasoning and compositional question answering", "reason": "This paper introduces the GQA dataset, which is relevant because it is a benchmark dataset for visual reasoning. The presence of visual reasoning tasks in GQA highlights the importance of evaluating the performance of LVLMs in scenarios that involve visual understanding and logical reasoning, which is directly addressed by MIA-DPO.", "section_number": 4}, {" publication_date": "2016", "fullname_first_author": "Aniruddha Kembhavi", "paper_title": "A diagram is worth a dozen images", "reason": "This paper introduces AI2D, a benchmark dataset focusing on diagram understanding and reasoning, demonstrating the growing importance of understanding visual information in more complex contexts.  AI2D highlights the need for LVLMs to handle visual information in diverse formats, which is directly addressed by the multi-image capabilities of MIA-DPO.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Hugo Lauren\u00e7on", "paper_title": "OBELICS: An open web-scale filtered dataset of interleaved image-text documents", "reason": "This work introduces OBELICS, a large-scale dataset containing interleaved image-text documents, which is crucial to the MIA-DPO paper because it represents the type of real-world data the model should ideally handle. The dataset serves as a representative example of the type of complex visual-textual data that the MIA-DPO method is designed to improve the handling of.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Dongfu Jiang", "paper_title": "Interleaved multi-image instruction tuning", "reason": "This paper focuses on instruction tuning for multi-image data, providing a method for improving the performance of LVLMs on such data.  This is directly relevant to the challenges addressed by MIA-DPO, as both papers aim to improve the performance of LVLMs on tasks involving multiple images.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Pan Zhang", "paper_title": "InternLM-Xcomposer-2.5: A versatile large vision language model supporting long-contextual input and output", "reason": "This paper introduces InternLM-XC2.5, a large vision language model used as one of the main models for evaluating the performance of MIA-DPO.  The use of this model as a benchmark highlights the increasing importance of robust, versatile LVLMs and underscores the significance of the improvements achieved by MIA-DPO in this context.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Xiang Yue", "paper_title": "MMMU: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi", "reason": "This paper introduces MMMU, a multi-discipline multimodal benchmark used for evaluating the performance of MIA-DPO.  The inclusion of MMMU showcases the versatility of the MIA-DPO method and its ability to handle a wide range of multi-image tasks, demonstrating its real-world applicability and potential impact on the field of LVLMs.", "section_number": 4}]}