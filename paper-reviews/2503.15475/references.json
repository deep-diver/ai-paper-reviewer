{"references": [{"fullname_first_author": "J. Achiam", "paper_title": "Gpt-4 technical report", "publication_date": "2023-03-01", "reason": "This paper is important as GPT-4 is used within text-to-shape and text-to-scene generation and is a key component that enables these applications, which are used in the models"}, {"fullname_first_author": "A. Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-01-01", "reason": "This work describes the CLIP model, which is used in the text-to-shape architecture as a pre-trained CLIP text encoder for encoding text prompts."}, {"fullname_first_author": "A. Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-01-01", "reason": "This reference introduces the Transformer architecture, which is the foundation of the GPT-2 model used in the text-to-shape application as a decoder-only transformer."}, {"fullname_first_author": "M. Oquab", "paper_title": "Dinov2: Learning robust visual features without supervision", "publication_date": "2023-04-01", "reason": "This work uses Dinov2, which is a self-supervised loss to regularize the latent space geometrically."}, {"fullname_first_author": "B. Zhang", "paper_title": "3dshape2vecset: A 3d shape representation for neural fields and generative diffusion models", "publication_date": "2023-01-01", "reason": "This paper provides the basis for representing 3D shapes and is used as a starting point for the shape tokenization process."}]}