[{"figure_path": "https://arxiv.org/html/2503.15475/extracted/6294257/figures/teaser_v3.png", "caption": "Figure 1: Scene Generation. We are developing a foundation model for 3D intelligence that will support applications like scene generation. This winter village scene was generated through a multi-turn conversation with our prototype scene generation tool.", "description": "This figure showcases a winter village scene generated using a prototype 3D scene generation tool. The scene is used as an example to illustrate the capabilities of the foundation model for 3D intelligence being developed by Roblox.  The model supports multi-turn conversations allowing users to iteratively refine generated scenes.  This demonstrates the system's potential for applications in virtual world creation.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.15475/x1.png", "caption": "Figure 2: Overview. We present an important step towards the foundation model for 3D intelligence. Specifically, our report focuses on 3D shape tokenization\u2014a technique for converting between shapes and discrete tokens. We also demonstrate how our tokenization scheme enables multiple applications including text-to-shape generation, shape-to-text generation, and text-to-scene generation.", "description": "This figure illustrates the key steps in the development of a foundation model for 3D intelligence at Roblox.  The central concept is 3D shape tokenization, a method for converting 3D shapes into discrete tokens, similar to how words are tokenized in natural language processing.  This tokenization process is shown to enable three core applications: text-to-shape generation (creating 3D shapes from text descriptions), shape-to-text generation (generating text descriptions from 3D shapes), and text-to-scene generation (building entire 3D scenes from textual descriptions). The figure visually depicts the workflow of each application, highlighting how the discrete shape tokens serve as a common intermediary.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.15475/x2.png", "caption": "Figure 3: Shape Tokenization. Our pipeline encodes an input mesh into discrete tokens through several steps: (1) We sample points from the mesh surface and embed them using our Phased-Modulated Positional Encoding; (2) A Perceiver-based transformer\u00a0(Jaegle et\u00a0al., 2021) encodes these points into continuous latent vectors, regularized with a self-supervised loss; (3) We apply optimal-transport vector quantization\u00a0(Zhang et\u00a0al., 2024) to convert these vectors into discrete shape tokens;\n(4) These tokens can later be decoded into an occupancy field for mesh extraction. To improve training stability and the reconstruction quality, we also introduce a Stochastic Gradient Shortcut layer that allows the decoder to utilize the continuous latent vectors directly during training.", "description": "Figure 3 illustrates the process of shape tokenization, a crucial step in converting 3D shapes into a format suitable for processing by a transformer-based neural network.  The process begins by sampling points from the surface of a 3D mesh. These points are then encoded using a custom 'Phased-Modulated Positional Encoding' technique designed to preserve spatial relationships between points. Next, a Perceiver transformer processes these encoded points, transforming them into continuous latent vectors. A self-supervised loss function is applied to regularize the latent space, leading to more robust and stable training.  The continuous latent vectors are subsequently converted to discrete shape tokens via optimal transport vector quantization. Finally, these discrete tokens can be decoded back into an occupancy field, which can then be used to reconstruct the original 3D mesh.  A 'Stochastic Gradient Shortcut' is incorporated to enhance training stability and reconstruction quality by enabling the decoder to directly use the continuous latent vectors during training.", "section": "2. Shape Tokenization"}, {"figure_path": "https://arxiv.org/html/2503.15475/extracted/6294257/figures/pmpe/pe.png", "caption": "(a) Baseline Positional Encoding \u03b3\u2062(\u22c5)\ud835\udefe\u22c5\\gamma(\\cdot)italic_\u03b3 ( \u22c5 )", "description": "The figure shows a comparison of traditional positional encoding with the proposed Phase-Modulated Positional Encoding (PMPE). Traditional positional encoding, shown in (a), uses sinusoidal functions to encode the position of input points but creates periodic variations across embedding dimensions, causing dot-product similarity to not reflect actual spatial proximity well. PMPE (b), in contrast, produces an encoding that clearly distinguishes between spatially distinct points via phase modulation, resulting in improved spatial proximity representation.", "section": "2.1. Phase-Modulated Positional Encoding"}, {"figure_path": "https://arxiv.org/html/2503.15475/extracted/6294257/figures/pmpe/pmpe.png", "caption": "(b) Phase-Modulated Positional Encoding \u03b3\u2032\u2062(\u22c5)superscript\ud835\udefe\u2032\u22c5\\gamma^{\\prime}(\\cdot)italic_\u03b3 start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ( \u22c5 )", "description": "The figure shows a comparison between traditional positional encoding and the proposed phase-modulated positional encoding (PMPE). Traditional positional encoding exhibits periodic variations across embedding dimensions, poorly reflecting spatial proximity. In contrast, PMPE maintains clear distinctions between spatially distant inputs, improving the transformer's ability to disambiguate spatially distinct points.", "section": "2.1 Phase-Modulated Positional Encoding"}, {"figure_path": "https://arxiv.org/html/2503.15475/x3.png", "caption": "Figure 4: Phase-Modulated Positional Encoding. Comparison of positional encoding methods using 128 base frequencies for 1D inputs in [\u22121,1]11[-1,1][ - 1 , 1 ], with corresponding dot-product similarity matrices. (a) Traditional positional encoding \u03b3\u2062(\u22c5)\ud835\udefe\u22c5\\gamma(\\cdot)italic_\u03b3 ( \u22c5 ) enhances high-frequency details but exhibits periodic variations across embedding dimensions as inputs vary from \u221211-1- 1 to 1111. This causes dot-product similarities between encoded vectors to poorly reflect their spatial proximity. (b) Our proposed PMPE \u03b3\u2032\u2062(\u22c5)superscript\ud835\udefe\u2032\u22c5\\gamma^{\\prime}(\\cdot)italic_\u03b3 start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ( \u22c5 ) maintains clear distinctions between spatially distant inputs, as reflected in the dot-product similarity of encoded vectors. Our final approach uses the combined encoding of \u03b3\u2032+\u03b3superscript\ud835\udefe\u2032\ud835\udefe\\gamma^{\\prime}+\\gammaitalic_\u03b3 start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT + italic_\u03b3 that preserves fine-grained details while ensuring similarity patterns align with global spatial proximity.", "description": "Figure 4 illustrates the comparison between two positional encoding methods for handling 1D inputs ranging from -1 to 1.  The top panel (a) shows a traditional positional encoding where high-frequency details are enhanced, but periodic variations across embedding dimensions cause issues with accurately reflecting spatial proximity in dot-product similarity matrices. In contrast, the bottom panel (b) presents the proposed Phase-Modulated Positional Encoding (PMPE). PMPE effectively maintains clear distinctions between spatially distant inputs, resulting in a dot-product similarity that better aligns with true spatial relationships. The final method combines both encoding schemes to retain fine-grained details while ensuring spatial proximity.", "section": "2.1. Phase-Modulated Positional Encoding"}, {"figure_path": "https://arxiv.org/html/2503.15475/x4.png", "caption": "Figure 5: Latent Space Regularization with Self-supervised Loss.\nWe regularize our latent space using a self-supervised loss inspired by DINOv2\u00a0(Oquab et\u00a0al., 2023). This loss is computed as the cross entropy between features extracted from the student and teacher encoders, where the teacher model is an Exponential Moving Average (EMA) of the student. Input queries to the student encoder are randomly masked for robustness.", "description": "This figure illustrates the self-supervised learning method used to regularize the latent space of the shape tokenizer.  A student encoder and a teacher encoder (an exponential moving average of the student) are employed.  A cross-entropy loss is calculated between features extracted from both encoders. To enhance robustness, input queries to the student encoder are randomly masked.", "section": "2. Shape Tokenization"}, {"figure_path": "https://arxiv.org/html/2503.15475/x5.png", "caption": "(a) Without self-supervised loss", "description": "This figure shows a matrix visualizing the cosine similarity between latent vectors of 3D meshes for a model trained without the self-supervised loss.  The lack of regularization results in latent vector similarities that do not correlate well with the geometric similarity of the input meshes.  For example, a model trained without the self-supervised loss might show high similarity between dissimilar shapes, and vice versa. This contrasts with the expected behavior that geometrically similar 3D meshes should have similar latent vectors.", "section": "2. Latent Space Regularization with Self-supervised Loss"}, {"figure_path": "https://arxiv.org/html/2503.15475/x6.png", "caption": "(b) With self-supervised loss", "description": "This figure shows the cosine similarity between latent vectors of 3D meshes for models trained with and without the self-supervised loss.  The left image (a) demonstrates that without the self-supervised loss, the similarity scores do not accurately reflect the geometric relationships (e.g., an ice cream is closer to a car than to another similar car). The right image (b), however, shows that with the self-supervised loss, the latent space similarities strongly align with the ground-truth geometric relationships, improving the model's ability to distinguish between similar shapes.", "section": "2.3. Learning geometrically clustered latents with self-supervised loss"}, {"figure_path": "https://arxiv.org/html/2503.15475/x7.png", "caption": "Figure 6: Impact of Self-Supervised Loss. Cosine similarity between the latent vectors of 3D meshes for models trained (a) with and (b) without the proposed self-supervised loss term. (a) Without regularization, latent cosine similarity scores fail to correlate with geometric structure (e.g., the ice-cream in 4t\u2062hsuperscript4\ud835\udc61\u210e4^{th}4 start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT row has higher similarity to the car in 1s\u2062tsuperscript1\ud835\udc60\ud835\udc611^{st}1 start_POSTSUPERSCRIPT italic_s italic_t end_POSTSUPERSCRIPT row than the similar car in 2n\u2062dsuperscript2\ud835\udc5b\ud835\udc512^{nd}2 start_POSTSUPERSCRIPT italic_n italic_d end_POSTSUPERSCRIPT row). (b) With the loss, latent space similarity aligns with ground-truth geometric relationships.", "description": "Figure 6 demonstrates the effect of a self-supervised loss on the latent space of a 3D shape encoding model.  Two heatmaps show the cosine similarity between latent vectors representing different 3D shapes. (a) shows the results without the self-supervised loss; note the lack of correlation between cosine similarity and visual similarity (e.g., an ice cream is more similar to a car than to another ice cream). (b) shows the results with the self-supervised loss; a strong correlation is now evident between cosine similarity and visual similarity.", "section": "2. Latent Space Regularization with Self-supervised Loss"}, {"figure_path": "https://arxiv.org/html/2503.15475/x8.png", "caption": "Figure 7: Qualitative Analysis of Shape Reconstruction. Comparison between our method and Craftsman\u00a0(Li et\u00a0al., 2024) demonstrates that both of our model variants achieve superior reconstruction quality, preserving finer geometric details while producing fewer artifacts.", "description": "Figure 7 presents a qualitative comparison of 3D shape reconstruction results between the proposed method and the Craftsman method (Li et al., 2024).  The figure visually showcases the superior reconstruction quality achieved by both variants of the proposed model. It highlights the preservation of finer geometric details and a significant reduction in artifacts compared to the Craftsman method.", "section": "2. Shape Tokenization"}, {"figure_path": "https://arxiv.org/html/2503.15475/x9.png", "caption": "Figure 8: Text-to-Shape Generation Result Gallery. Our model can generate a diverse set of 3D meshes, capturing sharp edges, smooth surfaces and complex structures.", "description": "Figure 8 showcases a gallery of 3D models generated by a text-to-shape model.  The models demonstrate the model's capacity to create a wide variety of shapes from textual descriptions, successfully rendering diverse geometries with sharp edges, smooth surfaces, and intricate structures. This highlights the model's ability to capture complex details from textual input, and generate high-fidelity 3D outputs.", "section": "3. Applications"}, {"figure_path": "https://arxiv.org/html/2503.15475/x10.png", "caption": "Figure 9: Shape-to-text examples. Shape-to-text captioning of example shapes from Toys4K dataset using short, medium and long captions. Words highlighted in blue indicate notable differences as captions increase in length. Short captions usually capture the shape category. More details about the shape\u2019s geometry and style are added as caption length increases.", "description": "Figure 9 presents examples of shape-to-text generation from the Toys4K dataset.  Three captions of varying lengths (short, medium, and long) are shown for each example shape. The short captions broadly identify the shape category (e.g., \"A vintage biplane\"). As the caption length increases, more details regarding the shape's specific geometry, style, and features are included, as highlighted by the blue words in the caption. This demonstrates the model's capacity to generate increasingly detailed and descriptive text from the same visual input as caption length grows.", "section": "3. Applications"}, {"figure_path": "https://arxiv.org/html/2503.15475/x11.png", "caption": "Figure 10: Shape cycle consistency. Our shape-to-text and text-to-shape models demonstrate the cycle consistency. Given a shape, we caption it using shape-to-text model, then then regenerate the 3D shape using the text-to-shape model. We highlight some keywords in blue for the shape category and style. The process preserves the overall geometry and key characteristics of the original shape, although there can be some loss in fine-scale details.", "description": "This figure showcases the cycle consistency of the shape-to-text and text-to-shape models.  Starting with a 3D shape, the shape-to-text model generates a textual description. This description is then fed into the text-to-shape model, which attempts to reconstruct the original 3D shape. Key words highlighting the shape category and style are shown in blue. While the overall geometry and main characteristics are preserved, some minor details may be lost in the reconstruction process.", "section": "3. Applications"}]