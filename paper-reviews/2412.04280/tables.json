[{"content": "| Add | Rmove | Replace | Action | Counting | Relation | Sum |\n|---|---|---|---|---|---|---|\n| HumanEdit-full | 801 | 1,813 | 1,370 | 659 | 698 | 410 | 5,751 |\n| HumanEdit-core | 30 | 188 | 97 | 37 | 20 | 28 | 400 |", "caption": "Table 1: \nDistribution of 6 types of our human-rewarded editing instructions.", "description": "This table shows the distribution of the six different types of image editing instructions used in the HumanEdit dataset.  The dataset was created using human annotators and administrators to ensure quality and alignment with human preferences. Each row represents a category of editing instructions, and the numbers indicate how many images in the dataset belong to each category. These numbers sum up to the total number of images in the HumanEdit dataset (5,751). The categories are Action, Add, Counting, Relation, Remove, and Replace.  This breakdown illustrates the diversity of editing tasks encompassed within the dataset, highlighting the dataset's scope and versatility.", "section": "Table 1: Distribution of 6 types of our human-rewarded editing instructions"}, {"content": "| Dataset | Real Image for Edit | Real-world Scenario | Human | Ability Classification | Mask | Non-Mask Editing |\n|---|---|---|---|---|---|---|\n| InstructPix2Pix (Brooks et al., 2023) | \u2717 | \u2717 | \u2717 | \u2717 | \u2717 | \u2713 |\n| MagicBrush (Zhang et al., 2024a) | \u2713 | \u2717 | \u2713 | \u2717 | \u2713 | \u2717 |\n| GIER (Shi et al., 2020) | \u2713 | \u2713 | \u2713 | \u2717 | \u2717 | \u2717 |\n| MA5k-Req (Shi et al., 2021) | \u2713 | \u2713 | \u2713 | \u2717 | \u2717 | \u2717 |\n| TEdBench (Kawar et al., 2023) | \u2713 | \u2713 | \u2713 | \u2717 | \u2717 | \u2713 |\n| HQ-Edit (Hui et al., 2024) | \u2717 | \u2717 | \u2717 | \u2717 | \u2717 | \u2713 |\n| SEED-Data-Edit (Ge et al., 2024a) | \u2713 | \u2713 | \u2713 | \u2717 | \u2717 | \u2713 |\n| AnyEdit (Yu et al., 2024a) | \u2713 | \u2713 | \u2717 | \u2717 | \u2713 | \u2713 |\n| HumanEdit | \u2713 | \u2713 | \u2713 | 6 | \u2713 | \u2713 |", "caption": "Table 2: Comparison of existing image editing datasets. \u201cReal Image for Edit\u201d denotes whether real images are used for editing instead of images generated by models. \u201cReal-world Scenario\u201d indicates whether images edited by users in the real world are included. \u201cHuman\u201d denotes whether human annotators are involved. \u201cAbility Classification\u201d refers to evaluating the edit ability in different dimensions. \u201cMask\u201d indicates whether rendering masks for editing is supported. \u201cNon-Mask Editing\u201d denotes the ability to edit without mask input.", "description": "This table compares several existing image editing datasets across key characteristics.  It highlights whether the datasets use real-world images as opposed to model-generated ones, whether they incorporate edits performed by real users, and the level of human involvement in the annotation process.  The comparison also includes information about whether the datasets categorize edits into distinct dimensions (ability classification), whether masks are provided to support editing, and whether mask-free editing is possible.", "section": "3 Dataset Statistics"}, {"content": "| Methods | L1\u2193 | L2\u2193 | CLIP-I\u2191 | DINO\u2191 | CLIP-T\u2191 |\n|---|---|---|---|---|---| \n| *HumanEdit-full* |  |  |  |  |  |\n| InstructPix2Pix (Brooks et al., 2023) | 0.1601 | 0.0551 | 0.7716 | 0.5335 | 0.2591 |\n| MGIE (Fu et al., 2023) | 0.1240 | 0.0535 | 0.8697 | 0.7221 | 0.2661 |\n| HIVE SD1.5 (Zhang et al., 2024b) | 0.1014 | **0.0278** | 0.8526 | 0.7726 | **0.2777** |\n| MagicBrush (Zhang et al., 2024a) | **0.0807** | 0.0298 | **0.8915** | **0.7963** | 0.2676 |\n| *HumanEdit-core* |  |  |  |  |  |\n| InstructPix2Pix (Brooks et al., 2023) | 0.1625 | 0.0570 | 0.7627 | 0.5349 | 0.2533 |\n| MGIE (Fu et al., 2023) | 0.1294 | 0.0610 | 0.8670 | 0.7359 | 0.2589 |\n| HIVE SD1.5 (Zhang et al., 2024b) | 0.1162 | 0.0373 | 0.8441 | 0.7038 | 0.2563 |\n| MagicBrush (Zhang et al., 2024a) | **0.0760** | **0.0283** | **0.8946** | **0.8121** | **0.2619** |", "caption": "Table 3: Quantitative study on mask-free baselines on HumanEdit. The best results are marked in bold.", "description": "This table presents a quantitative comparison of several mask-free baselines on the HumanEdit dataset.  The baselines are evaluated using five metrics: L1 loss, L2 loss, CLIP-I similarity, DINO similarity, and CLIP-T similarity.  Lower is better for L1 and L2 loss, while higher is better for the similarity metrics.  The results are presented for two subsets of the HumanEdit dataset: HumanEdit-full and HumanEdit-core,  allowing for a comparison of performance across different dataset scales and complexities. The best performing model for each metric in each dataset subset is highlighted in bold.", "section": "4 HI-EDIT Benchmark"}, {"content": "| Methods | L1 \u2193 | L2 \u2193 | CLIP-I \u2191 | DINO \u2191 | CLIP-T \u2191 |\n|---|---|---|---|---|---| \n| *HumanEdit-full* |  |  |  |  |  |\n| Blended Latent Diff. SDXL (Avrahami et al., 2023) | 0.0481 | 0.0151 | 0.9178 | 0.8481 | 0.2681 |\n| GLIDE (Nichol et al., 2021) | **0.0391** | **0.0120** | **0.9388** | 0.8800 | 0.2676 |\n| aMUSEd (Patil et al., 2024) | 0.0673 | 0.0187 | 0.9149 | 0.8588 | **0.2771** |\n| Meissonic (Bai et al., 2024) | 0.0627 | 0.0177 | 0.9324 | **0.8806** | 0.2710 |\n| *HumanEdit-core* |  |  |  |  |  |\n| Blended Latent Diff. SDXL (Avrahami et al., 2023) | 0.0496 | 0.0162 | 0.9116 | 0.8550 | 0.2640 |\n| GLIDE (Nichol et al., 2021) | **0.0379** | **0.0113** | **0.9413** | **0.8961** | 0.2656 |\n| aMUSEd (Patil et al., 2024) | 0.0665 | 0.0184 | 0.9138 | 0.8743 | **0.2747** |\n| Meissonic (Bai et al., 2024) | 0.0608 | 0.0166 | 0.9348 | 0.8943 | 0.2694 |\n| *HumanEdit-mask* |  |  |  |  |  |\n| Blended Latent Diff. SDXL (Avrahami et al., 2023) | 0.0478 | 0.0154 | 0.9065 | 0.8223 | 0.2650 |\n| GLIDE (Nichol et al., 2021) | **0.0377** | **0.0117** | **0.9343** | 0.8687 | 0.2665 |\n| aMUSEd (Patil et al., 2024) | 0.0654 | 0.0179 | 0.9097 | 0.8497 | **0.2785** |\n| Meissonic (Bai et al., 2024) | 0.0604 | 0.0166 | 0.9303 | **0.8783** | 0.2715 |", "caption": "Table 4: \nQuantitative study on mask-provided baselines on HumanEdit. The best results are marked in bold.", "description": "This table presents a quantitative comparison of different baselines on the HumanEdit dataset.  The baselines are models used for image editing, and they are evaluated using mask-provided settings, meaning that the models are given both the image to edit and a mask indicating the region of interest. The evaluation metrics used are L1, L2, CLIP-I, DINO, and CLIP-T, which measure various aspects of the quality and consistency between the generated edited images and the ground truth (actual) edited images.  The best performing model for each metric is highlighted in bold.", "section": "4 HI-EDIT Benchmark"}, {"content": "| Methods | L1\u2193 | L2\u2193 | CLIP-I\u2191 | DINO\u2191 | CLIP-T\u2191 |\n|---|---|---|---|---|---| \n| *HumanEdit-Add* |\n| InstructPix2Pix (Brooks et al., 2023) | 0.1152 | 0.0329 | 0.8135 | 0.6230 | 0.2764 |\n| MGIE (Fu et al., 2023) | 0.0934 | 0.0274 | 0.8770 | 0.7391 | 0.2806 |\n| HIVE SD1.5 (Zhang et al., 2024b) | 0.0885 | 0.0234 | 0.8863 | 0.7811 | 0.2706 |\n| MagicBrush (Zhang et al., 2024a) | 0.0580 | 0.0167 | 0.9102 | 0.8562 | 0.2745 |\n| Blended Latent Diff. SDXL (Avrahami et al., 2023) | 0.0344 | **0.0073** | 0.9285 | 0.8856 | 0.2665 |\n| GLIDE (Nichol et al., 2021) | **0.0315** | 0.0078 | **0.9410** | **0.8995** | 0.2600 |\n| aMUSEd (Patil et al., 2024) | 0.0581 | 0.0130 | 0.9148 | 0.8672 | **0.2695** |\n| Meissonic (Bai et al., 2024) | 0.0544 | 0.0129 | 0.9303 | 0.8787 | 0.2669 |\n| *HumanEdit-Action* |\n| InstructPix2Pix (Brooks et al., 2023) | 0.1324 | 0.0398 | 0.7514 | 0.5789 | 0.2617 |\n| MGIE (Fu et al., 2023) | 0.0982 | 0.0383 | 0.8788 | 0.7909 | 0.2658 |\n| HIVE SD1.5 (Zhang et al., 2024b) | 0.0972 | 0.0280 | 0.8592 | 0.7613 | 0.2640 |\n| MagicBrush (Zhang et al., 2024a) | 0.0723 | 0.0245 | 0.9028 | 0.8357 | 0.2668 |\n| Blended Latent Diff. SDXL (Avrahami et al., 2023) | 0.0416 | **0.0109** | 0.9391 | 0.9015 | 0.2712 |\n| GLIDE (Nichol et al., 2021) | **0.0384** | 0.0114 | **0.9487** | 0.9018 | 0.2683 |\n| aMUSEd (Patil et al., 2024) | 0.0629 | 0.0156 | 0.9230 | 0.8919 | **0.2732** |\n| Meissonic (Bai et al., 2024) | 0.0577 | 0.0145 | 0.9430 | **0.9126** | 0.2677 |\n| *HumanEdit-Counting* |\n| InstructPix2Pix (Brooks et al., 2023) | 0.1628 | 0.0586 | 0.8124 | 0.5850 | 0.2716 |\n| MGIE (Fu et al., 2023) | 0.1380 | 0.0641 | 0.8726 | 0.6971 | 0.2716 |\n| HIVE SD1.5 (Zhang et al., 2024b) | 0.1211 | 0.0442 | 0.8826 | 0.7431 | 0.2705 |\n| MagicBrush (Zhang et al., 2024a) | 0.1058 | 0.0434 | 0.8677 | 0.7103 | 0.2707 |\n| Blended Latent Diff. SDXL (Avrahami et al., 2023) | 0.0527 | 0.0180 | 0.9334 | 0.8892 | 0.2766 |\n| GLIDE (Nichol et al., 2021) | **0.0392** | **0.0127** | **0.9523** | **0.9104** | 0.2772 |\n| aMUSEd (Patil et al., 2024) | 0.0699 | 0.0213 | 0.9270 | 0.8816 | **0.2814** |\n| Meissonic (Bai et al., 2024) | 0.0674 | 0.0217 | 0.9394 | 0.8967 | 0.2750 |\n| *HumanEdit-Remove* |\n| InstructPix2Pix (Brooks et al., 2023) | 0.1624 | 0.0504 | 0.7240 | 0.4188 | 0.2325 |\n| MGIE (Fu et al., 2023) | 0.1259 | 0.0572 | 0.8677 | 0.7235 | 0.2525 |\n| HIVE SD1.5 (Zhang et al., 2024b) | 0.1179 | 0.0375 | 0.8362 | 0.6562 | 0.2474 |\n| MagicBrush (Zhang et al., 2024a) | 0.0690 | 0.0232 | 0.8985 | 0.8249 | 0.2572 |\n| Blended Latent Diff. SDXL (Avrahami et al., 2023) | 0.0451 | 0.0133 | 0.9055 | 0.8322 | 0.2608 |\n| GLIDE (Nichol et al., 2021) | **0.0313** | **0.0072** | **0.9493** | **0.9119** | 0.2661 |\n| aMUSEd (Patil et al., 2024) | 0.0621 | 0.0156 | 0.9148 | 0.8702 | **0.2715** |\n| Meissonic (Bai et al., 2024) | 0.0557 | 0.0132 | 0.9367 | 0.9048 | 0.2673 |\n| *HumanEdit-Relation* |\n| InstructPix2Pix (Brooks et al., 2023) | 0.1741 | 0.0647 | 0.8069 | 0.5851 | 0.2828 |\n| MGIE (Fu et al., 2023) | 0.1420 | 0.0656 | 0.8762 | 0.7061 | 0.2768 |\n| HIVE SD1.5 (Zhang et al., 2024b) | 0.1298 | 0.0460 | 0.8689 | 0.7005 | 0.2793 |\n| MagicBrush (Zhang et al., 2024a) | 0.0884 | 0.0334 | 0.8985 | 0.7865 | 0.2823 |\n| Blended Latent Diff. SDXL (Avrahami et al., 2023) | 0.0628 | 0.0213 | **0.9190** | **0.8174** | 0.2832 |\n| GLIDE (Nichol et al., 2021) | **0.0553** | **0.0192** | 0.9136 | 0.7983 | 0.2755 |\n| aMUSEd (Patil et al., 2024) | 0.0809 | 0.0267 | 0.9076 | 0.8095 | **0.2862** |\n| Meissonic (Bai et al., 2024) | 0.0825 | 0.0283 | 0.9171 | 0.8142 | 0.2768 |\n| *HumanEdit-Replace* |\n| InstructPix2Pix (Brooks et al., 2023) | 0.1910 | 0.0770 | 0.7887 | 0.5692 | 0.2697 |\n| MGIE (Fu et al., 2023) | 0.1391 | 0.0620 | 0.8603 | 0.6946 | 0.2698 |\n| HIVE SD1.5 (Zhang et al., 2024b) | 0.1265 | 0.0443 | 0.8582 | 0.7087 | 0.2726 |\n| MagicBrush (Zhang et al., 2024a) | 0.0984 | 0.0409 | 0.8757 | 0.7513 | 0.2716 |\n| Blended Latent Diff. SDXL (Avrahami et al., 2023) | 0.0567 | 0.0206 | 0.9095 | 0.8096 | 0.2683 |\n| GLIDE (Nichol et al., 2021) | **0.0495** | **0.0188** | 0.9194 | 0.8247 | 0.2663 |\n| aMUSEd (Patil et al., 2024) | 0.0761 | 0.0237 | 0.9072 | 0.8259 | **0.2861** |\n| Meissonic (Bai et al., 2024) | 0.0710 | 0.0227 | **0.9239** | **0.8462** | 0.2762 |", "caption": "Table 5: Quantitative study on six different types of editing instructions on HumanEdit. The best results are marked in bold.", "description": "Table 5 presents a quantitative analysis of six distinct image editing instruction types within the HumanEdit dataset.  The table compares the performance of various baselines (InstructPix2Pix, MGIE, HIVE SD1.5, MagicBrush, Blended Latent Diffusion SDXL, GLIDE, aMUSEd, and Meissonic) across these instructions.  Evaluation metrics include L1 and L2 distance (measuring pixel-level differences), CLIP-I and DINO scores (assessing image quality), and CLIP-T scores (evaluating text-image alignment). The best-performing method for each metric and instruction type is highlighted in bold, enabling a nuanced comparison of model capabilities across various editing tasks. ", "section": "4 HI-EDIT Benchmark"}]