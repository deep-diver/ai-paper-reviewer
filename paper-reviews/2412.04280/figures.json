[{"figure_path": "https://arxiv.org/html/2412.04280/x1.png", "caption": "Figure 1: Data examples of instruction-guided image editing in HumanEdit. Our dataset encompasses six distinct editing categories. In the images, gray shapes represent masks, which are provided for every photograph. Moreover, approximately half of the dataset includes instructions that are sufficiently detailed to enable editing without masks. It is important to note that, for conciseness, masks are depicted directly on the original images within this paper; however, in the dataset, the original images and masks are stored separately.", "description": "This figure showcases examples from the HumanEdit dataset, illustrating the six different image editing categories it covers: Action, Add, Counting, Relation, Remove, and Replace.  Each example shows an original image with an overlaid gray mask indicating the area to be modified.  The corresponding instruction specifies the desired edit. Importantly, the caption highlights that while masks are shown here for clarity, approximately half of the dataset's instructions are detailed enough for editing without explicit masks.  In the dataset itself, images and their respective masks are stored as separate files.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2412.04280/x2.png", "caption": "Figure 2: Overview of data collection process.", "description": "This figure illustrates the four-stage data collection process used to create the HumanEdit dataset.  Stage 1 involves training and selecting annotators through a tutorial and quiz. Stage 2 focuses on selecting and quality-checking images sourced from Unsplash.  In Stage 3, annotators create editing instructions, masks (where applicable), and generate edited images.  Finally, Stage 4 includes a two-tier quality review and feedback process to ensure accuracy and consistency, resulting in the final HumanEdit dataset.", "section": "2 Dataset Annotation Pipeline"}, {"figure_path": "https://arxiv.org/html/2412.04280/x7.png", "caption": "Figure 3: \nMore examples of instruction-guided image editing in HumanEdit.", "description": "Figure 3 presents additional examples showcasing the diverse capabilities of HumanEdit for instruction-guided image editing.  It visually demonstrates the wide range of edits the dataset encompasses, including adding, removing, replacing, changing actions of objects, counting and modifying relationships between objects within images. These examples highlight the realism and complexity of edits possible using the HumanEdit dataset, illustrating the detailed and nuanced instructions paired with the images.", "section": "Dataset Statistics"}, {"figure_path": "https://arxiv.org/html/2412.04280/x8.png", "caption": "Figure 4: \n(a) The distribution chart of the first 30 objects in the editing instructions for HumanEdit.\n(b) The word cloud representation of the objects present in the editing instructions for HumanEdit.", "description": "Figure 4 presents a dual visualization of the objects described within the editing instructions of the HumanEdit dataset.  Panel (a) shows a distribution chart of the top 30 most frequent objects mentioned in these instructions, providing a quantitative view of object prevalence.  Panel (b) offers a word cloud representation of all objects mentioned in the editing instructions, providing a qualitative and visual representation of the dataset's object diversity and the relative frequency of object mentions. Together, the two panels offer insights into the types and frequency of objects involved in the image editing tasks of HumanEdit.", "section": "3 Dataset Statistics"}, {"figure_path": "https://arxiv.org/html/2412.04280/x9.png", "caption": "Figure 5: The river chart of HumanEdit-full. The first node of the river represents the type of edit, the second node corresponds to the verb extracted from the instruction, and the final node corresponds to the noun in the instruction. To maintain clarity, we only selected the top 50 most frequent nouns. The river chart of HumanEdit-core can be seen in Figure\u00a011 in Appendix.", "description": "This river chart visualizes the relationships between different aspects of image editing instructions within the HumanEdit-full dataset.  It shows three levels of detail: the type of edit (e.g., Add, Remove, Replace), the verb used in the instruction (e.g., add, remove, change), and the noun representing the object of the edit (e.g., flower, tree, person). The chart's structure reveals common patterns in how users phrase editing instructions, highlighting frequently occurring combinations of edit type, verb, and noun. Only the top 50 most frequent nouns are included for clarity. A similar chart for the HumanEdit-core dataset is found in Figure 11 of the Appendix.", "section": "Dataset Statistics"}, {"figure_path": "https://arxiv.org/html/2412.04280/x10.png", "caption": "Figure 6: \n(a) The distribution of images for which HumanEdit requires masking, where no need for mask refers to editing instructions that are already clear and comprehensive enough, and we believe that no masking is necessary for the model to complete the editing.\n(b) The distribution of the sources of all input images for HumanEdit.\n(c) The distribution of resolutions for all input images in HumanEdit.\n(d) The distribution of resolutions for all input images in MagicBrush.", "description": "Figure 6 presents a comprehensive analysis of HumanEdit and MagicBrush datasets through four subfigures:\n(a) Compares the necessity of masks in HumanEdit, showing a significant portion (53.1%) of instructions are detailed enough to allow mask-free editing.\n(b) Shows the diverse sources of images for HumanEdit, highlighting the predominance of Unsplash images.\n(c) Presents a distribution analysis of the resolutions of images within HumanEdit, indicating a majority (62.3%) possess resolutions above 1000 pixels.\n(d) Provides a comparative resolution analysis between HumanEdit and MagicBrush datasets, illustrating the superior resolution of HumanEdit.", "section": "3 Dataset Statistics"}, {"figure_path": "https://arxiv.org/html/2412.04280/x11.png", "caption": "Figure 7: Qualitative comparisons between mask-provided baselines. The first three rows show the original images, corresponding masks, and ground truth edited images from DALL-E 2. The subsequent four rows present results generated by Blended Latent Diffusion SDXL, GLIDE, aMUSEd, and Meissonic, respectively.", "description": "Figure 7 presents a qualitative comparison of different image editing models' performance on a masked image editing task.  The top three rows display the original image, the corresponding mask (the region to be edited), and the ground truth (desired) edited image created using DALL-E 2. The four rows below show the results obtained from four other state-of-the-art models: Blended Latent Diffusion SDXL, GLIDE, aMUSEd, and Meissonic. This allows for a visual assessment of each model's ability to accurately and realistically modify the masked area of the image, based on implicit instructions.", "section": "4 HI-EDIT Benchmark"}, {"figure_path": "https://arxiv.org/html/2412.04280/x12.png", "caption": "Figure 8: Qualitative comparisons between mask-provided baselines. The first three rows show the original images, corresponding masks, and ground truth edited images from DALL-E 2. The subsequent four rows present results generated by Blended Latent Diffusion SDXL, GLIDE, aMUSEd, and Meissonic, respectively.", "description": "Figure 8 presents a qualitative comparison of different image editing models on a subset of the HumanEdit dataset.  The top three rows display example images from the dataset: the original image, the corresponding mask indicating the region to be edited, and the ground truth result produced using DALL-E 2.  The following four rows showcase how four other models, specifically Blended Latent Diffusion SDXL, GLIDE, aMUSEd, and Meissonic, performed on the same image editing tasks. This visual comparison highlights the strengths and weaknesses of each model in terms of accurately implementing the editing instructions and producing visually appealing results.", "section": "4 HI-EDIT Benchmark"}, {"figure_path": "https://arxiv.org/html/2412.04280/x13.png", "caption": "Figure 9: An Overview of Keywords in HumanEdit-core Edit Instructions: The inner circle represents the verb in the edit instruction, while the outer circle illustrates the noun following the verb in each instruction.", "description": "This figure is a sunburst chart visualizing the keywords used in the instructions of the HumanEdit-core dataset. The inner ring represents the action verbs used (e.g., 'add', 'remove', 'change'), and the outer ring displays the nouns (objects) associated with each verb. This hierarchical structure helps demonstrate the relationship between the actions and objects in the dataset's instruction set, providing insights into the types of editing operations and target elements prevalent within HumanEdit-core.", "section": "3 Dataset Statistics"}, {"figure_path": "https://arxiv.org/html/2412.04280/x14.png", "caption": "Figure 10: An Overview of Keywords in HumanEdit-full Edit Instructions: The inner circle represents the verb in the edit instruction, while the outer circle highlights the noun associated with the verb in each instruction.", "description": "This figure is a sunburst chart visualizing the keywords used in the instructions of the HumanEdit-full dataset.  The inner circle represents the verb (action word) from each instruction (e.g., \"add\", \"remove\", \"replace\"). The outer ring displays the nouns (objects) most frequently associated with those verbs. This provides a visual representation of the types of image editing operations included in the HumanEdit-full dataset and their object focus.  It shows the relationship between verbs and nouns within image editing instruction.", "section": "3 Dataset Statistics"}, {"figure_path": "https://arxiv.org/html/2412.04280/x15.png", "caption": "Figure 11: The river chart of HumanEdit-core. The first node of the river represents the type of edit, the second node corresponds to the verb extracted from the instruction, and the final node corresponds to the noun in the instruction. To maintain clarity, we only selected the top 50 most frequent nouns.", "description": "This river chart visualizes the relationships between different aspects of the editing instructions in the HumanEdit-core dataset.  It shows the flow from the type of edit (e.g., Add, Remove, Replace) to the verb used in the instruction (e.g., add, remove, change), and finally to the most frequent nouns involved in the instruction (e.g., flower, person, car). This hierarchical representation helps to understand the distribution and common patterns in the image editing tasks described within the HumanEdit-core dataset. Only the top 50 most frequent nouns are included for clarity.", "section": "3 Dataset Statistics"}, {"figure_path": "https://arxiv.org/html/2412.04280/x16.png", "caption": "Figure 12: Case of Object Removal.", "description": "This figure shows an example of object removal from an image in the HumanEdit dataset.  The original image contains multiple objects, including a yellow tent and trees.  The mask highlights the area where the tent is located. The output image shows the tent has been successfully removed leaving only the background and trees.", "section": "2 Dataset Annotation Pipeline"}, {"figure_path": "https://arxiv.org/html/2412.04280/x17.png", "caption": "Figure 13: Object Replacement Example I.", "description": "This figure shows an example of object replacement in image editing. The original image contains a wooden table with various items on it.  The editing instruction is to replace the table with a white puppy.  The mask highlights the area of the table. The output image shows a white puppy sitting where the table was, with the surrounding environment remaining the same.", "section": "B.1 Edit Cases for Annotators"}, {"figure_path": "https://arxiv.org/html/2412.04280/x18.png", "caption": "Figure 14: Object Replacement Example II.", "description": "This figure shows an example of object replacement in image editing. The original image shows a bathroom with a curved window above the bathtub.  The mask highlights the window area for modification. The edited image replaces the curved window with a square window, demonstrating the successful application of the object replacement technique.", "section": "B.1 Edit Cases for Annotators"}, {"figure_path": "https://arxiv.org/html/2412.04280/x19.png", "caption": "Figure 15: Case of Object Addition.", "description": "This figure shows an example of the 'Object Addition' category from the HumanEdit dataset. It illustrates how a new object (a mobile phone) is added to an existing image, demonstrating a typical instruction-guided image editing task included in the dataset.", "section": "B.1 Edit Cases for Annotators"}, {"figure_path": "https://arxiv.org/html/2412.04280/x20.png", "caption": "Figure 16: Case of Object Counting Change.", "description": "This figure shows an example of object counting change in the HumanEdit dataset.  The original image contains one object, and the edited image contains four objects. This illustrates the type of editing task involving modifying the number of objects in a scene. The task involves increasing or decreasing the count of objects already present in the image, not adding objects that weren't originally there, or removing them entirely (those would be classified as 'add' and 'remove' respectively). The number of objects cannot be changed from zero to a non-zero number or vice-versa. ", "section": "2 Dataset Annotation Pipeline"}, {"figure_path": "https://arxiv.org/html/2412.04280/x21.png", "caption": "Figure 17: Case of Action Change.", "description": "This figure shows an example of an image editing task where an action is changed. The original image shows an elephant. The mask highlights the elephant's trunk. The edited image shows the elephant with its trunk raised.", "section": "B.1 Edit Cases for Annotators"}, {"figure_path": "https://arxiv.org/html/2412.04280/extracted/6044538/images/app/note1.png", "caption": "Figure 18: Case of Relation Change.", "description": "This figure shows an example of relation change in image editing.  The original image shows a cloth basket with gloves on the right and other items on the left. The edited image shows the same items but with the gloves moved from the right side to the left side of the basket, demonstrating a change in spatial relationship between objects.", "section": "B.1 Edit Cases for Annotators"}, {"figure_path": "https://arxiv.org/html/2412.04280/extracted/6044538/images/app/note2.png", "caption": "Figure 19: An Example of Prompt Word Selection", "description": "This figure shows an example of how to choose effective prompt words when using the DALL-E 2 platform for image editing. The top row displays various prompt word combinations, while the bottom row shows the images generated based on each prompt. The different images generated illustrate how different word choices can lead to different results.  This highlights the importance of careful prompt selection for achieving desirable image editing outcomes.", "section": "B Guidance Book for Annotators"}, {"figure_path": "https://arxiv.org/html/2412.04280/extracted/6044538/images/app/note3.png", "caption": "Figure 20: An Example of Prompt Word Selection", "description": "Figure 20 shows an example of selecting appropriate prompt words when using the DALL-E 2 platform for image editing.  The image demonstrates the importance of using detailed and descriptive prompts to ensure that the model generates high-quality results that match the user's intent. Using vague or incomplete prompts can lead to undesired or inaccurate edits. The figure showcases different word choices, highlighting how the selection impacts the generated images.", "section": "B Guidance Book for Annotators"}, {"figure_path": "https://arxiv.org/html/2412.04280/extracted/6044538/images/app/note4.png", "caption": "Figure 21: Performing a Crop Operation on the DALL-E 2 Platform.", "description": "This figure shows a screenshot of the DALL-E 2 platform's interface.  The user is in the process of cropping an image before initiating an image editing task.  The screenshot highlights the cropping tools and options available within the platform, illustrating one step in the image preparation workflow for instruction-guided image editing.", "section": "Dataset Annotation Pipeline"}, {"figure_path": "https://arxiv.org/html/2412.04280/extracted/6044538/images/app/note5.png", "caption": "Figure 22: Performing an Editing Operation on the DALL-E 2 Platform.", "description": "This figure shows a screenshot of the DALL-E 2 platform interface during an image editing operation.  The user has selected an image and is presented with options to \"Edit or generate similar images.\"  The screenshot demonstrates the steps involved in using the platform for image editing, specifically highlighting the \"Edit Image\" option.", "section": "Dataset Annotation Pipeline"}, {"figure_path": "https://arxiv.org/html/2412.04280/extracted/6044538/images/app/note6.png", "caption": "Figure 23: An Illustration of Avoiding Edits in Irrelevant Areas.", "description": "The figure illustrates a common mistake in image editing where the mask used for editing is too large, encompassing unintended areas.  This leads to undesirable and unintended edits. In this example, a mask used to alter a section of a photograph inadvertently removes a portion of a boat's paddle.  The instruction for this task is implied to only be applied to the people in the boat.", "section": "B.2 Notes for Annotators"}, {"figure_path": "https://arxiv.org/html/2412.04280/extracted/6044538/images/app/note7.png", "caption": "Figure 24: An Illustration of Avoiding Edits in Irrelevant Areas.", "description": "This figure illustrates a common mistake in image editing tasks.  The instruction is to add a giraffe to the scene, but the mask is too large, encompassing areas unrelated to the addition. This results in unwanted edits to parts of the image besides the desired addition of a giraffe. In particular, the generated giraffe\u2019s head appears unnatural. The example highlights the importance of using precise masks that only target the intended editing area to prevent unexpected artifacts in the final image.", "section": "B.2 Notes for Annotators"}, {"figure_path": "https://arxiv.org/html/2412.04280/extracted/6044538/images/app/note8.png", "caption": "Figure 25: An Illustration of Avoiding Edits in Irrelevant Areas.", "description": "The figure demonstrates a common mistake during image editing: selecting too large of a mask area.  The instruction is to remove a person, but the selected mask includes a car. This highlights the importance of precise mask selection to avoid unwanted edits to irrelevant parts of the image. The example illustrates the effect of an improperly placed or sized mask in the DALL-E2 platform and the resulting unwanted changes to parts of the image outside the target edit area.", "section": "B.2 Notes for Annotators"}, {"figure_path": "https://arxiv.org/html/2412.04280/extracted/6044538/images/app/note9.png", "caption": "Figure 26: An Illustration of Avoiding Edits in Irrelevant Areas.", "description": "Figure 26 shows an example of proper masking in image editing.  The instruction is to change the background of an image featuring a dog. The annotator correctly masked all areas except the dog itself, ensuring that only the background is altered during the editing process. This demonstrates how a properly masked area prevents unintended changes to the main subject of the image, resulting in more precise and accurate editing.", "section": "B.2 Notes for Annotators"}, {"figure_path": "https://arxiv.org/html/2412.04280/extracted/6044538/images/app/note10.png", "caption": "Figure 27: A Case for Ensuring Edit Quality.", "description": "The figure shows an example where DALL-E 2 struggles to accurately interpret the instruction to edit an image.  Specifically, it shows an image where the instruction was to edit the hand in a tennis player's image to be outstretched, but instead, DALL-E 2 distorted the fingers making it look unnatural and unrealistic. This highlights the importance of attention to detail in editing instructions to obtain high-quality edits.", "section": "B.2 Notes for Annotators"}, {"figure_path": "https://arxiv.org/html/2412.04280/extracted/6044538/images/app/note11.png", "caption": "Figure 28: A Case for Ensuring Edit Quality.", "description": "The image shows a case where DALL-E 2 fails to accurately interpret editing instructions, resulting in an unrealistic outcome.  The instruction intended to remove a person from the scene, but instead part of the car's door also disappeared, highlighting a limitation of the model in precise object removal and maintaining realistic scene context.", "section": "B.2 Notes for Annotators"}, {"figure_path": "https://arxiv.org/html/2412.04280/extracted/6044538/images/app/note12.png", "caption": "Figure 29: A Case for Ensuring Edit Quality.", "description": "The image shows a failure case from the HumanEdit dataset. The instruction was to remove a person from the scene, but DALL-E 2 also removed part of a car, resulting in an unrealistic and inconsistent edit.  This highlights the challenges in ensuring high-quality results with instruction-based image editing models, and that excessive masking can lead to undesirable artifacts.", "section": "B.2 Notes for Annotators"}, {"figure_path": "https://arxiv.org/html/2412.04280/extracted/6044538/images/app/note13.png", "caption": "Figure 30: An Illustration of Consistency in Style Before and After Editing.", "description": "Figure 30 demonstrates the importance of maintaining stylistic consistency between the original image and its edited version.  The original image is in black and white, showing a woman in a dark dress. The successful edits retain this black and white aesthetic, and changes are made without introducing color.  Unsuccessful edits (marked with an X) fail to preserve the original style, resulting in a colorized or otherwise altered image. This highlights a key quality control aspect of the HumanEdit dataset, ensuring the edited images align with the original style to maintain data quality and consistency.", "section": "B.3 Initial Image Selection"}, {"figure_path": "https://arxiv.org/html/2412.04280/extracted/6044538/images/app/pipe1.png", "caption": "Figure 31: Examples of valid and invalid images. The first image is valid, while the following three images are invalid.", "description": "Figure 31 showcases examples of images deemed valid or invalid for inclusion in the HumanEdit dataset.  The first image is considered valid due to meeting the quality and resolution standards required by the HumanEdit dataset annotation pipeline. In contrast, the remaining three images are deemed invalid. Image (b) contains unusual artifacts, image (c) has poor image quality, and image (d) displays low resolution and lacks sufficient visual information; these flaws render them unsuitable for HumanEdit.", "section": "B.3 Initial Image Selection"}, {"figure_path": "https://arxiv.org/html/2412.04280/extracted/6044538/images/app/pipe2.png", "caption": "Figure 32: Log in to the DALL\u00b7E 2 platform and click \"Try DALL-E\" to upload an image.", "description": "This figure shows a screenshot of the DALL-E 2 platform's interface.  The user is presented with options to either start with a detailed description or upload an image.  To proceed with uploading an image, the user should click the \"Try DALL-E\" button. This step is the initial stage of using the DALL-E 2 platform for image editing within the HumanEdit dataset creation process.", "section": "B.4 Image Editing Process and Annotation Platform"}, {"figure_path": "https://arxiv.org/html/2412.04280/extracted/6044538/images/app/pipe3.png", "caption": "Figure 33: After uploading the image, a cropping page will be displayed.", "description": "This figure shows a screenshot of the DALL-E 2 platform's interface after an image has been uploaded.  The user is presented with a cropping tool to select the desired area of the image for editing. This step is crucial because it allows the user to focus on the specific part of the image they want to modify, ensuring precise and effective edits with the DALL-E 2's editing tools. The cropping stage refines the area of interest prior to applying editing instructions or generating variations.", "section": "Dataset Annotation Pipeline"}, {"figure_path": "https://arxiv.org/html/2412.04280/extracted/6044538/images/app/pipe4.png", "caption": "Figure 34: Click the \"Edit\" button to enter the editing window.", "description": "This figure shows a screenshot of the DALL-E 2 platform's editing interface.  After uploading an image and performing a crop, the user clicks the \"Edit\" button to proceed to the next stage of the image editing process, where they can provide editing instructions and a mask (if needed).", "section": "Dataset Annotation Pipeline"}, {"figure_path": "https://arxiv.org/html/2412.04280/extracted/6044538/images/app/pipe5.png", "caption": "Figure 35: Drag the editing points to select the area to be edited.", "description": "This figure shows a screenshot of the DALL-E 2 platform's image editing interface.  The user is in the process of selecting a region of the image to edit.  Using a rectangular selection tool, the user is dragging the selection points to define the area that will be the focus of subsequent editing instructions.", "section": "B.4 Image Editing Process and Annotation Platform"}, {"figure_path": "https://arxiv.org/html/2412.04280/extracted/6044538/images/app/pipe6.png", "caption": "Figure 36: Input the editing instructions in the text bo.", "description": "This figure shows a screenshot of the DALL-E 2 platform's image editing interface.  The user is in the process of adding instructions for an image edit. A mask has already been applied to a region of the image.  The user is about to type detailed instructions into the text box to guide the AI in creating the desired edit.  The interface includes options for selecting the editing area using a mask and inputting the desired edits through natural language.", "section": "B.4 Image Editing Process and Annotation Platform"}, {"figure_path": "https://arxiv.org/html/2412.04280/extracted/6044538/images/app/pipe7.png", "caption": "Figure 37: Generate edited images.", "description": "This figure shows the results of using the DALL-E 2 platform to generate edited images based on user-provided instructions and mask.  The user has input an instruction and selected an area of the image to modify. The platform then generates several versions of the edited image, showcasing the capabilities of the model to make changes according to user specifications.", "section": "B.4 Image Editing Process and Annotation Platform"}, {"figure_path": "https://arxiv.org/html/2412.04280/extracted/6044538/images/app/pipe8.png", "caption": "Figure 38: Regenerate edited images.", "description": "This figure shows the results of regenerating edited images using the DALL-E 2 platform.  The initial attempt at image editing yielded unsatisfactory results, so the user employed the platform's \"regenerate\" function to produce alternative versions.  The images illustrate the iterative process of refining image edits, where the original prompt may need to be modified or made more precise to achieve desired results.", "section": "B.4 Image Editing Process and Annotation Platform"}, {"figure_path": "https://arxiv.org/html/2412.04280/extracted/6044538/images/app/pipe9.png", "caption": "Figure 39: Regenerated images are still not satisfactory and may require revised instructions.", "description": "The figure displays multiple image generation attempts from the DALL-E 2 platform.  The user is trying to create an image of a girl with four parrots perched on her.  Despite multiple attempts using the 'regenerate' function, the results are still unsatisfactory and fall short of meeting the specified requirements. The parrots' positioning and overall image quality need improvement, illustrating that even with a detailed description, generating a precisely desired image using AI can be challenging and require careful refinement of instructions.", "section": "B.4 Image Editing Process and Annotation Platform"}, {"figure_path": "https://arxiv.org/html/2412.04280/extracted/6044538/images/app/pipe10.png", "caption": "Figure 40: Download and finish the editing process.", "description": "This figure shows the final step of the image editing process using the DALL-E 2 platform.  After generating multiple edited images based on the user's instructions, the user selects the most satisfactory result and clicks the download button.  This action concludes the annotation task for that particular image.", "section": "B.4 Image Editing Process and Annotation Platform"}, {"figure_path": "https://arxiv.org/html/2412.04280/extracted/6044538/images/app/pipe11.png", "caption": "Figure 41: Defective Image Example.", "description": "Figure 41 showcases examples of unsatisfactory image editing results generated by DALL-E 2. These examples highlight common issues such as a parrot incorrectly perched on another parrot, facial distortions, and an unrealistic depiction of a blue parrot seemingly suspended in mid-air. These examples are used to illustrate common errors that should be avoided during the image editing and annotation process to maintain high data quality.", "section": "B.1 Edit Cases for Annotators"}, {"figure_path": "https://arxiv.org/html/2412.04280/extracted/6044538/images/app/fail1.png", "caption": "Figure 42: Submission Example.", "description": "Figure 42 shows an example of the materials that annotators need to submit as a group to the platform.  It includes four columns: the original image before editing, the mask used during editing, the editing instruction provided, the expected image description, and the final edited image.", "section": "B.2 Notes for Annotators"}, {"figure_path": "https://arxiv.org/html/2412.04280/extracted/6044538/images/app/fail2.png", "caption": "Figure 43: An Illustration of the Mismatch Between Editing Results and Instructions.", "description": "The figure showcases an example where the instruction given to DALL-E 2 was to \"make the nose larger.\" However, the model failed to apply this modification to the image.  This demonstrates a case of mismatch between the intended editing outcome (as described in the instruction) and the actual result produced by the AI model.  Such discrepancies highlight the limitations of current image editing AI models in accurately interpreting and implementing specific instructions for image manipulation.", "section": "C Failure Cases (not included in HumanEdit)"}, {"figure_path": "https://arxiv.org/html/2412.04280/extracted/6044538/images/app/fail3.png", "caption": "Figure 44: An Illustration of the Mismatch Between Editing Results and Instructions.", "description": "The figure shows an example where the instruction \"a lantern hanging in front of the window\" was given to DALL-E 2.  However, instead of adding a lantern, DALL-E 2 simply removed the existing object without replacing it with a lantern, demonstrating a mismatch between the intended editing and the actual result. This highlights a limitation of DALL-E 2's editing capabilities where the model sometimes fails to follow the instructions accurately.", "section": "C Failure Cases (not included in HumanEdit)"}, {"figure_path": "https://arxiv.org/html/2412.04280/extracted/6044538/images/app/fail4.png", "caption": "Figure 45: An Illustration of the Mismatch Between Editing Results and Instructions.", "description": "This figure shows an example where the instruction given to DALL-E 2 was to generate an image containing 'a plate of cucumbers and a bouquet of roses'.  However, the generated image only includes cucumbers; the roses, as specified in the instruction, are missing. This illustrates a mismatch between the user's instruction and the model's output, highlighting a limitation of DALL-E 2's ability to reliably fulfill complex image editing requests.", "section": "C Failure Cases (not included in HumanEdit)"}, {"figure_path": "https://arxiv.org/html/2412.04280/extracted/6044538/images/app/fail11.png", "caption": "Figure 46: An Illustration of the Limited Editing Capabilities for Specific Types.", "description": "This figure shows an example where DALL-E 2 struggles with a specific type of editing task, highlighting its limitations. The instruction was to have a girl standing on tiptoe. Despite numerous attempts, the model failed to accurately achieve the desired effect, illustrating the limitations of the model in handling subtle or complex actions related to human poses.  This showcases a failure case from the dataset where such a result would have been excluded because it didn't meet quality standards.", "section": "C Failure Cases (not included in HumanEdit)"}, {"figure_path": "https://arxiv.org/html/2412.04280/extracted/6044538/images/app/fail5.png", "caption": "Figure 47: An Illustration of the Limited Editing Capabilities for Specific Types.", "description": "This figure shows several attempts to edit an image of an owl's face, specifically aiming to make the owl close its eyes. Despite numerous attempts using the DALL-E 2 model, the results were inconsistent and unsuccessful in achieving the desired effect, consistently altering the state of the owl's eyes without achieving the objective of closing them. This highlights the limited editing capabilities of the model for specific types of tasks or images.", "section": "C Failure Cases (not included in HumanEdit)"}, {"figure_path": "https://arxiv.org/html/2412.04280/extracted/6044538/images/app/fail10.png", "caption": "Figure 48: An Illustration of the Limited Editing Capabilities for Specific Types.", "description": "This figure illustrates the limitations of DALL-E 2 in performing specific types of image editing tasks.  Specifically, it shows examples where the instruction was to add a red barbell, yet the model struggled to add the barbell accurately, either omitting the object or decreasing the number of objects present.  This demonstrates the model's insensitivity to object count and its limitations in reliably executing addition-based edit instructions.", "section": "C Failure Cases (not included in HumanEdit)"}, {"figure_path": "https://arxiv.org/html/2412.04280/extracted/6044538/images/app/fail12.png", "caption": "Figure 49: An Illustration of the Limited Editing Capabilities for Specific Types.", "description": "This figure showcases examples where DALL-E 2 struggles with specific types of editing tasks.  It demonstrates limitations in its ability to precisely manipulate image details according to nuanced instructions. The examples highlight difficulties in tasks such as subtly shifting object positions or modifying counts of items.  It visually reinforces the limitations of current image editing models, especially when dealing with fine-grained control over image content.", "section": "C Failure Cases (not included in HumanEdit)"}, {"figure_path": "https://arxiv.org/html/2412.04280/extracted/6044538/images/app/fail13.png", "caption": "Figure 50: An Illustration of the Limited Editing Capabilities for Specific Types.", "description": "Figure 50 shows an example where DALL-E 2 struggles with a specific type of editing task.  The instruction likely involved a subtle change to an object's characteristics or its relation to other elements in the image. The generated images demonstrate DALL-E 2's inability to consistently and accurately perform this kind of fine-grained edit. This highlights the inherent limitations of current image editing models in handling certain types of instructions.", "section": "C Failure Cases (not included in HumanEdit)"}, {"figure_path": "https://arxiv.org/html/2412.04280/extracted/6044538/images/app/fail14.png", "caption": "Figure 51: An Illustration of the Limited Editing Capabilities for Specific Types.", "description": "Figure 51 shows several examples where DALL-E 2 struggles to perform edits, specifically focusing on limitations with editing certain types of content.  The examples highlight the model's inconsistencies and challenges in achieving the desired edits precisely, especially when dealing with complex or nuanced instructions. These examples are part of the failure cases presented in the paper, showing limitations of the DALL-E 2 model that were not included in the final HumanEdit dataset.", "section": "C Failure Cases (not included in HumanEdit)"}, {"figure_path": "https://arxiv.org/html/2412.04280/extracted/6044538/images/app/fail6.png", "caption": "Figure 52: An Illustration of the Limited Editing Capabilities for Specific Types.", "description": "Figure 52 shows examples where DALL-E 2 struggles to perform nuanced edits, specifically highlighting its limitations in handling certain types of content.  The model's ability to accurately and consistently modify aspects of an image is inconsistent in this case.  This illustrates that while DALL-E 2 may handle simpler editing tasks well, more complex or subtle instructions present significant challenges. The figure serves as an example of inherent limitations in the model's capabilities that would not make these images suitable for inclusion in the dataset.", "section": "C Failure Cases (not included in HumanEdit)"}, {"figure_path": "https://arxiv.org/html/2412.04280/extracted/6044538/images/app/fail7.png", "caption": "Figure 53: An example of object distortion.", "description": "In Figure 53, the generated image from DALL-E 2 exhibits distortion in the flower's petals.  The editing instruction likely aimed to add or modify a pattern on the flower, but the execution resulted in an unnatural, warped appearance of the flower petals, demonstrating limitations in the model's ability to precisely execute fine-grained image manipulations.", "section": "C.2 Editing Errors"}, {"figure_path": "https://arxiv.org/html/2412.04280/extracted/6044538/images/app/fail8.png", "caption": "Figure 54: The discrepancy between the instruction and the generated image.", "description": "This figure in the HumanEdit paper showcases a failure case where the DALL-E 2 model did not correctly implement the given instruction. The instruction was to add a printed pattern to the image, however, the model failed to do so and produced an image that is identical to the original one.", "section": "C Failure Cases (not included in HumanEdit)"}, {"figure_path": "https://arxiv.org/html/2412.04280/extracted/6044538/images/app/fail9.png", "caption": "Figure 55: An example of subtle editing effects.", "description": "The image showcases a subtle editing effect where the instruction was likely to make the dog's ears stand up.  However, the result shows only a very slight change and may not be noticeable to the casual observer. This exemplifies a limitation of the DALL-E 2 model's ability to execute fine-grained instructions precisely.", "section": "C.2 Editing Errors"}]