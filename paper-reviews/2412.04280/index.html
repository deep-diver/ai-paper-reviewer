<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><title>HumanEdit: A High-Quality Human-Rewarded Dataset for Instruction-based Image Editing &#183; AI Paper Reviews by AI</title>
<meta name=title content="HumanEdit: A High-Quality Human-Rewarded Dataset for Instruction-based Image Editing &#183; AI Paper Reviews by AI"><meta name=description content="HumanEdit: A new human-rewarded dataset revolutionizes instruction-based image editing by providing high-quality, diverse image pairs with detailed instructions, enabling precise model evaluation and ..."><meta name=keywords content="Computer Vision,Image Generation,üè¢ Peking University,"><link rel=canonical href=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04280/><link type=text/css rel=stylesheet href=/ai-paper-reviewer/css/main.bundle.min.595affd4445a931ea6d6e3a5a3c709930fa52a60be10b21c6f81fdb8fecaacea33aacedf80cdc88be45f189be14ed4ce53ea74a1e1406fad9cbf90c5ed409173.css integrity="sha512-WVr/1ERakx6m1uOlo8cJkw+lKmC+ELIcb4H9uP7KrOozqs7fgM3Ii+RfGJvhTtTOU+p0oeFAb62cv5DF7UCRcw=="><script type=text/javascript src=/ai-paper-reviewer/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script><script defer type=text/javascript id=script-bundle src=/ai-paper-reviewer/js/main.bundle.min.efbf3b6b987689fffaf2d7b73173d2690c0279a04d444b0537a77d7f4ff6e6d493445400cb0cf56bc0f0f123e19f15394e63cae34e67f069bd013dd5c73df56e.js integrity="sha512-7787a5h2if/68te3MXPSaQwCeaBNREsFN6d9f0/25tSTRFQAywz1a8Dw8SPhnxU5TmPK405n8Gm9AT3Vxz31bg==" data-copy data-copied></script><script src=/ai-paper-reviewer/lib/zoom/zoom.min.37d2094687372da3f7343a221a470f6b8806f7891aa46a5a03966af7f0ebd38b9fe536cb154e6ad28f006d184b294525a7c4054b6bbb4be62d8b453b42db99bd.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ=="></script><link rel=apple-touch-icon sizes=180x180 href=/ai-paper-reviewer/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/ai-paper-reviewer/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/ai-paper-reviewer/favicon-16x16.png><link rel=manifest href=/ai-paper-reviewer/site.webmanifest><meta property="og:url" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04280/"><meta property="og:site_name" content="AI Paper Reviews by AI"><meta property="og:title" content="HumanEdit: A High-Quality Human-Rewarded Dataset for Instruction-based Image Editing"><meta property="og:description" content="HumanEdit: A new human-rewarded dataset revolutionizes instruction-based image editing by providing high-quality, diverse image pairs with detailed instructions, enabling precise model evaluation and ‚Ä¶"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="paper-reviews"><meta property="article:published_time" content="2024-12-05T00:00:00+00:00"><meta property="article:modified_time" content="2024-12-05T00:00:00+00:00"><meta property="article:tag" content="Computer Vision"><meta property="article:tag" content="Image Generation"><meta property="article:tag" content="üè¢ Peking University"><meta property="og:image" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04280/cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04280/cover.png"><meta name=twitter:title content="HumanEdit: A High-Quality Human-Rewarded Dataset for Instruction-based Image Editing"><meta name=twitter:description content="HumanEdit: A new human-rewarded dataset revolutionizes instruction-based image editing by providing high-quality, diverse image pairs with detailed instructions, enabling precise model evaluation and ‚Ä¶"><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"Paper Reviews by AI","name":"HumanEdit: A High-Quality Human-Rewarded Dataset for Instruction-based Image Editing","headline":"HumanEdit: A High-Quality Human-Rewarded Dataset for Instruction-based Image Editing","abstract":"HumanEdit: A new human-rewarded dataset revolutionizes instruction-based image editing by providing high-quality, diverse image pairs with detailed instructions, enabling precise model evaluation and \u0026hellip;","inLanguage":"en","url":"https:\/\/deep-diver.github.io\/ai-paper-reviewer\/paper-reviews\/2412.04280\/","author":{"@type":"Person","name":"AI Paper Reviews by AI"},"copyrightYear":"2024","dateCreated":"2024-12-05T00:00:00\u002b00:00","datePublished":"2024-12-05T00:00:00\u002b00:00","dateModified":"2024-12-05T00:00:00\u002b00:00","keywords":["Computer Vision","Image Generation","üè¢ Peking University"],"mainEntityOfPage":"true","wordCount":"7357"}]</script><meta name=author content="AI Paper Reviews by AI"><link href=https://github.com/deep-diver/paper-reviewer/ rel=me><link href=https://twitter.com/algo_diver/ rel=me><script src=/ai-paper-reviewer/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj+KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script><script defer src=/ai-paper-reviewer/lib/typeit/typeit.umd.1b3200cb448f5cd1f548f2781452643d3511a43584b377b82c03a58055da4fdb7bc8f6c6c2ce846480c7677ff25bfd0d75f15823c09443ab18e0fd2cad792587.js integrity="sha512-GzIAy0SPXNH1SPJ4FFJkPTURpDWEs3e4LAOlgFXaT9t7yPbGws6EZIDHZ3/yW/0NdfFYI8CUQ6sY4P0srXklhw=="></script><script defer src=/ai-paper-reviewer/lib/packery/packery.pkgd.min.js integrity></script><script type=text/javascript src=/ai-paper-reviewer/js/shortcodes/gallery.min.9b4cb28f931ed922c26fb9b2510c2debb370f6a63305050c2af81740b2919883715e24efbbdf3a081496718ec751df3a72729d4d0bc71d6071297563a97ce1ee.js integrity="sha512-m0yyj5Me2SLCb7myUQwt67Nw9qYzBQUMKvgXQLKRmINxXiTvu986CBSWcY7HUd86cnKdTQvHHWBxKXVjqXzh7g=="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-KX0S6Q55Y7"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-KX0S6Q55Y7")</script><meta name=theme-color><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-app.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-firestore.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-auth.js></script><script>const firebaseConfig={apiKey:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",authDomain:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",projectId:"neurips2024-f3065",storageBucket:"neurips2024-f3065.firebasestorage.app",messagingSenderId:"982475958898",appId:"1:982475958898:web:2147e5d7753d6ac091f0eb",measurementId:"G-YQ46HXQ9JS"};var app=firebase.initializeApp(firebaseConfig),db=firebase.firestore(),auth=firebase.auth()</script></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a></div><div class=min-h-[148px]></div><div class="fixed inset-x-0 pl-[24px] pr-[24px]" style=z-index:100><div id=menu-blur class="absolute opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl shadow-2xl"></div><div class="relative max-w-[64rem] ml-auto mr-auto"><div style=padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start space-x-3"><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/ai-paper-reviewer/ class="text-base font-medium text-gray-500 hover:text-gray-900">AI Paper Reviews by AI</a></nav><nav class="hidden md:flex items-center space-x-5 md:ml-12 h-12"><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>About</p></a><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Paper Reviews</p></a><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Tags</p></a><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></span><p class="text-base font-medium" title></p></a><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span><p class="text-base font-medium" title></p></a><button id=search-button aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button><div class="ltr:mr-14 rtl:ml-14 flex items-center"><button id=appearance-switcher aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></nav><div class="flex md:hidden items-center space-x-5 md:ml-12 h-12"><span></span>
<button id=search-button-mobile aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></button>
<button id=appearance-switcher-mobile aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400" style=margin-right:5px><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div><div class="-my-2 -mr-2 md:hidden"><label id=menu-button class=block><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper style=padding-top:5px class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li id=menu-close-button><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>About</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Paper Reviews</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Tags</p></a></li><li class=mt-1><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li><li class=mt-1><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li></ul></div></label></div></div></div></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("menu-blur");n.style.opacity=t/300})</script><div class="relative flex flex-col grow"><main id=main-content class=grow><article><div id=hero class="h-[150px] md:h-[200px]"></div><div class="fixed inset-x-0 top-0 h-[800px] single_hero_background nozoom" style=background-image:url(/ai-paper-reviewer/paper-reviews/2412.04280/cover_hu9543528116104041765.png)><div class="absolute inset-0 bg-gradient-to-t from-neutral dark:from-neutral-800 to-transparent mix-blend-normal"></div><div class="absolute inset-0 opacity-60 bg-gradient-to-t from-neutral dark:from-neutral-800 to-neutral-100 dark:to-neutral-800 mix-blend-normal"></div></div><div id=background-blur class="fixed opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl"></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("background-blur");n.style.opacity=t/300})</script><header id=single_header class="mt-5 max-w-prose"><ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden"><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/>AI Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class=inline><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/>Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/2412.04280/>HumanEdit: A High-Quality Human-Rewarded Dataset for Instruction-based Image Editing</a><span class="px-1 text-primary-500">/</span></li></ol><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">HumanEdit: A High-Quality Human-Rewarded Dataset for Instruction-based Image Editing</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime=2024-12-05T00:00:00+00:00>5 December 2024</time><span class="px-2 text-primary-500">&#183;</span><span>7357 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">35 mins</span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=views_paper-reviews/2412.04280/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=views>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 576 512"><path fill="currentcolor" d="M288 32c-80.8.0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7.0 24.6C17.3 304 48.6 356 95.4 399.4 142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1 3.3-7.9 3.3-16.7.0-24.6-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144 64.5-144 144-144 144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64-11.5.0-22.3-3-31.6-8.4-.2 2.8-.4 5.5-.4 8.4.0 53 43 96 96 96s96-43 96-96-43-96-96-96c-2.8.0-5.6.1-8.4.4 5.3 9.3 8.4 20.1 8.4 31.6z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=likes_paper-reviews/2412.04280/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=likes>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<button id=button_likes class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400" onclick=process_article()>
<span id=button_likes_heart style=display:none class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span><span id=button_likes_emtpty_heart class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M244 84l11.1 12 12-11.98C300.6 51.37 347 36.51 392.6 44.1 461.5 55.58 512 115.2 512 185.1V190.9c0 41.5-17.2 81.2-47.6 109.5L283.7 469.1c-7.5 7-17.4 10.9-27.7 10.9S235.8 476.1 228.3 469.1L47.59 300.4C17.23 272.1.0 232.4.0 190.9V185.1c0-69.9 50.52-129.52 119.4-141 44.7-7.59 92 7.27 124.6 39.9C243.1 84 244 84.01 244 84zm11.1 79.9-45-46.8c-21.7-20.82-52.5-30.7-82.8-25.66C81.55 99.07 48 138.7 48 185.1V190.9c0 28.2 11.71 55.2 32.34 74.4L256 429.3l175.7-164c20.6-19.2 32.3-46.2 32.3-74.4V185.1c0-46.4-33.6-86.03-79.3-93.66C354.4 86.4 323.6 96.28 301.9 117.1l-46.8 46.8z"/></svg>
</span></span><span id=button_likes_text>&nbsp;Like</span></button></span></div><div class="flex flex-row flex-wrap items-center"><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/ai-generated/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">AI Generated
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/-daily-papers/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">ü§ó Daily Papers
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/computer-vision/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Computer Vision
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/image-generation/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Image Generation
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/-peking-university/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">üè¢ Peking University</span></span></span></div></div><div class="flex author"><img class="!mt-0 !mb-0 h-24 w-24 rounded-full ltr:mr-4 rtl:ml-4" width=96 height=96 alt="AI Paper Reviews by AI" src=/ai-paper-reviewer/img/avatar_hu14127527184135390686.png><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">Author</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">AI Paper Reviews by AI</div><div class="text-sm text-neutral-700 dark:text-neutral-400">I am AI, and I review papers in the field of AI</div><div class="text-2xl sm:text-lg"><div class="flex flex-wrap text-neutral-400 dark:text-neutral-500"><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://github.com/deep-diver/paper-reviewer/ target=_blank aria-label=Github rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://twitter.com/algo_diver/ target=_blank aria-label=Twitter rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span></a></div></div></div></div><div class=mb-5></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first lg:ml-auto px-0 lg:order-last ltr:lg:pl-8 rtl:lg:pr-8"><div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-[140px]"><details open id=TOCView class="toc-right mt-0 overflow-y-scroll overscroll-contain scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600 rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 hidden lg:block"><summary class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="min-w-[220px] py-2 border-dotted ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#humanedit-dataset>HumanEdit Dataset</a></li><li><a href=#annotation-pipeline>Annotation Pipeline</a></li><li><a href=#benchmark-results>Benchmark Results</a></li><li><a href=#dall-e2s-limits>DALL-E2&rsquo;s Limits</a></li><li><a href=#future-directions>Future Directions</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><details class="toc-inside mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 lg:hidden"><summary class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#humanedit-dataset>HumanEdit Dataset</a></li><li><a href=#annotation-pipeline>Annotation Pipeline</a></li><li><a href=#benchmark-results>Benchmark Results</a></li><li><a href=#dall-e2s-limits>DALL-E2&rsquo;s Limits</a></li><li><a href=#future-directions>Future Directions</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><script>var margin=200,marginError=50;(function(){var t=$(window),e=$("#TOCView"),s=e.height();function n(){var n=t.height()-margin;s>=n?(e.css("overflow-y","scroll"),e.css("max-height",n+marginError+"px")):(e.css("overflow-y","hidden"),e.css("max-height","9999999px"))}t.on("resize",n),$(document).ready(n)})()</script></div></div><div class="min-w-0 min-h-0 max-w-fit"><div class="article-content max-w-prose mb-20"><br><div class="flex flex-row flex-wrap items-center space-x-2"><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 48 48" fill="none"><rect width="48" height="48" fill="#fff" fill-opacity=".01"/><path d="M18 43V22c0-3.3137 2.6863-6 6-6s6 2.6863 6 6V43" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M12 40V22c0-6.6274 5.3726-12 12-12s12 5.3726 12 12V40" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M6 35V22C6 12.0589 14.0589 4 24 4s18 8.0589 18 18V35" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 44V31" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 24.625v-2.75" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/></svg>
</span></span><span>2412.04280</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg fill="#000" height="800" width="800" id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 511.999 511.999"><g><g><path d="M421.578 190.264l-99.847-99.847c-2.439-2.439-6.391-2.439-8.829.0L82.824 320.495c-2.439 2.439-2.439 6.392.0 8.829l99.847 99.847c2.439 2.439 6.391 2.439 8.829.0l230.078-230.078C424.017 196.655 424.017 192.703 421.578 190.264z"/></g></g><g><g><path d="M506.511 87.672 424.323 5.484c-7.308-7.31-19.175-7.315-26.488.0L348.219 55.1c-2.439 2.439-2.439 6.391.0 8.829l99.847 99.847c2.439 2.437 6.391 2.437 8.829.0l49.616-49.616C513.826 106.847 513.826 94.987 506.511 87.672z"/></g></g><g><g><path d="M508.133 491.11c-1.054-9.556-9.489-16.599-19.104-16.599H111.633l36.058-15.163c4.088-1.719 5.131-7.034 1.994-10.17l-86.854-86.854c-3.137-3.135-8.451-2.094-10.17 1.994C52.224 365.359 2.052 484.66 1.627 485.707c-5.815 13.208 4.855 27.01 18.107 26.263H489.52C500.566 511.97 509.379 502.408 508.133 491.11z"/></g></g></svg>
</span></span><span>Jinbin Bai et el.</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span>ü§ó 2024-12-06</span></span></span></div></div><p><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://arxiv.org/abs/2412.04280 target=_self role=button>‚Üó arXiv
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://huggingface.co/papers/2412.04280 target=_self role=button>‚Üó Hugging Face
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://paperswithcode.com/paper/humanedit-a-high-quality-human-rewarded target=_self role=button>‚Üó Papers with Code</a></p><audio controls><source src=https://ai-paper-reviewer.com/2412.04280/podcast.wav type=audio/wav>Your browser does not support the audio element.</audio><h3 class="relative group">TL;DR<div id=tldr class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#tldr aria-label=Anchor>#</a></span></h3><div class="lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl"><p>Instruction-guided image editing has seen significant progress, but existing datasets often lack the quality and diversity needed for reliable model evaluation. Many existing datasets rely on limited human feedback or are generated using models, potentially misaligning them with actual human preferences. This limits the ability to effectively evaluate and improve the performance of image editing models.</p><p>HumanEdit addresses these limitations by providing a high-quality human-rewarded dataset. <strong>It features 5,751 high-resolution images with diverse editing instructions, meticulously curated through a four-stage annotation pipeline</strong>. This rigorous process ensured accuracy and reliability, enabling more robust model evaluations. <strong>HumanEdit also includes masks for all images and supports mask-free editing for a subset of the data</strong>, promoting diverse and realistic editing scenarios. The dataset&rsquo;s comprehensive nature sets a new benchmark for future research and model development in instruction-guided image editing.</p></div><h4 class="relative group">Key Takeaways<div id=key-takeaways class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#key-takeaways aria-label=Anchor>#</a></span></h4><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-8d6eb8a32a98224c5e277e6c9fb9ef99></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-8d6eb8a32a98224c5e277e6c9fb9ef99",{strings:[" HumanEdit offers a high-quality dataset for instruction-based image editing with 5,751 high-resolution images and meticulously crafted instructions. "],speed:10,lifeLike:!0,startDelay:0,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-71f513d243746d547a054d6eb48fe7b2></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-71f513d243746d547a054d6eb48fe7b2",{strings:[" The dataset bridges the gap of human preference alignment in previous datasets by employing human annotators to construct data pairs and administrators to provide feedback, ensuring both accuracy and reliability. "],speed:10,lifeLike:!0,startDelay:1e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-a680e1013eab04752f85678b365acb79></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-a680e1013eab04752f85678b365acb79",{strings:[" HumanEdit supports diverse editing tasks including Action, Add, Counting, Relation, Remove, and Replace, and enables both mask-based and mask-free editing. "],speed:10,lifeLike:!0,startDelay:2e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><h4 class="relative group">Why does it matter?<div id=why-does-it-matter class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#why-does-it-matter aria-label=Anchor>#</a></span></h4><p>This paper is crucial for researchers in image editing and computer vision because it introduces a high-quality, human-annotated dataset, HumanEdit, addressing limitations in existing datasets. <strong>HumanEdit enables more accurate and reliable evaluation of image editing models, facilitating advancements in instruction-guided image editing.</strong> Its comprehensive nature, including diverse editing tasks and high-resolution images, sets a new benchmark for future research and opens avenues for exploring advanced editing techniques.</p><hr><h4 class="relative group">Visual Insights<div id=visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#visual-insights aria-label=Anchor>#</a></span></h4><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04280/x1.png alt></figure></p><blockquote><p>üîº This figure showcases examples from the HumanEdit dataset, illustrating the six different image editing categories it covers: Action, Add, Counting, Relation, Remove, and Replace. Each example shows an original image with an overlaid gray mask indicating the area to be modified. The corresponding instruction specifies the desired edit. Importantly, the caption highlights that while masks are shown here for clarity, approximately half of the dataset&rsquo;s instructions are detailed enough for editing without explicit masks. In the dataset itself, images and their respective masks are stored as separate files.</p><details><summary>read the caption</summary>Figure 1: Data examples of instruction-guided image editing in HumanEdit. Our dataset encompasses six distinct editing categories. In the images, gray shapes represent masks, which are provided for every photograph. Moreover, approximately half of the dataset includes instructions that are sufficiently detailed to enable editing without masks. It is important to note that, for conciseness, masks are depicted directly on the original images within this paper; however, in the dataset, the original images and masks are stored separately.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Add</th><th>Rmove</th><th>Replace</th><th>Action</th><th>Counting</th><th>Relation</th><th>Sum</th></tr></thead><tbody><tr><td>HumanEdit-full</td><td>801</td><td>1,813</td><td>1,370</td><td>659</td><td>698</td><td>410</td></tr><tr><td>HumanEdit-core</td><td>30</td><td>188</td><td>97</td><td>37</td><td>20</td><td>28</td></tr></tbody></table></table></figure><blockquote><p>üîº This table shows the distribution of the six different types of image editing instructions used in the HumanEdit dataset. The dataset was created using human annotators and administrators to ensure quality and alignment with human preferences. Each row represents a category of editing instructions, and the numbers indicate how many images in the dataset belong to each category. These numbers sum up to the total number of images in the HumanEdit dataset (5,751). The categories are Action, Add, Counting, Relation, Remove, and Replace. This breakdown illustrates the diversity of editing tasks encompassed within the dataset, highlighting the dataset&rsquo;s scope and versatility.</p><details><summary>read the caption</summary>Table 1: Distribution of 6 types of our human-rewarded editing instructions.</details></blockquote><h3 class="relative group">In-depth insights<div id=in-depth-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#in-depth-insights aria-label=Anchor>#</a></span></h3><h4 class="relative group">HumanEdit Dataset<div id=humanedit-dataset class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#humanedit-dataset aria-label=Anchor>#</a></span></h4><p>The HumanEdit dataset represents a significant contribution to the field of instruction-based image editing. Its <strong>high-quality, human-annotated images</strong> and diverse editing instructions address the limitations of previous datasets that often relied on minimal human feedback. The meticulous curation process, involving multiple rounds of validation and quality control, ensures <strong>high data accuracy and consistency</strong>. HumanEdit&rsquo;s <strong>mask differentiation</strong> capability supports both mask-free and mask-provided editing scenarios, enhancing its versatility. The dataset&rsquo;s broad range of editing tasks, encompassing six distinct categories, along with its high-resolution images and diverse sources, establishes it as a <strong>robust benchmark for evaluating and developing</strong> future image editing models. The inclusion of a detailed guidance book further facilitates research and ensures reproducibility. Overall, HumanEdit&rsquo;s comprehensive nature, combined with its focus on human preferences, positions it as a valuable tool for advancing research in instruction-based image editing.</p><h4 class="relative group">Annotation Pipeline<div id=annotation-pipeline class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#annotation-pipeline aria-label=Anchor>#</a></span></h4><p>The effectiveness of any image editing dataset hinges on the quality of its annotations. A robust annotation pipeline is crucial, and this paper&rsquo;s approach is noteworthy. It employs a four-stage process. First, <strong>rigorous annotator training and selection</strong> ensures consistency in annotation style. Second, <strong>careful image curation</strong> focuses on high-resolution, high-quality imagery, sourced from Unsplash, avoiding the limitations of smaller, model-generated datasets. The third stage involves <strong>creating diverse and detailed editing instructions</strong>, along with masks where needed, leveraging DALL-E 2. Finally, a <strong>multi-level quality control</strong> process is implemented, involving both automated checks and human review. This thorough approach is key in bridging the gap between model-generated datasets and the human preference for high-quality, diverse, and nuanced image editing instructions. The multi-stage review process, involving internal platform workers and administrators significantly reduces errors and ensures high-quality data suitable for rigorous benchmarks and model training.</p><h4 class="relative group">Benchmark Results<div id=benchmark-results class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#benchmark-results aria-label=Anchor>#</a></span></h4><p>A thorough analysis of benchmark results in a research paper requires a multifaceted approach. It&rsquo;s crucial to understand the metrics used (e.g., L1, L2, CLIP, DINO), as different metrics emphasize various aspects of image quality. <strong>Direct comparison across different methods</strong> necessitates careful consideration of factors like model architecture, training data, and hyperparameter choices. <strong>The dataset&rsquo;s characteristics</strong> (image resolution, diversity, and the inclusion of mask-free editing) significantly influence benchmark performance, and <strong>understanding the dataset&rsquo;s limitations</strong> is essential for interpreting results. <strong>Analyzing results across various editing categories</strong> (add, remove, replace, etc.) helps uncover the strengths and weaknesses of different methods for specific tasks. A robust analysis should also include a qualitative assessment of the generated images, comparing visual quality and fidelity to the reference images. Finally, <strong>a discussion of potential biases and limitations</strong> inherent in the benchmark process and the dataset itself is critical for drawing balanced and meaningful conclusions.</p><h4 class="relative group">DALL-E2&rsquo;s Limits<div id=dall-e2s-limits class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#dall-e2s-limits aria-label=Anchor>#</a></span></h4><p>DALL-E 2, while a powerful image generation model, reveals limitations when tasked with complex image editing. <strong>Inherent inconsistencies</strong> in generating edits, especially those involving subtle changes or precise counts, show the challenges in achieving pixel-perfect results based solely on textual instructions. Furthermore, <strong>the model&rsquo;s tendency toward oversimplification</strong> is apparent in the failure to properly execute more complex edits involving object relations or actions, instead often resulting in unintended changes or loss of detail. These issues, alongside <strong>limitations in understanding nuanced instructions</strong>, highlight the need for more sophisticated approaches for instruction-based image editing. <strong>Addressing these issues</strong> will likely require combining DALL-E 2 with more robust methods such as incorporating detailed masking, improving instruction parsing, and better handling of contextual relationships within images. The observed discrepancies underscore the importance of <strong>human oversight and iterative refinement</strong> in the image editing process to ensure alignment between desired outcomes and actual generated results.</p><h4 class="relative group">Future Directions<div id=future-directions class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#future-directions aria-label=Anchor>#</a></span></h4><p>Future research directions for instruction-based image editing should prioritize <strong>improving the robustness and generalization capabilities</strong> of current models. This includes addressing limitations in handling complex instructions, particularly those involving nuanced relationships or actions. <strong>Addressing inherent biases and inconsistencies</strong> in existing datasets is crucial, requiring more sophisticated annotation methods and potentially, the use of alternative data generation techniques beyond existing LLMs and diffusion models. Furthermore, research should focus on <strong>developing more efficient and scalable</strong> models that can process high-resolution images quickly and require minimal computational resources. Finally, <strong>exploring novel evaluation metrics</strong> beyond pixel-level comparisons is essential to better assess the quality and alignment of edits with user intent, and to ensure that models meet real-world user needs.</p><h3 class="relative group">More visual insights<div id=more-visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#more-visual-insights aria-label=Anchor>#</a></span></h3><details><summary>More on figures</summary><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04280/x2.png alt></figure></p><blockquote><p>üîº This figure illustrates the four-stage data collection process used to create the HumanEdit dataset. Stage 1 involves training and selecting annotators through a tutorial and quiz. Stage 2 focuses on selecting and quality-checking images sourced from Unsplash. In Stage 3, annotators create editing instructions, masks (where applicable), and generate edited images. Finally, Stage 4 includes a two-tier quality review and feedback process to ensure accuracy and consistency, resulting in the final HumanEdit dataset.</p><details><summary>read the caption</summary>Figure 2: Overview of data collection process.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04280/x7.png alt></figure></p><blockquote><p>üîº Figure 3 presents additional examples showcasing the diverse capabilities of HumanEdit for instruction-guided image editing. It visually demonstrates the wide range of edits the dataset encompasses, including adding, removing, replacing, changing actions of objects, counting and modifying relationships between objects within images. These examples highlight the realism and complexity of edits possible using the HumanEdit dataset, illustrating the detailed and nuanced instructions paired with the images.</p><details><summary>read the caption</summary>Figure 3: More examples of instruction-guided image editing in HumanEdit.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04280/x8.png alt></figure></p><blockquote><p>üîº Figure 4 presents a dual visualization of the objects described within the editing instructions of the HumanEdit dataset. Panel (a) shows a distribution chart of the top 30 most frequent objects mentioned in these instructions, providing a quantitative view of object prevalence. Panel (b) offers a word cloud representation of all objects mentioned in the editing instructions, providing a qualitative and visual representation of the dataset&rsquo;s object diversity and the relative frequency of object mentions. Together, the two panels offer insights into the types and frequency of objects involved in the image editing tasks of HumanEdit.</p><details><summary>read the caption</summary>Figure 4: (a) The distribution chart of the first 30 objects in the editing instructions for HumanEdit. (b) The word cloud representation of the objects present in the editing instructions for HumanEdit.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04280/x9.png alt></figure></p><blockquote><p>üîº This river chart visualizes the relationships between different aspects of image editing instructions within the HumanEdit-full dataset. It shows three levels of detail: the type of edit (e.g., Add, Remove, Replace), the verb used in the instruction (e.g., add, remove, change), and the noun representing the object of the edit (e.g., flower, tree, person). The chart&rsquo;s structure reveals common patterns in how users phrase editing instructions, highlighting frequently occurring combinations of edit type, verb, and noun. Only the top 50 most frequent nouns are included for clarity. A similar chart for the HumanEdit-core dataset is found in Figure 11 of the Appendix.</p><details><summary>read the caption</summary>Figure 5: The river chart of HumanEdit-full. The first node of the river represents the type of edit, the second node corresponds to the verb extracted from the instruction, and the final node corresponds to the noun in the instruction. To maintain clarity, we only selected the top 50 most frequent nouns. The river chart of HumanEdit-core can be seen in Figure¬†11 in Appendix.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04280/x10.png alt></figure></p><blockquote><p>üîº Figure 6 presents a comprehensive analysis of HumanEdit and MagicBrush datasets through four subfigures: (a) Compares the necessity of masks in HumanEdit, showing a significant portion (53.1%) of instructions are detailed enough to allow mask-free editing. (b) Shows the diverse sources of images for HumanEdit, highlighting the predominance of Unsplash images. (c) Presents a distribution analysis of the resolutions of images within HumanEdit, indicating a majority (62.3%) possess resolutions above 1000 pixels. (d) Provides a comparative resolution analysis between HumanEdit and MagicBrush datasets, illustrating the superior resolution of HumanEdit.</p><details><summary>read the caption</summary>Figure 6: (a) The distribution of images for which HumanEdit requires masking, where no need for mask refers to editing instructions that are already clear and comprehensive enough, and we believe that no masking is necessary for the model to complete the editing. (b) The distribution of the sources of all input images for HumanEdit. (c) The distribution of resolutions for all input images in HumanEdit. (d) The distribution of resolutions for all input images in MagicBrush.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04280/x11.png alt></figure></p><blockquote><p>üîº Figure 7 presents a qualitative comparison of different image editing models&rsquo; performance on a masked image editing task. The top three rows display the original image, the corresponding mask (the region to be edited), and the ground truth (desired) edited image created using DALL-E 2. The four rows below show the results obtained from four other state-of-the-art models: Blended Latent Diffusion SDXL, GLIDE, aMUSEd, and Meissonic. This allows for a visual assessment of each model&rsquo;s ability to accurately and realistically modify the masked area of the image, based on implicit instructions.</p><details><summary>read the caption</summary>Figure 7: Qualitative comparisons between mask-provided baselines. The first three rows show the original images, corresponding masks, and ground truth edited images from DALL-E 2. The subsequent four rows present results generated by Blended Latent Diffusion SDXL, GLIDE, aMUSEd, and Meissonic, respectively.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04280/x12.png alt></figure></p><blockquote><p>üîº Figure 8 presents a qualitative comparison of different image editing models on a subset of the HumanEdit dataset. The top three rows display example images from the dataset: the original image, the corresponding mask indicating the region to be edited, and the ground truth result produced using DALL-E 2. The following four rows showcase how four other models, specifically Blended Latent Diffusion SDXL, GLIDE, aMUSEd, and Meissonic, performed on the same image editing tasks. This visual comparison highlights the strengths and weaknesses of each model in terms of accurately implementing the editing instructions and producing visually appealing results.</p><details><summary>read the caption</summary>Figure 8: Qualitative comparisons between mask-provided baselines. The first three rows show the original images, corresponding masks, and ground truth edited images from DALL-E 2. The subsequent four rows present results generated by Blended Latent Diffusion SDXL, GLIDE, aMUSEd, and Meissonic, respectively.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04280/x13.png alt></figure></p><blockquote><p>üîº This figure is a sunburst chart visualizing the keywords used in the instructions of the HumanEdit-core dataset. The inner ring represents the action verbs used (e.g., &lsquo;add&rsquo;, &lsquo;remove&rsquo;, &lsquo;change&rsquo;), and the outer ring displays the nouns (objects) associated with each verb. This hierarchical structure helps demonstrate the relationship between the actions and objects in the dataset&rsquo;s instruction set, providing insights into the types of editing operations and target elements prevalent within HumanEdit-core.</p><details><summary>read the caption</summary>Figure 9: An Overview of Keywords in HumanEdit-core Edit Instructions: The inner circle represents the verb in the edit instruction, while the outer circle illustrates the noun following the verb in each instruction.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04280/x14.png alt></figure></p><blockquote><p>üîº This figure is a sunburst chart visualizing the keywords used in the instructions of the HumanEdit-full dataset. The inner circle represents the verb (action word) from each instruction (e.g., &lsquo;add&rsquo;, &lsquo;remove&rsquo;, &lsquo;replace&rsquo;). The outer ring displays the nouns (objects) most frequently associated with those verbs. This provides a visual representation of the types of image editing operations included in the HumanEdit-full dataset and their object focus. It shows the relationship between verbs and nouns within image editing instruction.</p><details><summary>read the caption</summary>Figure 10: An Overview of Keywords in HumanEdit-full Edit Instructions: The inner circle represents the verb in the edit instruction, while the outer circle highlights the noun associated with the verb in each instruction.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04280/x15.png alt></figure></p><blockquote><p>üîº This river chart visualizes the relationships between different aspects of the editing instructions in the HumanEdit-core dataset. It shows the flow from the type of edit (e.g., Add, Remove, Replace) to the verb used in the instruction (e.g., add, remove, change), and finally to the most frequent nouns involved in the instruction (e.g., flower, person, car). This hierarchical representation helps to understand the distribution and common patterns in the image editing tasks described within the HumanEdit-core dataset. Only the top 50 most frequent nouns are included for clarity.</p><details><summary>read the caption</summary>Figure 11: The river chart of HumanEdit-core. The first node of the river represents the type of edit, the second node corresponds to the verb extracted from the instruction, and the final node corresponds to the noun in the instruction. To maintain clarity, we only selected the top 50 most frequent nouns.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04280/x16.png alt></figure></p><blockquote><p>üîº This figure shows an example of object removal from an image in the HumanEdit dataset. The original image contains multiple objects, including a yellow tent and trees. The mask highlights the area where the tent is located. The output image shows the tent has been successfully removed leaving only the background and trees.</p><details><summary>read the caption</summary>Figure 12: Case of Object Removal.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04280/x17.png alt></figure></p><blockquote><p>üîº This figure shows an example of object replacement in image editing. The original image contains a wooden table with various items on it. The editing instruction is to replace the table with a white puppy. The mask highlights the area of the table. The output image shows a white puppy sitting where the table was, with the surrounding environment remaining the same.</p><details><summary>read the caption</summary>Figure 13: Object Replacement Example I.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04280/x18.png alt></figure></p><blockquote><p>üîº This figure shows an example of object replacement in image editing. The original image shows a bathroom with a curved window above the bathtub. The mask highlights the window area for modification. The edited image replaces the curved window with a square window, demonstrating the successful application of the object replacement technique.</p><details><summary>read the caption</summary>Figure 14: Object Replacement Example II.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04280/x19.png alt></figure></p><blockquote><p>üîº This figure shows an example of the &lsquo;Object Addition&rsquo; category from the HumanEdit dataset. It illustrates how a new object (a mobile phone) is added to an existing image, demonstrating a typical instruction-guided image editing task included in the dataset.</p><details><summary>read the caption</summary>Figure 15: Case of Object Addition.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04280/x20.png alt></figure></p><blockquote><p>üîº This figure shows an example of object counting change in the HumanEdit dataset. The original image contains one object, and the edited image contains four objects. This illustrates the type of editing task involving modifying the number of objects in a scene. The task involves increasing or decreasing the count of objects already present in the image, not adding objects that weren&rsquo;t originally there, or removing them entirely (those would be classified as &lsquo;add&rsquo; and &lsquo;remove&rsquo; respectively). The number of objects cannot be changed from zero to a non-zero number or vice-versa.</p><details><summary>read the caption</summary>Figure 16: Case of Object Counting Change.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04280/x21.png alt></figure></p><blockquote><p>üîº This figure shows an example of an image editing task where an action is changed. The original image shows an elephant. The mask highlights the elephant&rsquo;s trunk. The edited image shows the elephant with its trunk raised.</p><details><summary>read the caption</summary>Figure 17: Case of Action Change.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04280/extracted/6044538/images/app/note1.png alt></figure></p><blockquote><p>üîº This figure shows an example of relation change in image editing. The original image shows a cloth basket with gloves on the right and other items on the left. The edited image shows the same items but with the gloves moved from the right side to the left side of the basket, demonstrating a change in spatial relationship between objects.</p><details><summary>read the caption</summary>Figure 18: Case of Relation Change.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04280/extracted/6044538/images/app/note2.png alt></figure></p><blockquote><p>üîº This figure shows an example of how to choose effective prompt words when using the DALL-E 2 platform for image editing. The top row displays various prompt word combinations, while the bottom row shows the images generated based on each prompt. The different images generated illustrate how different word choices can lead to different results. This highlights the importance of careful prompt selection for achieving desirable image editing outcomes.</p><details><summary>read the caption</summary>Figure 19: An Example of Prompt Word Selection</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04280/extracted/6044538/images/app/note3.png alt></figure></p><blockquote><p>üîº Figure 20 shows an example of selecting appropriate prompt words when using the DALL-E 2 platform for image editing. The image demonstrates the importance of using detailed and descriptive prompts to ensure that the model generates high-quality results that match the user&rsquo;s intent. Using vague or incomplete prompts can lead to undesired or inaccurate edits. The figure showcases different word choices, highlighting how the selection impacts the generated images.</p><details><summary>read the caption</summary>Figure 20: An Example of Prompt Word Selection</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04280/extracted/6044538/images/app/note4.png alt></figure></p><blockquote><p>üîº This figure shows a screenshot of the DALL-E 2 platform&rsquo;s interface. The user is in the process of cropping an image before initiating an image editing task. The screenshot highlights the cropping tools and options available within the platform, illustrating one step in the image preparation workflow for instruction-guided image editing.</p><details><summary>read the caption</summary>Figure 21: Performing a Crop Operation on the DALL-E 2 Platform.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04280/extracted/6044538/images/app/note5.png alt></figure></p><blockquote><p>üîº This figure shows a screenshot of the DALL-E 2 platform interface during an image editing operation. The user has selected an image and is presented with options to &lsquo;Edit or generate similar images.&rsquo; The screenshot demonstrates the steps involved in using the platform for image editing, specifically highlighting the &lsquo;Edit Image&rsquo; option.</p><details><summary>read the caption</summary>Figure 22: Performing an Editing Operation on the DALL-E 2 Platform.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04280/extracted/6044538/images/app/note6.png alt></figure></p><blockquote><p>üîº The figure illustrates a common mistake in image editing where the mask used for editing is too large, encompassing unintended areas. This leads to undesirable and unintended edits. In this example, a mask used to alter a section of a photograph inadvertently removes a portion of a boat&rsquo;s paddle. The instruction for this task is implied to only be applied to the people in the boat.</p><details><summary>read the caption</summary>Figure 23: An Illustration of Avoiding Edits in Irrelevant Areas.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04280/extracted/6044538/images/app/note7.png alt></figure></p><blockquote><p>üîº This figure illustrates a common mistake in image editing tasks. The instruction is to add a giraffe to the scene, but the mask is too large, encompassing areas unrelated to the addition. This results in unwanted edits to parts of the image besides the desired addition of a giraffe. In particular, the generated giraffe‚Äôs head appears unnatural. The example highlights the importance of using precise masks that only target the intended editing area to prevent unexpected artifacts in the final image.</p><details><summary>read the caption</summary>Figure 24: An Illustration of Avoiding Edits in Irrelevant Areas.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04280/extracted/6044538/images/app/note8.png alt></figure></p><blockquote><p>üîº The figure demonstrates a common mistake during image editing: selecting too large of a mask area. The instruction is to remove a person, but the selected mask includes a car. This highlights the importance of precise mask selection to avoid unwanted edits to irrelevant parts of the image. The example illustrates the effect of an improperly placed or sized mask in the DALL-E2 platform and the resulting unwanted changes to parts of the image outside the target edit area.</p><details><summary>read the caption</summary>Figure 25: An Illustration of Avoiding Edits in Irrelevant Areas.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04280/extracted/6044538/images/app/note9.png alt></figure></p><blockquote><p>üîº Figure 26 shows an example of proper masking in image editing. The instruction is to change the background of an image featuring a dog. The annotator correctly masked all areas except the dog itself, ensuring that only the background is altered during the editing process. This demonstrates how a properly masked area prevents unintended changes to the main subject of the image, resulting in more precise and accurate editing.</p><details><summary>read the caption</summary>Figure 26: An Illustration of Avoiding Edits in Irrelevant Areas.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04280/extracted/6044538/images/app/note10.png alt></figure></p><blockquote><p>üîº The figure shows an example where DALL-E 2 struggles to accurately interpret the instruction to edit an image. Specifically, it shows an image where the instruction was to edit the hand in a tennis player&rsquo;s image to be outstretched, but instead, DALL-E 2 distorted the fingers making it look unnatural and unrealistic. This highlights the importance of attention to detail in editing instructions to obtain high-quality edits.</p><details><summary>read the caption</summary>Figure 27: A Case for Ensuring Edit Quality.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04280/extracted/6044538/images/app/note11.png alt></figure></p><blockquote><p>üîº The image shows a case where DALL-E 2 fails to accurately interpret editing instructions, resulting in an unrealistic outcome. The instruction intended to remove a person from the scene, but instead part of the car&rsquo;s door also disappeared, highlighting a limitation of the model in precise object removal and maintaining realistic scene context.</p><details><summary>read the caption</summary>Figure 28: A Case for Ensuring Edit Quality.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04280/extracted/6044538/images/app/note12.png alt></figure></p><blockquote><p>üîº The image shows a failure case from the HumanEdit dataset. The instruction was to remove a person from the scene, but DALL-E 2 also removed part of a car, resulting in an unrealistic and inconsistent edit. This highlights the challenges in ensuring high-quality results with instruction-based image editing models, and that excessive masking can lead to undesirable artifacts.</p><details><summary>read the caption</summary>Figure 29: A Case for Ensuring Edit Quality.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04280/extracted/6044538/images/app/note13.png alt></figure></p><blockquote><p>üîº Figure 30 demonstrates the importance of maintaining stylistic consistency between the original image and its edited version. The original image is in black and white, showing a woman in a dark dress. The successful edits retain this black and white aesthetic, and changes are made without introducing color. Unsuccessful edits (marked with an X) fail to preserve the original style, resulting in a colorized or otherwise altered image. This highlights a key quality control aspect of the HumanEdit dataset, ensuring the edited images align with the original style to maintain data quality and consistency.</p><details><summary>read the caption</summary>Figure 30: An Illustration of Consistency in Style Before and After Editing.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04280/extracted/6044538/images/app/pipe1.png alt></figure></p><blockquote><p>üîº Figure 31 showcases examples of images deemed valid or invalid for inclusion in the HumanEdit dataset. The first image is considered valid due to meeting the quality and resolution standards required by the HumanEdit dataset annotation pipeline. In contrast, the remaining three images are deemed invalid. Image (b) contains unusual artifacts, image (c) has poor image quality, and image (d) displays low resolution and lacks sufficient visual information; these flaws render them unsuitable for HumanEdit.</p><details><summary>read the caption</summary>Figure 31: Examples of valid and invalid images. The first image is valid, while the following three images are invalid.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04280/extracted/6044538/images/app/pipe2.png alt></figure></p><blockquote><p>üîº This figure shows a screenshot of the DALL-E 2 platform&rsquo;s interface. The user is presented with options to either start with a detailed description or upload an image. To proceed with uploading an image, the user should click the &lsquo;Try DALL-E&rsquo; button. This step is the initial stage of using the DALL-E 2 platform for image editing within the HumanEdit dataset creation process.</p><details><summary>read the caption</summary>Figure 32: Log in to the DALL¬∑E 2 platform and click 'Try DALL-E' to upload an image.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04280/extracted/6044538/images/app/pipe3.png alt></figure></p><blockquote><p>üîº This figure shows a screenshot of the DALL-E 2 platform&rsquo;s interface after an image has been uploaded. The user is presented with a cropping tool to select the desired area of the image for editing. This step is crucial because it allows the user to focus on the specific part of the image they want to modify, ensuring precise and effective edits with the DALL-E 2&rsquo;s editing tools. The cropping stage refines the area of interest prior to applying editing instructions or generating variations.</p><details><summary>read the caption</summary>Figure 33: After uploading the image, a cropping page will be displayed.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04280/extracted/6044538/images/app/pipe4.png alt></figure></p><blockquote><p>üîº This figure shows a screenshot of the DALL-E 2 platform&rsquo;s editing interface. After uploading an image and performing a crop, the user clicks the &lsquo;Edit&rsquo; button to proceed to the next stage of the image editing process, where they can provide editing instructions and a mask (if needed).</p><details><summary>read the caption</summary>Figure 34: Click the 'Edit' button to enter the editing window.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04280/extracted/6044538/images/app/pipe5.png alt></figure></p><blockquote><p>üîº This figure shows a screenshot of the DALL-E 2 platform&rsquo;s image editing interface. The user is in the process of selecting a region of the image to edit. Using a rectangular selection tool, the user is dragging the selection points to define the area that will be the focus of subsequent editing instructions.</p><details><summary>read the caption</summary>Figure 35: Drag the editing points to select the area to be edited.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04280/extracted/6044538/images/app/pipe6.png alt></figure></p><blockquote><p>üîº This figure shows a screenshot of the DALL-E 2 platform&rsquo;s image editing interface. The user is in the process of adding instructions for an image edit. A mask has already been applied to a region of the image. The user is about to type detailed instructions into the text box to guide the AI in creating the desired edit. The interface includes options for selecting the editing area using a mask and inputting the desired edits through natural language.</p><details><summary>read the caption</summary>Figure 36: Input the editing instructions in the text bo.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04280/extracted/6044538/images/app/pipe7.png alt></figure></p><blockquote><p>üîº This figure shows the results of using the DALL-E 2 platform to generate edited images based on user-provided instructions and mask. The user has input an instruction and selected an area of the image to modify. The platform then generates several versions of the edited image, showcasing the capabilities of the model to make changes according to user specifications.</p><details><summary>read the caption</summary>Figure 37: Generate edited images.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04280/extracted/6044538/images/app/pipe8.png alt></figure></p><blockquote><p>üîº This figure shows the results of regenerating edited images using the DALL-E 2 platform. The initial attempt at image editing yielded unsatisfactory results, so the user employed the platform&rsquo;s &lsquo;regenerate&rsquo; function to produce alternative versions. The images illustrate the iterative process of refining image edits, where the original prompt may need to be modified or made more precise to achieve desired results.</p><details><summary>read the caption</summary>Figure 38: Regenerate edited images.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04280/extracted/6044538/images/app/pipe9.png alt></figure></p><blockquote><p>üîº The figure displays multiple image generation attempts from the DALL-E 2 platform. The user is trying to create an image of a girl with four parrots perched on her. Despite multiple attempts using the &lsquo;regenerate&rsquo; function, the results are still unsatisfactory and fall short of meeting the specified requirements. The parrots&rsquo; positioning and overall image quality need improvement, illustrating that even with a detailed description, generating a precisely desired image using AI can be challenging and require careful refinement of instructions.</p><details><summary>read the caption</summary>Figure 39: Regenerated images are still not satisfactory and may require revised instructions.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04280/extracted/6044538/images/app/pipe10.png alt></figure></p><blockquote><p>üîº This figure shows the final step of the image editing process using the DALL-E 2 platform. After generating multiple edited images based on the user&rsquo;s instructions, the user selects the most satisfactory result and clicks the download button. This action concludes the annotation task for that particular image.</p><details><summary>read the caption</summary>Figure 40: Download and finish the editing process.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04280/extracted/6044538/images/app/pipe11.png alt></figure></p><blockquote><p>üîº Figure 41 showcases examples of unsatisfactory image editing results generated by DALL-E 2. These examples highlight common issues such as a parrot incorrectly perched on another parrot, facial distortions, and an unrealistic depiction of a blue parrot seemingly suspended in mid-air. These examples are used to illustrate common errors that should be avoided during the image editing and annotation process to maintain high data quality.</p><details><summary>read the caption</summary>Figure 41: Defective Image Example.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04280/extracted/6044538/images/app/fail1.png alt></figure></p><blockquote><p>üîº Figure 42 shows an example of the materials that annotators need to submit as a group to the platform. It includes four columns: the original image before editing, the mask used during editing, the editing instruction provided, the expected image description, and the final edited image.</p><details><summary>read the caption</summary>Figure 42: Submission Example.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04280/extracted/6044538/images/app/fail2.png alt></figure></p><blockquote><p>üîº The figure showcases an example where the instruction given to DALL-E 2 was to &lsquo;make the nose larger.&rsquo; However, the model failed to apply this modification to the image. This demonstrates a case of mismatch between the intended editing outcome (as described in the instruction) and the actual result produced by the AI model. Such discrepancies highlight the limitations of current image editing AI models in accurately interpreting and implementing specific instructions for image manipulation.</p><details><summary>read the caption</summary>Figure 43: An Illustration of the Mismatch Between Editing Results and Instructions.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04280/extracted/6044538/images/app/fail3.png alt></figure></p><blockquote><p>üîº The figure shows an example where the instruction &lsquo;a lantern hanging in front of the window&rsquo; was given to DALL-E 2. However, instead of adding a lantern, DALL-E 2 simply removed the existing object without replacing it with a lantern, demonstrating a mismatch between the intended editing and the actual result. This highlights a limitation of DALL-E 2&rsquo;s editing capabilities where the model sometimes fails to follow the instructions accurately.</p><details><summary>read the caption</summary>Figure 44: An Illustration of the Mismatch Between Editing Results and Instructions.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04280/extracted/6044538/images/app/fail4.png alt></figure></p><blockquote><p>üîº This figure shows an example where the instruction given to DALL-E 2 was to generate an image containing &lsquo;a plate of cucumbers and a bouquet of roses&rsquo;. However, the generated image only includes cucumbers; the roses, as specified in the instruction, are missing. This illustrates a mismatch between the user&rsquo;s instruction and the model&rsquo;s output, highlighting a limitation of DALL-E 2&rsquo;s ability to reliably fulfill complex image editing requests.</p><details><summary>read the caption</summary>Figure 45: An Illustration of the Mismatch Between Editing Results and Instructions.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04280/extracted/6044538/images/app/fail11.png alt></figure></p><blockquote><p>üîº This figure shows an example where DALL-E 2 struggles with a specific type of editing task, highlighting its limitations. The instruction was to have a girl standing on tiptoe. Despite numerous attempts, the model failed to accurately achieve the desired effect, illustrating the limitations of the model in handling subtle or complex actions related to human poses. This showcases a failure case from the dataset where such a result would have been excluded because it didn&rsquo;t meet quality standards.</p><details><summary>read the caption</summary>Figure 46: An Illustration of the Limited Editing Capabilities for Specific Types.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04280/extracted/6044538/images/app/fail5.png alt></figure></p><blockquote><p>üîº This figure shows several attempts to edit an image of an owl&rsquo;s face, specifically aiming to make the owl close its eyes. Despite numerous attempts using the DALL-E 2 model, the results were inconsistent and unsuccessful in achieving the desired effect, consistently altering the state of the owl&rsquo;s eyes without achieving the objective of closing them. This highlights the limited editing capabilities of the model for specific types of tasks or images.</p><details><summary>read the caption</summary>Figure 47: An Illustration of the Limited Editing Capabilities for Specific Types.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04280/extracted/6044538/images/app/fail10.png alt></figure></p><blockquote><p>üîº This figure illustrates the limitations of DALL-E 2 in performing specific types of image editing tasks. Specifically, it shows examples where the instruction was to add a red barbell, yet the model struggled to add the barbell accurately, either omitting the object or decreasing the number of objects present. This demonstrates the model&rsquo;s insensitivity to object count and its limitations in reliably executing addition-based edit instructions.</p><details><summary>read the caption</summary>Figure 48: An Illustration of the Limited Editing Capabilities for Specific Types.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04280/extracted/6044538/images/app/fail12.png alt></figure></p><blockquote><p>üîº This figure showcases examples where DALL-E 2 struggles with specific types of editing tasks. It demonstrates limitations in its ability to precisely manipulate image details according to nuanced instructions. The examples highlight difficulties in tasks such as subtly shifting object positions or modifying counts of items. It visually reinforces the limitations of current image editing models, especially when dealing with fine-grained control over image content.</p><details><summary>read the caption</summary>Figure 49: An Illustration of the Limited Editing Capabilities for Specific Types.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04280/extracted/6044538/images/app/fail13.png alt></figure></p><blockquote><p>üîº Figure 50 shows an example where DALL-E 2 struggles with a specific type of editing task. The instruction likely involved a subtle change to an object&rsquo;s characteristics or its relation to other elements in the image. The generated images demonstrate DALL-E 2&rsquo;s inability to consistently and accurately perform this kind of fine-grained edit. This highlights the inherent limitations of current image editing models in handling certain types of instructions.</p><details><summary>read the caption</summary>Figure 50: An Illustration of the Limited Editing Capabilities for Specific Types.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04280/extracted/6044538/images/app/fail14.png alt></figure></p><blockquote><p>üîº Figure 51 shows several examples where DALL-E 2 struggles to perform edits, specifically focusing on limitations with editing certain types of content. The examples highlight the model&rsquo;s inconsistencies and challenges in achieving the desired edits precisely, especially when dealing with complex or nuanced instructions. These examples are part of the failure cases presented in the paper, showing limitations of the DALL-E 2 model that were not included in the final HumanEdit dataset.</p><details><summary>read the caption</summary>Figure 51: An Illustration of the Limited Editing Capabilities for Specific Types.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04280/extracted/6044538/images/app/fail6.png alt></figure></p><blockquote><p>üîº Figure 52 shows examples where DALL-E 2 struggles to perform nuanced edits, specifically highlighting its limitations in handling certain types of content. The model&rsquo;s ability to accurately and consistently modify aspects of an image is inconsistent in this case. This illustrates that while DALL-E 2 may handle simpler editing tasks well, more complex or subtle instructions present significant challenges. The figure serves as an example of inherent limitations in the model&rsquo;s capabilities that would not make these images suitable for inclusion in the dataset.</p><details><summary>read the caption</summary>Figure 52: An Illustration of the Limited Editing Capabilities for Specific Types.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04280/extracted/6044538/images/app/fail7.png alt></figure></p><blockquote><p>üîº In Figure 53, the generated image from DALL-E 2 exhibits distortion in the flower&rsquo;s petals. The editing instruction likely aimed to add or modify a pattern on the flower, but the execution resulted in an unnatural, warped appearance of the flower petals, demonstrating limitations in the model&rsquo;s ability to precisely execute fine-grained image manipulations.</p><details><summary>read the caption</summary>Figure 53: An example of object distortion.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04280/extracted/6044538/images/app/fail8.png alt></figure></p><blockquote><p>üîº This figure in the HumanEdit paper showcases a failure case where the DALL-E 2 model did not correctly implement the given instruction. The instruction was to add a printed pattern to the image, however, the model failed to do so and produced an image that is identical to the original one.</p><details><summary>read the caption</summary>Figure 54: The discrepancy between the instruction and the generated image.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04280/extracted/6044538/images/app/fail9.png alt></figure></p><blockquote><p>üîº The image showcases a subtle editing effect where the instruction was likely to make the dog&rsquo;s ears stand up. However, the result shows only a very slight change and may not be noticeable to the casual observer. This exemplifies a limitation of the DALL-E 2 model&rsquo;s ability to execute fine-grained instructions precisely.</p><details><summary>read the caption</summary>Figure 55: An example of subtle editing effects.</details></blockquote></details><details><summary>More on tables</summary><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Dataset</th><th>Real Image for Edit</th><th>Real-world Scenario</th><th>Human</th><th>Ability Classification</th><th>Mask</th><th>Non-Mask Editing</th></tr></thead><tbody><tr><td>InstructPix2Pix (Brooks et al., 2023)</td><td>‚úó</td><td>‚úó</td><td>‚úó</td><td>‚úó</td><td>‚úó</td><td>‚úì</td></tr><tr><td>MagicBrush (Zhang et al., 2024a)</td><td>‚úì</td><td>‚úó</td><td>‚úì</td><td>‚úó</td><td>‚úì</td><td>‚úó</td></tr><tr><td>GIER (Shi et al., 2020)</td><td>‚úì</td><td>‚úì</td><td>‚úì</td><td>‚úó</td><td>‚úó</td><td>‚úó</td></tr><tr><td>MA5k-Req (Shi et al., 2021)</td><td>‚úì</td><td>‚úì</td><td>‚úì</td><td>‚úó</td><td>‚úó</td><td>‚úó</td></tr><tr><td>TEdBench (Kawar et al., 2023)</td><td>‚úì</td><td>‚úì</td><td>‚úì</td><td>‚úó</td><td>‚úó</td><td>‚úì</td></tr><tr><td>HQ-Edit (Hui et al., 2024)</td><td>‚úó</td><td>‚úó</td><td>‚úó</td><td>‚úó</td><td>‚úó</td><td>‚úì</td></tr><tr><td>SEED-Data-Edit (Ge et al., 2024a)</td><td>‚úì</td><td>‚úì</td><td>‚úì</td><td>‚úó</td><td>‚úó</td><td>‚úì</td></tr><tr><td>AnyEdit (Yu et al., 2024a)</td><td>‚úì</td><td>‚úì</td><td>‚úó</td><td>‚úó</td><td>‚úì</td><td>‚úì</td></tr><tr><td>HumanEdit</td><td>‚úì</td><td>‚úì</td><td>‚úì</td><td>6</td><td>‚úì</td><td>‚úì</td></tr></tbody></table></table></figure><blockquote><p>üîº This table compares several existing image editing datasets across key characteristics. It highlights whether the datasets use real-world images as opposed to model-generated ones, whether they incorporate edits performed by real users, and the level of human involvement in the annotation process. The comparison also includes information about whether the datasets categorize edits into distinct dimensions (ability classification), whether masks are provided to support editing, and whether mask-free editing is possible.</p><details><summary>read the caption</summary>Table 2: Comparison of existing image editing datasets. ‚ÄúReal Image for Edit‚Äù denotes whether real images are used for editing instead of images generated by models. ‚ÄúReal-world Scenario‚Äù indicates whether images edited by users in the real world are included. ‚ÄúHuman‚Äù denotes whether human annotators are involved. ‚ÄúAbility Classification‚Äù refers to evaluating the edit ability in different dimensions. ‚ÄúMask‚Äù indicates whether rendering masks for editing is supported. ‚ÄúNon-Mask Editing‚Äù denotes the ability to edit without mask input.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Methods</th><th>L1‚Üì</th><th>L2‚Üì</th><th>CLIP-I‚Üë</th><th>DINO‚Üë</th><th>CLIP-T‚Üë</th></tr></thead><tbody><tr><td><em>HumanEdit-full</em></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>InstructPix2Pix (Brooks et al., 2023)</td><td>0.1601</td><td>0.0551</td><td>0.7716</td><td>0.5335</td><td>0.2591</td></tr><tr><td>MGIE (Fu et al., 2023)</td><td>0.1240</td><td>0.0535</td><td>0.8697</td><td>0.7221</td><td>0.2661</td></tr><tr><td>HIVE SD1.5 (Zhang et al., 2024b)</td><td>0.1014</td><td><strong>0.0278</strong></td><td>0.8526</td><td>0.7726</td><td><strong>0.2777</strong></td></tr><tr><td>MagicBrush (Zhang et al., 2024a)</td><td><strong>0.0807</strong></td><td>0.0298</td><td><strong>0.8915</strong></td><td><strong>0.7963</strong></td><td>0.2676</td></tr><tr><td><em>HumanEdit-core</em></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>InstructPix2Pix (Brooks et al., 2023)</td><td>0.1625</td><td>0.0570</td><td>0.7627</td><td>0.5349</td><td>0.2533</td></tr><tr><td>MGIE (Fu et al., 2023)</td><td>0.1294</td><td>0.0610</td><td>0.8670</td><td>0.7359</td><td>0.2589</td></tr><tr><td>HIVE SD1.5 (Zhang et al., 2024b)</td><td>0.1162</td><td>0.0373</td><td>0.8441</td><td>0.7038</td><td>0.2563</td></tr><tr><td>MagicBrush (Zhang et al., 2024a)</td><td><strong>0.0760</strong></td><td><strong>0.0283</strong></td><td><strong>0.8946</strong></td><td><strong>0.8121</strong></td><td><strong>0.2619</strong></td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents a quantitative comparison of several mask-free baselines on the HumanEdit dataset. The baselines are evaluated using five metrics: L1 loss, L2 loss, CLIP-I similarity, DINO similarity, and CLIP-T similarity. Lower is better for L1 and L2 loss, while higher is better for the similarity metrics. The results are presented for two subsets of the HumanEdit dataset: HumanEdit-full and HumanEdit-core, allowing for a comparison of performance across different dataset scales and complexities. The best performing model for each metric in each dataset subset is highlighted in bold.</p><details><summary>read the caption</summary>Table 3: Quantitative study on mask-free baselines on HumanEdit. The best results are marked in bold.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Methods</th><th>L1 ‚Üì</th><th>L2 ‚Üì</th><th>CLIP-I ‚Üë</th><th>DINO ‚Üë</th><th>CLIP-T ‚Üë</th></tr></thead><tbody><tr><td><em>HumanEdit-full</em></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Blended Latent Diff. SDXL (Avrahami et al., 2023)</td><td>0.0481</td><td>0.0151</td><td>0.9178</td><td>0.8481</td><td>0.2681</td></tr><tr><td>GLIDE (Nichol et al., 2021)</td><td><strong>0.0391</strong></td><td><strong>0.0120</strong></td><td><strong>0.9388</strong></td><td>0.8800</td><td>0.2676</td></tr><tr><td>aMUSEd (Patil et al., 2024)</td><td>0.0673</td><td>0.0187</td><td>0.9149</td><td>0.8588</td><td><strong>0.2771</strong></td></tr><tr><td>Meissonic (Bai et al., 2024)</td><td>0.0627</td><td>0.0177</td><td>0.9324</td><td><strong>0.8806</strong></td><td>0.2710</td></tr><tr><td><em>HumanEdit-core</em></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Blended Latent Diff. SDXL (Avrahami et al., 2023)</td><td>0.0496</td><td>0.0162</td><td>0.9116</td><td>0.8550</td><td>0.2640</td></tr><tr><td>GLIDE (Nichol et al., 2021)</td><td><strong>0.0379</strong></td><td><strong>0.0113</strong></td><td><strong>0.9413</strong></td><td><strong>0.8961</strong></td><td>0.2656</td></tr><tr><td>aMUSEd (Patil et al., 2024)</td><td>0.0665</td><td>0.0184</td><td>0.9138</td><td>0.8743</td><td><strong>0.2747</strong></td></tr><tr><td>Meissonic (Bai et al., 2024)</td><td>0.0608</td><td>0.0166</td><td>0.9348</td><td>0.8943</td><td>0.2694</td></tr><tr><td><em>HumanEdit-mask</em></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Blended Latent Diff. SDXL (Avrahami et al., 2023)</td><td>0.0478</td><td>0.0154</td><td>0.9065</td><td>0.8223</td><td>0.2650</td></tr><tr><td>GLIDE (Nichol et al., 2021)</td><td><strong>0.0377</strong></td><td><strong>0.0117</strong></td><td><strong>0.9343</strong></td><td>0.8687</td><td>0.2665</td></tr><tr><td>aMUSEd (Patil et al., 2024)</td><td>0.0654</td><td>0.0179</td><td>0.9097</td><td>0.8497</td><td><strong>0.2785</strong></td></tr><tr><td>Meissonic (Bai et al., 2024)</td><td>0.0604</td><td>0.0166</td><td>0.9303</td><td><strong>0.8783</strong></td><td>0.2715</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents a quantitative comparison of different baselines on the HumanEdit dataset. The baselines are models used for image editing, and they are evaluated using mask-provided settings, meaning that the models are given both the image to edit and a mask indicating the region of interest. The evaluation metrics used are L1, L2, CLIP-I, DINO, and CLIP-T, which measure various aspects of the quality and consistency between the generated edited images and the ground truth (actual) edited images. The best performing model for each metric is highlighted in bold.</p><details><summary>read the caption</summary>Table 4: Quantitative study on mask-provided baselines on HumanEdit. The best results are marked in bold.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Methods</th><th>L1‚Üì</th><th>L2‚Üì</th><th>CLIP-I‚Üë</th><th>DINO‚Üë</th><th>CLIP-T‚Üë</th></tr></thead><tbody><tr><td><em>HumanEdit-Add</em></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>InstructPix2Pix (Brooks et al., 2023)</td><td>0.1152</td><td>0.0329</td><td>0.8135</td><td>0.6230</td><td>0.2764</td></tr><tr><td>MGIE (Fu et al., 2023)</td><td>0.0934</td><td>0.0274</td><td>0.8770</td><td>0.7391</td><td>0.2806</td></tr><tr><td>HIVE SD1.5 (Zhang et al., 2024b)</td><td>0.0885</td><td>0.0234</td><td>0.8863</td><td>0.7811</td><td>0.2706</td></tr><tr><td>MagicBrush (Zhang et al., 2024a)</td><td>0.0580</td><td>0.0167</td><td>0.9102</td><td>0.8562</td><td>0.2745</td></tr><tr><td>Blended Latent Diff. SDXL (Avrahami et al., 2023)</td><td>0.0344</td><td><strong>0.0073</strong></td><td>0.9285</td><td>0.8856</td><td>0.2665</td></tr><tr><td>GLIDE (Nichol et al., 2021)</td><td><strong>0.0315</strong></td><td>0.0078</td><td><strong>0.9410</strong></td><td><strong>0.8995</strong></td><td>0.2600</td></tr><tr><td>aMUSEd (Patil et al., 2024)</td><td>0.0581</td><td>0.0130</td><td>0.9148</td><td>0.8672</td><td><strong>0.2695</strong></td></tr><tr><td>Meissonic (Bai et al., 2024)</td><td>0.0544</td><td>0.0129</td><td>0.9303</td><td>0.8787</td><td>0.2669</td></tr><tr><td><em>HumanEdit-Action</em></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>InstructPix2Pix (Brooks et al., 2023)</td><td>0.1324</td><td>0.0398</td><td>0.7514</td><td>0.5789</td><td>0.2617</td></tr><tr><td>MGIE (Fu et al., 2023)</td><td>0.0982</td><td>0.0383</td><td>0.8788</td><td>0.7909</td><td>0.2658</td></tr><tr><td>HIVE SD1.5 (Zhang et al., 2024b)</td><td>0.0972</td><td>0.0280</td><td>0.8592</td><td>0.7613</td><td>0.2640</td></tr><tr><td>MagicBrush (Zhang et al., 2024a)</td><td>0.0723</td><td>0.0245</td><td>0.9028</td><td>0.8357</td><td>0.2668</td></tr><tr><td>Blended Latent Diff. SDXL (Avrahami et al., 2023)</td><td>0.0416</td><td><strong>0.0109</strong></td><td>0.9391</td><td>0.9015</td><td>0.2712</td></tr><tr><td>GLIDE (Nichol et al., 2021)</td><td><strong>0.0384</strong></td><td>0.0114</td><td><strong>0.9487</strong></td><td>0.9018</td><td>0.2683</td></tr><tr><td>aMUSEd (Patil et al., 2024)</td><td>0.0629</td><td>0.0156</td><td>0.9230</td><td>0.8919</td><td><strong>0.2732</strong></td></tr><tr><td>Meissonic (Bai et al., 2024)</td><td>0.0577</td><td>0.0145</td><td>0.9430</td><td><strong>0.9126</strong></td><td>0.2677</td></tr><tr><td><em>HumanEdit-Counting</em></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>InstructPix2Pix (Brooks et al., 2023)</td><td>0.1628</td><td>0.0586</td><td>0.8124</td><td>0.5850</td><td>0.2716</td></tr><tr><td>MGIE (Fu et al., 2023)</td><td>0.1380</td><td>0.0641</td><td>0.8726</td><td>0.6971</td><td>0.2716</td></tr><tr><td>HIVE SD1.5 (Zhang et al., 2024b)</td><td>0.1211</td><td>0.0442</td><td>0.8826</td><td>0.7431</td><td>0.2705</td></tr><tr><td>MagicBrush (Zhang et al., 2024a)</td><td>0.1058</td><td>0.0434</td><td>0.8677</td><td>0.7103</td><td>0.2707</td></tr><tr><td>Blended Latent Diff. SDXL (Avrahami et al., 2023)</td><td>0.0527</td><td>0.0180</td><td>0.9334</td><td>0.8892</td><td>0.2766</td></tr><tr><td>GLIDE (Nichol et al., 2021)</td><td><strong>0.0392</strong></td><td><strong>0.0127</strong></td><td><strong>0.9523</strong></td><td><strong>0.9104</strong></td><td>0.2772</td></tr><tr><td>aMUSEd (Patil et al., 2024)</td><td>0.0699</td><td>0.0213</td><td>0.9270</td><td>0.8816</td><td><strong>0.2814</strong></td></tr><tr><td>Meissonic (Bai et al., 2024)</td><td>0.0674</td><td>0.0217</td><td>0.9394</td><td>0.8967</td><td>0.2750</td></tr><tr><td><em>HumanEdit-Remove</em></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>InstructPix2Pix (Brooks et al., 2023)</td><td>0.1624</td><td>0.0504</td><td>0.7240</td><td>0.4188</td><td>0.2325</td></tr><tr><td>MGIE (Fu et al., 2023)</td><td>0.1259</td><td>0.0572</td><td>0.8677</td><td>0.7235</td><td>0.2525</td></tr><tr><td>HIVE SD1.5 (Zhang et al., 2024b)</td><td>0.1179</td><td>0.0375</td><td>0.8362</td><td>0.6562</td><td>0.2474</td></tr><tr><td>MagicBrush (Zhang et al., 2024a)</td><td>0.0690</td><td>0.0232</td><td>0.8985</td><td>0.8249</td><td>0.2572</td></tr><tr><td>Blended Latent Diff. SDXL (Avrahami et al., 2023)</td><td>0.0451</td><td>0.0133</td><td>0.9055</td><td>0.8322</td><td>0.2608</td></tr><tr><td>GLIDE (Nichol et al., 2021)</td><td><strong>0.0313</strong></td><td><strong>0.0072</strong></td><td><strong>0.9493</strong></td><td><strong>0.9119</strong></td><td>0.2661</td></tr><tr><td>aMUSEd (Patil et al., 2024)</td><td>0.0621</td><td>0.0156</td><td>0.9148</td><td>0.8702</td><td><strong>0.2715</strong></td></tr><tr><td>Meissonic (Bai et al., 2024)</td><td>0.0557</td><td>0.0132</td><td>0.9367</td><td>0.9048</td><td>0.2673</td></tr><tr><td><em>HumanEdit-Relation</em></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>InstructPix2Pix (Brooks et al., 2023)</td><td>0.1741</td><td>0.0647</td><td>0.8069</td><td>0.5851</td><td>0.2828</td></tr><tr><td>MGIE (Fu et al., 2023)</td><td>0.1420</td><td>0.0656</td><td>0.8762</td><td>0.7061</td><td>0.2768</td></tr><tr><td>HIVE SD1.5 (Zhang et al., 2024b)</td><td>0.1298</td><td>0.0460</td><td>0.8689</td><td>0.7005</td><td>0.2793</td></tr><tr><td>MagicBrush (Zhang et al., 2024a)</td><td>0.0884</td><td>0.0334</td><td>0.8985</td><td>0.7865</td><td>0.2823</td></tr><tr><td>Blended Latent Diff. SDXL (Avrahami et al., 2023)</td><td>0.0628</td><td>0.0213</td><td><strong>0.9190</strong></td><td><strong>0.8174</strong></td><td>0.2832</td></tr><tr><td>GLIDE (Nichol et al., 2021)</td><td><strong>0.0553</strong></td><td><strong>0.0192</strong></td><td>0.9136</td><td>0.7983</td><td>0.2755</td></tr><tr><td>aMUSEd (Patil et al., 2024)</td><td>0.0809</td><td>0.0267</td><td>0.9076</td><td>0.8095</td><td><strong>0.2862</strong></td></tr><tr><td>Meissonic (Bai et al., 2024)</td><td>0.0825</td><td>0.0283</td><td>0.9171</td><td>0.8142</td><td>0.2768</td></tr><tr><td><em>HumanEdit-Replace</em></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>InstructPix2Pix (Brooks et al., 2023)</td><td>0.1910</td><td>0.0770</td><td>0.7887</td><td>0.5692</td><td>0.2697</td></tr><tr><td>MGIE (Fu et al., 2023)</td><td>0.1391</td><td>0.0620</td><td>0.8603</td><td>0.6946</td><td>0.2698</td></tr><tr><td>HIVE SD1.5 (Zhang et al., 2024b)</td><td>0.1265</td><td>0.0443</td><td>0.8582</td><td>0.7087</td><td>0.2726</td></tr><tr><td>MagicBrush (Zhang et al., 2024a)</td><td>0.0984</td><td>0.0409</td><td>0.8757</td><td>0.7513</td><td>0.2716</td></tr><tr><td>Blended Latent Diff. SDXL (Avrahami et al., 2023)</td><td>0.0567</td><td>0.0206</td><td>0.9095</td><td>0.8096</td><td>0.2683</td></tr><tr><td>GLIDE (Nichol et al., 2021)</td><td><strong>0.0495</strong></td><td><strong>0.0188</strong></td><td>0.9194</td><td>0.8247</td><td>0.2663</td></tr><tr><td>aMUSEd (Patil et al., 2024)</td><td>0.0761</td><td>0.0237</td><td>0.9072</td><td>0.8259</td><td><strong>0.2861</strong></td></tr><tr><td>Meissonic (Bai et al., 2024)</td><td>0.0710</td><td>0.0227</td><td><strong>0.9239</strong></td><td><strong>0.8462</strong></td><td>0.2762</td></tr></tbody></table></table></figure><blockquote><p>üîº Table 5 presents a quantitative analysis of six distinct image editing instruction types within the HumanEdit dataset. The table compares the performance of various baselines (InstructPix2Pix, MGIE, HIVE SD1.5, MagicBrush, Blended Latent Diffusion SDXL, GLIDE, aMUSEd, and Meissonic) across these instructions. Evaluation metrics include L1 and L2 distance (measuring pixel-level differences), CLIP-I and DINO scores (assessing image quality), and CLIP-T scores (evaluating text-image alignment). The best-performing method for each metric and instruction type is highlighted in bold, enabling a nuanced comparison of model capabilities across various editing tasks.</p><details><summary>read the caption</summary>Table 5: Quantitative study on six different types of editing instructions on HumanEdit. The best results are marked in bold.</details></blockquote></details><h3 class="relative group">Full paper<div id=full-paper class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#full-paper aria-label=Anchor>#</a></span></h3><div id=gallery-be7f7a42e9036d46679d36828cde2823 class=gallery><img src=https://ai-paper-reviewer.com/2412.04280/1.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04280/2.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04280/3.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04280/4.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04280/5.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04280/6.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04280/7.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04280/8.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04280/9.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04280/10.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04280/11.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04280/12.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04280/13.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04280/14.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04280/15.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04280/16.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04280/17.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04280/18.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04280/19.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04280/20.png class="grid-w50 md:grid-w33 xl:grid-w25"></div></div><section class="flex flex-row flex-wrap justify-center pt-4 text-xl"><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04280/&amp;title=HumanEdit:%20A%20High-Quality%20Human-Rewarded%20Dataset%20for%20Instruction-based%20Image%20Editing" title="Share on LinkedIn" aria-label="Share on LinkedIn"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://twitter.com/intent/tweet/?url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04280/&amp;text=HumanEdit:%20A%20High-Quality%20Human-Rewarded%20Dataset%20for%20Instruction-based%20Image%20Editing" title="Tweet on Twitter" aria-label="Tweet on Twitter"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="mailto:?body=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04280/&amp;subject=HumanEdit:%20A%20High-Quality%20Human-Rewarded%20Dataset%20for%20Instruction-based%20Image%20Editing" title="Send via email" aria-label="Send via email"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1-27.64 140.9 68.65 266.2 199.1 285.1 19.01 2.888 36.17-12.26 36.17-31.49l1e-4-.6631c0-15.74-11.44-28.88-26.84-31.24-84.35-12.98-149.2-86.13-149.2-174.2.0-102.9 88.61-185.5 193.4-175.4 91.54 8.869 158.6 91.25 158.6 183.2v16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98.0036c-7.299.0-13.2 4.992-15.12 11.68-24.85-12.15-54.24-16.38-86.06-5.106-38.75 13.73-68.12 48.91-73.72 89.64-9.483 69.01 43.81 128 110.9 128 26.44.0 50.43-9.544 69.59-24.88 24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3c0-149.2-133.9-265.632-287.3-235.57zM239.1 304.3c-26.47.0-48-21.56-48-48.05s21.53-48.05 48-48.05 48 21.56 48 48.05-20.6 48.05-48 48.05z"/></svg></span></a></section></div><script>var oid="views_paper-reviews/2412.04280/index.md",oid_likes="likes_paper-reviews/2412.04280/index.md"</script><script type=text/javascript src=/ai-paper-reviewer/js/page.min.0860cf4e04fa2d72cc33ddba263083464d48f67de06114529043cb4623319efed4f484fd7f1730df5abea0e2da6f3538855634081d02f2d6e920b956f063e823.js integrity="sha512-CGDPTgT6LXLMM926JjCDRk1I9n3gYRRSkEPLRiMxnv7U9IT9fxcw31q+oOLabzU4hVY0CB0C8tbpILlW8GPoIw=="></script></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="flex group mr-3" href=/ai-paper-reviewer/paper-reviews/2412.04431/><span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&larr;</span>
<span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Infinity: Scaling Bitwise AutoRegressive Modeling for High-Resolution Image Synthesis</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2024-12-05T00:00:00+00:00>5 December 2024</time>
</span></span></a></span><span><a class="flex text-right group ml-3" href=/ai-paper-reviewer/paper-reviews/2412.04653/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Hidden in the Noise: Two-Stage Robust Watermarking for Images</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2024-12-05T00:00:00+00:00>5 December 2024</time>
</span></span><span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&larr;</span></a></span></div></div><div class=pt-3><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class=pt-3><script src=https://utteranc.es/client.js repo=pmnxis/pmnxis.github.io issue-term=pathname label=Comment theme=dark-blue crossorigin=anonymous async></script></div></div></footer></article><div id=top-scroller class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer id=site-footer class="py-10 print:hidden"><nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400"><ul class="flex flex-col list-none sm:flex-row"><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=/ai-paper-reviewer/tags/ title>Tags</a></li></ul></nav><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2024
AI Paper Reviews by AI</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a></p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/ai-paper-reviewer/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh+sCQ0E53ghYrxgYqw+0GCRyIEpA=="></script></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://deep-diver.github.io/ai-paper-reviewer/ style=z-index:500><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body><script data-name=BMC-Widget data-cfasync=false src=https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js data-id=chansung data-description="Support me on Buy me a coffee!" data-message data-color=#FFDD00 data-position=Left data-x_margin=18 data-y_margin=18></script></html>