{"references": [{"fullname_first_author": "Shazeer", "paper_title": "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer", "publication_date": "2017-00-00", "reason": "This paper is foundational for Mixture-of-Experts (MoE) models, introducing the sparsely-gated MoE layer which is a core concept in many subsequent MoE-based language models."}, {"fullname_first_author": "Lepikhin", "paper_title": "{GS}hard: Scaling giant models with conditional computation and automatic sharding", "publication_date": "2021-00-00", "reason": "This paper proposes GShard, a sharding technique that enables training of extremely large models, which is essential for scaling MoE models to massive sizes."}, {"fullname_first_author": "Fedus", "paper_title": "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity", "publication_date": "2022-00-00", "reason": "This paper introduces Switch Transformers, a highly efficient sparse model architecture that has significantly impacted the development of large language models, including MoE models."}, {"fullname_first_author": "Jiang", "paper_title": "Mixtral of experts", "publication_date": "2024-00-00", "reason": "This paper introduces Mixtral, a large language model using MoE, and is highly relevant due to its use as a model in several experiments reported in this paper."}, {"fullname_first_author": "Touvron", "paper_title": "Llama: Open and efficient foundation language models", "publication_date": "2023-02-23", "reason": "This paper introduces Llama, a foundational large language model whose architecture is widely used in subsequent models and is referenced here for use in experiments."}]}