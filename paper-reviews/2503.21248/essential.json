{"importance": "This paper is vital for researchers as it introduces the **ResearchBench** benchmark to assess LLMs' scientific discovery potential. By decomposing the discovery process and evaluating LLMs, it offers insights into how to improve these models for research, especially in **out-of-distribution inspiration retrieval**. It enables researchers to better leverage LLMs for automated scientific exploration and future research.", "summary": "ResearchBench: Benchmarking LLMs for Scientific Discovery via Inspiration-Based Task Decomposition.", "takeaways": ["Introduces ResearchBench, a large-scale benchmark for evaluating LLMs in scientific discovery.", "LLMs can effectively retrieve inspirations beyond established associations.", "LLMs can serve as 'research hypothesis mines,' facilitating innovative hypothesis generation with minimal human input."], "tldr": "LLMs show promise in aiding scientific research, but their ability to discover high-quality research hypotheses remains unexamined. To address this, the paper introduces a benchmark for evaluating LLMs, which covers inspiration retrieval, hypothesis composition, and hypothesis ranking. Critical components are extracted from scientific papers across 12 disciplines using an automated framework. The benchmark helps understand how LLMs perform in scientific discovery sub-tasks. \n\nThe paper develops **ResearchBench**, which collects papers from 12 disciplines and develops an LLM-based framework to extract research questions, background surveys, inspirations, and hypotheses from scientific papers. It finds that current LLMs perform well in retrieving inspirations across disciplines. The paper highlights LLMs' potential as research hypothesis mines, capable of generating novel scientific insights with minimal human involvement.", "affiliation": "Shanghai Artificial Intelligence Laboratory", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}, "podcast_path": "2503.21248/podcast.wav"}