[{"content": "| Dataset | Type | Size |\n|---|---|---|\n| ShareGPT4V [8] | General VQA | 31.3k |\n| ChartQA [38] | General VQA | 17.2k |\n| A-OKVQA [45] | General VQA | 16.1k |\n| AI2D [23] | Science-Targeted VQA | 11.4k |\n| GeoQA+ [7] | Science-Targeted VQA | 11.4k |\n| ScienceQA [34] | Science-Targeted VQA | 5.6k |\n| DocVQA [39] | General VQA | 4.0k |\n| PISC [28] | General VQA | 1.0k |\n| CLEVR [22] | General VQA | 0.5k |\n| CLEVR-Math [13] | Science-Targeted VQA | 0.5k |", "caption": "Table 1: The number of samples selected from each benchmark.", "description": "This table details the composition of the LLAVA-01-100k dataset used in the paper.  It lists the various visual question answering (VQA) datasets that were sampled from, categorizes them as either general-purpose VQA or science-focused VQA, and shows the number of samples taken from each dataset. The total number of samples in the LLAVA-01-100k dataset is 99,000.", "section": "3.1.2 Data Preparation and Model Training"}, {"content": "| Model | MMStar | MMBench | MMVet | MathVista | AI2D | Hallusion | Average |\n|---|---|---|---|---|---|---|---| \n| <em class=\"ltx_emph ltx_font_bold ltx_font_italic\" style=\"font-size:80%;\">Base Model</em> |  |  |  |  |  |  |  |\n| Llama-3.2-11B-Vision-Instruct | 49.8 | 65.8 | 57.6 | 48.6 | 77.3 | 40.3 | 56.6 |\n| <em class=\"ltx_emph ltx_font_bold ltx_font_italic\" style=\"font-size:80%;\">Our Models</em> |  |  |  |  |  |  |  |\n| LLaVA-o1 (with Direct Training) | 54.3 | 76.2 | 49.9 | 49.5 | 91.4 | 42.9 | 60.7 |\n| LLaVA-o1 (w/o Structured Tags) | 55.7 | 74.2 | 57.0 | 54.1 | 87.2 | 45.0 | 62.2 |\n| LLaVA-o1 | 57.6 | 75.0 | 60.3 | 54.8 | 85.7 | 47.8 | 63.5 |", "caption": "Table 2: Experimental results of different models on the benchmark. Here, LLaVA-o1\u00a0(with Direct Training) refers to the model trained directly on the original VQA dataset\u2019s Q&A pairs, while LLaVA-o1\u00a0(w/o Structured Tags) represents the model trained on the LLaVA-o1-100k dataset with the structured tags removed. LLaVA-o1\u00a0refers to the model trained on the complete LLaVA-o1-100k dataset, including the structured tags.", "description": "This table presents a comparison of the performance of different models on a multimodal reasoning benchmark.  Three variations of the LLaVA-01 model are included: one trained directly on the original VQA dataset (without the structured reasoning stages), one trained on the LLaVA-01-100k dataset but without the structured tags used to denote reasoning stages, and a final version trained on the complete LLaVA-01-100k dataset with the structured tags. A baseline model (Llama-3.2-11B-Vision-Instruct) is also included for comparison. The results highlight the impact of the structured training data and tags on the model's performance.", "section": "4. Experimental Setup"}, {"content": "| Model | CP | FP | IR | LR | Math | Science & Technology | Average |\n|---|---|---|---|---|---|---|---| \n| **Base Model** |  |  |  |  |  |  |  |\n| Llama-3.2-11B-Vision-Instruct | 66.0 | 46.4 | 57.6 | 50.8 | 45.2 | 32.8 | 49.8 |\n| **Our Models** |  |  |  |  |  |  |  |\n| LLaVA-o1 (with Direct Training) | 68.4 | 48.0 | 65.6 | 52.0 | 51.6 | 40.0 | 54.3 |\n| LLaVA-o1 (w/o Structured Tags) | 68.4 | 48.0 | 60.0 | 55.2 | 64.4 | 38.0 | 55.7 |\n| LLaVA-o1 | 68.8 | 46.8 | 63.2 | 58.0 | 64.0 | 44.8 | 57.6 |", "caption": "Table 3: Performance of different models on the MMStar benchmark across various skill areas. Here, CP represents coarse perception, FP represents fine-grained perception, IR represents instance reasoning, and LR represents logical reasoning. As shown in the table, our model demonstrates substantial improvement over the base model in instance reasoning, logical reasoning, math, and science & technology, indicating that structured reasoning can significantly enhance the model\u2019s reasoning capabilities.", "description": "Table 3 presents a detailed comparison of different models' performance on the MMStar benchmark, broken down by specific skill areas: Coarse Perception (CP), Fine-grained Perception (FP), Instance Reasoning (IR), Logical Reasoning (LR), Math, and Science & Technology.  The results highlight LLAVA-01's significant improvement over the baseline model, particularly in the more complex reasoning tasks (IR, LR, Math, and Science & Technology), demonstrating the effectiveness of its structured reasoning approach in enhancing overall reasoning capabilities.", "section": "4. Post-Training Performance"}, {"content": "| Model | MMStar | MMBench | MMVet | MathVista | AI2D | Hallusion | Average |\n|---|---|---|---|---|---|---|---| \n| **Base Model** |  |  |  |  |  |  |  |\n| Llama-3.2-11B-Vision-Instruct | 49.8 | 65.8 | 57.6 | 48.6 | 77.3 | 40.3 | 56.6 |\n| **Our Models** |  |  |  |  |  |  |  |\n| LLaVA-o1 | 57.6 | 75.0 | 60.3 | 54.8 | 85.7 | 47.8 | 63.5 |\n| LLaVA-o1 (BS = 2) | 58.1 | 75.6 | 61.7 | 56.1 | 87.5 | 48.2 | 64.5 |", "caption": "Table 4: Experimental results during inference time. LLaVA-o1\u00a0(BS = 2) denotes the model using stage-level beam search with a beam size of 2. The results show that stage-level beam search can achieve further significant performance improvements.", "description": "This table presents the performance comparison of different models during inference time.  Specifically, it contrasts the performance of the LLaVA-01 model without any inference-time scaling techniques, against the same model using a stage-level beam search with a beam size of 2 (LLaVA-01 (BS=2)). The results highlight the significant performance gains achieved by employing the stage-level beam search method, demonstrating its effectiveness in improving the model's reasoning capabilities during inference.", "section": "5. Inference Time Scaling"}, {"content": "| Method | Number of Beam | MMVet Score |\n|---|---|---|\n| No Inference Scaling | 1 | 60.3 |\n| Best-of-N Search | 10 | 60.9 |\n| Sentence-level Beam Search | 2 | 58.4 |\n| Stage-level Beam Search | 4 | 62.9 |", "caption": "Table 7: Experimental results of LLaVA-o1\u00a0and state-of-the-art models on reasoning benchmarks. Here, LLaVA-o1\u00a0refers to the model without inference scaling, while LLaVA-o1\u00a0(BS = 2) denotes the model using stage-level beam search with a beam size of 2.", "description": "Table 7 presents a comparative analysis of the performance of LLaVA-01 and other state-of-the-art vision-language models (VLMs) across six reasoning benchmarks.  These benchmarks assess various reasoning capabilities, including general visual question answering, mathematical reasoning, scientific reasoning, and handling of hallucinations and visual illusions. The table specifically contrasts the performance of LLaVA-01 without inference-time scaling and LLaVA-01 with a stage-level beam search (using a beam size of 2).  This comparison highlights the impact of the proposed inference-time scaling technique on the overall performance of the model.", "section": "6. Comparison to State-of-the-Art VLMs"}]