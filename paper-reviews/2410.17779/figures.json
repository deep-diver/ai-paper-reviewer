[{"figure_path": "2410.17779/figures/figures_4_0.png", "caption": "Fig. 1. Comparison of different vision-language tuning frameworks: (a) Methods that directly extend the input space of the language model with extracted vision features. (b) Methods that fuse vision information into the language model via cross-attention. (c) Our proposed ADEM-VL framework, which incorporates parameter-free cross-attention, multiscale visual prompting, and adaptive multimodal fusion designs. This approach ensures both parameter and computational efficiency while delivering promising performance.", "description": "The figure compares different vision-language tuning frameworks, highlighting the proposed ADEM-VL framework's efficiency and effectiveness.", "section": "III. METHOD"}, {"figure_path": "2410.17779/figures/figures_11_0.png", "caption": "Fig. 3. Visualization of image captioning results with LLaMA-7B. In each row, the left figure is the original image, while the middle and right figures demonstrate the dropping decisions for features at two different scales.", "description": "The figure shows examples of image captioning results, visualizing the adaptive fusion module's feature dropping decisions for two different image scales.", "section": "IV. EXPERIMENT"}, {"figure_path": "2410.17779/figures/figures_11_1.png", "caption": "Fig. 1. Comparison of different vision-language tuning frameworks: (a) Methods that directly extend the input space of the language model with extracted vision features. (b) Methods that fuse vision information into the language model via cross-attention. (c) Our proposed ADEM-VL framework, which incorporates parameter-free cross-attention, multiscale visual prompting, and adaptive multimodal fusion designs. This approach ensures both parameter and computational efficiency while delivering promising performance.", "description": "The figure compares three different vision-language tuning frameworks, highlighting the proposed ADEM-VL framework's efficiency and effectiveness in multimodal fusion.", "section": "III. METHOD"}, {"figure_path": "2410.17779/figures/figures_11_2.png", "caption": "Fig. 1. Comparison of different vision-language tuning frameworks: (a) Methods that directly extend the input space of the language model with extracted vision features. (b) Methods that fuse vision information into the language model via cross-attention. (c) Our proposed ADEM-VL framework, which incorporates parameter-free cross-attention, multiscale visual prompting, and adaptive multimodal fusion designs. This approach ensures both parameter and computational efficiency while delivering promising performance.", "description": "The figure compares different vision-language tuning frameworks, highlighting the proposed ADEM-VL framework's parameter and computational efficiency.", "section": "III. METHOD"}, {"figure_path": "2410.17779/figures/figures_11_3.png", "caption": "Fig. 3. Visualization of image captioning results with LLaMA-7B. In each row, the left figure is the original image, while the middle and right figures demonstrate the dropping decisions for features at two different scales.", "description": "The figure visualizes image captioning results, showing the original image and the model's decisions on dropping image features at different scales.", "section": "IV. EXPERIMENT"}, {"figure_path": "2410.17779/figures/figures_11_4.png", "caption": "Fig. 1. Comparison of different vision-language tuning frameworks: (a) Methods that directly extend the input space of the language model with extracted vision features. (b) Methods that fuse vision information into the language model via cross-attention. (c) Our proposed ADEM-VL framework, which incorporates parameter-free cross-attention, multiscale visual prompting, and adaptive multimodal fusion designs. This approach ensures both parameter and computational efficiency while delivering promising performance.", "description": "The figure compares different vision-language tuning frameworks, highlighting the proposed ADEM-VL framework's efficient and adaptive multimodal fusion approach.", "section": "III. METHOD"}, {"figure_path": "2410.17779/figures/figures_11_5.png", "caption": "Fig. 3. Visualization of image captioning results with LLaMA-7B. In each row, the left figure is the original image, while the middle and right figures demonstrate the dropping decisions for features at two different scales.", "description": "The figure visualizes image captioning results, showing the original image and how the model's attention mechanism dynamically discards less relevant visual features at different scales.", "section": "IV. EXPERIMENT"}, {"figure_path": "2410.17779/figures/figures_11_6.png", "caption": "Fig. 3. Visualization of image captioning results with LLaMA-7B. In each row, the left figure is the original image, while the middle and right figures demonstrate the dropping decisions for features at two different scales.", "description": "The figure visualizes the adaptive feature dropping mechanism of ADEM-VL for image captioning by showing original images and their corresponding feature attention maps at two scales.", "section": "IV. EXPERIMENT"}, {"figure_path": "2410.17779/figures/figures_12_0.png", "caption": "Fig. 4. Examples of zero-shot instruction-following tasks with LLaMA-7B.", "description": "The figure shows four examples of zero-shot instruction following tasks performed by the LLaMA-7B model, demonstrating its ability to generate appropriate responses to image and instruction pairs.", "section": "V. CONCLUSION"}, {"figure_path": "2410.17779/figures/figures_12_1.png", "caption": "Fig. 1. Comparison of different vision-language tuning frameworks: (a) Methods that directly extend the input space of the language model with extracted vision features. (b) Methods that fuse vision information into the language model via cross-attention. (c) Our proposed ADEM-VL framework, which incorporates parameter-free cross-attention, multiscale visual prompting, and adaptive multimodal fusion designs. This approach ensures both parameter and computational efficiency while delivering promising performance.", "description": "Figure 1 compares different vision-language tuning frameworks, highlighting the proposed ADEM-VL framework's parameter and computationally efficient design incorporating parameter-free cross-attention, multiscale visual prompting, and adaptive fusion.", "section": "III. METHOD"}, {"figure_path": "2410.17779/figures/figures_12_2.png", "caption": "Fig. 1. Comparison of different vision-language tuning frameworks: (a) Methods that directly extend the input space of the language model with extracted vision features. (b) Methods that fuse vision information into the language model via cross-attention. (c) Our proposed ADEM-VL framework, which incorporates parameter-free cross-attention, multiscale visual prompting, and adaptive multimodal fusion designs. This approach ensures both parameter and computational efficiency while delivering promising performance.", "description": "The figure compares different vision-language tuning frameworks, highlighting the proposed ADEM-VL framework's efficient multimodal fusion approach.", "section": "III. METHOD"}, {"figure_path": "2410.17779/figures/figures_12_3.png", "caption": "Fig. 1. Comparison of different vision-language tuning frameworks: (a) Methods that directly extend the input space of the language model with extracted vision features. (b) Methods that fuse vision information into the language model via cross-attention. (c) Our proposed ADEM-VL framework, which incorporates parameter-free cross-attention, multiscale visual prompting, and adaptive multimodal fusion designs. This approach ensures both parameter and computational efficiency while delivering promising performance.", "description": "The figure compares three different vision-language tuning frameworks: input space fusion, intermediate layer fusion with cross-attention, and the proposed ADEM-VL framework which uses parameter-free cross-attention, multiscale visual prompting, and adaptive fusion.", "section": "III. METHOD"}]