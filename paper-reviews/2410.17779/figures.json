[{"figure_path": "2410.17779/figures/figures_4_0.png", "caption": "Fig. 1. Comparison of different vision-language tuning frameworks: (a) Methods that directly extend the input space of the language model with extracted vision features. (b) Methods that fuse vision information into the language model via cross-attention. (c) Our proposed ADEM-VL framework, which incorporates parameter-free cross-attention, multiscale visual prompting, and adaptive multimodal fusion designs. This approach ensures both parameter and computational efficiency while delivering promising performance.", "description": "Figure 1 compares different vision-language tuning frameworks, highlighting the proposed ADEM-VL framework's efficient and adaptive multimodal fusion.", "section": "III. METHOD"}, {"figure_path": "2410.17779/figures/figures_11_0.png", "caption": "Fig. 3. Visualization of image captioning results with LLaMA-7B. In each row, the left figure is the original image, while the middle and right figures demonstrate the dropping decisions for features at two different scales.", "description": "The figure shows examples of image captioning results, visualizing the model's decisions on which image features to use at each layer for different scales.", "section": "IV. EXPERIMENT"}, {"figure_path": "2410.17779/figures/figures_11_1.png", "caption": "Fig. 1. Comparison of different vision-language tuning frameworks: (a) Methods that directly extend the input space of the language model with extracted vision features. (b) Methods that fuse vision information into the language model via cross-attention. (c) Our proposed ADEM-VL framework, which incorporates parameter-free cross-attention, multiscale visual prompting, and adaptive multimodal fusion designs. This approach ensures both parameter and computational efficiency while delivering promising performance.", "description": "The figure compares different vision-language tuning frameworks, highlighting the proposed ADEM-VL framework's efficiency and effectiveness.", "section": "III. METHOD"}, {"figure_path": "2410.17779/figures/figures_11_2.png", "caption": "Fig. 1. Comparison of different vision-language tuning frameworks: (a) Methods that directly extend the input space of the language model with extracted vision features. (b) Methods that fuse vision information into the language model via cross-attention. (c) Our proposed ADEM-VL framework, which incorporates parameter-free cross-attention, multiscale visual prompting, and adaptive multimodal fusion designs. This approach ensures both parameter and computational efficiency while delivering promising performance.", "description": "The figure compares different vision-language tuning frameworks, highlighting the proposed ADEM-VL framework's efficient and adaptive multimodal fusion approach.", "section": "III. METHOD"}, {"figure_path": "2410.17779/figures/figures_11_3.png", "caption": "Fig. 3. Visualization of image captioning results with LLaMA-7B. In each row, the left figure is the original image, while the middle and right figures demonstrate the dropping decisions for features at two different scales.", "description": "Figure 3 shows examples of image captioning results, visualizing how the model makes decisions on which image features to use based on their importance for each text token at different scales.", "section": "IV. EXPERIMENT"}, {"figure_path": "2410.17779/figures/figures_11_4.png", "caption": "Fig. 1. Comparison of different vision-language tuning frameworks: (a) Methods that directly extend the input space of the language model with extracted vision features. (b) Methods that fuse vision information into the language model via cross-attention. (c) Our proposed ADEM-VL framework, which incorporates parameter-free cross-attention, multiscale visual prompting, and adaptive multimodal fusion designs. This approach ensures both parameter and computational efficiency while delivering promising performance.", "description": "The figure compares three different vision-language tuning frameworks, highlighting the proposed ADEM-VL framework's efficiency and effectiveness in multimodal fusion.", "section": "III. METHOD"}, {"figure_path": "2410.17779/figures/figures_11_5.png", "caption": "Fig. 3. Visualization of image captioning results with LLaMA-7B. In each row, the left figure is the original image, while the middle and right figures demonstrate the dropping decisions for features at two different scales.", "description": "The figure visualizes image captioning results, showing the original image alongside visualizations of the model's decisions on which image features to drop at different scales.", "section": "IV. EXPERIMENT"}, {"figure_path": "2410.17779/figures/figures_11_6.png", "caption": "Fig. 3. Visualization of image captioning results with LLaMA-7B. In each row, the left figure is the original image, while the middle and right figures demonstrate the dropping decisions for features at two different scales.", "description": "The figure shows how the adaptive fusion scheme in ADEM-VL dynamically discards less relevant visual features for improved efficiency and performance in image captioning.", "section": "IV. EXPERIMENT"}, {"figure_path": "2410.17779/figures/figures_12_0.png", "caption": "Fig. 4. Examples of zero-shot instruction-following tasks with LLaMA-7B.", "description": "The figure shows example outputs from a vision-language model performing zero-shot instruction following tasks, demonstrating its ability to generate coherent and relevant responses based on image and instruction inputs.", "section": "V. CONCLUSION"}, {"figure_path": "2410.17779/figures/figures_12_1.png", "caption": "Fig. 1. Comparison of different vision-language tuning frameworks: (a) Methods that directly extend the input space of the language model with extracted vision features. (b) Methods that fuse vision information into the language model via cross-attention. (c) Our proposed ADEM-VL framework, which incorporates parameter-free cross-attention, multiscale visual prompting, and adaptive multimodal fusion designs. This approach ensures both parameter and computational efficiency while delivering promising performance.", "description": "The figure compares three different vision-language tuning frameworks, highlighting the proposed ADEM-VL framework's efficient and adaptive multimodal fusion approach.", "section": "III. METHOD"}, {"figure_path": "2410.17779/figures/figures_12_2.png", "caption": "Fig. 1. Comparison of different vision-language tuning frameworks: (a) Methods that directly extend the input space of the language model with extracted vision features. (b) Methods that fuse vision information into the language model via cross-attention. (c) Our proposed ADEM-VL framework, which incorporates parameter-free cross-attention, multiscale visual prompting, and adaptive multimodal fusion designs. This approach ensures both parameter and computational efficiency while delivering promising performance.", "description": "The figure compares different vision-language tuning frameworks, highlighting the proposed ADEM-VL framework's efficiency and effectiveness through parameter-free cross-attention, multiscale visual prompting, and adaptive fusion.", "section": "III. METHOD"}, {"figure_path": "2410.17779/figures/figures_12_3.png", "caption": "Fig. 1. Comparison of different vision-language tuning frameworks: (a) Methods that directly extend the input space of the language model with extracted vision features. (b) Methods that fuse vision information into the language model via cross-attention. (c) Our proposed ADEM-VL framework, which incorporates parameter-free cross-attention, multiscale visual prompting, and adaptive multimodal fusion designs. This approach ensures both parameter and computational efficiency while delivering promising performance.", "description": "The figure compares different vision-language tuning frameworks, highlighting the proposed ADEM-VL framework's efficiency and effectiveness through parameter-free cross-attention, multiscale visual prompting, and adaptive fusion.", "section": "III. METHOD"}]