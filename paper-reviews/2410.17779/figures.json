[{"figure_path": "2410.17779/figures/figures_4_0.png", "caption": "Fig. 1. Comparison of different vision-language tuning frameworks: (a) Methods that directly extend the input space of the language model with extracted vision features. (b) Methods that fuse vision information into the language model via cross-attention. (c) Our proposed ADEM-VL framework, which incorporates parameter-free cross-attention, multiscale visual prompting, and adaptive multimodal fusion designs. This approach ensures both parameter and computational efficiency while delivering promising performance.", "description": "This figure compares three different vision-language tuning frameworks. (a) shows input space fusion approaches, where visual features are directly concatenated with text tokens before being fed into the language model.  (b) illustrates intermediate layer fusion approaches, which use cross-attention modules to fuse visual and textual information at intermediate layers of the language model. Finally, (c) presents the proposed ADEM-VL framework, highlighting its key components: parameter-free cross-attention, multiscale visual prompting, and adaptive multimodal fusion.  These components aim to improve efficiency by reducing parameters and computation while enhancing performance.", "section": "III. METHOD"}, {"figure_path": "2410.17779/figures/figures_11_0.png", "caption": "Fig. 3. Visualization of image captioning results with LLaMA-7B. In each row, the left figure is the original image, while the middle and right figures demonstrate the dropping decisions for features at two different scales.", "description": "The figure displays three rows, each showing an original image on the left and two visualizations of the attention mechanism on the right. Each visualization represents one scale, showing the attention weights of visual features as an alpha mask over the original image.  Darker regions indicate the visual features given less attention by the model and may be dropped during adaptive fusion. This illustrates how ADEM-VL dynamically discards less relevant visual information for each text token based on its attention score.", "section": "IV. EXPERIMENT"}, {"figure_path": "2410.17779/figures/figures_11_1.png", "caption": "Comparison of different vision-language tuning frameworks: (a) Methods that directly extend the input space of the language model with extracted vision features. (b) Methods that fuse vision information into the language model via cross-attention. (c) Our proposed ADEM-VL framework, which incorporates parameter-free cross-attention, multiscale visual prompting, and adaptive multimodal fusion designs. This approach ensures both parameter and computational efficiency while delivering promising performance.", "description": "The figure compares three different vision-language tuning frameworks. (a) shows input space fusion approaches, where vision features are directly concatenated with text tokens as input to the language model. (b) illustrates intermediate layer fusion approaches, using cross-attention to fuse visual and textual information within the language model.  (c) presents the ADEM-VL framework, the authors' proposed method.  ADEM-VL uses parameter-free cross-attention to reduce parameters, multiscale visual prompting to generate richer visual representations, and adaptive fusion to focus on relevant information.  This design combines aspects of the prior approaches and aims to improve efficiency without sacrificing performance.", "section": "III. METHOD"}, {"figure_path": "2410.17779/figures/figures_11_2.png", "caption": "Fig. 1. Comparison of different vision-language tuning frameworks: (a) Methods that directly extend the input space of the language model with extracted vision features. (b) Methods that fuse vision information into the language model via cross-attention. (c) Our proposed ADEM-VL framework, which incorporates parameter-free cross-attention, multiscale visual prompting, and adaptive multimodal fusion designs. This approach ensures both parameter and computational efficiency while delivering promising performance.", "description": "The figure compares three different vision-language tuning frameworks. (a) shows input space fusion approaches, where vision features are directly concatenated with text tokens. (b) illustrates intermediate layer fusion approaches using cross-attention, which adds parameters and computational complexity. (c) presents the proposed ADEM-VL framework, which uses parameter-free cross-attention, multiscale visual prompting, and adaptive fusion to enhance efficiency and performance without significantly increasing parameters or computational cost.", "section": "III. METHOD"}, {"figure_path": "2410.17779/figures/figures_11_3.png", "caption": "Fig. 3. Visualization of image captioning results with LLaMA-7B. In each row, the left figure is the original image, while the middle and right figures demonstrate the dropping decisions for features at two different scales.", "description": "The figure displays three rows, each showing an original image on the left and two visualizations of feature dropping decisions on the right.  The middle and right images depict the same original image but represent the dropping decisions for visual features at two different scales. The alpha channel is used to represent the frequency value (normalized) for usage of each image patch for each text token.  More transparent patches represent less important features according to the model's dropping decisions.  This illustrates how ADEM-VL prioritizes features, often focusing on salient elements within the image while discarding background information or less relevant details.", "section": "IV. EXPERIMENT"}, {"figure_path": "2410.17779/figures/figures_11_4.png", "caption": "Comparison of different vision-language tuning frameworks: (a) Methods that directly extend the input space of the language model with extracted vision features. (b) Methods that fuse vision information into the language model via cross-attention. (c) Our proposed ADEM-VL framework, which incorporates parameter-free cross-attention, multiscale visual prompting, and adaptive multimodal fusion designs. This approach ensures both parameter and computational efficiency while delivering promising performance.", "description": "The figure compares three different vision-language tuning frameworks. (a) shows input space fusion approaches where vision features are directly concatenated with text tokens. (b) illustrates intermediate layer fusion methods using cross-attention to integrate visual and language features. Finally, (c) presents the ADEM-VL framework proposed in the paper. ADEM-VL incorporates three key improvements: parameter-free cross-attention to reduce parameters and computations, multiscale visual prompting for better representation learning, and adaptive fusion to selectively use only relevant visual information.", "section": "III. METHOD"}, {"figure_path": "2410.17779/figures/figures_11_5.png", "caption": "Fig. 3. Visualization of image captioning results with LLaMA-7B. In each row, the left figure is the original image, while the middle and right figures demonstrate the dropping decisions for features at two different scales.", "description": "The figure shows three rows, each illustrating the results of image captioning using a different image. In each row, the leftmost image is the original input image, while the middle and right images represent the same image with transparency applied to specific regions.  The transparency level in these images indicates the frequency with which features from those regions were dropped by the model's adaptive fusion scheme at two different scales (middle and right).  Darker regions show a high frequency of dropped features, while lighter regions indicate those features were retained more often.  The captions below the images provide the generated descriptions.", "section": "IV. EXPERIMENT"}, {"figure_path": "2410.17779/figures/figures_11_6.png", "caption": "Fig. 3. Visualization of image captioning results with LLaMA-7B. In each row, the left figure is the original image, while the middle and right figures demonstrate the dropping decisions for features at two different scales.", "description": "The figure shows three columns of images for three different image captioning examples.  The leftmost column displays the original input image. The middle and right columns visualize the attention weights applied to visual features from two different scales (presumably created through pooling) during the generation of the caption. Lighter areas in the middle and right images indicate that those visual features were deemed more important and thus were given more attention by the model during caption generation, and therefore had less \u201cdropping\u201d. Darker areas imply the model considered those visual features less relevant, thus dropping them and focusing attention elsewhere.", "section": "IV. EXPERIMENT"}, {"figure_path": "2410.17779/figures/figures_12_0.png", "caption": "Fig. 1. Comparison of different vision-language tuning frameworks: (a) Methods that directly extend the input space of the language model with extracted vision features. (b) Methods that fuse vision information into the language model via cross-attention. (c) Our proposed ADEM-VL framework, which incorporates parameter-free cross-attention, multiscale visual prompting, and adaptive multimodal fusion designs. This approach ensures both parameter and computational efficiency while delivering promising performance.", "description": "This figure compares three different vision-language tuning framework approaches.  (a) shows input space fusion, where visual features are directly added to the language model's input. (b) illustrates intermediate layer fusion using cross-attention, merging visual and textual information at intermediate layers of the language model. (c) presents the ADEM-VL framework proposed in the paper, combining parameter-free cross-attention, multiscale visual prompting, and adaptive fusion for efficient and effective multimodal tuning.  The figure highlights the differences in how vision information is integrated into the language model, emphasizing ADEM-VL's design for improved parameter and computational efficiency.", "section": "III. METHOD"}, {"figure_path": "2410.17779/figures/figures_12_1.png", "caption": "Comparison of different vision-language tuning frameworks: (a) Methods that directly extend the input space of the language model with extracted vision features. (b) Methods that fuse vision information into the language model via cross-attention. (c) Our proposed ADEM-VL framework, which incorporates parameter-free cross-attention, multiscale visual prompting, and adaptive multimodal fusion designs. This approach ensures both parameter and computational efficiency while delivering promising performance.", "description": "The figure compares three different vision-language tuning frameworks.  (a) shows input-space fusion approaches, where visual features are directly concatenated with text tokens. (b) illustrates intermediate-layer fusion approaches using cross-attention, which adds parameters and computational cost. (c) presents the ADEM-VL framework, which uses parameter-free cross-attention, a multiscale visual prompting scheme, and adaptive fusion to improve efficiency without sacrificing performance. ADEM-VL integrates visual information into the LLM layers using a cross-attention mechanism that reduces the number of trainable parameters. It also uses a multiscale approach to generate visual features and an adaptive fusion scheme to filter out less relevant information, making it more computationally efficient than the other methods.", "section": "III. METHOD"}, {"figure_path": "2410.17779/figures/figures_12_2.png", "caption": "Fig. 1. Comparison of different vision-language tuning frameworks: (a) Methods that directly extend the input space of the language model with extracted vision features. (b) Methods that fuse vision information into the language model via cross-attention. (c) Our proposed ADEM-VL framework, which incorporates parameter-free cross-attention, multiscale visual prompting, and adaptive multimodal fusion designs. This approach ensures both parameter and computational efficiency while delivering promising performance.", "description": "This figure compares three different vision-language tuning frameworks.  (a) shows input-space fusion approaches, where visual features are directly concatenated with text tokens before being fed to the language model. (b) depicts intermediate-layer fusion approaches, using cross-attention mechanisms to fuse visual features into the language model's intermediate layers. (c) illustrates the proposed ADEM-VL framework, which combines parameter-free cross-attention, a multiscale visual prompting scheme, and an adaptive fusion mechanism for efficient and effective vision-language tuning.", "section": "III. METHOD"}, {"figure_path": "2410.17779/figures/figures_12_3.png", "caption": "Fig. 1. Comparison of different vision-language tuning frameworks: (a) Methods that directly extend the input space of the language model with extracted vision features. (b) Methods that fuse vision information into the language model via cross-attention. (c) Our proposed ADEM-VL framework, which incorporates parameter-free cross-attention, multiscale visual prompting, and adaptive multimodal fusion designs. This approach ensures both parameter and computational efficiency while delivering promising performance.", "description": "This figure compares three different approaches to vision-language tuning frameworks.  (a) shows input space fusion methods, where visual features are directly concatenated with text tokens before entering the language model. (b) illustrates intermediate layer fusion methods, using cross-attention modules to fuse visual and textual features at intermediate layers of the language model.  Finally, (c) presents the proposed ADEM-VL framework, highlighting its parameter-free cross-attention, the incorporation of multiscale visual features, and its adaptive fusion mechanism which dynamically selects relevant visual information for each text token.", "section": "III. METHOD"}]