{"references": [{" publication_date": "2023", "fullname_first_author": "H. Touvron", "paper_title": "Llama: Open and efficient foundation language models", "reason": "This paper introduces LLaMA, a foundational large language model used as the base model in the ADEM-VL framework.  The efficiency and openness of LLaMA are crucial to the overall efficiency and reproducibility of ADEM-VL.  Its selection as the base model underscores the importance of using efficient pre-trained language models for vision-language tasks.", "section_number": 1}, {" publication_date": "2021", "fullname_first_author": "A. Radford", "paper_title": "Learning transferable visual models from natural language supervision", "reason": "CLIP is the foundation for visual feature extraction within ADEM-VL, demonstrating the crucial role of  pre-trained visual models in achieving efficient and effective vision-language integration.  The transferability of CLIP's visual representations, as highlighted in this paper, directly supports ADEM-VL's goal of efficient multimodal tuning.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "J.-B. Alayrac", "paper_title": "Flamingo: a visual language model for few-shot learning", "reason": "This paper presents Flamingo, a vision-language model that utilizes cross-attention for multimodal fusion.  The ADEM-VL model builds upon this concept but introduces key improvements to enhance efficiency by utilizing a parameter-free cross-attention mechanism.  The comparison with Flamingo clearly highlights ADEM-VL's advancements in efficiency.", "section_number": 1}, {" publication_date": "2021", "fullname_first_author": "E. J. Hu", "paper_title": "Lora: Low-rank adaptation of large language models", "reason": "This paper introduces LoRA, a parameter-efficient fine-tuning technique.  ADEM-VL draws inspiration from PEFT methods like LoRA, but it goes further by employing a parameter-free cross-attention mechanism which is even more computationally efficient than LoRA.  The discussion in this paper serves to contextualize the advancements in ADEM-VL.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "H. Liu", "paper_title": "Visual instruction tuning", "reason": "This paper introduces visual instruction tuning, an important method for efficient multimodal tuning.  ADEM-VL, although conceptually different in its fusion scheme, shares the same objective of efficient multimodal tuning.  The comparison with this method helps to highlight the unique contributions of ADEM-VL in the context of multimodal tuning.", "section_number": 1}, {" publication_date": "2021", "fullname_first_author": "A. Vaswani", "paper_title": "Attention is all you need", "reason": "This seminal paper introduced the transformer architecture and its attention mechanism, which forms the foundation for many modern vision-language models, including ADEM-VL. The ADEM-VL model uses the cross-attention mechanism which this paper heavily influenced.  Its mention thus sets the stage for explaining the modifications ADEM-VL introduces.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "R. Zhang", "paper_title": "Llama-adapter: Efficient fine-tuning of language models with zero-init attention", "reason": "This paper proposes Llama-adapter, a PEFT method for LLMs. ADEM-VL draws inspiration from parameter-efficient methods, but it differs significantly by focusing on the cross-attention module itself rather than adding adapter modules.  This direct optimization of cross-attention is a key innovation of ADEM-VL.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "G. Luo", "paper_title": "Cheap and quick: Efficient vision-language instruction tuning for large language models", "reason": "This paper presents LaVIN, an efficient vision-language model that is compared directly to ADEM-VL in the experiments.  LaVIN serves as a strong baseline, showing ADEM-VL's competitive and superior performance in accuracy and efficiency. The comparison is crucial for demonstrating the superiority of ADEM-VL.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "E. B. Zaken", "paper_title": "BitFit: Simple parameter-efficient fine-tuning for transformer-based masked language-models", "reason": "This paper introduces BitFit, a parameter-efficient fine-tuning technique.  ADEM-VL, while using a different approach, also prioritizes parameter efficiency.  The discussion of BitFit helps to place ADEM-VL within the broader context of PEFT methods, highlighting the specific contributions of the proposed framework.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "P. Gao", "paper_title": "Llama-adapter v2: Parameter-efficient visual instruction model", "reason": "This paper presents LLaMA-Adapter V2, a parameter-efficient visual instruction tuning method.  The ADEM-VL framework is compared directly to LLaMA-Adapter V2 in experiments and demonstrates superior performance. The comparison underscores the improvements achieved by ADEM-VL in efficiency and accuracy.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "R. Karimi Mahabadi", "paper_title": "Compacter: Efficient low-rank hypercomplex adapter layers", "reason": "This paper introduces Compacter, another PEFT method for LLMs which also attempts parameter efficiency.  By comparing against this and other PEFT approaches, the paper places ADEM-VL\u2019s innovation in context. The specific contribution of ADEM-VL lies in its focus on optimizing the cross-attention mechanism itself.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "J. Li", "paper_title": "Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation", "reason": "BLIP is an important vision-language model that is directly compared against ADEM-VL in experiments.  The results showcase the superiority of ADEM-VL, especially in terms of efficiency.  This paper helps set a benchmark for performance evaluation of ADEM-VL.", "section_number": 4}, {" publication_date": "2022", "fullname_first_author": "J. Li", "paper_title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models", "reason": "BLIP-2 is another strong vision-language model compared against ADEM-VL in the experiments.  Similar to BLIP, the comparison with BLIP-2 further highlights ADEM-VL's efficiency and performance gains, especially given the resource constraints that PEFT methods try to alleviate.", "section_number": 4}, {" publication_date": "2022", "fullname_first_author": "R. Mokady", "paper_title": "ClipCap: Clip prefix for image captioning", "reason": "This paper introduces ClipCap, a model for image captioning that is used as a baseline for comparison in ADEM-VL's experiments on COCO Caption dataset.  By outperforming ClipCap, ADEM-VL establishes its competitiveness in image captioning tasks, illustrating its versatility.", "section_number": 4}, {" publication_date": "2021", "fullname_first_author": "W. Wang", "paper_title": "Cogvlm: Visual expert for pretrained language models", "reason": "CogVLM is another relevant model compared to ADEM-VL in experiments. Its inclusion as a baseline reinforces the experimental rigor and supports a stronger claim regarding ADEM-VL's position in the field.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "D. Zhu", "paper_title": "Minigpt-4: Enhancing vision-language understanding with advanced large language models", "reason": "MiniGPT-4 is a strong contender for efficient vision-language models and is used as a comparison point for ADEM-VL.  The experimental comparisons highlight the advantages of ADEM-VL, especially in terms of parameter efficiency and performance.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "P. Lu", "paper_title": "Learn to explain: Multimodal reasoning via thought chains for science question answering", "reason": "ScienceQA is a key dataset used for evaluation in the paper, and this paper introduces the dataset itself.  The dataset is central to evaluating ADEM-VL's performance, highlighting its capabilities in multimodal reasoning.", "section_number": 4}, {" publication_date": "2015", "fullname_first_author": "X. Chen", "paper_title": "Microsoft coco captions: Data collection and evaluation server", "reason": "COCO Captions is a widely-used benchmark dataset for image captioning.  Its use demonstrates the breadth of the ADEM-VL's application to various vision-language tasks and the validity of its performance comparisons.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "R. Taori", "paper_title": "Stanford alpaca: An instruction-following llama model", "reason": "Alpaca is a key dataset used in the instruction-following task, showcasing ADEM-VL's ability to handle various instructions and highlighting its generalized capabilities in instruction-following tasks compared to methods specifically focused on image-text.", "section_number": 4}]}