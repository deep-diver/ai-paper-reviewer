[{"figure_path": "2410.17779/tables/table_6_0.html", "caption": "TABLE I\nEVALUATION RESULTS ON SCIENCEQA TEST SET. NAT = NATURAL SCIENCE, SOC = SOCIAL SCIENCE, LAN = LANGUAGE SCIENCE, TXT = TEXT\nCONTEXT, IMG = IMAGE CONTEXT, NO = NO CONTEXT, G1-6 = GRADES 1-6, G7-12 = GRADES 7-12.", "description": "Table 1 shows the quantitative comparison of different VL model tuning approaches on the ScienceQA dataset, reporting the average accuracy on the test set, and the number of parameters of different methods.", "section": "IV. EXPERIMENT"}, {"figure_path": "2410.17779/tables/table_7_0.html", "caption": "TABLE I\nEVALUATION RESULTS ON SCIENCEQA TEST SET. NAT = NATURAL SCIENCE, SOC = SOCIAL SCIENCE, LAN = LANGUAGE SCIENCE, TXT = TEXT CONTEXT, IMG = IMAGE CONTEXT, NO = NO CONTEXT, G1-6 = GRADES 1-6, G7-12 = GRADES 7-12.", "description": "Table I presents a comparison of various vision-language models' performance on the ScienceQA dataset, categorized by method type (zero-/few-shot, full training, parameter-efficient fine-tuning), model size, and context modality, showing the average accuracy achieved.", "section": "IV. EXPERIMENT"}, {"figure_path": "2410.17779/tables/table_7_1.html", "caption": "TABLE II\nEVALUATION RESULTS ON COCO CAPTION USING THE KARPATHY TEST\nSPLIT WITH LLAMA-13B AS THE LANGUAGE MODEL. #T. = TRAINABLE\nPARAMETERS. *PEFT METHODS.", "description": "The table presents quantitative results of different vision-language models on the COCO Caption dataset, comparing the performance of various approaches in terms of BLEU-4 and CIDEr scores, along with the number of trainable parameters.", "section": "IV. EXPERIMENT"}, {"figure_path": "2410.17779/tables/table_8_0.html", "caption": "TABLE III\nEVALUATION RESULTS ON THE MME BENCHMARK WITH LLAMA-13B AS THE LANGUAGE MODEL. MME-C AND MME-P MEASURE THE PERCEPTION\nAND COGNITION ABILITIES OF THE MODEL, RESPECTIVELY. EXTRA TOKENS REFER TO THE NUMBER OF ADDITIONAL TOKENS PROCESSED BY THE LLM\nBEYOND THE STANDARD TEXT TOKENS. #T. = TRAINABLE PARAMETERS. *PEFT METHODS.", "description": "Table III presents a comparison of different vision-language models on the MME benchmark, showing the number of trainable parameters, extra tokens, and performance scores for perception and cognition.", "section": "IV. EXPERIMENT"}, {"figure_path": "2410.17779/tables/table_8_1.html", "caption": "TABLE IV\nCOMPARISON AMONG DIFFERENT VL MODELS ON MORE IMAGE UNDERSTANDING TASKS. * BASELINE RESULTS EVALUATED THROUGH OUR\nIMPLEMENTATION USING THE OFFICIAL CHECKPOINT.", "description": "Table IV compares the performance of different vision-language models on various image understanding tasks, including the number of trainable parameters and the performance on VQAv2, GQA, MMB, and MMMU benchmarks.", "section": "IV. EXPERIMENT"}, {"figure_path": "2410.17779/tables/table_9_0.html", "caption": "TABLE V\nTRAINING AND INFERENCE SPEED OF DIFFERENT APPROACHES. MEMORY-SAVING OR SPEED-UP APPROACHES SUCH AS CHECKPOINTING AND\nFLAASHATTENTION ARE NOT ADOPTED. FLOPS ARE ESTIMATED FOR GENERATING A SINGLE NEW TOKEN WITH A TEXT SEQUENCE LENGTH OF 256.\nEXPERIMENTS ON COCO CAPTIONING AND INSTRUCTION-FOLLOWING WERE NOT IMPLEMENTED IN THE ORIGINAL PAPERS OF LLAVA-LORA AND\nMEMVP, SO THE OVERALL TRAINING TIME FOR THESE TASKS IS UNAVAILABLE.", "description": "This table compares the training and inference speed, along with the number of parameters and FLOPs, of various vision-language models.", "section": "IV. EXPERIMENT"}, {"figure_path": "2410.17779/tables/table_9_1.html", "caption": "TABLE VI\nABLATION STUDY OF EACH MODULE IN OUR ADEM-VL FRAMEWORK WITH LLAMA-7B AS THE LANGUAGE MODEL.", "description": "Table VI presents the ablation study of each component in the ADEM-VL framework using LLaMA-7B as the language model, showing the impact of each module on the average accuracy across various categories and grades.", "section": "IV. EXPERIMENT"}, {"figure_path": "2410.17779/tables/table_9_2.html", "caption": "TABLE VII\nCOMPARISON OF DIFFERENT LOCATIONS FOR INSERTING\nCROSS-ATTENTION MODULES WITH LLAMA-7B AS THE LANGUAGE\nMODEL. \"QUERY FROM\" INDICATES WHICH FEATURES OF THE LANGUAGE\nMODEL SERVE AS INPUTS TO THE CROSS-ATTENTION MODULES, WHILE\n\"ADD TO\" INDICATES WHERE THE OUTPUT OF THESE MODULES IS FUSED\nINTO THE FEATURES OF THE LANGUAGE MODEL BY ADDITION.", "description": "The table compares the average accuracy on the ScienceQA dataset using different configurations of cross-attention module placement within the language model.", "section": "IV. EXPERIMENT"}, {"figure_path": "2410.17779/tables/table_9_3.html", "caption": "TABLE VIII\nCOMPARISON OF DIFFERENT NON-PARAMETERIZED LINEAR PROJECTION\nIN EQUATION 3 WITH LLAMA-7B AS THE LANGUAGE MODEL.", "description": "Table VIII compares different non-parameterized linear projection methods used in Equation 3 of the ADEM-VL framework, showing their impact on the average accuracy when using LLaMA-7B as the language model.", "section": "IV. EXPERIMENT"}, {"figure_path": "2410.17779/tables/table_10_0.html", "caption": "TABLE IX\nCOMPARISON OF DIFFERENT DOWNSAMPLING METHODS AND SCALES IN\nGENERATING MULTIMODAL VISUAL PROMPTS WITH LLAMA-7B AS THE\nLANGUAGE MODEL.", "description": "Table IX shows the comparison of different downsampling methods and scales in generating multimodal visual prompts with LLaMA-7B as the language model, presenting average accuracy results for various configurations.", "section": "IV. EXPERIMENT"}, {"figure_path": "2410.17779/tables/table_10_1.html", "caption": "TABLE X\nINTEGRATION WITH DIFFERENT INPUT-STAGE FUSION SCHEMES WITH\nLLAMA-7B AS THE LANGUAGE MODEL.", "description": "Table X shows the impact of integrating different input-stage fusion schemes on the performance of the ADEM-VL model using LLaMA-7B, comparing the use of [cls] tokens and different numbers of visual tokens.", "section": "IV. EXPERIMENT"}]