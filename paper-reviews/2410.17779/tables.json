[{"figure_path": "2410.17779/tables/table_6_0.html", "caption": "TABLE I\nEVALUATION RESULTS ON SCIENCEQA TEST SET. NAT = NATURAL SCIENCE, SOC = SOCIAL SCIENCE, LAN = LANGUAGE SCIENCE, TXT = TEXT\nCONTEXT, IMG = IMAGE CONTEXT, NO = NO CONTEXT, G1-6 = GRADES 1-6, G7-12 = GRADES 7-12.", "description": "Table I presents a quantitative comparison of different vision-language model tuning approaches on the ScienceQA dataset, showing the average accuracy, number of trainable parameters, and context modality for each method.", "section": "IV. EXPERIMENT"}, {"figure_path": "2410.17779/tables/table_7_0.html", "caption": "TABLE I\nEVALUATION RESULTS ON SCIENCEQA TEST SET. NAT = NATURAL SCIENCE, SOC = SOCIAL SCIENCE, LAN = LANGUAGE SCIENCE, TXT = TEXT\nCONTEXT, IMG = IMAGE CONTEXT, NO = NO CONTEXT, G1-6 = GRADES 1-6, G7-12 = GRADES 7-12.", "description": "Table 1 presents a comparison of various vision-language models' performance on the ScienceQA dataset, categorized by method type, trainable parameters, and performance metrics across different subjects, context modalities, and grade levels.", "section": "IV. EXPERIMENT"}, {"figure_path": "2410.17779/tables/table_7_1.html", "caption": "TABLE II\nEVALUATION RESULTS ON COCO CAPTION USING THE KARPATHY TEST\nSPLIT WITH LLAMA-13B AS THE LANGUAGE MODEL. #T. = TRAINABLE\nPARAMETERS. *PEFT METHODS.", "description": "Table II presents quantitative results of different vision-language models on the COCO Caption dataset, showing the number of trainable parameters, BLEU-4 scores, and CIDEr scores.", "section": "IV. EXPERIMENT"}, {"figure_path": "2410.17779/tables/table_8_0.html", "caption": "TABLE III\nEVALUATION RESULTS ON THE MME BENCHMARK WITH LLAMA-13B AS THE LANGUAGE MODEL. MME-C AND MME-P MEASURE THE PERCEPTION\nAND COGNITION ABILITIES OF THE MODEL, RESPECTIVELY. EXTRA TOKENS REFER TO THE NUMBER OF ADDITIONAL TOKENS PROCESSED BY THE LLM\nBEYOND THE STANDARD TEXT TOKENS. #T. = TRAINABLE PARAMETERS. *PEFT METHODS.", "description": "Table III presents a comparison of different vision-language models on the MME benchmark, showing the number of trainable parameters, extra tokens processed, and performance metrics (MME-P and MME-C).", "section": "IV. EXPERIMENT"}, {"figure_path": "2410.17779/tables/table_8_1.html", "caption": "TABLE IV\nCOMPARISON AMONG DIFFERENT VL MODELS ON MORE IMAGE UNDERSTANDING TASKS. * BASELINE RESULTS EVALUATED THROUGH OUR\nIMPLEMENTATION USING THE OFFICIAL CHECKPOINT.", "description": "Table IV compares the performance of different vision-language models on various image understanding tasks, including the number of trainable parameters and the performance on VQAv2, GQA, MMB, and MMMU benchmarks.", "section": "IV. EXPERIMENT"}, {"figure_path": "2410.17779/tables/table_9_0.html", "caption": "TABLE V\nTRAINING AND INFERENCE SPEED OF DIFFERENT APPROACHES. MEMORY-SAVING OR SPEED-UP APPROACHES SUCH AS CHECKPOINTING AND\nFLAASHATTENTION ARE NOT ADOPTED. FLOPS ARE ESTIMATED FOR GENERATING A SINGLE NEW TOKEN WITH A TEXT SEQUENCE LENGTH OF 256.\nEXPERIMENTS ON COCO CAPTIONING AND INSTRUCTION-FOLLOWING WERE NOT IMPLEMENTED IN THE ORIGINAL PAPERS OF LLAVA-LORA AND\nMEMVP, SO THE OVERALL TRAINING TIME FOR THESE TASKS IS UNAVAILABLE.", "description": "Table V presents a comparison of training and inference speed across different vision-language models, highlighting the efficiency of the proposed ADEM-VL framework in terms of training and inference time and computational cost.", "section": "IV. EXPERIMENT"}, {"figure_path": "2410.17779/tables/table_9_1.html", "caption": "TABLE VI\nABLATION STUDY OF EACH MODULE IN OUR ADEM-VL FRAMEWORK WITH LLAMA-7B AS THE LANGUAGE MODEL.", "description": "Table VI presents the ablation study of each component in the ADEM-VL framework using LLaMA-7B, showing the impact of each module on the average accuracy of the ScienceQA dataset.", "section": "IV. EXPERIMENT"}, {"figure_path": "2410.17779/tables/table_9_2.html", "caption": "TABLE VII\nCOMPARISON OF DIFFERENT LOCATIONS FOR INSERTING\nCROSS-ATTENTION MODULES WITH LLAMA-7B AS THE LANGUAGE\nMODEL. \"QUERY FROM\" INDICATES WHICH FEATURES OF THE LANGUAGE\nMODEL SERVE AS INPUTS TO THE CROSS-ATTENTION MODULES, WHILE\n\"ADD TO\" INDICATES WHERE THE OUTPUT OF THESE MODULES IS FUSED\nINTO THE FEATURES OF THE LANGUAGE MODEL BY ADDITION.", "description": "This table compares the performance of different placements of cross-attention modules within the language model, showing where the input query comes from and where the output is added, using LLaMA-7B.", "section": "C. Ablation study"}, {"figure_path": "2410.17779/tables/table_9_3.html", "caption": "TABLE VIII\nCOMPARISON OF DIFFERENT NON-PARAMETERIZED LINEAR PROJECTION\nIN EQUATION 3 WITH LLAMA-7B AS THE LANGUAGE MODEL.", "description": "Table VIII compares the performance of different non-parameterized linear projection functions used in Equation 3 of the ADEM-VL model with LLaMA-7B.", "section": "IV. EXPERIMENT"}, {"figure_path": "2410.17779/tables/table_10_0.html", "caption": "TABLE IX\nCOMPARISON OF DIFFERENT DOWNSAMPLING METHODS AND SCALES IN\nGENERATING MULTIMODAL VISUAL PROMPTS WITH LLAMA-7B AS THE\nLANGUAGE MODEL.", "description": "Table IX shows the comparison of different downsampling methods and scales in generating multimodal visual prompts with LLaMA-7B as the language model.", "section": "IV. EXPERIMENT"}, {"figure_path": "2410.17779/tables/table_10_1.html", "caption": "TABLE X\nINTEGRATION WITH DIFFERENT INPUT-STAGE FUSION SCHEMES WITH\nLLAMA-7B AS THE LANGUAGE MODEL.", "description": "Table X shows the average accuracy results on the ScienceQA dataset when integrating different input-stage fusion schemes with LLaMA-7B as the language model, demonstrating the impact of adding various numbers of visual tokens.", "section": "IV. EXPERIMENT"}]