[{"figure_path": "https://arxiv.org/html/2412.01819/x2.png", "caption": "Figure 1: Switti produces high quality and aesthetic 512\u00d7512512512512{\\times}512512 \u00d7 512 image samples in around 0.130.130.130.13 seconds.", "description": "This figure showcases the high-quality and aesthetically pleasing 512x512 pixel images generated by the SWITTI model.  The remarkable speed of image generation, approximately 0.13 seconds per image, is a key highlight of the model's capabilities.  The images demonstrate the model's ability to translate text prompts into detailed and visually appealing artwork.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2412.01819/x3.png", "caption": "Figure 2: Transformer block in the Switti model.", "description": "This figure details the architecture of a transformer block within the SWITTI model.  It shows the flow of information through multiple components, including multi-head self-attention and cross-attention mechanisms, feed-forward networks (FFNs) with SwiGLU activation, layer normalization (LN), and RMSNorm layers.  The figure highlights how textual information from CLIP ViT-L and ViT-bigG encoders, along with positional embeddings and cropping parameters, is integrated into the model's processing. This detailed illustration helps to understand how the model processes input and generates the output image.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2412.01819/x4.png", "caption": "Figure 3: Last transformer block activation norms over training.", "description": "This figure shows the activation norms of the last transformer block over the course of training.  It illustrates how these norms change across various model architectures.  Specifically, the plot tracks the evolution of activation norms to reveal stability issues and the effectiveness of modifications made to address these issues. Different colored lines represent different model variations (e.g., using BF16 or FP32 precision, different normalization techniques, and different activation functions). The x-axis represents the training iteration, and the y-axis shows the activation norm (log scale).", "section": "3.2. Training dynamics of the basic architecture"}, {"figure_path": "https://arxiv.org/html/2412.01819/x5.png", "caption": "Figure 4: Evaluation metrics over training for various architectures.", "description": "This figure shows the training curves for different model architectures, comparing metrics such as CLIP score, FID, and PickScore over training iterations. It helps to visualize and analyze the performance and convergence of different architectural choices during the training process.", "section": "3.2 Training dynamics of the basic architecture"}, {"figure_path": "https://arxiv.org/html/2412.01819/x6.png", "caption": "Figure 5: Visualization of the block-wise self-attention masks in VAR (Left) and Switti (Right).", "description": "This figure compares the self-attention masks used in the VAR (Visual Autoregressive) and SWITTI models.  The left panel shows the block-wise causal mask of VAR, illustrating that the model attends only to previously generated tokens within a block. This is a standard approach in autoregressive models for image generation where the model predicts the next token(s) based on previous ones. The right panel displays the block-wise non-causal mask of SWITTI, showcasing that, unlike VAR, SWITTI's self-attention is not restricted to previously generated tokens within a block.  This change indicates that SWITTI predicts tokens across all scales concurrently, rather than sequentially. This difference in the attention mechanisms reflects the core architectural distinction between VAR and SWITTI\u2014autoregressive prediction versus non-autoregressive parallel prediction.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2412.01819/x7.png", "caption": "Figure 6: \nVisualization of self-attention maps at different scales for Switti (AR).\nThe model attends mostly to the current scale.", "description": "This figure visualizes the self-attention maps of the scale-wise autoregressive transformer model, SWITTI (AR), at different scales.  Each map shows the attention weights between different image tokens across various scales.  The key observation highlighted is that the model's attention is predominantly focused on the current scale being processed, with significantly weaker attention given to preceding scales. This indicates a reduced reliance on previously generated scales during the image generation process.", "section": "3.3 Exploring self-attention maps across different scales"}, {"figure_path": "https://arxiv.org/html/2412.01819/x9.png", "caption": "Figure 7: Visualization of cross-attention maps at different scales for a random text prompt.", "description": "This figure visualizes the cross-attention maps across different scales of the SWITTI model for a randomly selected text prompt.  The heatmaps show the attention weights between text embeddings and image tokens at each scale. Darker colors indicate stronger attention. Analyzing these maps reveals how the model's attention shifts across scales in response to the text prompt, offering insights into how the model integrates textual information across different levels of image detail during generation.", "section": "3.3 Exploring self-attention maps"}, {"figure_path": "https://arxiv.org/html/2412.01819/extracted/6034750/figures/model_comparison/main_image_grid_different_models.png", "caption": "Figure 8: Textual prompt switching during image generation.", "description": "This figure shows the results of an experiment where the textual prompt was switched during the image generation process at different scales. The results demonstrate that the model's reliance on the text decreases as the resolution increases. This behavior suggests that higher-resolution scales are primarily determined by lower resolution scales rather than the textual input alone.", "section": "3.4 The role of text conditioning"}, {"figure_path": "https://arxiv.org/html/2412.01819/x10.png", "caption": "Figure 9: Human study comparing Switti with competing AR, diffusion-based models. Error bars correspond to a 95% confidence interval.", "description": "This figure displays the results of a human evaluation study comparing the performance of SWITTI with other leading autoregressive (AR) and diffusion-based text-to-image models.  The models are evaluated across four key aspects: relevance, aesthetics, complexity, and the presence of defects. The chart shows the average scores for each model and aspect, with error bars representing 95% confidence intervals, providing a visual representation of the statistical significance of the results. This allows for a direct comparison of SWITTI's performance against established methods in terms of human perception of image quality.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.01819/x11.png", "caption": "Figure 10: Qualitative comparison of Switti against the baselines.", "description": "This figure presents a qualitative comparison of images generated by SWITTI and other state-of-the-art models, including HART, DMD2, SDXL, and SD3.  For each model, four different image samples are shown, illustrating the models' capabilities in generating diverse and detailed images from varied text prompts.  The prompts cover a range of styles and subject matter, allowing for a visual assessment of each model's strengths and weaknesses in terms of image quality, detail, and adherence to the prompt.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.01819/extracted/6034750/figures/VAE/anime-orig.png", "caption": "Figure 11: Illustrative examples when disabled CFG at the last scales (Bottom) mitigates the artifacts in fine-grained details (Top).", "description": "This figure shows a comparison of image generation results with and without classifier-free guidance (CFG) enabled at the final scales of the generation process. The top row displays images generated with CFG enabled, showing some artifacts in the fine details. The bottom row displays images generated with CFG disabled at the last scales, demonstrating that disabling CFG can mitigate these artifacts and improve the quality of fine details.", "section": "3.4. The role of text conditioning"}, {"figure_path": "https://arxiv.org/html/2412.01819/extracted/6034750/figures/VAE/anime-ft.png", "caption": "Figure 12: Transformer block of the basic architecture.", "description": "This figure details the architecture of a transformer block used in the SWITTI model.  It shows the flow of information through multiple components, including multi-head self-attention and cross-attention mechanisms, feed-forward networks (FFNs), layer normalization (LN), and RMSNorm layers.  The figure highlights how text embeddings from CLIP ViT-L and ViT-bigG are incorporated, along with positional embeddings.  The use of SwiGLU activation within the FFN is also depicted.  This block is a fundamental building block repeated within the larger SWITTI model, and the diagram helps clarify the internal processing and information flow within each block.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2412.01819/extracted/6034750/figures/VAE/text-orig.png", "caption": "Figure 13: Visual comparison between original RQ-VAE (left) and with fine-tuned decoder (right).", "description": "This figure shows a visual comparison of the image reconstruction quality between the original RQ-VAE model (left) and the RQ-VAE model with a fine-tuned decoder (right).  The fine-tuning process improved the quality of image reconstruction in terms of color accuracy and the reduction of artifacts.", "section": "4.3 RQ-VAE tuning"}, {"figure_path": "https://arxiv.org/html/2412.01819/extracted/6034750/figures/VAE/text-ft.png", "caption": "Figure 14: Qualitative comparison of Switti against the baselines.", "description": "This figure shows a qualitative comparison of images generated by SWITTI and several baseline models on different prompts. Each row represents a different prompt, and each column displays images generated by a different model (SWITTI, HART, DMD2, SDXL, and SD3). This allows for a visual comparison of the image quality and style produced by each model in response to various prompts.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.01819/extracted/6034750/figures/VAE/dolly-orig.png", "caption": "Figure 15: Impact of CFG at different resolution levels. There are 10101010 levels: higher index indicates higher resolution. The guidance scale is 6666.", "description": "This figure demonstrates the effect of classifier-free guidance (CFG) at different resolution levels during image generation.  The experiment uses 10 resolution levels, with higher indices representing higher resolutions.  Each row shows the results for a different image prompt, and each column represents the results obtained with CFG applied up to a specific level. The guidance scale is kept constant at 6 for all experiments. By comparing the images across columns in each row, one can observe how the level of CFG application affects the quality and detail of the generated images. This visualization helps to assess the impact of CFG at various resolutions, showing whether higher-resolution CFG application is necessary or even beneficial.", "section": "3.4. The role of text conditioning"}, {"figure_path": "https://arxiv.org/html/2412.01819/extracted/6034750/figures/VAE/dolly-ft.png", "caption": "Figure 16: Impact of the prompt switching during image generation.", "description": "This figure shows the results of an experiment where the text prompt is changed during image generation.  For each prompt, images are generated with the prompt switching at different scales (2nd, 4th, 5th, 6th, 8th, and 9th). The images showcase how the change in prompt affects the final image at different stages of the generation process, highlighting the impact of prompt switching at various resolutions.", "section": "3.4. The role of text conditioning"}, {"figure_path": "https://arxiv.org/html/2412.01819/extracted/6034750/figures/VAE/face-orig.png", "caption": "Figure 17: Impact of the prompt switching during image generation.", "description": "This figure shows the results of an experiment where the text prompt was changed during the image generation process at different scales.  It demonstrates how the model's response varies depending on when the prompt switch occurs.  Specifically, it shows that changes made in earlier scales have a larger impact on the final image than changes introduced in later scales, suggesting that the model's reliance on the text prompt diminishes as the resolution increases. Two example prompts are used: one for an image of a cat and one for an image of a forest.", "section": "3.4. The role of text conditioning"}, {"figure_path": "https://arxiv.org/html/2412.01819/extracted/6034750/figures/VAE/face-ft.png", "caption": "Figure 18: Human evaluation interface for aesthetics.", "description": "This figure shows the interface used in a human evaluation study to assess the aesthetic quality of images generated by different models. The evaluators were presented with pairs of images and asked to compare them based on multiple aspects of visual aesthetics, including brightness and contrast, presence of unnatural colors, glow effects, the visibility and number of main and secondary objects, background and environment, and the overall level of detail. The evaluators could provide a nuanced rating for each aspect by selecting options such as \"Image 1 is better,\" \"Image 2 is better,\" or \"The images are equal in this aspect.\" ", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.01819/extracted/6034750/figures/model_comparison/supp_image_grid_different_models.png", "caption": "Figure 19: Human evaluation interface for defects.", "description": "This figure shows the user interface used for the human evaluation of image defects.  The interface presents two images side-by-side and asks evaluators to assess various aspects of image quality, such as defects in the composition, presence of watermarks, extra objects, and defects in the main or secondary objects.  Evaluators select an option indicating which image is better or whether they are comparable. The final decision is based on the aggregated responses across several criteria.", "section": "5.2 Human evaluation"}, {"figure_path": "https://arxiv.org/html/2412.01819/extracted/6034750/figures/human_eval/aest1.png", "caption": "Figure 20: Human evaluation interface for relevance.", "description": "The figure shows the user interface used for human evaluation of the relevance of generated images to the given text prompt.  The interface presents two images side-by-side, allowing evaluators to compare them. Evaluators are asked to select which image is more relevant to the text prompt, indicating their preference through a simple selection mechanism. The interface also allows evaluators to indicate if the images are of comparable quality or if they cannot make a decision. This helps gather fine-grained feedback on model performance regarding text-image alignment.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.01819/extracted/6034750/figures/human_eval/aest2.png", "caption": "Figure 21: Human evaluation interface for image complexity.", "description": "This figure shows the user interface used for the human evaluation of image complexity in the SWITTI paper.  The interface presents two images side-by-side to assessors, who are asked to judge which image is more complex based on the given prompt. Assessors can choose between 'Image 1 is better', 'Image 2 is better', 'Can't decide', or 'The images are uncomparable' as options to select and submit their responses.", "section": "5. Experiments"}]