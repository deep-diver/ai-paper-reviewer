[{"content": "| Model | PSNR \u2191 | SSIM \u2191 | LPIPS \u2193 | CLIP-IQA \u2191 |\n|---|---|---|---|---|\n| Original | 21.603 | 0.634 | 0.200 | 0.727 |\n| Fine-tuned | 22.267 | 0.653 | 0.188 | 0.772 |", "caption": "Table 1: Comparison of a fine-tuned and an original RQ-VAE.", "description": "This table presents a quantitative comparison of the performance of an original RQ-VAE (Residual Quantization Variational Autoencoder) and a fine-tuned version.  The RQ-VAE is a crucial component of the SWITTI model, responsible for encoding and decoding images into a latent representation. The fine-tuning process aimed to optimize the decoder for better performance at a resolution of 512x512 pixels. The comparison uses four metrics: PSNR (Peak Signal-to-Noise Ratio), SSIM (Structural Similarity Index), LPIPS (Learned Perceptual Image Patch Similarity), and CLIP-IQA (CLIP Image Quality Assessment). Higher values are generally better for PSNR and SSIM, while lower values are better for LPIPS.  CLIP-IQA is a no-reference metric, where higher values are preferred. This comparison helps showcase the effectiveness of fine-tuning the RQ-VAE for improved image reconstruction quality within the SWITTI architecture.", "section": "4. Model Training"}, {"content": "| Model | Latency, s/image | Parameters count, B | COCO 30K eval prompts |  |  |  | MJHQ 30K eval prompts |  |  |  | GenEval | \n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| **Distilled Diffusion Models** |  |  | Pickscore \u2191 | CLIP \u2191 | IR \u2191 | FID \u2193 | Pickscore \u2191 | CLIP \u2191 | IR \u2191 | FID \u2193 |  \u2191 | \n| SDXL-Turbo [60] | 0.251 | 2.6B | 0.229 | 0.355 | 0.83 | 17.6 | 0.216 | 0.365 | 0.84 | 15.7 | 0.55 |\n| DMD2 [84] | 0.251 | 2.6B | 0.231 | 0.356 | 0.87 | 14.3 | 0.219 | 0.374 | 0.87 | 7.2 | 0.58 |\n| **Diffusion Models** |  |  |  |  |  |  |  |  |  |  |  |\n| SDXL [51] | 0.867 | 2.6B | 0.226 | 0.360 | 0.77 | 14.4 | 0.217 | 0.384 | 0.78 | 7.6 | 0.55 |\n| SD3 [15] | 0.934 | 2.0B | 0.227 | 0.354 | 1.01 | 19.5 | 0.215 | 0.363 | 0.91 | 13.1 | 0.65 |\n| Lumina-Next [92] | 5.812 | 2.0B | 0.224 | 0.329 | 0.55 | 18.4 | 0.216 | 0.353 | 0.75 | 5.9 | 0.47 |\n| **Autoregressive Models** |  |  |  |  |  |  |  |  |  |  |  |\n| LlamaGen [68] | 3.821 | 0.8B | 0.208 | 0.274 | -0.25 | 44.8 | 0.194 | 0.288 | -0.45 | 26.9 | 0.32 |\n| HART [69] | 0.063 | 0.7B | 0.223 | 0.341 | 0.75 | 20.9 | 0.216 | 0.366 | 0.84 | 5.8 | 0.55 |\n| Switti (AR) (ours) | 0.139 | 2.5B | 0.227 | 0.354 | 0.91 | 18.1 | 0.217 | 0.378 | 0.88 | 9.5 | 0.61 |\n| Switti (ours) | 0.127 | 2.5B | 0.227 | 0.356 | 0.95 | 17.6 | 0.217 | 0.381 | 0.91 | 9.5 | 0.63 |", "caption": "Table 2: Quantitative comparison of Switti to other competing open-source models. The best model is highlighted in red, the second-best in blue, and the third-best in yellow according to the respective automated metric.", "description": "Table 2 presents a quantitative comparison of the SWITTI model's performance against other open-source text-to-image generation models.  Metrics include CLIPScore, ImageReward, PickScore, FID, and GenEval, evaluated on the MS-COCO and MJHQ datasets.  The table highlights the top three performing models for each metric using color-coding (red for best, blue for second-best, yellow for third-best) to facilitate easy comparison and identify SWITTI's strengths and weaknesses relative to the competition.", "section": "5.1 Automated metrics evaluation"}, {"content": "| Latency, | s/image |\n|---|---|", "caption": "Table 3: Comparison of models\u2019 512\u00d7512512512512{\\times}512512 \u00d7 512 image generation time. All models are run in half-precision with a batch size of 8.", "description": "This table compares the time taken to generate a 512x512 image using various models.  It breaks down the generation time into two components: the time for a single generator step (excluding text encoding and VAE decoding) and the total time for the full text-to-image pipeline.  All models were run using half-precision and a batch size of 8 to ensure a fair comparison. The table differentiates between diffusion models, distilled diffusion models, and autoregressive models, highlighting the speed differences between different architectural approaches.", "section": "5.3 Inference performance evaluation"}, {"content": "| Parameters |\n|---|---| \n| count, B |", "caption": "Table 4: Human evaluation of Switti design choices.\nScores are the mean of Setup 1 wins plus half-ties, with subscripts denoting half the 95% confidence interval.", "description": "This table presents the results of a human evaluation study comparing different design choices for the SWITTI model.  The study measured user preferences across four key aspects: relevance, aesthetics, complexity, and presence of defects.  Two different setups were used, and scores represent the average of 'Setup 1 wins' and 'half-ties', with confidence intervals provided to indicate the statistical significance of the results. The study provides insights into which design choices contribute most to the overall quality and performance of the SWITTI model.", "section": "3.4. The role of text conditioning"}, {"content": "| Model | Generator size, B | N steps | 1 step, ms/image | Full, s/image |\n|---|---|---|---|---|\n| **Distilled Diffusion Models** |  |  |  |  |\n| SDXL-Turbo | 2.6 | 4 | 12.4 | 0.251 |\n| DMD2 | 2.6 | 4 | 12.4 | 0.251 |\n| **Diffusion Models** |  |  |  |  |\n| SDXL | 2.6 | 25 | 12.4 | 0.867 |\n| SD3 | 2.0 | 28 | 16.8 | 0.934 |\n| **Autoregressive Models** |  |  |  |  |\n| Lumina-mGPT | 7.0 | 1024 | \u2014 | 224.2 |\n| LlamaGen | 0.8 | 1024 | \u2014 | 3.821 |\n| HART | 0.7 | 10 | 4.7* | 0.063 |\n| Switti (AR) | 2.5 | 10 | 11.2* | 0.139 |\n| Switti | 2.5 | 10 | 9.5* | 0.127 |\n* Time averaged over 10 steps.", "caption": "Table 5: Latency comparison of architecture and sampling modifications proposed in Switti.", "description": "This table presents a comparison of the latency (time taken for image generation) of different model variations and sampling techniques within the SWITTI framework. It shows how various architectural choices and modifications to the sampling process impact the overall speed of image generation. Specifically, it contrasts the latency of the original scale-wise autoregressive (AR) model with the final non-autoregressive SWITTI model, along with variations in using classifier-free guidance (CFG).  The comparison helps to quantify the performance gains achieved by removing the autoregressive component and disabling CFG at higher resolution scales. The results highlight the efficiency improvements obtained through the architectural and sampling modifications.", "section": "5. Experiments"}, {"content": "| Generator | size, B |\n|---|---|", "caption": "Table 6: Human evaluation of Switti against its no-SFT version.", "description": "This table presents the results of a human evaluation comparing the performance of the SWITTI model with and without supervised fine-tuning (SFT).  It shows how SFT affects various aspects of image generation quality, such as relevance, aesthetics, complexity, and the presence of defects.  The scores represent the average of human preferences, providing a quantitative measure of the improvement gained by using SFT in SWITTI.", "section": "5.4. Ablation study"}]