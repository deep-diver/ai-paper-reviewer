[{"figure_path": "https://arxiv.org/html/2411.17945/x2.png", "caption": "Figure 1: Left: Examples of MARVEL annotations created using our proposed pipeline, which produces high-quality, domain-specific and multi-level text descriptions for 3D assets (Sec\u00a03.1). Right: Qualitative results from MARVEL-FX3D, our two-stage text-to-3D pipeline, which can generate textured mesh from text within 15s (Sec\u00a03.3). Please zoom in for details.", "description": "This figure showcases the capabilities of the MARVEL annotation pipeline and the MARVEL-FX3D text-to-3D pipeline. The left panel displays examples of high-quality, domain-specific, and multi-level text descriptions automatically generated for various 3D assets using the MARVEL pipeline.  These descriptions range in detail from brief semantic tags to detailed descriptions (~200 words) that encompass object names, shapes, materials, colors, contextual environments, etc. The descriptions are tailored to facilitate both rapid prototyping and fine-grained 3D reconstruction.  The right panel presents qualitative results from the MARVEL-FX3D pipeline.  This two-stage pipeline is able to create textured 3D meshes from text prompts in just 15 seconds. The image shows examples of 3D models generated by MARVEL-FX3D, demonstrating the quality and detail achievable using this novel approach. The images are diverse in subject matter and style, suggesting the versatility of the method.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2411.17945/x3.png", "caption": "Figure 2: Left: MARVEL annotation pipeline for 3D assets. Our pipeline starts with human metadata\u00a0[18, 17] and rendered multi-view images to create detailed visual descriptions using InternVL-2\u00a0[13]. These contain object names, shapes, textures, colors, and environments. Qwen2\u00a0[85] then processes these descriptions into five hierarchical levels, progressively compressing different aspects of the 3D assets. Right: Our Text-to-3D pipeline finetunes SD 3.5\u00a0[21, 3] with this dataset and uses pretrained SF3D\u00a0[7] to generate a textured mesh in 15s .", "description": "Figure 2 illustrates the MARVEL annotation pipeline and the MARVEL-FX3D text-to-3D pipeline. The left panel details the multi-stage MARVEL annotation pipeline.  It starts by leveraging human metadata and multi-view renderings of 3D assets to produce detailed descriptions using the InternVL-2 VLM. These descriptions encompass object names, shapes, textures, colors, and environmental contexts.  The Qwen-2 LLM then processes these descriptions, generating five hierarchical levels of annotations. Each level provides increasingly concise descriptions of the 3D assets, ranging from comprehensive descriptions to simple semantic tags. The right panel showcases the two-stage MARVEL-FX3D text-to-3D pipeline.  It fine-tunes Stable Diffusion 3.5 with the generated annotations and uses the pretrained SF3D model for rapid image-to-3D conversion, generating textured meshes from text within 15 seconds.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2411.17945/x4.png", "caption": "Figure 3: Qualitative Annotation Comparison: From top to bottom Cap3D\u00a0[53], 3DTopia\u00a0[28], Kabra\u00a0[34], MARVEL (Level-4) annotations and GPT-4\u00a0[60] evaluation. MARVEL consistently provides the most comprehensive and precise annotations, capturing intricate details such as object names, color, structure, and specific attributes. Red is for wrong captions.", "description": "Figure 3 presents a qualitative comparison of 3D model annotations from four different methods: Cap3D, 3DTopia, Kabra, and MARVEL.  For each method, example annotations for four distinct 3D models are displayed, along with an evaluation by GPT-4. The figure showcases that MARVEL's Level-4 annotations are significantly more comprehensive and detailed than those produced by the other methods. MARVEL captures intricate aspects, including object names, colors, shapes, textures, and specific attributes, while the other methods frequently miss key details or provide inaccurate descriptions. The red highlighting indicates instances where annotations incorrectly describe the image.", "section": "4.1 Annotation Evaluation"}, {"figure_path": "https://arxiv.org/html/2411.17945/x5.png", "caption": "Figure 4: Visual results of high fidelity TT3D generation. Left to right, the reconstructed 3D assets from Shap-E [33], DreamFusion [62], Lucid-Dreamer [43], HIFA [91] and MARVEL-FX3D.", "description": "Figure 4 presents a comparison of 3D model generation results from five different text-to-3D methods: Shap-E, DreamFusion, LucidDreamer, HIFA, and MARVEL-FX3D.  Each method is given the same text prompt, and the resulting 3D model is shown. This allows for a visual comparison of the fidelity and quality of the 3D models generated by each approach. The figure demonstrates the relative strengths and weaknesses of each method in terms of detail, texture, geometry accuracy, and overall visual appeal.", "section": "4.2. Text-to-3D Generation"}, {"figure_path": "https://arxiv.org/html/2411.17945/x7.png", "caption": "Figure 5: MARVEL uses human-generated metadata from source datasets to create detailed, accurate captions (e.g., names of the lunar craters, detection of human footprints) and reduce hallucinations. Without metadata, VLMs like GPT-4 [60] and InternVL2 [15] generate vague or speculative descriptions.", "description": "Figure 5 presents a comparative analysis of caption generation using various models, highlighting the impact of incorporating human-generated metadata. The figure shows that MARVEL, by integrating this metadata, generates highly detailed and accurate captions which include domain-specific information (e.g., names of lunar craters). In contrast, models like GPT-4 and InternVL2, which do not use metadata, produce descriptions that are vague and speculative.", "section": "4.3 Ablation Study"}, {"figure_path": "https://arxiv.org/html/2411.17945/x8.png", "caption": "Figure 6:  An example use case of MARVEL-FX3D, demonstrating how multiple prompts can be combined to create a detailed and complex 3D scene, with each prompt contributing specific elements such as characters, structures, and environmental details. Please zoom in for details.", "description": "Figure 6 showcases MARVEL-FX3D's ability to generate intricate and detailed 3D scenes by combining multiple text prompts.  Each prompt focuses on a specific aspect of the scene, such as characters, buildings, or environmental details. The resulting image demonstrates a complex scene with a high level of detail and realism, exceeding what is typically possible with single-prompt 3D generation models.  The image is rich and layered, with various objects interacting and visually relating to each other in a natural way.  It highlights the potential of combining multiple text descriptions to generate highly detailed and complex 3D content.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2411.17945/x9.png", "caption": "Figure 7: Effect of including human metadata, highlighting improvements in descriptive accuracy and contextual relevance compared to outputs generated without metadata, even when using state-of-the-art models like GPT-4\u00a0[60] and InternVL2\u00a0[13]. Metadata inclusion helps reduce hallucinations and enhances domain-specific understanding.", "description": "Figure 7 demonstrates the impact of integrating human-generated metadata into a multi-stage annotation pipeline for 3D models.  It compares the descriptive accuracy and contextual relevance of captions generated by state-of-the-art models (GPT-4 and InternVL2) with and without this metadata. The results show that including human metadata significantly improves the quality of annotations by reducing hallucinations and incorporating crucial domain-specific details. This enhancement in caption accuracy and context is particularly noticeable when dealing with complex, domain-specific 3D assets.", "section": "4.3 Ablation Study"}, {"figure_path": "https://arxiv.org/html/2411.17945/x10.png", "caption": "Figure 8: Failure cases of the MARVEL annotation pipeline. From left to right, the examples illustrate errors such as counting mistakes, object misidentification, and challenges with ambiguous views.", "description": "Figure 8 showcases instances where the MARVEL annotation pipeline encounters challenges.  The images illustrate three common types of errors: (1) Counting mistakes, where the generated caption incorrectly counts the number of items in the scene; (2) Object misidentification, where the caption wrongly labels or describes objects within the image; and (3) Difficulties interpreting ambiguous views, where the 3D model's perspective or the image's clarity makes accurate annotation challenging.", "section": "4.1 Annotation Evaluation"}, {"figure_path": "https://arxiv.org/html/2411.17945/x11.png", "caption": "Figure 9: Qualitative comparison of 3D annotations across baselines\u00a0[53, 28, 34] and the proposed MARVEL-40404040M+ for automotive (cars, planes, etc) and CAD models. MARVEL-40M+ provides more accurate and domain-specific annotations, compared to the baselines. Incorrect captions are highlighted in red, while important captions are highlighted in green.", "description": "This figure presents a qualitative comparison of 3D annotations generated by three existing methods (Cap3D [53], 3DTopia [28], Kabra [34]) and the new MARVEL-40M+ approach.  The comparison focuses specifically on the quality of annotations for 3D models of vehicles and CAD designs.  The figure visually demonstrates that MARVEL-40M+ produces more detailed and accurate captions than the baselines.  Incorrect annotations from the baseline methods are highlighted in red, while particularly strong or accurate elements in the MARVEL-40M+ annotations are highlighted in green.  This allows for a direct visual assessment of the relative strengths and weaknesses of each method in capturing intricate details and domain-specific terminology within 3D model descriptions.", "section": "4. Experiment"}, {"figure_path": "https://arxiv.org/html/2411.17945/x12.png", "caption": "Figure 10: Qualitative comparison of 3D annotations across baselines\u00a0[53, 28, 34] and the proposed MARVEL-40404040M+ for popular anime, movie, and cartoon characters. MARVEL-40M+ provides more accurate and domain-specific annotations, compared to the baselines. Incorrect captions are highlighted in red, while important captions are highlighted in green.", "description": "Figure 10 presents a qualitative comparison of 3D model annotations generated by three existing methods (Cap3D [53], 3DTopia [28], and Kabra et al. [34]) and the novel MARVEL-40M+ approach.  The figure uses examples of popular anime, movie, and cartoon characters to showcase the quality of the annotations produced by each method.  The annotations from MARVEL-40M+ are shown to be significantly more detailed and accurate, capturing specific characteristics, names, and contextual details that the baseline methods miss. Incorrect annotations from the baselines are highlighted in red, while the superior annotations from MARVEL-40M+ are highlighted in green. This visual comparison demonstrates the advantage of MARVEL-40M+ in generating high-quality, domain-specific annotations for 3D models.", "section": "4. Experiment"}, {"figure_path": "https://arxiv.org/html/2411.17945/x13.png", "caption": "Figure 11: Qualitative comparison of 3D annotations across baselines\u00a0[53, 28, 34] and the proposed MARVEL-40404040M+ for biological objects, including animals, plants, and molecular models. MARVEL-40M+ provides more accurate and domain-specific annotations, compared to the baselines. Incorrect captions are highlighted in red, while important captions are highlighted in green.", "description": "Figure 11 presents a qualitative comparison of 3D annotations generated by three existing methods (Cap3D [53], 3DTopia [28], and Kabra et al. [34]) and the newly proposed MARVEL-40M+ model. The comparison focuses on annotations for biological objects, including animals, plants, and molecular structures.  The figure highlights that MARVEL-40M+ produces more accurate and detailed annotations that better reflect domain-specific characteristics compared to the baselines. Incorrect or inaccurate annotations generated by the baseline methods are marked in red, while especially accurate and insightful annotations generated by MARVEL-40M+ are highlighted in green. This visual comparison demonstrates the improved quality and detail of annotations from the proposed MARVEL-40M+ method.", "section": "4.1 Annotation Evaluation"}, {"figure_path": "https://arxiv.org/html/2411.17945/x14.png", "caption": "Figure 12: Qualitative comparison of 3D annotations across baselines\u00a0[53, 28, 34] and the proposed MARVEL-40404040M+ for diverse items including daily objects, essentials. MARVEL-40M+ provides more accurate and domain-specific annotations, compared to the baselines. Incorrect captions are highlighted in red, while important captions are highlighted in green.", "description": "Figure 12 presents a qualitative comparison of 3D annotations generated by three existing methods (Cap3D [53], 3DTopia [28], and Kabra [34]) and the new MARVEL-40M+ approach.  The comparison focuses on diverse everyday items and essential objects.  The figure highlights MARVEL-40M+'s improved accuracy and detail in its annotations compared to the baseline methods.  Incorrect or insufficient captions from the baseline methods are shown in red, while accurate and detailed MARVEL-40M+ captions are highlighted in green.  This visual comparison demonstrates MARVEL-40M+'s superiority in generating high-quality, domain-specific 3D descriptions.", "section": "4. Experiment"}, {"figure_path": "https://arxiv.org/html/2411.17945/x15.png", "caption": "Figure 13: Qualitative comparison of 3D annotations across baselines\u00a0[53, 28, 34] and the proposed MARVEL-40404040M+ for historical elements including statues, places, memorials, etc. Incorrect captions are highlighted in red, while important captions are highlighted in green.", "description": "Figure 13 presents a qualitative comparison of 3D model annotations generated by different methods: Cap3D [53], 3DTopia [28], Kabra [34], and the proposed MARVEL-40M+. The comparison focuses on historical elements such as statues, places, and memorials.  For each 3D model, annotations from each method are shown alongside an evaluation highlighting the accuracy and detail of the description. Incorrect annotations are marked in red, while accurate and complete annotations are highlighted in green. The figure visually demonstrates the superior quality and detail provided by the MARVEL-40M+ annotation pipeline.", "section": "4.1 Annotation Evaluation"}, {"figure_path": "https://arxiv.org/html/2411.17945/x16.png", "caption": "Figure 14: Qualitative comparison of 3D annotations across baselines\u00a0[53, 28, 34] and the proposed MARVEL-40404040M+ for diverse scenes including digital elevation maps, places, realistic or animated scenes. Incorrect captions are in red, while important captions are in green.", "description": "Figure 14 presents a qualitative comparison of 3D annotations generated by three baseline methods (Cap3D [53], 3DTopia [28], Kabra [34]) and the proposed MARVEL-40M+ approach.  The comparison focuses on diverse scene types, including digital elevation maps, real-world locations, and both realistic and animated settings.  For each scene, the annotations produced by each method are displayed alongside an image of the 3D model, enabling a direct visual assessment of accuracy and detail.  Annotations judged as incorrect are highlighted in red, while particularly accurate or important elements are highlighted in green. This allows for a visual evaluation of which model is capable of producing the most comprehensive and precise annotations for these varied scenes.", "section": "4. Experiment"}, {"figure_path": "https://arxiv.org/html/2411.17945/x17.png", "caption": "Figure 15: Multi-level annotation examples of MARVEL for the Objaverse\u00a0[18] dataset. Words corresponding to Object and Components are highlighted in violet, Shape and Geometry in green, Texture and Materials in orange, Colors in blue, and Contextual Environment in purple. From top to bottom, we go from level-5 (Concise Tags) captions to level-1 (Comprehensive Description) captions.", "description": "Figure 15 presents a detailed breakdown of MARVEL's multi-level annotation process using examples from the Objaverse dataset.  Each example showcases five levels of annotation detail, progressing from concise tags (Level 5) to a comprehensive description (Level 1).  The visual representation uses color-coding to highlight different aspects of the annotation: object and components (violet), shape and geometry (green), texture and materials (orange), colors (blue), and contextual environment (purple). This figure visually demonstrates how MARVEL generates increasingly detailed and nuanced descriptions of 3D objects at each level.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2411.17945/x18.png", "caption": "Figure 16: Multi-level annotation examples of MARVEL for the Omni-Object\u00a0[80] dataset. Words corresponding to Object and Components are highlighted in violet, Shape and Geometry in green, Texture and Materials in orange, Colors in blue, and Contextual Environment in purple. From top to bottom, we go from level-5 (Concise Tags) captions to level-1 (Comprehensive Description) captions.", "description": "Figure 16 showcases multi-level annotations generated by the MARVEL pipeline for a subset of the Omni-Object dataset.  Each row displays the annotation at a different level of detail, from concise semantic tags (Level 5) to detailed, comprehensive descriptions (Level 1). The annotations are color-coded to highlight different aspects of the 3D model: Object and Components (violet), Shape and Geometry (green), Texture and Materials (orange), Colors (blue), and Contextual Environment (purple). This figure demonstrates the hierarchical nature of MARVEL's annotation scheme and its ability to generate descriptions ranging from very brief to extremely detailed.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2411.17945/x19.png", "caption": "Figure 17: Multi-level annotation examples of MARVEL for the ShapeNet\u00a0[10] dataset. Words corresponding to Object and Components are highlighted in violet, Shape and Geometry in green, Texture and Materials in orange, Colors in blue, and Contextual Environment in purple. From top to bottom, we go from level-5 (Concise Tags) captions to level-1 (Comprehensive Description) captions.", "description": "Figure 17 presents a detailed analysis of MARVEL's multi-level annotation capabilities using examples from the ShapeNet dataset.  Each example shows five levels of annotation detail (from concise tags at Level 5 to comprehensive descriptions at Level 1).  The different levels progressively elaborate on the 3D object, starting with basic semantic tags and advancing to detailed descriptions of object components, shapes, materials, colors, and environment.  Each level's text is color-coded to highlight these aspects: Object and Components (violet), Shape and Geometry (green), Texture and Materials (orange), Colors (blue), and Contextual Environment (purple). This visualization effectively demonstrates how MARVEL generates increasingly detailed annotations at each level, illustrating its versatility for diverse 3D modeling applications.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2411.17945/x20.png", "caption": "Figure 18: Multi-level annotation examples of MARVEL for the Toys4K dataset. Words corresponding to Object and Components are highlighted in violet, Shape and Geometry in green, Texture and Materials in orange, Colors in blue, and Contextual Environment in purple. From top to bottom, we go from level-5 (Concise Tags) captions to level-1 (Comprehensive Description) captions.", "description": "Figure 18 showcases multi-level annotations generated by the MARVEL pipeline for a subset of the Toys4K dataset.  Each row shows the same 3D model (a dinosaur and a dog) with annotations at different levels of detail. Level 5 provides concise semantic tags, while Level 1 offers rich, detailed descriptions.  The annotations are color-coded to highlight different aspects: object components (violet), shape and geometry (green), texture and materials (orange), colors (blue), and contextual environment (purple). This figure demonstrates the pipeline's ability to generate annotations that range from short, keyword-like tags to extensive, descriptive paragraphs, suitable for a variety of downstream tasks.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2411.17945/x21.png", "caption": "Figure 19: Multi-level annotation examples of MARVEL for the ABO (Amazon Berkeley Objects)\u00a0[16] dataset. Words corresponding to Object and Components are highlighted in violet, Shape and Geometry in green, Texture and Materials in orange, Colors in blue, and Contextual Environment in purple. From top to bottom, we go from level-5 (Concise Tags) captions to level-1 (Comprehensive Description) captions.", "description": "Figure 19 presents a detailed comparison of multi-level annotations generated by the MARVEL pipeline for a subset of the ABO dataset.  Each row showcases a 3D object from the dataset, with five annotation levels provided for each: Level 5 (Concise Tags) offers a brief list of keywords; Level 4 (Summary) gives a short overview; Level 3 (Functional-Semantic Description) provides a basic description; Level 2 (Moderately Descriptive) offers a more detailed description, and Level 1 (Comprehensive Description) gives a comprehensive description.  The annotations highlight different aspects of the object: Object and Components are in violet, Shape and Geometry are in green, Texture and Materials are in orange, Colors are in blue, and Contextual Environment is in purple. This visualization demonstrates the hierarchical nature of the MARVEL annotation pipeline and the richness of information captured at each level.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2411.17945/x22.png", "caption": "Figure 20: Multi-level annotation examples of MARVEL for the GSO (Google Scanned Objects)\u00a0[20] dataset. Words corresponding to Object and Components are highlighted in violet, Shape and Geometry in green, Texture and Materials in orange, Colors in blue, and Contextual Environment in purple. From top to bottom, we go from level-5 (Concise Tags) captions to level-1 (Comprehensive Description) captions.", "description": "Figure 20 presents a detailed analysis of MARVEL's multi-level annotation capabilities using examples from the Google Scanned Objects (GSO) dataset.  Each example showcases five levels of annotation detail, ranging from concise semantic tags (Level 5) to comprehensive descriptions (Level 1). The visual representation uses color-coding to distinguish the different aspects of each annotation: Object and Components (violet), Shape and Geometry (green), Texture and Materials (orange), Colors (blue), and Contextual Environment (purple). This figure demonstrates how MARVEL annotates 3D objects with increasing levels of detail and richness to support various downstream tasks.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2411.17945/x23.png", "caption": "Figure 21: Multi-level annotation examples of MARVEL for the Pix3D\u00a0[74] dataset. Words corresponding to Object and Components are highlighted in violet, Shape and Geometry in green, Texture and Materials in orange, Colors in blue, and Contextual Environment in purple. From top to bottom, we go from level-5 (Concise Tags) captions to level-1 (Comprehensive Description) captions.", "description": "Figure 21 showcases multi-level annotations generated by the MARVEL pipeline for a subset of the Pix3D dataset.  Each row presents a different 3D object from the dataset.  The annotations are broken down into five levels of detail, ranging from very short semantic tags (Level 5) to a comprehensive description (Level 1).  Each level of description includes details about the object's components, shape, materials, colors, and environment, with the text color-coded to highlight the type of information being conveyed. This figure demonstrates the pipeline's ability to generate rich and diverse annotations suitable for both quick prototyping and high-fidelity 3D reconstruction.", "section": "3. Methodology"}]