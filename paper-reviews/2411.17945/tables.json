[{"content": "| Dataset | ShapeNet | Pix3D | OmniObject3D | Toys4K | GSO | ABO | Objaverse | Objaverse-XL | Total 3D Objects | Total Captions |\n|---|---|---|---|---|---|---|---|---|---|---|\n| Cap3D [53] | \u2717 | \u2717 | \u2717 | \u2717 | \u2717 | 6,400 [52] | 785,150 | 221,632 | 1,013,182 | 1,013,182 |\n| 3DTopia [28] | \u2717 | \u2717 | \u2717 | \u2717 | \u2717 | \u2717 | 361,357 | \u2717 | 361,357 | 361,357 |\n| Kabra [34] | \u2717 | \u2717 | \u2717 | \u2717 | \u2717 | \u2717 | 763,827 | \u2717 | 763,827 | 763,827 |\n| MARVEL | 51,209 | 735 | 5,878 | 4,000 | 1,030 | 7,953 | 798,759 | 8,031,637 | 8,901,201 | 44,506,005 |", "caption": "Table 1: Overview of datasets [10, 74, 80, 73, 20, 16, 18, 17] annotated using our MARVEL pipeline. MARVEL provides the most extensive 3D asset annotations to date, encompassing over 8.98.98.98.9M 3D objects and 40404040M captions.", "description": "Table 1 presents a comprehensive comparison of various 3D datasets used for annotation, highlighting the scale and scope of the MARVEL-40M+ dataset.  It shows the number of 3D objects and captions in each dataset, demonstrating that MARVEL-40M+ significantly surpasses existing datasets in terms of the sheer volume of annotated 3D assets and their associated text descriptions.  This larger scale and annotation depth are crucial for training high-fidelity text-to-3D models.", "section": "3. Methodology"}, {"content": "| Dataset | Average Length | MTLD<sup>[56]</sup> (@50K) | Unigram (@50K) | Bi-Gram (@50K) | GPT4 (@5K) | Human (@1K) |\n|---|---|---|---|---|---|---|\n| Cap3D<sup>[53]</sup> | 16 | 39.71 | 15,189 | 123,071 | 14.55 | 9.50 |\n| 3D-Topia<sup>[28]</sup> | 29 | 41.43 | 10,329 | 95,856 | 10.80 | 14.00 |\n| Kabra<sup>[34]</sup> | 5 | 25.85 | 3,862 | 19,753 | 2.24 | 3.10 |\n| MARVEL (Level 4) | **44** | **47.43** | **27,659** | **239,052** | **72.41** | **73.40** |", "caption": "Table 2: Quantitative comparison of annotation quality across datasets. MARVEL surpasses existing datasets [34, 28, 53] in all metrics, showcasing superior linguistic diversity, vocabulary coverage, and significantly higher ratings from GPT-4 and humans.", "description": "Table 2 presents a quantitative comparison of annotation quality across four different datasets: Cap3D, 3DTopia, Kabra, and MARVEL.  The table compares the datasets across several key metrics that evaluate the richness and diversity of the annotations. These metrics include: Average Length (number of words per caption), MTLD (Measure of Textual Lexical Diversity), Unigram (number of unique words), Bigram (number of unique word pairs), GPT-4 rating (percentage of human-quality annotations as rated by GPT-4), and Human Rating (percentage of human-quality annotations as rated by humans).  The results show MARVEL significantly outperforming the other datasets in all metrics. This indicates that MARVEL annotations are not only longer but also more linguistically diverse, contain richer vocabulary, and are deemed to be of higher quality by both automated and human evaluations.", "section": "4.1 Annotation Evaluation"}, {"content": "| Average |\n|---|---| \n| Length |", "caption": "Table 3: Comparison of caption accuracy using GPT-4V [60] and humans, highlighting MARVEL\u2019s (Level 1) superior consistency desite significantly higher caption length.", "description": "This table presents a comparison of caption accuracy results obtained using two different evaluation methods: GPT-4V and human evaluation.  The comparison is made across four different datasets, including MARVEL and three baselines (Cap3D, 3DTopia, and Kabra).  The metrics presented include the number of correct captions, the average caption length, and the accuracy scores obtained using both GPT-4V and human evaluators. The table highlights MARVEL's superior consistency in caption accuracy (Level 1) even though its captions are considerably longer than those in the other datasets.", "section": "4.1 Annotation Evaluation"}, {"content": "| MTLD [56] |\n|---|---|", "caption": "Table 4: Quantitative evaluation focusing on time and human evaluation criteria: geometric consistency, visual quality, prompt fidelity, and overall preference.", "description": "This table presents a quantitative comparison of different text-to-3D generation methods, focusing on both speed and human perception.  Four key aspects of the generated 3D models are evaluated: geometric consistency (how realistic and structurally sound the model is), visual quality (the aesthetics of the model), prompt fidelity (how well the model reflects the input text prompt), and overall preference (an overall assessment combining the previous three factors).  The results are based on human evaluations comparing MARVEL-FX3D against several leading baselines.", "section": "4.2 Text-to-3D Generation"}, {"content": "| Unigram |\n|---| \n| (@50K) |", "caption": "Table 5: Ablation study results showing SCS across MARVEL-40M levels, illustrating strong semantic retention through Levels 1-4 and reduced detail at Level 5.", "description": "This ablation study investigates the semantic similarity and compression ratio across different levels of annotation in the MARVEL-40M dataset.  It evaluates how well the semantic meaning is preserved as annotations are compressed from detailed descriptions (Level 1) to concise semantic tags (Level 5). The results demonstrate strong semantic retention through Levels 1-4, while Level 5 shows reduced detail, highlighting a trade-off between compression and semantic preservation.", "section": "4.3 Ablation Study"}]