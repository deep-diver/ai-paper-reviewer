[{"content": "| Configuration | Hunyuan-Large |\n|---|---| \n| # Layers | 64 |\n| # Attention Heads | 80 |\n| # Key/Value Heads | 8 |\n| # Shared Experts | 1 |\n| # Specialized Experts | 16 |\n| # Activated Specialized Experts | 1 |\n| # Trained Tokens | 7T |\n| Activation Function | SwiGLU |\n| Vocabulary Size | 128K |\n| Hidden Size | 6,400 |", "caption": "Table 1: Overview of the architecture and key hyper-parameters of Hunyuan-Large. This model has 389B total parameters and 52B activated parameters. There are 1 shared expert and 1 specialized expert activated for each token.", "description": "Table 1 presents a detailed breakdown of the Hunyuan-Large model's architecture and key hyperparameters.  It highlights the model's impressive scale, with 389 billion total parameters and 52 billion activated parameters.  The table clarifies the model's structure, specifying the number of layers, attention heads, key/value heads, and the unique configuration of experts (1 shared and 1 specialized expert activated per token). This level of detail is crucial for understanding the model's complexity and resource requirements.", "section": "2.2 Model Structure"}, {"content": "| Attention Mechanism | KV Cache Memory |\n|---|---| \n| MHA | 4n<sub>h</sub>d<sub>h</sub>l |\n| GQA | 4n<sub>g</sub>d<sub>h</sub>l |\n| MQA | 4d<sub>h</sub>l |\n| CLA | 2n<sub>h</sub>d<sub>h</sub>l |\n| GQA+CLA | 2n<sub>g</sub>d<sub>h</sub>l |", "caption": "Table 2: Comparisons of KV cache memory (in bytes on bf16) for different attention mechanisms. The attention mechanisms include Multi-Head Attention (MHA), Grouped-Query Attention (GQA), Multi-Query Attention (MQA), Cross-Layer Attention (CLA), and GQA+CLA (the final setting in Hunyuan-Large). nhsubscript\ud835\udc5b\u210en_{h}italic_n start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT, dhsubscript\ud835\udc51\u210ed_{h}italic_d start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT, l\ud835\udc59litalic_l, and ngsubscript\ud835\udc5b\ud835\udc54n_{g}italic_n start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT represent the number of attention heads, the dimension per head, the number of layers, and the number of groups in GQA (ngsubscript\ud835\udc5b\ud835\udc54n_{g}italic_n start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT<nhsubscript\ud835\udc5b\u210en_{h}italic_n start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT), respectively. Our CLA shares the KV cache every 2 layers.", "description": "This table compares the memory usage (in bytes, using bf16 precision) of different attention mechanisms used in Transformer models.  The comparison includes Multi-Head Attention (MHA), Grouped-Query Attention (GQA), Multi-Query Attention (MQA), and Cross-Layer Attention (CLA).  It also shows the combined effect of GQA and CLA, which is used in the Hunyuan-Large model. The table shows how the memory usage scales with the number of attention heads (nh), dimension per head (dh), number of layers (l), and number of groups in GQA (ng, where ng < nh).  Cross-Layer Attention (CLA) is implemented by sharing the KV cache every 2 layers. The table helps illustrate the memory savings achieved by using GQA+CLA in Hunyuan-Large compared to traditional MHA.", "section": "2.2.2 KV Cache Compression"}, {"content": "| Model | LLama3.1-405B | LLama3.1-70B | Mixtral-8x22B | DeepSeek-V2 | Hunyuan-Large |\n|---|---|---|---|---|---| \n| Architecture | Dense | Dense | MoE | MoE | MoE |\n| # Activated Params | **405B** | 70B | 39B | 21B | 52B |\n| # Total Params | **405B** | 70B | 141B | 236B | 389B |\n| Context Length | 128k | 128k | 64k | 128k | **256k** |\n|  | English |  |  |  |  |\n| MMLU | 85.2 | 79.3 | 77.8 | 78.5 | **88.4** |\n| MMLU-Pro | **61.6** | 53.8 | 49.5 | - | 60.2 |\n| BBH | 85.9 | 81.6 | 78.9 | 78.9 | **86.3** |\n| HellaSwag | - | - | **88.7** | 87.8 | 86.8 |\n| CommonsenseQA | 85.8 | 84.1 | 82.4 | - | **92.9** |\n| WinoGrande | 86.7 | 85.3 | 85.0 | 84.9 | **88.7** |\n| PIQA | - | - | 83.6 | 83.7 | **88.3** |\n| NaturalQuestions | - | - | 39.6 | 38.7 | **52.8** |\n| DROP | 84.8 | 79.6 | 80.4 | 80.1 | **88.9** |\n| ARC-C | **96.1** | 92.9 | 91.2 | 92.4 | 95.0 |\n| TriviaQA | - | - | 82.1 | 79.9 | **89.2** |\n|  | Chinese |  |  |  |  |\n| CMMLU | - | - | 60.0 | 84.0 | **90.2** |\n| C-Eval | - | - | 59.6 | 81.7 | **91.9** |\n| C3 | - | - | 71.4 | 77.4 | **82.3** |\n|  | Math |  |  |  |  |\n| GSM8K | 89.0 | 83.7 | 83.7 | 79.2 | **92.8** |\n| MATH | 53.8 | 41.4 | 42.5 | 43.6 | **69.8** |\n| CMATH | - | - | 72.3 | 78.7 | **91.3** |\n|  | Code |  |  |  |  |\n| HumanEval | 61.0 | 58.5 | 53.1 | 48.8 | **71.4** |\n| MBPP | **73.4** | 68.6 | 64.2 | 66.6 | 72.6 |", "caption": "Table 3: Performance of Hunyuan-Large\u2019s pre-trained model and its competitors.", "description": "This table compares the performance of Tencent's Hunyuan-Large pre-trained model against several other leading large language models (LLMs), including LLaMA3.1-70B, Mistral-8x22B, and DeepSeek-V2.  The comparison encompasses various benchmark tasks across English and Chinese languages, covering areas like language understanding, reasoning, math, coding, and commonsense.  Key metrics are presented for each model to demonstrate the relative strengths and weaknesses of each LLM on different task types.  The table highlights Hunyuan-Large's performance relative to other models of similar scale, and also includes information on the number of parameters and activated parameters for each model.  The context length each model can handle is also included.", "section": "4.1 Evaluations on Pre-Trained Model"}, {"content": "| Model | LLAMA 3.1 405B Inst. | LLAMA 3.1 70B Inst. | Mixtral 8x22B Inst. | DeepSeek V2.5 Chat | Hunyuan-Large Inst. |\n|---|---|---|---|---|---| \n| MMLU | 87.3 | 83.6 | 77.8 | 80.4 | **89.9** |\n| CMMLU | - | - | 61.0 | - | **90.4** |\n| C-Eval | - | - | 60.0 | - | **88.6** |\n| BBH | - | - | 78.4 | 84.3 | **89.5** |\n| ARC-C | **96.9** | 94.8 | 90.0 | - | 94.6 |\n| GPQA_diamond | **51.1** | 46.7 | - | - | 42.4 |\n| MATH | 73.8 | 68.0 | 49.8 | 74.7 | **77.4** |\n| HumanEval | 89.0 | 80.5 | 75.0 | 89.0 | **90.0** |\n| AlignBench | 6.0 | 5.9 | 6.2 | 8.0 | **8.3** |\n| MT-Bench | 9.1 | 8.8 | 8.1 | 9.0 | **9.4** |\n| IFEval strict-prompt | **86.0** | 83.6 | 71.2 | - | 85.0 |\n| Arena-Hard | 69.3 | 55.7 | - | 76.2 | **81.8** |\n| AlpacaEval-2.0 | 39.3 | 34.3 | 30.9 | 50.5 | **51.8** |", "caption": "Table 4: Performance of our Hunyuan-Large-Instruct and its competitors.", "description": "This table presents a comparison of the performance of Tencent's Hunyuan-Large-Instruct model against several other leading large language models (LLMs) across a range of benchmark tasks.  The benchmarks cover diverse areas such as commonsense reasoning, knowledge-based question answering, mathematical problem-solving, coding ability, and more.  The table allows readers to quickly evaluate the relative strengths and weaknesses of Hunyuan-Large-Instruct compared to its competitors, including models like LLaMA and Mixtral, by showing each model's performance score on each benchmark task.  The inclusion of different instruction types for some benchmarks further enriches the comparative analysis.", "section": "4.2 Evaluations on Post-Trained Models"}, {"content": "| Model | 0-8K | 8K-32K | 32K-64K | 64K-128K | 0-32K | 32K-64K | 64K-128K |\n|---|---|---|---|---|---|---|---|\n| LLama3.1-70B-Instruct | 95.89 | 95.39 | 94.72 | 86.48 | 75.73 | 62.39 | 61.57 |\n| Hunyuan-Large-Instruct | 94.39 | 94.94 | 93.02 | 89.53 | 81.92 | 71.15 | 67.87 |", "caption": "Table 5: The performance of Hunyuan-Large-Instruct on RULER and LV-Eval.", "description": "This table presents the performance comparison of the Hunyuan-Large-Instruct model and the LLama3.1-70B-Instruct model on two long-context benchmarks: RULER and LV-Eval.  RULER assesses performance across various tasks and context lengths, while LV-Eval focuses on question-answering with varying complexities and context lengths. The table shows the accuracy scores for each model across different context length ranges (0-8K, 8K-32K, 32K-64K, 64K-128K tokens). This allows for an analysis of how the model's performance changes as the context length increases.", "section": "4.3 Long-Context Capability Evaluations"}, {"content": "| Model | Information Extraction | Information Localization | Qualitative Analysis | Numerical Reasoning | Overall |\n|---|---|---|---|---|---| \n| LLama3.1-70B-Instruct | 82.51 | 69.70 | 75.77 | 49.52 | 69.37 |\n| Hunyuan-Large-Instruct | **91.14** | **89.56** | **92.78** | **67.46** | **85.23** |", "caption": "Table 6: The performance of Hunyuan-Large-Instruct on PenguinScrolls.", "description": "This table presents the performance of the Hunyuan-Large-Instruct model on the PenguinScrolls benchmark.  PenguinScrolls is a newly introduced, in-house long-context benchmark designed to evaluate LLMs' performance on real-world, diverse long-form text data.  The table shows the model's performance across four key sub-tasks within the benchmark: Information Extraction, Information Localization, Qualitative Analysis, and Numerical Reasoning.  The results are presented to show the model's capabilities and efficiency in handling extended text inputs across different task types.", "section": "4.3 Long-Context Capability Evaluations"}]