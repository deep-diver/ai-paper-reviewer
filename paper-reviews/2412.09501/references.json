{"references": [{"fullname_first_author": "Jean-Baptiste Alayrac", "paper_title": "Flamingo: a visual language model for few-shot learning", "publication_date": "2022-12-01", "reason": "This paper introduces Flamingo, a visual language model that is foundational to the development of Lyra's vision capabilities."}, {"fullname_first_author": "Abhimanyu Dubey", "paper_title": "The LLaMA 3 herd of models", "publication_date": "2024-07-21", "reason": "This paper describes the LLaMA 3 models, which serve as a crucial base for Lyra's architecture, providing its core large language model capabilities."}, {"fullname_first_author": "Peng Wang", "paper_title": "Qwen2-VL: Enhancing vision-language model's perception of the world at any resolution", "publication_date": "2024-09-12", "reason": "This paper details Qwen2-VL, a key visual language model leveraged by Lyra, providing its efficient multi-modality processing and capabilities."}, {"fullname_first_author": "Edward J Hu", "paper_title": "LoRA: Low-rank adaptation of large language models", "publication_date": "2021-01-01", "reason": "This paper introduces the LoRA technique, a crucial efficiency strategy employed by Lyra, allowing for low-cost, effective model adaptation and training."}, {"fullname_first_author": "Vassil Panayotov", "paper_title": "LibriSpeech: an ASR corpus based on public domain audio books", "publication_date": "2015-01-01", "reason": "This paper introduces the LibriSpeech dataset, a major component in Lyra's training data, which is critical for its speech-centric focus and advanced speech understanding."}]}