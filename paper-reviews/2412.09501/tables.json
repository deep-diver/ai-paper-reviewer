[{"content": "| Function | Method | Vision |  | Audio |  |  |  |  |\n|---|---|---|---|---|---|---|---|---|\n|  |  | Image | Video | SU | SG | LS | Sound |\n| Vision | LLaVA-OV | \u2713 | \u2713 | \u2717 | \u2717 | \u2717 | \u2717 |\n|  | Intern-VL | \u2713 | \u2713 | \u2717 | \u2717 | \u2717 | \u2717 |\n|  | Mini-Gemini | \u2713 | \u2713 | \u2717 | \u2717 | \u2717 | \u2717 |\n| Audio | Qwen-Audio | \u2717 | \u2717 | \u2713 | \u2717 | \u2717 | \u2713 |\n|  | Mini-Omni | \u2717 | \u2717 | \u2713 | \u2713 | \u2717 | \u2717 |\n|  | LLaMA-Omni | \u2717 | \u2717 | \u2713 | \u2713 | \u2717 | \u2717 |\n|  | Intern-Omni | \u2713 | \u2717 | \u2713 | \u2717 | \u2717 | \u2717 |\n|  | VITA | \u2713 | \u2713 | \u2713 | \u2717 | \u2717 | \u2717 |\n|  | Any-GPT | \u2713 | \u2713 | \u2713 | \u2713 | \u2717 | \u2717 |\n|  | EMOVA | \u2713 | \u2717 | \u2713 | \u2713 | \u2717 | \u2717 |\n| Omni | **Lyra** | \u2713 | \u2713 | \u2713 | \u2713 | \u2713 | \u2713 |", "caption": "Table 1: Function comparison of related work. SU, SG, and LS represents speech understanding, speech generation,\nand long speech support, respectively.", "description": "This table compares the capabilities of various related works in terms of their functionalities across different modalities.  Specifically, it focuses on the ability of each model to perform speech understanding (SU), speech generation (SG), and support for long speech inputs (LS). The table provides a quick overview of the strengths and weaknesses of each model concerning speech-related tasks, allowing for easy comparison and identification of state-of-the-art capabilities.  This helps the reader understand the unique contributions of the Lyra model in relation to existing methods and highlight its focus on efficient and speech-centric omni-cognition.", "section": "2. Related Work"}, {"content": "| TextVQA (S+I) | TextVQA (T+I) | MM-Vet (S+I) | MM-Vet (T+I) |\n|---|---|---|---| \n| 76.7 (-2.8) | 79.5 | 53.1 (-8.0) | 63.1 |", "caption": "Table 2: Omni-comparison on vision-language-speech benchmarks. BenchS indicates that it uses speech instruction as the input.", "description": "This table presents a comprehensive comparison of Lyra's performance against state-of-the-art models across various vision-language-speech benchmarks.  It shows the performance of each model on tasks involving different combinations of modalities (text, image, video, and speech), highlighting Lyra's superior performance in various multi-modal understanding tasks. The 'BenchS' suffix indicates results where the model received instructions in speech format, offering additional insights into the model's capacity to handle speech-based inputs.", "section": "4. Experiments"}, {"content": "| #(Token) | 100 | 150 | 300 | 500 | 1500 |\n|---|---|---|---|---|---| \n| TextVQA<sup>S</sup> | 75.9% | 76.8% | 77.8% | 78.0% | 76.8% |\n| MM-Vet<sup>S</sup> | 55.3% | 54.4% | 56.3% | 58.8% | 58.9% |", "caption": "Table 3: Latent cross-modality regularizer. With our regularizer, the performance of both the speech-image and text-image modalities improves, and the gap narrows.", "description": "Table 3 presents a component-wise analysis of the Lyra model's performance, focusing on the impact of the proposed latent cross-modality regularizer.  The table shows how this regularizer enhances the alignment between different modalities (specifically speech and image, and text and image), leading to improved results on benchmark tasks. The improvement is measured by the increase in performance metrics across various tasks and a reduction in the discrepancy of performance between the speech-image and text-image modalities.", "section": "4.3 Component-Wise Analysis"}, {"content": "Omni Comparison | Text-Image | Text-Video | Image-Speech | Text-Speech\n---|---|---|---|---\nMethod | Params. | TextVQA | MME | MM-Vet | VideoMME | MVBench | Egoschema | TextVQA<sup>S</sup> | DocVQA<sup>S</sup> | ChartQA<sup>S</sup> | LibriSpeech\u2193\n---|---|---|---|---|---|---|---|---|---|---|---\nMini-Gemini | 8B | 71.9 | 1989 | 53.5 | - | - | - | - | - | - | -\nLLaVA-OV | 7B | 65.4 | 1998 | 57.5 | 58.2 | 56.7 | 60.1 | - | - | - | -\nIntern-VL2 | 8B | 77.4 | 2211 | 60.0 | 54.0 | 66.4 | - | - | - | - | -\nMini-Omni | 7B | - | - | - | - | - | - | - | - | - | 4.5\nSALMONN | 13B | - | - | - | - | - | - | - | - | - | 2.1\nQwen2-Audio | 8B | - | - | - | - | - | - | - | - | - | 1.6\nIntern-Omni | 8B | 80.6 | 2210 | 60.0 | - | - | - | 69.1 | 79.9 | 56.0 | -\nVITA | 66B | - | 2097 | 41.6 | 59.2 | - | - | - | - | - | 8.1\nEMOVA | 14B | 82.0 | 2205 | 55.8 | - | - | - | - | - | - | 4.0\nLyra-Mini | 3B | 78.3 | 1884 | 51.2 | 55.0 | 62.5 | 54.1 | 73.4 | 74.8 | 40.7 | 2.1\nLyra-Base | 9B | 82.6 | 2335 | 63.5 | 62.8 | 67.2 | 63.2 | 80.0 | 85.5 | 61.0 | 2.0\nLyra-Pro | 74B | 83.5 | 2485 | 71.4 | 69.9 | 72.3 | 75.8 | 81.0 | 89.4 | 68.5 | 1.8", "caption": "Table 7: Detailed training settings of Lyra.", "description": "This table details the hyperparameters and data used for training the Lyra model across four distinct stages.  Stage 1 focuses on speech projector pre-training using the LibriSpeech and Common Voice datasets. Stage 2 involves joint training of text, image, and speech modalities using the Lyra-MultiModal-1.5M dataset. Stage 3 extends training to incorporate long-speech capabilities using the Lyra-LongSpeech-12K dataset. Finally, Stage 4 trains the speech generator.  The table provides specifics such as audio length, number of tokens, dataset size, batch size, learning rate, and number of epochs for each stage, offering a comprehensive view of the model's training process.", "section": "A. Training Configuration and Data"}, {"content": "| Effectiveness | TexVQA |  | MM-Vet |  | LibriSpeech |\n|---|---|---|---|---|---|---|\n| Type | S+I | T+I | S+I | T+I | S+T |\n| Baseline | - | **82.3** | - | **62.8** | - |\n| $\n\\mathcal{L}\n_{\\rm CE}$ | 76.7 | 79.5 | 53.1 | 61.1 | **1.9** |\n| $\n\\mathcal{L}\n_{\\rm CE}$ + $\n\\lambda\n\\mathcal{L}\n_{\\rm LCMR}$ | **77.8** | 80.1 | **58.1** | 62.6 | 2.0 |", "caption": "Table 8: Latent multi-modality extractor training performance. The training time is reduced by an average of one-third, while the average performance does not degrade and even improves by 0.4%.", "description": "This table presents a comparison of training performance with and without the latent multi-modality extractor.  It shows that incorporating the extractor significantly reduces training time (by an average of one-third) without sacrificing model performance; in fact, average performance is slightly improved (by 0.4%). The table compares various metrics such as TextVQA, MME, MM-Vet, MMB-EN, SEED, MMMU, and average performance rate across different model configurations (baseline and with the extractor).", "section": "4.3 Component-Wise Analysis"}]