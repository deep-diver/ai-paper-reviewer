[{"figure_path": "https://arxiv.org/html/2412.09501/x3.png", "caption": "Figure 1: Overview of Lyra. Lyra shows superiority compared with leading models in the following aspects: 1. Stronger performance. Lyra achieves state-of-the-art results across a variety of modalities understanding and reasoning tasks. 2. More versatile. Lyra can directly handle images, videos and audio tasks even lasting several hours. 3. More efficient. Lyra is trained with less data and increases the speed, reduces memory usage, making it suitable for latency-sensitive and long-context multi-modality applications.", "description": "Figure 1 provides a high-level overview of the Lyra model, highlighting its key advantages over existing state-of-the-art models.  It showcases Lyra's superior performance across various multi-modal tasks (image, video, audio, text), emphasizing its ability to handle very long-duration audio and video inputs (hours of content) while maintaining efficiency through reduced data requirements and faster processing speed.  The figure visually represents these aspects through charts and diagrams comparing Lyra's performance and resource consumption to other leading models.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2412.09501/x4.png", "caption": "Figure 2: The framework of Lyra. Lyra supports multi-modal inputs. When the data contains a speech modality, we use the latent cross-modality regularizer to assist. Data from each modality is processed through encoders and projectors before being sent into the LLM. Within the LLM, multi-modality LoRA and latent multi-modality extraction modules operate synergistically, facilitating the simultaneous generation of both speech and text outputs.", "description": "Lyra, a multi-modal large language model (MLLM), processes various input modalities (text, image, video, audio) through modality-specific encoders.  A latent cross-modal regularizer helps align speech with other modalities. Encoded data then passes through projectors before being fed into a pre-trained large language model (LLM). Inside the LLM, a multi-modality Low-Rank Adaptation (LoRA) and a latent multi-modality extractor work together to efficiently learn relationships between different modalities and to select only relevant information for improved performance and reduced computational cost.  The result is the simultaneous generation of text and speech outputs.", "section": "3. Lyra"}, {"figure_path": "https://arxiv.org/html/2412.09501/x5.png", "caption": "Figure 3: Illustration of the DTW algorithm in our alignment. Our goal is to make the speech tokens as similar as possible to the corresponding translated tokens.", "description": "This figure illustrates the Dynamic Time Warping (DTW) algorithm used to align speech tokens with their corresponding text translations (produced by an automatic speech recognition system).  The goal is to minimize the distance between the two token sequences, even though they may differ in length. This alignment helps to improve the integration of speech data with other modalities in the model, as it ensures that speech information is effectively incorporated into downstream tasks.", "section": "3.1 Latent Cross-Modality Regularizer"}, {"figure_path": "https://arxiv.org/html/2412.09501/x6.png", "caption": "Figure 4: Long speech capability integration pipeline. (Middle) Our pipeline for generating instruction-following data for long speech. (Top) The proportion of question and speech categories in our long speech SFT dataset. (Bottom) Our long speech SFT pipeline. Long speech segments will be clipped and flattened.", "description": "Figure 4 illustrates the process of integrating long speech capabilities into the Lyra model.  The top panel shows the distribution of question types and speech categories within the custom long speech dataset used to train this aspect of the model.  The middle panel details the pipeline used to create the instruction-following data for training, highlighting the steps involved in preparing the long speech segments. Finally, the bottom panel provides a visual representation of the long speech Supervised Fine-Tuning (SFT) pipeline, which includes clipping and flattening long audio segments to make them suitable for training.", "section": "3.4 Long Speech Capability Integration"}, {"figure_path": "https://arxiv.org/html/2412.09501/x9.png", "caption": "(a) Prefill time, tokens per second (TPS), GPU memory comparison.", "description": "This figure presents a comparison of the prefill time, tokens per second (TPS), and GPU memory usage across three different models: a baseline model and two models incorporating the Latent Multi-Modality Extractor (LMME) with different hyperparameters.  The comparison is shown for various input token lengths, demonstrating the impact of LMME on efficiency for handling long contexts.  It shows how LMME reduces memory consumption and improves speed, particularly significant for longer inputs where the baseline model runs out of memory (OOM).", "section": "3.3. Latent Multi-Modality Extractor"}, {"figure_path": "https://arxiv.org/html/2412.09501/x10.png", "caption": "(b) Training time on multi-modality datasets comparison.", "description": "This figure compares the training time of different models on two multi-modality datasets: Lyra-MM-1.5M (containing 1.5 million text-image-speech samples) and Lyra-LongSpeech-12K (containing 12,000 long-speech samples). The baseline model is compared to three variants of the model that utilize the latent multi-modality extractor (LMME), with varying numbers of blocks (n) and top proportions of tokens retained (p).  The results show significant reductions in training time for the LMME models on both datasets, demonstrating improved training efficiency by selectively retaining important tokens and reducing redundancy.", "section": "4.3 Component-Wise Analysis"}, {"figure_path": "https://arxiv.org/html/2412.09501/x11.png", "caption": "Table 4: Efficiency of latent multi-modality extractor.", "description": "This table presents a comparison of the efficiency of the Latent Multi-Modality Extractor (LMME) module against a baseline model.  It shows the prefill time (in seconds), tokens per second (TPS), and GPU memory usage for different sequence lengths (211-217 tokens), under varying configurations of the LMME (splitting the model into 4 blocks and retaining the top 80% or 70% of tokens). The results demonstrate how LMME significantly improves efficiency in terms of memory usage and processing speed, particularly for longer sequences, while maintaining comparable performance.", "section": "3.3. Latent Multi-Modality Extractor"}, {"figure_path": "https://arxiv.org/html/2412.09501/x12.png", "caption": "Table 5: Effectiveness of long speech capability integration. Lyra integrated with long speech ability, using only audio input, can handle one-third of VideoMME cases, and its accuracies on long, medium, short metrics are better than the current best VLM.", "description": "Table 5 presents a detailed evaluation of Lyra's performance on long-speech tasks, specifically focusing on the Video Multimodal Multitask Evaluation (VideoMME) benchmark.  It shows that Lyra, leveraging its long-speech capabilities, successfully processes audio inputs that are significantly longer than what typical models handle.  The results highlight Lyra's ability to achieve comparable or even better accuracy compared to the state-of-the-art VLMs on VideoMME, demonstrating its improved performance on long, medium, and short-length audio inputs, despite using only audio data.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.09501/x13.png", "caption": "Table 6: Effectiveness of multi-modality LoRA\u00a0(MLoRA). For powerful pretrained models, adding new modality can impair the abilities of other modalities. MLoRA can effectively address it.", "description": "The table demonstrates the effectiveness of Multi-modality Low-Rank Adaptation (MLoRA) in addressing the performance degradation of pre-trained models when a new modality is added.  Powerful pre-trained models often suffer reduced capabilities in existing modalities after incorporating a new one.  This table shows that MLoRA mitigates this issue, maintaining or improving performance across multiple modalities, even with limited training data.", "section": "4.3 Component-Wise Analysis"}, {"figure_path": "https://arxiv.org/html/2412.09501/x14.png", "caption": "Figure 5: Visualization of latent multi-modality extractor in various modalities. The upper part is the video modality, and the lower part is the audio modality. Through latent multi-modality information extraction, semantic tokens related to the instruction are retained, reducing the computational cost of the MLLM. The visualization of the image modality and different blocks can be found in the appendix.", "description": "This figure visualizes the latent multi-modality extractor's functionality across video and audio data. The top half shows the video modality, while the bottom half displays the audio modality.  The key idea is that the extractor identifies and retains only the semantically relevant tokens from each modality that directly relate to the given instruction. By filtering out irrelevant tokens, the computational cost of processing this multimodal information is significantly reduced.  The appendix includes visualizations for the image modality and a breakdown of the process across different blocks within the model architecture.", "section": "3.3. Latent Multi-Modality Extractor"}, {"figure_path": "https://arxiv.org/html/2412.09501/x15.png", "caption": "(a)", "description": "This figure shows the results of the needle in a haystack experiment for evaluating the model's ability to handle long speech inputs. The x-axis represents the position of the needle (in seconds), and the y-axis represents the accuracy of the model in retrieving the correct information. Different lines represent different methods: the baseline model, a model trained with long speech data, and a model incorporating both long speech data and the latent multi-modality extractor.  The figure visually demonstrates the significant improvement in accuracy and the ability to handle increasingly longer audio inputs by incorporating the improvements in Lyra, especially with the multi-modality extractor.", "section": "Long-Speech Capability Integration"}]