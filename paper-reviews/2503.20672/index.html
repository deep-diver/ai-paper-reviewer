<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><title>BizGen: Advancing Article-level Visual Text Rendering for Infographics Generation &#183; HF Daily Paper Reviews by AI</title>
<meta name=title content="BizGen: Advancing Article-level Visual Text Rendering for Infographics Generation &#183; HF Daily Paper Reviews by AI"><meta name=description content="BIZGEN: Article-level Visual Text Rendering for Infographics Generation"><meta name=keywords content="Computer Vision,Image Generation,üè¢ Tsinghua University,"><link rel=canonical href=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.20672/><link type=text/css rel=stylesheet href=/ai-paper-reviewer/css/main.bundle.min.595affd4445a931ea6d6e3a5a3c709930fa52a60be10b21c6f81fdb8fecaacea33aacedf80cdc88be45f189be14ed4ce53ea74a1e1406fad9cbf90c5ed409173.css integrity="sha512-WVr/1ERakx6m1uOlo8cJkw+lKmC+ELIcb4H9uP7KrOozqs7fgM3Ii+RfGJvhTtTOU+p0oeFAb62cv5DF7UCRcw=="><script type=text/javascript src=/ai-paper-reviewer/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script><script defer type=text/javascript id=script-bundle src=/ai-paper-reviewer/js/main.bundle.min.efbf3b6b987689fffaf2d7b73173d2690c0279a04d444b0537a77d7f4ff6e6d493445400cb0cf56bc0f0f123e19f15394e63cae34e67f069bd013dd5c73df56e.js integrity="sha512-7787a5h2if/68te3MXPSaQwCeaBNREsFN6d9f0/25tSTRFQAywz1a8Dw8SPhnxU5TmPK405n8Gm9AT3Vxz31bg==" data-copy data-copied></script><script src=/ai-paper-reviewer/lib/zoom/zoom.min.37d2094687372da3f7343a221a470f6b8806f7891aa46a5a03966af7f0ebd38b9fe536cb154e6ad28f006d184b294525a7c4054b6bbb4be62d8b453b42db99bd.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ=="></script><link rel=apple-touch-icon sizes=180x180 href=/ai-paper-reviewer/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/ai-paper-reviewer/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/ai-paper-reviewer/favicon-16x16.png><link rel=manifest href=/ai-paper-reviewer/site.webmanifest><meta property="og:url" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.20672/"><meta property="og:site_name" content="HF Daily Paper Reviews by AI"><meta property="og:title" content="BizGen: Advancing Article-level Visual Text Rendering for Infographics Generation"><meta property="og:description" content="BIZGEN: Article-level Visual Text Rendering for Infographics Generation"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="paper-reviews"><meta property="article:published_time" content="2025-03-26T00:00:00+00:00"><meta property="article:modified_time" content="2025-03-26T00:00:00+00:00"><meta property="article:tag" content="Computer Vision"><meta property="article:tag" content="Image Generation"><meta property="article:tag" content="üè¢ Tsinghua University"><meta property="og:image" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.20672/cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.20672/cover.png"><meta name=twitter:title content="BizGen: Advancing Article-level Visual Text Rendering for Infographics Generation"><meta name=twitter:description content="BIZGEN: Article-level Visual Text Rendering for Infographics Generation"><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"Paper Reviews by AI","name":"BizGen: Advancing Article-level Visual Text Rendering for Infographics Generation","headline":"BizGen: Advancing Article-level Visual Text Rendering for Infographics Generation","abstract":"BIZGEN: Article-level Visual Text Rendering for Infographics Generation","inLanguage":"en","url":"https:\/\/deep-diver.github.io\/ai-paper-reviewer\/paper-reviews\/2503.20672\/","author":{"@type":"Person","name":"Hugging Face Daily Papers"},"copyrightYear":"2025","dateCreated":"2025-03-26T00:00:00\u002b00:00","datePublished":"2025-03-26T00:00:00\u002b00:00","dateModified":"2025-03-26T00:00:00\u002b00:00","keywords":["Computer Vision","Image Generation","üè¢ Tsinghua University"],"mainEntityOfPage":"true","wordCount":"10790"}]</script><meta name=author content="Hugging Face Daily Papers"><link href=https://github.com/deep-diver/paper-reviewer/ rel=me><link href=https://twitter.com/algo_diver/ rel=me><script src=/ai-paper-reviewer/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj+KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script><script defer src=/ai-paper-reviewer/lib/typeit/typeit.umd.1b3200cb448f5cd1f548f2781452643d3511a43584b377b82c03a58055da4fdb7bc8f6c6c2ce846480c7677ff25bfd0d75f15823c09443ab18e0fd2cad792587.js integrity="sha512-GzIAy0SPXNH1SPJ4FFJkPTURpDWEs3e4LAOlgFXaT9t7yPbGws6EZIDHZ3/yW/0NdfFYI8CUQ6sY4P0srXklhw=="></script><script defer src=/ai-paper-reviewer/lib/packery/packery.pkgd.min.js integrity></script><script type=text/javascript src=/ai-paper-reviewer/js/shortcodes/gallery.min.9b4cb28f931ed922c26fb9b2510c2debb370f6a63305050c2af81740b2919883715e24efbbdf3a081496718ec751df3a72729d4d0bc71d6071297563a97ce1ee.js integrity="sha512-m0yyj5Me2SLCb7myUQwt67Nw9qYzBQUMKvgXQLKRmINxXiTvu986CBSWcY7HUd86cnKdTQvHHWBxKXVjqXzh7g=="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-KX0S6Q55Y7"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-KX0S6Q55Y7")</script><meta name=theme-color><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-app.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-firestore.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-auth.js></script><script>const firebaseConfig={apiKey:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",authDomain:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",projectId:"neurips2024-f3065",storageBucket:"neurips2024-f3065.firebasestorage.app",messagingSenderId:"982475958898",appId:"1:982475958898:web:2147e5d7753d6ac091f0eb",measurementId:"G-YQ46HXQ9JS"};var app=firebase.initializeApp(firebaseConfig),db=firebase.firestore(),auth=firebase.auth()</script></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a></div><div class=min-h-[148px]></div><div class="fixed inset-x-0 pl-[24px] pr-[24px]" style=z-index:100><div id=menu-blur class="absolute opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl shadow-2xl"></div><div class="relative max-w-[64rem] ml-auto mr-auto"><div style=padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start space-x-3"><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/ai-paper-reviewer/ class="text-base font-medium text-gray-500 hover:text-gray-900">HF Daily Paper Reviews by AI</a></nav><nav class="hidden md:flex items-center space-x-5 md:ml-12 h-12"><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>About</p></a><a href=/ai-paper-reviewer/2025-03-31/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>2025-03-31</p></a><a href=/ai-paper-reviewer/2025-04-01/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>2025-04-01</p></a><a href=/ai-paper-reviewer/2025-04-02/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>2025-04-02</p></a><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Archive</p></a><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Tags</p></a><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></span><p class="text-base font-medium" title></p></a><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span><p class="text-base font-medium" title></p></a><button id=search-button aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button><div class="ltr:mr-14 rtl:ml-14 flex items-center"><button id=appearance-switcher aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></nav><div class="flex md:hidden items-center space-x-5 md:ml-12 h-12"><span></span>
<button id=search-button-mobile aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></button>
<button id=appearance-switcher-mobile aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400" style=margin-right:5px><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div><div class="-my-2 -mr-2 md:hidden"><label id=menu-button class=block><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper style=padding-top:5px class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li id=menu-close-button><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>About</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-03-31/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>2025-03-31</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-04-01/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>2025-04-01</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-04-02/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>2025-04-02</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Archive</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Tags</p></a></li><li class=mt-1><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li><li class=mt-1><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li></ul></div></label></div></div></div></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("menu-blur");n.style.opacity=t/300})</script><div class="relative flex flex-col grow"><main id=main-content class=grow><article><div id=hero class="h-[150px] md:h-[200px]"></div><div class="fixed inset-x-0 top-0 h-[800px] single_hero_background nozoom" style=background-image:url(/ai-paper-reviewer/paper-reviews/2503.20672/cover_hu10887999053553930756.png)><div class="absolute inset-0 bg-gradient-to-t from-neutral dark:from-neutral-800 to-transparent mix-blend-normal"></div><div class="absolute inset-0 opacity-60 bg-gradient-to-t from-neutral dark:from-neutral-800 to-neutral-100 dark:to-neutral-800 mix-blend-normal"></div></div><div id=background-blur class="fixed opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl"></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("background-blur");n.style.opacity=t/300})</script><header id=single_header class="mt-5 max-w-prose"><ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden"><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/>HF Daily Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class=inline><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/>Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/2503.20672/>BizGen: Advancing Article-level Visual Text Rendering for Infographics Generation</a><span class="px-1 text-primary-500">/</span></li></ol><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">BizGen: Advancing Article-level Visual Text Rendering for Infographics Generation</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime=2025-03-26T00:00:00+00:00>26 March 2025</time><span class="px-2 text-primary-500">&#183;</span><span>10790 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">51 mins</span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=views_paper-reviews/2503.20672/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=views>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 576 512"><path fill="currentcolor" d="M288 32c-80.8.0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7.0 24.6C17.3 304 48.6 356 95.4 399.4 142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1 3.3-7.9 3.3-16.7.0-24.6-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144 64.5-144 144-144 144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64-11.5.0-22.3-3-31.6-8.4-.2 2.8-.4 5.5-.4 8.4.0 53 43 96 96 96s96-43 96-96-43-96-96-96c-2.8.0-5.6.1-8.4.4 5.3 9.3 8.4 20.1 8.4 31.6z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=likes_paper-reviews/2503.20672/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=likes>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<button id=button_likes class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400" onclick=process_article()>
<span id=button_likes_heart style=display:none class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span><span id=button_likes_emtpty_heart class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M244 84l11.1 12 12-11.98C300.6 51.37 347 36.51 392.6 44.1 461.5 55.58 512 115.2 512 185.1V190.9c0 41.5-17.2 81.2-47.6 109.5L283.7 469.1c-7.5 7-17.4 10.9-27.7 10.9S235.8 476.1 228.3 469.1L47.59 300.4C17.23 272.1.0 232.4.0 190.9V185.1c0-69.9 50.52-129.52 119.4-141 44.7-7.59 92 7.27 124.6 39.9C243.1 84 244 84.01 244 84zm11.1 79.9-45-46.8c-21.7-20.82-52.5-30.7-82.8-25.66C81.55 99.07 48 138.7 48 185.1V190.9c0 28.2 11.71 55.2 32.34 74.4L256 429.3l175.7-164c20.6-19.2 32.3-46.2 32.3-74.4V185.1c0-46.4-33.6-86.03-79.3-93.66C354.4 86.4 323.6 96.28 301.9 117.1l-46.8 46.8z"/></svg>
</span></span><span id=button_likes_text>&nbsp;Like</span></button></span></div><div class="flex flex-row flex-wrap items-center"><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/ai-generated/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">AI Generated
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/-daily-papers/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">ü§ó Daily Papers
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/computer-vision/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Computer Vision
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/image-generation/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Image Generation
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/-tsinghua-university/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">üè¢ Tsinghua University</span></span></span></div></div><div class="flex author"><img class="!mt-0 !mb-0 h-24 w-24 rounded-full ltr:mr-4 rtl:ml-4" width=96 height=96 alt="Hugging Face Daily Papers" src=/ai-paper-reviewer/img/avatar_hu1570846118988919414.png><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">Author</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">Hugging Face Daily Papers</div><div class="text-sm text-neutral-700 dark:text-neutral-400">I am AI, and I review papers on HF Daily Papers</div><div class="text-2xl sm:text-lg"><div class="flex flex-wrap text-neutral-400 dark:text-neutral-500"><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://github.com/deep-diver/paper-reviewer/ target=_blank aria-label=Github rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://twitter.com/algo_diver/ target=_blank aria-label=Twitter rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span></a></div></div></div></div><div class=mb-5></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first lg:ml-auto px-0 lg:order-last ltr:lg:pl-8 rtl:lg:pr-8"><div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-[140px]"><details open id=TOCView class="toc-right mt-0 overflow-y-scroll overscroll-contain scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600 rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 hidden lg:block"><summary class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="min-w-[220px] py-2 border-dotted ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#article-level-gen>Article-Level Gen</a></li><li><a href=#infographics-650k>INFOGRAPHICS-650K</a></li><li><a href=#layout-guided-gen>Layout Guided Gen</a></li><li><a href=#bizeval-benchmark>BizEVAL Benchmark</a></li><li><a href=#glyph-sdxl-v2-base>Glyph-SDXL-v2 Base</a></li><li><a href=#iterative-refine>Iterative Refine</a></li><li><a href=#scaling-is-key>Scaling is Key</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><details class="toc-inside mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 lg:hidden"><summary class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#article-level-gen>Article-Level Gen</a></li><li><a href=#infographics-650k>INFOGRAPHICS-650K</a></li><li><a href=#layout-guided-gen>Layout Guided Gen</a></li><li><a href=#bizeval-benchmark>BizEVAL Benchmark</a></li><li><a href=#glyph-sdxl-v2-base>Glyph-SDXL-v2 Base</a></li><li><a href=#iterative-refine>Iterative Refine</a></li><li><a href=#scaling-is-key>Scaling is Key</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><script>var margin=200,marginError=50;(function(){var t=$(window),e=$("#TOCView"),s=e.height();function n(){var n=t.height()-margin;s>=n?(e.css("overflow-y","scroll"),e.css("max-height",n+marginError+"px")):(e.css("overflow-y","hidden"),e.css("max-height","9999999px"))}t.on("resize",n),$(document).ready(n)})()</script></div></div><div class="min-w-0 min-h-0 max-w-fit"><div class="article-content max-w-prose mb-20"><br><div class="flex flex-row flex-wrap items-center space-x-2"><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 48 48" fill="none"><rect width="48" height="48" fill="#fff" fill-opacity=".01"/><path d="M18 43V22c0-3.3137 2.6863-6 6-6s6 2.6863 6 6V43" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M12 40V22c0-6.6274 5.3726-12 12-12s12 5.3726 12 12V40" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M6 35V22C6 12.0589 14.0589 4 24 4s18 8.0589 18 18V35" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 44V31" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 24.625v-2.75" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/></svg>
</span></span><span>2503.20672</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg fill="#000" height="800" width="800" id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 511.999 511.999"><g><g><path d="M421.578 190.264l-99.847-99.847c-2.439-2.439-6.391-2.439-8.829.0L82.824 320.495c-2.439 2.439-2.439 6.392.0 8.829l99.847 99.847c2.439 2.439 6.391 2.439 8.829.0l230.078-230.078C424.017 196.655 424.017 192.703 421.578 190.264z"/></g></g><g><g><path d="M506.511 87.672 424.323 5.484c-7.308-7.31-19.175-7.315-26.488.0L348.219 55.1c-2.439 2.439-2.439 6.391.0 8.829l99.847 99.847c2.439 2.437 6.391 2.437 8.829.0l49.616-49.616C513.826 106.847 513.826 94.987 506.511 87.672z"/></g></g><g><g><path d="M508.133 491.11c-1.054-9.556-9.489-16.599-19.104-16.599H111.633l36.058-15.163c4.088-1.719 5.131-7.034 1.994-10.17l-86.854-86.854c-3.137-3.135-8.451-2.094-10.17 1.994C52.224 365.359 2.052 484.66 1.627 485.707c-5.815 13.208 4.855 27.01 18.107 26.263H489.52C500.566 511.97 509.379 502.408 508.133 491.11z"/></g></g></svg>
</span></span><span>Yuyang Peng et el.</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span>ü§ó 2025-03-27</span></span></span></div></div><p><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://arxiv.org/abs/2503.20672 target=_self role=button>‚Üó arXiv
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://huggingface.co/papers/2503.20672 target=_self role=button>‚Üó Hugging Face</a></p><audio controls><source src=https://ai-paper-reviewer.com/2503.20672/podcast.wav type=audio/wav>Your browser does not support the audio element.</audio><h3 class="relative group">TL;DR<div id=tldr class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#tldr aria-label=Anchor>#</a></span></h3><div class="lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl"><p>Generating business-related infographics and slides is tough because it demands lengthy context and high precision and suffers from the lack of datasets. Existing methods fail to adhere to ultra-dense layouts in business content due to the limited scope to only limited sub-regions and sentence-level prompts. The models also struggle with article-level visual text rendering, leading to spelling errors and disorganized layouts.</p><p>To address these challenges, <strong>BIZGEN</strong> is introduced. The new data engine creates high-quality INFOGRAPHICS-650K and offers tens or hundreds of sub-regions from ultra-dense layouts by layer-wise retrieval and augmentation. A layout-guided cross-attention scheme injects region-wise prompts into cropped region latent space with conditional CFG and leads to more flexible inference and refinement. The system shows strong results in comparison to FLUX and SD3.</p></div><h4 class="relative group">Key Takeaways<div id=key-takeaways class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#key-takeaways aria-label=Anchor>#</a></span></h4><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-b793de5b786c0ee991918c8ac29de56f></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-b793de5b786c0ee991918c8ac29de56f",{strings:[" Introduces INFOGRAPHICS-650K, a large dataset for business content generation. "],speed:10,lifeLike:!0,startDelay:0,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-3db3f4ffa9eda94c99ab164a7aa332c8></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-3db3f4ffa9eda94c99ab164a7aa332c8",{strings:[" Proposes a layout-guided cross-attention scheme for precise article-level visual text rendering. "],speed:10,lifeLike:!0,startDelay:1e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-32a1f1c5151de1247f33803b675559f1></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-32a1f1c5151de1247f33803b675559f1",{strings:[" Achieves state-of-the-art results in generating high-quality infographics and slides, outperforming existing models. "],speed:10,lifeLike:!0,startDelay:2e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><h4 class="relative group">Why does it matter?<div id=why-does-it-matter class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#why-does-it-matter aria-label=Anchor>#</a></span></h4><p><strong>BIZGEN</strong> enables high-quality infographics & slides generation from article-level text. It mitigates data scarcity and context length challenges. The new dataset and benchmarks open new avenues for business content generation research.</p><hr><h4 class="relative group">Visual Insights<div id=visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#visual-insights aria-label=Anchor>#</a></span></h4><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/flux_vs_our/img1-bizgen.png alt></figure></p><blockquote><p>üîº The figure shows an infographic with 386 characters generated by the BizGEN model. The Optical Character Recognition (OCR) accuracy for this infographic is 93%. This demonstrates the model&rsquo;s ability to generate visually accurate text within the context of an infographic, which is a challenging task for AI models.</p><details><summary>read the caption</summary>(a) 386¬Ø¬Ø386\underline{386}under¬Ø start_ARG 386 end_ARG characters / OCR: 93%¬Ø¬Øpercent93\underline{93\%}under¬Ø start_ARG 93 % end_ARG</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_align_middle" id=S4.T2.51.51.51><tr class=ltx_tr id=S4.T2.2.2.2.2><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r" id=S4.T2.2.2.2.2.3 rowspan=2 style=padding:1pt><span class=ltx_text id=S4.T2.2.2.2.2.3.1 style=font-size:80%>Method</span></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" colspan=4 id=S4.T2.1.1.1.1.1 style=padding:1pt><span class=ltx_text id=S4.T2.1.1.1.1.1.1 style=font-size:80%>Infographics Visual Text Spelling Precision (</span><math alttext="\%" class="ltx_Math" display="inline" id="S4.T2.1.1.1.1.1.m1.1"><semantics id="S4.T2.1.1.1.1.1.m1.1a"><mo id="S4.T2.1.1.1.1.1.m1.1.1" mathsize="80%" xref="S4.T2.1.1.1.1.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.1.1.m1.1b"><csymbol cd="latexml" id="S4.T2.1.1.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.1.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.1.1.m1.1c">\%</annotation><annotation encoding="application/x-llamapun" id="S4.T2.1.1.1.1.1.m1.1d">%</annotation></semantics></math><span class=ltx_text id=S4.T2.1.1.1.1.1.2 style=font-size:80%>)</span></td><td class="ltx_td ltx_nopad_l ltx_align_center" colspan=4 id=S4.T2.2.2.2.2.2 style=padding:1pt><span class=ltx_text id=S4.T2.2.2.2.2.2.1 style=font-size:80%>Slides Visual Text Spelling Precision (</span><math alttext="\%" class="ltx_Math" display="inline" id="S4.T2.2.2.2.2.2.m1.1"><semantics id="S4.T2.2.2.2.2.2.m1.1a"><mo id="S4.T2.2.2.2.2.2.m1.1.1" mathsize="80%" xref="S4.T2.2.2.2.2.2.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.2.2.m1.1b"><csymbol cd="latexml" id="S4.T2.2.2.2.2.2.m1.1.1.cmml" xref="S4.T2.2.2.2.2.2.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.2.2.m1.1c">\%</annotation><annotation encoding="application/x-llamapun" id="S4.T2.2.2.2.2.2.m1.1d">%</annotation></semantics></math><span class=ltx_text id=S4.T2.2.2.2.2.2.2 style=font-size:80%>)</span></td></tr><tr class=ltx_tr id=S4.T2.10.10.10.10><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id=S4.T2.3.3.3.3.1 style=padding:1pt><math alttext="\leq" class="ltx_Math" display="inline" id="S4.T2.3.3.3.3.1.m1.1"><semantics id="S4.T2.3.3.3.3.1.m1.1a"><mo id="S4.T2.3.3.3.3.1.m1.1.1" mathsize="80%" xref="S4.T2.3.3.3.3.1.m1.1.1.cmml">‚â§</mo><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.3.3.1.m1.1b"><leq id="S4.T2.3.3.3.3.1.m1.1.1.cmml" xref="S4.T2.3.3.3.3.1.m1.1.1"></leq></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.3.3.1.m1.1c">\leq</annotation><annotation encoding="application/x-llamapun" id="S4.T2.3.3.3.3.1.m1.1d">‚â§</annotation></semantics></math><span class=ltx_text id=S4.T2.3.3.3.3.1.1 style=font-size:80%>10 layers</span></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id=S4.T2.4.4.4.4.2 style=padding:1pt><math alttext="\leq" class="ltx_Math" display="inline" id="S4.T2.4.4.4.4.2.m1.1"><semantics id="S4.T2.4.4.4.4.2.m1.1a"><mo id="S4.T2.4.4.4.4.2.m1.1.1" mathsize="80%" xref="S4.T2.4.4.4.4.2.m1.1.1.cmml">‚â§</mo><annotation-xml encoding="MathML-Content" id="S4.T2.4.4.4.4.2.m1.1b"><leq id="S4.T2.4.4.4.4.2.m1.1.1.cmml" xref="S4.T2.4.4.4.4.2.m1.1.1"></leq></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.4.4.4.2.m1.1c">\leq</annotation><annotation encoding="application/x-llamapun" id="S4.T2.4.4.4.4.2.m1.1d">‚â§</annotation></semantics></math><span class=ltx_text id=S4.T2.4.4.4.4.2.1 style=font-size:80%>10-15 layers</span></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id=S4.T2.5.5.5.5.3 style=padding:1pt><math alttext="\leq" class="ltx_Math" display="inline" id="S4.T2.5.5.5.5.3.m1.1"><semantics id="S4.T2.5.5.5.5.3.m1.1a"><mo id="S4.T2.5.5.5.5.3.m1.1.1" mathsize="80%" xref="S4.T2.5.5.5.5.3.m1.1.1.cmml">‚â§</mo><annotation-xml encoding="MathML-Content" id="S4.T2.5.5.5.5.3.m1.1b"><leq id="S4.T2.5.5.5.5.3.m1.1.1.cmml" xref="S4.T2.5.5.5.5.3.m1.1.1"></leq></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.5.5.5.5.3.m1.1c">\leq</annotation><annotation encoding="application/x-llamapun" id="S4.T2.5.5.5.5.3.m1.1d">‚â§</annotation></semantics></math><span class=ltx_text id=S4.T2.5.5.5.5.3.1 style=font-size:80%>15-20 layers</span></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id=S4.T2.6.6.6.6.4 style=padding:1pt><math alttext="\geq" class="ltx_Math" display="inline" id="S4.T2.6.6.6.6.4.m1.1"><semantics id="S4.T2.6.6.6.6.4.m1.1a"><mo id="S4.T2.6.6.6.6.4.m1.1.1" mathsize="80%" xref="S4.T2.6.6.6.6.4.m1.1.1.cmml">‚â•</mo><annotation-xml encoding="MathML-Content" id="S4.T2.6.6.6.6.4.m1.1b"><geq id="S4.T2.6.6.6.6.4.m1.1.1.cmml" xref="S4.T2.6.6.6.6.4.m1.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.6.6.6.6.4.m1.1c">\geq</annotation><annotation encoding="application/x-llamapun" id="S4.T2.6.6.6.6.4.m1.1d">‚â•</annotation></semantics></math><span class=ltx_text id=S4.T2.6.6.6.6.4.1 style=font-size:80%>20 layers</span></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id=S4.T2.7.7.7.7.5 style=padding:1pt><math alttext="\leq" class="ltx_Math" display="inline" id="S4.T2.7.7.7.7.5.m1.1"><semantics id="S4.T2.7.7.7.7.5.m1.1a"><mo id="S4.T2.7.7.7.7.5.m1.1.1" mathsize="80%" xref="S4.T2.7.7.7.7.5.m1.1.1.cmml">‚â§</mo><annotation-xml encoding="MathML-Content" id="S4.T2.7.7.7.7.5.m1.1b"><leq id="S4.T2.7.7.7.7.5.m1.1.1.cmml" xref="S4.T2.7.7.7.7.5.m1.1.1"></leq></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.7.7.7.7.5.m1.1c">\leq</annotation><annotation encoding="application/x-llamapun" id="S4.T2.7.7.7.7.5.m1.1d">‚â§</annotation></semantics></math><span class=ltx_text id=S4.T2.7.7.7.7.5.1 style=font-size:80%>10 layers</span></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id=S4.T2.8.8.8.8.6 style=padding:1pt><math alttext="\leq" class="ltx_Math" display="inline" id="S4.T2.8.8.8.8.6.m1.1"><semantics id="S4.T2.8.8.8.8.6.m1.1a"><mo id="S4.T2.8.8.8.8.6.m1.1.1" mathsize="80%" xref="S4.T2.8.8.8.8.6.m1.1.1.cmml">‚â§</mo><annotation-xml encoding="MathML-Content" id="S4.T2.8.8.8.8.6.m1.1b"><leq id="S4.T2.8.8.8.8.6.m1.1.1.cmml" xref="S4.T2.8.8.8.8.6.m1.1.1"></leq></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.8.8.8.8.6.m1.1c">\leq</annotation><annotation encoding="application/x-llamapun" id="S4.T2.8.8.8.8.6.m1.1d">‚â§</annotation></semantics></math><span class=ltx_text id=S4.T2.8.8.8.8.6.1 style=font-size:80%>10-20 layers</span></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id=S4.T2.9.9.9.9.7 style=padding:1pt><math alttext="\leq" class="ltx_Math" display="inline" id="S4.T2.9.9.9.9.7.m1.1"><semantics id="S4.T2.9.9.9.9.7.m1.1a"><mo id="S4.T2.9.9.9.9.7.m1.1.1" mathsize="80%" xref="S4.T2.9.9.9.9.7.m1.1.1.cmml">‚â§</mo><annotation-xml encoding="MathML-Content" id="S4.T2.9.9.9.9.7.m1.1b"><leq id="S4.T2.9.9.9.9.7.m1.1.1.cmml" xref="S4.T2.9.9.9.9.7.m1.1.1"></leq></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.9.9.9.9.7.m1.1c">\leq</annotation><annotation encoding="application/x-llamapun" id="S4.T2.9.9.9.9.7.m1.1d">‚â§</annotation></semantics></math><span class=ltx_text id=S4.T2.9.9.9.9.7.1 style=font-size:80%>20-30 layers</span></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id=S4.T2.10.10.10.10.8 style=padding:1pt><math alttext="\geq" class="ltx_Math" display="inline" id="S4.T2.10.10.10.10.8.m1.1"><semantics id="S4.T2.10.10.10.10.8.m1.1a"><mo id="S4.T2.10.10.10.10.8.m1.1.1" mathsize="80%" xref="S4.T2.10.10.10.10.8.m1.1.1.cmml">‚â•</mo><annotation-xml encoding="MathML-Content" id="S4.T2.10.10.10.10.8.m1.1b"><geq id="S4.T2.10.10.10.10.8.m1.1.1.cmml" xref="S4.T2.10.10.10.10.8.m1.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.10.10.10.10.8.m1.1c">\geq</annotation><annotation encoding="application/x-llamapun" id="S4.T2.10.10.10.10.8.m1.1d">‚â•</annotation></semantics></math><span class=ltx_text id=S4.T2.10.10.10.10.8.1 style=font-size:80%>30 layers</span></td></tr><tr class=ltx_tr id=S4.T2.19.19.19.19><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t" id=S4.T2.11.11.11.11.1 style=padding:1pt><span class=ltx_text id=S4.T2.11.11.11.11.1.1 style=font-size:80%>DALL</span><math alttext="\cdot" class="ltx_Math" display="inline" id="S4.T2.11.11.11.11.1.m1.1"><semantics id="S4.T2.11.11.11.11.1.m1.1a"><mo id="S4.T2.11.11.11.11.1.m1.1.1" mathsize="80%" xref="S4.T2.11.11.11.11.1.m1.1.1.cmml">‚ãÖ</mo><annotation-xml encoding="MathML-Content" id="S4.T2.11.11.11.11.1.m1.1b"><ci id="S4.T2.11.11.11.11.1.m1.1.1.cmml" xref="S4.T2.11.11.11.11.1.m1.1.1">‚ãÖ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.11.11.11.11.1.m1.1c">\cdot</annotation><annotation encoding="application/x-llamapun" id="S4.T2.11.11.11.11.1.m1.1d">‚ãÖ</annotation></semantics></math><span class=ltx_text id=S4.T2.11.11.11.11.1.2 style=font-size:80%>E3</span></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id=S4.T2.12.12.12.12.2 style=padding:1pt><math alttext="{16.42}" class="ltx_Math" display="inline" id="S4.T2.12.12.12.12.2.m1.1"><semantics id="S4.T2.12.12.12.12.2.m1.1a"><mn id="S4.T2.12.12.12.12.2.m1.1.1" mathsize="80%" xref="S4.T2.12.12.12.12.2.m1.1.1.cmml">16.42</mn><annotation-xml encoding="MathML-Content" id="S4.T2.12.12.12.12.2.m1.1b"><cn id="S4.T2.12.12.12.12.2.m1.1.1.cmml" type="float" xref="S4.T2.12.12.12.12.2.m1.1.1">16.42</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.12.12.12.12.2.m1.1c">{16.42}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.12.12.12.12.2.m1.1d">16.42</annotation></semantics></math></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id=S4.T2.13.13.13.13.3 style=padding:1pt><math alttext="{14.94}" class="ltx_Math" display="inline" id="S4.T2.13.13.13.13.3.m1.1"><semantics id="S4.T2.13.13.13.13.3.m1.1a"><mn id="S4.T2.13.13.13.13.3.m1.1.1" mathsize="80%" xref="S4.T2.13.13.13.13.3.m1.1.1.cmml">14.94</mn><annotation-xml encoding="MathML-Content" id="S4.T2.13.13.13.13.3.m1.1b"><cn id="S4.T2.13.13.13.13.3.m1.1.1.cmml" type="float" xref="S4.T2.13.13.13.13.3.m1.1.1">14.94</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.13.13.13.13.3.m1.1c">{14.94}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.13.13.13.13.3.m1.1d">14.94</annotation></semantics></math></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id=S4.T2.14.14.14.14.4 style=padding:1pt><math alttext="{21.64}" class="ltx_Math" display="inline" id="S4.T2.14.14.14.14.4.m1.1"><semantics id="S4.T2.14.14.14.14.4.m1.1a"><mn id="S4.T2.14.14.14.14.4.m1.1.1" mathsize="80%" xref="S4.T2.14.14.14.14.4.m1.1.1.cmml">21.64</mn><annotation-xml encoding="MathML-Content" id="S4.T2.14.14.14.14.4.m1.1b"><cn id="S4.T2.14.14.14.14.4.m1.1.1.cmml" type="float" xref="S4.T2.14.14.14.14.4.m1.1.1">21.64</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.14.14.14.14.4.m1.1c">{21.64}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.14.14.14.14.4.m1.1d">21.64</annotation></semantics></math></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id=S4.T2.15.15.15.15.5 style=padding:1pt><math alttext="{24.29}" class="ltx_Math" display="inline" id="S4.T2.15.15.15.15.5.m1.1"><semantics id="S4.T2.15.15.15.15.5.m1.1a"><mn id="S4.T2.15.15.15.15.5.m1.1.1" mathsize="80%" xref="S4.T2.15.15.15.15.5.m1.1.1.cmml">24.29</mn><annotation-xml encoding="MathML-Content" id="S4.T2.15.15.15.15.5.m1.1b"><cn id="S4.T2.15.15.15.15.5.m1.1.1.cmml" type="float" xref="S4.T2.15.15.15.15.5.m1.1.1">24.29</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.15.15.15.15.5.m1.1c">{24.29}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.15.15.15.15.5.m1.1d">24.29</annotation></semantics></math></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id=S4.T2.16.16.16.16.6 style=padding:1pt><math alttext="{24.55}" class="ltx_Math" display="inline" id="S4.T2.16.16.16.16.6.m1.1"><semantics id="S4.T2.16.16.16.16.6.m1.1a"><mn id="S4.T2.16.16.16.16.6.m1.1.1" mathsize="80%" xref="S4.T2.16.16.16.16.6.m1.1.1.cmml">24.55</mn><annotation-xml encoding="MathML-Content" id="S4.T2.16.16.16.16.6.m1.1b"><cn id="S4.T2.16.16.16.16.6.m1.1.1.cmml" type="float" xref="S4.T2.16.16.16.16.6.m1.1.1">24.55</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.16.16.16.16.6.m1.1c">{24.55}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.16.16.16.16.6.m1.1d">24.55</annotation></semantics></math></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id=S4.T2.17.17.17.17.7 style=padding:1pt><math alttext="{24.88}" class="ltx_Math" display="inline" id="S4.T2.17.17.17.17.7.m1.1"><semantics id="S4.T2.17.17.17.17.7.m1.1a"><mn id="S4.T2.17.17.17.17.7.m1.1.1" mathsize="80%" xref="S4.T2.17.17.17.17.7.m1.1.1.cmml">24.88</mn><annotation-xml encoding="MathML-Content" id="S4.T2.17.17.17.17.7.m1.1b"><cn id="S4.T2.17.17.17.17.7.m1.1.1.cmml" type="float" xref="S4.T2.17.17.17.17.7.m1.1.1">24.88</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.17.17.17.17.7.m1.1c">{24.88}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.17.17.17.17.7.m1.1d">24.88</annotation></semantics></math></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id=S4.T2.18.18.18.18.8 style=padding:1pt><math alttext="{30.59}" class="ltx_Math" display="inline" id="S4.T2.18.18.18.18.8.m1.1"><semantics id="S4.T2.18.18.18.18.8.m1.1a"><mn id="S4.T2.18.18.18.18.8.m1.1.1" mathsize="80%" xref="S4.T2.18.18.18.18.8.m1.1.1.cmml">30.59</mn><annotation-xml encoding="MathML-Content" id="S4.T2.18.18.18.18.8.m1.1b"><cn id="S4.T2.18.18.18.18.8.m1.1.1.cmml" type="float" xref="S4.T2.18.18.18.18.8.m1.1.1">30.59</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.18.18.18.18.8.m1.1c">{30.59}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.18.18.18.18.8.m1.1d">30.59</annotation></semantics></math></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id=S4.T2.19.19.19.19.9 style=padding:1pt><math alttext="{22.52}" class="ltx_Math" display="inline" id="S4.T2.19.19.19.19.9.m1.1"><semantics id="S4.T2.19.19.19.19.9.m1.1a"><mn id="S4.T2.19.19.19.19.9.m1.1.1" mathsize="80%" xref="S4.T2.19.19.19.19.9.m1.1.1.cmml">22.52</mn><annotation-xml encoding="MathML-Content" id="S4.T2.19.19.19.19.9.m1.1b"><cn id="S4.T2.19.19.19.19.9.m1.1.1.cmml" type="float" xref="S4.T2.19.19.19.19.9.m1.1.1">22.52</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.19.19.19.19.9.m1.1c">{22.52}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.19.19.19.19.9.m1.1d">22.52</annotation></semantics></math></td></tr><tr class=ltx_tr id=S4.T2.27.27.27.27><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r" id=S4.T2.27.27.27.27.9 style=padding:1pt><span class=ltx_text id=S4.T2.27.27.27.27.9.1 style=font-size:80%>SD3 Large</span></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S4.T2.20.20.20.20.1 style=padding:1pt><math alttext="{30.92}" class="ltx_Math" display="inline" id="S4.T2.20.20.20.20.1.m1.1"><semantics id="S4.T2.20.20.20.20.1.m1.1a"><mn id="S4.T2.20.20.20.20.1.m1.1.1" mathsize="80%" xref="S4.T2.20.20.20.20.1.m1.1.1.cmml">30.92</mn><annotation-xml encoding="MathML-Content" id="S4.T2.20.20.20.20.1.m1.1b"><cn id="S4.T2.20.20.20.20.1.m1.1.1.cmml" type="float" xref="S4.T2.20.20.20.20.1.m1.1.1">30.92</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.20.20.20.20.1.m1.1c">{30.92}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.20.20.20.20.1.m1.1d">30.92</annotation></semantics></math></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S4.T2.21.21.21.21.2 style=padding:1pt><math alttext="{32.38}" class="ltx_Math" display="inline" id="S4.T2.21.21.21.21.2.m1.1"><semantics id="S4.T2.21.21.21.21.2.m1.1a"><mn id="S4.T2.21.21.21.21.2.m1.1.1" mathsize="80%" xref="S4.T2.21.21.21.21.2.m1.1.1.cmml">32.38</mn><annotation-xml encoding="MathML-Content" id="S4.T2.21.21.21.21.2.m1.1b"><cn id="S4.T2.21.21.21.21.2.m1.1.1.cmml" type="float" xref="S4.T2.21.21.21.21.2.m1.1.1">32.38</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.21.21.21.21.2.m1.1c">{32.38}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.21.21.21.21.2.m1.1d">32.38</annotation></semantics></math></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S4.T2.22.22.22.22.3 style=padding:1pt><math alttext="{24.10}" class="ltx_Math" display="inline" id="S4.T2.22.22.22.22.3.m1.1"><semantics id="S4.T2.22.22.22.22.3.m1.1a"><mn id="S4.T2.22.22.22.22.3.m1.1.1" mathsize="80%" xref="S4.T2.22.22.22.22.3.m1.1.1.cmml">24.10</mn><annotation-xml encoding="MathML-Content" id="S4.T2.22.22.22.22.3.m1.1b"><cn id="S4.T2.22.22.22.22.3.m1.1.1.cmml" type="float" xref="S4.T2.22.22.22.22.3.m1.1.1">24.10</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.22.22.22.22.3.m1.1c">{24.10}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.22.22.22.22.3.m1.1d">24.10</annotation></semantics></math></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id=S4.T2.23.23.23.23.4 style=padding:1pt><math alttext="{20.52}" class="ltx_Math" display="inline" id="S4.T2.23.23.23.23.4.m1.1"><semantics id="S4.T2.23.23.23.23.4.m1.1a"><mn id="S4.T2.23.23.23.23.4.m1.1.1" mathsize="80%" xref="S4.T2.23.23.23.23.4.m1.1.1.cmml">20.52</mn><annotation-xml encoding="MathML-Content" id="S4.T2.23.23.23.23.4.m1.1b"><cn id="S4.T2.23.23.23.23.4.m1.1.1.cmml" type="float" xref="S4.T2.23.23.23.23.4.m1.1.1">20.52</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.23.23.23.23.4.m1.1c">{20.52}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.23.23.23.23.4.m1.1d">20.52</annotation></semantics></math></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S4.T2.24.24.24.24.5 style=padding:1pt><math alttext="{38.09}" class="ltx_Math" display="inline" id="S4.T2.24.24.24.24.5.m1.1"><semantics id="S4.T2.24.24.24.24.5.m1.1a"><mn id="S4.T2.24.24.24.24.5.m1.1.1" mathsize="80%" xref="S4.T2.24.24.24.24.5.m1.1.1.cmml">38.09</mn><annotation-xml encoding="MathML-Content" id="S4.T2.24.24.24.24.5.m1.1b"><cn id="S4.T2.24.24.24.24.5.m1.1.1.cmml" type="float" xref="S4.T2.24.24.24.24.5.m1.1.1">38.09</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.24.24.24.24.5.m1.1c">{38.09}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.24.24.24.24.5.m1.1d">38.09</annotation></semantics></math></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S4.T2.25.25.25.25.6 style=padding:1pt><math alttext="{42.96}" class="ltx_Math" display="inline" id="S4.T2.25.25.25.25.6.m1.1"><semantics id="S4.T2.25.25.25.25.6.m1.1a"><mn id="S4.T2.25.25.25.25.6.m1.1.1" mathsize="80%" xref="S4.T2.25.25.25.25.6.m1.1.1.cmml">42.96</mn><annotation-xml encoding="MathML-Content" id="S4.T2.25.25.25.25.6.m1.1b"><cn id="S4.T2.25.25.25.25.6.m1.1.1.cmml" type="float" xref="S4.T2.25.25.25.25.6.m1.1.1">42.96</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.25.25.25.25.6.m1.1c">{42.96}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.25.25.25.25.6.m1.1d">42.96</annotation></semantics></math></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S4.T2.26.26.26.26.7 style=padding:1pt><math alttext="{43.46}" class="ltx_Math" display="inline" id="S4.T2.26.26.26.26.7.m1.1"><semantics id="S4.T2.26.26.26.26.7.m1.1a"><mn id="S4.T2.26.26.26.26.7.m1.1.1" mathsize="80%" xref="S4.T2.26.26.26.26.7.m1.1.1.cmml">43.46</mn><annotation-xml encoding="MathML-Content" id="S4.T2.26.26.26.26.7.m1.1b"><cn id="S4.T2.26.26.26.26.7.m1.1.1.cmml" type="float" xref="S4.T2.26.26.26.26.7.m1.1.1">43.46</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.26.26.26.26.7.m1.1c">{43.46}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.26.26.26.26.7.m1.1d">43.46</annotation></semantics></math></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S4.T2.27.27.27.27.8 style=padding:1pt><math alttext="{31.39}" class="ltx_Math" display="inline" id="S4.T2.27.27.27.27.8.m1.1"><semantics id="S4.T2.27.27.27.27.8.m1.1a"><mn id="S4.T2.27.27.27.27.8.m1.1.1" mathsize="80%" xref="S4.T2.27.27.27.27.8.m1.1.1.cmml">31.39</mn><annotation-xml encoding="MathML-Content" id="S4.T2.27.27.27.27.8.m1.1b"><cn id="S4.T2.27.27.27.27.8.m1.1.1.cmml" type="float" xref="S4.T2.27.27.27.27.8.m1.1.1">31.39</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.27.27.27.27.8.m1.1c">{31.39}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.27.27.27.27.8.m1.1d">31.39</annotation></semantics></math></td></tr><tr class=ltx_tr id=S4.T2.35.35.35.35><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r" id=S4.T2.35.35.35.35.9 style=padding:1pt><span class=ltx_text id=S4.T2.35.35.35.35.9.1 style=font-size:80%>FLUX</span></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S4.T2.28.28.28.28.1 style=padding:1pt><math alttext="{41.33}" class="ltx_Math" display="inline" id="S4.T2.28.28.28.28.1.m1.1"><semantics id="S4.T2.28.28.28.28.1.m1.1a"><mn id="S4.T2.28.28.28.28.1.m1.1.1" mathsize="80%" xref="S4.T2.28.28.28.28.1.m1.1.1.cmml">41.33</mn><annotation-xml encoding="MathML-Content" id="S4.T2.28.28.28.28.1.m1.1b"><cn id="S4.T2.28.28.28.28.1.m1.1.1.cmml" type="float" xref="S4.T2.28.28.28.28.1.m1.1.1">41.33</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.28.28.28.28.1.m1.1c">{41.33}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.28.28.28.28.1.m1.1d">41.33</annotation></semantics></math></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S4.T2.29.29.29.29.2 style=padding:1pt><math alttext="{29.06}" class="ltx_Math" display="inline" id="S4.T2.29.29.29.29.2.m1.1"><semantics id="S4.T2.29.29.29.29.2.m1.1a"><mn id="S4.T2.29.29.29.29.2.m1.1.1" mathsize="80%" xref="S4.T2.29.29.29.29.2.m1.1.1.cmml">29.06</mn><annotation-xml encoding="MathML-Content" id="S4.T2.29.29.29.29.2.m1.1b"><cn id="S4.T2.29.29.29.29.2.m1.1.1.cmml" type="float" xref="S4.T2.29.29.29.29.2.m1.1.1">29.06</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.29.29.29.29.2.m1.1c">{29.06}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.29.29.29.29.2.m1.1d">29.06</annotation></semantics></math></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S4.T2.30.30.30.30.3 style=padding:1pt><math alttext="{28.47}" class="ltx_Math" display="inline" id="S4.T2.30.30.30.30.3.m1.1"><semantics id="S4.T2.30.30.30.30.3.m1.1a"><mn id="S4.T2.30.30.30.30.3.m1.1.1" mathsize="80%" xref="S4.T2.30.30.30.30.3.m1.1.1.cmml">28.47</mn><annotation-xml encoding="MathML-Content" id="S4.T2.30.30.30.30.3.m1.1b"><cn id="S4.T2.30.30.30.30.3.m1.1.1.cmml" type="float" xref="S4.T2.30.30.30.30.3.m1.1.1">28.47</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.30.30.30.30.3.m1.1c">{28.47}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.30.30.30.30.3.m1.1d">28.47</annotation></semantics></math></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id=S4.T2.31.31.31.31.4 style=padding:1pt><math alttext="{27.12}" class="ltx_Math" display="inline" id="S4.T2.31.31.31.31.4.m1.1"><semantics id="S4.T2.31.31.31.31.4.m1.1a"><mn id="S4.T2.31.31.31.31.4.m1.1.1" mathsize="80%" xref="S4.T2.31.31.31.31.4.m1.1.1.cmml">27.12</mn><annotation-xml encoding="MathML-Content" id="S4.T2.31.31.31.31.4.m1.1b"><cn id="S4.T2.31.31.31.31.4.m1.1.1.cmml" type="float" xref="S4.T2.31.31.31.31.4.m1.1.1">27.12</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.31.31.31.31.4.m1.1c">{27.12}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.31.31.31.31.4.m1.1d">27.12</annotation></semantics></math></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S4.T2.32.32.32.32.5 style=padding:1pt><math alttext="{62.30}" class="ltx_Math" display="inline" id="S4.T2.32.32.32.32.5.m1.1"><semantics id="S4.T2.32.32.32.32.5.m1.1a"><mn id="S4.T2.32.32.32.32.5.m1.1.1" mathsize="80%" xref="S4.T2.32.32.32.32.5.m1.1.1.cmml">62.30</mn><annotation-xml encoding="MathML-Content" id="S4.T2.32.32.32.32.5.m1.1b"><cn id="S4.T2.32.32.32.32.5.m1.1.1.cmml" type="float" xref="S4.T2.32.32.32.32.5.m1.1.1">62.30</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.32.32.32.32.5.m1.1c">{62.30}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.32.32.32.32.5.m1.1d">62.30</annotation></semantics></math></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S4.T2.33.33.33.33.6 style=padding:1pt><math alttext="{70.54}" class="ltx_Math" display="inline" id="S4.T2.33.33.33.33.6.m1.1"><semantics id="S4.T2.33.33.33.33.6.m1.1a"><mn id="S4.T2.33.33.33.33.6.m1.1.1" mathsize="80%" xref="S4.T2.33.33.33.33.6.m1.1.1.cmml">70.54</mn><annotation-xml encoding="MathML-Content" id="S4.T2.33.33.33.33.6.m1.1b"><cn id="S4.T2.33.33.33.33.6.m1.1.1.cmml" type="float" xref="S4.T2.33.33.33.33.6.m1.1.1">70.54</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.33.33.33.33.6.m1.1c">{70.54}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.33.33.33.33.6.m1.1d">70.54</annotation></semantics></math></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S4.T2.34.34.34.34.7 style=padding:1pt><math alttext="{67.57}" class="ltx_Math" display="inline" id="S4.T2.34.34.34.34.7.m1.1"><semantics id="S4.T2.34.34.34.34.7.m1.1a"><mn id="S4.T2.34.34.34.34.7.m1.1.1" mathsize="80%" xref="S4.T2.34.34.34.34.7.m1.1.1.cmml">67.57</mn><annotation-xml encoding="MathML-Content" id="S4.T2.34.34.34.34.7.m1.1b"><cn id="S4.T2.34.34.34.34.7.m1.1.1.cmml" type="float" xref="S4.T2.34.34.34.34.7.m1.1.1">67.57</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.34.34.34.34.7.m1.1c">{67.57}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.34.34.34.34.7.m1.1d">67.57</annotation></semantics></math></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S4.T2.35.35.35.35.8 style=padding:1pt><math alttext="{55.37}" class="ltx_Math" display="inline" id="S4.T2.35.35.35.35.8.m1.1"><semantics id="S4.T2.35.35.35.35.8.m1.1a"><mn id="S4.T2.35.35.35.35.8.m1.1.1" mathsize="80%" xref="S4.T2.35.35.35.35.8.m1.1.1.cmml">55.37</mn><annotation-xml encoding="MathML-Content" id="S4.T2.35.35.35.35.8.m1.1b"><cn id="S4.T2.35.35.35.35.8.m1.1.1.cmml" type="float" xref="S4.T2.35.35.35.35.8.m1.1.1">55.37</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.35.35.35.35.8.m1.1c">{55.37}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.35.35.35.35.8.m1.1d">55.37</annotation></semantics></math></td></tr><tr class=ltx_tr id=S4.T2.43.43.43.43><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r" id=S4.T2.43.43.43.43.9 style=padding:1pt><span class=ltx_text id=S4.T2.43.43.43.43.9.1 style=font-size:80%>Glyph-SDXL-v2</span></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S4.T2.36.36.36.36.1 style=padding:1pt><math alttext="{78.18}" class="ltx_Math" display="inline" id="S4.T2.36.36.36.36.1.m1.1"><semantics id="S4.T2.36.36.36.36.1.m1.1a"><mn id="S4.T2.36.36.36.36.1.m1.1.1" mathsize="80%" xref="S4.T2.36.36.36.36.1.m1.1.1.cmml">78.18</mn><annotation-xml encoding="MathML-Content" id="S4.T2.36.36.36.36.1.m1.1b"><cn id="S4.T2.36.36.36.36.1.m1.1.1.cmml" type="float" xref="S4.T2.36.36.36.36.1.m1.1.1">78.18</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.36.36.36.36.1.m1.1c">{78.18}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.36.36.36.36.1.m1.1d">78.18</annotation></semantics></math></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S4.T2.37.37.37.37.2 style=padding:1pt><math alttext="{68.83}" class="ltx_Math" display="inline" id="S4.T2.37.37.37.37.2.m1.1"><semantics id="S4.T2.37.37.37.37.2.m1.1a"><mn id="S4.T2.37.37.37.37.2.m1.1.1" mathsize="80%" xref="S4.T2.37.37.37.37.2.m1.1.1.cmml">68.83</mn><annotation-xml encoding="MathML-Content" id="S4.T2.37.37.37.37.2.m1.1b"><cn id="S4.T2.37.37.37.37.2.m1.1.1.cmml" type="float" xref="S4.T2.37.37.37.37.2.m1.1.1">68.83</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.37.37.37.37.2.m1.1c">{68.83}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.37.37.37.37.2.m1.1d">68.83</annotation></semantics></math></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S4.T2.38.38.38.38.3 style=padding:1pt><math alttext="{51.73}" class="ltx_Math" display="inline" id="S4.T2.38.38.38.38.3.m1.1"><semantics id="S4.T2.38.38.38.38.3.m1.1a"><mn id="S4.T2.38.38.38.38.3.m1.1.1" mathsize="80%" xref="S4.T2.38.38.38.38.3.m1.1.1.cmml">51.73</mn><annotation-xml encoding="MathML-Content" id="S4.T2.38.38.38.38.3.m1.1b"><cn id="S4.T2.38.38.38.38.3.m1.1.1.cmml" type="float" xref="S4.T2.38.38.38.38.3.m1.1.1">51.73</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.38.38.38.38.3.m1.1c">{51.73}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.38.38.38.38.3.m1.1d">51.73</annotation></semantics></math></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id=S4.T2.39.39.39.39.4 style=padding:1pt><math alttext="{34.46}" class="ltx_Math" display="inline" id="S4.T2.39.39.39.39.4.m1.1"><semantics id="S4.T2.39.39.39.39.4.m1.1a"><mn id="S4.T2.39.39.39.39.4.m1.1.1" mathsize="80%" xref="S4.T2.39.39.39.39.4.m1.1.1.cmml">34.46</mn><annotation-xml encoding="MathML-Content" id="S4.T2.39.39.39.39.4.m1.1b"><cn id="S4.T2.39.39.39.39.4.m1.1.1.cmml" type="float" xref="S4.T2.39.39.39.39.4.m1.1.1">34.46</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.39.39.39.39.4.m1.1c">{34.46}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.39.39.39.39.4.m1.1d">34.46</annotation></semantics></math></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S4.T2.40.40.40.40.5 style=padding:1pt><math alttext="{80.18}" class="ltx_Math" display="inline" id="S4.T2.40.40.40.40.5.m1.1"><semantics id="S4.T2.40.40.40.40.5.m1.1a"><mn id="S4.T2.40.40.40.40.5.m1.1.1" mathsize="80%" xref="S4.T2.40.40.40.40.5.m1.1.1.cmml">80.18</mn><annotation-xml encoding="MathML-Content" id="S4.T2.40.40.40.40.5.m1.1b"><cn id="S4.T2.40.40.40.40.5.m1.1.1.cmml" type="float" xref="S4.T2.40.40.40.40.5.m1.1.1">80.18</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.40.40.40.40.5.m1.1c">{80.18}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.40.40.40.40.5.m1.1d">80.18</annotation></semantics></math></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S4.T2.41.41.41.41.6 style=padding:1pt><math alttext="{85.34}" class="ltx_Math" display="inline" id="S4.T2.41.41.41.41.6.m1.1"><semantics id="S4.T2.41.41.41.41.6.m1.1a"><mn id="S4.T2.41.41.41.41.6.m1.1.1" mathsize="80%" xref="S4.T2.41.41.41.41.6.m1.1.1.cmml">85.34</mn><annotation-xml encoding="MathML-Content" id="S4.T2.41.41.41.41.6.m1.1b"><cn id="S4.T2.41.41.41.41.6.m1.1.1.cmml" type="float" xref="S4.T2.41.41.41.41.6.m1.1.1">85.34</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.41.41.41.41.6.m1.1c">{85.34}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.41.41.41.41.6.m1.1d">85.34</annotation></semantics></math></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S4.T2.42.42.42.42.7 style=padding:1pt><math alttext="{79.89}" class="ltx_Math" display="inline" id="S4.T2.42.42.42.42.7.m1.1"><semantics id="S4.T2.42.42.42.42.7.m1.1a"><mn id="S4.T2.42.42.42.42.7.m1.1.1" mathsize="80%" xref="S4.T2.42.42.42.42.7.m1.1.1.cmml">79.89</mn><annotation-xml encoding="MathML-Content" id="S4.T2.42.42.42.42.7.m1.1b"><cn id="S4.T2.42.42.42.42.7.m1.1.1.cmml" type="float" xref="S4.T2.42.42.42.42.7.m1.1.1">79.89</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.42.42.42.42.7.m1.1c">{79.89}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.42.42.42.42.7.m1.1d">79.89</annotation></semantics></math></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S4.T2.43.43.43.43.8 style=padding:1pt><math alttext="{65.07}" class="ltx_Math" display="inline" id="S4.T2.43.43.43.43.8.m1.1"><semantics id="S4.T2.43.43.43.43.8.m1.1a"><mn id="S4.T2.43.43.43.43.8.m1.1.1" mathsize="80%" xref="S4.T2.43.43.43.43.8.m1.1.1.cmml">65.07</mn><annotation-xml encoding="MathML-Content" id="S4.T2.43.43.43.43.8.m1.1b"><cn id="S4.T2.43.43.43.43.8.m1.1.1.cmml" type="float" xref="S4.T2.43.43.43.43.8.m1.1.1">65.07</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.43.43.43.43.8.m1.1c">{65.07}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.43.43.43.43.8.m1.1d">65.07</annotation></semantics></math></td></tr><tr class=ltx_tr id=S4.T2.51.51.51.51><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r" id=S4.T2.51.51.51.51.9 style=padding:1pt><span class="ltx_text ltx_font_smallcaps" id=S4.T2.51.51.51.51.9.1 style=font-size:80%>BizGen</span></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S4.T2.44.44.44.44.1 style=padding:1pt><math alttext="{92.98}" class="ltx_Math" display="inline" id="S4.T2.44.44.44.44.1.m1.1"><semantics id="S4.T2.44.44.44.44.1.m1.1a"><mn id="S4.T2.44.44.44.44.1.m1.1.1" mathsize="80%" xref="S4.T2.44.44.44.44.1.m1.1.1.cmml">92.98</mn><annotation-xml encoding="MathML-Content" id="S4.T2.44.44.44.44.1.m1.1b"><cn id="S4.T2.44.44.44.44.1.m1.1.1.cmml" type="float" xref="S4.T2.44.44.44.44.1.m1.1.1">92.98</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.44.44.44.44.1.m1.1c">{92.98}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.44.44.44.44.1.m1.1d">92.98</annotation></semantics></math></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S4.T2.45.45.45.45.2 style=padding:1pt><math alttext="{84.25}" class="ltx_Math" display="inline" id="S4.T2.45.45.45.45.2.m1.1"><semantics id="S4.T2.45.45.45.45.2.m1.1a"><mn id="S4.T2.45.45.45.45.2.m1.1.1" mathsize="80%" xref="S4.T2.45.45.45.45.2.m1.1.1.cmml">84.25</mn><annotation-xml encoding="MathML-Content" id="S4.T2.45.45.45.45.2.m1.1b"><cn id="S4.T2.45.45.45.45.2.m1.1.1.cmml" type="float" xref="S4.T2.45.45.45.45.2.m1.1.1">84.25</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.45.45.45.45.2.m1.1c">{84.25}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.45.45.45.45.2.m1.1d">84.25</annotation></semantics></math></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S4.T2.46.46.46.46.3 style=padding:1pt><math alttext="{66.42}" class="ltx_Math" display="inline" id="S4.T2.46.46.46.46.3.m1.1"><semantics id="S4.T2.46.46.46.46.3.m1.1a"><mn id="S4.T2.46.46.46.46.3.m1.1.1" mathsize="80%" xref="S4.T2.46.46.46.46.3.m1.1.1.cmml">66.42</mn><annotation-xml encoding="MathML-Content" id="S4.T2.46.46.46.46.3.m1.1b"><cn id="S4.T2.46.46.46.46.3.m1.1.1.cmml" type="float" xref="S4.T2.46.46.46.46.3.m1.1.1">66.42</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.46.46.46.46.3.m1.1c">{66.42}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.46.46.46.46.3.m1.1d">66.42</annotation></semantics></math></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id=S4.T2.47.47.47.47.4 style=padding:1pt><math alttext="{55.48}" class="ltx_Math" display="inline" id="S4.T2.47.47.47.47.4.m1.1"><semantics id="S4.T2.47.47.47.47.4.m1.1a"><mn id="S4.T2.47.47.47.47.4.m1.1.1" mathsize="80%" xref="S4.T2.47.47.47.47.4.m1.1.1.cmml">55.48</mn><annotation-xml encoding="MathML-Content" id="S4.T2.47.47.47.47.4.m1.1b"><cn id="S4.T2.47.47.47.47.4.m1.1.1.cmml" type="float" xref="S4.T2.47.47.47.47.4.m1.1.1">55.48</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.47.47.47.47.4.m1.1c">{55.48}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.47.47.47.47.4.m1.1d">55.48</annotation></semantics></math></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S4.T2.48.48.48.48.5 style=padding:1pt><math alttext="{88.13}" class="ltx_Math" display="inline" id="S4.T2.48.48.48.48.5.m1.1"><semantics id="S4.T2.48.48.48.48.5.m1.1a"><mn id="S4.T2.48.48.48.48.5.m1.1.1" mathsize="80%" xref="S4.T2.48.48.48.48.5.m1.1.1.cmml">88.13</mn><annotation-xml encoding="MathML-Content" id="S4.T2.48.48.48.48.5.m1.1b"><cn id="S4.T2.48.48.48.48.5.m1.1.1.cmml" type="float" xref="S4.T2.48.48.48.48.5.m1.1.1">88.13</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.48.48.48.48.5.m1.1c">{88.13}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.48.48.48.48.5.m1.1d">88.13</annotation></semantics></math></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S4.T2.49.49.49.49.6 style=padding:1pt><math alttext="{88.41}" class="ltx_Math" display="inline" id="S4.T2.49.49.49.49.6.m1.1"><semantics id="S4.T2.49.49.49.49.6.m1.1a"><mn id="S4.T2.49.49.49.49.6.m1.1.1" mathsize="80%" xref="S4.T2.49.49.49.49.6.m1.1.1.cmml">88.41</mn><annotation-xml encoding="MathML-Content" id="S4.T2.49.49.49.49.6.m1.1b"><cn id="S4.T2.49.49.49.49.6.m1.1.1.cmml" type="float" xref="S4.T2.49.49.49.49.6.m1.1.1">88.41</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.49.49.49.49.6.m1.1c">{88.41}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.49.49.49.49.6.m1.1d">88.41</annotation></semantics></math></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S4.T2.50.50.50.50.7 style=padding:1pt><math alttext="{80.29}" class="ltx_Math" display="inline" id="S4.T2.50.50.50.50.7.m1.1"><semantics id="S4.T2.50.50.50.50.7.m1.1a"><mn id="S4.T2.50.50.50.50.7.m1.1.1" mathsize="80%" xref="S4.T2.50.50.50.50.7.m1.1.1.cmml">80.29</mn><annotation-xml encoding="MathML-Content" id="S4.T2.50.50.50.50.7.m1.1b"><cn id="S4.T2.50.50.50.50.7.m1.1.1.cmml" type="float" xref="S4.T2.50.50.50.50.7.m1.1.1">80.29</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.50.50.50.50.7.m1.1c">{80.29}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.50.50.50.50.7.m1.1d">80.29</annotation></semantics></math></td><td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id=S4.T2.51.51.51.51.8 style=padding:1pt><math alttext="{66.81}" class="ltx_Math" display="inline" id="S4.T2.51.51.51.51.8.m1.1"><semantics id="S4.T2.51.51.51.51.8.m1.1a"><mn id="S4.T2.51.51.51.51.8.m1.1.1" mathsize="80%" xref="S4.T2.51.51.51.51.8.m1.1.1.cmml">66.81</mn><annotation-xml encoding="MathML-Content" id="S4.T2.51.51.51.51.8.m1.1b"><cn id="S4.T2.51.51.51.51.8.m1.1.1.cmml" type="float" xref="S4.T2.51.51.51.51.8.m1.1.1">66.81</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.51.51.51.51.8.m1.1c">{66.81}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.51.51.51.51.8.m1.1d">66.81</annotation></semantics></math></td></tr></table></table></figure><blockquote><p>üîº This table provides the detailed prompts used to generate the infographics and slides shown in Figures 1, 10, and 11 of the paper. For each figure, the global prompt (describing the overall image content) and individual prompts for each layer (specifying text and non-text elements) are listed. This detailed breakdown allows for a comprehensive understanding of how the model&rsquo;s inputs shaped the final outputs.</p><details><summary>read the caption</summary>Table 9: Detailed prompt for generated infographics and slides in Figure 1, Figure 10 and Figure 11.</details></blockquote><h3 class="relative group">In-depth insights<div id=in-depth-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#in-depth-insights aria-label=Anchor>#</a></span></h3><h4 class="relative group">Article-Level Gen<div id=article-level-gen class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#article-level-gen aria-label=Anchor>#</a></span></h4><p>The research paper extensively explores visual text rendering for infographics generation at an article level, moving beyond the sentence-level focus typical in current models like FLUX and Ideogram 2.0. It specifically deals with the task of generating high-quality business content. The work introduces <strong>INFOGRAPHICS-650K</strong>, a scalable dataset of ultra-dense layouts and corresponding prompts constructed using a layer-wise retrieval-augmented scheme. It is complemented with a <strong>layout-guided cross-attention mechanism</strong>, that injects region-specific prompts into cropped region latent spaces, and uses a layout conditional CFG to refine sub-regions. Comparative results against SOTA systems such as FLUX and SD3, along with ablation studies, are presented to validate the BIZGEN system&rsquo;s effectiveness. Overall, the work makes a meaningful contribution by creating resources (dataset and benchmark) and methods that aim to spur further progress in business content generation.</p><h4 class="relative group">INFOGRAPHICS-650K<div id=infographics-650k class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#infographics-650k aria-label=Anchor>#</a></span></h4><p>The <strong>INFOGRAPHICS-650K</strong> dataset is a key contribution, addressing the <strong>data scarcity</strong> challenge in business content generation. It comprises a large number of high-quality, multilingual infographic samples. Each sample includes not only images and detailed global captions but also <strong>ultra-dense layouts and region-specific captions</strong>, facilitating detailed and nuanced learning. The dataset construction employs a retrieval-augmented infographic generation scheme. The scale and richness of <strong>INFOGRAPHICS-650K</strong> enable the study of complex layout structures and visual text rendering, areas that are critical for creating effective business communications. The multi-layered data benefits the research community, specifically within the context of creating high-quality infographics. The meticulous curation of <strong>INFOGRAPHICS-650K</strong> is a significant advancement that fosters innovation in content creation tasks by its sheer scale and the diversity and depth of the included data.</p><h4 class="relative group">Layout Guided Gen<div id=layout-guided-gen class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#layout-guided-gen aria-label=Anchor>#</a></span></h4><p>From the provided context, it seems the research paper focuses on generating infographics with accurate visual text. While prior works enhance image generation with text, this paper specifically tackles article-level visual text rendering. <strong>A key challenge</strong> is representing spatial layouts effectively in prompts. Previous methods have limitations, inspiring the authors to introduce new ways with fine-grained spatial controls. They also explore the possibilities in plug-and-play modules that inject visual guidance, aiming to achieve more precise and visually coherent infographic generation. The authors aim to solve existing problems by <strong>careful control over visual text and image generation</strong>.</p><h4 class="relative group">BizEVAL Benchmark<div id=bizeval-benchmark class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#bizeval-benchmark aria-label=Anchor>#</a></span></h4><p>The BizEVAL benchmark is a <strong>crucial component</strong> for assessing the quality of generated business content, particularly infographics and slides. It provides a <strong>standardized evaluation framework</strong>, allowing for objective comparison of different generation models. The benchmark includes detailed article-level prompts and ultra-dense layouts, representing a significant leap in complexity compared to traditional image generation tasks. It serves as a <strong>challenging testbed</strong> for evaluating visual text rendering accuracy, layout adherence, and overall aesthetic quality, pushing the boundaries of current text-to-image generation capabilities and highlighting areas for future research and improvement in business content creation. The benchmark uses metrics such as visual aesthetics, prompt following, and OCR.</p><h4 class="relative group">Glyph-SDXL-v2 Base<div id=glyph-sdxl-v2-base class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#glyph-sdxl-v2-base aria-label=Anchor>#</a></span></h4><p>While &lsquo;Glyph-SDXL-v2 Base&rsquo; is not explicitly discussed as a dedicated heading in the provided research paper snippets, it likely refers to the foundational model upon which the authors build their proposed system, BIZGEN. Based on the context, understanding the base model is crucial as it directly impacts several aspects of the research. Firstly, the <strong>choice of Glyph-SDXL-v2 provides an existing architecture</strong> for image generation and visual text rendering capabilities. This impacts the design choices for BIZGEN. The authors would have considered the existing strengths and weaknesses of Glyph-SDXL-v2 while designing BIZGEN. Secondly, <strong>initialization using pre-trained weights from Glyph-SDXL-v2 is crucial</strong> for faster training and better convergence. Fine-tuning pre-trained weights is generally more efficient than training from scratch. It would lead to improved performance, especially given the limited amount of high-quality business content data. Thirdly, the limitations in areas such as visual text spelling accuracy or adherence to complex layouts in infographics led to the <strong>innovations introduced in BIZGEN.</strong></p><h4 class="relative group">Iterative Refine<div id=iterative-refine class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#iterative-refine aria-label=Anchor>#</a></span></h4><p>While &lsquo;Iterative Refine&rsquo; wasn&rsquo;t a specific heading, the concept was inherent in the <strong>layout conditional CFG</strong> to reduce artifacts layer-wise during inference. The model starts with noise and progressively denoises. I found that <strong>joint generation</strong> is crucial, where all elements are generated together in a unified way. By applying a dense guidance scale map the <strong>layer-wise quality was improved</strong>, eliminating certain regional flaws during the fine-tuning process, therefore refining the overall result. It made visual element adhere <strong>more closely to the overall prompts</strong> as well as improve accuracy during the model iteration, so BIZGEN can output more impressive infographic results.</p><h4 class="relative group">Scaling is Key<div id=scaling-is-key class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#scaling-is-key aria-label=Anchor>#</a></span></h4><p>When it comes to machine learning and AI, <strong>scaling</strong> is a fundamental aspect that goes beyond simply increasing the size of datasets or models. It encompasses optimizing the entire pipeline, from data acquisition and preprocessing to model training, evaluation, and deployment. A well-designed scaling strategy considers computational resources, algorithmic efficiency, and infrastructure limitations. Furthermore, <strong>scaling</strong> is not just about handling larger volumes of data, but also about improving the model&rsquo;s ability to generalize to unseen data. This can be achieved through various techniques such as distributed training, model parallelism, and data augmentation. To unlock greater gains in model performance and applicability, we need a strategic plan for <strong>scaling</strong> to provide improvements across many metrics.</p><h3 class="relative group">More visual insights<div id=more-visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#more-visual-insights aria-label=Anchor>#</a></span></h3><details><summary>More on figures</summary><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/flux_vs_our/img2-bizgen.png alt></figure></p><blockquote><p>üîº This figure shows an infographic generated by the BizGEN model. The infographic contains 426 characters, and the Optical Character Recognition (OCR) accuracy is 96%. The image demonstrates the model&rsquo;s ability to generate high-quality visual text within an infographic context.</p><details><summary>read the caption</summary>(b) 426¬Ø¬Ø426\underline{426}under¬Ø start_ARG 426 end_ARG characters / OCR: 96%¬Ø¬Øpercent96\underline{96\%}under¬Ø start_ARG 96 % end_ARG</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/flux_vs_our/img3-bizgen.png alt></figure></p><blockquote><p>üîº The figure shows an infographic generated by the BizGEN model. The infographic contains 545 characters, and the Optical Character Recognition (OCR) accuracy is 99%. This demonstrates the model&rsquo;s ability to generate high-quality infographics with accurate text rendering. This particular infographic is one of several examples used to showcase the model&rsquo;s performance on varying lengths of text.</p><details><summary>read the caption</summary>(c) 545¬Ø¬Ø545\underline{545}under¬Ø start_ARG 545 end_ARG characters / OCR: 99%¬Ø¬Øpercent99\underline{99\%}under¬Ø start_ARG 99 % end_ARG</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/flux_vs_our/img4-bizgen.png alt></figure></p><blockquote><p>üîº This infographic contains 594 characters, and the Optical Character Recognition (OCR) accuracy is 99%. The figure shows one of the infographics generated by the BizGEN model, demonstrating its ability to render accurate visual text within an infographic.</p><details><summary>read the caption</summary>(d) 594¬Ø¬Ø594\underline{594}under¬Ø start_ARG 594 end_ARG characters / OCR: 99%¬Ø¬Øpercent99\underline{99\%}under¬Ø start_ARG 99 % end_ARG</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/flux_vs_our/img5-bizgen.png alt></figure></p><blockquote><p>üîº This figure shows one of the infographics generated by the BizGEN model. The infographic contains 737 characters, and the Optical Character Recognition (OCR) accuracy is 97%. This demonstrates the model&rsquo;s ability to generate infographics with high accuracy and a relatively large amount of text.</p><details><summary>read the caption</summary>(e) 737¬Ø¬Ø737\underline{737}under¬Ø start_ARG 737 end_ARG characters / OCR: 97%¬Ø¬Øpercent97\underline{97\%}under¬Ø start_ARG 97 % end_ARG</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/x1.png alt></figure></p><blockquote><p>üîº This figure showcases the quality of visual text rendering achieved by the BizGen model in generating infographics. Five examples of infographics are displayed, each with varying lengths of text (ranging from 386 to 737 characters). The purpose is to demonstrate BizGen&rsquo;s capability to accurately render article-level amounts of text within the visual design of an infographic, which is a challenging task for text-to-image models.</p><details><summary>read the caption</summary>Figure 1: Accurate article-level visual text rendering results in infographics generated with BizGen, ranging from 386386386386 to 737737737737 characters.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/x2.png alt></figure></p><blockquote><p>üîº This figure shows the results of infographic generation using different methods. The first row displays sample infographics with various numbers of characters, and the second row indicates their corresponding Optical Character Recognition (OCR) accuracy. Infographics (a) through (e) show increased complexity and length, corresponding to the increasing OCR accuracy. This demonstrates the relationship between character count and OCR performance in infographic generation.</p><details><summary>read the caption</summary>(a) 386¬Ø¬Ø386\underline{386}under¬Ø start_ARG 386 end_ARG characters / OCR: 11%¬Ø¬Øpercent11\underline{11\%}under¬Ø start_ARG 11 % end_ARG</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/x3.png alt></figure></p><blockquote><p>üîº The figure shows an infographic generated by a model other than BizGen. The infographic contains 426 characters, but only 30% of them were correctly recognized by the OCR system. This highlights a limitation of existing text-to-image generation models in terms of generating accurate visual text, particularly in complex layouts.</p><details><summary>read the caption</summary>(b) 426¬Ø¬Ø426\underline{426}under¬Ø start_ARG 426 end_ARG characters / OCR: 30%¬Ø¬Øpercent30\underline{30\%}under¬Ø start_ARG 30 % end_ARG</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/x4.png alt></figure></p><blockquote><p>üîº This infographic contains 545 characters. The Optical Character Recognition (OCR) process, which converts images of text into machine-readable text, achieved only 22% accuracy for this specific infographic. This low accuracy suggests that a significant portion of the text in the image was either not correctly recognized or completely missed by the OCR software. This could be due to factors such as image quality, font style, or the presence of visual elements interfering with text recognition.</p><details><summary>read the caption</summary>(c) 545¬Ø¬Ø545\underline{545}under¬Ø start_ARG 545 end_ARG characters / OCR: 22%¬Ø¬Øpercent22\underline{22\%}under¬Ø start_ARG 22 % end_ARG</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/infographics-example-final.png alt></figure></p><blockquote><p>üîº The figure shows an infographic with 594 characters. The Optical Character Recognition (OCR) accuracy for this infographic is 55%. This means that 55% of the characters in the infographic were correctly identified by the OCR software.</p><details><summary>read the caption</summary>(d) 594¬Ø¬Ø594\underline{594}under¬Ø start_ARG 594 end_ARG characters / OCR: 55%¬Ø¬Øpercent55\underline{55\%}under¬Ø start_ARG 55 % end_ARG</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/retrieval_augmented_infographics_gen_new.png alt></figure></p><blockquote><p>üîº This infographic contains 737 characters, and the Optical Character Recognition (OCR) accuracy is 46%. The low OCR accuracy suggests that a significant portion of the text in the infographic was not correctly recognized by the OCR system.</p><details><summary>read the caption</summary>(e) 737¬Ø¬Ø737\underline{737}under¬Ø start_ARG 737 end_ARG characters / OCR: 46%¬Ø¬Øpercent46\underline{46\%}under¬Ø start_ARG 46 % end_ARG</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/statistics/infographics_text_layer_cnt.png alt></figure></p><blockquote><p>üîº This figure showcases infographics generated using the SD3-Large model. While visually appealing at first glance, a closer inspection (specifically, the zoomed-in rectangular areas in the second row) reveals significant inaccuracies in the spelling of the article-level text. This highlights a key challenge addressed by the BizGEN model: achieving accurate article-level visual text rendering in infographics.</p><details><summary>read the caption</summary>Figure 2: Infographics generation results based on SD3-Large. While these results appear appealing, the spelling accuracy of the article-level visual text is inadequate, as shown in the zoomed-in marked rectangle regions in the second row.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/statistics/infographics_element_layer_cnt.png alt></figure></p><blockquote><p>üîº This figure illustrates the significant increase in context length required for business content generation, specifically infographics, compared to previous text-to-image models. The context length for infographics is more than 10 times longer than what is typical for other text-to-image tasks. This highlights a key challenge in generating high-quality business infographics due to the significantly greater amount of textual information needed to describe the complex visual elements and layouts. This large context length places higher demands on the model&rsquo;s capacity to process and understand the provided text.</p><details><summary>read the caption</summary>Figure 3: Significant challenge of business content generation: the context lengths increase by more than 10√ó10\times10 √ó compared to the previous text-to-image generation models.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/statistics/infographics_layer_cnt.png alt></figure></p><blockquote><p>üîº The figure shows a comparison of the performance of BizGen against DALL-E3 using a win-rate percentage. Three metrics were compared: Aesthetics, Text Accuracy, and Prompt Alignment. For each metric, the win rate percentage is shown for BizGen and the comparison model. A higher percentage indicates better performance for BizGen. The win-rate is derived from a user study, with details of the study population and methodology provided in the paper.</p><details><summary>read the caption</summary>(a) BizGen v.s. DALL‚ãÖ‚ãÖ\cdot‚ãÖE3</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/statistics/infographics_char_len.png alt></figure></p><blockquote><p>üîº The figure showcases a comparison between BizGen and SD3 Large models in terms of three aspects: aesthetics, text accuracy, and prompt alignment. It presents the win rate percentages for each model across these three criteria. The win rate is calculated based on a user study where participants compared the output of both models and judged which one performed better on each aspect. This comparison aims to demonstrate the relative advantages of BizGen over SD3 Large in generating high-quality business content, including infographics and slides.</p><details><summary>read the caption</summary>(b) BizGen v.s. SD3 Large</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/layout-guided-cross-attention.png alt></figure></p><blockquote><p>üîº This figure presents a comparison of the performance of BizGen and FLUX using the BizEVAL benchmark. Specifically, it shows the win-lose rates based on a user study involving approximately 10 participants. The study evaluates three aspects: visual aesthetics, text accuracy, and prompt alignment. Each aspect&rsquo;s win rate percentage is presented in a bar chart format, allowing for easy comparison between BizGen and FLUX for each evaluation criterion.</p><details><summary>read the caption</summary>(c) BizGen v.s. FLUX</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/x5.png alt></figure></p><blockquote><p>üîº This figure displays the results of a user study comparing the performance of BizGen against other state-of-the-art (SOTA) models on the BizEval benchmark. The study involved approximately 10 users who provided feedback on three key aspects: visual aesthetics, text accuracy, and prompt alignment. The bar chart visually represents the win-loss rates for BizGen compared to each SOTA method for each of the three criteria. A higher win rate indicates superior performance of BizGen in that specific area.</p><details><summary>read the caption</summary>Figure 4: BizGen v.s. SOTAs on BizEval: Illustrating the win-lose rates based on our user study, which collected feedback from ‚àº10similar-toabsent10\sim 10‚àº 10 users.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/x6.png alt></figure></p><blockquote><p>üîº This figure shows the layered structure of an infographic. The left side displays a sample infographic, while the right side shows a detailed layout diagram. The layout diagram illustrates how each visual element (text, images, icons, etc.) is positioned and organized within the infographic. This illustrates the complexity of creating infographics and the ultra-dense nature of the layout, which is a key challenge addressed in the paper.</p><details><summary>read the caption</summary>Figure 5: Decomposed visual elements (layers) that form a representative infographic sample. The right side displays the ultra-dense layout that specifies the spatial arrangements of each visual element.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/metrics.png alt></figure></p><blockquote><p>üîº This figure illustrates the process of creating variations of infographics using a retrieval-augmentation method. The process starts by selecting a template infographic and identifying its key visual layers. These key layers are then used as queries to search a database of high-quality, transparent layers. The most visually similar layers from the database are retrieved and replace the original layers in the template. This layer-wise replacement generates several new variations of the original infographic.</p><details><summary>read the caption</summary>Figure 6: Overall pipeline of retrieval-augmented infographics generation: we begin with a template infographic and perform layer selection to identify the most important visual layers. Then we conduct layer-wise retrieval from a constructed database of high-quality transparent layers and execute layer-wise replacement to generate various variants of the infographics.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/sota/bizgen1.png alt></figure></p><blockquote><p>üîº This figure shows the distribution of the number of text layers across the infographics dataset. It visualizes the frequency of different counts of text layers within each infographic, providing insights into the complexity and variability of the infographics in the dataset. A higher frequency at lower layer counts could indicate that many infographics have a relatively simple structure, while a broader distribution or higher values could suggest greater visual complexity or more detailed designs.</p><details><summary>read the caption</summary>(a) #text layers</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/sota/new/bizgen2.png alt></figure></p><blockquote><p>üîº This figure shows the distribution of the number of non-text layers across the infographics in the INFOGRAPHICS-650K dataset. The x-axis represents the number of non-text layers, and the y-axis represents the frequency or count of infographics with that number of layers. The graph visually illustrates the distribution of non-textual elements such as background, object, and decorative layers within the dataset&rsquo;s infographics.</p><details><summary>read the caption</summary>(b) #non-text layers</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/sota/new/bizgen3.png alt></figure></p><blockquote><p>üîº This figure shows the distribution of the total number of layers in the infographics dataset INFOGRAPHICS-650K. The distribution is visualized as a histogram, showing the frequency of different layer counts across the entire dataset. This helps to understand the complexity and variability of the infographics included in the dataset, revealing the prevalence of infographics with varying numbers of layers.</p><details><summary>read the caption</summary>(c) #total layers</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/sota/new/bizgen4.png alt></figure></p><blockquote><p>üîº The figure shows the distribution of the number of characters per layer in the INFOGRAPHICS-650K dataset. It visualizes the typical character count within each layer of the infographics, providing insights into the text density and length variations across different layers. This helps in understanding the complexity of text rendering required for generating high-quality infographics.</p><details><summary>read the caption</summary>(d) #chars/layer</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/sota/new/bizgen5.png alt></figure></p><blockquote><p>üîº This figure presents four histograms visualizing the distribution of different aspects related to the layers found in infographics from the INFOGRAPHICS-650K dataset. The first histogram shows the distribution of the total number of layers per infographic. The second displays the distribution of the number of layers containing visual text. The third shows the distribution of the number of non-text layers. Finally, the fourth histogram illustrates the distribution of the number of characters present within the visual text layers of each infographic. Median values are indicated by dashed red lines across the histograms.</p><details><summary>read the caption</summary>Figure 7: Distribution of the number of layers, visual text layers, non-text layers, and the number of characters in the visual text layers. We mark the median values with red dashed lines.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/sota/flux1.png alt></figure></p><blockquote><p>üîº This figure illustrates the architecture of the Layout Guided Cross Attention mechanism used in BizGen. It shows how the model processes the long context of an article-level prompt by dividing it into multiple shorter, region-wise prompts corresponding to the layout. The process begins with cropping the latent representation of the input image into multiple groups of layer-wise visual tokens. Then, layer-wise text tokens are extracted according to the region-wise prompts. Finally, region-wise cross-attention is performed between the corresponding layer-wise visual tokens and layer-wise text tokens to control visual content generation within each region.</p><details><summary>read the caption</summary>(a) Layout Guided Cross Attention</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/sota/new/flux2.png alt></figure></p><blockquote><p>üîº The figure illustrates the training process of the BizGen model. It shows the input components: noisy latent features, a layout, and regional prompts. These inputs are fed into a latent diffusion model which outputs a noise prediction. The predicted noise is then used to refine the latent features, ultimately leading to the generation of an infographic.</p><details><summary>read the caption</summary>(b) Training pipeline of BizGen</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/sota/new/flux3.png alt></figure></p><blockquote><p>üîº The figure illustrates the evaluation pipeline of the BizGEN model. It begins with a noise map, a layout, and region-wise prompts as inputs to a latent diffusion model. The model then undergoes multiple iterative refinement steps, generating a predicted clean latent representation. Finally, a VAE decoder transforms this clean latent into the predicted infographic image.</p><details><summary>read the caption</summary>(c) Evaluation pipeline of BizGen</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/sota/new/flux4.png alt></figure></p><blockquote><p>üîº Figure 8 illustrates the BizGen framework. (a) shows the Layout Guided Cross Attention mechanism: the model crops the latent representation of an image into groups of layer-wise visual tokens and extracts layer-wise text tokens based on region prompts. Region-wise cross-attention is then performed to control visual content generation within each region. (b) details the BizGen training pipeline: a noisy latent feature map, layout, and regional prompts are fed into a Latent Diffusion Model (LDM), which outputs a noise prediction. (c) outlines the BizGen evaluation pipeline: a noise map, layout, and regional prompts are inputted into the LDM. Iterative refinement generates a predicted clean latent, decodable into an infographic matching the provided layout and prompts (from users or an LLM such as GPT-4).</p><details><summary>read the caption</summary>Figure 8: Illustrating the Framework of Our Approach: (a) Layout Guided Cross Attention: we first crop the latent representations of the entire image into multiple groups of layer-wise visual tokens and extract the layer-wise text tokens according to the region prompts. Then, we perform region-wise cross attention over the corresponding layer-wise visual tokens and layer-wise text tokens to control the visual content generation within each region. (b) BizGen training pipeline: the inputs to the LDM include a noisy latent feature map, a layout, and the regional prompts, and the output of the LDM is a noise prediction. (c) BizGen evaluation pipeline: the inputs to the LDM include a noise map, a layout, and the regional prompts, and the outputs of the LDM (the combination of multiple iterative refinement) form a predicted clean latent that can be decoded into an infographic image that follows the given layout and regional prompts, which are provided by either users or an LLM like GPT-4o.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/sota/new/flux5.png alt></figure></p><blockquote><p>üîº This figure illustrates a two-stage pipeline for assessing the quality of locally generated image regions within an infographic. The first stage uses only the ground truth (GT) layer caption and the corresponding cropped region from the generated image. However, it&rsquo;s shown that GPT-40, used in this stage, sometimes provides overly cautious assessments (e.g., mistaking a black-and-white cat for a black-and-white dog). The second stage, the Classify-Caption-Score pipeline, addresses this issue by adding a layer classification step (&lsquo;block&rsquo; or &lsquo;object&rsquo;) before generating a score. This helps to reduce inaccurate assessments resulting from ambiguity in the image.</p><details><summary>read the caption</summary>Figure 9: Local region quality accessment pipeline: this pipeline estimates the layer generation success rate based on the reference ultra-dense layout and region-wise prompts as conditions. We find that given only the GT layer caption and corresponding cropped out area of the generated image, GPT-4o tends to give conservative answers. For example, GPT-4o may mistaken the black-and-white cat for a black-and-white dog, or imagine there to be a green apple in the occluded area as shown in the left side of the dashed line. So we design the Classify-Caption-Score pipeline on the right to eliminate the illusion.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/sota/sd1.png alt></figure></p><blockquote><p>üîº The figure shows an infographic generated by the BizGEN model. The infographic is about moving tips and contains several sections with text and illustrations. The OCR accuracy for this specific infographic is 97%. The image demonstrates the model&rsquo;s ability to generate high-quality visual text rendering within the context of a complex infographic layout.</p><details><summary>read the caption</summary>OCR: 97%¬Ø¬Øpercent97\underline{97\%}under¬Ø start_ARG 97 % end_ARG</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/sota/new/sd2.png alt></figure></p><blockquote><p>üîº The figure shows the result of infographics generation using the BizGen model. The image displays high accuracy in Optical Character Recognition (OCR), achieving a 100% score. This indicates that the model successfully rendered the text within the infographic, making it highly legible and easily understandable.</p><details><summary>read the caption</summary>OCR: 100%¬Ø¬Øpercent100\underline{100\%}under¬Ø start_ARG 100 % end_ARG</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/sota/new/sd3.png alt></figure></p><blockquote><p>üîº The figure shows the qualitative results of the infographics generated by the model. Each infographic is shown, and under each is a percentage representing the accuracy of the optical character recognition (OCR) of the text within that infographic. In every case, the accuracy is 100%, demonstrating that the model&rsquo;s text rendering is highly accurate.</p><details><summary>read the caption</summary>OCR: 100%¬Ø¬Øpercent100\underline{100\%}under¬Ø start_ARG 100 % end_ARG</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/sota/new/sd4.png alt></figure></p><blockquote><p>üîº The figure shows an infographic generated by the BizGen model. The infographic is about the benefits of pet ownership and contains several points describing the positive effects of having a pet, including increased social interaction, improved fitness, reduced loneliness, and stress reduction. The infographic features a visually appealing design with a cartoon illustration and several small icons that reinforce the main points. The text is clear and easy to read, and the overall tone is friendly and informative.</p><details><summary>read the caption</summary>OCR: 98%¬Ø¬Øpercent98\underline{98\%}under¬Ø start_ARG 98 % end_ARG</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/sota/new/sd5.png alt></figure></p><blockquote><p>üîº The figure shows an infographic generated by the BizGEN model. The infographic is about the benefits of pet ownership. It includes illustrations of a person with a pet and bullet points that list the benefits, including stress reduction, increased social interaction, fitness improvements, and relief from loneliness. The accuracy of the Optical Character Recognition (OCR) on the text within the infographic is 98%.</p><details><summary>read the caption</summary>OCR: 98%¬Ø¬Øpercent98\underline{98\%}under¬Ø start_ARG 98 % end_ARG</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/sota/dalle1.jpg alt></figure></p><blockquote><p>üîº The figure shows an infographic generated by a model (likely SD3-Large) from the paper, where the optical character recognition (OCR) accuracy is only 37%. This low accuracy indicates significant issues with the model&rsquo;s ability to correctly render the text within the infographic. The image likely shows visual artifacts, misspellings, and possibly other inaccuracies in the generated text, highlighting a key challenge the paper addresses: the difficulty of generating high-quality visual text within complex layouts at the article level.</p><details><summary>read the caption</summary>OCR: 37%¬Ø¬Øpercent37\underline{37\%}under¬Ø start_ARG 37 % end_ARG</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/sota/new/dalle2.jpg alt></figure></p><blockquote><p>üîº The figure shows infographic generation results using SD3-Large. While visually appealing, the results have significant issues with visual text spelling accuracy, as highlighted in a zoomed rectangle region in the image. This demonstrates a challenge for state-of-the-art models in rendering accurate visual text at the article level. This inaccuracy is a key focus of the paper, which proposes improvements to address this challenge.</p><details><summary>read the caption</summary>OCR: 52%¬Ø¬Øpercent52\underline{52\%}under¬Ø start_ARG 52 % end_ARG</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/sota/new/dalle3.jpg alt></figure></p><blockquote><p>üîº The figure shows a low-quality infographic generated by a state-of-the-art model. Only 15% of the text is correctly recognized by optical character recognition (OCR). This highlights the challenge of generating high-quality infographics with accurate text rendering.</p><details><summary>read the caption</summary>OCR: 15%¬Ø¬Øpercent15\underline{15\%}under¬Ø start_ARG 15 % end_ARG</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/sota/new/dalle4.jpg alt></figure></p><blockquote><p>üîº The figure shows an infographic generated by the model. The infographic displays information about consulting process overview. The OCR (Optical Character Recognition) accuracy for this specific infographic is 65%. This indicates that 65% of the text in the generated infographic was correctly recognized by the OCR system.</p><details><summary>read the caption</summary>OCR: 65%¬Ø¬Øpercent65\underline{65\%}under¬Ø start_ARG 65 % end_ARG</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/sota/new/dalle5.jpg alt></figure></p><blockquote><p>üîº The figure shows an infographic generated by a system other than BizGEN. The infographic has text with poor OCR accuracy, specifically only 19% accuracy.</p><details><summary>read the caption</summary>OCR: 19%¬Ø¬Øpercent19\underline{19\%}under¬Ø start_ARG 19 % end_ARG</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/slides/s1p1.png alt></figure></p><blockquote><p>üîº The figure shows the result of infographic generation using SD3-Large. The result shows significant visual artifacts such as spelling errors in the visual text, as indicated by the low OCR accuracy of 21%. This highlights a limitation of existing text-to-image generation models when dealing with longer, article-level text input.</p><details><summary>read the caption</summary>OCR: 21%¬Ø¬Øpercent21\underline{21\%}under¬Ø start_ARG 21 % end_ARG</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/slides/s1p2.png alt></figure></p><blockquote><p>üîº The figure shows an example of an infographic generated by a previous state-of-the-art model. Only 14% of the text was correctly recognized by optical character recognition (OCR). This highlights the challenge of accurately rendering text within complex infographic layouts, a problem that the authors&rsquo; model aims to address.</p><details><summary>read the caption</summary>OCR: 14%¬Ø¬Øpercent14\underline{14\%}under¬Ø start_ARG 14 % end_ARG</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/slides/s1p3.png alt></figure></p><blockquote><p>üîº The figure shows the results of infographic generation using SD3-Large. The zoomed-in rectangle highlights the poor spelling accuracy of the article-level visual text, despite the visually appealing nature of the generated infographics. This demonstrates the challenges of achieving accurate visual text rendering in complex, article-level infographics.</p><details><summary>read the caption</summary>OCR: 10%¬Ø¬Øpercent10\underline{10\%}under¬Ø start_ARG 10 % end_ARG</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/slides/s2p1.png alt></figure></p><blockquote><p>üîº The figure shows an infographic generated by a previous state-of-the-art model, SD3-Large. The infographic attempts to depict business content, but the visual text is marred by significant inaccuracies in spelling (OCR accuracy of only 38%). This highlights the challenges faced by earlier methods in accurately rendering article-level text into infographics.</p><details><summary>read the caption</summary>OCR: 38%¬Ø¬Øpercent38\underline{38\%}under¬Ø start_ARG 38 % end_ARG</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/slides/s2p2.png alt></figure></p><blockquote><p>üîº The figure shows an example of an infographic generated by a previous state-of-the-art model. The text within the infographic has a low OCR accuracy of only 43%, indicating significant issues with the quality of the text rendering. This highlights the challenge the authors of the paper are addressing in generating high-quality visual text within infographics.</p><details><summary>read the caption</summary>OCR: 43%¬Ø¬Øpercent43\underline{43\%}under¬Ø start_ARG 43 % end_ARG</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/slides/s2p4.png alt></figure></p><blockquote><p>üîº The figure shows a sample infographic generated using a state-of-the-art text-to-image generation model. The infographic is about pet hotels, but the text quality is quite low, with only 16% accuracy in Optical Character Recognition (OCR). This low accuracy highlights one of the key challenges addressed in the paper: generating high-quality visual text within infographics.</p><details><summary>read the caption</summary>OCR: 16%¬Ø¬Øpercent16\underline{16\%}under¬Ø start_ARG 16 % end_ARG</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/slides/s3p1.png alt></figure></p><blockquote><p>üîº The figure shows examples of infographics generated by a model with poor OCR accuracy. The caption indicates that the Optical Character Recognition (OCR) performance on these images is 0%, meaning no text could be accurately extracted from the images. This highlights a challenge in generating infographics where accurately rendering text is crucial.</p><details><summary>read the caption</summary>OCR: 0%¬Ø¬Øpercent0\underline{0\%}under¬Ø start_ARG 0 % end_ARG</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/slides/s3p2.png alt></figure></p><blockquote><p>üîº The figure shows the results of infographics generation using a model trained on a dataset with low OCR accuracy (40%). The low accuracy likely results in visual artifacts and inaccuracies in the generated infographics compared to models trained on higher-quality data. The image demonstrates the challenge of generating high-quality infographics from low-accuracy data.</p><details><summary>read the caption</summary>OCR: 40%¬Ø¬Øpercent40\underline{40\%}under¬Ø start_ARG 40 % end_ARG</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/slides/s3p4.png alt></figure></p><blockquote><p>üîº This figure shows an example of an infographic generated using a previous state-of-the-art model. The text in this infographic has very low accuracy (only 14% accuracy according to Optical Character Recognition or OCR). This highlights one of the key challenges the authors are addressing in their paper: generating high-quality, accurate visual text within infographics.</p><details><summary>read the caption</summary>OCR: 14%¬Ø¬Øpercent14\underline{14\%}under¬Ø start_ARG 14 % end_ARG</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/LCFG/lcfg.png alt></figure></p><blockquote><p>üîº The figure shows an example of infographic generation result using SD3-Large model. The OCR accuracy is only 27%, indicating significant issues in visual text rendering, such as misspellings and inaccuracies. This highlights the challenges faced by existing models in handling article-level visual text rendering and the need for improvements in this area.</p><details><summary>read the caption</summary>OCR: 27%¬Ø¬Øpercent27\underline{27\%}under¬Ø start_ARG 27 % end_ARG</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/data_engine_new.png alt></figure></p><blockquote><p>üîº This figure presents a qualitative comparison of infographics generated by BizGen and three state-of-the-art (SOTA) models: FLUX, SD3 Large, and DALL-E 3. Each row displays examples generated by a different model, allowing for a direct visual comparison of the models&rsquo; capabilities in terms of accuracy, aesthetics, and overall quality. The figure visually demonstrates BizGen&rsquo;s superior performance compared to the SOTAs.</p><details><summary>read the caption</summary>Figure 10: Qualitative comparison results with SOTAs. The 1st, 2nd, 3rd, and 4th rows correspond to the results generated with our BizGen, FLUX, SD3 Large, and DALL‚ãÖ‚ãÖ\cdot‚ãÖE3 .</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/x7.png alt></figure></p><blockquote><p>üîº This table compares the visual text spelling precision of BizGEN against state-of-the-art (SOTA) models, including DALL-E3, SD3 Large, and FLUX, for both infographic and slide generation. It showcases the performance difference across various scenarios categorized by the number of layers in the generated images. This allows for evaluating how each model handles different levels of visual text complexity.</p><details><summary>read the caption</summary>Table 1: Comparison with SOTAs on visual text spelling precision for infographic and slide generation.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/multi_layer_infographics/case1/paste_layer_0_rgba.png alt></figure></p><blockquote><p>üîº This table presents a comparison of the performance of different state-of-the-art (SOTA) models and the BIZGEN model on the task of generating infographics and slides. The evaluation is based on scores from GPT-40, a large language model, assessing aesthetics and prompt following. Higher scores indicate better performance. The table allows for comparison across various model types on the specified generation task.</p><details><summary>read the caption</summary>Table 2: Comparison with SOTAs on GPT-4o assessment scores for infographic and slide generation.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/multi_layer_infographics/case1/paste_layer_1_rgba.png alt></figure></p><blockquote><p>üîº The figure shows a set of infographics generated by the BizGen model. Each infographic contains accurately rendered visual text, demonstrating the model&rsquo;s ability to handle long, article-level prompts and ultra-dense layouts. The caption highlights the high accuracy (100%) of the optical character recognition (OCR) process applied to the generated text.</p><details><summary>read the caption</summary>OCR: 100%¬Ø¬Øpercent100\underline{100\%}under¬Ø start_ARG 100 % end_ARG</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/multi_layer_infographics/case1/paste_layer_5_rgba.png alt></figure></p><blockquote><p>üîº The figure displays a series of infographics generated by the BizGEN model. Each infographic contains text and images, demonstrating the model&rsquo;s ability to render visuals accurately from article-level prompts. The OCR accuracy for each infographic is noted as 100%, indicating perfect recognition of the text within the generated images. The figures illustrate the high quality and accuracy achieved by BizGEN in generating complex, article-level infographics.</p><details><summary>read the caption</summary>OCR: 100%¬Ø¬Øpercent100\underline{100\%}under¬Ø start_ARG 100 % end_ARG</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/multi_layer_infographics/case1/paste_layer_6_rgba.png alt></figure></p><blockquote><p>üîº The figure shows the result of an infographic generated by the proposed BizGEN model. The infographic contains multiple layers of visual elements including visual texts and non-visual objects. The caption indicates that the Optical Character Recognition (OCR) accuracy for this infographic is 100%, meaning all text within the image was correctly recognized by OCR software.</p><details><summary>read the caption</summary>OCR: 100%¬Ø¬Øpercent100\underline{100\%}under¬Ø start_ARG 100 % end_ARG</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/multi_layer_infographics/case1/paste_layer_7_rgba.png alt></figure></p><blockquote><p>üîº The figure shows the results of optical character recognition (OCR) on a generated infographic. The OCR accuracy is 100%, indicating that all text in the generated infographic was correctly recognized.</p><details><summary>read the caption</summary>OCR: 100%¬Ø¬Øpercent100\underline{100\%}under¬Ø start_ARG 100 % end_ARG</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/multi_layer_infographics/case1/paste_layer_8_rgba.png alt></figure></p><blockquote><p>üîº The figure shows the results of optical character recognition (OCR) on a generated image. The OCR process successfully recognized 100% of the characters in the image, indicating perfect accuracy. This highlights the quality of the image generation model&rsquo;s visual text rendering capabilities.</p><details><summary>read the caption</summary>OCR: 100%¬Ø¬Øpercent100\underline{100\%}under¬Ø start_ARG 100 % end_ARG</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/multi_layer_infographics/case1/paste_layer_9_rgba.png alt></figure></p><blockquote><p>üîº The figure shows the results of infographics generation using different text-to-image generation models. Each infographic was generated from a relatively short text prompt (386-737 characters). The image displays five infographics with varying levels of detail and complexity. The caption indicates that the optical character recognition (OCR) accuracy is 93% or greater for each image. This suggests that the generated text within the images is largely accurate and legible, although minor errors might be present.</p><details><summary>read the caption</summary>OCR: 93%¬Ø¬Øpercent93\underline{93\%}under¬Ø start_ARG 93 % end_ARG</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/multi_layer_infographics/case1/paste_layer_10_rgba.png alt></figure></p><blockquote><p>üîº The figure shows an infographic generated by the BizGEN model. The infographic includes text and images, and the OCR accuracy is 83%. The image shows a variety of elements, including text, illustrations, and photographs, suggesting a complex design.</p><details><summary>read the caption</summary>OCR: 83%¬Ø¬Øpercent83\underline{83\%}under¬Ø start_ARG 83 % end_ARG</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/multi_layer_infographics/case1/paste_layer_11_rgba.png alt></figure></p><blockquote><p>üîº The figure shows the result of an infographic generated by the model. The model achieved an Optical Character Recognition (OCR) accuracy of 81%. This indicates the model&rsquo;s ability to accurately render text within the generated image. While the original caption is short, this explanation provides more context and clarifies the meaning of the OCR metric.</p><details><summary>read the caption</summary>OCR: 81%¬Ø¬Øpercent81\underline{81\%}under¬Ø start_ARG 81 % end_ARG</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/multi_layer_infographics/case1/paste_layer_12_rgba.png alt></figure></p><blockquote><p>üîº The figure shows an infographic generated by the BizGEN model. The infographic contains text and visuals, and the caption indicates that the Optical Character Recognition (OCR) accuracy of the text within the image is 91%. This suggests that 91% of the text in the generated infographic was correctly identified by the OCR software.</p><details><summary>read the caption</summary>OCR: 91%¬Ø¬Øpercent91\underline{91\%}under¬Ø start_ARG 91 % end_ARG</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/multi_layer_infographics/case1/paste_layer_13_rgba.png alt></figure></p><blockquote><p>üîº This figure showcases the quality of slides generated by the BizGEN model. Each row presents three sample pages from different generated slide decks, offering a visual representation of the model&rsquo;s ability to produce coherent and aesthetically pleasing multi-page presentations. The slides demonstrate the model&rsquo;s capacity to handle diverse layouts, text content, and visual elements in a cohesive manner.</p><details><summary>read the caption</summary>Figure 11: Qualitative results of generated slides. Each row displays three selected pages of the generated slides.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/multi_layer_infographics/case1/paste_layer_14_rgba.png alt></figure></p><blockquote><p>üîº This table presents the results of an experiment evaluating the effect of scaling the size of the infographics dataset on the performance of the model. Increasing the dataset size from 6.5K to 65K and finally to 650K infographics led to significant improvements in the model&rsquo;s ability to generate high-quality infographics. The table likely shows metrics such as visual text spelling precision, non-text layer precision, and a GPT-40 score (a measure of overall quality), demonstrating how these metrics improve as the dataset increases in size. This illustrates the importance of large, high-quality training data for effective model performance in the task of infographic generation.</p><details><summary>read the caption</summary>Table 3: Scaling the infographics dataset improves performance.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/multi_layer_infographics/case1/paste_layer_15_rgba.png alt></figure></p><blockquote><p>üîº This table presents the ablation study results focusing on the impact of the layout-guided cross attention mechanism on the overall performance of the model. It compares the model&rsquo;s performance with and without the layout-guided cross attention, demonstrating its effectiveness in improving key metrics such as text spelling precision and non-text layer precision (LGSR). The results are presented for different numbers of layers in both infographics and slides, showing the impact of the layout-guided approach across various complexity levels.</p><details><summary>read the caption</summary>Table 4: Layout-guided cross attention improves performance.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/multi_layer_infographics/case1/paste_layer_16_rgba.png alt></figure></p><blockquote><p>üîº This table presents ablation study results that demonstrate the impact of image resolution on the performance of the BizGEN model. It shows that using higher resolutions (2240x896) significantly improves the model&rsquo;s ability to accurately render visual text, especially when dealing with a large number of layers. Lower resolutions lead to a substantial drop in performance, highlighting the importance of high-resolution input for effective article-level visual text rendering in infographics and slides.</p><details><summary>read the caption</summary>Table 5: High-resolution is critical for the improvements.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/multi_layer_infographics/case1/paste_layer_17_rgba.png alt></figure></p><blockquote><p>üîº This figure demonstrates the effectiveness of Layout Conditional Classifier-free Guidance (LCFG) in removing artifacts during image generation. The image on the left shows an infographic generated using a global CFG scale of 7. Noticeable artifacts are highlighted with red boxes. The image on the right displays the same infographic after applying LCFG with adjusted CFG values for different layers. These adjustments successfully eliminate the previously identified artifacts, resulting in a cleaner and more refined image.</p><details><summary>read the caption</summary>Figure 12: LCFG removes layer artifacts: the image on the left is generated with a global CFG of 7, with some local artifacts marked by red boxes. By adjusting the CFG values of different layers, these artifacts are eliminated in the second image.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/multi_layer_infographics/case1/paste_layer_18_rgba.png alt></figure></p><blockquote><p>üîº Figure 13 compares the performance of directly assembling infographics using a data engine versus generating them with the BizGen model. The data engine method involves retrieving pre-generated layers and combining them, while BizGen generates the infographics directly. The figure visually demonstrates that BizGen produces significantly better results in terms of aesthetics and prompt following due to its superior control over layer coherence, color contrast, and spatial arrangement, resulting in a more polished, cohesive final product. The data engine approach, while achieving 100% OCR accuracy, falls short in these qualitative aspects.</p><details><summary>read the caption</summary>Figure 13: Comparison of assembling through data engine against BizGen</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/multi_layer_infographics/case1/paste_layer_19_rgba.png alt></figure></p><blockquote><p>üîº This table compares the performance of the BizGEN model with a method that directly assembles infographics from a database of pre-generated layers. The BizGEN approach uses a layer-wise retrieval-augmented generation process, while the assembly method directly combines pre-existing components. The comparison is based on several metrics that evaluate the aesthetics and accuracy of the generated infographics. The results demonstrate BizGEN&rsquo;s superior performance in these aspects, suggesting the benefits of its generative approach over simply assembling existing layers.</p><details><summary>read the caption</summary>Table 6: Comparison with the assembled results produced by data engine.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/multi_layer_infographics/case1/paste_layer_20_rgba.png alt></figure></p><blockquote><p>üîº This table details the hyperparameter settings used during the training of the BizGen model for both infographic and slide generation. It lists values for various parameters, including the backbone model used (Glyph-SDXL), learning rate, batch size, number of epochs, weight decay, dropout rate, gradient clipping, image resolution, LoRA rank, text feature length, glyph loss weight, and the datasets used for training (INFOGRAPHICS-650K and SLIDES-500K). The specific values chosen for each parameter reflect the choices made by the authors for optimal model performance.</p><details><summary>read the caption</summary>Table 7: BizGen Training hyper-parameter choices.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/multi_layer_infographics/case1/paste_layer_21_rgba.png alt></figure></p><blockquote><p>üîº This figure showcases the BizGen framework&rsquo;s functionality by presenting example inputs and their corresponding outputs. The input consists of a combination of a global prompt (overall description of the desired infographic), a layout specifying the spatial arrangement of elements, and region-wise prompts (detailed descriptions for specific regions of the infographic). The output displays the generated infographic that BizGen produced based on these inputs, demonstrating its capability to render complex, multi-layered infographics accurately.</p><details><summary>read the caption</summary>Figure 14: Input and output examples of BizGen</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/multi_layer_infographics/case1/paste_layer_22_rgba.png alt></figure></p><blockquote><p>üîº This table presents a comprehensive evaluation of the multilingual capabilities of the BizGEN model in generating infographics and slides. It shows the visual text spelling precision, a key indicator of the model&rsquo;s accuracy in rendering text across multiple languages. The precision is broken down by the number of layers in each infographic and slide, giving insights into how the model&rsquo;s performance scales with complexity. This allows for a nuanced understanding of BizGEN&rsquo;s strengths and weaknesses across different linguistic and visual design challenges.</p><details><summary>read the caption</summary>Table 8: Illustrating the visual text spelling precision of the multilingual infographics and slides generation results.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/multi_layer_infographics/case1/paste_layer_23_rgba.png alt></figure></p><blockquote><p>üîº This figure shows the results of multi-layer transparent infographic generation. It demonstrates the ability of the model to generate complex infographics with numerous layers, showcasing the ability to control individual elements within the ultra-dense layout. Each row represents a different infographic, illustrating the model&rsquo;s ability to render a range of designs and layouts.</p><details><summary>read the caption</summary>Comp.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/statistics/ppt_set_text_layer_cnt.png alt></figure></p><blockquote><p>üîº This figure showcases the results of infographics generated using the BizGEN model. Each sub-figure shows an infographic example generated by BizGEN, demonstrating the model&rsquo;s capacity to accurately render visual text within ultra-dense layouts. The variations in character count and OCR accuracy across the sub-figures illustrate the model&rsquo;s robustness across different infographic complexities.</p><details><summary>read the caption</summary>#0</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/statistics/ppt_set_element_layer_cnt.png alt></figure></p><blockquote><p>üîº The figure showcases five examples of infographics generated by the BizGEN model. Each infographic demonstrates the model&rsquo;s ability to render accurate visual text within complex layouts. The examples vary in character count (386-737 characters), showcasing the scalability of the model for different lengths of article-level prompts. The caption in the paper indicates the Optical Character Recognition (OCR) accuracy for each infographic, ranging from 93% to 99%, suggesting a high degree of text rendering accuracy.</p><details><summary>read the caption</summary>#1</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/statistics/ppt_set_layer_cnt.png alt></figure></p><blockquote><p>üîº This figure showcases the strong performance of BizGen compared to other state-of-the-art models on the BizEVAL benchmark. The win rates, based on a user study, demonstrate BizGen&rsquo;s superiority in terms of aesthetics, text accuracy, and prompt alignment. The specific win rates are presented for BizGen against three other models, DALL-E3, SD3 Large, and FLUX, respectively.</p><details><summary>read the caption</summary>#2</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/statistics/ppt_page_cnt.png alt></figure></p><blockquote><p>üîº This figure shows an infographic about tax planning strategies. It&rsquo;s divided into four sections: &lsquo;Understand Your Tax Bracket,&rsquo; &lsquo;Maximize Tax-Advantaged Accounts,&rsquo; &lsquo;Tax Loss Harvesting,&rsquo; and &lsquo;Plan for Charitable Giving.&rsquo; Each section includes a concise description and a small illustrative graphic related to the topic. The overall style is informative and aimed at providing brief advice on tax planning.</p><details><summary>read the caption</summary>#3</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/statistics/ppt_text_layer_cnt.png alt></figure></p><blockquote><p>üîº This figure displays infographics generated by the BIZGEN model, showcasing its ability to render accurate visual text within ultra-dense layouts. Each infographic contains a substantial amount of text, ranging from 386 to 737 characters, demonstrating the model&rsquo;s capability in handling article-level visual text rendering tasks. The high accuracy of the OCR (93%-99%) further validates the efficacy of the BIZGEN model&rsquo;s text rendering.</p><details><summary>read the caption</summary>#4</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/statistics/ppt_element_layer_cnt.png alt></figure></p><blockquote><p>üîº Figure 5 demonstrates the decomposition of an infographic into its constituent visual elements (layers). It displays a sample infographic and highlights the different visual layers, such as background, title and subtitle, body text, objects, and decorative elements. The figure also illustrates the &lsquo;ultra-dense layout&rsquo;, which is a crucial element in the paper, specifying the precise spatial arrangement of each visual element within the infographic.</p><details><summary>read the caption</summary>#5</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/statistics/ppt_layer_cnt.png alt></figure></p><blockquote><p>üîº Figure 6 illustrates the overall pipeline of the retrieval-augmented infographics generation process. Starting with a template infographic, the system first selects dominant visual layers. Next, it retrieves the top K most similar transparent layers from a database of high-quality layers. Finally, it replaces layers in the template with retrieved layers to create varied infographic designs.</p><details><summary>read the caption</summary>#6</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/statistics/ppt_char_len.png alt></figure></p><blockquote><p>üîº Figure 7 presents a statistical overview of the INFOGRAPHICS-650K dataset, visualizing the distribution of several key features related to the infographic layers. Specifically, it shows the distribution of the number of text layers, the number of non-text layers, the total number of layers per infographic, and the average number of characters per text layer. These distributions provide insights into the complexity and density of the infographics within the dataset. The median values for each distribution are highlighted using red dashed lines, facilitating a comparison of central tendencies across these different features.</p><details><summary>read the caption</summary>#7</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/slides/s1p1.png alt></figure></p><blockquote><p>üîº Figure 18 presents qualitative results demonstrating multilingual infographics generation capabilities. The figure showcases examples of infographics generated in ten different languages: Chinese, Japanese, Korean, German, Spanish, French, Italian, Portuguese, and Russian. Each language has multiple infographic examples, showcasing variety in style and content. The purpose is to demonstrate the system&rsquo;s ability to handle diverse languages effectively and to achieve high quality outputs in various linguistic contexts.</p><details><summary>read the caption</summary>#8</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/slides/s1p5.png alt></figure></p><blockquote><p>üîº Figure 9 demonstrates the process of evaluating the quality of locally generated infographic regions. It uses GPT-4 to assess how well the generated regions match the intended content based on provided captions and layout information. The process involves classifying each region as either a &lsquo;block&rsquo; or &lsquo;object&rsquo;, and then scoring the quality of the generated content within the region&rsquo;s bounding box. The scoring considers factors like accuracy of the visual elements and their alignment with the caption. An example of the process is also provided to illustrate how captions are used, and how the model evaluates consistency between generated images and intended content.</p><details><summary>read the caption</summary>#9</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/slides/s1p3.png alt></figure></p><blockquote><p>üîº This figure shows qualitative comparison results of slides generation with state-of-the-art methods (SOTA). The first, second, third and fourth rows correspond to the results generated with BizGEN, FLUX, SD3 Large and DALL-E 3, respectively. The left three columns are in the same set, while the right three columns are in another set. The results highlight BizGEN&rsquo;s ability to generate slides with better aesthetics and adherence to the prompt compared to other SOTAs.</p><details><summary>read the caption</summary>#10</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/slides/s4p1.png alt></figure></p><blockquote><p>üîº Figure 11 shows qualitative results of multilingual slides generation. Each row displays three selected pages from generated slides in a specific language. The languages represented are Chinese, Japanese, Korean, German, Spanish, French, Italian, Portuguese, and Russian, respectively. The figure visually demonstrates the model&rsquo;s ability to generate slides in multiple languages while maintaining high visual quality and accurate text rendering.</p><details><summary>read the caption</summary>#11</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/slides/s4p2.png alt></figure></p><blockquote><p>üîº Figure 12 shows the effectiveness of the layout conditional classifier-free guidance (LCFG) in removing artifacts from generated images. The left image shows artifacts in the generated image, which are marked by red boxes. By applying LCFG, these artifacts are successfully removed, as shown in the image on the right.</p><details><summary>read the caption</summary>#12</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/slides/s4p3.png alt></figure></p><blockquote><p>üîº Figure 13 presents a comparison between generating infographics through the data engine and using the BIZGEN model. The data engine approach, while ensuring 100% OCR accuracy, results in less diverse styles, poorer aesthetics, and weaker prompt following compared to BIZGEN. The limitations of directly assembling infographics through the data engine are highlighted, particularly the lack of coherent relationships between text and non-text layers, impacting visual quality.</p><details><summary>read the caption</summary>#13</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/slides/flux/63bc2694336d085e8a33f1dc_page0.png alt></figure></p><blockquote><p>üîº Figure 14 shows an example of the input layout and regional prompts, and the output infographic generated by BizGEN. The input includes a layout specifying the spatial arrangement of visual elements (like text boxes and images), and regional prompts, which provide specific instructions for what should appear in each region. The output demonstrates the infographic generated according to these instructions. This figure illustrates the workflow and capabilities of the BizGEN framework in handling complex layout and region-specific requirements.</p><details><summary>read the caption</summary>#14</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/slides/flux/63bc2694336d085e8a33f1dc_page1.png alt></figure></p><blockquote><p>üîº Figure 15 presents qualitative results for multi-layer transparent infographic generation. It showcases the generation of infographics with more than 20 transparent layers, including various visual elements and visual text layers. This figure demonstrates the capability of the model to produce complex, layered graphics, highlighting the intricate detail and design achievable with the approach.</p><details><summary>read the caption</summary>#15</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/slides/flux/63bc2694336d085e8a33f1dc_page3.png alt></figure></p><blockquote><p>üîº Figure 16 presents a statistical overview of the SLIDES-500K dataset, which contains over 500,000 sets of slides. The figure shows the distributions of several key metrics across the dataset, including the number of text layers per set, the number of non-text layers per set, the total number of layers per set, the total number of pages per set, the number of text layers per page, the number of non-text layers per page, the total number of layers per page, and the number of characters per text layer. Each metric&rsquo;s distribution is illustrated with a histogram, visually representing the frequency of different values within the dataset. The median values are highlighted with red dashed lines for each metric. This figure provides insights into the size and structural properties of the SLIDES-500K dataset, which is important for understanding the scope of the data used for training and evaluation in the context of the paper.</p><details><summary>read the caption</summary>#16</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/slides/flux/63b69394f572983e102b4c56_page0.png alt></figure></p><blockquote><p>üîº Figure 17 shows qualitative comparison results of slides generated by BizGen and other state-of-the-art models. The figure consists of multiple slide sets, each containing several pages. Each row represents a comparison between BizGen and other methods such as FLUX, SD3 large, and DALL-E3. For each slide set, BizGen generally produces more visually appealing and coherent results, outperforming the other methods in terms of visual aesthetics and alignment with prompts.</p><details><summary>read the caption</summary>#17</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/slides/flux/63b69394f572983e102b4c56_page3.png alt></figure></p><blockquote><p>üîº Figure 18 presents qualitative results from generating multilingual infographics across ten different languages. Each row showcases examples generated in a specific language, demonstrating the model&rsquo;s ability to adapt to various writing systems and cultural contexts while maintaining visual fidelity and thematic consistency.</p><details><summary>read the caption</summary>#18</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/slides/flux/63b69394f572983e102b4c56_page4.png alt></figure></p><blockquote><p>üîº Figure 19 presents qualitative results of multilingual slide generation. The figure showcases examples of slides generated in ten different languages: Chinese, Japanese, Korean, German, Spanish, French, Italian, Portuguese, and Russian. Each row displays slides in a particular language, illustrating the model&rsquo;s ability to generate accurate visual text and appropriate aesthetics across multiple linguistic contexts.</p><details><summary>read the caption</summary>#19</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/slides/sd3/63bc2694336d085e8a33f1dc_page0.png alt></figure></p><blockquote><p>üîº This figure showcases the results of generating multi-layer transparent infographics using the BizGen framework. It visually demonstrates the model&rsquo;s ability to render complex infographics with numerous layers, each containing distinct visual elements and text. The high-quality, detailed rendering highlights the effectiveness of the approach in managing intricate layouts and accurately representing diverse visual components. The figure visually demonstrates BizGen&rsquo;s advanced capabilities in generating business content, showcasing the high-quality results achieved.</p><details><summary>read the caption</summary>Figure 15: Multi-layer Transparent Infographics Generation Results.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/slides/sd3/63bc2694336d085e8a33f1dc_page1.png alt></figure></p><blockquote><p>üîº The figure shows an infographic generated by the BIZGEN model. It depicts a business agency, showcasing its services offered, team expertise, and successful projects. The visual style is clean and modern, employing a combination of text and illustrative elements to effectively communicate the agency&rsquo;s capabilities and values.</p><details><summary>read the caption</summary>(a)</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/slides/sd3/63bc2694336d085e8a33f1dc_page3.png alt></figure></p><blockquote><p>üîº This figure shows infographics generated by using SD3-Large. Although visually appealing, the generated text within the infographics suffers from notable spelling inaccuracies. A zoomed-in portion of the figure highlights examples of these errors.</p><details><summary>read the caption</summary>(b)</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/slides/sd3/63b69394f572983e102b4c56_page0.png alt></figure></p><blockquote><p>üîº This infographic uses a light blue background with a white border. It shows four key tips for moving, each with an illustration and concise description. Tips include creating a moving budget, inquiring about utilities, packing efficiently, and planning for moving day. Two cartoon characters are also depicted in the process of moving.</p><details><summary>read the caption</summary>(c)</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/slides/sd3/63b69394f572983e102b4c56_page3.png alt></figure></p><blockquote><p>üîº The figure shows an infographic generated by BizGEN. It contains 594 characters and has an OCR accuracy of 99%. The infographic is about tips for moving, with sections on creating a moving budget, asking about utilities, packing smart, and making a plan for moving day. Each tip includes a short description and a small illustration.</p><details><summary>read the caption</summary>(d)</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/slides/sd3/63b69394f572983e102b4c56_page4.png alt></figure></p><blockquote><p>üîº This infographic uses a light blue background with white text and icons, divided into four sections, each detailing a tip for a smooth move. The tips, presented with numbers 1-4, cover budgeting, utility inquiries, smart packing strategies, and the importance of pre-move planning. Illustrative elements depict people moving boxes and plants.</p><details><summary>read the caption</summary>(e)</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/slides/dalle3/63bc2694336d085e8a33f1dc_page0.jpg alt></figure></p><blockquote><p>üîº This figure shows the distribution of the number of non-text layers per page in the SLIDES-500K dataset. The x-axis represents the number of non-text layers, and the y-axis represents the percentage of pages with that number of non-text layers. The distribution is heavily skewed towards smaller numbers of non-text layers, indicating that most slides in this dataset have a relatively simple layout with few non-text elements. The median number of non-text layers per page is shown as a red dashed line, giving a visual reference for the central tendency of the distribution.</p><details><summary>read the caption</summary>(f)</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/slides/dalle3/63bc2694336d085e8a33f1dc_page1.jpg alt></figure></p><blockquote><p>üîº The figure shows the distribution of the number of total layers per page in the SLIDES-500K dataset. The x-axis represents the number of total layers, and the y-axis shows the percentage of pages with that many layers. The distribution is heavily skewed towards a smaller number of layers, indicating that most slides in this dataset have a relatively simple layout. The median number of layers is marked by a red dashed line. This distribution helps characterize the complexity of slide layouts in the dataset and provides context for the model&rsquo;s ability to handle varying levels of complexity.</p><details><summary>read the caption</summary>(g)</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/slides/dalle3/63bc2694336d085e8a33f1dc_page3.jpg alt></figure></p><blockquote><p>üîº This histogram shows the distribution of the number of characters per text layer in the SLIDES-500K dataset. The x-axis represents the number of characters, and the y-axis represents the frequency of layers with that many characters. The graph shows that the majority of text layers have a relatively small number of characters, with the distribution tailing off as the number of characters increases.</p><details><summary>read the caption</summary>(h)</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/slides/dalle3/63b69394f572983e102b4c56_page0.jpg alt></figure></p><blockquote><p>üîº This figure presents a statistical analysis of the Slides-500K dataset, which consists of various slide presentations. Each sub-figure (a-h) shows the distribution of a specific characteristic across all the slide sets. Specifically, the sub-figures illustrate the distributions of: (a) Number of text layers per slide set (b) Number of non-text layers per slide set (c) Total number of layers per slide set (d) Number of pages per slide set (e) Number of text layers per page (f) Number of non-text layers per page (g) Total number of layers per page (h) Number of characters per text layer The median values for each distribution are marked with red dashed lines, providing a visual representation of the central tendency of the data.</p><details><summary>read the caption</summary>Figure 16: Illustrating the statistics of our Slides-500K:(a)# of text layers/set, (b)# of non-text layers/set, (c) # of total layers/set, (d) # of pages/set, (e)# of text layers/page, (f)# of non-text layers/page, (g) # of total layers/page, (h) # of chars/text layer. We mark the median values with red dashed lines.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/slides/dalle3/63b69394f572983e102b4c56_page3.jpg alt></figure></p><blockquote><p>üîº This figure presents a qualitative comparison of slide generation results from four different models: BizGen (the authors&rsquo; model), FLUX, SD3 Large, and DALL-E 3. Each row displays the results of one model, showcasing several slides within a set. The left three columns represent slides from one set, while the right three show slides from a different set, allowing for comparison of both the model&rsquo;s ability to generate cohesive sets and the overall quality of the generated slides.</p><details><summary>read the caption</summary>Figure 17: Qualitative comparison results of slides generation with SOTAs. The 1st, 2nd, 3rd, and 4th rows correspond to the results generated with our BizGen, FLUX, SD3 Large, and DALL‚ãÖ‚ãÖ\cdot‚ãÖE3. The left three columns are in the same set, while the right three columns are in another.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/slides/dalle3/63b69394f572983e102b4c56_page4.jpg alt></figure></p><blockquote><p>üîº This figure showcases qualitative results of multilingual infographics generation. Each row presents infographics generated for a specific language, illustrating the model&rsquo;s capability in handling diverse linguistic contexts and producing visually appealing outputs.</p><details><summary>read the caption</summary>Chinese</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/multilingual/info/cn/64abc22951ec861b8c72ce30.png alt></figure></p><blockquote><p>üîº This figure showcases qualitative results of multilingual infographics generation. Each row presents examples generated in a different language, namely Chinese, Japanese, Korean, German, Spanish, French, Italian, Portuguese, and Russian. The variations demonstrate the model&rsquo;s ability to adapt visual text rendering and overall style across multiple languages.</p><details><summary>read the caption</summary>Japanese</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/multilingual/info/jp/631b09712da6b05c2a74765e.png alt></figure></p><blockquote><p>üîº This figure showcases qualitative results of multilingual infographic generation, demonstrating the model&rsquo;s ability to generate high-quality infographics in various languages. Each column represents a different language (Chinese, Japanese, Korean, German, Spanish, French, Italian, Portuguese, Russian), with multiple examples of generated infographics displayed within each column. The visual diversity across languages highlights the model&rsquo;s adaptability and capacity to handle diverse textual and visual styles.</p><details><summary>read the caption</summary>Korean</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/multilingual/info/kr/6500721ac9f06ae1107df4bd.png alt></figure></p><blockquote><p>üîº Figure 10 presents a qualitative comparison of infographics generated by BizGEN and other state-of-the-art models (DALL-E3, SD3 Large, and FLUX). Each row displays samples generated by the different models, showing BizGEN&rsquo;s superior ability to produce high-quality, visually appealing results with accurate text compared to the other models.</p><details><summary>read the caption</summary>German</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/multilingual/info/de/65129c024dbe662f109682d9.png alt></figure></p><blockquote><p>üîº Figure 18 showcases a series of infographics generated by the BizGen model, each in a different language. The infographics demonstrate the model&rsquo;s ability to accurately render visual text in multiple languages, including Chinese, Japanese, Korean, German, Spanish, French, Italian, Portuguese, and Russian. The variety of styles and layouts in each infographic illustrates the model&rsquo;s versatility and robustness.</p><details><summary>read the caption</summary>Spanish</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/multilingual/info/es/64ac4c0e4bb13fccc5cd5bb3.png alt></figure></p><blockquote><p>üîº This figure showcases the results of multilingual infographics generation using the BizGEN model. It displays a series of infographics, each created in a different language (Chinese, Japanese, Korean, German, Spanish, French, Italian, Portuguese, Russian), demonstrating the model&rsquo;s ability to render accurate visual text and adhere to prompts across various languages. The figure highlights the model&rsquo;s capacity for high-quality multilingual infographic generation, supporting the study&rsquo;s claim of robust performance across multiple language settings.</p><details><summary>read the caption</summary>French</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/multilingual/info/fr/64a809bfe5100930c118e49c.png alt></figure></p><blockquote><p>üîº This figure showcases qualitative results from multilingual infographics generation. Each column represents a different language (Chinese, Japanese, Korean, German, Spanish, French, Italian, Portuguese, Russian), demonstrating the model&rsquo;s ability to generate infographics with accurate text in various languages.</p><details><summary>read the caption</summary>Italian</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/multilingual/info/it/64cb5efe82496ffb13f3965d.png alt></figure></p><blockquote><p>üîº This figure displays qualitative results of multilingual infographics generation. Each column represents a different language (Chinese, Japanese, Korean, German, Spanish, French, Italian, Portuguese, Russian), showcasing the effectiveness of the model in generating diverse infographics across multiple languages. The results demonstrate the model&rsquo;s ability to accurately render textual elements and adapt to various writing systems.</p><details><summary>read the caption</summary>Portuguese</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/multilingual/info/pt/EAE6cL4UHBI-1-0-web-2-ShV1irOdE98.png alt></figure></p><blockquote><p>üîº Figure 18 presents qualitative results of multilingual infographics generation. The figure showcases examples of infographics generated in ten different languages: Chinese, Japanese, Korean, German, Spanish, French, Italian, Portuguese, and Russian. Each language is represented by a row, visually demonstrating the accuracy and diversity of the model&rsquo;s output across various linguistic contexts. This highlights the model&rsquo;s capability to generate high-quality infographics adapted for different languages while maintaining visual coherence and accuracy.</p><details><summary>read the caption</summary>Russian</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/multilingual/info/ru/65035b9f51c6c24803e93f11.png alt></figure></p><blockquote><p>üîº This figure showcases the qualitative results of the BizGEN model&rsquo;s multilingual infographic generation capabilities. It presents a diverse collection of infographics created in various languages (including English, French, German, Spanish, Italian, Portuguese, Russian, Chinese, Japanese, and Korean), demonstrating BizGEN&rsquo;s ability to handle different languages and generate visually appealing and relevant content based on the language provided.</p><details><summary>read the caption</summary>Figure 18: Qualitative results of multilingual infographics generation.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/multilingual/info/cn/EAE5KJXVPpY-1-0-web-2-vOhIQUAy5a0.png alt></figure></p><blockquote><p>üîº This figure showcases the qualitative results of multilingual slide generation using the BizGen model. Each row displays slides generated using the model with the text in a different language: Chinese, Japanese, Korean, German, Spanish, French, Italian, Portuguese, and Russian. The figure demonstrates the model&rsquo;s ability to accurately render visual text in various languages and scripts, highlighting its cross-lingual capabilities.</p><details><summary>read the caption</summary>Figure 19: Qualitative results of multilingual slides generation.We show the Chinese, Japanese, Korean, German, Spanish, French, Italian, Portuguese, and Russian visual text results in the nine rows subsequently.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/multilingual/info/jp/64a53a00001673a340a9e7a6.png alt></figure></p><blockquote><p>üîº This figure showcases the transparent layers generated by the data engine. The top row displays examples of the diverse styles and high quality achieved in the generated layers. These layers are used in the construction of infographics. The bottom row presents layers that were filtered out, illustrating the quality control process and the criteria used for layer selection. This filtering ensures only high-quality, usable layers are included in the infographic dataset.</p><details><summary>read the caption</summary>Figure 20: Illustrating the transparent layers generated in the data engine: The first row shows examples of the generated multi-style high-quality transparent layers, while the second row demonstrates the filtered ones.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/multilingual/info/kr/6500b4beff23daaa42605467.png alt></figure></p><blockquote><p>üîº This figure showcases qualitative results of multilingual infographics generation. It presents a diverse range of infographic designs, each generated using the BizGEN model and reflecting different languages (Chinese, Japanese, Korean, German, Spanish, French, Italian, Portuguese, and Russian). The visual diversity demonstrates BizGEN&rsquo;s ability to adapt to various linguistic and cultural styles while maintaining high-quality visual text rendering.</p><details><summary>read the caption</summary>Chinese</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/multilingual/info/de/EAFysWRAxYY-1-0-web-2-UqP3-zO1RXU.png alt></figure></p><blockquote><p>üîº The figure showcases a variety of infographics generated using the BizGEN model, each demonstrating a distinct artistic style categorized as &lsquo;Comic&rsquo;. The &lsquo;Comic&rsquo; style is characterized by its playful, cartoonish aesthetic, bright colors, and simplified imagery. These infographics illustrate BizGEN&rsquo;s ability to produce diverse visual outputs while adhering to the user-specified article-level prompts. The visual elements in the comic style are designed to be easily understood and engaging for a broad audience.</p><details><summary>read the caption</summary>Comic</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/multilingual/info/es/64fb12399a07e0e733429bb0.png alt></figure></p><blockquote><p>üîº This figure showcases examples of infographics and slides generated by the BizGEN model. The images demonstrate the model&rsquo;s ability to render accurate visual text, adhere to ultra-dense layouts, and produce high-quality business content. Each example has a caption indicating the number of characters and the accuracy of Optical Character Recognition (OCR).</p><details><summary>read the caption</summary>Illustration</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/multilingual/info/fr/EAFkmiE_1iQ-1-0-web-2-brgfOvKEAl8.png alt></figure></p><blockquote><p>üîº The image showcases a minimalist design concept. It features three framed pieces of artwork or photographs with a simple, clean aesthetic. The frames are gold, white with a brown paper insert, and gold. To the right of the frames, a small potted plant is placed. Three brown leather-bound books are stacked vertically under the artwork. The backdrop is a light-colored wall, enhancing the minimalistic feel. A circular brown icon on the left shows a white line drawing of a couch and picture frame, along with the text &lsquo;Minimalist&rsquo; and &lsquo;CONCEPT&rsquo;. The overall style emphasizes simplicity, natural elements, and a clean, uncluttered look.</p><details><summary>read the caption</summary>Minimalism</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/multilingual/info/it/EAFjBxOfVGI-1-0-web-2-n18vrcq9Q-k.png alt></figure></p><blockquote><p>üîº This figure showcases the versatility of the BizGEN model in generating infographics across various artistic styles. Four distinct styles are presented: Chinese, Comic, Illustration, and Minimalism. Each column displays multiple examples of infographics produced in the corresponding style, demonstrating the model&rsquo;s ability to adapt to different visual aesthetics and thematic approaches while maintaining high-quality visual text rendering. This highlights BizGEN&rsquo;s capability to satisfy diverse user preferences and creative needs.</p><details><summary>read the caption</summary>Figure 21: Qualitative results of multi-style infographics generation. The four columns respectively attribute to four different styles: Chinese, Comic, Illustration and Minimalism.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/multilingual/info/pt/EAFsBextX4M-1-0-web-2-GhAGASa6Dg8.png alt></figure></p><blockquote><p>üîº This figure details the process of evaluating layer generation success rates (LGSR). It visually depicts how the LGSR assessment pipeline evaluates two example layers (one &lsquo;object&rsquo; and one &lsquo;block&rsquo;) from a generated infographic. For each layer, the process illustrates the information provided to GPT-40: the global caption, layer caption, bounding box, occlusion details, element type (object or block), and auxiliary description. The figure shows GPT-40&rsquo;s assessment (a score from 0-10) and justification for each layer.</p><details><summary>read the caption</summary>Figure 22: Example of LGSR Assessment Pipeline: We give the examples of two layers go through our LGSR assessment pipeline, which cover the two layer types: ‚Äúobject‚Äù and ‚Äúblock‚Äù. We demonstrate the different information fed to GPT-4o in every step, and highlight the response by GPT-4o, including its explanation.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.20672/extracted/6312479/img/multilingual/info/ru/EAFKGZ2GCGM-1-0-web-2-OwBZ0HOInf4.png alt></figure></p><blockquote><p>üîº This figure demonstrates the impact of the hyperparameter Œ± (alpha) on the Layout Conditional Classifier-Free Guidance (LCFG) method used in the BizGEN model. Different values of Œ± control the starting timestep at which the LCFG takes effect during the image generation process. The images shown illustrate the results obtained for various Œ± values. Red boxes highlight artifacts present in images generated without LCFG, while green boxes indicate the most aesthetically pleasing images among those generated using LCFG with different Œ± values. This visualization helps to determine the optimal Œ± value for balancing artifact removal and maintaining image quality.</p><details><summary>read the caption</summary>Figure 23: Effect of different choices of Œ±ùõº\alphaitalic_Œ± for LCFG: We use red boxes to mark the artifacts in the images generated without layout conditional CFG and use green boxes to highlight the most aesthetically flawless images across all the others generated with different Œ±ùõº\alphaitalic_Œ±.</details></blockquote></details><h3 class="relative group">Full paper<div id=full-paper class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#full-paper aria-label=Anchor>#</a></span></h3><div id=gallery-f0a61ce2c12227cf1818018804213123 class=gallery><img src=https://ai-paper-reviewer.com/2503.20672/1.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.20672/2.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.20672/3.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.20672/4.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.20672/5.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.20672/6.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.20672/7.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.20672/8.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.20672/9.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.20672/10.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.20672/11.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.20672/12.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.20672/13.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.20672/14.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.20672/15.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.20672/16.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.20672/17.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.20672/18.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.20672/19.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.20672/20.png class="grid-w50 md:grid-w33 xl:grid-w25"></div></div><section class="flex flex-row flex-wrap justify-center pt-4 text-xl"><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.20672/&amp;title=BizGen:%20Advancing%20Article-level%20Visual%20Text%20Rendering%20for%20Infographics%20Generation" title="Share on LinkedIn" aria-label="Share on LinkedIn"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://twitter.com/intent/tweet/?url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.20672/&amp;text=BizGen:%20Advancing%20Article-level%20Visual%20Text%20Rendering%20for%20Infographics%20Generation" title="Tweet on Twitter" aria-label="Tweet on Twitter"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="mailto:?body=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.20672/&amp;subject=BizGen:%20Advancing%20Article-level%20Visual%20Text%20Rendering%20for%20Infographics%20Generation" title="Send via email" aria-label="Send via email"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1-27.64 140.9 68.65 266.2 199.1 285.1 19.01 2.888 36.17-12.26 36.17-31.49l1e-4-.6631c0-15.74-11.44-28.88-26.84-31.24-84.35-12.98-149.2-86.13-149.2-174.2.0-102.9 88.61-185.5 193.4-175.4 91.54 8.869 158.6 91.25 158.6 183.2v16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98.0036c-7.299.0-13.2 4.992-15.12 11.68-24.85-12.15-54.24-16.38-86.06-5.106-38.75 13.73-68.12 48.91-73.72 89.64-9.483 69.01 43.81 128 110.9 128 26.44.0 50.43-9.544 69.59-24.88 24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3c0-149.2-133.9-265.632-287.3-235.57zM239.1 304.3c-26.47.0-48-21.56-48-48.05s21.53-48.05 48-48.05 48 21.56 48 48.05-20.6 48.05-48 48.05z"/></svg></span></a></section></div><script>var oid="views_paper-reviews/2503.20672/index.md",oid_likes="likes_paper-reviews/2503.20672/index.md"</script><script type=text/javascript src=/ai-paper-reviewer/js/page.min.0860cf4e04fa2d72cc33ddba263083464d48f67de06114529043cb4623319efed4f484fd7f1730df5abea0e2da6f3538855634081d02f2d6e920b956f063e823.js integrity="sha512-CGDPTgT6LXLMM926JjCDRk1I9n3gYRRSkEPLRiMxnv7U9IT9fxcw31q+oOLabzU4hVY0CB0C8tbpILlW8GPoIw=="></script></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="flex group mr-3" href=/ai-paper-reviewer/paper-reviews/2503.20220/><span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&larr;</span>
<span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">DINeMo: Learning Neural Mesh Models with no 3D Annotations</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2025-03-26T00:00:00+00:00>26 March 2025</time>
</span></span></a></span><span><a class="flex text-right group ml-3" href=/ai-paper-reviewer/paper-reviews/2503.20198/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Beyond Words: Advancing Long-Text Image Generation via Multimodal Autoregressive Models</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2025-03-26T00:00:00+00:00>26 March 2025</time>
</span></span><span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&larr;</span></a></span></div></div><div class=pt-3><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class=pt-3><script src=https://utteranc.es/client.js repo=pmnxis/pmnxis.github.io issue-term=pathname label=Comment theme=dark-blue crossorigin=anonymous async></script></div></div></footer></article><div id=top-scroller class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer id=site-footer class="py-10 print:hidden"><nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400"><ul class="flex flex-col list-none sm:flex-row"><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=/ai-paper-reviewer/tags/ title>Tags</a></li><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=https://deep-diver.github.io/neurips2024/ title>NeurIPS2024</a></li></ul></nav><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2025
Hugging Face Daily Papers</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a></p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/ai-paper-reviewer/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh+sCQ0E53ghYrxgYqw+0GCRyIEpA=="></script></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://deep-diver.github.io/ai-paper-reviewer/ style=z-index:500><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body><script data-name=BMC-Widget data-cfasync=false src=https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js data-id=chansung data-description="Support me on Buy me a coffee!" data-message data-color=#FFDD00 data-position=Left data-x_margin=18 data-y_margin=18></script></html>