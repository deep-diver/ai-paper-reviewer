{"references": [{"fullname_first_author": "Josh Achiam", "paper_title": "GPT-4 technical report", "publication_date": "2023-03-08", "reason": "This paper is a technical report describing GPT-4, a leading proprietary model that serves as a strong baseline for comparison in video understanding tasks."}, {"fullname_first_author": "Sally Applin", "paper_title": "GPT-4v (ision) system card", "publication_date": "2023-XX-XX", "reason": "This paper introduces GPT-4V, a vision-language model that sets a high bar for performance in video understanding, and is used for comparison in this paper."}, {"fullname_first_author": "Zhe Chen", "paper_title": "How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites", "publication_date": "2024-04-16", "reason": "This paper analyzes the gap between open-source and proprietary video-language models, a crucial context for evaluating Tarsier2's performance."}, {"fullname_first_author": "Jinze Bai", "paper_title": "Qwen-VL: A versatile vision-language model for understanding, localization, text reading, and beyond", "publication_date": "2023-08-12", "reason": "Qwen-VL serves as the foundational model for Tarsier2, making its architecture and capabilities highly relevant to the paper's findings."}, {"fullname_first_author": "Jiawei Wang", "paper_title": "Tarsier: Recipes for training and evaluating large video description models", "publication_date": "2024-XX-XX", "reason": "As a predecessor to Tarsier2, this paper introduces many of the foundational techniques and approaches used in Tarsier2's development."}]}