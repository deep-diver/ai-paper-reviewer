[{"figure_path": "https://arxiv.org/html/2501.07888/extracted/6130403/figs/pretrain_data.png", "caption": "Figure 1: Performance comparison of Tarsier2 with previous SOTA models at 7B-scale and GPT-4o. We report the overall average scores for benchmarks with multiple subtasks/metrics.", "description": "This figure compares the performance of the Tarsier2 model (at 7 billion parameters) with other state-of-the-art (SOTA) large vision-language models (LVLMs), including GPT-40, across various video understanding benchmarks.  Each benchmark typically consists of multiple subtasks or metrics, and the figure shows the average score across these subtasks to provide an overall performance comparison.  The benchmarks cover a range of tasks, such as video captioning, video question answering, video grounding, and hallucination detection, demonstrating the model's capabilities across different video understanding challenges.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2501.07888/extracted/6130403/figs/SFT_grounding.png", "caption": "Figure 2: Overview of Tarsier2 capabilities. Based on its strong ability for detailed video description, Tarsier2 excels in a variety of video-centric tasks. Click the play buttons to view the videos.", "description": "Figure 2 showcases the diverse capabilities of the Tarsier2 model in various video-related tasks.  These tasks demonstrate Tarsier2's proficiency in detailed video description, which serves as the foundation for its performance in more complex tasks.  Specifically, it excels at general video question answering (providing comprehensive and contextually relevant answers), detailed video description (offering frame-by-frame descriptions or concise summaries), video grounding (identifying and localizing specific events or actions), multi-video question answering (comparing and contrasting events across multiple videos), and embodied question answering (responding to questions in a way that shows understanding of a physical context).  The figure presents several examples of each capability, with play buttons to allow readers to view the corresponding video segments.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2501.07888/x2.png", "caption": "Figure 3: Summary of datasets used in the pre-training stage of Tarsier2.", "description": "This figure shows a detailed breakdown of the datasets used in the pre-training stage of the Tarsier2 model.  It visually represents the composition of the training data, highlighting the proportions of different data sources and the types of tasks they encompass.  The datasets include a mix of publicly available datasets and newly collected in-house datasets, categorized by source (e.g., open-source, in-house, academic), task (e.g., video captioning, question answering), and data type (e.g., images, videos, text). The visual representation helps to quickly understand the scale and diversity of the pre-training data used to develop Tarsier2.", "section": "3 Approach"}, {"figure_path": "https://arxiv.org/html/2501.07888/extracted/6130403/figs/human_sbs.png", "caption": "Figure 4: An example of a video description with fine-grained temporal grounding. \u201c\u00a1frame: i\ud835\udc56iitalic_i-j\ud835\udc57jitalic_j\u00bf\u201d indicates that the following event is inferred from frames i\ud835\udc56iitalic_i to j\ud835\udc57jitalic_j. Events are distinguished by color, with corresponding frames and descriptions marked in the same color to indicate their association.", "description": "Figure 4 illustrates a video description with fine-grained temporal grounding.  The notation \"frame: i-j\" indicates that the description of a particular event is based on frames i through j.  Each event is assigned a unique color, and this color is consistently applied to both the corresponding frames in the video and the text describing that event. This visual linkage helps to clearly show the temporal relationship between specific events and the video segments that illustrate them.", "section": "3.2 Supervised fine-tuning"}, {"figure_path": "https://arxiv.org/html/2501.07888/x3.png", "caption": "Figure 5: Preference data construction pipeline for DPO training.", "description": "This figure illustrates the process of creating preference data for Direct Preference Optimization (DPO) training in the Tarsier2 model.  It starts with a raw video, which is processed to generate a positive description using the model.  Then, the video undergoes various types of corruption (clip-switching, clip-reversing, clip-cropping, and down-sampling) to create a corrupted video, which is fed to the model to generate a negative description.  These positive and negative pairs are then filtered using the AutoDQ scorer to ensure high-quality data is retained for DPO training. The final output shows the pipeline used to obtain these preference pairs.", "section": "3.3 Direct Preference Optimization"}, {"figure_path": "https://arxiv.org/html/2501.07888/x4.png", "caption": "Figure 6: Human side-by-side evaluation results of Tarsier2 versus other models.", "description": "This figure displays the results of a human evaluation comparing the video descriptions generated by Tarsier2 against those of three other models: Tarsier-34B, Gemini 1.5 Pro, and GPT-40.  Human evaluators were shown pairs of descriptions for the same video, one from Tarsier2 and one from a competitor, and asked to choose the better description. The chart shows the percentage of times Tarsier2 was preferred over each competitor, providing a direct comparison of the models' performance in terms of human preference.", "section": "4.1 Quantitative Results"}]