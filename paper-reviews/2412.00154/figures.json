[{"figure_path": "https://arxiv.org/html/2412.00154/extracted/6033300/figs/0.jpg", "caption": "Figure 1: o1 replication efforts: upper part from academic institutions and open-source communities, and lower part from the industry.", "description": "This figure illustrates the timeline and various efforts to replicate OpenAI's o1 model. The upper part showcases contributions from academic research institutions and open-source communities, while the lower part highlights those from industry players. Each entry is marked with its name and the date of its release, providing a visual overview of the rapid development and wide interest in replicating and extending the o1 model.", "section": "1 INTRODUCTION"}, {"figure_path": "https://arxiv.org/html/2412.00154/extracted/6033300/figs/1.jpg", "caption": "Figure 2: Self-Play+RL training framework.", "description": "This figure illustrates the iterative training process of the O1-CODER model. It shows the interaction between the Test Case Generator (TCG), which creates test cases for evaluating the code generated by the model, the Policy Model (\u03c0\u03b8), which generates the code, and the Process Reward Model (PPRM), which evaluates the quality of reasoning steps during code generation. The training process is based on self-play, where the model iteratively generates reasoning-enhanced code data, which is then used to train the models and improve their performance. The figure details the flow of data and feedback between the models, illustrating how the training process works in a cycle of improvement.", "section": "Framework Overview"}, {"figure_path": "https://arxiv.org/html/2412.00154/x1.png", "caption": "Figure 3: Template format for TCG SFT", "description": "This figure shows the template used for training the Test Case Generator (TCG) during the Supervised Fine-Tuning (SFT) phase.  The template guides the model to generate test cases by providing a clear instruction, a problem statement, a code section (where a solution is randomly selected from existing solutions), and a test section where the model should generate multiple test cases (input and expected output) to validate the code provided. This structured approach is essential for ensuring consistency and quality in the generated test cases for subsequent steps.", "section": "3.1 Test Case Generator Training"}, {"figure_path": "https://arxiv.org/html/2412.00154/extracted/6033300/figs/3.jpg", "caption": "Figure 4: Pseudocode Prompt for Step-by-Step Refinement", "description": "This figure shows a detailed prompt used to guide a large language model in generating pseudocode. The prompt provides step-by-step instructions, outlining three distinct actions: defining the algorithm's structure using pseudocode, refining the pseudocode by adding more detail, and translating the refined pseudocode into Python code.  This approach is designed to enhance the model's reasoning capabilities by encouraging a structured thought process during code generation.", "section": "3.2 REASONING-ENHANCED CODE DATA SYNTHESIS"}]