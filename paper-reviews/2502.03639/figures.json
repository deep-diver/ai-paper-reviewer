[{"figure_path": "https://arxiv.org/html/2502.03639/x2.png", "caption": "Figure 1: Comparison on Task-Oriented Videos.\nWe present videos generated by different baselines (SVD\u00a0[6], I2VGen-XL\u00a0[41], and DynamiCrafter\u00a0[37]) and compare them with our method. We use the same input conditions for all methods (except that SVD is conditioned only on the image). It can be observed that existing baselines often exhibit severe distortions of hands or objects during human hand-object interactions. In contrast, our method preserves the shapes of both the hand and object during such interactions and ensures smooth transitions throughout the video.", "description": "This figure compares videos generated by four different methods: the authors' new method and three existing baselines (SVD, I2VGen-XL, and DynamiCrafter).  The comparison focuses on task-oriented videos, which involve complex human-object interactions.  The results show that the existing methods often produce unrealistic distortions of hands or objects, while the new method preserves their shapes and ensures smooth transitions, demonstrating better physical understanding in video generation.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2502.03639/x3.png", "caption": "Figure 2: Training Pipeline Overview. We sample video-point pairs, concatenate them in channel dimensions and used to train a UNet. In addition to standard condition and latent cross attention, we further add cross attention between video and point in corresponding channels for a better alignment between the two modalities. Furthermore, the 3D information from the points is utilized to regularize the RGB video generation by applying a misalignment penalty to the video diffusion process.", "description": "This figure illustrates the training pipeline for a video generation model enhanced with 3D point cloud information.  The pipeline begins by sampling paired video and 3D point data. These data are then concatenated along the channel dimension and fed into a UNet architecture.  The UNet utilizes standard conditional and latent cross-attention mechanisms; however, it notably incorporates an additional cross-attention layer between video and point data channels to ensure improved alignment.  Finally, 3D point information is leveraged to regularize the generation of RGB video frames by applying a misalignment penalty within the diffusion model. This penalty refines the generated video, ensuring consistency between the 3D motion information and the 2D video frames.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2502.03639/x4.png", "caption": "Figure 3: PointVid Dataset Generation Workflow. Given an input video, we use the first frame as a reference frame and perform semantic segmentation to obtain masks for foreground objects. Next, we randomly sample pixels with a distribution favoring pixels inside foreground objects. We perform 3D point tracking on these queried pixels, and map these points to the input video frames. The resulting data point contains 3D coordinates of tracked foreground pixels while remaining pixels are zeroed out.", "description": "This figure illustrates the PointVid dataset generation process.  Starting with a video, the first frame is used as a reference. Semantic segmentation isolates foreground objects, creating masks.  Pixels are then randomly selected, weighted to favor foreground areas. 3D point tracking follows these selected pixels across the video's frames. The output is a dataset where each frame's pixels contain 3D coordinates for tracked foreground points; background pixel data is set to zero.", "section": "3.1 PointVid Dataset"}, {"figure_path": "https://arxiv.org/html/2502.03639/x5.png", "caption": "Figure 4: Point Regularization. The reconstructed point cloud in the diffusion output often contains noise and deformations (middle). This issue is mitigated using our point regularization (right). The synthetic point cloud above (e.g. box and shoes falling on the ground) is generated by Kubric \u00a0[13] and trained with our pipeline.", "description": "This figure demonstrates the effectiveness of the proposed point regularization method. The middle column shows a point cloud with noise and deformations, a common issue in diffusion-based video generation.  The right column displays the result after applying the point regularization, showcasing a significantly improved, cleaner point cloud.  The examples used are synthetic point clouds generated by Kubric [13] and subsequently processed using the authors' pipeline, illustrating how the method addresses common issues like noise reduction and deformation correction in point cloud reconstructions.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2502.03639/x6.png", "caption": "Figure 5: Ablation on Point Augmentation and Regularization. Our point-augmented model demonstrates a degree of 3D-awareness compared to the model fine-tuned on video data alone, but it still exhibits some artifacts, which are mitigated through regularization.", "description": "This figure shows an ablation study comparing three models: a baseline model trained only on 2D video data, a model with point augmentation (adding 3D point cloud information), and a model with both point augmentation and regularization.  The results demonstrate that while point augmentation alone introduces a degree of 3D awareness, leading to improved generation quality compared to the baseline, it still produces some artifacts. Applying regularization to the model further improves the quality by mitigating these artifacts and resulting in more physically plausible video generation.", "section": "4.1 Qualitative Results"}, {"figure_path": "https://arxiv.org/html/2502.03639/x7.png", "caption": "Figure 6: Comparison on General Videos. We showcase the generated videos across various categories, including static and dynamic objects, humans, and animals. Our method ensures smooth transitions in object shape and motion and eliminates morphing artifacts.", "description": "Figure 6 presents a comparison of videos generated by different methods across various categories, including static objects, dynamic objects, humans and animals.  The figure demonstrates the superiority of the proposed method (Ours) in producing videos with smooth transitions between object shapes and motions, eliminating artifacts such as object morphing that are present in the baseline methods (I2VGen-XL, SVD, DynamiCrafter).  Each row shows examples of the same scene generated by the different approaches, highlighting the visual differences and the improved results obtained using the method described in the paper.", "section": "4. Qualitative Results"}, {"figure_path": "https://arxiv.org/html/2502.03639/x8.png", "caption": "Figure 7: Comparison on Task-oriented Videos.", "description": "Figure 7 presents a comparison of task-oriented video generation results across different models: the proposed method, SVD [6], I2VGen-XL [41], and DynamiCrafter [37].  Each row shows a different task, such as coloring a stuffed animal, stirring hot chocolate, cutting a sweet potato, preparing a salad, working on fabric, and holding a card.  The goal is to highlight the ability of each method to realistically portray hand-object interaction and maintain consistent object shapes and motions throughout the video.  The proposed method aims to demonstrate improved shape preservation, motion smoothness, and overall realism compared to existing baselines.", "section": "4. Qualitative Results"}, {"figure_path": "https://arxiv.org/html/2502.03639/x9.png", "caption": "Figure 8: Comparison on General Categories.", "description": "Figure 8 presents a qualitative comparison of video generation results across various general video categories, showcasing the performance of the proposed method alongside several baselines (Ours, SVD, I2VGen-XL, and DynamiCrafter).  The figure displays example video frames generated by each method for a selection of diverse scenes, including a gymnast on a pommel horse, a person on a rooftop, someone petting a fox, and metal melting in a furnace. These examples highlight the capabilities of the proposed approach to generate videos with improved realism and physical consistency compared to the baselines, particularly in terms of object motion, shape preservation, and overall coherence.", "section": "4. Qualitative Results"}]