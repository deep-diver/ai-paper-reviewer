[{"figure_path": "https://arxiv.org/html/2412.09624/x1.png", "caption": "Figure 1: Our data curation leverages physical engines, utilizing realistic city assets from UE5 and animated world assets from Unity.", "description": "This figure showcases examples of the virtual environments used to train GenEx.  The left example uses highly realistic city assets from Unreal Engine 5, while the right example demonstrates more stylized, animated assets from Unity.  Leveraging these diverse environments and their physics engines allows for the creation of a robust and varied dataset for training the generative model.  The use of game engines also enables precise control over the virtual camera, facilitating the collection of panoramic images and videos for training.", "section": "2. Generating an Explorable World"}, {"figure_path": "https://arxiv.org/html/2412.09624/x2.png", "caption": "Figure 2: Three panorama representations that can be transformed into one another.", "description": "The figure shows three different representations of a 360\u00b0 panoramic image: cubemap, equirectangular panorama, and sphere panorama.  These are mathematically equivalent and can be converted from one to another.  A cubemap represents the scene projected onto the six faces of a cube.  An equirectangular panorama maps the scene onto a 2D plane, creating a distorted, wide-angle view. A sphere panorama represents the scene on the surface of a sphere. The choice of representation depends on the specific application.  Cubemaps are commonly used in gaming and graphics, while equirectangular panoramas are often used for viewing and storing panoramic images. Sphere panoramas can be useful for certain types of image processing or visualization tasks.", "section": "2. Generating an Explorable World"}, {"figure_path": "https://arxiv.org/html/2412.09624/x3.png", "caption": "Figure 3: From single view to 360\u2218 panorama.", "description": "This figure illustrates the process of generating a 360\u00b0 panoramic view from a single input image.  A single view image is first warped, then inpainted using a diffusion model conditioned on the single image and a text description, resulting in the full 360 panorama.", "section": "2.2. World Initialization"}, {"figure_path": "https://arxiv.org/html/2412.09624/x4.png", "caption": "Figure 4: We model the world transition as a panoramic video generation process. Given the last explored 360\u2218 panorama and an action that rotates the viewing sphere, the model produces a sequence of newly generated panoramic views", "description": "The figure visually illustrates the world transition process within the GenEx system.  It begins with the last fully explored 360\u00b0 panoramic view. An action is then taken which rotates the agent's viewing direction (represented as a sphere). Based on this rotation and the previous panorama, a sequence of new panoramic views is generated, simulating forward movement and exploration in the imagined environment.", "section": "2.3. World Transition"}, {"figure_path": "https://arxiv.org/html/2412.09624/x5.png", "caption": "Figure 5: Three exploration modes \u2014 interactive, GPT-assisted, and goal-driven \u2014 each defined by distinct exploration instructions.", "description": "Figure 5 illustrates three distinct modes of exploration within the GenEx framework: **Interactive Exploration**, where users manually control the agent's movement; **GPT-assisted Free Exploration**, where a GPT guides exploration to ensure high-quality, coherent video generation while preventing model collapse; and **Goal-driven Navigation**, where the agent follows GPT-generated instructions to navigate towards a specified goal within the generated environment. Each mode offers a unique way to interact with and explore the virtual world, catering to different levels of user control and exploration objectives.", "section": "3. Exploration in the Generative World"}, {"figure_path": "https://arxiv.org/html/2412.09624/x6.png", "caption": "Figure 6: GenEx-driven imaginative exploration can gather observations that are just as informed as those obtained through physical exploration.", "description": "Figure 6 illustrates three distinct modes of exploration within the GenEx framework: **Interactive Exploration**, where a human user directly controls the agent's movements and exploration; **GPT-assisted Free Exploration**, where a GPT model guides the agent's exploration to maximize world fidelity and prevent model collapse; and **Goal-driven Navigation**, where the agent receives specific navigation instructions from a GPT based on an initial image and overall goal.  Each mode offers a unique approach to navigating and understanding the generated environment, showcasing the flexibility and potential of GenEx for diverse exploration tasks.  The generated images in this figure are panoramic, and small sections have been extracted from the panorama and arranged as cube faces for better visualization.", "section": "3. Exploration in the Generative World"}, {"figure_path": "https://arxiv.org/html/2412.09624/x7.png", "caption": "Figure 7: Single agent reasoning with imagination and multi-agent reasoning and planning with imagination. (a) The single agent can imagine previously unobserved views to better understand the environment. (b) In the multi-agent scenario, the agent infers the perspective of others to make decisions based on a more complete understanding of the situation. Input and generated images are panoramic; cubes are extracted for visualization.", "description": "Figure 7 illustrates how GenEx empowers single and multi-agents to enhance their decision-making through imagination in both single-agent and multi-agent scenarios. In (a) single-agent reasoning with imagination, a single agent leverages GenEx to generate imaginative views of previously unseen parts of the environment, gaining a more comprehensive understanding before deciding whether to stop or continue at an intersection or how to react to other moving agents. In (b) multi-agent reasoning and planning with imagination, agents in a multi-agent setting utilize GenEx to imagine the perspectives of other agents, enabling them to infer the other agents' intentions and make better informed decisions in collaborative scenarios such as avoiding collisions at intersections.  All input and generated images are panoramic, and cubes are extracted solely for the purpose of clearer visualization of the spatial relations and agent perspectives. ", "section": "4. Advancing Embodied AI"}, {"figure_path": "https://arxiv.org/html/2412.09624/x8.png", "caption": "Figure 8: Imaginative Exploration Loop Consistency (IELC) varying distance and rotations.", "description": "This figure, located in the **Exploration Loop Consistency** section, visualizes the Imaginative Exploration Loop Consistency (IELC) metric across varying distances and rotation amounts during exploration in a generated world. The heatmap uses Mean Squared Error (MSE) between the initial image and final generated image after completing a loop path, averaged over 1000 such paths. Lower MSE values (cooler colors) indicate better loop closure and higher consistency, meaning the generated world remains coherent even after extensive exploration.  The x-axis represents the total rotation, and the y-axis represents the total distance traveled within the loop.", "section": "5.2. Exploration Loop Consistency"}, {"figure_path": "https://arxiv.org/html/2412.09624/x9.png", "caption": "Figure 9: Through generative exploration in z-axis, we are able to generate the 2D bird-eye world view of the current scene.", "description": "This figure showcases the generation of a 2D bird's-eye view map derived directly from a single panoramic image by moving upwards along the z-axis within the generated environment.  The resulting overhead view provides the agent with a broader, more objective understanding of the scene's layout, which is beneficial for spatial reasoning and navigation.", "section": "5.3. Generating Bird's-Eye Worlds"}, {"figure_path": "https://arxiv.org/html/2412.09624/x10.png", "caption": "Figure 10: Through exploration, our model achieves higher quality in novel view synthesis for objects and better consistency in background synthesis, compared to SOTA 3D reconstruction models\u00a0(Voleti et\u00a0al., 2024; Tochilkin et\u00a0al., 2024; StabilityAI, 2023).", "description": "This figure visually compares the quality of novel view synthesis and background consistency generated by the described model against other state-of-the-art (SOTA) 3D reconstruction models. It showcases the model's enhanced capabilities in generating realistic and coherent scenes from novel viewpoints, surpassing existing techniques in terms of visual fidelity and background consistency. The figure likely presents a series of images or a video demonstrating the model's output compared side-by-side with the outputs of other SOTA methods like SV3D, TripoSR, and Stable Zero123, highlighting improvements in object details and seamless integration with the background. The comparison emphasizes the model's ability to generate more realistic and consistent 3D environments from single images, suitable for tasks like exploration and navigation.", "section": "5.4. 3D Consistency"}, {"figure_path": "https://arxiv.org/html/2412.09624/x11.png", "caption": "Figure 11: Active 3D mapping from a single image.", "description": "This figure showcases the capability of GenEx to perform active 3D mapping within its generated world. Starting from a single image, an agent explores the environment. As the agent navigates (as illustrated by the camera trajectory), it gathers observations from different viewpoints within the generated 3D environment. This collected data enables the agent to progressively construct a full 3D map of the scene, as shown in the final 3D model.", "section": "5. Applications"}, {"figure_path": "https://arxiv.org/html/2412.09624/x12.png", "caption": "Figure 12: Left: Pixel Grid coordinate and Spherical Polar coordinate systems; Middle: rotation in Spherical coordinates corresponds to rotation in 2D image; Right: expansion from panorama to cubemap or composition in reverse.", "description": "Figure 13 illustrates the relationship between different coordinate systems and panorama images. The left part of the figure visualizes the Pixel Grid coordinate system (u, v) used for representing pixels in a 2D image and the Spherical Polar coordinate system (\u03c6, \u03b8, r) which defines a point in 3D space using longitude, latitude, and radial distance. The middle portion demonstrates that rotating a panorama image corresponds to a rotation within the spherical coordinate system.  The right part shows how a panorama, which is a 2D representation of a 360\u00b0 view, can be converted into a cubemap, a 3D representation consisting of six square faces, and vice-versa.", "section": "A.1. Preliminary: Equirectangular Panorama Images"}]