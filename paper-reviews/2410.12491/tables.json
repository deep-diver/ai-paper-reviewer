[{"figure_path": "2410.12491/tables/table_7_0.html", "caption": "Table 1: Comparison of LLM toxicity for the groundtruth RLHF LLMs and the IRL-RLHF LLMs. IRL-RLHF LLMs are less toxic than the SFT models they were fine-tuned on and in the case of the 70M, the toxicity of the IRL-RLHF LLM is less than the original RLHF model.", "description": "Table 1 compares the toxicity levels of 70M and 410M parameter LLMs across three training stages: supervised fine-tuning (SFT), original RLHF, and IRL-RLHF, using two toxicity datasets.", "section": "5.1.3 Impact on Toxicity Reduction"}, {"figure_path": "2410.12491/tables/table_12_0.html", "caption": "Table 2: Hyperparameters and training configurations used for fine-tuning the 70M and 410M models with RLHF. Training steps and sequence lengths increase with model size, with the 410M model requiring more than the 70M. Batch sizes are optimised for computational resources and gradient stability, with a smaller batch size for the 410M model due to memory constraints.", "description": "This table presents the hyperparameters and training configurations used for fine-tuning the 70M and 410M language models using reinforcement learning from human feedback.", "section": "5 Experiments"}]