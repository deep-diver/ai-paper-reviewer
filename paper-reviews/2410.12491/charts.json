[{"figure_path": "2410.12491/charts/charts_5_0.png", "caption": "Figure 2: (a) Accuracy and correlation over 30 epochs\n(b) Best performing IRL reward models", "description": "The chart displays the accuracy and correlation metrics of the 70M model over 30 epochs, and compares the accuracy and F1-score of the best performing IRL extracted reward models in classifying toxic text.", "section": "5 Experiments"}, {"figure_path": "2410.12491/charts/charts_5_1.png", "caption": "Figure 2: (a) Accuracy and Pearson Correlation of the 70M Model Over 30 Epochs. The bar chart represents accuracy (%) for each epoch, while the lines denote various correlation metrics between the IRL model's rewards and the groundtruth rewards. The low correlation suggests that correlation is not sufficient to assess the reward model's effectiveness. (b) Accuracy and F1-score comparison of the best-performing IRL extracted reward models in classifying toxic text. The 70M model achieved 80.40% accuracy and 78.39% F1-score, while the 410M model reached 78.20% accuracy and 71.61% F1-score, demonstrating the effectiveness of the learned reward models.", "description": "The chart displays the accuracy and F1-score of the best performing IRL reward models (70M and 410M parameters) in classifying toxic text, showing the 70M model outperforming the 410M model.", "section": "5 Experiments"}, {"figure_path": "2410.12491/charts/charts_5_2.png", "caption": "Figure 3: 70M Model Total Loss & Policy Loss (left), Returns/Mean & Returns/Std (center), and Reward/Mean (right) metrics across 600 training steps for the Original and IRL-RLHF models. Solid lines represent the Original model, while dashed lines indicate the IRL-RLHF model. The IRL-RLHF model demonstrates lower losses compared to the Original model, indicating improved optimization. Although both models display similar return patterns, the IRL-RLHF model achieves a higher normalized mean reward, reflecting a refined optimization objective that aligns more closely with the original reward function.", "description": "Figure 3 shows a comparison of training metrics (total loss, policy loss, returns, and reward) for the original RLHF model and the IRL-RLHF model across 600 training steps, highlighting the improved optimization and reward alignment achieved by the IRL-RLHF model.", "section": "5 Experiments"}, {"figure_path": "2410.12491/charts/charts_5_3.png", "caption": "Figure 4: 410M Model Total Loss & Policy Loss (left), Returns/Mean & Returns/Std (center), and Reward/Mean (right) metrics across 12,000 training steps for the Original and IRL-RLHF models. Solid lines represent the Original model, while dashed lines indicate the IRL model. Metrics are smoothed and the Reward is normalised for better comparison. The alignment of losses and returns between the models suggests that the model's increased capacity improves the IRL process's ability to capture the nuances of the original reward function.", "description": "The chart displays the total loss, policy loss, returns (mean and standard deviation), and mean reward over 12,000 training steps for both the original RLHF model and the IRL-RLHF model, showing the model's performance across various metrics and highlighting the similarity in performance between the two models.", "section": "5 Experiments"}, {"figure_path": "2410.12491/charts/charts_6_0.png", "caption": "Figure 5: Variation in accuracy when running IRL with the same parameters over 30 epochs for (a) 70M and (b) 410M models. The 70M model (a) exhibits a broad range of accuracy values, from below 30% to above 80%, indicating significant fluctuations across different runs. Similarly, the 410M model (b) shows variability in accuracy, ranging from approximately 30% to 70%, underscoring non-identifiability is a challenge in reward learning, where multiple reward functions can produce similar behaviours.", "description": "The heatmaps in Figure 5 show the variability in accuracy across multiple runs of IRL with identical parameters for 70M and 410M language models over 30 epochs, highlighting the non-identifiability of reward functions.", "section": "5.1.4 Non-identifiability and Variability in Reward Models"}]