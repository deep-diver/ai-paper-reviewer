{"references": [{" publication_date": "2015", "fullname_first_author": "Leonardo Mendon\u00e7a de Moura", "paper_title": "The lean theorem prover (system description)", "reason": "This paper introduces the Lean theorem prover, a crucial tool used in the current research for verifying the correctness of formal proofs and for symbolic manipulation of theorems.  Its importance stems from the fact that the proposed method heavily relies on Lean's capabilities for data synthesis and proof verification, making it foundational to the entire approach.", "section_number": 1}, {" publication_date": "2020", "fullname_first_author": "Stanislas Polu", "paper_title": "Generative language modeling for automated theorem proving", "reason": "This is a seminal work in Neural Theorem Proving (NTP), introducing the idea of using neural networks for proof generation. It's highly relevant because the current research addresses a key limitation of this early work\u2014data scarcity\u2014by proposing a method for synthetic data generation to improve the performance of LLMs in theorem proving.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Stanislas Polu", "paper_title": "Formal mathematics statement curriculum learning", "reason": "This paper builds upon the previous work by Polu et al. (2020) and continues to explore using neural networks for automated theorem proving.  It's relevant because it addresses the same fundamental challenge of data scarcity and it's cited in the related work section, highlighting the continued importance of this challenge in the field of NTP.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Trieu H. Trinh", "paper_title": "Solving olympiad geometry without human demonstrations", "reason": "This paper demonstrates a successful application of symbolic reasoning in a specific domain of mathematics. Its relevance lies in its showcasing of the potential of symbolic methods for theorem generation, a key approach adopted in the current research. The paper is cited in both the introduction and related work sections.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Zhaoyu Li", "paper_title": "A survey on deep learning for theorem proving", "reason": "This survey paper provides a comprehensive overview of the current state of deep learning techniques in theorem proving, offering valuable context and background for the current research.  It is important as it gives a broader perspective on related work, highlighting the significance of the data scarcity problem addressed in the current research.", "section_number": 1}, {" publication_date": "2020", "fullname_first_author": "Wang", "paper_title": "Learning to prove theorems by learning to generate theorems", "reason": "This paper directly addresses the problem of data scarcity in theorem proving by proposing a method for synthesizing theorems.  It is highly relevant as it tackles the same central challenge as the current research, although using a different approach.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Yuhuai Wu", "paper_title": "Autoformalization with large language models", "reason": "This paper explores the use of large language models for automating the process of formalizing mathematical theorems, a task directly relevant to the data synthesis problem. It is important because it presents an alternative approach to augmenting datasets for NTP, providing a valuable comparison with the current method of direct symbolic synthesis.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Huajian Xin", "paper_title": "Deepseek-prover-v1.5: Harnessing proof assistant feedback for reinforcement learning and monte-carlo tree search", "reason": "This paper focuses on improving theorem proving using reinforcement learning and a proof assistant, which is highly relevant to the current research's aim of improving NTP performance. It explores a different technique for improving NTP, offering a valuable contrast to the data synthesis approach.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Kaiyu Yang", "paper_title": "Leandojo: Theorem proving with retrieval-augmented language models", "reason": "This paper presents Leandojo, a tool that is central to the current research for interacting with the Lean theorem prover.  The tool is used extensively in the proposed data synthesis method, making this reference crucial for understanding the implementation and experimental setup.", "section_number": 3}, {" publication_date": "2021", "fullname_first_author": "Samyam Rajbhandari", "paper_title": "Zero-infinity: Breaking the GPU memory wall for extreme scale deep learning", "reason": "This paper is important because it describes a technique (Deepspeed ZeRO Stage3) that is used in the current research to efficiently train large language models on the augmented datasets.  This highlights the computational challenges involved in training these large models.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Tri Dao", "paper_title": "Flashattention-2: Faster attention with better parallelism and work partitioning", "reason": "This paper introduces FlashAttention-2, an optimization technique used in the current research to accelerate the training process of the LLMs.  It is essential for understanding the efficiency aspects of the experimental setup.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Abhimanyu Dubey", "paper_title": "The llama 3 herd of models", "reason": "This paper introduces the Llama-3 language models, which are used as the base models in the current research for continual pre-training and fine-tuning.  It is critical to understanding the choice of LLMs for the experiments.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Daya Guo", "paper_title": "Deepseek-coder: When the large language model meets programming - the rise of code intelligence", "reason": "This paper introduces another LLM (deepseek-coder) used in the experiments.  It's significant as it shows the use of a different LLM architecture, offering a comparative analysis of the proposed method across different model types.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Zhangir Azerbayev", "paper_title": "Llemma: An open language model for mathematics", "reason": "This paper presents an open language model for mathematics, which is relevant to the current research because it provides a comparative baseline for evaluating the performance of LLMs in mathematical reasoning tasks.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Yaowei Zheng", "paper_title": "Llamafactory: Unified efficient fine-tuning of 100+ language models", "reason": "This paper is crucial because it introduces LlamaFactory, the open-source codebase used for the training experiments in the current research.  Understanding this tool and its functionalities is essential for interpreting the experimental results and reproducibility.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Kunhao Zheng", "paper_title": "minif2f: a cross-system benchmark for formal olympiad-level mathematics", "reason": "This paper introduces the miniF2F benchmark, one of the primary evaluation datasets used in the current research to assess the performance of LLMs on out-of-distribution theorem proving tasks.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Maciej Mikula", "paper_title": "Magnushammer: A transformer-based approach to premise selection", "reason": "This paper is highly relevant because it explores premise selection, a crucial aspect of theorem proving, which is implicitly related to the current research's method of finding 'invocable theorems'.  Understanding different strategies for premise selection enhances the understanding of the overall theorem-proving process.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Guillaume Lample", "paper_title": "Hypertree proof search for neural theorem proving", "reason": "This work explores a different approach to neural theorem proving using a novel search strategy.  It is important because it demonstrates alternative strategies to improve the efficiency of NTP, offering a valuable comparison with the data-centric approach of the current research.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Sean Welleck", "paper_title": "LLMSTEP: LLM proofstep suggestions in lean", "reason": "This paper introduces LLMSTEP, a tool for generating proof steps in Lean, which is indirectly related to the current research's focus on improving the efficiency and accuracy of theorem proving. It's important for understanding the landscape of tools used for automated theorem proving.", "section_number": 2}, {" publication_date": "1997", "fullname_first_author": "Bruno Barras", "paper_title": "The Coq proof assistant reference manual", "reason": "This paper describes the Coq proof assistant, a prominent system in formal verification. While not directly used in the current research, it provides valuable context and comparison, as Lean and Coq are both interactive theorem provers, illustrating that various approaches exist within formal verification.", "section_number": 2}]}