{"references": [{" publication_date": "2015", "fullname_first_author": "Leonardo Mendon\u00e7a de Moura", "paper_title": "The lean theorem prover (system description)", "reason": "This paper introduces the Lean theorem prover, a core tool used in the Alchemy framework for theorem synthesis and evaluation.  Lean is crucial for generating and verifying the correctness of the synthetic theorems, making this a foundational reference for understanding the technical implementation of the paper's contributions.", "section_number": 3}, {" publication_date": "2020", "fullname_first_author": "Stanislas Polu", "paper_title": "Generative language modeling for automated theorem proving", "reason": "This paper is a seminal work in Neural Theorem Proving (NTP), proposing the use of language models for automated theorem proving.  Alchemy builds upon this foundational work by addressing its limitation of data scarcity, a key issue highlighted in this foundational paper.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Kunhao Zheng", "paper_title": "minif2f: a cross-system benchmark for formal olympiad-level mathematics", "reason": "The miniF2F benchmark is used as a key evaluation metric in this paper to assess the generalizability of the proposed method beyond the in-distribution data.  This demonstrates the importance of this reference for evaluating the out-of-distribution performance of the improved models.", "section_number": 4}, {" publication_date": "2020", "fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "reason": "This paper introduces a family of large language models (LLMs) that are central to Alchemy's approach of improving theorem proving capabilities by fine-tuning these large models. The models' ability to learn from relatively limited data directly impacts the success of the synthetic data augmentation method.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Abhimanyu Dubey", "paper_title": "The llama 3 herd of models", "reason": "This paper introduces the Llama-3-8B LLM, one of the base models used for training in the Alchemy experiments.  The performance of this specific LLM significantly influences the results and interpretation of Alchemy's effectiveness.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Daya Guo", "paper_title": "Deepseek-coder: When the large language model meets programming - the rise of code intelligence", "reason": "This paper introduces deepseek-coder-base-v1.5-7B, the other base model used in Alchemy's experiments. Its selection and performance impact the overall results and comparison across different base models.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Zhangir Azerbayev", "paper_title": "Llemma: An open language model for mathematics", "reason": "This paper is referenced for its evaluation methodology applied to the Leandojo benchmark.  Consistent evaluation practices are crucial for accurately comparing the performance enhancements achieved by Alchemy.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Kaiyu Yang", "paper_title": "Leandojo: Theorem proving with retrieval-augmented language models", "reason": "Leandojo is a crucial tool used in the Alchemy pipeline for interacting with the Lean theorem prover. Its capabilities directly affect the data synthesis process, making this paper fundamentally important in understanding the implementation details.", "section_number": 4}, {" publication_date": "2021", "fullname_first_author": "Samyam Rajbhandari", "paper_title": "Zero-infinity: Breaking the GPU memory wall for extreme scale deep learning", "reason": "The DeepSpeed ZeRO Stage3 optimization technique (referenced in this paper) is used in Alchemy's training process to handle the large models and datasets. Efficient training is crucial for the success of Alchemy's methodology.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Tri Dao", "paper_title": "Flashattention-2: Faster attention with better parallelism and work partitioning", "reason": "FlashAttention-2 is used in Alchemy's training process to accelerate the training of large LLMs.  Faster training is critical for making the proposed method more practical and scalable.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Jesse Michael Han", "paper_title": "Proof artifact co-training for theorem proving with language models", "reason": "This paper addresses similar challenges in NTP, particularly regarding data scarcity.  Its relevance to Alchemy lies in its exploration of techniques to improve NTP performance, which serves as valuable background for the proposed Alchemy method.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Maciej Mikula", "paper_title": "Magnushammer: A transformer-based approach to premise selection", "reason": "This paper focuses on premise selection in theorem proving, a related task to theorem synthesis. Its techniques and findings contribute to the broader context of improving NTP systems, which is relevant to Alchemy's objective of improving LLM performance in theorem proving.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Yuhuai Wu", "paper_title": "Autoformalization with large language models", "reason": "This paper discusses autoformalization, a method contrasted with the direct symbolic synthesis method in Alchemy. Understanding the limitations and strengths of autoformalization is essential to appreciate the advantages of Alchemy's approach.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Trieu H. Trinh", "paper_title": "Solving olympiad geometry without human demonstrations", "reason": "This paper focuses on generating theorems using symbolic reasoning, similar to Alchemy's approach.  However, this paper is restricted to a specific domain (olympiad geometry), while Alchemy aims for a more general approach.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "Stanislas Polu", "paper_title": "Generative language modeling for automated theorem proving", "reason": "This paper introduces a foundational approach in Neural Theorem Proving (NTP). The Alchemy framework expands on this foundation to address the critical challenge of data scarcity.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Sean Welleck", "paper_title": "LLMSTEP: LLM proofstep suggestions in lean", "reason": "This work explores using LLMs to suggest proof steps in Lean, a task closely related to the theorem generation in Alchemy.  The findings of this paper inform the techniques used for training the LLMs in Alchemy.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Yaowei Zheng", "paper_title": "Llamafactory: Unified efficient fine-tuning of 100+ language models", "reason": "LlamaFactory is a key tool used in Alchemy's training process, providing the infrastructure for efficient training of the large language models on the generated dataset. This tool is directly involved in the experimental setup and results.", "section_number": 4}, {" publication_date": "1997", "fullname_first_author": "Bruno Barras", "paper_title": "The Coq proof assistant reference manual: Version 6.1", "reason": "This paper describes Coq, a popular proof assistant used for formal verification. While Lean is used in Alchemy, understanding Coq provides context and demonstrates the importance of using proof assistants in the field of formal verification.", "section_number": 2}, {" publication_date": "1994", "fullname_first_author": "Lawrence C. Paulson", "paper_title": "Isabelle - A Generic Theorem Prover", "reason": "This paper introduces Isabelle, another widely used proof assistant. Its inclusion highlights the landscape of automated theorem provers and the significance of the choice of Lean in Alchemy's methodology.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Zhaoyu Li", "paper_title": "A survey on deep learning for theorem proving", "reason": "This survey provides a comprehensive overview of the field of deep learning for theorem proving, offering a broad context for understanding Alchemy's contribution and its position within the current state of the research.", "section_number": 2}]}