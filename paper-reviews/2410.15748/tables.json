[{"figure_path": "2410.15748/tables/table_8_0.html", "caption": "Table 2: Number of theorems. Stage one: the number of invocable instructions for all candidate theorems. Stage two: the number of theorems that pass the verification of the Lean theorem prover.", "description": "The table shows the number of theorems at different stages of the data synthesis pipeline for the rw and apply tactics, indicating a significant increase in the number of theorems.", "section": "4.1 Implementation Details"}, {"figure_path": "2410.15748/tables/table_9_0.html", "caption": "Table 3: Results on Mathlib. tidy: a tactic in Mathlib that uses heuristics to complete a proof. We select the performance of each model solely fine-tuned using Mathlib-train as the main baseline. Mathlib-train + x: the performance of the model pre-trained and fine-tuned on a mixture of Mathlib-train and additional data about x.", "description": "Table 3 presents the results of the theorem proving experiments on the Mathlib benchmark, comparing the performance of models trained with different data combinations and search strategies.", "section": "4.3 EXPERIMENTAL RESULTS"}, {"figure_path": "2410.15748/tables/table_10_0.html", "caption": "Table 4: Effectiveness of continual pre-training. We grouped the dataset for CPT and SFT by the tactic employed in the additional state-tactic pairs.", "description": "This table shows the effectiveness of continual pre-training on the performance of LLMs across diverse supervised fine-tuning settings, demonstrating a positive influence of continual pre-training on the theorem-proving ability of LLMs.", "section": "4.3.2 EFFECTIVENESS OF CONTINUAL PRETRAINING"}, {"figure_path": "2410.15748/tables/table_10_1.html", "caption": "Table 5: Results on miniF2F. We evaluate the performance across different data compositions and list the ratio of rw, apply, norm_num and linarith used by Llama3-8b to prove these theorems.", "description": "Table 5 presents the results of the miniF2F benchmark, showing the performance of different model variations and the proportion of specific tactics used in successful proofs.", "section": "4.3 Experimental Results"}, {"figure_path": "2410.15748/tables/table_18_0.html", "caption": "Table 2: Number of theorems. Stage one: the number of invocable instructions for all candidate theorems. Stage two: the number of theorems that pass the verification of the Lean theorem prover.", "description": "Table 2 presents the number of theorems at different stages of the data synthesis pipeline, showing the expansion achieved by the method and the conversion ratio from potential to verified theorems.", "section": "4.1 Implementation Details"}, {"figure_path": "2410.15748/tables/table_25_0.html", "caption": "Table 2: Number of theorems. Stage one: the number of invocable instructions for all candidate theorems. Stage two: the number of theorems that pass the verification of the Lean theorem prover.", "description": "The table presents the number of theorems at different stages of the data synthesis process, showing a significant increase in the number of theorems after verification.", "section": "4.1 Implementation Details"}, {"figure_path": "2410.15748/tables/table_26_0.html", "caption": "Table 2: Number of theorems. Stage one: the number of invocable instructions for all candidate theorems. Stage two: the number of theorems that pass the verification of the Lean theorem prover.", "description": "Table 2 presents the number of theorems at different stages of the data synthesis pipeline, showing a significant increase in the number of theorems after verification.", "section": "4.1 Implementation Details"}, {"figure_path": "2410.15748/tables/table_28_0.html", "caption": "Table 6: The effectiveness of different tactics", "description": "Table 6 presents the performance of Llama-3-8b and deepseek-coder-7b-base-v1.5 models fine-tuned with different combinations of additional data (rw, apply, have tactics) on the random and novel premises splits of the Leandojo benchmark.", "section": "4.3.1 MAIN RESULTS"}, {"figure_path": "2410.15748/tables/table_28_1.html", "caption": "Table 3: Results on Mathlib. tidy: a tactic in Mathlib that uses heuristics to complete a proof. We select the performance of each model solely fine-tuned using Mathlib-train as the main baseline. Mathlib-train + x: the performance of the model pre-trained and fine-tuned on a mixture of Mathlib-train and additional data about x.", "description": "Table 3 presents the performance of different models on the Mathlib benchmark, comparing models fine-tuned solely on Mathlib-train against those also pre-trained and fine-tuned with additional synthetic data generated using the rw and apply tactics.", "section": "4.3 Experimental Results"}]