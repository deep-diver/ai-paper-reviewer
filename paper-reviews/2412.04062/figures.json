[{"figure_path": "https://arxiv.org/html/2412.04062/x1.png", "caption": "Figure 1: \nUp to 91% forward step reduction with ZipAR. Samples are generated by Emu3-Gen model with next-token prediction paradigm (the first column) and ZipAR (the right three columns).", "description": "Figure 1 showcases the significant speedup achieved by ZipAR, a novel parallel decoding method, in auto-regressive image generation.  The figure compares image generation using the Emu3-Gen model with the standard next-token prediction approach (leftmost column) to generation using ZipAR with different parameter settings (remaining columns).  The results demonstrate that ZipAR can reduce the number of forward steps required by up to 91%, substantially accelerating the image generation process without noticeable loss in image quality.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2412.04062/x2.png", "caption": "Figure 2: \n(a) An overview of the training and decoding pipeline for auto-regressive (AR) visual generation models. For models trained with a next-token prediction objective, each forward pass generates a single visual token. (b) Medusa\u00a0[2] and Jacobi\u00a0[16] decoding predict multiple adjacent tokens in sequence order. (c) MAR\u00a0[13] predicts multiple tokens in a random order. (d) The proposed ZipAR predicts multiple spatially adjacent tokens.", "description": "Figure 2 illustrates the decoding methods of autoregressive visual generation models. (a) shows the standard training and decoding pipeline for these models, where each forward pass produces one token. (b) shows how Medusa and Jacobi accelerate the process by predicting multiple adjacent tokens sequentially. (c) MAR improves on this by predicting multiple tokens in random order, rather than sequentially. (d) ZipAR, the proposed method, predicts multiple spatially adjacent tokens for improved efficiency.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2412.04062/x3.png", "caption": "Figure 3: \nThe attention scores of visual tokens in the Lumina-mGPT-7B\u00a0[14] and LlamaGen-XL\u00a0[17] models. Slash lines indicate that significant attention scores are allocated to tokens at fixed intervals, corresponding to tokens in the same column of previous rows. The full attention scores are presented by storing the attention scores of each visual token during decoding and concatenating them.", "description": "Figure 3 visualizes the attention patterns of two large language models (LLMs), Lumina-mGPT-7B and LlamaGen-XL, during image generation.  The heatmaps show how strongly each token (representing a part of the image) attends to other tokens.  Crucially, the diagonal slash lines highlight a strong correlation between tokens in the same column across different rows, demonstrating spatial locality in the attention mechanisms. This implies that the models are not purely sequential in their processing; they utilize contextual information from spatially related tokens to generate the image more efficiently.", "section": "3 Method"}, {"figure_path": "https://arxiv.org/html/2412.04062/x4.png", "caption": "Figure 4: \nA toy example of the ZipAR framework. The window size is set to 2222 in this toy example.", "description": "Figure 4 illustrates the ZipAR framework's operation using a simplified example.  The figure shows how multiple rows of image tokens (represented by X<sub>i,j</sub>) are processed concurrently. The window size, a key parameter in ZipAR, is set to 2.  This parameter dictates the spatial range of token dependencies considered for parallel decoding.  With a window size of 2, the algorithm can initiate the decoding of tokens in the next row (e.g., X<sub>1,0</sub>, X<sub>1,1</sub>, X<sub>1,2</sub>) once the tokens in the current row within the window are generated. This parallel processing accelerates image generation by reducing the number of sequential steps required by traditional autoregressive methods.", "section": "3.2 Inference with ZipAR"}, {"figure_path": "https://arxiv.org/html/2412.04062/x5.png", "caption": "Figure 5: \nSamples generated by Emu3-Gen model with next-token prediction paradigm (the first column) and ZipAR under different configurations (the right three columns). The classifier-free guidance is set to 6.0.", "description": "Figure 5 presents a comparison of image generation results using the Emu3-Gen model. The first column showcases images generated using the standard next-token prediction method. The subsequent three columns display images generated with the ZipAR framework under various configurations. All images share the same prompts, allowing for a direct comparison of image quality and generation efficiency across different methods. A classifier-free guidance value of 6.0 was consistently used throughout the experiment. This figure visually demonstrates the impact of ZipAR on accelerating image generation.", "section": "4.3 Qualitative Visualizations"}, {"figure_path": "https://arxiv.org/html/2412.04062/x6.png", "caption": "Figure 6: \nSamples generated by LlamaGen-XL model with next-token prediction paradigm (the first column) and ZipAR under different configurations (the right three columns). The classifier-free guidance is set to 7.5.", "description": "Figure 6 presents a comparison of image generation results using the LlamaGen-XL model.  The first column shows images generated using the standard next-token prediction method. The subsequent three columns illustrate the results obtained using the ZipAR method with varying configurations (different window sizes). All images share the same prompts and classifier-free guidance parameter (set to 7.5). This figure visually demonstrates the impact of ZipAR on accelerating image generation while maintaining image quality.", "section": "4.3 Qualitative Visualizations"}, {"figure_path": "https://arxiv.org/html/2412.04062/x7.png", "caption": "Figure 7: \nSamples generated by Lumina-mGPT-7B-768 model with next-token prediction paradigm (the first column) and ZipAR under different configurations (the right three columns). The classifier-free guidance is set to 3.", "description": "Figure 7 showcases image samples generated using the Lumina-mGPT-7B-768 model.  The leftmost column displays images created using the standard next-token prediction method. The remaining three columns illustrate images generated with the ZipAR method, each using a different configuration (various window sizes).  A consistent classifier-free guidance value of 3 was used across all generated samples.", "section": "4.3 Qualitative Visualizations"}]