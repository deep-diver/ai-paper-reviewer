[{"figure_path": "https://arxiv.org/html/2412.01800/x2.png", "caption": "Figure 1: Left: Comparisons of physical commonsense understanding capability. Our PhysVLM identifies that a motorcycle colliding and flipping a car is implausible while GPT-4o [92] and LLaVA-Next-Video [72] fail to accurately interpret the physical commonsense violations in the video; Right: The taxonomy of PhysGame benchmark including 4 primary categories and 12 fine-grained sub-categories.", "description": "This figure shows a comparison of physical commonsense understanding between different video LLMs. On the left, a gameplay video is shown where a motorcycle collides with a car, causing it to flip unrealistically. PhysVLM correctly identifies this as a violation of physical commonsense.  In contrast, GPT-4 and LLaVA-Next-Video fail to recognize this implausible event. The right side displays a taxonomy used in the PhysGame benchmark, illustrating its four primary categories (mechanics, kinematics, optics, and material properties) and the 12 associated fine-grained sub-categories, providing a detailed breakdown of the types of physical common sense violations included in the benchmark.", "section": "3. Dataset & Method"}, {"figure_path": "https://arxiv.org/html/2412.01800/x3.png", "caption": "Figure 2: The annotated multi-choice question in PhysGame. The correct option is annotated in green.", "description": "This figure shows an example of a multiple-choice question used in the PhysGame benchmark.  The question asks to describe a glitch or anomaly observed in a gameplay video. Four options are provided, each describing a different potential anomaly. The correct answer is highlighted in green, illustrating how the PhysGame dataset is annotated for evaluating video LLMs' understanding of physical common sense.", "section": "3.1. PhysGame Benchmark"}, {"figure_path": "https://arxiv.org/html/2412.01800/x4.png", "caption": "Figure 3: Overview of the direct preference optimization training, where the preferred data is generated with the guidance of associated meta-information (i.e., title) while dispreferred data is generated with misleading titles (i.e., meta-information hacking), fewer frames (i.e., temporal hacking) and lower resolutions (i.e., spatial hacking).", "description": "This figure illustrates the process of direct preference optimization (DPO) used to enhance the video LLM's ability to identify physical commonsense violations.  The preferred data consists of question-answer pairs generated using accurate video titles (meta-information) to guide the LLM.  In contrast, the dispreferred data is created using misleading titles (meta-information hacking), reduced frame counts (temporal hacking), and lowered spatial resolutions (spatial hacking). This approach helps the model learn to distinguish between physically plausible and implausible video content.", "section": "3. Dataset & Method"}, {"figure_path": "https://arxiv.org/html/2412.01800/x5.png", "caption": "Figure 4: Example cases in the PhysInstruct dataset with (w/) or without (w/o) meta-information hints.", "description": "This figure showcases two example question-answer pairs from the PhysInstruct dataset.  The dataset is used for instruction tuning of a video large language model (VLM) to improve its ability to understand physical commonsense in videos.  The first example uses the video title as a hint (w/), guiding the model to correctly identify the physical glitch shown in the video. The second example omits the title (w/o), and in this case, the model does not correctly identify the glitch, demonstrating how meta information such as the video title can aid in physical commonsense understanding.", "section": "3. Dataset & Method"}, {"figure_path": "https://arxiv.org/html/2412.01800/x6.png", "caption": "Table 7: Ablation studies of the temporal, spatial, and meta-info hacking in the PhysDPO dataset generation process.", "description": "This table presents the results of ablation studies conducted to evaluate the impact of different data augmentation techniques on the performance of the PhysVLM model.  Specifically, it examines the effect of removing \"temporal hacking\", \"spatial hacking\", and \"meta-info hacking\" during the generation of the PhysDPO dataset. The results show the average accuracy of the model on the PhysGame benchmark with each of these augmentations removed, revealing the relative contribution of each technique to overall model performance.", "section": "3. Dataset & Method"}, {"figure_path": "https://arxiv.org/html/2412.01800/x7.png", "caption": "(a)", "description": "The figure shows a qualitative comparison of three different video LLMs (PhysVLM, GPT-4, and LLaVA-Next-Video) in identifying visual glitches in gameplay videos.  The left column shows a sequence of frames from a gameplay video, while the right column shows the captions generated by the respective models, describing the glitches or physical commonsense violations identified in the video. The specific example focuses on a scenario involving a motorcycle colliding with a car, which is followed by the car flying unrealistically into the air. The models vary significantly in their ability to detect and describe the physics-related issues.", "section": "3.1. PhysGame Benchmark"}, {"figure_path": "https://arxiv.org/html/2412.01800/x8.png", "caption": "(b)", "description": "The figure shows qualitative examples of open-ended questions in the PhysGame benchmark.  PhysGame uses both open-ended and multiple-choice questions to assess the understanding of physical common sense violations in gameplay videos.  In this particular example (b), the questions ask about the physical commonsense violations shown in gameplay video clips. The answers from three video LLMs (PhysVLM, GPT-40, and LLaVA-Next-Video) are provided for comparison, highlighting differences in their abilities to detect and describe these violations.", "section": "3.1. PhysGame Benchmark"}, {"figure_path": "https://arxiv.org/html/2412.01800/x9.png", "caption": "Figure 5: Qualitative examples of open-ended questions.", "description": "Figure 5 presents two examples showcasing open-ended questioning in the PhysGame benchmark.  Each example displays a gameplay video clip with a physics glitch, followed by responses from three different video LLMs: PhysVLM, GPT-40, and LLaVA-Next-Video. The responses illustrate the varying capabilities of these models in identifying and describing the specific nature of the physical commonsense violations present in the video clips.  This highlights the nuanced challenges in evaluating physical reasoning within video LLMs.", "section": "3.1. PhysGame Benchmark"}, {"figure_path": "https://arxiv.org/html/2412.01800/x10.png", "caption": "(a)", "description": "The figure shows a comparison of three different Video LLMs' responses to a gameplay video glitch.  The video depicts a motorcycle colliding with a car, causing the car to flip unrealistically.  PhysVLM correctly identifies the physical commonsense violation, whereas GPT-4 and LLaVA-Next-Video fail to do so, highlighting the limitations of current video LLMs in understanding physics.", "section": "3.1. PhysGame Benchmark"}, {"figure_path": "https://arxiv.org/html/2412.01800/x11.png", "caption": "(b)", "description": "The figure shows qualitative examples of open-ended questions for evaluating video LLMs' understanding of physical commonsense.  It presents two videos and their corresponding answers from three different video LLMs: PhysVLM, GPT-40, and LLaVA-Next-Video. The responses highlight how each model interprets and explains the physical glitches or inconsistencies present in the gameplay videos. In (b), the video involves a character's transition from a dark area to a sunlit one, causing issues with shadow and lighting consistency. PhysVLM correctly points out the lighting inconsistencies, GPT-40 identifies a more generic game bug (the character is resetting to a previous position), and LLaVA-Next-Video highlights a jerky movement as the glitch.", "section": "3.1. PhysGame Benchmark"}]