[{"content": "| Benchmarks | #Videos | Len.(s) | #QA Pairs | QA Tokens | Anno. | Game-Bsd | Phys-Clsf | Meta-info |\n|---|---|---|---|---|---|---|---|---|\n| MSRVTT-QA [129] | 2,990 | 15.2 | 72,821 | 8.4 | A | \u2717 | \u2717 | \u2717 |\n| MSVD-QA [129] | 504 | 9.8 | 13,157 | 7.6 | A | \u2717 | \u2717 | \u2717 |\n| TGIF-QA [51] | 9,575 | 3.0 | 8,506 | 20.5 | A&M | \u2717 | \u2717 | \u2717 |\n| ActivityNet-QA [137] | 800 | 111.4 | 8,000 | 10.2 | M | \u2717 | \u2717 | \u2717 |\n| TVQA [56] | 2,179 | 11.2 | 15,253 | 27.8 | M | \u2717 | \u2717 | \u2713 |\n| How2QA [65] | 1,166 | 15.3 | 2,852 | 16.9 | M | \u2717 | \u2717 | \u2713 |\n| STAR [124] | 914 | 11.9 | 7,098 | 19.5 | A | \u2717 | \u2717 | \u2717 |\n| NExT-QA [128] | 1,000 | 39.5 | 8,564 | 25.3 | A | \u2717 | \u2717 | \u2717 |\n| MVBench [64] | 3,641 | 16.0 | 4,000 | 27.3 | A | \u2717 | \u2717 | \u2717 |\n| Video-Bench [91] | 5,917 | 56.0 | 17,036 | 21.3 | A&M | \u2717 | \u2717 | \u2717 |\n| EgoSchema [84] | 5,063 | 180.0 | 5,063 | 126.8 | A&M | \u2717 | \u2717 | \u2717 |\n| AutoEval-Video [27] | 327 | 14.6 | 327 | 11.9 | M | \u2717 | \u2717 | \u2717 |\n| TempCompass [79] | 410 | 11.4 | 7,540 | 49.2 | A&M | \u2717 | \u2717 | \u2717 |\n| Video-MME [38] | 900 | 1017.9 | 2,700 | 35.7 | M | \u2717 | \u2717 | \u2713 |\n| LVBench [121] | 103 | 4,101 | 1,549 | 32.0 | M | \u2717 | \u2717 | \u2717 |\n| LongVideoBench [125] | 3,763 | 473.0 | 6,678 | 84.1 | A&M | \u2717 | \u2717 | \u2717 |\n| PhysGame (Ours) | 880 | 25.9 | 880 | 66.9 | M | \u2713 | \u2713 | \u2713 |", "caption": "Table 1: Comparison with existing benchmarks for video LLMs in terms of the video number (#Videos), the average video duration (Len.), the number of QA pair (#QA Pairs), the average QA pair tokens (QA Tokens), the manually/automatic annotation manner (M/A), whether the benchmarks are gameplay video based (Game-Bsd), whether the questions are physical commonsense classified (Phys-Clsf), and whether the benchmarks contain meta information (Meta-info).", "description": "This table compares various video LLMs benchmarks across several key features.  It details the number of videos, average video length, the number of question-answer pairs, the average token count per pair, whether the annotation was manual or automatic, if the benchmark uses gameplay videos, if the questions assess physical commonsense, and if metadata is included.", "section": "2. Related Work"}, {"content": "| Benchmarks | Vid-Bsd | Instruct | MModal |\n|---|---|---|---| \n| GameBunny [107] | \u2717 | \u2713 | \u2713 |\n| Taesiri et.al [109] | \u2713 | \u2717 | \u2713 |\n| GameBugDescript [110] | \u2713 | \u2713 | \u2717 |\n| GlitchBench [111] | \u2717 | \u2713 | \u2713 |\n| **PhysGame (Ours)** | \u2713 | \u2713 | \u2713 |", "caption": "Table 2: Comparison with existing gameplay video benchmarks in terms of whether they are video-based (Vid-Bsd), whether they follow an instructional format (Instruct), and support multi-modal evaluations (MModal).", "description": "This table compares several existing benchmarks for evaluating video large language models (LLMs) specifically in the context of gameplay videos.  It focuses on three key aspects: whether the benchmark uses video data (Vid-Bsd), if the evaluation tasks are presented in an instructional format (Instruct), and if the benchmark supports the evaluation of multi-modal models (MModal). This allows for a clearer understanding of how these benchmarks differ in their approach and capabilities and the types of LLMs they are designed to assess.", "section": "2. Related Work"}, {"content": "|               | Opt. A | Opt. B | Opt. C | Opt. D |\n| :------------ | :----: | :----: | :----: | :----: |\n| Avg. tokens | 14.40  | 14.49  | 14.46  | 14.47  |", "caption": "Table 3: The average tokens of four options in the annotations of PhysGame benchmark.", "description": "This table presents the average number of tokens (words or sub-words) across the four answer choices for each multiple-choice question in the PhysGame benchmark.  It indicates the length of the distractor options relative to the correct option, helping to ensure the quality of the distractor options and mitigate any bias introduced by length differences.", "section": "3.1. PhysGame Benchmark"}, {"content": "## Table 1: Model Comparison\n\n| Models | Citation | AVG | Mechanics |  |  | Kinematics |  | Optics |  |  | Material |  |  |  |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|  |  | Grav. | Elast. | Fric. | Velo. | Acc. | Refl. | Refr. | Abs. | Col. | Rig. | Sha. | Gest. |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| <em style=\"color:#0000FF;\">Proprietary Multi-modal LLMs</em> |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| Claude3.5-Sonnet | [4] | 54.3 | **50.7** | 58.8 | 50.6 | **53.2** | 59.1 | **50.0** | 50.0 | 49.2 | 64.4 | 52.7 | 50.0 | **62.1** |\n| Claude3.5-SonnetV2 | [4] | 47.6 | 46.5 | 52.5 | 46.6 | 37.2 | 53.4 | 47.8 | 50.0 | 33.9 | 55.6 | 54.1 | 43.8 | 51.7 |\n| Gemini-1.5-pro | [114] | 55.2 | **50.7** | **70.0** | 48.9 | 51.1 | 59.1 | **50.0** | 42.9 | **52.5** | **71.1** | **56.8** | 53.1 | 58.6 |\n| Gemini-1.5-pro-flash | [114] | 48.5 | 47.9 | 52.5 | 51.7 | 43.6 | 51.1 | 43.5 | 53.6 | 33.9 | 64.4 | 43.2 | 46.9 | 49.4 |\n| GPT-4V | [1] | 45.9 | 40.8 | 60.0 | 48.3 | 34.0 | 48.9 | 43.5 | 46.4 | 42.4 | 53.3 | 45.9 | 37.5 | 44.8 |\n| GPT-4o-0806 | [92] | **56.1** | 47.9 | 61.3 | **59.1** | 43.6 | **61.4** | 43.5 | 53.6 | 50.8 | 68.9 | 54.1 | **65.6** | 63.2 |\n| GPT-4o-mini-0718 | [92] | 40.3 | 43.7 | 43.8 | 39.2 | 35.1 | 44.3 | 30.4 | 46.4 | 42.4 | 44.4 | 37.8 | 37.5 | 41.4 |\n| Qwen-VL-max | [6] | 50.9 | **50.7** | 53.8 | 51.1 | 31.9 | 46.6 | **50.0** | **60.7** | 50.8 | 64.4 | 48.6 | **65.6** | 59.8 |\n| <em style=\"color:#0000FF;\">Open-source Multi-modal LLMs</em> |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| LLaVA-Next-Video | [72] | 32.2 | 43.7 | 33.8 | 27.3 | 34.0 | 22.7 | 21.7 | 35.7 | 23.7 | 35.6 | 41.9 | 34.4 | 37.9 |\n| Video-LLaVA | [68] | 29.0 | 32.4 | 22.5 | 27.8 | 31.9 | 26.1 | 19.6 | 35.7 | 32.2 | 31.1 | 36.5 | 28.1 | 27.6 |\n| LLaVA-OneVision | [58] | 47.7 | 50.7 | 50.0 | 46.0 | 39.4 | 45.5 | 43.5 | **71.4** | **40.7** | 55.6 | 44.6 | **56.2** | 52.9 |\n| InternVL2 | [29] | 33.4 | 29.6 | 31.2 | 38.6 | 35.1 | 30.7 | 30.4 | 53.6 | 35.6 | 26.7 | 29.7 | 18.8 | 34.5 |\n| VideoChat2 | [64] | 34.3 | 33.8 | 35.0 | 29.5 | 41.5 | 28.4 | 28.3 | 32.1 | 33.9 | 33.3 | 41.9 | 21.9 | 44.8 |\n| ST-LLM | [77] | 32.8 | 32.4 | 26.2 | 26.7 | 37.2 | 28.4 | 37.0 | 25.0 | 28.8 | 33.3 | 40.5 | 37.5 | 46.0 |\n| Chat-UniVi | [54] | 29.5 | 28.2 | 27.5 | 29.5 | 39.4 | 23.9 | 28.3 | 32.1 | 30.5 | 31.1 | 18.9 | 28.1 | 35.6 |\n| PPLLaVA | [78] | 38.4 | 45.1 | 38.8 | 42.6 | 30.9 | 30.7 | 41.3 | 39.3 | 35.6 | 44.4 | 39.2 | 18.8 | 43.7 |\n| **PhysVLM-SFT** |  | 56.7 | 54.9 | 62.5 | **60.2** | 51.1 | **63.6** | **45.7** | 57.1 | 28.8 | **64.4** | 51.4 | 50.0 | 72.4 |\n| **PhysVLM-DPO** |  | **59.5** | **64.8** | **66.3** | **60.2** | **59.6** | 60.2 | 39.1 | 67.9 | 35.6 | 57.8 | **62.2** | 37.5 | **78.2** |", "caption": "Table 4: Evaluation results (%) of open-source and proprietary multi-modal LLMs on PhysGame. The fine-grained categories include gravity, elasticity, friction, velocity, acceleration, reflection, refraction, absorption & transmission, color, rigidity, object shape, and body gesture. AVG denotes the average accuracy. PhysVLM-SFT denotes PhysVLM only undergoes supervised fine-tuning while PhysVLM-DPO denotes PhysVLM with consecutive supervised fine-tuning and direct preference optimization.", "description": "Table 4 presents a detailed comparison of the performance of various open-source and proprietary Large Language Models (LLMs) on the PhysGame benchmark.  PhysGame assesses the ability of LLMs to identify and understand violations of physical common sense within gameplay videos. The table breaks down the results by several fine-grained subcategories of physics (gravity, elasticity, friction, velocity, acceleration, reflection, refraction, absorption & transmission, color, rigidity, object shape, and body gesture), providing a granular view of each model's strengths and weaknesses.  It also shows the overall average accuracy for each model and distinguishes between two versions of the PhysVLM model: one trained with supervised fine-tuning only (PhysVLM-SFT) and another trained with both supervised fine-tuning and direct preference optimization (PhysVLM-DPO). This allows for a direct comparison of the impact of the more advanced training technique on performance.", "section": "4.2. Evaluations on PhysGame"}, {"content": "| Models |  | LLM Params | Short (%) |  | Medium (%) |  | Long (%) |  | Overall (%) |  |\n|---|---|---|---|---|---|---|---|---|---|---|\n| InternVL-Chat-V1.5 | [29] | 20B | 60.2 | 61.7 | 46.4 | 49.1 | 45.6 | 46.6 | 50.7 | 52.4 |\n| LLaVA-NeXT-Video | [72] | 34B | 61.7 | 65.1 | 50.1 | 52.2 | 44.3 | 47.2 | 52.0 | 54.9 |\n| VILA-1.5 | [69] | 34B | 68.1 | 68.9 | 58.1 | 57.4 | 50.8 | 52.0 | 59.0 | 59.4 |\n| LLaVA-OneVision | [58] | 72B | 76.7 | 79.3 | 62.2 | 66.9 | 60.0 | 62.4 | 66.3 | 69.6 |\n| Qwen-VL-Chat | [6] | 7B | 46.9 | 47.3 | 38.7 | 40.4 | 37.8 | 37.9 | 41.1 | 41.9 |\n| Video-LLaVA | [68] | 7B | 45.3 | 46.1 | 38.0 | 40.7 | 36.2 | 38.1 | 39.9 | 41.6 |\n| ST-LLM | [76] | 7B | 45.7 | 48.4 | 36.8 | 41.4 | 31.3 | 36.9 | 37.9 | 42.3 |\n| VideoChat2-Mistral | [64] | 7B | 48.3 | 52.8 | 37.0 | 39.4 | 33.2 | 39.2 | 39.5 | 43.8 |\n| Chat-UniVi-V1.5 | [54] | 7B | 45.7 | 51.2 | 40.3 | 44.6 | 35.8 | 41.8 | 40.6 | 45.9 |\n| LLaVA-NeXT-Video | [72] | 7B | 45.9 | 49.8 | 40.3 | 44.3 | 36.6 | 41.0 | 40.9 | 45.0 |\n| PPLLaVA | [78] | 7B | 58.7 | 62.8 | 45.6 | 50.4 | 42.2 | 47.4 | 48.8 | 53.6 |\n| **PhysVLM-SFT** |  | 7B | 64.1 | 68.0 | **55.0** | **61.7** | 46.4 | 50.3 | 55.2 | 60.0 |\n| **PhysVLM-DPO** |  | 7B | **66.1** | **70.0** | 54.3 | 59.6 | **47.1** | **53.8** | **55.8** | **61.1** |", "caption": "Table 5: Evaluation results (%) on Video-MME. \u201cw/ subs\u201d and \u201cw/o subs\u201d respectively denote \u201cwith subtitles\u201d and \u201cwithout subtitles\u201d.", "description": "This table presents the performance comparison of various Large Language Models (LLMs) on the Video Multimodal Entailment (Video-MME) benchmark.  The benchmark assesses the ability of LLMs to understand and reason about video content. The table shows the performance scores (in percentages) for each LLM, categorized by video length (short, medium, long), and whether subtitles were used.  Higher percentages indicate better performance.  The results are broken down into \"with subtitles\" and \"without subtitles\" to show the impact of textual information on the models' video comprehension abilities.", "section": "4.2. Evaluations on PhysGame"}, {"content": "| Methods | CI | DO | CU | TU | CO | AVG |\n|---|---|---|---|---|---|---|\n| VideoChat | 2.23 | 2.50 | 2.53 | 1.94 | 2.24 | 2.29 |\n| Video-ChatGPT | 2.50 | 2.57 | 2.69 | 2.16 | 2.20 | 2.42 |\n| BT-Adapter | 2.68 | 2.69 | 3.27 | 2.34 | 2.46 | 2.69 |\n| Chat-UniVi | 2.89 | 2.91 | 3.46 | 2.89 | 2.81 | 2.99 |\n| VideoChat2 | 3.02 | 2.88 | 3.51 | 2.66 | 2.81 | 2.98 |\n| LLaMA-VID | 2.96 | 3.00 | 3.53 | 2.46 | 2.51 | 2.89 |\n| ST-LLM | 3.23 | 3.05 | 3.74 | 2.93 | 2.81 | 3.15 |\n| PLLaVA | 3.21 | 2.86 | 3.62 | 2.33 | 2.93 | 2.99 |\n| LLaVA-Next-Video | 3.39 | 3.29 | 3.92 | 2.60 | 3.12 | 3.26 |\n| PPLLaVA | 3.32 | 3.20 | 3.88 | 3.00 | 3.20 | 3.32 |\n| **PhysVLM-SFT** | **3.59** | 3.07 | 3.89 | 2.74 | **3.44** | **3.35** |\n| LLaVA-Next-Video* | 3.64 | 3.45 | 4.17 | 2.95 | 4.08 | 3.66 |\n| PPLLaVA* | 3.85 | 3.56 | 4.21 | 3.21 | 3.81 | 3.73 |\n| **PhysVLM-DPO*** | **3.89** | **3.69** | **4.26** | 3.11 | **4.19** | **3.83** |", "caption": "Table 6: Evaluation results on VCG benchmark [83]. Methods marked by \u2217 use DPO or PPO [104]. CI, DO, CU, TU, and CO respectively denote correctness of information, detail orientation, contextual understanding, temporal understanding, and consistency. AVG is the average result.", "description": "Table 6 presents a comprehensive evaluation of various video LLMs on the VCG benchmark [83], focusing on several key aspects of video understanding.  The benchmark assesses the models' capabilities across five dimensions: Correctness of Information (CI), Detail Orientation (DO), Contextual Understanding (CU), Temporal Understanding (TU), and Consistency (CO).  The table shows the individual scores for each model and metric, along with an overall average (AVG) score.  The models marked with an asterisk (*) utilize either Direct Preference Optimization (DPO) or Proximal Policy Optimization (PPO) [104], which are advanced training techniques aimed at improving model performance. This allows for comparison of models trained using traditional methods versus those employing more advanced techniques.", "section": "4.3 Evaluations of General Video Understanding"}, {"content": "| Methods | AVG |\n|---|---| \n| PhysVLM-DPO | 59.5 |\n| _w/o_ temporal hacking | 57.6 |\n| _w/o_ spatial hacking | 57.3 |\n| _w/o_ meta-info hacking | 57.4 |", "caption": "Table 8: Ablations of training data in SFT and DPO stages. AVG denotes the average accuracy on the PhysGame benchmark.", "description": "This table presents the results of ablation studies on the training data used for the PhysVLM model. It shows the impact of different training datasets on the model's performance, measured by average accuracy on the PhysGame benchmark. Specifically, it compares the performance when using only LLaVA-Hound data, LLaVA-Hound and LLaVA-Image data, and the full dataset including PhysInstruct.  The impact of using only LLaVA-Hound-DPO and the full dataset including PhysDPO is also analyzed in the DPO stage. This table helps to understand the contribution of each dataset to the overall model performance.", "section": "4. Experiments"}, {"content": "| Stage | Training Data | AVG |\n|---|---|---|\n| SFT | LLava-Hound | 40.7 |\n| SFT | LLava-Hound [142], LLaVA-Image [73] | 46.0 |\n| SFT | LLava-Hound, LLaVA-Image, **PhysInstruct** | **56.7** |\n| DPO | LLava-Hound-DPO [142] | 52.9 |\n| DPO | LLava-Hound-DPO, **PhysDPO** | **59.5** |", "caption": "Table 9: Hyper-parameter ablations of (a) the sampled frame number N\ud835\udc41Nitalic_N in temporal hacking and (b) the frame resolution scale factor \u03b3\ud835\udefe\\gammaitalic_\u03b3 in spatial hacking for PhysDPO construction.", "description": "This table presents the results of ablation studies conducted to analyze the impact of hyperparameters used in generating the PhysDPO dataset.  Specifically, it examines how variations in the number of sampled frames (N) during temporal hacking and the frame resolution scale factor (\u03b3) during spatial hacking affect the overall performance. The table helps determine the optimal settings for these hyperparameters to ensure the effectiveness of the PhysDPO dataset in improving the model's understanding of physical commonsense.", "section": "3. Dataset & Method"}, {"content": "| N | 1 | 2 | 4 |\n|---|---|---|---| \n| **AVG** | **59.5** | 58.1 | 57.8 |", "caption": "Table 10: Ablations on LLMs in PhysVLM with Vicuna-7B [30] or Qwen2-7B [131].", "description": "This table presents the ablation study results comparing the performance of PhysVLM when using either Vicuna-7B or Qwen-2-7B as the underlying large language model.  It shows the average accuracy and the performance across different fine-grained categories within four main physical domains (Mechanics, Kinematics, Optics, and Material) for both supervised fine-tuning (SFT) and direct preference optimization (DPO) stages.  This allows for a detailed assessment of the impact of the LLM choice on the model's ability to understand physical common sense.", "section": "4. Experiments"}, {"content": "| \n | 1/8 | 1/16 | 1/32 |\n|---|---|---|---|\n|\u03b3 | 57.1 | **59.5** | 58.6 |", "caption": "Table 11: Ablations on training data on VCG benchmark.", "description": "This table presents ablation study results on the VCG benchmark, evaluating the impact of different training data combinations on the model's performance.  It shows the average scores and individual scores across five sub-categories (correctness of information, detail orientation, contextual understanding, temporal understanding, and consistency) for various training data setups.  The setups include training with only LLaVA-Hound data, adding LLaVA-Image data, further adding the PhysInstruct dataset (for supervised fine-tuning), adding LLaVA-Hound-DPO data (for direct preference optimization), and finally adding both the PhysInstruct and PhysDPO datasets.", "section": "4.3. Evaluations of General Video Understanding"}, {"content": "| Stage | LLMs | AVG | Mechanics | Mechanics | Mechanics | Kinematics | Kinematics | Optics | Optics | Optics | Material | Material | Material | Material |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| |  |  | Grav. | Elast. | Fric. | Velo. | Acc. | Refl. | Refr. | Abs. | Col. | Rig. | Sha. | Gest. |\n| **SFT** | Vicuna | 44.7 | 47.9 | 45.0 | 48.9 | **52.1** | 48.9 | 30.4 | 42.9 | **28.8** | 28.9 | 50.0 | 31.2 | 48.3 |\n| **SFT** | Qwen-2 | **56.7** | **54.9** | **62.5** | **60.2** | 51.1 | **63.6** | **45.7** | **57.1** | **28.8** | **64.4** | **51.4** | **50.0** | **72.4** |\n| **DPO** | Vicuna | 48.2 | 56.3 | 52.5 | 50.6 | **59.6** | 48.9 | 28.3 | 35.7 | 28.8 | 31.1 | 47.3 | **37.5** | 60.9 |\n| **DPO** | Qwen-2 | **59.5** | **64.8** | **66.3** | **60.2** | **59.6** | **60.2** | **39.1** | **67.9** | **35.6** | **57.8** | **62.2** | **37.5** | **78.2** |", "caption": "Table 12: Ablations on training data on Video-MME benchmark.", "description": "This table presents the ablation study results on the Video-MME benchmark, showing the impact of different training data combinations on the model's performance. It breaks down the results by video length (short, medium, long) and indicates whether subtitles were used. The table helps to understand the contribution of each dataset to the overall performance of the model on Video-MME.", "section": "4. Experiments"}, {"content": "| Stage | Training Data | CI | DO | CU | TU | CO | AVG |\n|---|---|---|---|---|---|---|---| \n| **SFT** | LLava-Hound | 3.48 | 2.88 | 3.74 | 2.58 | 3.02 | 3.14 |\n| **SFT** | LLava-Hound, LLaVA-Image | 3.43 | 2.99 | 3.73 | 2.56 | 3.12 | 3.17 |\n| **SFT** | LLava-Hound, LLaVA-Image, PhysInstruct | **3.59** | **3.07** | **3.89** | **2.74** | **3.44** | **3.35** |\n| **DPO** | LLava-Hound-DPO | 3.94 | 3.43 | 4.25 | **3.12** | 4.05 | 3.76 |\n| **DPO** | LLava-Hound-DPO, PhysDPO | **3.89** | **3.69** | **4.26** | 3.11 | **4.19** | **3.83** |", "caption": "Table 13: Prompt for instruction-tuning data generation in PhysInstruct.", "description": "This table details the prompt template used to generate the instruction-tuning dataset, PhysInstruct.  The prompt instructs an AI to act as a visual assistant, analyzing a video and its title (which may or may not be accurate). The AI should identify and describe any violations of physics in the video, creating a conversational exchange between the AI and a user. The AI is explicitly told to base its analysis on its own observations and understanding of the video, not relying on the accuracy of the provided title.  All descriptions must be at the video level, not referencing individual images or frames.", "section": "3. Dataset & Method"}, {"content": "| Models | Training Data | Short (%) _w/o_ subs | Short (%) _w/_ subs | Medium (%) _w/o_ subs | Medium (%) _w/_ subs | Long (%) _w/o_ subs | Long (%) _w/_ subs | Overall (%) _w/o_ subs | Overall (%) _w/_ subs |\n|---|---|---|---|---|---|---|---|---|---|\n| SFT | LLava-Hound | 65.6 | 68.9 | 55.3 | 60.4 | 47.7 | 52.4 | 56.2 | 60.6 |\n| SFT | LLava-Hound, LLaVA-Image | 65.2 | 68.3 | 54.9 | 60.2 | 47.6 | 52.8 | 55.9 | 60.4 |\n| SFT | LLava-Hound, LLaVA-Image, PhysInstruct | 64.1 | 68.0 | 55.0 | 61.7 | 46.4 | 50.3 | 55.2 | 60.0 |\n| DPO | LLava-Hound-DPO | 66.0 | 70.2 | 53.6 | 60.5 | 47.3 | 52.8 | 55.6 | 61.2 |\n| DPO | LLava-Hound-DPO, PhysDPO | 66.1 | 70.0 | 54.3 | 59.6 | 47.1 | 53.8 | 55.8 | 61.1 |", "caption": "Table 14: Prompt for response generation in PhysDPO. The false_title is randomly selected from the other videos and the question is instantiated by the same instruction in PhysInstruct.", "description": "This table presents the prompt used for generating responses in the PhysDPO dataset.  PhysDPO uses a technique called 'direct preference optimization' where it needs both preferred and dispreferred responses for training.  To create the dispreferred responses, misleading information is given. Specifically, a false title is randomly selected from other videos in the dataset, and then this misleading title is combined with the question from the PhysInstruct dataset.  This table shows exactly the structure of the prompt given to the model in this process, to create the less desirable answers.", "section": "3. Dataset & Method"}]