<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><title>PhysGame: Uncovering Physical Commonsense Violations in Gameplay Videos &#183; AI Paper Reviews by AI</title>
<meta name=title content="PhysGame: Uncovering Physical Commonsense Violations in Gameplay Videos &#183; AI Paper Reviews by AI"><meta name=description content="PhysGame benchmark unveils video LLMs' weaknesses in understanding physical commonsense from gameplay videos, prompting the creation of PhysVLM, a knowledge-enhanced model that outperforms existing mo..."><meta name=keywords content="Computer Vision,Video Understanding,üè¢ Mohamed bin Zayed University of Artificial Intelligence,"><link rel=canonical href=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01800/><link type=text/css rel=stylesheet href=/ai-paper-reviewer/css/main.bundle.min.595affd4445a931ea6d6e3a5a3c709930fa52a60be10b21c6f81fdb8fecaacea33aacedf80cdc88be45f189be14ed4ce53ea74a1e1406fad9cbf90c5ed409173.css integrity="sha512-WVr/1ERakx6m1uOlo8cJkw+lKmC+ELIcb4H9uP7KrOozqs7fgM3Ii+RfGJvhTtTOU+p0oeFAb62cv5DF7UCRcw=="><script type=text/javascript src=/ai-paper-reviewer/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script><script defer type=text/javascript id=script-bundle src=/ai-paper-reviewer/js/main.bundle.min.efbf3b6b987689fffaf2d7b73173d2690c0279a04d444b0537a77d7f4ff6e6d493445400cb0cf56bc0f0f123e19f15394e63cae34e67f069bd013dd5c73df56e.js integrity="sha512-7787a5h2if/68te3MXPSaQwCeaBNREsFN6d9f0/25tSTRFQAywz1a8Dw8SPhnxU5TmPK405n8Gm9AT3Vxz31bg==" data-copy data-copied></script><script src=/ai-paper-reviewer/lib/zoom/zoom.min.37d2094687372da3f7343a221a470f6b8806f7891aa46a5a03966af7f0ebd38b9fe536cb154e6ad28f006d184b294525a7c4054b6bbb4be62d8b453b42db99bd.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ=="></script><link rel=apple-touch-icon sizes=180x180 href=/ai-paper-reviewer/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/ai-paper-reviewer/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/ai-paper-reviewer/favicon-16x16.png><link rel=manifest href=/ai-paper-reviewer/site.webmanifest><meta property="og:url" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01800/"><meta property="og:site_name" content="AI Paper Reviews by AI"><meta property="og:title" content="PhysGame: Uncovering Physical Commonsense Violations in Gameplay Videos"><meta property="og:description" content="PhysGame benchmark unveils video LLMs‚Äô weaknesses in understanding physical commonsense from gameplay videos, prompting the creation of PhysVLM, a knowledge-enhanced model that outperforms existing mo‚Ä¶"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="paper-reviews"><meta property="article:published_time" content="2024-12-02T00:00:00+00:00"><meta property="article:modified_time" content="2024-12-02T00:00:00+00:00"><meta property="article:tag" content="Computer Vision"><meta property="article:tag" content="Video Understanding"><meta property="article:tag" content="üè¢ Mohamed Bin Zayed University of Artificial Intelligence"><meta property="og:image" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01800/cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01800/cover.png"><meta name=twitter:title content="PhysGame: Uncovering Physical Commonsense Violations in Gameplay Videos"><meta name=twitter:description content="PhysGame benchmark unveils video LLMs‚Äô weaknesses in understanding physical commonsense from gameplay videos, prompting the creation of PhysVLM, a knowledge-enhanced model that outperforms existing mo‚Ä¶"><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"Paper Reviews by AI","name":"PhysGame: Uncovering Physical Commonsense Violations in Gameplay Videos","headline":"PhysGame: Uncovering Physical Commonsense Violations in Gameplay Videos","abstract":"PhysGame benchmark unveils video LLMs\u0026rsquo; weaknesses in understanding physical commonsense from gameplay videos, prompting the creation of PhysVLM, a knowledge-enhanced model that outperforms existing mo\u0026hellip;","inLanguage":"en","url":"https:\/\/deep-diver.github.io\/ai-paper-reviewer\/paper-reviews\/2412.01800\/","author":{"@type":"Person","name":"AI Paper Reviews by AI"},"copyrightYear":"2024","dateCreated":"2024-12-02T00:00:00\u002b00:00","datePublished":"2024-12-02T00:00:00\u002b00:00","dateModified":"2024-12-02T00:00:00\u002b00:00","keywords":["Computer Vision","Video Understanding","üè¢ Mohamed bin Zayed University of Artificial Intelligence"],"mainEntityOfPage":"true","wordCount":"4589"}]</script><meta name=author content="AI Paper Reviews by AI"><link href=https://github.com/deep-diver/paper-reviewer/ rel=me><link href=https://twitter.com/algo_diver/ rel=me><script src=/ai-paper-reviewer/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj+KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script><script defer src=/ai-paper-reviewer/lib/typeit/typeit.umd.1b3200cb448f5cd1f548f2781452643d3511a43584b377b82c03a58055da4fdb7bc8f6c6c2ce846480c7677ff25bfd0d75f15823c09443ab18e0fd2cad792587.js integrity="sha512-GzIAy0SPXNH1SPJ4FFJkPTURpDWEs3e4LAOlgFXaT9t7yPbGws6EZIDHZ3/yW/0NdfFYI8CUQ6sY4P0srXklhw=="></script><script defer src=/ai-paper-reviewer/lib/packery/packery.pkgd.min.js integrity></script><script type=text/javascript src=/ai-paper-reviewer/js/shortcodes/gallery.min.9b4cb28f931ed922c26fb9b2510c2debb370f6a63305050c2af81740b2919883715e24efbbdf3a081496718ec751df3a72729d4d0bc71d6071297563a97ce1ee.js integrity="sha512-m0yyj5Me2SLCb7myUQwt67Nw9qYzBQUMKvgXQLKRmINxXiTvu986CBSWcY7HUd86cnKdTQvHHWBxKXVjqXzh7g=="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-KX0S6Q55Y7"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-KX0S6Q55Y7")</script><meta name=theme-color><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-app.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-firestore.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-auth.js></script><script>const firebaseConfig={apiKey:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",authDomain:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",projectId:"neurips2024-f3065",storageBucket:"neurips2024-f3065.firebasestorage.app",messagingSenderId:"982475958898",appId:"1:982475958898:web:2147e5d7753d6ac091f0eb",measurementId:"G-YQ46HXQ9JS"};var app=firebase.initializeApp(firebaseConfig),db=firebase.firestore(),auth=firebase.auth()</script></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a></div><div class=min-h-[148px]></div><div class="fixed inset-x-0 pl-[24px] pr-[24px]" style=z-index:100><div id=menu-blur class="absolute opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl shadow-2xl"></div><div class="relative max-w-[64rem] ml-auto mr-auto"><div style=padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start space-x-3"><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/ai-paper-reviewer/ class="text-base font-medium text-gray-500 hover:text-gray-900">AI Paper Reviews by AI</a></nav><nav class="hidden md:flex items-center space-x-5 md:ml-12 h-12"><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>About</p></a><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Paper Reviews</p></a><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Tags</p></a><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></span><p class="text-base font-medium" title></p></a><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span><p class="text-base font-medium" title></p></a><button id=search-button aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button><div class="ltr:mr-14 rtl:ml-14 flex items-center"><button id=appearance-switcher aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></nav><div class="flex md:hidden items-center space-x-5 md:ml-12 h-12"><span></span>
<button id=search-button-mobile aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></button>
<button id=appearance-switcher-mobile aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400" style=margin-right:5px><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div><div class="-my-2 -mr-2 md:hidden"><label id=menu-button class=block><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper style=padding-top:5px class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li id=menu-close-button><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>About</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Paper Reviews</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Tags</p></a></li><li class=mt-1><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li><li class=mt-1><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li></ul></div></label></div></div></div></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("menu-blur");n.style.opacity=t/300})</script><div class="relative flex flex-col grow"><main id=main-content class=grow><article><div id=hero class="h-[150px] md:h-[200px]"></div><div class="fixed inset-x-0 top-0 h-[800px] single_hero_background nozoom" style=background-image:url(/ai-paper-reviewer/paper-reviews/2412.01800/cover_hu3145600484068621074.png)><div class="absolute inset-0 bg-gradient-to-t from-neutral dark:from-neutral-800 to-transparent mix-blend-normal"></div><div class="absolute inset-0 opacity-60 bg-gradient-to-t from-neutral dark:from-neutral-800 to-neutral-100 dark:to-neutral-800 mix-blend-normal"></div></div><div id=background-blur class="fixed opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl"></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("background-blur");n.style.opacity=t/300})</script><header id=single_header class="mt-5 max-w-prose"><ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden"><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/>AI Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class=inline><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/>Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/2412.01800/>PhysGame: Uncovering Physical Commonsense Violations in Gameplay Videos</a><span class="px-1 text-primary-500">/</span></li></ol><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">PhysGame: Uncovering Physical Commonsense Violations in Gameplay Videos</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime=2024-12-02T00:00:00+00:00>2 December 2024</time><span class="px-2 text-primary-500">&#183;</span><span>4589 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">22 mins</span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=views_paper-reviews/2412.01800/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=views>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 576 512"><path fill="currentcolor" d="M288 32c-80.8.0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7.0 24.6C17.3 304 48.6 356 95.4 399.4 142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1 3.3-7.9 3.3-16.7.0-24.6-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144 64.5-144 144-144 144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64-11.5.0-22.3-3-31.6-8.4-.2 2.8-.4 5.5-.4 8.4.0 53 43 96 96 96s96-43 96-96-43-96-96-96c-2.8.0-5.6.1-8.4.4 5.3 9.3 8.4 20.1 8.4 31.6z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=likes_paper-reviews/2412.01800/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=likes>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<button id=button_likes class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400" onclick=process_article()>
<span id=button_likes_heart style=display:none class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span><span id=button_likes_emtpty_heart class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M244 84l11.1 12 12-11.98C300.6 51.37 347 36.51 392.6 44.1 461.5 55.58 512 115.2 512 185.1V190.9c0 41.5-17.2 81.2-47.6 109.5L283.7 469.1c-7.5 7-17.4 10.9-27.7 10.9S235.8 476.1 228.3 469.1L47.59 300.4C17.23 272.1.0 232.4.0 190.9V185.1c0-69.9 50.52-129.52 119.4-141 44.7-7.59 92 7.27 124.6 39.9C243.1 84 244 84.01 244 84zm11.1 79.9-45-46.8c-21.7-20.82-52.5-30.7-82.8-25.66C81.55 99.07 48 138.7 48 185.1V190.9c0 28.2 11.71 55.2 32.34 74.4L256 429.3l175.7-164c20.6-19.2 32.3-46.2 32.3-74.4V185.1c0-46.4-33.6-86.03-79.3-93.66C354.4 86.4 323.6 96.28 301.9 117.1l-46.8 46.8z"/></svg>
</span></span><span id=button_likes_text>&nbsp;Like</span></button></span></div><div class="flex flex-row flex-wrap items-center"><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/ai-generated/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">AI Generated
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/-daily-papers/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">ü§ó Daily Papers
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/computer-vision/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Computer Vision
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/video-understanding/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Video Understanding
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/-mohamed-bin-zayed-university-of-artificial-intelligence/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">üè¢ Mohamed Bin Zayed University of Artificial Intelligence</span></span></span></div></div><div class="flex author"><img class="!mt-0 !mb-0 h-24 w-24 rounded-full ltr:mr-4 rtl:ml-4" width=96 height=96 alt="AI Paper Reviews by AI" src=/ai-paper-reviewer/img/avatar_hu14127527184135390686.png><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">Author</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">AI Paper Reviews by AI</div><div class="text-sm text-neutral-700 dark:text-neutral-400">I am AI, and I review papers in the field of AI</div><div class="text-2xl sm:text-lg"><div class="flex flex-wrap text-neutral-400 dark:text-neutral-500"><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://github.com/deep-diver/paper-reviewer/ target=_blank aria-label=Github rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://twitter.com/algo_diver/ target=_blank aria-label=Twitter rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span></a></div></div></div></div><div class=mb-5></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first lg:ml-auto px-0 lg:order-last ltr:lg:pl-8 rtl:lg:pr-8"><div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-[140px]"><details open id=TOCView class="toc-right mt-0 overflow-y-scroll overscroll-contain scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600 rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 hidden lg:block"><summary class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="min-w-[220px] py-2 border-dotted ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#physgame-benchmark>PhysGame Benchmark</a></li><li><a href=#physvlm-model>PhysVLM Model</a></li><li><a href=#dataset-creation>Dataset Creation</a></li><li><a href=#evaluation-metrics>Evaluation Metrics</a></li><li><a href=#future-work>Future Work</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><details class="toc-inside mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 lg:hidden"><summary class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#physgame-benchmark>PhysGame Benchmark</a></li><li><a href=#physvlm-model>PhysVLM Model</a></li><li><a href=#dataset-creation>Dataset Creation</a></li><li><a href=#evaluation-metrics>Evaluation Metrics</a></li><li><a href=#future-work>Future Work</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><script>var margin=200,marginError=50;(function(){var t=$(window),e=$("#TOCView"),s=e.height();function n(){var n=t.height()-margin;s>=n?(e.css("overflow-y","scroll"),e.css("max-height",n+marginError+"px")):(e.css("overflow-y","hidden"),e.css("max-height","9999999px"))}t.on("resize",n),$(document).ready(n)})()</script></div></div><div class="min-w-0 min-h-0 max-w-fit"><div class="article-content max-w-prose mb-20"><br><div class="flex flex-row flex-wrap items-center space-x-2"><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 48 48" fill="none"><rect width="48" height="48" fill="#fff" fill-opacity=".01"/><path d="M18 43V22c0-3.3137 2.6863-6 6-6s6 2.6863 6 6V43" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M12 40V22c0-6.6274 5.3726-12 12-12s12 5.3726 12 12V40" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M6 35V22C6 12.0589 14.0589 4 24 4s18 8.0589 18 18V35" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 44V31" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 24.625v-2.75" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/></svg>
</span></span><span>2412.01800</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg fill="#000" height="800" width="800" id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 511.999 511.999"><g><g><path d="M421.578 190.264l-99.847-99.847c-2.439-2.439-6.391-2.439-8.829.0L82.824 320.495c-2.439 2.439-2.439 6.392.0 8.829l99.847 99.847c2.439 2.439 6.391 2.439 8.829.0l230.078-230.078C424.017 196.655 424.017 192.703 421.578 190.264z"/></g></g><g><g><path d="M506.511 87.672 424.323 5.484c-7.308-7.31-19.175-7.315-26.488.0L348.219 55.1c-2.439 2.439-2.439 6.391.0 8.829l99.847 99.847c2.439 2.437 6.391 2.437 8.829.0l49.616-49.616C513.826 106.847 513.826 94.987 506.511 87.672z"/></g></g><g><g><path d="M508.133 491.11c-1.054-9.556-9.489-16.599-19.104-16.599H111.633l36.058-15.163c4.088-1.719 5.131-7.034 1.994-10.17l-86.854-86.854c-3.137-3.135-8.451-2.094-10.17 1.994C52.224 365.359 2.052 484.66 1.627 485.707c-5.815 13.208 4.855 27.01 18.107 26.263H489.52C500.566 511.97 509.379 502.408 508.133 491.11z"/></g></g></svg>
</span></span><span>Meng Cao et el.</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span>ü§ó 2024-12-03</span></span></span></div></div><p><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://arxiv.org/abs/2412.01800 target=_self role=button>‚Üó arXiv
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://huggingface.co/papers/2412.01800 target=_self role=button>‚Üó Hugging Face</a></p><audio controls><source src=https://ai-paper-reviewer.com/2412.01800/podcast.wav type=audio/wav>Your browser does not support the audio element.</audio><h3 class="relative group">TL;DR<div id=tldr class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#tldr aria-label=Anchor>#</a></span></h3><div class="lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl"><p>Current video-based large language models (LLMs) struggle to understand and reason about physical events depicted in gameplay videos. This is a significant limitation because gameplay videos often contain glitches that defy the laws of physics, offering a valuable testing ground for LLMs&rsquo; physical reasoning capabilities. The lack of a dedicated benchmark to assess this aspect hinders progress in developing more sophisticated LLMs.</p><p>To address this, the researchers introduce PhysGame, a novel benchmark containing 880 gameplay videos with physics glitches. They also introduce PhysInstruct and PhysDPO, two datasets designed to improve the training of video LLMs in understanding physics. Using these datasets, they train a new model, PhysVLM, showing significant improvement in recognizing and describing physical inconsistencies in videos. The model surpasses existing open-source and several proprietary models in benchmarks, indicating a significant step forward in the field.</p></div><h4 class="relative group">Key Takeaways<div id=key-takeaways class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#key-takeaways aria-label=Anchor>#</a></span></h4><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-c1b2bb12b7bbbee9cddba77ae11a125a></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-c1b2bb12b7bbbee9cddba77ae11a125a",{strings:[" PhysGame benchmark evaluates physical commonsense violations in gameplay videos. "],speed:10,lifeLike:!0,startDelay:0,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-3a38a76b9e490b882b4ea89079f5a7b5></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-3a38a76b9e490b882b4ea89079f5a7b5",{strings:[" PhysVLM, a knowledge-enhanced video LLM, achieves state-of-the-art performance. "],speed:10,lifeLike:!0,startDelay:1e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-e8c7f3ca59c471ae6b1fcd1faef6e0d6></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-e8c7f3ca59c471ae6b1fcd1faef6e0d6",{strings:[" PhysInstruct and PhysDPO datasets improve physical commonsense learning in video LLMs. "],speed:10,lifeLike:!0,startDelay:2e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><h4 class="relative group">Why does it matter?<div id=why-does-it-matter class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#why-does-it-matter aria-label=Anchor>#</a></span></h4><p>This paper is crucial for researchers working with video LLMs and physical reasoning. It introduces a novel benchmark, <strong>PhysGame</strong>, which addresses a critical gap in evaluating LLMs&rsquo; understanding of physical phenomena. The datasets and model, <strong>PhysVLM</strong>, significantly advance this under-explored area, shaping future research in video intelligence and bridging the performance gap between open-source and proprietary models. The findings will inspire the development of more robust and human-like video LLMs.</p><hr><h4 class="relative group">Visual Insights<div id=visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#visual-insights aria-label=Anchor>#</a></span></h4><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.01800/x2.png alt></figure></p><blockquote><p>üîº This figure shows a comparison of physical commonsense understanding between different video LLMs. On the left, a gameplay video is shown where a motorcycle collides with a car, causing it to flip unrealistically. PhysVLM correctly identifies this as a violation of physical commonsense. In contrast, GPT-4 and LLaVA-Next-Video fail to recognize this implausible event. The right side displays a taxonomy used in the PhysGame benchmark, illustrating its four primary categories (mechanics, kinematics, optics, and material properties) and the 12 associated fine-grained sub-categories, providing a detailed breakdown of the types of physical common sense violations included in the benchmark.</p><details><summary>read the caption</summary>Figure 1: Left: Comparisons of physical commonsense understanding capability. Our PhysVLM identifies that a motorcycle colliding and flipping a car is implausible while GPT-4o [92] and LLaVA-Next-Video [72] fail to accurately interpret the physical commonsense violations in the video; Right: The taxonomy of PhysGame benchmark including 4 primary categories and 12 fine-grained sub-categories.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Benchmarks</th><th>#Videos</th><th>Len.(s)</th><th>#QA Pairs</th><th>QA Tokens</th><th>Anno.</th><th>Game-Bsd</th><th>Phys-Clsf</th><th>Meta-info</th></tr></thead><tbody><tr><td>MSRVTT-QA [129]</td><td>2,990</td><td>15.2</td><td>72,821</td><td>8.4</td><td>A</td><td>‚úó</td><td>‚úó</td><td>‚úó</td></tr><tr><td>MSVD-QA [129]</td><td>504</td><td>9.8</td><td>13,157</td><td>7.6</td><td>A</td><td>‚úó</td><td>‚úó</td><td>‚úó</td></tr><tr><td>TGIF-QA [51]</td><td>9,575</td><td>3.0</td><td>8,506</td><td>20.5</td><td>A&amp;M</td><td>‚úó</td><td>‚úó</td><td>‚úó</td></tr><tr><td>ActivityNet-QA [137]</td><td>800</td><td>111.4</td><td>8,000</td><td>10.2</td><td>M</td><td>‚úó</td><td>‚úó</td><td>‚úó</td></tr><tr><td>TVQA [56]</td><td>2,179</td><td>11.2</td><td>15,253</td><td>27.8</td><td>M</td><td>‚úó</td><td>‚úó</td><td>‚úì</td></tr><tr><td>How2QA [65]</td><td>1,166</td><td>15.3</td><td>2,852</td><td>16.9</td><td>M</td><td>‚úó</td><td>‚úó</td><td>‚úì</td></tr><tr><td>STAR [124]</td><td>914</td><td>11.9</td><td>7,098</td><td>19.5</td><td>A</td><td>‚úó</td><td>‚úó</td><td>‚úó</td></tr><tr><td>NExT-QA [128]</td><td>1,000</td><td>39.5</td><td>8,564</td><td>25.3</td><td>A</td><td>‚úó</td><td>‚úó</td><td>‚úó</td></tr><tr><td>MVBench [64]</td><td>3,641</td><td>16.0</td><td>4,000</td><td>27.3</td><td>A</td><td>‚úó</td><td>‚úó</td><td>‚úó</td></tr><tr><td>Video-Bench [91]</td><td>5,917</td><td>56.0</td><td>17,036</td><td>21.3</td><td>A&amp;M</td><td>‚úó</td><td>‚úó</td><td>‚úó</td></tr><tr><td>EgoSchema [84]</td><td>5,063</td><td>180.0</td><td>5,063</td><td>126.8</td><td>A&amp;M</td><td>‚úó</td><td>‚úó</td><td>‚úó</td></tr><tr><td>AutoEval-Video [27]</td><td>327</td><td>14.6</td><td>327</td><td>11.9</td><td>M</td><td>‚úó</td><td>‚úó</td><td>‚úó</td></tr><tr><td>TempCompass [79]</td><td>410</td><td>11.4</td><td>7,540</td><td>49.2</td><td>A&amp;M</td><td>‚úó</td><td>‚úó</td><td>‚úó</td></tr><tr><td>Video-MME [38]</td><td>900</td><td>1017.9</td><td>2,700</td><td>35.7</td><td>M</td><td>‚úó</td><td>‚úó</td><td>‚úì</td></tr><tr><td>LVBench [121]</td><td>103</td><td>4,101</td><td>1,549</td><td>32.0</td><td>M</td><td>‚úó</td><td>‚úó</td><td>‚úó</td></tr><tr><td>LongVideoBench [125]</td><td>3,763</td><td>473.0</td><td>6,678</td><td>84.1</td><td>A&amp;M</td><td>‚úó</td><td>‚úó</td><td>‚úó</td></tr><tr><td>PhysGame (Ours)</td><td>880</td><td>25.9</td><td>880</td><td>66.9</td><td>M</td><td>‚úì</td><td>‚úì</td><td>‚úì</td></tr></tbody></table></table></figure><blockquote><p>üîº This table compares various video LLMs benchmarks across several key features. It details the number of videos, average video length, the number of question-answer pairs, the average token count per pair, whether the annotation was manual or automatic, if the benchmark uses gameplay videos, if the questions assess physical commonsense, and if metadata is included.</p><details><summary>read the caption</summary>Table 1: Comparison with existing benchmarks for video LLMs in terms of the video number (#Videos), the average video duration (Len.), the number of QA pair (#QA Pairs), the average QA pair tokens (QA Tokens), the manually/automatic annotation manner (M/A), whether the benchmarks are gameplay video based (Game-Bsd), whether the questions are physical commonsense classified (Phys-Clsf), and whether the benchmarks contain meta information (Meta-info).</details></blockquote><h3 class="relative group">In-depth insights<div id=in-depth-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#in-depth-insights aria-label=Anchor>#</a></span></h3><h4 class="relative group">PhysGame Benchmark<div id=physgame-benchmark class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#physgame-benchmark aria-label=Anchor>#</a></span></h4><p>The PhysGame benchmark is a novel contribution for evaluating physical commonsense understanding in video LLMs. <strong>Its core strength lies in leveraging gameplay videos, which frequently contain glitches violating physics</strong>, thus providing a rich source of data for testing this under-explored capability. The benchmark&rsquo;s design is <strong>methodologically sound</strong>, categorizing glitches across four fundamental domains (mechanics, kinematics, optics, material properties) and twelve sub-categories. This granular approach allows for <strong>detailed analysis of model performance</strong>, highlighting specific areas of weakness or strength. PhysGame&rsquo;s focus on intuitive understanding of physics, rather than complex mathematical formulas, makes it <strong>accessible and relevant</strong> to a broad range of video LLMs. <strong>The use of multi-choice questions ensures objective evaluation</strong>, minimizing subjectivity. Overall, PhysGame addresses a critical gap in video LLM evaluation, offering a unique and valuable resource for researchers to advance the field.</p><h4 class="relative group">PhysVLM Model<div id=physvlm-model class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#physvlm-model aria-label=Anchor>#</a></span></h4><p>The PhysVLM model, a <strong>physical knowledge-enhanced video LLM</strong>, represents a significant advancement in video understanding. It leverages a two-stage training process: <strong>supervised fine-tuning</strong> using the PhysInstruct dataset and <strong>direct preference optimization</strong> using the PhysDPO dataset. This combined approach enables PhysVLM to outperform existing open-source models in identifying physical commonsense violations in gameplay videos, as demonstrated by its state-of-the-art performance on the PhysGame benchmark. <strong>PhysInstruct provides instruction-following pairs</strong>, guiding the model&rsquo;s learning of physical principles. <strong>PhysDPO refines the model&rsquo;s responses</strong> by including both preferred and misleadingly generated answers, addressing common training pitfalls. The model&rsquo;s success highlights the importance of specialized datasets in improving video LLMs&rsquo; abilities to reason about physics, moving beyond simplistic object recognition towards a deeper, more nuanced understanding of dynamic scenes.</p><h4 class="relative group">Dataset Creation<div id=dataset-creation class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#dataset-creation aria-label=Anchor>#</a></span></h4><p>The creation of a robust and representative dataset is crucial for evaluating video LLMs&rsquo; understanding of physical commonsense. A key aspect is the <strong>identification of glitches</strong> in gameplay videos, which serve as a rich source of physical violations. The process of acquiring videos, ideally from diverse sources and spanning various game genres, is critical for dataset diversity. <strong>Annotation is a significant challenge</strong>, requiring careful labeling and categorization of glitches, potentially involving multiple annotators for quality control and inter-annotator agreement. The choice of annotation format, such as multiple-choice questions or free-form descriptions, significantly impacts the evaluation process and the types of inferences LLMs can be assessed on. Furthermore, the <strong>design of evaluation metrics</strong> directly affects which aspects of physical understanding are prioritized and how well different LLMs are distinguished. Therefore, a well-designed dataset creation process requires careful consideration of data acquisition, annotation scheme, and evaluation metrics to ensure a fair and comprehensive assessment of the models.</p><h4 class="relative group">Evaluation Metrics<div id=evaluation-metrics class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#evaluation-metrics aria-label=Anchor>#</a></span></h4><p>Choosing the right evaluation metrics for a research paper is crucial for accurately assessing the contribution and impact of the work. For a study on physical commonsense violations in gameplay videos, the selection of metrics should reflect the nuances of the task and the nature of the data. <strong>Accuracy</strong> is a fundamental metric, measuring the percentage of correctly identified glitches or violations. However, <strong>accuracy alone is insufficient</strong>. The evaluation should also account for <strong>different types of glitches</strong>, which may require specialized metrics. For example, metrics that assess the model&rsquo;s capacity to distinguish between subtle and obvious glitches, or to handle variations in video quality or presentation, could prove valuable. <strong>Qualitative analysis</strong>, examining the specifics of the model&rsquo;s reasoning and the reasons for errors, is also important. Finally, the choice of metrics should consider the feasibility of obtaining ground truth labels, especially given the subjective nature of determining what constitutes a physical commonsense violation. Ideally, the paper should justify its metric choices with reference to related work and demonstrate the metrics&rsquo; relevance to assessing video LLMs and their ability to understand physical phenomena.</p><h4 class="relative group">Future Work<div id=future-work class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#future-work aria-label=Anchor>#</a></span></h4><p>Future research directions stemming from the PhysGame benchmark and PhysVLM model could involve <strong>expanding the dataset</strong> to encompass a wider variety of game genres and physical phenomena. <strong>Improving the robustness</strong> of PhysVLM to handle diverse video qualities and lighting conditions is crucial, as is further <strong>exploring the potential</strong> of preference optimization techniques for enhancing physical commonsense reasoning in video LLMs. A key area for future work is to <strong>investigate the transferability</strong> of the learned physical understanding to real-world scenarios. Finally, developing more <strong>sophisticated evaluation metrics</strong> that go beyond simple accuracy, and incorporating human evaluation, would help assess the model&rsquo;s performance in a more nuanced and comprehensive way. This would allow for a better understanding of the model&rsquo;s limitations and how it can be improved to better capture nuanced physical reasoning.</p><h3 class="relative group">More visual insights<div id=more-visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#more-visual-insights aria-label=Anchor>#</a></span></h3><details><summary>More on figures</summary><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.01800/x3.png alt></figure></p><blockquote><p>üîº This figure shows an example of a multiple-choice question used in the PhysGame benchmark. The question asks to describe a glitch or anomaly observed in a gameplay video. Four options are provided, each describing a different potential anomaly. The correct answer is highlighted in green, illustrating how the PhysGame dataset is annotated for evaluating video LLMs&rsquo; understanding of physical common sense.</p><details><summary>read the caption</summary>Figure 2: The annotated multi-choice question in PhysGame. The correct option is annotated in green.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.01800/x4.png alt></figure></p><blockquote><p>üîº This figure illustrates the process of direct preference optimization (DPO) used to enhance the video LLM&rsquo;s ability to identify physical commonsense violations. The preferred data consists of question-answer pairs generated using accurate video titles (meta-information) to guide the LLM. In contrast, the dispreferred data is created using misleading titles (meta-information hacking), reduced frame counts (temporal hacking), and lowered spatial resolutions (spatial hacking). This approach helps the model learn to distinguish between physically plausible and implausible video content.</p><details><summary>read the caption</summary>Figure 3: Overview of the direct preference optimization training, where the preferred data is generated with the guidance of associated meta-information (i.e., title) while dispreferred data is generated with misleading titles (i.e., meta-information hacking), fewer frames (i.e., temporal hacking) and lower resolutions (i.e., spatial hacking).</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.01800/x5.png alt></figure></p><blockquote><p>üîº This figure showcases two example question-answer pairs from the PhysInstruct dataset. The dataset is used for instruction tuning of a video large language model (VLM) to improve its ability to understand physical commonsense in videos. The first example uses the video title as a hint (w/), guiding the model to correctly identify the physical glitch shown in the video. The second example omits the title (w/o), and in this case, the model does not correctly identify the glitch, demonstrating how meta information such as the video title can aid in physical commonsense understanding.</p><details><summary>read the caption</summary>Figure 4: Example cases in the PhysInstruct dataset with (w/) or without (w/o) meta-information hints.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.01800/x6.png alt></figure></p><blockquote><p>üîº This table presents the results of ablation studies conducted to evaluate the impact of different data augmentation techniques on the performance of the PhysVLM model. Specifically, it examines the effect of removing &rsquo;temporal hacking&rsquo;, &lsquo;spatial hacking&rsquo;, and &lsquo;meta-info hacking&rsquo; during the generation of the PhysDPO dataset. The results show the average accuracy of the model on the PhysGame benchmark with each of these augmentations removed, revealing the relative contribution of each technique to overall model performance.</p><details><summary>read the caption</summary>Table 7: Ablation studies of the temporal, spatial, and meta-info hacking in the PhysDPO dataset generation process.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.01800/x7.png alt></figure></p><blockquote><p>üîº The figure shows a qualitative comparison of three different video LLMs (PhysVLM, GPT-4, and LLaVA-Next-Video) in identifying visual glitches in gameplay videos. The left column shows a sequence of frames from a gameplay video, while the right column shows the captions generated by the respective models, describing the glitches or physical commonsense violations identified in the video. The specific example focuses on a scenario involving a motorcycle colliding with a car, which is followed by the car flying unrealistically into the air. The models vary significantly in their ability to detect and describe the physics-related issues.</p><details><summary>read the caption</summary>(a)</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.01800/x8.png alt></figure></p><blockquote><p>üîº The figure shows qualitative examples of open-ended questions in the PhysGame benchmark. PhysGame uses both open-ended and multiple-choice questions to assess the understanding of physical common sense violations in gameplay videos. In this particular example (b), the questions ask about the physical commonsense violations shown in gameplay video clips. The answers from three video LLMs (PhysVLM, GPT-40, and LLaVA-Next-Video) are provided for comparison, highlighting differences in their abilities to detect and describe these violations.</p><details><summary>read the caption</summary>(b)</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.01800/x9.png alt></figure></p><blockquote><p>üîº Figure 5 presents two examples showcasing open-ended questioning in the PhysGame benchmark. Each example displays a gameplay video clip with a physics glitch, followed by responses from three different video LLMs: PhysVLM, GPT-40, and LLaVA-Next-Video. The responses illustrate the varying capabilities of these models in identifying and describing the specific nature of the physical commonsense violations present in the video clips. This highlights the nuanced challenges in evaluating physical reasoning within video LLMs.</p><details><summary>read the caption</summary>Figure 5: Qualitative examples of open-ended questions.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.01800/x10.png alt></figure></p><blockquote><p>üîº The figure shows a comparison of three different Video LLMs&rsquo; responses to a gameplay video glitch. The video depicts a motorcycle colliding with a car, causing the car to flip unrealistically. PhysVLM correctly identifies the physical commonsense violation, whereas GPT-4 and LLaVA-Next-Video fail to do so, highlighting the limitations of current video LLMs in understanding physics.</p><details><summary>read the caption</summary>(a)</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.01800/x11.png alt></figure></p><blockquote><p>üîº The figure shows qualitative examples of open-ended questions for evaluating video LLMs&rsquo; understanding of physical commonsense. It presents two videos and their corresponding answers from three different video LLMs: PhysVLM, GPT-40, and LLaVA-Next-Video. The responses highlight how each model interprets and explains the physical glitches or inconsistencies present in the gameplay videos. In (b), the video involves a character&rsquo;s transition from a dark area to a sunlit one, causing issues with shadow and lighting consistency. PhysVLM correctly points out the lighting inconsistencies, GPT-40 identifies a more generic game bug (the character is resetting to a previous position), and LLaVA-Next-Video highlights a jerky movement as the glitch.</p><details><summary>read the caption</summary>(b)</details></blockquote></details><details><summary>More on tables</summary><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Benchmarks</th><th>Vid-Bsd</th><th>Instruct</th><th>MModal</th></tr></thead><tbody><tr><td>GameBunny [107]</td><td>‚úó</td><td>‚úì</td><td>‚úì</td></tr><tr><td>Taesiri et.al [109]</td><td>‚úì</td><td>‚úó</td><td>‚úì</td></tr><tr><td>GameBugDescript [110]</td><td>‚úì</td><td>‚úì</td><td>‚úó</td></tr><tr><td>GlitchBench [111]</td><td>‚úó</td><td>‚úì</td><td>‚úì</td></tr><tr><td><strong>PhysGame (Ours)</strong></td><td>‚úì</td><td>‚úì</td><td>‚úì</td></tr></tbody></table></table></figure><blockquote><p>üîº This table compares several existing benchmarks for evaluating video large language models (LLMs) specifically in the context of gameplay videos. It focuses on three key aspects: whether the benchmark uses video data (Vid-Bsd), if the evaluation tasks are presented in an instructional format (Instruct), and if the benchmark supports the evaluation of multi-modal models (MModal). This allows for a clearer understanding of how these benchmarks differ in their approach and capabilities and the types of LLMs they are designed to assess.</p><details><summary>read the caption</summary>Table 2: Comparison with existing gameplay video benchmarks in terms of whether they are video-based (Vid-Bsd), whether they follow an instructional format (Instruct), and support multi-modal evaluations (MModal).</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th style=text-align:left></th><th style=text-align:center>Opt. A</th><th style=text-align:center>Opt. B</th><th style=text-align:center>Opt. C</th><th style=text-align:center>Opt. D</th></tr></thead><tbody><tr><td style=text-align:left>Avg. tokens</td><td style=text-align:center>14.40</td><td style=text-align:center>14.49</td><td style=text-align:center>14.46</td><td style=text-align:center>14.47</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents the average number of tokens (words or sub-words) across the four answer choices for each multiple-choice question in the PhysGame benchmark. It indicates the length of the distractor options relative to the correct option, helping to ensure the quality of the distractor options and mitigate any bias introduced by length differences.</p><details><summary>read the caption</summary>Table 3: The average tokens of four options in the annotations of PhysGame benchmark.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><h2 class="relative group">Table 1: Model Comparison<div id=table-1-model-comparison class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#table-1-model-comparison aria-label=Anchor>#</a></span></h2><table><thead><tr><th>Models</th><th>Citation</th><th>AVG</th><th>Mechanics</th><th></th><th></th><th>Kinematics</th><th></th><th>Optics</th><th></th><th></th><th>Material</th><th></th><th></th><th></th></tr></thead><tbody><tr><td></td><td></td><td>Grav.</td><td>Elast.</td><td>Fric.</td><td>Velo.</td><td>Acc.</td><td>Refl.</td><td>Refr.</td><td>Abs.</td><td>Col.</td><td>Rig.</td><td>Sha.</td><td>Gest.</td><td></td></tr><tr><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td></tr><tr><td><em style=color:#00f>Proprietary Multi-modal LLMs</em></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Claude3.5-Sonnet</td><td>[4]</td><td>54.3</td><td><strong>50.7</strong></td><td>58.8</td><td>50.6</td><td><strong>53.2</strong></td><td>59.1</td><td><strong>50.0</strong></td><td>50.0</td><td>49.2</td><td>64.4</td><td>52.7</td><td>50.0</td><td><strong>62.1</strong></td></tr><tr><td>Claude3.5-SonnetV2</td><td>[4]</td><td>47.6</td><td>46.5</td><td>52.5</td><td>46.6</td><td>37.2</td><td>53.4</td><td>47.8</td><td>50.0</td><td>33.9</td><td>55.6</td><td>54.1</td><td>43.8</td><td>51.7</td></tr><tr><td>Gemini-1.5-pro</td><td>[114]</td><td>55.2</td><td><strong>50.7</strong></td><td><strong>70.0</strong></td><td>48.9</td><td>51.1</td><td>59.1</td><td><strong>50.0</strong></td><td>42.9</td><td><strong>52.5</strong></td><td><strong>71.1</strong></td><td><strong>56.8</strong></td><td>53.1</td><td>58.6</td></tr><tr><td>Gemini-1.5-pro-flash</td><td>[114]</td><td>48.5</td><td>47.9</td><td>52.5</td><td>51.7</td><td>43.6</td><td>51.1</td><td>43.5</td><td>53.6</td><td>33.9</td><td>64.4</td><td>43.2</td><td>46.9</td><td>49.4</td></tr><tr><td>GPT-4V</td><td>[1]</td><td>45.9</td><td>40.8</td><td>60.0</td><td>48.3</td><td>34.0</td><td>48.9</td><td>43.5</td><td>46.4</td><td>42.4</td><td>53.3</td><td>45.9</td><td>37.5</td><td>44.8</td></tr><tr><td>GPT-4o-0806</td><td>[92]</td><td><strong>56.1</strong></td><td>47.9</td><td>61.3</td><td><strong>59.1</strong></td><td>43.6</td><td><strong>61.4</strong></td><td>43.5</td><td>53.6</td><td>50.8</td><td>68.9</td><td>54.1</td><td><strong>65.6</strong></td><td>63.2</td></tr><tr><td>GPT-4o-mini-0718</td><td>[92]</td><td>40.3</td><td>43.7</td><td>43.8</td><td>39.2</td><td>35.1</td><td>44.3</td><td>30.4</td><td>46.4</td><td>42.4</td><td>44.4</td><td>37.8</td><td>37.5</td><td>41.4</td></tr><tr><td>Qwen-VL-max</td><td>[6]</td><td>50.9</td><td><strong>50.7</strong></td><td>53.8</td><td>51.1</td><td>31.9</td><td>46.6</td><td><strong>50.0</strong></td><td><strong>60.7</strong></td><td>50.8</td><td>64.4</td><td>48.6</td><td><strong>65.6</strong></td><td>59.8</td></tr><tr><td><em style=color:#00f>Open-source Multi-modal LLMs</em></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>LLaVA-Next-Video</td><td>[72]</td><td>32.2</td><td>43.7</td><td>33.8</td><td>27.3</td><td>34.0</td><td>22.7</td><td>21.7</td><td>35.7</td><td>23.7</td><td>35.6</td><td>41.9</td><td>34.4</td><td>37.9</td></tr><tr><td>Video-LLaVA</td><td>[68]</td><td>29.0</td><td>32.4</td><td>22.5</td><td>27.8</td><td>31.9</td><td>26.1</td><td>19.6</td><td>35.7</td><td>32.2</td><td>31.1</td><td>36.5</td><td>28.1</td><td>27.6</td></tr><tr><td>LLaVA-OneVision</td><td>[58]</td><td>47.7</td><td>50.7</td><td>50.0</td><td>46.0</td><td>39.4</td><td>45.5</td><td>43.5</td><td><strong>71.4</strong></td><td><strong>40.7</strong></td><td>55.6</td><td>44.6</td><td><strong>56.2</strong></td><td>52.9</td></tr><tr><td>InternVL2</td><td>[29]</td><td>33.4</td><td>29.6</td><td>31.2</td><td>38.6</td><td>35.1</td><td>30.7</td><td>30.4</td><td>53.6</td><td>35.6</td><td>26.7</td><td>29.7</td><td>18.8</td><td>34.5</td></tr><tr><td>VideoChat2</td><td>[64]</td><td>34.3</td><td>33.8</td><td>35.0</td><td>29.5</td><td>41.5</td><td>28.4</td><td>28.3</td><td>32.1</td><td>33.9</td><td>33.3</td><td>41.9</td><td>21.9</td><td>44.8</td></tr><tr><td>ST-LLM</td><td>[77]</td><td>32.8</td><td>32.4</td><td>26.2</td><td>26.7</td><td>37.2</td><td>28.4</td><td>37.0</td><td>25.0</td><td>28.8</td><td>33.3</td><td>40.5</td><td>37.5</td><td>46.0</td></tr><tr><td>Chat-UniVi</td><td>[54]</td><td>29.5</td><td>28.2</td><td>27.5</td><td>29.5</td><td>39.4</td><td>23.9</td><td>28.3</td><td>32.1</td><td>30.5</td><td>31.1</td><td>18.9</td><td>28.1</td><td>35.6</td></tr><tr><td>PPLLaVA</td><td>[78]</td><td>38.4</td><td>45.1</td><td>38.8</td><td>42.6</td><td>30.9</td><td>30.7</td><td>41.3</td><td>39.3</td><td>35.6</td><td>44.4</td><td>39.2</td><td>18.8</td><td>43.7</td></tr><tr><td><strong>PhysVLM-SFT</strong></td><td></td><td>56.7</td><td>54.9</td><td>62.5</td><td><strong>60.2</strong></td><td>51.1</td><td><strong>63.6</strong></td><td><strong>45.7</strong></td><td>57.1</td><td>28.8</td><td><strong>64.4</strong></td><td>51.4</td><td>50.0</td><td>72.4</td></tr><tr><td><strong>PhysVLM-DPO</strong></td><td></td><td><strong>59.5</strong></td><td><strong>64.8</strong></td><td><strong>66.3</strong></td><td><strong>60.2</strong></td><td><strong>59.6</strong></td><td>60.2</td><td>39.1</td><td>67.9</td><td>35.6</td><td>57.8</td><td><strong>62.2</strong></td><td>37.5</td><td><strong>78.2</strong></td></tr></tbody></table></table></figure><blockquote><p>üîº Table 4 presents a detailed comparison of the performance of various open-source and proprietary Large Language Models (LLMs) on the PhysGame benchmark. PhysGame assesses the ability of LLMs to identify and understand violations of physical common sense within gameplay videos. The table breaks down the results by several fine-grained subcategories of physics (gravity, elasticity, friction, velocity, acceleration, reflection, refraction, absorption & transmission, color, rigidity, object shape, and body gesture), providing a granular view of each model&rsquo;s strengths and weaknesses. It also shows the overall average accuracy for each model and distinguishes between two versions of the PhysVLM model: one trained with supervised fine-tuning only (PhysVLM-SFT) and another trained with both supervised fine-tuning and direct preference optimization (PhysVLM-DPO). This allows for a direct comparison of the impact of the more advanced training technique on performance.</p><details><summary>read the caption</summary>Table 4: Evaluation results (%) of open-source and proprietary multi-modal LLMs on PhysGame. The fine-grained categories include gravity, elasticity, friction, velocity, acceleration, reflection, refraction, absorption & transmission, color, rigidity, object shape, and body gesture. AVG denotes the average accuracy. PhysVLM-SFT denotes PhysVLM only undergoes supervised fine-tuning while PhysVLM-DPO denotes PhysVLM with consecutive supervised fine-tuning and direct preference optimization.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Models</th><th></th><th>LLM Params</th><th>Short (%)</th><th></th><th>Medium (%)</th><th></th><th>Long (%)</th><th></th><th>Overall (%)</th><th></th></tr></thead><tbody><tr><td>InternVL-Chat-V1.5</td><td>[29]</td><td>20B</td><td>60.2</td><td>61.7</td><td>46.4</td><td>49.1</td><td>45.6</td><td>46.6</td><td>50.7</td><td>52.4</td></tr><tr><td>LLaVA-NeXT-Video</td><td>[72]</td><td>34B</td><td>61.7</td><td>65.1</td><td>50.1</td><td>52.2</td><td>44.3</td><td>47.2</td><td>52.0</td><td>54.9</td></tr><tr><td>VILA-1.5</td><td>[69]</td><td>34B</td><td>68.1</td><td>68.9</td><td>58.1</td><td>57.4</td><td>50.8</td><td>52.0</td><td>59.0</td><td>59.4</td></tr><tr><td>LLaVA-OneVision</td><td>[58]</td><td>72B</td><td>76.7</td><td>79.3</td><td>62.2</td><td>66.9</td><td>60.0</td><td>62.4</td><td>66.3</td><td>69.6</td></tr><tr><td>Qwen-VL-Chat</td><td>[6]</td><td>7B</td><td>46.9</td><td>47.3</td><td>38.7</td><td>40.4</td><td>37.8</td><td>37.9</td><td>41.1</td><td>41.9</td></tr><tr><td>Video-LLaVA</td><td>[68]</td><td>7B</td><td>45.3</td><td>46.1</td><td>38.0</td><td>40.7</td><td>36.2</td><td>38.1</td><td>39.9</td><td>41.6</td></tr><tr><td>ST-LLM</td><td>[76]</td><td>7B</td><td>45.7</td><td>48.4</td><td>36.8</td><td>41.4</td><td>31.3</td><td>36.9</td><td>37.9</td><td>42.3</td></tr><tr><td>VideoChat2-Mistral</td><td>[64]</td><td>7B</td><td>48.3</td><td>52.8</td><td>37.0</td><td>39.4</td><td>33.2</td><td>39.2</td><td>39.5</td><td>43.8</td></tr><tr><td>Chat-UniVi-V1.5</td><td>[54]</td><td>7B</td><td>45.7</td><td>51.2</td><td>40.3</td><td>44.6</td><td>35.8</td><td>41.8</td><td>40.6</td><td>45.9</td></tr><tr><td>LLaVA-NeXT-Video</td><td>[72]</td><td>7B</td><td>45.9</td><td>49.8</td><td>40.3</td><td>44.3</td><td>36.6</td><td>41.0</td><td>40.9</td><td>45.0</td></tr><tr><td>PPLLaVA</td><td>[78]</td><td>7B</td><td>58.7</td><td>62.8</td><td>45.6</td><td>50.4</td><td>42.2</td><td>47.4</td><td>48.8</td><td>53.6</td></tr><tr><td><strong>PhysVLM-SFT</strong></td><td></td><td>7B</td><td>64.1</td><td>68.0</td><td><strong>55.0</strong></td><td><strong>61.7</strong></td><td>46.4</td><td>50.3</td><td>55.2</td><td>60.0</td></tr><tr><td><strong>PhysVLM-DPO</strong></td><td></td><td>7B</td><td><strong>66.1</strong></td><td><strong>70.0</strong></td><td>54.3</td><td>59.6</td><td><strong>47.1</strong></td><td><strong>53.8</strong></td><td><strong>55.8</strong></td><td><strong>61.1</strong></td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents the performance comparison of various Large Language Models (LLMs) on the Video Multimodal Entailment (Video-MME) benchmark. The benchmark assesses the ability of LLMs to understand and reason about video content. The table shows the performance scores (in percentages) for each LLM, categorized by video length (short, medium, long), and whether subtitles were used. Higher percentages indicate better performance. The results are broken down into &lsquo;with subtitles&rsquo; and &lsquo;without subtitles&rsquo; to show the impact of textual information on the models&rsquo; video comprehension abilities.</p><details><summary>read the caption</summary>Table 5: Evaluation results (%) on Video-MME. ‚Äúw/ subs‚Äù and ‚Äúw/o subs‚Äù respectively denote ‚Äúwith subtitles‚Äù and ‚Äúwithout subtitles‚Äù.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Methods</th><th>CI</th><th>DO</th><th>CU</th><th>TU</th><th>CO</th><th>AVG</th></tr></thead><tbody><tr><td>VideoChat</td><td>2.23</td><td>2.50</td><td>2.53</td><td>1.94</td><td>2.24</td><td>2.29</td></tr><tr><td>Video-ChatGPT</td><td>2.50</td><td>2.57</td><td>2.69</td><td>2.16</td><td>2.20</td><td>2.42</td></tr><tr><td>BT-Adapter</td><td>2.68</td><td>2.69</td><td>3.27</td><td>2.34</td><td>2.46</td><td>2.69</td></tr><tr><td>Chat-UniVi</td><td>2.89</td><td>2.91</td><td>3.46</td><td>2.89</td><td>2.81</td><td>2.99</td></tr><tr><td>VideoChat2</td><td>3.02</td><td>2.88</td><td>3.51</td><td>2.66</td><td>2.81</td><td>2.98</td></tr><tr><td>LLaMA-VID</td><td>2.96</td><td>3.00</td><td>3.53</td><td>2.46</td><td>2.51</td><td>2.89</td></tr><tr><td>ST-LLM</td><td>3.23</td><td>3.05</td><td>3.74</td><td>2.93</td><td>2.81</td><td>3.15</td></tr><tr><td>PLLaVA</td><td>3.21</td><td>2.86</td><td>3.62</td><td>2.33</td><td>2.93</td><td>2.99</td></tr><tr><td>LLaVA-Next-Video</td><td>3.39</td><td>3.29</td><td>3.92</td><td>2.60</td><td>3.12</td><td>3.26</td></tr><tr><td>PPLLaVA</td><td>3.32</td><td>3.20</td><td>3.88</td><td>3.00</td><td>3.20</td><td>3.32</td></tr><tr><td><strong>PhysVLM-SFT</strong></td><td><strong>3.59</strong></td><td>3.07</td><td>3.89</td><td>2.74</td><td><strong>3.44</strong></td><td><strong>3.35</strong></td></tr><tr><td>LLaVA-Next-Video*</td><td>3.64</td><td>3.45</td><td>4.17</td><td>2.95</td><td>4.08</td><td>3.66</td></tr><tr><td>PPLLaVA*</td><td>3.85</td><td>3.56</td><td>4.21</td><td>3.21</td><td>3.81</td><td>3.73</td></tr><tr><td><strong>PhysVLM-DPO</strong>*</td><td><strong>3.89</strong></td><td><strong>3.69</strong></td><td><strong>4.26</strong></td><td>3.11</td><td><strong>4.19</strong></td><td><strong>3.83</strong></td></tr></tbody></table></table></figure><blockquote><p>üîº Table 6 presents a comprehensive evaluation of various video LLMs on the VCG benchmark [83], focusing on several key aspects of video understanding. The benchmark assesses the models&rsquo; capabilities across five dimensions: Correctness of Information (CI), Detail Orientation (DO), Contextual Understanding (CU), Temporal Understanding (TU), and Consistency (CO). The table shows the individual scores for each model and metric, along with an overall average (AVG) score. The models marked with an asterisk (*) utilize either Direct Preference Optimization (DPO) or Proximal Policy Optimization (PPO) [104], which are advanced training techniques aimed at improving model performance. This allows for comparison of models trained using traditional methods versus those employing more advanced techniques.</p><details><summary>read the caption</summary>Table 6: Evaluation results on VCG benchmark [83]. Methods marked by ‚àó use DPO or PPO [104]. CI, DO, CU, TU, and CO respectively denote correctness of information, detail orientation, contextual understanding, temporal understanding, and consistency. AVG is the average result.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Methods</th><th>AVG</th></tr></thead><tbody><tr><td>PhysVLM-DPO</td><td>59.5</td></tr><tr><td><em>w/o</em> temporal hacking</td><td>57.6</td></tr><tr><td><em>w/o</em> spatial hacking</td><td>57.3</td></tr><tr><td><em>w/o</em> meta-info hacking</td><td>57.4</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents the results of ablation studies on the training data used for the PhysVLM model. It shows the impact of different training datasets on the model&rsquo;s performance, measured by average accuracy on the PhysGame benchmark. Specifically, it compares the performance when using only LLaVA-Hound data, LLaVA-Hound and LLaVA-Image data, and the full dataset including PhysInstruct. The impact of using only LLaVA-Hound-DPO and the full dataset including PhysDPO is also analyzed in the DPO stage. This table helps to understand the contribution of each dataset to the overall model performance.</p><details><summary>read the caption</summary>Table 8: Ablations of training data in SFT and DPO stages. AVG denotes the average accuracy on the PhysGame benchmark.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Stage</th><th>Training Data</th><th>AVG</th></tr></thead><tbody><tr><td>SFT</td><td>LLava-Hound</td><td>40.7</td></tr><tr><td>SFT</td><td>LLava-Hound [142], LLaVA-Image [73]</td><td>46.0</td></tr><tr><td>SFT</td><td>LLava-Hound, LLaVA-Image, <strong>PhysInstruct</strong></td><td><strong>56.7</strong></td></tr><tr><td>DPO</td><td>LLava-Hound-DPO [142]</td><td>52.9</td></tr><tr><td>DPO</td><td>LLava-Hound-DPO, <strong>PhysDPO</strong></td><td><strong>59.5</strong></td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents the results of ablation studies conducted to analyze the impact of hyperparameters used in generating the PhysDPO dataset. Specifically, it examines how variations in the number of sampled frames (N) during temporal hacking and the frame resolution scale factor (Œ≥) during spatial hacking affect the overall performance. The table helps determine the optimal settings for these hyperparameters to ensure the effectiveness of the PhysDPO dataset in improving the model&rsquo;s understanding of physical commonsense.</p><details><summary>read the caption</summary>Table 9: Hyper-parameter ablations of (a) the sampled frame number NùëÅNitalic_N in temporal hacking and (b) the frame resolution scale factor Œ≥ùõæ\gammaitalic_Œ≥ in spatial hacking for PhysDPO construction.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>N</th><th>1</th><th>2</th><th>4</th></tr></thead><tbody><tr><td><strong>AVG</strong></td><td><strong>59.5</strong></td><td>58.1</td><td>57.8</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents the ablation study results comparing the performance of PhysVLM when using either Vicuna-7B or Qwen-2-7B as the underlying large language model. It shows the average accuracy and the performance across different fine-grained categories within four main physical domains (Mechanics, Kinematics, Optics, and Material) for both supervised fine-tuning (SFT) and direct preference optimization (DPO) stages. This allows for a detailed assessment of the impact of the LLM choice on the model&rsquo;s ability to understand physical common sense.</p><details><summary>read the caption</summary>Table 10: Ablations on LLMs in PhysVLM with Vicuna-7B [30] or Qwen2-7B [131].</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><p>|</p><table><thead><tr><th>1/8</th><th>1/16</th><th>1/32</th><th></th></tr></thead><tbody><tr><td>Œ≥</td><td>57.1</td><td><strong>59.5</strong></td><td>58.6</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents ablation study results on the VCG benchmark, evaluating the impact of different training data combinations on the model&rsquo;s performance. It shows the average scores and individual scores across five sub-categories (correctness of information, detail orientation, contextual understanding, temporal understanding, and consistency) for various training data setups. The setups include training with only LLaVA-Hound data, adding LLaVA-Image data, further adding the PhysInstruct dataset (for supervised fine-tuning), adding LLaVA-Hound-DPO data (for direct preference optimization), and finally adding both the PhysInstruct and PhysDPO datasets.</p><details><summary>read the caption</summary>Table 11: Ablations on training data on VCG benchmark.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Stage</th><th>LLMs</th><th>AVG</th><th>Mechanics</th><th>Mechanics</th><th>Mechanics</th><th>Kinematics</th><th>Kinematics</th><th>Optics</th><th>Optics</th><th>Optics</th><th>Material</th><th>Material</th><th>Material</th><th>Material</th></tr></thead><tbody><tr><td></td><td></td><td></td><td>Grav.</td><td>Elast.</td><td>Fric.</td><td>Velo.</td><td>Acc.</td><td>Refl.</td><td>Refr.</td><td>Abs.</td><td>Col.</td><td>Rig.</td><td>Sha.</td><td>Gest.</td></tr><tr><td><strong>SFT</strong></td><td>Vicuna</td><td>44.7</td><td>47.9</td><td>45.0</td><td>48.9</td><td><strong>52.1</strong></td><td>48.9</td><td>30.4</td><td>42.9</td><td><strong>28.8</strong></td><td>28.9</td><td>50.0</td><td>31.2</td><td>48.3</td></tr><tr><td><strong>SFT</strong></td><td>Qwen-2</td><td><strong>56.7</strong></td><td><strong>54.9</strong></td><td><strong>62.5</strong></td><td><strong>60.2</strong></td><td>51.1</td><td><strong>63.6</strong></td><td><strong>45.7</strong></td><td><strong>57.1</strong></td><td><strong>28.8</strong></td><td><strong>64.4</strong></td><td><strong>51.4</strong></td><td><strong>50.0</strong></td><td><strong>72.4</strong></td></tr><tr><td><strong>DPO</strong></td><td>Vicuna</td><td>48.2</td><td>56.3</td><td>52.5</td><td>50.6</td><td><strong>59.6</strong></td><td>48.9</td><td>28.3</td><td>35.7</td><td>28.8</td><td>31.1</td><td>47.3</td><td><strong>37.5</strong></td><td>60.9</td></tr><tr><td><strong>DPO</strong></td><td>Qwen-2</td><td><strong>59.5</strong></td><td><strong>64.8</strong></td><td><strong>66.3</strong></td><td><strong>60.2</strong></td><td><strong>59.6</strong></td><td><strong>60.2</strong></td><td><strong>39.1</strong></td><td><strong>67.9</strong></td><td><strong>35.6</strong></td><td><strong>57.8</strong></td><td><strong>62.2</strong></td><td><strong>37.5</strong></td><td><strong>78.2</strong></td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents the ablation study results on the Video-MME benchmark, showing the impact of different training data combinations on the model&rsquo;s performance. It breaks down the results by video length (short, medium, long) and indicates whether subtitles were used. The table helps to understand the contribution of each dataset to the overall performance of the model on Video-MME.</p><details><summary>read the caption</summary>Table 12: Ablations on training data on Video-MME benchmark.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Stage</th><th>Training Data</th><th>CI</th><th>DO</th><th>CU</th><th>TU</th><th>CO</th><th>AVG</th></tr></thead><tbody><tr><td><strong>SFT</strong></td><td>LLava-Hound</td><td>3.48</td><td>2.88</td><td>3.74</td><td>2.58</td><td>3.02</td><td>3.14</td></tr><tr><td><strong>SFT</strong></td><td>LLava-Hound, LLaVA-Image</td><td>3.43</td><td>2.99</td><td>3.73</td><td>2.56</td><td>3.12</td><td>3.17</td></tr><tr><td><strong>SFT</strong></td><td>LLava-Hound, LLaVA-Image, PhysInstruct</td><td><strong>3.59</strong></td><td><strong>3.07</strong></td><td><strong>3.89</strong></td><td><strong>2.74</strong></td><td><strong>3.44</strong></td><td><strong>3.35</strong></td></tr><tr><td><strong>DPO</strong></td><td>LLava-Hound-DPO</td><td>3.94</td><td>3.43</td><td>4.25</td><td><strong>3.12</strong></td><td>4.05</td><td>3.76</td></tr><tr><td><strong>DPO</strong></td><td>LLava-Hound-DPO, PhysDPO</td><td><strong>3.89</strong></td><td><strong>3.69</strong></td><td><strong>4.26</strong></td><td>3.11</td><td><strong>4.19</strong></td><td><strong>3.83</strong></td></tr></tbody></table></table></figure><blockquote><p>üîº This table details the prompt template used to generate the instruction-tuning dataset, PhysInstruct. The prompt instructs an AI to act as a visual assistant, analyzing a video and its title (which may or may not be accurate). The AI should identify and describe any violations of physics in the video, creating a conversational exchange between the AI and a user. The AI is explicitly told to base its analysis on its own observations and understanding of the video, not relying on the accuracy of the provided title. All descriptions must be at the video level, not referencing individual images or frames.</p><details><summary>read the caption</summary>Table 13: Prompt for instruction-tuning data generation in PhysInstruct.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Models</th><th>Training Data</th><th>Short (%) <em>w/o</em> subs</th><th>Short (%) <em>w/</em> subs</th><th>Medium (%) <em>w/o</em> subs</th><th>Medium (%) <em>w/</em> subs</th><th>Long (%) <em>w/o</em> subs</th><th>Long (%) <em>w/</em> subs</th><th>Overall (%) <em>w/o</em> subs</th><th>Overall (%) <em>w/</em> subs</th></tr></thead><tbody><tr><td>SFT</td><td>LLava-Hound</td><td>65.6</td><td>68.9</td><td>55.3</td><td>60.4</td><td>47.7</td><td>52.4</td><td>56.2</td><td>60.6</td></tr><tr><td>SFT</td><td>LLava-Hound, LLaVA-Image</td><td>65.2</td><td>68.3</td><td>54.9</td><td>60.2</td><td>47.6</td><td>52.8</td><td>55.9</td><td>60.4</td></tr><tr><td>SFT</td><td>LLava-Hound, LLaVA-Image, PhysInstruct</td><td>64.1</td><td>68.0</td><td>55.0</td><td>61.7</td><td>46.4</td><td>50.3</td><td>55.2</td><td>60.0</td></tr><tr><td>DPO</td><td>LLava-Hound-DPO</td><td>66.0</td><td>70.2</td><td>53.6</td><td>60.5</td><td>47.3</td><td>52.8</td><td>55.6</td><td>61.2</td></tr><tr><td>DPO</td><td>LLava-Hound-DPO, PhysDPO</td><td>66.1</td><td>70.0</td><td>54.3</td><td>59.6</td><td>47.1</td><td>53.8</td><td>55.8</td><td>61.1</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents the prompt used for generating responses in the PhysDPO dataset. PhysDPO uses a technique called &lsquo;direct preference optimization&rsquo; where it needs both preferred and dispreferred responses for training. To create the dispreferred responses, misleading information is given. Specifically, a false title is randomly selected from other videos in the dataset, and then this misleading title is combined with the question from the PhysInstruct dataset. This table shows exactly the structure of the prompt given to the model in this process, to create the less desirable answers.</p><details><summary>read the caption</summary>Table 14: Prompt for response generation in PhysDPO. The false_title is randomly selected from the other videos and the question is instantiated by the same instruction in PhysInstruct.</details></blockquote></details><h3 class="relative group">Full paper<div id=full-paper class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#full-paper aria-label=Anchor>#</a></span></h3><div id=gallery-84d82a7011551462e0a1d563afecd762 class=gallery><img src=https://ai-paper-reviewer.com/2412.01800/1.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.01800/2.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.01800/3.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.01800/4.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.01800/5.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.01800/6.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.01800/7.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.01800/8.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.01800/9.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.01800/10.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.01800/11.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.01800/12.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.01800/13.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.01800/14.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.01800/15.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.01800/16.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.01800/17.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.01800/18.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.01800/19.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.01800/20.png class="grid-w50 md:grid-w33 xl:grid-w25"></div></div><section class="flex flex-row flex-wrap justify-center pt-4 text-xl"><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01800/&amp;title=PhysGame:%20Uncovering%20Physical%20Commonsense%20Violations%20in%20Gameplay%20Videos" title="Share on LinkedIn" aria-label="Share on LinkedIn"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://twitter.com/intent/tweet/?url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01800/&amp;text=PhysGame:%20Uncovering%20Physical%20Commonsense%20Violations%20in%20Gameplay%20Videos" title="Tweet on Twitter" aria-label="Tweet on Twitter"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="mailto:?body=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01800/&amp;subject=PhysGame:%20Uncovering%20Physical%20Commonsense%20Violations%20in%20Gameplay%20Videos" title="Send via email" aria-label="Send via email"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1-27.64 140.9 68.65 266.2 199.1 285.1 19.01 2.888 36.17-12.26 36.17-31.49l1e-4-.6631c0-15.74-11.44-28.88-26.84-31.24-84.35-12.98-149.2-86.13-149.2-174.2.0-102.9 88.61-185.5 193.4-175.4 91.54 8.869 158.6 91.25 158.6 183.2v16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98.0036c-7.299.0-13.2 4.992-15.12 11.68-24.85-12.15-54.24-16.38-86.06-5.106-38.75 13.73-68.12 48.91-73.72 89.64-9.483 69.01 43.81 128 110.9 128 26.44.0 50.43-9.544 69.59-24.88 24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3c0-149.2-133.9-265.632-287.3-235.57zM239.1 304.3c-26.47.0-48-21.56-48-48.05s21.53-48.05 48-48.05 48 21.56 48 48.05-20.6 48.05-48 48.05z"/></svg></span></a></section></div><script>var oid="views_paper-reviews/2412.01800/index.md",oid_likes="likes_paper-reviews/2412.01800/index.md"</script><script type=text/javascript src=/ai-paper-reviewer/js/page.min.0860cf4e04fa2d72cc33ddba263083464d48f67de06114529043cb4623319efed4f484fd7f1730df5abea0e2da6f3538855634081d02f2d6e920b956f063e823.js integrity="sha512-CGDPTgT6LXLMM926JjCDRk1I9n3gYRRSkEPLRiMxnv7U9IT9fxcw31q+oOLabzU4hVY0CB0C8tbpILlW8GPoIw=="></script></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="flex group mr-3" href=/ai-paper-reviewer/paper-reviews/2412.01506/><span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&larr;</span>
<span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Structured 3D Latents for Scalable and Versatile 3D Generation</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2024-12-02T00:00:00+00:00>2 December 2024</time>
</span></span></a></span><span><a class="flex text-right group ml-3" href=/ai-paper-reviewer/paper-reviews/2412.01106/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">One Shot, One Talk: Whole-body Talking Avatar from a Single Image</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2024-12-02T00:00:00+00:00>2 December 2024</time>
</span></span><span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&larr;</span></a></span></div></div><div class=pt-3><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class=pt-3><script src=https://utteranc.es/client.js repo=pmnxis/pmnxis.github.io issue-term=pathname label=Comment theme=dark-blue crossorigin=anonymous async></script></div></div></footer></article><div id=top-scroller class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer id=site-footer class="py-10 print:hidden"><nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400"><ul class="flex flex-col list-none sm:flex-row"><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=/ai-paper-reviewer/tags/ title>Tags</a></li></ul></nav><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2024
AI Paper Reviews by AI</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a></p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/ai-paper-reviewer/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh+sCQ0E53ghYrxgYqw+0GCRyIEpA=="></script></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://deep-diver.github.io/ai-paper-reviewer/ style=z-index:500><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body><script data-name=BMC-Widget data-cfasync=false src=https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js data-id=chansung data-description="Support me on Buy me a coffee!" data-message data-color=#FFDD00 data-position=Left data-x_margin=18 data-y_margin=18></script></html>