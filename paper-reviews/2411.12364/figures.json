[{"figure_path": "https://arxiv.org/html/2411.12364/x1.png", "caption": "(a) Validation loss", "description": "This figure shows the validation loss curves for various model architectures during training.  The x-axis represents the number of consumed tokens (in billions) during training, indicating the training progress. The y-axis displays the validation loss, which is a measure of how well the model generalizes to unseen data.  Lower validation loss signifies better generalization performance. Different lines represent different models, allowing for a comparison of their training performance and convergence rates.  The models compared include dense models (with various numbers of parameters), MoE models (Mixture of Experts), and UltraMem models (the proposed model in this paper).  The figure helps illustrate the effectiveness of UltraMem in maintaining performance while scaling up the model's capacity.", "section": "1 INTRODUCTION"}, {"figure_path": "https://arxiv.org/html/2411.12364/x2.png", "caption": "(b) Inference time", "description": "This figure shows the inference time of different Transformer models as the number of consumed tokens increases. The models compared are a dense Transformer, a Mixture-of-Experts (MoE) model, and the UltraMem model introduced in the paper.  The x-axis represents the number of consumed tokens (in billions) and is plotted on a logarithmic scale. The y-axis represents the inference time in milliseconds.  The figure demonstrates that UltraMem has significantly lower inference time than MoE, and comparable inference time with the dense Transformer, despite having the same computational cost and twelve times more parameters than the dense model.", "section": "1 INTRODUCTION"}, {"figure_path": "https://arxiv.org/html/2411.12364/x3.png", "caption": "(c) Memory access", "description": "This figure displays the memory access (in GB) for different models across varying batch sizes.  It shows how memory usage increases as batch size increases for different architectures. The models include a standard Transformer, Mixture of Experts (MoE), and the proposed UltraMem model.  This visualization helps demonstrate the memory efficiency of UltraMem compared to traditional methods and MoE, particularly highlighting its lower memory footprint at larger batch sizes. The x-axis represents batch size (log scale), and the y-axis represents memory access (log scale).", "section": "1 INTRODUCTION"}, {"figure_path": "https://arxiv.org/html/2411.12364/x4.png", "caption": "Figure 1: We ensured that three models have the same computation, and MoE and UltraMem have the same parameters. The x-axis is plotted on a logarithmic scale. In (b) and (c), the sequence length is 1 because during decoding time, we can only predict one token at a time, and the key/value cache length is 2048. The experiments in (b) and (c) are conducted on the A100-SXM-80GB.", "description": "This figure compares the performance of three different model architectures: a standard Transformer, a Mixture-of-Experts (MoE) model, and the proposed Ultra-Sparse Memory Network (UltraMem).  All three models were designed to have approximately the same computational cost, and the MoE and UltraMem models have the same number of parameters.  The plots show how validation loss, inference time, and memory access vary as a function of the number of consumed tokens (a proxy for the input sequence length). The x-axis uses a logarithmic scale to better visualize the trends across different input sizes. For the inference time (b) and memory access (c) measurements, the sequence length was set to 1 (since only one token can be predicted at a time during inference), and the key/value cache length was set to 2048. The inference time and memory access tests were performed on an NVIDIA A100-SXM-80GB GPU.", "section": "1 INTRODUCTION"}, {"figure_path": "https://arxiv.org/html/2411.12364/x5.png", "caption": "(a) Multilayer perceptron", "description": "This figure shows the architecture of a Multilayer Perceptron (MLP), a fundamental component in many neural networks, including the UltraMem model discussed in the paper.  An MLP consists of multiple layers of interconnected nodes (neurons), where each node performs a linear transformation followed by a non-linear activation function (GeLU in this case). The input to the network is transformed through multiple linear layers and activation functions. The final layer outputs the final result after multiple linear transformations and activation functions. The weights of the linear layers are represented as 'keys', and the outputs of the linear layers are referred to as 'values'. This illustration emphasizes the role of these layers and their weights in the function of the MLP.", "section": "3 ULTRAMEM"}, {"figure_path": "https://arxiv.org/html/2411.12364/x7.png", "caption": "(b) Large memory layer", "description": "This figure shows the architecture of a large memory layer (LML) used in the UltraMem model.  It illustrates how row and column keys are used to determine a 2D logical address to fetch values from a large-scale, ultra-sparse memory. This contrasts with a traditional MLP which utilizes a 1D logical address.  The process of retrieving values based on indices with higher scores is also depicted.", "section": "3 ULTRAMEM"}, {"figure_path": "https://arxiv.org/html/2411.12364/x8.png", "caption": "Figure 2: An overview of multilayer perceptron (MLP) and large memory layer (LML). For the sake of brevity, we omit the third top-m\ud835\udc5amitalic_m operation from memory layer. An MLP typically consists of two linear layers and a GeLU activation. We consider the weights of the first linear layer as keys, and those of the second linear layer as values. LML uses row and column keys to determine the 2-D logical address to index memory values, whereas MLP uses 1-D logical address. \u201cfetch value\u201d refers to retrieving values based on the indices with higher scores.", "description": "This figure provides a comparison of the architectures of a Multilayer Perceptron (MLP) and a Large Memory Layer (LML).  The MLP, a standard component in neural networks, consists of two linear layers and a GeLU activation function.  The weights of the first linear layer are interpreted as keys and the weights of the second linear layer as values.  The LML, designed for efficient access to large memory, uses both row and column keys to create a 2-dimensional logical address, which allows for retrieving specific value vectors from a memory space.  This contrasts with the MLP, which uses a simpler 1-dimensional logical address for accessing values.  The process of selecting the most relevant values based on their indices and associated scores is denoted as \"fetch value\".  The third top-m operation in the LML, a step to select only the most relevant m values, is omitted for brevity in this diagram.", "section": "ULTRAMEM"}, {"figure_path": "https://arxiv.org/html/2411.12364/x9.png", "caption": "Figure 3: Overall of PKM and UltraMem.", "description": "This figure provides a visual comparison of the Product-Key Memory (PKM) architecture and the proposed UltraMem architecture.  PKM is shown to integrate a memory layer in parallel or in place of an MLP layer within a standard Transformer architecture.  UltraMem improves upon PKM by decomposing the large memory layer into multiple smaller layers distributed across multiple Transformer layers.  This allows for greater efficiency and scalability by overlapping the execution of memory and Transformer layers.", "section": "3 ULTRAMEM"}, {"figure_path": "https://arxiv.org/html/2411.12364/x10.png", "caption": "Figure 4: Flow of Tucker Decomposed Query-Key Retrieval, here r=2\ud835\udc5f2r=2italic_r = 2. \u201cfetch\u201d refers to retrieving score based on given index.", "description": "This figure illustrates the Tucker Decomposed Query-Key Retrieval (TDQKR) method, a technique used to improve the efficiency of retrieving relevant values from a large memory layer.  The process begins with a query (x) which is transformed into row and column queries through two linear projections. These queries are then used to calculate row scores and column scores based on product keys (Krow and Kcol). A Tucker decomposition of rank-r (in this case r=2) is applied to these scores to form a score grid (Sgrid).  The Tucker decomposition uses a core tensor (C) which significantly reduces the computational cost compared to a full matrix multiplication.  Finally, the top-m largest values from Sgrid are selected, their indices are determined, and the corresponding values from the memory are retrieved. The diagram visually represents the step-by-step flow of the process, highlighting the key steps and operations involved in the TDQKR method.", "section": "3.2 STRUCTURE IMPROVEMENTS"}, {"figure_path": "https://arxiv.org/html/2411.12364/x11.png", "caption": "Figure 5: Flow of Implicit Value Expansion, here E=4\ud835\udc384E=4italic_E = 4, m=16\ud835\udc5a16m=16italic_m = 16.", "description": "This figure illustrates the process of Implicit Value Expansion (IVE) in UltraMem, a technique to reduce memory access during training.  The figure shows how, with an expansion rate of E=4, the original memory values (V) are expanded into four virtual memory blocks (V1, V2, V3, V4).  Each virtual block is a reparameterized version of the original value, created using linear projectors (Wp).  This expansion increases the effective size of the memory table without proportionally increasing memory access costs.  A shuffle operation is applied to the virtual memory addresses to further enhance memory access efficiency. The figure also demonstrates how the virtual blocks are accessed using linear layers (Linear1-4) and weighted sum pooling (\u03a3) to produce the final output. The 'm' parameter refers to the sparsity, here it is 16, meaning only top 16 values are considered for the next step.", "section": "3 ULTRAMEM"}, {"figure_path": "https://arxiv.org/html/2411.12364/x12.png", "caption": "(a) Scaling validation loss", "description": "This figure shows the C4 validation loss for different models across various scales.  The x-axis represents the number of consumed tokens (in billions) during training, while the y-axis represents the validation loss. Different lines represent different models, including dense models (Dense-151M, Dense-680M, Dense-1.6B, Dense-6.5B) and sparse models (UltraMem-151M-x12, UltraMem-680M-x12, UltraMem-1.6B-x12, MoE-151M-2in32, MoE-680M-2in33, MoE-1.6B-2in34).  The plot demonstrates how the validation loss decreases as the number of consumed tokens increases for all models, with the dense models generally exhibiting lower loss than the sparse models. The figure illustrates the scaling behavior of different model architectures in terms of training loss.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2411.12364/x13.png", "caption": "(b) Loss across varying sparsity", "description": "This figure shows the validation loss of different models with varying sparsity levels while maintaining the same number of activated parameters.  Each line represents a different sparsity level; for example, 20K indicates approximately 1 in 20,000 values are activated. The x-axis represents the ratio of sparse to dense parameters, showcasing how the model's capacity scales with different sparsity levels.  The y-axis shows the validation loss. This graph demonstrates the relationship between model sparsity and performance during training.", "section": "5.3 Value Number and Top-m"}, {"figure_path": "https://arxiv.org/html/2411.12364/x14.png", "caption": "(c) Speed across varying sparsity", "description": "This figure shows the inference speed of UltraMem and MoE models with 1.6 billion activated parameters.  The x-axis represents the ratio of sparse parameters to dense parameters, illustrating different sparsity levels. The y-axis shows the inference time in milliseconds. The plot demonstrates that UltraMem's inference speed remains relatively constant even as sparsity increases, unlike MoE, whose inference time increases significantly with increasing sparsity. This highlights UltraMem's efficiency and robustness in inference scenarios.", "section": "5.3 Value Number and Top-m"}, {"figure_path": "https://arxiv.org/html/2411.12364/x15.png", "caption": "Figure 6: (a). C4 validation loss of different models at different scale.\n(b). Scaling curves at different sparsity with 151M activated parameters. Each line represents the same model sparsity; e.g., 20K indicates that approximately one out of every 20,000 values will be activated. The loss decreases linearly as the sparse parameters increase exponentially. (c). Inference time for UltraMem and MoE with 1.6B activated parameters. The batch size is 512, sequence length is 1, and key/value cache length is 2048. With fixed activation parameters, UltraMem\u2019s inference time remains nearly constant as sparse parameters increase, while MoE\u2019s inference time increases significantly.", "description": "Figure 6 presents a comparison of UltraMem and MoE model performance across different scales and sparsity levels.  Subfigure (a) shows the validation loss on the C4 dataset for various model sizes, demonstrating the scaling behavior. Subfigure (b) focuses on the impact of sparsity, showing that while UltraMem's loss decreases linearly with exponentially increasing parameters,  MoE's performance is significantly affected by sparsity. Subfigure (c) compares the inference time of UltraMem and MoE with 1.6B activated parameters under different sparsity levels, highlighting that UltraMem exhibits significantly better inference speed at larger sparsity.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2411.12364/x16.png", "caption": "(a) Inference time", "description": "This figure shows the inference time of different models with varying batch sizes.  The x-axis represents the batch size, and the y-axis represents the inference time in milliseconds.  The models compared include a dense Transformer, MoE (Mixture of Experts), and UltraMem. The figure illustrates that UltraMem achieves significantly faster inference times compared to MoE, particularly at smaller batch sizes, while maintaining performance comparable to the dense model.", "section": "1 INTRODUCTION"}, {"figure_path": "https://arxiv.org/html/2411.12364/x17.png", "caption": "(b) Memory access", "description": "The figure shows the memory access in gigabytes (GB) of three different model architectures: a dense Transformer model, a Mixture of Experts (MoE) model, and an Ultra-Sparse Memory Network (UltraMem) model.  The x-axis represents the batch size, showing how memory usage scales with increasing batch size. The plot demonstrates that UltraMem has significantly lower memory access compared to MoE, especially at smaller batch sizes, while remaining comparable to a dense model.", "section": "1 INTRODUCTION"}, {"figure_path": "https://arxiv.org/html/2411.12364/x18.png", "caption": "Figure 7: Inference time and memory access of Transformer, MoE and UltraMem. We ensured that three models have the same computation, and MoE and UltraMem have the same parameters. The x-axis and y-axis are both plotted on a logarithmic scale. The sequence length is 1 because during inference, we can only predict one token at a time, and the key/value cache length is 2048. The modes run on the A100-SXM.", "description": "This figure compares the inference time and memory access of three different model architectures: a standard Transformer, a Mixture of Experts (MoE) model, and the proposed Ultra-Sparse Memory Network (UltraMem).  All three models were designed to have the same computational complexity and number of parameters.  The x-axis shows the batch size (on a logarithmic scale), and the y-axis shows either inference time or memory access (also on a logarithmic scale). The sequence length is fixed at 1 because, during inference, only one token can be predicted at a time. The key/value cache length is set to 2048.  The experiments were conducted on an A100-SXM GPU. The graph visually demonstrates UltraMem's superior performance in terms of both inference speed and memory efficiency, especially as batch size increases.", "section": "QUANTITATIVE ANALYSIS WHY ULTRAMEM INSTEAD OF MOE"}, {"figure_path": "https://arxiv.org/html/2411.12364/x19.png", "caption": "(a) Number-wise partitioning", "description": "This figure illustrates the number-wise partitioning strategy used in the UltraMem model for efficient parallel processing of large memory tables.  In number-wise partitioning, the memory table is divided into multiple parts, each assigned to a different device or GPU. The indices are initially distributed across all devices using an all-to-all communication operation. After the lookup operation retrieves the necessary values from the memory table partitions, the results are sent back to the original devices for the weighted sum pooling operation.  This method optimizes memory access and communication efficiency during training by distributing the workload.", "section": "C MEGATRON SUPPORT FOR TRAINING EFFICIENCY"}, {"figure_path": "https://arxiv.org/html/2411.12364/x20.png", "caption": "(b) Dimension-wise partitioning", "description": "This figure illustrates the process of dimension-wise partitioning of the memory table across multiple devices in a parallel training setup.  The memory table is initially partitioned across devices.  An all-gather operation is performed on indices, meaning each device obtains all indices from all devices. Subsequently, a lookup operation is carried out on all devices using the gathered indices, which retrieves the values from the corresponding partitions of the memory table. Finally, a dimension-wise reduction is performed, generating a final aggregated result that combines contributions from all the devices.  This approach effectively leverages distributed computation to handle large memory tables in parallel training.", "section": "C MEGATRON SUPPORT FOR TRAINING EFFICIENCY"}, {"figure_path": "https://arxiv.org/html/2411.12364/x21.png", "caption": "Figure 8: Process of Number-wise partitioning and Dimension-wise-partitioning. The weighted sum pooling step is omitted in the diagram.", "description": "This figure illustrates the data parallelism strategies used for the memory table in UltraMem, specifically focusing on the number-wise and dimension-wise partitioning methods.  Number-wise partitioning involves dividing the memory table among multiple devices, where each device handles a portion of the values.  An all-to-all communication step is shown to distribute indices across devices, followed by individual devices performing lookups and sending the results back.  Dimension-wise partitioning instead distributes the dimensions of the memory table across devices, requiring an all-gather step to collect all indices. Lookups are then performed, and the results are aggregated and sent back to each device. The weighted sum pooling operation, which is a crucial part of the memory table processing, is omitted for simplicity.", "section": "C MEGATRON SUPPORT FOR TRAINING EFFICIENCY"}, {"figure_path": "https://arxiv.org/html/2411.12364/x22.png", "caption": "Figure 9: Relationship between P and v_dim for communication volume of number-wise / dimension-wise equals 1, the shaded area is number-wise / dimension-wise greater than 1", "description": "Figure 9 illustrates the trade-off between the number of processors (P) and the value dimension (v_dim) when choosing between number-wise and dimension-wise partitioning strategies for the memory table in UltraMem.  The communication volume is compared for both methods, showing that number-wise partitioning is preferable in the unshaded area (number-wise volume < dimension-wise volume), while dimension-wise partitioning becomes advantageous in the shaded region.  Different top-m values impact this tradeoff, altering the regions of preference for each partitioning strategy.  This figure helps guide the selection of the optimal partitioning strategy based on the system configuration (number of processors and value dimension).", "section": "D NUMBER-WISE AND DIMENSION-WISE PARTITION DETAILS"}, {"figure_path": "https://arxiv.org/html/2411.12364/x23.png", "caption": "(a) Training perplexity", "description": "This figure displays the training perplexity of different models over the course of training. The x-axis represents the number of tokens processed during training (in billions), while the y-axis shows the training perplexity.  Different lines represent different models, allowing for a comparison of their training performance. Lower perplexity indicates better model performance. The plot shows the learning curves, demonstrating how the perplexity decreases (ideally) over time as the model learns from the data.", "section": "5.4 Ablation"}, {"figure_path": "https://arxiv.org/html/2411.12364/x24.png", "caption": "(b) Top1 score", "description": "The graph displays the top-1 score achieved during the training process of the UltraMem model. The top-1 score represents the highest score among all retrieved keys in the memory layer, indicating the relevance of the retrieved values. The x-axis denotes the number of tokens processed during training, while the y-axis shows the top-1 score.  The plot illustrates how the top-1 score evolves throughout training, providing insights into the model's ability to retrieve relevant information from its memory.", "section": "5.4 Ablation"}, {"figure_path": "https://arxiv.org/html/2411.12364/x25.png", "caption": "(c) UltraMem output standard deviation", "description": "This figure shows the standard deviation of the outputs from the last layer of the UltraMem model during training.  It helps to visualize the stability and consistency of the model's predictions over time and across different parts of the training data. Lower standard deviation indicates more stable and consistent predictions.", "section": "Quantitative Analysis Why UltraMem Instead of MoE"}]