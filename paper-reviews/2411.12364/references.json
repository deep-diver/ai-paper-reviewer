{"references": [{"fullname_first_author": "Noam Shazeer", "paper_title": "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer", "publication_date": "2017-01-17", "reason": "This paper introduced the Mixture of Experts (MoE) layer, a crucial concept that the current paper builds upon and improves."}, {"fullname_first_author": "Guillaume Lample", "paper_title": "Large memory layers with product keys", "publication_date": "2019", "reason": "This paper introduced the Product Key Memory (PKM) architecture, which serves as the foundation for the UltraMem architecture proposed in the current paper."}, {"fullname_first_author": "Alec Radford", "paper_title": "Language models are unsupervised multitask learners", "publication_date": "2019", "reason": "This paper is highly influential in the field of large language models (LLMs), and its concepts are relevant to the current paper's focus on improving LLM performance."}, {"fullname_first_author": "Tom B Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-05-14", "reason": "This paper demonstrated the impressive few-shot learning capabilities of LLMs, providing a strong justification for further research on scaling LLMs, which is the focus of the current paper."}, {"fullname_first_author": "Mohammad Shoeybi", "paper_title": "Megatron-lm: Training multi-billion parameter language models using model parallelism", "publication_date": "2019-09-08", "reason": "This paper introduced efficient training techniques for large language models, which are highly relevant to the current paper's work on training large-scale UltraMem models."}]}