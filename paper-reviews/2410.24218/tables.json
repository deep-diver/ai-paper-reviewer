[{"content": "| Env | Image Observation | Instruction Manual | Text State Description |\n|---|---|---|---|\n| HomeGrid | Yes | No | No |\n| AlfWorld | No | No | Yes |\n| Messenger | No | Yes | No |\n| MetaWorld | No | No | No |", "caption": "Table 1: Information provided by each environment.", "description": "This table details the type of information each environment provides to the agents, regardless of whether they are trained with language or not.  It shows whether each environment offers image observation data, instruction manuals, text descriptions, and state information, to provide a comprehensive view of available sensory input for agents during both training and testing phases.", "section": "3.3 Environments"}, {"content": "| Env | # Hind Templates | # Fore Templates | # AUG |\n|---|---|---|---| \n| HomeGrid | 20 | 9 | 70 |\n| AlfWorld | 4 | 4 | 200 |\n| Messenger | 4 | 4 | 80 |\n| MetaWorld | 2 | 6 | 180 |", "caption": "Table 2: Number of templates and augmented sentences for each environment, where \u2019# Hind Templates\u2019 refers to the number of hindsight templates, \u2019# Fore Templates\u2019 refers to the number of foresight templates, and \u2019# AUG\u2019 refers to the number of GPT-augmented sentences per template.", "description": "This table shows the number of hand-crafted templates for hindsight and foresight feedback used in each of the four simulated environments for the reinforcement learning experiments.  It also indicates the number of augmented sentences generated by GPT-4 for each template, increasing the diversity of language feedback used to train the agents.", "section": "3.2 Language Feedback: Informativeness and Diversity"}, {"content": "| HomeGrid Env on RQ 1                                  | Aligned Eval | Online GPT Eval |\n|--------------------------------------------------------|---------------|-----------------|\n| **Training Language**                               | **Aligned Eval** | **Online GPT Eval** |\n| No Lang                                              | 0.235          | 0.212            |\n| Template H                                           | 0.260          | 0.246            |\n| Template F                                           | 0.305          | 0.262            |\n| Template H + F                                       | 0.325          | 0.285            |\n| GPT-augmented H + F                                  | 0.472          | 0.442            |\n| **Messenger Env on RQ 2 (20 Shots)**                 |                |                  |\n| **Training Language**                               | **Aligned Adapt & Eval** | **Online GPT Eval** |\n| No Lang                                              | 0.323          | 0.270            |\n| GPT-augmented H                                     | 0.450          | 0.378            |\n| GPT-augmented F                                     | 0.512          | 0.464            |\n| GPT-augmented H + F                                  | 0.623          | 0.608            |", "caption": "Table 3: Comparison of agents\u2019 performance adapted (for RQ 2) and evaluated with aligned language type in HomeGrid environment on RQ 1 and Messenger environment on RQ 2. \u2018Aligned (Adapt &) Eval\u2019 refers to (adaptation &) evaluation with same type of language in training and \u2018Online GPT Eval\u2019 refers to online GPT evaluation (results in Section 6.2). The results show that GPT-augmented Hindsight + Foresight evaluated with online GPT still outperforms other training settings even with aligned language evaluation, indicating higher language informativeness and diversity enhance intrinsic task understanding.", "description": "This table compares the performance of agents trained with different types of language feedback (no language, template-based hindsight, template-based foresight, template-based hindsight and foresight, GPT-augmented hindsight and foresight) when evaluated using either the same type of language used during training or online GPT-generated language.  The results demonstrate the superior performance of agents trained with GPT-augmented hindsight and foresight language feedback, regardless of the evaluation language used. This highlights the importance of informative and diverse language for improving agent performance and intrinsic task understanding.", "section": "6 Experiment"}, {"content": "| Mistake Type | No Lang (%) | Template Hindsight (%) |\n|---|---|---|\n| Navigation | 37.6 \u00b1 0.3 | 46.2 \u00b1 0.2 |\n| Object Pick/Drop | 37.4 \u00b1 2.5 | 41.8 \u00b1 1.6 |\n| Bin manipulation | 23.5 \u00b1 1.2 | 24.8 \u00b1 0.9 |", "caption": "Table 4: Comparison of performance between No Language Agent and Template Hindsight Agent on different Mistake Types.", "description": "This table presents a comparison of the performance of two agent types, 'No Language Agent' and 'Template Hindsight Agent', across three distinct error scenarios in the HomeGrid environment.  The error scenarios are: navigation mistakes (incorrect directional movement), object pick/drop mistakes (incorrectly picking up or dropping an object), and bin manipulation mistakes (incorrect interaction with bins). The table quantifies the success rate (percentage) of each agent in each error scenario, demonstrating the impact of hindsight language feedback on correcting specific error types.", "section": "6.3 Ablation Study"}, {"content": "| Hyperparameters | Value |\n|---|---| \n| Number of transformer layers | 3 |\n| Number of attention heads | 1 |\n| Embedding dimension | 128 |\n| Nonlinearity function | ReLU |\n| Batch size | 64 |\n| Context length K | 10 |\n| Return-to-go conditioning | 1.5 |\n| Dropout | 0.1 |\n| Optimizer | AdamW |\n| Learning Rate | 1e<sup>-4</sup> |\n| Grad norm clip | 0.25 |\n| Weight decay | 1e<sup>-4</sup> |\n| Learning rate decay | Linear warmup for first 1e<sup>5</sup> training steps |", "caption": "Table 5: Hyperparameters of Language-Teachable Decision Transformer for HomeGrid experiments.", "description": "This table lists the hyperparameters used in training the Language-Teachable Decision Transformer model for the HomeGrid environment.  It details the settings for various aspects of the model architecture and training process, such as the number of transformer layers, attention heads, embedding dimensions, activation functions, batch size, context length, optimizer, learning rate, and other regularization parameters. These hyperparameters were tuned to optimize the model's performance on the HomeGrid tasks. The table provides a comprehensive overview of the specific configurations used for this particular experiment.", "section": "G Models and Training"}, {"content": "| Hyperparameters | Value |\n|---|---| \n| Number of transformer layers | 3 |\n| Number of attention heads | 1 |\n| Embedding dimension | 128 |\n| Nonlinearity function | ReLU |\n| Batch size | 64 |\n| Context length K | 10 |\n| Return-to-go conditioning | 1.5 |\n| Dropout | 0.1 |\n| Optimizer | AdamW |\n| Learning Rate | 1e<sup>-3</sup> |\n| Grad norm clip | 0.25 |\n| Weight decay | 1e<sup>-4</sup> |\n| Learning rate decay | Cosine Annealing with minimum lr=1e<sup>-5</sup> |", "caption": "Table 6: Hyperparameters of Language-Teachable Decision Transformer for ALFWorld experiments.", "description": "This table lists the hyperparameters used for training the Language-Teachable Decision Transformer model on the ALFWorld environment.  It details the settings for various aspects of the model's architecture and training process, including the number of transformer layers, attention heads, embedding dimension, nonlinearity function, batch size, context length (K), return-to-go conditioning, dropout rate, optimizer, learning rate, gradient norm clipping, weight decay, and learning rate decay schedule.  These hyperparameters are crucial in determining the model's performance and efficiency during training.", "section": "G Models and Training"}, {"content": "| Hyperparameters | Value |\n|---|---| \n| Number of transformer layers | 5 |\n| Number of attention heads | 2 |\n| Embedding dimension | 128 |\n| Nonlinearity function | ReLU |\n| Batch size | 128 for pertaining and 1 for adaptation |\n| Context length K | 10 |\n| Return-to-go conditioning | 1.5 |\n| Dropout | 0.1 |\n| Optimizer | AdamW |\n| Learning Rate | 1e\u207b\u00b3 for pretraining and 1e\u207b\u2074 for adaptation |\n| Grad norm clip | 0.25 |\n| Weight decay | 1e\u207b\u2074 |\n| Learning rate decay | Linear warmup for first 1e\u2075 training steps |", "caption": "Table 7: Hyperparameters of Language-Teachable Decision Transformer for Messenger experiments.", "description": "This table lists the hyperparameters used to configure the Language-Teachable Decision Transformer model during the Messenger experiments.  It details the settings for various aspects of the model's architecture and training process, such as the number of transformer layers, attention heads, embedding dimensions, optimizer used, learning rate, and more. These hyperparameters are crucial for optimizing the model's performance on the Messenger task.", "section": "G Models and Training"}, {"content": "| Hyperparameters | Value |\n|---|---| \n| Number of transformer layers | 5 |\n| Number of attention heads | 2 |\n| Embedding dimension | 256 |\n| Nonlinearity function | ReLU |\n| Batch size | 128 for pertaining and 5 for adaptation |\n| Context length  K | 12 |\n| Return-to-go conditioning | 20 |\n| Return scale | 10 |\n| Dropout | 0.1 |\n| Optimizer | AdamW |\n| Learning Rate | 1e-5 for pertaining and 1e-6 for adaptation |\n| Weight decay | 1e-4 |\n| Learning rate decay | Linear warmup for first 1e5 training steps |", "caption": "Table 8: Hyperparameters of Language-Teachable Decision Transformer for MetaWorld experiments.", "description": "This table lists the hyperparameters used for training the Language-Teachable Decision Transformer model on the MetaWorld environment.  It details the settings for various parameters that control the model's architecture, training process, and optimization strategy.  These parameters include those related to the transformer network itself (e.g., number of layers, attention heads, embedding dimension), the training process (e.g., batch size, learning rate, optimizer), and regularization techniques (e.g., dropout, weight decay).  The specific values chosen for each hyperparameter are crucial for the model's performance and generalization ability on the MetaWorld tasks.", "section": "G Models and Training"}]