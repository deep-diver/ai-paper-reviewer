[{"figure_path": "https://arxiv.org/html/2410.24218/x1.png", "caption": "Figure 1: An overview of four environments used for experiments. It shows tasks to be learned in each environment; examples of hindsight (marked H) and foresight (F) language feedback (next to the gear icon are hand-crafted templates and next to the GPT icon are GPT-4 generated feedback); as well as low-level actions in each environment.", "description": "This figure provides a visual overview of the four experimental environments used in the paper: HomeGrid, ALFWorld, Messenger, and MetaWorld.  For each environment, it displays:\n\n1. **The task(s) to be learned:**  A brief description of the goal the agent needs to achieve in each environment.\n2. **Examples of language feedback:**  Illustrations of both hand-crafted and GPT-4 generated language feedback, categorized as either 'hindsight' (comments on past actions) or 'foresight' (guidance for future actions).  The hand-crafted templates are represented by the gear icon, while the GPT-4 generated feedback is indicated by the GPT icon. \n3. **Low-level actions:** A list of the basic actions the agent can take within each specific environment to interact with it and achieve the tasks. This provides context for understanding how the language feedback influences the agent's actions.\n\nThe figure aims to show the diversity of tasks and the different types of language used to guide agent learning in different settings.", "section": "3.3 Environments"}, {"figure_path": "https://arxiv.org/html/2410.24218/x2.png", "caption": "Figure 2: A demonstration of hindsight and foresight language feedback generation. In our framework, the agent \u03c0\ud835\udf0b\\piitalic_\u03c0 executes the trajectory, while the expert agent \u03c0\u2217superscript\ud835\udf0b\\pi^{*}italic_\u03c0 start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT, with access to privileged ground truth knowledge, is used solely to provide information for generating language feedback to \u03c0\ud835\udf0b\\piitalic_\u03c0. At time step t\ud835\udc61titalic_t, hindsight language is generated by comparing the agent\u2019s action at\u22121subscript\ud835\udc4e\ud835\udc611a_{t-1}italic_a start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT with the expert agent\u2019s action at\u22121\u2217superscriptsubscript\ud835\udc4e\ud835\udc611a_{t-1}^{*}italic_a start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT, whereas foresight language is generated by referring to the expert agent\u2019s action at\u2217superscriptsubscript\ud835\udc4e\ud835\udc61a_{t}^{*}italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT to guide the agent on the next step. To increase the diversity of language feedback, we construct a pool of language templates comprising GPT-augmented languages, and sample candidate instructions as online language feedback.", "description": "This figure illustrates the process of generating both hindsight and foresight language feedback within a reinforcement learning framework.  An agent (\u03c0) interacts with an environment, taking actions.  Simultaneously, an expert agent (\u03c0*) with complete knowledge of the environment's state generates feedback based on the agent's actions. Hindsight feedback comments on the agent's past action at time t-1, by comparing it to the expert agent's corresponding action at t-1. Foresight feedback, on the other hand, guides the agent's future action at time t by suggesting an action based on the expert agent's action at time t. To enhance the diversity of feedback, the system employs a pool of GPT-augmented language templates, randomly selecting one to deliver instructions.", "section": "Data Generation"}, {"figure_path": "https://arxiv.org/html/2410.24218/x3.png", "caption": "Figure 3: Language-Teachable Decision Transformer.", "description": "The Language-Teachable Decision Transformer (LTDT) architecture takes as input a sequence of states, rewards, actions, and language feedback. The task description is provided at the beginning of the sequence.  All inputs are embedded and then processed by a causal transformer, which maintains the order of the sequence. The output of the transformer predicts the next action, conditioned on the prior sequence.", "section": "5 Model"}, {"figure_path": "https://arxiv.org/html/2410.24218/x4.png", "caption": "Figure 4: Comparison of agent performance in four environments (averaged across 100 seeds in each environment) under varying levels of language feedback informativeness and diversity. Agents trained with more informative language feedback exhibit progressively higher performance. Furthermore, given the same informativeness (Hindsight + Foresight), increasing diversity with the GPT-augmented language pool leads to the highest performance.", "description": "This figure displays the performance of reinforcement learning agents across four distinct environments (HomeGrid, ALFWorld, Messenger, and MetaWorld).  The performance is evaluated under different conditions of language feedback: no language, only foresight language, only hindsight language, both hindsight and foresight using hand-crafted templates, and finally both hindsight and foresight using GPT-augmented language templates.  The results demonstrate that agents trained with increasingly more informative language feedback (hindsight and foresight being most informative) achieve higher performance. Furthermore, when comparing agents with the same level of informativeness (hindsight + foresight), the agents trained with the diverse GPT-generated language templates significantly outperformed those trained with hand-crafted templates, highlighting the positive impact of language diversity on agent learning.", "section": "6 Experiment"}, {"figure_path": "https://arxiv.org/html/2410.24218/x5.png", "caption": "Figure 5: Comparison of agent performance on unseen tasks in four environments (averaged across 100 seeds in each environment) under varying language informativeness in agent pre-training. Agent trained with more informative language adapts to new tasks faster and better.", "description": "This figure displays the performance of agents pre-trained with varying levels of language informativeness when adapting to unseen tasks.  Four different environments (HomeGrid, ALFWorld, Messenger, and MetaWorld) were used, with results averaged across 100 random seeds for each.  The agents were pre-trained using either no language, hindsight language, foresight language, or both.  The x-axis represents the number of shots (5, 10, or 20) provided during the adaptation phase, and the y-axis indicates the average reward achieved. The results clearly demonstrate that pre-training with more informative language (hindsight and foresight) leads to significantly better adaptation performance on unseen tasks, outperforming agents trained with less informative feedback.", "section": "6 Experiment"}, {"figure_path": "https://arxiv.org/html/2410.24218/x6.png", "caption": "Figure 6: Efficiency gain vs. task difficulty. We fit the scatter plots with a second-degree polynomial to visualize the overall trend. As task difficulty increases, the general trend of the efficiency gain is to rise initially and then decline, suggesting: (1) for tasks that are too easy or too hard, language feedback does not improve efficiency; (2) language feedback is most helpful in increasing efficiency for moderate tasks.", "description": "This figure shows the relationship between task difficulty and the efficiency gain achieved by using language feedback in reinforcement learning.  The x-axis represents task difficulty, with easier tasks on the left and harder tasks on the right. Task difficulty is measured by the success rate of agents *without* language feedback.  The y-axis shows the efficiency gain, which is calculated as the difference in efficiency between agents trained *with* informative and diverse language feedback and agents trained *without* any language feedback. Efficiency is measured by a path-weighted reward. The plot shows that the efficiency gain increases initially as task difficulty rises, reaching a peak at a moderate level of difficulty.  Beyond that moderate point, the efficiency gain begins to decrease as tasks become harder. This suggests that language feedback is most beneficial for tasks of moderate difficulty.  For very easy tasks, language feedback provides little additional benefit, and for very hard tasks, the challenges may be too significant for language feedback to substantially improve performance.", "section": "6.3 Ablation Study"}, {"figure_path": "https://arxiv.org/html/2410.24218/x7.png", "caption": "Figure 7: Performance vs. language frequency. Agents perform better with more frequent language feedback across four environments.", "description": "This figure displays the results of an experiment comparing the performance of reinforcement learning agents trained with varying frequencies of language feedback. The x-axis represents the percentage of timesteps during training where language feedback was provided, ranging from 0% to 100%. The y-axis represents the average reward achieved by the agents across four different environments (HomeGrid, ALFWorld, Messenger, and MetaWorld).  The graph shows a positive correlation between language feedback frequency and agent performance across all four environments, indicating that more frequent feedback leads to better learning outcomes.  The results suggest that continuous interaction and guidance, through frequent language feedback, significantly benefits the learning process of embodied reinforcement learning agents.", "section": "6.3 Ablation Study"}, {"figure_path": "https://arxiv.org/html/2410.24218/x8.png", "caption": "Figure 8: We investigate two special evaluation settings: (1) no language feedback is provided during evaluation and (2) disturbed language feedback is given at every step. Results show that agents trained with the GPT-augmented language still outperform the no-language agent (the black dotted line) in the disturbed setting, and also achieve better performance in some environments while no language is given.", "description": "This figure displays the results of an ablation study that investigates the impact of corrupted language feedback on agent performance. Two scenarios are considered: (1) no language feedback is provided during evaluation and (2) at each step, disturbed language feedback is given. The results demonstrate that agents trained with GPT-augmented language consistently outperform agents trained without any language, even when dealing with disturbed feedback.  Interestingly, in some environments, the GPT-augmented agents still perform better even when no feedback is given, highlighting the robustness and effectiveness of this language training approach.", "section": "6.3 Ablation Study"}, {"figure_path": "https://arxiv.org/html/2410.24218/x27.png", "caption": "Figure 9: In the Messenger environment, when trained with more diverse foresight and hindsight languages, the agents can perform better than those trained without languages. Furthermore, agents trained with more informative languages demonstrate stronger performance.", "description": "This figure displays the results of an experiment conducted in the Messenger environment, which is a grid world where an agent must retrieve a message from one entity and deliver it to another, avoiding enemies.  The experiment compared the performance of agents trained with varying degrees of informativeness and diversity in their language feedback, showing that agents trained with more diverse and informative language (both foresight and hindsight) perform significantly better than those trained without language.  The graph shows reward performance for agents trained under four language conditions: no language, GPT-augmented hindsight only, GPT-augmented foresight only, and GPT-augmented hindsight and foresight together. The combined hindsight and foresight training results in the best performance, highlighting the importance of both types of feedback for improving agents' ability to learn and perform the task.", "section": "6.2 Experimental Results"}, {"figure_path": "https://arxiv.org/html/2410.24218/x28.png", "caption": "Figure 10: Examples for language feedback generated by online GPT in evaluation.", "description": "Figure 10 presents three examples illustrating how the online GPT model generates language feedback during evaluation.  In the first example, both hindsight (commenting on past actions) and foresight (guidance for future actions) information are combined into a single, fluent sentence. The second example shows GPT prioritizing foresight feedback and omitting the hindsight feedback. The third example demonstrates a scenario where GPT chooses not to provide feedback because it judges that the agent does not currently need assistance.", "section": "H Examples for Language Feedback in Evaluation"}]