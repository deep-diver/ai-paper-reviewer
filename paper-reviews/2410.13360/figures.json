[{"figure_path": "2410.13360/figures/figures_1_0.png", "caption": "Figure 1: Introduce some user-specific concepts to our RAP-LLaVA, it can remember them and achieve excellent performance in a variety of personalized multimodal generation tasks.", "description": "The figure illustrates the RAP-LLaVA framework, showcasing how user-specific concepts are remembered and used for personalized multimodal generation.", "section": "Introduction"}, {"figure_path": "2410.13360/figures/figures_4_0.png", "caption": "Figure 2: Retrieval-Augmented Personalization Framework. Region-of-interest detected by an open world detector are used to retrieve concepts from the database. The images and accompanying information of the retrieved concepts are then integrated into the input for the MLLM.", "description": "The figure illustrates the Retrieval Augmented Personalization (RAP) framework, showing how region-of-interest detection, concept retrieval, and multimodal language model integration work together for personalized responses.", "section": "3 RETRIEVAL AUGMENTED PERSONALIZATION"}, {"figure_path": "2410.13360/figures/figures_5_0.png", "caption": "Figure 3: Our Pipeline for Data Collection. We first crop the target concept from the image based on the dataset annotations and then query Gemini to generate its personalized description. We also apply data augmentation to diversify these cropped images. Then we combine them with the original image to derive a series of instructions and answers from Gemini.", "description": "The figure illustrates the pipeline used for data collection to create a dataset for training personalized MLLMs, involving image cropping, Gemini-based description generation, augmentation, and instruction/answer pairing.", "section": "3.2 PERSONALIZATION DATASET"}]