{"references": [{" publication_date": "2024", "fullname_first_author": "Yuval Alaluf", "paper_title": "MyVLM: Personalizing VLMs for user-specific queries", "reason": "MyVLM is frequently mentioned in the paper as a comparative method for personalizing vision-language models (VLMs).  Its inclusion as a keypoint in both the introduction and related work sections highlights its significance in the field of MLLM personalization.  The paper's comparison with MyVLM provides crucial context for evaluating the proposed RAP framework's advancements and contributions to the area of personalization in MLLMs.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Akari Asai", "paper_title": "Self-RAG: Learning to retrieve, generate, and critique through self-reflection", "reason": "This paper is cited in the \"Related Work\" section under the subheading \"Retrieval Augmented Generation.\"  Its focus on retrieval-augmented generation (RAG) aligns directly with the core methodology of the current paper. The mention in the \"Related Work\" section suggests that this paper is crucial to understanding the broader context of RAG methods and their relevance to the paper's proposed framework.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Andreas Blattmann", "paper_title": "Retrieval-augmented diffusion models", "reason": "This paper is mentioned in the \"Related Work\" section, specifically under the subheading discussing personalization methods of MLLMs. The relevance of this citation lies in its exploration of retrieval-augmented methods for image generation, which is directly related to the goals of the current paper in personalizing MLLMs.  The mention in the \"Related Work\" section establishes this paper's contribution to the broader context.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Wenhu Chen", "paper_title": "MURAG: Multimodal retrieval-augmented generator for open question answering over images and text", "reason": "This paper is cited in the \"Related Work\" section, focusing on the use of multimodal knowledge to augment language generation. The connection between this paper and the current research lies in the intersection of multimodal information and language generation, which is central to the RAP framework. Its citation in the \"Related Work\" section highlights its relevance to the broader literature review.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Yang Chen", "paper_title": "Can pre-trained vision and language models answer visual information-seeking questions?", "reason": "This citation appears in the \"Related Work\" section, under the subheading on multimodal large language models.  The paper's relevance stems from its exploration of the capabilities of pre-trained vision and language models in handling visual information-seeking questions. This theme directly relates to the current research on personalizing MLLMs to handle complex tasks such as visual question answering.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Tianheng Cheng", "paper_title": "YOLO-World: Real-time open-vocabulary object detection", "reason": "This paper is cited in the \"Experiment\" section under \"Implementation Details.\"  The choice of YOLO-Worldv2 as a detector in the implementation signifies its importance in the overall methodological framework.  The detector plays a crucial role in the RAP framework by identifying regions of interest for the retrieval process.", "section_number": 4}, {" publication_date": "2020", "fullname_first_author": "Achal Dave", "paper_title": "TAO: A large-scale benchmark for tracking any object", "reason": "This dataset (TAO) is explicitly mentioned in the \"Experiment\" section and is part of the datasets used to create the personalized training and evaluation datasets for MLLMs. The inclusion of TAO in the dataset creation pipeline demonstrates its importance for achieving the robust results presented in the paper. The use of TAO indicates that the dataset is comprehensive and supports the generalizability claims of RAP.", "section_number": 4}, {" publication_date": "2022", "fullname_first_author": "Rinon Gal", "paper_title": "An image is worth one word: Personalizing text-to-image generation using textual inversion", "reason": "This paper is cited in \"Related Work\" to show an example of personalizing MLLMs, although the focus is different from this work. This citation helps the authors establish the state-of-the-art in the field and allows them to position their contribution in relation to prior work and emphasizes the challenges associated with other methods.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Gemini-Team", "paper_title": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context", "reason": "Gemini-1.5 is cited multiple times in the \"Experiment\" section, indicating its importance for both data generation and as a model for comparison. The use of Gemini-1.5 illustrates that the dataset is comprehensive and supports the generalizability claims of RAP.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Qiushan Guo", "paper_title": "RegionGPT: Towards region understanding vision language model", "reason": "This paper is cited in \"Related Work\" under \"Multimodal Large Language Models.\"  It highlights the advancements in MLLMs which use region-level instruction datasets for enhancing fine-grained understanding.  This provides context for understanding the advancements in MLLMs and the specific focus of this paper on personalized multimodal generation.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Jiaming Han", "paper_title": "OneLLM: One framework to align all modalities with language", "reason": "This paper is cited in \"Related Work\" to demonstrate the advancements in multi-modal LLMs and the integration of visual and textual abilities.  This citation helps the authors set the context for the advancements made in multi-modal LLMs and positions the contribution of this paper in advancing the capability and effectiveness of MLLMs.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Edward J. Hu", "paper_title": "LoRA: Low-rank adaptation of large language models", "reason": "LoRA is used in the experimental setup and is mentioned in \"Implementation Details.\" It is employed to reduce the number of trainable parameters in the RAP-MLLMs during training, highlighting its role in improving efficiency and reducing computational costs. The impact of LoRA on the final model's performance is thus implicitly significant.", "section_number": 4}, {" publication_date": "2021", "fullname_first_author": "Jeff Johnson", "paper_title": "Billion-scale similarity search with GPUs", "reason": "This paper is referenced in the \"Experiment\" section under \"Implementation Details,\" specifically regarding the multimodal retriever used.  The choice of FAISS (Facebook AI Similarity Search) as the retriever implementation is crucial to the RAP framework's effectiveness, directly impacting the efficiency and speed of the retrieval step.", "section_number": 4}, {" publication_date": "2020", "fullname_first_author": "Vladimir Karpukhin", "paper_title": "Dense passage retrieval for open-domain question answering", "reason": "This paper, on Dense Passage Retrieval (DPR), is mentioned in the \"Related Work\" section under \"Retrieval Augmented Generation.\" It introduces the concept of DPR, which forms the foundation of knowledge retrieval methods. The mention of DPR in \"Related Work\" establishes its importance in the context of retrieval-augmented generation, which is relevant to the current paper's methodology.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Nupur Kumari", "paper_title": "Multi-concept customization of text-to-image diffusion", "reason": "This dataset (CustomConcept101) is explicitly mentioned in the \"Experiment\" section.  The dataset's incorporation into the training data pipeline highlights its role in supporting the broader claims about the RAP framework's ability to handle a wide range of concepts.  The selection of CustomConcept101 shows a focus on comprehensive, high-quality training data.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Jiaxuan Li", "paper_title": "EvCap: Retrieval-augmented image captioning with external visual-name memory for open-world comprehension", "reason": "This paper is referenced in the \"Related Work\" section. The reference to EvCap highlights its relevance as a retrieval-based approach to image captioning.  EvCap is used as a comparative method that demonstrates a related technique in the field of retrieval-augmented generation.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Weizhe Lin", "paper_title": "Fine-grained late-interaction multimodal retrieval for retrieval augmented visual question answering", "reason": "This paper is cited in the \"Related Work\" section, providing further context for the application of RAG techniques to multimodal tasks.  The reference to this paper in \"Related Work\" highlights the significant contributions in the use of retrieval methods for enhancing multimodal question answering.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Haotian Liu", "paper_title": "LLaVA: Visual instruction tuning", "reason": "LLaVA is a foundational model for the RAP framework. This paper is cited multiple times and extensively in the \"Experiment\" section, emphasizing the importance of LLaVA as the base model for creating RAP-LLaVA, and in the results analysis, showcasing the relative performance and benefits of RAP's methodology. The close relationship between LLaVA and RAP makes this citation essential.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Xiaoxiao Long", "paper_title": "Wonder3D: Single image to 3D using cross-domain diffusion", "reason": "This paper, on Wonder3D, is referenced in the \"Experiment\" section, showing its application in data augmentation for generating novel views of concepts. Data augmentation is mentioned as an important technique for creating the dataset used in this work, so this paper plays a significant role in the effectiveness of the proposed method.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Thao Nguyen", "paper_title": "Yo'LLaVA: Your personalized language and vision assistant", "reason": "Yo'LLaVA is frequently mentioned throughout the paper as a key comparative method in the field of MLLM personalization. The paper compares its proposed framework, RAP, with Yo'LLaVA, highlighting the advantages of RAP in terms of efficiency and scalability.  The comparison with Yo'LLaVA provides essential context for understanding the contribution and novel aspects of the RAP framework.", "section_number": 1}]}