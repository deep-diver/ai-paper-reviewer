[{"content": "| Model | Size | MMMU | MMMU-Pro | MMBench | MME | ChartQA | MMStar | MathVista | Average |\n|---|---|---|---|---|---|---|---|---|---| \n| DeepSeek-VL [29] | 7B | 35.4 | - | 73.5 | -/- | 59.1 | 37.1 | 36.1 | - |\n| VILA-1.5 [20] | 8B | 38.6 | - | 75.3 | 1634.9/- | - | 39.7 | - | - |\n| Cambrian-1 [43] | 8B | 42.7 | - | 75.9 | 1547.1/- | 73.3 | - | 49.0 | - |\n| InternLM-XComposer2 [7] | 7B | 41.1 | - | 77.6 | 2220.4 | 71.8 | 56.2 | 59.5 | - |\n| POINTS [26] | 7B | 51.4 | - | 78.0 | 2184.1 | - | 60.9 | 63.0 | - |\n| IXC-2.5 [55] | 7B | 42.9 | - | 79.4 | 2233.1 | 82.2 | 59.9 | **63.7** | - |\n| Bunny-LLaMA3 [12] | 8B | 43.4 | - | 77.2 | 1588.9/321.1 | - | - | 34.4 | - |\n| MM-1.5 [54] | 7B | 41.8 | - | - | 1514.9/346.4 | 78.6 | - | 47.6 | - |\n| MiniCPM-LLaMA3-V 2.5 [49] | 8B | 45.8 | 19.6 | 77.2 | 2024.6 | - | 51.8 | 54.3 | - |\n| MiniCPM-V-2.6 [50] | 7B | 49.8 | **27.2** | 78.0 | 2268.7 | - | 57.5 | 60.6 | - |\n| Qwen2-VL [38] | 7B | **53.7** | - | 81.0 | - | **83.0** | 60.7 | 61.4 | - |\n| Idefics3-LLaMA3 [14] | 8B | 46.6 | 22.9 | 77.5 | 1937.4 | 74.8 | 55.9 | 58.4 | 48.1 |\n| Ovis1.5-LLaMA3 [31] | 8B | 48.3 | 23.6 | 76.6 | 1948.5 | 76.4 | 57.3 | 63.0 | 49.4 |\n| LLaVA-NeXT-LLaMA3 [22] | 8B | 36.9 | 13.2 | 72.3 | 1611.1/346.0 | 69.4 | 43.1 | 45.9 | 40.2 |\n| + Multi-Agent | 8B | 40.8 | 17.8 | 77.6 | 1603.7/469.3 | 74.6 | 52.6 | 47.4 | 44.5 |\n| + Iterative DPO (**Insight-V-LLaVA**) | 8B | 42.0 | 21.0 | 81.7 | 1583.9/485.4 | 77.4 | 57.4 | 49.8 | **47.2** (+7.0) |\n| Our Base Model | 7B | 47.1 | 22.6 | 81.3 | 1573.7/482.5 | 75.7 | 57.0 | 56.9 | 48.7 |\n| + Multi-Agent | 7B | 49.7 | 23.8 | 82.2 | 1662.2/**629.3**/ | 81.2 | 58.6 | 58.7 | 50.7 |\n| + Iterative DPO (**Insight-V**) | 7B | 50.2 | 24.9 | **82.3** | **1685.1**/627.0 | 81.5 | **61.5** | 59.9 | **51.6** (+2.9) |", "caption": "Table 1: Results on Visual Reasoning Tasks. We conduct evaluation experiments across 7 benchmarks, covering both general reasoning and task-specific reasoning assessments. Insight-V exhibits notable effectiveness and generalizability when applied to LLaVA-NeXT and our baseline model, surpassing other state-of-the-art MLLMs by a large margin.", "description": "Table 1 presents a comprehensive evaluation of various Multimodal Large Language Models (MLLMs) on seven visual reasoning benchmarks.  These benchmarks assess both general reasoning capabilities and task-specific skills. The table compares the performance of Insight-V (applied to both LLaVA-NeXT and a strong baseline MLLM) against several state-of-the-art MLLMs.  The results demonstrate that Insight-V significantly improves performance across all benchmarks, showcasing its effectiveness and generalizability in enhancing visual reasoning capabilities.", "section": "4. Experiments"}, {"content": "| Model | TextVQA | DocVQA | OCRBench | AI2D |\n|---|---|---|---|---|\n| LLaVA-NeXT-LLaMA3 | 65.2 | 78.2 | 553 | 71.5 |\n| + Multi-Agent | 68.9 | 81.8 | 631 | 75.7 |\n| + Iterative DPO (**Insight-V-LLaVA**) | **70.5** | **82.9** | **663** | **77.3** |\n| Our Base Model | 75.4 | 90.2 | 713 | 79.7 |\n| + Multi-Agent | **77.0** | 91.4 | **738** | **80.1** |\n| + Iterative DPO (**Insight-V**) | 76.8 | **91.5** | 735 | 79.8 |", "caption": "Table 2: Results on other multimodal benchmarks. Insight-V enhances reasoning capabilities without compromising general visual perception and even achieves improvements on benchmarks requiring perception ability more.", "description": "Table 2 presents the results of evaluating the Insight-V model on various multimodal benchmarks that assess different aspects of vision and language understanding.  The table shows that Insight-V improves performance on these benchmarks without negatively impacting its general visual perception capabilities.  In fact, Insight-V even enhances performance on some tasks which heavily rely on strong visual understanding. This highlights the model's ability to effectively integrate reasoning and perception in multimodal tasks.", "section": "4.2. Main Results on Visual Reasoning"}, {"content": "| Model | MMMU | ChartQA | MathVista | MMStar | Avg |\n|---|---|---|---|---|---| \n| Baseline | 47.1 | 75.7 | 56.9 | 57.0 | 59.2 |\n| Vanilla - Direct SFT | 47.0 | 79.2 | 57.6 | 58.4 | 60.6 |\n| Multi-Turn Supervised | 48.1 | 79.6 | 57.9 | 58.2 | 61.0 |\n| Summary Agent Only | 47.5 | 76.3 | 57.3 | 57.9 | 59.8 |\n| Multi-Agent | 49.7 | 81.2 | 58.7 | 58.6 | 62.1 |", "caption": "Table 3: Ablations on the Insight-V Design Choice. The multi-agent design outperforms other configurations, highlighting the critical role of reasoning and summarization decomposition.", "description": "This table presents ablation studies comparing the performance of Insight-V's multi-agent design against various alternative configurations.  It shows the average performance across multiple visual reasoning benchmarks (MMMU, ChartQA, MathVista, MMStar) for different model variations.  The variations include a baseline model, a model trained with direct supervised fine-tuning, a model incorporating a multi-agent system, and Insight-V which includes both the multi-agent system and iterative direct preference optimization. By comparing these results, the table highlights the substantial improvement and the crucial role of decomposing the visual reasoning task into distinct reasoning and summarization steps, which is a core feature of the Insight-V architecture.", "section": "4.3 Further Analysis"}, {"content": "| Model | MMMU | ChartQA | MathVista | MMStar | Avg |\n|---|---|---|---|---|---| \n| Insight-V (Multi-Agent) | 49.7 | 81.2 | 58.7 | 58.6 | 62.1 |\n| + RLAIF | 49.5 | 81.4 | 59.1 | 59.2 | 62.3 |\n| + DPO | 50.8 | 80.8 | 59.3 | 59.9 | 62.7 |\n| + Iterative DPO | 50.2 | 81.5 | 59.9 | 61.5 | 63.3 |", "caption": "Table 4: Ablations on the DPO training strategy. Iterative DPO progressively enhances the model\u2019s reasoning capabilities, leading to improved performance.", "description": "This table presents ablation studies on different DPO (Direct Preference Optimization) training strategies.  It compares the performance of a model trained with a standard DPO approach against one trained using iterative DPO.  The iterative DPO method repeatedly refines the model by generating new preference pairs at each iteration using the updated model, thus leading to a potentially more accurate reflection of human preferences. The table shows the impact of these different DPO strategies across multiple visual reasoning benchmarks (MMMU, ChartQA, MathVista, and MMStar), highlighting the improvement in performance achieved through iterative DPO.", "section": "4. Experiments"}]