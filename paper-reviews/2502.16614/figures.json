[{"figure_path": "https://arxiv.org/html/2502.16614/x1.png", "caption": "Figure 1: Illustration of the Basic Critique Evaluation and Advanced Critique Evaluation.", "description": "This figure illustrates the two evaluation protocols used in CodeCriticBench: basic and advanced critique evaluation.  The basic critique evaluation involves a simple correct/incorrect judgment with a justification. The advanced critique evaluation uses a more detailed, fine-grained evaluation checklist with scores (1-10) for various aspects of the AI assistant's response, leading to a comprehensive score.", "section": "3 CodeCriticBench"}, {"figure_path": "https://arxiv.org/html/2502.16614/x2.png", "caption": "Figure 2: Illustration of data collection process.", "description": "The figure illustrates the process of collecting data for the CodeCriticBench benchmark.  It begins with selecting algorithm problems from CodeForces, MBPP, and LiveCodeBench datasets.  These problems are then used to generate code using LLMs.  A bug insertion process is implemented, introducing various error types into the correct code generated by LLMs. These samples undergo filtering rounds: first a sandbox execution to confirm error triggering, then a manual review to confirm error alignment with the intended categories. For the code QA task, a rule-based filtering process first cleans the raw question-answer pairs collected from StackOverflow. Qwen2.5-72B is then used to generate new questions, while manual filtering and LLM-assisted reviews ensure high quality.", "section": "Data Collection"}, {"figure_path": "https://arxiv.org/html/2502.16614/x3.png", "caption": "Figure 3: Scaling law on basic critique evaluation (ACC) across models. \u201c*\u201d indicates an estimated parameter size.", "description": "This figure shows the relationship between the size of large language models (LLMs) and their accuracy in basic code critique tasks. The x-axis represents the number of parameters (model size) in billions, while the y-axis shows the accuracy (ACC) in percentage. Each point represents a specific LLM, and the size of the points may represent the model's relative performance. The plot indicates a general trend of improved accuracy as the model size increases. The '*' symbol is used to denote that the parameter size is estimated for certain models.", "section": "4.2 Main Results"}, {"figure_path": "https://arxiv.org/html/2502.16614/x4.png", "caption": "Figure 4: Comparison across different models on \u201cCode QA\u201d (Basic Critique Evaluation).", "description": "This figure displays a comparison of the performance of various models on the Code QA task, specifically focusing on the basic critique evaluation.  It showcases how different models, categorized by size, perform on the task of evaluating code correctness based on the accuracy metric.  The chart likely presents a bar graph, where each bar represents a model's performance, and models are grouped by parameter size.  It provides a visual representation of the effectiveness of different models in correctly identifying errors or assessing the accuracy of code provided in response to natural language questions.", "section": "4.2 Main Results"}, {"figure_path": "https://arxiv.org/html/2502.16614/x5.png", "caption": "Figure 5: Model performance (ACC) on different difficulty levels (Basic Critique Evaluation) .", "description": "This figure displays the accuracy (ACC) of various large language models (LLMs) in performing basic code critique tasks, categorized by the difficulty level of the tasks.  The difficulty levels (Easy, Medium, Hard) are determined by the percentage of LLMs that correctly answer the questions. The x-axis represents the different LLMs tested, and the y-axis shows the accuracy, providing a visual comparison of model performance across different difficulty levels.", "section": "4.2 Main Results"}, {"figure_path": "https://arxiv.org/html/2502.16614/x6.png", "caption": "Figure 6: Comparison of the accuracy of different models in identifying five common programming error types.", "description": "This figure visualizes the performance of various LLMs in identifying five common programming errors: Input Validation and Data Processing Errors, Performance Issues, Security Vulnerabilities, Type Errors, and Reference Errors.  The accuracy of each model in correctly identifying these errors is presented, allowing for a comparison across different models.  The results show that model accuracy generally improves with increased model size, reflecting the enhanced capabilities of larger models in identifying and classifying these types of programming errors.", "section": "4.2 Main Results"}, {"figure_path": "https://arxiv.org/html/2502.16614/x7.png", "caption": "Figure 7: Rating distribution of the CodeCriticBench.", "description": "This figure shows the distribution of ratings in the CodeCriticBench dataset.  It displays the number of code samples that received each rating (1-10), separately for correct and incorrect code solutions.  This visualization helps to understand the difficulty level and quality distribution of the benchmark problems.", "section": "3.3 Dataset Construction"}, {"figure_path": "https://arxiv.org/html/2502.16614/x8.png", "caption": "Figure 8: Experimental accuracy results of different models across various error types.", "description": "The figure shows the accuracy of various large language models in identifying different types of programming errors.  The x-axis represents the different error types (Performance Issues, Security Vulnerabilities, Reference Errors, etc.), while the y-axis shows the accuracy percentage.  Each bar represents a different model, with colors differentiating between open-source and closed-source models. The model's size (in terms of parameters) is also implicitly included by grouping similar sized models.  This allows for a comparison of accuracy across various error types and model sizes.", "section": "4.2 Main Results"}, {"figure_path": "https://arxiv.org/html/2502.16614/x9.png", "caption": "Figure 9: Comparison of ranking of model responses by three methods: basic critique, advanced critique and human evaluations.", "description": "This figure compares the ranking of large language models (LLMs) based on three different evaluation methods: basic critique, advanced critique, and human evaluation.  The x-axis represents different LLMs, and the y-axis shows their rank across the three methods.  A lower rank indicates better performance. The figure visually demonstrates whether the different evaluation methods yield consistent ranking results and helps understand the correlation between automated evaluation (basic and advanced critique) and human judgment in evaluating LLM code critique capabilities.  The goal is to assess the effectiveness and alignment of the automated critique evaluation methods with human assessment.", "section": "4 Experiments"}, {"figure_path": "https://arxiv.org/html/2502.16614/x10.png", "caption": "Figure 10: Example of correct case of code qa.", "description": "This figure shows a sample from the Code QA subset of the CodeCriticBench dataset. It shows an example of a correct answer provided by an LLM. The figure includes the question, the correct answer, the fine-grained evaluation checklists and associated labels, such as correctness and overall score.", "section": "3 CodeCriticBench"}, {"figure_path": "https://arxiv.org/html/2502.16614/x11.png", "caption": "Figure 11: Scoring results of QA pairs before and after applying critiques to refine the answers.", "description": "This figure displays the results of code question-answering (QA) pairs before and after applying critiques generated by LLMs. It illustrates how leveraging model-generated critiques can lead to improved scores for QA pairs by enhancing the answers.  The figure likely shows a comparison of scores (e.g., accuracy or a more nuanced evaluation metric) before and after the critique intervention. This visual representation demonstrates the effectiveness of the model's critique capability in improving the overall quality of the answers.", "section": "3. CodeCriticBench"}, {"figure_path": "https://arxiv.org/html/2502.16614/x12.png", "caption": "(a) The Code Gen subset.", "description": "This figure shows the distribution of ratings for the Code Generation subset of the CodeCriticBench dataset.  The x-axis represents the rating score (from 1 to 10), and the y-axis represents the number of code samples. The bars are separated by whether the code sample was correctly generated or not.  This visualization helps assess the quality and difficulty of the Code Generation tasks within the benchmark.", "section": "3.3 Dataset Construction"}, {"figure_path": "https://arxiv.org/html/2502.16614/x13.png", "caption": "(b) The Code QA subset.", "description": "Figure 12(b) shows the distribution of ratings for the Code QA subset of the CodeCriticBench dataset.  The x-axis represents the rating score, ranging from 1 to 10, with higher scores indicating better quality. The y-axis represents the number of samples with that score. The two bars for each rating show the separate counts for 'Correct' and 'Error' answers. The distribution helps illustrate the dataset's balance in difficulty levels and the correlation between the basic correctness label and the fine-grained, multi-dimensional evaluation scores.", "section": "3.3 Dataset Construction"}, {"figure_path": "https://arxiv.org/html/2502.16614/x14.png", "caption": "Figure 12: Rating distribution.", "description": "This figure shows the distribution of ratings for the CodeCriticBench dataset, separated into the Code Generation and Code QA subsets.  The histograms illustrate the frequency of different rating scores (presumably 1-10) given by evaluators to each code example.  The distribution helps to demonstrate the difficulty level and the quality of the data samples within each task type.  A balanced distribution of ratings across the scores would suggest a well-designed benchmark.", "section": "3.3 Dataset Construction"}, {"figure_path": "https://arxiv.org/html/2502.16614/x15.png", "caption": "Figure 13: Example of correct case of code generation.", "description": "Figure 13 shows an example from the CodeCriticBench dataset where the LLM correctly generated code.  The figure displays the problem statement, the constraints, the example input/output, the generated code, and the evaluation results. The problem requires the LLM to write a function that calculates the minimum cost to make all characters in a binary string equal using two types of operations.  The evaluation shows that the generated code passed the correctness verification and time complexity optimization checks, indicating that the model successfully performed the code generation task.", "section": "3. CodeCriticBench"}, {"figure_path": "https://arxiv.org/html/2502.16614/x16.png", "caption": "Figure 14: Example of error case of code generation.", "description": "This figure shows an example from the CodeCriticBench dataset where the generated code is incorrect.  It displays the question, the incorrect code generated by a large language model (LLM), the fine-grained evaluation checklist showing the errors, and the final evaluation rating. This example highlights the benchmark's capability to identify errors in code generation and provide detailed feedback on LLM performance.", "section": "3.2 Data Collection"}, {"figure_path": "https://arxiv.org/html/2502.16614/x17.png", "caption": "Figure 15: Example of error case of code qa.", "description": "This figure shows an example of an erroneous response from a large language model (LLM) to a code-based question-answering (QA) task.  The example demonstrates an incomplete and flawed response. The provided answer lacks critical elements, fails to handle edge cases, and contains logical errors.  The figure highlights the challenges LLMs face in accurately evaluating code and the need for more robust evaluation methods. The details shown include the question, the incorrect LLM answer, and a set of detailed checklists evaluating the response against various quality dimensions, including correctness, time complexity, and maintainability.", "section": "3. CodeCriticBench"}, {"figure_path": "https://arxiv.org/html/2502.16614/x18.png", "caption": "Figure 16: Scaling law on advanced critique evaluation (MSE) across models. \u201c*\u201d indicates an estimated parameter size.", "description": "This figure demonstrates the relationship between the number of parameters in a language model and its performance on advanced code critique tasks.  The mean squared error (MSE) is used as the evaluation metric, representing the difference between the model's predicted scores and human-assigned scores across multiple fine-grained evaluation dimensions. As the number of parameters in the model increases, the MSE generally decreases. This indicates that larger models tend to provide more accurate and comprehensive feedback for code critique.", "section": "4.2 Main Results"}]