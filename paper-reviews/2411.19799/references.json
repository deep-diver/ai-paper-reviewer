{"references": [{"fullname_first_author": "Josh Achiam", "paper_title": "Gpt-4 technical report", "publication_date": "2023-03-08", "reason": "This paper is a technical report describing GPT-4, a state-of-the-art multilingual and general-purpose model used as a baseline for the study."}, {"fullname_first_author": "Dan Hendrycks", "paper_title": "Measuring massive multitask language understanding", "publication_date": "2020-09-03", "reason": "This paper introduced the MMLU benchmark, a widely used evaluation dataset for multilingual language models, and its methodology influenced the current work's evaluation approach."}, {"fullname_first_author": "Viraat Aryabumi", "paper_title": "Aya 23: Open weight releases to further multilingual progress", "publication_date": "2024-05-15", "reason": "The Aya-expanse model, presented in this paper, is one of the multilingual models evaluated in the study, offering insights into the role of scaling in multilingual language understanding."}, {"fullname_first_author": "An Yang", "paper_title": "Qwen2 technical report", "publication_date": "2024-07-10", "reason": "Another prominent multilingual model, Qwen-2.5, evaluated in the paper, showcases a comparison of model performance across multiple languages."}, {"fullname_first_author": "Ahmet \u00dcst\u00fcn", "paper_title": "Aya model: An instruction finetuned open-access multilingual language model", "publication_date": "2024-08-01", "reason": "This paper presents the Aya model, a significant multilingual language model evaluated in the study, which helps in comparing the performance across different model architectures."}]}