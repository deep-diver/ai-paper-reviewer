[{"figure_path": "https://arxiv.org/html/2411.19799/extracted/6034225/figures/main_figure.png", "caption": "Figure 1:  Overview of Include. (a) Motivation: Multilingual benchmarks must reflect the cultural and regional knowledge of the language environments in which they would used. (b) Include is a multilingual benchmark compiled from academic, professional, and occupational license examinations reflecting regional and cultural knowledge in 44 languages.", "description": "Figure 1(a) illustrates the importance of incorporating cultural and regional knowledge into multilingual benchmarks for evaluating large language models (LLMs).  It highlights how the same question, when posed in different languages, can require different contextual understandings due to variations in regional laws, cultural norms, or historical contexts. This highlights the need for more representative and nuanced evaluation datasets. Figure 1(b) shows the structure of the INCLUDE benchmark, which addresses these issues by compiling questions from a wide range of academic exams, professional certification tests, and occupational licensing examinations.  This diverse dataset covers 44 languages, ensuring that regional and cultural knowledge are accurately reflected in the evaluation of multilingual LLMs.", "section": "3 The INCLUDE Benchmark"}, {"figure_path": "https://arxiv.org/html/2411.19799/extracted/6034225/figures/descriptives.png", "caption": "Figure 2:  Overview of the collected data grouped by script. We depict the languages associated with each script, the total samples in each script, and the percentage of the samples that were collected from new sources that have not been published by the community yet.", "description": "Figure 2 shows the number of samples collected for each of the 15 scripts used in the INCLUDE benchmark.  For each script, the figure displays the languages using that script, the total number of samples for that script, and the percentage of those samples that are from original, unpublished sources.  This illustrates the diversity of languages and scripts included in the benchmark, as well as the proportion of novel data that was collected.", "section": "3.1 Data Collection"}, {"figure_path": "https://arxiv.org/html/2411.19799/extracted/6034225/figures/knowledge_transfer.png", "caption": "Figure 3:  Performance of models stratified by language using in-language prompting. Results are grouped by whether the language was explicitly included in the pretraining dataset of the model (Trained on Language), whether a similar language with the same script was in the pretraining corpus (Trained on Script), or whether there was no linguistically similar language in the pretraining corpus (Neither). Color dotted lines represent average performance for each category for a particular model. Black dotted lines represent average performance across all script-aligned languages.", "description": "This figure displays the performance of three large language models (LLMs) across different languages, categorized based on their relationship to the models' training data.  The x-axis represents the accuracy of each model on various languages, while the y-axis represents the languages. Languages are grouped into three categories:  'Trained on Language' (languages explicitly included in the training data), 'Trained on Script' (languages sharing the same script as languages in the training data), and 'Neither' (languages not linguistically similar to those in the training data).  The colored dotted lines show the average performance for each language category within each model, while the black dotted lines indicate the average performance across all languages that share a script.", "section": "5.2 Language Analysis"}, {"figure_path": "https://arxiv.org/html/2411.19799/extracted/6034225/figures/regional_results.png", "caption": "Figure 4:  GPT-4o performance (In-language Prompt) on regional history exams (cultural) and global history exams from that region (region-implicit) based on a total of 11,148 questions from Include. In each language (except Telugu), models perform better on the global history exam than the regional history exam.", "description": "This figure displays the performance of the GPT-40 language model on history-related questions from the INCLUDE benchmark.  The questions are categorized into two types: regional history (cultural knowledge specific to a region) and global history (general historical knowledge). The results show that GPT-40 performs better on global history questions than on regional history questions, across most languages.  This suggests that the model may struggle with questions requiring nuanced cultural knowledge specific to particular regions. The dataset included a total of 11,148 questions in this analysis.", "section": "5.3 REGIONAL & ACADEMIC DOMAIN KNOWLEDGE PERFORMANCE"}, {"figure_path": "https://arxiv.org/html/2411.19799/extracted/6034225/figures/languages.png", "caption": "Figure 5: GPT-4o performance across academic disciplines for Korean, Persian, Armenian, Hindi, Greek, and Russian. Each bar is annotated with the number of questions with correct answers.", "description": "This figure displays GPT-40's performance across various academic disciplines for six different languages: Korean, Persian, Armenian, Hindi, Greek, and Russian.  Each bar graph represents a specific language and is further broken down by academic disciplines within that language. The height of each bar visually represents the model's accuracy (percentage of correctly answered questions) for that specific discipline within that language.  The number of questions used to calculate the accuracy for each discipline is also indicated on each bar.", "section": "5.3 REGIONAL & ACADEMIC DOMAIN KNOWLEDGE PERFORMANCE"}, {"figure_path": "https://arxiv.org/html/2411.19799/extracted/6034225/figures/regional_stem.png", "caption": "Figure 6: GPT-4o model performance on Include-base. (a) Performance across regional labels. While models typically perform better across region-explicit and regional-implicit questions, it is difficult to disentangle the difficult of questions due to regionality from the subject matter itself (i.e., region-agnostic questions may contain more STEM subjects that are traditionally harder for LLMs). (b) Performance across academic disciplines within STEM area. We observe models perform particularly poorly on Math and Chemistry questions.", "description": "Figure 6 shows GPT-4's performance on the INCLUDE-BASE benchmark.  Panel (a) compares performance across three question categories based on their regional knowledge dependence: region-agnostic (no regional knowledge needed), region-explicit (requiring specific regional knowledge), and region-implicit (regional knowledge potentially relevant but not explicitly required).  The figure reveals that while performance is generally higher on explicit and implicit regional questions, this may be confounded by the fact that region-agnostic questions often involve STEM topics, which are known to be more challenging for LLMs. Panel (b) focuses specifically on STEM subjects and shows that the model's accuracy is particularly low for math and chemistry questions.", "section": "3.2 CATEGORIZING KNOWLEDGE"}, {"figure_path": "https://arxiv.org/html/2411.19799/extracted/6034225/figures/Cat_A_Cat_B.png", "caption": "Figure 7: Academic domain and academic fields with the number of examples across all languages.", "description": "This figure provides a visual representation of the distribution of questions across different academic domains and fields within the INCLUDE benchmark.  The figure uses a circular layout, with each academic area (e.g., Humanities, STEM, Social Sciences) represented as a section. Within each section, the different academic fields are further broken down and shown with the number of questions from that field. The size of each section and the visual representation of each field within it are scaled according to the number of questions in that area or field, offering a clear representation of the dataset's composition across various disciplines.", "section": "3 THE INCLUDE BENCHMARK"}, {"figure_path": "https://arxiv.org/html/2411.19799/extracted/6034225/figures/form.png", "caption": "Figure 8: Exam source collection form sent to the academic community.", "description": "This figure shows the Google Form used to solicit exam questions from the academic community to create the INCLUDE benchmark.  The form requests details such as the name and description of the exam, the language used, URLs to the exam source, and a description of how the answers are provided. It specifically targets three types of exams: educational (high school, university), professional (law, medical licenses), and practical tests (driver's license). The form also collects additional metadata about the exam, such as the approximate number of questions and their format.", "section": "3.1 DATA COLLECTION"}, {"figure_path": "https://arxiv.org/html/2411.19799/extracted/6034225/figures/original_published_scatter_all.png", "caption": "Figure 9: Accuracy of different models on languages where both existing benchmark data and newly collected data are available. Each point represents the accuracy score of a model for a specific language. (a) Points of the same color represent the accuracy scores of a single model across different languages. (b) Points of the same color represent the accuracy scores for a single language across different models.", "description": "This figure visualizes the performance of various large language models (LLMs) on a multilingual question-answering benchmark.  It compares the models' accuracy across different languages, offering insights into their cross-lingual capabilities. Panel (a) focuses on a single model's accuracy across multiple languages, while panel (b) displays multiple models' accuracy in a single language. This dual perspective helps analyze both the strengths and weaknesses of individual models and the overall challenges of multilingual language understanding.", "section": "5.1 General Performance"}]