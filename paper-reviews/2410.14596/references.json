{"references": [{" publication_date": "2024", "fullname_first_author": "Justin Chen", "paper_title": "ReConcile: Round-table conference improves reasoning via consensus among diverse LLMs", "reason": "This paper is highly relevant because it explores using diverse LLMs in a round-table setting to improve reasoning, similar to the multi-agent dialogue approach used in the target paper for generating balanced persuasion data.  Improving reasoning is critical for handling persuasion effectively, and this paper's methodology provides insights for creating training data to train LLMs for better reasoning and handling of complex conversations involving persuasion.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Philippe Laban", "paper_title": "Are you sure? challenging LLMs leads to performance drops in the flipflop experiment", "reason": "This paper directly addresses the issue of LLM flip-flopping, which is a central concern in the target paper. The \"Are you sure?\" prompt used in this paper is directly adopted and used as one of the key evaluation methods in the target paper to measure the models' resistance to changing answers when challenged.  Therefore, it's a crucial foundational study that underpins the evaluation methodology and the core problem being addressed.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Rongwu Xu", "paper_title": "The earth is flat because...: Investigating LLMs' belief towards misinformation via persuasive conversation", "reason": "This paper is highly relevant as it directly addresses the problem of LLMs' susceptibility to misinformation, a core issue discussed in the target paper.  The methodology of using persuasive conversations to measure the rate of misinformation adoption directly informs the target paper's methodology for creating a balanced persuasion dataset. The FARM dataset, derived from this work, becomes a key evaluation metric in the target paper.", "section_number": 3}, {" publication_date": "2017", "fullname_first_author": "Mandar Joshi", "paper_title": "TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension", "reason": "The TriviaQA dataset is the foundation for the question-answer pairs used in the target paper's multi-agent dialogue tree method.  The dataset's questions and answers provide the context for the dialogues, enabling the creation of training data for both positive and negative persuasion.  Its quality and scale directly impact the effectiveness of the training data and the overall results of the target paper.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Albert Q. Jiang", "paper_title": "Mistral 7b", "reason": "This paper introduces the Mistral-7B model, which is one of the LLMs used in the target paper's experiments. The model's architecture and performance characteristics are crucial factors impacting the results and the overall generalizability of the target paper's findings.  The choice of model directly influences the quality and representativeness of the generated training data and experimental outcomes.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "AI@Meta", "paper_title": "Llama 3.1 model card", "reason": "This paper describes the Llama 3.1 model, one of the key LLMs used in the target paper's experiments. The properties of this LLM are crucial in influencing the generated dialogue trees and the subsequent training and evaluation.  The model's architecture and characteristics directly impact the experimental findings and the generalizability of the target paper's results.", "section_number": 3}, {" publication_date": "2022", "fullname_first_author": "Edward J Hu", "paper_title": "Lora: Low-rank adaptation of large language models", "reason": "LoRA, a low-rank adaptation technique, is used in the target paper for efficiently fine-tuning large language models.  Understanding LoRA is critical for comprehending the training process and its efficiency in adapting models to the task of balanced persuasion. LoRA's efficiency impacts the feasibility and scalability of the proposed method.", "section_number": 3}, {" publication_date": "2017", "fullname_first_author": "Ilya Loshchilov", "paper_title": "Decoupled weight decay regularization", "reason": "AdamW, an optimization algorithm incorporating decoupled weight decay regularization, is used for training in the target paper.  Understanding AdamW and its properties is important for interpreting the training process, its convergence behavior, and the overall performance of the trained models. This optimization algorithm directly influences the quality and efficiency of training.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Rafael Rafailov", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "reason": "This paper introduces Direct Preference Optimization (DPO), which is the core optimization method used in the target paper's PBT training. DPO is a crucial component of the methodology, and understanding its workings is essential for interpreting the results and the overall effectiveness of the training approach.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Thomas Hofweber", "paper_title": "Are language models rational? the case of coherence norms and belief revision", "reason": "This paper is relevant to the discussion on LLMs' beliefs and whether they can be persuaded to change them, as it discusses the rationality of LLMs and how they handle belief revision. The question of LLM beliefs is directly discussed in the target paper's limitations section, highlighting the relevance of this work to understanding the philosophical underpinnings of the work.", "section_number": 5}, {" publication_date": "1975", "fullname_first_author": "Martin Fishbein", "paper_title": "Belief, attitude, intention, and behavior: An introduction to theory and research", "reason": "This paper provides foundational theoretical background on belief, attitude, intention, and behavior, which is relevant to understanding the complexities of human persuasion, and indirectly informs the discussions about model beliefs in LLMs. It's relevant to the limitation section, providing context for the challenges of assessing persuasion in LLMs.", "section_number": 6}, {" publication_date": "2012", "fullname_first_author": "Philip M Podsakoff", "paper_title": "Sources of method bias in social science research and recommendations on how to control it", "reason": "This paper discusses method bias in social science research, providing important context for understanding and mitigating potential biases in the methodology of the target paper, particularly for the qualitative aspects of the research. It helps in evaluating the reliability and generalizability of the findings, which is especially important for understanding the limitations of the work.", "section_number": 6}, {" publication_date": "2024", "fullname_first_author": "Kevin Wu", "paper_title": "Clashe-val: Quantifying the tug-of-war between an Ilm's internal prior and external evidence", "reason": "This paper directly relates to the discussions in the target paper about models' reliance on answer plausibility to decide on accepting or rejecting persuasion.  It also explores the conflicts between a model's internal knowledge and external evidence, which relates to the target paper's central theme of balancing resistance and acceptance of persuasion.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Yike Wang", "paper_title": "Resolving knowledge conflicts in large language models", "reason": "This paper addresses the issue of knowledge conflict in large language models, relevant to the target paper's exploration of how models handle conflicting information when making decisions during persuasion. This paper explores a similar problem of LLM responses in the presence of potentially conflicting information.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Shayne Longpre", "paper_title": "Entity-based knowledge conflicts in question answering", "reason": "This paper addresses the issue of knowledge conflicts within LLMs, which is directly relevant to the topic of persuasion in the target paper.  Persuasion often involves presenting information that conflicts with a model's existing knowledge, and this paper explores the mechanisms involved in how LLMs deal with such conflicts.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Alexander Wan", "paper_title": "What evidence do language models find convincing?", "reason": "This paper investigates what type of evidence convinces LLMs, which is directly relevant to the target paper's exploration of persuasion.  Understanding how LLMs weigh different forms of evidence is crucial for devising methods to effectively persuade them, whether positively or negatively.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Tian Liang", "paper_title": "Encouraging divergent thinking in large language models through multi-agent debate", "reason": "This paper employs multi-agent debate to improve LLM reasoning, which is highly relevant to the target paper's methodology using multi-agent dialogues.  The focus on multi-agent interaction and improved reasoning is essential for creating robust and effective LLMs that can handle persuasion.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Zihao Yi", "paper_title": "A survey on recent advances in Ilm-based multi-turn dialogue systems", "reason": "This survey paper provides a broad overview of recent work on multi-turn dialogue systems using LLMs, which is the context of the target paper.  The paper contextualizes the target paper's contributions within the wider research landscape, highlighting its novelty and significance within the field of multi-turn dialogues and persuasion.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Yi Zeng", "paper_title": "How johnny can persuade LLMs to jailbreak them: Rethinking persuasion to challenge AI safety by humanizing LLMs", "reason": "This paper explores the vulnerabilities of LLMs to jailbreaking through persuasion techniques and it provides valuable insights and context to the problem of negative persuasion within the context of AI safety and security. This is highly relevant to the target paper which examines the risks and challenges presented by the susceptibility of LLMs to adversarial manipulation and the importance of creating more robust and reliable models that are less susceptible to manipulation through persuasion.", "section_number": 2}]}