{"references": [{" publication_date": "2017", "fullname_first_author": "Ilya Loshchilov", "paper_title": "Decoupled weight decay regularization", "reason": "This paper introduces a novel weight decay regularization technique that improves the training of deep neural networks.  Weight decay is a crucial hyperparameter in training LLMs, and this technique's impact on the optimization process and model performance is relevant to the current research which uses LORA.", "section_number": 3}, {" publication_date": "2022", "fullname_first_author": "Edward J Hu", "paper_title": "Lora: Low-rank adaptation of large language models", "reason": "This work introduces LORA, a low-rank adaptation technique for efficiently fine-tuning large language models.  Since the current paper uses LORA for training their models, understanding this method is crucial. The efficiency and effectiveness of LORA directly impact the scalability and practical applicability of the PBT method.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Justin Chen", "paper_title": "ReConcile: Round-table conference improves reasoning via consensus among diverse LLMs", "reason": "This paper explores improving reasoning in LLMs via consensus among diverse models. The current research uses multi-agent dialogues which share the same underlying collaborative approach, and understanding this related work in multi-agent reasoning is essential for context and comparison.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Philippe Laban", "paper_title": "Are you sure? challenging LLMs leads to performance drops in the flipflop experiment", "reason": "This research investigates the phenomenon of LLMs 'flip-flopping'\u2014changing answers when challenged. The current research directly addresses this vulnerability, using the same method to assess improvement. The study's findings and methodology are directly relevant to assessing the effectiveness of PBT in mitigating this issue. The use of the \"Are you sure?\" prompt as an evaluation metric is core to the present work's assessment of flip-flopping.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Rongwu Xu", "paper_title": "The earth is flat because...: Investigating LLMs' belief towards misinformation via persuasive conversation", "reason": "This paper examines LLMs' susceptibility to misinformation through persuasive conversations.  This directly addresses the central problem of the current paper, which focuses on creating models resistant to misinformation. The study's methodology, focusing on persuasive dialogues and assessing the rate of misinformation, is closely aligned with the current research.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Albert Q. Jiang", "paper_title": "Mistral 7b", "reason": "This paper introduces the Mistral-7B language model, one of the models used in the current research.  Understanding the architecture, capabilities, and limitations of this model is essential for interpreting the results presented. The baseline performance of Mistral-7B serves as a critical point of comparison for evaluating the effectiveness of PBT in this specific model architecture.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Kevin Du", "paper_title": "Context versus prior knowledge in language models", "reason": "This paper explores the interplay between contextual information and prior knowledge in language models.  The research on knowledge conflicts and updating is highly relevant to the current work, as the agents' interactions in the dialogues often involve resolving conflicting pieces of information and updating their beliefs.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Yilun Du", "paper_title": "Improving factuality and reasoning in language models through multi-agent debate", "reason": "This research uses multi-agent debate to enhance the factuality and reasoning capabilities of LLMs. The methodology of using multi-agent dialogues is directly related to the approach used in this paper and helps establish the broader context and effectiveness of dialogue-based training methods for LLMs.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Stephanie Lin", "paper_title": "TruthfulQA: Measuring how models mimic human falsehoods", "reason": "This work focuses on evaluating the truthfulness of LLMs using a curated dataset. The current research aims to make LLMs more resistant to false information, making TruthfulQA a relevant benchmark for assessing the success of PBT in improving factual accuracy.", "section_number": 4}, {" publication_date": "2019", "fullname_first_author": "Tom Kwiatkowski", "paper_title": "Natural Questions: A benchmark for question answering research", "reason": "This paper introduces the Natural Questions benchmark dataset, used in the current research to create a balanced dataset containing questions from popular question-answering benchmarks. The relevance of this dataset to the study lies in its contribution to evaluating the models\u2019 abilities to resist misinformation and flip-flopping by providing a diverse range of questions for both positive and negative persuasion.", "section_number": 4}, {" publication_date": "2017", "fullname_first_author": "Mandar Joshi", "paper_title": "TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension", "reason": "This paper introduces the TriviaQA dataset, which is the primary source of question-answer pairs used in the current research's methodology for generating training data. TriviaQA is central to the work because it provides the foundational data for creating dialogue trees that facilitate the exploration of both positive and negative persuasion.", "section_number": 3}, {" publication_date": "2018", "fullname_first_author": "Esin Durmus", "paper_title": "Exploring the role of prior beliefs for argument persuasion", "reason": "This paper explores the role of prior beliefs in argument persuasion which relates to how LLMs handle contradictory information during persuasive dialogues.  The current study builds upon this by analyzing the models' decision-making processes when presented with conflicting information during persuasive interactions.", "section_number": 2}, {" publication_date": "2019", "fullname_first_author": "Esin Durmus", "paper_title": "Modeling the factors of user success in online debate", "reason": "This research investigates factors that determine the success of users in online debates, which is relevant to the current work because the methodology relies on simulating debates to generate training data. The findings on what constitutes a successful debate strategy provide a framework for assessing the effectiveness of PBT in creating models that can engage in productive debates.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Shayne Longpre", "paper_title": "Entity-based knowledge conflicts in question answering", "reason": "This paper examines knowledge conflicts in question answering, which directly relates to the scenarios encountered in the multi-agent dialogues used for PBT training data generation.  Understanding how LLMs handle these conflicts is essential for evaluating the effectiveness of PBT in handling situations with conflicting or contradictory information.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Thomas Hofweber", "paper_title": "Are language models rational?: The case of coherence norms and belief revision", "reason": "This paper explores the rationality of language models, which is relevant to the current research\u2019s focus on improving the decision-making process in LLMs to accept positive and resist negative persuasion. The concept of rationality in models is closely tied to the ability to make consistent and appropriate decisions under persuasive influences, thus the importance of understanding the rationality of models to evaluate the proposed PBT methodology.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Mrinank Sharma", "paper_title": "Towards understanding sycophancy in language models", "reason": "This paper investigates sycophancy in LLMs, which relates to how models tend to agree with their interlocutors, even when the models' internal beliefs or judgements might differ.  The current study addresses this issue by focusing on creating models that are both resistant to negative persuasion and receptive to valid arguments, which mitigates the risk of models exhibiting sycophantic behavior during persuasive interactions.", "section_number": 5}, {" publication_date": "1975", "fullname_first_author": "Martin Fishbein", "paper_title": "Belief, attitude, intention, and behavior: An introduction to theory and research", "reason": "This foundational work in social psychology provides a theoretical framework for understanding the relationship between beliefs, attitudes, and behaviors.  The current research investigates how to modify LLMs' behavior when presented with persuasive arguments, therefore understanding these psychological dynamics is crucial for interpreting the study's results.  The paper's focus on the reasoned action approach helps shape the understanding of how persuasive arguments lead to changes in attitudes and consequent behavior in the context of the study\u2019s LLMs.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Elias Stengel-Eskin", "paper_title": "Lacie: Listener-aware finetuning for confidence calibration in large language models", "reason": "This paper explores confidence calibration in LLMs, which is closely related to the current research's focus on making models more resistant to being manipulated and more discerning about accepting positive and negative persuasion. This relates to the paper because confidence is a factor that can impact the model's propensity to accept or reject persuasive information. Understanding how confidence influences a model's decisions and how that can be improved is relevant to the overall goal of creating more robust and reliable LLMs that handle persuasion effectively.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Tian Liang", "paper_title": "Encouraging divergent thinking in large language models through multi-agent debate", "reason": "This paper explores how multi-agent debate can improve the reasoning capabilities of LLMs, which is related to the current paper's use of multi-agent dialogues for training data generation and the focus on improving collaborative interactions among LLMs.  The methodology used in the cited paper is significantly similar to that employed in the current study, especially in leveraging multi-agent discussions to enhance model reasoning and decision-making.  Understanding the strengths and limitations of this multi-agent training approach is important for interpreting the results of the current paper.", "section_number": 4}]}