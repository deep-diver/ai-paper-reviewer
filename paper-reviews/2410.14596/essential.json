{"importance": "This paper is crucial for researchers working on improving the robustness and reliability of large language models (LLMs).  It addresses the critical issue of LLM susceptibility to persuasion, highlighting the need for a balanced approach that combines resistance to harmful persuasion with the ability to accept beneficial influence.  The proposed Persuasion-Balanced Training (PBT) method and the associated findings offer a significant contribution to the ongoing efforts in making LLMs more reliable and safe for real-world applications.  The research also opens new avenues for exploring multi-agent interactions and team dynamics in LLMs.", "summary": "LLMs can be easily persuaded; this paper introduces Persuasion-Balanced Training (PBT) to make them both resistant to misinformation and receptive to helpful influence, leading to improved performance and reliability.", "takeaways": ["Persuasion-Balanced Training (PBT) improves LLMs' resistance to misinformation and their ability to accept helpful persuasion.", "PBT models exhibit improved performance in multi-agent debates, showing less sensitivity to the order of argument presentation.", "Analysis reveals that PBT models decide whether to accept or reject persuasion based on the plausibility of the proposed answer, improving overall decision-making reliability. "], "tldr": "Large language models (LLMs) are vulnerable to manipulation through persuasion.  This research tackles this issue by developing a new training method called Persuasion-Balanced Training (PBT).  PBT uses multi-agent dialogues to generate training data that represents both positive and negative persuasion scenarios.  The results show that models trained with PBT are significantly better at resisting misinformation and harmful persuasion compared to models trained only to resist or only to accept persuasion.  Furthermore, PBT leads to more stable and robust performance in team settings, eliminating the issue of performance being heavily dependent on which model presents its argument first. The study also delves into the factors that determine a model's acceptance or resistance to persuasion, finding that answer plausibility is the key determinant, resulting in more reliable decision-making. This work is important because it offers a more comprehensive approach to building robust and reliable LLMs, addressing both the risks and the potential benefits of persuasion in human-computer interaction and multi-agent systems."}