{"importance": "This paper is crucial for researchers working on improving the robustness and reliability of large language models (LLMs).  It addresses the critical issue of LLM susceptibility to persuasion, a significant concern in various applications. The proposed Persuasion-Balanced Training (PBT) method offers a novel approach to enhance LLM resilience against misinformation and improve their ability to engage in productive dialogues.", "summary": "LLMs are taught to both resist harmful and accept helpful persuasion using Persuasion-Balanced Training, resulting in more reliable and collaborative AI.", "takeaways": ["Persuasion-Balanced Training (PBT) improves LLMs' ability to both resist misinformation and accept helpful persuasion.", "PBT models exhibit more stable performance in multi-agent settings, unlike models trained to only resist or accept persuasion.", "Model decisions to accept or reject persuasion are influenced by the plausibility of the proposed answer, rather than confidence scores."], "tldr": "Large language models (LLMs) are easily manipulated. This paper introduces Persuasion-Balanced Training (PBT), a novel method that teaches LLMs to both resist harmful persuasion (like misinformation) and accept helpful persuasion.  PBT uses multi-agent dialogues to create training data where models debate and learn to evaluate the quality of arguments. Experiments show PBT improves resistance to misinformation and flip-flopping. Importantly, PBT makes LLMs better teammates in multi-agent debates, reducing the impact of which model speaks first on overall team performance. The study also reveals that model decisions hinge on the plausibility of the answer, not confidence alone.  This work is significant because it moves beyond simply making LLMs resistant to manipulation, instead aiming for a balanced approach that enhances both accuracy and collaboration."}