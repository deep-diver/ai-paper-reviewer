{"references": [{" publication_date": "2015", "fullname_first_author": "Mart\u00edn Abadi", "paper_title": "TensorFlow: Large-scale machine learning on heterogeneous systems", "reason": "This paper introduced TensorFlow, a foundational deep learning library that underpins many modern machine learning systems, including AutoTrain.  Its impact on the field is immense, providing a flexible and scalable framework for various machine learning tasks.  TensorFlow's influence on AutoTrain is undeniable, as it is used as the core training backend within AutoTrain Advanced.", "section_number": 3}, {" publication_date": "2016", "fullname_first_author": "Tianqi Chen", "paper_title": "Xgboost: A scalable tree boosting system", "reason": "Xgboost is a highly efficient and scalable machine learning algorithm, particularly effective for tabular data.  Because AutoTrain supports tabular data processing and regression tasks, the use of Xgboost as a preferred model within the AutoTrain Advanced library is directly relevant and highlights the importance of this algorithm in the broader field of machine learning.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Axolotl AI Cloud", "paper_title": "Axolotl: A tool for streamlining fine-tuning of ai models", "reason": "While not directly used within AutoTrain, Axolotl represents a related tool focusing on fine-tuning LLMs, making it relevant to compare in the related work section.  Its focus on streamlining fine-tuning aligns with the goals of AutoTrain, offering a context for the novelty and improvements provided by AutoTrain.", "section_number": 2}, {" publication_date": "2015", "fullname_first_author": "Matthias Feurer", "paper_title": "Efficient and robust automated machine learning", "reason": "This paper presented Auto-sklearn, a significant early AutoML system, offering valuable context for the evolution of AutoML techniques within the related work discussion.  The techniques and methodologies detailed contribute to the understanding of the landscape of AutoML solutions.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Haifeng Jin", "paper_title": "Autokeras: An automl library for deep learning", "reason": "AutoKeras provides another important AutoML tool, specifically focusing on deep learning and relevant to AutoTrain's goals.  The comparison within the related work section highlights AutoTrain's ability to handle a broader range of tasks and models compared to solutions such as AutoKeras.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Quentin Lhoest", "paper_title": "Datasets: A community library for natural language processing", "reason": "The Hugging Face Datasets library plays a crucial role in AutoTrain Advanced by providing a standardized way to handle diverse datasets. This paper introduces the library and highlights its importance for efficient data management within the AutoTrain workflow.", "section_number": 3}, {" publication_date": "2019", "fullname_first_author": "Thomas Wolf", "paper_title": "Huggingface's transformers: State-of-the-art natural language processing", "reason": "The Hugging Face Transformers library is fundamental to AutoTrain Advanced, providing a comprehensive collection of pretrained models and tools for training and deploying transformer-based models. This paper describes the importance of the library and how it's used by AutoTrain.", "section_number": 3}, {" publication_date": "2022", "fullname_first_author": "Sylvain Gugger", "paper_title": "Accelerate: Training and inference at scale made simple, efficient and adaptable", "reason": "Hugging Face Accelerate significantly improves the efficiency of AutoTrain Advanced by enabling seamless distributed training across multiple GPUs, handling mixed precision training and efficient memory management. The importance of this library for model training at scale is undeniable.", "section_number": 3}, {" publication_date": "2022", "fullname_first_author": "Sourab Mangrulkar", "paper_title": "Peft: State-of-the-art parameter-efficient fine-tuning methods", "reason": "PEFT (Parameter-Efficient Fine-Tuning) is a crucial component of AutoTrain, enabling efficient training of large language models.  This paper describes PEFT methods which directly affect the performance and efficiency of AutoTrain.", "section_number": 3}, {" publication_date": "2020", "fullname_first_author": "Leandro von Werra", "paper_title": "Trl: Transformer reinforcement learning", "reason": "While not directly cited in the paper, the TRL library (Transformer Reinforcement Learning) from Hugging Face has likely contributed to some functionalities within AutoTrain Advanced, especially if the training process is applied to reinforcement learning models.", "section_number": 3}, {" publication_date": "2019", "fullname_first_author": "Adam Paszke", "paper_title": "Pytorch: An imperative style, high-performance deep learning library", "reason": "PyTorch forms the underlying deep learning framework for AutoTrain. Its imperative style, high performance, and extensive community support are pivotal to the ease of use and functionality provided by AutoTrain.", "section_number": 3}, {" publication_date": "2014", "fullname_first_author": "Stefan Van der Walt", "paper_title": "scikit-image: image processing in python", "reason": "Scikit-image is a crucial component used within AutoTrain Advanced for image processing tasks.  Its functionality is leveraged by the library to provide essential image manipulation and preprocessing capabilities within the AutoTrain framework.", "section_number": 3}, {" publication_date": "2015", "fullname_first_author": "Abhishek Thakur", "paper_title": "Autocompete: A framework for machine learning competition", "reason": "AutoCompete, a previous work by the author, provides context for the development of AutoTrain. Its experience gained from AutoCompete directly contributed to addressing many challenges faced by users in the design of AutoTrain.", "section_number": 2}, {" publication_date": "2015", "fullname_first_author": "Matthias Feurer", "paper_title": "Efficient and robust automated machine learning", "reason": "This work is highly relevant to the context of AutoML and AutoTrain.  The discussion of efficient and robust automated machine learning techniques provides essential background and sets the stage for the presentation of AutoTrain as an advanced solution within this domain.", "section_number": 2}, {" publication_date": "2019", "fullname_first_author": "Nils Reimers", "paper_title": "Sentence-bert: Sentence embeddings using siamese bert-networks", "reason": "Sentence-BERT is a powerful model architecture for generating sentence embeddings.  The techniques are highly relevant to the text processing capabilities of AutoTrain, which relies on robust sentence embeddings for various text-based tasks.", "section_number": 1}, {" publication_date": "2015", "fullname_first_author": "Mart\u00edn Abadi", "paper_title": "TensorFlow: Large-scale machine learning on heterogeneous systems", "reason": "TensorFlow's role as the core training backend makes it arguably the most important reference in the paper due to its direct and extensive influence on the functionality and architecture of AutoTrain Advanced.", "section_number": 3}, {" publication_date": "2019", "fullname_first_author": "Thomas Wolf", "paper_title": "Huggingface's transformers: State-of-the-art natural language processing", "reason": "The Hugging Face Transformers library is the backbone of AutoTrain Advanced. Its functionalities are heavily relied upon, making it a cornerstone of the project's design and capabilities.", "section_number": 3}, {" publication_date": "2022", "fullname_first_author": "Sylvain Gugger", "paper_title": "Accelerate: Training and inference at scale made simple, efficient and adaptable", "reason": "Hugging Face Accelerate is crucial for the performance and scalability of AutoTrain Advanced. It enables efficient training on multiple GPUs, significantly improving speed and handling of larger datasets.", "section_number": 3}, {" publication_date": "2022", "fullname_first_author": "Sourab Mangrulkar", "paper_title": "Peft: State-of-the-art parameter-efficient fine-tuning methods", "reason": "PEFT (Parameter-Efficient Fine-Tuning) is a key component of AutoTrain. This paper's methods directly impact the efficiency and effectiveness of AutoTrain in training large language models with reduced computational needs.", "section_number": 3}, {" publication_date": "2021", "fullname_first_author": "Quentin Lhoest", "paper_title": "Datasets: A community library for natural language processing", "reason": "The Hugging Face Datasets library provides AutoTrain Advanced with essential data handling capabilities. The ease of use and versatility of this library contributes significantly to the ease of use that AutoTrain offers.", "section_number": 3}]}