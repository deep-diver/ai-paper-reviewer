[{"figure_path": "https://arxiv.org/html/2503.01688/x1.png", "caption": "Figure 1. Question complexity evaluation pipeline. See more details in the methods section", "description": "This figure illustrates the pipeline used to evaluate the uncertainty estimation of large language models (LLMs) in a multiple-choice question answering setting.  It starts with a set of multiple-choice questions from the MMLU-Pro dataset, which are then processed by various LLMs.  The pipeline then uses several techniques to estimate uncertainty, including token-wise entropy and model-as-judge (MASJ), and employs a separate model to assess various aspects of question complexity (reasoning requirements, knowledge requirements, and number of steps). The results, including correctness labels, are finally used to calculate ROC-AUC scores which are used to evaluate the effectiveness of the uncertainty estimation methods. ", "section": "\u00a73. Methods"}, {"figure_path": "https://arxiv.org/html/2503.01688/x2.png", "caption": "((a)) Distribution of questions that require complex reasoning", "description": "This figure shows the distribution of questions across different topics that require complex reasoning, as determined by a model-as-judge (MASJ) approach.  The x-axis represents the different subject areas (e.g., Computer Science, Economics, Biology, etc.), while the y-axis represents the percentage of questions within each subject that required complex reasoning according to the MASJ.  This visualization helps illustrate the variation in reasoning demands across different domains within the MMLU-Pro dataset.", "section": "\u00a74. Results"}, {"figure_path": "https://arxiv.org/html/2503.01688/x3.png", "caption": "((b)) Distribution of the required number of reasoning steps", "description": "This bar chart visualizes the distribution of questions across different levels of required reasoning steps, as determined by a model-as-judge (MASJ) approach.  Each bar represents a category (e.g., Biology, Economics, etc.) from the MMLU-Pro dataset, and its height corresponds to the number of questions in that category. The bars are further segmented into sub-sections representing three levels of reasoning complexity: low, medium, and high.  This visualization reveals how the distribution of reasoning requirements varies across different subject areas within the dataset.", "section": "4. Results"}, {"figure_path": "https://arxiv.org/html/2503.01688/x4.png", "caption": "Figure 2. Estimation by MASJ of the required reasoning amount. Better to view in zoom", "description": "This figure shows the results of using a Model-as-Judge (MASJ) approach to estimate the reasoning requirement and the number of reasoning steps needed for different questions in the MMLU-Pro dataset.  The left subplot displays the distribution of questions categorized as requiring complex reasoning versus those that do not. The right subplot illustrates the distribution of the estimated number of reasoning steps required for each question.  The distribution varies considerably across different subject areas, with some topics like engineering requiring substantially more reasoning than others like philosophy.", "section": "\u00a74. Results"}, {"figure_path": "https://arxiv.org/html/2503.01688/x5.png", "caption": "((a)) Phi-4 model", "description": "This figure shows the distribution of entropy values for the Phi-4 language model.  The x-axis represents the entropy values, while the y-axis represents the frequency or count of answers with those entropy values.  The figure is separated into two bars for each entropy value, representing the number of correct and incorrect answers at that entropy level. This visualization helps to understand the relationship between entropy (a measure of uncertainty) and the accuracy of the model's predictions.", "section": "4.3. Distribution of uncertainty measures"}, {"figure_path": "https://arxiv.org/html/2503.01688/x6.png", "caption": "((b)) Qwen-72B model", "description": "This figure shows the distribution of entropy values for the Qwen-72B language model's answers, categorized by whether the answer is correct or incorrect.  The x-axis represents the entropy values, and the y-axis represents the count of answers.  The distribution for correct answers is skewed towards lower entropy values, while incorrect answers have a flatter and more spread out distribution across entropy levels, indicating higher uncertainty in incorrect responses.  This visualization supports the paper's findings regarding the correlation between entropy and model prediction accuracy.", "section": "4.3. Distribution of uncertainty measures"}, {"figure_path": "https://arxiv.org/html/2503.01688/x7.png", "caption": "((c)) Qwen-1.5B model", "description": "This figure shows the distribution of entropy values for the Qwen-1.5B language model. The x-axis represents the entropy values, and the y-axis represents the count of answers with that specific entropy.  The bars are separated into correct and incorrect answers, allowing for a visual comparison of entropy levels in relation to answer accuracy.  This visualization helps to understand how well the entropy metric captures model uncertainty for this specific model.", "section": "4.3. Distribution of uncertainty measures"}, {"figure_path": "https://arxiv.org/html/2503.01688/x8.png", "caption": "Figure 3. Entropy distribution of answers. Best viewed when zoomed in", "description": "This figure displays the distribution of entropy values for answers generated by different language models.  It shows separate distributions for correct and incorrect answers, allowing for a visual comparison of entropy levels across various response types.  The distributions reveal insights into the relationship between model confidence (as reflected in entropy) and the accuracy of the generated answers.  It helps in understanding how well the entropy measure correlates with the models\u2019 ability to accurately assess its uncertainty.", "section": "4.3. Distribution of uncertainty measures"}, {"figure_path": "https://arxiv.org/html/2503.01688/x9.png", "caption": "Figure 4. ROC-AUC for error prediction by subject for four different LLMs", "description": "This figure displays the area under the ROC curve (ROC-AUC) for four different large language models (LLMs): Phi-4, Mistral, Qwen 1.5B, and Qwen 72B.  The ROC-AUC values are presented for each LLM's ability to predict errors in answering questions categorized by subject. Higher ROC-AUC values indicate better performance in error prediction.  The subjects appear to represent diverse academic fields, and the bars show the ROC-AUC score for each model within each subject.", "section": "4. Results"}, {"figure_path": "https://arxiv.org/html/2503.01688/x10.png", "caption": "Figure 5. ROC-AUC by reasoning for four different LLMs", "description": "This figure shows the area under the ROC curve (ROC-AUC) for four different large language models (LLMs): Qwen 72B, Qwen 1.5B, Phi-4 14B, and Mistral 24B.  The ROC-AUC values are displayed for different levels of reasoning required to answer the questions in the MMLU-Pro dataset.  The x-axis categorizes the questions based on their reasoning requirements: 'no reasoning required', 'reasoning required', 'reasoning (low)', 'reasoning (medium)', and 'reasoning (high)'.  The y-axis represents the ROC-AUC score, indicating the model's performance in predicting whether an answer is correct or incorrect based on its associated entropy value. Higher ROC-AUC values suggest better performance.", "section": "4.4. Complexity prediction via uncertainty estimate by category and question type"}]