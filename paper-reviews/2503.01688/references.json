{"references": [{"fullname_first_author": "Yasin Abbasi Yadkori", "paper_title": "To believe or not to believe your llm: Iterative prompting for estimating epistemic uncertainty", "publication_date": "2025-01-01", "reason": "This paper is important because it addresses the crucial issue of estimating epistemic uncertainty in LLMs using iterative prompting."}, {"fullname_first_author": "Dan Hendrycks", "paper_title": "Measuring massive multitask language understanding", "publication_date": "2021-01-01", "reason": "This paper introduced the original MMLU dataset which became a popular and widely used benchmark for evaluating language models across diverse domains."}, {"fullname_first_author": "Eyke H\u00fcllermeier", "paper_title": "Aleatoric and epistemic uncertainty in machine learning: An introduction to concepts and methods", "publication_date": "2021-01-01", "reason": "This paper is an introduction to the concepts and methods of aleatoric and epistemic uncertainty in machine learning, providing a fundamental framework for understanding different types of uncertainty."}, {"fullname_first_author": "Saurav Kadavath", "paper_title": "Language models (mostly) know what they know", "publication_date": "2022-07-05", "reason": "This work is essential as it investigates the crucial topic of whether language models understand their own limitations and can accurately assess their knowledge."}, {"fullname_first_author": "Lianmin Zheng", "paper_title": "Judging LLM-as-a-judge with MT-bench and chatbot arena", "publication_date": "2023-01-01", "reason": "This paper is relevant because it introduces the model-as-judge approach, a method for leveraging an LLM's ability to evaluate responses, which is employed in the current study."}]}