[{"content": "| Method | ASQA Hit@1 | ASQA EM | Hotpot-QA EM | NQ Hit@1 | NQ EM | Trivia-QA Hit@1 | Trivia-QA EM | MuSiQue ROUGE-L | ELI5 BLEU | ELI5 Hit@1 |\n|---|---|---|---|---|---|---|---|---|---|---|\n| Llama-3.1-8B-Instruct-4K |  |  |  |  |  |  |  |  |  |  |\n| BM25 | 45.00 | 19.84 | 36.25 | 40.75 | 30.66 | 84.75 | 26.17 | 5.75 | 15.90 | 6.56 |\n| BGE | 68.50 | 31.47 | 43.25 | 59.00 | 44.59 | 92.25 | 27.50 | 10.00 | 15.87 | 6.30 |\n| E5-Mistral | 62.50 | 28.51 | 38.50 | 56.50 | 41.73 | 90.00 | 27.05 | 9.00 | 15.77 | 5.85 |\n| LongLLMLingua | 59.25 | 26.34 | 40.75 | 55.25 | 41.82 | 90.00 | 27.02 | 9.00 | 16.08 | 6.45 |\n| JinaAI Reader | 53.50 | 23.14 | 34.00 | 47.25 | 34.41 | 84.75 | 24.83 | 6.75 | 15.80 | 5.65 |\n| HtmlRAG | 71.75<sup>\u2020</sup> | 33.31<sup>\u2020</sup> | 43.75<sup>\u2020</sup> | 61.75<sup>\u2020</sup> | 45.90<sup>\u2020</sup> | 91.75<sup>\u2020</sup> | 27.82<sup>\u2020</sup> | 8.75 | 15.51 | 5.84 |\n| Llama-3.1-70B-Instruct-4K |  |  |  |  |  |  |  |  |  |  |\n| BM25 | 49.50 | 21.95 | 38.25 | 47.00 | 35.56 | 88.00 | 25.63 | 9.50 | 16.15 | 6.99 |\n| BGE | 68.00 | 30.57 | 41.75 | 59.50 | 45.05 | 93.00 | 27.04 | 12.50 | 16.20 | 6.64 |\n| E5-Mistral | 63.00 | 28.75 | 36.75 | 59.50 | 44.07 | 90.75 | 26.27 | 11.00 | 16.17 | 6.72 |\n| LongLLMLingua | 62.50 | 27.74 | 45.00 | 56.75 | 42.89 | 92.50 | 27.23 | 10.25 | 15.84 | 6.39 |\n| JinaAI Reader | 55.25 | 23.73 | 34.25 | 48.25 | 35.40 | 90.00 | 25.35 | 9.25 | 16.06 | 6.41 |\n| HtmlRAG | 68.50<sup>\u2020</sup> | 30.53<sup>\u2020</sup> | 46.25<sup>\u2020</sup> | 60.50<sup>\u2020</sup> | 45.26<sup>\u2020</sup> | 93.50<sup>\u2020</sup> | 27.03 | 13.25<sup>\u2020</sup> | 16.33<sup>\u2020</sup> | 6.77<sup>\u2020</sup> |", "caption": "Table 1. Results of HtmlRAG and baselines under the short-context setting. Hit@1 is the proportion of instances where at least one short answer matches. The best and second best results are in bold and underlined. The symbol \u2020\u2020\\dagger\u2020 signifies that our model achieves superior results among baselines in a statistically significant manner (t-test, p\ud835\udc5dpitalic_p-value \u00a1 0.05).", "description": "Table 1 presents a comparison of HtmlRAG's performance against several baseline methods for question answering under short-context conditions.  It shows the Exact Match (EM) and Hit@1 scores (the percentage of instances where at least one short answer correctly matches the model's response) across six different datasets (ASQA, Hotpot-QA, NQ, TriviaQA, MuSiQue, and ELI5).  The results highlight HtmlRAG's superior performance, indicated by bold and underlined scores, and statistically significant improvements over baseline methods in many cases (denoted by \u2020).  The datasets vary in their question types and difficulty, allowing for a comprehensive evaluation of the model's capabilities.", "section": "4 Experimental Results"}, {"content": "| Method | ASQA Hit@1 | ASQA EM | Hotpot-QA Hit@1 | NQ EM | NQ Hit@1 | Trivia-QA EM | Trivia-QA EM | MuSiQue ROUGE-L | ELI5 BLEU | ELI5 | \n|---|---|---|---|---|---|---|---|---|---|---|\n| Llama-3.1-8B-Instruct-128K |  |  |  |  |  |  |  |  |  |  | \n| Vanilla HTML | 47.75 | 20.08 | 28.75 | 47.25 | 36.09 | 85.00 | 24.85 | 6.00 | 16.13 | 6.28 | \n| Plain Text | 61.50 | 27.82 | 39.25 | 59.25 | 44.31 | 94.00 | 28.23 | 7.75 | 16.02 | 6.35 | \n| Markdown | 61.75 | 26.70 | 37.50 | 57.50 | 42.85 | 91.50 | 26.67 | 7.50 | 16.12 | 5.91 | \n| HtmlRAG w/o Prune | 61.00 | 26.70\u2020 | 39.50\u2020 | 59.00\u2020 | 43.46\u2020 | 92.00\u2020 | 27.50\u2020 | 8.75\u2020 | 15.62 | 5.87 | \n| Llama-3.1-70B-Instruct-128K |  |  |  |  |  |  |  |  |  |  | \n| Vanilla HTML | 44.00 | 17.52 | 28.00 | 46.75 | 36.06 | 81.50 | 22.58 | 3.25 | 15.69 | 5.16 | \n| Plain Text | 59.75 | 25.16 | 41.00 | 59.75 | 44.11 | 93.50 | 26.75 | 8.75 | 16.88 | 7.44 | \n| Markdown | 56.00 | 24.00 | 39.00 | 57.00 | 42.00 | 92.00 | 26.43 | 8.25 | 16.91 | 6.74 | \n| HtmlRAG w/o Prune | 58.75\u2020 | 25.28\u2020 | 42.25\u2020 | 58.00\u2020 | 43.65\u2020 | 95.00\u2020 | 27.21\u2020 | 10.75\u2020 | 16.57 | 6.32 | ", "caption": "Table 2. Results of HtmlRAG without pruning and baselines under the long-context setting. Hit@1 is the proportion of instances where at least one short answer matches. The best and second best results are in bold and underlined. The symbol \u2020\u2020\\dagger\u2020 signifies that our method achieves superior results among baselines in a statistically significant manner (t-test, p\ud835\udc5dpitalic_p-value \u00a1 0.05).", "description": "Table 2 presents a comparison of HtmlRAG (without pruning) and several baseline methods using long-context settings (128K tokens).  The evaluation metrics are Hit@1 (percentage of instances where at least one short answer in the LLM's output matched the gold standard answers), Exact Match (EM) for short answers, and ROUGE-L and BLEU for long answers.  Results across six QA datasets are shown, highlighting the performance of different methods in answering different question types and the statistically significant improvements achieved by HtmlRAG in various metrics.", "section": "4 Experimental Results"}, {"content": "| Method | ASQA Hit@1 | ASQA EM | Hotpot-QA EM | NQ Hit@1 | NQ EM | Trivia-QA Hit@1 | Trivia-QA EM | MuSiQue EM |\n|---|---|---|---|---|---|---|---|---|\n| HtmlRAG | 68.50 | 30.53 | 46.25 | 60.50 | 45.26 | 93.50 | 27.03 | 13.25 |\n| *w/o Block Tree* | 59.50 (9.00% \u2193) | 25.50 (5.03% \u2193) | 40.25 (6.00% \u2193) | 56.25 (4.25% \u2193) | 42.07 (3.19% \u2193) | 92.00 (1.50% \u2193) | 26.59 (0.44% \u2193) | 8.00 (5.25% \u2193) |\n| *w/o Prune-Embed* | 56.75 (11.75% \u2193) | 24.05 (6.48% \u2193) | 37.50 (8.75% \u2193) | 49.50 (11.00% \u2193) | 37.27 (7.99% \u2193) | 91.75 (1.75% \u2193) | 26.02 (1.01% \u2193) | 9.75 (3.50% \u2193) |\n| *w/o Prune-Gen* | 62.00 (6.50% \u2193) | 26.74 (3.79% \u2193) | 38.75 (7.50% \u2193) | 57.75 (2.75% \u2193) | 42.91 (2.35% \u2193) | 89.50 (4.00% \u2193) | 25.55 (1.48% \u2193) | 7.00 (6.25% \u2193) |", "caption": "Table 3. Ablation studies for HtmlRAG.", "description": "This table presents the ablation study results for the HtmlRAG model.  It shows the impact of removing key components of the model, such as the block tree structure, the text embedding-based pruning, and the generative model-based pruning.  By comparing the performance of HtmlRAG with and without each component, we can understand the individual contributions of each component to the overall effectiveness of the system. The results are presented in terms of different metrics across six different question answering datasets.", "section": "4 Experiments"}, {"content": "| Result Length | # Params | Storage | # In-Tokens | # Out-Tokens |\n|---|---|---|---|---|\n| BGE | 200M | 2.5G | 93.54K | 740.3 |\n| Prune-Embed | 200M | 2.5G | 152.5K | 2653 |\n| Prune-Gen | 3B | 7.2G | 6750 | 28.70 |\n| LLM Chat | 70B | 131G | 3661 | 182.9 |", "caption": "Table 4. Analysis of inference cost on ELI5 dataset We compare the chunking-based refiner using BGE (BGE), the two HTML pruning steps basing on the text embedding (Prune-Embed) and the generative model (Prune-Gen) in HtmlRAG, and LLM chatting (LLM Chat) by model parameters, storage, average input tokens, and average output tokens.", "description": "Table 4 compares the computational resource requirements and the performance of four different methods for processing text in a Retrieval Augmented Generation (RAG) system using the ELI5 dataset.  The methods compared are: a chunking-based refiner using the BGE embedding model (BGE), the text embedding-based pruning step (Prune-Embed), the generative model-based pruning step (Prune-Gen), both from the HtmlRAG method, and using a Large Language Model directly for chatting (LLM Chat). The comparison is based on model parameters, storage space used, average number of input tokens, and average number of output tokens.", "section": "4.6.4 Light Weight HTML Pruning"}]