[{"figure_path": "https://arxiv.org/html/2411.07199/x1.png", "caption": "Figure 1: Editing high-resolution multi-aspect images with Omni-Edit. Omni-Edit is an instruction-based image editing generalist capable of performing diverse editing tasks across different aspect ratios and resolutions. It accurately follows instructions while preserving the original image\u2019s fidelity. We suggest zooming in for better visualization.", "description": "This figure showcases Omni-Edit's ability to edit high-resolution images with various aspect ratios.  It demonstrates the model's versatility by accurately executing diverse editing instructions across different image sizes and orientations, while maintaining the original image quality.  The example edits range from simple object replacements to complex scene modifications, highlighting Omni-Edit's proficiency in instruction-based image manipulation. Zooming in on the images allows for a more detailed observation of the results. ", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2411.07199/x2.png", "caption": "Figure 2: Overview of the Omni-Edit training pipeline.", "description": "The Omni-Edit training pipeline consists of four stages. Stage 1 involves training seven specialist models, each focusing on a specific image editing task (object swap, removal, addition, attribute modification, background swap, environment change, style transfer).  These specialists are trained using a combination of pre-trained text-to-image models and task-specific augmentations. Stage 2 uses these specialists to generate synthetic image editing datasets for each task. Stage 3 incorporates an importance sampling method using a large multimodal model (like GPT-4) to filter noisy or low-quality data from the synthetic datasets, ensuring high-quality training data.  Finally, Stage 4 trains the Omni-Edit generalist model using the high-quality, multi-task data generated in the previous stages. The specialist models act as supervisors to guide the learning of the generalist model.  This approach allows Omni-Edit to handle diverse and complex image editing instructions.", "section": "3 LEARNING WITH SPECIALIST SUPERVISION"}, {"figure_path": "https://arxiv.org/html/2411.07199/x3.png", "caption": "Figure 3: InternVL2 as a scoring function before (top right) and after (bottom right) fine-tuning on GPT-4o\u2019s response. On the top right, the original InternVL2 fails to identify the unusual distortions in the edited image it also does not spot the error when the edited image fails to meet the specified editing instructions. On the bottom right, finetuned-InternVL2 successfully detects such failures and serve as a reliable scoring function.", "description": "This figure demonstrates the improvement in InternVL2's performance as a scoring function after fine-tuning with GPT-40 responses. The top-right panel shows the original InternVL2 failing to detect distortions or inconsistencies in an edited image, even when it does not adhere to instructions.  The bottom-right panel shows the fine-tuned InternVL2 accurately identifying such issues, showcasing its enhanced ability to evaluate the quality of image edits.  This improved scoring function is crucial for selecting high-quality training data.", "section": "3 LEARNING WITH SPECIALIST SUPERVISION"}, {"figure_path": "https://arxiv.org/html/2411.07199/x4.png", "caption": "Figure 4: Architecture Comparison between EditNet(ours), ControlNet and InstructPix2Pix(Channel-wise concatenation) for DiT models. Unlike ControlNet\u2019s parallel execution, EditNet allows adaptive adjustment of control signals by intermediate representations interaction between the control branch and the original branch. EditNet also updates the text representation, enabling better task understanding.", "description": "Figure 4 compares the architecture of three different diffusion-based image editing models: EditNet (the authors' model), ControlNet, and InstructPix2Pix.  The figure highlights the key differences in how these models incorporate control signals (from text prompts and other conditioning information) to modify the image generation process.  ControlNet uses parallel execution of a control branch alongside the main generation branch. In contrast, EditNet allows for a more dynamic and adaptive adjustment of control signals through an interaction between the control and main branches, facilitated by intermediate representations.  This interaction allows for better understanding of the text prompt and thus, more effective editing. Finally, EditNet also updates the text representation itself, further enhancing task comprehension. InstructPix2Pix employs a simple channel-wise concatenation of control signals with the main image representation.", "section": "4 EDITNET"}, {"figure_path": "https://arxiv.org/html/2411.07199/x5.png", "caption": "Figure 5: Qualitative comparison between baselines and Omni-Edit on a subset of the test set.", "description": "Figure 5 presents a qualitative comparison of image editing results produced by OMNI-Edit and several baseline methods.  The figure showcases examples from a subset of the test set, highlighting OMNI-Edit's superior performance in various editing tasks. By directly comparing the visual outputs side-by-side, the reader can readily assess the differences in editing quality, accuracy, and adherence to instructions across the various models.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2411.07199/x6.png", "caption": "Figure 6: Omni-Edit-ControlNet fails to grasp the task intent, while Omni-Edit-ControlNet-TextControl\u2014a variant with a text-updating branch\u2014recognizes the intent but struggles with content removal. In contrast, Omni-Edit accurately removes content.", "description": "Figure 6 presents a comparative analysis of three different models on an object removal task.  The first model, Omni-Edit-ControlNet, demonstrates a failure to understand the task instructions, resulting in an unsuccessful edit. The second model, Omni-Edit-ControlNet-TextControl, which includes a text-updating component, correctly interprets the task; however, it struggles to fully remove the targeted object, leaving remnants.  The third model, Omni-Edit, successfully executes the object removal task, completely eliminating the desired object.", "section": "4 EditNet"}, {"figure_path": "https://arxiv.org/html/2411.07199/x7.png", "caption": "Figure 7: (a) shows the source image. (d) presents images generated by SD3 in response to prompts for \u201can upper body picture of Batman\u201d and \u201ca shiny red vintage Chevrolet Bel Air car.\u201d We use the prompts \u201cReplace the man with Batman\u201d and \u201cAdd a shiny red vintage Chevrolet Bel Air car to the right\u201d to Omni-Edit and Omni-Edit-Channel-Wise-Concatenation, which was trained on Omni-Edit training data. From (b) and (c), one can observe that Omni-Edit preserves the generation capabilities of SD3, while Omni-Edit-Channel-Wise-Concatenation exhibits a notable degradation in generation capability.", "description": "Figure 7 demonstrates a comparison of image editing results between Omni-Edit and Omni-Edit-Channel-Wise-Concatenation, highlighting Omni-Edit's ability to maintain the original image generation capabilities of the base model (SD3) while performing edits.  The experiment involves replacing a person in an image with Batman and adding a vintage car.  Omni-Edit successfully integrates these edits while preserving image quality.  In contrast, Omni-Edit-Channel-Wise-Concatenation shows a significant decline in image generation quality after edits, indicating a compromise in the base model's generation capabilities.", "section": "4 EditNet"}, {"figure_path": "https://arxiv.org/html/2411.07199/x8.png", "caption": "Figure 8: Prompt for evaluating SC score.", "description": "This figure shows the prompt used to evaluate the Semantic Consistency (SC) score in the OMNI-EDIT model's performance.  The prompt instructs an evaluator (acting as a professional digital artist) to assess two images: an original AI-generated image and an edited version.  The evaluator must rate how well the edited image follows the given editing instructions on a scale of 0 to 10, with 0 representing complete failure and 10 representing perfect adherence. A second rating (also 0-10) assesses the degree of overediting in the image. The prompt provides detailed instructions for how to format the numerical scores and associated textual rationale.", "section": "5 Experiments"}, {"figure_path": "https://arxiv.org/html/2411.07199/x9.png", "caption": "Figure 9: Prompt for evaluating PQ score.", "description": "This figure shows the prompt used for human evaluators to assess the perceptual quality (PQ) of images generated by the OMNI-EDIT model and its baselines.  The evaluators are instructed to act as professional digital artists, rating the image quality on a scale of 0-10, based solely on technical aspects like distortions, unnatural proportions, and artifacts.  They are explicitly told to ignore contextual realism or the naturalness of the scene.", "section": "5 Experiments"}]