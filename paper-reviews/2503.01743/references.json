{"references": [{"fullname_first_author": "Marah Abdin", "paper_title": "Phi-4 Technical Report", "publication_date": "2024-12-05", "reason": "This paper introduces Phi-4-Multimodal, and describes its architecture, capabilities, and performance."}, {"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-01-01", "reason": "This paper introduces the Transformer architecture, which Phi-4-Multimodal uses as its decoder-only backbone."}, {"fullname_first_author": "Joshua Ainslie", "paper_title": "GQA: Training generalized multi-query transformer models from multi-head checkpoints", "publication_date": "2023-05-19", "reason": "This paper describes Group Query Attention (GQA) which Phi-4-Multimodal uses to optimize memory usage for long-context generation."}, {"fullname_first_author": "Edward J. Hu", "paper_title": "LoRA: Low-Rank Adaptation of Large Language Models", "publication_date": "2022-01-01", "reason": "This paper introduces LoRA, a parameter-efficient technique used for modality extension in Phi-4-Multimodal."}, {"fullname_first_author": "Yuan Liu", "paper_title": "MMBench: Is your multi-modal model an all-around player?", "publication_date": "2024-01-01", "reason": "This paper introduces the MMBench benchmark, a key part of the overall evaluation of a models performance."}]}