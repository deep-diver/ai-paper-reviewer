[{"figure_path": "https://arxiv.org/html/2503.19953/x1.png", "caption": "Figure 1: Extracting flow and occlusion with counterfactual perturbation: (A) CWMs learn to predict the next frame with a temporally factored masking policy\u00a0[4]. (B) The motion of a point can be estimated using a simple counterfactual probing program FLOW: the model predicts the next frame with and without a local perturbation placed on the point, and the difference image between the clean and perturbed predictions reveals the estimated motion. (C) Occlusion is estimated using a related probe OCC: when the perturbation difference image is diffuse and low magnitude, that indicates the perturbed point has been occluded.", "description": "This figure illustrates the process of extracting optical flow and occlusion information using counterfactual perturbations within the context of Counterfactual World Modeling (CWM). Panel (A) introduces the core concept of CWM, where a model learns to predict the next frame of a video sequence based on a sparsely masked input. The asymmetry in masking the frames enforces the model to learn temporal dynamics efficiently. Panel (B) describes how to estimate optical flow by using a 'FLOW' probe. This probe involves applying a local perturbation to a specific point in the first frame and comparing the model's prediction of the next frame with and without the perturbation. The difference between these two predictions reveals the estimated motion of that point. Finally, panel (C) explains how to estimate occlusion using a similar probe named 'OCC'. If the resulting difference image is diffuse and has low magnitude, it suggests that the point has been occluded in the following frame.", "section": "3. Methods"}, {"figure_path": "https://arxiv.org/html/2503.19953/x2.png", "caption": "Figure 2: Parameterizing the counterfactual intervention policy as an input-conditioned function. (A) Building on a pre-trained RGB-conditioned predictor \ud835\udebfRGBsuperscript\ud835\udebfRGB\\boldsymbol{\\Psi}^{\\texttt{RGB}}bold_\u03a8 start_POSTSUPERSCRIPT RGB end_POSTSUPERSCRIPT, Opt-CWM\u00a0uses an image-conditioned perturbation prediction function \u03b4\u03b8subscript\ud835\udeff\ud835\udf03\\delta_{\\theta}italic_\u03b4 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT containing a small MLP\u03b8. As illustrated in B, \u03b4\u03b8subscript\ud835\udeff\ud835\udf03\\delta_{\\theta}italic_\u03b4 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT can learn to predict image-conditioned perturbations that blend naturally with the underlying scene, potentially allowing for the perturbation to be accurately carried over to the next frame prediction. But how should the parameters of \u03b4\u03b8subscript\ud835\udeff\ud835\udf03\\delta_{\\theta}italic_\u03b4 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT be learned to achieve this, without any flow supervision labels? See Figure\u00a03.", "description": "This figure illustrates how the Opt-CWM model parameterizes counterfactual perturbations to improve motion estimation.  Panel (A) shows the architecture, where a learned perturbation generator (\u03b4\u03b8) is added to a pre-trained next-frame prediction model (\u03a8RGB).  This generator uses an MLP to predict perturbations based on image content. Panel (B) contrasts hand-designed perturbations, which often fail to move consistently with objects, against learned perturbations, which integrate more seamlessly with the scene dynamics. This shows how the learnable perturbation generator addresses limitations of hand-designed methods. The caption asks how to learn the parameters of the perturbation generator without labeled data, which is answered in Figure 3.", "section": "3. Methods"}, {"figure_path": "https://arxiv.org/html/2503.19953/x3.png", "caption": "Figure 3: A generic principle for learning optimal counterfactuals. A) The parameterized counterfactual flow function FLOW\u03b8subscriptFLOW\ud835\udf03\\textbf{{FLOW}}_{\\theta}FLOW start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT extracts motion from a frozen RGB-conditioned predictor \ud835\udebfRGBsuperscript\ud835\udebfRGB\\boldsymbol{\\Psi}^{\\texttt{RGB}}bold_\u03a8 start_POSTSUPERSCRIPT RGB end_POSTSUPERSCRIPT through counterfactual perturbation (details in Figure\u00a02). Its parameters \u03b8\ud835\udf03\\thetaitalic_\u03b8 are trained using gradients from a flow-conditioned predictor \ud835\udebf\u03b7flowsubscriptsuperscript\ud835\udebfflow\ud835\udf02\\boldsymbol{\\Psi}^{\\texttt{flow}}_{\\eta}bold_\u03a8 start_POSTSUPERSCRIPT flow end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_\u03b7 end_POSTSUBSCRIPT that is jointly trained to perform next-frame prediction. The predictor \ud835\udebfflowsuperscript\ud835\udebfflow\\boldsymbol{\\Psi}^{\\texttt{flow}}bold_\u03a8 start_POSTSUPERSCRIPT flow end_POSTSUPERSCRIPT can only learn to predict future frames if it is given correct flow vectors. This explicit information bottleneck ensures useful gradients will get passed back to FLOW\u03b8subscriptFLOW\ud835\udf03\\textbf{{FLOW}}_{\\theta}FLOW start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT . This setup allows us to get better extractions from a pre-trained \ud835\udebfRGBsuperscript\ud835\udebfRGB\\boldsymbol{\\Psi}^{\\texttt{RGB}}bold_\u03a8 start_POSTSUPERSCRIPT RGB end_POSTSUPERSCRIPT predictor by training another flow-conditoned predictor \ud835\udebfflowsuperscript\ud835\udebfflow\\boldsymbol{\\Psi}^{\\texttt{flow}}bold_\u03a8 start_POSTSUPERSCRIPT flow end_POSTSUPERSCRIPT using the same principle of next-frame prediction. (B) As a consequence of tight coupling between the flow-conditioned predictor \ud835\udebfflowsuperscript\ud835\udebfflow\\boldsymbol{\\Psi}^{\\texttt{flow}}bold_\u03a8 start_POSTSUPERSCRIPT flow end_POSTSUPERSCRIPT and the learned flow estimation function FLOW\u03b8subscriptFLOW\ud835\udf03\\textbf{{FLOW}}_{\\theta}FLOW start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT, both motion estimation and pixel reconstruction simultaneously improve.", "description": "Figure 3 illustrates the training process of the Opt-CWM model.  Panel A shows the architecture: a pre-trained RGB-conditioned next-frame predictor (\u03a8RGB) is used as a base model.  A learnable counterfactual perturbation generator (FLOW\u03b8) creates perturbations, which are applied to the input frame before feeding it to \u03a8RGB. The output of \u03a8RGB is compared with the prediction without perturbation.  This difference provides information about motion, which is used to train a flow-conditioned next-frame predictor (\u03a8flow\u03b7).  The latter model learns to predict the next frame based on both the original frame and the sparse flow information (which necessitates accurate flow estimates from FLOW\u03b8). Panel B shows how the reconstruction and flow estimation errors change during training. The tight coupling between the two models ensures that improvements in motion estimation directly benefit frame reconstruction, and vice-versa.", "section": "3. Methods"}, {"figure_path": "https://arxiv.org/html/2503.19953/x4.png", "caption": "Figure 4: \nQualitative comparison with baselines on real-world videos. The above examples show the failure modes of previous methods that rely on visual similarity or photometric loss. We observe that the baseline models struggle against subtle but functionally important changes in largely homogeneous scenes depicting objects of similar color and texture ((a) - (e)). Further, the use of photometric loss in self-supervised methods such as SMURF can also be susceptible to differences in light intensity across frame pairs ((f) - (h)). Opt-CWM, however, relies on a holistic understanding of scene transformations and object dynamics and is able to find correspondence without arbitrary heuristics.", "description": "This figure showcases a qualitative comparison of Opt-CWM's performance against other state-of-the-art methods on real-world video sequences. The examples highlight scenarios where methods relying on visual similarity or photometric loss fail.  Specifically, it demonstrates that baselines struggle with subtle but significant changes within homogeneous scenes, particularly when objects share similar colors and textures (examples (a) through (e)).  It also demonstrates the vulnerability of photometric loss-based methods, like SMURF, to variations in light intensity between frames (examples (f) through (h)). In contrast, Opt-CWM, due to its holistic approach which considers scene transformations and dynamics, consistently achieves superior performance in these challenging situations. ", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.19953/x5.png", "caption": "Figure 5: Perturbation maps emergently reflect scene properties. For two example frame pairs, we show the amplitudes and standard deviations, at each spatial position and for each color channel, of the optimal Gaussian perturbations predicted by MLP\u03b8. These \u201cperturbation maps\u201d emergently reflect scene properties, with perturbation parameters varying in size and magnitude depending on where they are located in the image, corresponding to the presence of foreground objects and their parts.", "description": "This figure visualizes learned perturbation maps generated by a learned MLP (Multilayer Perceptron).  The maps show the amplitudes and standard deviations of optimal Gaussian perturbations predicted for each pixel location and color channel across two example frame pairs.  The size and magnitude of these perturbations dynamically adapt to local scene content.  Specifically, they reflect the presence and characteristics of foreground objects.  The perturbations are not uniform but are shaped to match features of the image, suggesting the model has learned to represent scene properties within them.", "section": "3.2.1 Parameterized Perturbations"}, {"figure_path": "https://arxiv.org/html/2503.19953/x6.png", "caption": "Figure 6: <\ud835\udf39avgabsentsubscript\ud835\udf39avg<\\boldsymbol{\\delta}_{\\text{avg}}< bold_italic_\u03b4 start_POSTSUBSCRIPT avg end_POSTSUBSCRIPT broken down across thresholds (x\ud835\udc65xitalic_x-axis). Fraction of points with error less than a fixed threshold, as a function of number of multiscale (MS) iterations, for pixel thresholds 1, 2, 4, 8, and 16. We find that 4 zoom iterations tends to perform the best, especially for robustness on difficult examples (evidenced by better performance on higher thresholds).", "description": "This figure analyzes the impact of multiscale refinement on the accuracy of flow estimation. The x-axis represents different pixel error thresholds, while the y-axis shows the fraction of points whose error is below each threshold.  Different colored bars represent varying numbers of multiscale iterations. The results indicate that 4 zoom iterations provide the best performance overall, particularly when dealing with challenging cases (as demonstrated by its better performance at higher error thresholds).", "section": "A.3.2. Multiscale"}, {"figure_path": "https://arxiv.org/html/2503.19953/x7.png", "caption": "Figure 7: Model comparison as a function of frame gap. Higher frame gaps present harder flow estimation problems due to including more motion, as reflected by improved performance across models in lower frame gap settings. Opt-CWM\u00a0and Doduo perform better as frame gap increases, in contrast to optical flow methods SEA-RAFT and SMURF which decay in performance as motion magnitude increases, especially on the AD metric.", "description": "This figure compares the performance of various optical flow estimation methods (Opt-CWM, Doduo, SEA-RAFT, and SMURF) as the gap between frames increases. The x-axis represents the frame gap, while the y-axis shows the average pixel distance (AD) error, a key metric of accuracy. The results reveal that Opt-CWM and Doduo maintain relatively high accuracy even with larger frame gaps, unlike SEA-RAFT and SMURF, whose accuracy significantly decreases as the frame gap and motion magnitude increase.  This suggests that Opt-CWM and Doduo are more robust to challenging real-world scenarios with substantial motion between frames.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.19953/x8.png", "caption": "Figure 8: TAP-Vid First: comparing baseline models on <\u03b4avgabsentsubscript\ud835\udeffavg<\\boldsymbol{\\delta}_{\\text{avg}}< bold_italic_\u03b4 start_POSTSUBSCRIPT avg end_POSTSUBSCRIPT broken down across thresholds (x\ud835\udc65xitalic_x-axis). Fraction of points with error less than a fixed threshold, as a function of baseline model. Compared to baseline models, Opt-CWM\u00a0maintains high performance on all thresholds even when making predictions across large frame gaps, as is necessary for TAP-Vid First.", "description": "Figure 8 presents a detailed analysis of the accuracy of different models in predicting the location of points across various error thresholds.  The x-axis represents the error threshold (in pixels), while the y-axis shows the percentage of points for which the predicted location falls within that threshold of the ground truth. The graph compares Opt-CWM with three baseline models: SEA-RAFT, Doduo, and SMURF.  Importantly, the evaluation considers videos with large gaps between frames (the TAP-Vid First protocol), making accurate point tracking particularly challenging. The results demonstrate that Opt-CWM consistently achieves higher accuracy across all error thresholds, particularly for larger thresholds. This highlights the robustness of Opt-CWM in handling scenarios with significant motion and longer frame intervals, which cause challenges for other methods.", "section": "B.3. Precision Analysis"}, {"figure_path": "https://arxiv.org/html/2503.19953/x9.png", "caption": "Figure 9: TAP-Vid CFG: comparing baseline models on <\u03b4avgabsentsubscript\ud835\udeffavg<\\boldsymbol{\\delta}_{\\text{avg}}< bold_italic_\u03b4 start_POSTSUBSCRIPT avg end_POSTSUBSCRIPT broken down across thresholds (x\ud835\udc65xitalic_x-axis). Fraction of points with error less than a fixed threshold, as a function of baseline model. For fair comparison, we also evaluate on a constant frame gap setting that is more favorable to optical flow baselines. While baseline methods show strong performance for very low thresholds (<2absent2<2< 2 pixels), we see that in general Opt-CWM\u00a0outperforms self-supervised methods and is comparable with SEA-RAFT in predicting more points within a reasonable boundary.", "description": "Figure 9 presents a comparative analysis of various models' performance on the TAP-Vid CFG (Constant Frame Gap) benchmark, focusing on the average point tracking error.  The x-axis represents different error thresholds (in pixels), and the y-axis shows the percentage of points whose error falls below each threshold.  The dataset uses a fixed frame gap, making it advantageous for optical flow methods. While supervised and unsupervised optical flow methods perform well at very low error thresholds (under 2 pixels), Opt-CWM demonstrates superior performance overall, outperforming self-supervised approaches and achieving comparable results to the supervised SEA-RAFT method in predicting a higher proportion of points within an acceptable error range.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.19953/x10.png", "caption": "Figure 10: Evolution of perturbations across training epochs: We observe how the predicted perturbations change as the model trains. The perturbation starts as a disjoint streak of colors and converges to a localized peak. This in turn increasingly concentrates the difference image \ud835\udeab\ud835\udeab\\boldsymbol{\\Delta}bold_\u0394 and leads to better flow prediction. Green is the ground truth flow obtained from the TAP-Vid dataset, and blue is our model\u2019s prediction.", "description": "This figure visualizes the evolution of learned perturbations throughout the training process of the Opt-CWM model.  It shows how the model's ability to generate effective perturbations improves over time. Initially, the perturbations are scattered and lack coherence.  As training progresses, these perturbations become more concentrated and localized, improving the accuracy of the resulting flow predictions. The figure demonstrates this by showing example perturbation maps at various training epochs alongside a comparison of the ground truth flow (green) from the TAP-Vid dataset and the Opt-CWM model's predicted flow (blue) for those same examples. The decreasing error between the ground truth flow and the model's prediction, as demonstrated by the diminishing numerical error value shown, is directly correlated to the evolution of the perturbation.", "section": "3. Methods"}]