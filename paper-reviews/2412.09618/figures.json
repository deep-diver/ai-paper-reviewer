[{"figure_path": "https://arxiv.org/html/2412.09618/x2.png", "caption": "Figure 1: EasyRef is capable of modeling the consistent visual elements of various input reference images with a single generalist multimodal LLM in a zero-shot setting.", "description": "This figure demonstrates EasyRef's ability to generate images that maintain consistent visual elements from multiple reference images, even without any fine-tuning.  It uses a single, general-purpose multimodal large language model (MLLM) to understand and combine the visual information from the different references, resulting in a generated image that reflects the shared visual characteristics. This is showcased across three examples, demonstrating style, identity, and character consistency.  Each example provides a text prompt and several reference images, highlighting the model's ability to incorporate these diverse references into a single cohesive output.", "section": "Abstract"}, {"figure_path": "https://arxiv.org/html/2412.09618/x3.png", "caption": "Figure 2: Spatial misalignment issue of the embedding averaging operation. The images with faces are synthetic.", "description": "This figure illustrates the problem of inconsistent image generation when using a simple averaging method for embedding multiple reference images.  The method averages the image embeddings as input conditions for image generation.  However, when the spatial location of the target subject varies across multiple reference images, the generated image becomes inconsistent, resulting in unsatisfactory results.  The example images show that images with faces are particularly affected by this issue. This highlights the limitation of simple averaging methods that do not account for the spatial relationships between different images when generating images conditioned on multiple references.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2412.09618/x4.png", "caption": "Figure 3: Overview of EasyRef with SDXL. EasyRef extracts consistent visual eliments from multiple reference images and the text prompt via a MLLM, injecting the condition representations into the diffusion model through cross-attention layers. We only plot 1 cross-attention layer for simplicity.", "description": "EasyRef uses a multimodal large language model (MLLM) to process multiple reference images and a text prompt.  The MLLM extracts consistent visual elements from these inputs and creates a representation that is injected into a Stable Diffusion XL (SDXL) model via cross-attention layers. This allows the diffusion model to generate images that reflect the style, content, and other characteristics present in the reference images and the text prompt.  The diagram simplifies the process by showing only one cross-attention layer.", "section": "3. EasyRef"}, {"figure_path": "https://arxiv.org/html/2412.09618/x5.png", "caption": "Figure 4: Distribution of our curated dataset.", "description": "This histogram displays the distribution of the number of images per group in the curated dataset used for training and evaluating the EasyRef model.  The x-axis represents the number of images in a group, and the y-axis shows the percentage of groups containing that number of images.  The dataset is designed to have a balanced distribution of group sizes for robust training and evaluation.  It shows that a significant portion of the groups contains around 10-15 images, with the distribution tapering off toward both smaller and larger group sizes.", "section": "4. Multi-Reference Generation Benchmark"}, {"figure_path": "https://arxiv.org/html/2412.09618/x6.png", "caption": "Figure 5: Comparisons of EasyRef with other counterparts in various single-image reference scenarios. The same image prompts as in\u00a0[48] are used for clear comparisons.", "description": "This figure compares the image generation results of EasyRef against several other methods across various single-image reference scenarios.  The same prompts used in a previous paper ([48]) were replicated here for a controlled comparison to highlight the differences in image generation quality, style consistency, and adherence to the source image.  Each method is presented side-by-side for a clear visual comparison.", "section": "5.2. Quantitative and Qualitative Results"}, {"figure_path": "https://arxiv.org/html/2412.09618/x7.png", "caption": "Figure 6: Comparisons of human preference evaluation on our MRBench. EasyRef can surpass other methods across the aesthetic quality and reference alignment.", "description": "This figure presents a bar chart comparing the performance of EasyRef against other methods (IP-Adapter and LoRA) based on human preference evaluations using the MRBench dataset.  The chart shows the win rate (percentage of times a method was preferred over another in paired comparisons) for each model across two key aspects: aesthetic quality and reference consistency.  The results demonstrate EasyRef's superior performance in generating images that are both visually appealing and closely aligned with the provided reference images.", "section": "5.2. Quantitative and Qualitative Results"}, {"figure_path": "https://arxiv.org/html/2412.09618/x8.png", "caption": "Figure 7: Visualization of generated samples with various multi-reference inputs. These reference contents encompass style, identity, and character, and are encoded by a single generalist MLLM in EasyRef.", "description": "Figure 7 showcases the results of image generation using EasyRef with multiple reference images.  Each row presents a different set of reference images, demonstrating EasyRef's ability to synthesize images that capture various styles, identities, and characters present in the references. A single multimodal large language model (MLLM) within EasyRef encodes all these diverse aspects simultaneously, highlighting its capacity for omni-generalized group image referencing.", "section": "5.2. Quantitative and Qualitative Results"}, {"figure_path": "https://arxiv.org/html/2412.09618/x9.png", "caption": "Figure 8: Comparison between EasyRef and IP-Adapter-SDXL with additional structure controls.", "description": "Figure 8 presents a comparison of image generation results between EasyRef and IP-Adapter-SDXL, both utilizing Stable Diffusion XL (SDXL).  The key difference is that additional structural controls are provided as input to both models.  This allows for an assessment of how well each model handles both multi-reference images and the incorporation of explicit structural guidance in the image generation process.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.09618/x10.png", "caption": "Figure 9: Evaluation of inference group size scaling. We randomly select 112 reference images and 1 target image-text pair with the same topic. Then we compute the similarities between the generated images and the target image. \u201cLatency\u201d in the figure is measured in seconds per image.", "description": "This figure analyzes how EasyRef's performance changes with varying numbers of reference images used during inference.  The experiment randomly selects 112 reference images and 1 target image-text pair with the same topic. The generated images are then compared to the target image using similarity metrics (CLIP-I, CLIP-T, and DINO-I). The inference time, or latency, for each test is also measured and reported in seconds.  This demonstrates EasyRef's efficiency and robustness across different numbers of reference images.", "section": "5.3 Ablation Study"}, {"figure_path": "https://arxiv.org/html/2412.09618/x11.png", "caption": "Figure 10: Impact of the multimodal instruction design.", "description": "This ablation study investigates the effect of using multimodal instructions (combining image and text prompts) within the EasyRef model.  The figure demonstrates a comparison of results obtained when using multimodal instructions versus the results using only image prompts or only text prompts.  It likely shows that combining image and text information in the prompts leads to improved generation quality, as measured by CLIP-I, CLIP-T, and DINO-I metrics.", "section": "5.3 Ablation Study"}, {"figure_path": "https://arxiv.org/html/2412.09618/x13.png", "caption": "Figure 11: Effect of the progressive training scheme. \u201cstage 1\u201d and \u201cstage 2\u201d denote the alignment pretraining stage and single-reference finetuning stage, respectively.", "description": "This figure demonstrates the impact of each training stage (alignment pretraining and single-reference finetuning) on the model's ability to generate high-quality images.  By systematically removing each stage, the experiment visualizes how each stage contributes to the overall image quality, focusing on fine-grained details and identity preservation. The results show that while the model performs well even without all stages for some reference images, there are noticeable improvements in image quality, specifically for reference images involving identity preservation (like a portrait of Taylor Swift) or complex compositions when all phases are included in the training.", "section": "5.3 Ablation Study"}, {"figure_path": "https://arxiv.org/html/2412.09618/x14.png", "caption": "Figure 1: More generated samples of identity preservation with EasyRef in a zero-shot setting. We use the face images of celebrities in this experiment.", "description": "This figure showcases additional examples of EasyRef's ability to maintain identity consistency when generating images from multiple references in a zero-shot setting.  The input references consist of various images of celebrities' faces, demonstrating EasyRef's capability to generate new images that accurately reflect the consistent facial features of the provided references without any fine-tuning.", "section": "5.2. Quantitative and Qualitative Results"}]