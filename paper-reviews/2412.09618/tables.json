[{"content": "| Method | Consistency mining | Zero-shot generalization | Multi-reference input |\n|---|---|---|---|\n| LoRA [14] | \u2705 | \u274c | \u274c |\n| IP-Adapter [48] | \u274c | \u2705 | \u2705 |\n| EasyRef | \u2705 | \u2705 | \u2705 |", "caption": "Table 1: Comparison among LoRA, IP-Adapter, and EasyRef.", "description": "This table compares three different methods for adapting diffusion models to generate images based on multiple references: LoRA (Low-Rank Adaptation), IP-Adapter (Image Prompt Adapter), and EasyRef (the proposed method in this paper).  It highlights key differences in their approach to handling multiple reference images and achieving consistency in the generated output, including whether they require fine-tuning, their ability to handle zero-shot generalization, and the type of multi-reference input they support.  This allows readers to understand the relative advantages and disadvantages of each method compared to the proposed EasyRef.", "section": "1. Introduction"}, {"content": "| Method | CLIP-I \u2191 | CLIP-T \u2191 | DINO-I \u2191 |\n|---|---|---|---|\n| *Training from scratch* |  |  |  |\n| Open unCLIP [32] | 0.858 | 0.608 | - |\n| Kandinsky-2-1 [1] | 0.855 | 0.599 | - |\n| Versatile Diffusion [46] | 0.830 | 0.587 | - |\n| *Finetuning* |  |  |  |\n| SD Image Variations | 0.760 | 0.548 | - |\n| SD unCLIP | 0.810 | 0.584 | - |\n| *Adapters* |  |  |  |\n| Uni-ControlNet [50] (Global Control) | 0.736 | 0.506 | - |\n| T2I-Adapter [26] (Style) | 0.648 | 0.485 | - |\n| ControlNet Shuffle [49] | 0.616 | 0.421 | - |\n| IP-Adapter\u2217 [48] | 0.828 | 0.588 | - |\n| IP-Adapter-SDXL\u2217 [48] | 0.836 | 0.617 | 0.650 |\n| EasyRef | **0.876** | **0.621** | **0.873** |", "caption": "Table 2: Evaluation for generation conditioned by COCO validation images. Methods with * use CLIP embeddings and tend to achieve higher scores of CLIP-based metrics due to its preference.", "description": "This table presents the quantitative evaluation results of different image generation methods using the COCO validation dataset.  The evaluation focuses on generating images conditioned on single COCO images.  The table shows the CLIP-I, CLIP-T, and DINO-I scores for various methods.  Note that methods marked with an asterisk (*) utilized CLIP embeddings, which may bias the results towards higher scores on CLIP-based metrics due to the inherent preference of the CLIP model.", "section": "5.2. Quantitative and Qualitative Results"}, {"content": "| Method | CLIP-I \u2191 | CLIP-T \u2191 | DINO-I \u2191 |\n|---|---|---|---| \n| *Held-in split* |  |  |  |\n| LoRA [14] | 0.831 | 0.715 | 0.654 |\n| IP-Adapter-SDXL [48] | 0.768 | 0.632 | 0.527 |\n| EasyRef | **0.843** | **0.726** | **0.672** |\n| *Held-out split* |  |  |  |\n| LoRA [14] | failed | failed | failed |\n| IP-Adapter-SDXL [48] | 0.795 | 0.645 | 0.579 |\n| EasyRef | **0.833** | **0.709** | **0.614** |", "caption": "Table 3: Evaluation for multi-reference image generation on MRBench. \u201cfailed\u201d means LoRA fails to generalize to the unseen held-out split in a zero-shot setting.", "description": "This table presents the quantitative results of evaluating different methods on the MRBench benchmark for multi-reference image generation.  It compares the performance of EasyRef, LoRA, and IP-Adapter on both a held-in (seen during training) and a held-out (unseen during training) split of the dataset. The metrics used are CLIP-I (image-image similarity), CLIP-T (image-text similarity), and DINO-I (image-image similarity).  The results demonstrate the generalization capability of each method by showing the performance on unseen data. LoRA's inability to generalize to the unseen data in a zero-shot setting (indicated by 'failed') highlights the advantage of EasyRef's approach.", "section": "5. Experiments"}, {"content": "| Number | Position | CLIP-I \u2191 | CLIP-T \u2191 | DINO-I \u2191 |\n|---|---|---|---|---|\n| 32 tokens | -1 | 0.813 | 0.693 | 0.591 |\n| 128 tokens | -1 | 0.827 | 0.705 | 0.611 |\n| 64 tokens | -2 | 0.831 | 0.704 | 0.616 |\n| 64 tokens | -3 | 0.828 | 0.702 | **0.617** |\n| 64 tokens | -1 | **0.833** | **0.709** | 0.614 |", "caption": "Table 4: Ablation of reference token design.", "description": "This table presents the ablation study on the number of reference tokens used in the EasyRef model. It shows the impact of varying the number of reference tokens (32, 128, and 64) on the model's performance, measured by CLIP-I, CLIP-T, and DINO-I scores.  It also investigates the effect of placing the reference tokens at different positions within the LLM's layers (-1, -2, -3 denoting the position relative to the final layer). The results demonstrate the optimal number of reference tokens and their ideal placement for balancing performance and efficiency.", "section": "3.3 Progressive Training Scheme"}, {"content": "| Method | Average token number | CLIP-I \u2191 | CLIP-T \u2191 | DINO-I \u2191 |\n|---|---|---|---|---|\n| Average | 64 | 0.818 | 0.688 | 0.584 |\n| Concatenation | 354 | 0.821 | 0.692 | 0.579 |\n| EasyRef | 64 | **0.833** | **0.709** | **0.614** |", "caption": "Table 5: Ablation of reference representation aggregation on the MRBench held-out set. In the implementation, we average or concatenate the MLLM\u2019s representations of reference images.", "description": "This table presents an ablation study on how different methods of aggregating reference image representations affect the performance of the EasyRef model on the MRBench held-out set.  Specifically, it compares the impact of averaging versus concatenating the multimodal large language model (MLLM)'s representations of multiple reference images.  The results show how different aggregation techniques affect the model's ability to generate images consistent with the provided references.", "section": "5.3 Ablation Study"}]