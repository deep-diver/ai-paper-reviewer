[{"figure_path": "https://arxiv.org/html/2411.15671/extracted/6020576/Figures/GSM.png", "caption": "Figure 1: Overview of Graph Sequence Model (GSM). GSM Consists of three stages: (1) Tokenization, (2) Local Encoding, and (3) Global Encoding. We provide a foundation for strengths and weaknesses of different tokenizations and sequence models. Finally, we present three methods to\u00a0enhance\u00a0the\u00a0power\u00a0of\u00a0GSMs.", "description": "The figure illustrates the Graph Sequence Model (GSM), a framework for applying sequence models to graph data. GSM comprises three main stages: Tokenization, which converts graph data into sequences; Local Encoding, which processes local graph structures; and Global Encoding, which utilizes sequence models like RNNs or Transformers to capture long-range dependencies within sequences.  The figure also highlights the strengths and weaknesses of different tokenization techniques (e.g., node vs. subgraph tokenization) and the suitability of various sequence models for specific graph tasks. Finally, the figure introduces three enhancement methods to improve GSM's performance: Hierarchical Affinity Clustering (HAC) for improved tokenization, a hybrid encoder combining RNNs and Transformers, and a Mixture of Tokenization (MoT) approach for adaptive encoding strategies.", "section": "Encoding Graphs to Sequences: A Unified Model"}, {"figure_path": "https://arxiv.org/html/2411.15671/extracted/6020576/Figures/GSM++.png", "caption": "Figure 2: Overview of GSM++. GSM++ is a special instance of GSMs that uses: (1) HAC tokenization, (2) hierarchical PE, and (3) a hybrid sequence model.", "description": "GSM++ is a model that leverages the strengths of both recurrent neural networks and transformers.  It processes graph data in three stages: First, hierarchical affinity clustering (HAC) is used for tokenization, creating a hierarchical sequence representation of the graph.  Second, a local encoding step captures local graph characteristics. Finally, a hybrid global encoding (using both recurrent and transformer architectures) processes the sequences, combining the ability of recurrent networks to handle sequential data effectively and the capability of transformers to capture long-range dependencies. This hybrid approach aims to overcome limitations of solely using either recurrent networks or transformers for graph-based tasks.", "section": "4 Enhancing Graph to Sequence Models"}, {"figure_path": "https://arxiv.org/html/2411.15671/extracted/6020576/Figures/PE-plot.png", "caption": "Figure 3: Normalized score of different combination of tokenization and global encoder (sequence models). Even TTT + HAC is in Top-3 only in 3/7 datasets.", "description": "This figure visualizes the performance of various combinations of tokenization methods and global encoder (sequence model) architectures on seven benchmark graph datasets. Each cell in the heatmap represents the normalized performance score for a specific combination.  The color intensity indicates the ranking, with darker shades representing higher ranks. The figure demonstrates that no single combination consistently outperforms others across all datasets, highlighting the task-dependent nature of optimal model choices.  The caption notes that even the strong combination of TTT (a sequence model) and HAC (Hierarchical Affinity Clustering for tokenization) only achieves a top-3 ranking in three out of the seven datasets.", "section": "5 Experiments"}]