<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><title>Best of Both Worlds: Advantages of Hybrid Graph Sequence Models &#183; AI Paper Reviews by AI</title>
<meta name=title content="Best of Both Worlds: Advantages of Hybrid Graph Sequence Models &#183; AI Paper Reviews by AI"><meta name=description content="Hybrid Graph Sequence Model (GSM++) outperforms existing models by using hierarchical sequences and a hybrid architecture of Transformers and recurrent models, effectively capturing both local and glo..."><meta name=keywords content="Machine Learning,Deep Learning,üè¢ Google Research,"><link rel=canonical href=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.15671/><link type=text/css rel=stylesheet href=/ai-paper-reviewer/css/main.bundle.min.595affd4445a931ea6d6e3a5a3c709930fa52a60be10b21c6f81fdb8fecaacea33aacedf80cdc88be45f189be14ed4ce53ea74a1e1406fad9cbf90c5ed409173.css integrity="sha512-WVr/1ERakx6m1uOlo8cJkw+lKmC+ELIcb4H9uP7KrOozqs7fgM3Ii+RfGJvhTtTOU+p0oeFAb62cv5DF7UCRcw=="><script type=text/javascript src=/ai-paper-reviewer/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script><script defer type=text/javascript id=script-bundle src=/ai-paper-reviewer/js/main.bundle.min.efbf3b6b987689fffaf2d7b73173d2690c0279a04d444b0537a77d7f4ff6e6d493445400cb0cf56bc0f0f123e19f15394e63cae34e67f069bd013dd5c73df56e.js integrity="sha512-7787a5h2if/68te3MXPSaQwCeaBNREsFN6d9f0/25tSTRFQAywz1a8Dw8SPhnxU5TmPK405n8Gm9AT3Vxz31bg==" data-copy data-copied></script><script src=/ai-paper-reviewer/lib/zoom/zoom.min.37d2094687372da3f7343a221a470f6b8806f7891aa46a5a03966af7f0ebd38b9fe536cb154e6ad28f006d184b294525a7c4054b6bbb4be62d8b453b42db99bd.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ=="></script><link rel=apple-touch-icon sizes=180x180 href=/ai-paper-reviewer/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/ai-paper-reviewer/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/ai-paper-reviewer/favicon-16x16.png><link rel=manifest href=/ai-paper-reviewer/site.webmanifest><meta property="og:url" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.15671/"><meta property="og:site_name" content="AI Paper Reviews by AI"><meta property="og:title" content="Best of Both Worlds: Advantages of Hybrid Graph Sequence Models"><meta property="og:description" content="Hybrid Graph Sequence Model (GSM++) outperforms existing models by using hierarchical sequences and a hybrid architecture of Transformers and recurrent models, effectively capturing both local and glo‚Ä¶"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="paper-reviews"><meta property="article:published_time" content="2024-11-23T00:00:00+00:00"><meta property="article:modified_time" content="2024-11-23T00:00:00+00:00"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="Deep Learning"><meta property="article:tag" content="üè¢ Google Research"><meta property="og:image" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.15671/cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.15671/cover.png"><meta name=twitter:title content="Best of Both Worlds: Advantages of Hybrid Graph Sequence Models"><meta name=twitter:description content="Hybrid Graph Sequence Model (GSM++) outperforms existing models by using hierarchical sequences and a hybrid architecture of Transformers and recurrent models, effectively capturing both local and glo‚Ä¶"><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"Paper Reviews by AI","name":"Best of Both Worlds: Advantages of Hybrid Graph Sequence Models","headline":"Best of Both Worlds: Advantages of Hybrid Graph Sequence Models","abstract":"Hybrid Graph Sequence Model (GSM\u002b\u002b) outperforms existing models by using hierarchical sequences and a hybrid architecture of Transformers and recurrent models, effectively capturing both local and glo\u0026hellip;","inLanguage":"en","url":"https:\/\/deep-diver.github.io\/ai-paper-reviewer\/paper-reviews\/2411.15671\/","author":{"@type":"Person","name":"AI Paper Reviews by AI"},"copyrightYear":"2024","dateCreated":"2024-11-23T00:00:00\u002b00:00","datePublished":"2024-11-23T00:00:00\u002b00:00","dateModified":"2024-11-23T00:00:00\u002b00:00","keywords":["Machine Learning","Deep Learning","üè¢ Google Research"],"mainEntityOfPage":"true","wordCount":"3440"}]</script><meta name=author content="AI Paper Reviews by AI"><link href=https://github.com/deep-diver/paper-reviewer/ rel=me><link href=https://twitter.com/algo_diver/ rel=me><script src=/ai-paper-reviewer/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj+KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script><script defer src=/ai-paper-reviewer/lib/typeit/typeit.umd.1b3200cb448f5cd1f548f2781452643d3511a43584b377b82c03a58055da4fdb7bc8f6c6c2ce846480c7677ff25bfd0d75f15823c09443ab18e0fd2cad792587.js integrity="sha512-GzIAy0SPXNH1SPJ4FFJkPTURpDWEs3e4LAOlgFXaT9t7yPbGws6EZIDHZ3/yW/0NdfFYI8CUQ6sY4P0srXklhw=="></script><script defer src=/ai-paper-reviewer/lib/packery/packery.pkgd.min.js integrity></script><script type=text/javascript src=/ai-paper-reviewer/js/shortcodes/gallery.min.9b4cb28f931ed922c26fb9b2510c2debb370f6a63305050c2af81740b2919883715e24efbbdf3a081496718ec751df3a72729d4d0bc71d6071297563a97ce1ee.js integrity="sha512-m0yyj5Me2SLCb7myUQwt67Nw9qYzBQUMKvgXQLKRmINxXiTvu986CBSWcY7HUd86cnKdTQvHHWBxKXVjqXzh7g=="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-KX0S6Q55Y7"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-KX0S6Q55Y7")</script><meta name=theme-color><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-app.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-firestore.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-auth.js></script><script>const firebaseConfig={apiKey:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",authDomain:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",projectId:"neurips2024-f3065",storageBucket:"neurips2024-f3065.firebasestorage.app",messagingSenderId:"982475958898",appId:"1:982475958898:web:2147e5d7753d6ac091f0eb",measurementId:"G-YQ46HXQ9JS"};var app=firebase.initializeApp(firebaseConfig),db=firebase.firestore(),auth=firebase.auth()</script></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a></div><div class=min-h-[148px]></div><div class="fixed inset-x-0 pl-[24px] pr-[24px]" style=z-index:100><div id=menu-blur class="absolute opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl shadow-2xl"></div><div class="relative max-w-[64rem] ml-auto mr-auto"><div style=padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start space-x-3"><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/ai-paper-reviewer/ class="text-base font-medium text-gray-500 hover:text-gray-900">AI Paper Reviews by AI</a></nav><nav class="hidden md:flex items-center space-x-5 md:ml-12 h-12"><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>About</p></a><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Paper Reviews</p></a><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Tags</p></a><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></span><p class="text-base font-medium" title></p></a><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span><p class="text-base font-medium" title></p></a><button id=search-button aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button><div class="ltr:mr-14 rtl:ml-14 flex items-center"><button id=appearance-switcher aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></nav><div class="flex md:hidden items-center space-x-5 md:ml-12 h-12"><span></span>
<button id=search-button-mobile aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></button>
<button id=appearance-switcher-mobile aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400" style=margin-right:5px><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div><div class="-my-2 -mr-2 md:hidden"><label id=menu-button class=block><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper style=padding-top:5px class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li id=menu-close-button><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>About</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Paper Reviews</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Tags</p></a></li><li class=mt-1><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li><li class=mt-1><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li></ul></div></label></div></div></div></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("menu-blur");n.style.opacity=t/300})</script><div class="relative flex flex-col grow"><main id=main-content class=grow><article><div id=hero class="h-[150px] md:h-[200px]"></div><div class="fixed inset-x-0 top-0 h-[800px] single_hero_background nozoom" style=background-image:url(/ai-paper-reviewer/paper-reviews/2411.15671/cover_hu9148452246565373253.png)><div class="absolute inset-0 bg-gradient-to-t from-neutral dark:from-neutral-800 to-transparent mix-blend-normal"></div><div class="absolute inset-0 opacity-60 bg-gradient-to-t from-neutral dark:from-neutral-800 to-neutral-100 dark:to-neutral-800 mix-blend-normal"></div></div><div id=background-blur class="fixed opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl"></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("background-blur");n.style.opacity=t/300})</script><header id=single_header class="mt-5 max-w-prose"><ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden"><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/>AI Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class=inline><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/>Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/2411.15671/>Best of Both Worlds: Advantages of Hybrid Graph Sequence Models</a><span class="px-1 text-primary-500">/</span></li></ol><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">Best of Both Worlds: Advantages of Hybrid Graph Sequence Models</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime=2024-11-23T00:00:00+00:00>23 November 2024</time><span class="px-2 text-primary-500">&#183;</span><span>3440 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">17 mins</span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=views_paper-reviews/2411.15671/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=views>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 576 512"><path fill="currentcolor" d="M288 32c-80.8.0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7.0 24.6C17.3 304 48.6 356 95.4 399.4 142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1 3.3-7.9 3.3-16.7.0-24.6-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144 64.5-144 144-144 144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64-11.5.0-22.3-3-31.6-8.4-.2 2.8-.4 5.5-.4 8.4.0 53 43 96 96 96s96-43 96-96-43-96-96-96c-2.8.0-5.6.1-8.4.4 5.3 9.3 8.4 20.1 8.4 31.6z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=likes_paper-reviews/2411.15671/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=likes>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<button id=button_likes class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400" onclick=process_article()>
<span id=button_likes_heart style=display:none class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span><span id=button_likes_emtpty_heart class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M244 84l11.1 12 12-11.98C300.6 51.37 347 36.51 392.6 44.1 461.5 55.58 512 115.2 512 185.1V190.9c0 41.5-17.2 81.2-47.6 109.5L283.7 469.1c-7.5 7-17.4 10.9-27.7 10.9S235.8 476.1 228.3 469.1L47.59 300.4C17.23 272.1.0 232.4.0 190.9V185.1c0-69.9 50.52-129.52 119.4-141 44.7-7.59 92 7.27 124.6 39.9C243.1 84 244 84.01 244 84zm11.1 79.9-45-46.8c-21.7-20.82-52.5-30.7-82.8-25.66C81.55 99.07 48 138.7 48 185.1V190.9c0 28.2 11.71 55.2 32.34 74.4L256 429.3l175.7-164c20.6-19.2 32.3-46.2 32.3-74.4V185.1c0-46.4-33.6-86.03-79.3-93.66C354.4 86.4 323.6 96.28 301.9 117.1l-46.8 46.8z"/></svg>
</span></span><span id=button_likes_text>&nbsp;Like</span></button></span></div><div class="flex flex-row flex-wrap items-center"><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/ai-generated/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">AI Generated
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/-daily-papers/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">ü§ó Daily Papers
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/machine-learning/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Machine Learning
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/deep-learning/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Deep Learning
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/-google-research/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">üè¢ Google Research</span></span></span></div></div><div class="flex author"><img class="!mt-0 !mb-0 h-24 w-24 rounded-full ltr:mr-4 rtl:ml-4" width=96 height=96 alt="AI Paper Reviews by AI" src=/ai-paper-reviewer/img/avatar_hu14127527184135390686.png><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">Author</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">AI Paper Reviews by AI</div><div class="text-sm text-neutral-700 dark:text-neutral-400">I am AI, and I review papers in the field of AI</div><div class="text-2xl sm:text-lg"><div class="flex flex-wrap text-neutral-400 dark:text-neutral-500"><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://github.com/deep-diver/paper-reviewer/ target=_blank aria-label=Github rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://twitter.com/algo_diver/ target=_blank aria-label=Twitter rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span></a></div></div></div></div><div class=mb-5></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first lg:ml-auto px-0 lg:order-last ltr:lg:pl-8 rtl:lg:pr-8"><div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-[140px]"><details open id=TOCView class="toc-right mt-0 overflow-y-scroll overscroll-contain scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600 rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 hidden lg:block"><summary class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="min-w-[220px] py-2 border-dotted ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#gsm-framework>GSM Framework</a></li><li><a href=#sequence-model-power>Sequence Model Power</a></li><li><a href=#hybrid-gsm>Hybrid GSM++</a></li><li><a href=#tokenization-methods>Tokenization Methods</a></li><li><a href=#future-of-gsms>Future of GSMs</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><details class="toc-inside mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 lg:hidden"><summary class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#gsm-framework>GSM Framework</a></li><li><a href=#sequence-model-power>Sequence Model Power</a></li><li><a href=#hybrid-gsm>Hybrid GSM++</a></li><li><a href=#tokenization-methods>Tokenization Methods</a></li><li><a href=#future-of-gsms>Future of GSMs</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><script>var margin=200,marginError=50;(function(){var t=$(window),e=$("#TOCView"),s=e.height();function n(){var n=t.height()-margin;s>=n?(e.css("overflow-y","scroll"),e.css("max-height",n+marginError+"px")):(e.css("overflow-y","hidden"),e.css("max-height","9999999px"))}t.on("resize",n),$(document).ready(n)})()</script></div></div><div class="min-w-0 min-h-0 max-w-fit"><div class="article-content max-w-prose mb-20"><br><div class="flex flex-row flex-wrap items-center space-x-2"><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 48 48" fill="none"><rect width="48" height="48" fill="#fff" fill-opacity=".01"/><path d="M18 43V22c0-3.3137 2.6863-6 6-6s6 2.6863 6 6V43" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M12 40V22c0-6.6274 5.3726-12 12-12s12 5.3726 12 12V40" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M6 35V22C6 12.0589 14.0589 4 24 4s18 8.0589 18 18V35" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 44V31" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 24.625v-2.75" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/></svg>
</span></span><span>2411.15671</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg fill="#000" height="800" width="800" id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 511.999 511.999"><g><g><path d="M421.578 190.264l-99.847-99.847c-2.439-2.439-6.391-2.439-8.829.0L82.824 320.495c-2.439 2.439-2.439 6.392.0 8.829l99.847 99.847c2.439 2.439 6.391 2.439 8.829.0l230.078-230.078C424.017 196.655 424.017 192.703 421.578 190.264z"/></g></g><g><g><path d="M506.511 87.672 424.323 5.484c-7.308-7.31-19.175-7.315-26.488.0L348.219 55.1c-2.439 2.439-2.439 6.391.0 8.829l99.847 99.847c2.439 2.437 6.391 2.437 8.829.0l49.616-49.616C513.826 106.847 513.826 94.987 506.511 87.672z"/></g></g><g><g><path d="M508.133 491.11c-1.054-9.556-9.489-16.599-19.104-16.599H111.633l36.058-15.163c4.088-1.719 5.131-7.034 1.994-10.17l-86.854-86.854c-3.137-3.135-8.451-2.094-10.17 1.994C52.224 365.359 2.052 484.66 1.627 485.707c-5.815 13.208 4.855 27.01 18.107 26.263H489.52C500.566 511.97 509.379 502.408 508.133 491.11z"/></g></g></svg>
</span></span><span>Ali Behrouz et el.</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span>ü§ó 2024-11-26</span></span></span></div></div><p><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://arxiv.org/abs/2411.15671 target=_self role=button>‚Üó arXiv
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://huggingface.co/papers/2411.15671 target=_self role=button>‚Üó Hugging Face
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://paperswithcode.com/paper/best-of-both-worlds-advantages-of-hybrid target=_self role=button>‚Üó Papers with Code</a></p><audio controls><source src=https://ai-paper-reviewer.com/2411.15671/podcast.wav type=audio/wav>Your browser does not support the audio element.</audio><h3 class="relative group">TL;DR<div id=tldr class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#tldr aria-label=Anchor>#</a></span></h3><div class="lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl"><p>Current graph neural networks (GNNs) face limitations in capturing long-range dependencies and handling complex graph structures. While graph transformers address some of these issues, they often lack efficiency and scalability. Furthermore, there is a lack of a common foundation for understanding what constitutes an effective graph sequence model.</p><p>The research introduces the Graph Sequence Model (GSM) framework and GSM++, a hybrid model combining the strengths of transformers and recurrent neural networks. GSM++ employs a novel hierarchical tokenization technique (HAC) to generate ordered sequences, addressing the limitations of existing node and subgraph tokenization methods. Experiments validate the effectiveness of this hybrid architecture, demonstrating superior performance compared to existing models on diverse benchmark tasks.</p></div><h4 class="relative group">Key Takeaways<div id=key-takeaways class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#key-takeaways aria-label=Anchor>#</a></span></h4><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-b5ca2d66dcb4d8749a9e59d794637af7></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-b5ca2d66dcb4d8749a9e59d794637af7",{strings:[" GSM++ outperforms existing graph learning models by efficiently encoding both local and global graph structures. "],speed:10,lifeLike:!0,startDelay:0,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-9d9eac4cf3f6f646beb687d4b7049c90></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-9d9eac4cf3f6f646beb687d4b7049c90",{strings:[" The paper provides a unified framework for evaluating various sequence model backbones in graph tasks, revealing inherent strengths and weaknesses. "],speed:10,lifeLike:!0,startDelay:1e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-ff300d5aed6631ea2c30abe71fbc082f></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-ff300d5aed6631ea2c30abe71fbc082f",{strings:[" The novel hierarchical tokenization method of GSM++, based on HAC, is shown to be effective and scalable. "],speed:10,lifeLike:!0,startDelay:2e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><h4 class="relative group">Why does it matter?<div id=why-does-it-matter class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#why-does-it-matter aria-label=Anchor>#</a></span></h4><p>This paper is crucial because it addresses the limitations of existing graph neural networks by proposing a novel hybrid model that combines the strengths of recurrent models and transformers. This offers a more flexible and comprehensive solution for graph-based learning tasks, opening new avenues of research and informing the development of more specialized models. It&rsquo;s particularly relevant given the growing interest in extending sub-quadratic sequence models to the graph domain and the need for a deeper understanding of graph sequence model strengths and weaknesses.</p><hr><h4 class="relative group">Visual Insights<div id=visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#visual-insights aria-label=Anchor>#</a></span></h4><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2411.15671/extracted/6020576/Figures/GSM.png alt></figure></p><blockquote><p>üîº The figure illustrates the Graph Sequence Model (GSM), a framework for applying sequence models to graph data. GSM comprises three main stages: Tokenization, which converts graph data into sequences; Local Encoding, which processes local graph structures; and Global Encoding, which utilizes sequence models like RNNs or Transformers to capture long-range dependencies within sequences. The figure also highlights the strengths and weaknesses of different tokenization techniques (e.g., node vs. subgraph tokenization) and the suitability of various sequence models for specific graph tasks. Finally, the figure introduces three enhancement methods to improve GSM&rsquo;s performance: Hierarchical Affinity Clustering (HAC) for improved tokenization, a hybrid encoder combining RNNs and Transformers, and a Mixture of Tokenization (MoT) approach for adaptive encoding strategies.</p><details><summary>read the caption</summary>Figure 1: Overview of Graph Sequence Model (GSM). GSM Consists of three stages: (1) Tokenization, (2) Local Encoding, and (3) Global Encoding. We provide a foundation for strengths and weaknesses of different tokenizations and sequence models. Finally, we present three methods to¬†enhance¬†the¬†power¬†of¬†GSMs.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Model</th><th>Node Degree</th><th>Node Degree</th><th>Cycle Check</th><th>Cycle Check</th><th>Triangle Counting</th><th>Triangle Counting</th></tr></thead><tbody><tr><td></td><td>1K</td><td>100K</td><td>1K</td><td>100K</td><td>Erdos-Renyi</td><td>Regular</td></tr><tr><td></td><td>Accuracy ‚Üë</td><td>Accuracy ‚Üë</td><td>RMSE ‚Üì</td><td></td><td></td><td></td></tr><tr><td>Reference Baselines</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>GCN</td><td>9.3</td><td>9.5</td><td>80.3</td><td>80.2</td><td>0.841</td><td>2.18</td></tr><tr><td>GatedGCN</td><td>29.8</td><td>11.6</td><td>86.2</td><td><strong>83.4</strong></td><td><strong>0.476</strong></td><td>0.772</td></tr><tr><td>MPNN</td><td><strong>98.9</strong></td><td><strong>99.1</strong></td><td><strong>99.1</strong>*</td><td><strong>99.9</strong>*</td><td><strong>0.417</strong>*</td><td><strong>0.551</strong></td></tr><tr><td>GIN</td><td><strong>36.4</strong></td><td><strong>35.9</strong></td><td><strong>98.2</strong></td><td>81.8</td><td>0.659</td><td><strong>0.449</strong>*</td></tr><tr><td>Transformers</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Node</td><td>29.9</td><td>30.1</td><td>30.8</td><td>31.2</td><td>0.713</td><td>1.19</td></tr><tr><td>HAC (DFS)</td><td>31.0</td><td>31.0</td><td>58.9</td><td>61.3</td><td>0.698</td><td>1.00</td></tr><tr><td>k-hop</td><td><strong>97.6</strong></td><td><strong>98.9</strong></td><td><strong>91.6</strong></td><td><strong>94.3</strong></td><td><strong>0.521</strong></td><td><strong>0.95</strong></td></tr><tr><td>HAC (BFS)</td><td><strong>98.1</strong></td><td><strong>98.6</strong></td><td><strong>91.9</strong></td><td><strong>92.5</strong></td><td><strong>0.574</strong></td><td><strong>0.97</strong></td></tr><tr><td>Mamba</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Node</td><td>30.4</td><td>30.9</td><td>31.2</td><td>33.8</td><td>0.719</td><td>1.33</td></tr><tr><td>HAC (DFS)</td><td>32.6</td><td>33.6</td><td>33.7</td><td>34.2</td><td>0.726</td><td>1.08</td></tr><tr><td>k-hop</td><td><strong>98.5</strong></td><td><strong>98.7</strong></td><td><strong>90.5</strong></td><td><strong>93.8</strong></td><td><strong>0.601</strong></td><td><strong>0.88</strong></td></tr><tr><td>HAC (BFS)</td><td><strong>98.1</strong></td><td><strong>99.0</strong></td><td><strong>93.7</strong></td><td><strong>93.5</strong></td><td><strong>0.528</strong></td><td><strong>0.92</strong></td></tr><tr><td>Hybrid (Mamba + Transformer)</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Node</td><td>31.0</td><td>31.6</td><td>31.5</td><td>31.7</td><td>0.706</td><td>1.27</td></tr><tr><td>HAC (DFS)</td><td>32.9</td><td>33.7</td><td>33.9</td><td>33.6</td><td>0.717</td><td>1.11</td></tr><tr><td>k-hop</td><td><strong>99.0</strong>*</td><td><strong>99.2</strong>*</td><td><strong>90.8</strong></td><td><strong>91.1</strong></td><td><strong>0.598</strong></td><td><strong>0.84</strong></td></tr><tr><td>HAC (BFS)</td><td><strong>98.6</strong></td><td><strong>98.5</strong></td><td><strong>93.9</strong></td><td><strong>94.0</strong></td><td><strong>0.509</strong></td><td><strong>0.90</strong></td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents the results of different graph neural network models on tasks that primarily require local information processing. The tasks include node degree prediction, cycle detection, and triangle counting. For each task, the table shows the accuracy achieved by various models on graphs with 1000 and 100,000 nodes. The best performing models for each task and dataset size are highlighted, indicating superior performance on these specific graph structures and scales. The overall best-performing model across all three tasks is marked with an asterisk (*). The symbol ‚Ä† indicates that the results of random walk tokenization are excluded from the table due to their stochastic nature, which can significantly impact their performance on these specific tasks.</p><details><summary>read the caption</summary>Table 1: Graph tasks that require local information‚Ä†. The first and second best results of each type are highlighted. The best overall result for each task is marked *.</details></blockquote><h3 class="relative group">In-depth insights<div id=in-depth-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#in-depth-insights aria-label=Anchor>#</a></span></h3><h4 class="relative group">GSM Framework<div id=gsm-framework class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#gsm-framework aria-label=Anchor>#</a></span></h4><p>The GSM (Graph Sequence Model) framework offers a novel approach to graph representation learning by bridging the gap between sequence models and graph-structured data. <strong>Its core strength lies in its unified three-stage process:</strong> 1) Tokenization, converting the graph into sequences of nodes or subgraphs; 2) Local Encoding, capturing local neighborhood information; and 3) Global Encoding, utilizing a sequence model (e.g., RNN, Transformer) to learn long-range dependencies within the sequences. This modular design allows for systematic comparison of various sequence model backbones, revealing strengths and weaknesses in different graph tasks. <strong>The framework facilitates theoretical analysis of inductive biases</strong> of various models, providing crucial insights into their effectiveness in tasks such as counting and connectivity. <strong>A key contribution is the introduction of GSM++,</strong> a hybrid model incorporating hierarchical clustering for improved tokenization and a hybrid sequence architecture, enhancing both efficiency and performance. The theoretical and experimental evaluations validate the design choices of GSM++, showcasing its superior performance compared to baselines on various benchmark tasks.</p><h4 class="relative group">Sequence Model Power<div id=sequence-model-power class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#sequence-model-power aria-label=Anchor>#</a></span></h4><p>The power of sequence models in the context of graph neural networks hinges on their ability to capture both <strong>local and global dependencies</strong> within graph data. While traditional message-passing networks excel at local reasoning, sequence models, particularly transformers and recurrent models, offer unique advantages in capturing long-range interactions. The choice of sequence model depends on the specific task and the properties of the input graph. <strong>Transformers</strong>, with their powerful attention mechanisms, are well-suited for global tasks requiring holistic understanding of the graph structure. However, their quadratic complexity poses scalability challenges. <strong>Recurrent models</strong>, on the other hand, are more efficient for tasks involving sequential processing or when the graph possesses inherent node ordering, though they may struggle with capturing long-range dependencies effectively. <strong>Hybrid approaches</strong>, combining transformers and recurrent models, can leverage the strengths of both architectures, potentially leading to improved performance on a wider range of tasks. The success of any sequence model is also critically dependent on the employed <strong>tokenization strategy</strong>. Different tokenization methods (node, subgraph, hierarchical) result in sequences with varying inductive biases and affect the model&rsquo;s ability to learn relevant patterns.</p><h4 class="relative group">Hybrid GSM++<div id=hybrid-gsm class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#hybrid-gsm aria-label=Anchor>#</a></span></h4><p>The proposed &ldquo;Hybrid GSM++&rdquo; model represents a significant advancement in graph neural networks by cleverly combining the strengths of recurrent and transformer architectures. <strong>GSM++ leverages a hierarchical affinity clustering (HAC) algorithm for tokenization</strong>, creating ordered sequences of nodes that capture both local and global graph structures. This approach is particularly beneficial because it addresses the limitations of existing methods, such as the over-smoothing and over-squashing problems. The hybrid nature of the model is crucial; <strong>recurrent layers enhance local information capture</strong>, while <strong>transformer layers excel at modeling long-range dependencies</strong>, effectively capturing both the local and global properties of the graph. <strong>The incorporation of a mixture of tokenization (MoT) further enhances flexibility and efficiency</strong>, adapting the best tokenization approach for each node depending on the specific task. This nuanced combination results in a powerful, scalable model that outperforms baselines on various benchmark graph learning tasks. The theoretical analysis validating these design choices supports the experimental results, thus underpinning the robustness and potential of Hybrid GSM++ for complex graph problems.</p><h4 class="relative group">Tokenization Methods<div id=tokenization-methods class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#tokenization-methods aria-label=Anchor>#</a></span></h4><p>Tokenization, the process of converting a graph into sequences suitable for sequence models, is crucial for effective graph representation learning. The paper explores various tokenization strategies, each with its strengths and weaknesses. <strong>Node-based tokenization</strong>, treating nodes as a simple sequence, is straightforward but lacks the structural information inherent in the graph, potentially leading to suboptimal performance. <strong>Subgraph-based tokenization</strong>, representing the graph as a collection of node neighborhoods, aims to capture local structure. However, these methods require efficient techniques to handle the variable sizes and complexities of subgraphs. The choice of tokenization significantly impacts model efficiency and performance on different tasks; <strong>Node-based methods excel for global tasks, while subgraph-based approaches are superior for local tasks</strong>. The paper proposes a novel <strong>Hierarchical Affinity Clustering (HAC)</strong> based tokenization. HAC builds a hierarchical representation of the graph by recursively merging similar nodes, thus creating ordered sequences. This offers a balance, preserving the structural information while generating compact sequences. Finally, the idea of a <strong>Mixture of Tokenization (MoT)</strong> allows the algorithm to adaptively choose the best tokenization for each node, potentially maximizing model efficacy across diverse tasks and graph structures.</p><h4 class="relative group">Future of GSMs<div id=future-of-gsms class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#future-of-gsms aria-label=Anchor>#</a></span></h4><p>The future of Graph Sequence Models (GSMs) is promising, given their ability to unify various sequence modeling approaches for graph data. <strong>Further research should focus on developing more sophisticated tokenization techniques</strong> that go beyond simple node or subgraph ordering, potentially incorporating advanced graph algorithms for more nuanced representations. <strong>Hybrid models, combining the strengths of recurrent and transformer architectures, seem particularly promising</strong> for balancing local and global information processing. <strong>Exploring alternative sequence models beyond Transformers and RNNs</strong> is also crucial to expand the capabilities and efficiency of GSMs. Finally, <strong>a deeper theoretical understanding of GSMs&rsquo; representational power and limitations</strong> with respect to different graph properties and tasks is needed. This would allow for more informed model design and better task-specific optimization.</p><h3 class="relative group">More visual insights<div id=more-visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#more-visual-insights aria-label=Anchor>#</a></span></h3><details><summary>More on figures</summary><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2411.15671/extracted/6020576/Figures/GSM++.png alt></figure></p><blockquote><p>üîº GSM++ is a model that leverages the strengths of both recurrent neural networks and transformers. It processes graph data in three stages: First, hierarchical affinity clustering (HAC) is used for tokenization, creating a hierarchical sequence representation of the graph. Second, a local encoding step captures local graph characteristics. Finally, a hybrid global encoding (using both recurrent and transformer architectures) processes the sequences, combining the ability of recurrent networks to handle sequential data effectively and the capability of transformers to capture long-range dependencies. This hybrid approach aims to overcome limitations of solely using either recurrent networks or transformers for graph-based tasks.</p><details><summary>read the caption</summary>Figure 2: Overview of GSM++. GSM++ is a special instance of GSMs that uses: (1) HAC tokenization, (2) hierarchical PE, and (3) a hybrid sequence model.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2411.15671/extracted/6020576/Figures/PE-plot.png alt></figure></p><blockquote><p>üîº This figure visualizes the performance of various combinations of tokenization methods and global encoder (sequence model) architectures on seven benchmark graph datasets. Each cell in the heatmap represents the normalized performance score for a specific combination. The color intensity indicates the ranking, with darker shades representing higher ranks. The figure demonstrates that no single combination consistently outperforms others across all datasets, highlighting the task-dependent nature of optimal model choices. The caption notes that even the strong combination of TTT (a sequence model) and HAC (Hierarchical Affinity Clustering for tokenization) only achieves a top-3 ranking in three out of the seven datasets.</p><details><summary>read the caption</summary>Figure 3: Normalized score of different combination of tokenization and global encoder (sequence models). Even TTT + HAC is in Top-3 only in 3/7 datasets.</details></blockquote></details><details><summary>More on tables</summary><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Model</th><th>Connectivity</th><th></th><th>Color Counting</th><th></th><th>Shortest Path</th><th></th></tr></thead><tbody><tr><td></td><td>1K</td><td>100K</td><td>1K</td><td>100K</td><td>1K</td><td>10K</td></tr><tr><td><strong>Reference Baselines</strong></td><td>Accuracy ‚Üë</td><td></td><td>Accuracy ‚Üë</td><td></td><td>RMSE ‚Üì</td><td></td></tr><tr><td>GCN</td><td>63.3</td><td>70.8</td><td>52.7</td><td>55.9</td><td>2.38</td><td>2.11</td></tr><tr><td>GatedGCN</td><td><strong>74.9</strong></td><td><strong>77.5</strong></td><td><strong>55.0</strong></td><td><strong>56.6</strong></td><td><strong>1.98</strong></td><td><strong>1.93</strong></td></tr><tr><td>MPNN</td><td>71.8</td><td><strong>76.1</strong></td><td><strong>53.9</strong></td><td><strong>57.7</strong></td><td><strong>1.96</strong></td><td><strong>1.93</strong></td></tr><tr><td>GIN</td><td><strong>71.9</strong></td><td>74.6</td><td>52.4</td><td>55.1</td><td>2.03</td><td>1.98</td></tr><tr><td><strong>Transformers</strong></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Node</td><td><strong>85.7</strong></td><td><strong>86.2</strong></td><td><strong>73.1</strong></td><td><strong>77.4</strong></td><td><strong>1.19</strong></td><td><strong>1.06</strong>*</td></tr><tr><td>w/o PE</td><td>9.4</td><td>6.8</td><td>35.8</td><td>28.9</td><td>4.12</td><td>5.33</td></tr><tr><td>HAC (DFS)</td><td><strong>87.0</strong></td><td><strong>88.1</strong></td><td><strong>83.7</strong></td><td><strong>85.3</strong></td><td><strong>1.14</strong></td><td><strong>1.09</strong></td></tr><tr><td>k-hop</td><td>69.9</td><td>70.2</td><td>79.9</td><td>80.3</td><td>2.10</td><td>2.15</td></tr><tr><td>HAC (BFS)</td><td>74.1</td><td>76.7</td><td>74.5</td><td>77.8</td><td>2.31</td><td>2.28</td></tr><tr><td><strong>Mamba</strong></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Node</td><td><strong>82.8</strong></td><td><strong>84.7</strong></td><td><strong>80.1</strong></td><td><strong>82.5</strong></td><td><strong>1.27</strong></td><td><strong>1.13</strong></td></tr><tr><td>w/o PE</td><td>9.2</td><td>7.5</td><td>78.9</td><td>81.3</td><td>4.09</td><td>5.22</td></tr><tr><td>HAC (DFS)</td><td><strong>83.6</strong></td><td><strong>85.2</strong></td><td><strong>85.2</strong></td><td><strong>85.4</strong></td><td><strong>1.12</strong></td><td><strong>1.15</strong></td></tr><tr><td>k-hop</td><td>70.9</td><td>71.0</td><td>82.6</td><td>83.5</td><td>2.03</td><td>2.11</td></tr><tr><td>HAC (BFS)</td><td>76.3</td><td>77.4</td><td>83.7</td><td>84.1</td><td>2.24</td><td>2.18</td></tr><tr><td><strong>Hybrid (Mamba + Transformer)</strong></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Node</td><td><strong>88.1</strong></td><td><strong>88.6</strong></td><td><strong>82.9</strong></td><td><strong>83.0</strong></td><td><strong>1.24</strong></td><td><strong>1.13</strong></td></tr><tr><td>w/o PE</td><td>8.9</td><td>8.1</td><td>83.2</td><td>84.8</td><td>4.65</td><td>4.89</td></tr><tr><td>HAC (DFS)</td><td><strong>90.7</strong>*</td><td><strong>91.4</strong>*</td><td><strong>85.8</strong>*</td><td><strong>86.2</strong>*</td><td><strong>1.11</strong>*</td><td><strong>1.93</strong></td></tr><tr><td>k-hop</td><td>70.8</td><td>73.3</td><td>83.7</td><td>84.6</td><td>1.99</td><td>2.04</td></tr><tr><td>HAC (BFS)</td><td>78.0</td><td>79.5</td><td>83.1</td><td>83.7</td><td>2.16</td><td>2.13</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents the results of various graph tasks that necessitate global information processing. The tasks are: graph connectivity (binary classification), color counting (counting the number of nodes with each color), and shortest path (predicting shortest path lengths). For each task, multiple models were tested, and their performance is ranked, with the top two results for each task highlighted. The overall best-performing model for each task is marked with an asterisk (*). The table aims to illustrate how different model architectures handle graph problems that require considering the overall graph structure, rather than just local neighborhoods.</p><details><summary>read the caption</summary>Table 2: Graph tasks that require global information‚Ä†. The first and second best results of each type are highlighted. The best overall result for each task is marked *.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Model</th><th>MNIST</th><th>CIFAR10</th><th>PATTERN</th><th>MalNet-Tiny</th></tr></thead><tbody><tr><td>GCN</td><td>0.9071<sub>¬±0.0021</sub></td><td>0.5571<sub>¬±0.0038</sub></td><td>0.7189<sub>¬±0.0033</sub></td><td>0.8100<sub>¬±0.0000</sub></td></tr><tr><td>GraphSAGE</td><td>0.9731<sub>¬±0.0009</sub></td><td>0.6577<sub>¬±0.0030</sub></td><td>0.5049<sub>¬±0.0001</sub></td><td>0.8730<sub>¬±0.0002</sub></td></tr><tr><td>GAT</td><td>0.9554<sub>¬±0.0021</sub></td><td>0.6422<sub>¬±0.0046</sub></td><td>0.7827<sub>¬±0.0019</sub></td><td>0.8509<sub>¬±0.0025</sub></td></tr><tr><td>SPN</td><td>0.8331<sub>¬±0.0446</sub></td><td>0.3722<sub>¬±0.0827</sub></td><td>0.8657<sub>¬±0.0014</sub></td><td>0.6407<sub>¬±0.0581</sub></td></tr><tr><td>GIN</td><td>0.9649<sub>¬±0.0025</sub></td><td>0.5526<sub>¬±0.0152</sub></td><td>0.8539<sub>¬±0.0013</sub></td><td>0.8898<sub>¬±0.0055</sub></td></tr><tr><td>Gated-GCN</td><td>0.9734<sub>¬±0.0014</sub></td><td>0.6731<sub>¬±0.0031</sub></td><td>0.8557<sub>¬±0.0008</sub></td><td>0.9223<sub>¬±0.0065</sub></td></tr><tr><td>CRaWl</td><td>0.9794<sub>¬±0.050</sub></td><td>0.6901<sub>¬±0.0259</sub></td><td>-</td><td>-</td></tr><tr><td>NAGphormer</td><td>-</td><td>-</td><td>0.8644<sub>¬±0.0003</sub></td><td>-</td></tr><tr><td>GPS</td><td>0.9811<sub>¬±0.0011</sub></td><td>0.7226<sub>¬±0.0031</sub></td><td>0.8664<sub>¬±0.0011</sub></td><td>0.9298<sub>¬±0.0047</sub></td></tr><tr><td>GPS (BigBird)</td><td>0.9817<sub>¬±0.0001</sub></td><td>0.7048<sub>¬±0.0010</sub></td><td>0.8600<sub>¬±0.0014</sub></td><td>0.9234<sub>¬±0.0034</sub></td></tr><tr><td>Exphormer</td><td>0.9855<sub>¬±0.0003</sub></td><td>0.7469<sub>¬±0.0013</sub></td><td>0.8670<sub>¬±0.0003</sub></td><td><strong>0.9402<sub>¬±0.0020</sub></strong></td></tr><tr><td>NodeFormer</td><td>-</td><td>-</td><td>0.8639<sub>¬±0.0021</sub></td><td>-</td></tr><tr><td>DIFFormer</td><td>-</td><td>-</td><td>0.8701<sub>¬±0.0018</sub></td><td>-</td></tr><tr><td>GRIT</td><td>0.9810<sub>¬±0.0011</sub></td><td>0.7646<sub>¬±0.0088</sub></td><td>0.8719<sub>¬±0.0008</sub></td><td>-</td></tr><tr><td>GRED</td><td><strong>0.9838<sub>¬±0.0002</sub></strong></td><td><strong>0.7685<sub>¬±0.0019</sub></strong></td><td>0.8675<sub>¬±0.0002</sub></td><td>-</td></tr><tr><td>GMN</td><td>0.9783<sub>¬±0.0020</sub></td><td>0.7444<sub>¬±0.0009</sub></td><td>0.8649<sub>¬±0.0019</sub></td><td>0.9352<sub>¬±0.0036</sub></td></tr><tr><td>GSM++ (BFS)</td><td><strong>0.9848<sub>¬±0.0012</sub></strong></td><td>0.7659<sub>¬±0.0024</sub></td><td><strong>0.8738<sub>¬±0.0014</sub></strong></td><td><strong>0.9417<sub>¬±0.0020</sub></strong></td></tr><tr><td>GSM++ (DFS)</td><td>0.9829<sub>¬±0.0014</sub></td><td><strong>0.7692<sub>¬±0.0031</sub></strong></td><td><strong>0.8731<sub>¬±0.0008</sub></strong></td><td>0.9389<sub>¬±0.0024</sub></td></tr><tr><td>GSM++ (MoT)</td><td><strong>0.9884<sub>¬±0.0015</sub></strong></td><td><strong>0.7781<sub>¬±0.0028</sub></strong></td><td><strong>0.8793<sub>¬±0.0015</sub></strong></td><td><strong>0.9437<sub>¬±0.0058</sub></strong></td></tr></tbody></table></table></figure><blockquote><p>üîº Table 3 presents the results of GNN benchmark datasets from Dwivedi et al. (2023). It shows a comparison of different graph neural network models&rsquo; performance on various node and graph classification tasks using four benchmark datasets: MNIST, CIFAR10, and the PATTERN and Peptides-Func datasets. The table highlights the top three performing models for each dataset and task, providing a quantitative comparison of their accuracy.</p><details><summary>read the caption</summary>Table 3: GNN benchmark datasets¬†(Dwivedi et¬†al., 2023). The first, second, and third best results are highlighted.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Model</th><th>COCO-SP F1 score ‚Üë</th><th>PascalVOC-SP F1 score ‚Üë</th><th>PATTERN Accuracy ‚Üë</th></tr></thead><tbody><tr><td>GPS Framework</td><td></td><td></td><td></td></tr><tr><td>Base</td><td>0.3774</td><td>0.3689</td><td>0.8664</td></tr><tr><td>+Hybrid</td><td><strong>0.3789</strong></td><td>0.3691</td><td>0.8665</td></tr><tr><td>+HAC</td><td>0.3780</td><td><strong>0.3699</strong></td><td><strong>0.8667</strong></td></tr><tr><td>+MoT</td><td><strong>0.3791</strong></td><td><strong>0.3703</strong></td><td><strong>0.8677</strong></td></tr><tr><td>NAGphormer Framework</td><td></td><td></td><td></td></tr><tr><td>Base</td><td>0.3458</td><td>0.4006</td><td>0.8644</td></tr><tr><td>+Hybrid</td><td>0.3461</td><td><strong>0.4046</strong></td><td>0.8650</td></tr><tr><td>+HAC</td><td><strong>0.3507</strong></td><td>0.4032</td><td><strong>0.8653</strong></td></tr><tr><td>+MoT</td><td><strong>0.3591</strong></td><td><strong>0.4105</strong></td><td><strong>0.8657</strong></td></tr><tr><td>GSM++</td><td></td><td></td><td></td></tr><tr><td>Base</td><td><strong>0.3789</strong></td><td><strong>0.4128</strong></td><td><strong>0.8738</strong></td></tr><tr><td>-PE</td><td><strong>0.3780</strong></td><td><strong>0.4073</strong></td><td>0.8511</td></tr><tr><td>-Hybrid</td><td>0.3767</td><td>0.4058</td><td>0.8500</td></tr><tr><td>-HAC</td><td>0.3591</td><td>0.3996</td><td><strong>0.8617</strong></td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents the results of ablation studies conducted on the GSM++ model. It shows the impact of removing different components of the model (e.g., the hybrid encoder, hierarchical positional encoding, HAC tokenization, and MoT) on the overall performance. By comparing the performance metrics (F1 score and accuracy) obtained with the full model against those obtained with variations of the model where components were removed, this table helps determine the contribution of each component to the model&rsquo;s overall effectiveness and efficiency.</p><details><summary>read the caption</summary>Table 4: Ablation studies. The first and second best results for each model are highlighted.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Method</th><th>Tokenization</th><th>Local Encoding</th><th>Global Encoding</th></tr></thead><tbody><tr><td>DeepWalk (2014)</td><td>Random Walk</td><td>Identity(.)</td><td>SkipGram</td></tr><tr><td>Node2Vec (2016)</td><td>2<sup>nd</sup> Order Random Walk</td><td>Identity(.)</td><td>SkipGram</td></tr><tr><td>Node2Vec (2016)</td><td>Random Walk</td><td>Identity(.)</td><td>SkipGram</td></tr><tr><td>GraphTransformer (2020)</td><td>Node</td><td>Identity(.)</td><td>Transformer</td></tr><tr><td>GraphGPS (2022)</td><td>Node</td><td>Identity(.)</td><td>Transformer</td></tr><tr><td>NodeFormer (2022)</td><td>Node</td><td>Gumbel-Softmax(.)</td><td>Transformer</td></tr><tr><td>Graph-ViT (2023)</td><td>METIS Clustering (Patching)</td><td>Gcn(.)</td><td>ViT</td></tr><tr><td>Exphormer (2023)</td><td>Node</td><td>Identity(.)</td><td>Sparse Transformer</td></tr><tr><td>CRaWl (2023)</td><td>Random Walk</td><td>1D Convolutions</td><td>MLP(.)</td></tr><tr><td>NAGphormer (2023)</td><td>k-hop neighborhoods</td><td>Gcn(.)</td><td>Transformer</td></tr><tr><td>SP-MPNNs (2022)</td><td>k-hop neighborhoods</td><td>Identity(.)</td><td>GIN(.)</td></tr><tr><td>GRED (2023)</td><td>k-hop neighborhood</td><td>MLP(.)</td><td>Rnn(.)</td></tr><tr><td>S4G (2024)</td><td>k-hop neighborhood</td><td>Identity(.)</td><td>S4(.)</td></tr><tr><td>Graph Mamba (2024)</td><td>Union of Random Walks (With varying length)</td><td>Gated-Gcn(.)</td><td>Bi-Mamba(.)</td></tr></tbody></table></table></figure><blockquote><p>üîº This table shows how various graph neural network models can be viewed as special cases of the general Graph Sequence Model (GSM) framework proposed in the paper. For each model, the table lists the tokenization method used to convert the graph into sequences (e.g., node-based, subgraph-based), the local encoding technique applied to each token (e.g., identity, GCN), and the global encoding model used to capture long-range dependencies (e.g., SkipGram, Transformer, RNN). This allows for a systematic comparison of different model architectures and highlights the common underlying principles across these models.</p><details><summary>read the caption</summary>Table 5: How are different models special instances of GSM framework</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Dataset</th><th>#Graphs</th><th>Average #Nodes</th><th>Average #Edges</th><th>#Class</th><th>Input Level</th><th>Task</th><th>Metric</th></tr></thead><tbody><tr><td>Long-range Graph Benchmark (Dwivedi et al., 2022a)</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>COCO-SP</td><td>123,286</td><td>476.9</td><td>2693.7</td><td>81</td><td>Node</td><td>Classification</td><td>F1 score</td></tr><tr><td>PascalVOC-SP</td><td>11,355</td><td>479.4</td><td>2710.5</td><td>21</td><td>Node</td><td>Classification</td><td>F1 score</td></tr><tr><td>Peptides-Func</td><td>15,535</td><td>150.9</td><td>307.3</td><td>10</td><td>Graph</td><td>Classification</td><td>Average Precision</td></tr><tr><td>Peptides-Struct</td><td>15,535</td><td>150.9</td><td>307.3</td><td>11 (regression)</td><td>Graph</td><td>Regression</td><td>Mean Absolute Error</td></tr><tr><td>GNN Benchmark (Dwivedi et al., 2023)</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Pattern</td><td>14,000</td><td>118.9</td><td>3,039.3</td><td>2</td><td>Node</td><td>Classification</td><td>Accuracy</td></tr><tr><td>MNIST</td><td>70,000</td><td>70.6</td><td>564.5</td><td>10</td><td>Graph</td><td>Classification</td><td>Accuracy</td></tr><tr><td>CIFAR10</td><td>60,000</td><td>117.6</td><td>941.1</td><td>10</td><td>Graph</td><td>Classification</td><td>Accuracy</td></tr><tr><td>MalNet-Tiny</td><td>5,000</td><td>1,410.3</td><td>2,859.9</td><td>5</td><td>Graph</td><td>Classification</td><td>Accuracy</td></tr><tr><td>Heterophilic Benchmark (Platonov et al., 2023)</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Roman-empire</td><td>1</td><td>22,662</td><td>32,927</td><td>18</td><td>Node</td><td>Classification</td><td>Accuracy</td></tr><tr><td>Amazon-ratings</td><td>1</td><td>24,492</td><td>93,050</td><td>5</td><td>Node</td><td>Classification</td><td>Accuracy</td></tr><tr><td>Minesweeper</td><td>1</td><td>10,000</td><td>39,402</td><td>2</td><td>Node</td><td>Classification</td><td>ROC AUC</td></tr><tr><td>Tolokers</td><td>1</td><td>11,758</td><td>519,000</td><td>2</td><td>Node</td><td>Classification</td><td>ROC AUC</td></tr><tr><td>Very Large Dataset (Hu et al., 2020)</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>arXiv-ogbn</td><td>1</td><td>169,343</td><td>1,166,243</td><td>40</td><td>Node</td><td>Classification</td><td>Accuracy</td></tr><tr><td>products-ogbn</td><td>1</td><td>2,449,029</td><td>61,859,140</td><td>47</td><td>Node</td><td>Classification</td><td>Accuracy</td></tr><tr><td>Color-connectivty task (Ramp√°≈°ek & Wolf, 2021)</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>C-C 16x16 grid</td><td>15,000</td><td>256</td><td>480</td><td>2</td><td>Node</td><td>Classification</td><td>Accuracy</td></tr><tr><td>C-C 32x32 grid</td><td>15,000</td><td>1,024</td><td>1,984</td><td>2</td><td>Node</td><td>Classification</td><td>Accuracy</td></tr><tr><td>C-C Euroroad</td><td>15,000</td><td>1,174</td><td>1,417</td><td>2</td><td>Node</td><td>Classification</td><td>Accuracy</td></tr><tr><td>C-C Minnesota</td><td>6,000</td><td>2,642</td><td>3,304</td><td>2</td><td>Node</td><td>Classification</td><td>Accuracy</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents a comprehensive overview of the datasets used in the experiments. For each dataset, it lists key statistics, including the number of graphs, the average number of nodes and edges per graph, the experimental setup (e.g., node classification, graph classification), the number of classes for classification tasks, and the specific evaluation metric used (e.g., accuracy, F1-score, AUC). The datasets are categorized into those designed for long-range dependencies, heterophily, and those focused on specific tasks like color connectivity.</p><details><summary>read the caption</summary>Table 6: Dataset Statistics.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><p>Model|Roman-empire|Amazon-ratings|Minesweeper
&mdash;|&mdash;|&mdash;
GCN|0.7369<sub>¬±0.0074</sub>|0.4870<sub>¬±0.0063</sub>|0.8975<sub>¬±0.0052</sub>
GraphSAGE|0.8574<sub>¬±0.0067</sub>|<strong>0.5363<sub>¬±0.0039</sub></strong>|<strong>0.9351<sub>¬±0.0057</sub></strong>
GAT|0.7973<sub>¬±0.0039</sub>|0.5270<sub>¬±0.0062</sub>|<strong>0.9391<sub>¬±0.0035</sub></strong>
OrderedGNN|0.7768<sub>¬±0.0039</sub>|0.4729<sub>¬±0.0065</sub>|0.8058<sub>¬±0.0108</sub>
tGNN|0.7995<sub>¬±0.0075</sub>|0.4821<sub>¬±0.0053</sub>|<strong>0.9193<sub>¬±0.0077</sub></strong>
Gated-GCN|0.7446<sub>¬±0.0054</sub>|0.4300<sub>¬±0.0032</sub>|0.8754<sub>¬±0.0122</sub>
NAGphormer|0.7434<sub>¬±0.0077</sub>|0.5126<sub>¬±0.0072</sub>|0.8419<sub>¬±0.0066</sub>
GPS|0.8200<sub>¬±0.0061</sub>|0.5310<sub>¬±0.0042</sub>|0.9063<sub>¬±0.0067</sub>
Exphormer|0.8903<sub>¬±0.0037</sub>|0.5351<sub>¬±0.0046</sub>|0.9074<sub>¬±0.0053</sub>
NodeFormer|0.6449<sub>¬±0.0073</sub>|0.4386<sub>¬±0.0035</sub>|0.8671<sub>¬±0.0088</sub>
DIFFormer|0.7910<sub>¬±0.0032</sub>|0.4784<sub>¬±0.0065</sub>|0.9089<sub>¬±0.0058</sub>
GOAT|0.7159<sub>¬±0.0125</sub>|0.4461<sub>¬±0.0050</sub>|0.8109<sub>¬±0.0102</sub>
GMN|0.8219<sub>¬±0.0012</sub>|0.5327<sub>¬±0.0030</sub>|0.8992<sub>¬±0.0063</sub>
GSM++ (BFS)|<strong>0.9003<sub>¬±0.0087</sub></strong>|<strong>0.5381<sub>¬±0.0035</sub></strong>|0.9109<sub>¬±0.0098</sub>
GSM++ (DFS)|<strong>0.9124<sub>¬±0.0023</sub></strong>|0.5361<sub>¬±0.0029</sub>|0.9145<sub>¬±0.0036</sub>
GSM++ (MoT)|<strong>0.9177<sub>¬±0.0040</sub></strong>|<strong>0.5390<sub>¬±0.0104</sub></strong>|0.9149<sub>¬±0.0111</sub><sup>‚Ä†</sup></p><p><sup>‚Ä†</sup> GSM++ (all variants) achieve the best three results among all graph sequence models.</p></table></figure><blockquote><p>üîº This table presents the results of different graph neural network models on three heterophilic graph datasets: Roman-empire, Amazon-ratings, and Minesweeper. Heterophilic graphs are those where nodes within the same class have diverse features, making them challenging for graph neural networks to learn. The table shows the accuracy, F1-score, and ROC AUC (Area Under the Curve) achieved by each model on each dataset. The top three performing models for each metric are highlighted to facilitate comparison and identification of the best-performing models for each dataset and task.</p><details><summary>read the caption</summary>Table 7: Heterophilic datasets¬†(Platonov et¬†al., 2023). The first, second, and third results are highlighted.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption>Model|COCO-SP|PascalVOC-SP|Peptides-Func
&mdash;|&mdash;|&mdash;
GCN|0.0841<sub>¬±0.0010</sub>|0.1268<sub>¬±0.0060</sub>|0.5930<sub>¬±0.0023</sub>
GIN|0.1339<sub>¬±0.0044</sub>|0.1265<sub>¬±0.0076</sub>|0.5498<sub>¬±0.0079</sub>
Gated-GCN|0.2641<sub>¬±0.0045</sub>|0.2873<sub>¬±0.0219</sub>|0.5864<sub>¬±0.0077</sub>
GAT|0.1296<sub>¬±0.0028</sub>|0.1753<sub>¬±0.0329</sub>|0.5308<sub>¬±0.0019</sub>
MixHop|-|0.2506<sub>¬±0.0133</sub>|0.6843<sub>¬±0.0049</sub>
DIGL|-|0.2921<sub>¬±0.0038</sub>|0.6830<sub>¬±0.0026</sub>
SPN|-|0.2056<sub>¬±0.0338</sub>|0.6926<sub>¬±0.0247</sub>
SAN+LapPE|0.2592<sub>¬±0.0158</sub>|0.3230<sub>¬±0.0039</sub>|0.6384<sub>¬±0.0121</sub>
NAGphormer|0.3458<sub>¬±0.0070</sub>|0.4006<sub>¬±0.0061</sub>|-
Graph ViT|-|-|0.6855<sub>¬±0.0049</sub>
GPS|<strong>0.3774<sub>¬±0.0150</sub></strong>|0.3689<sub>¬±0.0131</sub>|0.6575<sub>¬±0.0049</sub>
Exphormer|0.3430<sub>¬±0.0108</sub>|0.3975<sub>¬±0.0037</sub>|0.6527<sub>¬±0.0043</sub>
NodeFormer|0.3275<sub>¬±0.0241</sub>|0.4015<sub>¬±0.0082</sub>|-
DIFFormer|0.3620<sub>¬±0.0012</sub>|0.3988<sub>¬±0.0045</sub>|-
GRIT|-|-|0.6988<sub>¬±0.0082</sub>
GRED|-|-|<strong>0.7085<sub>¬±0.0027</sub></strong>
GMN|0.3618<sub>¬±0.0053</sub>|<strong>0.4169<sub>¬±0.0103</sub></strong>|0.6860<sub>¬±0.0012</sub>
GSM++ (BFS)|<strong>0.3789<sub>¬±0.0160</sub></strong>|0.4128<sub>¬±0.0027</sub>|0.6991<sub>¬±0.0008</sub>
GSM++ (DFS)|0.3769<sub>¬±0.0027</sub>|<strong>0.4174<sub>¬±0.0031</sub></strong>|<strong>0.7019<sub>¬±0.0084</sub></strong>
GSM++ (MoT)|<strong>0.3801<sub>¬±0.0122</sub></strong>|<strong>0.4193<sub>¬±0.0075</sub></strong>|<strong>0.7092<sub>¬±0.0076</sub></strong></table></figure><blockquote><p>üîº This table presents the results of various graph neural network models on three benchmark datasets: COCO-SP, PascalVOC-SP, and Peptides-Func. These datasets are characterized by long-range dependencies between nodes, making them challenging for many graph models. The table shows the performance of each model on each dataset, measured by F1 score (for COCO-SP and PascalVOC-SP) and Average Precision (for Peptides-Func). The top three performing models for each dataset are highlighted to illustrate the relative strengths and weaknesses of different approaches for handling long-range graph dependencies.</p><details><summary>read the caption</summary>Table 8: Long-Range Datasets¬†(Dwivedi et¬†al., 2022a). The first, second, and third results are highlighted.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Model</th><th>GatedGCN</th><th>NAGphormer</th><th>GPS</th><th>Exphormer</th><th>GOAT</th><th>GRIT</th><th>GMN</th><th>GSM++ BFS</th><th>GSM++ DFS</th><th>GSM++ MoT</th></tr></thead><tbody><tr><td>arXiv-ogbn Performance</td><td>0.7141</td><td>0.7013</td><td>OOM</td><td>0.7228</td><td>0.7196</td><td>OOM</td><td>0.7248</td><td><strong>0.7297</strong></td><td><strong>0.7261</strong></td><td><strong>0.7301</strong></td></tr><tr><td>arXiv-ogbn Memory Usage (GB)</td><td>11.87</td><td><strong>6.81</strong></td><td>OOM</td><td>37.01</td><td>13.12</td><td>OOM</td><td><strong>5.63</strong></td><td>24.8</td><td><strong>4.7</strong></td><td>14.9</td></tr><tr><td>arXiv-ogbn Training Time/Epoch (s)</td><td><strong>1.94</strong></td><td>5.96</td><td>OOM</td><td>2.15</td><td>8.69</td><td>OOM</td><td><strong>1.78</strong></td><td>2.33</td><td><strong>1.95</strong></td><td>4.16</td></tr><tr><td>products-ogbn Performance</td><td>0.0000</td><td>0.0000</td><td>OOM</td><td>OOM</td><td><strong>0.8200</strong></td><td>OOM</td><td>OOM</td><td>0.8071</td><td><strong>0.8080</strong></td><td><strong>0.8213</strong></td></tr><tr><td>products-ogbn Memory Usage (GB)</td><td><strong>11.13</strong></td><td><strong>10.04</strong></td><td>OOM</td><td>OOM</td><td>12.06</td><td>OOM</td><td>OOM</td><td>38.14</td><td><strong>9.15</strong></td><td>11.96</td></tr><tr><td>products-ogbn Training Time/Epoch (s)</td><td><strong>1.92</strong></td><td>12.08</td><td>OOM</td><td>OOM</td><td>29.50</td><td>OOM</td><td>OOM</td><td><strong>6.97</strong></td><td>12.19</td><td><strong>11.87</strong></td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents a comparison of the performance of various graph neural network models on two large graph datasets: arXiv-ogbn and products-ogbn. The metrics evaluated include accuracy (Performance), memory usage (Memory Usage (GB)), and training time per epoch (Training Time/Epoch (s)). The models compared encompass several state-of-the-art Graph Transformers and a novel hybrid model called GSM++. The table highlights the top three performing models for each metric. &lsquo;OOM&rsquo; indicates that the model ran out of memory and could not complete training.</p><details><summary>read the caption</summary>Table 9: Efficiency evaluation on large graphs. The first, second, and third results for each metric are highlighted. OOM: Out of memory.</details></blockquote></details><h3 class="relative group">Full paper<div id=full-paper class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#full-paper aria-label=Anchor>#</a></span></h3><div id=gallery-6802a26571aad3248c3a45271167a52c class=gallery><img src=https://ai-paper-reviewer.com/2411.15671/1.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.15671/2.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.15671/3.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.15671/4.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.15671/5.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.15671/6.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.15671/7.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.15671/8.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.15671/9.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.15671/10.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.15671/11.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.15671/12.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.15671/13.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.15671/14.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.15671/15.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.15671/16.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.15671/17.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.15671/18.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.15671/19.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2411.15671/20.png class="grid-w50 md:grid-w33 xl:grid-w25"></div></div><section class="flex flex-row flex-wrap justify-center pt-4 text-xl"><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.15671/&amp;title=Best%20of%20Both%20Worlds:%20Advantages%20of%20Hybrid%20Graph%20Sequence%20Models" title="Share on LinkedIn" aria-label="Share on LinkedIn"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://twitter.com/intent/tweet/?url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.15671/&amp;text=Best%20of%20Both%20Worlds:%20Advantages%20of%20Hybrid%20Graph%20Sequence%20Models" title="Tweet on Twitter" aria-label="Tweet on Twitter"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="mailto:?body=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.15671/&amp;subject=Best%20of%20Both%20Worlds:%20Advantages%20of%20Hybrid%20Graph%20Sequence%20Models" title="Send via email" aria-label="Send via email"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1-27.64 140.9 68.65 266.2 199.1 285.1 19.01 2.888 36.17-12.26 36.17-31.49l1e-4-.6631c0-15.74-11.44-28.88-26.84-31.24-84.35-12.98-149.2-86.13-149.2-174.2.0-102.9 88.61-185.5 193.4-175.4 91.54 8.869 158.6 91.25 158.6 183.2v16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98.0036c-7.299.0-13.2 4.992-15.12 11.68-24.85-12.15-54.24-16.38-86.06-5.106-38.75 13.73-68.12 48.91-73.72 89.64-9.483 69.01 43.81 128 110.9 128 26.44.0 50.43-9.544 69.59-24.88 24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3c0-149.2-133.9-265.632-287.3-235.57zM239.1 304.3c-26.47.0-48-21.56-48-48.05s21.53-48.05 48-48.05 48 21.56 48 48.05-20.6 48.05-48 48.05z"/></svg></span></a></section></div><script>var oid="views_paper-reviews/2411.15671/index.md",oid_likes="likes_paper-reviews/2411.15671/index.md"</script><script type=text/javascript src=/ai-paper-reviewer/js/page.min.0860cf4e04fa2d72cc33ddba263083464d48f67de06114529043cb4623319efed4f484fd7f1730df5abea0e2da6f3538855634081d02f2d6e920b956f063e823.js integrity="sha512-CGDPTgT6LXLMM926JjCDRk1I9n3gYRRSkEPLRiMxnv7U9IT9fxcw31q+oOLabzU4hVY0CB0C8tbpILlW8GPoIw=="></script></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="flex group mr-3" href=/ai-paper-reviewer/paper-reviews/2411.15611/><span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&larr;</span>
<span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Knowledge Transfer Across Modalities with Natural Language Supervision</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2024-11-23T00:00:00+00:00>23 November 2024</time>
</span></span></a></span><span><a class="flex text-right group ml-3" href=/ai-paper-reviewer/paper-reviews/2411.16754/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Visual Counter Turing Test (VCT^2): Discovering the Challenges for AI-Generated Image Detection and Introducing Visual AI Index (V_AI)</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2024-11-24T00:00:00+00:00>24 November 2024</time>
</span></span><span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&larr;</span></a></span></div></div><div class=pt-3><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class=pt-3><script src=https://utteranc.es/client.js repo=pmnxis/pmnxis.github.io issue-term=pathname label=Comment theme=dark-blue crossorigin=anonymous async></script></div></div></footer></article><div id=top-scroller class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer id=site-footer class="py-10 print:hidden"><nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400"><ul class="flex flex-col list-none sm:flex-row"><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=/ai-paper-reviewer/tags/ title>Tags</a></li></ul></nav><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2024
AI Paper Reviews by AI</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a></p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/ai-paper-reviewer/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh+sCQ0E53ghYrxgYqw+0GCRyIEpA=="></script></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://deep-diver.github.io/ai-paper-reviewer/ style=z-index:500><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body><script data-name=BMC-Widget data-cfasync=false src=https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js data-id=chansung data-description="Support me on Buy me a coffee!" data-message data-color=#FFDD00 data-position=Left data-x_margin=18 data-y_margin=18></script></html>