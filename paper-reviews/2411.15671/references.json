{"references": [{"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-01", "reason": "This paper introduced the Transformer architecture, a foundational model for many of the sequence models discussed and used in the paper."}, {"fullname_first_author": "Thomas N. Kipf", "paper_title": "Semi-supervised classification with graph convolutional networks", "publication_date": "2016-09-01", "reason": "This paper introduced Graph Convolutional Networks (GCNs), a fundamental model for graph neural networks, which is heavily related to the hybrid models explored in this paper."}, {"fullname_first_author": "Petar Velickovic", "paper_title": "Graph attention networks", "publication_date": "2017-10-01", "reason": "This paper introduced Graph Attention Networks (GATs), another important type of graph neural network that is related to the hybrid models discussed in the paper."}, {"fullname_first_author": "Vijay Prakash Dwivedi", "paper_title": "Long range graph benchmark", "publication_date": "2022-06-01", "reason": "This paper introduced a benchmark dataset for evaluating graph neural networks, which was used in the experimental evaluation of this paper."}, {"fullname_first_author": "Ladislav Ramp\u00e1\u0161ek", "paper_title": "Recipe for a general, powerful, scalable graph transformer", "publication_date": "2022-12-01", "reason": "This paper introduced GraphGPS, a scalable graph transformer model that serves as a strong baseline model and is closely related to the hybrid models proposed in this paper."}]}