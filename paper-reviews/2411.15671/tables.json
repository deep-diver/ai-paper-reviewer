[{"content": "| Model | Node Degree | Node Degree | Cycle Check | Cycle Check | Triangle Counting | Triangle Counting |\n|---|---|---|---|---|---|---|\n|  | 1K | 100K | 1K | 100K | Erdos-Renyi | Regular |\n|  | Accuracy \u2191 | Accuracy \u2191 | RMSE \u2193 |\n| Reference Baselines |  |  |  |  |  |  |\n| GCN | 9.3 | 9.5 | 80.3 | 80.2 | 0.841 | 2.18 |\n| GatedGCN | 29.8 | 11.6 | 86.2 | **83.4** | **0.476** | 0.772 |\n| MPNN | **98.9** | **99.1** | **99.1*** | **99.9*** | **0.417*** | **0.551** |\n| GIN | **36.4** | **35.9** | **98.2** | 81.8 | 0.659 | **0.449*** |\n| Transformers |  |  |  |  |  |  |\n| Node | 29.9 | 30.1 | 30.8 | 31.2 | 0.713 | 1.19 |\n| HAC (DFS) | 31.0 | 31.0 | 58.9 | 61.3 | 0.698 | 1.00 |\n| k-hop | **97.6** | **98.9** | **91.6** | **94.3** | **0.521** | **0.95** |\n| HAC (BFS) | **98.1** | **98.6** | **91.9** | **92.5** | **0.574** | **0.97** |\n| Mamba |  |  |  |  |  |  |\n| Node | 30.4 | 30.9 | 31.2 | 33.8 | 0.719 | 1.33 |\n| HAC (DFS) | 32.6 | 33.6 | 33.7 | 34.2 | 0.726 | 1.08 |\n| k-hop | **98.5** | **98.7** | **90.5** | **93.8** | **0.601** | **0.88** |\n| HAC (BFS) | **98.1** | **99.0** | **93.7** | **93.5** | **0.528** | **0.92** |\n| Hybrid (Mamba + Transformer) |  |  |  |  |  |  |\n| Node | 31.0 | 31.6 | 31.5 | 31.7 | 0.706 | 1.27 |\n| HAC (DFS) | 32.9 | 33.7 | 33.9 | 33.6 | 0.717 | 1.11 |\n| k-hop | **99.0*** | **99.2*** | **90.8** | **91.1** | **0.598** | **0.84** |\n| HAC (BFS) | **98.6** | **98.5** | **93.9** | **94.0** | **0.509** | **0.90** |", "caption": "Table 1: Graph tasks that require local information\u2020. The first and second best results of each type are highlighted. The best overall result for each task is marked *.", "description": "This table presents the results of different graph neural network models on tasks that primarily require local information processing.  The tasks include node degree prediction, cycle detection, and triangle counting.  For each task, the table shows the accuracy achieved by various models on graphs with 1000 and 100,000 nodes.  The best performing models for each task and dataset size are highlighted, indicating superior performance on these specific graph structures and scales.  The overall best-performing model across all three tasks is marked with an asterisk (*). The symbol \u2020 indicates that the results of random walk tokenization are excluded from the table due to their stochastic nature, which can significantly impact their performance on these specific tasks.", "section": "3.1 Counting Tasks on Graphs"}, {"content": "| Model | Connectivity |  | Color Counting |  | Shortest Path |  |\n|---|---|---|---|---|---|---|\n|  | 1K | 100K | 1K | 100K | 1K | 10K |\n| **Reference Baselines** | Accuracy \u2191 |  | Accuracy \u2191 |  | RMSE \u2193 |  |\n| GCN | 63.3 | 70.8 | 52.7 | 55.9 | 2.38 | 2.11 |\n| GatedGCN | **74.9** | **77.5** | **55.0** | **56.6** | **1.98** | **1.93** |\n| MPNN | 71.8 | **76.1** | **53.9** | **57.7** | **1.96** | **1.93** |\n| GIN | **71.9** | 74.6 | 52.4 | 55.1 | 2.03 | 1.98 |\n| **Transformers** |  |  |  |  |  |  |\n| Node | **85.7** | **86.2** | **73.1** | **77.4** | **1.19** | **1.06*** |\n| w/o PE | 9.4 | 6.8 | 35.8 | 28.9 | 4.12 | 5.33 |\n| HAC (DFS) | **87.0** | **88.1** | **83.7** | **85.3** | **1.14** | **1.09** |\n| k-hop | 69.9 | 70.2 | 79.9 | 80.3 | 2.10 | 2.15 |\n| HAC (BFS) | 74.1 | 76.7 | 74.5 | 77.8 | 2.31 | 2.28 |\n| **Mamba** |  |  |  |  |  |  |\n| Node | **82.8** | **84.7** | **80.1** | **82.5** | **1.27** | **1.13** |\n| w/o PE | 9.2 | 7.5 | 78.9 | 81.3 | 4.09 | 5.22 |\n| HAC (DFS) | **83.6** | **85.2** | **85.2** | **85.4** | **1.12** | **1.15** |\n| k-hop | 70.9 | 71.0 | 82.6 | 83.5 | 2.03 | 2.11 |\n| HAC (BFS) | 76.3 | 77.4 | 83.7 | 84.1 | 2.24 | 2.18 |\n| **Hybrid (Mamba + Transformer)** |  |  |  |  |  |  |\n| Node | **88.1** | **88.6** | **82.9** | **83.0** | **1.24** | **1.13** |\n| w/o PE | 8.9 | 8.1 | 83.2 | 84.8 | 4.65 | 4.89 |\n| HAC (DFS) | **90.7*** | **91.4*** | **85.8*** | **86.2*** | **1.11*** | **1.93** |\n| k-hop | 70.8 | 73.3 | 83.7 | 84.6 | 1.99 | 2.04 |\n| HAC (BFS) | 78.0 | 79.5 | 83.1 | 83.7 | 2.16 | 2.13 |", "caption": "Table 2: Graph tasks that require global information\u2020. The first and second best results of each type are highlighted. The best overall result for each task is marked *.", "description": "This table presents the results of various graph tasks that necessitate global information processing.  The tasks are: graph connectivity (binary classification), color counting (counting the number of nodes with each color), and shortest path (predicting shortest path lengths).  For each task, multiple models were tested, and their performance is ranked, with the top two results for each task highlighted.  The overall best-performing model for each task is marked with an asterisk (*).  The table aims to illustrate how different model architectures handle graph problems that require considering the overall graph structure, rather than just local neighborhoods.", "section": "3.1 COUNTING TASKS ON GRAPHS"}, {"content": "Model|MNIST|CIFAR10|PATTERN|MalNet-Tiny\n---|---|---|---|---\nGCN|0.9071<sub>\u00b10.0021</sub>|0.5571<sub>\u00b10.0038</sub>|0.7189<sub>\u00b10.0033</sub>|0.8100<sub>\u00b10.0000</sub>\nGraphSAGE|0.9731<sub>\u00b10.0009</sub>|0.6577<sub>\u00b10.0030</sub>|0.5049<sub>\u00b10.0001</sub>|0.8730<sub>\u00b10.0002</sub>\nGAT|0.9554<sub>\u00b10.0021</sub>|0.6422<sub>\u00b10.0046</sub>|0.7827<sub>\u00b10.0019</sub>|0.8509<sub>\u00b10.0025</sub>\nSPN|0.8331<sub>\u00b10.0446</sub>|0.3722<sub>\u00b10.0827</sub>|0.8657<sub>\u00b10.0014</sub>|0.6407<sub>\u00b10.0581</sub>\nGIN|0.9649<sub>\u00b10.0025</sub>|0.5526<sub>\u00b10.0152</sub>|0.8539<sub>\u00b10.0013</sub>|0.8898<sub>\u00b10.0055</sub>\nGated-GCN|0.9734<sub>\u00b10.0014</sub>|0.6731<sub>\u00b10.0031</sub>|0.8557<sub>\u00b10.0008</sub>|0.9223<sub>\u00b10.0065</sub>\nCRaWl|0.9794<sub>\u00b10.050</sub>|0.6901<sub>\u00b10.0259</sub>|-|-\nNAGphormer|-|-|0.8644<sub>\u00b10.0003</sub>|-\nGPS|0.9811<sub>\u00b10.0011</sub>|0.7226<sub>\u00b10.0031</sub>|0.8664<sub>\u00b10.0011</sub>|0.9298<sub>\u00b10.0047</sub>\nGPS (BigBird)|0.9817<sub>\u00b10.0001</sub>|0.7048<sub>\u00b10.0010</sub>|0.8600<sub>\u00b10.0014</sub>|0.9234<sub>\u00b10.0034</sub>\nExphormer|0.9855<sub>\u00b10.0003</sub>|0.7469<sub>\u00b10.0013</sub>|0.8670<sub>\u00b10.0003</sub>|**0.9402<sub>\u00b10.0020</sub>**\nNodeFormer|-|-|0.8639<sub>\u00b10.0021</sub>|-\nDIFFormer|-|-|0.8701<sub>\u00b10.0018</sub>|-\nGRIT|0.9810<sub>\u00b10.0011</sub>|0.7646<sub>\u00b10.0088</sub>|0.8719<sub>\u00b10.0008</sub>|-\nGRED|**0.9838<sub>\u00b10.0002</sub>**|**0.7685<sub>\u00b10.0019</sub>**|0.8675<sub>\u00b10.0002</sub>|-\nGMN|0.9783<sub>\u00b10.0020</sub>|0.7444<sub>\u00b10.0009</sub>|0.8649<sub>\u00b10.0019</sub>|0.9352<sub>\u00b10.0036</sub>\nGSM++ (BFS)|**0.9848<sub>\u00b10.0012</sub>**|0.7659<sub>\u00b10.0024</sub>|**0.8738<sub>\u00b10.0014</sub>**|**0.9417<sub>\u00b10.0020</sub>**\nGSM++ (DFS)|0.9829<sub>\u00b10.0014</sub>|**0.7692<sub>\u00b10.0031</sub>**|**0.8731<sub>\u00b10.0008</sub>**|0.9389<sub>\u00b10.0024</sub>\nGSM++ (MoT)|**0.9884<sub>\u00b10.0015</sub>**|**0.7781<sub>\u00b10.0028</sub>**|**0.8793<sub>\u00b10.0015</sub>**|**0.9437<sub>\u00b10.0058</sub>**", "caption": "Table 3: GNN benchmark datasets\u00a0(Dwivedi et\u00a0al., 2023). The first, second, and third best results are highlighted.", "description": "Table 3 presents the results of GNN benchmark datasets from Dwivedi et al. (2023).  It shows a comparison of different graph neural network models' performance on various node and graph classification tasks using four benchmark datasets: MNIST, CIFAR10, and the PATTERN and Peptides-Func datasets. The table highlights the top three performing models for each dataset and task, providing a quantitative comparison of their accuracy.", "section": "5 Experiments"}, {"content": "| Model | COCO-SP F1 score \u2191 | PascalVOC-SP F1 score \u2191 | PATTERN Accuracy \u2191 |\n|---|---|---|---| \n| GPS Framework |  |  |  |\n| Base | 0.3774 | 0.3689 | 0.8664 |\n| +Hybrid | **0.3789** | 0.3691 | 0.8665 |\n| +HAC | 0.3780 | **0.3699** | **0.8667** |\n| +MoT | **0.3791** | **0.3703** | **0.8677** |\n| NAGphormer Framework |  |  |  |\n| Base | 0.3458 | 0.4006 | 0.8644 |\n| +Hybrid | 0.3461 | **0.4046** | 0.8650 |\n| +HAC | **0.3507** | 0.4032 | **0.8653** |\n| +MoT | **0.3591** | **0.4105** | **0.8657** |\n| GSM++ |  |  |  |\n| Base | **0.3789** | **0.4128** | **0.8738** |\n| -PE | **0.3780** | **0.4073** | 0.8511 |\n| -Hybrid | 0.3767 | 0.4058 | 0.8500 |\n| -HAC | 0.3591 | 0.3996 | **0.8617** |", "caption": "Table 4: Ablation studies. The first and second best results for each model are highlighted.", "description": "This table presents the results of ablation studies conducted on the GSM++ model.  It shows the impact of removing different components of the model (e.g., the hybrid encoder, hierarchical positional encoding, HAC tokenization, and MoT) on the overall performance. By comparing the performance metrics (F1 score and accuracy) obtained with the full model against those obtained with variations of the model where components were removed, this table helps determine the contribution of each component to the model's overall effectiveness and efficiency.", "section": "4 Enhancing Graph to Sequence Models"}, {"content": "| Method | Tokenization | Local Encoding | Global Encoding |\n|---|---|---|---|\n| DeepWalk (2014) | Random Walk | Identity(.) | SkipGram |\n| Node2Vec (2016) | 2<sup>nd</sup> Order Random Walk | Identity(.) | SkipGram |\n| Node2Vec (2016) | Random Walk | Identity(.) | SkipGram |\n| GraphTransformer (2020) | Node | Identity(.) | Transformer |\n| GraphGPS (2022) | Node | Identity(.) | Transformer |\n| NodeFormer (2022) | Node | Gumbel-Softmax(.) | Transformer |\n| Graph-ViT (2023) | METIS Clustering (Patching) | Gcn(.) | ViT |\n| Exphormer (2023) | Node | Identity(.) | Sparse Transformer |\n| CRaWl (2023) | Random Walk | 1D Convolutions | MLP(.) |\n| NAGphormer (2023) | k-hop neighborhoods | Gcn(.) | Transformer |\n| SP-MPNNs (2022) | k-hop neighborhoods | Identity(.) | GIN(.) |\n| GRED (2023) | k-hop neighborhood | MLP(.) | Rnn(.) |\n| S4G (2024) | k-hop neighborhood | Identity(.) | S4(.) |\n| Graph Mamba (2024) | Union of Random Walks (With varying length) | Gated-Gcn(.) | Bi-Mamba(.) |", "caption": "Table 5: How are different models special instances of GSM framework", "description": "This table shows how various graph neural network models can be viewed as special cases of the general Graph Sequence Model (GSM) framework proposed in the paper.  For each model, the table lists the tokenization method used to convert the graph into sequences (e.g., node-based, subgraph-based), the local encoding technique applied to each token (e.g., identity, GCN), and the global encoding model used to capture long-range dependencies (e.g., SkipGram, Transformer, RNN). This allows for a systematic comparison of different model architectures and highlights the common underlying principles across these models.", "section": "Encoding Graphs to Sequences: A Unified Model"}, {"content": "| Dataset | #Graphs | Average #Nodes | Average #Edges | #Class | Input Level | Task | Metric |\n|---|---|---|---|---|---|---|---| \n| Long-range Graph Benchmark (Dwivedi et al., 2022a) |  |  |  |  |  |  |  |\n| COCO-SP | 123,286 | 476.9 | 2693.7 | 81 | Node | Classification | F1 score |\n| PascalVOC-SP | 11,355 | 479.4 | 2710.5 | 21 | Node | Classification | F1 score |\n| Peptides-Func | 15,535 | 150.9 | 307.3 | 10 | Graph | Classification | Average Precision |\n| Peptides-Struct | 15,535 | 150.9 | 307.3 | 11 (regression) | Graph | Regression | Mean Absolute Error |\n| GNN Benchmark (Dwivedi et al., 2023) |  |  |  |  |  |  |  |\n| Pattern | 14,000 | 118.9 | 3,039.3 | 2 | Node | Classification | Accuracy |\n| MNIST | 70,000 | 70.6 | 564.5 | 10 | Graph | Classification | Accuracy |\n| CIFAR10 | 60,000 | 117.6 | 941.1 | 10 | Graph | Classification | Accuracy |\n| MalNet-Tiny | 5,000 | 1,410.3 | 2,859.9 | 5 | Graph | Classification | Accuracy |\n| Heterophilic Benchmark (Platonov et al., 2023) |  |  |  |  |  |  |  |\n| Roman-empire | 1 | 22,662 | 32,927 | 18 | Node | Classification | Accuracy |\n| Amazon-ratings | 1 | 24,492 | 93,050 | 5 | Node | Classification | Accuracy |\n| Minesweeper | 1 | 10,000 | 39,402 | 2 | Node | Classification | ROC AUC |\n| Tolokers | 1 | 11,758 | 519,000 | 2 | Node | Classification | ROC AUC |\n| Very Large Dataset (Hu et al., 2020) |  |  |  |  |  |  |  |\n| arXiv-ogbn | 1 | 169,343 | 1,166,243 | 40 | Node | Classification | Accuracy |\n| products-ogbn | 1 | 2,449,029 | 61,859,140 | 47 | Node | Classification | Accuracy |\n| Color-connectivty task (Ramp\u00e1\u0161ek & Wolf, 2021) |  |  |  |  |  |  |  |\n| C-C 16x16 grid | 15,000 | 256 | 480 | 2 | Node | Classification | Accuracy |\n| C-C 32x32 grid | 15,000 | 1,024 | 1,984 | 2 | Node | Classification | Accuracy |\n| C-C Euroroad | 15,000 | 1,174 | 1,417 | 2 | Node | Classification | Accuracy |\n| C-C Minnesota | 6,000 | 2,642 | 3,304 | 2 | Node | Classification | Accuracy |", "caption": "Table 6: Dataset Statistics.", "description": "This table presents a comprehensive overview of the datasets used in the experiments.  For each dataset, it lists key statistics, including the number of graphs, the average number of nodes and edges per graph, the experimental setup (e.g., node classification, graph classification), the number of classes for classification tasks, and the specific evaluation metric used (e.g., accuracy, F1-score, AUC).  The datasets are categorized into those designed for long-range dependencies, heterophily, and those focused on specific tasks like color connectivity.", "section": "5 Experiments"}, {"content": "Model|Roman-empire|Amazon-ratings|Minesweeper\n---|---|---\nGCN|0.7369<sub>\u00b10.0074</sub>|0.4870<sub>\u00b10.0063</sub>|0.8975<sub>\u00b10.0052</sub>\nGraphSAGE|0.8574<sub>\u00b10.0067</sub>|**0.5363<sub>\u00b10.0039</sub>**|**0.9351<sub>\u00b10.0057</sub>**\nGAT|0.7973<sub>\u00b10.0039</sub>|0.5270<sub>\u00b10.0062</sub>|**0.9391<sub>\u00b10.0035</sub>**\nOrderedGNN|0.7768<sub>\u00b10.0039</sub>|0.4729<sub>\u00b10.0065</sub>|0.8058<sub>\u00b10.0108</sub>\ntGNN|0.7995<sub>\u00b10.0075</sub>|0.4821<sub>\u00b10.0053</sub>|**0.9193<sub>\u00b10.0077</sub>**\nGated-GCN|0.7446<sub>\u00b10.0054</sub>|0.4300<sub>\u00b10.0032</sub>|0.8754<sub>\u00b10.0122</sub>\nNAGphormer|0.7434<sub>\u00b10.0077</sub>|0.5126<sub>\u00b10.0072</sub>|0.8419<sub>\u00b10.0066</sub>\nGPS|0.8200<sub>\u00b10.0061</sub>|0.5310<sub>\u00b10.0042</sub>|0.9063<sub>\u00b10.0067</sub>\nExphormer|0.8903<sub>\u00b10.0037</sub>|0.5351<sub>\u00b10.0046</sub>|0.9074<sub>\u00b10.0053</sub>\nNodeFormer|0.6449<sub>\u00b10.0073</sub>|0.4386<sub>\u00b10.0035</sub>|0.8671<sub>\u00b10.0088</sub>\nDIFFormer|0.7910<sub>\u00b10.0032</sub>|0.4784<sub>\u00b10.0065</sub>|0.9089<sub>\u00b10.0058</sub>\nGOAT|0.7159<sub>\u00b10.0125</sub>|0.4461<sub>\u00b10.0050</sub>|0.8109<sub>\u00b10.0102</sub>\nGMN|0.8219<sub>\u00b10.0012</sub>|0.5327<sub>\u00b10.0030</sub>|0.8992<sub>\u00b10.0063</sub>\nGSM++ (BFS)|**0.9003<sub>\u00b10.0087</sub>**|**0.5381<sub>\u00b10.0035</sub>**|0.9109<sub>\u00b10.0098</sub>\nGSM++ (DFS)|**0.9124<sub>\u00b10.0023</sub>**|0.5361<sub>\u00b10.0029</sub>|0.9145<sub>\u00b10.0036</sub>\nGSM++ (MoT)|**0.9177<sub>\u00b10.0040</sub>**|**0.5390<sub>\u00b10.0104</sub>**|0.9149<sub>\u00b10.0111</sub><sup>\u2020</sup>\n\n<sup>\u2020</sup> GSM++ (all variants) achieve the best three results among all graph sequence models.", "caption": "Table 7: Heterophilic datasets\u00a0(Platonov et\u00a0al., 2023). The first, second, and third results are highlighted.", "description": "This table presents the results of different graph neural network models on three heterophilic graph datasets: Roman-empire, Amazon-ratings, and Minesweeper.  Heterophilic graphs are those where nodes within the same class have diverse features, making them challenging for graph neural networks to learn. The table shows the accuracy, F1-score, and ROC AUC (Area Under the Curve) achieved by each model on each dataset.  The top three performing models for each metric are highlighted to facilitate comparison and identification of the best-performing models for each dataset and task.", "section": "5 Experiments"}, {"content": "Model|COCO-SP|PascalVOC-SP|Peptides-Func\n---|---|---\nGCN|0.0841<sub>\u00b10.0010</sub>|0.1268<sub>\u00b10.0060</sub>|0.5930<sub>\u00b10.0023</sub>\nGIN|0.1339<sub>\u00b10.0044</sub>|0.1265<sub>\u00b10.0076</sub>|0.5498<sub>\u00b10.0079</sub>\nGated-GCN|0.2641<sub>\u00b10.0045</sub>|0.2873<sub>\u00b10.0219</sub>|0.5864<sub>\u00b10.0077</sub>\nGAT|0.1296<sub>\u00b10.0028</sub>|0.1753<sub>\u00b10.0329</sub>|0.5308<sub>\u00b10.0019</sub>\nMixHop|-|0.2506<sub>\u00b10.0133</sub>|0.6843<sub>\u00b10.0049</sub>\nDIGL|-|0.2921<sub>\u00b10.0038</sub>|0.6830<sub>\u00b10.0026</sub>\nSPN|-|0.2056<sub>\u00b10.0338</sub>|0.6926<sub>\u00b10.0247</sub>\nSAN+LapPE|0.2592<sub>\u00b10.0158</sub>|0.3230<sub>\u00b10.0039</sub>|0.6384<sub>\u00b10.0121</sub>\nNAGphormer|0.3458<sub>\u00b10.0070</sub>|0.4006<sub>\u00b10.0061</sub>|-\nGraph ViT|-|-|0.6855<sub>\u00b10.0049</sub>\nGPS|**0.3774<sub>\u00b10.0150</sub>**|0.3689<sub>\u00b10.0131</sub>|0.6575<sub>\u00b10.0049</sub>\nExphormer|0.3430<sub>\u00b10.0108</sub>|0.3975<sub>\u00b10.0037</sub>|0.6527<sub>\u00b10.0043</sub>\nNodeFormer|0.3275<sub>\u00b10.0241</sub>|0.4015<sub>\u00b10.0082</sub>|-\nDIFFormer|0.3620<sub>\u00b10.0012</sub>|0.3988<sub>\u00b10.0045</sub>|-\nGRIT|-|-|0.6988<sub>\u00b10.0082</sub>\nGRED|-|-|**0.7085<sub>\u00b10.0027</sub>**\nGMN|0.3618<sub>\u00b10.0053</sub>|**0.4169<sub>\u00b10.0103</sub>**|0.6860<sub>\u00b10.0012</sub>\nGSM++ (BFS)|**0.3789<sub>\u00b10.0160</sub>**|0.4128<sub>\u00b10.0027</sub>|0.6991<sub>\u00b10.0008</sub>\nGSM++ (DFS)|0.3769<sub>\u00b10.0027</sub>|**0.4174<sub>\u00b10.0031</sub>**|**0.7019<sub>\u00b10.0084</sub>**\nGSM++ (MoT)|**0.3801<sub>\u00b10.0122</sub>**|**0.4193<sub>\u00b10.0075</sub>**|**0.7092<sub>\u00b10.0076</sub>**", "caption": "Table 8: Long-Range Datasets\u00a0(Dwivedi et\u00a0al., 2022a). The first, second, and third results are highlighted.", "description": "This table presents the results of various graph neural network models on three benchmark datasets: COCO-SP, PascalVOC-SP, and Peptides-Func.  These datasets are characterized by long-range dependencies between nodes, making them challenging for many graph models. The table shows the performance of each model on each dataset, measured by F1 score (for COCO-SP and PascalVOC-SP) and Average Precision (for Peptides-Func).  The top three performing models for each dataset are highlighted to illustrate the relative strengths and weaknesses of different approaches for handling long-range graph dependencies.", "section": "5 EXPERIMENTS"}, {"content": "| Model | GatedGCN | NAGphormer | GPS | Exphormer | GOAT | GRIT | GMN | GSM++ BFS | GSM++ DFS | GSM++ MoT |\n|---|---|---|---|---|---|---|---|---|---|---| \n| arXiv-ogbn Performance | 0.7141 | 0.7013 | OOM | 0.7228 | 0.7196 | OOM | 0.7248 | **0.7297** | **0.7261** | **0.7301** |\n| arXiv-ogbn Memory Usage (GB) | 11.87 | **6.81** | OOM | 37.01 | 13.12 | OOM | **5.63** | 24.8 | **4.7** | 14.9 |\n| arXiv-ogbn Training Time/Epoch (s) | **1.94** | 5.96 | OOM | 2.15 | 8.69 | OOM | **1.78** | 2.33 | **1.95** | 4.16 |\n| products-ogbn Performance | 0.0000 | 0.0000 | OOM | OOM | **0.8200** | OOM | OOM | 0.8071 | **0.8080** | **0.8213** |\n| products-ogbn Memory Usage (GB) | **11.13** | **10.04** | OOM | OOM | 12.06 | OOM | OOM | 38.14 | **9.15** | 11.96 |\n| products-ogbn Training Time/Epoch (s) | **1.92** | 12.08 | OOM | OOM | 29.50 | OOM | OOM | **6.97** | 12.19 | **11.87** |", "caption": "Table 9: Efficiency evaluation on large graphs. The first, second, and third results for each metric are highlighted. OOM: Out of memory.", "description": "This table presents a comparison of the performance of various graph neural network models on two large graph datasets: arXiv-ogbn and products-ogbn.  The metrics evaluated include accuracy (Performance), memory usage (Memory Usage (GB)), and training time per epoch (Training Time/Epoch (s)).  The models compared encompass several state-of-the-art Graph Transformers and a novel hybrid model called GSM++. The table highlights the top three performing models for each metric.  'OOM' indicates that the model ran out of memory and could not complete training.", "section": "5 Experiments"}]