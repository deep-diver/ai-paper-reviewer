{"importance": "This research offers a **cost-effective** alternative to scaling LLMs. It provides insights into trade-offs and lays the foundation for scalable, reasoning-capable LLMs in **resource-limited** environments. By releasing code/datasets as open-source resources, it fosters reproducibility and further exploration by the research community.", "summary": "RL fine-tuning enhances reasoning in small LLMs, achieving competitive performance with limited resources, despite optimization & length challenges.", "takeaways": ["Small LLMs can achieve rapid reasoning improvements with limited high-quality data using Reinforcement Learning.", "Mixing easy and hard problems with reduced length constraints enhances early performance and stabilizes reasoning.", "Cosine rewards stabilize completion lengths, improving training consistency."], "tldr": "Large language models (LLMs) typically need massive computational resources and datasets. This limits accessibility for resource-constrained settings. It is important to investigate the potential of reinforcement learning (RL) to improve reasoning in smaller LLMs. The study will focus on a 1.5B parameter model under strict constraints. Training is done on 4 NVIDIA A40 GPUs within 24 hours. The study aims to addresses the question of whether LLM can be elevated using an RL-based approach.\n\nThe paper adapts the Group Relative Policy Optimization (GRPO) algorithm and curates a compact, high-quality mathematical reasoning dataset. This enables three experiments to explore model behavior and performance. The findings show reasoning gains using only 7,000 samples at a low training cost. The code and datasets are released as open-source resources. The **Open-RS** models outperform other models. For example, the Open-RS3 achieves the highest AIME24 score, surpassing o1-preview.", "affiliation": "VNU University of Science, Vietnam", "categories": {"main_category": "Machine Learning", "sub_category": "Reinforcement Learning"}, "podcast_path": "2503.16219/podcast.wav"}