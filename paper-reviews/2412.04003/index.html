<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><title>Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement &#183; HF Daily Paper Reviews by AI</title>
<meta name=title content="Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement &#183; HF Daily Paper Reviews by AI"><meta name=description content="Marco-LLM: A groundbreaking multilingual LLM significantly enhances cross-lingual capabilities via massive multilingual training, bridging the performance gap between high- and low-resource languages."><meta name=keywords content="Natural Language Processing,Large Language Models,üè¢ Alibaba International Digital Commerce,"><link rel=canonical href=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04003/><link type=text/css rel=stylesheet href=/ai-paper-reviewer/css/main.bundle.min.595affd4445a931ea6d6e3a5a3c709930fa52a60be10b21c6f81fdb8fecaacea33aacedf80cdc88be45f189be14ed4ce53ea74a1e1406fad9cbf90c5ed409173.css integrity="sha512-WVr/1ERakx6m1uOlo8cJkw+lKmC+ELIcb4H9uP7KrOozqs7fgM3Ii+RfGJvhTtTOU+p0oeFAb62cv5DF7UCRcw=="><script type=text/javascript src=/ai-paper-reviewer/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script><script defer type=text/javascript id=script-bundle src=/ai-paper-reviewer/js/main.bundle.min.efbf3b6b987689fffaf2d7b73173d2690c0279a04d444b0537a77d7f4ff6e6d493445400cb0cf56bc0f0f123e19f15394e63cae34e67f069bd013dd5c73df56e.js integrity="sha512-7787a5h2if/68te3MXPSaQwCeaBNREsFN6d9f0/25tSTRFQAywz1a8Dw8SPhnxU5TmPK405n8Gm9AT3Vxz31bg==" data-copy data-copied></script><script src=/ai-paper-reviewer/lib/zoom/zoom.min.37d2094687372da3f7343a221a470f6b8806f7891aa46a5a03966af7f0ebd38b9fe536cb154e6ad28f006d184b294525a7c4054b6bbb4be62d8b453b42db99bd.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ=="></script><link rel=apple-touch-icon sizes=180x180 href=/ai-paper-reviewer/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/ai-paper-reviewer/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/ai-paper-reviewer/favicon-16x16.png><link rel=manifest href=/ai-paper-reviewer/site.webmanifest><meta property="og:url" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04003/"><meta property="og:site_name" content="HF Daily Paper Reviews by AI"><meta property="og:title" content="Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement"><meta property="og:description" content="Marco-LLM: A groundbreaking multilingual LLM significantly enhances cross-lingual capabilities via massive multilingual training, bridging the performance gap between high- and low-resource languages."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="paper-reviews"><meta property="article:published_time" content="2024-12-05T00:00:00+00:00"><meta property="article:modified_time" content="2024-12-05T00:00:00+00:00"><meta property="article:tag" content="Natural Language Processing"><meta property="article:tag" content="Large Language Models"><meta property="article:tag" content="üè¢ Alibaba International Digital Commerce"><meta property="og:image" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04003/cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04003/cover.png"><meta name=twitter:title content="Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement"><meta name=twitter:description content="Marco-LLM: A groundbreaking multilingual LLM significantly enhances cross-lingual capabilities via massive multilingual training, bridging the performance gap between high- and low-resource languages."><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"Paper Reviews by AI","name":"Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement","headline":"Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement","abstract":"Marco-LLM: A groundbreaking multilingual LLM significantly enhances cross-lingual capabilities via massive multilingual training, bridging the performance gap between high- and low-resource languages.","inLanguage":"en","url":"https:\/\/deep-diver.github.io\/ai-paper-reviewer\/paper-reviews\/2412.04003\/","author":{"@type":"Person","name":"Hugging Face Daily Papers"},"copyrightYear":"2024","dateCreated":"2024-12-05T00:00:00\u002b00:00","datePublished":"2024-12-05T00:00:00\u002b00:00","dateModified":"2024-12-05T00:00:00\u002b00:00","keywords":["Natural Language Processing","Large Language Models","üè¢ Alibaba International Digital Commerce"],"mainEntityOfPage":"true","wordCount":"7239"}]</script><meta name=author content="Hugging Face Daily Papers"><link href=https://github.com/deep-diver/paper-reviewer/ rel=me><link href=https://twitter.com/algo_diver/ rel=me><script src=/ai-paper-reviewer/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj+KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script><script defer src=/ai-paper-reviewer/lib/typeit/typeit.umd.1b3200cb448f5cd1f548f2781452643d3511a43584b377b82c03a58055da4fdb7bc8f6c6c2ce846480c7677ff25bfd0d75f15823c09443ab18e0fd2cad792587.js integrity="sha512-GzIAy0SPXNH1SPJ4FFJkPTURpDWEs3e4LAOlgFXaT9t7yPbGws6EZIDHZ3/yW/0NdfFYI8CUQ6sY4P0srXklhw=="></script><script defer src=/ai-paper-reviewer/lib/packery/packery.pkgd.min.js integrity></script><script type=text/javascript src=/ai-paper-reviewer/js/shortcodes/gallery.min.9b4cb28f931ed922c26fb9b2510c2debb370f6a63305050c2af81740b2919883715e24efbbdf3a081496718ec751df3a72729d4d0bc71d6071297563a97ce1ee.js integrity="sha512-m0yyj5Me2SLCb7myUQwt67Nw9qYzBQUMKvgXQLKRmINxXiTvu986CBSWcY7HUd86cnKdTQvHHWBxKXVjqXzh7g=="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-KX0S6Q55Y7"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-KX0S6Q55Y7")</script><meta name=theme-color><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-app.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-firestore.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-auth.js></script><script>const firebaseConfig={apiKey:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",authDomain:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",projectId:"neurips2024-f3065",storageBucket:"neurips2024-f3065.firebasestorage.app",messagingSenderId:"982475958898",appId:"1:982475958898:web:2147e5d7753d6ac091f0eb",measurementId:"G-YQ46HXQ9JS"};var app=firebase.initializeApp(firebaseConfig),db=firebase.firestore(),auth=firebase.auth()</script></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a></div><div class=min-h-[148px]></div><div class="fixed inset-x-0 pl-[24px] pr-[24px]" style=z-index:100><div id=menu-blur class="absolute opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl shadow-2xl"></div><div class="relative max-w-[64rem] ml-auto mr-auto"><div style=padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start space-x-3"><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/ai-paper-reviewer/ class="text-base font-medium text-gray-500 hover:text-gray-900">HF Daily Paper Reviews by AI</a></nav><nav class="hidden md:flex items-center space-x-5 md:ml-12 h-12"><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title="About This Project">About</p></a><a href=/ai-paper-reviewer/2025-01-31/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title=2025-01-31s>2025-01-31</p></a><a href=/ai-paper-reviewer/2025-02-04/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title=2025-02-04s>2025-02-04</p></a><a href=/ai-paper-reviewer/2025-02-05/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title=2025-02-05s>2025-02-05</p></a><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title="Paper Reviews by AI">Archive</p></a><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title=Tags>Tags</p></a><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></span><p class="text-base font-medium" title></p></a><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span><p class="text-base font-medium" title></p></a><button id=search-button aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button><div class="ltr:mr-14 rtl:ml-14 flex items-center"><button id=appearance-switcher aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></nav><div class="flex md:hidden items-center space-x-5 md:ml-12 h-12"><span></span>
<button id=search-button-mobile aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></button>
<button id=appearance-switcher-mobile aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400" style=margin-right:5px><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div><div class="-my-2 -mr-2 md:hidden"><label id=menu-button class=block><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper style=padding-top:5px class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li id=menu-close-button><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title="About This Project">About</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-01-31/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title=2025-01-31s>2025-01-31</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-02-04/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title=2025-02-04s>2025-02-04</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-02-05/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title=2025-02-05s>2025-02-05</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title="Paper Reviews by AI">Archive</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title=Tags>Tags</p></a></li><li class=mt-1><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li><li class=mt-1><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li></ul></div></label></div></div></div></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("menu-blur");n.style.opacity=t/300})</script><div class="relative flex flex-col grow"><main id=main-content class=grow><article><div id=hero class="h-[150px] md:h-[200px]"></div><div class="fixed inset-x-0 top-0 h-[800px] single_hero_background nozoom" style=background-image:url(/ai-paper-reviewer/paper-reviews/2412.04003/cover_hu_992405d00b1d918.png)><div class="absolute inset-0 bg-gradient-to-t from-neutral dark:from-neutral-800 to-transparent mix-blend-normal"></div><div class="absolute inset-0 opacity-60 bg-gradient-to-t from-neutral dark:from-neutral-800 to-neutral-100 dark:to-neutral-800 mix-blend-normal"></div></div><div id=background-blur class="fixed opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl"></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("background-blur");n.style.opacity=t/300})</script><header id=single_header class="mt-5 max-w-prose"><ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden"><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/>HF Daily Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class=inline><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/>Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/2412.04003/>Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement</a><span class="px-1 text-primary-500">/</span></li></ol><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime=2024-12-05T00:00:00+00:00>5 December 2024</time><span class="px-2 text-primary-500">&#183;</span><span>7239 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">34 mins</span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=views_paper-reviews/2412.04003/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=views>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 576 512"><path fill="currentcolor" d="M288 32c-80.8.0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7.0 24.6C17.3 304 48.6 356 95.4 399.4 142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1 3.3-7.9 3.3-16.7.0-24.6-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144 64.5-144 144-144 144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64-11.5.0-22.3-3-31.6-8.4-.2 2.8-.4 5.5-.4 8.4.0 53 43 96 96 96s96-43 96-96-43-96-96-96c-2.8.0-5.6.1-8.4.4 5.3 9.3 8.4 20.1 8.4 31.6z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=likes_paper-reviews/2412.04003/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=likes>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<button id=button_likes class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400" onclick=process_article()>
<span id=button_likes_heart style=display:none class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span><span id=button_likes_emtpty_heart class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M244 84l11.1 12 12-11.98C300.6 51.37 347 36.51 392.6 44.1 461.5 55.58 512 115.2 512 185.1V190.9c0 41.5-17.2 81.2-47.6 109.5L283.7 469.1c-7.5 7-17.4 10.9-27.7 10.9S235.8 476.1 228.3 469.1L47.59 300.4C17.23 272.1.0 232.4.0 190.9V185.1c0-69.9 50.52-129.52 119.4-141 44.7-7.59 92 7.27 124.6 39.9C243.1 84 244 84.01 244 84zm11.1 79.9-45-46.8c-21.7-20.82-52.5-30.7-82.8-25.66C81.55 99.07 48 138.7 48 185.1V190.9c0 28.2 11.71 55.2 32.34 74.4L256 429.3l175.7-164c20.6-19.2 32.3-46.2 32.3-74.4V185.1c0-46.4-33.6-86.03-79.3-93.66C354.4 86.4 323.6 96.28 301.9 117.1l-46.8 46.8z"/></svg>
</span></span><span id=button_likes_text>&nbsp;Like</span></button></span></div><div class="flex flex-row flex-wrap items-center"><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/ai-generated/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">AI Generated
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/-daily-papers/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">ü§ó Daily Papers
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/natural-language-processing/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Natural Language Processing
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/large-language-models/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Large Language Models
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/-alibaba-international-digital-commerce/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">üè¢ Alibaba International Digital Commerce</span></span></span></div></div><div class="flex author"><img class="!mt-0 !mb-0 h-24 w-24 rounded-full ltr:mr-4 rtl:ml-4" width=96 height=96 alt="Hugging Face Daily Papers" src=/ai-paper-reviewer/img/avatar_hu_97e7d424fadd1c26.png><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">Author</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">Hugging Face Daily Papers</div><div class="text-sm text-neutral-700 dark:text-neutral-400">I am AI, and I review papers on HF Daily Papers</div><div class="text-2xl sm:text-lg"><div class="flex flex-wrap text-neutral-400 dark:text-neutral-500"><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://github.com/deep-diver/paper-reviewer/ target=_blank aria-label=Github rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://twitter.com/algo_diver/ target=_blank aria-label=Twitter rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span></a></div></div></div></div><div class=mb-5></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first lg:ml-auto px-0 lg:order-last ltr:lg:pl-8 rtl:lg:pr-8"><div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-[140px]"><details open id=TOCView class="toc-right mt-0 overflow-y-scroll overscroll-contain scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600 rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 hidden lg:block"><summary class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="min-w-[220px] py-2 border-dotted ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#multilingual-llm>Multilingual LLM</a></li><li><a href=#continual-pretraining>Continual Pretraining</a></li><li><a href=#parallel-data-impact>Parallel Data Impact</a></li><li><a href=#post-training-methods>Post-training Methods</a></li><li><a href=#future-research>Future Research</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><details class="toc-inside mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 lg:hidden"><summary class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#multilingual-llm>Multilingual LLM</a></li><li><a href=#continual-pretraining>Continual Pretraining</a></li><li><a href=#parallel-data-impact>Parallel Data Impact</a></li><li><a href=#post-training-methods>Post-training Methods</a></li><li><a href=#future-research>Future Research</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><script>var margin=200,marginError=50;(function(){var t=$(window),e=$("#TOCView"),s=e.height();function n(){var n=t.height()-margin;s>=n?(e.css("overflow-y","scroll"),e.css("max-height",n+marginError+"px")):(e.css("overflow-y","hidden"),e.css("max-height","9999999px"))}t.on("resize",n),$(document).ready(n)})()</script></div></div><div class="min-w-0 min-h-0 max-w-fit"><div class="article-content max-w-prose mb-20"><br><div class="flex flex-row flex-wrap items-center space-x-2"><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 48 48" fill="none"><rect width="48" height="48" fill="#fff" fill-opacity=".01"/><path d="M18 43V22c0-3.3137 2.6863-6 6-6s6 2.6863 6 6V43" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M12 40V22c0-6.6274 5.3726-12 12-12s12 5.3726 12 12V40" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M6 35V22C6 12.0589 14.0589 4 24 4s18 8.0589 18 18V35" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 44V31" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 24.625v-2.75" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/></svg>
</span></span><span>2412.04003</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg fill="#000" height="800" width="800" id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 511.999 511.999"><g><g><path d="M421.578 190.264l-99.847-99.847c-2.439-2.439-6.391-2.439-8.829.0L82.824 320.495c-2.439 2.439-2.439 6.392.0 8.829l99.847 99.847c2.439 2.439 6.391 2.439 8.829.0l230.078-230.078C424.017 196.655 424.017 192.703 421.578 190.264z"/></g></g><g><g><path d="M506.511 87.672 424.323 5.484c-7.308-7.31-19.175-7.315-26.488.0L348.219 55.1c-2.439 2.439-2.439 6.391.0 8.829l99.847 99.847c2.439 2.437 6.391 2.437 8.829.0l49.616-49.616C513.826 106.847 513.826 94.987 506.511 87.672z"/></g></g><g><g><path d="M508.133 491.11c-1.054-9.556-9.489-16.599-19.104-16.599H111.633l36.058-15.163c4.088-1.719 5.131-7.034 1.994-10.17l-86.854-86.854c-3.137-3.135-8.451-2.094-10.17 1.994C52.224 365.359 2.052 484.66 1.627 485.707c-5.815 13.208 4.855 27.01 18.107 26.263H489.52C500.566 511.97 509.379 502.408 508.133 491.11z"/></g></g></svg>
</span></span><span>Lingfeng Ming et el.</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span>ü§ó 2024-12-06</span></span></span></div></div><p><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://arxiv.org/abs/2412.04003 target=_self role=button>‚Üó arXiv
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://huggingface.co/papers/2412.04003 target=_self role=button>‚Üó Hugging Face
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://paperswithcode.com/paper/marco-llm-bridging-languages-via-massive target=_self role=button>‚Üó Papers with Code</a></p><audio controls><source src=https://ai-paper-reviewer.com/2412.04003/podcast.wav type=audio/wav>Your browser does not support the audio element.</audio><h3 class="relative group">TL;DR<div id=tldr class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#tldr aria-label=Anchor>#</a></span></h3><div class="lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl"><p>Current large language models (LLMs) struggle with multilingual tasks, especially for low-resource languages. This limitation stems from the scarcity of training data and the inherent difficulty of balancing multiple languages during model training. This research introduces a novel approach to overcome these issues.</p><p>The researchers developed Marco-LLM, a multilingual LLM, using a two-stage continual pre-training strategy and a massive multilingual dataset. This method significantly improved performance on various multilingual benchmarks, showcasing substantial enhancements in any-to-any machine translation tasks. The study demonstrates the effectiveness of their multilingual training strategy in improving LLM performance across various languages, including those with limited resources.</p></div><h4 class="relative group">Key Takeaways<div id=key-takeaways class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#key-takeaways aria-label=Anchor>#</a></span></h4><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-50f75dd6359ed346878fda204e1671d3></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-50f75dd6359ed346878fda204e1671d3",{strings:[" Marco-LLM, a multilingual LLM, substantially improves performance on low-resource language tasks by utilizing massive multilingual data and continual pre-training strategies. "],speed:10,lifeLike:!0,startDelay:0,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-a4ed68cf376f9000f64c952feb727585></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-a4ed68cf376f9000f64c952feb727585",{strings:[" The two-stage continual pre-training approach effectively balances adaptation and prevents catastrophic forgetting, leading to improved multilingual performance. "],speed:10,lifeLike:!0,startDelay:1e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-6b6d0b5d0ebfa5a299326abe6b5af1e3></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-6b6d0b5d0ebfa5a299326abe6b5af1e3",{strings:[" Marco-LLM achieves state-of-the-art performance on various multilingual benchmarks, including MMMLU, AGIEval, Belebele, Flores-200, and XCOPA, demonstrating its effectiveness in multilingual understanding, reasoning, and machine translation. "],speed:10,lifeLike:!0,startDelay:2e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><h4 class="relative group">Why does it matter?<div id=why-does-it-matter class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#why-does-it-matter aria-label=Anchor>#</a></span></h4><p>This paper is crucial for researchers in multilingual NLP due to its novel approach of using <strong>massive multilingual continual pre-training</strong> to address the challenges of low-resource languages. It presents a significant advancement in the field by creating a multilingual LLM that outperforms existing models. Researchers can use this work as a stepping stone to create efficient and scalable multilingual LLMs, leading to breakthroughs in cross-lingual understanding and machine translation. Moreover, it provides valuable insights into continual pre-training strategies and data optimization for low-resource language, creating avenues for future research.</p><hr><h4 class="relative group">Visual Insights<div id=visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#visual-insights aria-label=Anchor>#</a></span></h4><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04003/extracted/6047129/Figures/marco_fig_init.png alt></figure></p><blockquote><p>üîº This figure compares the performance of Marco-LLM and other LLMs on two benchmark datasets: MMMLU and Flores. MMMLU assesses multilingual understanding capabilities across various subjects, while Flores evaluates any-to-any machine translation quality. The x-axis represents English-centric performance (e.g., English understanding or English-to-other language translation), and the y-axis shows multilingual performance. Each point represents an LLM. The plot shows that Marco-LLM achieves high performance on both English-centric and multilingual tasks, significantly outperforming other LLMs, especially in multilingual performance, closing the gap between high- and low-resource language capabilities. This demonstrates the success of Marco-LLM&rsquo;s massive multilingual training approach.</p><details><summary>read the caption</summary>Figure 1: Comparison of English-centric performance vs Multilingual performance on MMMLU and Flores. Our Marco-LLM demonstrates strong performance on both dimensions.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Category</th><th>Language</th><th>Total Tokens (B)</th><th>Used Tokens (B)</th><th>Utilization Rate (%)</th></tr></thead><tbody><tr><td><strong>High-Resource Languages</strong></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td>English (en)</td><td>1,459.7</td><td>90.4</td><td>6.2</td></tr><tr><td></td><td>Chinese (zh)</td><td>214.7</td><td>48.2</td><td>22.4</td></tr><tr><td></td><td>Arabic (ar)</td><td>45.8</td><td>10.6</td><td>23.0</td></tr><tr><td></td><td>German (de)</td><td>442.8</td><td>10.6</td><td>2.4</td></tr><tr><td></td><td>Spanish (es)</td><td>397.8</td><td>10.6</td><td>2.7</td></tr><tr><td></td><td>French (fr)</td><td>320.8</td><td>10.6</td><td>3.3</td></tr><tr><td></td><td>Korean (ko)</td><td>41.8</td><td>10.6</td><td>25.2</td></tr><tr><td></td><td>Japanese (ja)</td><td>224.2</td><td>10.6</td><td>4.7</td></tr><tr><td></td><td>Portuguese (pt)</td><td>145.3</td><td>10.6</td><td>7.3</td></tr><tr><td></td><td>Turkish (tr)</td><td>80.6</td><td>10.6</td><td>13.1</td></tr><tr><td><strong>Low-Resource Languages</strong></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td>Bengali (bn)</td><td>6.5</td><td>1.9</td><td>28.7</td></tr><tr><td></td><td>Hebrew (he)</td><td>11.3</td><td>1.9</td><td>16.4</td></tr><tr><td></td><td>Indonesian (id)</td><td>23.8</td><td>1.9</td><td>7.8</td></tr><tr><td></td><td>Italian (it)</td><td>56.3</td><td>1.9</td><td>3.3</td></tr><tr><td></td><td>Malay (ms)</td><td>2.9</td><td>1.9</td><td>64.4</td></tr><tr><td></td><td>Dutch (nl)</td><td>31.3</td><td>1.9</td><td>6.0</td></tr><tr><td></td><td>Polish (pl)</td><td>45.3</td><td>1.9</td><td>4.1</td></tr><tr><td></td><td>Russian (ru)</td><td>251.0</td><td>1.9</td><td>0.7</td></tr><tr><td></td><td>Thai (th)</td><td>8.3</td><td>1.9</td><td>22.4</td></tr><tr><td></td><td>Ukrainian (uk)</td><td>18.4</td><td>1.9</td><td>10.1</td></tr><tr><td></td><td>Urdu (ur)</td><td>8.7</td><td>1.9</td><td>21.4</td></tr><tr><td></td><td>Vietnamese (vi)</td><td>24.4</td><td>1.9</td><td>7.6</td></tr><tr><td></td><td>Czech (cs)</td><td>270.2</td><td>1.9</td><td>0.7</td></tr><tr><td></td><td>Greek (el)</td><td>376.8</td><td>1.9</td><td>0.5</td></tr><tr><td></td><td>Hungarian (hu)</td><td>214.4</td><td>1.9</td><td>0.9</td></tr><tr><td></td><td>Kazakh (kk)</td><td>16.8</td><td>1.9</td><td>11.1</td></tr><tr><td></td><td>Romanian (ro)</td><td>160.0</td><td>1.9</td><td>1.2</td></tr><tr><td></td><td>Azerbaijani (az)</td><td>19.4</td><td>1.9</td><td>9.6</td></tr><tr><td></td><td>Nepali (ne)</td><td>22.6</td><td>1.9</td><td>8.2</td></tr><tr><td><strong>Other Data Sources</strong></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td>Parallel Data</td><td>103.0</td><td>20.8</td><td>20.2</td></tr><tr><td></td><td>High-quality Knowledge Data</td><td>65.0</td><td>16.4</td><td>25.2</td></tr><tr><td></td><td>Synthetic Data</td><td>6.0</td><td>4.4</td><td>73.3</td></tr><tr><td><strong>Total</strong></td><td></td><td>5,115.9</td><td>300.0</td><td>5.9</td></tr></tbody></table></table></figure><blockquote><p>üîº Table 1 provides a detailed breakdown of the data used to train the Marco-LLM model. It shows the total number of tokens (in billions) available and utilized for both high-resource and low-resource languages, as well as other data sources such as synthetic and parallel data. The table also indicates the utilization rate (percentage) for each language and category, offering insights into the balance of data representation within the training dataset.</p><details><summary>read the caption</summary>Table 1: Overview of corpus utilization rates across various languages and categories of our corpus, we show the total number of tokens (in Billion Tokens) available and used for high-resource and low-resource languages, as well as other data sources such as synthetic data.</details></blockquote><h3 class="relative group">In-depth insights<div id=in-depth-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#in-depth-insights aria-label=Anchor>#</a></span></h3><h4 class="relative group">Multilingual LLM<div id=multilingual-llm class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#multilingual-llm aria-label=Anchor>#</a></span></h4><p>Multilingual LLMs represent a significant advancement in natural language processing, aiming to bridge the performance gap between high and low-resource languages. <strong>Their development is challenging due to data scarcity and linguistic diversity.</strong> Effective multilingual LLMs require massive multilingual datasets, carefully curated and processed to ensure high quality. <strong>Continual pre-training, incorporating both high and low-resource languages, is crucial</strong> for achieving robust performance across various tasks. Successful models demonstrate substantial improvements on multilingual benchmarks. <strong>Addressing bias and achieving preference alignment across different languages is also vital</strong> for creating fair and reliable systems. Future work should focus on expanding language coverage, improving efficiency, and addressing remaining challenges in low-resource scenarios.</p><h4 class="relative group">Continual Pretraining<div id=continual-pretraining class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#continual-pretraining aria-label=Anchor>#</a></span></h4><p>Continual pretraining, as discussed in the research paper, is a crucial technique for enhancing the capabilities of large language models (LLMs), particularly in multilingual contexts. The core idea is to <strong>incrementally train</strong> the model on new data without forgetting previously learned information. This approach addresses the limitations of traditional pretraining methods, which often involve training a model from scratch on a massive dataset. The paper highlights several advantages. First, it is <strong>more efficient</strong> as it avoids the computational cost of retraining the entire model. Second, it allows for <strong>continuous improvement</strong> by incorporating new data and adapting to evolving language trends. Third, it enables a <strong>more effective way to handle low-resource languages</strong>, which typically lack extensive training data. By continuously incorporating data from low-resource languages, continual pretraining helps bridge the performance gap between high- and low-resource language tasks. This process is further enhanced by sophisticated strategies such as data mixing, which carefully balances the input from various languages to avoid catastrophic forgetting. The paper demonstrates that <strong>continual pretraining with a focus on multilingual data substantially improves the LLM&rsquo;s ability</strong> to perform well on cross-lingual and multilingual benchmarks. It also leads to significant gains in machine translation.</p><h4 class="relative group">Parallel Data Impact<div id=parallel-data-impact class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#parallel-data-impact aria-label=Anchor>#</a></span></h4><p>The impact of parallel data in multilingual language model training is multifaceted. High-quality parallel data significantly enhances <strong>cross-lingual understanding and machine translation</strong>, improving performance in low-resource languages. However, the inclusion of low-quality parallel data can hinder performance, especially in larger models. This suggests a <strong>sensitivity to data quality</strong> that&rsquo;s more pronounced in larger models due to increased parameter redundancy. The optimal utilization of parallel data requires careful filtering and preprocessing to maximize its benefits. <strong>Balancing parallel data with monolingual data</strong> is also crucial, avoiding conflicts that can occur due to data inconsistency and impacting performance in various languages. Furthermore, the impact of parallel data may also interact with learning rate, leading to trade-offs in model adaptation and forgetting. Therefore, a comprehensive approach to parallel data management, which includes rigorous quality control, careful selection and optimization strategies, is essential for robust multilingual model development.</p><h4 class="relative group">Post-training Methods<div id=post-training-methods class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#post-training-methods aria-label=Anchor>#</a></span></h4><p>Post-training methods for large language models (LLMs) are crucial for enhancing their performance and addressing limitations like bias and low-resource language support. These techniques refine a pre-trained model, typically focusing on specific downstream tasks. <strong>Supervised fine-tuning (SFT)</strong> involves training the model on a labeled dataset of input-output pairs, aligning its behavior with desired outputs for particular tasks. <strong>Direct preference optimization (DPO)</strong>, another key strategy, prioritizes aligning the LLM&rsquo;s preferences with human evaluations instead of direct output matching. <strong>Reinforcement learning from human feedback (RLHF)</strong> is used to further refine model behavior by using human feedback as a reward signal, iteratively improving its responses. Effective post-training often involves combining different methods to leverage their individual strengths, resulting in more robust and capable models. The choice of post-training methods depends on the specific application, the nature of the pre-trained model, and the availability of high-quality labeled data.</p><h4 class="relative group">Future Research<div id=future-research class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#future-research aria-label=Anchor>#</a></span></h4><p>The paper&rsquo;s &lsquo;Future Research&rsquo; section would benefit from exploring several key areas. <strong>Expanding Marco-LLM&rsquo;s multilingual capabilities</strong> to encompass a broader range of languages is crucial, especially focusing on those with limited resources. This requires expanding data collection efforts. Furthermore, research into <strong>improving the model&rsquo;s efficiency and scalability</strong> is needed for wider deployment, particularly in resource-constrained settings. Investigating <strong>multilingual reasoning capabilities</strong> within the model would enhance its understanding of complex linguistic structures. Finally, addressing the <strong>performance gap between high and low-resource languages</strong>, particularly the persistent gap observed in training, requires further investigation into optimal training methodologies. This may involve exploring alternative training data or techniques to better bridge the resource gap and improve performance across the board.</p><h3 class="relative group">More visual insights<div id=more-visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#more-visual-insights aria-label=Anchor>#</a></span></h3><details><summary>More on figures</summary><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04003/extracted/6047129/Figures/Marco_Figure.png alt></figure></p><blockquote><p>üîº This figure illustrates the training and evaluation process for the Marco-LLM model. It highlights the three main stages: 1) Massive Multilingual Continual Pre-training, which involves using a large multilingual dataset for initial model training; 2) Multilingual Supervised Fine-tuning, where the model is further trained using high-quality multilingual data focusing on specific tasks; and 3) Preference Alignment, which ensures the model&rsquo;s output aligns with human preferences. The figure concludes by indicating extensive evaluation on multilingual benchmarks to validate Marco-LLM&rsquo;s effectiveness.</p><details><summary>read the caption</summary>Figure 2: An overview of the training and evaluation paradigm of our Marco-LLM, we conducted massive multilingual continual pre-training, multilingual supervised finetuning and preference alignment. We further perform extensive evaluation on multilingual benchmarks to validate the efficacy of our Marco-LLM.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04003/x1.png alt></figure></p><blockquote><p>üîº This figure shows the distribution of the training data used for Marco-LLM, categorized by language (high-resource vs. low-resource), data type (parallel data, high-quality knowledge data, synthetic data), and the overall volume in billions of tokens. The visualization allows for easy comparison of the amount of data used for various languages and data sources, giving insight into the balance of data in the Marco-LLM&rsquo;s training regimen. High-resource languages have much more data compared to the low-resource languages.</p><details><summary>read the caption</summary>Figure 3: The amount of tokens per category in our multilingual continual pretraining corpus for Marco-LLM.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04003/x2.png alt></figure></p><blockquote><p>üîº This figure displays the results of an ablation study on the impact of parallel data filtering in the continual pre-training of the Marco-LLM model. Three different sizes of the Marco-LLM were evaluated on the Flores benchmark: 1.5B, 7B, and 72B parameters. The &lsquo;Marco-w/o-parallel-data-filtering&rsquo; condition represents the results obtained when the model was trained without filtering noisy data from the parallel corpus, while the &lsquo;Marco&rsquo; condition represents the results obtained with parallel data filtering. This comparison helps to assess the effect of data quality on the model&rsquo;s performance in machine translation across different model scales.</p><details><summary>read the caption</summary>Figure 7: The performance of different model size on Flores benchmark. Marco-w/o-parallel-data-filtering denotes that we continuously pre-trained Marco-LLM based on Qwen2 without applying any filtering to parallel data.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04003/x3.png alt></figure></p><blockquote><p>üîº This figure displays the accuracy trends across various training checkpoints for different languages when evaluated using the MMMLU benchmark. The x-axis represents the training steps, and the y-axis shows the accuracy. Multiple lines are plotted, each representing a different language. The figure demonstrates that the model exhibits rapid initial learning in the first 230 steps, followed by a period of performance stabilization. Importantly, it highlights a consistent performance gap between high-resource languages (like Chinese and Italian) and low-resource languages (like Yoruba). This gap remains approximately 29% throughout the training process. This observation underscores the challenge of achieving balanced performance across diverse language resources in multilingual language models.</p><details><summary>read the caption</summary>Figure 9: Accuracy trends across training checkpoints for different languages on MMMLU. The model shows rapid initial learning (0-230 steps) followed by performance stabilization. High-resource languages (ZH-CN, IT-IT) consistently outperform low-resource ones (YO-NG), with a persistent performance gap of ¬†29%.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2412.04003/x4.png alt></figure></p><blockquote><p>üîº Figure 10 presents a comprehensive comparison of Marco-LLM&rsquo;s performance against six other state-of-the-art multilingual language models across 28 different languages. The evaluation metric used is multilingual MT-bench, a benchmark designed to assess the quality of machine-translated text in diverse languages. For each language, a bar chart displays the win rate (blue), indicating instances where Marco-LLM produced superior translations; the loss rate (green), showing cases where the other baselines surpassed Marco-LLM; and the tie rate (red), representing instances of comparable translation quality between Marco-LLM and each of the other models. This figure provides a detailed visualization of Marco-LLM&rsquo;s cross-lingual capabilities, highlighting its strengths and weaknesses across various languages, which enables a better understanding of its performance in multilingual translation.</p><details><summary>read the caption</summary>Figure 10: Performance comparison of Marco-LLM against baseline models across 28 languages on multilingual MT-bench. Each subplot shows the win rate (blue), loss rate (green), and tie rate (red) for a specific language. Win rates indicate Marco-LLM‚Äôs superior responses, loss rates represent baseline models‚Äô better performance, and tie rates show equivalent quality responses.</details></blockquote></details><details><summary>More on tables</summary><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Data</th><th>Stage-I</th><th>Stage-II</th></tr></thead><tbody><tr><td>English (en)</td><td>32%</td><td>28%</td></tr><tr><td>Chinese (zh)</td><td>17%</td><td>15%</td></tr><tr><td>High-Resourced (Others)</td><td>30%</td><td>26%</td></tr><tr><td>Low-Resourced</td><td>9%</td><td>15%</td></tr><tr><td>Parallel Multilingual Data</td><td>6%</td><td>8%</td></tr><tr><td>High-quality Knowledge Data</td><td>5%</td><td>6%</td></tr><tr><td>Synthetic Data</td><td>1%</td><td>2%</td></tr></tbody></table></table></figure><blockquote><p>üîº This table shows the proportion of different data sources used in each stage of the two-stage continual pretraining. It breaks down the percentage of English, Chinese, other high-resource languages, low-resource languages, parallel multilingual data, high-quality knowledge data, and synthetic data used in both Stage I and Stage II of the training process. This breakdown is crucial to understanding the strategy behind balancing the adaptation to new data with the preservation of knowledge learned in earlier stages.</p><details><summary>read the caption</summary>Table 2: The proportion of high-quality and multilingual sources.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Stage</th><th>Training Tokens (B)</th><th>LR</th></tr></thead><tbody><tr><td>Stage-I</td><td>160</td><td>1e-5</td></tr><tr><td>Stage-II</td><td>140</td><td>6e-6</td></tr></tbody></table></table></figure><blockquote><p>üîº This table details the hyperparameters used during the two-stage continual pre-training process for the Marco-LLM model. It shows the amount of training tokens used in each stage (Stage-I and Stage-II) and the corresponding learning rate applied. The two-stage approach helps to balance the adaptation of multilingual capabilities and prevent catastrophic forgetting.</p><details><summary>read the caption</summary>Table 3: The training corpus tokens and learning rate in two Stage Continual Pretraining.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Task</th><th>Dataset</th><th>Split</th><th>Metric</th><th>#Languages</th><th>#-shots</th></tr></thead><tbody><tr><td><strong>General</strong></td><td>CEVAL</td><td>Val</td><td>Acc.</td><td>One</td><td>5-shot</td></tr><tr><td><strong>Knowledge</strong></td><td>AGIEval</td><td>Test</td><td>Acc.</td><td>One</td><td>5-shot</td></tr><tr><td></td><td>ARC</td><td>Test</td><td>Acc.</td><td>One</td><td>25-shot</td></tr><tr><td></td><td>MMLU</td><td>Test</td><td>Acc.</td><td>One</td><td>5-shot</td></tr><tr><td><strong>Multilingual</strong></td><td>XCOPA</td><td>Val</td><td>Acc.</td><td>Six</td><td>5-shot</td></tr><tr><td><strong>Understanding</strong></td><td>X-MMLU</td><td>Val</td><td>Acc.</td><td>Thirteen</td><td>5-shot</td></tr><tr><td></td><td>XStoryCloze</td><td>Val</td><td>Acc.</td><td>Six</td><td>5-shot</td></tr><tr><td><strong>Question Answering</strong></td><td>TyDiQA</td><td>Val</td><td>F1</td><td>Six</td><td>1-shot</td></tr><tr><td></td><td>Belebele</td><td>Test</td><td>Acc.</td><td>Twenty-Eight</td><td>1-shot</td></tr><tr><td><strong>Machine Translation</strong></td><td>Flores</td><td>Devtest</td><td>BLEU</td><td>Twenty-Eight</td><td>1-shot</td></tr><tr><td></td><td>WMT-16</td><td>Test</td><td>BLEU</td><td>Three</td><td>1-shot</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents a comprehensive evaluation of the Marco-LLM model across various natural language processing tasks. It includes benchmarks from four categories: general knowledge, multilingual understanding, question answering, and machine translation. Each benchmark is evaluated using metrics appropriate to the specific task, such as accuracy, F1 score, or BLEU. The table shows the number of languages included in each benchmark and the evaluation setting (e.g., 1-shot, 5-shot), providing a complete overview of the model&rsquo;s performance across multiple languages and tasks.</p><details><summary>read the caption</summary>Table 4: Evaluation benchmarks overview. The table presents the comprehensive evaluation suite used in our experiments, spanning four major task categories: general knowledge, multilingual understanding, question answering, and machine translation. Each dataset is evaluated using either accuracy (Acc.), F1 score, or BLEU metric, covering a diverse range of languages from single-language (en, zh) to multilingual scenarios.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Model</th><th>AGIEval</th><th>Belebele</th><th>CEval</th><th>Flores</th><th>MMLU</th><th>TyDiQA</th><th>WMT16</th><th>XCOPA</th><th>XMMLU</th><th>XStoryCloze</th></tr></thead><tbody><tr><td>Qwen2-7B</td><td>64.6</td><td>73.4</td><td>83.0</td><td>27.1</td><td>71.9</td><td>52.3</td><td>18.1</td><td>70.6</td><td>60.2</td><td>70.6</td></tr><tr><td>Qwen2.5-7B</td><td>66.5</td><td>72.3</td><td>81.4</td><td>27.2</td><td>75.4</td><td>59.9</td><td>18.2</td><td>73.6</td><td>62.6</td><td>70.3</td></tr><tr><td>Llama3-8B</td><td>24.3</td><td>55.3</td><td>37.5</td><td>33.1</td><td>53.6</td><td>50.5</td><td>24.6</td><td>71.7</td><td>49.7</td><td>66.5</td></tr><tr><td>Llama3.1-8B</td><td>44.9</td><td>63.3</td><td>52.8</td><td>33.4</td><td>66.2</td><td>57.0</td><td>25.8</td><td>71.6</td><td>49.2</td><td>71.7</td></tr><tr><td>Marco-7B</td><td>68.8</td><td>78.8</td><td>83.5</td><td>35.0</td><td>74.4</td><td>60.8</td><td>29.0</td><td>76.6</td><td>61.2</td><td>71.9</td></tr><tr><td><strong>70B+ Models</strong></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Qwen2-72B</td><td>78.2</td><td>86.5</td><td>90.4</td><td>38.7</td><td>83.8</td><td>58.7</td><td>30.2</td><td>80.9</td><td>78.5</td><td>77.1</td></tr><tr><td>Qwen2.5-72B</td><td>80.8</td><td>87.6</td><td>90.6</td><td>35.0</td><td>86.3</td><td>63.7</td><td>31.0</td><td>84.7</td><td>79.9</td><td>76.3</td></tr><tr><td>Llama3-70B</td><td>60.6</td><td>85.5</td><td>66.8</td><td>37.4</td><td>79.2</td><td>64.3</td><td>34.3</td><td>81.1</td><td>72.0</td><td>76.9</td></tr><tr><td>Llama3.1-70B</td><td>61.7</td><td>86.2</td><td>67.3</td><td>36.9</td><td>78.8</td><td>62.8</td><td>35.0</td><td>83.0</td><td>71.4</td><td>75.4</td></tr><tr><td>Marco-72B</td><td>84.4</td><td>90.0</td><td>93.7</td><td>45.0</td><td>86.3</td><td>62.7</td><td>35.1</td><td>85.7</td><td>81.2</td><td>78.7</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents a comparison of the performance of various Large Language Models (LLMs) across multiple benchmark datasets. It shows results for LLMs with 7 billion parameters (7B) and LLMs with 70 billion parameters (70B) and compares their performance across different tasks, including general knowledge, multilingual understanding, question answering, and machine translation. The best performance for each benchmark is highlighted in bold, allowing for easy identification of the top-performing models in each category.</p><details><summary>read the caption</summary>Table 5: Performance comparison of LLMs across various benchmarks: Results for LLMs with parameters of both 7B and 70B. The best performance in each benchmark is in bold.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Language</th><th>Llama3-8B</th><th>Llama3.1-8B</th><th>Qwen2-7B</th><th>Qwen2.5-7B</th><th>Marco-7B</th></tr></thead><tbody><tr><td>Chinese (zh)</td><td>55.1</td><td>63.5</td><td>75.8</td><td>75.5</td><td><strong>76.0</strong></td></tr><tr><td>English (en)</td><td>69.6</td><td>74.2</td><td>77.4</td><td><strong>78.0</strong></td><td>77.9</td></tr><tr><td>Arabic (ar)</td><td>40.5</td><td>52.6</td><td>61.3</td><td>64.5</td><td><strong>66.0</strong></td></tr><tr><td>German (de)</td><td>47.3</td><td>59.8</td><td>69.5</td><td>69.0</td><td><strong>72.9</strong></td></tr><tr><td>Spanish (es)</td><td>57.0</td><td>64.9</td><td>70.4</td><td>71.9</td><td><strong>72.6</strong></td></tr><tr><td>French (fr)</td><td>56.0</td><td>63.5</td><td>69.8</td><td>71.2</td><td><strong>72.5</strong></td></tr><tr><td>Japanese (ja)</td><td>63.4</td><td>74.9</td><td>76.7</td><td>76.4</td><td><strong>77.3</strong></td></tr><tr><td>Korean (ko)</td><td>43.2</td><td>63.8</td><td>76.0</td><td><strong>79.7</strong></td><td>78.3</td></tr><tr><td>Portuguese (pt)</td><td>56.8</td><td>64.7</td><td>70.7</td><td>72.3</td><td><strong>72.3</strong></td></tr><tr><td>Turkish (tr)</td><td>51.1</td><td>62.9</td><td>66.0</td><td>66.6</td><td><strong>73.4</strong></td></tr><tr><td>Azerbaijani (az)</td><td>39.1</td><td>48.2</td><td>52.2</td><td>53.2</td><td><strong>69.4</strong></td></tr><tr><td>Bengali (bn)</td><td>45.9</td><td>49.8</td><td>63.9</td><td>62.3</td><td><strong>68.9</strong></td></tr><tr><td>Hebrew (he)</td><td>50.2</td><td>56.6</td><td>69.8</td><td>68.6</td><td><strong>77.3</strong></td></tr><tr><td>Indonesian (id)</td><td>64.0</td><td>66.1</td><td>77.3</td><td>77.8</td><td><strong>82.3</strong></td></tr><tr><td>Italian (it)</td><td>68.1</td><td>69.4</td><td>80.3</td><td>79.4</td><td><strong>83.4</strong></td></tr><tr><td>Polish (pl)</td><td>62.2</td><td>60.7</td><td>77.2</td><td>76.3</td><td><strong>80.9</strong></td></tr><tr><td>Malay (ms)</td><td>56.8</td><td>57.7</td><td>73.2</td><td>73.9</td><td><strong>80.2</strong></td></tr><tr><td>Dutch (nl)</td><td>61.0</td><td>65.2</td><td>80.2</td><td>71.6</td><td><strong>82.1</strong></td></tr><tr><td>Romanian (ro)</td><td>65.6</td><td>67.4</td><td>77.3</td><td>72.6</td><td><strong>80.8</strong></td></tr><tr><td>Russian (ru)</td><td>69.2</td><td>70.7</td><td>81.2</td><td>77.1</td><td><strong>83.2</strong></td></tr><tr><td>Thai (th)</td><td>54.8</td><td>53.3</td><td>69.1</td><td><strong>73.7</strong></td><td>72.9</td></tr><tr><td>Ukrainian (uk)</td><td>60.9</td><td>60.4</td><td>72.7</td><td>70.4</td><td><strong>79.9</strong></td></tr><tr><td>Urdu (ur)</td><td>50.0</td><td>56.7</td><td>63.9</td><td>59.9</td><td><strong>71.3</strong></td></tr><tr><td>Vietnamese (vi)</td><td>67.6</td><td>70.3</td><td>76.1</td><td>79.0</td><td><strong>81.2</strong></td></tr><tr><td>Czech (cs)</td><td>59.2</td><td>63.6</td><td>76.9</td><td>70.0</td><td><strong>78.2</strong></td></tr><tr><td>Greek (el)</td><td>68.1</td><td>67.7</td><td>65.0</td><td>67.2</td><td><strong>77.1</strong></td></tr><tr><td>Hungarian (hu)</td><td>59.8</td><td>61.0</td><td>63.3</td><td>57.9</td><td><strong>69.7</strong></td></tr><tr><td>Kazakh (kk)</td><td>41.3</td><td>44.0</td><td>43.1</td><td>45.9</td><td><strong>66.1</strong></td></tr><tr><td>Nepali (ne)</td><td>37.0</td><td>41.56</td><td>36.33</td><td>42.1</td><td><strong>65.8</strong></td></tr><tr><td>Avg. Scores</td><td>55.9</td><td>61.2</td><td>69.4</td><td>69.1</td><td><strong>75.5</strong></td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents a detailed comparison of the performance of five different 7B-parameter Large Language Models (LLMs) across 29 languages. The models are evaluated on a suite of multilingual benchmarks described earlier in the paper. For each language, the table shows the performance score achieved by each of the five LLMs. The highest score for each language is highlighted in bold, enabling easy identification of the best-performing model for each language.</p><details><summary>read the caption</summary>Table 6: Average performance on multilingual benchmarks shown in Table¬†4 for five 7B-parameter LLMs divided by 29 languages. The best performance for each language is highlighted in bold.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Language</th><th>Llama3-70B</th><th>Llama3.1-70B</th><th>Qwen2-72B</th><th>Qwen2.5-72B</th><th>Marco-72B</th></tr></thead><tbody><tr><td>Chinese (zh)</td><td>75.2</td><td>75.0</td><td>83.7</td><td>84.8</td><td><strong>86.4</strong></td></tr><tr><td>English (en)</td><td>82.7</td><td>83.2</td><td>84.6</td><td>85.1</td><td><strong>86.0</strong></td></tr><tr><td>Arabic (ar)</td><td>73.0</td><td>73.4</td><td>76.2</td><td>77.4</td><td><strong>79.7</strong></td></tr><tr><td>German (de)</td><td>80.8</td><td>80.9</td><td>84.0</td><td>85.0</td><td><strong>87.0</strong></td></tr><tr><td>Spanish (es)</td><td>80.7</td><td>79.4</td><td>82.5</td><td>83.1</td><td><strong>84.8</strong></td></tr><tr><td>French (fr)</td><td>78.9</td><td>80.7</td><td>80.4</td><td>82.7</td><td><strong>82.8</strong></td></tr><tr><td>Japanese (ja)</td><td>82.5</td><td>84.5</td><td><strong>86.6</strong></td><td>86.1</td><td>86.2</td></tr><tr><td>Korean (ko)</td><td>87.1</td><td>87.0</td><td>88.8</td><td>88.9</td><td><strong>90.6</strong></td></tr><tr><td>Portuguese (pt)</td><td>81.1</td><td>80.8</td><td>83.6</td><td>83.8</td><td><strong>85.5</strong></td></tr><tr><td>Turkish (tr)</td><td>80.8</td><td>81.3</td><td>79.0</td><td>81.5</td><td><strong>84.4</strong></td></tr><tr><td>Azerbaijani (az)</td><td>77.2</td><td>77.9</td><td>78.8</td><td>79.1</td><td><strong>85.8</strong></td></tr><tr><td>Bengali (bn)</td><td>79.7</td><td>79.7</td><td>83.2</td><td>82.9</td><td><strong>86.7</strong></td></tr><tr><td>Hebrew (he)</td><td>80.1</td><td>82.3</td><td>83.9</td><td>84.1</td><td><strong>85.1</strong></td></tr><tr><td>Indonesian (id)</td><td>87.7</td><td>88.0</td><td>87.6</td><td>88.4</td><td><strong>93.0</strong></td></tr><tr><td>Italian (it)</td><td>87.1</td><td>87.9</td><td>88.1</td><td>89.9</td><td><strong>91.0</strong></td></tr><tr><td>Polish (pl)</td><td>87.2</td><td>87.1</td><td>88.6</td><td><strong>88.8</strong></td><td>88.2</td></tr><tr><td>Malay (ms)</td><td>85.2</td><td>87.2</td><td>83.4</td><td>87.9</td><td><strong>90.7</strong></td></tr><tr><td>Dutch (nl)</td><td>88.9</td><td>88.8</td><td>89.0</td><td>90.3</td><td><strong>93.0</strong></td></tr><tr><td>Romanian (ro)</td><td>88.4</td><td>88.2</td><td>88.2</td><td>87.3</td><td><strong>90.4</strong></td></tr><tr><td>Russian (ru)</td><td>87.9</td><td>88.3</td><td>89.9</td><td><strong>91.0</strong></td><td>90.3</td></tr><tr><td>Thai (th)</td><td>80.2</td><td>80.4</td><td>85.7</td><td>87.6</td><td><strong>88.3</strong></td></tr><tr><td>Ukrainian (uk)</td><td>89.0</td><td>88.6</td><td>88.2</td><td>90.2</td><td><strong>91.7</strong></td></tr><tr><td>Urdu (ur)</td><td>80.6</td><td>81.2</td><td>81.4</td><td>82.1</td><td><strong>87.9</strong></td></tr><tr><td>Vietnamese (vi)</td><td>87.1</td><td>89.6</td><td>88.7</td><td>90.0</td><td><strong>90.8</strong></td></tr><tr><td>Czech (cs)</td><td>87.1</td><td>87.6</td><td>87.8</td><td>90.0</td><td><strong>91.4</strong></td></tr><tr><td>Greek (el)</td><td>88.3</td><td>89.6</td><td>89.4</td><td>89.0</td><td><strong>91.1</strong></td></tr><tr><td>Hungarian (hu)</td><td>83.8</td><td>83.0</td><td>80.1</td><td>88.1</td><td><strong>88.3</strong></td></tr><tr><td>Kazakh (kk)</td><td>74.7</td><td>77.9</td><td>70.3</td><td>72.2</td><td><strong>84.8</strong></td></tr><tr><td>Nepali (ne)</td><td>70.1</td><td>73.8</td><td>67.1</td><td>74.1</td><td><strong>86.7</strong></td></tr><tr><td>Avg. Scores</td><td>82.5</td><td>83.2</td><td>83.8</td><td>85.2</td><td><strong>87.9</strong></td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents a detailed comparison of the performance of five different 70B-parameter Large Language Models (LLMs) across 29 languages. The LLMs are evaluated on the multilingual benchmarks described in Table 4 of the paper. For each language, the table shows the average performance score achieved by each of the five LLMs. The highest score for each language is highlighted in bold, allowing for easy identification of the top-performing model for each language. This provides a comprehensive view of the relative strengths and weaknesses of each LLM across a range of languages, highlighting their multilingual capabilities.</p><details><summary>read the caption</summary>Table 7: Average performance on multilingual benchmarks shown in Table¬†4 for five 70B-parameter LLMs divided by 29 languages. The best performance for each language is highlighted in bold.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Model</th><th>MMMLU</th><th>TydiQA</th><th>AGIEval</th><th>CEval</th><th>Belebele</th></tr></thead><tbody><tr><td><strong>7B Models</strong></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Aya-23-8B</td><td>41.0</td><td>47.2</td><td>37.1</td><td>43.9</td><td>52.5</td></tr><tr><td>Aya-expanse-8B</td><td>48.2</td><td>28.3</td><td>36.7</td><td>48.5</td><td>64.3</td></tr><tr><td>Llama3-8B</td><td>46.6</td><td>39.7</td><td>43.4</td><td>50.8</td><td>50.7</td></tr><tr><td>Llama3.1-8B</td><td>49.2</td><td>53.0</td><td>41.8</td><td>55.6</td><td>63.9</td></tr><tr><td>Qwen2-7B</td><td>52.2</td><td>29.2</td><td>57.1</td><td>81.8</td><td>69.4</td></tr><tr><td>Qwen2.5-7B</td><td>56.0</td><td>39.0</td><td>59.0</td><td>77.9</td><td>70.0</td></tr><tr><td><strong>Marco-Chat-7B</strong></td><td><strong>60.1</strong></td><td><strong>57.7</strong></td><td><strong>61.5</strong></td><td><strong>86.4</strong></td><td><strong>79.3</strong></td></tr><tr><td><strong>70B Models</strong></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Aya-23-35B</td><td>50.1</td><td>50.2</td><td>44.4</td><td>53.6</td><td>66.3</td></tr><tr><td>Aya-expanse-32B</td><td>58.9</td><td>30.0</td><td>45.7</td><td>56.9</td><td>72.7</td></tr><tr><td>Llama3-70B</td><td>64.3</td><td>52.0</td><td>57.1</td><td>66.7</td><td>76.2</td></tr><tr><td>Llama3.1-70B</td><td>71.7</td><td>53.1</td><td>55.0</td><td>71.6</td><td>84.4</td></tr><tr><td>Qwen2-72B</td><td>69.2</td><td>40.3</td><td>66.0</td><td>90.6</td><td>85.3</td></tr><tr><td>Qwen2.5-72B</td><td>69.0</td><td>48.4</td><td>67.5</td><td>88.2</td><td>88.9</td></tr><tr><td><strong>Marco-72B</strong></td><td><strong>76.1</strong></td><td><strong>61.0</strong></td><td><strong>72.7</strong></td><td><strong>94.5</strong></td><td><strong>89.6</strong></td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents a comparative analysis of various Large Language Models (LLMs) across multiple widely used evaluation benchmarks. The benchmarks assess different aspects of LLM capabilities, such as multilingual understanding, commonsense reasoning, and knowledge. The table allows for a direct comparison of the models&rsquo; performance on each benchmark, highlighting the strengths and weaknesses of each LLM. The &lsquo;best&rsquo; performance for each benchmark is clearly identified in bold.</p><details><summary>read the caption</summary>Table 8: Performance comparison of LLMs across multiple major benchmarks. Best performance in each benchmark is marked in bold.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Language</th><th>Qwen2</th><th>Qwen2.5</th><th>Llama3</th><th>Llama3.1</th><th>Aya-23</th><th>Aya-expanse</th><th>Marco-Chat</th><th>GPT-4</th></tr></thead><tbody><tr><td><strong>7B Models</strong></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Arabic</td><td>50.9</td><td>56.6</td><td>40.5</td><td>42.2</td><td>42.1</td><td>48.8</td><td><strong>60.6</strong></td><td>62.7</td></tr><tr><td>Bengali</td><td>42.6</td><td>45.3</td><td>36.4</td><td>39.8</td><td>27.4</td><td>33.4</td><td><strong>54.4</strong></td><td>60.0</td></tr><tr><td>German</td><td>57.3</td><td>62.3</td><td>53.5</td><td>55.6</td><td>43.3</td><td>53.9</td><td><strong>65.9</strong></td><td>68.0</td></tr><tr><td>Spanish</td><td>60.3</td><td>65.3</td><td>55.8</td><td>59.1</td><td>47.9</td><td>56.1</td><td><strong>67.7</strong></td><td>68.2</td></tr><tr><td>French</td><td>61.1</td><td>65.0</td><td>55.8</td><td>58.9</td><td>46.9</td><td>55.5</td><td><strong>67.6</strong></td><td>67.5</td></tr><tr><td>Hindi</td><td>44.5</td><td>46.6</td><td>41.4</td><td>45.8</td><td>38.9</td><td>46.2</td><td><strong>54.3</strong></td><td>62.2</td></tr><tr><td>Indonesian</td><td>56.6</td><td>61.4</td><td>51.0</td><td>54.3</td><td>46.7</td><td>53.3</td><td><strong>62.3</strong></td><td>66.1</td></tr><tr><td>Italian</td><td>60.2</td><td>64.7</td><td>53.3</td><td>56.3</td><td>47.1</td><td>55.3</td><td><strong>65.4</strong></td><td>68.3</td></tr><tr><td>Japanese</td><td>56.3</td><td>61.0</td><td>42.3</td><td>52.1</td><td>45.6</td><td>51.5</td><td><strong>64.2</strong></td><td>64.4</td></tr><tr><td>Korean</td><td>54.1</td><td>59.1</td><td>46.5</td><td>50.8</td><td>43.6</td><td>50.7</td><td><strong>63.0</strong></td><td>63.6</td></tr><tr><td>Chinese</td><td>62.0</td><td>64.3</td><td>51.4</td><td>55.5</td><td>45.7</td><td>52.5</td><td><strong>66.5</strong></td><td>65.9</td></tr><tr><td>Portuguese</td><td>59.9</td><td>64.4</td><td>55.5</td><td>59.0</td><td>46.9</td><td>55.8</td><td><strong>67.6</strong></td><td>68.8</td></tr><tr><td>Swahili</td><td>34.9</td><td>35.7</td><td>37.5</td><td>40.3</td><td>26.2</td><td>32.0</td><td><strong>43.9</strong></td><td>53.1</td></tr><tr><td>Yoruba</td><td>30.5</td><td>32.8</td><td>31.0</td><td>31.4</td><td>26.4</td><td>29.9</td><td><strong>37.2</strong></td><td>38.0</td></tr><tr><td>Avg. Score</td><td>52.2</td><td>56.0</td><td>46.6</td><td>50.1</td><td>41.0</td><td>48.2</td><td><strong>60.1</strong></td><td>62.6</td></tr><tr><td><strong>70B Models</strong></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Arabic</td><td>72.0</td><td>74.3</td><td>60.6</td><td>71.1</td><td>51.8</td><td>61.6</td><td><strong>79.3</strong></td><td>71.1</td></tr><tr><td>Bengali</td><td>68.3</td><td>67.2</td><td>53.8</td><td>66.5</td><td>32.9</td><td>43.9</td><td><strong>76.6</strong></td><td>64.8</td></tr><tr><td>German</td><td>74.4</td><td>72.5</td><td>71.4</td><td>77.0</td><td>55.5</td><td>64.7</td><td><strong>80.7</strong></td><td>75.7</td></tr><tr><td>Spanish</td><td>77.0</td><td>77.5</td><td>74.3</td><td>79.3</td><td>58.0</td><td>67.5</td><td><strong>82.6</strong></td><td>76.8</td></tr><tr><td>French</td><td>75.6</td><td>76.0</td><td>73.1</td><td>77.9</td><td>58.1</td><td>67.5</td><td><strong>80.7</strong></td><td>75.8</td></tr><tr><td>Hindi</td><td>69.9</td><td>69.1</td><td>65.0</td><td>72.7</td><td>47.6</td><td>58.8</td><td><strong>76.9</strong></td><td>70.1</td></tr><tr><td>Indonesian</td><td>73.1</td><td>73.3</td><td>70.6</td><td>75.7</td><td>55.5</td><td>65.4</td><td><strong>79.0</strong></td><td>73.7</td></tr><tr><td>Italian</td><td>75.3</td><td>72.5</td><td>73.3</td><td>77.8</td><td>57.8</td><td>66.5</td><td><strong>81.6</strong></td><td>75.8</td></tr><tr><td>Japanese</td><td>74.1</td><td>74.7</td><td>65.6</td><td>73.8</td><td>54.5</td><td>64.3</td><td><strong>81.6</strong></td><td>71.6</td></tr><tr><td>Korean</td><td>72.3</td><td>71.8</td><td>64.5</td><td>72.7</td><td>53.7</td><td>62.4</td><td><strong>78.8</strong></td><td>71.3</td></tr><tr><td>Chinese</td><td>77.5</td><td>76.7</td><td>69.5</td><td>74.8</td><td>54.1</td><td>63.4</td><td><strong>82.0</strong></td><td>72.5</td></tr><tr><td>Portuguese</td><td>76.8</td><td>76.9</td><td>73.7</td><td>78.9</td><td>58.3</td><td>67.2</td><td><strong>81.7</strong></td><td>76.2</td></tr><tr><td>Swahili</td><td>47.3</td><td>48.8</td><td>51.1</td><td>64.0</td><td>33.6</td><td>38.4</td><td><strong>63.7</strong></td><td>68.1</td></tr><tr><td>Yoruba</td><td>34.6</td><td>35.5</td><td>33.6</td><td>41.2</td><td>30.4</td><td>33.4</td><td><strong>44.0</strong></td><td>47.3</td></tr><tr><td>Avg. Score</td><td>69.2</td><td>69.0</td><td>64.3</td><td>71.7</td><td>50.1</td><td>58.9</td><td><strong>76.1</strong></td><td>70.8</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents the performance comparison of various Large Language Models (LLMs) on the Massive Multitask Language Understanding (MMMLU) benchmark. It breaks down the results by language, showing the accuracy scores for models with 7 billion parameters and models with 70 billion parameters. The best performing model for each language is highlighted in bold, allowing for easy comparison of model performance across different languages and parameter sizes.</p><details><summary>read the caption</summary>Table 9: MMMLU Results: Performance of various LLMs across different languages in the MMMLU dataset for both 7B and 70B models. The best performance in each language is highlighted in bold.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Language</th><th>Qwen2</th><th>Qwen2.5</th><th>Llama-3</th><th>Llama3.1</th><th>Aya-23</th><th>Aya-expanse</th><th>Marco-Chat</th></tr></thead><tbody><tr><td><strong>7B Models</strong></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Azerbaijani</td><td>59.9</td><td>60.4</td><td>41.3</td><td>57.7</td><td>34.1</td><td>49.9</td><td><strong>72.3</strong></td></tr><tr><td>Bengali</td><td>64.9</td><td>64.2</td><td>46.8</td><td>57.2</td><td>28.7</td><td>41.6</td><td><strong>75.1</strong></td></tr><tr><td>Czech</td><td>75.4</td><td>75.9</td><td>54.3</td><td>73.4</td><td>61.6</td><td>76.9</td><td><strong>84.4</strong></td></tr><tr><td>Greek</td><td>68.7</td><td>75.2</td><td>59.0</td><td>74.3</td><td>65.2</td><td>80.3</td><td><strong>81.4</strong></td></tr><tr><td>Hebrew</td><td>74.2</td><td>72.7</td><td>45.9</td><td>59.3</td><td>61.7</td><td>77.9</td><td><strong>82.1</strong></td></tr><tr><td>Hungarian</td><td>57.3</td><td>63.0</td><td>45.1</td><td>52.4</td><td>35.0</td><td>44.0</td><td><strong>68.0</strong></td></tr><tr><td>Indonesian</td><td>77.2</td><td>78.4</td><td>61.9</td><td>75.0</td><td>64.7</td><td>77.6</td><td><strong>82.8</strong></td></tr><tr><td>Italian</td><td>78.2</td><td>81.9</td><td>60.6</td><td>71.8</td><td>67.0</td><td>75.7</td><td><strong>86.8</strong></td></tr><tr><td>Japanese</td><td>78.0</td><td>69.8</td><td>49.9</td><td>64.7</td><td>64.2</td><td>74.1</td><td><strong>83.1</strong></td></tr><tr><td>Kazakh</td><td>48.0</td><td>51.2</td><td>36.0</td><td>49.1</td><td>28.3</td><td>38.0</td><td><strong>73.1</strong></td></tr><tr><td>Malay</td><td>78.9</td><td>77.1</td><td>57.9</td><td>67.4</td><td>53.2</td><td>73.3</td><td><strong>83.4</strong></td></tr><tr><td>Dutch</td><td>58.7</td><td>74.6</td><td>56.3</td><td>70.1</td><td>66.2</td><td>72.1</td><td><strong>85.3</strong></td></tr><tr><td>Nepali</td><td>44.3</td><td>49.4</td><td>38.9</td><td>46.7</td><td>32.9</td><td>40.8</td><td><strong>70.0</strong></td></tr><tr><td>Polish</td><td>78.4</td><td>70.3</td><td>55.0</td><td>65.0</td><td>61.1</td><td>73.9</td><td><strong>73.3</strong></td></tr><tr><td>Romanian</td><td>72.4</td><td>74.0</td><td>55.2</td><td>68.7</td><td>65.9</td><td><strong>74.8</strong></td><td>73.2</td></tr><tr><td>Russian</td><td>79.7</td><td>73.3</td><td>52.7</td><td>70.2</td><td>64.1</td><td>74.3</td><td><strong>87.9</strong></td></tr><tr><td>Ukrainian</td><td>76.1</td><td>73.2</td><td>51.1</td><td>69.8</td><td>61.8</td><td>76.2</td><td><strong>83.4</strong></td></tr><tr><td>Urdu</td><td>64.9</td><td>64.4</td><td>49.0</td><td>59.2</td><td>35.6</td><td>50.2</td><td><strong>76.2</strong></td></tr><tr><td>Thai</td><td>72.3</td><td>74.5</td><td>43.0</td><td>57.2</td><td>37.6</td><td>41.8</td><td><strong>76.9</strong></td></tr><tr><td>Vietnamese</td><td>80.0</td><td>77.0</td><td>53.8</td><td>68.3</td><td>61.4</td><td>73.2</td><td><strong>86.3</strong></td></tr><tr><td><strong>Avg. Scores</strong></td><td>69.4</td><td>70.0</td><td>50.7</td><td>63.9</td><td>52.5</td><td>64.3</td><td><strong>79.3</strong></td></tr><tr><td><strong>70B Models</strong></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Azerbaijani</td><td>79.9</td><td>81.6</td><td>63.3</td><td>79.7</td><td>51.9</td><td>58.3</td><td><strong>85.6</strong></td></tr><tr><td>Bengali</td><td>84.9</td><td>87.3</td><td>75.3</td><td>81.4</td><td>42.1</td><td>64.8</td><td><strong>89.2</strong></td></tr><tr><td>Czech</td><td>89.7</td><td>91.9</td><td>79.0</td><td>86.8</td><td>79.6</td><td>85.8</td><td><strong>91.8</strong></td></tr><tr><td>Greek</td><td>89.2</td><td><strong>92.6</strong></td><td>87.0</td><td>89.4</td><td>80.1</td><td>83.6</td><td>91.9</td></tr><tr><td>Hebrew</td><td>85.0</td><td>86.9</td><td>75.9</td><td>78.9</td><td>77.0</td><td>83.1</td><td><strong>86.0</strong></td></tr><tr><td>Hungarian</td><td>72.8</td><td><strong>89.3</strong></td><td>58.0</td><td>74.1</td><td>53.6</td><td>50.8</td><td>87.0</td></tr><tr><td>Indonesian</td><td>88.9</td><td>91.7</td><td>82.6</td><td>87.3</td><td>77.8</td><td>81.1</td><td><strong>93.1</strong></td></tr><tr><td>Italian</td><td>89.8</td><td>90.4</td><td>83.8</td><td>87.9</td><td>81.1</td><td>77.7</td><td><strong>91.1</strong></td></tr><tr><td>Japanese</td><td>87.8</td><td>90.2</td><td>82.2</td><td>86.9</td><td>73.7</td><td>78.2</td><td><strong>90.1</strong></td></tr><tr><td>Kazakh</td><td>73.6</td><td>76.0</td><td>54.1</td><td>78.2</td><td>40.7</td><td>55.1</td><td><strong>81.7</strong></td></tr><tr><td>Malay</td><td>88.7</td><td>91.2</td><td>87.7</td><td>88.7</td><td>74.8</td><td>76.4</td><td><strong>92.1</strong></td></tr><tr><td>Dutch</td><td>90.9</td><td>93.2</td><td>80.4</td><td>87.1</td><td>80.8</td><td>85.7</td><td><strong>94.4</strong></td></tr><tr><td>Nepali</td><td>70.6</td><td>80.1</td><td>55.7</td><td>76.0</td><td>39.9</td><td>50.1</td><td><strong>84.4</strong></td></tr><tr><td>Polish</td><td>86.4</td><td>89.7</td><td>75.0</td><td>88.7</td><td>73.9</td><td>83.7</td><td><strong>90.6</strong></td></tr><tr><td>Romanian</td><td>88.4</td><td>92.1</td><td>73.4</td><td>86.3</td><td>75.7</td><td>77.7</td><td><strong>90.6</strong></td></tr><tr><td>Russian</td><td>90.0</td><td>94.1</td><td>86.1</td><td>87.2</td><td>73.2</td><td>84.6</td><td><strong>92.7</strong></td></tr><tr><td>Ukrainian</td><td>90.9</td><td><strong>93.4</strong></td><td>84.1</td><td>90.2</td><td>74.3</td><td>79.8</td><td>93.0</td></tr><tr><td>Urdu</td><td>83.1</td><td>86.4</td><td>78.9</td><td><strong>90.2</strong></td><td>47.2</td><td>63.7</td><td>88.2</td></tr><tr><td>Thai</td><td>86.2</td><td>87.1</td><td>79.7</td><td>82.7</td><td>53.6</td><td>63.8</td><td><strong>87.6</strong></td></tr><tr><td>Vietnamese</td><td>88.8</td><td>92.0</td><td>81.7</td><td>83.9</td><td>75.8</td><td>69.2</td><td><strong>92.2</strong></td></tr><tr><td><strong>Avg. Scores</strong></td><td>85.3</td><td>88.9</td><td>76.2</td><td>84.4</td><td>66.3</td><td>72.7</td><td><strong>89.6</strong></td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents a comprehensive comparison of various large language models (LLMs) on the Belebele benchmark. The Belebele benchmark is specifically designed to evaluate the performance of LLMs on reading comprehension tasks across a wide variety of languages, including many low-resource languages. The table displays the performance of each model (Qwen2, Qwen2.5, Llama-3, Llama3.1, Aya-23, Aya-expanse, Marco-Chat) for both 7B and 70B parameter models across multiple languages. The performance metric used is accuracy. This allows for a direct comparison of the models&rsquo; abilities to understand and answer reading comprehension questions across different linguistic backgrounds. Results are presented separately for 7B and 70B parameter models, facilitating analysis of the impact of model size on performance across languages.</p><details><summary>read the caption</summary>Table 10: Performance comparison of language models on Belebele benchmark¬†[Bandarkar et¬†al., 2024] across different languages.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th style=text-align:left></th><th style=text-align:left>GPT4</th><th style=text-align:left>DeepL</th><th style=text-align:left>Google</th><th style=text-align:left>Aya-32B</th><th style=text-align:left>Aya-35B</th><th style=text-align:left>Qwen2-72B</th><th style=text-align:left>Qwen2.5-72B</th><th style=text-align:left>Llama3-70B</th><th style=text-align:left>Llama3.1-70B</th><th style=text-align:left>Marco-7B</th><th style=text-align:left>Marco-72B</th></tr></thead><tbody><tr><td style=text-align:left>En‚ÜíXX</td><td style=text-align:left></td><td style=text-align:left></td><td style=text-align:left></td><td style=text-align:left></td><td style=text-align:left></td><td style=text-align:left></td><td style=text-align:left></td><td style=text-align:left></td><td style=text-align:left></td><td style=text-align:left></td><td style=text-align:left></td></tr><tr><td style=text-align:left>En2Ar</td><td style=text-align:left>40.4</td><td style=text-align:left>48.1</td><td style=text-align:left>50.0</td><td style=text-align:left>31.5</td><td style=text-align:left>24.4</td><td style=text-align:left>17.1</td><td style=text-align:left>29.9</td><td style=text-align:left>29.2</td><td style=text-align:left>33.5</td><td style=text-align:left>41.5</td><td style=text-align:left><strong>61.2</strong></td></tr><tr><td style=text-align:left>En2De</td><td style=text-align:left>45.9</td><td style=text-align:left>48.7</td><td style=text-align:left><strong>49.3</strong></td><td style=text-align:left>32.3</td><td style=text-align:left>33.1</td><td style=text-align:left>37.7</td><td style=text-align:left>40.4</td><td style=text-align:left>43.0</td><td style=text-align:left>44.7</td><td style=text-align:left>41.6</td><td style=text-align:left>47.7</td></tr><tr><td style=text-align:left>En2Es</td><td style=text-align:left>33.1</td><td style=text-align:left>32.9</td><td style=text-align:left>34.6</td><td style=text-align:left>28.4</td><td style=text-align:left>20.6</td><td style=text-align:left>32.0</td><td style=text-align:left>31.7</td><td style=text-align:left>31.5</td><td style=text-align:left>32.5</td><td style=text-align:left>33.1</td><td style=text-align:left><strong>37.2</strong></td></tr><tr><td style=text-align:left>En2Fr</td><td style=text-align:left>54.4</td><td style=text-align:left>59.1</td><td style=text-align:left>57.9</td><td style=text-align:left>50.5</td><td style=text-align:left>34.5</td><td style=text-align:left>52.3</td><td style=text-align:left>53.2</td><td style=text-align:left>52.6</td><td style=text-align:left>55.2</td><td style=text-align:left>54.9</td><td style=text-align:left><strong>58.8</strong></td></tr><tr><td style=text-align:left>En2It</td><td style=text-align:left>37.2</td><td style=text-align:left>41.5</td><td style=text-align:left>39.1</td><td style=text-align:left>32.4</td><td style=text-align:left>25.3</td><td style=text-align:left>34.2</td><td style=text-align:left>34.8</td><td style=text-align:left>34.8</td><td style=text-align:left>36.6</td><td style=text-align:left>36.2</td><td style=text-align:left><strong>40.6</strong></td></tr><tr><td style=text-align:left>En2Ja</td><td style=text-align:left>34.6</td><td style=text-align:left>36.8</td><td style=text-align:left><strong>41.1</strong></td><td style=text-align:left>31.0</td><td style=text-align:left>9.6</td><td style=text-align:left>29.6</td><td style=text-align:left>33.0</td><td style=text-align:left>14.3</td><td style=text-align:left>33.0</td><td style=text-align:left>36.9</td><td style=text-align:left>37.7</td></tr><tr><td style=text-align:left>En2Ko</td><td style=text-align:left>28.5</td><td style=text-align:left>32.9</td><td style=text-align:left><strong>33.7</strong></td><td style=text-align:left>27.2</td><td style=text-align:left>12.3</td><td style=text-align:left>19.2</td><td style=text-align:left>24.8</td><td style=text-align:left>0.1</td><td style=text-align:left>27.7</td><td style=text-align:left>29.0</td><td style=text-align:left>31.6</td></tr><tr><td style=text-align:left>En2Nl</td><td style=text-align:left>34.8</td><td style=text-align:left>37.0</td><td style=text-align:left><strong>36.3</strong></td><td style=text-align:left>25.4</td><td style=text-align:left>24.2</td><td style=text-align:left>28.4</td><td style=text-align:left>30.9</td><td style=text-align:left>32.0</td><td style=text-align:left>34.7</td><td style=text-align:left>33.0</td><td style=text-align:left>35.9</td></tr><tr><td style=text-align:left>En2Pl</td><td style=text-align:left>30.3</td><td style=text-align:left>33.4</td><td style=text-align:left><strong>33.7</strong></td><td style=text-align:left>24.8</td><td style=text-align:left>16.5</td><td style=text-align:left>20.4</td><td style=text-align:left>26.0</td><td style=text-align:left>28.6</td><td style=text-align:left>29.5</td><td style=text-align:left>28.3</td><td style=text-align:left>30.6</td></tr><tr><td style=text-align:left>En2Pt</td><td style=text-align:left>54.8</td><td style=text-align:left>45.7</td><td style=text-align:left>56.0</td><td style=text-align:left>44.0</td><td style=text-align:left>41.7</td><td style=text-align:left>50.1</td><td style=text-align:left>52.6</td><td style=text-align:left>52.0</td><td style=text-align:left>54.8</td><td style=text-align:left>54.5</td><td style=text-align:left><strong>57.9</strong></td></tr><tr><td style=text-align:left>En2Ru</td><td style=text-align:left>36.8</td><td style=text-align:left>40.5</td><td style=text-align:left>40.9</td><td style=text-align:left>32.3</td><td style=text-align:left>23.8</td><td style=text-align:left>33.6</td><td style=text-align:left>36.1</td><td style=text-align:left>35.6</td><td style=text-align:left>37.9</td><td style=text-align:left>37.7</td><td style=text-align:left><strong>43.4</strong></td></tr><tr><td style=text-align:left>En2Tr</td><td style=text-align:left>36.9</td><td style=text-align:left><strong>45.0</strong></td><td style=text-align:left>44.2</td><td style=text-align:left>33.1</td><td style=text-align:left>26.4</td><td style=text-align:left>22.8</td><td style=text-align:left>30.4</td><td style=text-align:left>32.7</td><td style=text-align:left>36.8</td><td style=text-align:left>35.8</td><td style=text-align:left>37.7</td></tr><tr><td style=text-align:left>En2Uk</td><td style=text-align:left>37.0</td><td style=text-align:left>42.9</td><td style=text-align:left>41.6</td><td style=text-align:left>33.5</td><td style=text-align:left>17.0</td><td style=text-align:left>25.2</td><td style=text-align:left>30.0</td><td style=text-align:left>36.5</td><td style=text-align:left>36.8</td><td style=text-align:left>36.3</td><td style=text-align:left><strong>44.3</strong></td></tr><tr><td style=text-align:left>En2Zh</td><td style=text-align:left>44.2</td><td style=text-align:left>48.6</td><td style=text-align:left><strong>50.6</strong></td><td style=text-align:left>26.0</td><td style=text-align:left>15.6</td><td style=text-align:left>28.0</td><td style=text-align:left>33.9</td><td style=text-align:left>13.3</td><td style=text-align:left>31.3</td><td style=text-align:left>45.3</td><td style=text-align:left>48.6</td></tr><tr><td style=text-align:left>Avg. Scores</td><td style=text-align:left>39.2</td><td style=text-align:left>42.4</td><td style=text-align:left>43.5</td><td style=text-align:left>32.3</td><td style=text-align:left>23.2</td><td style=text-align:left>30.8</td><td style=text-align:left>34.8</td><td style=text-align:left>31.2</td><td style=text-align:left>37.5</td><td style=text-align:left>38.9</td><td style=text-align:left><strong>43.8</strong></td></tr><tr><td style=text-align:left>XX‚ÜíEn</td><td style=text-align:left></td><td style=text-align:left></td><td style=text-align:left></td><td style=text-align:left></td><td style=text-align:left></td><td style=text-align:left></td><td style=text-align:left></td><td style=text-align:left></td><td style=text-align:left></td><td style=text-align:left></td><td style=text-align:left></td></tr><tr><td style=text-align:left>Ar2En</td><td style=text-align:left>42.7</td><td style=text-align:left>47.7</td><td style=text-align:left>46.8</td><td style=text-align:left>33.3</td><td style=text-align:left>41.1</td><td style=text-align:left>41.5</td><td style=text-align:left>44.6</td><td style=text-align:left>37.1</td><td style=text-align:left>46.1</td><td style=text-align:left>48.0</td><td style=text-align:left><strong>58.0</strong></td></tr><tr><td style=text-align:left>De2En</td><td style=text-align:left>47.7</td><td style=text-align:left>51.0</td><td style=text-align:left>51.3</td><td style=text-align:left>30.8</td><td style=text-align:left>40.9</td><td style=text-align:left>46.9</td><td style=text-align:left>48.4</td><td style=text-align:left>46.3</td><td style=text-align:left>49.4</td><td style=text-align:left>50.6</td><td style=text-align:left><strong>54.7</strong></td></tr><tr><td style=text-align:left>Es2En</td><td style=text-align:left>34.3</td><td style=text-align:left>36.9</td><td style=text-align:left>36.3</td><td style=text-align:left>24.8</td><td style=text-align:left>33.8</td><td style=text-align:left>34.5</td><td style=text-align:left>35.3</td><td style=text-align:left>33.7</td><td style=text-align:left>35.0</td><td style=text-align:left>40.2</td><td style=text-align:left><strong>47.2</strong></td></tr><tr><td style=text-align:left>Fr2En</td><td style=text-align:left>48.9</td><td style=text-align:left>50.8</td><td style=text-align:left>52.7</td><td style=text-align:left>27.7</td><td style=text-align:left>45.1</td><td style=text-align:left>48.8</td><td style=text-align:left>49.5</td><td style=text-align:left>47.5</td><td style=text-align:left>50.7</td><td style=text-align:left>51.5</td><td style=text-align:left><strong>56.8</strong></td></tr><tr><td style=text-align:left>It2En</td><td style=text-align:left>36.7</td><td style=text-align:left>40.2</td><td style=text-align:left>40.2</td><td style=text-align:left>28.6</td><td style=text-align:left>37.5</td><td style=text-align:left>36.7</td><td style=text-align:left>38.2</td><td style=text-align:left>36.5</td><td style=text-align:left>38.4</td><td style=text-align:left>42.9</td><td style=text-align:left><strong>49.2</strong></td></tr><tr><td style=text-align:left>Ja2En</td><td style=text-align:left>30.4</td><td style=text-align:left>37.0</td><td style=text-align:left>36.7</td><td style=text-align:left>20.5</td><td style=text-align:left>22.6</td><td style=text-align:left>29.8</td><td style=text-align:left>31.9</td><td style=text-align:left>26.2</td><td style=text-align:left>32.3</td><td style=text-align:left>36.3</td><td style=text-align:left><strong>49.5</strong></td></tr><tr><td style=text-align:left>Ko2En</td><td style=text-align:left>33.3</td><td style=text-align:left>39.3</td><td style=text-align:left>38.2</td><td style=text-align:left>21.9</td><td style=text-align:left>25.9</td><td style=text-align:left>32.1</td><td style=text-align:left>34.5</td><td style=text-align:left>28.7</td><td style=text-align:left>33.8</td><td style=text-align:left>37.0</td><td style=text-align:left><strong>49.0</strong></td></tr><tr><td style=text-align:left>Nl2En</td><td style=text-align:left>36.0</td><td style=text-align:left>37.7</td><td style=text-align:left>38.7</td><td style=text-align:left>23.3</td><td style=text-align:left>32.8</td><td style=text-align:left>35.9</td><td style=text-align:left>36.3</td><td style=text-align:left>34.8</td><td style=text-align:left>37.0</td><td style=text-align:left>39.8</td><td style=text-align:left><strong>46.4</strong></td></tr><tr><td style=text-align:left>Pl2En</td><td style=text-align:left>33.5</td><td style=text-align:left>35.8</td><td style=text-align:left>37.0</td><td style=text-align:left>19.6</td><td style=text-align:left>27.6</td><td style=text-align:left>33.7</td><td style=text-align:left>34.7</td><td style=text-align:left>32.1</td><td style=text-align:left>35.3</td><td style=text-align:left>38.4</td><td style=text-align:left><strong>45.9</strong></td></tr><tr><td style=text-align:left>Pt2En</td><td style=text-align:left>53.1</td><td style=text-align:left>55.8</td><td style=text-align:left>56.3</td><td style=text-align:left>37.9</td><td style=text-align:left>50.7</td><td style=text-align:left>53.0</td><td style=text-align:left>53.5</td><td style=text-align:left>51.8</td><td style=text-align:left>54.9</td><td style=text-align:left>54.5</td><td style=text-align:left><strong>60.3</strong></td></tr><tr><td style=text-align:left>Ru2En</td><td style=text-align:left>38.7</td><td style=text-align:left>43.3</td><td style=text-align:left>42.9</td><td style=text-align:left>23.6</td><td style=text-align:left>36.2</td><td style=text-align:left>39.0</td><td style=text-align:left>40.2</td><td style=text-align:left>37.9</td><td style=text-align:left>41.0</td><td style=text-align:left>43.8</td><td style=text-align:left><strong>49.2</strong></td></tr><tr><td style=text-align:left>Tr2En</td><td style=text-align:left>42.6</td><td style=text-align:left>48.5</td><td style=text-align:left>47.7</td><td style=text-align:left>31.1</td><td style=text-align:left>36.5</td><td style=text-align:left>39.9</td><td style=text-align:left>42.3</td><td style=text-align:left>37.9</td><td style=text-align:left>43.1</td><td style=text-align:left>43.4</td><td style=text-align:left><strong>52.0</strong></td></tr><tr><td style=text-align:left>Uk2En</td><td style=text-align:left>43.4</td><td style=text-align:left>47.2</td><td style=text-align:left>47.3</td><td style=text-align:left>27.4</td><td style=text-align:left>38.8</td><td style=text-align:left>41.6</td><td style=text-align:left>43.7</td><td style=text-align:left>40.5</td><td style=text-align:left>44.9</td><td style=text-align:left>46.3</td><td style=text-align:left><strong>58.8</strong></td></tr><tr><td style=text-align:left>Zh2En</td><td style=text-align:left>31.3</td><td style=text-align:left>36.8</td><td style=text-align:left>37.7</td><td style=text-align:left>24.3</td><td style=text-align:left>26.7</td><td style=text-align:left>31.0</td><td style=text-align:left>35.4</td><td style=text-align:left>29.0</td><td style=text-align:left>34.7</td><td style=text-align:left>38.2</td><td style=text-align:left><strong>45.5</strong></td></tr><tr><td style=text-align:left>Avg. Scores</td><td style=text-align:left>36.8</td><td style=text-align:left>43.4</td><td style=text-align:left>43.6</td><td style=text-align:left>26.8</td><td style=text-align:left>35.4</td><td style=text-align:left>38.9</td><td style=text-align:left>40.6</td><td style=text-align:left>37.2</td><td style=text-align:left>41.2</td><td style=text-align:left>43.6</td><td style=text-align:left><strong>51.6</strong></td></tr></tbody></table></table></figure><blockquote><p>üîº Table 11 presents a comprehensive comparison of the performance of various Large Language Models (LLMs) and Machine Translation (MT) systems on the Flores benchmark. Flores is a multilingual machine translation benchmark that focuses on low-resource languages. The table shows the BLEU scores, a metric measuring the quality of machine translation, for each model across multiple language pairs. These pairs are categorized by source language (English or other) and target language, allowing for an assessment of translation accuracy in various scenarios. The best-performing model for each language pair is highlighted in bold, allowing for easy identification of superior performance. This table helps to demonstrate the relative strengths and weaknesses of different models, especially regarding their ability to handle the diverse linguistic characteristics presented in low-resource languages.</p><details><summary>read the caption</summary>Table 11: Performance comparison of various LLMs and MT systems on the Flores benchmark. The best performance in each column is highlighted in bold.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Trans. Dir.</th><th>Qwen2-7B</th><th>Qwen2.5-7B</th><th>Llama3-8B</th><th>Llama3.1-8B</th><th>Aya-expanse-8B</th><th>Aya-23-8B</th><th>Marco-7B</th></tr></thead><tbody><tr><td>Ar2Ja</td><td>16.2</td><td>14.3</td><td>13.1</td><td>17.5</td><td>16.9</td><td>17.1</td><td><strong>21.8</strong></td></tr><tr><td>Es2Ja</td><td>17.2</td><td>10.7</td><td>11.2</td><td>18.1</td><td>18.3</td><td>19.9</td><td><strong>22.4</strong></td></tr><tr><td>Fr2Ja</td><td>19.9</td><td>14.0</td><td>14.1</td><td>21.6</td><td>17.6</td><td>22.3</td><td><strong>25.8</strong></td></tr><tr><td>Hu2Ja</td><td>15.2</td><td>9.6</td><td>12.3</td><td>16.0</td><td>10.7</td><td>12.6</td><td><strong>20.3</strong></td></tr><tr><td>Hu2Ko</td><td>10.5</td><td>6.9</td><td>9.4</td><td>10.2</td><td>9.2</td><td>11.0</td><td><strong>14.0</strong></td></tr><tr><td>Ja2Ar</td><td>9.8</td><td>7.4</td><td>8.6</td><td>11.1</td><td>12.3</td><td>9.6</td><td><strong>15.5</strong></td></tr><tr><td>Ja2Es</td><td>16.1</td><td>15.0</td><td>15.9</td><td>16.7</td><td>12.9</td><td>16.2</td><td><strong>19.1</strong></td></tr><tr><td>Ja2Ko</td><td>16.3</td><td>12.1</td><td>12.2</td><td>17.3</td><td>18.7</td><td>17.2</td><td><strong>22.6</strong></td></tr><tr><td>Ja2Th</td><td>11.7</td><td>11.4</td><td>11.1</td><td>11.6</td><td>0.8</td><td>2.0</td><td><strong>16.4</strong></td></tr><tr><td>Ja2Zh</td><td>19.5</td><td>17.6</td><td>6.9</td><td>10.2</td><td>14.9</td><td>12.7</td><td><strong>22.9</strong></td></tr><tr><td>Kk2Ar</td><td>7.3</td><td>5.5</td><td>7.1</td><td>8.6</td><td>2.3</td><td>5.7</td><td><strong>11.6</strong></td></tr><tr><td>Kk2Fr</td><td>13.8</td><td>8.9</td><td>17.9</td><td>14.1</td><td>5.6</td><td>10.6</td><td><strong>20.5</strong></td></tr><tr><td>Kk2Ja</td><td>11.7</td><td>6.0</td><td>10.4</td><td>12.2</td><td>4.1</td><td>9.6</td><td><strong>16.8</strong></td></tr><tr><td>Kk2Ko</td><td>8.3</td><td>4.7</td><td>9.6</td><td>9.3</td><td>4.5</td><td>7.5</td><td><strong>13.1</strong></td></tr><tr><td>Kk2Pt</td><td>12.7</td><td>8.8</td><td>15.2</td><td>10.2</td><td>4.2</td><td>9.7</td><td><strong>16.9</strong></td></tr><tr><td>Kk2Th</td><td>6.7</td><td>7.1</td><td>8.9</td><td>10.4</td><td>0.4</td><td>1.2</td><td><strong>12.7</strong></td></tr><tr><td>Kk2Zh</td><td>11.9</td><td>10.8</td><td>10.2</td><td>13.1</td><td>3.0</td><td>7.6</td><td><strong>18.5</strong></td></tr><tr><td>Ko2Ja</td><td>22.4</td><td>21.2</td><td>17.5</td><td>22.3</td><td>23.9</td><td>19.3</td><td><strong>26.2</strong></td></tr><tr><td>Ko2Th</td><td>11.9</td><td>9.7</td><td>11.2</td><td>12.2</td><td>0.8</td><td>2.0</td><td><strong>14.7</strong></td></tr><tr><td>Ko2Zh</td><td>20.0</td><td>20.3</td><td>10.2</td><td>16.2</td><td>16.6</td><td>14.7</td><td><strong>22.7</strong></td></tr><tr><td>Th2Ar</td><td>11.3</td><td>9.1</td><td>8.7</td><td>5.5</td><td>1.7</td><td>6.8</td><td><strong>15.4</strong></td></tr><tr><td>Th2Es</td><td>16.4</td><td>15.1</td><td>16.4</td><td>9.1</td><td>1.5</td><td>10.0</td><td><strong>18.6</strong></td></tr><tr><td>Th2Fr</td><td>22.2</td><td>20.3</td><td>19.3</td><td>12.4</td><td>5.1</td><td>14.2</td><td><strong>25.2</strong></td></tr><tr><td>Th2Ja</td><td>16.5</td><td>15.4</td><td>12.6</td><td>14.5</td><td>0.0</td><td>5.5</td><td><strong>22.7</strong></td></tr><tr><td>Th2Kk</td><td>2.2</td><td>1.7</td><td>4.4</td><td>5.4</td><td>0.3</td><td>0.8</td><td><strong>9.3</strong></td></tr><tr><td>Th2Ko</td><td>12.2</td><td>9.4</td><td>8.3</td><td>7.6</td><td>0.8</td><td>5.1</td><td><strong>16.5</strong></td></tr><tr><td>Th2Zh</td><td>18.8</td><td>18.0</td><td>9.5</td><td>9.2</td><td>0.0</td><td>6.4</td><td><strong>22.3</strong></td></tr><tr><td>Tr2Ja</td><td>17.7</td><td>7.3</td><td>11.9</td><td>20.0</td><td>19.6</td><td>21.4</td><td><strong>23.2</strong></td></tr><tr><td>Uk2Fr</td><td>27.5</td><td>24.3</td><td>31.2</td><td>33.0</td><td>31.5</td><td>33.0</td><td><strong>34.6</strong></td></tr><tr><td>Uk2Ja</td><td>17.3</td><td>13.0</td><td>14.1</td><td>20.1</td><td>16.9</td><td>21.8</td><td><strong>26.6</strong></td></tr><tr><td>Uk2Kk</td><td>3.4</td><td>2.7</td><td>7.9</td><td>8.3</td><td>0.7</td><td>1.8</td><td><strong>12.4</strong></td></tr><tr><td>Uk2Ko</td><td>13.2</td><td>8.9</td><td>10.7</td><td>15.8</td><td>16.0</td><td>17.3</td><td><strong>18.9</strong></td></tr><tr><td>Uk2Th</td><td>12.4</td><td>11.2</td><td>13.7</td><td>15.2</td><td>1.1</td><td>2.9</td><td><strong>19.9</strong></td></tr><tr><td>Uk2Zh</td><td>20.7</td><td>18.4</td><td>11.6</td><td>9.8</td><td>16.1</td><td>17.8</td><td><strong>25.6</strong></td></tr><tr><td>Ur2Ar</td><td>8.0</td><td>7.4</td><td>5.2</td><td>4.8</td><td>3.4</td><td>6.4</td><td><strong>10.7</strong></td></tr><tr><td>Ur2Ko</td><td>8.7</td><td>6.8</td><td>8.5</td><td>9.3</td><td>4.5</td><td>9.0</td><td><strong>12.8</strong></td></tr><tr><td>Zh2Ar</td><td>11.9</td><td>9.8</td><td>10.6</td><td>13.5</td><td>14.9</td><td>13.6</td><td><strong>17.0</strong></td></tr><tr><td>Zh2Fr</td><td>24.5</td><td>21.7</td><td>21.8</td><td>25.7</td><td>24.5</td><td>21.3</td><td><strong>28.9</strong></td></tr><tr><td>Zh2Ja</td><td>19.2</td><td>15.0</td><td>10.9</td><td>18.4</td><td>14.4</td><td>17.3</td><td><strong>27.7</strong></td></tr><tr><td>Zh2Ko</td><td>14.2</td><td>10.4</td><td>9.7</td><td>14.8</td><td>15.7</td><td>15.4</td><td><strong>21.2</strong></td></tr><tr><td>Zh2Pt</td><td>22.6</td><td>20.9</td><td>19.5</td><td>20.5</td><td>21.0</td><td>21.5</td><td><strong>25.4</strong></td></tr><tr><td>Zh2Th</td><td>13.3</td><td>10.8</td><td>12.8</td><td>13.9</td><td>1.0</td><td>2.5</td><td><strong>19.8</strong></td></tr><tr><td>Avg. Score</td><td>14.6</td><td>11.9</td><td>12.2</td><td>13.9</td><td>9.7</td><td>11.9</td><td><strong>19.7</strong></td></tr></tbody></table></table></figure><blockquote><p>üîº Table 12 presents a detailed comparison of Any2Any (any-to-any) machine translation performance across various language pairs, using several large language models (LLMs). The LLMs compared are instruction-tuned versions of Qwen2, Qwen2.5, Llama 3, and Llama 3.1, alongside the authors&rsquo; Marco-LLM. The table shows BLEU scores for each language pair translation, indicating the quality of the translation produced by each model. The best performing model for each language pair is highlighted in bold, enabling easy identification of superior performance in specific translation scenarios.</p><details><summary>read the caption</summary>Table 12: Any2Any translation performance on Flores benchmark across various language pairs for LLMs. The table compares results from the Instruct models of Qwen2, Qwen2.5, Llama3, Llama3.1, and Marco-LLM, with the best performance highlighted in bold for each translation direction.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>ID</th><th>Template (in English)</th></tr></thead><tbody><tr><td>A</td><td><code>&lt;src_lang></code> phrase: <code>&lt;input></code> ‚óá <code>&lt;tgt_lang></code> phrase: <code>&lt;output></code></td></tr><tr><td>B</td><td><code>&lt;src_lang></code> text: <code>&lt;input></code> ‚óá <code>&lt;tgt_lang></code> text: <code>&lt;output></code></td></tr><tr><td>C</td><td>Translate the text from <code>&lt;src_lang></code> to <code>&lt;tgt_lang></code>: ‚óá <code>&lt;src_lang></code> text: <code>&lt;input></code> ‚óá <code>&lt;tgt_lang></code> text: <code>&lt;output></code></td></tr><tr><td>D</td><td>Translate the words from <code>&lt;src_lang></code> to <code>&lt;tgt_lang></code>: ‚óá <code>&lt;src_lang></code> words: <code>&lt;input></code> ‚óá <code>&lt;tgt_lang></code> words: <code>&lt;output></code></td></tr><tr><td>E</td><td>Convert the phrase from <code>&lt;src_lang></code> to <code>&lt;tgt_lang></code>: ‚óá <code>&lt;src_lang></code> phrase: <code>&lt;input></code> ‚óá <code>&lt;tgt_lang></code> phrase: <code>&lt;output></code></td></tr><tr><td>F</td><td>Render the <code>&lt;src_lang></code> sentence <code>&lt;input></code> to <code>&lt;tgt_lang></code>: <code>&lt;output></code></td></tr><tr><td>G</td><td>Provide the translation of the sentence <code>&lt;input></code> from <code>&lt;src_lang></code> to <code>&lt;tgt_lang></code>: <code>&lt;output></code></td></tr><tr><td>H</td><td>Change the phrase <code>&lt;input></code> to <code>&lt;tgt_lang></code>, the translated phrase is: <code>&lt;output></code></td></tr><tr><td>I</td><td>Please change the sentence <code>&lt;input></code> to <code>&lt;tgt_lang></code>, and the resulting translation is: <code>&lt;output></code></td></tr><tr><td>J</td><td>Change the phrase <code>&lt;input></code> to <code>&lt;tgt_lang></code>, resulting in: <code>&lt;output></code></td></tr><tr><td>K</td><td>The sentence <code>&lt;input></code> in <code>&lt;src_lang></code> means <code>&lt;output></code> in <code>&lt;tgt_lang></code></td></tr></tbody></table></table></figure><blockquote><p>üîº This table details the various sentence templates used to generate parallel data for the Marco-LLM model training. Each template provides instructions for translating text from a source language to a target language. The variations in wording are designed to increase the diversity of the training data and improve model generalization. Placeholders within the templates represent the source language, target language, input text, and resulting output text. The symbol &lsquo;‚óá‚óá\Diamond‚óá&rsquo; indicates a line break within the template. The table aids in understanding how the researchers constructed their parallel dataset, which is a key part of the Marco-LLM&rsquo;s multilingual capabilities.</p><details><summary>read the caption</summary>Table 13: Translation templates used in our experiments. Note: The translation templates are used to construct our parallel data, where ‚óá‚óá\Diamond‚óá indicates the position of a line break. The placeholders <src_lang>, <tgt_lang>, <input>, and <output>represent the source language name, target language name, source text, and target text in the parallel pair, respectively.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Method</th><th>Prompt</th></tr></thead><tbody><tr><td><strong>Keywords-based Explanation</strong></td><td>Suppose that you are a/an <code>{role_1}</code> in <code>{subject}</code>. Please explain the following keywords and meet the following requirements: (1) The keywords: <code>{keywords}</code>; (2) Each keyword explanation should contain at least three sentences. You can generate a story about the keyword for better explanation; (3) The explanations suit <code>{role_2}</code> students; (4) Summarize the explanations. Your answer should be a list of keywords. Make the explanations correct, useful, understandable, and diverse.</td></tr><tr><td><strong>Keywords-based Story</strong></td><td>Assume that you are a/an <code>{role_1}</code> in <code>{subject}</code>. Before you teach students new vocabulary, please write a <code>{type_passage}</code> about the new knowledge and meet the following requirements: (1) It must contain keywords: <code>{keywords}</code>; (2) Its setting should be <code>{scene}</code>; (3) Should be between <code>{min_length}</code> and <code>{max_length}</code> words in length; (4) The writing style should be <code>{style}</code>; (5) The suitable audience is <code>{role_2}</code>; (6) Should end with <code>{ending}</code>; (7) Should be written in <code>{language}</code>.</td></tr><tr><td><strong>Few-shot Based SFT Data</strong></td><td>I want you to act as a Sample Generator. Your goal is to draw inspiration from the <code>Given Sample</code> to create a brand new sample. This new sample should belong to the same domain as the <code>Given Sample</code> but be even rarer. The length and complexity of the <code>Created Sample</code> should be similar to that of the <code>Given Sample</code>. The <code>Created Sample</code> must be reasonable and understandable by humans. The terms <code>Given Sample</code>, <code>Created Sample</code>, &lsquo;given sample&rsquo;, and &lsquo;created sample&rsquo; are not allowed to appear in the <code>Created Sample</code>.</td></tr><tr><td><code>Given Sample:</code></td><td></td></tr><tr><td>(1) <code>Sample doc 1</code></td><td></td></tr><tr><td>(2) <code>Sample doc 2</code></td><td></td></tr><tr><td>(3) <code>Sample doc 3</code></td><td></td></tr><tr><td>(4) ‚Ä¶</td><td></td></tr><tr><td><code>Created Sample:</code></td><td></td></tr></tbody></table></table></figure><blockquote><p>üîº This table details the prompt templates used to generate synthetic data for training the Marco-LLM. It breaks down the prompts used for three methods of synthetic data generation: Keywords-based Explanation, Keywords-based Story, and Few-shot Based SFT Data. Each method has multiple prompts, showcasing the diverse approaches used to generate a wide variety of synthetic training examples. These prompts vary in style, content, and structure to provide a comprehensive and diverse dataset.</p><details><summary>read the caption</summary>Table 14: Prompt Templates for Synthetic Data.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Language</th><th>Qwen2</th><th>Qwen2.5</th><th>Llama-3</th><th>Llama3.1</th><th>Aya-23</th><th>Aya-expanse</th><th>Marco-Chat</th></tr></thead><tbody><tr><td><strong>7B Models</strong></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Arabic</td><td>47.5</td><td>48.0</td><td>58.8</td><td>68.3</td><td>67.2</td><td>45.0</td><td><strong>78.4</strong></td></tr><tr><td>Bengali</td><td>42.3</td><td>61.6</td><td>56.8</td><td>64.3</td><td>50.6</td><td>21.3</td><td><strong>74.6</strong></td></tr><tr><td>English</td><td>29.0</td><td>42.7</td><td>24.4</td><td>41.6</td><td>53.6</td><td>49.6</td><td><strong>44.4</strong></td></tr><tr><td>Finnish</td><td>27.1</td><td>48.4</td><td>54.8</td><td>47.3</td><td>44.7</td><td>23.5</td><td><strong>51.5</strong></td></tr><tr><td>Indonesian</td><td>28.6</td><td>38.4</td><td>34.5</td><td>31.8</td><td>33.4</td><td>30.2</td><td><strong>48.1</strong></td></tr><tr><td>Japanese</td><td>0.8</td><td>8.7</td><td>17.6</td><td>31.8</td><td><strong>71.0</strong></td><td>41.7</td><td>70.5</td></tr><tr><td>Korean</td><td>38.2</td><td>45.6</td><td>25.4</td><td>57.9</td><td>76.0</td><td>20.3</td><td><strong>77.9</strong></td></tr><tr><td>Russian</td><td>22.4</td><td>33.7</td><td>31.2</td><td>40.8</td><td><strong>49.9</strong></td><td>25.7</td><td>46.6</td></tr><tr><td>Swahili</td><td>17.5</td><td>8.9</td><td>30.1</td><td>45.2</td><td>11.8</td><td>13.3</td><td><strong>31.3</strong></td></tr><tr><td>Telugu</td><td>28.7</td><td>22.8</td><td>48.8</td><td><strong>83.3</strong></td><td>5.5</td><td>11.3</td><td>35.2</td></tr><tr><td>Thai</td><td>39.7</td><td>55.7</td><td>54.3</td><td>70.8</td><td>55.2</td><td>29.3</td><td><strong>76.2</strong></td></tr><tr><td>Avg. Scores</td><td>29.2</td><td>39.0</td><td>39.7</td><td>53.0</td><td>47.2</td><td>28.3</td><td><strong>57.7</strong></td></tr><tr><td><strong>70B Models</strong></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Arabic</td><td>42.5</td><td>61.6</td><td>72.7</td><td>62.4</td><td>67.5</td><td>40.9</td><td><strong>78.4</strong></td></tr><tr><td>Bengali</td><td>47.1</td><td>65.7</td><td>55.9</td><td>68.1</td><td>43.6</td><td>28.0</td><td><strong>73.3</strong></td></tr><tr><td>English</td><td>43.0</td><td>45.5</td><td>46.6</td><td>41.2</td><td>53.6</td><td>46.8</td><td><strong>51.2</strong></td></tr><tr><td>Finnish</td><td>55.8</td><td>39.0</td><td>57.0</td><td>50.9</td><td>52.5</td><td>26.0</td><td><strong>57.0</strong></td></tr><tr><td>Indonesian</td><td>39.9</td><td><strong>47.8</strong></td><td>44.0</td><td>31.9</td><td>35.8</td><td>27.6</td><td>46.5</td></tr><tr><td>Japanese</td><td>38.6</td><td>44.7</td><td>43.8</td><td>49.9</td><td><strong>73.6</strong></td><td>22.1</td><td>68.4</td></tr><tr><td>Korean</td><td>41.1</td><td>43.2</td><td>43.8</td><td>63.0</td><td><strong>73.1</strong></td><td>29.5</td><td>69.4</td></tr><tr><td>Russian</td><td>38.5</td><td>43.2</td><td>42.5</td><td>36.3</td><td>52.4</td><td>35.5</td><td><strong>45.9</strong></td></tr><tr><td>Swahili</td><td>18.6</td><td>21.3</td><td>46.0</td><td>33.8</td><td>20.4</td><td>12.5</td><td><strong>49.2</strong></td></tr><tr><td>Telugu</td><td>36.9</td><td>41.4</td><td>45.5</td><td><strong>80.4</strong></td><td>26.6</td><td>22.9</td><td>62.8</td></tr><tr><td>Thai</td><td>41.5</td><td>68.2</td><td>74.0</td><td>66.4</td><td>52.7</td><td>38.7</td><td><strong>76.5</strong></td></tr><tr><td>Avg. Scores</td><td>40.3</td><td>48.4</td><td>52.0</td><td>53.1</td><td>50.2</td><td>30.0</td><td><strong>61.0</strong></td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents a performance comparison of various large language models (LLMs) on the TyDiQA benchmark. TyDiQA is a question answering benchmark dataset designed to evaluate multilingual capabilities across diverse languages. The table shows the performance of several LLMs, broken down by specific languages, allowing for a detailed comparison of their strengths and weaknesses in multilingual question answering. The results provide insight into which models handle diverse languages effectively and highlight the challenges of building robust multilingual NLP systems.</p><details><summary>read the caption</summary>Table 15: Performance comparison of language models on TydiQA benchmark¬†[Clark et¬†al., 2020] across different languages.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Category</th><th>Qwen2</th><th>Qwen2.5</th><th>Llama3</th><th>Llama3.1</th><th>Aya-23</th><th>Aya-expanse</th><th>Marco</th></tr></thead><tbody><tr><td><strong>7B Models</strong></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Average</td><td>81.8</td><td>77.9</td><td>50.8</td><td>55.6</td><td>43.9</td><td>48.5</td><td><strong>86.4</strong></td></tr><tr><td>Hard</td><td>63.1</td><td>51.8</td><td>33.9</td><td>36.9</td><td>32.6</td><td>32.4</td><td><strong>79.9</strong></td></tr><tr><td>Other</td><td><strong>84.9</strong></td><td>82.8</td><td>53.5</td><td>56.3</td><td>44.9</td><td>48.3</td><td>81.0</td></tr><tr><td>Humanities</td><td>84.3</td><td>79.6</td><td>47.2</td><td>53.8</td><td>43.2</td><td>50.3</td><td><strong>84.8</strong></td></tr><tr><td>Social Science</td><td><strong>91.3</strong></td><td>87.8</td><td>58.9</td><td>64.9</td><td>51.2</td><td>54.3</td><td>90.4</td></tr><tr><td>STEM</td><td>73.9</td><td>69.4</td><td>47.2</td><td>51.5</td><td>40.1</td><td>44.7</td><td><strong>88.3</strong></td></tr><tr><td><strong>70B Models</strong></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Average</td><td>90.6</td><td>88.2</td><td>66.7</td><td>71.6</td><td>53.6</td><td>56.9</td><td><strong>94.5</strong></td></tr><tr><td>Hard</td><td>77.8</td><td>73.3</td><td>51.1</td><td>56.0</td><td>35.2</td><td>36.5</td><td><strong>94.0</strong></td></tr><tr><td>Other</td><td>92.0</td><td>88.4</td><td>63.1</td><td>69.2</td><td>52.8</td><td>54.7</td><td><strong>92.5</strong></td></tr><tr><td>Humanities</td><td>92.8</td><td>91.0</td><td>66.5</td><td>70.0</td><td>56.9</td><td>60.3</td><td><strong>95.2</strong></td></tr><tr><td>Social Science</td><td>94.8</td><td>91.5</td><td>75.6</td><td>82.0</td><td>61.8</td><td>66.4</td><td><strong>95.7</strong></td></tr><tr><td>STEM</td><td>88.4</td><td>84.9</td><td>64.2</td><td>68.5</td><td>48.1</td><td>51.4</td><td><strong>94.6</strong></td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents a performance comparison of various Large Language Models (LLMs) on the CEVAL benchmark. CEVAL is a comprehensive evaluation suite designed to assess the capabilities of Chinese language models across a range of tasks. The table breaks down the performance across different categories within the benchmark, offering a detailed view of how well each LLM performs on various aspects of Chinese language understanding and reasoning. It allows for comparison across models of different sizes (7B parameters and 70B+ parameters) by displaying their average scores in various categories. The categories are designed to measure diverse abilities including general knowledge, difficulty of tasks, and specific subject matters. This detailed analysis helps assess the strengths and weaknesses of each LLM, especially regarding its multilingual comprehension capabilities.</p><details><summary>read the caption</summary>Table 16: Performance comparison on the CEVAL benchmark across different categories.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Category</th><th>Qwen2</th><th>Qwen2.5</th><th>Llama3</th><th>Llama3.1</th><th>Aya-23</th><th>Aya-expanse</th><th>Marco-Chat</th></tr></thead><tbody><tr><td><strong>7B Models</strong></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Chinese</td><td>59.5</td><td>57.1</td><td>37.7</td><td>62.5</td><td>36.5</td><td>34.4</td><td><strong>65.3</strong></td></tr><tr><td>English</td><td>54.0</td><td><strong>61.5</strong></td><td>41.5</td><td>51.0</td><td>37.9</td><td>39.9</td><td>56.6</td></tr><tr><td>Gaokao</td><td>64.5</td><td>63.4</td><td>41.5</td><td>53.0</td><td>40.3</td><td>37.3</td><td><strong>71.0</strong></td></tr><tr><td>Gaokao-Chinese</td><td>77.6</td><td>67.9</td><td>47.6</td><td>58.5</td><td>46.3</td><td>42.3</td><td><strong>80.5</strong></td></tr><tr><td>Gaokao-English</td><td>89.5</td><td>85.3</td><td>79.1</td><td>89.5</td><td>82.0</td><td>67.0</td><td><strong>93.1</strong></td></tr><tr><td>Gaokao-Geography</td><td>81.4</td><td>80.4</td><td>49.8</td><td>75.9</td><td>50.8</td><td>50.3</td><td><strong>86.9</strong></td></tr><tr><td>Gaokao-History</td><td>86.0</td><td>82.1</td><td>55.7</td><td>74.9</td><td>51.1</td><td>49.8</td><td><strong>89.8</strong></td></tr><tr><td>Gaokao-Biology</td><td>81.9</td><td>80.5</td><td>52.9</td><td>76.2</td><td>43.8</td><td>35.7</td><td><strong>87.6</strong></td></tr><tr><td>Gaokao-Chemistry</td><td>56.5</td><td>55.6</td><td>32.4</td><td>46.9</td><td>29.5</td><td>30.4</td><td><strong>74.4</strong></td></tr><tr><td>Gaokao-MathQA</td><td>45.3</td><td>55.3</td><td>32.5</td><td>16.2</td><td>27.4</td><td>27.4</td><td><strong>60.7</strong></td></tr><tr><td>Gaokao-Physics</td><td>45.0</td><td>44.5</td><td>15.5</td><td>34.0</td><td>30.5</td><td>26.0</td><td><strong>57.5</strong></td></tr><tr><td>Gaokao-MathCloze</td><td>17.0</td><td><strong>18.6</strong></td><td>8.5</td><td>5.1</td><td>1.7</td><td>6.8</td><td>8.5</td></tr><tr><td>LogiQA-ZH</td><td>60.8</td><td>54.7</td><td>42.1</td><td>61.1</td><td>36.3</td><td>37.8</td><td><strong>68.2</strong></td></tr><tr><td>LSAT-AR</td><td>26.5</td><td>23.9</td><td>23.5</td><td><strong>30.9</strong></td><td>21.3</td><td>23.0</td><td>29.6</td></tr><tr><td>LSAT-LR</td><td>57.3</td><td>66.3</td><td>54.5</td><td><strong>84.5</strong></td><td>41.0</td><td>45.3</td><td>79.2</td></tr><tr><td>LSAT-RC</td><td>66.2</td><td>74.4</td><td>69.5</td><td><strong>89.2</strong></td><td>55.8</td><td>56.1</td><td>75.8</td></tr><tr><td>LogiQA-EN</td><td>46.7</td><td>50.4</td><td>43.5</td><td>59.6</td><td>36.7</td><td>35.3</td><td><strong>61.4</strong></td></tr><tr><td>SAT-Math</td><td>82.7</td><td><strong>88.6</strong></td><td>68.2</td><td>44.1</td><td>35.9</td><td>49.1</td><td>62.3</td></tr><tr><td>SAT-EN</td><td>83.0</td><td><strong>84.5</strong></td><td>82.0</td><td>91.3</td><td>77.2</td><td>64.1</td><td>84.0</td></tr><tr><td>SAT-EN-Without-Passage</td><td>46.6</td><td>42.2</td><td>46.1</td><td>44.7</td><td>42.2</td><td>39.8</td><td><strong>52.9</strong></td></tr><tr><td>Math</td><td>82.7</td><td>48.5</td><td>20.0</td><td><strong>57.6</strong></td><td>7.1</td><td>13.6</td><td>12.8</td></tr><tr><td>Aqua-RAT</td><td>59.8</td><td><strong>74.8</strong></td><td>52.0</td><td>61.0</td><td>24.0</td><td>32.7</td><td>51.2</td></tr><tr><td>JEC-QA-KD</td><td><strong>38.0</strong></td><td>33.1</td><td>17.5</td><td>26.9</td><td>18.2</td><td>17.8</td><td>37.5</td></tr><tr><td>JEC-QA-CA</td><td>34.7</td><td>27.4</td><td>19.1</td><td>26.0</td><td>20.4</td><td>21.1</td><td><strong>38.4</strong></td></tr><tr><td>Average</td><td>57.1</td><td>59.0</td><td>43.4</td><td>41.8</td><td>37.1</td><td>36.7</td><td><strong>61.5</strong></td></tr><tr><td><strong>70B Models</strong></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Chinese</td><td>66.3</td><td>66.0</td><td>51.0</td><td>49.3</td><td>43.5</td><td>42.1</td><td><strong>74.5</strong></td></tr><tr><td>English</td><td>65.7</td><td>69.4</td><td>65.3</td><td>62.5</td><td>45.5</td><td>50.6</td><td><strong>70.4</strong></td></tr><tr><td>Gaokao</td><td>71.8</td><td>71.9</td><td>54.1</td><td>53.0</td><td>47.6</td><td>46.6</td><td><strong>80.6</strong></td></tr><tr><td>Gaokao-Chinese</td><td>85.0</td><td>84.6</td><td>56.5</td><td>58.5</td><td>50.8</td><td>52.4</td><td><strong>92.7</strong></td></tr><tr><td>Gaokao-English</td><td>89.5</td><td>92.2</td><td>90.9</td><td>89.5</td><td>88.9</td><td>71.6</td><td><strong>96.1</strong></td></tr><tr><td>Gaokao-Geography</td><td>88.9</td><td>87.9</td><td>74.9</td><td>75.9</td><td>66.3</td><td>65.8</td><td><strong>96.0</strong></td></tr><tr><td>Gaokao-History</td><td>92.3</td><td>87.2</td><td>71.1</td><td>74.9</td><td>68.9</td><td>63.4</td><td><strong>95.3</strong></td></tr><tr><td>Gaokao-Biology</td><td>86.2</td><td>86.2</td><td>74.3</td><td>76.2</td><td>54.8</td><td>53.3</td><td><strong>95.2</strong></td></tr><tr><td>Gaokao-Chemistry</td><td>68.1</td><td>66.2</td><td>39.6</td><td>46.9</td><td>37.2</td><td>35.8</td><td><strong>84.1</strong></td></tr><tr><td>Gaokao-MathQA</td><td>60.7</td><td><strong>66.1</strong></td><td>45.0</td><td>16.2</td><td>29.1</td><td>36.8</td><td>74.6</td></tr><tr><td>Gaokao-Physics</td><td>53.5</td><td>56.5</td><td>23.0</td><td>34.0</td><td>31.5</td><td>32.0</td><td><strong>75.0</strong></td></tr><tr><td>Gaokao-MathCloze</td><td><strong>22.0</strong></td><td>20.3</td><td>11.9</td><td>5.1</td><td>0.9</td><td>8.5</td><td>16.1</td></tr><tr><td>LogiQA-ZH</td><td>70.2</td><td>70.1</td><td>61.1</td><td>61.1</td><td>46.2</td><td>46.2</td><td><strong>82.3</strong></td></tr><tr><td>LSAT-AR</td><td>32.2</td><td>27.4</td><td>31.7</td><td>30.9</td><td>19.6</td><td>20.9</td><td><strong>40.0</strong></td></tr><tr><td>LSAT-LR</td><td>73.1</td><td>88.0</td><td>80.8</td><td>84.5</td><td>60.0</td><td>59.4</td><td><strong>94.5</strong></td></tr><tr><td>LSAT-RC</td><td>77.7</td><td>85.5</td><td>84.8</td><td>89.2</td><td>72.9</td><td>62.1</td><td><strong>91.1</strong></td></tr><tr><td>LogiQA-EN</td><td>56.8</td><td>59.6</td><td>55.8</td><td>59.6</td><td>43.9</td><td>40.3</td><td><strong>75.7</strong></td></tr><tr><td>SAT-Math</td><td><strong>90.5</strong></td><td>84.1</td><td>86.4</td><td>44.1</td><td>43.6</td><td>66.8</td><td>74.6</td></tr><tr><td>SAT-EN</td><td>89.8</td><td><strong>91.3</strong></td><td>89.8</td><td><strong>91.3</strong></td><td>87.9</td><td>77.7</td><td>90.8</td></tr><tr><td>SAT-EN w/o Passage</td><td>49.0</td><td>49.5</td><td>55.3</td><td>44.7</td><td>44.7</td><td>45.6</td><td><strong>69.4</strong></td></tr><tr><td>Math</td><td>46.5</td><td><strong>63.1</strong></td><td>34.2</td><td>57.6</td><td>8.9</td><td>25.7</td><td>22.6</td></tr><tr><td>AQUA-RAT</td><td>75.2</td><td><strong>76.0</strong></td><td>69.3</td><td>61.0</td><td>28.0</td><td>32.0</td><td>75.2</td></tr><tr><td>JEC-QA-KD</td><td>39.0</td><td>35.5</td><td>34.4</td><td>26.9</td><td>23.2</td><td>19.3</td><td><strong>41.4</strong></td></tr><tr><td>JEC-QA-CA</td><td>39.9</td><td>20.3</td><td>28.9</td><td>26.0</td><td>24.1</td><td>19.5</td><td><strong>44.6</strong></td></tr><tr><td>Avg. Scores</td><td>66.0</td><td>67.5</td><td>57.1</td><td>55.0</td><td>44.4</td><td>45.7</td><td><strong>72.7</strong></td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents a detailed comparison of the performance of several large language models (LLMs) on the AGIEval benchmark, specifically focusing on the 7B parameter models. The AGIEval benchmark is designed to evaluate the reasoning and problem-solving abilities of AI models on tasks that mimic human examinations. The table breaks down the performance across various subcategories of the AGIEval dataset, including general categories like Chinese and English, and more specialized categories like Gaokao (Chinese college entrance exam) sub-sections (Chinese, English, Geography, History, Biology, Chemistry, MathQA, Physics, MathCloze), LogiQA (logical reasoning) sub-sections (Chinese and English), LSAT (Law School Admission Test) sub-sections (Analytical Reasoning, Logical Reasoning, Reading Comprehension), SAT (Scholastic Assessment Test) sub-sections (Math, English, English without passage), and other categories like Math, Aqua-RAT (reading comprehension), JEC-QA (Knowledge and Common Sense Reasoning). For each subcategory, the performance of each LLM is measured and presented, with the highest-performing model for each subcategory highlighted in bold. This allows for a granular comparison of model capabilities across diverse reasoning tasks and language contexts.</p><details><summary>read the caption</summary>Table 17: Agieval 7B Results: Performance of various models across different categories of the Agieval dataset. The best performance in each category is highlighted in bold.</details></blockquote></details><h3 class="relative group">Full paper<div id=full-paper class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#full-paper aria-label=Anchor>#</a></span></h3><div id=gallery-419238cf8104efb4a102591ad561634c class=gallery><img src=https://ai-paper-reviewer.com/2412.04003/1.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04003/2.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04003/3.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04003/4.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04003/5.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04003/6.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04003/7.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04003/8.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04003/9.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04003/10.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04003/11.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04003/12.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04003/13.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04003/14.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04003/15.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04003/16.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04003/17.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04003/18.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04003/19.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2412.04003/20.png class="grid-w50 md:grid-w33 xl:grid-w25"></div></div><section class="flex flex-row flex-wrap justify-center pt-4 text-xl"><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04003/&amp;title=Marco-LLM:%20Bridging%20Languages%20via%20Massive%20Multilingual%20Training%20for%20Cross-Lingual%20Enhancement" title="Share on LinkedIn" aria-label="Share on LinkedIn"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://twitter.com/intent/tweet/?url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04003/&amp;text=Marco-LLM:%20Bridging%20Languages%20via%20Massive%20Multilingual%20Training%20for%20Cross-Lingual%20Enhancement" title="Tweet on Twitter" aria-label="Tweet on Twitter"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="mailto:?body=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04003/&amp;subject=Marco-LLM:%20Bridging%20Languages%20via%20Massive%20Multilingual%20Training%20for%20Cross-Lingual%20Enhancement" title="Send via email" aria-label="Send via email"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1-27.64 140.9 68.65 266.2 199.1 285.1 19.01 2.888 36.17-12.26 36.17-31.49l1e-4-.6631c0-15.74-11.44-28.88-26.84-31.24-84.35-12.98-149.2-86.13-149.2-174.2.0-102.9 88.61-185.5 193.4-175.4 91.54 8.869 158.6 91.25 158.6 183.2v16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98.0036c-7.299.0-13.2 4.992-15.12 11.68-24.85-12.15-54.24-16.38-86.06-5.106-38.75 13.73-68.12 48.91-73.72 89.64-9.483 69.01 43.81 128 110.9 128 26.44.0 50.43-9.544 69.59-24.88 24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3c0-149.2-133.9-265.632-287.3-235.57zM239.1 304.3c-26.47.0-48-21.56-48-48.05s21.53-48.05 48-48.05 48 21.56 48 48.05-20.6 48.05-48 48.05z"/></svg></span></a></section></div><script>var oid="views_paper-reviews/2412.04003/index.md",oid_likes="likes_paper-reviews/2412.04003/index.md"</script><script type=text/javascript src=/ai-paper-reviewer/js/page.min.0860cf4e04fa2d72cc33ddba263083464d48f67de06114529043cb4623319efed4f484fd7f1730df5abea0e2da6f3538855634081d02f2d6e920b956f063e823.js integrity="sha512-CGDPTgT6LXLMM926JjCDRk1I9n3gYRRSkEPLRiMxnv7U9IT9fxcw31q+oOLabzU4hVY0CB0C8tbpILlW8GPoIw=="></script></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="flex group mr-3" href=/ai-paper-reviewer/paper-reviews/2412.04139/><span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&larr;</span>
<span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Monet: Mixture of Monosemantic Experts for Transformers</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2024-12-05T00:00:00+00:00>5 December 2024</time>
</span></span></a></span><span><a class="flex text-right group ml-3" href=/ai-paper-reviewer/paper-reviews/2412.04431/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Infinity: Scaling Bitwise AutoRegressive Modeling for High-Resolution Image Synthesis</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2024-12-05T00:00:00+00:00>5 December 2024</time>
</span></span><span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&larr;</span></a></span></div></div><div class=pt-3><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class=pt-3><script src=https://utteranc.es/client.js repo=pmnxis/pmnxis.github.io issue-term=pathname label=Comment theme=dark-blue crossorigin=anonymous async></script></div></div></footer></article><div id=top-scroller class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer id=site-footer class="py-10 print:hidden"><nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400"><ul class="flex flex-col list-none sm:flex-row"><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=/ai-paper-reviewer/tags/ title=Tags>Tags</a></li><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=https://deep-diver.github.io/neurips2024/ title>NeurIPS2024</a></li></ul></nav><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2025
Hugging Face Daily Papers</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a></p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/ai-paper-reviewer/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh+sCQ0E53ghYrxgYqw+0GCRyIEpA=="></script></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://deep-diver.github.io/ai-paper-reviewer/ style=z-index:500><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body><script data-name=BMC-Widget data-cfasync=false src=https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js data-id=chansung data-description="Support me on Buy me a coffee!" data-message data-color=#FFDD00 data-position=Left data-x_margin=18 data-y_margin=18></script></html>