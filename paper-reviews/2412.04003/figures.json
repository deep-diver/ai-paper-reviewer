[{"figure_path": "https://arxiv.org/html/2412.04003/extracted/6047129/Figures/marco_fig_init.png", "caption": "Figure 1: Comparison of English-centric performance vs Multilingual performance on MMMLU and Flores. Our Marco-LLM demonstrates strong performance on both dimensions.", "description": "This figure compares the performance of Marco-LLM and other LLMs on two benchmark datasets: MMMLU and Flores.  MMMLU assesses multilingual understanding capabilities across various subjects, while Flores evaluates any-to-any machine translation quality.  The x-axis represents English-centric performance (e.g., English understanding or English-to-other language translation), and the y-axis shows multilingual performance. Each point represents an LLM. The plot shows that Marco-LLM achieves high performance on both English-centric and multilingual tasks, significantly outperforming other LLMs, especially in multilingual performance, closing the gap between high- and low-resource language capabilities. This demonstrates the success of Marco-LLM's massive multilingual training approach.", "section": "3.3 Evaluation Results"}, {"figure_path": "https://arxiv.org/html/2412.04003/extracted/6047129/Figures/Marco_Figure.png", "caption": "Figure 2: An overview of the training and evaluation paradigm of our Marco-LLM, we conducted massive multilingual continual pre-training, multilingual supervised finetuning and preference alignment. We further perform extensive evaluation on multilingual benchmarks to validate the efficacy of our Marco-LLM.", "description": "This figure illustrates the training and evaluation process for the Marco-LLM model.  It highlights the three main stages: 1) Massive Multilingual Continual Pre-training, which involves using a large multilingual dataset for initial model training; 2) Multilingual Supervised Fine-tuning, where the model is further trained using high-quality multilingual data focusing on specific tasks; and 3) Preference Alignment, which ensures the model's output aligns with human preferences. The figure concludes by indicating extensive evaluation on multilingual benchmarks to validate Marco-LLM's effectiveness.", "section": "Massive Multilingual Continual Pretraining for Large Language Models"}, {"figure_path": "https://arxiv.org/html/2412.04003/x1.png", "caption": "Figure 3: The amount of tokens per category in our multilingual continual pretraining corpus for Marco-LLM.", "description": "This figure shows the distribution of the training data used for Marco-LLM, categorized by language (high-resource vs. low-resource), data type (parallel data, high-quality knowledge data, synthetic data), and the overall volume in billions of tokens.  The visualization allows for easy comparison of the amount of data used for various languages and data sources, giving insight into the balance of data in the Marco-LLM's training regimen. High-resource languages have much more data compared to the low-resource languages. ", "section": "3.1 Data Collection and Filtering"}, {"figure_path": "https://arxiv.org/html/2412.04003/x2.png", "caption": "Figure 7: The performance of different model size on Flores benchmark. Marco-w/o-parallel-data-filtering denotes that we continuously pre-trained Marco-LLM based on Qwen2 without applying any filtering to parallel data.", "description": "This figure displays the results of an ablation study on the impact of parallel data filtering in the continual pre-training of the Marco-LLM model.  Three different sizes of the Marco-LLM were evaluated on the Flores benchmark: 1.5B, 7B, and 72B parameters.  The 'Marco-w/o-parallel-data-filtering' condition represents the results obtained when the model was trained without filtering noisy data from the parallel corpus, while the 'Marco' condition represents the results obtained with parallel data filtering. This comparison helps to assess the effect of data quality on the model's performance in machine translation across different model scales.", "section": "3.5 Data ablations on Parallel Data"}, {"figure_path": "https://arxiv.org/html/2412.04003/x3.png", "caption": "Figure 9: Accuracy trends across training checkpoints for different languages on MMMLU. The model shows rapid initial learning (0-230 steps) followed by performance stabilization. High-resource languages (ZH-CN, IT-IT) consistently outperform low-resource ones (YO-NG), with a persistent performance gap of \u00a029%.", "description": "This figure displays the accuracy trends across various training checkpoints for different languages when evaluated using the MMMLU benchmark.  The x-axis represents the training steps, and the y-axis shows the accuracy. Multiple lines are plotted, each representing a different language. The figure demonstrates that the model exhibits rapid initial learning in the first 230 steps, followed by a period of performance stabilization.  Importantly, it highlights a consistent performance gap between high-resource languages (like Chinese and Italian) and low-resource languages (like Yoruba).  This gap remains approximately 29% throughout the training process. This observation underscores the challenge of achieving balanced performance across diverse language resources in multilingual language models.", "section": "3.4 Evolution of performance during continual pretraining"}, {"figure_path": "https://arxiv.org/html/2412.04003/x4.png", "caption": "Figure 10: Performance comparison of Marco-LLM against baseline models across 28 languages on multilingual MT-bench. Each subplot shows the win rate (blue), loss rate (green), and tie rate (red) for a specific language. Win rates indicate Marco-LLM\u2019s superior responses, loss rates represent baseline models\u2019 better performance, and tie rates show equivalent quality responses.", "description": "Figure 10 presents a comprehensive comparison of Marco-LLM's performance against six other state-of-the-art multilingual language models across 28 different languages.  The evaluation metric used is multilingual MT-bench, a benchmark designed to assess the quality of machine-translated text in diverse languages. For each language, a bar chart displays the win rate (blue), indicating instances where Marco-LLM produced superior translations; the loss rate (green), showing cases where the other baselines surpassed Marco-LLM; and the tie rate (red), representing instances of comparable translation quality between Marco-LLM and each of the other models. This figure provides a detailed visualization of Marco-LLM's cross-lingual capabilities, highlighting its strengths and weaknesses across various languages, which enables a better understanding of its performance in multilingual translation.", "section": "4.2.3 Evaluation Results"}]