[{"figure_path": "https://arxiv.org/html/2503.16413/extracted/6288040/images/teaser.png", "caption": "Figure 1: Our proposed MultiModal Memory integrates Gaussian splatting with foundation models to efficiently store multimodal memory in a Gaussian structure.\nThe feature maps rendered by our approach exhibit high fidelity, preserving the strong expressive capabilities of the foundation models.", "description": "This figure illustrates the architecture of the proposed MultiModal Memory (M3) system.  M3 combines 3D Gaussian splatting with various foundation models (like CLIP, LLaMA, DINO, etc.) to create a memory system that efficiently stores and retrieves multimodal information about a scene. The Gaussian splatting technique provides an effective way to represent the spatial structure of the scene, while the foundation models contribute rich semantic understanding.  The figure shows how these components work together to generate high-fidelity feature maps that maintain the expressiveness of the foundation models, enabling tasks such as rendering, retrieval, and captioning.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2503.16413/extracted/6288040/images/method1.png", "caption": "Figure 2: A scene (\ud835\udc15\ud835\udc15\\mathbf{V}bold_V) is composed of both structure (\ud835\udc12\ud835\udc12\\mathbf{S}bold_S) and knowledge (\ud835\udc08\ud835\udc08\\mathbf{I}bold_I). To model these, we leverage multiple foundation models to extract multi-granularity scene knowledge, and employ 3D Gaussian splatting to represent the spatial structure. By combining these techniques, we construct a spatial multimodal memory (M3), which enables downstream applications such as retrieval, captioning and grounding.", "description": "The figure illustrates the architecture of M3, a 3D Spatial Multimodal Memory system.  It shows how the system combines multiple foundation models (e.g., CLIP, LLaMA, DINO) to extract multi-granular semantic knowledge from a scene represented as a video (V).  This knowledge, along with the scene's spatial structure captured using 3D Gaussian splatting, is integrated into a unified multimodal memory representation. This memory allows for various downstream tasks, including image retrieval, caption generation, and object grounding.  The figure visually depicts the process, showing how foundation models process the scene's input video to generate features and how Gaussian splatting structures this information for efficient storage and retrieval.", "section": "3.1 3D-SPATIAL MULTIMODAL MEMORY (M3) PIPELINE"}, {"figure_path": "https://arxiv.org/html/2503.16413/extracted/6288040/images/method2.png", "caption": "Figure 3: Given a video sequence, we utilize foundation models (\ud835\udc05\ud835\udc05\\mathbf{F}bold_F) to extract raw features (\ud835\udc11\ud835\udc11\\mathbf{R}bold_R). These features are reduced using Algorithm\u00a01, producing principal scene components (\ud835\udc0f\ud835\udc12\ud835\udc02\ud835\udc0f\ud835\udc12\ud835\udc02\\mathbf{PSC}bold_PSC), which are stored in a memory bank. We introduce optimizable attribute queries (q\ud835\udc5eqitalic_q) to Gaussian primitives, and apply a Gaussian Memory Attention (\ud835\udc00g\u2062msubscript\ud835\udc00\ud835\udc54\ud835\udc5a\\mathbf{A}_{gm}bold_A start_POSTSUBSCRIPT italic_g italic_m end_POSTSUBSCRIPT) mechanism to produce the final rendered features (\ud835\udc11^^\ud835\udc11\\hat{\\mathbf{R}}over^ start_ARG bold_R end_ARG), which can be linked back to various heads of the foundation models.", "description": "Figure 3 illustrates the MultiModal Memory (M3) pipeline.  A video sequence is input, and foundation models extract raw features.  These high-dimensional features are then reduced using Algorithm 1 to create principal scene components (PSCs), which are stored in a memory bank.  Optimizable attribute queries are applied to Gaussian primitives, and a Gaussian Memory Attention mechanism generates final rendered features. These rendered features retain the semantic information from the foundation models and are used for downstream tasks via connections to various model heads.", "section": "3 METHOD"}, {"figure_path": "https://arxiv.org/html/2503.16413/extracted/6288040/images/manifold.png", "caption": "Figure 4: The UMAP visualization of model embedding manifolds reveals distinct shapes, reflecting different focus.", "description": "This UMAP visualization shows the embedding manifolds for different foundation models used in the paper.  Each model's embeddings form a distinct cluster shape, illustrating how each model focuses on different aspects of scene understanding. The different shapes highlight the unique features and perspectives extracted by the diverse foundation models, which range from vision-language models to perception models.  This visualization emphasizes the multi-modality and multi-granularity of features extracted for the proposed multimodal memory.", "section": "3.2 M3 PRELIMINARIES"}, {"figure_path": "https://arxiv.org/html/2503.16413/extracted/6288040/images/data_engine.png", "caption": "Figure 5: Illustration of patch-level visual embedding extraction their applications.", "description": "This figure illustrates the process of extracting patch-level visual embeddings from various foundation models for downstream tasks.  It shows how different models (DINOv2, CLIP, SigLIP, LLaMA3, LLaMAv) process input images, focusing on their respective strengths in vision and language understanding. The extracted features from different layers of these models are used to build a comprehensive understanding of the scene at a granular level. The output shows how the combined features can be effectively used in applications such as visual question answering, captioning, and retrieval.", "section": "3.3 SPATIAL MULTIMODAL MEMORY"}, {"figure_path": "https://arxiv.org/html/2503.16413/extracted/6288040/images/qr3.png", "caption": "Figure 6: Qualitative results across datasets using M3. The figure showcases the consistent performance of the M3\u00a0across various datasets (Garden, Playroom, Drjohnson, Table-top).", "description": "Figure 6 presents a qualitative comparison of the M3 model's performance across four diverse datasets: Garden, Playroom, Drjohnson, and Tabletop.  The images showcase the model's consistent ability to accurately capture and render fine-grained details, intricate textures, and complex scene structures, regardless of the dataset's specific characteristics. The results demonstrate M3's robustness and generalizability in handling various visual environments.", "section": "4.3 Qualitative Results"}, {"figure_path": "https://arxiv.org/html/2503.16413/extracted/6288040/images/demo.jpg", "caption": "Figure 7: Real robot deployment.", "description": "The figure shows a quadruped robot grasping a rubber duck. The robot uses the M3 model to locate and identify the object based on a text query (\"yellow bath duck\"). The image sequence demonstrates the robot's ability to identify the object, determine its position using a depth camera, and execute a grasping action. This showcases the real-world applicability of the M3 model for tasks involving object manipulation.", "section": "4.4 DEMONSTRATION RESULTS"}, {"figure_path": "https://arxiv.org/html/2503.16413/extracted/6288040/images/real_robot.png", "caption": "Figure 8: Real robot deployment on part-level understanding, multi-scene and long-horizon tasks.", "description": "This figure showcases the capabilities of the M3 model in real-world robot manipulation tasks. Three scenarios are depicted: part-level understanding (picking up a specific object by its handle), multi-scene understanding (grounding and grasping objects across different environments), and long-horizon tasks (completing a multi-step task such as retrieving an object from a different location and placing it elsewhere). Each scenario is demonstrated with a sequence of images illustrating the robot's actions.", "section": "4.4 DEMONSTRATION RESULTS"}]