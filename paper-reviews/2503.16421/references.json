{"references": [{"fullname_first_author": "Lvmin Zhang", "paper_title": "Adding conditional control to text-to-image diffusion models", "publication_date": "2023-01-01", "reason": "This paper introduces ControlNet, a neural network architecture that adds conditional control to text-to-image diffusion models."}, {"fullname_first_author": "Zhuoyi Yang", "paper_title": "Cogvideox: Text-to-video diffusion models with an expert transformer", "publication_date": "2024-08-01", "reason": "This paper describes CogVideoX, which is a key baseline image-to-video model that serves as the base model in the MagicMotion framework."}, {"fullname_first_author": "Yuwei Guo", "paper_title": "Animatediff: Animate your personalized text-to-image diffusion models without specific tuning", "publication_date": "2024-01-01", "reason": "This paper presents AnimateDiff, a popular video generation approach and baseline for comparison in the field."}, {"fullname_first_author": "Robin Rombach", "paper_title": "High-resolution image synthesis with latent diffusion models", "publication_date": "2021-01-01", "reason": "This paper introduced Latent Diffusion Models, a key approach in modern video and image generation that has inspired further research."}, {"fullname_first_author": "Zhouxia Wang", "paper_title": "Motionctrl: A unified and flexible motion controller for video generation", "publication_date": "2024-01-01", "reason": "This paper introduced MotionCtrl, a method that uses a unified motion controller for video generation."}]}