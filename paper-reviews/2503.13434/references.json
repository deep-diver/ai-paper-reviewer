{"references": [{"fullname_first_author": "Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-01-01", "reason": "This paper introduces CLIP, a foundational model for aligning visual and textual representations, which is widely used for controlling image generation and assessing the similarity between images and text prompts."}, {"fullname_first_author": "Rombach", "paper_title": "High-resolution image synthesis with latent diffusion models", "publication_date": "2022-01-01", "reason": "This work presents Latent Diffusion Models, which are the foundation for much of the recent progress in high-quality image generation, providing an efficient approach to training diffusion models in a latent space."}, {"fullname_first_author": "Zhang", "paper_title": "Adding conditional control to text-to-image diffusion models", "publication_date": "2023-01-01", "reason": "This paper introduces ControlNet, a neural network structure that allows diffusion models to accept additional conditional inputs like edge maps or segmentation maps, significantly enhancing the controllability of image generation."}, {"fullname_first_author": "Caron", "paper_title": "Emerging properties in self-supervised vision transformers", "publication_date": "2021-01-01", "reason": "This paper introduces DINO (self-distillation with no labels), which is used in the paper for preserving identity and performing instance recognition tasks."}, {"fullname_first_author": "Ruiz", "paper_title": "Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation", "publication_date": "2023-01-01", "reason": "This paper presents a fine tuning approach using diffusion models that allows the generation of the subject based on the text prompt."}]}