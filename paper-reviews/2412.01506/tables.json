[{"content": "| Method | Appearance PSNR \u2191 | Appearance LPIPS \u2193 | Geometry CD \u2193 | Geometry F-score \u2191 | Geometry PSNR-N \u2191 | Geometry LPIPS-N \u2193 |\n|---|---|---|---|---|---|---|\n| LN3Diff | 26.44 | 0.076 | 0.0299 | 0.9649 | 27.10 | 0.094 |\n| 3DTopia-XL | 25.34\u2020 | 0.074\u2020 | 0.0128 | 0.9939 | 31.87 | 0.080 |\n| CLAY | \u2013 | \u2013 | 0.0124 | 0.9976 | 35.35 | 0.035 |\n| Ours | 32.74/<sub>32.19\u2021</sub> | 0.025/<sub>0.029\u2021</sub> | 0.0083 | 0.9999 | 36.11 | 0.024 |", "caption": "Table 1: Reconstruction fidelity of different latent representations. (\u2020: evaluated using albedo color; \u2021: evaluated via Radiance Fields)", "description": "This table presents a quantitative comparison of the reconstruction fidelity achieved by different latent representations, including the proposed Structured Latent (SLAT) method and existing alternatives. The comparison is done in terms of both appearance and geometry, evaluating various metrics such as PSNR (Peak Signal-to-Noise Ratio), LPIPS (Learned Perceptual Image Patch Similarity), CD (Chamfer Distance), and F-score. The appearance fidelity is assessed using albedo color for some methods and radiance fields for others, as indicated by the symbols (\u2020 and \u2021). This detailed analysis demonstrates the superior performance of SLAT in accurately reconstructing high-quality 3D assets compared to other techniques.", "section": "4.1. Reconstruction Results"}, {"content": "| Method | CLIP \u2191 | FDincep \u2193 | KDincep \u2193 | FDdinov2 \u2193 | KDdinov2 \u2193 | FDpoint \u2193 | CLIP \u2191 | FDincep \u2193 | KDincep \u2193 | FDdinov2 \u2193 | KDdinov2 \u2193 | FDpoint \u2193 |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| Shap-E | 25.04 | 37.93 | 0.78 | 497.17 | 49.96 | 6.58 | 82.11 | 34.72 | 0.87 | 465.74 | 62.72 | 8.20 |\n| LGM | 24.83 | 36.18 | 0.77 | 507.47 | 61.89 | 24.73 | 83.97 | 26.31 | 0.48 | 322.71 | 38.27 | 15.90 |\n| InstantMesh | 25.56 | 36.73 | 0.62 | 478.92 | 49.77 | 10.79 | 84.43 | 20.22 | 0.30 | 264.36 | 25.99 | 9.63 |\n| 3DTopia-XL | 22.48\u2020 | 53.46\u2020 | 1.39\u2020 | 756.37\u2020 | 87.40\u2020 | 13.72 | 78.45\u2020 | 37.68\u2020 | 1.20\u2020 | 437.37\u2020 | 53.24\u2020 | 18.21 |\n| Ln3Diff | 18.69 | 71.79 | 2.85 | 976.40 | 154.18 | 19.40 | 82.74 | 26.61 | 0.68 | 357.93 | 50.72 | 7.86 |\n| GaussianCube | 24.91 | 27.35 | 0.30 | 460.07 | 39.01 | 29.95 | \u2013 | \u2013 | \u2013 | \u2013 | \u2013 | \u2013 |\n| Ours L | 26.60 | 20.54 | 0.08 | 238.60 | 4.24 | 5.24 | 85.77 | 9.35 | 0.02 | 67.21 | 0.72 | 2.03 |\n| Ours XL | 26.70 | 20.48 | 0.08 | 237.48 | 4.10 | 5.21 | \u2013 | \u2013 | \u2013 | \u2013 | \u2013 | \u2013 |", "caption": "Table 2: Quantitative comparisons using Toys4k\u00a0[80]. (KD is reported \u00d7100absent100\\times 100\u00d7 100. \u2020: evaluated using shaded images of PBR meshes.)", "description": "Table 2 presents a quantitative comparison of different 3D generation methods using the Toys4k dataset.  It compares performance on both text-to-3D and image-to-3D tasks. Metrics include CLIP score (a measure of alignment between generated assets and input text/image prompts), Fr\u00e9chet Inception Distance (FDincep), Kernel Inception Distance (KDincep) (both multiplied by 100), Fr\u00e9chet DINOv2 Distance (FDdinov2), Kernel DINOv2 Distance (KDdinov2) (both multiplied by 100), and Fr\u00e9chet Point Distance (FDpoint).  The '\u2020' symbol indicates that the PSNR and LPIPS values were calculated using shaded images of Physically Based Rendering (PBR) meshes for those specific metrics.  This allows for a more comprehensive evaluation of both the appearance and geometry quality of the generated 3D models.", "section": "4.2 Generation Results"}, {"content": "| Resolution | Channel | PSNR \u2191 | LPIPS \u2193 |\n|---|---|---|---| \n| 32 | 16 | 31.64 | 0.0297 |\n| 32 | 32 | 31.80 | 0.0289 |\n| 32 | 64 | 31.85 | 0.0283 |\n| 64 | 8 | 32.74 | 0.0250 |", "caption": "Table 3: Ablation study on the size of SLat.", "description": "This ablation study investigates the impact of different SLAT (Structured Latent) sizes on the model's performance.  It varies the resolution and number of channels within the SLAT representation and assesses the resulting PSNR (Peak Signal-to-Noise Ratio) and LPIPS (Learned Perceptual Image Patch Similarity) scores.  Higher PSNR indicates better reconstruction quality, while lower LPIPS suggests improved perceptual similarity to the ground truth.  The results show how changes to SLAT size affect the balance between reconstruction quality and perceptual fidelity.", "section": "4.3. Ablation Study"}, {"content": "|       | Method      | Training set **CLIP**\u2191 | Training set **FD**<sub>**dinov2**</sub>\u2193 | Toys4k **CLIP**\u2191 | Toys4k **FD**<sub>**dinov2**</sub>\u2193 |\n| :---- | :---------- | :-----------------------: | :----------------------------: | :---------------: | :-----------------------: |\n| Stage 1 | Diffusion   | 25.09                     | 132.71                         | 25.86             | 295.90                    |\n|        | Rectified flow | **25.40**                 | **113.42**                     | **26.37**         | **269.56**                 |\n| Stage 2 | Diffusion   | 25.58                     | 100.88                         | 26.45             | 244.08                    |\n|        | Rectified flow | **25.65**                 | **95.97**                      | **26.61**         | **240.20**                 |", "caption": "Table 4: Ablation study on different generation paradigms.", "description": "This table presents the results of an ablation study comparing different generation paradigms.  Specifically, it investigates the impact of using diffusion models versus rectified flow models at each stage of the two-stage generation process (structure generation and latent generation). The study is performed using the Toys4k dataset, assessing the performance using the CLIP score and Fr\u00e9chet Inception Distance (FD). The table allows readers to analyze the effectiveness of each paradigm combination (diffusion or rectified flow for each stage) in generating high-quality 3D assets.", "section": "4.3 Ablation Study"}, {"content": "| Method | Training set **CLIP**\u2191 | Training set **FD**<sub>**dinov2**</sub>\u2193 | Toys4k **CLIP**\u2191 | Toys4k **FD**<sub>**dinov2**</sub>\u2193 |\n|---|---|---|---|---|\n| B | 25.41 | 121.45 | 26.47 | 265.26 |\n| L | 25.62 | 99.92 | 26.60 | 238.60 |\n| XL | 25.71 | 93.96 | 26.70 | 237.48 |", "caption": "Table 5: Ablation study on model size.", "description": "This table presents the results of an ablation study investigating the effect of model size on the performance of the proposed 3D generation method.  It shows how different model sizes (Basic, Large, and X-Large), each with varying numbers of parameters, impact the quality of 3D asset generation, as measured by CLIP score and Fr\u00e9chet Inception Distance (FD-Inception).  The performance is assessed using the Toys4k dataset, which is held-out from the training process, allowing for a fair evaluation of generalization ability.", "section": "4.3 Ablation Study"}, {"content": "| Network | #Layer | #Dim. | #Head | Block Arch. | Special Modules | #Param. |\n|---|---|---|---|---|---|---|\n| \ud835\udcd4<sub>S</sub> | \u2013 | \u2013 | \u2013 | \u2013 | 3D Conv. U-Net | 59.3M |\n| \ud835\udcd3<sub>S</sub> | \u2013 | \u2013 | \u2013 | \u2013 | 3D Conv. U-Net | 73.7M |\n| \ud835\udcd4 | 12 | 768 | 12 | 3D-SW-MSA + FFN | 3D Swin Attn. | 85.8M |\n| \ud835\udcd3<sub>GS</sub> | 12 | 768 | 12 | 3D-SW-MSA + FFN | 3D Swin Attn. | 85.4M |\n| \ud835\udcd3<sub>RF</sub> | 12 | 768 | 12 | 3D-SW-MSA + FFN | 3D Swin Attn. | 85.4M |\n| \ud835\udcd3<sub>M</sub> | 12 | 768 | 12 | 3D-SW-MSA + FFN | 3D Swin Attn. + Sp. Conv. Upsampler | 90.9M |\n| \ud835\udcd6<sub>S</sub>-B (text ver.) | 12 | 768 | 12 | MSA + MCA + FFN | QK Norm. | 157M |\n| \ud835\udcd6<sub>S</sub>-L (text ver.) | 24 | 1024 | 16 | MSA + MCA + FFN | QK Norm. | 543M |\n| \ud835\udcd6<sub>S</sub>-XL (text ver.) | 28 | 1280 | 16 | MSA + MCA + FFN | QK Norm. | 975M |\n| \ud835\udcd6<sub>S</sub>-L (image ver.) | 24 | 1024 | 16 | MSA + MCA + FFN | QK Norm. | 556M |\n| \ud835\udcd6<sub>L</sub>-B (text ver.) | 12 | 768 | 12 | MSA + MCA + FFN | QK Norm. + Sp. Conv. Downsampler / Upsampler + Skip Conn. | 185M |\n| \ud835\udcd6<sub>L</sub>-L (text ver.) | 24 | 1024 | 16 | MSA + MCA + FFN | QK Norm. + Sp. Conv. Downsampler / Upsampler + Skip Conn. | 588M |\n| \ud835\udcd6<sub>L</sub>-XL (text ver.) | 28 | 1280 | 16 | MSA + MCA + FFN | QK Norm. + Sp. Conv. Downsampler / Upsampler + Skip Conn. | 1073M |\n| \ud835\udcd6<sub>L</sub>-L (image ver.) | 24 | 1024 | 16 | MSA + MCA + FFN | QK Norm. + Sp. Conv. Downsampler / Upsampler + Skip Conn. | 600M |", "caption": "Table 6: Network configurations used in this paper. SW stands for \u201cShifted Window\u201d, MSA and MCA for \u201cMultihead Self-Attention\u201d and \u201cMultihead Cross-Attention\u201d, and Sp. Conv. for \u201cSparse Convolution\u201d.", "description": "This table details the architectures of the neural networks used in the paper.  It shows the number of layers, dimensions, number of attention heads, the types of blocks used (including Shifted Window Multihead Self-Attention and Multihead Cross-Attention blocks, and Sparse Convolutional blocks), and the total number of parameters for each network.  The networks are categorized by their role in the overall system:  encoders, decoders, and the two-stage generation process (sparse structure generation and latent generation).  Understanding this table is key to comprehending the computational complexity and design choices made in the model.", "section": "A. More Implementation Details"}, {"content": "| ![x8.png](https://arxiv.org/html/2412.01506/x8.png) | ![x9.png](https://arxiv.org/html/2412.01506/x9.png) | ![x10.png](https://arxiv.org/html/2412.01506/x10.png) | ![x11.png](https://arxiv.org/html/2412.01506/x11.png) | ![x12.png](https://arxiv.org/html/2412.01506/x12.png) |", "caption": "Table 7: Ablation study on timestep sampling distributions.", "description": "This table presents the results of an ablation study conducted to investigate the impact of different timestep sampling distributions on the performance of the model.  Specifically, it examines the effect of using different probability distributions to sample timesteps during the training process of the model.  The table shows how different sampling techniques (using logit Normal (0,1) and logit Normal (1,1) distributions) impact two key metrics: the CLIP score (a measure of prompt alignment) and the Fr\u00e9chet Inception Distance (FID) with DINOv2 features (a measure of image quality).  This helps to determine the optimal timestep sampling strategy for improved model performance. The study is conducted separately for stage 1 and stage 2 of the model's generation process.", "section": "A.2. Training Details"}, {"content": "|   | Distribution | **CLIP**\u2191 | **FD<sub>dinov2</sub>**\u2193 |\n|---|---|---|---| \n| Stage 1 | logitNorm(0,1) | 26.03 | 287.33 |\n|  | logitNorm(1,1) | **26.37** | **269.56** |\n| Stage 2 | logitNorm(0,1) | **26.61** | 242.36 |\n|  | logitNorm(1,1) | **26.61** | **240.20** |", "caption": "Table 8: Composition of the training set and evaluation set.", "description": "This table details the composition of the training and evaluation datasets used in the study.  It shows the source of the 3D models (Objaverse-XL (Sketchfab), Objaverse-XL (Github), ABO, 3D-FUTURE, and HSSD), the threshold for aesthetic scores used to filter low-quality models, the number of samples remaining after filtering, and the average aesthetic score for each dataset.  It also shows the size of the Toys4k dataset used for evaluation.", "section": "B. Data Preparation Details"}, {"content": "| Image | Score |\n|---|---| \n| [https://arxiv.org/html/2412.01506/figures/aesthetic_scores/2.32.jpg](https://arxiv.org/html/2412.01506/figures/aesthetic_scores/2.32.jpg) | 2.32 |\n| [https://arxiv.org/html/2412.01506/figures/aesthetic_scores/3.84.jpg](https://arxiv.org/html/2412.01506/figures/aesthetic_scores/3.84.jpg) | 3.84 |\n| [https://arxiv.org/html/2412.01506/figures/aesthetic_scores/4.91.jpg](https://arxiv.org/html/2412.01506/figures/aesthetic_scores/4.91.jpg) | 4.91 |\n| [https://arxiv.org/html/2412.01506/figures/aesthetic_scores/5.24.jpg](https://arxiv.org/html/2412.01506/figures/aesthetic_scores/5.24.jpg) | 5.24 |\n| [https://arxiv.org/html/2412.01506/figures/aesthetic_scores/5.85.jpg](https://arxiv.org/html/2412.01506/figures/aesthetic_scores/5.85.jpg) | 5.85 |\n| [https://arxiv.org/html/2412.01506/figures/aesthetic_scores/6.04.jpg](https://arxiv.org/html/2412.01506/figures/aesthetic_scores/6.04.jpg) | 6.04 |\n| [https://arxiv.org/html/2412.01506/figures/aesthetic_scores/6.29.jpg](https://arxiv.org/html/2412.01506/figures/aesthetic_scores/6.29.jpg) | 6.29 |\n| [https://arxiv.org/html/2412.01506/figures/aesthetic_scores/7.03.jpg](https://arxiv.org/html/2412.01506/figures/aesthetic_scores/7.03.jpg) | 7.03 |", "caption": "Table 9: Detailed statistics of the user study.", "description": "This table presents a detailed breakdown of the results from a user study comparing different 3D asset generation methods.  It shows the number of times each method was selected as the preferred model for both text-to-3D and image-to-3D generation tasks, along with the corresponding selection percentages.  The \"Not Sure\" category indicates instances where participants were unable to make a confident choice.", "section": "C.2. User Study"}]