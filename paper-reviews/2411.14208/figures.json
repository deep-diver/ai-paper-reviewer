[{"figure_path": "https://arxiv.org/html/2411.14208/x2.png", "caption": "Figure 1: We introduce ViewExtrapolator, a novel approach that leverages the generative priors of Stable Video Diffusion for novel view extrapolation, where the novel views lie far beyond the range of the training views. ViewExtrapolator effectively refines the artifact-prone renderings (left side of arrows) of radiance fields or point clouds, to more realistic renderings with fewer artifacts (right side of arrows).", "description": "This figure showcases the ViewExtrapolator method, which enhances novel view extrapolation using Stable Video Diffusion.  The left side displays input renderings from radiance fields or point clouds, often exhibiting artifacts because the novel views are outside the training data range.  The right side demonstrates how ViewExtrapolator refines these renderings, resulting in more realistic and artifact-free images.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2411.14208/x3.png", "caption": "Figure 2: The setting differences between novel view interpolation and novel view extrapolation: Radiance fields excel at novel view interpolation but struggle at novel view extrapolation.", "description": "This figure illustrates the key difference between novel view interpolation and novel view extrapolation.  Novel view interpolation involves generating new views from within the range of the provided training views.  In this scenario, radiance fields perform very well, producing high-quality results. Novel view extrapolation, on the other hand, requires the generation of views that are significantly outside the range of the training views.  As shown in the figure, radiance field methods struggle with this task and produce artifacts.", "section": "2. Related Work"}, {"figure_path": "https://arxiv.org/html/2411.14208/x4.png", "caption": "Figure 3: Overview of the proposed ViewExtrapolator. We render an artifact-prone video from the closest training view to an extrapolative novel view, and then refine it by guiding SVD to preserve the original scene content and eliminate the artifacts with guidance annealing and resampling annealing.", "description": "The figure illustrates the ViewExtrapolator process.  First, a video is rendered showing a gradual transition from a known training view to a novel view far outside the training data range. This initial rendering often produces artifacts due to the lack of training data in the extrapolated region. Then, the Stable Video Diffusion (SVD) model is used to refine this video.  The SVD model's denoising process is modified with 'guidance annealing' (gradually reducing the influence of the initial, artifact-prone video over time) and 'resampling annealing' (repeatedly denoising to further refine the video). The result is a refined video with significantly fewer artifacts, demonstrating the ability of ViewExtrapolator to generate realistic novel views despite limited training data.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2411.14208/x5.png", "caption": "Figure 4: Qualitative comparisons. We compare ViewExtrapolator with 3DGS and DRGS on novel view extrapolation. ViewExtrapolator demonstrates superior generation quality with much fewer artifacts. The last column shows the distribution of training and test views as well as the corresponding extrapolation degree e\ud835\udc52eitalic_e. Zoom in for details.", "description": "Figure 4 presents a qualitative comparison of novel view extrapolation results between ViewExtrapolator and two other methods, 3DGS and DRGS.  The comparison highlights ViewExtrapolator's superior ability to generate high-quality images with significantly fewer artifacts. Each row displays the same scene from multiple viewpoints. The leftmost column shows 3D Gaussian Splatting (3DGS) results, the next column displays Depth-Regularized Gaussian Splatting (DRGS) results, and the third column shows the results produced by the proposed ViewExtrapolator. The rightmost column provides context, illustrating the distribution of training and testing viewpoints used, and their respective extrapolation degrees (e).  The extrapolation degree (e) quantifies how far the novel viewpoint is from the training viewpoints.  A higher value of e indicates a novel viewpoint that is more distant from training viewpoints, representing a more challenging extrapolation task.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2411.14208/x6.png", "caption": "Figure 5: The definition of extrapolation degree e\ud835\udc52eitalic_e by the ratio between \ud835\udc1d\ud835\udc1d\\mathbf{d}bold_d and r\ud835\udc5fritalic_r (\ud835\udc1d\ud835\udc1d\\mathbf{d}bold_d stands for the distance between the novel view and the central point of training views, and r\ud835\udc5fritalic_r stands for the training view range as the maximum extent of the training views along the direction of \ud835\udc1d\ud835\udc1d\\mathbf{d}bold_d). A higher e\ud835\udc52eitalic_e means that the novel view is farther away from the training views.", "description": "Figure 5 illustrates how the extrapolation degree (e) is calculated to quantify the distance of a novel view from existing training views.  The distance (d) between the novel view and the central point of all training views is measured, and the training view range (r) is defined as the maximum extent of the training views along the direction of d. The extrapolation degree (e) is the ratio of d to r (e = d/r). A larger e indicates a greater distance between the novel view and the training views, signifying that the novel view is further outside the bounds of the observed training data.", "section": "4.1. Dataset"}, {"figure_path": "https://arxiv.org/html/2411.14208/x7.png", "caption": "Figure 6: Distributions of extrapolation degree e\ud835\udc52eitalic_e across existing benchmarks and our proposed LLFF-Extra. Unlike LLFF-Extra, all existing benchmarks exhibit a small e\ud835\udc52eitalic_e, indicating that they predominantly focus on the evaluation of novel view interpolation instead of extrapolation.", "description": "This figure (Figure 6) is a histogram that visually compares the distribution of extrapolation degree *e* across different novel view synthesis benchmarks.  The extrapolation degree *e* quantifies how far a novel view lies from the training views; a higher *e* indicates extrapolation, while a lower *e* suggests interpolation. The figure shows that existing benchmarks primarily contain data points with low *e* values, meaning their evaluations focused heavily on novel view interpolation. In contrast, the proposed LLFF-Extra benchmark has a distribution skewed towards high *e* values, demonstrating its focus on novel view extrapolation.", "section": "4.1. Dataset"}, {"figure_path": "https://arxiv.org/html/2411.14208/x8.png", "caption": "Figure 7: Results from different rendering methods. Our method can refine view sequences rendered from (a) 3D Gaussian Splatting, (b) Instant-NGP, and point cloud from (c) a single view or (d) monocular video. (The top row in each section is the rendered artifact-prone video and the bottom row is the refined video.)", "description": "This figure demonstrates the versatility of the proposed ViewExtrapolator method by showcasing its ability to refine videos rendered from various 3D representations.  Specifically, it presents four sets of video sequences, each containing a top row showing the original artifact-prone video and a bottom row demonstrating the refined video after processing with ViewExtrapolator. The four sets are: (a) Videos rendered using 3D Gaussian Splatting; (b) Videos rendered using Instant NGP; (c) Videos rendered from a single-view point cloud; and (d) Videos rendered from a monocular video point cloud.  The comparison highlights the effectiveness of ViewExtrapolator in improving visual quality and reducing artifacts irrespective of the underlying 3D representation technique.", "section": "4.3 Broad Applicability"}, {"figure_path": "https://arxiv.org/html/2411.14208/x10.png", "caption": "Figure 8: Ablation studies. We show the ablation results for 3DGS and point cloud renderings. As point clouds are used for single-image novel view extrapolation without ground truth, we show the input image for reference instead.\nAs highlighted in the red circles, both guidance annealing and resampling annealing are essential for artifact refinement. Please zoom in for details.", "description": "This figure presents an ablation study on the effectiveness of guidance annealing and resampling annealing in ViewExtrapolator.  It shows results for both 3D Gaussian Splatting (3DGS) renderings and renderings from point clouds.  Since ground truth isn't available for point cloud single-image novel view extrapolation, the input image is shown for comparison. The results demonstrate that both guidance annealing and resampling annealing are crucial for effective artifact reduction during the video refinement process.  Red circles highlight specific areas where the benefits of these techniques are evident.", "section": "4.4. Ablation Studies"}]