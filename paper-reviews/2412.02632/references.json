{"references": [{"fullname_first_author": "Patrick Esser", "paper_title": "Taming transformers for high-resolution image synthesis", "publication_date": "2021-06-01", "reason": "This paper introduced VQ-GAN, a foundational model for the image tokenization methods explored in the current paper."}, {"fullname_first_author": "Lijun Yu", "paper_title": "MagVIT: Masked generative video transformer", "publication_date": "2023-06-01", "reason": "This paper introduced MagVIT, a powerful architecture that is closely related to the methods used in the current paper."}, {"fullname_first_author": "Aaron van den Oord", "paper_title": "Neural discrete representation learning", "publication_date": "2017-12-01", "reason": "This paper introduced VQ-VAE, a highly influential model for vector quantization that is directly relevant to the current work."}, {"fullname_first_author": "Mohammad Adiban", "paper_title": "Hierarchical residual learning based vector quantized variational autoencoder for image reconstruction and generation", "publication_date": "2022-01-01", "reason": "This paper presents a relevant model for image reconstruction and generation that is compared against in this work."}, {"fullname_first_author": "Fabian Mentzer", "paper_title": "Finite scalar quantization: VQ-VAE made simple", "publication_date": "2024-01-01", "reason": "This paper introduced a novel quantization method (FSQ) that is directly compared against the proposed method (GSQ) in this work."}]}