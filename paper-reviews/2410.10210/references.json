{"references": [{" publication_date": "2024", "fullname_first_author": "Yushi Bai", "paper_title": "Longwriter: Unleashing 10,000+ word generation from long context llms", "reason": "This paper is foundational to the current work, introducing the concept of aligning foundation models with long-output data to enhance the ability of LLMs to generate lengthy text.  The current research builds upon this work by emphasizing the importance of data quality and explores a more efficient approach to achieve similar results.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Yushi Bai", "paper_title": "Longwriter-6k dataset", "reason": "This paper introduces the LongWriter-6k dataset, which is the basis for the current study's data refinement process.  The current research uses this dataset, improving its quality and ultimately validating the hypothesis presented in the original Longwriter paper.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Yushi Bai", "paper_title": "LongWriter-glm4-9b", "reason": "This paper introduces a specific model (LongWriter-glm4-9b) trained using the LongWriter-6k dataset. The current research uses this model as a comparison baseline to demonstrate the efficiency of the proposed tuning approach and to demonstrate that the results are not limited to a specific model.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Chau Minh Pham", "paper_title": "Suri: Multi-constraint instruction following for long-form text generation", "reason": "This paper addresses the challenge of long-form text generation with multiple constraints. The current research relates to this paper by focusing on a critical aspect of this task, namely generating text with a specified length. This paper provides a more general context for the specified task.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Machel Reid", "paper_title": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context", "reason": "This paper demonstrates significant progress in handling extensive inputs in LLMs.  While focused on input processing, it establishes a relevant context for the challenge the current research addresses: efficiently generating long outputs comparable to the length of inputs.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Team GLM", "paper_title": "Chatglm: A family of large language models from glm-130b to glm-4 all tools", "reason": "This paper introduces GLM models, which are used in the current research for experiments and comparisons.  The models are used to test the methodology of the current paper to see if they are generalisable to multiple families of models.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Jianheng Huang", "paper_title": "Mitigating catastrophic forgetting in large language models with self-synthesized rehearsal", "reason": "This paper tackles catastrophic forgetting in LLMs, a relevant challenge when retraining models for new tasks.  The current study uses alignment data in addition to task-specific data, so it's relevant to this paper which also uses alignment data in their experiments.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Zhangchen Xu", "paper_title": "Magpie 200k synthesis-data generated by qwen/qwen2-72b-instruct - chinese", "reason": "This paper provides a dataset used as supplementary alignment data in some of the current research's experiments.  The dataset helps to test the generalisability of the tuning approach, and prevent catastrophic forgetting.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Zhangchen Xu", "paper_title": "Magpie 200k synthesis-data generated by qwen/qwen2-72b-instruct - english", "reason": "Similar to the previous reference, this dataset is used as supplementary alignment data in the experiments.  The dataset also improves the model's generalizability and helps prevent catastrophic forgetting.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Zhangchen Xu", "paper_title": "Magpie: Alignment data synthesis from scratch by prompting aligned llms with nothing", "reason": "This paper introduces the Magpie method for data synthesis, used to generate alignment data for the current experiments.  The method is relevant to the current research as it helps in obtaining alignment data for the experiments.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "An Yang", "paper_title": "Qwen2 technical report", "reason": "This paper presents the Qwen2 model family, which includes the Qwen2-7B-Instruct model used in this research.  It provides background information on the model architecture and capabilities.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Weizhe Yuan", "paper_title": "Following length constraints in instructions", "reason": "This paper directly addresses the challenge of following length constraints in instructions, which is central to the current research's goal. The paper introduces relevant methods and approaches to solving this problem.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Yuze Zhao", "paper_title": "Swift:a scalable lightweight infrastructure for fine-tuning", "reason": "This paper introduces MS-Swift, the training framework used in the current research. Understanding the framework is crucial to understanding the experimental setup and reproducibility of the current work.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Chunting Zhou", "paper_title": "Lima: Less is more for alignment", "reason": "This paper discusses the importance of data quality in model alignment, which is highly relevant to the current research's focus on high-quality data for efficient tuning. The results of this paper provide a theoretical framework for the current paper.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Yushi Bai", "paper_title": "Longwriter-6k dataset", "reason": "This paper describes the initial dataset used in the current research, providing background information on its origin and content. It also highlights the limitations of the original dataset, which is key to understanding the need for data refinement.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "EvalScope", "paper_title": "A open source implementation for longbench-write benchmark", "reason": "This paper describes the open-source implementation used for the LongBench-Write benchmark, which is the evaluation methodology of the current work.  This reference is key for the reproducibility of results.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "ModelScope", "paper_title": "Longwriter-6k-filtered dataset", "reason": "This paper describes the refined dataset created by the authors, which is central to the current research.  The refined dataset is essential to the model training process and the results obtained.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "GLM Team", "paper_title": "GLM-4-9B", "reason": "This reference provides information on the GLM-4-9B model, one of the models used in the experiments.  This is essential context to understand the experimental setup and results.", "section_number": 4}]}