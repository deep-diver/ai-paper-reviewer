[{"figure_path": "https://arxiv.org/html/2502.07531/x1.png", "caption": "Figure 1. VidCRAFT3 is a high-quality image-to-video generation model that supports large object motion, view changes, and strong lighting effects. It offers user-friendly control over camera motion (a trajectory in blue), object motion (sparse trajectories in red), and lighting direction. VidCRAFT3 can take any combination of supported control signals and deliver fine-grained and faithful generation results.", "description": "Figure 1 showcases the capabilities of VidCRAFT3, a novel image-to-video generation model.  It demonstrates the model's ability to generate high-quality videos with control over multiple visual aspects simultaneously. The figure presents five example videos, each illustrating a different combination of controls: (a) camera motion only; (b) object motion only; (c) both camera and object motion; (d) camera motion and lighting direction control; and (e) camera motion, object motion, and lighting direction control.  The controls are visualized using trajectories (blue for camera, red for objects) and arrows indicating lighting direction. This figure highlights VidCRAFT3's capacity for fine-grained control and realistic video generation from a single image, allowing users to manipulate various aspects of the scene independently.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2502.07531/x2.png", "caption": "Figure 2. Architecture of VidCRAFT3 for controlled image-to-video generation. The model builds on Video Diffusion Model (VDM) and consists of three main components: the Image2Cloud module generates high-quality 3D point cloud renderings from a reference image; the ObjMotionNet module encodes object motion represented by sparse trajectories, the output is integrated into the UNet encoder by element-wise addition; the Spatial Triple-Attention Transformer module integrates image, text, and lighting information via parallel cross-attention modules. The model generates video by conditioning on camera motion, object motion, and lighting direction, ensuring realistic and consistent outputs across different modalities.", "description": "VidCRAFT3 architecture diagram.  It shows how the model, based on a Video Diffusion Model (VDM), takes an input image and text description and uses three main components to generate a video.  The Image2Cloud module converts the image into a 3D point cloud, which is used for camera motion control. The ObjMotionNet module processes sparse object motion trajectories, integrating them into the UNet encoder.  The Spatial Triple-Attention Transformer combines image, text, and lighting direction embeddings, using cross-attention layers to generate the video frames.  Crucially, the model generates video frames by conditioning on camera motion, object motion, and lighting direction to make the video more realistic and consistent.", "section": "3 METHOD"}, {"figure_path": "https://arxiv.org/html/2502.07531/x3.png", "caption": "Figure 3. Illustrations of examples from the Video Lighting Direction (VLD) Dataset, showcasing samples with 3D models and backgrounds. (a) Poly Haven-based VLD Samples with HDR backgrounds. (b) BOP-based VLD Samples with textured backgrounds. Each set includes video frames of two samples under two different lighting conditions.", "description": "Figure 3 shows examples from the Video Lighting Direction (VLD) dataset, a synthetic dataset created to provide precise lighting direction annotations.  The figure illustrates how the dataset incorporates both 3D models and backgrounds to create realistic lighting scenarios. Panel (a) displays examples using Poly Haven assets, characterized by HDR (High Dynamic Range) backgrounds. Panel (b) shows examples using BOP (Benchmark for Object Pose Estimation) assets, which feature textured backgrounds.  In both panels, each set shows video frames of two different samples, each rendered under two different lighting conditions, highlighting the variety of lighting situations present within the dataset. This dataset helps train a model capable of understanding and generating videos with realistic lighting effects under different conditions.", "section": "4 DATASET CONSTRUCTION"}, {"figure_path": "https://arxiv.org/html/2502.07531/x4.png", "caption": "Figure 4. Qualitative comparisons with SOTA methods on RealEstate10K.", "description": "Figure 4 presents a qualitative comparison of VidCRAFT3's performance against state-of-the-art (SOTA) methods on the RealEstate10K dataset, focusing on camera motion control.  It visually demonstrates the generated video sequences produced by VidCRAFT3 and the competing models. The figure allows for a direct visual comparison of the quality and accuracy of camera motion control achieved by each method. This comparison highlights VidCRAFT3's ability to produce more realistic and smoother camera movements.", "section": "5 EXPERIMENTS"}, {"figure_path": "https://arxiv.org/html/2502.07531/x5.png", "caption": "Figure 5. Qualitative comparisons with SOTA methods on WebVid-10M.", "description": "Figure 5 presents a qualitative comparison of VidCRAFT3 against state-of-the-art (SOTA) methods for object motion control on the WebVid-10M dataset.  The figure visually demonstrates the performance of each model in generating videos with accurate and realistic object motion.  Each row represents a different video sequence, showing the ground truth (GT) video alongside the results generated by VidCRAFT3 and other SOTA methods. This allows for a visual assessment of each model's ability to faithfully recreate the object's movement based on provided motion signals.", "section": "5. EXPERIMENTS"}, {"figure_path": "https://arxiv.org/html/2502.07531/x6.png", "caption": "Figure 6. Additional experimental results on camera motion control + lighting direction control.", "description": "Figure 6 presents supplementary experimental results demonstrating the combined effects of camera motion and lighting direction control in the VidCRAFT3 model.  The figure displays several image sequences showcasing the consistent and realistic video generation achieved when simultaneously manipulating camera trajectory and lighting direction. Each sequence provides a visual comparison, highlighting the fidelity and control capabilities of the VidCRAFT3 model under varying conditions. The results underscore the model's robustness in handling complex scene dynamics when presented with multiple, coordinated control signals.", "section": "5 Experiments"}, {"figure_path": "https://arxiv.org/html/2502.07531/x7.png", "caption": "Figure 7. Additional experimental results on camera motion control.", "description": "This figure displays qualitative comparisons of video generation results using different methods, specifically focusing on camera motion control. It showcases the generated videos produced by VidCRAFT3 alongside those from other state-of-the-art methods such as CameraCtrl, CamI2V, and MotionCtrl.  The comparison helps to visually demonstrate VidCRAFT3's superior performance in terms of camera motion accuracy, smoothness, and visual realism. The ground truth videos are also included as a benchmark for comparison.", "section": "5 Experiments"}, {"figure_path": "https://arxiv.org/html/2502.07531/x8.png", "caption": "Figure 8. Additional experimental results on camera motion control + object motion control.", "description": "Figure 8 presents a qualitative comparison of video generation results, focusing on the combined control of camera and object motion.  It showcases how different methods compare to VidCRAFT3's performance in creating videos where both camera and object movements are specified by the user. The figure visually demonstrates VidCRAFT3's superiority in generating high-quality, temporally consistent videos with accurate control over both camera trajectories and object motion, surpassing other state-of-the-art methods.", "section": "5 Experiments"}, {"figure_path": "https://arxiv.org/html/2502.07531/x9.png", "caption": "Figure 9. Qualitative results of the ablation study on training strategies with WebVId-10M.", "description": "Figure 9 shows a comparison of video generation results using different training strategies on the WebVid-10M dataset.  The strategies being compared are training with dense trajectories, sparse trajectories, and a combination of both (dense+sparse). Each row displays a sequence of frames, showing the ground truth (GT) alongside the output generated by the model trained with each strategy. The visual differences highlight the effect of different trajectory densities on the quality and accuracy of the generated videos, specifically on the smoothness and fidelity of object motions.", "section": "5.3 Ablation Study"}, {"figure_path": "https://arxiv.org/html/2502.07531/x10.png", "caption": "Figure 10. Qualitative results of the ablation study on lighting embedding integration strategies on VLD.", "description": "This ablation study compares different methods for integrating lighting information into the image-to-video generation model.  It shows the generated videos using three different methods: Text Cross-Attention, Time Embedding, and Lighting Cross-Attention.  The results demonstrate the effectiveness of the Lighting Cross-Attention method in achieving the highest quality in video generation that closely matches the ground truth, as judged by visual inspection.", "section": "5.3 Ablation Study"}, {"figure_path": "https://arxiv.org/html/2502.07531/x11.png", "caption": "Figure 11. Qualitative results of the ablation study on the representation of lighting direction on VLD.", "description": "Figure 11 shows a comparison of video generation results using different lighting direction representations within the VidCRAFT3 model.  The ablation study focuses on the impact of representing the lighting direction using either Spherical Harmonic (SH) encoding or Fourier embedding. The figure visually demonstrates the quality of videos generated using each method, showing how accurately the lighting effects match the ground truth lighting direction.  This comparison highlights the superior performance of SH encoding in terms of visual realism and fidelity to the intended lighting.", "section": "5.3 Ablation Study"}, {"figure_path": "https://arxiv.org/html/2502.07531/x12.png", "caption": "Figure 12. Additional experimental results on object motion control.", "description": "Figure 12 displays further examples showcasing VidCRAFT3's capabilities in controlling object motion during video generation.  Each row presents a comparison between the ground truth video (GT), outputs from competitor models (Image Conductor and Motion-I2V), and VidCRAFT3's generated video. This visual comparison highlights VidCRAFT3's superior performance in terms of accuracy, smoothness, and naturalness of object motion.", "section": "5 Experiments"}, {"figure_path": "https://arxiv.org/html/2502.07531/x13.png", "caption": "Figure 13. Additional experimental results on lighting direction control.", "description": "Figure 13 shows additional examples demonstrating VidCRAFT3's ability to control lighting direction during video generation.  The figure showcases various scenes with different lighting conditions, illustrating the model's capacity to realistically alter the illumination in a video based on user-specified lighting direction parameters.  Each example shows a comparison between the original scene, the scene with user-specified lighting direction changes, and a ground truth example to evaluate the accuracy of the model\u2019s manipulation.", "section": "Experiments"}]