[{"figure_path": "https://arxiv.org/html/2502.13145/x2.png", "caption": "Figure 1: \nComprehensive comparison of mmMamba.\n(a) Our mmMamba can build linear-complexity and hybrid decoder-only VLM by distilling the knowledge in Transformer to Mamba-2.\n(b) By distilling from the quadratic-complexity decoder-only VLM HoVLE, our mmMamba-linear achieves competitive performance against existing linear and quadratic-complexity VLMs with fewer parameters (e.g., 2\u00d7\\times\u00d7 fewer than EVE-7B), while mmMamba-hybrid surpasses them across all benchmarks and approaches the teacher model HoVLE\u2019s performance.\n(c)-(d) We compare the speed and memory of mmMamba-linear and mmMamba-hybrid with the teacher model HoVLE on the same single NVIDIA 4090 GPU. mmMamba-linear maintains consistently low latency and memory usage, while mmMamba-hybrid\u2019s resource consumption scales significantly better than HoVLE. At 103K tokens, mmMamba-linear demonstrates 20.6\u00d7\\times\u00d7 speedup compared to HoVLE and saves 75.8% GPU memory, while mmMamba-hybrid achieves 13.5\u00d7\\times\u00d7 speedup and saves 60.2% GPU memory.", "description": "Figure 1 provides a comprehensive comparison of the proposed multimodal Mamba model. Subfigure (a) illustrates the architecture of mmMamba, highlighting its linear-complexity and hybrid decoder-only VLM design achieved through knowledge distillation from Transformer to Mamba-2. Subfigure (b) presents a performance comparison showing that mmMamba-linear achieves competitive results against existing models, while mmMamba-hybrid outperforms them across various benchmarks. Subfigures (c) and (d) compare the speed and memory usage of mmMamba with the teacher model HoVLE on an NVIDIA 4090 GPU. mmMamba-linear exhibits significantly lower latency and memory consumption than HoVLE, while mmMamba-hybrid scales more efficiently with increasing sequence length.  Specifically, at 103K tokens, mmMamba-linear achieves a 20.6x speedup and 75.8% memory saving, whereas mmMamba-hybrid shows a 13.5x speedup and 60.2% memory saving compared to HoVLE.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2502.13145/x3.png", "caption": "Figure 2: \nInitialize Mamba-2 from Transformer. By comparing the mechanism similarity in Sec.\u00a03, we directly inherit \ud835\udc7eQsubscript\ud835\udc7e\ud835\udc44\\boldsymbol{W}_{Q}bold_italic_W start_POSTSUBSCRIPT italic_Q end_POSTSUBSCRIPT, \ud835\udc7eKsubscript\ud835\udc7e\ud835\udc3e\\boldsymbol{W}_{K}bold_italic_W start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT, \ud835\udc7eVsubscript\ud835\udc7e\ud835\udc49\\boldsymbol{W}_{V}bold_italic_W start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT, \ud835\udc7eOsubscript\ud835\udc7e\ud835\udc42\\boldsymbol{W}_{O}bold_italic_W start_POSTSUBSCRIPT italic_O end_POSTSUBSCRIPT parameters (blue) from trained Transformer layer and carefully initialize the extra parameters (orange) including a\ud835\udc4eaitalic_a, \ud835\udc7e\u03b3subscript\ud835\udc7e\ud835\udefe\\boldsymbol{W}_{\\gamma}bold_italic_W start_POSTSUBSCRIPT italic_\u03b3 end_POSTSUBSCRIPT, \ud835\udc7econvsubscript\ud835\udc7econv\\boldsymbol{W}_{\\text{conv}}bold_italic_W start_POSTSUBSCRIPT conv end_POSTSUBSCRIPT, and \ud835\udc7eGsubscript\ud835\udc7e\ud835\udc3a\\boldsymbol{W}_{G}bold_italic_W start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT in Mamba-2 to initially mimic the Transformer\u2019s behavior, providing a strong foundation for subsequent distillation.", "description": "Figure 2 illustrates the initialization process of the Mamba-2 model parameters using knowledge transferred from a pre-trained Transformer model.  Leveraging the similarities between the Transformer's attention mechanism and the Mamba-2 state space model (SSM), key parameters (WQ, WK, WV, WO) are directly inherited from the Transformer.  Additional SSM-specific parameters (a, W\u03b3, Wconv, WG) are carefully initialized to ensure the Mamba-2 model initially mimics the Transformer's behavior. This initialization serves as a strong foundation for subsequent distillation steps, ensuring a smooth and effective knowledge transfer during the training process.", "section": "4. Method"}]