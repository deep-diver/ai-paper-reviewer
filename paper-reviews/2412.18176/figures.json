[{"figure_path": "https://arxiv.org/html/2412.18176/x1.png", "caption": "Figure 1: Comparison of LLM-based recommendation methods and our Molar. (a) Existing methods prematurely integrate ID and text modalities into the LLM, leading to limited utilization of multimodal content features. (b) Our approach first processes text and non-text modalities through the LLM to generate rich multimodal representations and then incorporates ID information via post-alignment, ensuring a better balance between multimodal content and collaborative signals.", "description": "This figure compares existing LLM-based recommendation methods with the proposed Molar method. Panel (a) illustrates a common approach where user and item IDs and text are directly input into the LLM. This approach often underutilizes multimodal data. Panel (b) shows Molar, which first generates rich item representations using an MLLM processing text and other modalities before incorporating ID information via a post-alignment step. This refined approach better balances multimodal and collaborative signals.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2412.18176/x2.png", "caption": "Figure 2: \nIllustration of the Molar framework. The Multimodal Item Representation Model (MIRM) processes multimodal item information to generate item embeddings, while the Dynamic User Embedding Generator (DUEG) models user embeddings based on interaction histories for next-item prediction. First, MIRM is fine-tuned for multimodal feature alignment. Then, a joint optimization framework integrates ID-based and content-based user embeddings using a contrastive learning mechanism to enhance recommendation performance.", "description": "The figure illustrates the Molar framework, which consists of two main modules: the Multimodal Item Representation Model (MIRM) and the Dynamic User Embedding Generator (DUEG).  MIRM processes various types of item information (text, images, etc.) to create a unified embedding for each item. This process involves a fine-tuning step focusing on aligning multimodal features.  DUEG generates user embeddings based on the user's interaction history. Finally, a joint optimization using contrastive learning integrates ID-based and content-based user embeddings to improve recommendation accuracy.", "section": "3 Methods"}, {"figure_path": "https://arxiv.org/html/2412.18176/extracted/6092461/images/2_GURM.png", "caption": "Figure 3: Performance comparison of different DUEGs. Qwen2vl-2b is used as MIRM for all. The LLM backbone DUEG outperforms traditional DUEGs.", "description": "This figure compares the performance of different Dynamic User Embedding Generators (DUEGs) in a sequential recommendation system.  All models use the same Multimodal Item Representation Model (MIRM), which is Qwen2vl-2b. The results show that the DUEG based on a Large Language Model (LLM) significantly outperforms traditional DUEGs (FPMC, SASRec, GRU4Rec), demonstrating the advantage of using LLMs for user representation in this context.", "section": "4.2 Performance Comparison (RQ1)"}]