[{"figure_path": "https://arxiv.org/html/2412.16429/x1.png", "caption": "Figure 1: An overview of our three-stage human evaluation pipeline and our results for comparing LearnLM with other systems. (1) Learning scenarios are developed that allow raters to role-play specific learners interacting with pairs of AI tutors (2). Grounding material (e.g. an essay, homework problem, diagram, etc.) and System Instructions specific to each scenario are passed as context to each model. The resulting conversation pairs are reviewed by pedagogy experts (3) who answer a range of questions assessing each model on its own as well as their comparative performance. These comparative ratings (on a seven-point -3 to +3 Likert scale) are aggregated (4) to show overall preference for LearnLM over GPT-4o, Claude 3.5, and Gemini 1.5 Pro. See Section 4 for more detailed results.", "description": "This figure presents a three-stage human evaluation pipeline used to compare LearnLM's performance against other large language models (LLMs) in educational settings.  Stage 1 involves creating diverse learning scenarios where human raters simulate learners interacting with pairs of AI tutors. Each scenario includes background materials (like essays or diagrams) and specific system instructions for the AI. Stage 2 consists of collecting the conversations generated by each LLM in response to these scenarios. In the final stage, pedagogy experts assess these conversations, providing both individual ratings and comparative ratings on a seven-point Likert scale (-3 to +3).  These ratings are then aggregated to calculate the overall preference for each LLM, demonstrating LearnLM's superior performance compared to GPT-40, Claude 3.5, and Gemini 1.5 Pro.", "section": "3. Human Evaluation Design"}, {"figure_path": "https://arxiv.org/html/2412.16429/x3.png", "caption": "Figure 2: Workflow to generate conversations based on educational scenarios. A participant enacts conversations with prompted models as defined by scenarios. The participant then fills out a survey capturing quality and preference between models.", "description": "This figure illustrates the three-stage human evaluation pipeline used in the study.  First, scenarios are created that define the context of an interaction between a human learner (played by a participant) and an AI tutor (represented by two different LLMs). These scenarios include specifics such as learner personas, learning goals, and system instructions that specify desired pedagogical behaviors. Second, participants role-play as learners within the defined scenarios, engaging in conversations with each of the AI tutors. Third, raters assess the conversations, providing comparative ratings across different models based on various pedagogical criteria.  This allows quantitative analysis of the relative quality and preference for different LLMs in educational settings.", "section": "3. Human Evaluation Design"}, {"figure_path": "https://arxiv.org/html/2412.16429/x4.png", "caption": "Figure 3: (Top) The specific LLMs compared, along with aggregate statistics across all conversations collected: average number of model turns per conversation and average number of words per turn; (Bottom) Histograms of the number of words used per turn by each model.", "description": "This figure displays a comparison of four large language models (LLMs): LearnLM, Gemini 1.5 Pro, GPT-40, and Claude 3.5 Sonnet.  The top panel presents aggregate statistics from conversations involving these LLMs, specifically the average number of turns per conversation and the average number of words used per turn. The bottom panel provides histograms illustrating the distribution of the number of words used per turn for each LLM, offering a visual representation of the variation in response length for each model.", "section": "3. Human Evaluation Design"}, {"figure_path": "https://arxiv.org/html/2412.16429/x7.png", "caption": "Figure 4: Pedagogy experts\u2019 preferences over LearnLM and other contemporaneous systems (Claude 3.5, GPT-4o, and Gemini 1.5 Pro). The scatterplots represent the underlying distribution of seven-point preference ratings. Given the large number of ratings we collected, these scatterplots proportionally downsample to 500 ratings per measure, color-coded based on the preference scale (dark purple corresponds to strong preference for LearnLM), and randomly positioned around each integer rating for readability. The red points and error bars indicate the estimated mean and its 95% credible interval for each measure. These means are also shown in Figure\u00a01.", "description": "Figure 4 presents a comparison of user preferences for LearnLM against three other large language models: Claude 3.5, GPT-40, and Gemini 1.5 Pro.  Expert raters provided seven-point preference ratings across various pedagogical aspects.  Due to the large number of ratings, the scatter plots display a proportionally downsampled set of 500 ratings per measure for clarity. The color intensity represents the strength of preference for LearnLM (dark purple indicates strong preference).  Red points and error bars show the average preference and 95% credible intervals.  These averages are also summarized in Figure 1.", "section": "3. Human Evaluation Design"}, {"figure_path": "https://arxiv.org/html/2412.16429/x8.png", "caption": "Figure 5: Evaluation of systems on each category of our pedagogy rubric from a 7-point Likert scale (\"Strongly disagree\" to \"Strongly agree\"). Error bars reflect 95% credible intervals from the posterior distrubtion for the mean.", "description": "Figure 5 presents a detailed comparison of four different AI models (LearnLM, GPT-40, Claude 3.5, and Gemini 1.5 Pro) across various pedagogical aspects.  Each model's performance is assessed using a 7-point Likert scale ranging from \"Strongly disagree\" to \"Strongly agree.\" The evaluation covers several key pedagogical criteria, allowing for a nuanced comparison of the models' strengths and weaknesses in different areas of instructional effectiveness. Error bars are included to represent the 95% credible intervals derived from the posterior distribution, adding a layer of statistical confidence to the results and conveying the uncertainty inherent in the evaluation process.", "section": "3. Human Evaluation Design"}, {"figure_path": "https://arxiv.org/html/2412.16429/x11.png", "caption": "Figure 6: Impressions shared by the pedagogy experts role-playing as learners in our pedagogical scenarios. Error bars reflect 95% credible intervals from the posterior distribution for the mean.\nThe rating scales for impression questions (left) were 5-point extent scales (\u201cNot at all\u201d to \u201cExtremely\u201d), and 7-point Likert scales (\u201cStrongly disagree\u201d to \u201cStrongly agree\u201d) for experience questions (right).", "description": "Figure 6 presents the results of human evaluation on AI tutors' impact on learner impressions and experiences. The left side shows impressions using a 5-point scale ('Not at all' to 'Extremely'), while the right side shows experiences using a 7-point Likert scale ('Strongly disagree' to 'Strongly agree'). Each bar represents the average rating, with error bars indicating the 95% credible interval, reflecting the uncertainty in the estimate.  The figure illustrates the relative effectiveness of different AI models in terms of learner interest, perceived competence and warmth of the AI tutor, and willingness to use the AI tutor again.", "section": "3. Human Evaluation Design"}, {"figure_path": "https://arxiv.org/html/2412.16429/x12.png", "caption": "Figure 7: Preferences over LearnLM and other contemporary models (Claude-3.5, GPT-4o, and Gemini 1.5 Pro) according to the pedagogical experts role-playing as learners. The scatterplots represent the underlying distribution of seven-point preference ratings. Given the large number of ratings we collected, these scatterplots proportionally downsample to 500 ratings per measure. The red points and error bars indicate the estimated mean and its 95% credible interval for each measure.", "description": "Figure 7 presents a comparative analysis of user preferences among LearnLM and three other leading language models (Claude-3.5, GPT-40, and Gemini 1.5 Pro) in educational scenarios. Pedagogical experts, acting as learners, provided seven-point Likert scale ratings reflecting their preferences.  Due to the extensive dataset, the scatterplots are created with a proportional downsampling of 500 ratings per assessment category.  The plots visually depict the distribution of these ratings, highlighting the average preference towards LearnLM (indicated by red points) and quantifying the uncertainty using 95% credible intervals (error bars).  The figure provides a clear visual summary of the comparative performance of LearnLM.", "section": "3. Human Evaluation Design"}, {"figure_path": "https://arxiv.org/html/2412.16429/x13.png", "caption": "Figure 8: At the beginning of the pedagogical assessment process, we asked experts to evaluate how closely the human participants in the conversation transcripts followed the scenario instructions (i.e., how effectively they role-played the learner in the scenario) on a seven-point scale. This plot shows the responses grouped and averaged by transcript. These aggregate ratings indicated that the \u201clearner\u201d followed the scenario instructions in 93.4% of conversation transcripts.", "description": "This figure displays the distribution of ratings indicating how well human participants followed the instructions of the scenarios in the conversation collection phase of the study.  Experts evaluated each conversation transcript on a seven-point Likert scale, rating how effectively the participant role-played the designated learner persona within the scenario. The aggregated results show that participants adhered to the scenario instructions in 93.4% of the conversation transcripts.", "section": "3. Human Evaluation Design"}, {"figure_path": "https://arxiv.org/html/2412.16429/x14.png", "caption": "Figure 9: Evaluation of tutor models on specific subdimensions of the \u201cCognitive load\u201d rubric category. Error bars reflect 95% credible intervals from the posterior distribution for the mean.", "description": "This figure displays the performance of different AI tutor models across various aspects contributing to the \"Cognitive Load\" in a learning scenario.  The x-axis lists these specific sub-dimensions of cognitive load, such as appropriate response length, use of analogies, clear communication, and organization of information.  The y-axis represents the score, indicating the model's effectiveness in managing cognitive load along that specific dimension.  The bars represent the average score for each model (LearnLM, GPT-40, Claude 3.5, Gemini 1.5 Pro), and the error bars show the 95% credible interval, reflecting the uncertainty in the estimates.  This visualization helps to understand the strengths and weaknesses of each model in terms of minimizing cognitive load and improving the learning experience.", "section": "3. Human Evaluation Design"}, {"figure_path": "https://arxiv.org/html/2412.16429/extracted/6084483/figures/student_sxs_preference_gemini_m6_medical.png", "caption": "Figure 10: Evaluation of tutor models on specific subdimensions of the \u201cActive learning\u201d rubric category. Error bars reflect 95% credible intervals from the posterior distribution for the mean.", "description": "This figure displays the results of evaluating different large language models (LLMs) as AI tutors, focusing on their ability to inspire active learning.  The evaluation used a rubric with several subdimensions, such as asking questions, providing opportunities for active engagement, and utilizing a Socratic approach. The figure presents the average scores for each LLM on each subdimension, with error bars showing the 95% credible intervals, reflecting the uncertainty in the estimates.  This helps to visualize which models performed better in different facets of active learning.", "section": "3. Human Evaluation Design"}]