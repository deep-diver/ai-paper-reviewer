[{"figure_path": "https://arxiv.org/html/2412.13871/x1.png", "caption": "Figure 1: Comparison of LLaVA-UHD v2 with other MLLMs. (a) MLLMs typically align ViT features to language space using MLPs\u00a0[63] or perceiver re-samplers\u00a0[6, 52], lacking visual granularity. (b) Combining multiple visual encoders is non-universal and computationally intensive. (c) LLaVA-UHD v2 employs the Hiwin transformer to build an inverse feature pyramid and compress it into visual tokens, providing various semantic granularity for language generation.", "description": "This figure compares three different architectures for multimodal large language models (MLLMs): (a) illustrates recent MLLMs that align vision transformer (ViT) features to language space using multilayer perceptrons (MLPs) or perceiver re-samplers, often lacking visual granularity. (b) shows a method combining multiple visual encoders which is non-universal and computationally intensive. (c) presents the proposed LLaVA-UHD v2, which employs a Hiwin transformer to construct an inverse feature pyramid, subsequently compressing it into visual tokens to provide various levels of semantic granularity for enhanced language generation.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2412.13871/x2.png", "caption": "Figure 2: The overall architecture of proposed LLaVA-UHD v2, consisting of a ViT, our hierarchical window transformer (Hiwin transformer), and an LLM. Hiwin transformers process sliced patches and the overview image by capturing inner multi-level representations and compressing them into spatially consistent tokens for a better vision-language alignment.", "description": "LLaVA-UHD v2 processes image slices and an overview image with a CLIP-ViT. The resulting visual features are passed to a hierarchical window transformer (Hiwin Transformer) that builds an inverse feature pyramid and compresses it into visual tokens. These tokens are then fed to an LLM, facilitating vision-language alignment.", "section": "3 Method"}, {"figure_path": "https://arxiv.org/html/2412.13871/x3.png", "caption": "Figure 3: Flowchart of the Joint Bilateral Upsampling (JBU) module, which leverages the image pyramid to guide feature up-sampling, integrating high-frequency information into the up-sampled feature maps.", "description": "The Joint Bilateral Upsampling (JBU) module guides feature upsampling by leveraging the image pyramid, effectively integrating high-frequency details into upsampled feature maps.  This addresses the issue of standard upsampling methods failing to introduce fine-grained details, which are crucial for tasks demanding high-resolution visual information.  The JBU module learns convolutional layers on progressively downsampled image versions. These learned kernels capture local high-frequency textures that guide the upsampling of the initial feature map. Each pixel's value in the upsampled map is a weighted average of values from the original feature map.  These weights are determined by both the spatial distance between pixels and their similarity in terms of local textural patterns captured by the image pyramid. This process results in upsampled feature maps enriched with detailed texture information, making them more suitable for high-resolution tasks.", "section": "3 Method"}, {"figure_path": "https://arxiv.org/html/2412.13871/x4.png", "caption": "Figure 4: The flowchart of hierarchical window attention. Feature maps from different levels of the feature pyramid are adaptively RoI-aligned into sampling features and then concatenated along the length axis to serve as the key for the learnable queries.", "description": "The figure illustrates the hierarchical window attention mechanism. It begins by constructing an inverse feature pyramid from the input image using a vision transformer (ViT) and a learned upsampling module. The feature maps from different levels of this pyramid are then divided into a uniform grid of windows. These windows, which align across different pyramid levels and serve as regions of interest, undergo RoI alignment to generate spatially consistent sampling features. These sampling features from each level are concatenated together lengthwise and used as the key in the cross-attention block. For each grid location, there is a corresponding learnable query vector. Finally, the output of this cross-attention operation is aggregated and projected into a feature map, which serves as the visual representation for the input image.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2412.13871/x6.png", "caption": "Figure 5: Performance on different visual tasks with JBU module and vanilla bilinear interpolation. \u201cOCR\u201d denotes the optical character recognition, \u201cSeg\u201d the Linear probing semantic segmentation, and \u201cCls\u201d the fine-grained classification on SUB-200.", "description": "This figure compares the performance of a vision-language model on three visual tasks -- optical character recognition (OCR), linear probing semantic segmentation (Seg), and fine-grained image classification (Cls) -- when using two different up-sampling methods: bilinear interpolation and the proposed Joint Bilateral Upsampling (JBU) module.  The JBU module consistently outperforms bilinear interpolation across all tasks, suggesting its ability to preserve more detailed visual information during upsampling. Specifically, JBU leads to a 4% improvement in OCR, 4.7% in segmentation, and 3.8% in classification over bilinear interpolation.", "section": "4.4 Analytical Study"}, {"figure_path": "https://arxiv.org/html/2412.13871/x7.png", "caption": "Figure 6: \nQualitative comparison of proposed LLaVA-UHD v2 and advanced MLLMs, including LLaVA-Next, Mini-Gemini, and GPT-4V on high-resolution complex perception tasks, which require the integration of both fine-grained visual information and high-level semantic contexts.", "description": "This figure presents a qualitative comparison showcasing the enhanced capabilities of LLaVA-UHD v2 in handling high-resolution images involving complex perceptual tasks.  It highlights two specific examples (A and B) where LLaVA-UHD v2 demonstrates accurate understanding of image content and question answering compared to other MLLMs like  LLaVA-Next, Mini-Gemini, and GPT-4V. These tasks necessitate the model to effectively integrate both fine-grained details (e.g., text in images, small objects) with high-level semantic context.", "section": "4.3 Main Performance"}, {"figure_path": "https://arxiv.org/html/2412.13871/x8.png", "caption": "Figure 7: Activation response of specific textual tokens to different visual feature levels. Red circles highlight the obvious difference between levels. (Best viewed in color and zoomed-in)", "description": "This figure visualizes how different textual tokens activate differently across various levels of the visual feature pyramid.  Red circles emphasize noticeable differences in activation patterns between these levels.  Essentially, it demonstrates how certain visual features are captured more strongly at different scales (levels of the pyramid). For instance, finer details, such as text, might activate more strongly at higher resolution levels, while broader semantic concepts may have stronger activations at lower resolution levels.  This multi-level analysis helps in understanding how the model connects language to visual information at varying granularities.", "section": "4.5 Visualization Analysis"}, {"figure_path": "https://arxiv.org/html/2412.13871/x9.png", "caption": "Figure 8: Qualitative comparison on high-resolution dense perception task which requires the capabilities of fine-grained details perception.", "description": "Qualitative comparison of LLaVA-UHD v2 with LLaVA-Next, Mini-Gemini, and GPT-4V on high-resolution, complex perceptual tasks requiring both fine-grained visual detail and higher-level semantic understanding, demonstrating superior performance in accurately extracting information from dense text and differentiating targets from similar objects (e.g., identifying the TV program start time, the show date, workout duration, and sale price).", "section": "4.4 Analytical Study"}, {"figure_path": "https://arxiv.org/html/2412.13871/x10.png", "caption": "Figure 9: Qualitative comparison on high-resolution fine-grained perception task which requires robust fine-grained visual texture perception capabilities.", "description": "LLaVA-UHD v2 and GPT-4V are able to accurately perceive fine-grained details like the beginning time of the television program or small numbers on jerseys and buses, outperforming other models which fail to locate targets or discern them from similar objects.", "section": "4.5 Visualization Analysis"}, {"figure_path": "https://arxiv.org/html/2412.13871/x11.png", "caption": "Figure 10: Qualitative comparison on high-resolution spatial perception which necessitates the capabilities of high-level spatial contexts.", "description": "LLaVA-UHD v2 and GPT4V are compared with LLaVA-Next and Mini-Gemini on their spatial perception abilities. The figure presents four examples where models need to extract objects and their spatial relationship in high-resolution images. LLaVA-UHD v2 and GPT4V demonstrate better spatial understanding compared to the other two models.", "section": "4.5 Visualization Analysis"}, {"figure_path": "https://arxiv.org/html/2412.13871/x12.png", "caption": "Figure 11: PCA visualization of the up-sampled features by JBU module on nature scene. With hierarchical supervision, the high-resolution features (8\u00d78\\times8 \u00d7) could clearly depict object boundary and text appearance. (Best viewed in color and zoomed in)", "description": "This figure shows a PCA visualization of upsampled features produced by the Joint Bilateral Upsampling (JBU) module applied to a natural scene.  Three different upsampling methods are compared: bilinear interpolation, JBU without hierarchical supervision, and JBU with hierarchical supervision.  The goal is to demonstrate the effect of these methods on high-resolution (8x the original resolution) feature representation. The figure visually illustrates how hierarchical supervision, when used with the JBU module, helps maintain clearer object boundaries and sharper text details compared to the other two methods.  Without hierarchical supervision, the JBU-upsampled features appear somewhat blurry. Bilinear interpolation results in the least clear output.", "section": "B.2.1 Enhanced high-resolution features"}]