[{"content": "| Method | #Data | MaxRes. | #FLOPs. | Avg. | OCR & Chart | Knowledge | General | Vision Spatial | High Res | \n|---|---|---|---|---|---|---|---|---|---|---| \n| Qwen-VL [10] | 1.45B | 448\u00d7448 | 4.0T | 56.9 | 62.6 | 48.8 | **66.3** | 61.5 | 57.7 | 68.2 | 35.9 | 57.5 | 65.4 | 61.8 | 74.4 | 49.3 | 30.5 |\n| MiniGPT-v2 [16] | 326M | 448\u00d7448 | 4.0T | - | - | 15.7 | - | - | - | - | - | 60.3 | - | - | - | - | - |\n| mPLUG-Owl2 [105] | 401M | 448\u00d7448 | 1.7T | - | - | - | - | 58.2 | - | 68.7 | - | 56.1 | 57.8 | 64.5 | 72.5 | - | - |\n| UReader [104] | 86M | 896\u00d71120 | 20.3T | - | 65.4 | - | 59.3 | 57.6 | - | - | - | - | - | - | - | - | - |\n| LLaVA-1.5 [63] | 1.22M | 336\u00d7336 | 8.0T | 49.0 | 21.8 | 31.8 | 17.8 | 45.5 | 55.5 | 66.8 | 37.0 | 62.0 | 65.8 | 66.5 | 75.3 | 54.8 | 36.1 |\n| SPHINX-2k [60] | 1.01B | 762\u00d7762 | 42.2T | - | - | - | - | 61.2 | - | <u>70.6</u> | - | 63.1 | **71.6** | 65.9 | 73.6 | - | - |\n| SPHINX-X [28] | 15.3M | 448\u00d7448 | 21.3T | - | 56.3 | - | 39.7 | 58.1 | 63.0 | 70.4 | - | 56.2 | 68.8 | 57.9 | 63.0 | - | - |\n| LLaVA-HR [73] | 1.22M | 1024\u00d71024 | 24.3T | - | - | - | - | 67.1 | - | 65.1 | - | 64.2 | 64.2 | - | <u>77.7</u> | - | - |\n| VILA [56] | 51M | 336\u00d7336 | 8.2T | - | - | - | - | 64.4 | - | 68.2 | - | 62.3 | 61.1 | <u>68.9</u> | 76.7 | - | - |\n| Honey-bee [15] | 52.5M | 336\u00d7336 | 2.6T | - | - | - | - | - | - | - | 35.3 | - | 64.5 | **70.1** | **79.2** | - | - |\n| Mini-Gemini [54] | 3.0M | 672\u00d7672 | 54.6T | 59.4 | 61.9 | 47.7 | 47.4 | 65.2 | <u>68.2</u> | 69.6 | 36.8 | <u>64.5</u> | 66.9 | 65.8 | 77.3 | 51.1 | <u>50.1</u> |\n| Monkey [55] | 1.40B | 896\u00d71344 | 28.0T | 59.2 | <u>66.5</u> | 51.4 | <u>65.1</u> | 67.6 | 62.6 | 69.4 | **38.9** | 60.7 | 64.3 | 59.8 | 73.6 | 51.6 | 38.0 |\n| LLaVA-Next [62] | 1.34M | 672\u00d7672 | 44.4T | 61.0 | 63.6 | <u>53.2</u> | 54.3 | 64.9 | 67.0 | 70.1 | 35.8 | 64.2 | <u>70.2</u> | 67.4 | 76.0 | <u>57.8</u> | 47.9 |\n| **LLaVA-UHD v2 (ours)** | 1.42M | 1008\u00d7672 | 17.5T | **63.2** | **68.1** | **53.9** | 64.5 | **67.6** | **70.5** | **71.3** | <u>38.2</u> | **65.4** | 70.0 | 68.2 | 74.7 | **58.2** | **51.5** |", "caption": "Table 1: Main performance on popular benchmarks. For a fair comparison, we only report the method using 7B level LLM (e.g.formulae-sequence\ud835\udc52\ud835\udc54e.g.italic_e . italic_g ., Vicuna-7B).\n#Data denotes the volume of overall data during MLLM pre-training and supervised fine-tuning. \u201cMaxRes.\u201d is the maximum accessible resolution of MLLM. \u201cAvg.\u201d: average results of 13 benchmarks. \u201cVQAD: DocVQA. \u201cBenchOCR\u201d: OCR-Bench. \u201cVQAC\u201d: ChartQA. \u201cVQAT\u201d: TextVQA. \u201cSQA\u201d: Science-QA. \u201cMMMUv\u201d: MMMU-val. \u201cSEEDI\u201d: SEED-Image. \u201cMMEP\u201d: perception sub-set of MME. \u201cRWQA\u201d: RealWorldQA. \u201cBenchHR\u201d: HR-Bench.", "description": "This table presents a performance comparison of various Multimodal Large Language Models (MLLMs) on a set of 14 benchmark datasets, encompassing tasks like Visual Question Answering (VQA), Optical Character Recognition (OCR), and high-resolution image understanding. The comparison focuses on models using a 7B parameter Large Language Model (LLM), such as Vicuna-7B. The table includes metrics like the total training data size, maximum supported image resolution, computational cost (#FLOPs), and average performance across 13 benchmarks, along with individual scores on datasets like DocVQA, OCR-Bench, ChartQA, TextVQA, ScienceQA, MMMU, SEED-Image, a subset of MME focused on perception, RealWorldQA, and HR-Bench.", "section": "4.3 Main Performance"}, {"content": "| Method | Average | OCR & Chart | Knowledge | General | Vision Spatial | High Res. |\n|---|---|---|---|---|---|---|\n| LLaVA-UHD [31] | 58.0 | 56.7 | 55.4 | 63.8 | 54.4 | 45.6 |\n|  |  | 40.9 | 70.7 | 65.6 | 68.3 |  |\n|  |  | 56.3 | 37.0 | 64.8 |  |  |\n|  |  | 62.2 |  | 70.0 |  |  |\n| + *JBU module* | 60.0 | 60.2 | 57.8 | 64.0 | 51.9 | 43.9 |\n|  |  | 50.4 | 70.5 | 66.7 | 72.3 |  |\n|  |  | 60.4 | 38.2 | 65.6 |  |  |\n|  |  | 67.1 |  | 71.2 |  |  |\n| + *HFP integration* | 61.5 | 65.0 | 58.1 | 64.6 | 55.5 | 48.9 |\n|  |  | 51.3 | 69.2 | 67.4 | 73.3 |  |\n|  |  | 62.5 | 38.9 | 65.5 |  |  |\n|  |  | 68.5 |  | 73.0 |  |  |\n| + *Token organization* | 61.7 | 66.0 | 59.4 | 64.0 | 56.9 | 49.0 |\n|  |  | 50.1 | 69.8 | 67.4 | 74.0 |  |\n|  |  | 62.8 | 37.6 | 66.1 |  |  |\n|  |  | 66.8 |  | 73.6 |  |  |\n| \u0394 | +3.7 | +9.3 | +4.0 | +0.2 | +2.5 | +3.4 |\n|  |  | +9.2 | -0.9 | +1.8 | +5.7 |  |\n|  |  | +6.5 | +0.6 | +1.3 |  |  |\n|  |  | +4.6 |  | +3.6 |  |  |", "caption": "Table 2: Ablation studies of modules in our proposed method. \u201cHFP\u201d is the abbreviation of high-resolution feature pyramid. \u201c\u0394\u0394\\Deltaroman_\u0394\u201d denotes the overall improvement compared to the baseline. REC reports the average accuracy of RefCOCO/g/+.", "description": "This table presents the ablation study results for evaluating the effectiveness of different modules within the LLaVA-UHD v2 model, in particular, the JBU module, High-Resolution Feature Pyramid (HFP) integration, and spatially-consistent token organization. It compares their performance against the baseline LLaVA-UHD model on a variety of visual tasks, including OCR-related tasks, knowledge-based VQA, general vision benchmarks, spatial understanding and high-resolution image perception. The metrics include average accuracy, scores on specific datasets like DocVQA, BenchOCR, and average accuracy on RefCOCO benchmarks.  \u201c\u0394\u201d represents the overall performance improvement relative to the LLaVA-UHD baseline.", "section": "4 Experiment"}, {"content": "| Method | Average | MMEP | GQA | AI2D | VQAC |VQAT |VQAD | BenchHR |\n|---|---|---|---|---|---|---|---|---| \n| LLaVA-UHD | 58.6 | 70.0 | 63.8 | 55.4 | 56.3 | 62.2 | 56.7 | 45.6 |\n| *w. ConvNext* | 59.7 | 68.2 | 62.7 | 55.6 | 61.8 | 63.5 | 61.8 | 44.0 |\n| *w. DeConv.* | 61.7 | 71.2 | 64.2 | 57.4 | 61.8 | 67.8 | 63.4 | 46.3 |\n| *w. Bilinear* | 62.0 | 72.0 | 64.5 | 57.8 | 62.2 | 67.6 | 63.7 | 46.5 |\n| *w. JBU module* | 63.0 | 73.0 | 64.6 | 58.3 | 62.5 | 68.5 | 65.0 | 48.9 |", "caption": "Table 3: Comparison of different methods for feature pyramid construction. \u201cConvNext\u201d means we replace the CILP-ViT with CLIP-ConvNext\u00a0[68] as visual encoder and directly use the feature maps from multiple stages as the final hierarchical feature pyramid.", "description": "This table compares different methods for constructing a feature pyramid for visual encoding in multimodal large language models (MLLMs). It explores replacing the standard CLIP-ViT with CLIP-ConvNext and directly using its multi-stage feature maps as the hierarchical feature pyramid. It also investigates using deconvolution and bilinear interpolation for feature upsampling, alongside the proposed JBU module, and evaluates their impact on performance across various tasks including general knowledge VQA, OCR-based VQA, and high-resolution image perception.", "section": "4.4 Analytical Study"}, {"content": "| Method | Period(h) | Latency(s) | Memory(G) | Average | MME<sup><span class=\"ltx_text ltx_font_italic\">P</span></sup> | GQA | AI2D | VQA<sup><span class=\"ltx_text ltx_font_italic\">C</span></sup> | VQA<sup><span class=\"ltx_text ltx_font_italic\">T</span></sup> | VQA<sup><span class=\"ltx_text ltx_font_italic\">D</span></sup> |\n|---|---|---|---|---|---|---|---|---|---|---| \n| *Pyramid* | 62.4 | 1.26 | 60.3 | 62.4 | 69.0 | 60.8 | 57.3 | 60.7 | 67.5 | 58.9 |\n| *Fix [3<math alttext=\"x\" class=\"ltx_Math\" display=\"inline\"><semantics><mo xref=\"S4.T4.21.5.1.1.1.1.1.1.m1.1.1.cmml\">\u00d7</mo><annotation-xml encoding=\"MathML-Content\"><times /></annotation-xml><annotation encoding=\"application/x-tex\">\\times</annotation><annotation encoding=\"application/x-llamapun\">\u00d7</annotation></semantics></math>3]* | 26.9 | 0.62 | 41.7 | 64.6 | 73.8 | 63.9 | 58.8 | 60.9 | 66.2 | 63.8 |\n| *Selective* | 27.7 | 0.54 | 39.4 | 65.3 | 73.0 | 64.6 | 58.3 | 62.5 | 68.5 | 65.0 |", "caption": "Table 4: Comparison of different choice of grid sizes on performance and efficiency. \u201cPyramid\u201d means the feature grids from different levels form a region-level feature pyramid, e.g.formulae-sequence\ud835\udc52\ud835\udc54e.g.italic_e . italic_g ., [2\u00d7\\times\u00d73] for level-0, [4\u00d7\\times\u00d76] for level-1, [8\u00d7\\times\u00d712] for leval-2. \u201cFix\u201d represents all feature maps are pooled into a 3\u00d7\\times\u00d73 feature grid. We measure the training period on 8\u00d7\\times\u00d7A100s, the latency on an A100 with a 1008\u00d7\\times\u00d7672 image, and the GPU memory on 8\u00d7\\times\u00d7A100s with 1 image per GPU in supervised fine-tune phase.", "description": "This table (Table 4) compares different grid sizes for the RoI-Align operation within the hierarchical window attention mechanism, evaluating their impact on performance and efficiency.  The \"Pyramid\" method uses a multi-level grid where the size increases with the feature level (e.g., 2x3 for level 0, 4x6 for level 1), creating a region-level feature pyramid. The \"Fix\" method uses a fixed 3x3 grid for all feature levels. The table reports training time (Period on 8xA100 GPUs), inference latency (Latency on a single A100 for a 1008x672 image), and GPU memory usage (Memory on 8xA100 GPUs with one image per GPU) during the supervised fine-tuning stage of the MLLM, as well as performance metrics (Average score across tasks, MMEP, GQA, AI2D, VQAC, VQAT, and VQAD) to show the effectiveness of different grid options.", "section": "4.4 Analytical Study"}, {"content": "| Data | Size | Response formatting prompts |\n|---|---|---|\n| LLaVA [63] | 158K | \u2013 |\n| ShareGPT [90] | 40K | \u2013 |\n| VQAv2 [29] | 83K | Answer the question using a single word or phrase. |\n| GQA [38] | 72K |  |\n| OKVQA [75] | 9K |  |\n| OCRVQA [82] | 80K |  |\n| DocVQA [95] | 15K |  |\n| ChartQA [76] | 20K |  |\n| A-OKVQA [88] | 66K | Answer directly with the option\u2019s letter from the given choices. |\n| DVQA [41] | 20K | \u2013 |\n| TextCaps [92] | 22K | Provide a one-sentence caption for the provided image. |\n| ShareGPT4V [18] | 55K | \u2013 |\n| AI2D [43] | 3K | \u2013 |\n| LAION-GPT4V [3] | 11K | \u2013 |\n| SythDog-EN [46] | 40K | \u2013 |\n| LRV-Instruct [61] | 30K | \u2013 |\n| RefCOCO | 48K |  |\n| [42, 74] |  | Provide a short description for this region. _(for Region Caption)_ |\n| VG [48] | 86K | Provide the bounding box coordinate of the region this sentence describes. _(for Referring Expression Comprehension)_ |\n| Total | 858K |  |", "caption": "Table 5: Detailed composition of our 858k-mixed dataset.", "description": "This table details the composition of the 858k dataset used for supervised fine-tuning of the Multimodal Large Language Model (MLLM).  It lists various data sources, their sizes, and the prompt templates used for response formatting during training.", "section": "4.1 Implementation Details"}, {"content": "| Level | Period(h) | Memory(G) | Average | GQA | SQA | REC | VQAC | VQAT | ESTVQA | MMEP |\n|---|---|---|---|---|---|---|---|---|---|---| \n| *0,2* | 27.7 | 41.9 | 63.4 | 63.9 | 69.5 | 71.5 | 60.5 | 66.5 | 40.6 | 71.0 |\n| *0,1,2* | 28.0 | 41.9 | 63.7 | 63.8 | 70.2 | 71.8 | 60.5 | 66.9 | 40.8 | 72.1 |\n| *0,1,2,3* | 45.6 | 53.0 | 63.8 | 64.4 | 69.3 | 72.6 | 60.7 | 66.4 | 41.6 | 71.4 |\n| *0,1,2,3* (w/o HS.) | 45.6 | 52.6 | 62.4 | 63.6 | 69.8 | 67.1 | 57.8 | 66.5 | 39.9 | 72.0 |", "caption": "Table 6: Comparison of different choices of feature level on performance and efficiency. \u201cHS.\u201d: hierarchical supervision. ESTVQA\u00a0[101] is a VQA benchmark focusing on scene text recognition.", "description": "This table presents the impact of different feature level choices on model performance and efficiency. It explores configurations using level 0, 2; levels 0, 1, 2; and levels 0, 1, 2, and 3 of the feature pyramid.  A configuration without hierarchical supervision is included for levels 0, 1, 2, and 3. The metrics include efficiency measurements like training period and memory usage, as well as performance scores on various VQA benchmarks such as Average, GQA, SQA, REC, VQAC, VQAT, ESTVQA, and MMEP. The table demonstrates how different feature levels and hierarchical supervision affect model performance and resource consumption.", "section": "4.4 Analytical Study"}]