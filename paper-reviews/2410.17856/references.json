{"references": [{" publication_date": "2023", "fullname_first_author": "O. J. Achiam", "paper_title": "Gpt-4 technical report", "reason": "This paper is foundational for the field of large language models (LLMs) and their capabilities, especially in relation to vision and language understanding.  The GPT-4 model's performance is crucial for evaluating the high-level reasoning aspects of hierarchical architectures like ROCKET-1. It's referenced in the Introduction for its influence on the vision language capabilities used in the architecture.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "B. Baker", "paper_title": "Video pretraining (vpt): Learning to act by watching unlabeled online videos", "reason": "This paper introduces the Video Pretraining (VPT) model, a foundational model used as a baseline for comparison in the experimental results section. VPT is a strong baseline for embodied agents, highlighting the effectiveness of VPT in learning actions directly from visual input. The comparison with VPT helps establish the superiority of ROCKET-1 in solving complex tasks.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "A. Brohan", "paper_title": "Rt-2: Robotics transformer for real-world control at scale", "reason": "This work explores end-to-end vision-language-action models, providing a contrasting approach to ROCKET-1's hierarchical design.  Comparing ROCKET-1 against RT-2's end-to-end approach in the introduction highlights the advantages of a hierarchical architecture and the need for a different approach to address scalability and resource-intensive training data issues.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "S. Cai", "paper_title": "Groot: Learning to follow instructions by watching gameplay videos", "reason": "This paper's focus on instruction following using video data is highly relevant to ROCKET-1's objective of handling complex tasks.  Understanding GROOT-1 helps define the capabilities needed for the high-level reasoner to complement the visual-temporal context prompting in ROCKET-1.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "S. Cai", "paper_title": "GROOT-1.5: Learning to follow multi-modal instructions from weak supervision", "reason": "The use of multi-modal instructions in this paper is a key advancement for complex task execution.  Referencing GROOT-1.5 helps to show how the methodology in ROCKET-1 effectively adapts to the complexities of open-world interaction.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Y. Cheng", "paper_title": "Exploring large language model based intelligent agents: Definitions, methods, and prospects", "reason": "This paper offers a broad overview of the field of large language model (LLM)-based agents, providing context for the advancements in this research field and placing ROCKET-1 in this broader context. The introduction uses this paper to define and explain the capabilities and scope of LLM usage in robotics.", "section_number": 1}, {" publication_date": "2019", "fullname_first_author": "Z. Dai", "paper_title": "Transformer-xl: Attentive language models beyond a fixed-length context", "reason": "This paper introduces the TransformerXL model, which is a core component of ROCKET-1's architecture.  The TransformerXL's ability to handle long sequences effectively is crucial for processing the visual-temporal context information provided by ROCKET-1's inputs.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "D. Driess", "paper_title": "Rt-2: Vision-language-action models transfer web knowledge to robotic control", "reason": "This paper discusses another approach to end-to-end vision-language-action (VLA) models for robot control, which is relevant to the introduction's discussion of hierarchical agents. The introduction contrasts this approach with ROCKET-1 to provide a better understanding of the different methodologies.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "M. Deitke", "paper_title": "Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models", "reason": "This work is a crucial reference for Molmo, a large language model used in the hierarchical architecture of ROCKET-1 for generating object interaction cues.  The use of Molmo is discussed in the methods section for its role in the high-level reasoning and segmentation annotation generation.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "A. Kirillov", "paper_title": "Segment anything", "reason": "This paper introduces the Segment Anything Model (SAM), a crucial component of ROCKET-1. SAM's ability to segment objects in images and videos is vital to ROCKET-1's visual-temporal context prompting method.  SAM-2 builds on the SAM architecture and extends it to a temporal domain, therefore it's crucial for understanding the functionalities of the proposed methodology.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "S. Lifshitz", "paper_title": "Steve-1: A generative model for text-to-behavior in minecraft", "reason": "STEVE-1 is a key baseline for comparison in the results section.  The comparison with STEVE-1 helps to show the advantages of ROCKET-1 in the context of hierarchical agent architectures for long horizon tasks.  STEVE-1's limitations in spatial understanding and zero-shot generalization further highlight ROCKET-1's improvements.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "N. Ravi", "paper_title": "Sam 2: Segment anything in images and videos", "reason": "This paper presents SAM-2, an extension of SAM to the temporal domain, which is a crucial component in ROCKET-1's backward trajectory relabeling pipeline. SAM-2's ability to track objects over time is essential for efficient training data generation, which is a core feature of the ROCKET-1 method.", "section_number": 3}, {" publication_date": "2022", "fullname_first_author": "M. Shridhar", "paper_title": "Cliport: What and where pathways for robotic manipulation", "reason": "CLIPort, another pick-and-place robotic manipulation method, helps to provide a basis for comparison in the related works section.  Highlighting CLIPort's limitations further emphasizes the novelty and benefits of ROCKET-1's visual-temporal context prompting method.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "G. Team", "paper_title": "Gemini: a family of highly capable multimodal models", "reason": "The Gemini model is a powerful multimodal model that provides a strong foundation for the vision-language capabilities of ROCKET-1's high-level reasoning components.  Its advanced capabilities in multi-modal understanding and generation are critical for the success of the proposed method.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Z. Wang", "paper_title": "Groot: Learning to follow instructions by watching gameplay videos", "reason": "This paper provides a strong baseline for comparison in the experimental results section. It directly addresses the task of instruction following, a key aspect of the tasks tackled by ROCKET-1. The comparison helps show the relative success of the ROCKET-1 approach in terms of performance improvement.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Z. Wang", "paper_title": "Open-world multi-task control through goal-aware representation learning and adaptive horizon prediction", "reason": "This paper is highly relevant to the overall context of handling open-world, multi-task challenges, which directly relates to the motivation and objectives of ROCKET-1 in the introduction. The comparison of methods helps emphasize the contributions made by the paper.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Z. Wang", "paper_title": "Describe, explain, plan and select: interactive planning with llms enables open-world multi-task agents", "reason": "This paper highlights the potential of LLMs in multi-task planning, showing that language-based approaches can be effective in open-world scenarios.  This is directly relevant to the discussion of hierarchical architectures in the introduction, contrasting the language-based approach with ROCKET-1\u2019s use of visual cues.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Z. Wang", "paper_title": "Unified vision-language-action tokenization enables open-world instruction following agents", "reason": "This paper's exploration of unified vision-language-action tokenization directly addresses the challenges that ROCKET-1 seeks to overcome. The comparison of this tokenization strategy with the visual-temporal context approach helps to show the strengths and weaknesses of the approaches.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Z. Wang", "paper_title": "Jarvis-1: Open-world multi-task agents with memory-augmented multimodal language models", "reason": "This paper explores integrating multimodal language models with memory augmentation for enhanced performance in multi-task environments, which is an area closely related to the advancements presented in ROCKET-1's design. The related work section uses this paper to highlight improvements on memory and goal awareness.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "E. Zhou", "paper_title": "Minedreamer: Learning to follow instructions via chain-of-imagination for simulated-world control", "reason": "MineDreamer is another important baseline for comparison, particularly in its use of future image generation as a communication mechanism between a high-level reasoner and a low-level policy.  MineDreamer's results are compared to ROCKET-1 to show the advantages of visual-temporal context prompting.", "section_number": 4}]}