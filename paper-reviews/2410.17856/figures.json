[{"figure_path": "2410.17856/figures/figures_1_0.png", "caption": "Figure 1 | Our pipeline solves creative tasks, such as get the obsidian in the original Minecraft version, using the action space identical to human players (mouse & keyboard). We present a novel instruction interface, visual-temporal context prompting, under which we learn a spatial-sensitive policy, ROCKET-1. VLMs identify regions of interest within each observation, effectively guiding ROCKET-1 interactions.", "description": "The figure illustrates the ROCKET-1 pipeline solving a creative task in Minecraft by using visual-temporal context prompting to guide interactions.", "section": "Introduction"}, {"figure_path": "2410.17856/figures/figures_3_0.png", "caption": "Figure 2 | Different pipelines in solving embodied decision-making tasks. (a) End-to-end pipeline modeling token sequences of language, observations, and actions. (b) Language prompting: VLMs decompose instructions for language-conditioned policy execution. (c) Latent prompting: maps discrete behavior tokens to low-level actions. (d) Future-image prompting: fine-tunes VLMs and diffusion models for image-conditioned control. (e) Visual-temporal prompting: VLMs generate segmentations and interaction cues to guide ROCKET-1.", "description": "Figure 2 illustrates five different pipelines for embodied decision-making, highlighting the differences in how they connect vision-language models (VLMs) to low-level policies.", "section": "1. Introduction"}, {"figure_path": "2410.17856/figures/figures_4_0.png", "caption": "Figure 3 | ROCKET-1 architecture. ROCKET-1 processes interaction types (c), observations (o), and object segmentations (m) to predict actions (a) using a causal transformer. Observations and segmentations are concatenated and passed through a visual backbone for deep fusion. Interaction types and segmentations are randomly dropped with a set probability during training.", "description": "Figure 3 illustrates the architecture of ROCKET-1, showing how it processes interaction types, observations, and object segmentations to predict actions using a causal transformer.", "section": "3. Methods"}, {"figure_path": "2410.17856/figures/figures_5_0.png", "caption": "Figure 4 | Trajectory relabeling pipeline in Minecraft. A bounding box and point selection are applied to the image center in the frame preceding the interaction event to identify the interaction object. SAM-2 is then run in reverse temporal order for a specified duration, with the interaction type remaining consistent throughout.", "description": "The figure illustrates the backward trajectory relabeling pipeline in Minecraft, showing how SAM-2 is used to generate object segmentations for training ROCKET-1.", "section": "3. Methods"}, {"figure_path": "2410.17856/figures/figures_6_0.png", "caption": "Figure 5 | A hierarchical agent structure based on our proposed visual-temporal context prompting. A GPT-40 model decomposes complex tasks into steps based on the current observation, while the Molmo model identifies interactive objects by outputting points. SAM-2 segments these objects based on the point prompts, and ROCKET-1 uses the object masks and interaction types to make decisions. GPT-40 and Molmo run at low frequencies, while SAM-2 and ROCKET-1 operate at the same frequency as the environment.", "description": "The figure illustrates the hierarchical agent architecture of ROCKET-1, showing how GPT-40, Molmo, SAM-2, and ROCKET-1 work together to solve complex tasks using visual-temporal context prompting.", "section": "Integration with High-level Reasoner"}, {"figure_path": "2410.17856/figures/figures_7_0.png", "caption": "Figure 6 | A benchmark for evaluating open-world interaction capabilities of agents. The benchmark contains six interaction types in Minecraft, totaling 12 tasks. Unlike previous benchmarks, these tasks emphasize interacting with objects at specific spatial locations. For example, in \u201chunt the sheep in the right fence,\u201d the task fails if the agent kills the sheep on the left side. Some tasks, such as \u201cplace the oak door on the diamond block,\u201d never appear in the training set. It is also designed to evaluate zero-shot generalization capabilities.", "description": "The figure shows a benchmark of 12 tasks in Minecraft designed to evaluate the open-world interaction capabilities of agents, emphasizing spatial reasoning and zero-shot generalization.", "section": "4. Results and Analysis"}, {"figure_path": "2410.17856/figures/figures_8_0.png", "caption": "Figure 7 | Screenshots of our agent when completing long-horizon tasks.", "description": "Figure 7 shows screenshots of the ROCKET-1 agent successfully completing several long-horizon tasks in Minecraft, showcasing its ability to handle complex, multi-step processes.", "section": "4.3 Performance on Long-Horizon Tasks"}]