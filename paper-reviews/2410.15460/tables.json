[{"figure_path": "2410.15460/tables/table_7_0.html", "caption": "Figure 4: Regular finetuning vs. SeND on two datasets. (a) presents the results of training Pythia 1B on HELM with regular finetuning and SeND. (b) uses the same training setup as (a), but the LLM is trained on MedHALT 2k. In both plots, performance is reported as the average EES over 5 runs on the validation set. Models are trained until loss convergence. Training with SeND shows a more controlled reduction in EES compared to regular finetuning, suggesting that SeND optimizes for hallucinations as well as loss, with less overall confidence variability during training. Since finetuning without dropout consistently achieves better factual consistency than random dropout in our experiments, we compare SeND directly with standard finetuning.", "description": "This figure compares the performance of regular finetuning versus SeND (Sensitive Neuron Dropout) on two datasets (HELM and MedHALT) by plotting the average EES (Efficient EigenScore) over five runs, showing SeND's superior control over hallucination and loss.", "section": "4 PERFORMANCE OF SEND ON PYTHIA 1B"}, {"figure_path": "2410.15460/tables/table_9_0.html", "caption": "Algorithm 2 Sensitive Neuron Dropout", "description": "Algorithm 2 outlines the steps involved in the Sensitive Neuron Dropout (SeND) training process, detailing the iterative procedure of training, variability calculation, sensitive neuron identification, and neuron dropping to mitigate hallucination.", "section": "4 SENSITIVE NEURON DROPOUT (SEND)"}]