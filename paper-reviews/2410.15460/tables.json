[{"figure_path": "2410.15460/tables/table_7_0.html", "caption": "Figure 4: Regular finetuning vs. SeND on two datasets. (a) presents the results of training Pythia 1B on HELM with regular finetuning and SeND. (b) uses the same training setup as (a), but the LLM is trained on MedHALT 2k. In both plots, performance is reported as the average EES over 5 runs on the validation set. Models are trained until loss convergence. Training with SeND shows a more controlled reduction in EES compared to regular finetuning, suggesting that SeND optimizes for hallucinations as well as loss, with less overall confidence variability during training. Since finetuning without dropout consistently achieves better factual consistency than random dropout in our experiments, we compare SeND directly with standard finetuning.", "description": "This figure compares the performance of regular finetuning versus SeND (Sensitive Neuron Dropout) on two datasets (HELM and MedHALT) in terms of EigenScore (EES) reduction during training.", "section": "4 PERFORMANCE OF SEND ON PYTHIA 1B"}]