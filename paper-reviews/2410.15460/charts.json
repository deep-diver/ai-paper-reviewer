[{"figure_path": "2410.15460/charts/charts_4_0.png", "caption": "Figure 1: Visualization of Oscillatory Behavior Across Varying LLM Sizes. Hallucination metrics are evaluated at equidistant checkpoints of the Pythia models, with sizes 70M, 160M, 410M, 1B, 1.4B, 2.8B, 6.9B, 12B. Part (a) presents the performance of the Pythia models under the SelfCheckGPT metric. Average performance is indicated by solid lines, while the shaded regions represent the standard deviation. Higher SelfCheckGPT score indicates a higher probability of self-contradiction and higher probability of confabulation. Part (b) depicts the same experimental setup, but hallucination measured on the XSum v2 dataset, where Rouge1 is used as the performance metric. A higher Rouge1 score suggests a better alignment of the generated text to that of the reference summary. For all model sizes, we observe a pronounced trend of high variance and oscillatory behavior in hallucination rates. This fluctuation highlights the models' uncertainty at specific time stamps and emphasizes the need for a robust mitigation strategy to stabilize performance during training.", "description": "Figure 1 visualizes the oscillatory behavior of hallucination metrics (SelfCheckGPT and Rouge1) across various sizes of LLMs (70M to 12B parameters) during training, revealing high variance and fluctuating hallucination rates.", "section": "2 OSCILLATORY BEHAVIOUR VALIDATION"}, {"figure_path": "2410.15460/charts/charts_6_0.png", "caption": "Figure 2: Comparison of sensitive neuron dropout on inference of Eleuther AI's Pythia various model sizes with random neuron dropout. (a) Average sensitive neuron dropout with standard deviation plotted as scale of the model increases. (b) Average sensitive neuron dropout for hallucinatory inputs and non-hallucinatory inputs. Input size for each test is 80 I.I.D. texts. Sensitive neuron dropping presents a clear, significant reduction in EigenScore compared to that of random neuron dropping across model sizes. Hallucinatory generations experience a larger drop in EigenScore, meaning that our protocol scales with likelihood of hallucination.", "description": "The chart compares the effect of sensitive neuron dropout versus random neuron dropout on EigenScore, showing a significant reduction in hallucination likelihood with sensitive neuron dropout, especially for hallucinatory inputs.", "section": "3.2 SENSITIVE NEURON IMPACT ON EIGENSCORES"}, {"figure_path": "2410.15460/charts/charts_8_0.png", "caption": "Figure 3: Efficient EigenScore approximation scaling investigation. The figure shows the difference in computation time between regular EigenScore calculation and EES with a moments value of 20. The x-axis represents the product of the matrix's rows and columns, and the y-axis shows the computation time. As matrix size increases, EES consistently reduces computation time, making it a practical choice for large LLMs.", "description": "Figure 3 shows that Efficient EigenScore (EES) provides significant computational speedup over regular EigenScore, especially for large matrices.", "section": "3.3 EFFICIENT EIGENSCORE APPROXIMATION"}, {"figure_path": "2410.15460/charts/charts_10_0.png", "caption": "Figure 4: Regular finetuning vs. SeND on two datasets. (a) presents the results of training Pythia 1B on HELM with regular finetuning and SeND. (b) uses the same training setup as (a), but the LLM is trained on MedHALT 2k. In both plots, performance is reported as the average EES over 5 runs on the validation set. Models are trained until loss convergence. Training with SeND shows a more controlled reduction in EES compared to regular finetuning, suggesting that SeND optimizes for hallucinations as well as loss, with less overall confidence variability during training. Since finetuning without dropout consistently achieves better factual consistency than random dropout in our experiments, we compare SeND directly with standard finetuning.", "description": "The chart visualizes the effectiveness of SeND (Sensitive Neuron Dropout) compared to regular finetuning on reducing hallucination (measured by EES) during the training of Pythia 1B on two datasets: HELM and MedHALT 2k.", "section": "4 PERFORMANCE OF SEND ON PYTHIA 1B"}, {"figure_path": "2410.15460/charts/charts_15_0.png", "caption": "Figure 5: Net change of sentence embeddings between checkpoints 125,000 and 143,000. Each different colour is a different input text. As depicted, there are specific neurons that go through drastic changes between the two checkpoints of the training regardless of the input.", "description": "The chart displays the net change of sentence embeddings between specific training checkpoints, highlighting neurons with drastic activation changes irrespective of input text.", "section": "3 INTERNAL TRAINING DYNAMICS"}, {"figure_path": "2410.15460/charts/charts_18_0.png", "caption": "Figure 6: Effect of changing number of moments on EES calculation time (seconds). More moments gives more accurate approximation but higher computation time.", "description": "The chart displays the computation time of Efficient EigenScore (EES) in relation to the number of rows in the matrix and the number of moments used in the calculation.", "section": "3.3 EFFICIENT EIGENSCORE APPROXIMATION"}, {"figure_path": "2410.15460/charts/charts_18_1.png", "caption": "Figure 7: Performance of SeND on Pythia 1B wih HELM dataset computed with both EES and regular EigenScore. EES is able to closely track the true EigenScore performance metric, showing that it is a good approximator.", "description": "Figure 7 shows that the Efficient EigenScore (EES) closely approximates the EigenScore, validating EES as a reliable and efficient alternative for hallucination detection.", "section": "3.3 EFFICIENT EIGENSCORE APPROXIMATION"}]