[{"figure_path": "https://arxiv.org/html/2411.12240/x3.png", "caption": "Figure 1: Evaluation pipeline: (1) We collect example texts for all 22 languages. (2) We send the example texts to the LLMs\u2019 tokenizer. (3) Evaluate the tokenized outputs. (4) We construct leaderboards using our evaluation.", "description": "This figure illustrates the evaluation pipeline used in the study.  The process begins with collecting example texts in all 22 official Indian languages. These texts are then fed into the tokenizers of 12 different large language models (LLMs). The resulting tokenized outputs are evaluated using a chosen metric (likely Normalized Sequence Length, as described in the paper). Finally, the results are compiled into leaderboards to compare the performance of each LLM's tokenizer across the different languages.", "section": "3 Methodology"}, {"figure_path": "https://arxiv.org/html/2411.12240/x4.png", "caption": "Figure 2: Assamese text used for evaluating tokenizer performance.", "description": "This figure shows an example of Assamese text used in the study to evaluate the performance of different tokenizers.  The text is shown in the Assamese script and its English translation is provided for context. This example, along with similar examples in other Indian languages, is used to assess how effectively various language models' tokenizers handle the complexities of different Indic scripts and linguistic structures.", "section": "3.1 Example Texts"}, {"figure_path": "https://arxiv.org/html/2411.12240/x5.png", "caption": "Figure 3: Number of Best Performances Achieved by Each Tokenizer Across 22 Languages.", "description": "This bar chart visualizes the count of languages for which each tokenizer achieved the best performance, as measured by the Normalized Sequence Length (NSL) metric.  The chart displays the superiority of the SUTRA tokenizer, which exhibits the best NSL score in 14 out of 22 Indian languages.  It also highlights the relative strengths and weaknesses of other tokenizers across the tested languages, illustrating the varying performance levels of different models in processing Indic language text.", "section": "4 Results"}, {"figure_path": "https://arxiv.org/html/2411.12240/x6.png", "caption": "Figure 4: Number of tokens required for a single example text in Assamese. Lower values are better.", "description": "This bar chart displays the number of tokens generated by 12 different large language models (LLMs) for a single example sentence in the Assamese language.  Each bar represents a different LLM's tokenizer, showing the total token count produced.  A lower number of tokens is generally preferable as it indicates greater efficiency in processing the text. The chart helps visualize and compare the performance of various LLMs' tokenizers in handling Assamese text.", "section": "A.1 Bar Charts of Token Counts for Each Language"}, {"figure_path": "https://arxiv.org/html/2411.12240/x7.png", "caption": "Figure 5: Number of tokens required for a single example text in Bengali. Lower values are better.", "description": "This bar chart displays the number of tokens generated by 12 different large language models (LLMs) when tokenizing a single example sentence in Bengali.  Each bar represents a specific LLM's tokenizer, showing the token count for that model.  The chart helps compare the efficiency of different tokenizers, where lower token counts indicate better performance because more concise tokenization is generally more efficient.", "section": "3 Methodology"}, {"figure_path": "https://arxiv.org/html/2411.12240/x8.png", "caption": "Figure 6: Number of tokens required for a single example text in Bodo. Lower values are better.", "description": "This bar chart displays the number of tokens generated by twelve different large language models (LLMs) for a single example sentence in the Bodo language.  Each bar represents an LLM, and the height of the bar indicates the number of tokens produced.  Lower values are preferable because fewer tokens generally signify more efficient processing and a better understanding of the language by the model's tokenizer.  The chart allows for a comparison of tokenization efficiency among the different LLMs for the Bodo language.", "section": "A.1 Bar Charts of Token Counts for Each Language"}, {"figure_path": "https://arxiv.org/html/2411.12240/x9.png", "caption": "Figure 7: Number of tokens required for a single example text in Dogri. Lower values are better.", "description": "This bar chart displays the number of tokens generated by twelve different large language models (LLMs) for a single example sentence in the Dogri language.  Each bar represents an LLM, and the bar's height corresponds to the token count.  The models are: GPT-40, GPT-4, SUTRA, Llama 3.1, Nanda, Project Indus, OpenHathi, Indic Gemma 7B, MahaMarathi, Phi-3.5-MoE, Airavata, and Aya. Lower values are generally preferred as they indicate a more efficient use of tokens and computational resources.", "section": "Appendix A.1 Bar Charts of Token Counts for Each Language"}, {"figure_path": "https://arxiv.org/html/2411.12240/x10.png", "caption": "Figure 8: Number of tokens required for a single example text in Gujarati. Lower values are better.", "description": "This bar chart visualizes the number of tokens generated by 12 different Large Language Model (LLM) tokenizers for a single Gujarati text example.  Each bar represents a tokenizer, and its height corresponds to the token count.  Lower values are generally preferred, indicating more efficient tokenization (fewer tokens needed to represent the same text).  The chart helps compare the efficiency of various tokenizers in handling Gujarati text.", "section": "4 Results"}, {"figure_path": "https://arxiv.org/html/2411.12240/x11.png", "caption": "Figure 9: Number of tokens required for a single example text in Hindi. Lower values are better.", "description": "This bar chart displays the number of tokens generated by 12 different Large Language Model (LLM) tokenizers for a single example sentence in Hindi.  Each bar represents a tokenizer, and the bar's height corresponds to the token count. Lower values indicate that the tokenizer is more efficient, as it breaks the sentence into fewer parts to process.  The goal is to identify which tokenizer is most efficient for Hindi text.", "section": "4 Results"}, {"figure_path": "https://arxiv.org/html/2411.12240/x12.png", "caption": "Figure 10: Number of tokens required for a single example text in Kannada. Lower values are better.", "description": "This bar chart displays the number of tokens generated by twelve different large language models (LLMs) for a single example sentence in the Kannada language.  Each bar represents an LLM's tokenizer, showing the token count.  Lower values indicate better performance, as a more efficient tokenizer produces fewer tokens while maintaining meaning. The comparison allows for analysis of tokenization efficiency across various models.", "section": "3 Methodology"}, {"figure_path": "https://arxiv.org/html/2411.12240/x13.png", "caption": "Figure 11: Number of tokens required for a single example text in Kashmiri. Lower values are better.", "description": "This bar chart visualizes the number of tokens generated by 12 different language models (LLMs) for a single example sentence in the Kashmiri language.  Each bar represents an LLM's tokenizer, showing the quantity of tokens produced.  The chart allows for a comparison of the tokenization efficiency across various models.  Shorter bars indicate superior performance, reflecting a more concise and effective tokenization process, which is generally desirable.", "section": "3 Methodology"}, {"figure_path": "https://arxiv.org/html/2411.12240/x14.png", "caption": "Figure 12: Number of tokens required for a single example text in Konkani. Lower values are better.", "description": "This bar chart visualizes the number of tokens generated by twelve different large language model (LLM) tokenizers for a single Konkani text example.  Each bar represents a specific LLM tokenizer, showing the token count it produced. Shorter bars indicate more efficient tokenization, as fewer tokens mean less computational overhead for the LLM during processing.  The chart allows for a comparison of tokenizer efficiency across different LLMs, highlighting which models produce the most concise token representations for Konkani text.", "section": "3 Methodology"}, {"figure_path": "https://arxiv.org/html/2411.12240/x15.png", "caption": "Figure 13: Number of tokens required for a single example text in Maithili. Lower values are better.", "description": "This bar chart visualizes the number of tokens generated by 12 different Large Language Model (LLM) tokenizers for a single example sentence in the Maithili language. Each bar represents a tokenizer, and its height corresponds to the token count produced.  A lower bar indicates that the tokenizer produced fewer tokens, which is generally preferred as it often implies better efficiency and potentially better model performance. The chart aids in comparing the efficiency of various LLMs' tokenizers in processing Maithili text.", "section": "3 Methodology"}, {"figure_path": "https://arxiv.org/html/2411.12240/x16.png", "caption": "Figure 14: Number of tokens required for a single example text in Malayalam. Lower values are better.", "description": "This bar chart visualizes the number of tokens generated by 12 different Large Language Model (LLM) tokenizers for a single example sentence in the Malayalam language. Each bar represents a tokenizer, showing the token count produced.  The chart facilitates a comparison of the efficiency of various tokenizers in handling Malayalam text. Lower values indicate more efficient tokenization, requiring fewer computational resources.", "section": "3 Methodology"}, {"figure_path": "https://arxiv.org/html/2411.12240/x17.png", "caption": "Figure 15: Number of tokens required for a single example text in Manipuri. Lower values are better.", "description": "This bar chart visualizes the number of tokens generated by twelve different Large Language Model (LLM) tokenizers for a single example sentence in the Manipuri language.  Each bar represents a tokenizer, and its height corresponds to the token count.  The chart highlights the efficiency of different tokenizers, with shorter bars indicating better performance (fewer tokens needed to represent the same text). Lower token counts are generally preferable because they result in faster processing and lower computational resource consumption.", "section": "3 Methodology"}, {"figure_path": "https://arxiv.org/html/2411.12240/x18.png", "caption": "Figure 16: Number of tokens required for a single example text in Marathi. Lower values are better.", "description": "This bar chart displays the number of tokens generated by 12 different language models' tokenizers for a single sample sentence in Marathi.  Each bar represents a model (GPT-40, GPT-4, SUTRA, Llama 3.1, Nanda, Project Indus, OpenHathi, Indic Gemma, MahaMarathi, Phi-3.5-MoE, Airavata, and Aya), showing the token count produced by each model's tokenizer.  The length of the bar corresponds to the number of tokens; shorter bars indicate more efficient tokenization (fewer tokens generated for the same input). The chart helps compare the efficiency of different tokenizers for Marathi.", "section": "Results"}, {"figure_path": "https://arxiv.org/html/2411.12240/x19.png", "caption": "Figure 17: Number of tokens required for a single example text in Nepali. Lower values are better.", "description": "This bar chart visualizes the number of tokens generated by twelve different Large Language Model (LLM) tokenizers for a single example sentence in Nepali.  Each bar represents a specific tokenizer, and its height corresponds to the token count.  Lower values indicate more efficient tokenization, as fewer tokens generally imply less computational cost and improved model performance. The chart allows for a comparison of the efficiency of various tokenizers across different LLMs when processing Nepali text.", "section": "Results"}, {"figure_path": "https://arxiv.org/html/2411.12240/x20.png", "caption": "Figure 18: Number of tokens required for a single example text in Odia. Lower values are better.", "description": "This bar chart displays the number of tokens generated by 12 different Large Language Model (LLM) tokenizers for a single example sentence in the Odia language.  Each bar represents a specific tokenizer, and the height of the bar corresponds to the token count. Lower values indicate that the tokenizer is more efficient, breaking the sentence into fewer tokens. This efficiency is important for model processing speed and resource usage. The chart allows for a comparison of the tokenization efficiency of various LLMs across different algorithms and architectures.", "section": "3 Methodology"}, {"figure_path": "https://arxiv.org/html/2411.12240/x21.png", "caption": "Figure 19: Number of tokens required for a single example text in Punjabi. Lower values are better.", "description": "This bar chart displays the number of tokens generated by 12 different large language model (LLM) tokenizers for a single Punjabi sentence.  Each bar represents a different LLM tokenizer, and the height of the bar indicates the number of tokens produced.  Lower values are preferable, as they suggest a more efficient tokenizer that requires fewer computational resources for processing.  The chart allows for a comparison of the tokenization efficiency across various LLMs in the context of the Punjabi language.", "section": "3 Methodology"}, {"figure_path": "https://arxiv.org/html/2411.12240/x22.png", "caption": "Figure 20: Number of tokens required for a single example text in Sanskrit. Lower values are better.", "description": "This bar chart displays the number of tokens generated by twelve different large language model (LLM) tokenizers for a single example sentence in Sanskrit.  Each bar represents a tokenizer, and the height of the bar corresponds to the token count.  Lower values indicate better tokenizer performance, signifying greater efficiency and potentially reduced computational cost in processing the text.", "section": "A.1 Bar Charts of Token Counts for Each Language"}, {"figure_path": "https://arxiv.org/html/2411.12240/x23.png", "caption": "Figure 21: Number of tokens required for a single example text in Santali. Lower values are better.", "description": "This bar chart displays the number of tokens generated by twelve different Large Language Model (LLM) tokenizers for a single example sentence in the Santali language.  Each bar represents a different tokenizer, showing the token count. Lower values indicate more efficient tokenization, as fewer tokens generally mean less computational cost and faster processing. The comparison allows for an assessment of the relative performance of various tokenizers in handling Santali.", "section": "A.1 Bar Charts of Token Counts for Each Language"}, {"figure_path": "https://arxiv.org/html/2411.12240/x24.png", "caption": "Figure 22: Number of tokens required for a single example text in Sindhi. Lower values are better.", "description": "This bar chart displays the number of tokens generated by twelve different large language models (LLMs) for a single example sentence in Sindhi.  Each bar represents a different LLM's tokenizer, showing the token count produced.  The chart helps to compare the efficiency of the tokenizers across different LLMs; lower values are preferable, indicating a more efficient and concise tokenization.", "section": "Appendix A.1 Bar Charts of Token Counts for Each Language"}, {"figure_path": "https://arxiv.org/html/2411.12240/x25.png", "caption": "Figure 23: Number of tokens required for a single example text in Tamil. Lower values are better.", "description": "This bar chart visualizes the number of tokens generated by twelve different large language model (LLM) tokenizers for a single example sentence in the Tamil language.  Each bar represents a specific LLM tokenizer, showing the token count. Shorter bars indicate more efficient tokenization, as fewer tokens generally mean better performance and reduced computational costs. The chart allows for a comparison of the tokenization efficiency of various LLMs when processing Tamil text.  The goal is to identify which tokenizers are most efficient for the Tamil language.", "section": "A.1 Bar Charts of Token Counts for Each Language"}, {"figure_path": "https://arxiv.org/html/2411.12240/x26.png", "caption": "Figure 24: Number of tokens required for a single example text in Telugu. Lower values are better.", "description": "This bar chart displays the number of tokens generated by twelve different large language model (LLM) tokenizers for a single example sentence in Telugu.  Each bar represents a tokenizer, and its height corresponds to the token count. Lower token counts are preferred, as they indicate more efficient tokenization and potentially better LLM performance. The chart allows for a comparison of the efficiency of various tokenizers, highlighting which models produce the fewest tokens for the same input, suggesting better performance.", "section": "4 Results"}, {"figure_path": "https://arxiv.org/html/2411.12240/x27.png", "caption": "Figure 25: Number of tokens required for a single example text in Urdu. Lower values are better.", "description": "This bar chart displays the number of tokens generated by 12 different large language model (LLM) tokenizers for a single Urdu sentence.  Each bar represents a tokenizer, and the bar's height corresponds to the token count.  A shorter bar indicates that the tokenizer produced fewer tokens, which is generally more efficient and desirable.  The chart allows for a comparison of tokenizer efficiency across various LLMs in processing Urdu text.", "section": "4 Results"}, {"figure_path": "https://arxiv.org/html/2411.12240/x28.png", "caption": "Figure 26: Example Texts for Assamese, Bengali, Bodo, Dogri, Gujarati, Hindi, Kannada, Kashmiri, Konkani, Maithili.", "description": "Figure 26 shows example texts used in the study for evaluating tokenizer performance.  It provides sample sentences in ten of the twenty-two official Indian languages evaluated: Assamese, Bengali, Bodo, Dogri, Gujarati, Hindi, Kannada, Kashmiri, Konkani, and Maithili. Each example is presented with its translation in English, along with the source of the text, such as a literary work or a well-known saying.", "section": "3 Methodology"}, {"figure_path": "https://arxiv.org/html/2411.12240/x29.png", "caption": "Figure 27: Example Texts for Maithili, Malayalam, Manipuri, Marathi, Nepali, Odia, Punjabi, Sanskrit, Santali, Sindhi, Tamil, Telugu, Urdu.", "description": "Figure 27 shows example texts used for evaluating the tokenizers' performance in 13 Indian languages: Maithili, Malayalam, Manipuri, Marathi, Nepali, Odia, Punjabi, Sanskrit, Santali, Sindhi, Tamil, Telugu, and Urdu.  Each example sentence is provided with its translation in English to aid comprehension and to illustrate the diversity of scripts and sentence structures among these languages.", "section": "3.1 Example Texts"}]