{"references": [{" publication_date": "2017", "fullname_first_author": "Paul F Christiano", "paper_title": "Deep reinforcement learning from human preferences", "reason": "This paper is foundational for the field of reward modeling, introducing the concept of using deep reinforcement learning and human feedback to train reward models for aligning AI systems with human values.  Its impact is far-reaching, influencing much subsequent research on reward model design and evaluation.  The paper's methods and insights are highly relevant to the current paper's goals of improving reward model evaluation in multilingual settings.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "reason": "This paper is highly influential for its introduction of a method for aligning language models with human preferences through human feedback. This approach has become a standard in the field and is directly relevant to the task of reward model evaluation, forming the basis of many modern approaches to aligning LLMs. This paper is relevant to the current paper's focus on reward model evaluation in multilingual settings, highlighting the use of preference data in aligning language models to human preferences.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Yuntao Bai", "paper_title": "Constitutional AI: Harmlessness from AI feedback", "reason": "This paper presents a significant advance in aligning language models with human values by using a novel Constitutional AI approach.  Its focus on safety and harmlessness is highly relevant to reward model evaluation, which often incorporates safety criteria.  The paper's techniques are important for understanding and designing more robust and safe multilingual reward models.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Nathan Lambert", "paper_title": "Rewardbench: Evaluating reward models for language modeling", "reason": "RewardBench serves as a critical benchmark for evaluating reward models in English, providing a baseline against which the performance of multilingual models can be compared.  It is a widely used benchmark in the field, making its role in comparing performance crucial to this paper.", "section_number": 1}, {" publication_date": "2017", "fullname_first_author": "Paul F Christiano", "paper_title": "Deep reinforcement learning from human preferences", "reason": "This paper is foundational for the field of reward modeling, introducing the concept of using deep reinforcement learning and human feedback to train reward models for aligning AI systems with human values.  Its impact is far-reaching, influencing much subsequent research on reward model design and evaluation.  The paper's methods and insights are highly relevant to the current paper's goals of improving reward model evaluation in multilingual settings.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Rafael Rafailov", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "reason": "This paper introduces a novel approach to aligning language models without explicitly training a reward model, directly optimizing the model's policy using human preference data. This method is particularly relevant in the context of multilingual reward model evaluation, offering a potentially more efficient way to align models across multiple languages.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Wen Lai", "paper_title": "LLMs Beyond English: Scaling the Multilingual Capability of LLMs with Cross-Lingual Feedback", "reason": "This work directly addresses multilingual aspects of LLMs, which is essential for the context of multilingual reward model evaluation.  Its focus on cross-lingual feedback mechanisms is directly relevant to the challenge of creating and evaluating reward models across various languages. The paper's methodologies can inform how to design and evaluate RMs for diverse languages.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "John Dang", "paper_title": "RLHF can speak many languages: Unlocking multilingual preference optimization for LLMs", "reason": "This study directly tackles multilingual preference optimization for LLMs, making it highly relevant to the current paper's focus.  It provides methods and insights into aligning models with human preferences in multilingual settings, which is critical for evaluating the performance of reward models across various languages.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Viraat Aryabumi", "paper_title": "Aya 23: Open weight releases to further multilingual progress", "reason": "This paper is highly relevant because it introduces a multilingual LLM which is essential for evaluating the quality of reward models in multilingual contexts. The models presented in this work are used in the present paper's benchmark and thus understanding their development and capabilities is important.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Dawei Zhu", "paper_title": "A preference-driven paradigm for enhanced translation with large language models", "reason": "This paper presents a novel approach to machine translation using large language models (LLMs), directly relevant to the multilingual capabilities of M-REWARDBENCH.  The methods used for enhancing translation quality could also be applied to improving reward model performance in multilingual settings.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Nathan Lambert", "paper_title": "Rewardbench: Evaluating reward models for language modeling", "reason": "This paper introduces RewardBench, which serves as the baseline for comparing multilingual reward model performance. By providing detailed benchmarks for English reward models, it sets up a framework for assessing multilingual capabilities. The comparative analysis with M-REWARDBENCH results is a key contribution of this work.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "John Dang", "paper_title": "RLHF can speak many languages: Unlocking multilingual preference optimization for LLMs", "reason": "This paper is directly relevant due to its explicit focus on multilingual preference optimization for LLMs.  The methods and findings presented directly inform the design and evaluation of multilingual reward models, which is a key focus of this study.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Jinze Bai", "paper_title": "Qwen Technical Report", "reason": "This paper presents Qwen, a large language model, whose reward model capabilities are evaluated in the current paper's multilingual benchmark. The report contains information about the model architecture and training procedures which is relevant to evaluating the RM's performance and design.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Viraat Aryabumi", "paper_title": "Aya 23: Open weight releases to further multilingual progress", "reason": "The paper is highly relevant because it introduces Aya-23 model, a multilingual LLM which is essential for evaluating the quality of reward models in multilingual contexts. The models presented in this work are used in the current paper's benchmark and thus understanding their development and capabilities is important.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Hamish Ivison", "paper_title": "Unpacking DPO and PPO: Disentangling best practices for learning from preference feedback", "reason": "This paper provides essential background information on Direct Preference Optimization (DPO) and Proximal Policy Optimization (PPO), two key techniques used in reward model training.  A deep understanding of DPO and PPO is important for interpreting the results and for developing better approaches to multilingual reward model design and training.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Albert Q Jiang", "paper_title": "Mistral 7b", "reason": "This paper describes Mistral 7B, one of the language models used to evaluate multilingual reward model performance in the experiments of this paper.  Understanding the capabilities and characteristics of this language model is necessary to accurately interpret the results obtained.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Abhimanyu Dubey", "paper_title": "The Llama 3 herd of models", "reason": "This paper introduces the Llama 3 family of LLMs, which are used in the evaluation of multilingual reward models. It provides essential context on the design, training, and capabilities of these models, crucial for interpreting the results of the benchmark.  Understanding the properties of these LLMs informs how reward models are designed and evaluated.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Viet Dac Lai", "paper_title": "Okapi: Instruction-tuned large language models in multiple languages with reinforcement learning from human feedback", "reason": "This paper is closely related to the current work because it also addresses the issue of aligning LLMs with human preferences in multilingual settings.  Their approach and findings provide valuable comparative insights into the challenges of multilingual preference optimization and reward model design.", "section_number": 7}, {" publication_date": "2024", "fullname_first_author": "Shuaijie She", "paper_title": "Mapo: Advancing multilingual reasoning through multilingual alignment-as-preference optimization", "reason": "This paper addresses the specific challenge of multilingual reasoning in LLMs, which is a key component of the current work's benchmark.  Its focus on multilingual alignment through preference optimization provides comparative insights into methods for improving RM performance across multiple languages.", "section_number": 7}]}