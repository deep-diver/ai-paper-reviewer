{"references": [{" publication_date": "2017", "fullname_first_author": "Paul F Christiano", "paper_title": "Deep reinforcement learning from human preferences", "reason": "This paper is foundational to the field of reward modeling, introducing the concept of using deep reinforcement learning to align AI systems with human preferences.  Its impact on the development of reward models for large language models (LLMs) is significant, making it one of the most influential works in the area.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "reason": "This work is highly significant because it details the training of language models to follow instructions using human feedback, a core technique in aligning LLMs with human preferences. The methods and results presented have greatly influenced the development of reward models and alignment techniques within the field.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Yuntao Bai", "paper_title": "Constitutional AI: Harmlessness from AI feedback", "reason": "This paper introduces the concept of Constitutional AI, a method to improve the safety and harmlessness of AI systems.  The work is highly relevant to the paper because it demonstrates a different approach to aligning language models with human values, which is a key consideration in the development and evaluation of reward models.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Nathan Lambert", "paper_title": "Rewardbench: Evaluating reward models for language modeling", "reason": "This paper is directly relevant as it introduces RewardBench, a benchmark for evaluating reward models in English that the current work extends to multilingual settings.  Understanding its methodology and limitations forms a crucial baseline for the proposed M-REWARDBENCH.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Aakanksha", "paper_title": "The multilingual alignment prism: Aligning global and local preferences to reduce harm", "reason": "This work directly addresses the multilingual aspect of reward model evaluation. The focus on aligning global and local preferences highlights the challenges and complexities of multilingual reward modeling, offering insights relevant to the current study's approach.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "John Dang", "paper_title": "Rlhf can speak many languages: Unlocking multilingual preference optimization for llms", "reason": "This paper explores RLHF in multilingual settings, addressing the challenges in aligning language models with human preferences across various languages, directly influencing the focus and methodology of the proposed benchmark.", "section_number": 1}, {" publication_date": "1952", "fullname_first_author": "Ralph Allan Bradley", "paper_title": "Rank analysis of incomplete block designs: I. the method of paired comparisons", "reason": "This paper introduces the Bradley-Terry model, a fundamental statistical model for paired comparisons. The model is widely used in preference learning and reward model evaluation, underlying many of the methods used in current reward model research and evaluation techniques.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Rafael Rafailov", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "reason": "This paper proposes direct preference optimization (DPO), a method for directly optimizing the language model's policy based on preference data, bypassing explicit reward model training.  This is a significant alternative approach to reward modeling, relevant to understanding the range of techniques in the field.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Jinze Bai", "paper_title": "Qwen Technical Report", "reason": "This is a technical report providing details about a significant language model, Qwen.  The report details model architecture, training data, and performance metrics, which are highly relevant to the current study because it demonstrates the capabilities of the large language model in the development and testing of reward models.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Viraat Aryabumi", "paper_title": "Aya 23: Open weight releases to further multilingual progress", "reason": "This paper presents the Aya 23 language model, directly relevant because the models from Aya 23 were used in the reward model evaluation. Understanding the characteristics of the models used is essential to accurately interpret the results of the benchmark.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Dawei Zhu", "paper_title": "A preference-driven paradigm for enhanced translation with large language models", "reason": "This paper focuses on a preference-driven paradigm for machine translation, a task that is directly addressed in M-REWARDBENCH.  The methodologies and results presented are highly relevant to the translation component of the benchmark, informing the design and analysis choices.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "John Dang", "paper_title": "Rlhf can speak many languages: Unlocking multilingual preference optimization for llms", "reason": "This paper's focus on RLHF in multilingual settings and its evaluation methodology provides a valuable comparison to the methods used in the creation of M-REWARDBENCH.  The results and observations directly impact the benchmark's design and interpretation of results.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Abhimanyu Dubey", "paper_title": "The llama 3 herd of models", "reason": "This work describes the LLaMa 3 language models, which are directly relevant to the experiments as these models were used in the evaluation. Understanding their architecture and capabilities is vital for interpreting the results.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Hamish Ivison", "paper_title": "Unpacking dpo and ppo: Disentangling best practices for learning from preference feedback", "reason": "This work explores different approaches to preference-based learning, providing a deeper understanding of the methods behind training reward models.  Comparing the performance of DPO and PPO, discussed in this paper, offers valuable context for interpreting the results presented in the study.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Albert Q Jiang", "paper_title": "Mistral 7b", "reason": "This paper introduces the Mistral 7B language model, directly relevant to the experiments as this model was used in the evaluation.  Understanding the capabilities of this model provides crucial context for interpreting the results of the benchmark.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Lianmin Zheng", "paper_title": "Judging llm-as-a-judge with mt-bench and chatbot arena", "reason": "This paper focuses on evaluating LLMs as judges for various tasks, directly relevant to the work on using generated outputs to judge the quality of responses in reward models.  The methods and results presented offer insight into the limitations of LLMs in making such judgments.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Jinze Bai", "paper_title": "Qwen Technical Report", "reason": "This paper presents a technical report describing the Qwen large language model. It is pertinent to the present study because the Qwen model was one of the models that participated in the experiments.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Junlong Li", "paper_title": "Generative judge for evaluating alignment", "reason": "This paper introduces a novel approach to evaluating the alignment of language models using a generative judge.  The idea of using a generative model for evaluation is directly related to one of the approaches to reward model evaluation discussed in the current work.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Xuechen Li", "paper_title": "AlpacaEval: An Automatic Evaluator of Instruction-following Models", "reason": "This paper introduces AlpacaEval, an automatic evaluation benchmark specifically for instruction-following models. It is highly relevant because a subset of the AlpacaEval dataset is translated and used in the Chat category of the M-REWARDBENCH.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Viet Dac Lai", "paper_title": "Okapi: Instruction-tuned large language models in multiple languages with reinforcement learning from human feedback", "reason": "This paper is closely related because it explores instruction-tuned large language models in multiple languages using reinforcement learning from human feedback (RLHF). This aligns directly with the goals and context of reward modeling and the M-REWARDBENCH's focus on multilingual settings.", "section_number": 7}]}