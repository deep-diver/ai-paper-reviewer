{"references": [{" publication_date": "2017", "fullname_first_author": "Paul F Christiano", "paper_title": "Deep reinforcement learning from human preferences", "reason": "This paper is foundational to the field of reward modeling, introducing the concept of using deep reinforcement learning to learn from human preferences. This approach is central to aligning large language models with human values, which is the core focus of the current paper.  The cited work's methodology and impact on the development of reward models make it a highly relevant and seminal piece in the field.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "reason": "This work is highly influential in the field of reward modeling, detailing a method of training LLMs to better follow instructions using human feedback. The method is directly relevant to the paper's central theme of using human feedback to improve the alignment of LLMs in multilingual settings, particularly considering the methodology's significance in the field.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Yuntao Bai", "paper_title": "Constitutional AI: Harmlessness from AI feedback", "reason": "This paper significantly advances the field of AI safety and alignment through the introduction of constitutional AI.  The methodology of using AI feedback to guide the development of more harmless models resonates strongly with the paper's focus on using human feedback to improve the alignment of reward models, particularly in a way that reduces harm.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Nathan Lambert", "paper_title": "RewardBench: Evaluating reward models for language modeling", "reason": "RewardBench is a crucial benchmark used for evaluating reward models and is specifically mentioned in the paper. The study directly uses RewardBench as a reference point for developing and testing its new benchmark, making it a pivotal reference. The methodology of RewardBench is relevant to the development of the new benchmark.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Viraat Aryabumi", "paper_title": "Aya 23: Open weight releases to further multilingual progress", "reason": "The Aya model is one of the models evaluated in the study, and this paper describes the model's development, its multilingual capabilities, and its performance.  This is essential for the paper's analysis, as the performance and capabilities of Aya are integral to the results and evaluation of the benchmark.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Jinze Bai", "paper_title": "Qwen Technical Report", "reason": "The Qwen model is another model tested in the study, and this technical report details the model's architecture, training, and performance characteristics. This information is essential in understanding the results of the paper's benchmark, particularly how the model performs in different multilingual scenarios.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Albert Q Jiang", "paper_title": "Mistral 7b", "reason": "Mistral is one of the evaluated models, and this paper is important because it explains the design, training, and evaluation of this model. This model's multilingual performance is key to the study's findings, which compare across multiple models.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Abhimanyu Dubey", "paper_title": "The llama 3 herd of models", "reason": "The LLaMA model family is heavily used in the evaluation, making this paper about the LLaMA models' characteristics essential.  The paper's description of the models and their performance across different tasks, such as instruction following, informs the paper's interpretation and understanding of the multilingual benchmark results.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "John Dang", "paper_title": "Rlhf can speak many languages: Unlocking multilingual preference optimization for llms", "reason": "This paper is important as it directly addresses multilingual preference optimization for LLMs which is the main focus of the current paper.  The cited paper's approach to aligning LLMs in multilingual settings is highly relevant to the new benchmark.", "section_number": 7}, {" publication_date": "2024", "fullname_first_author": "John Dang", "paper_title": "Rlhf can speak many languages: Unlocking multilingual preference optimization for llms", "reason": "This paper is significant because it is one of the few works that specifically address multilingual preference alignment in LLMs. The paper's methodology and findings are directly relevant to the current paper's approach to benchmarking multilingual reward models.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Junlong Li", "paper_title": "Generative judge for evaluating alignment", "reason": "The paper introduces a novel approach for evaluating alignment by using generative models as judges.  This methodology is relevant to the paper, particularly the use of generative models in the evaluation of multilingual reward models, thus informing the study's design.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Xuechen Li", "paper_title": "AlpacaEval: An Automatic Evaluator of Instruction-following Models", "reason": "AlpacaEval is used as a source dataset for the Chat and Chat-Hard tasks within the M-REWARDBENCH. Understanding AlpacaEval\u2019s methodology and limitations is therefore essential to understanding the evaluation method and limitations of M-REWARDBENCH.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Enyu Zhou", "paper_title": "RMB: Comprehensively benchmarking reward models in llm alignment", "reason": "The paper introduces the RMB benchmark which also tackles multilingual reward model evaluation, offering a comparison point for methodology and results.  The detailed examination of RMB in this paper will provide an opportunity for a comprehensive comparison.", "section_number": 7}, {" publication_date": "2024", "fullname_first_author": "Dawei Zhu", "paper_title": "A preference-driven paradigm for enhanced translation with large language models", "reason": "MAPLE (Zhu et al., 2024) is used as a source dataset for the Translation-Easy and Translation-Hard tasks. This paper provides crucial background on the dataset's creation, methodology, and limitations, and this context is essential for understanding the choices made in the creation of M-REWARDBENCH.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Hamish Ivison", "paper_title": "Unpacking dpo and ppo: Disentangling best practices for learning from preference feedback", "reason": "This paper offers an in-depth analysis of Direct Preference Optimization (DPO) and Proximal Policy Optimization (PPO), two key techniques used in preference learning for LLMs.  Understanding these methodologies is vital for a deep understanding of the methods used in training the reward models and interpreting their performance.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Viet Dac Lai", "paper_title": "Okapi: Instruction-tuned large language models in multiple languages with reinforcement learning from human feedback", "reason": "Okapi demonstrates a successful application of RLHF in a multilingual setting, closely aligned with the goal of the current paper.  The methodology of Okapi offers a valuable comparison point and context for the development and evaluation of the multilingual reward models.", "section_number": 7}, {" publication_date": "2024", "fullname_first_author": "Wen Lai", "paper_title": "LLMs Beyond English: Scaling the Multilingual Capability of LLMs with Cross-Lingual Feedback", "reason": "The paper directly addresses scaling the multilingual capability of LLMs. Its focus aligns with the core goal of the current paper and offers a relevant comparison of techniques. The analysis of cross-lingual feedback mechanisms is directly applicable to the interpretation of results and discussion in the main paper.", "section_number": 7}, {" publication_date": "2020", "fullname_first_author": "Pratik Joshi", "paper_title": "The state and fate of linguistic diversity and inclusion in the NLP world", "reason": "This paper provides valuable context for the study by highlighting the importance of linguistic diversity and inclusion in NLP.  Understanding this context is important for evaluating the relevance and impact of a multilingual benchmark for reward models.", "section_number": 6}, {" publication_date": "2022", "fullname_first_author": "Marta R Costa-juss\u00e0", "paper_title": "No language left behind: Scaling human-centered machine translation", "reason": "NLLB is used for translating portions of the benchmark. This paper provides crucial background information on the capabilities and limitations of NLLB, which is critical for interpreting and analyzing the impact of translation quality on the performance of reward models.", "section_number": 6}, {" publication_date": "2024", "fullname_first_author": "Lianmin Zheng", "paper_title": "Judging llm-as-a-judge with mt-bench and chatbot arena", "reason": "MT-Bench is used as a source dataset for the Chat and Chat-Hard tasks. Understanding its methodology, limitations, and the results of the analysis of LLMs acting as judges is essential for a full understanding of this aspect of the M-REWARDBENCH benchmark.", "section_number": 3}]}