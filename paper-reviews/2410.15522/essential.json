{"importance": "This paper is crucial for researchers working on reward models and large language models (LLMs), particularly those focusing on multilingual applications.  It introduces a novel benchmark dataset and systematic evaluation, directly addressing the under-researched area of multilingual reward model performance. The findings highlight significant performance gaps between English and non-English languages, prompting further investigation into techniques for improving multilingual RM capabilities. The publicly available dataset and codebase will significantly aid future research in this area.", "summary": "M-REWARDBENCH: A new multilingual benchmark reveals significant performance gaps in reward models across languages, highlighting the need for improved cross-lingual alignment in LLMs.", "takeaways": ["M-REWARDBENCH, the first large-scale multilingual reward model benchmark, shows substantial performance differences across languages.", "Reward model performance strongly correlates with translation quality and language resource availability.", "Generative reward models generally outperform classifier and implicit reward models in multilingual settings."], "tldr": "This research tackles the under-studied area of multilingual reward model (RM) performance in large language models (LLMs).  It introduces M-REWARDBENCH, a new benchmark dataset with preference data across 23 languages, covering various tasks like chat, safety, reasoning, and translation.  The study rigorously evaluates a wide range of RMs on this benchmark, uncovering significant performance discrepancies between English and other languages. They find that RM preferences can vary substantially across languages. Importantly, they demonstrate that RMs perform better with higher-quality translations and for high-resource languages.  The research emphasizes the need for further research to understand and address the limitations of current reward models in multilingual settings.  The dataset and code are publicly released to aid this research."}