[{"figure_path": "2410.15522/charts/charts_1_0.png", "caption": "Figure 1: Performance gap between RewardBench (English) and the average M-REWARDBENCH scores across 23 languages for various reward models (Pearson r: 0.92, Spearman p: 0.89). All models underperform on our multilingual benchmark compared to their performance on the corresponding English benchmark.", "description": "The chart visualizes the performance gap between RewardBench (English-only benchmark) and M-REWARDBENCH (multilingual benchmark) scores for various reward models, showing significant underperformance in multilingual settings.", "section": "Results"}, {"figure_path": "2410.15522/charts/charts_4_0.png", "caption": "Figure 2: Label agreement, as measured by Cohen's k, of various RMs with respect to RewardBench (English) averaged across 23 languages. No model achieves complete agreement (\u043a = 1) between other languages and English, with some exhibiting greater volatility across languages and others demonstrating more stability.", "description": "The chart displays the average inner-model agreement across 23 languages for various reward models, using Cohen's kappa to measure label agreement.", "section": "5 Results"}, {"figure_path": "2410.15522/charts/charts_5_0.png", "caption": "Figure 3: (Top) Distribution of label agreement, as measured by Cohen's \u03ba, across the six Generative RMs in the top ten (Table 2) with respect to RewardBench (English) on Indonesian. Interpretation of Cohen's k scores is based on McHugh (2012). (Bottom) Percentage of categories in M-REWARDBENCH for each bin in the histogram.", "description": "The chart displays the distribution of label agreement, measured by Cohen's kappa, across six generative reward models for Indonesian, comparing their performance to the English RewardBench, showing the consistency of models in labeling the same instances across different languages.", "section": "5 Results"}, {"figure_path": "2410.15522/charts/charts_6_0.png", "caption": "Figure 4: Performance of ten selected reward models across different RM types on a version of M-REWARDBENCH translated using NLLB 3.3B (Costa-juss\u00e0 et al., 2022) and the Google Translate API. The performance of RMs improves when they are provided with higher-quality translations.", "description": "The chart displays the performance of ten reward models on a translated subset of M-REWARDBENCH, comparing results using NLLB and Google Translate to show the impact of translation quality on reward model performance.", "section": "6 Analysis"}, {"figure_path": "2410.15522/charts/charts_7_0.png", "caption": "Figure 5: Performance across different linguistic dimensions: resource availability, language family, and script. Resource availability is based on Joshi et al. (2020)'s language categorization, with higher-numbered classes having more data resources. Information on language family and script are based on Aryabumi et al. (2024).", "description": "The chart displays the performance of reward models across various linguistic dimensions, including resource availability, language family, and script.", "section": "6 Analysis"}]