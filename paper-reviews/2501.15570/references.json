{"references": [{"fullname_first_author": "A. Bick", "paper_title": "Transformers to ssms: Distilling quadratic knowledge to subquadratic models", "publication_date": "2024-08-10", "reason": "This paper is foundational to the knowledge distillation method used in the current work, which is a key part of the proposed model."}, {"fullname_first_author": "X. Dong", "paper_title": "Hymba: A hybrid-head architecture for small language models", "publication_date": "2024-11-13", "reason": "This paper provides a state-of-the-art hybrid model that is directly compared to in the evaluation section, making it crucial for contextualizing the performance results."}, {"fullname_first_author": "B. Peng", "paper_title": "Eagle and finch: RWKV with matrix-valued states and dynamic recurrence", "publication_date": "2024-04-05", "reason": "This paper introduces the RWKV architecture, which is the basis for the RNN-Attention model proposed in this work.  Its significance lies in its foundational contribution to the architecture being studied."}, {"fullname_first_author": "J. Wang", "paper_title": "The mamba in the llama: Distilling and accelerating hybrid models", "publication_date": "2024-08-15", "reason": "The techniques explored in this paper, particularly concerning distillation and acceleration of hybrid models, are relevant to the approach taken in this paper to train more efficient large language models."}, {"fullname_first_author": "X. Xu", "paper_title": "A survey on knowledge distillation of large language models", "publication_date": "2024-02-13", "reason": "This paper provides a comprehensive overview of knowledge distillation techniques used in large language models, offering crucial background and context for the distillation strategies employed in the current research."}]}