[{"figure_path": "https://arxiv.org/html/2501.15570/extracted/6157273/arwkv-2.drawio.png", "caption": "Figure 1: replace self-attention by RWKV-7 time mixing module", "description": "This figure illustrates the substitution of the self-attention mechanism in a transformer architecture with the RWKV-7 time mixing module.  It depicts the detailed architecture of the time mixing module replacing the self-attention, showing the input and output flow.  The components shown highlight the key changes made to adapt the transformer layer for RNN-based processing.  Different components such as Embedding, RMS Norm, QwenMLP, SwiGLU, and Linear layers are clearly marked, indicating how the signals flow through these layers to finally produce a replaced attention output.  The Nx represents the context length.", "section": "3 From Transformer to RNN"}, {"figure_path": "https://arxiv.org/html/2501.15570/extracted/6157273/rwkv7.png", "caption": "Figure 2: RWKV-7 architecture.capability of attention is the key for RNN-based LLMs, which in this case is Time mixing module", "description": "Figure 2 illustrates the architecture of the RWKV-7 model, highlighting its key component: the time mixing module.  Unlike traditional Transformer models that rely on self-attention mechanisms, RWKV-7 leverages this time mixing module as its core attention mechanism. This module is crucial for enabling RNN-based large language models (LLMs) by enabling efficient processing of sequential data.  The diagram shows the flow of information through the model, from the input embedding to the final output. It depicts layers like Layer Norm, channel mix modules, and feed-forward networks (FFNs), all interconnected to form the overall model architecture. The use of a time-mixing module instead of self-attention is a core difference that allows RWKV-7 to achieve better performance in long-context tasks.", "section": "2 Architecture"}, {"figure_path": "https://arxiv.org/html/2501.15570/extracted/6157273/arwkv-decoderlayer.png", "caption": "Figure 3: General Decoder Layer in transformer", "description": "This figure depicts a general decoder layer within a transformer architecture.  It illustrates the flow of information through the layer, starting with the self-attention mechanism which processes the input sequence, followed by a residual connection and layer normalization to stabilize training. Finally, a feed-forward network (MLP) further processes the information before passing it to the next layer.  The diagram highlights the key components and their order of operation in a typical transformer decoder layer.", "section": "3 From Transformer to RNN"}, {"figure_path": "https://arxiv.org/html/2501.15570/extracted/6157273/arwkv-workflow.png", "caption": "Figure 4: We replace the standard Attention with an AttentionWrapper that contains both the original self-attention mechanism and a TimeMixer. The TimeMixer is trained to minimize the gap between its output and that of the self-attention module. The final output combines the hidden states from the original self-attention with the residual difference between self-attention and TimeMixer outputs. This architecture enables the model to optimize the TimeMixer to progressively reduce the discrepancy between self-attention and TimeMixer outputs.", "description": "This figure illustrates a modified decoder layer where the standard self-attention mechanism is replaced by an 'AttentionWrapper'.  This wrapper incorporates both the original self-attention and a newly introduced 'TimeMixer' module. The TimeMixer is trained using a loss function that aims to minimize the difference between its output and the output of the original self-attention. The final output of the layer is a combination of the original self-attention's hidden states and the residual difference between the self-attention and TimeMixer outputs. This design allows the model to learn a more effective representation by progressively reducing the discrepancy between the self-attention and the TimeMixer.", "section": "3.1 Stage 1 - Time Mixing module replacing Self-Attention"}, {"figure_path": "https://arxiv.org/html/2501.15570/extracted/6157273/stage1_no_norm_loss.png", "caption": "Figure 5: Stage-1 loss, 18 hours with one 8*h800 80G , context length 2048, 4B tokens", "description": "This figure shows the training loss curve for Stage 1 of the model training process. The training took 18 hours using a single 8x A800 80GB GPU. The context length during training was 2048, and a total of 4 billion tokens were used. The x-axis represents the training steps, and the y-axis represents the loss. The curve demonstrates the decrease in loss over the training period.", "section": "3.1 Stage 1 - Time Mixing module replacing Self-Attention"}]