{"references": [{" publication_date": "2017", "fullname_first_author": "B. M. Lake", "paper_title": "Building machines that learn and think like people", "reason": "This paper is foundational for the field of AI and robotics, highlighting the importance of developing machines that can learn and think like humans.  The authors' emphasis on predictive capabilities and intuitive understanding of object dynamics directly aligns with the goals of this research, making this a highly relevant and important foundational paper. The concepts presented here provide a critical context for the current research in learning object dynamics from real-world data.", "section_number": 1}, {" publication_date": "2017", "fullname_first_author": "C. Finn", "paper_title": "Deep visual foresight for planning robot motion", "reason": "This work is highly relevant because it pioneers the use of deep learning for visual foresight in robot motion planning. This research builds upon the foundational concepts explored in the cited paper, addressing the challenge of learning object dynamics from visual data, thereby pushing the frontiers of model-based robot planning in dynamic environments.", "section_number": 1}, {" publication_date": "2020", "fullname_first_author": "A. Nagabandi", "paper_title": "Deep dynamics models for learning dexterous manipulation", "reason": "This paper is highly relevant to the current work as it directly addresses the problem of learning object dynamics for dexterous manipulation, a central challenge in robotics. The authors use deep learning models to capture the complexities of object dynamics, which provides a valuable foundation for learning more accurate and robust dynamics models that are essential for model-based planning. The approach is similar to the current work's goal to learn accurate dynamic models that can be used for object manipulation tasks.", "section_number": 1}, {" publication_date": "2020", "fullname_first_author": "L. Manuelli", "paper_title": "Keypoints into the future: Self-supervised correspondence in model-based reinforcement learning", "reason": "This paper focuses on self-supervised correspondence in model-based reinforcement learning, which is directly relevant to the current research's approach of learning object dynamics from video data. The authors explore the ability to use keypoints to predict future object states, which serves as a crucial foundation for developing accurate and reliable dynamics models. The method described here provides a framework for integrating the self-supervised learning of object dynamics into a model-based planning context.", "section_number": 1}, {" publication_date": "2020", "fullname_first_author": "A. Sanchez-Gonzalez", "paper_title": "Learning to simulate complex physics with graph networks", "reason": "This paper is significant because it introduces the use of graph neural networks (GNNs) for simulating complex physics, a technique that is directly applicable to modeling object dynamics.  The authors' successful use of GNNs for physics simulation provides crucial justification for using similar techniques in the current research, laying the groundwork for employing GNNs to model the dynamic behavior of objects.", "section_number": 1}, {" publication_date": "2020", "fullname_first_author": "T. Pfaff", "paper_title": "Learning mesh-based simulation with graph networks", "reason": "This paper is highly important to the current work because it directly explores the use of graph neural networks (GNNs) for learning mesh-based simulation of physical phenomena, which directly inspires the current research's use of GNNs for learning the dynamics of objects represented by 3D Gaussian Splatting.  The paper provides evidence that GNNs can be effective in learning complex dynamics from sparse, mesh-based representations of objects.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Y. Wang", "paper_title": "Dynamic-resolution model learning for object pile manipulation", "reason": "This work directly addresses the topic of learning dynamic models for object manipulation, which is central to the current research. The authors' focus on learning dynamic models from data and applying them to manipulation tasks provides a strong basis and inspiration for the current research's model-based planning approach. Their method, using a dynamic-resolution model for manipulation, demonstrates a practical approach to handling the challenges associated with modeling complex object dynamics for robotics applications.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "J. Luiten", "paper_title": "Dynamic 3d gaussians: Tracking by persistent dynamic view synthesis", "reason": "This paper is highly significant because it directly introduces the use of Dynamic 3D Gaussians (Dyn3DGS) for tracking objects, a method that is core to the current research's methodology. Dyn3DGS provides a powerful framework for 3D object representation and tracking, offering the ability to efficiently model the dynamic behavior of objects in 3D space. The paper lays the foundation for using this approach for object manipulation.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "C. Doersch", "paper_title": "Tap-vid: A benchmark for tracking any point in a video", "reason": "This paper introduces a benchmark dataset (Tap-Vid) for tracking any point in a video, which is highly relevant to the current research as it provides a standard dataset for evaluating 3D tracking methods. The dataset's focus on tracking points in videos is aligned with this paper's core approach of representing and tracking objects in a particle-based fashion. The results obtained from the benchmark can then be compared to the proposed method's results, leading to a better understanding of its overall tracking performance and capabilities.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "C. Doersch", "paper_title": "Tapir: Tracking any point with per-frame initialization and temporal refinement", "reason": "This paper introduces a novel method for tracking any point in a video (Tapir), offering improvements over existing tracking methods, which is relevant to the current research's approach of tracking the motions of 3D Gaussian particles representing objects. The improved accuracy and efficiency of Tapir provide a valuable baseline for comparison and highlight the need for advanced tracking techniques for accurate representation of the object motion for learning object dynamics.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "A. W. Harley", "paper_title": "Particle video revisited: Tracking through occlusions using point trajectories", "reason": "This paper explores the use of point trajectories for robust tracking through occlusions in videos, which is critical to the current research's problem of accurately tracking 3D Gaussian particles representing objects under challenging conditions. The authors highlight the ability of point trajectories to provide robust tracking, which forms a crucial aspect of this paper\u2019s approach to learning object dynamics in realistic environments.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Q. Wang", "paper_title": "Tracking everything everywhere all at once", "reason": "This paper provides a comprehensive overview of state-of-the-art object tracking methods, which helps to contextualize and compare the proposed method's 3D tracking capabilities. The research's focus on tracking a vast number of objects simultaneously provides important insights into the challenges and opportunities in developing more accurate and efficient tracking systems, that are essential for robustly learning object dynamics from real-world video data.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Y. Xiao", "paper_title": "Spatialtracker: Tracking any 2d pixels in 3d space", "reason": "This paper introduces a method (SpatialTracker) for tracking 2D pixels in 3D space, which offers a relevant and comparable approach to this paper\u2019s method of tracking 3D Gaussian particles.  The authors demonstrate the potential of 2D-3D tracking, which is a key component of the current research's approach to learning object dynamics from real-world video data. The comparative analysis of tracking performance enhances the understanding of the method's capabilities.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "C. Wang", "paper_title": "Neural trajectory fields for dynamic novel view synthesis", "reason": "This paper introduces neural trajectory fields for dynamic novel view synthesis, which is highly relevant to the current research's method of predicting future object motions. The authors demonstrate the potential of neural networks for learning and generating complex dynamic motions, thus providing justification for the use of GNNs in this paper. The application to novel view synthesis indicates the potential of the approach for generating realistic video predictions.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "B. P. Duisterhof", "paper_title": "Md-splatting: Learning metric deformation from 4d gaussians in highly deformable scenes", "reason": "This paper introduces a method (Md-splatting) for learning metric deformation from 4D Gaussians in highly deformable scenes.  This is highly relevant to the current research's use of 3D Gaussian Splatting for representing objects as collections of 3D Gaussians. The concept of learning metric deformations provides crucial support for this paper\u2019s focus on modeling deformable objects and their dynamics, thereby contributing to the robustness of the approach.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "H. Bharadhwaj", "paper_title": "Track2act: Predicting point tracks from internet videos enables diverse zero-shot robot manipulation", "reason": "This work is highly relevant because it directly addresses the challenge of using internet videos to enable robot manipulation, which shares a similar goal with this research of learning object dynamics from real-world video data for robotic applications.  The use of point tracks for enabling zero-shot robot manipulation provides a compelling example of how this paper's predicted motions can be utilized for robotic control and planning in real-world settings.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "M. Xu", "paper_title": "Flow as the cross-domain manipulation interface", "reason": "This paper explores the use of optical flow for cross-domain manipulation, providing insights into the possibilities of using motion information extracted from videos to enable robot manipulation. The paper's focus on using motion information to perform manipulation tasks directly relates to the core concept of using learned dynamics models for planning robot actions, thus enhancing the understanding of how the current research's predictions can be applied in real-world robotic manipulation scenarios.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "T. Xie", "paper_title": "Physgaussian: Physics-integrated 3d gaussians for generative dynamics", "reason": "This paper is directly relevant because it also uses 3D Gaussians for generative dynamics, a core component of this paper's proposed method.  The authors show that a physics-integrated approach can significantly improve the accuracy and realism of object dynamics prediction, which is critical for this research. The paper provides insights into combining physics-based modelling with data-driven methods for accurate and generalizable dynamic modelling.", "section_number": 2}, {" publication_date": "2019", "fullname_first_author": "Y. Wu", "paper_title": "Learning to manipulate deformable objects without demonstrations", "reason": "This paper addresses the challenge of learning to manipulate deformable objects without demonstrations, a crucial aspect of this research, which focuses on learning the dynamics of deformable objects from videos.  The method's ability to learn manipulation skills without explicit demonstrations shows the potential of using learning-based approaches to acquire complex manipulation skills, without extensive manual programming. It provides insights into the feasibility of learning directly from observations, rather than relying solely on pre-programmed models.", "section_number": 2}]}