[{"figure_path": "2410.18912/figures/figures_1_0.png", "caption": "Figure 1: We propose a novel approach for learning a neural dynamics model from real-world data. Using videos captured from robot-object interactions, we obtain dense 3D tracking with a dynamic 3D Gaussian Splatting framework. We train a graph-based neural dynamics model on top of the 3D Gaussian particles for action-conditioned video prediction and model-based planning.", "description": "The figure illustrates the overall framework of the proposed approach, which involves real-world data collection, dynamic 3D Gaussian tracking, graph-based neural dynamics learning, video prediction, and model-based planning.", "section": "Introduction"}, {"figure_path": "2410.18912/figures/figures_3_0.png", "caption": "Figure 2: Overview of Our Framework: We first achieve dense 3D tracking of long-horizon robot-object interactions using multi-view videos and Dyn3DGS optimization. We then learn the object dynamics through a graph-based neural network. This approach enables applications such as (i) action-conditioned video prediction using linear blend skinning for motion prediction, and (ii) model-based planning for robotics.", "description": "The figure illustrates the proposed framework, starting from multi-view videos to dense 3D tracking with Dyn3DGS optimization, then to learning object dynamics via a graph-based neural network, and finally enabling action-conditioned video prediction and model-based planning.", "section": "3 Method"}, {"figure_path": "2410.18912/figures/figures_6_0.png", "caption": "Figure 3: Qualitative Results of 3D Gaussian Tracking. We demonstrate point-level correspondence on the objects across various timesteps. Please check our website for more videos showcasing precise dense tracking even under different object deformations and occlusions.", "description": "Figure 3 shows qualitative results of 3D Gaussian tracking demonstrating point-level correspondence on cloth, rope, and toy doll objects across various time steps.", "section": "4.2 3D Tracking with Dynamic 3D Gaussians"}, {"figure_path": "2410.18912/figures/figures_7_0.png", "caption": "Figure 4: Qualitative Results of Action-Conditioned 3D Video Prediction. Our videos are generated by rendering predicted Gaussians on virtual backgrounds. Robot trajectories are visualized as curved lines (yellow: current end-effector positions, purple: history end-effector positions). Compared to the MPM baseline, our video prediction results align with the ground truth frames (GT) more accurately.", "description": "Figure 4 shows a comparison of action-conditioned 3D video prediction results between the proposed method and the MPM baseline, demonstrating the superior accuracy of the proposed method in aligning with ground truth frames.", "section": "4.3 Action-Conditioned Video Prediction"}]