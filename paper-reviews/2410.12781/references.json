{"references": [{" publication_date": "2021", "fullname_first_author": "Ben Mildenhall", "paper_title": "Nerf: Representing scenes as neural radiance fields for view synthesis", "reason": "This paper introduced NeRF, a foundational method in neural rendering that revolutionized the field.  Its impact on the approach described in the current paper is significant as it laid the groundwork for implicit scene representations, although NeRF itself is slow and requires per-scene optimization, which is addressed by the current method.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Bernhard Kerbl", "paper_title": "3d gaussian splatting for real-time radiance field rendering", "reason": "This paper introduced 3D Gaussian splatting, a significant advancement in efficiency for neural rendering, which is directly relevant to this paper. The paper improves the rendering speed significantly over previous methods, while maintaining high quality.  This paper's contribution serves as the basis for improving the speed in the current method, reducing the time taken for optimization.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "David Charatan", "paper_title": "pixelsplat: 3d gaussian splats from image pairs for scalable generalizable 3d reconstruction", "reason": "This paper is highly relevant to this work as it introduced a novel method for generalizable 3D Gaussian splatting reconstruction.  It also used a similar approach that combines several aspects (like self-attention) with Gaussian splatting. The study presented in this paper builds on the approach used in this paper.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Yuedong Chen", "paper_title": "Mvsplat: Efficient 3d gaussian splatting from sparse multi-view images", "reason": "This paper presents another approach for efficient 3D Gaussian splatting. This work presents a methodology very similar to the current paper, however, the approach in this paper is significantly more scalable.  This paper is also similar because it uses a feed-forward reconstruction network.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Jiahao Li", "paper_title": "Instant3d: Fast text-to-3d with sparse-view generation and large reconstruction model", "reason": "This paper is important because it demonstrates the use of large reconstruction models. The paper describes a method which has very similar architecture compared to the current paper, but only handles a small number of input images (around 4). The current work extends the same idea to large-scale input images.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Kai Zhang", "paper_title": "Gs-lrm: Large reconstruction model for 3d gaussian splatting", "reason": "This paper proposes the use of the Large Reconstruction Model (LRM) which is directly used as the backbone of the current model's architecture.  This paper was influential in choosing to use transformers, and it also established the use of tokenization for the input images.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Albert Gu", "paper_title": "Mamba: Linear-time sequence modeling with selective state spaces", "reason": "This paper introduces Mamba, a novel state-space model that offers linear time complexity. This model's efficiency is crucial for processing long sequences of images and is directly incorporated into the architecture of the current paper to improve speed.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Tri Dao", "paper_title": "Transformers are ssms: Generalized models and efficient algorithms through structured state space duality", "reason": "This paper is important because it establishes a relationship between transformers and state-space models (SSMs). This helps improve the understanding of why the current model uses a hybrid approach, combining the strengths of both transformers and SSMs.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Tri Dao", "paper_title": "Transformers are ssms: Generalized models and efficient algorithms through structured state space duality", "reason": "This paper establishes the theoretical foundation that shows the equivalence of transformers and state-space models. This theoretical insight is crucial to understand why the current paper uses a hybrid architecture combining both transformers and Mamba2, leveraging the strengths of both.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Jonathan T Barron", "paper_title": "Mip-nerf 360: Unbounded anti-aliased neural radiance fields", "reason": "This paper is a significant advancement in NeRF technology which addresses limitations of the original NeRF architecture. This work is important because it shows a direction for improving the resolution and efficiency which is also an indirect contribution to the current model.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Jonathan T Barron", "paper_title": "Zip-nerf: Anti-aliased grid-based neural radiance fields", "reason": "This paper is highly relevant because it provides another high quality solution for neural radiance fields. This paper showcases a different approach to neural radiance fields which leads to high-quality reconstruction, and the lessons learned from this paper are applicable in the context of the current paper.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Ben Mildenhall", "paper_title": "Nerf: Representing scenes as neural radiance fields for view synthesis", "reason": "This paper is a foundational paper for the field of neural rendering, introducing the concept of Neural Radiance Fields (NeRFs). While NeRFs are slow, they set a new benchmark for the quality of novel view synthesis, paving the way for approaches like the one presented in the current paper which aim to improve the speed without compromising the quality significantly.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Bernhard Kerbl", "paper_title": "3d gaussian splatting for real-time radiance field rendering", "reason": "This paper significantly advanced the efficiency of neural rendering by introducing 3D Gaussian splatting (3D GS).  This efficiency improvement serves as a crucial motivation and comparison point for the current paper, which aims to improve the speed even further via a novel feed-forward approach.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Lu Ling", "paper_title": "Dl3dv-10k: A large-scale scene dataset for deep learning-based 3d vision", "reason": "This paper introduced the DL3DV dataset, a crucial component of the experiments in the current paper.  The large scale of the dataset and the quality of the data allow the researchers to train a model that is capable of handling long sequences of images and creating high-quality 3D reconstructions.", "section_number": 4}, {" publication_date": "2017", "fullname_first_author": "Arno Knapitsch", "paper_title": "Tanks and temples: Benchmarking large-scale scene reconstruction", "reason": "This paper introduced the Tanks and Temples dataset, another important dataset used for evaluation in this paper.  The use of this dataset allows the researchers to compare the performance of their model to previous methods and demonstrate its effectiveness on a widely used benchmark.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Lihe Yang", "paper_title": "Depth anything: Unleashing the power of large-scale unlabeled data", "reason": "This paper is important because it offers a method for estimating depth maps from images, which is used for regularization purposes in this paper.  The depth estimates assist in stabilizing the training process.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Tri Dao", "paper_title": "Mamba2 blocks", "reason": "This paper presents Mamba2, a crucial component of the proposed architecture.  Mamba2 is an improved version of the Mamba state-space model, offering better efficiency and enabling the model to handle the extremely long sequences of images used in this work.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Tri Dao", "paper_title": "Flashattention-2: Faster attention with better parallelism and work partitioning", "reason": "This paper introduces FlashAttentionV2, an optimization technique used to improve GPU utilization for long sequences.  The use of this technique is essential for enabling efficient training and inference for the model.", "section_number": 4}, {" publication_date": "2017", "fullname_first_author": "Ilya Loshchilov", "paper_title": "Decoupled weight decay regularization", "reason": "This paper introduced the AdamW optimizer, a variant of the Adam optimizer that incorporates decoupled weight decay.  This optimizer is used in the training process of the Long-LRM model, demonstrating its importance to the overall training procedure and efficiency.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Yue Liu", "paper_title": "Dmv3d: Denoising multi-view diffusion using 3d large reconstruction model", "reason": "This paper is important for the model architecture and training approach as it provides a baseline approach that is related to this paper's model architecture. The study in this paper showcases the usefulness of using large reconstruction models for high-quality 3D reconstructions. ", "section_number": 2}]}