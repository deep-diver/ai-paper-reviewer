[{"content": "| Zero\nShot | Net. / Post./\nw/o LiDAR | 384x512 | 768x1024 | 1440x1920 |\n|---|---|---|---|---|---|\n| L1 \u2193 | RMSE \u2193 | L1 \u2193 | RMSE \u2193 | L1 \u2193 | RMSE \u2193 |\n| No | **Ours** | **0.0135** | **0.0326** | **0.0132** | **0.0315** |\n| | MSPF | 0.0153 | 0.0369 | 0.0149 | 0.0362 |\n| | Depth Pro* | 0.0437 | 0.0672 | 0.0435 | 0.0665 |\n| | DepthAny. v2* | 0.0464 | 0.0715 | 0.0423 | 0.0660 |\n| | ZoeDepth* | 0.0831 | 0.2873 | 0.0679 | 0.1421 |\n| | Depth Pro* | 0.1222 | 0.1424 | 0.1225 | 0.1427 |\n| | DepthAny. v2* | 0.0978 | 0.1180 | 0.0771 | 0.0647 |\n| | ZoeDepth* | 0.2101 | 0.2784 | 0.1780 | 0.2319 |\n| Yes | **Ours<ts>syn</ts>** | **0.0161** | **0.0376** | **0.0163** | **0.0371** |\n| | D.P. | 0.0251 | 0.0422 | 0.0253 | 0.0422 |\n| | BPNet | 0.1494 | 0.2106 | 0.1493 | 0.2107 |\n| | ARKit Depth | 0.0251 | 0.0424 | 0.0250 | 0.0423 |\n| | DepthAny. v2 | 0.0716 | 0.1686 | 0.0616 | 0.1368 |\n| | DepthAny. v1 | 0.0733 | 0.1757 | 0.0653 | 0.1530 |\n| | Metric3D v2 | 0.0626 | 0.2104 | 0.0524 | 0.1721 |\n| | ZoeDepth | 0.1007 | 0.1917 | 0.0890 | 0.1627 |\n| | Lotus | 0.0853 | 0.1793 | 0.1038 | 0.1782 |\n| | Marigold | 0.0908 | 0.1849 | 0.0807 | 0.1565 |\n| | Metric3D v2 | 0.1777 | 0.2766 | 0.1663 | 0.2491 |\n| | ZoeDepth | 0.6158 | 0.9577 | 0.5688 | 0.6129 |", "caption": "Table 1: Quantitative comparisons on ARKitScenes dataset.\nThe terms Net., Post. and w/o LiDAR refer to the LiDAR depth usage of models, where \u201cNet.\u201d denotes network fusion, \u201cPost.\u201d indicates post-alignment using RANSAC, and \u201cw/o LiDAR\u201d means the output is metric depth.\nMethods marked with \u2217 are finetuned with their released models and code on ARKitScenes\u00a0[3] and ScanNet++\u00a0[72] datasets.", "description": "This table presents a quantitative comparison of various depth estimation methods on the ARKitScenes dataset.  The methods are categorized by their usage of LiDAR data: Network fusion integrates LiDAR within the model, Post-alignment uses LiDAR with RANSAC for refinement after initial depth prediction, and w/o LiDAR indicates methods that don't use LiDAR but produce metric depth directly.  Methods marked with an asterisk have been fine-tuned on both ARKitScenes and ScanNet++ datasets.  The table columns represent different output resolutions and metrics for depth evaluation. \"Zero Shot\" denotes models that predict depth in a zero-shot manner and \"Ours_syn\" represents a model trained only on synthetic data. The metrics include L1 loss, RMSE, relative depth error. The lower the better for those metrics.", "section": "4. Experiments"}, {"content": "| Zero Shot | Net. / Post. / w/o LiDAR | Depth Estimation |       |       | TSDF Reconstruction |         |          |        |\n| -------- | --------------------------- | :----------------: | :------------: | :--------: | :---------------: | :------: | :-------: | :-----: |\n|         |                            | L1 \u2193 | RMSE \u2193 | AbsRel \u2193 | \\[\ndelta_{0.5} \\] \u2191 | Acc \u2193 | Comp \u2193 | Prec \u2191 | Recall \u2191 | F-score \u2191 |\n|   No   |       **Ours**            | **0.0250** | **0.0829** | **0.0175** | **0.9781** | **0.0699** | **0.0616** | **0.7255** | **0.8187** | **0.7619** |\n|         | MSPF*                       | 0.0326 | 0.0975 | 0.0226 | 0.9674 | 0.0772 | 0.0695 | 0.6738 | 0.7761 | 0.7133 |\n|         | DepthAny. v2*               | 0.0510 | 0.1010 | 0.0371 | 0.9437 | 0.0808 | 0.0735 | 0.6275 | 0.7107 | 0.6595 |\n|         | ZoeDepth*                    | 0.0582 | 0.1069 | 0.0416 | 0.9325 | 0.0881 | 0.0801 | 0.5721 | 0.6640 | 0.6083 |\n|         | DepthAny. v2*               | 0.0903 | 0.1347 | 0.0624 | 0.8657 | 0.1264 | 0.0917 | 0.4256 | 0.5954 | 0.4882 |\n|         | ZoeDepth*                    | 0.1675 | 0.1984 | 0.1278 | 0.5807 | 0.1567 | 0.1553 | 0.2164 | 0.2553 | 0.2323 |\n|  Yes   | **Ours<sub>syn</sub>** | **0.0327** | **0.0966** | **0.0224** | **0.9700** | **0.0746** | **0.0666** | **0.6903** | **0.7931** | **0.7307** |\n|         | D.P.                        | 0.0353 | 0.0983 | 0.0242 | 0.9657 | 0.0820 | 0.0747 | 0.6431 | 0.7234 | 0.6734 |\n|         | ARKit Depth                 | 0.0351 | 0.0987 | 0.0241 | 0.9659 | 0.0811 | 0.0743 | 0.6484 | 0.7280 | 0.6785 |\n|         | DepthAny. v2                 | 0.0592 | 0.1145 | 0.0402 | 0.9404 | 0.0881 | 0.0747 | 0.5562 | 0.6946 | 0.6127 |\n|         | Depth Pro                   | 0.0638 | 0.1212 | 0.0510 | 0.9212 | 0.0904 | 0.0760 | 0.5695 | 0.6916 | 0.6187 |\n|         | Metric3D v2                 | 0.0585 | 0.3087 | 0.0419 | 0.9529 | 0.0785 | 0.0752 | 0.6216 | 0.6994 | 0.6515 |\n|         | Marigold                    | 0.0828 | 0.1412 | 0.0603 | 0.8718 | 0.0999 | 0.0781 | 0.5128 | 0.6694 | 0.5740 |\n|         | DepthPro                    | 0.2406 | 0.2836 | 0.2015 | 0.5216 | 0.1537 | 0.1467 | 0.2684 | 0.3752 | 0.3086 |\n|         | Metric3D v2                 | 0.1226 | 0.3403 | 0.0841 | 0.8009 | 0.0881 | 0.0801 | 0.5721 | 0.6640 | 0.6083 |", "caption": "Table 2: Quantitative comparisons on ScanNet++ dataset.\nThe terms Net., Post. and w/o LiDAR refer to the LiDAR depth usage of models as the last table.\nMethods marked with \u2217 are finetuned with their released code on ARKitScenes\u00a0[3] and ScanNet++\u00a0[72] datasets.", "description": "This table presents a quantitative evaluation of various depth estimation models on the ScanNet++ dataset. The metrics used for evaluation include L1, RMSE, AbsRel, Accuracy (Acc.), Completeness (Comp.), Precision (Prec.), Recall, and F-score.  The table categorizes the models based on their usage of LiDAR data (Net.: Network Fusion; Post.: Post-alignment; w/o LiDAR: metric depth output without LiDAR input). Some models were finetuned using additional datasets (ARKitScenes and ScanNet++). The zero-shot models, denoted as \"Ourssyn\" and others, were trained on synthetic data only. The table aims to showcase the effectiveness of the proposed method, \"Ours,\" in comparison to existing state-of-the-art methods for depth estimation.", "section": "4. Experiments -> 4.2. Comparisons with the State of the Art"}, {"content": "| | ARKitScenes |  | ScanNet++ |  |  |\n|---|---|---|---|---|---| \n|  | L1 \u2193 | AbsRel \u2193 | Acc \u2193 | Comp \u2193 | F-Score \u2191 |\n|---:|---:|---:|---:|---:|---:|\n| (a) Ours<sub>syn</sub> (synthetic data) | 0.0163 | 0.0142 | 0.0746 | 0.0666 | 0.7307 |\n| (b) w/o prompting | 0.0605 | 0.0505 | 0.0923 | 0.0801 | 0.5696 |\n| (c) w/o foundation model | 0.0194 | 0.0169 | 0.0774 | 0.0713 | 0.7077 |\n| (d) AdaLN prompting | 0.0197 | 0.0165 | 0.0795 | 0.0725 | 0.6943 |\n| (e) Cross-atten. prompting | 0.0523 | 0.0443 | 0.0932 | 0.0819 | 0.5595 |\n| (f) Controlnet prompting | 0.0239 | 0.0206 | 0.0785 | 0.0726 | 0.6899 |\n| (g) a + ARKitScenes data | 0.0134 | 0.0115 | 0.0744 | 0.0662 | 0.7341 |\n| (h) g + ScanNet++ anno. GT | 0.0132 | 0.0114 | 0.0670 | 0.0614 | 0.7647 |\n| (i) g + ScanNet++ pseudo GT | 0.0139 | 0.0121 | 0.0835 | 0.0766 | 0.6505 |\n| (j) **Ours** (h,i+edge loss) | 0.0132 | 0.0115 | 0.0699 | 0.0616 | 0.7619 |", "caption": "Table 3: Quantitative ablations on ARKitScenes and ScanNet++ datasets. Please refer to Sec.\u00a04.3 for detailed descriptions.", "description": "This table presents the ablation study results of the proposed method, Prompt Depth Anything,  quantitatively evaluated on ARKitScenes and ScanNet++ datasets using L1, AbsRel, Acc, Comp, and F-score metrics.  Different settings are explored including: using synthetic data only,  removing prompting, removing foundation model pre-training, using different prompting architectures (AdaLN, Cross-Attention, and ControlNet), and using and combining different real datasets (ARKitScenes and ScanNet++ with annotated and pseudo ground truth).  The table analyzes the impact of prompting a depth foundation model, different architecture designs, training data, and an edge-aware loss on the performance.", "section": "4.3. Ablations and Analysis"}, {"content": "| Input Signal | Diffusive | Diffusive | Transparent | Specular |\n|---|---|---|---|---| \n| | Red Can | Green Can | | |\n| **Ours** | **1.0/1.0/1.0** | **1.0/1.0/1.0** | **0.3/1.0/1.0** | **0.8/1.0/0.9** |\n| LiDAR | 1.0/1.0/1.0 | 1.0/1.0/0.2 | 0.5/0.4/0.0 | 0.7/1.0/0.0 |\n| RGB | 1.0/1.0/0.0 | 1.0/1.0/0.0 | 0.2/1.0/0.0 | 0.0/0.9/0.9 |", "caption": "Table 4: Grasping success rate on various objects. Three numbers indicate objects placed at near, middle, and far positions. The grasping policy is trained on diffusive and tested on all objects.", "description": "This table presents the grasping success rates of a robotic arm attempting to grasp various objects (Red Can, Green Can, Transparent, Specular) placed at different distances (Near, Mid, and Far). The grasping policy used was trained only on diffusive objects (red and green cans). The success rate is evaluated by how often the robot places each object into the designated box from the positions near, middle, and far. The table shows that depth data obtained from the proposed model generally exhibits better performance compared to using RGB images or LiDAR depth from the iPhone, especially for grasping transparent or specular objects.", "section": "4.6. Application: Generalized Robotic Grasping"}, {"content": "|                       | ARKitScenes | ARKitScenes | ScanNet++ | ScanNet++ | ScanNet++ | \n| :-------------------- | :----------- | :----------- | :-------- | :-------- | :-------- | \n|                       | L1 \u2193        | AbsRel \u2193     | Acc \u2193     | Comp \u2193    | F-Score \u2191 | \n| --------------------- | ----------- | ----------- | -------- | -------- | -------- | \n| (a) Depth Any. as foundation | 0.0132      | 0.0115      | 0.0699   | 0.0616   | 0.7619   | \n| (b) Depth Pro as foundation | 0.0169      | 0.0150      | 0.0754   | 0.0676   | 0.7202   | \n| (c) Depth Pro        | 0.1225      | 0.1038      | 0.0904   | 0.0760   | 0.6187   |", "caption": "Table 5: Additional quantitative ablations.\nPlease refer to Sec.\u00a0A.4 for detailed descriptions.", "description": "This table presents additional quantitative ablation study results on ARKitScenes and ScanNet++ datasets by replacing the depth foundation model from Depth Anything to Depth Pro.  It shows that using Depth Anything as the base model yields better performance, as indicated in row (a). It also shows that while DepthPro (with prompting) in row (b) performs better than the original Depth Pro in row (c), it still doesn't match the performance of Depth Anything with prompting in row (a).", "section": "4.3. Ablations and Analysis"}, {"content": "| Metric | Definition |\n|---|---| \n| L1 | $\\frac{1}{N}\\sum_{i=1}^{N}|\\mathbf{D}_{i}-\\hat{\\mathbf{D}}_{i}|$ |\n| RMSE | $\\sqrt{\\frac{1}{N}\\sum_{i=1}^{N}(\\mathbf{D}_{i}-\\hat{\\mathbf{D}}_{i})^{2}}$ |\n| AbsRel | $\\frac{1}{N}\\sum_{i=1}^{N}|\\mathbf{D}_{i}-\\hat{\\mathbf{D}}_{i}|/\\mathbf{D}_{i}$ |\n| $\\delta_{0.5}$ | $\\frac{1}{N}\\sum_{i=1}^{N}\\mathbb{I}\\left(\\max\\left(\\frac{\\mathbf{D}_{i}}{\\hat{\\mathbf{D}}_{i}},\\frac{\\hat{\\mathbf{D}}_{i}}{\\mathbf{D}_{i}}\\right)&lt;1.25^{0.5}\\right)$ |", "caption": "Table 6: Depth metric definitions. \ud835\udc03\ud835\udc03\\mathbf{D}bold_D and \ud835\udc03^^\ud835\udc03\\hat{\\mathbf{D}}over^ start_ARG bold_D end_ARG are the ground-truth and predicted depth, respectively. \ud835\udd40\ud835\udd40\\mathbb{I}blackboard_I is the indicator function.", "description": "This table defines the metrics used to evaluate depth estimation performance in the paper.  Lower values for L1, RMSE, and AbsRel indicate better performance, while a higher value for \u03b4<0.5 indicates better performance. Specifically:\n\n* **L1:** Average absolute difference between the predicted depth and the ground truth depth.\n* **RMSE:** Root mean squared error between the predicted depth and the ground truth depth.\n* **AbsRel:** Average relative absolute difference between the predicted depth and the ground truth depth.\n* **\u03b4<0.5:** Percentage of pixels where the ratio between the predicted depth and the ground truth depth is within a threshold of 0.5, indicating higher accuracy.", "section": "C.3. Evaluation Metrics"}, {"content": "| Metric | Definition |\n|---|---| \n| Acc | $\\mbox{mean}_{p\\in P}(\\min_{p^{*}\\in P^{*}}||p-p^{*}||)$ |\n| Comp | $\\mbox{mean}_{p^{*}\\in P^{*}}(\\min_{p\\in P}||p-p^{*}||)$ |\n| Prec | $\\mbox{mean}_{p\\in P}(\\min_{p^{*}\\in P^{*}}||p-p^{*}||&lt;.05)$ |\n| Recal | $\\mbox{mean}_{p^{*}\\in P^{*}}(\\min_{p\\in P}||p-p^{*}||&lt;.05)$ |\n| F-score | $\\frac{2\\times\\text{Perc}\\times\\text{Recal}}{\\text{Prec}+\\text{Recal}}$ |", "caption": "Table 7: Reconstruction metric definitions. P\ud835\udc43Pitalic_P and P\u2217superscript\ud835\udc43P^{*}italic_P start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT are the point clouds sampled from predicted and ground truth mesh.", "description": "This table defines metrics used to evaluate the quality of 3D reconstruction. The metrics compare point clouds *P* (predicted) and *P*** (ground truth) and include Accuracy (Acc), Completeness (Comp), Precision (Prec), Recall, and F-score.  These metrics measure how well the predicted point cloud aligns with the ground truth point cloud. Lower values for Acc and Comp indicate better performance, while higher values for Prec, Recall and F-score indicate better performance.", "section": "C. More Details"}]