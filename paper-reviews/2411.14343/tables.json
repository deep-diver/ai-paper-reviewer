[{"content": "| Language (ISO) | Fraction of CC | # Speakers(M) | Geographical Region |\n|---|---|---|---| \n| Hausa (hau) | 0.0036% | 80 | Nigeria, Chad, Cameroon, Ghana |\n| Pashto (pus) | 0.0033% | 60 | Afghanistan, Pakistan |\n| Amharic (amh) | 0.0036% | 60 | Ethiopia |\n| Yoruba (yor) | 0.0011% | 50 | Benin, Nigeria, Togo |\n| Sundanese (sun) | 0.0011% | 40 | Indonesia |\n| Sindhi (snd) | 0.0017% | 30 | Pakistan, India |\n| Zulu (zul) | 0.0016% | 30 | South Africa, Lesotho |", "caption": "Table 1: Details of 7 languages used for Data Collection Evaluation", "description": "This table lists seven low-resource languages selected for data collection and evaluation in the paper. For each language, it provides its ISO code, the approximate fraction of its presence in the Common Crawl dataset (specifically the CC-MAIN-2023-14 archive), the estimated number of speakers in millions, and the geographical regions where these languages are primarily spoken.", "section": "4.1 Languages and Benchmark Datasets and Dataset Collection"}, {"content": "| Languages (ISO) | Size | Max Size |\n|---|---|---|\n| Hausa (hau) | 2.1 | 7 |\n| Pashto (pus) | 5.5 | 20 |\n| Amharic (amh) | 4.0 | 24 |\n| Yoruba (yor) | 0.9 | 2 |\n| Sundanese (sun) | 1.9 | 6 |\n| Sindhi (snd) | 4.2 | 15 |\n| Zulu (zul) | 1.7 | 6 |", "caption": "Table 2: UnifiedCrawl-Language Dataset Size. The Size and Max Size are in GBs", "description": "This table presents the sizes of the UnifiedCrawl datasets for seven different low-resource languages. The 'Size' column indicates the size of the dataset containing only the specified language, while the 'Max Size' column provides an estimated upper bound on the dataset size if documents containing multiple languages are included.  The sizes are given in gigabytes (GB).", "section": "5.1 Data Collection Evaluation"}, {"content": "| Languages (ISO) | OSCAR | mC4 | CC-100 | Wikipedia | UnifiedCrawl |\n|---|---|---|---|---|---| \n| Hausa (hau) | - | 850 | 60 | 60 | 2100 |\n| Pashto (pus) | 380 | 1500 | 110 | 100 | 5500 |\n| Amharic (amh) | 380 | 1200 | 130 | 20 | 4000 |\n| Yoruba (yor) | 0.1 | 160 | 1 | 20 | 900 |\n| Sundanese (sun) | 0.2 | 460 | 20 | 40 | 1900 |\n| Sindhi (snd) | 360 | 4000 | 70 | 40 | 4200 |\n| Zulu (zul) | - | 840 | 4 | 6 | 1700 |", "caption": "Table 3: Size of UnifiedCrawl-Language vs. Prior Works", "description": "This table compares the size of the UnifiedCrawl dataset for several low-resource languages to the size of other commonly used datasets in prior works, such as OSCAR, mC4, CC-100, and Wikipedia. The size of each dataset is presented in Megabytes (MB).  It highlights the significantly larger scale of the UnifiedCrawl dataset compared to existing resources for the same low-resource languages.", "section": "4.1 Languages and Benchmark Datasets and Dataset Collection"}, {"content": "| Models | PPL |\n|---|---| \n| XGLM-564M | 14,974.70 |\n| XGLM-564M (ours) | **105.5** |\n| XGLM-4.5B | 35.6 |\n| XGLM-4.5B (ours) | **19.6** |", "caption": "Table 4: Language Modeling Evaluation on Amharic", "description": "This table presents the results of language modeling evaluation on the Amharic language. It compares the perplexity (PPL) scores of the original XGLM-564M and XGLM-4.5B models with their counterparts fine-tuned using QLoRA on the UnifiedCrawl-Amharic dataset. Lower perplexity indicates better performance in predicting the next word in a sequence.", "section": "5.2.1 Language Modeling Evaluation"}, {"content": "| Models | F1 | EM |\n|---|---|---|\n| XGLM-564M | 0 | 0 |\n| XGLM-564M (ours) | 0 | 0 |\n| XGLM-4.5B | 8.0 | 1.3 |\n| XGLM-4.5B (ours) | 9.9 | 2.3 |", "caption": "Table 5: Few-shot Prompting Score on AmQA.", "description": "This table presents the results of few-shot prompting on the Amharic Question Answering (AmQA) dataset.  It compares the performance of several XGLM models, both original and fine-tuned using QLoRA on the UnifiedCrawl-Amharic dataset. The results are measured using F1 and Exact Match (EM) scores, common metrics for evaluating question answering performance.", "section": "5.2 Downstream Few Shot Prompting"}, {"content": "| Model | LM PPL | Few-shot F1 | Few-shot EM |\n|---|---|---|---| \n| XGLM-564M (full finetune) | **76.7** | 0 | 0 |\n| XGLM-564M (ours) | 105.6 | 0 | 0 |\n| XGLM-4.5B (full finetune) | OOM | - | - |\n| XGLM-4.5B (ours) | **19.6** | **9.9** | **2.3** |", "caption": "Table 6: Comparison of QLoRA with Full-fine-tuning", "description": "This table compares the performance of using the QLoRA method (Quantized Low-Rank Adaptation) versus full fine-tuning for training large language models.  It shows the Language Modeling Perplexity (LM PPL) on the UnifiedCrawl-Amharic dataset and the few-shot prompting F1 and EM scores on the AmQA (Amharic Question Answering) dataset for both XGLM-564M and XGLM-4.5B models.  The results highlight the trade-off between model size, training method, and performance.", "section": "6 Ablation Studies"}, {"content": "| Model | LM PPL | Few-shot F1 | Few-shot EM |\n|---|---|---|---| \n| GPT2-74M (scratch) | 105.2 | 1.2 | 0 |\n| GPT2-110M (scratch) | 106.1 | 1.3 | 0 |\n| XGLM-4.5B (Ours) | **19.6** | **9.9** | **2.3** |", "caption": "Table 7: Comparison of QLoRA with training from scratch", "description": "This table compares the performance of fine-tuning a pre-trained language model using the QLoRA method against training a model from scratch.  It shows the language modeling perplexity (LM PPL) on the Amharic language dataset and the few-shot prompting F1 and EM scores on the AmQA Question Answering dataset for different model sizes. The results highlight the efficiency and effectiveness of QLoRA in achieving comparable or better performance with significantly reduced computational resources.", "section": "6 Ablation Studies"}, {"content": "| Models | PPL | F1 | EM |\n|---|---|---|---|\n| XGLM-564M (QLoRA) | 99.4 | 0.6 | 0.2 |\n| XGLM-564M (ours) | 59.2 | 2.9 | 0.7 |\n| XGLM-4.5B (QLoRA) | 2.2 | 35.0 | 20.5 |\n| XGLM-4.5B (ours) | 2.2 | 34.7 | 20 |", "caption": "Table 8: Supervised-Training Score on AmQA.", "description": "This table presents the results of supervised training on the Amharic Question Answering (AmQA) dataset.  It compares the performance of several models, including the original XGLM-564M and XGLM-4.5B models, and their respective counterparts fine-tuned using QLoRA on the UnifiedCrawl-Amharic dataset.  The evaluation metrics used are Perplexity (PPL), F1 score, and Exact Match (EM) score, providing a comprehensive assessment of the models' performance on a downstream question-answering task.", "section": "6.3 Comparison on Downstream Supervised Training"}, {"content": "| Model Type | Multilingual LLMs | Size (# Params) | # Languages |\n|---|---|---|---| \n| **Encoder-Only** | mBERT [Devlin et al. (2019)](https://arxiv.org/html/2411.14343v1#bib.bib17) | 180M | 104 |\n|  | XLM-R [Conneau et al. (2020)](https://arxiv.org/html/2411.14343v1#bib.bib13) | 225M-10.7B | 15/100 |\n|  | XY-LENT [Patra et al. (2023)](https://arxiv.org/html/2411.14343v1#bib.bib40) | 480M-2.1B | 21 |\n| **Decoder-Only** | XGLM [Lin et al. (2022)](https://arxiv.org/html/2411.14343v1#bib.bib29) | 540M-7.5B | 30/134 |\n|  | mGPT [Tan et al. (2022)](https://arxiv.org/html/2411.14343v1#bib.bib46) | 1.3B | 101 |\n|  | PaLM [Chowdhery et al. (2023)](https://arxiv.org/html/2411.14343v1#bib.bib11) | 540B | 122 |\n|  | BLOOM [Scao et al. (2022)](https://arxiv.org/html/2411.14343v1#bib.bib45) | 560M-175B | 46 |\n|  | BLOOMZ [Muennighoff et al. (2023)](https://arxiv.org/html/2411.14343v1#bib.bib36) | 560M-175B | 46 |\n|  | GPT-3 [Brown et al. (2020)](https://arxiv.org/html/2411.14343v1#bib.bib10) | 175B | 1 |\n| **Encoder-Decoder** | mT5 [Xue et al. (2021)](https://arxiv.org/html/2411.14343v1#bib.bib52) | 580M-13B | 101 |\n|  | mT0 [Muennighoff et al. (2023)](https://arxiv.org/html/2411.14343v1#bib.bib36) | 580M-13B | 101 |\n|  | mBART [Liu et al. (2020)](https://arxiv.org/html/2411.14343v1#bib.bib31) | 680M | 25 |", "caption": "Table 9: Overview of Multilingual LLMs", "description": "This table provides an overview of various multilingual Large Language Models (LLMs), categorized by their model type (Encoder-Only, Decoder-Only, or Encoder-Decoder), size (in number of parameters), and the number of languages they support.  It showcases the diversity of approaches and scales in multilingual LLM development.", "section": "2 Related Works"}]