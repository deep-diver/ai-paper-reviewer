[{"figure_path": "https://arxiv.org/html/2412.06673/x1.png", "caption": "Figure 1: Performance on various visual understanding (blue for General and green for Document-oriented), generation (red), and editing (gray) benchmarks. ILLUME\u00a0achieves competitive results with state-of-the-art works.", "description": "This figure presents a radar chart comparing the performance of ILLUME against other state-of-the-art models across various benchmark datasets for visual understanding, image generation, and image editing tasks.  The blue sections represent the visual understanding benchmarks; specifically, blue denotes general visual understanding tasks, and green indicates document-oriented visual understanding benchmarks.  Red sections depict results for image generation tasks, while gray represents image editing tasks. The chart visually demonstrates that ILLUME achieves competitive performance compared to the other models in all three categories.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2412.06673/x2.png", "caption": "Figure 2: ILLUME\u00a0can handle various multimodal tasks, including understanding for images and charts; text-to-image generation; and mixed-modal generation task such as object modification and style transfer.", "description": "Figure 2 showcases ILLUME's capabilities across diverse multimodal tasks.  The top row demonstrates image understanding, highlighting ILLUME's ability to correctly answer questions about diverse image types, including diagrams, charts, and photographs. The middle row exemplifies text-to-image generation; given a textual prompt, ILLUME generates a corresponding image. The bottom row displays mixed-modal generation capabilities, specifically showcasing object removal, modification, inpainting, and style transfer.  These examples illustrate ILLUME's proficiency in understanding and generating various multimodal data.", "section": "3. ILLUME"}, {"figure_path": "https://arxiv.org/html/2412.06673/x3.png", "caption": "Figure 3: Overall architecture of ILLUME. (a) We enhance LLMs with the capability to \u201csee\u201d images by employing a vision adapter that maps features from a vision encoder into LLM\u2019s input spaces. To expand the model\u2019s abilities to generate images, the LLM is extended with an additional vision vocabulary to produce discrete vision tokens. (b) In the vision tokenizer, we utilize a pretrained vision encoder to extract semantic features and supervise quantization process through feature reconstruction loss. The reconstructed features are then processed by a Stable Diffusion model to recover the original images.", "description": "ILLUME's architecture is shown in Figure 3, with (a) illustrating how the LLM is enhanced to process image features via a vision adapter and additional vision vocabulary for image generation.  Discrete vision tokens are created to represent the images.  Part (b) details the vision tokenizer which uses a pre-trained encoder to extract semantic image features.  A quantization process, guided by feature reconstruction loss, converts these features into discrete tokens. Finally, a Stable Diffusion model reconstructs the original image from these tokens.", "section": "3. ILLUME"}, {"figure_path": "https://arxiv.org/html/2412.06673/x4.png", "caption": "Figure 4: Overview of the three-stage training procedure and its corresponding data composition of different stages in MLLM training.", "description": "This figure illustrates the three-stage training process for the ILLUME model.  Stage 1 focuses on initializing visual embeddings using a smaller dataset, primarily for image-to-text alignment. Stage 2 refines the model's understanding and generation capabilities through text-to-image alignment using a larger dataset.  Stage 3 involves fine-tuning the model on various multimodal tasks for improved accuracy and versatility using a diverse set of training data. The figure also shows the data composition for each stage, highlighting the different data types used (e.g., image-to-text, text-to-image, mixed-modal generation). This visualization helps to understand how the ILLUME model is trained progressively using different data and tasks to achieve efficient visual understanding and generation.", "section": "3. ILLUME"}, {"figure_path": "https://arxiv.org/html/2412.06673/x5.png", "caption": "Figure 5: Procedure of self-enhancing multimodal alignment scheme, which contains three steps: corpus self-generation, assessment generation and SFT for multimodal alignment. This scheme supervises the MLLM to self-assess the consistency between text descriptions and self-generated images, enabling the model to more accurately interpret images and avoid potential mistakes in image generation.", "description": "This figure illustrates the self-enhancing multimodal alignment scheme, a three-step process designed to improve the model's ability to generate images that accurately reflect the provided text descriptions.  The first step involves the model generating its own image-text pairs from the training data (corpus self-generation). In the second step, the model assesses these generated pairs to identify inconsistencies between the image and text description using criteria such as object presence, color, counting, and spatial relations (assessment generation). Finally, supervised fine-tuning (SFT) leverages the assessment results from step two to train the model, improving its alignment and generating more accurate images that match the descriptions (SFT for multimodal alignment). This iterative process allows the model to learn from its own mistakes and enhance its understanding and consistency in image generation.", "section": "4. Self-Enhancing Multimodal Alignment"}, {"figure_path": "https://arxiv.org/html/2412.06673/x6.png", "caption": "Figure 6: Comparison of different tokenizers for MLLM training. We compare two types of tokenizers: 1) Reconstruction tokenizer: supervised by image reconstruction loss. 2) Semantic tokenizer: supervised by feature reconstruction loss. The results manifest that vision tokenizer with semantics significantly accelerates the convergence of MLLM pretraining.", "description": "This figure compares the performance of two different types of vision tokenizers used in training a multimodal large language model (MLLM).  The first tokenizer, a reconstruction tokenizer, is trained using image reconstruction loss, focusing on accurately recreating the input images. The second, a semantic tokenizer, is trained with a feature reconstruction loss, prioritizing the preservation of semantic information within the images.  The results show that the semantic tokenizer converges much faster during MLLM pretraining, indicating that encoding semantic meaning rather than just pixel-level detail is significantly more efficient for the task.", "section": "3. ILLUME"}, {"figure_path": "https://arxiv.org/html/2412.06673/x7.png", "caption": "Figure A: Comparison of different hyper-parameters in inference.", "description": "This figure shows how different hyperparameter choices in the inference stage affect the quality of generated images.  Specifically, it illustrates the impact of varying temperature, top-k sampling, and the classifier-free guidance scale on image generation quality.  Different settings are shown to achieve different levels of detail and visual fidelity in the output images.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.06673/x8.png", "caption": "Figure B: More qualitative results on understanding tasks. Regions that related to the QAs are marked with red ellipses.", "description": "Figure B presents more qualitative results illustrating ILLUME's performance on various image understanding tasks.  Several examples are shown, each demonstrating ILLUME's ability to understand and extract information from diverse image types, including charts, graphs, diagrams, infographics, and photographs. The areas of the images directly relevant to the question and answer pairs are highlighted with red ellipses, providing clear visual connections between the input image and the extracted information.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.06673/x9.png", "caption": "Figure C: More qualitative results on text-to-image generation tasks.", "description": "This figure displays several example images generated by ILLUME, showcasing its capabilities in text-to-image generation.  Each image is accompanied by a short text description specifying the prompt used to generate the image.  The variety of styles, objects, and levels of detail illustrate the model's versatility and ability to understand and respond to a wide range of creative instructions.", "section": "5.2. Multimodal Image Generation"}, {"figure_path": "https://arxiv.org/html/2412.06673/x10.png", "caption": "Figure D: More qualitative results on mixed-modal generation tasks.", "description": "Figure D presents qualitative results demonstrating ILLUME's capabilities in mixed-modal generation tasks. It showcases examples of single-turn editing (object removal, material modification, style transfer, color modification) and multi-turn editing tasks.  Each example shows an image, the instructions given to the model, and the generated image. This illustrates the model's ability to follow complex and multi-step instructions involving image manipulation and style changes.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.06673/x11.png", "caption": "Figure E: Data example of assessment data for self-enhancing multimodal alignment.", "description": "This figure shows an example of assessment data used in the self-enhancing multimodal alignment process.  The self-enhancing multimodal alignment scheme consists of three steps: 1. Corpus Self-generation: The model generates images from a subset of text-to-image data. 2. Assessment Generation: Inconsistencies between generated images and text descriptions are identified and analyzed based on criteria such as Object, Counting, Color, and Spatial Relation, and a JSON formatted assessment is generated. 3. Supervised Fine-tuning (SFT): The assessment data is used to refine the model, particularly focusing on correcting misalignments between images and texts. This example highlights how the model assesses the consistency between a text description and its corresponding generated image, identifying discrepancies in color and spatial relation, and then using this feedback for improvement.", "section": "4. Self-Enhancing Multimodal Alignment"}]