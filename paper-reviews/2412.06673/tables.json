[{"content": "| Method | LLM | Num. of image-text pairs | Num. of interleaved data |\n|---|---|---|---| \n| Chameleon [47] | 7B from scratch | 1.4B | 400B tokens |\n| LWM [30] | LLaMA-2-7B | 1B | - |\n| Unified IO 2 [33] | 6.8B from scratch | 970M | 157M |\n| SEED-LLaMA [15] | Vicuna-7B | 600M | 150M |\n| AnyGPT [59] | LLaMA-2 7B | 300M | 7.3M |\n| Janus [51] | DeepSeek-LLM-1.3B | 65M | - |\n| ILLUME (Ours) | Vicuna-7B | 15M | - |", "caption": "Table 1: Statistics on the data volumes required for image-text alignment in previous next-token prediction-based works. Notably, ILLUME\u00a0 utilizes only 15M image-text pairs, which is 4 times fewer than Janus, yet achieves superior performance.", "description": "This table compares the amount of image-text data used for pretraining various multimodal large language models (MLLMs) that utilize a next-token prediction approach.  It highlights the data efficiency of the proposed ILLUME model.  ILLUME requires only 15 million image-text pairs for pretraining, significantly less than other models like Janus (which used 60 million pairs). Despite using a much smaller dataset, ILLUME achieves comparable or better performance.", "section": "3. ILLUME"}, {"content": "| Tasks | GenAI-Bench | GenEval | POPE | MME-P | MMBench | SEED | MMVet |\n|---|---|---|---|---|---|---|---| \n| Gen. only | 0.63 | 0.58 | - | - | - | - |  |\n| Und. only | - | - | 84.6 | 1339.0 | 60.9 | 64.0 | 28.0 |\n| Gen. and Und. | 0.63 | 0.56 | 86.4 | 1358.6 | 61.6 | 65.0 | 27.4 |", "caption": "Table 2: Comparison between the specialist model and unified model. Joint training presents no significant negative impact on the two tasks, but it also does not obviously promote each other.", "description": "This table compares the performance of a specialist model (trained for a single task) and a unified model (trained for multiple tasks) on two different tasks: visual understanding and generation. The results indicate that joint training (training the unified model on both tasks) does not significantly hinder the model's performance in either task compared to a specialist model focused on just one task; however, it does not demonstrate a significant improvement in the unified model's performance either.  This suggests that, in this case, there aren't strong synergistic benefits between training on visual understanding and generation tasks simultaneously.", "section": "4. Self-Enhancing Multimodal Alignment"}, {"content": "| Method | LLM. | POPE | MMBench | SEED | MME-P | MM-Vet | MMMU | AI2D | VQA-text | ChartQA | DocVQA | InfoVQA | OCRBench |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---| \n| InstructBLIP [10] | Vicuna-7B | - | 36.0 | 53.4 | - | 26.2 | 30.6 | 33.8 | 50.1 | 12.5 | 13.9 | - | 276 |\n| Qwen-VL-Chat [1] | Qwen-7B | - | 60.6 | 58.2 | 1487.5 | - | 35.9 | 45.9 | 61.5 | 66.3 | 62.6 | - | 488 |\n| LLaVA-1.5 [27] | Vicuna-7B | 85.9 | 64.3 | 58.6 | 1510.7 | 31.1 | 35.4 | 54.8 | 58.2 | 18.2 | 28.1 | 25.8 | 318 |\n| ShareGPT4V [7] | Vicuna-7B | - | 68.8 | 69.7 | 1567.4 | 37.6 | 37.2 | 58 | 60.4 | 21.3 | - | - | 371 |\n| LLaVA-NeXT [28] | Vicuna-7B | 86.5 | 67.4 | 64.7 | - | 43.9 | 35.1 | 66.6 | 64.9 | 54.8 | 74.4 | 37.1 | 532 |\n| Emu3-Chat [49] | 8B from scratch | 85.2 | 58.5 | 68.2 | - | 37.2 | 31.6 | 70.0 | 64.7 | 68.6 | 76.3 | 43.8 | 687 |\n| Unified-IO 2 [33] | 6.8B from scratch | 87.7 | - | 61.8 | - | - | - | - | - | - | - | - | - |\n| Chameleon [47] | 7B from scratch | - | - | - | - | 8.3 | 22.4 | - | - | - | - | - | - |\n| LWM [30] | LLaMA-2-7B | 75.2 | - | - | - | 9.6 | - | - | 18.8 | - | - | - | - |\n| Show-o [54] | Phi-1.5B | 73.8 | - | - | 948.4 | - | 25.1 | - | - | - | - | - | - |\n| VILA-U (256) [52] | LLaMA-2-7B | 83.9 | - | 56.3 | 1336.2 | 27.7 | - | - | 48.3 | - | - | - | - |\n| VILA-U (384) [52] | LLaMA-2-7B | 85.8 | - | 59 | 1401.8 | 33.5 | - | - | 60.8 | - | - | - | - |\n| Janus [51] | DeepSeek-LLM-1.3B | 87.0 | 69.4 | 63.7 | 1338.0 | 34.3 | 30.5 | - | - | - | - | - | - |\n| ILLUME (Ours) | Vicuna-7B | 88.5 | 75.1 | 72.9 | 1445.3 | 37.0 | 38.2 | 71.4 | 72.1 | 66.7 | 76.0 | 45.5 | 669 |", "caption": "Table 3: Quantitative results on visual understanding benchmarks. Our performance is close to and even outperforms both understanding only and unified models. The performance with top-1 and top-2 value are denoted in bold and underline respectively.", "description": "Table 3 presents a quantitative comparison of ILLUME's performance on various visual understanding benchmarks against other state-of-the-art models.  These benchmarks assess different aspects of visual understanding, categorized into general and document-oriented tasks.  The table shows ILLUME achieves competitive results, often surpassing or nearly matching the performance of specialized models designed for understanding only, as well as unified models that combine understanding and generation capabilities.  Top-performing scores are highlighted in bold and underlined.", "section": "5. Experiments"}, {"content": "| Method | Params. | Type | FID | MJHQ30k | GenAI-bench | GenEval | Overall | Single Obj | Two Obj. | Counting | Colors | Position | Color Attri. |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| **Generation Only** |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| SDv1.5 [40] | 0.9B | Diffusion | - | - | - | 0.43 | 0.97 | 0.38 | 0.35 | 0.76 | 0.04 | 0.06 |\n| PixArt-\u03b1 [4] | 0.6B | Diffusion | 6.14 | - | - | 0.48 | 0.98 | 0.50 | 0.44 | 0.80 | 0.08 | 0.07 |\n| SDXL [38] | 2.6B | Diffusion | 9.55 | 0.83 | 0.63 | 0.55 | 0.98 | 0.74 | 0.39 | 0.85 | 0.15 | 0.23 |\n| Emu3-Gen [49] | 8B | Autoregressive | - | - | - | 0.54 | 0.98 | 0.71 | 0.34 | 0.81 | 0.17 | 0.21 |\n| **Unify Understanding and Generation** |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| Chameleon [47] | 7B | Autoregressive | - | - | - | 0.39 | - | - | - | - | - | - |\n| LWM [30] | 7B | Autoregressive | 17.77 | 0.63 | 0.53 | 0.47 | 0.93 | 0.41 | 0.46 | 0.79 | 0.09 | 0.15 |\n| Show-o [54] | 1.5B | Autoregressive | 15.18 | 0.70 | 0.60 | 0.53 | 0.95 | 0.52 | 0.49 | 0.82 | 0.11 | 0.28 |\n| VILA-U(256) [52] | 7B | Autoregressive | 12.81 | 0.76 | 0.64 | - | - | - | - | - | - | - |\n| VILA-U(384) [52] | 7B | Autoregressive | 7.69 | 0.73 | 0.61 | - | - | - | - | - | - | - |\n| Janus [51] | 1.3B | Autoregressive | 10.10 | - | - | 0.61 | 0.97 | 0.68 | 0.30 | 0.84 | 0.46 | 0.42 |\n| ILLUME (Ours) | 7B | Autoregressive | 7.76 | 0.75 | 0.60 | 0.61 | 0.99 | 0.86 | 0.45 | 0.71 | 0.39 | 0.28 |", "caption": "Table 4: Quantitative results on text-to-image generation benchmarks. ILLUME\u00a0achieves comparable results with specialist models and unified MLLMs. The performance with top-1 and top-2 value are denoted in bold and underline respectively.", "description": "Table 4 presents a quantitative comparison of various models' performance on text-to-image generation benchmarks.  The metrics used assess different aspects of image quality, including overall quality, object fidelity, color accuracy, and more.  The table highlights ILLUME's performance relative to specialized text-to-image models (designed solely for this task) and unified multimodal large language models (MLLMs) that combine image generation with other capabilities.  The top-performing results for each metric are emphasized in bold and underlined, clarifying the best overall performance and second-best results.", "section": "5.2 Multimodal Image Generation"}, {"content": "| Setting | Stage-1 | Stage-2 | Stage-3 |\n|---|---|---|---|\n|  | Vision adapter 1.0\u00d710\u207b\u00b3 | Vision adapter 5.0\u00d710\u207b\u2075 | Vision encoder 2.0\u00d710\u207b\u2076 |\n| LR. | Vision Embed. & Head 2.0\u00d710\u207b\u2074 | LLM 5.0\u00d710\u207b\u2075 | LLM & Vision adapter 2.0\u00d710\u207b\u2075 |\n| Batch size | 256 | 1024 | 1024 |\n| Training Step | 5000 | 15000 | 8000 |", "caption": "Table 5: Detailed hyperparameters of our ILLUME. LR denotes learning rate for training. Vision Embed. & Head refers to the vision embedding and LM head of vision part.", "description": "Table 5 presents the hyperparameters used during the training of the ILLUME model.  It details the learning rates (LR) applied to different components of the model during its three training stages: Visual Embedding Initialization, Unified Image-Text Alignment, and Supervised Fine-tuning.  Specifically, it shows separate learning rates for the vision adapter, vision embedding and head, and the main language model (LLM). The batch size and number of training steps are also included for each stage.  Understanding these hyperparameters is crucial for replicating the model's training process and interpreting the impact of the design choices on the overall performance.", "section": "3. ILLUME"}, {"content": "| Method | Type | Tasks | DINO | CLIP-I | CLIP-T |\n|---|---|---|---|---|---| \n| InstructPix2Pix [2] | Diffusion | Edit only | 0.762 | 0.834 | 0.219 |\n| MagicBrush [60] | Diffusion | Edit only | 0.776 | 0.838 | 0.222 |\n| OmniGen [53] | Diffusion | Edit only | 0.804 | 0.836 | 0.233 |\n| Emu Edit [42] | Diffusion | Edit only | **0.819** | 0.859 | 0.231 |\n| PUMA [13] | AR | Edit only | 0.785 | 0.846 | **0.270** |\n| ILLUME (Ours) | AR | Und, Gen, Edit | 0.791 | **0.879** | **0.260** |", "caption": "Table 6: Quantitative results on image editing benchmarks.  The performance with top-1 and top-2 value are denoted in bold and underline respectively.", "description": "Table 6 presents a quantitative comparison of different image editing models' performance on various benchmarks.  The models are evaluated based on three metrics: CLIP-I, CLIP-T, and DINO, which assess different aspects of image editing quality, including the preservation of elements from the source image and the consistency between the output image and the target caption.  The table highlights the top performing models for each metric, denoted in bold and underlined font, offering insights into the strengths and weaknesses of each approach in image editing tasks.", "section": "5.3 Comparison with State-of-the-arts"}, {"content": "| Understanding | POPE | MME-P | MMBench | SEED | GQA | MM-Vet | MMMU |\n|---|---|---|---|---|---|---|---| \n| baseline | 86.4 | 1358.6 | 61.7 | 65.0 | 60.0 | 27.4 | 31.2 |\n| + assessment | 86.1 | 1446.7 | 63.1 | 66.0 | 60.7 | 29.0 | 32.0 |\n| Generation | Overall | Single Obj | Two Obj. | Counting | Colors | Position | Color Attri. |\n|---|---|---|---|---|---|---|---| \n| baseline | 0.56 | 0.98 | 0.8 | 0.35 | 0.69 | 0.34 | 0.22 |\n| + assessment | 0.59 | 0.99 | 0.84 | 0.43 | 0.72 | 0.33 | 0.24 |", "caption": "Table 7: Ablation of self-enhancing multimodal alignment.", "description": "This table presents the ablation study results evaluating the effectiveness of the self-enhancing multimodal alignment scheme.  It compares the performance on various multimodal understanding and generation benchmarks with and without the proposed scheme, demonstrating its contribution to improved model performance.  The results are split to show the impact on both the understanding and generation tasks.", "section": "5.4 Ablation Studies"}, {"content": "| Method | Params. | Type | Attribute | Scene | Spatial | Action | Part | Overall | Count | Differ | Compare | Negate | Universal | Overall |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| SDXL | 2.6B | Diffusion | 0.84 | 0.84 | 0.82 | 0.83 | 0.89 | 0.83 | 0.71 | 0.73 | 0.69 | 0.50 | 0.66 | 0.63 |\n| LWM | 7B | Autoregressive | 0.63 | 0.62 | 0.65 | 0.63 | 0.70 | 0.63 | 0.59 | 0.58 | 0.54 | 0.49 | 0.52 | 0.53 |\n| Show-o | 1.5B | Autoregressive | 0.72 | 0.72 | 0.70 | 0.70 | 0.75 | 0.70 | 0.70 | 0.62 | 0.71 | 0.51 | 0.65 | 0.60 |\n| VILA-U(256) | 7B | Autoregressive | 0.78 | 0.78 | 0.77 | 0.78 | 0.79 | 0.76 | 0.70 | 0.71 | 0.74 | 0.53 | 0.66 | 0.64 |\n| VILA-U(384) | 7B | Autoregressive | 0.75 | 0.76 | 0.75 | 0.73 | 0.75 | 0.73 | 0.68 | 0.67 | 0.71 | 0.51 | 0.64 | 0.61 |\n| ILLUME (Ours) | 7B | Autoregressive | 0.75 | 0.79 | 0.75 | 0.77 | 0.73 | 0.75 | 0.66 | 0.68 | 0.67 | 0.49 | 0.63 | 0.60 |", "caption": "Table A: Detailed quantitative results on GenAI-bench.", "description": "Table A presents a detailed breakdown of the quantitative results obtained from evaluating the ILLUME model on the GenAI-bench benchmark.  It compares ILLUME's performance against several other models, including those using diffusion and autoregressive methods. The table assesses various aspects of image generation quality, such as attribute accuracy, scene understanding, spatial relationships, action recognition, and overall image quality. Separate scores are provided for basic and advanced generation tasks, allowing for a nuanced comparison of model capabilities across different levels of complexity.", "section": "5.2 Multimodal Evaluation Setup"}]