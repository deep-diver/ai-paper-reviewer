[{"figure_path": "https://arxiv.org/html/2503.17358/x1.png", "caption": "Figure 1: Existing methods rely on establishing correspondences between multiple frames to estimate inter-frame camera motion (a). This leads to failures during fast motion with lots of blur. We propose a method that can estimate intra-frame camera motion from a single image (b), making our method robust to aggressive motions.", "description": "Figure 1 illustrates the difference between traditional visual odometry/SLAM methods and the proposed method. Traditional methods (a) rely on feature matching between multiple frames to compute camera motion. However, this approach fails when significant motion blur is present during fast camera movements. The proposed method (b) directly estimates camera motion from a single motion-blurred image by exploiting motion blur as a cue, enabling robust estimation even under aggressive motions.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.17358/x2.png", "caption": "Figure 2: Overview of our method. Given a single motion blurred image, we pass it through the network to obtain a dense flow field and monocular depth prediction (Section 4.1). These are then formulated in a linear system, where the optimal velocity parameters are solved for using linear least squares (Section 4.2). Because the linear solver is fully differentiable, we can train the entire network end-to-end, supervised on the camera motion.", "description": "This figure illustrates the proposed method for estimating camera motion from a single motion-blurred image. The process begins by inputting a motion-blurred image into a neural network. This network predicts a dense flow field and a monocular depth map. These predictions are then used to construct a linear system of equations, solved using linear least squares to estimate the optimal camera velocity parameters. The method's differentiability allows for end-to-end training of the entire network, directly supervised using known camera motion.", "section": "4. Method"}, {"figure_path": "https://arxiv.org/html/2503.17358/x3.png", "caption": "Figure 3: Overview for our synthetic dataset generation process. After preprocessing the dataset, we run selected frames through an interpolation network, which we use to synthesize our blurred image. We also take the first and last virtual frames to generate \ud835\udc9f^^\ud835\udc9f\\mathcal{\\hat{D}}over^ start_ARG caligraphic_D end_ARG, which is subsequently used for computing \u2131^^\u2131\\mathcal{\\hat{F}}over^ start_ARG caligraphic_F end_ARG.", "description": "This figure illustrates the process of creating a synthetic dataset for training a model to estimate camera motion from a single motion-blurred image.  The process begins with preprocessing a subset of the ScanNet++v2 dataset. Selected frames are then passed through an interpolation network (like RIFE) to generate a synthetic motion-blurred image.  Simultaneously, the first and last frames from the sequence are used to create ground truth depth information (represented as  \\hat{D}).  This depth map, along with the motion blurred image, is used to compute a ground truth dense flow field (represented as \\hat{F}). These generated data points (blurred image,  \\hat{D}, and \\hat{F}) comprise the synthetic training dataset.", "section": "5. Dataset Curation"}, {"figure_path": "https://arxiv.org/html/2503.17358/x4.png", "caption": "Figure 4: Visualization of the predicted velocities for the billiards sequence using our method, MASt3R and COLMAP (w/ DISK+LightGlue). The shaded area under the curve shows the error between the predicted velocity and GT velocity. Our translations and rotations are significantly better than MASt3R. While COLMAP with DISK + LightGlue feature matching does well on rotations, our method significantly outperforms it on translations.", "description": "Figure 4 presents a comparison of the estimated velocities (rotational and translational) for the 'billiards' sequence obtained using three different methods: the proposed method, MASt3R, and COLMAP (combined with DISK and LightGlue).  The shaded regions in the plots represent the error between each method's velocity prediction and the ground truth velocity. The results demonstrate that the proposed method outperforms MASt3R in both rotational and translational velocity estimation. While COLMAP with DISK+LightGlue achieves comparable accuracy in rotational velocity estimation, the proposed method shows a clear advantage in translational velocity estimation.", "section": "6. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.17358/x5.png", "caption": "Figure 5: Error CDFs for the billiards sequence, such that the left plot shows the distribution of translational error in the sequence and the right plot the rotational error. Curves closer to the top-left corner are better.", "description": "Figure 5 presents the cumulative distribution functions (CDFs) of the errors for translational and rotational velocities obtained from the billiards sequence. The left plot displays the CDF of the translational velocity errors, and the right plot shows the CDF of the rotational velocity errors.  A curve closer to the top-left corner indicates better performance, meaning smaller errors in both translational and rotational velocity estimation.  The closer the curve is to the top-left corner, the lower the errors are.", "section": "6. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.17358/extracted/6300094/figures/robot_arm_cropped.png", "caption": "Figure 6: Real-world application example of our method using a camera attached to a fast-moving RoArm-M1 robot arm platform. (a) The robot arm used for recording. (b) The predicted and GT velocity time series for the camera attached to the end-effector.", "description": "This figure showcases a real-world application of the proposed method.  Panel (a) displays the robotic arm (RoArm-M1) used to capture the data. The arm's rapid movement introduces significant motion blur into the captured images.  Panel (b) presents a comparison of the velocity estimations. The graph plots the predicted camera velocity from the single motion-blurred images against the ground truth velocity obtained through IMU measurements and precise robotic arm pose estimations. This demonstrates the method's effectiveness in accurately estimating camera motion from a single motion-blurred image, even during high-speed movements.", "section": "6. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.17358/x6.png", "caption": "Figure 7: Comparison of our method to using IMU integration. The IMU velocity estimate is accurate for a few seconds until it drifts. Our method provides accurate and drift-free estimates throughout the sequence.", "description": "This figure compares the camera velocity estimates from the proposed method and IMU integration. The IMU initially provides accurate estimates, but it starts to drift after a few seconds. In contrast, the proposed method produces accurate and consistent velocity estimates throughout the entire sequence, highlighting its superior robustness and stability in handling long-duration camera motion.", "section": "6. Experiments"}]