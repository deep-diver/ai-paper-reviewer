[{"content": "| Dataset | Domain | Annotations | Videos | Task | Used in pt | Synthetic data |\n|---|---|---|---|---|---|---|\n| QVHighlights | Vlog / News | 10.3K | 12.5K | MR, HD | \u2713 | 187682 |\n| Charades-STA | Activity | 16.1K | 6.7K | MR | \u2713 | 23193 |\n| TVSum | Web | 50 | 50 | HD |  |  |", "caption": "TABLE I: Comparison of datasets used in this study.", "description": "This table compares the characteristics of three datasets used in the paper: QVHighlights, Charades-STA, and TVSum.  It lists the domain from which the videos originate, the type of annotations provided, the number of videos and annotations, the tasks (Moment Retrieval and Highlight Detection) each dataset is used for, whether the data was used for pre-training, and the size of any synthetic data generated for pre-training.", "section": "IV. EXPERIMENTS"}, {"content": "| Method | MR R1 @0.5 | MR R1 @0.7 | MR mAP @0.5 | MR mAP @0.75 | MR mAP Avg | HD mAP | HD HIT@1 |\n|---|---|---|---|---|---|---|---| \n| Moment-DETR [16] | 52.89 | 33.02 | 54.82 | 29.4 | 30.73 | 35.69 | 55.6 |\n| UMT [17] \u2020 | 56.23 | 41.18 | 53.83 | 37.01 | 36.12 | 38.18 | 59.99 |\n| MH-DETR [72] | 60.05 | 42.48 | 60.75 | 38.13 | 38.38 | 38.22 | 60.51 |\n| EaTR [21] | 61.36 | 45.79 | 61.86 | 41.91 | 41.74 | 37.15 | 58.65 |\n| QD-DETR [19] | 62.40 | 44.98 | 63.17 | 42.05 | 41.44 | 39.13 | 63.1 |\n| UVCOM [62] | 63.55 | 47.47 | 63.37 | 42.67 | 43.18 | 39.74 | 64.20 |\n| TR-DETR [22] | 64.66 | 48.96 | 63.98 | 43.73 | 42.62 | 39.91 | 63.42 |\n| UniVTG [20] | 58.86 | 40.86 | 57.60 | 35.59 | 35.47 | 38.20 | 60.96 |\n| VideoLights | 63.36 | 48.70 | 63.81 | 42.87 | 43.38 | 40.57 | 65.30 |\n| Moment-DETR(pt) [16] | 59.78 | 40.33 | 60.51 | 35.36 | 36.14 | 37.43 | 60.17 |\n| UMT(pt) [17] | 60.83 | 43.26 | 57.33 | 39.12 | 38.08 | 39.12 | 62.39 |\n| QD-DETR (pt) [19] | 64.10 | 46.10 | 64.30 | 40.50 | 40.62 | 38.52 | 62.27 |\n| UVCOM(pt) [62] | 64.53 | 48.31 | 64.78 | 43.65 | 43.80 | 39.98 | 65.58 |\n| UniVTG(pt) [20] | 65.43 | 50.06 | 64.06 | 45.02 | 43.63 | 40.54 | 66.28 |\n| VideoLights-pt | 68.48 | 52.53 | 67.31 | 46.76 | 45.01 | 41.48 | 65.89 |\n| VideoLights-B | 68.29 | 52.79 | 67.58 | 47.30 | 46.53 | 42.43 | 68.94 |\n| VideoLights-B-pt | 70.36 | 55.25 | 69.53 | 49.17 | 47.94 | 42.84 | 70.56 |", "caption": "TABLE II: Results on QVHighlights test split. \u2020\u2020{\\dagger}\u2020 represents the use of audio modality. Here, bold represents the best result, and underline represents the 2nd best result.", "description": "Table II presents a comprehensive comparison of different video highlight detection and moment retrieval methods on the QVHighlights test dataset.  The table includes various evaluation metrics such as Recall@0.5, Recall@0.7, MAP@0.5, MAP@0.75, Average MAP, and HIT@1.  Results are shown for both Moment Retrieval (MR) and Highlight Detection (HD) tasks. The table also indicates whether methods utilized audio modalities. The best performing method for each metric is highlighted in bold, and the second-best is underlined. This allows readers to easily compare the performance of various approaches and identify top-performing models for both tasks.", "section": "IV. Experiments"}, {"content": "Methods|VT|VU|GA|MS|PK|PR|FM|BK|BT|DS|Avg.\nsLSTM [7]\u2021|41.1|46.2|46.3|47.7|44.8|46.1|45.2|40.6|47.1|45.5|45.1\nSG [5]\u2021|42.3|47.2|47.5|48.9|45.6|47.3|46.4|41.7|48.3|46.6|46.2\nLIM-S [76]\u2021|55.9|42.9|61.2|54.0|60.3|47.5|43.2|66.3|69.1|62.6|56.3\nTrailer [77]\u2021|61.3|54.6|65.7|60.8|59.1|70.1|58.2|64.7|65.6|68.1|62.8\nSL-Module [78]\u2021|86.5|68.7|74.9|86.2|79|63.2|58.9|72.6|78.9|64.0|73.3\nUMT [17]\u2020\u2021|87.5|81.5|81.5|81.5|81.4|87.0|76.0|86.9|84.4|79.6|83.1\nQD-DETR [19]\u2021|88.2|87.4|85.6|85.0|85.8|86.9|76.4|91.3|89.2|73.7|85.0\nUVCOM [62]\u2021|87.6|91.6|91.4|86.7|86.9|86.9|76.9|92.3|87.4|75.6|86.3\nTR-DETR [22]\u2021|89.3|93.0|94.3|85.1|88.0|88.6|80.4|91.3|89.5|81.6|88.1\nVideoLights \u2021|89.8|88.7|95.0|88.0|83.6|90.1|79.4|94.2|88.6|81.2|87.9\nUniVTG [20]|83.9|85.1|89.0|80.1|84.6|81.4|70.9|91.7|73.5|69.3|81.0\nVideoLights|89.1|92.7|92.3|86.7|89.8|88.9|78.5|94.0|87.4|78.3|87.8\nUniVTG (pt) [20]|92.0|77.8|89.8|83.8|82.2|85.8|74.3|91.8|90.5|77.6|84.6\nVideoLights-pt|90.8|91.8|95.0|85.3|88.6|89.6|76.7|94.0|88.5|78.6|87.9\nVideoLights-B|91.3|92.5|93.3|84.3|88.0|88.3|77.3|92.7|88.2|81.6|87.75\nVideoLights-B-pt|91.4|88.2|93.0|95.2|87.2|89.1|76.1|95.1|88.6|81.3|88.52", "caption": "TABLE III: Evaluation of highlight detection methods on TVSum using Top-5 mAP. \u2020\u2020{\\dagger}\u2020 represents the use of audio modality. \u2021\u2021{\\ddagger}\u2021 indicates the use of I3D for visual features. Here, bold represents the best result, and underline represents the 2nd best result.", "description": "Table III presents a comparison of various highlight detection methods on the TVSum dataset.  The evaluation metric used is Top-5 mean Average Precision (mAP). The table includes results for several methods, indicating whether they utilized audio modality (\u2020) and/or I3D visual features (\u2021).  The best performing method for each metric is highlighted in bold, with the second-best result underlined. This allows for a comprehensive comparison of the performance of different approaches to video highlight detection on this specific dataset.", "section": "IV. EXPERIMENTS"}, {"content": "| Method | R@0.3 | R@0.5 | R@0.7 | mIoU |\n|---|---|---|---|---|\n| 2D-TAN [35] | 58.76 | 46.02 | 27.5 | 41.25 |\n| VSLNet [48] | 60.30 | 42.69 | 24.14 | 41.58 |\n| Moment-DETR [16] | 65.83 | 52.07 | 30.59 | 45.54 |\n| QD-DETR [19] | - | 57.31 | 32.55 | - |\n| TR-DETR [22] | - | 57.61 | 33.52 | - |\n| UniVTG [20] | 70.81 | 58.01 | 35.65 | 50.10 |\n| VideoLights | 70.67 | 58.04 | 36.88 | 50.20 |\n| UniVTG (pt) [20] | 72.63 | 60.19 | 38.55 | 52.17 |\n| VideoLights-pt | 72.26 | 60.11 | 37.80 | 51.44 |\n| VideoLights-B | 71.72 | 60.30 | 37.23 | 51.25 |\n| VideoLights-B-pt | 73.33 | 61.96 | 41.05 | 52.94 |", "caption": "TABLE IV: Results on Charades-STA test set. Here, bold represents the best result, and underline represents the 2nd best result.", "description": "Table IV presents the performance comparison of various methods on the Charades-STA test dataset for moment retrieval and highlight detection tasks.  The metrics used are Recall@0.3, Recall@0.5, Recall@0.7, and mean Intersection over Union (mIoU).  The best performing method for each metric is highlighted in bold, while the second-best is underlined. This allows for a clear view of the relative strengths and weaknesses of each approach on this specific dataset.", "section": "IV. EXPERIMENTS"}, {"content": "| Modules | Losses | MR @0.5 | MR @0.7 | MR @0.5 | MR @0.75 | MR mAP | HD HIT@1 |\n|---|---|---|---|---|---|---|---|---|\n| sl. | fra | bi | bf | pt | hl | tcl | scsl | al |  |  |  |  |  |  |\n| 1. | \u2717 | \u2717 | \u2717 | \u2717 | \u2713 | \u2713 | \u2713 | \u2713 | 61.42 | 46.77 | 60.82 | 41.36 | 41.28 | 38.08 | 60.45 |\n| 2. | \u2717 | \u2717 | \u2713 | \u2717 | \u2713 | \u2713 | \u2713 | \u2713 | 64.45 | 49.48 | 63.69 | 43.08 | 43.28 | 39.98 | 64.13 |\n| 3. | \u2713 | \u2713 | \u2717 | \u2717 | \u2713 | \u2713 | \u2713 | \u2713 | 66.77 | 51.23 | 65.83 | 45.38 | 45.12 | 40.74 | 66.9 |\n| 4. | \u2717 | \u2713 | \u2713 | \u2717 | \u2713 | \u2713 | \u2713 | \u2713 | 65.42 | 52.84 | 64.89 | 46.67 | 45.69 | 40.75 | 65.55 |\n| 5. | \u2713 | \u2717 | \u2713 | \u2717 | \u2713 | \u2713 | \u2713 | \u2713 | 69.55 | 53.94 | 67.53 | 47.86 | 47.14 | 42.09 | 68.77 |\n| 6. | \u2713 | \u2713 | \u2713 | \u2717 | \u2713 | \u2713 | \u2713 | \u2713 | 70.06 | 55.35 | 68.75 | 49.22 | 48.44 | 42.84 | 70.71 |\n| 7. | \u2713 | \u2713 | \u2713 | \u2717 | \u2717 | \u2717 | \u2717 | \u2717 | 69.55 | 54.39 | 68.34 | 49.00 | 47.32 | 41.96 | 68.06 |\n| 8. | \u2713 | \u2713 | \u2713 | \u2717 | \u2713 | \u2717 | \u2717 | \u2717 | 70.19 | 54.77 | 68.59 | 49.00 | 48.35 | 42.73 | 69.10 |\n| 9. | \u2713 | \u2713 | \u2713 | \u2717 | \u2717 | \u2713 | \u2717 | \u2717 | 69.55 | 54.00 | 68.37 | 47.80 | 47.63 | 41.85 | 69.61 |\n| 10. | \u2713 | \u2713 | \u2713 | \u2717 | \u2717 | \u2717 | \u2713 | \u2717 | 69.81 | 54.39 | 69.06 | 49.21 | 48.56 | 42.76 | 69.74 |\n| 11. | \u2713 | \u2713 | \u2713 | \u2717 | \u2717 | \u2717 | \u2717 | \u2713 | 69.68 | 54.71 | 67.80 | 47.80 | 54.71 | 41.79 | 68.26 |\n| 12. | \u2713 | \u2713 | \u2717 | \u2713 | \u2713 | \u2713 | \u2713 | \u2713 | 71.03 | 54.84 | 68.07 | 47.36 | 46.06 | 42.16 | 69.16 |\n| 13. | \u2713 | \u2713 | \u2713 | \u2713 | \u2713 | \u2713 | \u2713 | \u2713 | **72.06** | **57.94** | **70.38** | **51.12** | **49.71** | **43.12** | **71.48** |\n| No Pretraining |  |  |  |  |  |  |  |  | 66.77 | 51.23 | 65.83 | 45.38 | 45.12 | 40.74 | 66.9 |\n| ASR Pretraining [16] |  |  |  |  |  |  |  |  | 67.94 | 51.48 | 65.84 | 44.03 | 43.74 | 40.71 | 67.03 |\n| Our BLIP Pretraining |  |  |  |  |  |  |  |  | **71.03** | **54.84** | **68.07** | **47.36** | **46.06** | **42.16** | **69.16** |", "caption": "TABLE V: Ablation study on QVHighlights val split. fra stands for FRA module, bi stands for Bi-CMF module, bf stans for Blip features, pt stands for pre-train on the synthetic dataset using Blip Backend, hl stands for adaptive hard positive and negative loss, tcl stands for task coupled loss, scsl stands for saliency cosine similarity loss, and al stands for alignment loss. The effect of different pretraining data is in the bottom block without any new losses.", "description": "This ablation study analyzes the impact of individual components of the VideoLights model on the QVHighlights validation set.  It examines the contributions of the Feature Refinement and Alignment (FRA) module, the Bi-directional Cross-Modal Fusion (Bi-CMF) network, the inclusion of BLIP features, synthetic data pretraining using a BLIP backend, adaptive hard positive/negative losses, task-coupled loss, saliency cosine similarity loss, and alignment loss.  The bottom section of the table isolates the effects of different pretraining data sources without introducing any of the additional loss functions.", "section": "IV. Experiments"}, {"content": "| Cross-Attention Type | MR R1@0.5 | MR R1@0.75 | MR mAP@Avg | HD mAP | HD HIT@1 |\n|---|---|---|---|---|---| \n| Bi-CMF | **70.06** | **55.35** | **48.44** | **42.84** | **70.71** |\n| Uni-CMF | 69.55 | 53.94 | 47.14 | 42.09 | 68.77 |", "caption": "TABLE VI: Effect of Bi-CMF vs Uni-CMF on VideoLights\u00a0 on QVHighlights val set", "description": "This table presents an ablation study comparing the performance of VideoLights with a bidirectional cross-modal fusion network (Bi-CMF) against a unidirectional version (Uni-CMF).  It shows the impact of the Bi-CMF module on the overall performance of the VideoLights model, as measured by various metrics including Recall@0.5, Recall@0.75, mean Average Precision (mAP), and HIT@1, on the QVHighlights validation set. This helps quantify the benefit of bidirectional attention for the joint video highlight detection and moment retrieval task.", "section": "IV. Experiments, A. Main Results"}, {"content": "| Method | MR R1@0.5 | MR R1@0.75 | MR mAP@Avg | HD mAP | HD HIT@1 |\n|---|---|---|---|---|---| \n| Moment-DETR [16] | 53.94 | 34.84 | 32.2 | 35.36 | 55.55 |\n| Moment-DETR \u2020 | **61.48** | **40.26** | **35.17** | **38.88** | **63.16** |\n| QD-DETR [19] | 62.68 | 46.66 | 41.22 | 39.13 | 63.03 |\n| QD-DETR \u2020 | **63.81** | **46.84** | **41.71** | **39.77** | **63.87** |\n| TR-DETR [22] | 67.1 | 51.48 | 45.09 | 40.55 | 64.77 |\n| TR-DETR \u2020 | **67.81** | **51.68** | **45.19** | **41.37** | **67.03** |", "caption": "TABLE VII: Effect of FRA on different methods on QVHighlights val set. \u2020\u2020{\\dagger}\u2020 represents the use of the FRA module", "description": "This table presents an ablation study evaluating the impact of the Feature Refinement and Alignment (FRA) module on the performance of several video highlight detection and moment retrieval models.  It shows the results for four different models (Moment-DETR, QD-DETR, and TR-DETR) with and without the FRA module included. The results are presented in terms of Recall@0.5, Recall@0.75, mean average precision (mAP), average mAP across various IoU thresholds, and HIT@1. This allows for a direct comparison of the performance gains achieved by adding the FRA module to each model. The study highlights how FRA enhances model performance across different metrics.", "section": "IV. Experiments"}, {"content": "| Feature type | MR R1@0.5 | MR R1@0.75 | MR mAP@Avg | HD mAP | HD HIT@1 |\n|---|---|---|---|---|---| \n| SF + C | 66.77 | 51.23 | 45.12 | 40.74 | 66.9 |\n| SF + B | 69.23 | 53.42 | 46.86 | 42.20 | 69.68 |\n| SF + C + B | **70.06** | **55.35** | **48.44** | **42.84** | **70.71** |", "caption": "TABLE VIII: Effect of integrating features from different VLM\u2019s on VideoLights\u00a0 on QVHighlights val set. Here SF stands for SlowFast, C stands for CLIP, and B stands for BLIP-2.", "description": "This table presents an ablation study evaluating the impact of using different combinations of visual features extracted by various Vision-Language Models (VLMs) on the performance of the VideoLights model.  Specifically, it shows how using SlowFast (SF), CLIP (C), and BLIP-2 (B) features, either individually or in combination, affects the model's performance on the QVHighlights validation set.  The metrics used to assess performance are Recall@0.5, Recall@0.75, Mean Average Precision (mAP) averaged across various IoU thresholds, and average mAP. The results demonstrate the effectiveness of combining features from multiple VLMs for improved accuracy.", "section": "IV. Experiments"}]