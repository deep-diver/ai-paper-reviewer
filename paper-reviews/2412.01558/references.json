{"references": [{"fullname_first_author": "A. Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-01", "reason": "This paper introduces CLIP, a foundational vision-language model used extensively in the VideoLights framework for multimodal feature extraction and alignment."}, {"fullname_first_author": "J. Lei", "paper_title": "Detecting moments and highlights in videos via natural language queries", "publication_date": "2021-12-01", "reason": "This paper introduces the QVHighlights dataset and the Moment-DETR model, which serve as a baseline and key component for evaluating joint video highlight detection and moment retrieval techniques in VideoLights."}, {"fullname_first_author": "Y. Liu", "paper_title": "Umt: Unified multi-modal transformers for joint video moment retrieval and highlight detection", "publication_date": "2022-06-01", "reason": "This paper proposes UMT, a unified architecture for handling multimodal data in HD/MR, influencing the VideoLights design by integrating multiple modalities and addressing cross-task relationships."}, {"fullname_first_author": "W. Moon", "paper_title": "Query-dependent video representation for moment retrieval and highlight detection", "publication_date": "2023-06-01", "reason": "This paper introduces QD-DETR, enhancing HD/MR by using a query-dependent video representation module, which inspired VideoLights's design of a query-aware video representation component."}, {"fullname_first_author": "J. Li", "paper_title": "Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation", "publication_date": "2022-07-01", "reason": "This paper introduces BLIP, a vision-language model used in VideoLights for enhanced multimodal feature integration and intelligent model pre-training, improving feature representations and model performance."}]}