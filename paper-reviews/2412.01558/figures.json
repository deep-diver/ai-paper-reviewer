[{"figure_path": "https://arxiv.org/html/2412.01558/x1.png", "caption": "Figure 1: Relevance heat map illustrating multimodal alignment dynamics across video understanding models. Color intensity (blue to red) quantifies query-video clip correspondence, with the green line indicating ground truth clip-wise saliency. Comparative visualization reveals VideoLights\u2019s progressive refinement of query-clip relevance through projection, feature refinement, and bi-directional cross-attention stages, in contrast to Moment-DETR\u00a0[16] and QD-DETR\u00a0[19]\u2019s limited multimodal interaction.", "description": "Figure 1 presents a comparative analysis of multimodal alignment in video understanding models, focusing on the relationship between text queries and video clips.  Six heatmaps illustrate the alignment scores between query words and video clips for three different models: Moment-DETR, QD-DETR, and VideoLights.  Color intensity ranging from blue (low relevance) to red (high relevance) represents the strength of the alignment. A green line indicates the ground truth saliency scores for each video clip. The heatmaps reveal that VideoLights progressively refines the alignment between query and video through distinct stages (projection, feature refinement, and bi-directional cross-attention) leading to improved query-clip relevance. Conversely, Moment-DETR and QD-DETR exhibit more limited multimodal interaction and less refined alignment.", "section": "III. PROPOSED VIDEOLIGHTS MODEL"}, {"figure_path": "https://arxiv.org/html/2412.01558/x2.png", "caption": "Figure 2: Overall VideoLights architecture. FRA models the video-text cross-modal correlations from projected embeddings and passes them to Bi-CMF in the encoder. A trainable saliency vector predicts output saliency levels. Class and moment prediction heads predict logits and video moments, while saliency cosine similarity and task-coupled HD/MR losses together provide cross-task feedback Uni-JFM. Proposed new losses are in purple.", "description": "The VideoLights architecture uses a two-stage approach for joint video highlight detection and moment retrieval.  First, the Feature Refinement and Alignment (FRA) module processes video and text features, aligning them and refining their representations.  The output of FRA feeds into the Bi-directional Cross-Modal Fusion (Bi-CMF) network, which further integrates and refines the multimodal embeddings via transformer layers.  The Bi-CMF output goes to prediction heads (Class, Localization, Saliency).  These heads output the final highlight scores (saliency), video moment locations, and class predictions for each clip.  A Unidirectional Joint-Task Feedback Mechanism (Uni-JFM) provides cross-task feedback using saliency cosine similarity and task-coupled loss functions to improve the learning process. The figure highlights the new proposed loss functions in purple.", "section": "III. PROPOSED VIDEOLIGHTS MODEL"}, {"figure_path": "https://arxiv.org/html/2412.01558/x3.png", "caption": "Figure 3: (a) is the input video, (b) and (c) are correspondence maps of query and video tokens using linear and convolution layers, respectively, which show that queries are more aligned for the convolution layer, video, and text than linear projection layers. (d) The effect of the Feature Refinement module that effectively aligns video and text tokens that match ground truth saliency levels (green line) in each heat map saliency level is shown with green line plot.", "description": "This figure visualizes the impact of different projection methods (linear vs. convolutional) and the Feature Refinement module on aligning video and text tokens. Subfigure (a) shows the input video. Subfigures (b) and (c) present correspondence maps illustrating the alignment between query and video tokens using linear and convolutional projection layers, respectively.  The convolutional layer demonstrates better alignment of query and video tokens with the text.  Subfigure (d) shows how the Feature Refinement module further improves this alignment, particularly for video and text tokens corresponding to ground truth saliency levels (indicated by the green line).", "section": "III. PROPOSED VIDEOLIGHTS MODEL"}, {"figure_path": "https://arxiv.org/html/2412.01558/x4.png", "caption": "Figure 4: Bi-CMF learns query-oriented video via text2video, video2text, then text2video attentions. In this process, dropout and normalization are applied after each step, and activation is applied at the last stage.", "description": "This figure illustrates the architecture of the Bi-Directional Cross-Modal Fusion (Bi-CMF) Network.  The Bi-CMF network is a three-layer multi-head attention mechanism designed to generate a query-aware video representation by integrating text and video information. The first layer uses video features as queries and text features as keys and values to identify video tokens that are relevant to the text query (text2video attention). The second layer reverses this process, using text features as queries and video features as keys and values, identifying text tokens relevant to the video (video2text attention). The final layer refines the process using the outputs from the previous layers, strengthening the alignment between text and video tokens (text2video attention). Dropout and layer normalization are applied between each attention layer for robustness, with an activation layer at the output. This multi-stage attention mechanism helps to strongly couple video and text modalities.", "section": "III. PROPOSED VIDEOLIGHTS MODEL"}, {"figure_path": "https://arxiv.org/html/2412.01558/x5.png", "caption": "Figure 5: (a) and (b) show video-query correspondence maps: (a) after text-to-video (t2v) attention and (b) after the Bi-CMF layer. The green line represents the ground truth saliency scores. Bi-CMF attends to the correct video region better than t2v (highlighted in the magenta box). The word \u2018Is\u2019 asserts that \u2018a\u2019 refers to one basket, unlike \u2018is not\u2019.", "description": "This figure visualizes the effectiveness of the Bi-Directional Cross-Modal Fusion (Bi-CMF) network in VideoLights. It presents two heatmaps showing the alignment between video clips and query words: (a) after the text-to-video attention mechanism and (b) after the Bi-CMF module. The green line indicates the ground truth saliency scores. The magenta box highlights the region where the Bi-CMF significantly improves the focus on the relevant video clips compared to the plain text-to-video attention.  The caption also points out how the word \"is\" in the query helps disambiguate the reference of \"a\" to a single basket in the video, contrasting with a hypothetical \"is not\" which would have a different meaning.", "section": "III. PROPOSED VIDEOLIGHTS MODEL"}, {"figure_path": "https://arxiv.org/html/2412.01558/extracted/6038314/tr_neg_example.jpg", "caption": "(a)", "description": "This figure shows the visualization of multimodal alignment dynamics across video understanding models. It compares the query-video clip correspondence heatmaps of three models: Moment-DETR, QD-DETR, and VideoLights.  Color intensity (blue to red) represents the query-video clip correspondence strength, with green lines indicating ground truth clip-wise saliency. The visualization demonstrates how VideoLights progressively refines query-clip relevance through its different modules (projection, feature refinement, and bi-directional cross-attention), achieving stronger alignment with ground truth compared to the other two models.  This highlights the improved multimodal interaction and alignment capabilities of VideoLights.", "section": "III. PROPOSED VIDEOLIGHTS MODEL"}, {"figure_path": "https://arxiv.org/html/2412.01558/extracted/6038314/both_wrong_but_logical.jpg", "caption": "(b)", "description": "This figure shows a heatmap visualization of the query-aware clip representation generated by VideoLights after applying the Bi-Directional Cross-Modal Fusion (Bi-CMF) network.  The heatmap illustrates the alignment between the query words and the video clips, where higher color intensity (red) indicates stronger relevance. This visualization highlights how Bi-CMF effectively captures cross-modal interactions and enhances the alignment between text and video, improving the accuracy of moment retrieval and highlight detection compared to methods with only unidirectional attention mechanisms. The green line indicates the ground truth saliency scores for comparison.", "section": "III. PROPOSED VIDEOLIGHTS MODEL"}, {"figure_path": "https://arxiv.org/html/2412.01558/extracted/6038314/bi-cmf-count.png", "caption": "Figure 6: \nQualitative results. (a) demonstrates VideoLights\u00a0 outperformed TR-DETR\u00a0[22] in both MR and HD. (b) Both VideoLights\u00a0 and TR-DETR performed below the ground truth, but upon closer examination, it is evident that incorrectly predicted clips are still related to the given query.", "description": "Figure 6 presents a qualitative comparison of VideoLights and TR-DETR on two video clips. Subfigure (a) showcases a clear advantage for VideoLights, demonstrating superior performance in both Moment Retrieval (MR) and Highlight Detection (HD) tasks compared to TR-DETR. Subfigure (b) shows instances where both models underperform compared to the ground truth. However, a closer inspection reveals that while the predictions aren't perfect, the clips identified by both methods are still semantically relevant to the given query, indicating some level of success in capturing relevant video segments despite inaccuracies in precise localization or ranking.", "section": "IV. EXPERIMENTS"}]