[{"figure_path": "https://arxiv.org/html/2503.06053/x1.png", "caption": "Figure 1: Comparisons between Composable Spatio-temporal Consistency and Integral Spatio-temporal Consistency.\n(a) Composable Spatio-Temporal Consistency refers to the straightforward combination of temporal and spatial consistency, without limiting the effects of camera movement. Studies such as MovieGen [49] and VBench++ [26] are dedicated to realizing this consistency. Despite the potential emergence of a new scene post camera movement, the introduced scene tends to be stationary, precluding the onset of further motion.\n(b) Integral Spatio-Temporal Consistency considers the interplay between plot development and camera techniques, along with the enduring influence of antecedent content on subsequent creation.\nThis is because a camera movement may introduce or eliminate objects, thereby overlaying and impacting the preceding storyline.\nFor example in the \u201cForrest Gump\u201d clip, achieving integral spatio-temporal consistency requires incorporating the motion of the \u201ccar\u201d as it recedes following the camera\u2019s \u201cturn right\u201d action while maintaining the scene of Forrest running, ensuring that \u201cForrest Gump\u2019s right remains at a consistent distance\u201d, preserving the correct spatial relationships.\nTemporal consistency in plot progression is highlighted in the blue region, while the red region denotes spatial consistency induced by camera movement", "description": "Figure 1 illustrates the difference between two types of spatio-temporal consistency in video generation: Composable and Integral.  Composable Spatio-Temporal Consistency (a) involves simply combining temporal and spatial consistency, often without fully considering the impact of camera movement on the scene.  Examples like MovieGen and VBench++ represent this approach. While this ensures basic consistency, new scenes introduced after a camera movement tend to be static, lacking dynamic interaction. In contrast, Integral Spatio-Temporal Consistency (b) emphasizes the interrelationship between plot progression, camera techniques, and the influence of previous scene elements on subsequent ones. Camera movement is not simply added; it's integrated into the narrative. The example of a \"Forrest Gump\" clip demonstrates this: Maintaining Forrest running while incorporating the movement of a car that enters the scene due to a camera pan requires considering how the car's motion interacts with Forrest's action and ensuring consistent spatial relationships are preserved. The blue region highlights the temporal consistency of the plot, while the red region shows the spatial consistency maintained despite camera movement.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2503.06053/x2.png", "caption": "Figure 2: The DropletVideo-10M dataset features diverse camera movements, long-captioned contextual descriptions, and strong spatio-temporal consistency. (a) Existing datasets, such as Panda-70M [11], place less emphasis on camera movement and contain relatively brief captions. (b) In contrast, DropletVideo-10M consists of spatio-temporal videos that incorporate both camera movement and event progression. Each video is paired with a caption that conveys detailed spatio-temporal information aligned with the video content, with an average caption length of 206 words. The spatio-temporal information is highlighted in red in the figure.", "description": "This figure compares the DropletVideo-10M dataset with existing datasets.  Subfigure (a) shows examples from existing datasets like Panda-70M, highlighting their limited camera movement and short captions.  Subfigure (b) showcases DropletVideo-10M, emphasizing its rich diversity in camera motion, detailed long-form captions (averaging 206 words), and strong spatio-temporal consistency (shown via red highlighted text in the example caption). The key difference is that DropletVideo-10M explicitly incorporates both camera movement and event progression within its videos and captions, making it more suitable for research on integral spatio-temporal video generation.", "section": "3 Dataset"}, {"figure_path": "https://arxiv.org/html/2503.06053/x3.png", "caption": "Figure 3: The pipeline we proposed to curate the DropletVideo-10M dataset.", "description": "This figure illustrates the multi-stage pipeline used to create the DropletVideo-10M dataset.  The process begins with collecting raw videos from YouTube using a set of keywords, resulting in a large pool of videos.  This pool is then processed through several filtering and segmentation steps. First, video clips are segmented into shorter, more manageable pieces focusing on those containing both object motion and camera movement. Next, these segments are further filtered based on aesthetic and image quality scores to ensure high visual fidelity. The final step involves generating spatio-temporal consistent captions for each video clip using a fine-tuned video captioning model. These captions provide detailed descriptions of both object motion and camera movement, which are crucial for training models focused on integral spatio-temporal consistency.", "section": "3 Dataset"}, {"figure_path": "https://arxiv.org/html/2503.06053/x4.png", "caption": "Figure 4: The aesthetics distribution and the image quality distribution of DropletVideo-10M. These distributions demonstrate that our dataset achieves high scores in both aesthetics and image quality, indicating an overall high-quality standard for the dataset.", "description": "This figure displays two histograms showing the distributions of aesthetic scores and image quality scores for the DropletVideo-10M dataset.  The aesthetic scores were generated using the LAION aesthetics model, while image quality scores came from the DOVER-Technical model. The histograms illustrate that the majority of videos in the dataset receive high scores in both categories, indicating a high overall quality.", "section": "3 Dataset"}, {"figure_path": "https://arxiv.org/html/2503.06053/x5.png", "caption": "Figure 5: Captions generated by the fine-tuned models, including InternVL2-8B[13, 14], ShareGPT4Video-8B[10], ShareCaptioner-video[10], and MA-LMM[22]. InternVL2-8B[13, 14] captures intricate camera work and narrative elements with high efficacy.", "description": "Figure 5 presents example captions generated by four different fine-tuned video-to-text models (InternVL2-8B, ShareGPT4Video-8B, ShareCaptioner-video, and MA-LMM).  The captions describe the same video, demonstrating the variation in detail and accuracy across different models.  InternVL2-8B is highlighted for its superior ability to capture intricate camera movements and narrative elements.", "section": "3.4 Video Captioning"}, {"figure_path": "https://arxiv.org/html/2503.06053/x6.png", "caption": "Figure 6: Results of the fine-tuned video captioning model. In the prompts, descriptions related to camera motions are highlighted in red. It is evident from the training samples that the camera undergoes multiple motion changes. Moreover, the scene details in the videos are clearly described and accurately followed as the camera moves. These high-density informational text captions significantly enhance the spatio-temporal semantics of the videos. Consequently, our video captions in the DropletVideo-10M dataset provide enriched guidance for training video generation models.", "description": "This figure showcases examples of video captions generated by a fine-tuned video captioning model. The captions, shown alongside corresponding video stills, highlight in red the parts describing camera movements.  The examples demonstrate that the model generates detailed and accurate captions that precisely reflect not only the camera's actions (e.g., zooming, panning, rotating) but also the changes in the scene and object details as the camera moves. These rich, spatio-temporally detailed captions provide valuable training data for video generation models, helping them better understand and reproduce complex camera movements and their impact on the visual narrative.", "section": "3 Dataset"}, {"figure_path": "https://arxiv.org/html/2503.06053/x7.png", "caption": "Figure 7: Overview of the DropletVideo Framework.\nThe video is processed by the 3D causal Variational Autoencoder (VAE) following adaptive equalization sampling, which is steered by the motion intensity M\ud835\udc40Mitalic_M.\nThe video feature xvsubscript\ud835\udc65\ud835\udc63x_{v}italic_x start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT is then input into the Modality-Expert Transformer, depicted on the right side of the figure, to facilitate video generation in conjunction with the text encoding xtsubscript\ud835\udc65\ud835\udc61x_{t}italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, and the combined encoding xT&Msubscript\ud835\udc65\ud835\udc47\ud835\udc40x_{T\\&M}italic_x start_POSTSUBSCRIPT italic_T & italic_M end_POSTSUBSCRIPT of the temporal T\ud835\udc47Titalic_T and the motion intensity M\ud835\udc40Mitalic_M.\nThe upper left part illustrates the contrast between (a) the traditional sampling approach and (b) DropletVideo\u2019s adaptive equalization sampling.\nTraditional methods involve random segment interception followed by fixed-frame-rate sampling of the intercepted segments, whereas DropletVideo employs adaptive frame rate sampling across the entire video segments, guided by M\ud835\udc40Mitalic_M.", "description": "The DropletVideo framework processes video data using a 3D causal Variational Autoencoder (VAE) with adaptive equalization sampling, controlled by motion intensity (M).  The extracted video features (xv) are fed, along with text embeddings (xt), into a Modality-Expert Transformer. This transformer generates video output using a combined encoding of temporal information (T) and motion intensity (M) (xT&M).  The figure compares traditional video sampling (fixed-frame rate after random segment selection) with DropletVideo's adaptive method, which uses motion intensity (M) to guide adaptive frame rate sampling across the whole video for better consistency.", "section": "4 The DropletVideo Model"}, {"figure_path": "https://arxiv.org/html/2503.06053/x8.png", "caption": "Figure 8: DropletVideo facilitates the generation of videos that maintain integral spatio-temporal consistency. New objects or scenes introduced via camera movement are seamlessly integrated and interact logically with the pre-existing scenes. In video (a), as the camera moves, a new boat appears on the lake, the boat on the right of the original two boats continues to slowly chase the boat on the left, and the leaves on the shore still sway gently in the breeze. In video (b), as the camera moves left, the tree called for in the text prompt successfully appears in the shot, the original flock of birds continues to fly, and the grass and sky show continuity as the camera moves.", "description": "Figure 8 demonstrates DropletVideo's ability to generate videos with integral spatio-temporal consistency. This means that new objects or scenes introduced by camera movements are smoothly integrated into the existing scene and behave logically within the context.  The top example (a) shows a lake scene with two boats. As the camera pans, a third boat appears, and the original boats continue their interaction.  The leaves on the shore continue to sway naturally, showing the seamless integration of the new element.  The bottom example (b) shows a forest scene with birds. As the camera pans, a tree (requested in the text prompt) appears, while the birds and the rest of the scene maintain consistency, illustrating that the new element fits seamlessly into the existing narrative. Both examples highlight DropletVideo's ability to preserve both spatial and temporal consistency across the video generation process.", "section": "5.2 Qualitative Evaluation"}, {"figure_path": "https://arxiv.org/html/2503.06053/x9.png", "caption": "Figure 9: DropletVideo demonstrates advanced controllability in generating scenes where new objects emerges due to camera movement. In video (a), as the camera pans right, the red apple specified in the prompt appears seamlessly, while the chef continues cooking, illustrating smooth integration of new objects. Video (b) showcases the system\u2019s ability to handle detailed descriptions, as the prompt\u2019s depiction of an apple with water droplets is rendered accurately, highlighting complex textures. In video (c), a prompt modification adds brown spots to the apple, which are visibly integrated, showing dynamic visual adjustments. Finally, in video (d), the prompt changes the apple to bananas, and the system adeptly features bananas, demonstrating versatility and precision in object transformation.", "description": "Figure 9 demonstrates DropletVideo's ability to generate videos with new objects appearing seamlessly due to camera movements, while maintaining consistency with existing scene elements and prompt descriptions. Four example scenarios are shown:\n(a) A red apple appears as the camera pans right, while a chef continues cooking, showcasing smooth object integration.\n(b) An apple with water droplets is accurately rendered, demonstrating the model's ability to handle detailed descriptions and complex textures.\n(c) Brown spots are added to the apple by modifying the prompt, showing dynamic visual adjustments based on prompt changes.\n(d) The apple is changed to bananas, illustrating versatility and precision in object transformation.  These examples highlight DropletVideo's advanced control over object generation and its ability to incorporate new elements into a scene without disrupting established consistency.", "section": "5.2 Qualitative Evaluation"}, {"figure_path": "https://arxiv.org/html/2503.06053/x10.png", "caption": "Figure 10: DropletVideo demonstrates excellent 3D consistency. In the top example, the camera moves around a snowflake, showcasing significant camera movement while maintaining the snowflake\u2019s details from multiple perspectives. In the bottom example, the camera circles around an insect, and DropletVideo ensures the insect\u2019s 3D consistency across a wide range of rotation angles. However, DropletVideo still has limitations in generating content for a full 360-degree rotation, which will be addressed in future work. Overall, these examples illustrate DropletVideo\u2019s strong performance in spatial 3D consistency.", "description": "This figure showcases DropletVideo's ability to maintain 3D consistency in video generation. The top panel shows a snowflake with a camera rotating around it.  Despite the camera movement, the snowflake's details remain consistent from all angles, demonstrating the model's ability to maintain spatial coherence. The bottom panel shows a similar example with an insect as the main subject; again, the model accurately represents its 3D form despite the camera rotating around it.  While DropletVideo performs well, the authors acknowledge limitations in generating a full 360-degree rotation, an area for future improvement.", "section": "5.2 Qualitative Evaluation"}, {"figure_path": "https://arxiv.org/html/2503.06053/x11.png", "caption": "Figure 11: DropletVideo facilitates precision control over video generation speed. Modifying the Input Speed parameter alters the movement speed of both the camera and target. In the third line, the camera motion parameter M\ud835\udc40Mitalic_M is doubled, and the snowflake\u2019s rotation speed is substantially decreased compared to the initial setting.", "description": "Figure 11 demonstrates DropletVideo's ability to control video generation speed.  By adjusting the motion intensity parameter (M), users can precisely alter the speed of both camera movements and the movement of the primary subject within the video. The figure shows that doubling the M parameter significantly slows down the rotation of a snowflake, showcasing the model's fine-grained control over the dynamic aspects of video generation.", "section": "5.2 Qualitative Evaluation"}, {"figure_path": "https://arxiv.org/html/2503.06053/x12.png", "caption": "Figure 12: DropletVideo showcases its robust capabilities in generating videos with diverse camera movements. Panels (a)-(e) illustrate the outcomes of specific camera motions: Camera Truck Right, Camera Truck Left, Camera Pedestal Down, Camera Tilt Up, and Camera Dolly In. Panel (f) presents a composite camera shot that combines Camera Pan Right and Tilt Up.", "description": "Figure 12 visually demonstrates DropletVideo's versatility in generating diverse camera movements.  Each sub-figure [(a) through (e)] showcases a distinct camera technique resulting in a unique visual effect.  (a) Camera Truck Right shows a smooth rightward pan, (b) Camera Truck Left illustrates a leftward pan, (c) Camera Pedestal Down depicts a downward vertical movement, (d) Camera Tilt Up demonstrates an upward tilting motion, and (e) Camera Dolly In showcases a forward movement towards the subject. Finally, (f) showcases a composite shot combining a pan to the right and an upward tilt, demonstrating complex camera control.", "section": "5.2.4 Camera Motion"}, {"figure_path": "https://arxiv.org/html/2503.06053/x13.png", "caption": "Figure 13: Snow example. The videos generated by DropletVideo, Kling, and Vivago all maintain consistency with the prompt in terms of camera movement and various elements within the video. Their video quality is at the same level.", "description": "Figure 13 presents a comparison of video generation results for a \"snow scene\" prompt across several models: DropletVideo, Kling, Vivago, Gen3, Alpha Turbo, Hailuo, I2V-01-Live, Vidu 2.0, Qingying, I2V 2.0, WanX 2.1, and CogVideoX-Fun.  The results demonstrate that DropletVideo, Kling, and Vivago successfully generate videos that accurately reflect the prompt's specifications regarding camera movement and scene elements.  All three models achieve comparable video quality.", "section": "5.2 Qualitative Evaluation"}, {"figure_path": "https://arxiv.org/html/2503.06053/x14.png", "caption": "Figure 14: Boat example. Our DropletVideo, along with Hailuo, WanX, and Kling v1.6, correctly understood the movement of the boat and the camera motion. However, these three models failed to ensure that the motion of the leaves remained logically consistent with the camera movement, resulting in the leaves moving synchronously with the camera, which is an unnatural effect. In contrast, our model maintains the relative motion consistency between the camera, boat, and leaves in the generated video. This is a typical demonstration of its integral spatio-temporal consistency capability.", "description": "Figure 14 compares video generation results for a scene of boats on a lake with a moving camera.  DropletVideo, Hailuo, WanX, and Kling v1.6 all correctly depict the boats' and camera's movements. However, Hailuo, WanX, and Kling v1.6 incorrectly synchronize the movement of the lake's leaves with the camera, making the result unrealistic.  DropletVideo successfully maintains the natural relative motion between the leaves, boats, and camera, showcasing its ability to preserve integral spatiotemporal consistency.", "section": "5.2 Qualitative Evaluation"}, {"figure_path": "https://arxiv.org/html/2503.06053/x15.png", "caption": "Figure 15: Sunset example. Only DropletVideo and Kling v1.6 successfully ensure the correct alignment between camera movement and object positioning. However, in Kling\u2019s generated video, the lighting reflections on the clouds remain unchanged, lacking natural variation. In contrast, in our model\u2019s generated video, as the camera moves, the light reflections on the clouds dynamically adjust, making the scene more consistent with real-world natural phenomena.", "description": "Figure 15 presents a comparison of video generation results for a sunset scene, focusing on the alignment of camera movement with object positioning and the realistic depiction of lighting changes.  DropletVideo and Kling v1.6 both correctly position elements within the scene as the camera moves. However, Kling v1.6 fails to dynamically adjust the lighting reflections on the clouds, creating an unrealistic effect.  In contrast, DropletVideo's output shows natural variation in cloud lighting reflections, accurately reflecting real-world phenomena, thereby demonstrating a higher level of spatio-temporal consistency.", "section": "5.2 Qualitative Evaluation"}, {"figure_path": "https://arxiv.org/html/2503.06053/x16.png", "caption": "Figure 16: Kitchen example. We expect the focus of the video to transition from the chef to a red apple as the camera moves. Only DropletVideo successfully achieved this transition, while other models failed to correctly generate \u201ca red apple\u201d after the camera movement. Besides, it also ensures that the apple it generates are of a reasonable size and are positioned appropriately within the scene.", "description": "Figure 16 presents a comparison of video generation results from different models, focusing on a scene where a chef is preparing food and a red apple appears as the camera pans.  The other models struggle to produce the apple correctly after the camera movement, often failing to generate it entirely or creating an apple of the wrong size or in an inappropriate location. DropletVideo, in contrast, successfully generates the apple with accurate size and placement, seamlessly integrating it into the scene. This highlights DropletVideo's superior ability to maintain spatial and temporal consistency during dynamic video generation.", "section": "5.2 Qualitative Evaluation"}, {"figure_path": "https://arxiv.org/html/2503.06053/x17.png", "caption": "Figure 17: Staircase example. We required the camera to move smoothly up the stairs, ensuring that its trajectory remains logically consistent with the staircase in the video. Only our DropletVideo and Gen3 successfully maintained the correct camera movement path. However, Runway failed to generate key elements such as wall decorations and lights.", "description": "Figure 17 presents a comparison of video generation models' ability to produce a video of a person smoothly ascending a staircase while maintaining consistent camera movement and scene details.  The prompt instructed the models to generate a video showing the camera's upward movement along the staircase, focusing on elements such as the red carpeting, intricate railings, and wall decorations. DropletVideo and Gen-3 successfully achieved this, accurately depicting the camera's smooth trajectory and including the specified scene elements.  However, Runway's output failed to incorporate key details, like wall decorations and lighting, illustrating the superiority of DropletVideo and Gen-3 in maintaining both visual and motion consistency.", "section": "5.2 Qualitative Evaluation"}, {"figure_path": "https://arxiv.org/html/2503.06053/x18.png", "caption": "Figure 18: Lake example. The camera movement path is complex\u2014it first moves to the right, then tilts upward, while the elements in the video change accordingly. All other models failed to accurately capture this camera movement, except for our DropletVideo. Our model not only strictly followed the prompt in executing the camera motion but also dynamically altered the scene, successfully revealing the sky and white clouds, which were not present in the initial image.", "description": "Figure 18 presents a comparison of video generation results for a lake scene. The prompt describes a complex camera movement: panning right and then tilting upwards, with corresponding changes in the scene's elements.  While other models failed to accurately reproduce this complex camera motion and scene evolution, DropletVideo successfully generated a video that precisely matched the prompt's specifications. This highlights DropletVideo's ability to maintain spatiotemporal consistency during both camera movement and content generation, even adding elements to the scene (sky and clouds) not initially visible in the image.", "section": "5.2 Qualitative Evaluation"}]