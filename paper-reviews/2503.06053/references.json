{"references": [{"fullname_first_author": "Robin Rombach", "paper_title": "High-resolution image synthesis with latent diffusion models", "publication_date": "2022-01-01", "reason": "This work is important because it introduced Latent Diffusion Models (LDMs), which significantly reduced the computational demands of diffusion models, enabling high-resolution image synthesis and influencing subsequent video generation approaches."}, {"fullname_first_author": "Colin Raffel", "paper_title": "Exploring the limits of transfer learning with a unified text-to-text transformer", "publication_date": "2020-01-01", "reason": "This work introduced the T5 transformer, a unified text-to-text model that significantly advanced transfer learning in NLP and is used in this paper as the text encoder."}, {"fullname_first_author": "Patrick Esser", "paper_title": "Scaling rectified flow transformers for high-resolution image synthesis", "publication_date": "2024-01-01", "reason": "This work describes a scaling approach to improving the quality of high-resolution images by a Transformer model."}, {"fullname_first_author": "Prafulla Dhariwal", "paper_title": "Diffusion models beat gans on image synthesis", "publication_date": "2021-01-01", "reason": "This work helped in showing that Diffusion Models are able to perform better than GANs on image synthesis."}, {"fullname_first_author": "Jonathan Ho", "paper_title": "Denoising diffusion probabilistic models", "publication_date": "2020-01-01", "reason": "This is an important paper because it laid the foundation for Denoising Diffusion Probabilistic Models (DDPMs), which the proposed method builds upon."}]}