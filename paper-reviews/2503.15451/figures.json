[{"figure_path": "https://arxiv.org/html/2503.15451/x3.png", "caption": "Figure 1: Visualization of streaming motion generation process. Texts are incrementally inputted and motions are generated online.", "description": "This figure visualizes the process of online motion generation.  The system receives text input incrementally, meaning one word or phrase at a time, rather than a complete sentence or paragraph. As each piece of text is added, the model generates the corresponding portion of the motion sequence.  The five depicted poses illustrate how the model adapts to changes in text, creating a continuous, flowing motion that accurately reflects the text's meaning. This continuous update of both the text input and motion output is a key aspect of the 'streaming' approach.", "section": "Abstract"}, {"figure_path": "https://arxiv.org/html/2503.15451/extracted/6294214/figure/cfg.png", "caption": "Figure 2: Overview of MotionStreamer.\nDuring inference,\nthe AR model streamingly predicts next motion latents conditioned on the current text and previous motion latents. Each latent can be decoded into motion frames online as soon as it is generated.", "description": "MotionStreamer processes text input and previous motion information using an autoregressive (AR) model to predict the next motion latent in a streaming fashion.  This prediction is continuously updated with new text inputs and previous motion. A diffusion head helps refine the latent representation.  The predicted latent is instantly decoded to generate a frame of the motion sequence, allowing for online motion generation. The figure shows both the overall streaming process (a) and a detailed view of the AR model with the diffusion head (b).", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.15451/extracted/6294214/figure/ik_failure.jpg", "caption": "Figure 3: Architecture of Causal TAE.\n1D temporal causal convolution is applied in both the encoder and decoder.\nVariables z1:nsubscript\ud835\udc67:1\ud835\udc5bz_{1:n}italic_z start_POSTSUBSCRIPT 1 : italic_n end_POSTSUBSCRIPT are sampled as continuous motion latent representations.", "description": "The figure illustrates the architecture of the Causal Temporal Autoencoder (Causal TAE), a key component of the MotionStreamer framework.  It shows a network with both a causal encoder and a causal decoder. The encoder takes in raw motion sequences as input and transforms them into a continuous latent space representation, using 1D causal convolutions.  These 1D causal convolutions ensure that only past data influences the representation of the current time step, respecting the temporal causality of motion data.  The decoder then takes the generated latents and reconstructs the motion sequence. The resulting continuous latent representations (z1:n) are crucial for mitigating information loss and error accumulation during streaming motion generation.", "section": "3.2. Causal Temporal AutoEncoder"}, {"figure_path": "https://arxiv.org/html/2503.15451/x6.png", "caption": "Figure 4: Comparison on the First-frame Latency of different methods.\nThe horizontal axis represents the number of generated frames, while the vertical axis indicates the time required to produce the first output frame.", "description": "Figure 4 illustrates the first-frame latency for various motion generation methods.  The x-axis shows the number of frames generated, and the y-axis shows the time (in seconds) it took each method to produce its very first frame. This metric is crucial for evaluating the speed and responsiveness of real-time motion generation, particularly in streaming scenarios where immediate feedback is essential. The figure clearly demonstrates the significant performance advantage of MotionStreamer (Causal TAE) in terms of producing the initial frame much faster than other models, highlighting the efficiency of its causal approach.", "section": "4.3. Qualitative Results"}, {"figure_path": "https://arxiv.org/html/2503.15451/x7.png", "caption": "Figure 5: Visualization results between our method and some baseline methods [60, 19, 64, 4].\nThe first row shows text-to-motion generation results, the second row shows long-term generation results\nand the third row shows the application of dynamic motion composition.", "description": "Figure 5 presents a comparison of MotionStreamer's motion generation capabilities against several baseline methods (T2M-GPT [60], MoMask [19], AttT2M [64], and FlowMDM [4]).  The figure is structured in three rows, each demonstrating a different aspect of motion generation. The first row showcases text-to-motion generation, comparing how accurately each method translates a single text prompt into a corresponding motion sequence. The second row focuses on long-term motion generation, where a series of text descriptions are used to generate a longer, continuous motion. This row highlights each algorithm's ability to maintain coherence and context across multiple text inputs. Finally, the third row shows dynamic motion composition. In this scenario, multiple, short motion sequences are combined in response to different prompts, demonstrating the system's ability to generate a seamless and natural-looking flow between diverse movements.", "section": "4.3 Qualitative Results"}]