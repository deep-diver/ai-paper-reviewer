[{"figure_path": "https://arxiv.org/html/2503.16282/x1.png", "caption": "Figure 1: \nComparison of our framework with previous work.\nTop: Prior work\u00a0[56, 66] primarily enhances prototypes through interaction modules that integrate support/query features, making predictions based on refined prototypes. However, they are limited by the sparse knowledge from few-shot samples.\nBottom: Our framework addresses this limitation by leveraging the extensive open-world knowledge from 3D VLMs through pseudo-labels. We mitigate the noise inherent in 3D VLMs by calibrating their raw pseudo-labels with precise few-shot samples, thereby effectively expanding novel class knowledge while ensuring reliability.", "description": "Figure 1 compares the proposed framework with existing methods for generalized few-shot 3D point cloud segmentation. The top panel illustrates traditional approaches that refine class prototypes using support and query features. These methods are limited by the sparse data available in few-shot learning scenarios. In contrast, the bottom panel shows the proposed framework, which leverages the rich knowledge of 3D vision-language models (VLMs) to generate pseudo-labels for novel classes.  To address the noise inherent in VLM predictions, the framework calibrates these pseudo-labels with precise few-shot samples, effectively expanding the knowledge about novel classes while maintaining reliability.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.16282/x2.png", "caption": "Figure 2: \nOverview of the proposed\u00a0GFS-VL.\n(a), (b) Given an input point cloud \ud835\udc17bsubscript\ud835\udc17b\\mathbf{X}_{\\rm b}bold_X start_POSTSUBSCRIPT roman_b end_POSTSUBSCRIPT, we apply a novel-base mix to embed support samples into the training scene while preserving essential context.\nThe scene is then processed by a 3D VLM, using all class names as prompts to generate raw predictions \ud835\udc18^^\ud835\udc18\\mathbf{\\hat{Y}}over^ start_ARG bold_Y end_ARG.\nLeveraging support prototypes {\ud835\udc29c}superscript\ud835\udc29c\\{\\mathbf{p}^{\\rm c}\\}{ bold_p start_POSTSUPERSCRIPT roman_c end_POSTSUPERSCRIPT }, the raw predictions undergo pseudo-label selection to filter out noisy regions, followed by adaptive infilling to label the filtered, unlabeled areas, yielding refined supervision \ud835\udc18b\u2032\u2032subscriptsuperscript\ud835\udc18\u2032\u2032b\\mathbf{Y}^{\\rm\\prime\\prime}_{\\rm b}bold_Y start_POSTSUPERSCRIPT \u2032 \u2032 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT roman_b end_POSTSUBSCRIPT for training the 3D segmentor. (c), (d) illustrate the details of the pseudo-label selection and adaptive infilling processes.", "description": "Figure 2 illustrates the GFS-VL framework.  First, a novel-base mix (a,b) integrates support samples into the input point cloud (\ud835\udc17b) while preserving context. This augmented data is fed into a 3D Vision-Language Model (VLM) to generate raw predictions (\ud835\udc18\u0302). Then, pseudo-label selection (c) filters out noisy predictions from \ud835\udc18\u0302 using support prototypes (\ud835\udc29c), and adaptive infilling (d) labels remaining unlabeled regions. The refined supervision (\ud835\udc18b\u2032\u2032) is then used to train the 3D segmentor.", "section": "4. Method"}, {"figure_path": "https://arxiv.org/html/2503.16282/x3.png", "caption": "Figure 3: \nQualitative comparison between GW\u00a0[66] and our GFS-VL\u00a0on ScanNet200. Class colors are shown at the top.", "description": "This figure presents a qualitative comparison of the segmentation results obtained using two different methods: GW [66] and the proposed GFS-VL method, on the ScanNet200 dataset.  The comparison highlights the effectiveness of the GFS-VL approach. For each scene, there are three columns: the ground truth segmentation, the segmentation produced by GW, and the segmentation produced by GFS-VL.  The color coding for each class is shown at the top of the figure, allowing for easy visual comparison between the methods. This allows one to visually assess the accuracy and quality of each segmentation method in segmenting various objects and scene elements within the selected scenes from the ScanNet200 dataset.", "section": "5. Experimental Results"}, {"figure_path": "https://arxiv.org/html/2503.16282/x4.png", "caption": "Figure 4: \nVisualization of the improvements in pseudo-label quality after applying Pseudo-label Selection (PS) and Adaptive Infilling (AI). Note that AI effectively discovers missed novel classes in the red circles and completes partial pseudo-labels in the green circles.", "description": "Figure 4 presents a visual comparison of pseudo-labels before and after applying the proposed pseudo-label selection (PS) and adaptive infilling (AI) methods.  The figure showcases how PS effectively filters out noisy predictions from the 3D Vision-Language Model (VLM), resulting in cleaner pseudo-labels.  It then demonstrates how AI successfully addresses incomplete or missing labels, particularly for novel classes.  This is visualized by highlighting regions where AI identifies missed novel object classes (red circles) and fills in partially labeled regions (green circles) to improve the overall quality and completeness of the pseudo-labels.", "section": "4. Method"}, {"figure_path": "https://arxiv.org/html/2503.16282/x5.png", "caption": "Figure 5: \nVisual illustration of mixing strategies.\nThe red and green boxes represent the two novel samples mixed into the scene.", "description": "Figure 5 compares three different data augmentation strategies: (a) Instance Mix randomly inserts novel class objects from foreground masks into scenes; (b) Mix3D overlays two scenes for out-of-context augmentation; and (c) Novel-Base Mix, which is the proposed method in this paper.  The red and green boxes highlight the novel samples incorporated into the training scene using the different techniques. Novel-Base Mix focuses on preserving the contextual cues in the scene, which is crucial for improved novel class learning.", "section": "4.4. Novel-Base Mix"}, {"figure_path": "https://arxiv.org/html/2503.16282/x6.png", "caption": "Figure 6: \nVisualization of the outputs from the proposed Novel-Base Mix. The red and green boxes represent the two novel samples mixed into the scene. The novel class colors are shown at the top.", "description": "This figure visualizes the results of the Novel-Base Mix data augmentation technique.  The Novel-Base Mix method integrates novel class samples into the training data while preserving the essential context of the base scene. This helps the model learn to identify novel classes more effectively.  The image shows several point cloud segmentation results. Each segmentation shows the same scene with base classes and two novel classes added.  The red and green boxes highlight the locations of the two novel samples which have been incorporated into the scene. The novel class labels are displayed at the top of the image for reference. The image demonstrates how the technique integrates the novel classes into a typical scene.", "section": "4. Method"}, {"figure_path": "https://arxiv.org/html/2503.16282/x7.png", "caption": "Figure 7: \nQualitative comparison between GW\u00a0[66] and our GFS-VL\u00a0on ScanNet200.\nThe visualizations demonstrate the superior segmentation performance and novel class generalization capabilities of\u00a0GFS-VL.\nFor clarity, class colors are displayed on the right and are restricted to those present in the ground truth annotations.", "description": "Figure 7 presents a qualitative comparison of the performance of the proposed GFS-VL model against the GW model on the ScanNet200 dataset.  The figure displays several example point cloud segmentation results. Each row shows a different scene, with the 'Query' column showing the input point cloud, 'Ground Truth' showing the accurate segmentation labels, 'GW' showing the segmentation results produced by the GW model, and 'GFS-VL (Ours)' displaying the results from the proposed GFS-VL model.  The visualizations clearly highlight GFS-VL's improved accuracy and ability to generalize to novel classes, demonstrating its superior performance compared to the GW model. To enhance clarity and focus on the key differences, the color legend only includes classes present in the ground truth annotations for each scene.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.16282/x8.png", "caption": "Figure 8: \nVisualization of pseudo-label refinement using Pseudo-label Selection (PS) and Adaptive Infilling (AI).\nRed circles indicate novel objects discovered by AI that were missed in the raw pseudo-labels, while green circles indicate regions where AI completes previously partially segmented areas.\nFor clarity, class colors are displayed at the top and correspond to labels present in the full class annotations.", "description": "Figure 8 visualizes the improvement in pseudo-label quality achieved by the proposed method's Pseudo-label Selection (PS) and Adaptive Infilling (AI) modules.  The figure showcases several examples of point cloud segmentations at different stages of processing. The first column ('Query') shows the initial input point cloud. The second column ('Base Class Labels') displays the base class labels before any novel class prediction. The third column ('Full Class Labels') displays the ground truth, including novel classes. The next three columns show the results after applying the 3D VLM, applying the Pseudo-label Selection (PS) module, and applying the Adaptive Infilling (AI) module. Red circles highlight novel objects identified by AI but missed by the initial 3D VLM prediction, while green circles show areas where AI successfully fills in previously incomplete or partially incorrect segmentation of novel classes. The colors at the top correspond to the class labels, offering a clear visual comparison between the different processing stages. This demonstrates how AI and PS refine the initially noisy 3D VLM predictions to produce much higher quality pseudo-labels.", "section": "4. Method"}]