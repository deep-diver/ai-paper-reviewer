{"references": [{" publication_date": "2020", "fullname_first_author": "Tom Brown", "paper_title": "Language Models are Few-Shot Learners", "reason": "This paper is foundational for the field of large language models (LLMs) and their ability to perform well with few examples.  The introduction of chain-of-thought prompting and in-context learning is directly relevant to the capabilities of MobA, which makes extensive use of these techniques in its design and execution.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Jason Wei", "paper_title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models", "reason": "This paper introduces chain-of-thought prompting, a crucial technique utilized by MobA for task planning and decomposition.  The ability of LLMs to reason effectively through step-by-step reasoning, as demonstrated in this paper, is a core component of MobA's functionality.", "section_number": 1}, {" publication_date": "2021", "fullname_first_author": "Chongyang Bai", "paper_title": "UIBert: Learning Generic Multimodal Representations for UI Understanding", "reason": "This paper addresses the challenge of understanding user interfaces (UIs), which is directly relevant to the GUI interaction capabilities of MobA.  The ability to learn generic multimodal representations for UI understanding is crucial for an agent operating in a dynamic environment like a mobile phone.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Zecheng He", "paper_title": "Actionbert: Leveraging user actions for semantic understanding of user interfaces", "reason": "This paper focuses on leveraging user actions to improve the semantic understanding of user interfaces. This is highly relevant to MobA's approach, which leverages both visual and textual information to better understand user intent and plan actions accordingly.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Songqin Nong", "paper_title": "MobileFlow: A Multimodal LLM For Mobile GUI Agent", "reason": "This paper presents a multimodal LLM-based agent for mobile GUI interaction, similar to MobA.  Comparing MobA against this prior work helps to establish the novelty and improvements offered by the proposed system.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Sunjae Lee", "paper_title": "Explore, Select, Derive, and Recall: Augmenting LLM with Human-like Memory for Mobile Task Automation", "reason": "This paper explores enhancing LLMs with human-like memory for mobile task automation, a key aspect of MobA's design.  This comparison highlights MobA's unique memory mechanisms and how it differs from previous approaches in enhancing LLM performance.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Junyang Wang", "paper_title": "Mobile-Agent-v2: Mobile Device Operation Assistant with Effective Navigation via Multi-Agent Collaboration", "reason": "This work presents MobileAgent(v2), a key baseline system in the MobA experimental evaluation.  Analyzing the differences between MobA and MobileAgent(v2) helps highlight the strengths of the novel architecture proposed by MobA.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Chi Zhang", "paper_title": "AppAgent: Multimodal Agents as Smartphone Users", "reason": "This paper introduces AppAgent, another key baseline system in the MobA experimental evaluation. By comparing the results of MobA against AppAgent, the improvements and novel aspects of MobA's architecture can be better highlighted.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Liangtai Sun", "paper_title": "META-GUI: Towards Multi-modal Conversational Agents on Mobile GUI", "reason": "This paper addresses the challenge of creating multimodal conversational agents for mobile GUI interaction, similar to MobA.  Comparing MobA's performance against META-GUI helps to establish its improvements and novel aspects.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Xu Huang", "paper_title": "Understanding the planning of LLM agents: A survey", "reason": "This survey paper provides a comprehensive overview of LLM-based agent planning techniques.  It serves as valuable background information, contextualizing MobA's approach to planning within the broader landscape of LLM agent planning.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Aman Madaan", "paper_title": "Self-Refine: Iterative Refinement with Self-Feedback", "reason": "This paper presents the Self-Refine method, which utilizes self-feedback for iterative refinement. This technique is relevant to MobA's double-reflection mechanism, which uses similar iterative feedback loops to improve task completion accuracy.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Alec Radford", "paper_title": "Learning Transferable Visual Models From Natural Language Supervision", "reason": "This paper introduces the CLIP model, which is crucial for multimodal understanding in MobA.  The ability to effectively combine textual and visual information is a key aspect of MobA's functionality, and this paper is foundational to that capability.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Xinbei Ma", "paper_title": "CoCo-Agent: A Comprehensive Cognitive MLLM Agent for Smartphone GUI Automation", "reason": "This paper introduces CoCo-Agent, another LLM-powered agent for smartphone automation. Comparing MobA to CoCo-Agent highlights the unique aspects of MobA's architecture, such as the two-level agent structure and the double-reflection mechanism.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Toby Jia-Jun Li", "paper_title": "SUGILITE: Creating Multimodal Smartphone Automation by Demonstration", "reason": "This paper explores multimodal smartphone automation using demonstrations, which provides a valuable point of comparison for evaluating MobA's approach.  The paper highlights the limitations of traditional methods compared to the capabilities of MLLM-based approaches.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Zhenyu Wu", "paper_title": "Embodied Task Planning with Large Language Models", "reason": "This paper explores the use of LLMs for embodied task planning, which is a related area to the work done in MobA. The similarities and differences between MobA's approach and this paper's method illuminate the uniqueness and efficiency of the MOBA architecture.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Junyang Wang", "paper_title": "Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception", "reason": "This paper introduces MobileAgent, a significant baseline system used to compare MobA's performance.  This comparison helps to underscore MobA's improvements in efficiency and success rate.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Eunkyung Jo", "paper_title": "Understanding the Benefits and Challenges of Deploying Conversational AI Leveraging Large Language Models for Public Health Intervention", "reason": "This paper explores the application of LLMs in public health interventions, highlighting the benefits and challenges of this approach.  This provides a valuable perspective for understanding the potential societal impact and limitations of applying LLMs in mobile assistance systems, as explored by MobA.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Noah Shinn", "paper_title": "ReAct: Synergizing Reasoning and Acting in Language Models", "reason": "This paper introduces the ReAct framework for synergizing reasoning and acting in language models.  This is directly relevant to the design of MobA, which integrates reasoning (planning) and acting (execution) through a two-level agent architecture.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Bryan Wang", "paper_title": "Screen2Words: Automatic Mobile UI Summarization with Multimodal Learning", "reason": "This paper explores the use of multimodal learning for automatic mobile UI summarization. This is highly relevant to MobA's approach, which leverages multimodal data to improve the accuracy and efficiency of GUI interactions.", "section_number": 2}]}