[{"figure_path": "https://arxiv.org/html/2411.15033/extracted/6017801/images/architecture.png", "caption": "Figure 1: Architecture of the system.", "description": "This figure illustrates the system's architecture, which consists of two main modules: the Perception Module and the Planner Module.  The Perception Module is responsible for acquiring and interpreting environmental information using RGB-D cameras and building a semantic map (a directed graph representing both geometric and semantic features). This information is then passed to the Planner Module. The Planner Module leverages a modified ReAct framework with Large Language Models (LLMs) to translate user commands (in natural language) into executable robot actions.  The Planner Module incorporates real-time environmental feedback and dynamically updates plans based on this feedback.", "section": "3. Architecture"}, {"figure_path": "https://arxiv.org/html/2411.15033/extracted/6017801/images/planner_architecture.png", "caption": "Figure 2: Architecture of the planner module.", "description": "This figure illustrates the architecture of the Planner module, a key component of a robotic system designed to translate natural language commands into executable robot actions. The module consists of five sub-modules: Task Planner, which translates high-level user requests into sequences of skills; Skill Planner, which converts these skills into low-level commands executable by the robot; Executor, which executes these commands; Controller, which monitors the execution and handles errors; and Explainer, which analyzes failures and provides suggestions for corrective actions.  The figure shows how these modules interact to translate user input (commands), incorporating feedback loops for dynamic plan adjustments. It shows the flow of information and the decision-making process for executing robotic actions based on user input and real-time feedback from the robot's environment.", "section": "3.1. Planner module"}, {"figure_path": "https://arxiv.org/html/2411.15033/extracted/6017801/images/robee.png", "caption": "Figure 3: Robee, humaniod robot developed by Oversonic Robotics.", "description": "This image shows RoBee, a humanoid robot developed by Oversonic Robotics.  RoBee is used in the experiments described in the paper to test the proposed natural language-based robotic task planning system. Its features, relevant to the research, include 32 degrees of freedom, allowing for flexible movement; multiple sensors (cameras, microphones, force sensors, LIDAR) providing data for perception; and two arms capable of bimanual manipulation.", "section": "5. Robot Hardware"}]