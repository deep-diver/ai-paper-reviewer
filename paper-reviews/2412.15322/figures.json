[{"figure_path": "https://arxiv.org/html/2412.15322/x1.png", "caption": "Figure 1: In addition to training on audio-visual-(text) datasets, we perform multimodal joint training with high-quality, abundant audio-text data which enables effective data scaling.\nAt inference, MMAudio generates conditions-aligned audio with video and/or text guidance.", "description": "This figure illustrates the MMAudio model's training and inference processes.  During training, MMAudio leverages two types of datasets: 1) audio-visual data (potentially including text) which provide associations between audio and visual scenes, and 2) large-scale audio-text data that teach semantic relationships between sounds and textual descriptions. The combination of these datasets allows for significant data scaling and improved model performance.  The multimodal training approach enables the model to better capture semantic alignment between the modalities.  At inference, the model takes video and/or text as input and generates synchronized audio based on the input conditions. ", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2412.15322/x2.png", "caption": "Figure 2: \nOverview of the MMAudio flow-prediction network.\nVideo conditions, text conditions, and audio latents jointly interact in the multimodal transformer network.\nA synchronization model (Section\u00a03.4) injects frame-aligned synchronization features for precise audio-visual synchrony.", "description": "The figure provides a detailed illustration of the MMAudio model architecture.  It shows how video, text, and audio features are processed through a series of multimodal transformer blocks, which allow the different modalities to interact and inform one another. This is followed by audio-only transformer blocks that further refine the generated audio. A crucial element is the conditional synchronization module which helps align the audio with the visual input at the frame level.  The diagram highlights the flow of information, from input modalities to the final audio output, demonstrating the model's ability to handle complex audio-visual interactions and generate high-quality audio samples that are semantically and temporally aligned with the input video and/or text.", "section": "3 MMAudio"}, {"figure_path": "https://arxiv.org/html/2412.15322/x3.png", "caption": "Figure 3: We visualize the spectrograms of generated audio (by prior works and our method) and the ground-truth.\nNote our method generates the audio effects most closely aligned to the ground-truth, while other methods often generate sounds not explained by the visual input and not present in the ground-truth.", "description": "Figure 3 presents a comparison of spectrograms generated by various video-to-audio synthesis methods, including the proposed MMAudio model, against ground truth spectrograms.  The figure visually demonstrates MMAudio's superior performance in generating audio that closely matches the ground truth.  In contrast, other methods often produce audio that is either semantically misaligned with the input video or contains sounds that are not present in or explained by the visual content. This highlights MMAudio's ability to accurately capture and synthesize the audio components corresponding to visual events in the input video.", "section": "4 Experiments"}, {"figure_path": "https://arxiv.org/html/2412.15322/x4.png", "caption": "Table 2: Text-to-audio results on the AudioCaps test set.\nFor a fair comparison, we follow the evaluation protocol of\u00a0[11] and transcribe all baselines directly from\u00a0[11], who have reproduced those results using officially released checkpoints under the same evaluation protocol.", "description": "Table 2 presents a comparison of text-to-audio models' performance on the AudioCaps test set.  To ensure fairness, the authors replicated the results of other models using the exact evaluation methods and officially released checkpoints detailed in reference [11], thus controlling for any variations in implementation or training conditions. The table compares metrics such as Fr\u00e9chet Distance (FD) using both PaSST and VGGish, Inception Score (IS), and CLAP, providing a comprehensive evaluation of the various models.", "section": "4 Experiments"}]