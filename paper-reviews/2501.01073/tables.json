[{"content": "| Model (Rep.) | Likelihood | Illustration | #Network Calls | #Variables | Decomposition |\n|---|---|---|---|---|---| \n| Diffusion (<math alttext=\"\\mathbf{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.T1.1.1.1.1.m1.1\"><semantics id=\"S1.T1.1.1.1.1.m1.1a\"><mi id=\"S1.T1.1.1.1.1.m1.1.1\" xref=\"S1.T1.1.1.1.1.m1.1.1.cmml\">\ud835\udc00</mi><annotation-xml encoding=\"MathML-Content\" id=\"S1.T1.1.1.1.1.m1.1b\"><ci id=\"S1.T1.1.1.1.1.m1.1.1.cmml\" xref=\"S1.T1.1.1.1.1.m1.1.1\">\ud835\udc00</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S1.T1.1.1.1.1.m1.1c\">\\mathbf{A}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S1.T1.1.1.1.1.m1.1d\">bold_A</annotation></semantics></math>) | <math alttext=\"p(\\mathbf{A}^{T})\\displaystyle\\prod_{t=1}^{T}p(\\mathbf{A}^{t-1}|\\mathbf{A}^{t})\" class=\"ltx_Math\" display=\"inline\" id=\"S1.T1.2.2.2.2.m1.2\"><semantics id=\"S1.T1.2.2.2.2.m1.2a\"><mrow id=\"S1.T1.2.2.2.2.m1.2.2\" xref=\"S1.T1.2.2.2.2.m1.2.2.cmml\"><mi id=\"S1.T1.2.2.2.2.m1.2.2.4\" xref=\"S1.T1.2.2.2.2.m1.2.2.4.cmml\">p</mi><mo id=\"S1.T1.2.2.2.2.m1.2.2.3\" xref=\"S1.T1.2.2.2.2.m1.2.2.3.cmml\">\\,\\prod_{t=1}^{T}p(\\mathbf{A}^{t-1}|\\mathbf{A}^{t})</annotation></semantics></math> | <img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_img_landscape\" height=\"66\" id=\"S1.T1.3.3.3.3.1.g1\" src=\"https://arxiv.org/html/2501.01073/x1.png\" width=\"332\"/> | <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S1.T1.4.4.4.4.m1.1\"><semantics id=\"S1.T1.4.4.4.4.m1.1a\"><mi id=\"S1.T1.4.4.4.4.m1.1.1\" xref=\"S1.T1.4.4.4.4.m1.1.1.cmml\">T</mi><annotation-xml encoding=\"MathML-Content\" id=\"S1.T1.4.4.4.4.m1.1b\"><ci id=\"S1.T1.4.4.4.4.m1.1.1.cmml\" xref=\"S1.T1.4.4.4.4.m1.1.1\">\ud835\udc47</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S1.T1.4.4.4.4.m1.1c\">T</annotation><annotation encoding=\"application/x-llamapun\" id=\"S1.T1.4.4.4.4.m1.1d\">italic_T</annotation></semantics></math> | <math alttext=\"O(Tn^{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S1.T1.5.5.5.5.m1.1\"><semantics id=\"S1.T1.5.5.5.5.m1.1a\"><mrow id=\"S1.T1.5.5.5.5.m1.1.1\" xref=\"S1.T1.5.5.5.5.m1.1.1.cmml\"><mi id=\"S1.T1.5.5.5.5.m1.1.1.3\" xref=\"S1.T1.5.5.5.5.m1.1.1.3.cmml\">O</mi><mo id=\"S1.T1.5.5.5.5.m1.1.1.2\" xref=\"S1.T1.5.5.5.5.m1.1.1.2.cmml\">\\,\\prod_{t=1}^{T}p(\\mathbf{A}^{t-1}|\\mathbf{A}^{t})</annotation></semantics></math> | Conditional independent |\n| Auto-regressive (<math alttext=\"\\mathbf{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.T1.6.6.6.1.m1.1\"><semantics id=\"S1.T1.6.6.6.1.m1.1a\"><mi id=\"S1.T1.6.6.6.1.m1.1.1\" xref=\"S1.T1.6.6.6.1.m1.1.1.cmml\">\ud835\udc00</mi><annotation-xml encoding=\"MathML-Content\" id=\"S1.T1.6.6.6.1.m1.1b\"><ci id=\"S1.T1.6.6.6.1.m1.1.1.cmml\" xref=\"S1.T1.6.6.6.1.m1.1.1\">\ud835\udc00</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S1.T1.6.6.6.1.m1.1c\">\\mathbf{A}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S1.T1.6.6.6.1.m1.1d\">bold_A</annotation></semantics></math>) | <math alttext=\"\\displaystyle\\prod_{i=2}^{n}\\prod_{j=1}^{i-1}p(\\mathbf{A}_{i,j}|\\mathbf{A}_{&lt;i,% <i-1},\\mathbf{A}_{i,&lt;j})\" class=\"ltx_Math\" display=\"inline\" id=\"S1.T1.7.7.7.2.m1.7\"><semantics id=\"S1.T1.7.7.7.2.m1.7a\"><mrow id=\"S1.T1.7.7.7.2.m1.7.7\" xref=\"S1.T1.7.7.7.2.m1.7.7.cmml\"><mstyle displaystyle=\"true\" id=\"S1.T1.7.7.7.2.m1.7.7.2\" xref=\"S1.T1.7.7.7.2.m1.7.7.2.cmml\"><munderover id=\"S1.T1.7.7.7.2.m1.7.7.2a\" xref=\"S1.T1.7.7.7.2.m1.7.7.2.cmml\"><mo id=\"S1.T1.7.7.7.2.m1.7.7.2.2.2\" movablelimits=\"false\" xref=\"S1.T1.7.7.7.2.m1.7.7.2.2.2.cmml\">\\prod_{i=2}^{n}\\prod_{j=1}^{i-1}p(\\mathbf{A}_{i,j}|\\mathbf{A}_{&lt;i,% <i-1},\\mathbf{A}_{i,&lt;j})</annotation></semantics></math> | <img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_img_landscape\" height=\"66\" id=\"S1.T1.8.8.8.3.1.g1\" src=\"https://arxiv.org/html/2501.01073/x2.png\" width=\"332\"/> | <math alttext=\"O(n^{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S1.T1.9.9.9.4.m1.1\"><semantics id=\"S1.T1.9.9.9.4.m1.1a\"><mrow id=\"S1.T1.9.9.9.4.m1.1.1\" xref=\"S1.T1.9.9.9.4.m1.1.1.cmml\"><mi id=\"S1.T1.9.9.9.4.m1.1.1.3\" xref=\"S1.T1.9.9.9.4.m1.1.1.3.cmml\">O</mi><mo id=\"S1.T1.9.9.9.4.m1.1.1.2\" xref=\"S1.T1.9.9.9.4.m1.1.1.2.cmml\">\\,\\prod_{i=2}^{n}\\prod_{j=1}^{i-1}p(\\mathbf{A}_{i,j}|\\mathbf{A}_{&lt;i,% <i-1},\\mathbf{A}_{i,&lt;j})</annotation></semantics></math> | <math alttext=\"O(n^{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S1.T1.10.10.10.5.m1.1\"><semantics id=\"S1.T1.10.10.10.5.m1.1a\"><mrow id=\"S1.T1.10.10.10.5.m1.1.1\" xref=\"S1.T1.10.10.10.5.m1.1.1.cmml\"><mi id=\"S1.T1.10.10.10.5.m1.1.1.3\" xref=\"S1.T1.10.10.10.5.m1.1.1.3.cmml\">O</mi><mo id=\"S1.T1.10.10.10.5.m1.1.1.2\" xref=\"S1.T1.10.10.10.5.m1.1.1.2.cmml\">\\,\\prod_{i=2}^{n}\\prod_{j=1}^{i-1}p(\\mathbf{A}_{i,j}|\\mathbf{A}_{&lt;i,% <i-1},\\mathbf{A}_{i,&lt;j})</annotation></semantics></math> | Full factorization |\n| Auto-regressive (<math alttext=\"E\" class=\"ltx_Math\" display=\"inline\" id=\"S1.T1.11.11.11.1.m1.1\"><semantics id=\"S1.T1.11.11.11.1.m1.1a\"><mi id=\"S1.T1.11.11.11.1.m1.1.1\" xref=\"S1.T1.11.11.11.1.m1.1.1.cmml\">E</mi><annotation-xml encoding=\"MathML-Content\" id=\"S1.T1.11.11.11.1.m1.1b\"><ci id=\"S1.T1.11.11.11.1.m1.1.1.cmml\" xref=\"S1.T1.11.11.11.1.m1.1.1\">\ud835\udc38</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S1.T1.11.11.11.1.m1.1c\">E</annotation><annotation encoding=\"application/x-llamapun\" id=\"S1.T1.11.11.11.1.m1.1d\">italic_E</annotation></semantics></math>) | <math alttext=\"p(e_{1})\\displaystyle\\prod_{i=2}^{m}p(e_{i}|e_{&lt;i})\" class=\"ltx_Math\" display=\"inline\" id=\"S1.T1.12.12.12.2.m1.2\"><semantics id=\"S1.T1.12.12.12.2.m1.2a\"><mrow id=\"S1.T1.12.12.12.2.m1.2.2\" xref=\"S1.T1.12.12.12.2.m1.2.2.cmml\"><mi id=\"S1.T1.12.12.12.2.m1.2.2.4\" xref=\"S1.T1.12.12.12.2.m1.2.2.4.cmml\">p</mi><mo id=\"S1.T1.12.12.12.2.m1.2.2.3\" xref=\"S1.T1.12.12.12.2.m1.2.2.3.cmml\">\\,\\prod_{i=2}^{m}p(e_{i}|e_{&lt;i})</annotation></semantics></math> | <img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_img_landscape\" height=\"66\" id=\"S1.T1.13.13.13.3.1.g1\" src=\"https://arxiv.org/html/2501.01073/x3.png\" width=\"332\"/> | <math alttext=\"O(m)\" class=\"ltx_Math\" display=\"inline\" id=\"S1.T1.14.14.14.4.m1.1\"><semantics id=\"S1.T1.14.14.14.4.m1.1a\"><mrow id=\"S1.T1.14.14.14.4.m1.1.2\" xref=\"S1.T1.14.14.14.4.m1.1.2.cmml\"><mi id=\"S1.T1.14.14.14.4.m1.1.2.2\" xref=\"S1.T1.14.14.14.4.m1.1.2.2.cmml\">O</mi><mo id=\"S1.T1.14.14.14.4.m1.1.2.1\" xref=\"S1.T1.14.14.14.4.m1.1.2.1.cmml\">\\,\\prod_{i=2}^{m}p(e_{i}|e_{&lt;i})</annotation></semantics></math> | <math alttext=\"O(m)\" class=\"ltx_Math\" display=\"inline\" id=\"S1.T1.15.15.15.5.m1.1\"><semantics id=\"S1.T1.15.15.15.5.m1.1a\"><mrow id=\"S1.T1.15.15.15.5.m1.1.2\" xref=\"S1.T1.15.15.15.5.m1.1.2.cmml\"><mi id=\"S1.T1.15.15.15.5.m1.1.2.2\" xref=\"S1.T1.15.15.15.5.m1.1.2.2.cmml\">O</mi><mo id=\"S1.T1.15.15.15.5.m1.1.2.1\" xref=\"S1.T1.15.15.15.5.m1.1.2.1.cmml\">\\,\\prod_{i=2}^{m}p(e_{i}|e_{&lt;i})</annotation></semantics></math> | Full factorization |", "caption": "Table 1: Overview of graph generative model families combined with the used data representation (Rep.). n\ud835\udc5bnitalic_n: number of nodes. m\ud835\udc5amitalic_m: number of edges. In the illustration, we use solid line for edges and dash line for non-edges, (non-)edges generated at current step are colored in blue. Our proposed G2PT is an Auto-regressive model that learns on E\ud835\udc38Eitalic_E representation.", "description": "This table provides a comparison of different graph generative model families, categorized by their data representation (adjacency matrix or edge set) and likelihood decomposition approach (diffusion-based or autoregressive). It shows the computational complexity in terms of the number of network calls and variables involved.  The illustration visually explains how each approach generates graph structures, step-by-step, using solid and dashed lines to depict edges and non-edges, with the current step highlighted in blue. Notably, the table highlights that the proposed Graph Generative Pre-trained Transformer (G2PT) is an autoregressive model working with the edge set representation.", "section": "2. A Review of Graph Generative Models"}, {"content": "Model|Planar|Planar|Planar|Planar|Planar|Planar|Tree|Tree|Tree|Tree|Tree|Tree\n---|---|---|---|---|---|---|---|---|---|---|---|---\nDeg. \u2193|Clus. \u2193|Orbit \u2193|Spec. \u2193|Wavelet \u2193|V.U.N. \u2191|Deg. \u2193|Clus. \u2193|Orbit \u2193|Spec. \u2193|Wavelet \u2193|V.U.N. \u2191\nGRAN (Liao et al., 2019)|7e-4|4.3e-2|9e-4|7.5e-3|1.9e-3|0|1.9e-1|8e-3|2e-2|2.8e-1|3.3e-1|0\nBiGG (Dai et al., 2020)|7e-4|5.7e-2|3.7e-2|1.1e-2|5.2e-3|5|1.4e-3|0.00|0.00|1.2e-2|5.8e-3|75\nDiGress (Vignac et al., 2022)|7e-4|7.8e-2|7.9e-3|9.8e-3|3.1e-3|77.5|2e-4|0.00|0.00|1.1e-2|4.3e-3|90\nBwR (Diamant et al., 2023)|2.3e-2|2.6e-1|5.5e-1|4.4e-2|1.3e-1|0|1.6e-3|1.2e-1|3e-4|4.8e-2|3.9e-2|0\nHSpectre (Bergmeister et al., 2023)|5e-4|6.3e-2|1.7e-3|7.5e-3|1.3e-3|95|1e-4|0.00|0.00|1.2e-2|4.7e-3|100\nDeFoG (Qin et al., 2024)|5e-4|5e-2|6e-4|7.2e-3|1.4e-3|99.5|2e-4|0.00|0.00|1.1e-2|4.6e-3|96.5\nG2PT<sub>small</sub>|4.7e-3|2.4e-3|0.00|1.6e-2|1.4e-2|95|2e-3|0.00|0.00|7.4e-3|3.9e-3|99\nG2PT<sub>base</sub>|1.8e-3|4.7e-3|0.00|8.1e-3|5.1e-3|100|4.3e-3|0.00|1e-4|7.3e-3|5.7e-3|99\nModel|Lobster|Lobster|Lobster|Lobster|Lobster|Lobster|SBM|SBM|SBM|SBM|SBM|SBM\n---|---|---|---|---|---|---|---|---|---|---|---|---\nDeg. \u2193|Clus. \u2193|Orbit \u2193|Spec. \u2193|Wavelet \u2193|V.U.N. \u2191|Deg. \u2193|Clus. \u2193|Orbit \u2193|Spec. \u2193|Wavelet \u2193|V.U.N. \u2191\nGRAN (Liao et al., 2019)|3.8e-2|0.00|1e-3|2.7e-2|-|-|1.1e-2|5.5e-2|5.4e-2|5.4e-3|2.1e-2|25\nBiGG (Dai et al., 2020)|0.00|0.00|0.00|9e-3|-|-|1.2e-3|6.0e-2|6.7e-2|5.9e-3|3.7e-2|10\nDiGress (Vignac et al., 2022)|2.1e-2|0.00|4e-3|-|-|-|1.8e-3|4.9e-2|4.2e-2|4.5e-3|1.4e-3|60\nBwR (Diamant et al., 2023)|3.2e-1|0.00|2.5e-1|-|-|-|4.8e-2|6.4e-2|1.1e-1|1.7e-2|8.9e-2|7.5\nHSpectre (Bergmeister et al., 2023)|-|-|-|-|-|-|1.2e-2|5.2e-2|6.7e-2|6.7e-3|2.2e-2|45\nDeFoG (Qin et al., 2024)|-|-|-|-|-|-|6e-4|5.2e-2|5.6e-2|5.4e-3|8e-3|90\nG2PT<sub>small</sub>|2e-3|0.00|0.00|5e-3|8.5e-3|100|3.5e-3|1.2e-2|7e-4|7.6e-3|9.8e-3|100\nG2PT<sub>base</sub>|1e-3|0.00|0.00|4e-3|1e-2|100|4.2e-3|5.3e-3|3e-4|6.1e-3|6.9e-3|100", "caption": "Table 2: Generative performance on generic graph datasets.", "description": "This table presents a quantitative comparison of the generative performance of different graph generative models on four generic graph datasets: Planar, Tree, Lobster, and Stochastic Block Model (SBM).  The performance is evaluated using several metrics including degree distribution, clustering coefficient, orbit counts, spectral properties, wavelet statistics, and the percentage of valid, unique, and novel graphs generated.  These metrics assess various structural and topological aspects of the generated graphs, offering a comprehensive evaluation of each model's ability to generate realistic and diverse graph structures.", "section": "5. Experiments"}, {"content": "| Rep. | #Tokens \u2193 | Deg. \u2193 | Clus. \u2193 | Orbit \u2193 | Spec. \u2193 | Wavelet \u2193 | V.U.N. \u2191 |\n|---|---|---|---|---|---|---|---| \n| \ud835\udc00 | 2018 | 8.6e-3 | 1e-1 | 8e-3 | 3.2e-2 | 6.1e-2 | 94 |\n| Ours | 737 | 4.7e-3 | 2.4e-3 | 0.00 | 1.6e-2 | 1.4e-2 | 95 |", "caption": "Table 3: Generative performance comparison between the proposed edge sequence and adjacency matrix representations.", "description": "This table compares the performance of graph generative models using two different graph representations: the proposed edge sequence representation and the traditional adjacency matrix representation.  It evaluates the generative quality on planar graphs, using metrics such as the number of valid, unique and novel graphs generated,  and the degree, clustering coefficient, orbital count, spectral properties, and wavelet statistics of the generated graphs. The comparison allows assessment of the effectiveness of the novel edge sequence representation against the standard adjacency matrix method.", "section": "5.2 A Demonstrative Experiment using Planar Graphs"}, {"content": "| **A** | **A** | **Ours** | **Ours** |\n|---|---|---|---|\n| ![demo_planar_A_1](https://arxiv.org/html/2501.01073/images/demo_planar_A_1.png) | ![demo_planar_A_2](https://arxiv.org/html/2501.01073/images/demo_planar_A_2.png) | ![demo_planar_ours_1](https://arxiv.org/html/2501.01073/images/demo_planar_ours_1.png) | ![demo_planar_ours_2](https://arxiv.org/html/2501.01073/images/demo_planar_ours_2.png) |", "caption": "Table 4: Generative performance on molecular graph datasets", "description": "This table presents a comparison of the performance of different graph generative models on molecular graph datasets.  It shows quantitative metrics evaluating the quality of generated molecules, including validity (percentage of valid molecules), uniqueness (percentage of unique molecules), novelty (percentage of novel molecules not present in training data),  fraction of molecules that pass certain filters, Frechet ChemNet Distance (FCD), Scaffold similarity, and other metrics specific to the MOSES and GuacaMol benchmarks. This allows for a direct comparison of how well each model generates realistic and diverse molecules.", "section": "5. Experiments"}, {"content": "| Model | MOSES Validity \u2191 | MOSES Unique \u2191 | MOSES Novelty \u2191 | MOSES Filters \u2191 | MOSES FCD \u2193 | MOSES SNN \u2191 | MOSES Scaf \u2191 | GuacaMol Validity \u2191 | GuacaMol Unique \u2191 | GuacaMol Novelty \u2191 | GuacaMol KL Div \u2191 | GuacaMol FCD \u2191 |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| DiGress (Vignac et al., 2022) | 85.7 | **100** | **95.0** | 97.1 | 1.19 | 0.52 | 14.8 | 85.2 | **100** | **99.9** | 92.9 | 68 |\n| DisCo (Xu et al., 2024) | 88.3 | **100** | **97.7** | 95.6 | 1.44 | 0.5 | **15.1** | 86.6 | 86.6 | 86.5 | 92.6 | 59.7 |\n| Cometh (Siraudin et al., 2024) | 90.5 | 99.9 | 92.6 | **99.1** | 1.27 | **0.54** | **16.0** | **98.9** | 98.9 | 97.6 | **96.7** | 72.7 |\n| DeFoG (Qin et al., 2024) | 92.8 | 99.9 | 92.1 | **99.9** | 1.95 | **0.55** | 14.4 | **99.0** | **99.0** | 97.9 | **97.9** | 73.8 |\n| G2PT<sub>small</sub> | 95.1 | **100** | 91.7 | 97.4 | 1.10 | 0.52 | 5.0 | 90.4 | **100** | **99.8** | 92.8 | 86.6 |\n| G2PT<sub>base</sub> | **96.4** | **100** | 86.0 | 98.3 | **0.97** | **0.55** | 3.3 | 94.6 | **100** | 99.5 | 96.0 | **93.4** |\n| G2PT<sub>large</sub> | **97.2** | **100** | 79.4 | 98.9 | **1.02** | **0.55** | 2.9 | 95.3 | **100** | 99.5 | 95.6 | **92.7** |", "caption": "Table 5: Results for molecule property prediction in terms of ROC-AUC. We report mean and standard deviation over three runs.", "description": "This table presents the results of molecular property prediction experiments using the Graph Generative Pre-trained Transformer (G2PT) model.  The performance is measured by the area under the Receiver Operating Characteristic curve (ROC-AUC).  The table shows ROC-AUC scores for multiple datasets (BBBP, Tox21, ToxCast, SIDER, ClinTox, MUV, HIV, BACE) and multiple model sizes, with mean and standard deviation reported across three independent runs, providing a comprehensive evaluation of the G2PT's predictive capabilities in various molecular property prediction tasks.", "section": "5. Predictive Performance on Downstream Tasks"}, {"content": "| Model | QM9 |  |  | \n|---|---|---|---|\n|  | Validity\u2191 | Unique\u2191 | FCD\u2193 | \n| DiGress (Vignac et al., 2022) | 99.0 | 96.2 | - | \n| DisCo (Xu et al., 2024) | 99.6 | 96.2 | 0.25 | \n| Cometh (Siraudin et al., 2024) | 99.2 | 96.7 | 0.11 | \n| DeFoG (Qin et al., 2024) | 99.3 | 96.3 | 0.12 | \n| G2PT<sub>small</sub> | 99.0 | 96.7 | 0.06 | \n| G2PT<sub>base</sub> | 99.0 | 96.8 | 0.06 | \n| G2PT<sub>large</sub> | 98.9 | 96.7 | 0.06 |", "caption": "Table 6: Dataset statistics.", "description": "This table provides a comprehensive overview of the datasets used in the experiments.  For each dataset, it lists the number of node types, edge types, the average, minimum, and maximum number of nodes in the graphs, the number of training sequences used for each dataset, and the size of the vocabulary used for tokenization during training.  This information is crucial for understanding the scale and complexity of the datasets and how they were prepared for model training.", "section": "5. Experiments"}, {"content": "| MOSES |  |  | GuacaMol |  |  |\n|---|---|---|---|---|---|---|\n| Train | G2PT<sub>small</sub> | G2PT<sub>base</sub> | Train | G2PT<sub>small</sub> | G2PT<sub>base</sub> |\n| ![](https://arxiv.org/html/2501.01073/images/moses_train_1.png) | ![](https://arxiv.org/html/2501.01073/images/moses_10m_1.png) | ![](https://arxiv.org/html/2501.01073/images/moses_85m_1.png) | ![](https://arxiv.org/html/2501.01073/images/guac_train_1.png) | ![](https://arxiv.org/html/2501.01073/images/guac_10m_1.png) | ![](https://arxiv.org/html/2501.01073/images/guac_85m_1.png) |\n| ![](https://arxiv.org/html/2501.01073/images/moses_train_2.png) | ![](https://arxiv.org/html/2501.01073/images/moses_10m_2.png) | ![](https://arxiv.org/html/2501.01073/images/moses_85m_2.png) | ![](https://arxiv.org/html/2501.01073/images/guac_train_2.png) | ![](https://arxiv.org/html/2501.01073/images/guac_10m_2.png) | ![](https://arxiv.org/html/2501.01073/images/guac_85m_2.png) |", "caption": "Table 7: Hyperparameters for graph generative pre-training.", "description": "This table details the hyperparameters used during the pre-training phase of the Graph Generative Pre-trained Transformer (G2PT) model.  It shows how these settings varied across three different model sizes (10M, 85M, and 300M parameters), impacting aspects like architecture (number of layers, heads, and the model's embedding dimension), optimization (optimizer, learning rate scheduler, weight decay), training process (batch size, gradient accumulation, gradient clipping), and warmup iterations.", "section": "5. Experiments"}, {"content": "| Density | QED Score | SA Score | GSK3\u03b2 Score |\n|---|---|---|---|\n| ![https://arxiv.org/html/2501.01073/x5.png](https://arxiv.org/html/2501.01073/x5.png) |  ![https://arxiv.org/html/2501.01073/x6.png](https://arxiv.org/html/2501.01073/x6.png) | ![https://arxiv.org/html/2501.01073/x7.png](https://arxiv.org/html/2501.01073/x7.png) | (a) Rejection sampling fine-tuning (with self-bootstrap) |\n| ![https://arxiv.org/html/2501.01073/x8.png](https://arxiv.org/html/2501.01073/x8.png) | ![https://arxiv.org/html/2501.01073/x9.png](https://arxiv.org/html/2501.01073/x9.png) | ![https://arxiv.org/html/2501.01073/x10.png](https://arxiv.org/html/2501.01073/x10.png) | (b) Reinforcement learning framework (PPO) |", "caption": "Table 8: Hyperparameters used for PPO training.", "description": "This table lists the hyperparameters used for training the Proximal Policy Optimization (PPO) algorithm in the goal-oriented molecule generation experiments.  It shows hyperparameters specific to three different molecular properties being optimized: Quantitative Evaluation of Druglikeness (QED), Synthetic Accessibility (SA), and Glycogen Synthase Kinase 3 Beta (GSK3\u03b2) activity.  The table includes settings for advantage and reward normalization and clipping, entropy regularization, learning rates for the actor and critic networks, and the number of training iterations.", "section": "4. Fine-tuning"}, {"content": "BBBP|Tox21|ToxCast|SIDER|ClinTox|MUV|HIV|BACE|Avg.\n---|---|---|---|---|---|---|---|---\nAttrMask (Hu et al., 2020)|70.2 \u00b1 0.5|74.2 \u00b1 0.8|62.5 \u00b1 0.4|60.4 \u00b1 0.6|68.6 \u00b1 9.6|73.9 \u00b1 1.3|74.3 \u00b1 1.3|77.2 \u00b1 1.4|70.2\nInfoGraph (Sun et al., 2020)|69.2 \u00b1 0.8|73.0 \u00b1 0.7|62.0 \u00b1 0.3|59.2 \u00b1 0.2|75.1 \u00b1 5.0|74.0 \u00b1 1.5|74.5 \u00b1 1.8|73.9 \u00b1 2.5|70.1\nContextPred (Hu et al., 2020)|71.2 \u00b1 0.9|73.3 \u00b1 0.5|62.8 \u00b1 0.3|59.3 \u00b1 1.4|73.7 \u00b1 4.0|72.5 \u00b1 2.2|75.8 \u00b1 1.1|78.6 \u00b1 1.4|70.9\nGraphCL (You et al., 2021)|67.5 \u00b1 2.5|75.0 \u00b1 0.5|62.8 \u00b1 0.2|60.1 \u00b1 1.3|78.9 \u00b1 4.2|77.1 \u00b1 1.0|75.0 \u00b1 0.4|68.7 \u00b1 7.8|70.6\nGraphMVP (Liu et al., 2022a)|68.5 \u00b1 0.2|74.5 \u00b1 0.0|62.7 \u00b1 0.1|62.3 \u00b1 1.6|79.0 \u00b1 2.5|75.0 \u00b1 1.4|74.8 \u00b1 1.4|76.8 \u00b1 1.1|71.7\nGraphMAE (Hou et al., 2022b)|70.9 \u00b1 0.9|75.0 \u00b1 0.4|64.1 \u00b1 0.1|59.9 \u00b1 0.5|81.5 \u00b1 2.8|76.9 \u00b1 2.6|76.7 \u00b1 0.9|81.4 \u00b1 1.4|73.3\nG2PT<sub>small</sub> (No pre-training)|60.7 \u00b1 0.3|66.4 \u00b1 0.5|57.0 \u00b1 0.3|61.6 \u00b1 0.2|67.8 \u00b1 1.1|45.8 \u00b1 8.5|70.1 \u00b1 7.5|68.8 \u00b1 1.3|62.3\nG2PT<sub>base</sub> (No pre-training)|56.5 \u00b1 0.2|67.4 \u00b1 0.4|57.9 \u00b1 0.1|60.2 \u00b1 2.8|71.0 \u00b1 5.6|60.1 \u00b1 1.3|72.7 \u00b1 1.1|73.4 \u00b1 0.3|64.9\nG2PT<sub>small</sub>|68.5 \u00b1 0.5|74.7 \u00b1 0.2|61.2 \u00b1 0.1|61.7 \u00b1 1.0|82.3 \u00b1 2.2|74.9 \u00b1 0.1|75.7 \u00b1 0.4|81.3 \u00b1 0.5|72.5\nG2PT<sub>base</sub>|71.0 \u00b1 0.4|75.0 \u00b1 0.3|63.0 \u00b1 0.5|61.9 \u00b1 0.2|82.1 \u00b1 1.1|74.5 \u00b1 0.3|76.3 \u00b1 0.4|82.3 \u00b1 1.6|73.3", "caption": "Table 9: Sensitivity analysis on edge orderings.", "description": "This table presents a sensitivity analysis of different edge ordering methods used in the Graph Generative Pre-trained Transformer (G2PT) model. It compares the performance of four different edge ordering approaches: degree-based ordering, Depth-First Search (DFS) ordering, Breadth-First Search (BFS) ordering, and uniform random ordering. The evaluation metrics used include validity, uniqueness, novelty, filtering, Frechet ChemNet Distance (FCD), Scaffold similarity, and structural similarity.  The results demonstrate that degree-based and BFS ordering strategies generally perform better than DFS and uniform ordering strategies, highlighting the impact of edge ordering on the model's performance.", "section": "D.1. Sensitivity Analysis of Edge Orderings"}]