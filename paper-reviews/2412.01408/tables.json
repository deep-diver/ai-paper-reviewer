[{"content": "| Language | Abusive |  | Non-Abusive |  | Total |\n|---|---|---|---|---|---|---|\n|  | Train | Test | Train | Test |  |  |\n| Bengali | 394 | 148 | 428 | 222 |  | 1192 |\n| Bhojpuri | 253 | 122 | 506 | 214 |  | 1095 |\n| Gujarati | 516 | 255 | 301 | 107 |  | 1179 |\n| Haryanvi | 419 | 193 | 399 | 173 |  | 1184 |\n| Hindi | 449 | 186 | 373 | 183 |  | 1191 |\n| Kannada | 530 | 243 | 289 | 126 |  | 1188 |\n| Malayalam | 582 | 257 | 237 | 115 |  | 1191 |\n| Odia | 491 | 209 | 323 | 156 |  | 1179 |\n| Punjabi | 405 | 176 | 413 | 191 |  | 1185 |\n| Tamil | 572 | 267 | 248 | 104 |  | 1191 |\n| **Total** | **4611** | **2056** | **3517** | **1591** |  | **11775** |", "caption": "Table 1: ADIMA Dataset distribution across languages and classes. Train and Test being the ones provided by authors.", "description": "This table shows the distribution of data points in the ADIMA dataset across different Indian languages and the two classes (abusive and non-abusive).  It breaks down the number of training and testing samples for each language in both classes. This provides insights into the class balance and the amount of data available for training and evaluation in each language, which is crucial for understanding the challenges and potential biases in the dataset, especially in the context of low-resource scenarios.", "section": "4.1 Dataset"}, {"content": "| Language | ADIMA | Ours |\n|---|---|---|\n| Bengali | 79.1 | **82.45** |\n| Bhojpuri | - | **81.34** |\n| Gujarati | - | **81.73** |\n| Haryanvi | - | **84.69** |\n| Hindi | 80.7 | **84.26** |\n| Kannada | 78.4 | **78.77** |\n| Malayalam | - | **83.33** |\n| Odia | - | **83.18** |\n| Punjabi | **83.4** | 81.87 |\n| Tamil | 75.2 | **75.88** |", "caption": "Table 2: Few-shot Classification Results for Wav2Vec \nAcc: Accuracy, F1: Macro F1-Score", "description": "This table presents the few-shot classification results achieved using the Wav2Vec model.  It shows the accuracy and macro F1-score for 10 different Indian languages across four different shot sizes (50, 100, 150, and 200). The results are broken down by two feature normalization techniques: Temporal Mean and L2-Norm, providing a comprehensive performance evaluation under various experimental settings.", "section": "4 Experiments"}]