[{"figure_path": "2410.08584/charts/charts_2_0.png", "caption": "Figure 1: The attention maps exhibit distinct sparse patterns across different layers (subfigures (a) and (b)) and vary significantly between tasks (subfigures (b) and (c)). Data was collected from the LLaVA-Next-7B model using input samples from the VQAv2 and ChartQA datasets.", "description": "The chart displays attention maps from different layers of a large vision-language model (LLaVA-Next-7B) across two different tasks (VQAv2 and ChartQA), highlighting their distinct sparse patterns and variations.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.08584/charts/charts_5_0.png", "caption": "Figure 3: The ratio of important tokens distributed across layers. Data was collected from the LLaVA-Next-7B model using input samples from the VQAv2 and ChartQA datasets.", "description": "The chart displays the varying ratios of important tokens across different layers for the VQAv2 and ChartQA datasets, comparing ZipVL's dynamic approach to FastV's fixed ratio.", "section": "4.1 LAYER-WISE ADAPTIVE RATIO ASSIGNMENT FOR IMPORTANT TOKENS"}, {"figure_path": "2410.08584/charts/charts_8_0.png", "caption": "Figure 4: The ratio of important tokens across different methods on different tasks. The proposed ZipVL can adaptively determine this ratio based on the attention scores, assigning more ratio to important tokens on complex tasks.", "description": "The chart displays the ratio of important tokens across different tasks and models, showing that ZipVL dynamically adjusts this ratio based on task complexity.", "section": "5.2.1 EVALUATION ON IMAGE BENCHMARKS"}, {"figure_path": "2410.08584/charts/charts_8_1.png", "caption": "Figure 4: The ratio of important tokens across different methods on different tasks. The proposed ZipVL can adaptively determine this ratio based on the attention scores, assigning more ratio to important tokens on complex tasks.", "description": "The chart shows the ratio of important tokens across different tasks for three methods: ZipVL-0.96, ZipVL-0.975, and FastV, demonstrating ZipVL's adaptive token ratio adjustment based on task complexity.", "section": "5.2.1 EVALUATION ON IMAGE BENCHMARKS"}, {"figure_path": "2410.08584/charts/charts_8_2.png", "caption": "Figure 4: The ratio of important tokens across different methods on different tasks. The proposed ZipVL can adaptively determine this ratio based on the attention scores, assigning more ratio to important tokens on complex tasks.", "description": "The chart shows the ratio of important tokens used by different methods (ZipVL with thresholds 0.96 and 0.975, and FastV) across five image comprehension tasks.", "section": "5.2.1 EVALUATION ON IMAGE BENCHMARKS"}, {"figure_path": "2410.08584/charts/charts_10_0.png", "caption": "Figure 5: The effect of attention scores retention threshold \u03c4 on the ratio of important tokens and the model performance. Data was collected on GQA benchmark over LLaVA-v1.5-7B model.", "description": "The chart shows the relationship between the attention retention threshold (\u03c4) and both the ratio of important tokens and the model's accuracy on the GQA benchmark, revealing an optimal threshold around 0.97.", "section": "5.3.2 EFFECT OF THE THRESHOLD T"}, {"figure_path": "2410.08584/charts/charts_10_1.png", "caption": "Figure 6: Comparisons of prefill phase latency and GPU memory across different sequence lengths. Data is collected from LongVA-7B model.", "description": "The chart compares the prefill phase latency and GPU memory usage of FlashAttention, MInference, and the proposed ZipVL method across various sequence lengths.", "section": "5.4 DEPLOYMENT EFFICIENCY"}]