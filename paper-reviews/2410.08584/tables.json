[{"figure_path": "2410.08584/tables/table_7_0.html", "caption": "Table 1: Performance comparisons of image LVLMs on various benchmarks. Here, \u201cRatio", "description": "Table 1 presents a comparison of the performance of various image-based Large Vision-Language Models (LVLMs) across five different benchmark datasets, showing the impact of different methods on accuracy and the ratio of important tokens used in the computation.", "section": "5.2.1 EVALUATION ON IMAGE BENCHMARKS"}, {"figure_path": "2410.08584/tables/table_8_0.html", "caption": "Table 1: Performance comparisons of image LVLMs on various benchmarks. Here, \u201cRatio\u201d denotes the proportion of tokens participating in attention computation. \u201c\u2020\u201d denotes token-level sparsity is only employed in attention modules.", "description": "Table 1 compares the performance of different image LVLMs on various benchmark datasets using different methods with varying ratios of important tokens.", "section": "5.2.1 EVALUATION ON IMAGE BENCHMARKS"}, {"figure_path": "2410.08584/tables/table_9_0.html", "caption": "Table 2: Performance comparisons of video LVLMs on Video-MME benchmark. Here, \"Attn FLOPs Reduction\" denotes the reduction in floating-point operations (FLOPs) of the attention mechanism. \"+\" denotes token-level sparsity is only employed in attention modules.", "description": "Table 2 compares the performance of different methods on the Video-MME benchmark, showing the reduction in FLOPs of attention mechanisms and overall accuracy for different video lengths.", "section": "5.2.2 EVALUATION ON VIDEO BENCHMARKS"}, {"figure_path": "2410.08584/tables/table_9_1.html", "caption": "Table 1: Performance comparisons of image LVLMs on various benchmarks. Here, \u201cRatio", "description": "Table 1 compares the performance of different image LVLMs on various benchmarks using different methods with varying ratios of important tokens.", "section": "5.2.1 EVALUATION ON IMAGE BENCHMARKS"}]