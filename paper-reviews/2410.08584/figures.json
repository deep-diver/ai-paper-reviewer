[{"figure_path": "2410.08584/figures/figures_3_0.png", "caption": "Figure 2: Overview of the proposed ZipVL framework during the prefill phase. Here, \u03c4 represents the threshold for retaining attention scores, n and p are the total number of tokens and the number of important tokens, respectively. After determining the ratio of important tokens and identifying them, we optimize the prefill phase by exclusively computing attention for important tokens. Additionally, we apply mixed-precision quantization to the KV cache, where the KV cache of less important tokens is quantized to a lower bit-width.", "description": "Figure 2 illustrates the ZipVL framework's prefill phase, showing dynamic ratio allocation for important tokens, token-level sparse attention using FlashAttention, and mixed-precision KV cache compression.", "section": "METHOD"}]