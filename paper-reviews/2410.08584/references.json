{"references": [{" publication_date": "2023", "fullname_first_author": "Josh Achiam", "paper_title": "Gpt-4 technical report", "reason": "This paper is foundational as it presents a comprehensive technical report on GPT-4, a highly influential large language model. Its findings and details regarding model architecture, training, and capabilities are relevant to the discussion and comparison of advanced language models in the field.  The report provides valuable insights into the current state-of-the-art in LLMs, influencing the development of future models and research.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "Iz Beltagy", "paper_title": "Longformer: The long-document transformer", "reason": "This paper introduces the Longformer architecture, a significant advancement in handling long sequences, addressing limitations of traditional transformers. Its relevance lies in its contributions to addressing long-context limitations in LLMs, a critical aspect for efficient handling of high-resolution visual inputs in LVLMs which are the focus of the current paper.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Liang Chen", "paper_title": "An image is worth 1/2 tokens after layer 2: Plug-and-play inference acceleration for large vision-language models", "reason": "This work directly addresses the computational efficiency of vision-language models, focusing on reducing the computational cost of attention mechanisms.  Its relevance to the current study lies in its exploration of techniques to improve efficiency, which aligns with the core goal of ZipVL, which also aims to optimize both computational and memory aspects of the model.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Tri Dao", "paper_title": "Flashattention: Fast and memory-efficient exact attention with io-awareness", "reason": "FlashAttention is a key component in ZipVL, enabling fast and efficient attention computations. Its importance stems from its significant impact on the speed and efficiency of the proposed method, directly contributing to the claimed performance improvements.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Chaoyou Fu", "paper_title": "Mme: A comprehensive evaluation benchmark for multimodal large language models", "reason": "This paper provides a benchmark framework for multimodal language models. It is highly relevant because the current paper uses this benchmark to validate and demonstrate the efficiency gains of ZipVL, showing the practical benefits in real-world applications.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Chaoyou Fu", "paper_title": "Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis", "reason": "This paper extends the MME benchmark to include video understanding, which is a critical aspect for evaluating the performance of LVLMs. Its relevance stems from the fact that the presented work evaluates ZipVL using this specific benchmark.", "section_number": 5}, {" publication_date": "2017", "fullname_first_author": "Yash Goyal", "paper_title": "Making the v in vqa matter: Elevating the role of image understanding in visual question answering", "reason": "This paper is fundamental to the field of Visual Question Answering (VQA). Its citation indicates that the current work's contributions are situated within the broader context of VQA research, providing a valuable benchmark to assess the models' capabilities.", "section_number": 5}, {" publication_date": "2022", "fullname_first_author": "Coleman Hooper", "paper_title": "Kvquant: Towards 10 million context length llm inference with kv cache quantization", "reason": "This paper directly addresses the KV cache compression problem, which is one of the two main bottlenecks that ZipVL aims to improve.  Understanding the techniques and trade-offs presented in KVQuant is vital for situating and evaluating the current work's contributions to this specific area.", "section_number": 2}, {" publication_date": "2019", "fullname_first_author": "Drew A Hudson", "paper_title": "Gqa: A new dataset for real-world visual reasoning and compositional question answering", "reason": "This paper introduces a dataset (GQA) that is used for evaluating the performance of the models in the current work. This dataset is a crucial component in the validation of the claimed improvements in performance and efficiency.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Huiqiang Jiang", "paper_title": "Minference: Accelerating pre-filling for long-context llms via dynamic sparse attention", "reason": "This paper introduces MInference, a method directly compared against in the experimental evaluation of the current work. The comparison highlights the relative strengths and weaknesses of different sparse attention approaches, providing valuable context and justification for the proposed ZipVL framework.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Hao Kang", "paper_title": "Gear: An efficient kv cache compression recipefor near-lossless generative inference of llm", "reason": "This work is highly relevant due to its focus on KV cache compression, a key problem addressed by ZipVL. Comparing the approaches and results provides insight into the relative effectiveness and efficiency of different compression techniques.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Benjamin Lefaudeux", "paper_title": "Xformers: A modular and hackable transformer modelling library", "reason": "This paper is relevant as it concerns a library (xformers) that is used in some of the experiments.  While not directly addressing the core problem, mentioning its usage shows the practical relevance of the methods employed in the study.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Bin Lin", "paper_title": "Video-llava: Learning united visual representation by alignment before projection", "reason": "This paper is a relevant contribution to the field of video understanding, which is one of the primary application domains of ZipVL. By introducing Video-LLaVA, the authors have provided a benchmark to compare the performance and efficiency improvements in a challenging and relevant application scenario.", "section_number": 5}, {" publication_date": "2022", "fullname_first_author": "Ahmed Masry", "paper_title": "Chartqa: A benchmark for question answering about charts with visual and logical reasoning", "reason": "This paper is significant as it presents the ChartQA benchmark which is used to evaluate the performance of the models. Using ChartQA as a benchmark further demonstrates the efficacy of ZipVL on challenging, complex tasks.  The inclusion of this specific benchmark strengthens the overall argument.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Matteo Pagliardini", "paper_title": "Fast attention over long sequences with dynamic sparse flash attention", "reason": "This paper introduces a fast attention mechanism that is relevant to the efficient implementation of the proposed method. Comparing and contrasting approaches helps in understanding the trade-offs and justifying the specific choices made in ZipVL.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "reason": "This paper is foundational as it presents CLIP, a model highly relevant to the development of LVLMs.  Understanding CLIP's architecture and contributions provides valuable background and context for comprehending and analyzing the LVLMs studied in the current work.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Siyu Ren", "paper_title": "On the efficacy of eviction policy for key-value constrained generative language model inference", "reason": "This paper discusses key-value (KV) cache management, a crucial aspect in efficient LLM inference that is directly addressed by ZipVL. Examining this paper provides valuable context for comparing the specific strategies and improvements.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Luka Ribar", "paper_title": "Sparq attention: Bandwidth-efficient Ilm inference", "reason": "This paper focuses on improving the bandwidth efficiency of LLMs which is also a significant concern in processing large visual data.  Comparing this with ZipVL helps understand the tradeoffs and relative advantages of different approaches to improve inference speed.", "section_number": 2}, {" publication_date": "2019", "fullname_first_author": "Amanpreet Singh", "paper_title": "Towards vqa models that can read", "reason": "This paper is relevant as it deals with VQA, a significant application domain for LVLMs. Referencing this work demonstrates that the presented research is within the broader context of VQA, which is a critical aspect for justifying and understanding the significance of improving efficiency in LVLMs.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Zhongwei Wan", "paper_title": "Look-m: Look-once optimization in kv cache for efficient multimodal long-context inference", "reason": "This paper directly addresses the efficiency of KV cache in multimodal models which is precisely what ZipVL aims to improve. Comparing the techniques discussed here with those used in ZipVL helps to understand the unique aspects and potential benefits of the proposed approach.", "section_number": 2}]}