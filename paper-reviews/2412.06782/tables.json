[{"content": "| Policy | Lift | Can | Square | Params/M | Speed/s |\n|---|---|---|---|---|---| \n| BET [52] | 0.96 | 0.88 | 0.54 | **0.27** | **2.12** |\n| DP-C [11] | **1.00** | 0.94 | **0.94** | 65.88 | 35.21 |\n| DP-T [11] | **1.00** | **1.00** | 0.88 | 8.97 | 37.83 |\n| CARP (Ours) | **1.00** | **1.00** | **0.98** | **0.65** | **3.07** |", "caption": "Table 1: \nState-Based Simulation Results (State Policy).\nWe report the average success rate of the top 3 checkpoints, along with model parameter scales and inference time for generating 400 actions. CARP significantly outperforms BET and achieves competitive performance with state-of-the-art diffusion models, while also surpassing DP in terms of model size and inference speed.", "description": "This table presents a comparison of the performance of different robotic control policies on state-based simulation tasks.  The policies are evaluated on three tasks of increasing difficulty: Lift, Can, and Square. The metrics reported include the average success rate (across the top three performing checkpoints), the model size (in millions of parameters), and the inference speed (in actions per second) required to generate 400 actions. The results show that CARP outperforms BET and achieves comparable performance to state-of-the-art diffusion-based policies (DP), while being significantly smaller and faster.", "section": "4.1 Evaluation on Simulation Benchmark"}, {"content": "| Policy | Lift | Can | Square | Params/M | Speed/s |\n|---|---|---|---|---|---| \n| IBC [15] | 0.72 | 0.02 | 0.00 | **3.44** | 32.35 |\n| DP-C [11] | **1.00** | 0.97 | **0.92** | 255.61 | 47.37 |\n| DP-T [11] | **1.00** | **0.98** | 0.86 | 9.01 | 45.12 |\n| CARP (Ours) | **1.00** | **0.98** | **0.88** | **7.58** | **4.83** |", "caption": "Table 2: \nImage-Based Simulation Results (Visual Policy).\nResults show that CARP consistently balances high performance and high efficiency. We highlight our results in light-blue.", "description": "This table presents the results of image-based visuomotor policy learning experiments on the Robomimic benchmark.  It compares the performance of CARP against other state-of-the-art methods (IBC, DP-C, DP-T) across three tasks: Lift, Can, and Square, ordered by increasing difficulty. The metrics reported include the success rate (percentage of successful task completions) for each task, model size (in millions of parameters), and inference speed (in Hz). The results show that CARP achieves high success rates while maintaining computational efficiency, outperforming or matching the other methods.", "section": "4.1 Evaluation on Simulation Benchmark"}, {"content": "| Policy | p1 | p2 | p3 | p4 | Params | Speed |\n|---|---|---|---|---|---|---|\n| BET [52] | 0.96 | 0.84 | 0.6 | 0.20 | **0.30** | **1.95** |\n| DP-C [11] | **1.00** | **1.00** | **1.00** | 0.96 | 66.94 | 56.14 |\n| DP-T [11] | **1.00** | 0.99 | 0.98 | 0.96 | 80.42 | 56.32 |\n| CARP (Ours) | **1.00** | **1.00** | 0.98 | **0.98** | **3.88** | **2.01** |", "caption": "Table 3: \nMulti-Stage Task Results (State Policy).\nIn the Kitchen, px\ud835\udc65xitalic_x represents the frequency of interactions with x\ud835\udc65xitalic_x or more objects.\nCARP outperforms BET, especially on challenging metrics like p4, and achieves competitive performance compared to DP, with fewer parameters and faster inference speed.", "description": "This table presents a comparison of different robot control policies on a multi-stage task, specifically the Franka Kitchen task.  The policies compared are Behavior Transformer (BET), Diffusion Policy with CNN (DP-C), Diffusion Policy with Transformer (DP-T), and the proposed CARP method.  The table shows the success rate (p1 to p4 indicating interaction with 1, 2, 3, or 4 or more objects respectively) for each policy, along with the number of parameters and inference speed in Hz. CARP demonstrates superior performance to BET, especially in more complex scenarios (p4), and matches DP's accuracy while using fewer parameters and achieving faster inference speed.", "section": "4.1 Evaluation on Simulation Benchmark"}, {"content": "| Policy | Prams/M | Speed/s | Coffee | Hammer | Mug | Nut | Square | Stack | Stack three | Threading | Avg. |\n|---|---|---|---|---|---|---|---|---|---|---|---| \n| TCD [34] | 156.11 | 107.15 | 0.77 | 0.92 | 0.53 | 0.44 | 0.63 | 0.95 | 0.62 | 0.56 | 0.68 |\n| SDP [61] | 159.85 | 112.39 | 0.82 | **1.00** | 0.62 | 0.54 | 0.82 | 0.96 | 0.80 | **0.70** | 0.78 |\n| CARP (Ours) | **16.08** | **6.92** | **0.86** | 0.98 | **0.74** | **0.78** | **0.90** | **1.00** | **0.82** | **0.70** | **0.85** |", "caption": "Table 4: \nMulti-Task Simulation Results (Visual Policy).\nSuccess rates are averaged across the top three checkpoints for each task, as well as the overall average across all tasks. We also report parameter count and inference time for generating 400 actions. CARP outperforms diffusion-based policies by 9%-25% in average performance, with significantly fewer parameters and over 10\u00d7 faster inference.", "description": "This table presents a comparison of the performance of different models on multiple robotic manipulation tasks using visual input.  The metrics include success rates (averaged across the top 3 model checkpoints for each task and overall), the number of model parameters, and the inference speed (time to generate 400 action predictions).  CARP demonstrates significantly improved performance (9-25% higher success rates) and efficiency (over 10x faster inference and substantially fewer parameters) compared to the baseline diffusion-based models.", "section": "4.2. Evaluation on Multi-Task Benchmark"}]