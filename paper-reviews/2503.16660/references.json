{"references": [{"fullname_first_author": "Alexey Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2021-01-01", "reason": "This paper introduces Vision Transformers (ViT), which are fundamental to many modern vision encoders discussed in the paper."}, {"fullname_first_author": "Junnan Li", "paper_title": "BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models", "publication_date": "2023-01-01", "reason": "This paper is relevant because it describes a method using learnable queries to extract important visual information, a key aspect of vision context reduction discussed in the paper."}, {"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-01-01", "reason": "CLIP, introduced in this paper, is used as a visual encoder in one of the multimodal models (LLaVA-NeXT) that is evaluated in the paper."}, {"fullname_first_author": "Tsung-Yi Lin", "paper_title": "Microsoft coco: Common objects in context", "publication_date": "2015-01-01", "reason": "COCO is used to pre-train the models used as benchmarks."}, {"fullname_first_author": "Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-01-01", "reason": "This provides the theoretical background for the transformer networks."}]}