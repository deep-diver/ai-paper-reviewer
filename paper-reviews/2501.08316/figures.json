[{"figure_path": "https://arxiv.org/html/2501.08316/x1.png", "caption": "Figure 1: Architecture overview. Both the generator and the discriminator backbone share the diffusion transformer architecture (blue). We add additional output heads on the discriminator network to produce the scalar logit (green).", "description": "This figure presents a high-level overview of the architecture used in the proposed Adversarial Post-Training (APT) model for one-step video generation.  It illustrates that both the generator and the discriminator share the same fundamental backbone architecture, a diffusion transformer, represented in blue. The key difference lies in their output heads. The generator produces the final video output, while the discriminator, which evaluates whether the generated video is real or fake, uses additional output heads (shown in green) to generate a scalar logit, representing the discriminator's confidence score. This design is central to the adversarial training process, where the generator attempts to produce realistic videos that fool the discriminator, and the discriminator learns to distinguish between real and fake videos.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2501.08316/extracted/6130580/img/ablation/r1/loss2.png", "caption": "Figure 2: Image generation comparison between the original diffusion 25-step model and adversarial post-trained (APT) 1-step model. The diffusion model with classifier-free guidance can generate over-exposed images that look unnatural. APT improves visual fidelity.", "description": "This figure compares image generation results between a standard diffusion model (25 steps with classifier-free guidance) and the proposed Adversarial Post-Training (APT) model (1 step).  The left side shows images generated by the standard diffusion model, highlighting how classifier-free guidance can lead to overexposed and unnatural-looking images. The right side presents images generated by the APT model.  The comparison demonstrates that the APT model significantly improves the visual fidelity and realism of the generated images, addressing the issues of overexposure and unnatural appearance present in the standard diffusion model's output.", "section": "4.1. Qualitative Evaluation"}, {"figure_path": "https://arxiv.org/html/2501.08316/extracted/6130580/img/showreel/0000.jpg", "caption": "(a) A frustrated child.", "description": "A photo of a child displaying a frustrated expression. The image is used to demonstrate the quality of one-step image generation by comparing it against the results from a 25-step diffusion model, highlighting the improvement in image details and realism achieved with the proposed Adversarial Post-Training method.", "section": "4.1 Qualitative Evaluation"}, {"figure_path": "https://arxiv.org/html/2501.08316/extracted/6130580/img/showreel/0001.jpg", "caption": "(b) The city of London.", "description": "A comparison of different one-step image generation methods and the original diffusion model with 25 steps.  The image shows a depiction of the city of London, generated using a variety of methods. It illustrates the differences in visual fidelity, structural integrity and detail between a single-step generation approach and a 25-step approach. This highlights the challenges and progress made in accelerating the diffusion model generation process.", "section": "4.1 Qualitative Evaluation"}, {"figure_path": "https://arxiv.org/html/2501.08316/extracted/6130580/img/showreel/0002.jpg", "caption": "(c) A close-up of the eyes of an owl.", "description": "A detailed close-up of an owl's eyes, showcasing the intricate details of their irises, pupils, and surrounding feathers.  The image likely highlights the texture and color variations in the owl's eyes, possibly demonstrating the model's ability to render fine details and realistic textures.", "section": "4.1 Qualitative Evaluation"}, {"figure_path": "https://arxiv.org/html/2501.08316/extracted/6130580/img/showreel/0003.jpg", "caption": "(d) A tree growing through a fence.", "description": "An image generated by different methods showing a tree growing through a wooden fence.  The image demonstrates the ability of various one-step image generation models to capture details and structural integrity.  Comparing the one-step methods against the original diffusion model highlights strengths and weaknesses in generating photorealistic images.", "section": "4.1 Qualitative Evaluation"}, {"figure_path": "https://arxiv.org/html/2501.08316/extracted/6130580/img/showreel/0004.jpg", "caption": "Figure 3: Image generation comparison across methods and models. We show results of 1-step generation and the corresponding diffusion model 25-step generation. Our method is significantly better in image details and is among the best in structural integrity.", "description": "This figure compares the image generation quality of different models, focusing on one-step generation methods versus a standard 25-step diffusion model.  It showcases the results for four different prompts: 'a frustrated child,' 'the city of London,' 'a close-up of the eyes of an owl,' and 'a tree growing through a fence.' For each prompt, the figure presents a comparison between outputs from various one-step generation methods (FLUX, SD3.5-Turbo, SDXL-DMD2, SDXL-Hyper, SDXL-Lightning, SDXL-Nitro) and the authors' Adversarial Post-Training (APT) method, alongside the output of a 25-step diffusion model. The goal is to illustrate how well the one-step methods achieve the quality of the multi-step diffusion model, especially concerning image details and structural integrity. The APT method is highlighted as achieving significantly better image detail and competitive structural integrity compared to the other one-step approaches.", "section": "4.1 Qualitative Evaluation"}, {"figure_path": "https://arxiv.org/html/2501.08316/extracted/6130580/img/showreel/0005.jpg", "caption": "(a) Good case: Adversarial post-training enhances details and realism. A Western princess, with sunlight shining through the leaves on her face, facial close-up.", "description": "The image shows a video generated by the Adversarial Post-Training (APT) model.  The video depicts a close-up of a Western princess's face, with sunlight filtering through leaves.  The APT model's post-processing improved the realism and detail of the video compared to a standard diffusion model. This example highlights the positive impact of APT on enhancing visual quality in video generation.", "section": "4. Experimental Results"}, {"figure_path": "https://arxiv.org/html/2501.08316/extracted/6130580/img/showreel/0006.jpg", "caption": "(b) Good case: Adversarial post-training produces more realistic and cinematic results, whereas the diffusion results look synthetic. Wong Kar-wai style, on the streets of Shanghai, back shot of a woman walking in a cheongsam, nostalgic sepia tone, brightly saturated colors.", "description": "The figure shows a comparison of video generation results between the original diffusion model and the proposed adversarial post-training (APT) method.  The prompt is a description evocative of Wong Kar-wai's film style: a scene set in Shanghai, featuring a woman in a cheongsam walking down the street, captured from behind. The APT method generated a video that is more realistic and cinematic, while the original diffusion model's video appears more synthetic.", "section": "4. Experimental Results"}, {"figure_path": "https://arxiv.org/html/2501.08316/extracted/6130580/img/showreel/0007.jpg", "caption": "(c) Average case: Adversarial post-training can produce the scene but with degradation in structure and text alignment. First-person perspective, the camera passes through a classroom entering the school playground.", "description": "The figure shows a comparison of video generation results using three different methods: a 25-step diffusion model, a 2-step adversarially post-trained model, and a 1-step adversarially post-trained model.  The scene depicts a first-person perspective, where the camera moves through a classroom and exits into a school playground. The 25-step diffusion model shows a complete and detailed scene, while the 2-step and 1-step models show degradation in the structural integrity of the scene and do not completely match the prompt's description.  The 1-step model shows the most significant reduction in quality.", "section": "4. Experimental Results"}, {"figure_path": "https://arxiv.org/html/2501.08316/extracted/6130580/img/showreel/0008.jpg", "caption": "(d) Failure case: Adversarial post-training can fail at some prompts. A terracotta warrior holds a white paper in one hand, and the paper flutters in the wind. The background is a museum.", "description": "The adversarial post-training method, while generally successful, sometimes fails to generate realistic images.  This example shows a prompt requesting a terracotta warrior holding a piece of paper in the wind, set within a museum.  The generated image shows the figure and some background detail, but the paper is not realistically rendered and the scene lacks overall coherence or detail.", "section": "4.1 Qualitative Evaluation"}, {"figure_path": "https://arxiv.org/html/2501.08316/extracted/6130580/img/showreel/0009.jpg", "caption": "Figure 4: Video generation results. Adversarial post-training can improve visual fidelity, i.e. details and realism, but few-step generation still has degradation in structure and text alignment.", "description": "Figure 4 presents a comparison of video generation results from three different approaches: a standard diffusion model with 25 steps, an adversarially post-trained model with 2 steps, and the same post-trained model using only 1 step.  Each row shows a different video prompt, and the images depict frames from the generated videos. The results demonstrate that the adversarially trained model, even with only a single step, produces videos with improved visual fidelity (details and realism) when compared to the 25-step diffusion model. However, using fewer steps (either 2 or 1) leads to a decline in the structural integrity and text alignment of the generated videos.  The comparison highlights the trade-off between generating videos quickly (fewer steps) and maintaining high-quality results.", "section": "4. Experimental Results"}, {"figure_path": "https://arxiv.org/html/2501.08316/extracted/6130580/img/showreel/0010.jpg", "caption": "Figure 5: Without approximated R1 regularization, the discriminator loss reaches zero (grey) and the training collapses. With approximated R1 regularization, the discriminator loss does not reach zero (green).", "description": "This figure shows the effect of using the approximated R1 regularization during adversarial training.  The grey line represents the discriminator loss without R1 regularization.  As shown, the loss rapidly approaches zero, indicating training instability and collapse (mode collapse, where the generator produces unrealistic artifacts). The green line shows the discriminator loss with the addition of approximated R1 regularization. In this case, the loss remains stable and does not approach zero, showing the effectiveness of R1 regularization in stabilizing the training process and preventing the generator from collapsing.", "section": "5.1. The Effect of Approximated R1 Regularization"}, {"figure_path": "https://arxiv.org/html/2501.08316/extracted/6130580/img/showreel/0011.jpg", "caption": "Figure 6: Using a deeper discriminator that includes the full depth of the pre-trained network leads to better generation quality.", "description": "This figure compares the performance of discriminators with varying depths when used in adversarial training for image generation.  A shallower discriminator, using only a portion of the pre-trained network's layers, is compared to a discriminator employing the full depth of the pre-trained network. The results demonstrate that using the full depth of the pre-trained network in the discriminator leads to better image generation quality.", "section": "3.3. Discriminator"}]