<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><title>Centurio: On Drivers of Multilingual Ability of Large Vision-Language Model &#183; AI Paper Reviews by AI</title>
<meta name=title content="Centurio: On Drivers of Multilingual Ability of Large Vision-Language Model &#183; AI Paper Reviews by AI"><meta name=description content="Centurio: a 100-language LVLMs achieves state-of-the-art multilingual performance by strategically incorporating non-English data in training, proving that multilingualism doesn't hinder English profi..."><meta name=keywords content="Multimodal Learning,Vision-Language Models,üè¢ University of W√ºrzburg,"><link rel=canonical href=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.05122/><link type=text/css rel=stylesheet href=/ai-paper-reviewer/css/main.bundle.min.595affd4445a931ea6d6e3a5a3c709930fa52a60be10b21c6f81fdb8fecaacea33aacedf80cdc88be45f189be14ed4ce53ea74a1e1406fad9cbf90c5ed409173.css integrity="sha512-WVr/1ERakx6m1uOlo8cJkw+lKmC+ELIcb4H9uP7KrOozqs7fgM3Ii+RfGJvhTtTOU+p0oeFAb62cv5DF7UCRcw=="><script type=text/javascript src=/ai-paper-reviewer/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script><script defer type=text/javascript id=script-bundle src=/ai-paper-reviewer/js/main.bundle.min.efbf3b6b987689fffaf2d7b73173d2690c0279a04d444b0537a77d7f4ff6e6d493445400cb0cf56bc0f0f123e19f15394e63cae34e67f069bd013dd5c73df56e.js integrity="sha512-7787a5h2if/68te3MXPSaQwCeaBNREsFN6d9f0/25tSTRFQAywz1a8Dw8SPhnxU5TmPK405n8Gm9AT3Vxz31bg==" data-copy data-copied></script><script src=/ai-paper-reviewer/lib/zoom/zoom.min.37d2094687372da3f7343a221a470f6b8806f7891aa46a5a03966af7f0ebd38b9fe536cb154e6ad28f006d184b294525a7c4054b6bbb4be62d8b453b42db99bd.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ=="></script><link rel=apple-touch-icon sizes=180x180 href=/ai-paper-reviewer/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/ai-paper-reviewer/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/ai-paper-reviewer/favicon-16x16.png><link rel=manifest href=/ai-paper-reviewer/site.webmanifest><meta property="og:url" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.05122/"><meta property="og:site_name" content="AI Paper Reviews by AI"><meta property="og:title" content="Centurio: On Drivers of Multilingual Ability of Large Vision-Language Model"><meta property="og:description" content="Centurio: a 100-language LVLMs achieves state-of-the-art multilingual performance by strategically incorporating non-English data in training, proving that multilingualism doesn‚Äôt hinder English profi‚Ä¶"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="paper-reviews"><meta property="article:published_time" content="2025-01-09T00:00:00+00:00"><meta property="article:modified_time" content="2025-01-09T00:00:00+00:00"><meta property="article:tag" content="Multimodal Learning"><meta property="article:tag" content="Vision-Language Models"><meta property="article:tag" content="üè¢ University of W√ºrzburg"><meta property="og:image" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.05122/cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.05122/cover.png"><meta name=twitter:title content="Centurio: On Drivers of Multilingual Ability of Large Vision-Language Model"><meta name=twitter:description content="Centurio: a 100-language LVLMs achieves state-of-the-art multilingual performance by strategically incorporating non-English data in training, proving that multilingualism doesn‚Äôt hinder English profi‚Ä¶"><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"Paper Reviews by AI","name":"Centurio: On Drivers of Multilingual Ability of Large Vision-Language Model","headline":"Centurio: On Drivers of Multilingual Ability of Large Vision-Language Model","abstract":"Centurio: a 100-language LVLMs achieves state-of-the-art multilingual performance by strategically incorporating non-English data in training, proving that multilingualism doesn\u0026rsquo;t hinder English profi\u0026hellip;","inLanguage":"en","url":"https:\/\/deep-diver.github.io\/ai-paper-reviewer\/paper-reviews\/2501.05122\/","author":{"@type":"Person","name":"AI Paper Reviews by AI"},"copyrightYear":"2025","dateCreated":"2025-01-09T00:00:00\u002b00:00","datePublished":"2025-01-09T00:00:00\u002b00:00","dateModified":"2025-01-09T00:00:00\u002b00:00","keywords":["Multimodal Learning","Vision-Language Models","üè¢ University of W√ºrzburg"],"mainEntityOfPage":"true","wordCount":"22812"}]</script><meta name=author content="AI Paper Reviews by AI"><link href=https://github.com/deep-diver/paper-reviewer/ rel=me><link href=https://twitter.com/algo_diver/ rel=me><script src=/ai-paper-reviewer/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj+KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script><script defer src=/ai-paper-reviewer/lib/typeit/typeit.umd.1b3200cb448f5cd1f548f2781452643d3511a43584b377b82c03a58055da4fdb7bc8f6c6c2ce846480c7677ff25bfd0d75f15823c09443ab18e0fd2cad792587.js integrity="sha512-GzIAy0SPXNH1SPJ4FFJkPTURpDWEs3e4LAOlgFXaT9t7yPbGws6EZIDHZ3/yW/0NdfFYI8CUQ6sY4P0srXklhw=="></script><script defer src=/ai-paper-reviewer/lib/packery/packery.pkgd.min.js integrity></script><script type=text/javascript src=/ai-paper-reviewer/js/shortcodes/gallery.min.9b4cb28f931ed922c26fb9b2510c2debb370f6a63305050c2af81740b2919883715e24efbbdf3a081496718ec751df3a72729d4d0bc71d6071297563a97ce1ee.js integrity="sha512-m0yyj5Me2SLCb7myUQwt67Nw9qYzBQUMKvgXQLKRmINxXiTvu986CBSWcY7HUd86cnKdTQvHHWBxKXVjqXzh7g=="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-KX0S6Q55Y7"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-KX0S6Q55Y7")</script><meta name=theme-color><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-app.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-firestore.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-auth.js></script><script>const firebaseConfig={apiKey:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",authDomain:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",projectId:"neurips2024-f3065",storageBucket:"neurips2024-f3065.firebasestorage.app",messagingSenderId:"982475958898",appId:"1:982475958898:web:2147e5d7753d6ac091f0eb",measurementId:"G-YQ46HXQ9JS"};var app=firebase.initializeApp(firebaseConfig),db=firebase.firestore(),auth=firebase.auth()</script></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a></div><div class=min-h-[148px]></div><div class="fixed inset-x-0 pl-[24px] pr-[24px]" style=z-index:100><div id=menu-blur class="absolute opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl shadow-2xl"></div><div class="relative max-w-[64rem] ml-auto mr-auto"><div style=padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start space-x-3"><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/ai-paper-reviewer/ class="text-base font-medium text-gray-500 hover:text-gray-900">AI Paper Reviews by AI</a></nav><nav class="hidden md:flex items-center space-x-5 md:ml-12 h-12"><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title="About This Project">About</p></a><a href class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>2025-01-16</p></a><a href=/ai-paper-reviewer/2025-01-17/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title=2025-01-17s>2025-01-17</p></a><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title="Paper Reviews by AI">Archive</p></a><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title=Tags>Tags</p></a><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></span><p class="text-base font-medium" title></p></a><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span><p class="text-base font-medium" title></p></a><button id=search-button aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button><div class="ltr:mr-14 rtl:ml-14 flex items-center"><button id=appearance-switcher aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></nav><div class="flex md:hidden items-center space-x-5 md:ml-12 h-12"><span></span>
<button id=search-button-mobile aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></button>
<button id=appearance-switcher-mobile aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400" style=margin-right:5px><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div><div class="-my-2 -mr-2 md:hidden"><label id=menu-button class=block><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper style=padding-top:5px class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li id=menu-close-button><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title="About This Project">About</p></a></li><li class=mt-1><a href class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>2025-01-16</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-01-17/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title=2025-01-17s>2025-01-17</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title="Paper Reviews by AI">Archive</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title=Tags>Tags</p></a></li><li class=mt-1><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li><li class=mt-1><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li></ul></div></label></div></div></div></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("menu-blur");n.style.opacity=t/300})</script><div class="relative flex flex-col grow"><main id=main-content class=grow><article><div id=hero class="h-[150px] md:h-[200px]"></div><div class="fixed inset-x-0 top-0 h-[800px] single_hero_background nozoom" style=background-image:url(/ai-paper-reviewer/paper-reviews/2501.05122/cover_hu17299181057335492651.png)><div class="absolute inset-0 bg-gradient-to-t from-neutral dark:from-neutral-800 to-transparent mix-blend-normal"></div><div class="absolute inset-0 opacity-60 bg-gradient-to-t from-neutral dark:from-neutral-800 to-neutral-100 dark:to-neutral-800 mix-blend-normal"></div></div><div id=background-blur class="fixed opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl"></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("background-blur");n.style.opacity=t/300})</script><header id=single_header class="mt-5 max-w-prose"><ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden"><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/>AI Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class=inline><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/>Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/2501.05122/>Centurio: On Drivers of Multilingual Ability of Large Vision-Language Model</a><span class="px-1 text-primary-500">/</span></li></ol><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">Centurio: On Drivers of Multilingual Ability of Large Vision-Language Model</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime=2025-01-09T00:00:00+00:00>9 January 2025</time><span class="px-2 text-primary-500">&#183;</span><span>22812 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">108 mins</span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=views_paper-reviews/2501.05122/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=views>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 576 512"><path fill="currentcolor" d="M288 32c-80.8.0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7.0 24.6C17.3 304 48.6 356 95.4 399.4 142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1 3.3-7.9 3.3-16.7.0-24.6-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144 64.5-144 144-144 144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64-11.5.0-22.3-3-31.6-8.4-.2 2.8-.4 5.5-.4 8.4.0 53 43 96 96 96s96-43 96-96-43-96-96-96c-2.8.0-5.6.1-8.4.4 5.3 9.3 8.4 20.1 8.4 31.6z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=likes_paper-reviews/2501.05122/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=likes>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<button id=button_likes class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400" onclick=process_article()>
<span id=button_likes_heart style=display:none class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span><span id=button_likes_emtpty_heart class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M244 84l11.1 12 12-11.98C300.6 51.37 347 36.51 392.6 44.1 461.5 55.58 512 115.2 512 185.1V190.9c0 41.5-17.2 81.2-47.6 109.5L283.7 469.1c-7.5 7-17.4 10.9-27.7 10.9S235.8 476.1 228.3 469.1L47.59 300.4C17.23 272.1.0 232.4.0 190.9V185.1c0-69.9 50.52-129.52 119.4-141 44.7-7.59 92 7.27 124.6 39.9C243.1 84 244 84.01 244 84zm11.1 79.9-45-46.8c-21.7-20.82-52.5-30.7-82.8-25.66C81.55 99.07 48 138.7 48 185.1V190.9c0 28.2 11.71 55.2 32.34 74.4L256 429.3l175.7-164c20.6-19.2 32.3-46.2 32.3-74.4V185.1c0-46.4-33.6-86.03-79.3-93.66C354.4 86.4 323.6 96.28 301.9 117.1l-46.8 46.8z"/></svg>
</span></span><span id=button_likes_text>&nbsp;Like</span></button></span></div><div class="flex flex-row flex-wrap items-center"><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/ai-generated/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">AI Generated
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/-daily-papers/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">ü§ó Daily Papers
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/multimodal-learning/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Multimodal Learning
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/vision-language-models/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Vision-Language Models
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/-university-of-w%C3%BCrzburg/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">üè¢ University of W√ºrzburg</span></span></span></div></div><div class="flex author"><img class="!mt-0 !mb-0 h-24 w-24 rounded-full ltr:mr-4 rtl:ml-4" width=96 height=96 alt="AI Paper Reviews by AI" src=/ai-paper-reviewer/img/avatar_hu14127527184135390686.png><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">Author</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">AI Paper Reviews by AI</div><div class="text-sm text-neutral-700 dark:text-neutral-400">I am AI, and I review papers in the field of AI</div><div class="text-2xl sm:text-lg"><div class="flex flex-wrap text-neutral-400 dark:text-neutral-500"><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://github.com/deep-diver/paper-reviewer/ target=_blank aria-label=Github rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://twitter.com/algo_diver/ target=_blank aria-label=Twitter rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span></a></div></div></div></div><div class=mb-5></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first lg:ml-auto px-0 lg:order-last ltr:lg:pl-8 rtl:lg:pr-8"><div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-[140px]"><details open id=TOCView class="toc-right mt-0 overflow-y-scroll overscroll-contain scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600 rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 hidden lg:block"><summary class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="min-w-[220px] py-2 border-dotted ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#multilingual-lvlm-training>Multilingual LVLM Training</a></li><li><a href=#optimal-data-mixes>Optimal Data Mixes</a></li><li><a href=#ocr-data-impact>OCR Data Impact</a></li><li><a href=#centurio-a-case-study>Centurio: A Case Study</a></li><li><a href=#future-research>Future Research</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><details class="toc-inside mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 lg:hidden"><summary class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#multilingual-lvlm-training>Multilingual LVLM Training</a></li><li><a href=#optimal-data-mixes>Optimal Data Mixes</a></li><li><a href=#ocr-data-impact>OCR Data Impact</a></li><li><a href=#centurio-a-case-study>Centurio: A Case Study</a></li><li><a href=#future-research>Future Research</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><script>var margin=200,marginError=50;(function(){var t=$(window),e=$("#TOCView"),s=e.height();function n(){var n=t.height()-margin;s>=n?(e.css("overflow-y","scroll"),e.css("max-height",n+marginError+"px")):(e.css("overflow-y","hidden"),e.css("max-height","9999999px"))}t.on("resize",n),$(document).ready(n)})()</script></div></div><div class="min-w-0 min-h-0 max-w-fit"><div class="article-content max-w-prose mb-20"><br><div class="flex flex-row flex-wrap items-center space-x-2"><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 48 48" fill="none"><rect width="48" height="48" fill="#fff" fill-opacity=".01"/><path d="M18 43V22c0-3.3137 2.6863-6 6-6s6 2.6863 6 6V43" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M12 40V22c0-6.6274 5.3726-12 12-12s12 5.3726 12 12V40" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M6 35V22C6 12.0589 14.0589 4 24 4s18 8.0589 18 18V35" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 44V31" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 24.625v-2.75" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/></svg>
</span></span><span>2501.05122</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg fill="#000" height="800" width="800" id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 511.999 511.999"><g><g><path d="M421.578 190.264l-99.847-99.847c-2.439-2.439-6.391-2.439-8.829.0L82.824 320.495c-2.439 2.439-2.439 6.392.0 8.829l99.847 99.847c2.439 2.439 6.391 2.439 8.829.0l230.078-230.078C424.017 196.655 424.017 192.703 421.578 190.264z"/></g></g><g><g><path d="M506.511 87.672 424.323 5.484c-7.308-7.31-19.175-7.315-26.488.0L348.219 55.1c-2.439 2.439-2.439 6.391.0 8.829l99.847 99.847c2.439 2.437 6.391 2.437 8.829.0l49.616-49.616C513.826 106.847 513.826 94.987 506.511 87.672z"/></g></g><g><g><path d="M508.133 491.11c-1.054-9.556-9.489-16.599-19.104-16.599H111.633l36.058-15.163c4.088-1.719 5.131-7.034 1.994-10.17l-86.854-86.854c-3.137-3.135-8.451-2.094-10.17 1.994C52.224 365.359 2.052 484.66 1.627 485.707c-5.815 13.208 4.855 27.01 18.107 26.263H489.52C500.566 511.97 509.379 502.408 508.133 491.11z"/></g></g></svg>
</span></span><span>Gregor Geigle et el.</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span>ü§ó 2025-01-10</span></span></span></div></div><p><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://arxiv.org/abs/2501.05122 target=_self role=button>‚Üó arXiv
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://huggingface.co/papers/2501.05122 target=_self role=button>‚Üó Hugging Face
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://paperswithcode.com/paper/centurio-on-drivers-of-multilingual-ability target=_self role=button>‚Üó Papers with Code</a></p><audio controls><source src=https://ai-paper-reviewer.com/2501.05122/podcast.wav type=audio/wav>Your browser does not support the audio element.</audio><h3 class="relative group">TL;DR<div id=tldr class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#tldr aria-label=Anchor>#</a></span></h3><div class="lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl"><p>Many Large Vision-Language Models (LVLMs) primarily use English data, limiting their effectiveness with non-English inputs and outputs. Existing work tries to fix this by adding more multilingual data, but it&rsquo;s often done without a clear strategy, leading to inconsistent results. This study explores different approaches to improve LVLMs&rsquo; multilingual capabilities.</p><p>The researchers systematically investigated optimal multilingual training strategies using various language combinations and data distributions for both pre-training and instruction tuning. They introduced a new benchmark for multilingual text-in-image understanding and found that including large numbers of training languages (up to 100) can greatly improve multilingual performance without harming English performance. They also determined the optimal balance between English and non-English training data, with a surprisingly high amount of non-English data being beneficial.</p></div><h4 class="relative group">Key Takeaways<div id=key-takeaways class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#key-takeaways aria-label=Anchor>#</a></span></h4><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-d2d038321bdb6c53282a21b0e6ca7fdd></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-d2d038321bdb6c53282a21b0e6ca7fdd",{strings:[" Including up to 100 languages in LVLM training improves performance without significantly impacting English proficiency. "],speed:10,lifeLike:!0,startDelay:0,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-38a5f8dbddb3d6e56402c5fb5b2b0f9c></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-38a5f8dbddb3d6e56402c5fb5b2b0f9c",{strings:[" A balanced English/multilingual training data split (e.g., 50/50) is optimal for multilingual LVLM training. "],speed:10,lifeLike:!0,startDelay:1e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-977db76c0f852502e6899b15f6d82d9e></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-977db76c0f852502e6899b15f6d82d9e",{strings:[" Incorporating synthetic multilingual OCR data substantially improves multilingual text-in-image understanding. "],speed:10,lifeLike:!0,startDelay:2e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><h4 class="relative group">Why does it matter?<div id=why-does-it-matter class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#why-does-it-matter aria-label=Anchor>#</a></span></h4><p>This paper is crucial because <strong>it systematically investigates the optimal training strategies for multilingual Vision-Language Models (LVLMs)</strong>, a critical area in the current AI research landscape. The findings challenge existing assumptions and offer valuable insights for researchers working to develop more inclusive and performant LVLMs. The novel benchmark introduced opens up new avenues for future research on multilingual text-in-image understanding, which is vital for improving the accessibility and usefulness of these powerful models.</p><hr><h4 class="relative group">Visual Insights<div id=visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#visual-insights aria-label=Anchor>#</a></span></h4><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2501.05122/x1.png alt></figure></p><blockquote><p>üîº This figure illustrates the key factors investigated in the paper to understand multilingual capabilities in large vision-language models (LVLMs). It&rsquo;s broken down into three parts: (1) <strong>Training Data Languages:</strong> Shows a tiered structure of languages included in the training data, categorized from high to low-resource languages. This helps to visualize the different language mixes used in experiments. (2) <strong>Language Data Distribution:</strong> Illustrates different proportions of English versus multilingual data in the training. It demonstrates how altering this ratio affects the model‚Äôs performance. (3) <strong>Multilingual Text in Images:</strong> Presents examples of how multilingual text is incorporated into images in the training data. This element specifically focuses on improving the model&rsquo;s ability to understand OCR data from various languages. The figure is designed to show the various ways in which language is introduced in the data for LVLMs, aiming to improve multilingual performance.</p><details><summary>read the caption</summary>Figure 1: Exploring drivers of multilingual ability: (1) Languages in the training data; (2) Distribution of languages in the training data; (3) Incorporating multilingual OCR samples to understand non-English text in images.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Train Lang.</th><th>T1</th><th>T2</th><th>T3</th><th>T4</th><th>T5</th><th>en</th></tr></thead><tbody><tr><td><strong>All tasks</strong></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>English</td><td>14.4</td><td>30.4</td><td>24.4</td><td>23.6</td><td>28.5</td><td>53.6</td></tr><tr><td>T5</td><td>16.5</td><td>31.0</td><td>26.3</td><td>26.7</td><td>34.0</td><td>53.7</td></tr><tr><td>T5-4</td><td>17.4</td><td>30.6</td><td>27.9</td><td>29.6</td><td>33.5</td><td>51.5</td></tr><tr><td>T5-3</td><td>17.7</td><td>31.4</td><td>32.1</td><td>29.0</td><td>34.1</td><td>52.7</td></tr><tr><td>T5-2</td><td>17.0</td><td>34.5</td><td>30.0</td><td>28.2</td><td>33.4</td><td>54.1</td></tr><tr><td>L100</td><td>19.3</td><td>32.6</td><td>30.7</td><td>28.9</td><td>34.4</td><td>52.6</td></tr><tr><td><strong>Tasks unaffected by language fidelity</strong></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>English</td><td>33.0</td><td>32.5</td><td>36.3</td><td>38.5</td><td>42.9</td><td>55.7</td></tr><tr><td>T5</td><td>35.3</td><td>33.2</td><td>36.4</td><td>38.7</td><td>42.4</td><td>56.0</td></tr><tr><td>T5-T4</td><td>35.8</td><td>32.6</td><td>37.8</td><td>40.1</td><td>42.2</td><td>55.7</td></tr><tr><td>T5-T3</td><td>35.9</td><td>33.6</td><td>40.5</td><td>39.7</td><td>42.6</td><td>56.3</td></tr><tr><td>T5-T2</td><td>35.2</td><td>36.5</td><td>38.5</td><td>39.5</td><td>42.8</td><td>55.5</td></tr><tr><td>L100</td><td>36.1</td><td>34.3</td><td>39.1</td><td>39.8</td><td>42.7</td><td>54.6</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents the results of an experiment evaluating the performance of models trained with different sets of languages. The scores represent an average across multiple tasks, grouped by language tier. It shows that for some tasks (XM3600, MaXM, MTVQA), language fidelity (accuracy of the generated language) is a significant factor influencing the overall score. This table is crucial for understanding the impact of language diversity in the training data on model performance.</p><details><summary>read the caption</summary>(a) Scores are averaged over results from all tasks grouped by language tier. The performance on the following tasks is affected by language fidelity: XM3600, MaXM, MTVQA.</details></blockquote><h3 class="relative group">In-depth insights<div id=in-depth-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#in-depth-insights aria-label=Anchor>#</a></span></h3><h4 class="relative group">Multilingual LVLM Training<div id=multilingual-lvlm-training class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#multilingual-lvlm-training aria-label=Anchor>#</a></span></h4><p>Multilingual Large Vision-Language Model (LVLM) training presents significant challenges and opportunities. A naive approach of simply adding multilingual data to an existing English-centric training pipeline often yields suboptimal results, a phenomenon sometimes called the &ldquo;curse of multilingualism.&rdquo; <strong>Effective multilingual LVLM training requires careful consideration of several factors.</strong> These include the <strong>optimal number of languages</strong> to include in the training set, the <strong>ideal distribution of data across languages</strong>, and the <strong>impact of different training strategies</strong> like pre-training and instruction-tuning. Research suggests that a surprisingly large number of languages can be included without significantly harming English performance, and that a balanced approach with a substantial portion of non-English data is beneficial. Furthermore, incorporating multilingual OCR data, particularly for instruction-tuning, can greatly improve performance on tasks involving text within images. <strong>Finding the optimal balance between data quantity and quality across many languages is crucial,</strong> as the cost of acquiring and processing high-quality multilingual data can be prohibitive. Ultimately, successful multilingual LVLM training hinges on a well-defined strategy that accounts for these multifaceted linguistic and computational complexities, leading to more robust and inclusive models.</p><h4 class="relative group">Optimal Data Mixes<div id=optimal-data-mixes class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#optimal-data-mixes aria-label=Anchor>#</a></span></h4><p>The concept of &ldquo;Optimal Data Mixes&rdquo; in multilingual vision-language models (LVLMs) is crucial. The paper investigates the impact of various training data compositions on model performance across multiple languages. <strong>A key finding is that a balanced approach, rather than prioritizing English data, yields superior multilingual performance.</strong> The research explores the optimal proportion of English versus non-English data, suggesting a sweet spot where a significant portion of non-English data improves results without severely compromising English performance. Furthermore, the study delves into the optimal number of languages to include in training, highlighting a surprising finding: including a large number of languages can be beneficial. Finally, the role of instruction tuning data and the integration of multilingual OCR data are discussed, demonstrating that these can be critical factors for enhancing performance in lower-resource languages. <strong>The research emphasizes the need for careful consideration of data distribution and the absence of a one-size-fits-all solution.</strong> These findings have significant implications for training cost-effective and highly performant multilingual LVLMs.</p><h4 class="relative group">OCR Data Impact<div id=ocr-data-impact class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#ocr-data-impact aria-label=Anchor>#</a></span></h4><p>The integration of OCR (Optical Character Recognition) data significantly impacts the performance of multilingual vision-language models (LVLMs). <strong>The study reveals that including even a small amount of synthetic multilingual OCR data during pre-training and instruction-tuning substantially improves the model&rsquo;s ability to understand text within images, especially in low-resource languages.</strong> This improvement is particularly notable for Latin-script languages. However, <strong>the impact is less pronounced for languages with non-Latin scripts</strong>, suggesting a need for more extensive OCR data for these languages to achieve similar gains in performance. The findings highlight the importance of incorporating diverse and high-quality multilingual OCR data in training, <strong>emphasizing the trade-off between the quantity of data and its overall quality</strong>. While machine-translated data can be cost-effective, its inherent limitations necessitate a strategic balance between cost and accuracy. Therefore, <strong>a well-designed multilingual OCR data strategy is critical for building robust and effective LVLMs</strong> capable of handling various languages with varying levels of available data.</p><h4 class="relative group">Centurio: A Case Study<div id=centurio-a-case-study class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#centurio-a-case-study aria-label=Anchor>#</a></span></h4><p>A hypothetical case study on Centurio would delve into its multilingual capabilities and the factors influencing its performance across various languages and tasks. It would likely involve a detailed analysis of Centurio&rsquo;s architecture, training data, and evaluation metrics. <strong>The study would likely compare Centurio&rsquo;s performance against other state-of-the-art multilingual vision-language models (LVLMs),</strong> highlighting its strengths and weaknesses in handling different language families and resource levels. It would also focus on the effect of different training strategies on performance, such as the optimal number of languages included in training and the distribution of the data across languages. <strong>A key aspect would be examining Centurio&rsquo;s ability to handle text-in-image tasks effectively,</strong> as this is often a major challenge for multilingual LVLMs. This involves understanding the impact of training data with multilingual OCR samples and the overall performance improvements and cost-benefit analysis. <strong>Ultimately, the case study should provide valuable insights into the drivers of multilingual ability in LVLMs and offer recommendations for future research and development.</strong></p><h4 class="relative group">Future Research<div id=future-research class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#future-research aria-label=Anchor>#</a></span></h4><p>Future research directions stemming from this multilingual large vision-language model (LVLM) study should prioritize <strong>addressing the performance gap between Latin and non-Latin script languages</strong> in text-in-image understanding. This suggests a need for significantly more training data for non-Latin scripts, potentially through crowdsourcing or improved synthetic data generation techniques. Further investigation into <strong>optimal training data compositions beyond the 50/50 English/multilingual split</strong> explored here is also warranted, exploring the impact of varying data quality and language family representation. A crucial area for future work is <strong>rigorously evaluating the impact of machine translation on data quality</strong>, developing benchmarks that explicitly account for translation artifacts. Finally, this research could be extended by incorporating <strong>multicultural aspects into LVLM training</strong>, moving beyond language proficiency to encompass cultural knowledge and understanding in model outputs, better reflecting the complexity of human understanding.</p><h3 class="relative group">More visual insights<div id=more-visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#more-visual-insights aria-label=Anchor>#</a></span></h3><details><summary>More on figures</summary><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2501.05122/extracted/6121021/img/smpqa/bar_plot_1_en.png alt></figure></p><blockquote><p>üîº This figure details the prompts used for each dataset in the paper&rsquo;s evaluation suite. It highlights the diversity of input types across different tasks. For instance, some tasks involve single images, while others require multiple images as inputs. Furthermore, the options within certain multiple-choice questions (e.g., M3Exam and xMMMU) can also include images. The figure provides a concise visualization of the variations in question design and the complexity of the visual and textual elements required for each task.</p><details><summary>read the caption</summary>Figure 2: Prompts used for the different datasets of our test suite. For M3Exam and xMMMU, the questions contain images at individual positions, and also the options can consist of images. In total, a sample of M3Exam can contain up to 8 images and 8 options, and a sample of xMMMU can contain up to 4 images and 4 options.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2501.05122/extracted/6121021/img/smpqa/bar_plot_1_id.png alt></figure></p><blockquote><p>üîº The figure shows two types of questions for a bar chart. Grounding questions verify the understanding of the chart&rsquo;s structure, for example, identifying the tallest bar or the color of a specific bar. Reading questions test the ability to extract information from the chart, such as identifying the label of the tallest bar or the color of a specific bar. The example uses an English bar chart, but the paper states that the SMPQA dataset includes various languages and scripts.</p><details><summary>read the caption</summary>(a) Example of a bar plot in SMPQA for English. Questions for Grounding: 'Is the bar with label ‚Äôreward‚Äô the biggest?', 'Is the bar with label ‚Äôincredible‚Äô the biggest?', 'Is the bar with label ‚Äôreverse‚Äô the smallest?', 'Is the bar with label ‚Äôsunset‚Äô the smallest?', 'Is the bar with label ‚Äôclosed‚Äô colored in yellow?', 'Is the bar with label ‚Äôclosed‚Äô colored in purple?', 'Is the bar with label ‚Äôtwitter‚Äô colored in purple?', 'Is the bar with label ‚Äôtwitter‚Äô colored in red?' Questions for Reading: 'What is the label of the biggest bar?', 'What is the label of the smallest bar?', 'What is the label of the yellow bar?', 'What is the label of the red bar?', 'What is the label of the purple bar?'</details></blockquote></details><details><summary>More on tables</summary><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Train Lang.</th><th>T1</th><th>T2</th><th>T3</th><th>T4</th><th>T5</th><th>en</th></tr></thead><tbody><tr><td>English</td><td>0.2</td><td>0.2</td><td>0.1</td><td>2.4</td><td>6.2</td><td>100.0</td></tr><tr><td>T5</td><td>39.1</td><td>36.1</td><td>82.2</td><td>83.9</td><td><strong>99.1</strong></td><td>100.0</td></tr><tr><td>T5-T4</td><td>61.8</td><td>84.6</td><td>87.5</td><td><strong>99.2</strong></td><td>98.4</td><td>100.0</td></tr><tr><td>T5-T3</td><td><strong>72.9</strong></td><td>84.4</td><td><strong>98.2</strong></td><td>95.2</td><td>97.9</td><td>100.0</td></tr><tr><td>T5-T2</td><td>68.5</td><td><strong>99.0</strong></td><td>97.9</td><td>98.4</td><td>98.1</td><td>100.0</td></tr><tr><td>L100</td><td><strong>72.9</strong></td><td>98.2</td><td>95.4</td><td>97.8</td><td>98.2</td><td>100.0</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents the average language fidelity scores achieved by various multilingual large vision-language models (LVLMs) on the XM3600 dataset. Language fidelity refers to the model&rsquo;s ability to generate outputs (image captions in this case) in the target language specified in the input prompt. The table shows how well each model can generate captions in the correct language for a range of languages, indicating the model&rsquo;s multilingual performance level. Higher percentages indicate better language fidelity. The scores are broken down by language tiers (T1-T5) representing different language resource levels, with T5 being the high-resource languages and T1 being the low-resource languages. This allows for analysis of how the models perform across different language groups. The column &rsquo;en&rsquo; represents the English language results.</p><details><summary>read the caption</summary>(b) Average language fidelity on XM3600 in %.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>English %</th><th>T1</th><th>T2</th><th>T3</th><th>T4</th><th>T5</th><th>en</th></tr></thead><tbody><tr><td>1</td><td>19.1</td><td>30.3</td><td>28.8</td><td>27.1</td><td>31.7</td><td>48.9</td></tr><tr><td>10</td><td>18.1</td><td>32.4</td><td>29.4</td><td>27.4</td><td>32.5</td><td>50.1</td></tr><tr><td>25</td><td><strong>19.7</strong></td><td><strong>35.5</strong></td><td>29.9</td><td>27.9</td><td>33.0</td><td>50.3</td></tr><tr><td>50</td><td><s>19.3</s></td><td><s>32.6</s></td><td><s>30.7</s></td><td><strong>28.9</strong></td><td><s>34.4</s></td><td>52.6</td></tr><tr><td>75</td><td>18.5</td><td>31.5</td><td><strong>30.7</strong></td><td><s>28.4</s></td><td><strong>34.6</strong></td><td><s>54.1</s></td></tr><tr><td>90</td><td>15.9</td><td>31.2</td><td>27.6</td><td>26.9</td><td>34.1</td><td><strong>54.8</strong></td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents the results of experiments evaluating the impact of the number of training languages on the performance of large vision-language models (LVLMs). Different model configurations were trained with varying sets of languages, ranging from a small number to a large number (100). The table shows the performance of each model configuration on various downstream vision-language tasks, providing average scores across all tasks and per-language scores grouped by language resource tiers. The best and second-best performance in each column (task) are highlighted to show the relative gains from increasing the number of languages included in training. This helps in determining an optimal multilingual training mix without compromising performance on English, a common challenge in multilingual LVLMs.</p><details><summary>read the caption</summary>Table 1: RQ1 (¬ß2.2) results for models trained with different sets of languages. We emphasize the best and second-best result in each column.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>English %</th><th>T1</th><th>T2</th><th>T3</th><th>T4</th><th>T5</th><th>en</th></tr></thead><tbody><tr><td>No pre-training</td><td>19.3</td><td>32.6</td><td>30.7</td><td>28.9</td><td>34.4</td><td>52.6</td></tr><tr><td>100</td><td>19.3</td><td>33.3</td><td>32.1</td><td>29.4</td><td>34.5</td><td><strong>55.2</strong></td></tr><tr><td>50</td><td><strong>22.8</strong></td><td><strong>39.5</strong></td><td><strong>33.8</strong></td><td>30.8</td><td><strong>35.7</strong></td><td>54.9</td></tr><tr><td>1</td><td>22.7</td><td>38.9</td><td>33.7</td><td><strong>31.2</strong></td><td>35.4</td><td>55.1</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents the results of experiments evaluating the impact of different ratios of English to multilingual data in the instruction-tuning phase of training large vision-language models (LVLMs). It shows the average performance across 13 downstream vision-language tasks, broken down by language tier (T1-T5). The table helps to understand the optimal balance between English and multilingual data during instruction tuning to achieve strong performance across diverse languages, while maintaining good English performance. The different language tiers represent different levels of resource availability for those languages, helping to assess the impact of the training data balance on resource-constrained languages.</p><details><summary>read the caption</summary>Table 2: RQ2 (¬ß2.3) results for models trained with different ratios of English to multilingual data in the instruction-tuning phase. Scores are averaged over results from all tasks grouped by language tier.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Setup</th><th>SMPQA Ground</th><th></th><th></th><th>SMPQA Read</th><th></th><th></th></tr></thead><tbody><tr><td></td><td>en</td><td>Latin</td><td>other</td><td>en</td><td>Latin</td><td>other</td></tr><tr><td>No pre-training</td><td>69.6</td><td>67.2</td><td>51.9</td><td>33.4</td><td>12.8</td><td>0.1</td></tr><tr><td>No OCR</td><td>76.1</td><td>73.0</td><td>55.3</td><td>41.8</td><td>23.1</td><td>0.2</td></tr><tr><td>100% Eng.</td><td>78.4</td><td>74.7</td><td>57.9</td><td>55.8</td><td>39.9</td><td>3.9</td></tr><tr><td>50% Eng.</td><td>81.2</td><td>76.7</td><td>60.0</td><td>53.8</td><td>41.8</td><td>7.1</td></tr><tr><td>50% (frozen)</td><td>76.1</td><td>70.8</td><td>56.3</td><td>47.2</td><td>34.1</td><td>3.5</td></tr><tr><td>1% Eng.</td><td>81.0</td><td>78.3</td><td>64.1</td><td>54.8</td><td>43.5</td><td>8.0</td></tr><tr><td>Latin down</td><td>78.9</td><td>74.2</td><td>59.5</td><td>54.6</td><td>41.0</td><td>9.9</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents the results of experiments investigating the impact of different English-to-multilingual ratios in pre-training data on the performance of a large vision-language model (LVM). The model was trained with 100 languages, and the instruction-tuning phase consistently used a 50% English, 50% multilingual data split across the 100 languages. The table shows how varying the proportion of English data in pre-training (from 1% to 100%) affects performance across different language tiers (T1-T5), which represent language resourceness, for both overall tasks and tasks not affected by language fidelity. This allows for an assessment of the trade-off between the inclusion of more languages and overall performance, revealing whether an optimal multilingual pre-training data mix exists, and if so, what its characteristics might be.</p><details><summary>read the caption</summary>Table 3: RQ3 (¬ß2.4) results with different English-to-multilingual ratios (L100) for pre-training. All variants are identically instruction-tuned (L100, 50% En.).</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><h2 class="relative group">Table 1: Performance Comparison of Different Vision-Language Models<div id=table-1-performance-comparison-of-different-vision-language-models class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#table-1-performance-comparison-of-different-vision-language-models aria-label=Anchor>#</a></span></h2><table><thead><tr><th>Model Name</th><th>AVG.</th><th>XM3600 en</th><th>XM3600 mul</th><th>XM3600 fid.</th><th>MT-VQA</th><th>SMPQA G. en</th><th>SMPQA G. mul</th><th>SMPQA N. en</th><th>SMPQA N. mul</th><th>M3Exam en</th><th>M3Exam mul</th><th>xMMMU en</th><th>xMMMU mul</th><th>C-VQA</th></tr></thead><tbody><tr><td>Parrot</td><td>25.8</td><td>5.6</td><td>0.4</td><td>25.0</td><td>2.0</td><td>51.0</td><td>49.9</td><td>0.0</td><td>0.0</td><td>46.6</td><td>36.2</td><td>35.3</td><td>32.4</td><td>41.1</td></tr><tr><td>PALO 7B</td><td>28.7</td><td>65.9</td><td>13.5</td><td>72.0</td><td>5.8</td><td>55.5</td><td>52.8</td><td>22.4</td><td>2.7</td><td>41.0</td><td>29.1</td><td>31.8</td><td>30.9</td><td>37.1</td></tr><tr><td>PALO 13B</td><td>29.9</td><td>67.3</td><td>17.0</td><td>60.1</td><td>6.3</td><td>54.0</td><td>51.5</td><td>25.6</td><td>4.0</td><td>45.2</td><td>28.3</td><td>32.4</td><td>28.9</td><td>39.6</td></tr><tr><td>Llama-Vision 3.2 11B</td><td>*32.3</td><td>35.9</td><td>7.2</td><td>33.3</td><td>15.2</td><td>91.1</td><td>84.8</td><td>58.4</td><td>22.8</td><td>-</td><td>-</td><td>-</td><td>-</td><td>38.8</td></tr><tr><td>Maya</td><td>33.4</td><td>55.9</td><td>14.6</td><td>65.7</td><td>5.3</td><td>51.4</td><td>50.9</td><td>14.6</td><td>1.8</td><td>49.2</td><td>36.3</td><td>37.9</td><td>33.3</td><td>39.8</td></tr><tr><td>Pixtral 12B</td><td>38.1</td><td>26.5</td><td>22.1</td><td>96.8</td><td>14.1</td><td>91.1</td><td>71.0</td><td>85.0</td><td>35.9</td><td>49.4</td><td>33.7</td><td>30.3</td><td>26.2</td><td>33.5</td></tr><tr><td>Phi 3.5 Vision</td><td>39.5</td><td>32.3</td><td>6.3</td><td>40.8</td><td>11.1</td><td>92.2</td><td>79.4</td><td>84.8</td><td>35.9</td><td>56.3</td><td>40.7</td><td>41.7</td><td>37.4</td><td>40.9</td></tr><tr><td>Qwen2VL 2B</td><td>41.2</td><td>68.8</td><td>5.2</td><td>13.2</td><td>19.0</td><td>85.0</td><td>83.5</td><td>68.8</td><td>47.4</td><td>47.9</td><td>40.5</td><td>36.8</td><td>35.5</td><td>33.6</td></tr><tr><td>MiniCPM 2.6</td><td>41.7</td><td>87.5</td><td>14.2</td><td>92.3</td><td>16.1</td><td>89.0</td><td>74.3</td><td>80.8</td><td>39.3</td><td>55.0</td><td>48.2</td><td>39.1</td><td>36.5</td><td>34.1</td></tr><tr><td>InternVL 2.5 4B</td><td>45.3</td><td>38.9</td><td>17.5</td><td>91.0</td><td>25.1</td><td>87.0</td><td>78.3</td><td>77.8</td><td>47.5</td><td>63.2</td><td>50.3</td><td>49.2</td><td>42.7</td><td>48.1</td></tr><tr><td>InternVL 2.5 8B</td><td>47.4</td><td>38.3</td><td>15.7</td><td>91.1</td><td>25.0</td><td>91.0</td><td>79.2</td><td>80.6</td><td>48.2</td><td>67.0</td><td>53.3</td><td>50.7</td><td>45.2</td><td>48.6</td></tr><tr><td>Qwen2VL 7B</td><td>47.7</td><td>50.3</td><td>24.6</td><td>90.0</td><td>23.2</td><td>91.2</td><td>90.9</td><td>85.0</td><td>64.9</td><td>56.1</td><td>49.7</td><td>43.0</td><td>40.7</td><td>37.6</td></tr><tr><td>Pangea</td><td>48.2</td><td>70.1</td><td>34.6</td><td>87.9</td><td>19.3</td><td>87.2</td><td>72.2</td><td>72.0</td><td>23.8</td><td>58.0</td><td>45.5</td><td>43.1</td><td>42.0</td><td>55.2</td></tr><tr><td>Centurio Aya</td><td>48.5</td><td>78.4</td><td>39.2</td><td>95.7</td><td>11.1</td><td>83.1</td><td>74.2</td><td>60.0</td><td>30.1</td><td>53.0</td><td>41.2</td><td>37.6</td><td>37.2</td><td>49.4</td></tr><tr><td>Centurio Qwen</td><td>51.6</td><td>79.1</td><td>34.4</td><td>95.2</td><td>11.9</td><td>84.8</td><td>76.1</td><td>65.2</td><td>31.7</td><td>61.2</td><td>46.9</td><td>46.4</td><td>43.0</td><td>52.9</td></tr></tbody></table><h2 class="relative group">Table 2: MAXM, xGQA, BIN-MC, XVNLI, MaRVL, VGR, and VLOD Performance Comparison<div id=table-2-maxm-xgqa-bin-mc-xvnli-marvl-vgr-and-vlod-performance-comparison class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#table-2-maxm-xgqa-bin-mc-xvnli-marvl-vgr-and-vlod-performance-comparison aria-label=Anchor>#</a></span></h2><table><thead><tr><th>Model Name</th><th>MAXM en</th><th>MAXM mul</th><th>xGQA en</th><th>xGQA mul</th><th>BIN-MC en</th><th>BIN-MC mul</th><th>XVNLI en</th><th>XVNLI mul</th><th>MaRVL en</th><th>MaRVL mul</th><th>VGR en</th><th>VGR mul</th><th>VLOD en</th><th>VLOD mul</th></tr></thead><tbody><tr><td>Parrot</td><td>28.2</td><td>3.6</td><td>37.7</td><td>21.2</td><td>30.5</td><td>25.7</td><td>28.7</td><td>31.4</td><td>63.5</td><td>55.1</td><td>59.2</td><td>52.9</td><td>0.0</td><td>0.0</td></tr><tr><td>PALO 7B</td><td>54.0</td><td>22.5</td><td>59.1</td><td>36.6</td><td>58.7</td><td>38.6</td><td>58.0</td><td>53.4</td><td>62.7</td><td>24.1</td><td>48.3</td><td>25.6</td><td>5.8</td><td>6.8</td></tr><tr><td>PALO 13B</td><td>51.7</td><td>33.1</td><td>58.0</td><td>27.8</td><td>61.4</td><td>41.1</td><td>56.6</td><td>53.6</td><td>63.8</td><td>33.1</td><td>63.3</td><td>26.2</td><td>2.5</td><td>4.9</td></tr><tr><td>Llama-Vision 3.2 11B</td><td>0.0</td><td>4.7</td><td>39.3</td><td>27.6</td><td>75.6</td><td>50.8</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Maya</td><td>55.4</td><td>17.3</td><td>58.2</td><td>49.1</td><td>54.0</td><td>43.2</td><td>50.1</td><td>43.9</td><td>60.3</td><td>56.3</td><td>46.7</td><td>42.3</td><td>20.0</td><td>20.1</td></tr><tr><td>Pixtral 12B</td><td>59.4</td><td>43.4</td><td>59.9</td><td>3.8</td><td>71.0</td><td>54.2</td><td>60.9</td><td>52.7</td><td>67.7</td><td>60.7</td><td>55.8</td><td>47.7</td><td>9.2</td><td>12.4</td></tr><tr><td>Phi 3.5 Vision</td><td>43.6</td><td>17.9</td><td>65.2</td><td>38.0</td><td>63.1</td><td>36.8</td><td>58.9</td><td>53.3</td><td>73.4</td><td>46.4</td><td>81.7</td><td>50.3</td><td>45.8</td><td>31.5</td></tr><tr><td>Qwen2VL 2B</td><td>53.7</td><td>26.5</td><td>60.5</td><td>38.2</td><td>78.2</td><td>47.2</td><td>61.9</td><td>56.2</td><td>67.9</td><td>55.9</td><td>61.7</td><td>50.5</td><td>22.5</td><td>20.4</td></tr><tr><td>MiniCPM 2.6</td><td>53.4</td><td>22.3</td><td>57.9</td><td>45.7</td><td>72.6</td><td>47.4</td><td>71.9</td><td>65.4</td><td>70.2</td><td>57.9</td><td>52.5</td><td>49.1</td><td>9.2</td><td>14.6</td></tr><tr><td>InternVL 2.5 4B</td><td>46.0</td><td>42.5</td><td>63.6</td><td>28.0</td><td>68.4</td><td>45.4</td><td>69.0</td><td>58.7</td><td>74.9</td><td>59.0</td><td>72.5</td><td>49.7</td><td>24.2</td><td>21.0</td></tr><tr><td>InternVL 2.5 8B</td><td>45.6</td><td>38.2</td><td>63.4</td><td>32.0</td><td>70.3</td><td>44.2</td><td>73.5</td><td>66.4</td><td>83.0</td><td>63.3</td><td>87.5</td><td>51.6</td><td>57.5</td><td>29.0</td></tr><tr><td>Qwen2VL 7B</td><td>54.7</td><td>31.2</td><td>62.5</td><td>49.3</td><td>80.7</td><td>57.5</td><td>62.1</td><td>59.6</td><td>69.8</td><td>60.2</td><td>60.0</td><td>52.9</td><td>5.8</td><td>13.2</td></tr><tr><td>Pangea</td><td>61.4</td><td>55.0</td><td>64.6</td><td>60.4</td><td>70.3</td><td>52.1</td><td>69.0</td><td>65.2</td><td>75.8</td><td>70.5</td><td>69.2</td><td>58.9</td><td>0.0</td><td>6.7</td></tr><tr><td>Centurio Aya</td><td>55.7</td><td>49.3</td><td>59.1</td><td>53.2</td><td>69.7</td><td>54.7</td><td>65.0</td><td>62.4</td><td>85.0</td><td>77.9</td><td>82.5</td><td>66.8</td><td>12.5</td><td>20.7</td></tr><tr><td>Centurio Qwen</td><td>60.1</td><td>47.7</td><td>60.6</td><td>54.8</td><td>72.7</td><td>56.2</td><td>75.4</td><td>70.2</td><td>89.6</td><td>81.7</td><td>87.5</td><td>73.1</td><td>28.3</td><td>27.0</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents the results of experiments evaluating the impact of adding synthetic OCR data to the training of multilingual vision-language models (LVLMs). The experiments are performed on the SMPQA benchmark, which assesses the model&rsquo;s ability to read and understand text within images. Multiple model configurations are examined varying in several key aspects: 1. <strong>Pre-training:</strong> Models are tested with and without a pre-training phase, using the data distribution found optimal in previous sections of the paper. 2. <strong>Image Encoder:</strong> Models are tested with frozen versus unfrozen image encoders. 3. <strong>OCR Data Distribution:</strong> The proportion of English vs. non-English OCR data is varied (1%, 25%, 50%, 100%). 4. <strong>Latin Script Emphasis:</strong> A specific condition where Latin-script languages receive 2.5k samples while others get 10k.</p><details><summary>read the caption</summary>Table 4: RQ4 (¬ß2.5) results of models trained with additional synthetic OCR data on SMPQA for English, Latin-script languages, and languages with other scripts. No pre-training: from Table¬†2; No OCR: from Table¬†3; frozen: image encoder frozen; N% Eng.: N%percentùëÅN\%italic_N % of OCR data is English, rest uniform distributed over L100 languages; Latin down: 2.5k samples for all Latin-script languages, 10k samples for others.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Model</th><th>T1</th><th>T2</th><th>T3</th><th>T4</th><th>T5</th><th>en</th></tr></thead><tbody><tr><td>Centurio Aya</td><td>35.1</td><td>46.4</td><td>47.0</td><td>46.7</td><td>48.3</td><td>60.6</td></tr><tr><td>Centurio Qwen</td><td>38.1</td><td>51.0</td><td>48.3</td><td>47.0</td><td>50.9</td><td>66.6</td></tr><tr><td>InternVL 2.5 8B</td><td>29.9</td><td>37.0</td><td>37.4</td><td>41.0</td><td>50.5</td><td>64.4</td></tr><tr><td>Qwen2VL 7B</td><td>30.6</td><td>36.8</td><td>40.5</td><td>46.2</td><td>48.0</td><td>56.8</td></tr><tr><td>Pangea</td><td>38.5</td><td>38.6</td><td>46.9</td><td>44.2</td><td>49.9</td><td>59.8</td></tr><tr><td><strong>Without multi-image tasks (MaRVL, VGR, VLOD):</strong></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Centurio Aya</td><td>35.1</td><td>44.5</td><td>45.7</td><td>46.2</td><td>47.7</td><td>60.7</td></tr><tr><td>Centurio Qwen</td><td>38.1</td><td>49.5</td><td>45.6</td><td>45.8</td><td>49.6</td><td>66.0</td></tr><tr><td>InternVL 2.5 8B</td><td>29.9</td><td>40.4</td><td>35.2</td><td>39.4</td><td>49.7</td><td>62.3</td></tr><tr><td>Qwen2VL 7B</td><td>30.6</td><td>38.7</td><td>40.8</td><td>46.8</td><td>48.3</td><td>61.7</td></tr><tr><td>Pangea</td><td>38.5</td><td>46.5</td><td>47.7</td><td>44.4</td><td>49.9</td><td>64.9</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents a comprehensive comparison of Centurio&rsquo;s performance against 13 other Large Vision-Language Models (LVLMs) across 14 diverse tasks. The evaluation metrics include accuracy scores (using CIDEr for the XM3600 task) and language fidelity, along with more granular results for specific tasks like SMPQA grounding and naming. The table distinguishes between English-only performance and averaged multilingual performance across various language tiers, providing insights into the models&rsquo; multilingual capabilities. The &lsquo;*&rsquo; indicates models that only support single-image input, and &lsquo;AVG&rsquo; represents the average performance across all tasks. Additional details about the experimental setup and models are available in Appendix C.</p><details><summary>read the caption</summary>Table 5: Comparison of Centurio and 13 other LVLMs across 14 tasks. We highlight the best and second-best results. Scores are accuracy (CIDEr for XM3600). en & mul are the English and averaged multilingual results. XM3600 fid. is the language fidelity over all languages; SMPQA G. & N are Grounding and Naming. *: supports only single-image input. AVG.: average over all tasks. Details on the setup and models are provided in Appendix¬†C.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Name</th><th>Script</th><th>ISO-639</th><th>Flores-200</th><th>Tier</th></tr></thead><tbody><tr><td>Arabic</td><td>Arabic</td><td><code>ar</code></td><td><code>arb_Arab</code></td><td>5</td></tr><tr><td>Chinese</td><td>Trad. Han</td><td><code>zh</code></td><td><code>zho_Hant</code></td><td>5</td></tr><tr><td>English</td><td>Latin</td><td><code>en</code></td><td><code>eng_Latn</code></td><td>5</td></tr><tr><td>French</td><td>Latin</td><td><code>fr</code></td><td><code>fra_Latn</code></td><td>5</td></tr><tr><td>German</td><td>Latin</td><td><code>de</code></td><td><code>deu_Latn</code></td><td>5</td></tr><tr><td>Japanese</td><td>Japanese</td><td><code>ja</code></td><td><code>jpn_Jpan</code></td><td>5</td></tr><tr><td>Spanish</td><td>Latin</td><td><code>es</code></td><td><code>spa_Latn</code></td><td>5</td></tr><tr><td>Basque</td><td>Latin</td><td><code>eu</code></td><td><code>eus_Latn</code></td><td>4</td></tr><tr><td>Catalan</td><td>Latin</td><td><code>ca</code></td><td><code>cat_Latn</code></td><td>4</td></tr><tr><td>Croatian</td><td>Latin</td><td><code>hr</code></td><td><code>hrv_Latn</code></td><td>4</td></tr><tr><td>Czech</td><td>Latin</td><td><code>cs</code></td><td><code>ces_Latn</code></td><td>4</td></tr><tr><td>Dutch</td><td>Latin</td><td><code>nl</code></td><td><code>nld_Latn</code></td><td>4</td></tr><tr><td>Finnish</td><td>Latin</td><td><code>fi</code></td><td><code>fin_Latn</code></td><td>4</td></tr><tr><td>Hindi</td><td>Devanagari</td><td><code>hi</code></td><td><code>hin_Deva</code></td><td>4</td></tr><tr><td>Hungarian</td><td>Latin</td><td><code>hu</code></td><td><code>hun_Latn</code></td><td>4</td></tr><tr><td>Italian</td><td>Latin</td><td><code>it</code></td><td><code>ita_Latn</code></td><td>4</td></tr><tr><td>Korean</td><td>Hangul</td><td><code>ko</code></td><td><code>kor_Hang</code></td><td>4</td></tr><tr><td>Persian</td><td>Arabic</td><td><code>fa</code></td><td><code>pes_Arab</code></td><td>4</td></tr><tr><td>Polish</td><td>Latin</td><td><code>pl</code></td><td><code>pol_Latn</code></td><td>4</td></tr><tr><td>Portuguese</td><td>Latin</td><td><code>pt</code></td><td><code>por_Latn</code></td><td>4</td></tr><tr><td>Russian</td><td>Cyrillic</td><td><code>ru</code></td><td><code>rus_Cyrl</code></td><td>4</td></tr><tr><td>Serbian</td><td>Cyrillic</td><td><code>sr</code></td><td><code>srp_Cyrl</code></td><td>4</td></tr><tr><td>Swedish</td><td>Latin</td><td><code>sv</code></td><td><code>swe_Latn</code></td><td>4</td></tr><tr><td>Turkish</td><td>Latin</td><td><code>tr</code></td><td><code>tur_Latn</code></td><td>4</td></tr><tr><td>Vietnamese</td><td>Latin</td><td><code>vi</code></td><td><code>vie_Latn</code></td><td>4</td></tr><tr><td>Afrikaans</td><td>Latin</td><td><code>af</code></td><td><code>afr_Latn</code></td><td>3</td></tr><tr><td>Bangla</td><td>Bengali</td><td><code>bn</code></td><td><code>ben_Beng</code></td><td>3</td></tr><tr><td>Belarusian</td><td>Cyrillic</td><td><code>be</code></td><td><code>bel_Cyrl</code></td><td>3</td></tr><tr><td>Bosnian</td><td>Latin</td><td><code>bs</code></td><td><code>bos_Latn</code></td><td>3</td></tr><tr><td>Bulgarian</td><td>Cyrillic</td><td><code>bg</code></td><td><code>bul_Cyrl</code></td><td>3</td></tr><tr><td>Cebuano</td><td>Latin</td><td><code>ceb</code></td><td><code>ceb_Latn</code></td><td>3</td></tr><tr><td>Danish</td><td>Latin</td><td><code>da</code></td><td><code>dan_Latn</code></td><td>3</td></tr><tr><td>Egyptian Arabic</td><td>Arabic</td><td><code>ar-eg</code></td><td><code>arz_Arab</code></td><td>3</td></tr><tr><td>Estonian</td><td>Latin</td><td><code>et</code></td><td><code>est_Latn</code></td><td>3</td></tr><tr><td>Galician</td><td>Latin</td><td><code>gl</code></td><td><code>glg_Latn</code></td><td>3</td></tr><tr><td>Georgian</td><td>Georgian</td><td><code>ka</code></td><td><code>kat_Geor</code></td><td>3</td></tr><tr><td>Greek</td><td>Greek</td><td><code>el</code></td><td><code>ell_Grek</code></td><td>3</td></tr><tr><td>Indonesian</td><td>Latin</td><td><code>id</code></td><td><code>ind_Latn</code></td><td>3</td></tr><tr><td>Kazakh</td><td>Cyrillic</td><td><code>kk</code></td><td><code>kaz_Cyrl</code></td><td>3</td></tr><tr><td>Latin</td><td>Latin</td><td><code>la</code></td><td><code>NO</code></td><td>3</td></tr><tr><td>Latvian</td><td>Latin</td><td><code>lv</code></td><td><code>lvs_Latn</code></td><td>3</td></tr><tr><td>Lithuanian</td><td>Latin</td><td><code>lt</code></td><td><code>lit_Latn</code></td><td>3</td></tr><tr><td>Malay</td><td>Latin</td><td><code>ms</code></td><td><code>zsm_Latn</code></td><td>3</td></tr><tr><td>Romanian</td><td>Latin</td><td><code>ro</code></td><td><code>ron_Latn</code></td><td>3</td></tr><tr><td>Slovak</td><td>Latin</td><td><code>sk</code></td><td><code>slk_Latn</code></td><td>3</td></tr><tr><td>Slovenian</td><td>Latin</td><td><code>sl</code></td><td><code>slv_Latn</code></td><td>3</td></tr><tr><td>Tagalog</td><td>Latin</td><td><code>tl</code></td><td><code>tgl_Latn</code></td><td>3</td></tr><tr><td>Tamil</td><td>Tamil</td><td><code>ta</code></td><td><code>tam_Taml</code></td><td>3</td></tr><tr><td>Thai</td><td>Thai</td><td><code>th</code></td><td><code>tha_Thai</code></td><td>3</td></tr><tr><td>Ukrainian</td><td>Cyrillic</td><td><code>uk</code></td><td><code>ukr_Cyrl</code></td><td>3</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents a comparison of Centurio&rsquo;s performance against the top three models from Table 5 across fourteen vision-language tasks. The results are averaged across all fourteen tasks and grouped by language tier (T1-T5, representing language resource levels, with T5 being high-resource and T1 low-resource), providing a comprehensive evaluation of multilingual capabilities. The table highlights Centurio&rsquo;s performance relative to other state-of-the-art models across different language groups, illustrating its strengths and weaknesses in various tasks and language scenarios.</p><details><summary>read the caption</summary>Table 6: Comparison between Centurio and the top-3 models of Table¬†5. Scores are averages over results from all 14 tasks grouped by language tier.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Name</th><th>Script</th><th>ISO-639</th><th>Flores-200</th><th>Tier</th></tr></thead><tbody><tr><td>Urdu</td><td>Arabic</td><td>ur</td><td>urd_Arab</td><td>3</td></tr><tr><td>Uzbek</td><td>Latin</td><td>uz</td><td>uzn_Latn</td><td>3</td></tr><tr><td>Hebrew</td><td>Hebrew</td><td>iwhe</td><td>heb_Hebr</td><td>3</td></tr><tr><td>Amharic</td><td>Ethiopic</td><td>am</td><td>amh_Ethi</td><td>2</td></tr><tr><td>Haitian</td><td>Latin</td><td>ht</td><td>hat_Latn</td><td>2</td></tr><tr><td>Hausa</td><td>Latin</td><td>ha</td><td>hau_Latn</td><td>2</td></tr><tr><td>Icelandic</td><td>Latin</td><td>is</td><td>isl_Latn</td><td>2</td></tr><tr><td>Irish</td><td>Latin</td><td>ga</td><td>gle_Latn</td><td>2</td></tr><tr><td>Lao</td><td>Lao</td><td>lo</td><td>lao_Laoo</td><td>2</td></tr><tr><td>Maltese</td><td>Latin</td><td>mt</td><td>mlt_Latn</td><td>2</td></tr><tr><td>Marathi</td><td>Devanagari</td><td>mr</td><td>mar_Deva</td><td>2</td></tr><tr><td>Punjabi</td><td>Gurmukhi</td><td>pa</td><td>pan_Guru</td><td>2</td></tr><tr><td>Sanskrit</td><td>Devanagari</td><td>sa</td><td>san_Deva</td><td>2</td></tr><tr><td>Swahili</td><td>Latin</td><td>sw</td><td>swh_Latn</td><td>2</td></tr><tr><td>Tigrinya</td><td>Ethiopic</td><td>ti</td><td>tir_Ethi</td><td>2</td></tr><tr><td>Tswana</td><td>Latin</td><td>tn</td><td>tsn_Latn</td><td>2</td></tr><tr><td>Wolof</td><td>Latin</td><td>wo</td><td>wol_Latn</td><td>2</td></tr><tr><td>Xhosa</td><td>Latin</td><td>xh</td><td>xho_Latn</td><td>2</td></tr><tr><td>Yoruba</td><td>Latin</td><td>yo</td><td>yor_Latn</td><td>2</td></tr><tr><td>Zulu</td><td>Latin</td><td>zu</td><td>zul_Latn</td><td>2</td></tr><tr><td>Albanian</td><td>Latin</td><td>sq</td><td>als_Latn</td><td>1</td></tr><tr><td>Assamese</td><td>Bengali</td><td>as</td><td>asm_Beng</td><td>1</td></tr><tr><td>Azerbaijani</td><td>Arabic</td><td>azb</td><td>azb_Arab</td><td>1</td></tr><tr><td>Bambara</td><td>Latin</td><td>bm</td><td>bam_Latn</td><td>1</td></tr><tr><td>Burmese</td><td>Myanmar</td><td>my</td><td>mya_Mymr</td><td>1</td></tr><tr><td>Esperanto</td><td>Latin</td><td>eo</td><td>epo_Latn</td><td>1</td></tr><tr><td>Igbo</td><td>Latin</td><td>ig</td><td>ibo_Latn</td><td>1</td></tr><tr><td>Javanese</td><td>Latin</td><td>jv</td><td>jav_Latn</td><td>1</td></tr><tr><td>Khmer</td><td>Khmer</td><td>km</td><td>khm_Khmr</td><td>1</td></tr><tr><td>Kikuyu</td><td>Latin</td><td>ki</td><td>kik_Latn</td><td>1</td></tr><tr><td>Lingala</td><td>Latin</td><td>ln</td><td>lin_Latn</td><td>1</td></tr><tr><td>Luxembourgish</td><td>Latin</td><td>lb</td><td>ltz_Latn</td><td>1</td></tr><tr><td>Maori</td><td>Latin</td><td>mi</td><td>mri_Latn</td><td>1</td></tr><tr><td>Norwegian</td><td>Latin</td><td>no</td><td>nob_Latn</td><td>1</td></tr><tr><td>Occitan</td><td>Latin</td><td>oc</td><td>oci_Latn</td><td>1</td></tr><tr><td>Quechua</td><td>Latin</td><td>qu</td><td>quy_Latn</td><td>1</td></tr><tr><td>Samoan</td><td>Latin</td><td>sm</td><td>smo_Latn</td><td>1</td></tr><tr><td>Sango</td><td>Latin</td><td>sg</td><td>sag_Latn</td><td>1</td></tr><tr><td>Sardinian</td><td>Latin</td><td>sc</td><td>srd_Latn</td><td>1</td></tr><tr><td>Scottish Gaelic</td><td>Latin</td><td>gd</td><td>gla_Latn</td><td>1</td></tr><tr><td>Sindhi</td><td>Arabic</td><td>sd</td><td>snd_Arab</td><td>1</td></tr><tr><td>Somali</td><td>Latin</td><td>so</td><td>som_Latn</td><td>1</td></tr><tr><td>Swati</td><td>Latin</td><td>ss</td><td>ssw_Latn</td><td>1</td></tr><tr><td>Telugu</td><td>Telugu</td><td>te</td><td>tel_Telu</td><td>1</td></tr><tr><td>Tibetan</td><td>Tibetan</td><td>bo</td><td>bod_Tibt</td><td>1</td></tr><tr><td>Tok Pisin</td><td>Latin</td><td>tpi</td><td>tpi_Latn</td><td>1</td></tr><tr><td>Tsonga</td><td>Latin</td><td>ts</td><td>tso_Latn</td><td>1</td></tr><tr><td>Twi</td><td>Latin</td><td>tw</td><td>twi_Latn</td><td>1</td></tr><tr><td>Waray</td><td>Latin</td><td>war</td><td>war_Latn</td><td>1</td></tr><tr><td>Welsh</td><td>Latin</td><td>cy</td><td>cym_Latn</td><td>1</td></tr></tbody></table></table></figure><blockquote><p>üîº Table 7 presents a comprehensive list of the 100 languages included in the training data for the multilingual vision-language model. Each language is categorized into one of five tiers (T1-T5) based on the resource availability, as defined in the taxonomy by Joshi et al. (2020). A higher tier number indicates a greater abundance of resources (like training data and other linguistic tools) for that language. This tiering system helps to understand the relative scarcity or abundance of training data across the different languages used, which is crucial for evaluating the impact of various multilingual training strategies on model performance.</p><details><summary>read the caption</summary>Table 7: The list of 100 languages used in our training experiments. The ‚ÄùTier‚Äù column represents the tier in the taxonomy proposed by Joshi et¬†al. (2020), where a higher tier indicates more available resources, i.e., data, in the respective language.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Dataset</th><th>Size (Images)</th><th>Translated?</th><th></th></tr></thead><tbody><tr><td><strong>Natural Image:</strong></td><td></td><td></td><td></td></tr><tr><td>LLaVA Instruct <a href=https://arxiv.org/pdf/2501.05122.pdf target=_blank>Liu et al. (2023b)</a></td><td>160k</td><td>yes</td><td></td></tr><tr><td>VQAv2 <a href=https://arxiv.org/pdf/2501.05122.pdf target=_blank>Goyal et al. (2017)</a></td><td>83k</td><td>yes</td><td></td></tr><tr><td>GQA <a href=https://arxiv.org/pdf/2501.05122.pdf target=_blank>Hudson and Manning (2019)</a></td><td>72k</td><td>yes</td><td></td></tr><tr><td>OKVQA <a href=https://arxiv.org/pdf/2501.05122.pdf target=_blank>Marino et al. (2019)</a></td><td>9k</td><td>yes</td><td></td></tr><tr><td>A-OKVQA <a href=https://arxiv.org/pdf/2501.05122.pdf target=_blank>Schwenk et al. (2022)</a></td><td>30k</td><td>yes</td><td></td></tr><tr><td>RefCOCO <a href=https://arxiv.org/pdf/2501.05122.pdf target=_blank>Kazemzadeh et al. (2014); Mao et al. (2016)</a></td><td>48k</td><td>yes</td><td></td></tr><tr><td>VG <a href=https://arxiv.org/pdf/2501.05122.pdf target=_blank>Krishna et al. (2017)</a></td><td>86k</td><td>yes</td><td></td></tr><tr><td>MSCOCO <a href=https://arxiv.org/pdf/2501.05122.pdf target=_blank>Lin et al. (2014)</a></td><td>50k (subset)</td><td>yes</td><td></td></tr><tr><td><strong>Multiple Images:</strong></td><td></td><td></td><td></td></tr><tr><td>NLVR <a href=https://arxiv.org/pdf/2501.05122.pdf target=_blank>Suhr et al. (2019)</a></td><td>86k</td><td>yes</td><td></td></tr><tr><td>Spot-the-difference <a href=https://arxiv.org/pdf/2501.05122.pdf target=_blank>Jhamtani and Berg-Kirkpatrick (2018)</a></td><td>8k</td><td>yes</td><td></td></tr><tr><td><strong>OCR:</strong></td><td></td><td></td><td></td></tr><tr><td>OCRVQA <a href=https://arxiv.org/pdf/2501.05122.pdf target=_blank>Mishra et al. (2019)</a></td><td>50k (subset)</td><td>no</td><td></td></tr><tr><td>DocVQA <a href=https://arxiv.org/pdf/2501.05122.pdf target=_blank>Mathew et al. (2021)</a></td><td>10k</td><td>no</td><td></td></tr><tr><td>AI2D <a href=https://arxiv.org/pdf/2501.05122.pdf target=_blank>Kembhavi et al. (2016)</a></td><td>3k</td><td>no</td><td></td></tr><tr><td>ChartQA <a href=https://arxiv.org/pdf/2501.05122.pdf target=_blank>Masry et al. (2022)</a></td><td>18k</td><td>no</td><td></td></tr><tr><td>DVQA <a href=https://arxiv.org/pdf/2501.05122.pdf target=_blank>Kafle et al. (2018)</a></td><td>50k (subset)</td><td>no</td><td></td></tr><tr><td>ScienceQA <a href=https://arxiv.org/pdf/2501.05122.pdf target=_blank>Lu et al. (2022)</a></td><td>6k</td><td>no</td><td></td></tr><tr><td><strong>Total</strong></td><td>766k</td><td></td><td></td></tr></tbody></table></table></figure><blockquote><p>üîº This table lists the datasets used for the instruction-tuning phase of the experiments in the paper. The datasets are categorized into those containing natural images, those containing multiple images (where each data point includes several images), and those with OCR text. The table provides the name of each dataset, the number of unique images in the dataset, and whether machine translation was used to make the dataset multilingual. Note that for datasets with multiple images or text, only unique image examples are counted, so if multiple sentences pertain to a single image, it&rsquo;s still only counted as one image.</p><details><summary>read the caption</summary>Table 8: List of datasets included in the instruct tuning phase in our analysis experiments. All sizes are based on unique images; examples about the same image are packed into one sequence.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Dataset</th><th>Size (Images)</th><th>Translated?</th></tr></thead><tbody><tr><td><strong>Natural Image:</strong></td><td></td><td></td></tr><tr><td>ALLaVA Instruct<sup>1</sup> Chen et al. (2024a)</td><td>760k</td><td>yes</td></tr><tr><td>LVIS Instruct4V Wang et al. (2023)</td><td>223k</td><td>yes</td></tr><tr><td>Visual7W Zhu et al. (2016)</td><td>14k</td><td>no</td></tr><tr><td>VizWiz QA Gurari et al. (2018)</td><td>21k</td><td>no</td></tr><tr><td>TallyQA Acharya et al. (2019)</td><td>133k</td><td>yes</td></tr><tr><td>SketchyVQA Tu et al. (2023)</td><td>4k</td><td>yes</td></tr><tr><td>OODVQA Tu et al. (2023)</td><td>3k</td><td>no</td></tr><tr><td><strong>OCR:</strong></td><td></td><td></td></tr><tr><td>ScienceQA (Cambrian version)</td><td>6k</td><td>no</td></tr><tr><td>AI2D (Cambrian version)</td><td>4k</td><td>no</td></tr><tr><td>Rendered Text<sup>2</sup></td><td>10k</td><td>no</td></tr><tr><td>ScreenQA Hsiao et al. (2022)</td><td>33k</td><td>no</td></tr><tr><td>LLaVAR Zhang et al. (2023b)</td><td>20k</td><td>no</td></tr><tr><td>ArxivQA Li et al. (2024)</td><td>54k</td><td>no</td></tr><tr><td>Chart2Text Obeid and Hoque (2020)</td><td>25k</td><td>no</td></tr><tr><td>InfographicVQA Mathew et al. (2022)</td><td>2k</td><td>no</td></tr><tr><td>VisText Tang et al. (2023)</td><td>10k</td><td>no</td></tr><tr><td>TQA Kembhavi et al. (2017)</td><td>1k</td><td>no</td></tr><tr><td>STVQA Biten et al. (2019)</td><td>17k</td><td>no</td></tr><tr><td>TAT-QA Zhu et al. (2021)</td><td>2k</td><td>no</td></tr><tr><td>TabMWP Lu et al. (2023)</td><td>23k</td><td>no</td></tr><tr><td>HiTab Cheng et al. (2022)</td><td>2k</td><td>no</td></tr><tr><td>IconQA Lu et al. (2021b)</td><td>27k</td><td>no</td></tr><tr><td>VisualMRC Tanaka et al. (2021)</td><td>3k</td><td>no</td></tr><tr><td>RobuT Zhao et al. (2023)</td><td>113k</td><td>no</td></tr><tr><td>FinQA Chen et al. (2021)</td><td>5k</td><td>no</td></tr><tr><td><strong>Math & Code:</strong></td><td></td><td></td></tr><tr><td>WebSight Lauren√ßon et al. (2024b)</td><td>10k</td><td>yes</td></tr><tr><td>Design2Code Si et al. (2024)</td><td>0k</td><td>yes</td></tr><tr><td>DaTikz Belouadi et al. (2024)</td><td>48k</td><td>no</td></tr><tr><td>CLEVR Johnson et al. (2017)</td><td>70k</td><td>yes</td></tr><tr><td>CLEVR-Math Lindstr√∂m and Abraham (2022)</td><td>70k</td><td>yes</td></tr><tr><td>Geo170k Gao et al. (2023)</td><td>9k</td><td>no</td></tr><tr><td>GeomVerse Kazemi et al. (2023)</td><td>9k</td><td>no</td></tr><tr><td>Inter-GPS Lu et al. (2021a)</td><td>1k</td><td>no</td></tr><tr><td>MathVision Wang et al. (2024a)</td><td>3k</td><td>no</td></tr><tr><td>Raven Zhang et al. (2019)</td><td>42k</td><td>no</td></tr><tr><td><strong>Text (no images):</strong></td><td></td><td></td></tr><tr><td>Aya Dataset Singh et al. (2024)</td><td>202k</td><td>‚Äì</td></tr><tr><td>Tagengo-GPT4 Devine (2024)</td><td>70k</td><td>‚Äì</td></tr><tr><td>Magpie<sup>2</sup> Xu et al. (2024)</td><td>400k</td><td>‚Äì</td></tr><tr><td><strong>Total</strong></td><td>2.47M</td><td></td></tr></tbody></table></table></figure><blockquote><p>üîº Table 9 details the datasets used in the instruction tuning phase for the Centurio model. It builds upon the datasets listed in Table 8. The table shows the name of each additional dataset, the number of images in the dataset, and whether or not machine translation was used. Note that one dataset includes web-scraped images from LAION (which contain textual elements), and another dataset combines three separate subsets from different sources.</p><details><summary>read the caption</summary>Table 9: Datasets used on top of the datasets from Table¬†8 for the instruct tuning phase of Centurio. 1: also contains web-scraped images from LAION Schuhmann et¬†al. (2022) which contain textual elements. 2222:%****‚ê£A1_details.tex‚ê£Line‚ê£275‚ê£****https://huggingface.co/datasets/wendlerc/RenderedText. 2: Combining magpie-ultra-v0.1 (50k), Magpie-Qwen2-Pro-200K-English (200k), Magpie-Llama-3.1-Pro-MT-300K-Filtered (150k subset).</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Dataset</th><th>Task</th><th>Visual Input</th><th>Textual Input</th><th>Target Output</th><th>Metric</th><th>#Lang.</th></tr></thead><tbody><tr><td>MaXM</td><td>VQA</td><td>Single-Image</td><td>Question (TL)</td><td>WoP (TL)</td><td>E. Acc.</td><td>6</td></tr><tr><td>xGQA</td><td>VQA</td><td>Single-Image</td><td>Question (TL)</td><td>WoP (EN)</td><td>E. Acc.</td><td>8</td></tr><tr><td>XVNLI</td><td>VNLI</td><td>Single-Image</td><td>Hypothesis (TL)</td><td>‚Äòyes‚Äô / ‚Äôno‚Äô / ‚Äômaybe‚Äô</td><td>E. Acc.</td><td>5</td></tr><tr><td>M5B-VLOD</td><td>VLOD</td><td>Multi-Image</td><td>Hypothesis (TL)</td><td>LoC</td><td>R. Acc.</td><td>12</td></tr><tr><td>M5B-VGR</td><td>VGR</td><td>Multi-Image</td><td>Hypothesis (TL)</td><td>‚Äòyes‚Äô / ‚Äôno‚Äô</td><td>E. Acc.</td><td>12</td></tr><tr><td>MaRVL</td><td>VGR</td><td>Multi-Image</td><td>Hypothesis (TL)</td><td>‚Äòyes‚Äô / ‚Äôno‚Äô</td><td>E. Acc.</td><td>6</td></tr><tr><td>MTVQA</td><td>TH VQA</td><td>Single-Image</td><td>Question (TL)</td><td>WoP (TL)</td><td>E. Acc.</td><td>9</td></tr><tr><td>SMPQA - Name</td><td>TH VQA</td><td>Single-Image</td><td>Question (TL)</td><td>WoP (TL)</td><td>E. Acc.</td><td>11</td></tr><tr><td>SMPQA - Ground</td><td>TH VGR</td><td>Single-Image</td><td>Question (TL)</td><td>‚Äòyes‚Äô / ‚Äôno‚Äô</td><td>E. Acc.</td><td>11</td></tr><tr><td>M3Exam</td><td>TH MC VQA</td><td>Single or Multi-Image</td><td>Question (TL)</td><td>LoC</td><td>R. Acc.</td><td>7</td></tr><tr><td>MMMU</td><td>TH MC VQA</td><td>Single or Multi-Image</td><td>Question (EN)</td><td>LoC</td><td>R. Acc.</td><td>1</td></tr><tr><td>xMMMU</td><td>TH MC VQA</td><td>Single or Multi-Image</td><td>Question (TL)</td><td>LoC</td><td>R. Acc.</td><td>7</td></tr><tr><td>BabelImageNet-MC</td><td>MC VQA</td><td>Single-Image</td><td>Question (TL)</td><td>LoC</td><td>R. Acc.</td><td>20</td></tr><tr><td>CVQA</td><td>MC VQA</td><td>Single-Image</td><td>Question (TL)</td><td>LoC</td><td>R. Acc.</td><td>39</td></tr><tr><td>XM3600</td><td>Captioning</td><td>Single-Image</td><td>Prompt (EN)</td><td>Caption (TL)</td><td>CIDEr</td><td>36</td></tr></tbody></table></table></figure><blockquote><p>üîº Table 10 details the datasets used to evaluate the Centurio model&rsquo;s performance. It lists 15 vision-language datasets, specifying the task type (Visual Question Answering (VQA), Visual Natural Language Inference (VNLI), Visio-Linguistic Outlier Detection (VLOD), Visually Grounded Reasoning (VGR), Text-Heavy (TH), and Multiple-Choice (MC)), the type of visual input (single image, multiple images), the textual input, target output (single word or phrase (WoP), Letter of Correct Choice (LoC), in Target Language (TL), or in English (EN)), and evaluation metric (Exact Accuracy (E. Acc.) or Relaxed Accuracy (R. Acc.)). The table notes that CVQA is excluded from section 2 of the paper because its test set is not publicly available.</p><details><summary>read the caption</summary>Table 10: List of datasets contained in our test suite. In the Task column, ‚ÄùVQA‚Äù ‚ÄùVNLI‚Äù, ‚ÄùVLOD‚Äù, ‚ÄùVGR‚Äù, ‚ÄùTH‚Äù, and ‚ÄùMC‚Äù are acronyms for ‚ÄùVisual Question Answering‚Äù, ‚ÄùVisual Natural Language Inference‚Äù, ‚ÄùVisio-Linguistic Outlier Detection‚Äù, ‚ÄùVisually Grounded Reasoning‚Äù, ‚ÄùText-Heavy‚Äù, and ‚ÄùMultiple-Choice‚Äù, respectively. In the ‚ÄùTextual Input‚Äù and ‚ÄùTarget Output‚Äù columns, the acronyms ‚ÄùWoP‚Äù, ‚ÄùLoC‚Äù, ‚ÄùTL‚Äù, and ‚ÄùEN‚Äù stand for ‚Äù(Single) Word or Phrase‚Äù, ‚ÄùLetter of the correct Choice‚Äù, ‚ÄùTarget Language‚Äù, and ‚ÄùEnglish‚Äù, respectively. Further, ‚ÄùE. Acc.‚Äù is ‚ÄùExact Accuracy‚Äù and ‚ÄùR. Acc.‚Äù is ‚ÄùRelaxed Accuracy‚Äù. CVQA is not used in ¬ß2 due to its hidden test set with limited submissions.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Name</th><th>Tier</th><th>ISO-639-3</th><th>ISO-639-1</th><th>Datasets</th></tr></thead><tbody><tr><td>Afrikaans</td><td>3</td><td>afr</td><td>af</td><td>BabelImageNet-MC, M3Exam</td></tr><tr><td>Amharic</td><td>2</td><td>amh</td><td>am</td><td>BabelImageNet-MC, CVQA, M5B-VGR, M5B-VLOD</td></tr><tr><td>Arabic</td><td>5</td><td>ara</td><td>ar</td><td>MTVQA, SMPQA, XM3600, xMMMU, XVNLI</td></tr><tr><td>Bengali</td><td>3</td><td>ben</td><td>bn</td><td>CVQA, M5B-VGR, M5B-VLOD, xGQA, XM3600</td></tr><tr><td>Berber (macrolanguage)</td><td>0</td><td>ber</td><td>-</td><td>M5B-VGR, M5B-VLOD</td></tr><tr><td>Breton</td><td>1</td><td>bre</td><td>br</td><td>CVQA</td></tr><tr><td>Bulgarian</td><td>3</td><td>bul</td><td>bg</td><td>CVQA</td></tr><tr><td>Chinese</td><td>5</td><td>zho</td><td>zh</td><td>CVQA, M3Exam, MaRVL, MaXM, SMPQA, xGQA, XM3600</td></tr><tr><td>Croatian</td><td>4</td><td>hrv</td><td>hr</td><td>BabelImageNet-MC, XM3600</td></tr><tr><td>Cusco Quechua</td><td>1</td><td>quz</td><td>-</td><td>XM3600</td></tr><tr><td>Czech</td><td>4</td><td>ces</td><td>cs</td><td>BabelImageNet-MC, XM3600</td></tr><tr><td>Danish</td><td>3</td><td>dan</td><td>da</td><td>XM3600</td></tr><tr><td>Dutch</td><td>4</td><td>nld</td><td>nl</td><td>BabelImageNet-MC, XM3600</td></tr><tr><td>Egyptian Arabic</td><td>3</td><td>arz</td><td>-</td><td>CVQA</td></tr><tr><td>English</td><td>5</td><td>eng</td><td>en</td><td>BabelImageNet-MC, M3Exam, M5B-VGR, M5B-VLOD, MaRVL, MaXM, MME, MMMU, SMPQA, xGQA, XM3600, xMMMU, XVNLI</td></tr><tr><td>Filipino</td><td>3</td><td>fil</td><td>-</td><td>CVQA, M5B-VGR, M5B-VLOD, XM3600</td></tr><tr><td>Finnish</td><td>4</td><td>fin</td><td>fi</td><td>BabelImageNet-MC, XM3600</td></tr><tr><td>French</td><td>5</td><td>fra</td><td>fr</td><td>MaXM, MTVQA, XM3600, xMMMU, XVNLI</td></tr><tr><td>German</td><td>5</td><td>deu</td><td>de</td><td>M5B-VGR, M5B-VLOD, MTVQA, SMPQA, xGQA, XM3600</td></tr><tr><td>Hausa</td><td>2</td><td>hau</td><td>ha</td><td>BabelImageNet-MC, M5B-VGR, M5B-VLOD</td></tr><tr><td>Hebrew</td><td>3</td><td>heb</td><td>he</td><td>XM3600</td></tr><tr><td>Hindi</td><td>4</td><td>hin</td><td>hi</td><td>M5B-VGR, M5B-VLOD, MaXM, SMPQA, XM3600, xMMMU</td></tr><tr><td>Hungarian</td><td>4</td><td>hun</td><td>hu</td><td>BabelImageNet-MC, XM3600</td></tr><tr><td>Igbo</td><td>1</td><td>ibo</td><td>ig</td><td>CVQA</td></tr><tr><td>Indonesian</td><td>3</td><td>ind</td><td>id</td><td>CVQA, MaRVL, SMPQA, xGQA, XM3600, xMMMU</td></tr><tr><td>Irish</td><td>2</td><td>gle</td><td>ga</td><td>CVQA</td></tr><tr><td>Italian</td><td>4</td><td>ita</td><td>it</td><td>M3Exam, MTVQA, SMPQA, XM3600</td></tr><tr><td>Japanese</td><td>5</td><td>jpn</td><td>ja</td><td>BabelImageNet-MC, CVQA, MTVQA, XM3600, xMMMU</td></tr><tr><td>Javanese</td><td>1</td><td>jav</td><td>jv</td><td>CVQA</td></tr><tr><td>Kanuri</td><td>0</td><td>kau</td><td>kr</td><td>CVQA</td></tr><tr><td>Kinyarwanda</td><td>1</td><td>kin</td><td>rw</td><td>CVQA</td></tr><tr><td>Korean</td><td>4</td><td>kor</td><td>ko</td><td>CVQA, SMPQA, xGQA, XM3600</td></tr><tr><td>Malay (macrolanguage)</td><td>3</td><td>msa</td><td>ms</td><td>CVQA</td></tr><tr><td>Maori</td><td>1</td><td>mri</td><td>mi</td><td>BabelImageNet-MC, XM3600</td></tr><tr><td>Mi-gkabau</td><td>1</td><td>min</td><td>-</td><td>CVQA</td></tr><tr><td>Modern Greek</td><td>3</td><td>ell</td><td>el</td><td>BabelImageNet-MC, XM3600</td></tr><tr><td>Mongolian</td><td>1</td><td>mon</td><td>mn</td><td>CVQA</td></tr><tr><td>Norwegian</td><td>1</td><td>nor</td><td>no</td><td>BabelImageNet-MC, CVQA, XM3600</td></tr><tr><td>Oromo</td><td>1</td><td>orm</td><td>om</td><td>CVQA</td></tr><tr><td>Persian</td><td>4</td><td>fas</td><td>fa</td><td>BabelImageNet-MC, XM3600</td></tr><tr><td>Polish</td><td>4</td><td>pol</td><td>pl</td><td>BabelImageNet-MC, XM3600</td></tr><tr><td>Portuguese</td><td>4</td><td>por</td><td>pt</td><td>CVQA, M3Exam, xGQA, XM3600, xMMMU</td></tr><tr><td>Romanian</td><td>3</td><td>ron</td><td>ro</td><td>BabelImageNet-MC, CVQA, MaXM, XM3600</td></tr><tr><td>Russian</td><td>4</td><td>rus</td><td>ru</td><td>CVQA, M5B-VGR, M5B-VLOD, MTVQA, SMPQA, xGQA, XM3600, XVNLI</td></tr><tr><td>Sinhala</td><td>0</td><td>sin</td><td>si</td><td>CVQA</td></tr><tr><td>Spanish</td><td>5</td><td>spa</td><td>es</td><td>BabelImageNet-MC, CVQA, XM3600, XVNLI</td></tr><tr><td>Sundanese</td><td>1</td><td>sun</td><td>su</td><td>CVQA</td></tr><tr><td>Swahili (macrolanguage)</td><td>2</td><td>swa</td><td>sw</td><td>CVQA, M5B-VGR, M5B-VLOD, MaRVL, XM3600</td></tr><tr><td>Swedish</td><td>4</td><td>swe</td><td>sv</td><td>XM3600</td></tr><tr><td>Tamil</td><td>3</td><td>tam</td><td>ta</td><td>BabelImageNet-MC, CVQA, MaRVL</td></tr><tr><td>Telugu</td><td>1</td><td>tel</td><td>te</td><td>BabelImageNet-MC, CVQA, XM3600</td></tr><tr><td>Thai</td><td>3</td><td>tha</td><td>th</td><td>M3Exam, M5B-VGR, M5B-VLOD, MaXM, MTVQA, SMPQA, XM3600</td></tr><tr><td>Turkish</td><td>4</td><td>tur</td><td>tr</td><td>MaRVL, XM3600</td></tr><tr><td>Ukrainian</td><td>3</td><td>ukr</td><td>uk</td><td>XM3600</td></tr><tr><td>Urdu</td><td>3</td><td>urd</td><td>ur</td><td>CVQA</td></tr><tr><td>Vietnamese</td><td>4</td><td>vie</td><td>vi</td><td>M3Exam, MTVQA, XM3600</td></tr><tr><td>Zulu</td><td>2</td><td>zul</td><td>zu</td><td>BabelImageNet-MC, M5B-VGR, M5B-VLOD, SMPQA</td></tr><tr><td>Unique Languages</td><td></td><td></td><td></td><td>56 (43 without CVQA)</td></tr></tbody></table></table></figure><blockquote><p>üîº This table lists the 56 languages used in the evaluation of the Centurio model, categorized by their resource tier according to the Joshi et al. (2020) taxonomy. Tier 5 represents high-resource languages with ample available data, while Tier 1 indicates low-resource languages with limited resources. The table also notes that the CVQA dataset was excluded from the analysis in Section 2 due to its closed-off test set and limited submission opportunities.</p><details><summary>read the caption</summary>Table 11: List of languages covered in the datasets of our test suite. The ‚ÄùTier‚Äù column represents the tier in the taxonomy proposed by Joshi et¬†al. (2020), where a higher tier indicates more available resources, i.e., data, in the respective language. CVQA is not used in ¬ß2 due to its hidden test set with limited submissions.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>HuggingFace Model ID</th><th>Params</th></tr></thead><tbody><tr><td><a href=https://www.huggingface.com/Qwen/Qwen2-VL-2B-Instruct target=_blank>Qwen/Qwen2-VL-2B-Instruct</a> [2024c]</td><td>2B</td></tr><tr><td><a href=https://www.huggingface.com/Qwen/Qwen2-VL-7B-Instruct target=_blank>Qwen/Qwen2-VL-7B-Instruct</a> [2024c]</td><td>7B</td></tr><tr><td><a href=https://www.huggingface.com/microsoft/Phi-3.5-vision-instruct target=_blank>microsoft/Phi-3.5-vision-instruct</a> [2024a]</td><td>4B</td></tr><tr><td><a href=https://www.huggingface.com/neulab/Pangea-7B-hf target=_blank>neulab/Pangea-7B-hf</a> [2024b]</td><td>7B</td></tr><tr><td><a href=https://www.huggingface.com/openbmb/MiniCPM-V-2_6 target=_blank>openbmb/MiniCPM-V-2_6</a> [2024b]</td><td>8B</td></tr><tr><td><a href=https://www.huggingface.com/meta-llama/Llama-3.2-11B-Vision-Instruct target=_blank>meta-llama/Llama-3.2-11B-Vision-Instruct</a> [2024]</td><td>11B</td></tr><tr><td><a href=https://www.huggingface.com/mistralai/Pixtral-12B-2409 target=_blank>mistralai/Pixtral-12B-2409</a> [2024]</td><td>12B</td></tr><tr><td><a href=https://www.huggingface.com/AIDC-AI/Parrot-7B target=_blank>AIDC-AI/Parrot-7B</a> [2024b]</td><td>7B</td></tr><tr><td><a href=https://www.huggingface.com/MBZUAI/PALO-7B target=_blank>MBZUAI/PALO-7B</a> [2024a]</td><td>7B</td></tr><tr><td><a href=https://www.huggingface.com/MBZUAI/PALO-13B target=_blank>MBZUAI/PALO-13B</a> [2024a]</td><td>13B</td></tr><tr><td><a href=https://www.huggingface.com/OpenGVLab/InternVL2_5-4B target=_blank>OpenGVLab/InternVL2_5-4B</a> [2024e]</td><td>4B</td></tr><tr><td><a href=https://www.huggingface.com/OpenGVLab/InternVL2_5-8B target=_blank>OpenGVLab/InternVL2_5-8B</a> [2024e]</td><td>8B</td></tr><tr><td><a href=https://www.huggingface.com/maya-multimodal/maya target=_blank>maya-multimodal/maya</a> [2024]</td><td>8B</td></tr></tbody></table></table></figure><blockquote><p>üîº This table lists the large vision-language models (LVLMs) used in the paper&rsquo;s experiments to evaluate the models&rsquo; multilingual capabilities. The models are listed along with their sizes (number of parameters), allowing for a comparison of performance across models of different scales. The table is crucial for understanding which LVLMs were considered and used as baselines for comparison against the Centurio model developed in the paper.</p><details><summary>read the caption</summary>Table 12: List of models considered in our evaluation experiments.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Train Lang.</th><th>T1</th><th>T2</th><th>T3</th><th>T4</th><th>T5</th><th>en</th></tr></thead><tbody><tr><td>English</td><td>16.1</td><td>34.7</td><td>26.3</td><td>24.3</td><td>26.2</td><td>56.4</td></tr><tr><td>T5</td><td>19.1</td><td>32.5</td><td>29.3</td><td>27.2</td><td>35.5</td><td>54.3</td></tr><tr><td>L100</td><td>31.1</td><td>43.0</td><td>39.4</td><td>35.9</td><td>36.4</td><td>56.6</td></tr><tr><td><strong>Without tasks affected by language fidelity:</strong></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>English</td><td>36.6</td><td>37.1</td><td>39.0</td><td>39.6</td><td>40.0</td><td>54.6</td></tr><tr><td>T5</td><td>38.8</td><td>34.8</td><td>40.1</td><td>40.2</td><td>40.4</td><td>53.5</td></tr><tr><td>L100</td><td>46.3</td><td>44.0</td><td>45.0</td><td>42.8</td><td>42.9</td><td>55.3</td></tr></tbody></table></table></figure><blockquote><p>üîº This table replicates the results from Table 1, but uses the Llama 3 language model instead of Phi 3.5. It compares the performance of models trained with only English, the top 6 high-resource languages (T5), and all 100 languages (L100) across multiple downstream tasks. This allows for analysis of the impact of increasing the number of training languages on model performance.</p><details><summary>read the caption</summary>Table 13: Experimental setup of Table¬†1 repeated with Llama 3 and the setups: just English, T5 languages, and L100 languages.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>English %</th><th>T1</th><th>T2</th><th>T3</th><th>T4</th><th>T5</th><th>en</th></tr></thead><tbody><tr><td>10</td><td><strong>32.9</strong></td><td><strong>43.1</strong></td><td><strong>38.7</strong></td><td><strong>35.4</strong></td><td>35.4</td><td>54.2</td></tr><tr><td>50</td><td><strong>31.1</strong></td><td><strong>43.0</strong></td><td><strong>39.4</strong></td><td><strong>35.9</strong></td><td><strong>36.4</strong></td><td><strong>56.6</strong></td></tr><tr><td>90</td><td>26.9</td><td>38.7</td><td>36.9</td><td>34.2</td><td><strong>35.8</strong></td><td><strong>56.6</strong></td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents the results of experiments investigating the effect of different proportions of English and multilingual data in the instruction-tuning phase of training a large vision-language model (LLM). The experiment setup replicates that of Table 2 but uses the Llama 3 LLM. It systematically varies the percentage of English data (10%, 50%, and 90%), while keeping the multilingual portion constant, and evaluates performance across multiple language tiers and tasks. The table provides insights into the optimal balance between English and multilingual data for instruction tuning in multilingual LVLMs, highlighting the impact of different data distributions on the overall performance.</p><details><summary>read the caption</summary>Table 14: Experimental setup of Table¬†2 repeated with Llama 3 and the setups: 10, 50, and 90% English instruct tune data.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>English %</th><th>T1</th><th>T2</th><th>T3</th><th>T4</th><th>T5</th><th>en</th></tr></thead><tbody><tr><td>No pretrain</td><td>31.1</td><td>43.0</td><td>39.4</td><td>35.9</td><td>36.4</td><td>56.6</td></tr><tr><td>100</td><td>33.9</td><td>44.7</td><td>43.3</td><td>39.9</td><td>39.9</td><td>60.8</td></tr><tr><td>1</td><td>37.8</td><td>47.4</td><td>45.0</td><td>41.1</td><td>40.7</td><td>61.4</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents the results of experiments evaluating the impact of different English-to-multilingual ratios in pre-training data on the performance of a large vision-language model (LLM). It expands upon the findings of Table 3, specifically showing how performance varies when pre-training is performed using either 1% or 100% English data. The model used is Llama 3, and the results are presented for various language tiers (T1-T5), and the English performance is also included as a separate metric.</p><details><summary>read the caption</summary>Table 15: Results of Table¬†3 repeated with Llama 3 and the setups: 1 and 100% English pre-train data.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Distribution</th><th>T1</th><th>T2</th><th>T3</th><th>T4</th><th>T5</th><th>en</th></tr></thead><tbody><tr><td>Uniform</td><td>18.9</td><td>32.6</td><td>30.7</td><td>28.8</td><td>34.4</td><td>52.6</td></tr><tr><td>Stratified-1</td><td>18.6</td><td>32.5</td><td>30.7</td><td>28.0</td><td>33.8</td><td>53.0</td></tr><tr><td>Stratified-2</td><td>19.2</td><td>32.6</td><td>29.5</td><td>27.4</td><td>33.9</td><td>52.0</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents a comparative analysis of three different language data allocation strategies for training a multilingual vision-language model. The strategies are: a uniform distribution across all languages, a stratified distribution that gives more weight to low-resource languages (Stratified-1), and another stratified distribution that gives even more weight to low-resource languages (Stratified-2). The table shows the performance of models trained with these different strategies on multiple evaluation tasks, allowing researchers to understand the effects of varying language data distributions on overall multilingual model performance.</p><details><summary>read the caption</summary>Table 16: Comparison between our uniform allocation of data compared to two stratified allocations that upsample low-resource languages.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>LLM</th><th>T1</th><th>T2</th><th>T3</th><th>T4</th><th>T5</th><th>en</th></tr></thead><tbody><tr><td>Phi-3.5-mini-instruct</td><td>18.9</td><td>32.6</td><td>30.7</td><td>28.8</td><td>34.4</td><td>52.6</td></tr><tr><td>gemma-2-9b-it</td><td>29.2</td><td>40.9</td><td>36.4</td><td>33.5</td><td>35.3</td><td>52.8</td></tr><tr><td>Meta-Llama-3-8B-Instruct</td><td><strong>31.1</strong></td><td>43.0</td><td>39.4</td><td>35.9</td><td>36.4</td><td>56.6</td></tr><tr><td>Qwen2.5-7B-Instruct</td><td>30.7</td><td><strong>43.7</strong></td><td>42.0</td><td>38.1</td><td>40.5</td><td><strong>62.7</strong></td></tr><tr><td>aya-expanse-8b</td><td>28.3</td><td>42.5</td><td><strong>43.0</strong></td><td><strong>39.8</strong></td><td><strong>40.9</strong></td><td>59.9</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents a comparison of the performance achieved by different large language models (LLMs) after being fine-tuned using instruction tuning data. The key characteristic of this fine-tuning is that it involves 100 languages and a training data composition where 50% is in English, with the other 50% distributed equally across the remaining 99 languages. The models are evaluated across different language tiers (T1-T5), allowing for an assessment of performance across varying resource levels for different languages. The results highlight the impact of different LLM architectures on multilingual performance under these specific training conditions.</p><details><summary>read the caption</summary>Table 17: Comparison between different LLM backbones all trained with the instruct tuning data with L100 languages and 50% English (as in ¬ß2.3).</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>English</th><th>avg.</th><th>af</th><th>am</th><th>cs</th><th>el</th><th>es</th><th>fa</th><th>fi</th><th>ha</th><th>hr</th><th>hu</th><th>ja</th><th>mi</th><th>nl</th><th>no</th><th>pl</th><th>ro</th><th>ta</th><th>te</th><th>zu</th></tr></thead><tbody><tr><td>Phi 3.5 - English</td><td>64.7</td><td>38.1</td><td>43.3</td><td>29.7</td><td>41.5</td><td>35.5</td><td>55.9</td><td>33.6</td><td>36.4</td><td>24.5</td><td>43.3</td><td>39.0</td><td>49.8</td><td>27.8</td><td>47.3</td><td>44.2</td><td>41.4</td><td>42.8</td><td>30.0</td><td>27.1</td></tr><tr><td>Phi 3.5 - T5 50</td><td>66.0</td><td>39.6</td><td>46.0</td><td>30.3</td><td>43.1</td><td>36.3</td><td>56.3</td><td>33.4</td><td>36.5</td><td>35.1</td><td>45.1</td><td>40.7</td><td>50.9</td><td>30.1</td><td>48.4</td><td>46.2</td><td>41.1</td><td>43.1</td><td>31.0</td><td>29.6</td></tr><tr><td>Phi 3.5 - T5-4 50</td><td>65.2</td><td>40.6</td><td>46.8</td><td>29.6</td><td>44.6</td><td>37.9</td><td>59.1</td><td>36.7</td><td>37.5</td><td>29.0</td><td>46.4</td><td>42.5</td><td>52.0</td><td>31.1</td><td>50.7</td><td>47.4</td><td>43.0</td><td>43.5</td><td>31.5</td><td>29.0</td></tr><tr><td>Phi 3.5 - T5-3 50</td><td>65.5</td><td>40.6</td><td>50.0</td><td>28.8</td><td>43.3</td><td>37.4</td><td>58.6</td><td>34.4</td><td>38.4</td><td>33.0</td><td>46.1</td><td>41.4</td><td>50.9</td><td>31.2</td><td>49.7</td><td>47.0</td><td>41.9</td><td>43.8</td><td>32.5</td><td>29.6</td></tr><tr><td>Phi 3.5 - T5-2 50</td><td>64.8</td><td>39.1</td><td>47.2</td><td>25.6</td><td>41.9</td><td>35.8</td><td>57.9</td><td>34.0</td><td>36.0</td><td>29.8</td><td>44.8</td><td>39.5</td><td>50.0</td><td>30.5</td><td>47.7</td><td>45.8</td><td>41.2</td><td>42.4</td><td>30.4</td><td>29.2</td></tr><tr><td>Phi 3.5 - L100 50</td><td>64.7</td><td>39.9</td><td>48.1</td><td>28.2</td><td>42.8</td><td>36.8</td><td>57.2</td><td>34.7</td><td>37.0</td><td>28.2</td><td>44.7</td><td>40.4</td><td>51.2</td><td>31.6</td><td>47.8</td><td>46.4</td><td>40.9</td><td>43.8</td><td>30.6</td><td>30.1</td></tr><tr><td>Llama 3 - English</td><td>65.4</td><td>40.9</td><td>44.0</td><td>28.2</td><td>46.9</td><td>42.2</td><td>53.0</td><td>42.4</td><td>38.7</td><td>31.1</td><td>47.6</td><td>46.3</td><td>48.6</td><td>30.1</td><td>48.2</td><td>47.4</td><td>44.0</td><td>44.9</td><td>31.6</td><td>32.5</td></tr><tr><td>Llama 3 - T5 50</td><td>63.9</td><td>43.7</td><td>50.6</td><td>28.7</td><td>49.2</td><td>46.4</td><td>54.6</td><td>46.6</td><td>41.7</td><td>35.4</td><td>50.7</td><td>50.8</td><td>51.9</td><td>30.0</td><td>51.2</td><td>50.9</td><td>47.0</td><td>48.4</td><td>31.1</td><td>35.6</td></tr><tr><td>Llama 3 - L100 50</td><td>66.2</td><td>48.8</td><td>55.3</td><td>35.1</td><td>54.2</td><td>51.2</td><td>56.2</td><td>47.6</td><td>46.2</td><td>37.2</td><td>56.1</td><td>54.1</td><td>53.3</td><td>33.7</td><td>54.6</td><td>54.3</td><td>50.8</td><td>51.9</td><td>43.6</td><td>50.9</td></tr><tr><td>Phi 3.5 - L100 1</td><td>63.1</td><td>39.7</td><td>47.4</td><td>26.8</td><td>42.9</td><td>36.7</td><td>56.2</td><td>34.3</td><td>35.9</td><td>33.5</td><td>46.8</td><td>40.5</td><td>49.0</td><td>32.7</td><td>48.4</td><td>46.7</td><td>41.3</td><td>43.1</td><td>29.0</td><td>29.9</td></tr><tr><td>Phi 3.5 - L100 10</td><td>62.7</td><td>39.4</td><td>47.1</td><td>27.1</td><td>43.1</td><td>36.8</td><td>56.5</td><td>34.4</td><td>36.5</td><td>29.3</td><td>43.8</td><td>40.9</td><td>49.8</td><td>29.8</td><td>47.2</td><td>48.2</td><td>41.4</td><td>43.6</td><td>30.2</td><td>28.0</td></tr><tr><td>Phi 3.5 - L100 24</td><td>63.3</td><td>40.4</td><td>48.0</td><td>29.0</td><td>43.3</td><td>37.7</td><td>56.5</td><td>35.2</td><td>36.7</td><td>32.4</td><td>46.9</td><td>40.7</td><td>50.7</td><td>33.0</td><td>49.3</td><td>47.2</td><td>41.8</td><td>44.4</td><td>31.9</td><td>31.2</td></tr><tr><td>Phi 3.5 - L100 50</td><td>64.7</td><td>39.9</td><td>48.1</td><td>28.2</td><td>42.8</td><td>36.8</td><td>57.2</td><td>34.7</td><td>37.0</td><td>28.2</td><td>44.7</td><td>40.4</td><td>51.2</td><td>31.6</td><td>47.8</td><td>46.4</td><td>40.9</td><td>43.8</td><td>30.6</td><td>30.1</td></tr><tr><td>Phi 3.5 - L100 75</td><td>65.4</td><td>39.8</td><td>47.1</td><td>26.0</td><td>42.0</td><td>37.1</td><td>57.4</td><td>34.7</td><td>36.9</td><td>32.2</td><td>44.2</td><td>40.4</td><td>51.3</td><td>31.5</td><td>49.4</td><td>46.8</td><td>41.6</td><td>42.9</td><td>31.1</td><td>28.5</td></tr><tr><td>Phi 3.5 - L100 90</td><td>64.7</td><td>37.5</td><td>43.8</td><td>24.1</td><td>40.3</td><td>35.8</td><td>57.1</td><td>31.8</td><td>35.7</td><td>25.0</td><td>43.1</td><td>39.2</td><td>49.1</td><td>24.9</td><td>47.9</td><td>44.4</td><td>39.3</td><td>42.8</td><td>28.7</td><td>27.7</td></tr><tr><td>Llama 3 - L100 10</td><td>65.9</td><td>49.8</td><td>58.4</td><td>38.1</td><td>55.0</td><td>50.9</td><td>58.5</td><td>49.3</td><td>45.7</td><td>40.7</td><td>59.4</td><td>56.3</td><td>54.1</td><td>34.9</td><td>53.7</td><td>56.8</td><td>51.8</td><td>51.3</td><td>42.6</td><td>51.9</td></tr><tr><td>Llama 3 - L100 50</td><td>66.2</td><td>48.8</td><td>55.3</td><td>35.1</td><td>54.2</td><td>51.2</td><td>56.2</td><td>47.6</td><td>46.2</td><td>37.2</td><td>56.1</td><td>54.1</td><td>53.3</td><td>33.7</td><td>54.6</td><td>54.3</td><td>50.8</td><td>51.9</td><td>43.6</td><td>50.9</td></tr><tr><td>Llama 3 - L100 90</td><td>64.4</td><td>45.3</td><td>52.5</td><td>26.8</td><td>51.0</td><td>47.2</td><td>54.8</td><td>45.9</td><td>44.0</td><td>29.5</td><td>54.1</td><td>50.0</td><td>51.2</td><td>31.3</td><td>52.1</td><td>51.8</td><td>48.6</td><td>49.8</td><td>36.5</td><td>48.3</td></tr><tr><td>Phi 3.5 - L100 50</td><td>64.7</td><td>39.9</td><td>48.1</td><td>28.2</td><td>42.8</td><td>36.8</td><td>57.2</td><td>34.7</td><td>37.0</td><td>28.2</td><td>44.7</td><td>40.4</td><td>51.2</td><td>31.6</td><td>47.8</td><td>46.4</td><td>40.9</td><td>43.8</td><td>30.6</td><td>30.1</td></tr><tr><td>Phi 3.5 - PT 100</td><td>66.3</td><td>38.9</td><td>48.4</td><td>25.0</td><td>42.7</td><td>36.0</td><td>57.3</td><td>33.2</td><td>36.5</td><td>22.3</td><td>44.8</td><td>39.8</td><td>49.9</td><td>31.3</td><td>48.6</td><td>46.4</td><td>41.3</td><td>43.2</td><td>30.5</td><td>30.8</td></tr><tr><td>Phi 3.5 - PT 50</td><td>65.7</td><td>42.2</td><td>50.0</td><td>37.8</td><td>44.2</td><td>40.0</td><td>57.8</td><td>36.0</td><td>36.5</td><td>33.0</td><td>45.2</td><td>41.8</td><td>49.3</td><td>35.0</td><td>49.0</td><td>48.1</td><td>42.0</td><td>44.1</td><td>33.7</td><td>37.7</td></tr><tr><td>Phi 3.5 - PT 1</td><td>65.8</td><td>42.8</td><td>50.1</td><td>35.1</td><td>44.8</td><td>38.9</td><td>56.9</td><td>37.9</td><td>37.5</td><td>41.2</td><td>49.1</td><td>42.1</td><td>49.6</td><td>33.4</td><td>49.6</td><td>48.2</td><td>43.6</td><td>45.9</td><td>34.9</td><td>36.1</td></tr><tr><td>Llama 3 - L100 50</td><td>66.2</td><td>48.8</td><td>55.3</td><td>35.1</td><td>54.2</td><td>51.2</td><td>56.2</td><td>47.6</td><td>46.2</td><td>37.2</td><td>56.1</td><td>54.1</td><td>53.3</td><td>33.7</td><td>54.6</td><td>54.3</td><td>50.8</td><td>51.9</td><td>43.6</td><td>50.9</td></tr><tr><td>Llama 3 - PT 1</td><td>69.6</td><td>55.5</td><td>62.4</td><td>44.0</td><td>60.4</td><td>60.4</td><td>62.9</td><td>55.3</td><td>51.7</td><td>40.4</td><td>63.0</td><td>62.1</td><td>59.9</td><td>36.6</td><td>59.4</td><td>62.1</td><td>58.0</td><td>58.6</td><td>50.6</td><td>60.6</td></tr><tr><td>Llama 3 - PT 100</td><td>68.7</td><td>53.6</td><td>63.4</td><td>36.8</td><td>59.6</td><td>58.1</td><td>62.5</td><td>54.1</td><td>50.8</td><td>37.5</td><td>63.1</td><td>61.6</td><td>60.7</td><td>36.9</td><td>59.9</td><td>61.0</td><td>58.0</td><td>58.0</td><td>46.5</td><td>54.0</td></tr><tr><td>Gemma 2 - L100 50</td><td>60.5</td><td>44.8</td><td>49.1</td><td>42.5</td><td>47.5</td><td>45.3</td><td>52.0</td><td>44.8</td><td>41.6</td><td>30.9</td><td>50.7</td><td>47.6</td><td>51.4</td><td>32.8</td><td>49.8</td><td>51.1</td><td>47.2</td><td>47.5</td><td>41.8</td><td>45.1</td></tr><tr><td>Llama 3 - L100 50</td><td>66.2</td><td>48.8</td><td>55.3</td><td>35.1</td><td>54.2</td><td>51.2</td><td>56.2</td><td>47.6</td><td>46.2</td><td>37.2</td><td>56.1</td><td>54.1</td><td>53.3</td><td>33.7</td><td>54.6</td><td>54.3</td><td>50.8</td><td>51.9</td><td>43.6</td><td>50.9</td></tr><tr><td>Qwen 2.5 - L100 50</td><td>68.2</td><td>50.6</td><td>62.4</td><td>37.1</td><td>57.9</td><td>50.8</td><td>63.4</td><td>49.6</td><td>42.6</td><td>28.7</td><td>61.0</td><td>48.3</td><td>63.1</td><td>33.5</td><td>58.8</td><td>58.2</td><td>57.2</td><td>55.4</td><td>36.8</td><td>55.6</td></tr><tr><td>Aya-Expanse - L100 50</td><td>67.6</td><td>52.0</td><td>62.2</td><td>31.0</td><td>65.3</td><td>65.5</td><td>63.2</td><td>58.9</td><td>39.8</td><td>33.2</td><td>60.8</td><td>46.3</td><td>65.1</td><td>33.1</td><td>61.3</td><td>55.5</td><td>60.2</td><td>61.9</td><td>43.5</td><td>43.2</td></tr><tr><td><code>Centurio</code> Aya</td><td>69.7</td><td>54.7</td><td>63.6</td><td>29.4</td><td>66.2</td><td>67.8</td><td>65.1</td><td>60.0</td><td>43.3</td><td>37.5</td><td>63.6</td><td>49.8</td><td>66.7</td><td>37.0</td><td>62.4</td><td>59.1</td><td>62.6</td><td>64.0</td><td>46.9</td><td>50.9</td></tr><tr><td><code>Centurio</code> Qwen</td><td>72.7</td><td>56.2</td><td>65.3</td><td>47.4</td><td>62.2</td><td>56.7</td><td>67.0</td><td>53.6</td><td>48.8</td><td>36.7</td><td>65.4</td><td>54.1</td><td>67.6</td><td>39.1</td><td>63.7</td><td>63.6</td><td>60.4</td><td>58.5</td><td>45.2</td><td>63.4</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents the results of the BIN-MC (Babel ImageNet Multiple Choice) task, which evaluates the model&rsquo;s ability to correctly identify objects in images across various languages. It shows the accuracy scores for different models trained with varying numbers of languages, and different data composition strategies. The scores are shown per language and averaged, allowing for a comparison of performance based on language type and training setup.</p><details><summary>read the caption</summary>Table 18: BIN-MC</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th style=text-align:left></th><th style=text-align:center>en</th><th style=text-align:center>avg.</th><th style=text-align:center>af</th><th style=text-align:center>zh</th><th style=text-align:center>it</th><th style=text-align:center>pt</th><th style=text-align:center>th</th><th style=text-align:center>vi</th></tr></thead><tbody><tr><td style=text-align:left>Phi 3.5 - English</td><td style=text-align:center>52.9</td><td style=text-align:center>32.7</td><td style=text-align:center>32.5</td><td style=text-align:center>37.0</td><td style=text-align:center>49.6</td><td style=text-align:center>39.7</td><td style=text-align:center>25.4</td><td style=text-align:center>12.2</td></tr><tr><td style=text-align:left>Phi 3.5 - T5 50</td><td style=text-align:center>51.2</td><td style=text-align:center>35.3</td><td style=text-align:center>39.9</td><td style=text-align:center>35.9</td><td style=text-align:center>46.4</td><td style=text-align:center>39.7</td><td style=text-align:center>28.2</td><td style=text-align:center>21.7</td></tr><tr><td style=text-align:left>Phi 3.5 - T5-4 50</td><td style=text-align:center>52.2</td><td style=text-align:center>34.2</td><td style=text-align:center>40.5</td><td style=text-align:center>32.4</td><td style=text-align:center>49.1</td><td style=text-align:center>38.6</td><td style=text-align:center>25.2</td><td style=text-align:center>19.1</td></tr><tr><td style=text-align:left>Phi 3.5 - T5-3 50</td><td style=text-align:center>51.3</td><td style=text-align:center>35.3</td><td style=text-align:center>43.6</td><td style=text-align:center>34.0</td><td style=text-align:center>47.4</td><td style=text-align:center>37.3</td><td style=text-align:center>27.9</td><td style=text-align:center>21.7</td></tr><tr><td style=text-align:left>Phi 3.5 - T5-2 50</td><td style=text-align:center>49.2</td><td style=text-align:center>33.7</td><td style=text-align:center>39.3</td><td style=text-align:center>32.9</td><td style=text-align:center>45.1</td><td style=text-align:center>38.4</td><td style=text-align:center>22.2</td><td style=text-align:center>24.3</td></tr><tr><td style=text-align:left>Phi 3.5 - L100 50</td><td style=text-align:center>50.8</td><td style=text-align:center>36.0</td><td style=text-align:center>39.3</td><td style=text-align:center>36.1</td><td style=text-align:center>50.9</td><td style=text-align:center>40.1</td><td style=text-align:center>26.2</td><td style=text-align:center>23.5</td></tr><tr><td style=text-align:left>Llama 3 - English</td><td style=text-align:center>46.1</td><td style=text-align:center>32.5</td><td style=text-align:center>38.6</td><td style=text-align:center>32.6</td><td style=text-align:center>41.6</td><td style=text-align:center>35.0</td><td style=text-align:center>25.9</td><td style=text-align:center>20.9</td></tr><tr><td style=text-align:left>Llama 3 - T5 50</td><td style=text-align:center>45.0</td><td style=text-align:center>33.8</td><td style=text-align:center>40.5</td><td style=text-align:center>34.3</td><td style=text-align:center>41.9</td><td style=text-align:center>34.1</td><td style=text-align:center>25.7</td><td style=text-align:center>26.1</td></tr><tr><td style=text-align:left>Llama 3 - L100 50</td><td style=text-align:center>46.6</td><td style=text-align:center>34.2</td><td style=text-align:center>44.2</td><td style=text-align:center>31.0</td><td style=text-align:center>42.4</td><td style=text-align:center>34.6</td><td style=text-align:center>27.2</td><td style=text-align:center>26.1</td></tr><tr><td style=text-align:left>Phi 3.5 - L100 1</td><td style=text-align:center>50.3</td><td style=text-align:center>35.1</td><td style=text-align:center>39.9</td><td style=text-align:center>35.4</td><td style=text-align:center>46.6</td><td style=text-align:center>39.2</td><td style=text-align:center>23.9</td><td style=text-align:center>25.2</td></tr><tr><td style=text-align:left>Phi 3.5 - L100 10</td><td style=text-align:center>48.8</td><td style=text-align:center>33.9</td><td style=text-align:center>35.0</td><td style=text-align:center>33.6</td><td style=text-align:center>48.1</td><td style=text-align:center>36.1</td><td style=text-align:center>24.7</td><td style=text-align:center>26.1</td></tr><tr><td style=text-align:left>Phi 3.5 - L100 24</td><td style=text-align:center>50.8</td><td style=text-align:center>36.5</td><td style=text-align:center>41.7</td><td style=text-align:center>37.0</td><td style=text-align:center>51.6</td><td style=text-align:center>35.9</td><td style=text-align:center>27.7</td><td style=text-align:center>25.2</td></tr><tr><td style=text-align:left>Phi 3.5 - L100 50</td><td style=text-align:center>50.8</td><td style=text-align:center>36.0</td><td style=text-align:center>39.3</td><td style=text-align:center>36.1</td><td style=text-align:center>50.9</td><td style=text-align:center>40.1</td><td style=text-align:center>26.2</td><td style=text-align:center>23.5</td></tr><tr><td style=text-align:left>Phi 3.5 - L100 75</td><td style=text-align:center>48.0</td><td style=text-align:center>36.1</td><td style=text-align:center>44.2</td><td style=text-align:center>35.9</td><td style=text-align:center>47.1</td><td style=text-align:center>38.4</td><td style=text-align:center>26.7</td><td style=text-align:center>24.3</td></tr><tr><td style=text-align:left>Phi 3.5 - L100 90</td><td style=text-align:center>51.7</td><td style=text-align:center>35.1</td><td style=text-align:center>36.8</td><td style=text-align:center>38.0</td><td style=text-align:center>48.1</td><td style=text-align:center>36.8</td><td style=text-align:center>26.4</td><td style=text-align:center>24.3</td></tr><tr><td style=text-align:left>Llama 3 - L100 10</td><td style=text-align:center>43.7</td><td style=text-align:center>33.6</td><td style=text-align:center>41.7</td><td style=text-align:center>29.4</td><td style=text-align:center>44.9</td><td style=text-align:center>35.3</td><td style=text-align:center>23.7</td><td style=text-align:center>27.0</td></tr><tr><td style=text-align:left>Llama 3 - L100 50</td><td style=text-align:center>46.6</td><td style=text-align:center>34.2</td><td style=text-align:center>44.2</td><td style=text-align:center>31.0</td><td style=text-align:center>42.4</td><td style=text-align:center>34.6</td><td style=text-align:center>27.2</td><td style=text-align:center>26.1</td></tr><tr><td style=text-align:left>Llama 3 - L100 90</td><td style=text-align:center>43.3</td><td style=text-align:center>34.6</td><td style=text-align:center>37.4</td><td style=text-align:center>32.2</td><td style=text-align:center>44.9</td><td style=text-align:center>35.3</td><td style=text-align:center>30.2</td><td style=text-align:center>27.8</td></tr><tr><td style=text-align:left>Phi 3.5 - L100 50</td><td style=text-align:center>50.8</td><td style=text-align:center>36.0</td><td style=text-align:center>39.3</td><td style=text-align:center>36.1</td><td style=text-align:center>50.9</td><td style=text-align:center>40.1</td><td style=text-align:center>26.2</td><td style=text-align:center>23.5</td></tr><tr><td style=text-align:left>Phi 3.5 - PT 100</td><td style=text-align:center>50.3</td><td style=text-align:center>35.8</td><td style=text-align:center>41.7</td><td style=text-align:center>37.5</td><td style=text-align:center>49.4</td><td style=text-align:center>36.6</td><td style=text-align:center>24.2</td><td style=text-align:center>25.2</td></tr><tr><td style=text-align:left>Phi 3.5 - PT 50</td><td style=text-align:center>49.7</td><td style=text-align:center>33.1</td><td style=text-align:center>41.1</td><td style=text-align:center>36.1</td><td style=text-align:center>44.4</td><td style=text-align:center>35.0</td><td style=text-align:center>21.7</td><td style=text-align:center>20.0</td></tr><tr><td style=text-align:left>Phi 3.5 - PT 1</td><td style=text-align:center>48.4</td><td style=text-align:center>33.8</td><td style=text-align:center>41.7</td><td style=text-align:center>35.9</td><td style=text-align:center>46.4</td><td style=text-align:center>34.8</td><td style=text-align:center>23.2</td><td style=text-align:center>20.9</td></tr><tr><td style=text-align:left>Llama 3 - L100 50</td><td style=text-align:center>46.6</td><td style=text-align:center>34.2</td><td style=text-align:center>44.2</td><td style=text-align:center>31.0</td><td style=text-align:center>42.4</td><td style=text-align:center>34.6</td><td style=text-align:center>27.2</td><td style=text-align:center>26.1</td></tr><tr><td style=text-align:left>Llama 3 - PT 1</td><td style=text-align:center>50.2</td><td style=text-align:center>37.9</td><td style=text-align:center>44.8</td><td style=text-align:center>34.7</td><td style=text-align:center>48.1</td><td style=text-align:center>40.6</td><td style=text-align:center>31.4</td><td style=text-align:center>27.8</td></tr><tr><td style=text-align:left>Llama 3 - PT 100</td><td style=text-align:center>52.9</td><td style=text-align:center>37.1</td><td style=text-align:center>50.3</td><td style=text-align:center>33.8</td><td style=text-align:center>46.6</td><td style=text-align:center>37.5</td><td style=text-align:center>30.2</td><td style=text-align:center>24.3</td></tr><tr><td style=text-align:left>Gemma 2 - L100 50</td><td style=text-align:center>42.5</td><td style=text-align:center>33.4</td><td style=text-align:center>43.6</td><td style=text-align:center>33.6</td><td style=text-align:center>41.6</td><td style=text-align:center>30.4</td><td style=text-align:center>27.7</td><td style=text-align:center>23.5</td></tr><tr><td style=text-align:left>Llama 3 - L100 50</td><td style=text-align:center>46.6</td><td style=text-align:center>34.2</td><td style=text-align:center>44.2</td><td style=text-align:center>31.0</td><td style=text-align:center>42.4</td><td style=text-align:center>34.6</td><td style=text-align:center>27.2</td><td style=text-align:center>26.1</td></tr><tr><td style=text-align:left>Qwen 2.5 - L100 50</td><td style=text-align:center>53.6</td><td style=text-align:center>39.6</td><td style=text-align:center>46.0</td><td style=text-align:center>44.7</td><td style=text-align:center>50.6</td><td style=text-align:center>42.4</td><td style=text-align:center>29.7</td><td style=text-align:center>24.3</td></tr><tr><td style=text-align:left>Aya-Expanse - L100 50</td><td style=text-align:center>49.3</td><td style=text-align:center>36.5</td><td style=text-align:center>46.6</td><td style=text-align:center>36.8</td><td style=text-align:center>51.9</td><td style=text-align:center>39.0</td><td style=text-align:center>26.2</td><td style=text-align:center>18.3</td></tr><tr><td style=text-align:left><code>Centurio</code> Aya</td><td style=text-align:center>53.0</td><td style=text-align:center>41.2</td><td style=text-align:center>52.8</td><td style=text-align:center>40.3</td><td style=text-align:center>51.4</td><td style=text-align:center>47.7</td><td style=text-align:center>27.4</td><td style=text-align:center>27.8</td></tr><tr><td style=text-align:left><code>Centurio</code> Qwen</td><td style=text-align:center>61.2</td><td style=text-align:center>46.9</td><td style=text-align:center>50.9</td><td style=text-align:center>64.1</td><td style=text-align:center>55.6</td><td style=text-align:center>49.0</td><td style=text-align:center>31.9</td><td style=text-align:center>29.6</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents the results of the M3Exam task, a multiple-choice visual question-answering task, across various language models and training configurations. It shows the accuracy scores obtained by different models, categorized by language tier and training setup (English-only, multilingual mixes with varying proportions of English data, different numbers of training languages, etc.). The metrics show how different amounts of English vs. multilingual data, different language distributions, and the presence or absence of pre-training affect performance on this task, allowing for a comparison of model effectiveness in multilingual settings. The performance is broken down by language tiers (T1-T5), revealing performance differences across resource levels.</p><details><summary>read the caption</summary>Table 19: M3Exam</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th></th><th>en</th><th>avg.</th><th>am</th><th>ber</th><th>bn</th><th>de</th><th>fil</th><th>ha</th><th>hi</th><th>ru</th><th>sw</th><th>th</th><th>zu</th></tr></thead><tbody><tr><td>Phi 3.5 - English</td><td>80.8</td><td>54.1</td><td>45.0</td><td>50.8</td><td>41.5</td><td>71.7</td><td>55.8</td><td>41.7</td><td>62.7</td><td>85.0</td><td>35.8</td><td>68.3</td><td>36.2</td></tr><tr><td>Phi 3.5 - T5 50</td><td>75.8</td><td>50.9</td><td>49.2</td><td>49.2</td><td>40.7</td><td>72.5</td><td>55.0</td><td>42.5</td><td>54.2</td><td>60.8</td><td>37.5</td><td>60.8</td><td>37.9</td></tr><tr><td>Phi 3.5 - T5-4 50</td><td>83.3</td><td>55.1</td><td>51.7</td><td>43.3</td><td>49.2</td><td>70.8</td><td>65.8</td><td>42.5</td><td>61.9</td><td>70.8</td><td>38.3</td><td>75.0</td><td>36.2</td></tr><tr><td>Phi 3.5 - T5-3 50</td><td>83.3</td><td>56.6</td><td>43.3</td><td>50.8</td><td>50.8</td><td>74.2</td><td>69.2</td><td>42.5</td><td>57.6</td><td>76.7</td><td>43.3</td><td>71.7</td><td>42.2</td></tr><tr><td>Phi 3.5 - T5-2 50</td><td>81.7</td><td>57.5</td><td>45.8</td><td>52.5</td><td>44.1</td><td>73.3</td><td>64.2</td><td>39.2</td><td>59.3</td><td>73.3</td><td>60.0</td><td>60.8</td><td>59.5</td></tr><tr><td>Phi 3.5 - L100 50</td><td>76.7</td><td>56.4</td><td>46.7</td><td>46.7</td><td>54.2</td><td>71.7</td><td>60.0</td><td>45.0</td><td>57.6</td><td>70.8</td><td>57.5</td><td>65.8</td><td>44.0</td></tr><tr><td>Llama 3 - English</td><td>82.5</td><td>56.3</td><td>66.7</td><td>30.8</td><td>49.2</td><td>77.5</td><td>50.8</td><td>48.3</td><td>63.6</td><td>75.8</td><td>46.7</td><td>70.0</td><td>39.7</td></tr><tr><td>Llama 3 - T5 50</td><td>77.5</td><td>55.9</td><td>47.5</td><td>49.2</td><td>49.2</td><td>71.7</td><td>63.3</td><td>42.5</td><td>62.7</td><td>73.3</td><td>45.8</td><td>70.8</td><td>38.8</td></tr><tr><td>Llama 3 - L100 50</td><td>80.0</td><td>64.8</td><td>58.3</td><td>47.5</td><td>64.4</td><td>75.8</td><td>61.7</td><td>67.5</td><td>64.4</td><td>73.3</td><td>59.2</td><td>67.5</td><td>73.3</td></tr><tr><td>Phi 3.5 - L100 1</td><td>65.0</td><td>47.5</td><td>42.5</td><td>50.0</td><td>38.1</td><td>65.0</td><td>58.3</td><td>40.0</td><td>45.8</td><td>58.3</td><td>39.2</td><td>42.5</td><td>42.2</td></tr><tr><td>Phi 3.5 - L100 10</td><td>73.3</td><td>54.5</td><td>43.3</td><td>50.0</td><td>51.7</td><td>67.5</td><td>60.0</td><td>45.0</td><td>51.7</td><td>63.3</td><td>53.3</td><td>63.3</td><td>50.0</td></tr><tr><td>Phi 3.5 - L100 24</td><td>73.3</td><td>60.3</td><td>54.2</td><td>47.5</td><td>58.5</td><td>72.5</td><td>55.0</td><td>58.3</td><td>60.2</td><td>72.5</td><td>64.2</td><td>59.2</td><td>61.2</td></tr><tr><td>Phi 3.5 - L100 50</td><td>76.7</td><td>56.4</td><td>46.7</td><td>46.7</td><td>54.2</td><td>71.7</td><td>60.0</td><td>45.0</td><td>57.6</td><td>70.8</td><td>57.5</td><td>65.8</td><td>44.0</td></tr><tr><td>Phi 3.5 - L100 75</td><td>80.0</td><td>56.7</td><td>51.7</td><td>53.3</td><td>55.1</td><td>70.8</td><td>67.5</td><td>41.7</td><td>63.6</td><td>75.8</td><td>38.3</td><td>69.2</td><td>36.2</td></tr><tr><td>Phi 3.5 - L100 90</td><td>79.2</td><td>54.6</td><td>43.3</td><td>50.0</td><td>44.9</td><td>80.8</td><td>60.0</td><td>42.5</td><td>55.9</td><td>77.5</td><td>45.0</td><td>55.8</td><td>44.8</td></tr><tr><td>Llama 3 - L100 10</td><td>77.5</td><td>65.4</td><td>65.0</td><td>45.0</td><td>63.6</td><td>76.7</td><td>58.3</td><td>70.8</td><td>64.4</td><td>74.2</td><td>63.3</td><td>69.2</td><td>69.0</td></tr><tr><td>Llama 3 - L100 50</td><td>80.0</td><td>64.8</td><td>58.3</td><td>47.5</td><td>64.4</td><td>75.8</td><td>61.7</td><td>67.5</td><td>64.4</td><td>73.3</td><td>59.2</td><td>67.5</td><td>73.3</td></tr><tr><td>Llama 3 - L100 90</td><td>82.5</td><td>63.0</td><td>45.8</td><td>39.2</td><td>66.1</td><td>80.8</td><td>58.3</td><td>68.3</td><td>61.9</td><td>75.0</td><td>63.3</td><td>75.0</td><td>59.5</td></tr><tr><td>Phi 3.5 - L100 50</td><td>76.7</td><td>56.4</td><td>46.7</td><td>46.7</td><td>54.2</td><td>71.7</td><td>60.0</td><td>45.0</td><td>57.6</td><td>70.8</td><td>57.5</td><td>65.8</td><td>44.0</td></tr><tr><td>Phi 3.5 - PT 100</td><td>80.8</td><td>58.6</td><td>44.2</td><td>49.2</td><td>56.8</td><td>78.3</td><td>56.7</td><td>47.5</td><td>65.3</td><td>75.0</td><td>47.5</td><td>73.3</td><td>50.9</td></tr><tr><td>Phi 3.5 - PT 50</td><td>80.0</td><td>63.2</td><td>58.3</td><td>50.0</td><td>55.1</td><td>78.3</td><td>63.3</td><td>60.0</td><td>61.9</td><td>76.7</td><td>55.0</td><td>75.0</td><td>61.2</td></tr><tr><td>Phi 3.5 - PT 1</td><td>80.0</td><td>62.0</td><td>55.8</td><td>50.0</td><td>51.7</td><td>81.7</td><td>62.5</td><td>60.0</td><td>66.1</td><td>75.0</td><td>50.0</td><td>66.7</td><td>62.1</td></tr><tr><td>Llama 3 - L100 50</td><td>80.0</td><td>64.8</td><td>58.3</td><td>47.5</td><td>64.4</td><td>75.8</td><td>61.7</td><td>67.5</td><td>64.4</td><td>73.3</td><td>59.2</td><td>67.5</td><td>73.3</td></tr><tr><td>Llama 3 - PT 1</td><td>87.5</td><td>71.2</td><td>70.0</td><td>50.8</td><td>65.3</td><td>79.2</td><td>63.3</td><td>83.3</td><td>68.6</td><td>82.5</td><td>66.7</td><td>85.8</td><td>68.1</td></tr><tr><td>Llama 3 - PT 100</td><td>85.0</td><td>68.8</td><td>65.8</td><td>49.2</td><td>67.8</td><td>80.8</td><td>61.7</td><td>70.0</td><td>66.9</td><td>85.0</td><td>70.0</td><td>74.2</td><td>65.5</td></tr><tr><td>Gemma 2 - L100 50</td><td>77.5</td><td>61.8</td><td>64.2</td><td>52.5</td><td>48.3</td><td>70.8</td><td>51.7</td><td>64.2</td><td>58.5</td><td>71.7</td><td>54.2</td><td>70.8</td><td>73.3</td></tr><tr><td>Llama 3 - L100 50</td><td>80.0</td><td>64.8</td><td>58.3</td><td>47.5</td><td>64.4</td><td>75.8</td><td>61.7</td><td>67.5</td><td>64.4</td><td>73.3</td><td>59.2</td><td>67.5</td><td>73.3</td></tr><tr><td>Qwen 2.5 - L100 50</td><td>91.7</td><td>71.2</td><td>76.7</td><td>50.0</td><td>69.5</td><td>81.7</td><td>77.5</td><td>57.5</td><td>72.9</td><td>83.3</td><td>71.7</td><td>80.8</td><td>62.1</td></tr><tr><td>Aya-Expanse - L100 50</td><td>92.5</td><td>69.9</td><td>52.5</td><td>54.2</td><td>55.9</td><td>80.8</td><td>85.0</td><td>72.5</td><td>79.7</td><td>83.3</td><td>63.3</td><td>78.3</td><td>63.8</td></tr><tr><td><code>Centurio</code> Aya</td><td>82.5</td><td>66.8</td><td>71.7</td><td>54.2</td><td>59.3</td><td>73.3</td><td>59.2</td><td>65.0</td><td>71.2</td><td>75.8</td><td>67.5</td><td>72.5</td><td>65.5</td></tr><tr><td><code>Centurio</code> Qwen</td><td>87.5</td><td>73.1</td><td>77.5</td><td>49.2</td><td>62.7</td><td>80.8</td><td>78.3</td><td>76.7</td><td>72.9</td><td>85.0</td><td>70.0</td><td>81.7</td><td>69.0</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents a comparison of various Large Vision-Language Models (LVLMs) on the Visually Grounded Reasoning (VGR) task. The models&rsquo; performance is evaluated across multiple languages, grouped into tiers based on resource availability (T1-T5, with T5 representing high-resource languages like English and T1 representing low-resource languages). The table shows the accuracy scores achieved by each model in each language tier, illustrating the models&rsquo; multilingual capabilities and the impact of different training strategies. English performance is also shown separately for each model. The results help determine which models handle multilingual VGR effectively and which training techniques, such as varying the proportion of English versus multilingual data, lead to the best outcomes.</p><details><summary>read the caption</summary>Table 20: VGR</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th></th><th>en</th><th>avg.</th><th>am</th><th>ber</th><th>bn</th><th>de</th><th>fil</th><th>ha</th><th>hi</th><th>ru</th><th>sw</th><th>th</th><th>zu</th></tr></thead><tbody><tr><td>Phi 3.5 - English</td><td>16.7</td><td>21.3</td><td>20.8</td><td>20.8</td><td>19.2</td><td>16.7</td><td>25.8</td><td>28.3</td><td>17.0</td><td>12.5</td><td>25.0</td><td>26.7</td><td>22.0</td></tr><tr><td>Phi 3.5 - T5 50</td><td>23.3</td><td>20.0</td><td>15.0</td><td>18.3</td><td>20.8</td><td>21.7</td><td>16.7</td><td>20.0</td><td>23.2</td><td>27.5</td><td>22.3</td><td>15.8</td><td>18.6</td></tr><tr><td>Phi 3.5 - T5-4 50</td><td>17.5</td><td>18.2</td><td>19.2</td><td>20.8</td><td>13.3</td><td>20.8</td><td>17.5</td><td>16.7</td><td>21.4</td><td>26.7</td><td>16.1</td><td>10.0</td><td>17.8</td></tr><tr><td>Phi 3.5 - T5-3 50</td><td>25.8</td><td>19.8</td><td>16.7</td><td>17.5</td><td>21.7</td><td>21.7</td><td>20.0</td><td>21.7</td><td>23.2</td><td>20.8</td><td>18.8</td><td>16.7</td><td>18.6</td></tr><tr><td>Phi 3.5 - T5-2 50</td><td>21.7</td><td>20.5</td><td>21.7</td><td>18.3</td><td>16.7</td><td>22.5</td><td>27.5</td><td>27.5</td><td>17.9</td><td>21.7</td><td>17.0</td><td>13.3</td><td>21.2</td></tr><tr><td>Phi 3.5 - L100 50</td><td>18.3</td><td>19.5</td><td>16.7</td><td>20.8</td><td>19.2</td><td>25.8</td><td>20.0</td><td>16.7</td><td>25.0</td><td>20.8</td><td>13.4</td><td>17.5</td><td>18.6</td></tr><tr><td>Llama 3 - English</td><td>12.5</td><td>20.8</td><td>18.3</td><td>21.7</td><td>20.0</td><td>10.8</td><td>24.2</td><td>29.2</td><td>15.2</td><td>12.5</td><td>28.6</td><td>29.2</td><td>19.5</td></tr><tr><td>Llama 3 - T5 50</td><td>20.8</td><td>20.1</td><td>18.3</td><td>19.2</td><td>17.5</td><td>16.7</td><td>25.0</td><td>21.7</td><td>24.1</td><td>15.0</td><td>19.6</td><td>23.3</td><td>20.3</td></tr><tr><td>Llama 3 - L100 50</td><td>12.5</td><td>20.6</td><td>19.2</td><td>20.8</td><td>20.0</td><td>10.8</td><td>24.2</td><td>30.0</td><td>15.2</td><td>10.8</td><td>28.6</td><td>27.5</td><td>19.5</td></tr><tr><td>Phi 3.5 - L100 1</td><td>24.2</td><td>19.3</td><td>15.0</td><td>21.7</td><td>17.5</td><td>20.0</td><td>29.2</td><td>22.5</td><td>17.9</td><td>14.2</td><td>16.1</td><td>22.5</td><td>16.1</td></tr><tr><td>Phi 3.5 - L100 10</td><td>23.3</td><td>19.2</td><td>23.3</td><td>15.0</td><td>16.7</td><td>21.7</td><td>20.8</td><td>20.8</td><td>20.5</td><td>24.2</td><td>10.7</td><td>15.8</td><td>22.0</td></tr><tr><td>Phi 3.5 - L100 24</td><td>25.0</td><td>18.3</td><td>20.8</td><td>18.3</td><td>16.7</td><td>20.8</td><td>16.7</td><td>20.8</td><td>17.9</td><td>21.7</td><td>14.3</td><td>16.7</td><td>16.9</td></tr><tr><td>Phi 3.5 - L100 50</td><td>18.3</td><td>19.5</td><td>16.7</td><td>20.8</td><td>19.2</td><td>25.8</td><td>20.0</td><td>16.7</td><td>25.0</td><td>20.8</td><td>13.4</td><td>17.5</td><td>18.6</td></tr><tr><td>Phi 3.5 - L100 75</td><td>16.7</td><td>18.0</td><td>15.0</td><td>20.0</td><td>19.2</td><td>19.2</td><td>16.7</td><td>23.3</td><td>17.0</td><td>13.3</td><td>17.9</td><td>15.8</td><td>20.3</td></tr><tr><td>Phi 3.5 - L100 90</td><td>22.5</td><td>19.0</td><td>20.0</td><td>16.7</td><td>15.8</td><td>20.0</td><td>16.7</td><td>23.3</td><td>21.4</td><td>23.3</td><td>16.1</td><td>15.8</td><td>19.5</td></tr><tr><td>Llama 3 - L100 10</td><td>13.3</td><td>20.4</td><td>18.3</td><td>21.7</td><td>19.2</td><td>10.8</td><td>23.3</td><td>26.7</td><td>17.9</td><td>10.0</td><td>28.6</td><td>28.3</td><td>19.5</td></tr><tr><td>Llama 3 - L100 50</td><td>12.5</td><td>20.6</td><td>19.2</td><td>20.8</td><td>20.0</td><td>10.8</td><td>24.2</td><td>30.0</td><td>15.2</td><td>10.8</td><td>28.6</td><td>27.5</td><td>19.5</td></tr><tr><td>Llama 3 - L100 90</td><td>12.5</td><td>19.9</td><td>18.3</td><td>21.7</td><td>15.0</td><td>10.8</td><td>22.5</td><td>28.3</td><td>15.2</td><td>10.8</td><td>28.6</td><td>28.3</td><td>19.5</td></tr><tr><td>Phi 3.5 - L100 50</td><td>18.3</td><td>19.5</td><td>16.7</td><td>20.8</td><td>19.2</td><td>25.8</td><td>20.0</td><td>16.7</td><td>25.0</td><td>20.8</td><td>13.4</td><td>17.5</td><td>18.6</td></tr><tr><td>Phi 3.5 - PT 100</td><td>23.3</td><td>20.0</td><td>16.7</td><td>16.7</td><td>24.2</td><td>20.0</td><td>25.0</td><td>21.7</td><td>19.6</td><td>15.0</td><td>20.5</td><td>20.0</td><td>20.3</td></tr><tr><td>Phi 3.5 - PT 50</td><td>20.0</td><td>18.6</td><td>18.3</td><td>17.5</td><td>15.0</td><td>15.8</td><td>14.2</td><td>21.7</td><td>17.9</td><td>23.3</td><td>20.5</td><td>20.8</td><td>19.5</td></tr><tr><td>Phi 3.5 - PT 1</td><td>25.0</td><td>19.4</td><td>21.7</td><td>22.5</td><td>19.2</td><td>22.5</td><td>16.7</td><td>15.8</td><td>20.5</td><td>21.7</td><td>16.1</td><td>15.0</td><td>22.0</td></tr><tr><td>Llama 3 - L100 50</td><td>12.5</td><td>20.6</td><td>19.2</td><td>20.8</td><td>20.0</td><td>10.8</td><td>24.2</td><td>30.0</td><td>15.2</td><td>10.8</td><td>28.6</td><td>27.5</td><td>19.5</td></tr><tr><td>Llama 3 - PT 1</td><td>19.2</td><td>20.5</td><td>15.8</td><td>19.2</td><td>22.5</td><td>15.0</td><td>23.3</td><td>23.3</td><td>17.9</td><td>13.3</td><td>25.9</td><td>28.3</td><td>21.2</td></tr><tr><td>Llama 3 - PT 100</td><td>13.3</td><td>20.8</td><td>18.3</td><td>21.7</td><td>20.0</td><td>12.5</td><td>23.3</td><td>29.2</td><td>17.0</td><td>10.8</td><td>28.6</td><td>28.3</td><td>19.5</td></tr><tr><td>Gemma 2 - L100 50</td><td>14.2</td><td>21.1</td><td>18.3</td><td>22.5</td><td>20.8</td><td>10.8</td><td>25.0</td><td>28.3</td><td>16.1</td><td>11.7</td><td>27.7</td><td>30.0</td><td>20.3</td></tr><tr><td>Llama 3 - L100 50</td><td>12.5</td><td>20.6</td><td>19.2</td><td>20.8</td><td>20.0</td><td>10.8</td><td>24.2</td><td>30.0</td><td>15.2</td><td>10.8</td><td>28.6</td><td>27.5</td><td>19.5</td></tr><tr><td>Qwen 2.5 - L100 50</td><td>26.7</td><td>27.3</td><td>25.0</td><td>21.7</td><td>26.7</td><td>27.5</td><td>27.5</td><td>25.0</td><td>29.5</td><td>25.0</td><td>29.5</td><td>40.0</td><td>22.9</td></tr><tr><td>Aya-Expanse - L100 50</td><td>12.5</td><td>20.7</td><td>18.3</td><td>21.7</td><td>20.0</td><td>10.8</td><td>24.2</td><td>29.2</td><td>15.2</td><td>10.8</td><td>28.6</td><td>29.2</td><td>19.5</td></tr><tr><td>Centurio Aya</td><td>12.5</td><td>20.7</td><td>18.3</td><td>21.7</td><td>20.0</td><td>11.7</td><td>24.2</td><td>29.2</td><td>15.2</td><td>10.8</td><td>28.6</td><td>29.2</td><td>19.5</td></tr><tr><td>Centurio Qwen</td><td>28.3</td><td>27.0</td><td>18.3</td><td>20.0</td><td>33.3</td><td>32.5</td><td>29.2</td><td>22.5</td><td>25.0</td><td>22.5</td><td>30.4</td><td>30.0</td><td>33.1</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents the results of the Visio-Linguistic Outlier Detection (VLOD) task, which involves identifying the outlier image among a set of images. The performance of several models is evaluated across different languages, categorized into tiers based on resource availability, showing the accuracy achieved by each model for each language tier. The table also includes results for models trained with various data distributions and settings, offering insights into the impact of different training strategies on the model&rsquo;s performance.</p><details><summary>read the caption</summary>Table 21: VLOD</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th></th><th>en</th><th>avg.</th><th>id</th><th>sw</th><th>ta</th><th>tr</th><th>zh</th></tr></thead><tbody><tr><td>Phi 3.5 - English</td><td>82.1</td><td>61.4</td><td>65.6</td><td>50.8</td><td>53.3</td><td>63.8</td><td>73.2</td></tr><tr><td>Phi 3.5 - T5 50</td><td>81.5</td><td>61.8</td><td>66.4</td><td>53.4</td><td>53.7</td><td>61.6</td><td>73.8</td></tr><tr><td>Phi 3.5 - T5-4 50</td><td>81.2</td><td>64.3</td><td>68.7</td><td>52.3</td><td>54.3</td><td>70.2</td><td>76.2</td></tr><tr><td>Phi 3.5 - T5-3 50</td><td>81.5</td><td>65.9</td><td>70.8</td><td>56.4</td><td>56.7</td><td>68.9</td><td>76.7</td></tr><tr><td>Phi 3.5 - T5-2 50</td><td>79.7</td><td>66.4</td><td>70.2</td><td>62.2</td><td>57.5</td><td>66.7</td><td>75.4</td></tr><tr><td>Phi 3.5 - L100 50</td><td>79.6</td><td>64.4</td><td>69.0</td><td>59.0</td><td>53.6</td><td>67.5</td><td>73.0</td></tr><tr><td>Llama 3 - English</td><td>85.2</td><td>65.0</td><td>68.8</td><td>52.5</td><td>54.3</td><td>69.7</td><td>79.8</td></tr><tr><td>Llama 3 - T5 50</td><td>84.5</td><td>67.1</td><td>73.8</td><td>55.7</td><td>53.6</td><td>72.7</td><td>79.6</td></tr><tr><td>Llama 3 - L100 50</td><td>83.7</td><td>74.2</td><td>75.3</td><td>71.4</td><td>68.4</td><td>79.8</td><td>76.0</td></tr><tr><td>Phi 3.5 - L100 1</td><td>71.9</td><td>61.4</td><td>65.1</td><td>56.1</td><td>54.3</td><td>65.2</td><td>66.1</td></tr><tr><td>Phi 3.5 - L100 10</td><td>74.1</td><td>63.4</td><td>66.8</td><td>58.1</td><td>57.2</td><td>65.1</td><td>70.0</td></tr><tr><td>Phi 3.5 - L100 24</td><td>76.0</td><td>61.6</td><td>63.4</td><td>57.6</td><td>56.9</td><td>64.0</td><td>66.3</td></tr><tr><td>Phi 3.5 - L100 50</td><td>79.6</td><td>64.4</td><td>69.0</td><td>59.0</td><td>53.6</td><td>67.5</td><td>73.0</td></tr><tr><td>Phi 3.5 - L100 75</td><td>81.7</td><td>64.7</td><td>71.3</td><td>54.4</td><td>56.1</td><td>64.8</td><td>77.0</td></tr><tr><td>Phi 3.5 - L100 90</td><td>83.1</td><td>64.3</td><td>70.7</td><td>56.3</td><td>53.8</td><td>62.8</td><td>77.8</td></tr><tr><td>Llama 3 - L100 10</td><td>80.0</td><td>72.9</td><td>71.9</td><td>70.8</td><td>71.7</td><td>75.7</td><td>74.2</td></tr><tr><td>Llama 3 - L100 50</td><td>83.7</td><td>74.2</td><td>75.3</td><td>71.4</td><td>68.4</td><td>79.8</td><td>76.0</td></tr><tr><td>Llama 3 - L100 90</td><td>85.1</td><td>71.1</td><td>73.4</td><td>63.7</td><td>65.1</td><td>75.7</td><td>77.6</td></tr><tr><td>Phi 3.5 - L100 50</td><td>79.6</td><td>64.4</td><td>69.0</td><td>59.0</td><td>53.6</td><td>67.5</td><td>73.0</td></tr><tr><td>Phi 3.5 - PT 100</td><td>82.0</td><td>65.6</td><td>68.6</td><td>59.4</td><td>57.9</td><td>67.6</td><td>74.5</td></tr><tr><td>Phi 3.5 - PT 50</td><td>82.5</td><td>69.9</td><td>75.2</td><td>64.0</td><td>64.1</td><td>71.1</td><td>74.9</td></tr><tr><td>Phi 3.5 - PT 1</td><td>81.9</td><td>67.9</td><td>74.0</td><td>64.0</td><td>60.2</td><td>68.0</td><td>73.4</td></tr><tr><td>Llama 3 - L100 50</td><td>83.7</td><td>74.2</td><td>75.3</td><td>71.4</td><td>68.4</td><td>79.8</td><td>76.0</td></tr><tr><td>Llama 3 - PT 1</td><td>87.5</td><td>80.4</td><td>82.5</td><td>75.5</td><td>77.1</td><td>84.5</td><td>82.3</td></tr><tr><td>Llama 3 - PT 100</td><td>86.5</td><td>78.9</td><td>81.3</td><td>73.0</td><td>75.1</td><td>83.4</td><td>81.5</td></tr><tr><td>Gemma 2 - L100 50</td><td>82.5</td><td>73.0</td><td>72.6</td><td>71.4</td><td>68.3</td><td>76.4</td><td>76.2</td></tr><tr><td>Llama 3 - L100 50</td><td>83.7</td><td>74.2</td><td>75.3</td><td>71.4</td><td>68.4</td><td>79.8</td><td>76.0</td></tr><tr><td>Qwen 2.5 - L100 50</td><td>89.6</td><td>79.4</td><td>84.8</td><td>73.9</td><td>65.2</td><td>86.6</td><td>86.6</td></tr><tr><td>Aya-Expanse - L100 50</td><td>87.0</td><td>80.2</td><td>83.9</td><td>75.6</td><td>71.7</td><td>86.9</td><td>83.0</td></tr><tr><td><code>Centurio</code> Aya</td><td>85.0</td><td>77.9</td><td>79.5</td><td>70.9</td><td>73.4</td><td>83.4</td><td>82.4</td></tr><tr><td><code>Centurio</code> Qwen</td><td>89.6</td><td>81.7</td><td>85.0</td><td>76.8</td><td>76.0</td><td>84.2</td><td>86.7</td></tr></tbody></table></table></figure><blockquote><p>üîº Table 22 presents the results of the MaRVL (Multilingual Reasoning over Vision and Language) task. The table compares the performance of various large vision-language models (LVLMs) on this task across different languages, grouped into five tiers (T1-T5) based on resource availability. Each language tier represents a range of languages with similar levels of available training data. The table shows the performance (accuracy) of each model on the MaRVL dataset for each language tier, as well as the overall average performance across all tiers. It allows for an analysis of how well these models generalize to low-resource languages compared to higher-resource languages.</p><details><summary>read the caption</summary>Table 22: MaRVL</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th></th><th><strong>en</strong></th><th><strong>avg.</strong></th><th><strong>fr</strong></th><th><strong>hi</strong></th><th><strong>he</strong></th><th><strong>ro</strong></th><th><strong>th</strong></th><th><strong>zh</strong></th></tr></thead><tbody><tr><td>Phi 3.5 - English</td><td>53.0</td><td>9.2</td><td>14.3</td><td>11.9</td><td>7.9</td><td>7.2</td><td>7.0</td><td>7.2</td></tr><tr><td>Phi 3.5 - T5 50</td><td>51.3</td><td>25.6</td><td>41.0</td><td>30.6</td><td>17.5</td><td>15.6</td><td>27.5</td><td>21.5</td></tr><tr><td>Phi 3.5 - T5-4 50</td><td>51.0</td><td>33.1</td><td>45.4</td><td>50.7</td><td>27.0</td><td>23.7</td><td>32.5</td><td>19.5</td></tr><tr><td>Phi 3.5 - T5-3 50</td><td>53.7</td><td>36.7</td><td>41.0</td><td>45.9</td><td>33.0</td><td>36.6</td><td>40.4</td><td>23.5</td></tr><tr><td>Phi 3.5 - T5-2 50</td><td>53.4</td><td>35.9</td><td>42.3</td><td>48.0</td><td>33.3</td><td>35.1</td><td>32.8</td><td>23.8</td></tr><tr><td>Phi 3.5 - L100 50</td><td>54.4</td><td>36.6</td><td>43.0</td><td>48.0</td><td>30.8</td><td>35.1</td><td>39.1</td><td>23.5</td></tr><tr><td>Llama 3 - English</td><td>55.4</td><td>7.7</td><td>9.2</td><td>10.9</td><td>6.7</td><td>4.5</td><td>8.3</td><td>6.8</td></tr><tr><td>Llama 3 - T5 50</td><td>41.3</td><td>20.2</td><td>45.1</td><td>12.6</td><td>2.9</td><td>24.3</td><td>14.6</td><td>21.8</td></tr><tr><td>Llama 3 - L100 50</td><td>52.7</td><td>42.3</td><td>42.3</td><td>54.4</td><td>40.6</td><td>40.5</td><td>52.6</td><td>23.1</td></tr><tr><td>Phi 3.5 - L100 1</td><td>48.0</td><td>33.8</td><td>39.9</td><td>45.2</td><td>32.4</td><td>32.4</td><td>32.8</td><td>19.9</td></tr><tr><td>Phi 3.5 - L100 10</td><td>52.0</td><td>35.4</td><td>44.7</td><td>45.6</td><td>34.6</td><td>36.0</td><td>29.5</td><td>22.1</td></tr><tr><td>Phi 3.5 - L100 24</td><td>50.7</td><td>35.1</td><td>44.0</td><td>44.6</td><td>29.8</td><td>33.0</td><td>38.1</td><td>21.2</td></tr><tr><td>Phi 3.5 - L100 50</td><td>54.4</td><td>36.6</td><td>43.0</td><td>48.0</td><td>30.8</td><td>35.1</td><td>39.1</td><td>23.5</td></tr><tr><td>Phi 3.5 - L100 75</td><td>51.0</td><td>32.5</td><td>42.0</td><td>36.4</td><td>29.8</td><td>33.3</td><td>31.8</td><td>21.8</td></tr><tr><td>Phi 3.5 - L100 90</td><td>54.7</td><td>29.7</td><td>41.6</td><td>28.2</td><td>27.3</td><td>28.5</td><td>30.5</td><td>21.8</td></tr><tr><td>Llama 3 - L100 10</td><td>49.0</td><td>41.9</td><td>37.9</td><td>53.4</td><td>45.7</td><td>41.4</td><td>51.0</td><td>21.8</td></tr><tr><td>Llama 3 - L100 50</td><td>52.7</td><td>42.3</td><td>42.3</td><td>54.4</td><td>40.6</td><td>40.5</td><td>52.6</td><td>23.1</td></tr><tr><td>Llama 3 - L100 90</td><td>52.7</td><td>40.6</td><td>43.3</td><td>52.7</td><td>36.2</td><td>40.2</td><td>49.0</td><td>22.1</td></tr><tr><td>Phi 3.5 - L100 50</td><td>54.4</td><td>36.6</td><td>43.0</td><td>48.0</td><td>30.8</td><td>35.1</td><td>39.1</td><td>23.5</td></tr><tr><td>Phi 3.5 - PT 100</td><td>54.0</td><td>36.2</td><td>44.0</td><td>48.6</td><td>32.4</td><td>33.9</td><td>36.8</td><td>21.5</td></tr><tr><td>Phi 3.5 - PT 50</td><td>53.4</td><td>39.0</td><td>45.7</td><td>49.3</td><td>39.4</td><td>36.6</td><td>40.7</td><td>22.1</td></tr><tr><td>Phi 3.5 - PT 1</td><td>55.7</td><td>39.7</td><td>44.7</td><td>52.0</td><td>41.0</td><td>40.8</td><td>40.1</td><td>19.9</td></tr><tr><td>Llama 3 - L100 50</td><td>52.7</td><td>42.3</td><td>42.3</td><td>54.4</td><td>40.6</td><td>40.5</td><td>52.6</td><td>23.1</td></tr><tr><td>Llama 3 - PT 1</td><td>55.0</td><td>48.5</td><td>47.4</td><td>57.1</td><td>56.2</td><td>47.4</td><td>57.3</td><td>25.7</td></tr><tr><td>Llama 3 - PT 100</td><td>58.1</td><td>47.4</td><td>44.7</td><td>54.8</td><td>54.0</td><td>47.1</td><td>57.3</td><td>26.4</td></tr><tr><td>Gemma 2 - L100 50</td><td>51.7</td><td>41.5</td><td>39.6</td><td>52.4</td><td>44.1</td><td>39.3</td><td>48.7</td><td>24.8</td></tr><tr><td>Llama 3 - L100 50</td><td>52.7</td><td>42.3</td><td>42.3</td><td>54.4</td><td>40.6</td><td>40.5</td><td>52.6</td><td>23.1</td></tr><tr><td>Qwen 2.5 - L100 50</td><td>58.7</td><td>45.8</td><td>46.4</td><td>51.4</td><td>50.2</td><td>41.7</td><td>57.9</td><td>27.0</td></tr><tr><td>Aya-Expanse - L100 50</td><td>53.4</td><td>47.2</td><td>46.4</td><td>58.8</td><td>59.4</td><td>49.9</td><td>41.4</td><td>27.4</td></tr><tr><td><code>Centurio</code> Aya</td><td>55.7</td><td>49.3</td><td>45.1</td><td>62.9</td><td>58.7</td><td>51.1</td><td>46.7</td><td>31.6</td></tr><tr><td><code>Centurio</code> Qwen</td><td>60.1</td><td>47.7</td><td>47.1</td><td>56.8</td><td>45.1</td><td>47.7</td><td>57.0</td><td>32.2</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents the results of the MaXM (Massively Multilingual Cross-lingual Visual Question Answering) dataset experiment. It compares the performance of various large vision-language models (LVLMs) across different language groups and configurations, including different multilingual training data ratios and various pre-training strategies. The evaluation metrics likely involve accuracy scores, averaged across different language tiers (e.g., low-resource, high-resource languages). Each row represents a different model and training configuration, enabling a comparison of multilingual abilities and the impact of various training parameters. The columns likely represent different languages or groups of languages, showing performance scores for each model in those language groups.</p><details><summary>read the caption</summary>Table 23: MaXM</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Model</th><th>avg.</th><th>ar</th><th>de</th><th>fr</th><th>it</th><th>ja</th><th>ko</th><th>ru</th><th>th</th><th>vi</th></tr></thead><tbody><tr><td>Phi 3.5 - English</td><td>3.2</td><td>0.9</td><td>6.5</td><td>9.3</td><td>8.1</td><td>0.8</td><td>0.7</td><td>1.6</td><td>0.0</td><td>1.1</td></tr><tr><td>Phi 3.5 - T5 50</td><td>5.7</td><td>1.7</td><td>12.0</td><td>15.9</td><td>10.1</td><td>2.4</td><td>3.8</td><td>2.6</td><td>0.9</td><td>1.8</td></tr><tr><td>Phi 3.5 - T5-4 50</td><td>5.9</td><td>2.7</td><td>14.0</td><td>15.1</td><td>9.6</td><td>3.5</td><td>3.8</td><td>1.9</td><td>0.9</td><td>1.6</td></tr><tr><td>Phi 3.5 - T5-3 50</td><td>5.8</td><td>2.0</td><td>13.5</td><td>14.6</td><td>9.4</td><td>3.9</td><td>3.8</td><td>2.4</td><td>0.9</td><td>2.0</td></tr><tr><td>Phi 3.5 - T5-2 50</td><td>6.6</td><td>5.3</td><td>15.9</td><td>15.1</td><td>9.4</td><td>4.1</td><td>3.8</td><td>2.5</td><td>0.4</td><td>2.7</td></tr><tr><td>Phi 3.5 - L100 50</td><td>6.3</td><td>2.8</td><td>15.8</td><td>16.8</td><td>8.9</td><td>3.9</td><td>2.7</td><td>2.8</td><td>0.4</td><td>2.9</td></tr><tr><td>Llama 3 - English</td><td>3.2</td><td>0.3</td><td>6.9</td><td>8.0</td><td>8.7</td><td>0.7</td><td>0.5</td><td>0.7</td><td>0.4</td><td>2.7</td></tr><tr><td>Llama 3 - T5 50</td><td>5.6</td><td>2.0</td><td>14.2</td><td>15.0</td><td>9.1</td><td>1.9</td><td>1.4</td><td>2.6</td><td>1.3</td><td>2.8</td></tr><tr><td>Llama 3 - L100 50</td><td>6.0</td><td>2.1</td><td>11.9</td><td>15.8</td><td>7.2</td><td>2.1</td><td>3.2</td><td>2.4</td><td>4.8</td><td>4.1</td></tr><tr><td>Phi 3.5 - L100 1</td><td>4.7</td><td>2.0</td><td>12.0</td><td>9.4</td><td>7.5</td><td>3.4</td><td>3.4</td><td>1.9</td><td>0.9</td><td>2.3</td></tr><tr><td>Phi 3.5 - L100 10</td><td>5.7</td><td>3.0</td><td>12.1</td><td>14.2</td><td>8.6</td><td>4.6</td><td>4.1</td><td>2.1</td><td>0.9</td><td>1.5</td></tr><tr><td>Phi 3.5 - L100 24</td><td>6.2</td><td>3.6</td><td>14.0</td><td>15.8</td><td>8.7</td><td>3.1</td><td>3.8</td><td>3.3</td><td>0.9</td><td>2.5</td></tr><tr><td>Phi 3.5 - L100 50</td><td>6.3</td><td>2.8</td><td>15.8</td><td>16.8</td><td>8.9</td><td>3.9</td><td>2.7</td><td>2.8</td><td>0.4</td><td>2.9</td></tr><tr><td>Phi 3.5 - L100 75</td><td>6.3</td><td>2.6</td><td>13.8</td><td>18.3</td><td>8.7</td><td>4.3</td><td>2.9</td><td>2.8</td><td>0.9</td><td>2.8</td></tr><tr><td>Phi 3.5 - L100 90</td><td>7.0</td><td>2.6</td><td>14.7</td><td>19.3</td><td>10.4</td><td>3.6</td><td>4.1</td><td>3.2</td><td>3.5</td><td>1.5</td></tr><tr><td>Llama 3 - L100 10</td><td>5.3</td><td>1.6</td><td>11.3</td><td>13.8</td><td>7.5</td><td>2.9</td><td>3.4</td><td>2.6</td><td>0.9</td><td>3.5</td></tr><tr><td>Llama 3 - L100 50</td><td>6.0</td><td>2.1</td><td>11.9</td><td>15.8</td><td>7.2</td><td>2.1</td><td>3.2</td><td>2.4</td><td>4.8</td><td>4.1</td></tr><tr><td>Llama 3 - L100 90</td><td>6.5</td><td>2.1</td><td>14.0</td><td>17.8</td><td>9.7</td><td>2.5</td><td>3.8</td><td>2.8</td><td>2.2</td><td>3.5</td></tr><tr><td>Phi 3.5 - L100 50</td><td>6.3</td><td>2.8</td><td>15.8</td><td>16.8</td><td>8.9</td><td>3.9</td><td>2.7</td><td>2.8</td><td>0.4</td><td>2.9</td></tr><tr><td>Phi 3.5 - PT 100</td><td>6.9</td><td>3.7</td><td>16.0</td><td>15.9</td><td>11.3</td><td>3.4</td><td>3.2</td><td>2.9</td><td>2.2</td><td>3.5</td></tr><tr><td>Phi 3.5 - PT 50</td><td>6.1</td><td>1.8</td><td>14.8</td><td>15.8</td><td>10.5</td><td>3.5</td><td>2.9</td><td>2.6</td><td>0.9</td><td>2.1</td></tr><tr><td>Phi 3.5 - PT 1</td><td>6.2</td><td>1.6</td><td>14.9</td><td>15.9</td><td>11.1</td><td>3.7</td><td>3.0</td><td>1.7</td><td>0.9</td><td>2.7</td></tr><tr><td>Llama 3 - L100 50</td><td>6.0</td><td>2.1</td><td>11.9</td><td>15.8</td><td>7.2</td><td>2.1</td><td>3.2</td><td>2.4</td><td>4.8</td><td>4.1</td></tr><tr><td>Llama 3 - PT 1</td><td>6.9</td><td>2.4</td><td>17.1</td><td>16.6</td><td>9.1</td><td>3.4</td><td>4.5</td><td>2.5</td><td>1.7</td><td>5.2</td></tr><tr><td>Llama 3 - PT 100</td><td>8.3</td><td>2.6</td><td>18.7</td><td>19.6</td><td>11.4</td><td>4.0</td><td>4.3</td><td>4.0</td><td>4.8</td><td>5.3</td></tr><tr><td>Gemma 2 - L100 50</td><td>4.3</td><td>1.7</td><td>11.1</td><td>8.1</td><td>7.1</td><td>3.0</td><td>2.3</td><td>2.1</td><td>1.7</td><td>1.7</td></tr><tr><td>Llama 3 - L100 50</td><td>6.0</td><td>2.1</td><td>11.9</td><td>15.8</td><td>7.2</td><td>2.1</td><td>3.2</td><td>2.4</td><td>4.8</td><td>4.1</td></tr><tr><td>Qwen 2.5 - L100 50</td><td>6.4</td><td>5.5</td><td>12.0</td><td>13.0</td><td>10.3</td><td>3.0</td><td>3.2</td><td>2.9</td><td>2.2</td><td>5.2</td></tr><tr><td>Aya-Expanse - L100 50</td><td>6.2</td><td>3.7</td><td>13.2</td><td>13.9</td><td>9.5</td><td>3.0</td><td>3.4</td><td>3.4</td><td>1.7</td><td>3.6</td></tr><tr><td>Centurio Aya</td><td>11.1</td><td>6.7</td><td>19.9</td><td>22.5</td><td>16.7</td><td>5.0</td><td>9.0</td><td>5.2</td><td>5.2</td><td>9.7</td></tr><tr><td>Centurio Qwen</td><td>11.9</td><td>4.6</td><td>22.7</td><td>26.5</td><td>18.6</td><td>5.9</td><td>9.9</td><td>5.0</td><td>5.2</td><td>8.9</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents the results of the MTVQA (Multilingual Text-heavy Visual Question Answering) task. It shows the performance of various large vision-language models across different languages, broken down by language group (tier), reflecting the models&rsquo; abilities to answer questions about images with text-heavy content. The results are expressed as average scores, indicating the accuracy of each model&rsquo;s answers. The table helps to analyze how well these models perform on this specific task, considering varying degrees of language resource availability.</p><details><summary>read the caption</summary>Table 24: MTVQA</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th></th><th>en</th><th>avg.</th><th>bn</th><th>de</th><th>id</th><th>ko</th><th>pt</th><th>ru</th><th>zh</th></tr></thead><tbody><tr><td>Phi 3.5 - English</td><td>59.7</td><td>37.2</td><td>4.9</td><td>47.8</td><td>33.2</td><td>38.2</td><td>47.1</td><td>42.1</td><td>47.2</td></tr><tr><td>Phi 3.5 - T5 50</td><td>54.1</td><td>34.1</td><td>2.6</td><td>44.6</td><td>34.3</td><td>36.3</td><td>43.8</td><td>36.4</td><td>41.0</td></tr><tr><td>Phi 3.5 - T5-4 50</td><td>52.0</td><td>37.4</td><td>5.7</td><td>45.6</td><td>38.7</td><td>40.4</td><td>45.2</td><td>43.4</td><td>42.7</td></tr><tr><td>Phi 3.5 - T5-3 50</td><td>54.8</td><td>40.6</td><td>22.7</td><td>46.5</td><td>42.1</td><td>39.8</td><td>46.0</td><td>43.6</td><td>43.6</td></tr><tr><td>Phi 3.5 - T5-2 50</td><td>57.8</td><td>45.3</td><td>27.4</td><td>50.3</td><td>46.0</td><td>46.4</td><td>48.6</td><td>49.5</td><td>48.9</td></tr><tr><td>Phi 3.5 - L100 50</td><td>56.6</td><td>45.1</td><td>27.0</td><td>51.4</td><td>44.8</td><td>44.9</td><td>50.8</td><td>48.2</td><td>48.7</td></tr><tr><td>Llama 3 - English</td><td>61.9</td><td>39.2</td><td>13.2</td><td>49.0</td><td>35.6</td><td>39.1</td><td>44.9</td><td>44.1</td><td>48.4</td></tr><tr><td>Llama 3 - T5 50</td><td>49.3</td><td>33.8</td><td>5.9</td><td>43.8</td><td>38.0</td><td>32.4</td><td>41.7</td><td>37.3</td><td>37.4</td></tr><tr><td>Llama 3 - L100 50</td><td>60.6</td><td>51.0</td><td>46.7</td><td>54.1</td><td>51.2</td><td>49.4</td><td>53.4</td><td>51.2</td><td>51.3</td></tr><tr><td>Phi 3.5 - L100 1</td><td>48.4</td><td>40.3</td><td>28.2</td><td>43.9</td><td>41.1</td><td>40.6</td><td>43.0</td><td>42.8</td><td>42.8</td></tr><tr><td>Phi 3.5 - L100 10</td><td>51.8</td><td>42.2</td><td>27.6</td><td>46.3</td><td>43.0</td><td>42.2</td><td>45.7</td><td>44.8</td><td>45.6</td></tr><tr><td>Phi 3.5 - L100 24</td><td>53.8</td><td>42.9</td><td>29.1</td><td>47.6</td><td>43.4</td><td>42.2</td><td>46.6</td><td>45.9</td><td>45.4</td></tr><tr><td>Phi 3.5 - L100 50</td><td>56.6</td><td>45.1</td><td>27.0</td><td>51.4</td><td>44.8</td><td>44.9</td><td>50.8</td><td>48.2</td><td>48.7</td></tr><tr><td>Phi 3.5 - L100 75</td><td>58.6</td><td>45.8</td><td>26.4</td><td>52.4</td><td>44.4</td><td>45.4</td><td>51.9</td><td>49.9</td><td>50.0</td></tr><tr><td>Phi 3.5 - L100 90</td><td>58.5</td><td>42.1</td><td>14.2</td><td>53.0</td><td>39.8</td><td>43.3</td><td>51.4</td><td>45.7</td><td>47.7</td></tr><tr><td>Llama 3 - L100 10</td><td>54.9</td><td>45.0</td><td>40.5</td><td>46.4</td><td>45.7</td><td>42.5</td><td>46.5</td><td>46.2</td><td>47.4</td></tr><tr><td>Llama 3 - L100 50</td><td>60.6</td><td>51.0</td><td>46.7</td><td>54.1</td><td>51.2</td><td>49.4</td><td>53.4</td><td>51.2</td><td>51.3</td></tr><tr><td>Llama 3 - L100 90</td><td>61.9</td><td>51.4</td><td>42.5</td><td>56.2</td><td>51.7</td><td>50.1</td><td>54.6</td><td>52.5</td><td>52.1</td></tr><tr><td>Phi 3.5 - L100 50</td><td>56.6</td><td>45.1</td><td>27.0</td><td>51.4</td><td>44.8</td><td>44.9</td><td>50.8</td><td>48.2</td><td>48.7</td></tr><tr><td>Phi 3.5 - PT 100</td><td>58.0</td><td>46.1</td><td>29.5</td><td>52.8</td><td>46.1</td><td>44.5</td><td>51.7</td><td>49.5</td><td>48.3</td></tr><tr><td>Phi 3.5 - PT 50</td><td>58.3</td><td>47.6</td><td>35.4</td><td>52.8</td><td>48.7</td><td>45.5</td><td>52.5</td><td>49.6</td><td>48.6</td></tr><tr><td>Phi 3.5 - PT 1</td><td>58.3</td><td>47.0</td><td>37.6</td><td>52.6</td><td>46.8</td><td>44.1</td><td>51.5</td><td>48.1</td><td>48.1</td></tr><tr><td>Llama 3 - L100 50</td><td>60.6</td><td>51.0</td><td>46.7</td><td>54.1</td><td>51.2</td><td>49.4</td><td>53.4</td><td>51.2</td><td>51.3</td></tr><tr><td>Llama 3 - PT 1</td><td>61.1</td><td>55.1</td><td>52.8</td><td>56.6</td><td>56.0</td><td>53.9</td><td>56.0</td><td>55.4</td><td>55.0</td></tr><tr><td>Llama 3 - PT 100</td><td>61.6</td><td>53.0</td><td>50.4</td><td>54.9</td><td>53.6</td><td>52.4</td><td>53.0</td><td>53.1</td><td>53.4</td></tr><tr><td>Gemma 2 - L100 50</td><td>56.5</td><td>47.5</td><td>43.9</td><td>51.6</td><td>47.6</td><td>44.2</td><td>50.1</td><td>47.5</td><td>47.5</td></tr><tr><td>Llama 3 - L100 50</td><td>60.6</td><td>51.0</td><td>46.7</td><td>54.1</td><td>51.2</td><td>49.4</td><td>53.4</td><td>51.2</td><td>51.3</td></tr><tr><td>Qwen 2.5 - L100 50</td><td>60.3</td><td>51.9</td><td>44.2</td><td>54.8</td><td>53.1</td><td>51.3</td><td>54.3</td><td>53.2</td><td>52.8</td></tr><tr><td>Aya-Expanse - L100 50</td><td>60.5</td><td>52.5</td><td>45.2</td><td>54.6</td><td>53.8</td><td>51.7</td><td>54.7</td><td>53.9</td><td>53.4</td></tr><tr><td><code>Centurio</code> Aya</td><td>59.1</td><td>53.2</td><td>43.4</td><td>56.9</td><td>54.4</td><td>53.6</td><td>56.2</td><td>54.0</td><td>54.3</td></tr><tr><td><code>Centurio</code> Qwen</td><td>60.6</td><td>54.8</td><td>49.9</td><td>57.0</td><td>54.9</td><td>53.5</td><td>57.2</td><td>55.8</td><td>55.6</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents the results of the xGQA (cross-lingual visual question answering) task, comparing the performance of various large vision-language models (LVLMs). The models are evaluated across multiple languages, with the scores reflecting their accuracy in answering questions about images. Different training setups are compared, including variations in the number of languages included in training and different proportions of English versus non-English data. This allows analysis of the tradeoffs between multilingual performance and the cost of obtaining multilingual training data. The table helps quantify the impact of various multilingual training strategies on the model&rsquo;s ability to understand and correctly respond to the questions across different languages.</p><details><summary>read the caption</summary>Table 25: xGQA</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption>Model Name | en | avg. | ar | bn | cs | da | de | el | es | fa | fi | fil | fr | he | hi | hr | hu | id | it | ja | ko | mi | nl | no | pl | pt | quz | ro | ru | sv | sw | te | th | tr | uk | vi | zh
&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;
Phi 3.5 - English | 33.6 | 1.2 | 0.0 | 0.0 | 0.7 | 1.1 | 1.7 | 0.0 | 10.5 | 0.0 | 0.4 | 1.6 | 4.4 | 0.0 | 0.0 | 0.5 | 0.6 | 1.5 | 9.2 | 0.1 | 0.0 | 0.0 | 1.6 | 1.5 | 0.7 | 1.9 | 0.1 | 0.7 | 0.4 | 1.1 | 0.6 | 0.0 | 0.2 | 0.3 | 0.1 | 0.6 | 0.0
Phi 3.5 - T5 50 | 33.0 | 9.5 | 7.8 | 0.6 | 3.9 | 8.2 | 24.7 | 0.6 | 34.4 | 0.4 | 1.8 | 1.9 | 39.1 | 3.4 | 3.5 | 2.6 | 4.0 | 7.9 | 28.5 | 27.6 | 1.6 | 0.1 | 20.4 | 8.8 | 4.8 | 30.2 | 0.7 | 5.4 | 17.5 | 11.8 | 1.3 | 0.0 | 4.0 | 3.2 | 5.7 | 2.6 | 12.8
Phi 3.5 - T5-4 50 | 25.2 | 11.8 | 6.9 | 1.0 | 13.8 | 10.8 | 24.4 | 1.4 | 27.3 | 8.6 | 7.0 | 3.6 | 31.3 | 3.5 | 9.7 | 9.2 | 10.5 | 8.4 | 24.9 | 27.9 | 3.1 | 2.1 | 21.7 | 12.0 | 14.1 | 24.0 | 0.5 | 6.0 | 22.5 | 24.1 | 2.1 | 0.0 | 5.8 | 9.4 | 4.6 | 18.8 | 10.6
Phi 3.5 - T5-3 50 | 32.7 | 13.6 | 6.5 | 5.9 | 13.8 | 18.2 | 25.4 | 7.0 | 31.0 | 7.1 | 5.7 | 11.9 | 30.7 | 7.6 | 7.2 | 9.4 | 8.2 | 22.9 | 26.0 | 27.3 | 3.0 | 1.8 | 28.5 | 14.1 | 13.6 | 22.1 | 0.4 | 11.0 | 16.7 | 23.8 | 1.5 | 0.0 | 12.9 | 7.6 | 10.0 | 15.9 | 20.2
Phi 3.5 - T5-2 50 | 29.9 | 11.1 | 4.5 | 5.5 | 10.7 | 12.4 | 20.4 | 6.4 | 20.4 | 5.7 | 5.5 | 10.1 | 24.8 | 7.4 | 6.7 | 8.0 | 7.1 | 17.5 | 18.5 | 27.0 | 2.5 | 2.0 | 18.9 | 11.2 | 10.0 | 18.1 | 0.4 | 7.6 | 21.1 | 17.8 | 7.0 | 0.0 | 12.6 | 7.9 | 10.3 | 12.7 | 9.5
Phi 3.5 - L100 50 | 31.0 | 13.2 | 5.6 | 3.3 | 10.9 | 18.0 | 26.4 | 4.5 | 30.9 | 4.1 | 4.4 | 11.1 | 38.5 | 6.7 | 6.3 | 7.3 | 7.8 | 22.4 | 30.6 | 23.7 | 2.5 | 2.6 | 24.2 | 18.8 | 10.8 | 29.0 | 1.1 | 8.1 | 18.6 | 17.8 | 8.1 | 1.5 | 10.2 | 7.2 | 8.3 | 14.7 | 15.6
Llama 3 - English | 75.6 | 1.1 | 0.1 | 0.0 | 1.3 | 2.3 | 1.6 | 0.1 | 2.1 | 0.1 | 0.8 | 2.9 | 3.4 | 0.0 | 0.0 | 0.7 | 1.3 | 2.1 | 1.5 | 0.2 | 0.0 | 0.2 | 3.0 | 1.8 | 1.1 | 2.6 | 0.8 | 1.2 | 0.7 | 2.0 | 0.8 | 0.0 | 0.4 | 0.5 | 0.3 | 0.5 | 0.3
Llama 3 - T5 50 | 76.1 | 12.6 | 27.9 | 0.5 | 1.6 | 20.7 | 29.2 | 0.6 | 61.6 | 0.4 | 1.1 | 2.9 | 58.2 | 0.0 | 0.4 | 0.9 | 1.6 | 19.6 | 26.8 | 35.8 | 0.1 | 0.2 | 34.9 | 15.7 | 11.3 | 12.5 | 1.3 | 5.4 | 12.8 | 18.3 | 0.6 | 0.0 | 4.4 | 6.1 | 0.2 | 10.1 | 17.2
Llama 3 - L100 50 | 72.6 | 28.5 | 25.6 | 14.0 | 30.5 | 38.1 | 27.6 | 23.3 | 56.0 | 24.3 | 13.8 | 29.0 | 50.9 | 15.5 | 22.5 | 19.7 | 18.0 | 39.9 | 44.1 | 33.8 | 13.4 | 24.9 | 50.1 | 41.5 | 26.9 | 45.1 | 1.3 | 22.6 | 23.5 | 42.7 | 28.9 | 11.0 | 27.7 | 21.8 | 21.9 | 49.7 | 16.9
Phi 3.5 - L100 1 | 43.3 | 13.3 | 5.2 | 4.3 | 11.1 | 15.3 | 24.5 | 5.3 | 25.4 | 5.4 | 6.1 | 13.3 | 37.1 | 7.0 | 7.2 | 7.9 | 6.7 | 22.6 | 28.1 | 25.4 | 2.1 | 4.0 | 22.3 | 17.7 | 12.0 | 24.9 | 1.4 | 10.7 | 21.4 | 17.8 | 11.1 | 1.4 | 12.6 | 7.6 | 8.2 | 17.2 | 16.9
Phi 3.5 - L100 10 | 38.9 | 12.7 | 4.7 | 4.1 | 11.4 | 12.5 | 23.6 | 5.9 | 28.1 | 4.8 | 5.3 | 14.2 | 33.3 | 8.5 | 8.6 | 8.2 | 6.3 | 22.6 | 23.3 | 25.2 | 2.4 | 3.0 | 23.0 | 15.6 | 12.0 | 25.8 | 0.7 | 8.2 | 20.0 | 15.9 | 9.9 | 1.8 | 11.8 | 6.7 | 11.9 | 14.7 | 11.8
Phi 3.5 - L100 24 | 31.5 | 13.2 | 5.2 | 5.0 | 11.6 | 15.1 | 22.6 | 6.9 | 30.9 | 4.9 | 6.1 | 12.0 | 39.2 | 8.2 | 7.7 | 8.2 | 5.1 | 22.4 | 24.7 | 25.8 | 3.3 | 4.8 | 24.8 | 18.5 | 13.6 | 17.7 | 0.7 | 9.1 | 17.8 | 17.5 | 9.3 | 2.3 | 13.3 | 6.5 | 9.6 | 15.4 | 14.4
Phi 3.5 - L100 50 | 31.0 | 13.2 | 5.6 | 3.3 | 10.9 | 18.0 | 26.4 | 4.5 | 30.9 | 4.1 | 4.4 | 11.1 | 38.5 | 6.7 | 6.3 | 7.3 | 7.8 | 22.4 | 30.6 | 23.7 | 2.5 | 2.6 | 24.2 | 18.8 | 10.8 | 29.0 | 1.1 | 8.1 | 18.6 | 17.8 | 8.1 | 1.5 | 10.2 | 7.2 | 8.3 | 14.7 | 15.6
Phi 3.5 - L100 75 | 36.5 | 12.0 | 4.4 | 2.5 | 9.1 | 13.6 | 25.0 | 3.0 | 25.7 | 3.4 | 3.8 | 7.1 | 33.6 | 6.3 | 5.2 | 5.9 | 7.0 | 20.2 | 29.7 | 24.8 | 2.0 | 3.3 | 23.0 | 17.1 | 9.8 | 27.8 | 0.8 | 6.2 | 19.6 | 15.2 | 5.0 | 1.4 | 10.9 | 5.7 | 8.9 | 13.0 | 18.5
Phi 3.5 - L100 90 | 34.2 | 9.4 | 4.0 | 1.9 | 6.7 | 9.2 | 21.8 | 2.8 | 23.4 | 2.0 | 3.8 | 4.1 | 26.2 | 4.4 | 3.7 | 4.7 | 4.9 | 12.5 | 21.3 | 21.3 | 2.0 | 1.2 | 12.7 | 11.8 | 7.3 | 22.5 | 0.8 | 5.9 | 16.3 | 16.1 | 5.6 | 0.6 | 7.5 | 3.9 | 7.8 | 8.8 | 20.3
Llama 3 - L100 10 | 74.8 | 28.9 | 23.0 | 11.9 | 25.8 | 43.6 | 26.0 | 24.6 | 53.7 | 24.9 | 16.0 | 30.2 | 52.6 | 17.1 | 20.1 | 20.5 | 18.5 | 43.3 | 40.3 | 35.0 | 13.9 | 29.4 | 53.4 | 41.9 | 25.6 | 44.8 | 1.6 | 19.8 | 25.3 | 44.0 | 30.3 | 13.8 | 28.8 | 22.1 | 21.1 | 47.4 | 20.2
Llama 3 - L100 50 | 72.6 | 28.5 | 25.6 | 14.0 | 30.5 | 38.1 | 27.6 | 23.3 | 56.0 | 24.3 | 13.8 | 29.0 | 50.9 | 15.5 | 22.5 | 19.7 | 18.0 | 39.9 | 44.1 | 33.8 | 13.4 | 24.9 | 50.1 | 41.5 | 26.9 | 45.1 | 1.3 | 22.6 | 23.5 | 42.7 | 28.9 | 11.0 | 27.7 | 21.8 | 21.9 | 49.7 | 16.9
Llama 3 - L100 90 | 73.6 | 23.0 | 18.2 | 7.8 | 24.1 | 36.7 | 23.0 | 19.5 | 54.2 | 17.6 | 10.5 | 24.0 | 51.9 | 9.7 | 20.2 | 15.4 | 15.3 | 33.0 | 38.0 | 25.6 | 10.4 | 17.5 | 46.1 | 33.1 | 19.8 | 41.2 | 0.2 | 17.1 | 20.6 | 38.1 | 14.6 | 5.8 | 23.2 | 16.3 | 0.3 | 43.9 | 13.3
Phi 3.5 - L100 50 | 31.0 | 13.2 | 5.6 | 3.3 | 10.9 | 18.0 | 26.4 | 4.5 | 30.9 | 4.1 | 4.4 | 11.1 | 38.5 | 6.7 | 6.3 | 7.3 | 7.8 | 22.4 | 30.6 | 23.7 | 2.5 | 2.6 | 24.2 | 18.8 | 10.8 | 29.0 | 1.1 | 8.1 | 18.6 | 17.8 | 8.1 | 1.5 | 10.2 | 7.2 | 8.3 | 14.7 | 15.6
Phi 3.5 - PT 100 | 35.9 | 13.5 | 5.3 | 5.0 | 13.9 | 15.7 | 26.5 | 5.9 | 29.6 | 5.4 | 4.1 | 9.1 | 33.5 | 8.3 | 6.9 | 8.8 | 7.1 | 22.3 | 30.3 | 25.7 | 2.7 | 3.9 | 21.6 | 20.1 | 12.0 | 21.8 | 0.9 | 9.5 | 19.5 | 18.9 | 8.5 | 1.4 | 13.6 | 7.5 | 8.5 | 14.9 | 23.9
Phi 3.5 - PT 50 | 37.1 | 17.3 | 7.7 | 9.0 | 16.5 | 21.2 | 27.8 | 9.3 | 38.0 | 8.2 | 7.0 | 15.2 | 42.4 | 10.9 | 10.4 | 11.8 | 9.7 | 28.5 | 33.9 | 26.2 | 3.2 | 7.2 | 30.0 | 24.7 | 16.1 | 29.1 | 2.5 | 14.7 | 21.3 | 24.1 | 15.3 | 4.3 | 18.6 | 8.0 | 9.8 | 19.4 | 22.3
Phi 3.5 - PT 1 | 33.1 | 17.4 | 6.3 | 9.3 | 17.2 | 22.1 | 26.9 | 8.2 | 37.5 | 9.1 | 7.2 | 13.9 | 40.6 | 12.2 | 9.1 | 11.5 | 11.1 | 28.9 | 34.8 | 30.5 | 2.9 | 7.9 | 27.7 | 26.4 | 14.9 | 31.2 | 2.3 | 14.4 | 22.4 | 23.8 | 14.7 | 4.4 | 18.4 | 10.8 | 10.5 | 18.8 | 21.1
Llama 3 - L100 50 | 72.6 | 28.5 | 25.6 | 14.0 | 30.5 | 38.1 | 27.6 | 23.3 | 56.0 | 24.3 | 13.8 | 29.0 | 50.9 | 15.5 | 22.5 | 19.7 | 18.0 | 39.9 | 44.1 | 33.8 | 13.4 | 24.9 | 50.1 | 41.5 | 26.9 | 45.1 | 1.3 | 22.6 | 23.5 | 42.7 | 28.9 | 11.0 | 27.7 | 21.8 | 21.9 | 49.7 | 16.9
Llama 3 - PT 1 | 80.8 | 35.3 | 30.6 | 15.4 | 35.5 | 51.3 | 34.0 | 28.2 | 65.4 | 32.3 | 17.9 | 36.3 | 62.5 | 24.6 | 27.4 | 26.9 | 24.7 | 49.2 | 51.6 | 38.7 | 15.2 | 35.9 | 59.1 | 49.2 | 32.5 | 51.1 | 2.9 | 30.7 | 32.3 | 51.8 | 38.0 | 17.4 | 36.2 | 29.3 | 26.4 | 58.1 | 16.5
Llama 3 - PT 100 | 77.9 | 31.8 | 26.1 | 14.4 | 35.4 | 43.5 | 33.4 | 27.0 | 60.7 | 23.0 | 14.6 | 31.7 | 58.9 | 18.2 | 24.6 | 22.2 | 22.6 | 45.2 | 49.2 | 35.9 | 14.0 | 26.5 | 55.3 | 45.8 | 32.3 | 51.6 | 0.9 | 25.2 | 30.7 | 47.7 | 29.3 | 12.4 | 32.0 | 27.0 | 25.2 | 56.0 | 15.2
Gemma 2 - L100 50 | 66.6 | 27.5 | 24.5 | 17.9 | 28.6 | 35.1 | 26.0 | 18.2 | 54.7 | 29.4 | 13.7 | 26.8 | 54.3 | 22.8 | 21.6 | 17.7 | 20.1 | 43.8 | 39.7 | 36.3 | 11.5 | 21.5 | 46.2 | 38.7 | 25.1 | 45.3 | 1.8 | 21.7 | 24.0 | 37.9 | 27.2 | 13.1 | 28.5 | 19.0 | 0.2 | 50.1 | 18.5
Llama 3 - L100 50 | 72.6 | 28.5 | 25.6 | 14.0 | 30.5 | 38.1 | 27.6 | 23.3 | 56.0 | 24.3 | 13.8 | 29.0 | 50.9 | 15.5 | 22.5 | 19.7 | 18.0 | 39.9 | 44.1 | 33.8 | 13.4 | 24.9 | 50.1 | 41.5 | 26.9 | 45.1 | 1.3 | 22.6 | 23.5 | 42.7 | 28.9 | 11.0 | 27.7 | 21.8 | 21.9 | 49.7 | 16.9
Qwen 2.5 - L100 50 | 74.8 | 27.8 | 28.6 | 13.9 | 26.1 | 35.4 | 29.6 | 11.6 | 58.7 | 17.1 | 10.2 | 26.0 | 55.6 | 22.2 | 16.3 | 19.8 | 11.7 | 40.3 | 43.8 | 39.0 | 13.4 | 21.8 | 48.9 | 36.6 | 25.0 | 52.3 | 0.9 | 16.9 | 33.6 | 36.7 | 18.9 | 8.0 | 38.1 | 17.4 | 18.1 | 59.5 | 19.9
Aya-Expanse - L100 50 | 75.6 | 33.4 | 40.4 | 12.2 | 39.4 | 37.7 | 32.7 | 31.9 | 69.2 | 41.6 | 8.4 | 26.2 | 67.6 | 42.5 | 24.5 | 19.1 | 12.5 | 50.3 | 53.6 | 39.5 | 18.4 | 20.1 | 59.6 | 36.3 | 35.5 | 50.6 | 0.7 | 31.3 | 31.9 | 34.9 | 21.3 | 9.2 | 19.0 | 29.8 | 27.7 | 68.2 | 26.1
Centurio Aya | 78.4 | 39.2 | 40.4 | 18.5 | 33.9 | 40.0 | 38.6 | 35.3 | 69.7 | 55.8 | 11.0 | 34.0 | 71.3 | 47.1 | 26.3 | 24.9 | 19.6 | 58.3 | 60.4 | 49.1 | 21.3 | 33.7 | 61.7 | 42.5 | 37.9 | 59.3 | 1.7 | 34.6 | 38.0 | 45.9 | 29.9 | 15.1 | 26.0 | 30.6 | 30.6 | 72.7 | 56.9
Centurio Qwen | 79.1 | 34.4 | 36.6 | 17.1 | 29.7 | 43.1 | 32.0 | 19.2 | 69.2 | 31.2 | 12.0 | 33.6 | 67.6 | 27.6 | 20.3 | 22.0 | 18.7 | 50.4 | 53.7 | 43.5 | 13.4 | 34.9 | 56.2 | 41.4 | 30.0 | 59.9 | 2.1 | 23.4 | 39.2 | 42.7 | 30.2 | 13.5 | 42.3 | 23.3 | 20.3 | 69.4 | 33.8</table></figure><blockquote><p>üîº This table presents the results of the XM3600 image captioning task, comparing the performance of various models across multiple languages. The rows represent different model configurations and training setups, while the columns correspond to individual languages, with English denoted as &rsquo;en&rsquo; and multilingual results as &lsquo;mul&rsquo;. Metrics include the CIDEr score (a metric for evaluating image caption quality) and language fidelity. This table allows us to analyze how different training approaches affect multilingual performance on image captioning, showing scores for each language and the average across all languages.</p><details><summary>read the caption</summary>Table 26: XM3600</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th></th><th>ar</th><th>bn</th><th>cs</th><th>da</th><th>de</th><th>el</th><th>en</th><th>es</th><th>fa</th><th>fi</th><th>fil</th><th>fr</th><th>he</th><th>hi</th><th>hr</th><th>hu</th><th>id</th><th>it</th><th>ja</th><th>ko</th><th>mi</th><th>nl</th><th>no</th><th>pl</th><th>pt</th><th>quz</th><th>ro</th><th>ru</th><th>sv</th><th>sw</th><th>te</th><th>th</th><th>tr</th><th>uk</th><th>vi</th><th>zh</th></tr></thead><tbody><tr><td>Phi 3.5 - L100</td><td>93.8</td><td>99.0</td><td>99.0</td><td>91.8</td><td>100.0</td><td>88.3</td><td>100.0</td><td>100.0</td><td>98.8</td><td>100.0</td><td>96.7</td><td>100.0</td><td>99.6</td><td>97.7</td><td>76.2</td><td>100.0</td><td>95.3</td><td>100.0</td><td>99.2</td><td>99.8</td><td>100.0</td><td>100.0</td><td>98.4</td><td>100.0</td><td>97.7</td><td>0.2</td><td>100.0</td><td>99.8</td><td>98.6</td><td>98.2</td><td>93.2</td><td>99.8</td><td>100.0</td><td>92.6</td><td>100.0</td><td>96.5</td></tr><tr><td>Phi 3.5 - T5-2</td><td>94.7</td><td>100.0</td><td>99.4</td><td>98.4</td><td>100.0</td><td>99.8</td><td>100.0</td><td>100.0</td><td>100.0</td><td>100.0</td><td>99.6</td><td>100.0</td><td>100.0</td><td>98.8</td><td>79.1</td><td>100.0</td><td>88.5</td><td>100.0</td><td>99.8</td><td>100.0</td><td>97.3</td><td>100.0</td><td>85.7</td><td>100.0</td><td>99.2</td><td>27.5</td><td>100.0</td><td>99.8</td><td>99.6</td><td>99.0</td><td>63.5</td><td>100.0</td><td>100.0</td><td>96.9</td><td>100.0</td><td>93.9</td></tr><tr><td>Phi 3.5 - T5-3</td><td>92.2</td><td>99.8</td><td>99.2</td><td>97.9</td><td>100.0</td><td>99.8</td><td>100.0</td><td>100.0</td><td>100.0</td><td>100.0</td><td>99.6</td><td>100.0</td><td>99.8</td><td>99.0</td><td>77.9</td><td>100.0</td><td>91.8</td><td>100.0</td><td>100.0</td><td>100.0</td><td>95.9</td><td>100.0</td><td>75.2</td><td>100.0</td><td>52.1</td><td>37.5</td><td>100.0</td><td>100.0</td><td>99.2</td><td>84.4</td><td>82.8</td><td>100.0</td><td>100.0</td><td>96.5</td><td>100.0</td><td>95.3</td></tr><tr><td>Phi 3.5 - T5-4</td><td>96.5</td><td>96.1</td><td>99.2</td><td>78.3</td><td>100.0</td><td>98.4</td><td>100.0</td><td>100.0</td><td>100.0</td><td>100.0</td><td>99.6</td><td>100.0</td><td>99.4</td><td>98.6</td><td>89.8</td><td>100.0</td><td>98.0</td><td>100.0</td><td>100.0</td><td>100.0</td><td>95.7</td><td>100.0</td><td>71.5</td><td>100.0</td><td>100.0</td><td>12.7</td><td>100.0</td><td>100.0</td><td>100.0</td><td>84.6</td><td>67.2</td><td>99.0</td><td>100.0</td><td>30.9</td><td>100.0</td><td>94.1</td></tr><tr><td>Phi 3.5 - T5</td><td>98.2</td><td>97.3</td><td>78.7</td><td>87.5</td><td>100.0</td><td>50.2</td><td>100.0</td><td>99.8</td><td>2.3</td><td>76.4</td><td>49.4</td><td>100.0</td><td>96.7</td><td>99.2</td><td>73.6</td><td>99.2</td><td>95.1</td><td>100.0</td><td>100.0</td><td>99.8</td><td>1.6</td><td>99.8</td><td>92.0</td><td>96.5</td><td>100.0</td><td>0.2</td><td>96.3</td><td>100.0</td><td>98.6</td><td>36.1</td><td>62.5</td><td>90.2</td><td>94.9</td><td>91.6</td><td>39.8</td><td>96.7</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents the language fidelity results for the XM3600 dataset, focusing on the ability of different language models to produce outputs in the correct target language. The results are broken down by language tier and model configuration, providing a detailed view of language-specific performance.</p><details><summary>read the caption</summary>Table 27: XM3600 language fidelity (¬ß1(b))</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th></th><th>en</th><th>avg.</th><th>ar</th><th>es</th><th>fr</th><th>ru</th></tr></thead><tbody><tr><td>Phi 3.5 - English</td><td>59.6</td><td>55.0</td><td>52.3</td><td>54.9</td><td>57.6</td><td>55.2</td></tr><tr><td>Phi 3.5 - T5 50</td><td>59.9</td><td>51.8</td><td>49.7</td><td>51.3</td><td>55.2</td><td>51.0</td></tr><tr><td>Phi 3.5 - T5-4 50</td><td>58.9</td><td>48.3</td><td>47.7</td><td>47.1</td><td>51.2</td><td>47.3</td></tr><tr><td>Phi 3.5 - T5-3 50</td><td>58.6</td><td>50.5</td><td>46.6</td><td>51.3</td><td>52.6</td><td>51.3</td></tr><tr><td>Phi 3.5 - T5-2 50</td><td>58.5</td><td>53.6</td><td>50.7</td><td>54.7</td><td>55.1</td><td>54.0</td></tr><tr><td>Phi 3.5 - L100 50</td><td>59.6</td><td>53.3</td><td>49.9</td><td>53.7</td><td>56.8</td><td>52.6</td></tr><tr><td>Llama 3 - English</td><td>46.1</td><td>36.3</td><td>33.4</td><td>37.0</td><td>36.5</td><td>38.2</td></tr><tr><td>Llama 3 - T5 50</td><td>45.4</td><td>37.5</td><td>36.6</td><td>36.1</td><td>38.7</td><td>38.7</td></tr><tr><td>Llama 3 - L100 50</td><td>59.7</td><td>54.8</td><td>53.0</td><td>54.7</td><td>56.3</td><td>55.4</td></tr><tr><td>Phi 3.5 - L100 1</td><td>55.2</td><td>48.2</td><td>42.2</td><td>50.6</td><td>51.1</td><td>48.8</td></tr><tr><td>Phi 3.5 - L100 10</td><td>58.3</td><td>53.4</td><td>50.5</td><td>54.3</td><td>55.7</td><td>53.1</td></tr><tr><td>Phi 3.5 - L100 24</td><td>58.2</td><td>48.4</td><td>43.5</td><td>50.3</td><td>52.7</td><td>47.3</td></tr><tr><td>Phi 3.5 - L100 50</td><td>59.6</td><td>53.3</td><td>49.9</td><td>53.7</td><td>56.8</td><td>52.6</td></tr><tr><td>Phi 3.5 - L100 75</td><td>61.9</td><td>54.5</td><td>50.3</td><td>56.2</td><td>57.5</td><td>54.2</td></tr><tr><td>Phi 3.5 - L100 90</td><td>60.0</td><td>50.5</td><td>43.6</td><td>53.8</td><td>55.1</td><td>49.5</td></tr><tr><td>Llama 3 - L100 10</td><td>60.8</td><td>55.3</td><td>56.3</td><td>52.6</td><td>55.0</td><td>57.1</td></tr><tr><td>Llama 3 - L100 50</td><td>59.7</td><td>54.8</td><td>53.0</td><td>54.7</td><td>56.3</td><td>55.4</td></tr><tr><td>Llama 3 - L100 90</td><td>57.8</td><td>51.1</td><td>48.9</td><td>52.3</td><td>52.1</td><td>51.0</td></tr><tr><td>Phi 3.5 - L100 50</td><td>59.6</td><td>53.3</td><td>49.9</td><td>53.7</td><td>56.8</td><td>52.6</td></tr><tr><td>Phi 3.5 - PT 100</td><td>54.3</td><td>45.4</td><td>40.1</td><td>47.8</td><td>49.4</td><td>44.4</td></tr><tr><td>Phi 3.5 - PT 50</td><td>58.9</td><td>52.5</td><td>49.0</td><td>53.8</td><td>54.6</td><td>52.5</td></tr><tr><td>Phi 3.5 - PT 1</td><td>56.8</td><td>49.7</td><td>46.8</td><td>49.6</td><td>53.9</td><td>48.6</td></tr><tr><td>Llama 3 - L100 50</td><td>59.7</td><td>54.8</td><td>53.0</td><td>54.7</td><td>56.3</td><td>55.4</td></tr><tr><td>Llama 3 - PT 1</td><td>61.7</td><td>59.4</td><td>58.8</td><td>59.0</td><td>60.0</td><td>59.7</td></tr><tr><td>Llama 3 - PT 100</td><td>60.3</td><td>57.3</td><td>56.5</td><td>56.5</td><td>58.3</td><td>57.8</td></tr><tr><td>Gemma 2 - L100 50</td><td>59.9</td><td>55.0</td><td>53.1</td><td>54.6</td><td>57.1</td><td>55.1</td></tr><tr><td>Llama 3 - L100 50</td><td>59.7</td><td>54.8</td><td>53.0</td><td>54.7</td><td>56.3</td><td>55.4</td></tr><tr><td>Qwen 2.5 - L100 50</td><td>57.8</td><td>52.6</td><td>55.7</td><td>47.5</td><td>52.5</td><td>54.8</td></tr><tr><td>Aya-Expanse - L100 50</td><td>58.2</td><td>54.7</td><td>54.7</td><td>54.0</td><td>56.4</td><td>53.5</td></tr><tr><td>Centurio Aya</td><td>65.0</td><td>62.4</td><td>61.7</td><td>61.0</td><td>64.3</td><td>62.7</td></tr><tr><td>Centurio Qwen</td><td>75.4</td><td>70.2</td><td>68.8</td><td>70.9</td><td>70.5</td><td>70.8</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents the results of the XVNLI (Cross-lingual Visual Natural Language Inference) task across various multilingual vision-language models. XVNLI assesses a model&rsquo;s ability to determine the relationship (entailment, contradiction, or neutral) between a given textual hypothesis and a pair of images. The table compares performance across different models and language distributions during training, focusing on English and other languages. Metrics used are likely accuracy scores and may also include standard deviations for each language or language group.</p><details><summary>read the caption</summary>Table 28: XVNLI</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th></th><th>en</th><th>avg.</th><th>ar</th><th>fr</th><th>hi</th><th>id</th><th>ja</th><th>pt</th></tr></thead><tbody><tr><td>Phi 3.5 - English</td><td>38.4</td><td>36.2</td><td>36.2</td><td>41.9</td><td>29.9</td><td>35.4</td><td>34.2</td><td>39.7</td></tr><tr><td>Phi 3.5 - T5 50</td><td>36.7</td><td>36.2</td><td>31.5</td><td>38.9</td><td>31.6</td><td>37.0</td><td>34.9</td><td>43.4</td></tr><tr><td>Phi 3.5 - T5-4 50</td><td>37.0</td><td>33.9</td><td>33.2</td><td>39.9</td><td>29.2</td><td>32.3</td><td>31.2</td><td>37.7</td></tr><tr><td>Phi 3.5 - T5-3 50</td><td>37.3</td><td>35.8</td><td>32.5</td><td>39.3</td><td>32.3</td><td>37.0</td><td>36.8</td><td>37.0</td></tr><tr><td>Phi 3.5 - T5-2 50</td><td>37.6</td><td>35.1</td><td>32.2</td><td>40.3</td><td>32.6</td><td>34.7</td><td>32.3</td><td>38.7</td></tr><tr><td>Phi 3.5 - L100 50</td><td>36.6</td><td>32.0</td><td>28.5</td><td>35.9</td><td>27.8</td><td>32.0</td><td>31.2</td><td>36.7</td></tr><tr><td>Llama 3 - English</td><td>33.2</td><td>32.4</td><td>30.9</td><td>34.2</td><td>30.6</td><td>32.7</td><td>30.5</td><td>35.7</td></tr><tr><td>Llama 3 - T5 50</td><td>33.4</td><td>32.4</td><td>34.9</td><td>36.6</td><td>28.9</td><td>31.3</td><td>30.9</td><td>31.6</td></tr><tr><td>Llama 3 - L100 50</td><td>33.0</td><td>31.7</td><td>31.5</td><td>34.6</td><td>34.0</td><td>31.6</td><td>27.9</td><td>30.6</td></tr><tr><td>Phi 3.5 - L100 1</td><td>37.3</td><td>34.1</td><td>32.5</td><td>40.3</td><td>30.9</td><td>31.3</td><td>31.6</td><td>37.7</td></tr><tr><td>Phi 3.5 - L100 10</td><td>36.1</td><td>30.9</td><td>27.5</td><td>33.9</td><td>28.2</td><td>28.6</td><td>32.7</td><td>34.7</td></tr><tr><td>Phi 3.5 - L100 24</td><td>34.4</td><td>31.9</td><td>28.5</td><td>35.9</td><td>29.2</td><td>30.3</td><td>33.5</td><td>34.0</td></tr><tr><td>Phi 3.5 - L100 50</td><td>36.6</td><td>32.0</td><td>28.5</td><td>35.9</td><td>27.8</td><td>32.0</td><td>31.2</td><td>36.7</td></tr><tr><td>Phi 3.5 - L100 75</td><td>36.2</td><td>33.2</td><td>31.9</td><td>38.9</td><td>29.2</td><td>32.7</td><td>29.0</td><td>37.4</td></tr><tr><td>Phi 3.5 - L100 90</td><td>37.1</td><td>31.9</td><td>30.5</td><td>35.6</td><td>25.8</td><td>31.0</td><td>33.8</td><td>34.7</td></tr><tr><td>Llama 3 - L100 10</td><td>32.6</td><td>30.0</td><td>26.8</td><td>31.5</td><td>26.8</td><td>31.6</td><td>32.0</td><td>31.3</td></tr><tr><td>Llama 3 - L100 50</td><td>33.0</td><td>31.7</td><td>31.5</td><td>34.6</td><td>34.0</td><td>31.6</td><td>27.9</td><td>30.6</td></tr><tr><td>Llama 3 - L100 90</td><td>32.7</td><td>33.5</td><td>30.5</td><td>35.9</td><td>30.9</td><td>35.4</td><td>31.2</td><td>37.0</td></tr><tr><td>Phi 3.5 - L100 50</td><td>36.6</td><td>32.0</td><td>28.5</td><td>35.9</td><td>27.8</td><td>32.0</td><td>31.2</td><td>36.7</td></tr><tr><td>Phi 3.5 - PT 100</td><td>33.4</td><td>30.2</td><td>28.5</td><td>32.9</td><td>28.9</td><td>30.0</td><td>27.5</td><td>33.3</td></tr><tr><td>Phi 3.5 - PT 50</td><td>35.0</td><td>33.4</td><td>30.9</td><td>39.3</td><td>33.7</td><td>31.0</td><td>30.5</td><td>35.0</td></tr><tr><td>Phi 3.5 - PT 1</td><td>36.0</td><td>31.3</td><td>26.5</td><td>35.9</td><td>29.2</td><td>32.0</td><td>28.3</td><td>36.0</td></tr><tr><td>Llama 3 - L100 50</td><td>33.0</td><td>31.7</td><td>31.5</td><td>34.6</td><td>34.0</td><td>31.6</td><td>27.9</td><td>30.6</td></tr><tr><td>Llama 3 - PT 1</td><td>38.6</td><td>35.2</td><td>33.9</td><td>34.2</td><td>34.0</td><td>35.0</td><td>36.1</td><td>38.0</td></tr><tr><td>Llama 3 - PT 100</td><td>36.9</td><td>36.1</td><td>34.6</td><td>36.2</td><td>36.8</td><td>36.7</td><td>36.1</td><td>36.0</td></tr><tr><td>Gemma 2 - L100 50</td><td>32.8</td><td>32.0</td><td>32.5</td><td>30.9</td><td>33.0</td><td>30.6</td><td>32.7</td><td>32.0</td></tr><tr><td>Llama 3 - L100 50</td><td>33.0</td><td>31.7</td><td>31.5</td><td>34.6</td><td>34.0</td><td>31.6</td><td>27.9</td><td>30.6</td></tr><tr><td>Qwen 2.5 - L100 50</td><td>39.8</td><td>39.7</td><td>38.6</td><td>40.3</td><td>34.4</td><td>40.7</td><td>38.7</td><td>45.5</td></tr><tr><td>Aya-Expanse - L100 50</td><td>36.8</td><td>35.4</td><td>34.9</td><td>35.2</td><td>37.5</td><td>36.4</td><td>34.6</td><td>33.7</td></tr><tr><td><code>Centurio</code> Aya</td><td>37.6</td><td>37.2</td><td>36.2</td><td>38.9</td><td>38.8</td><td>39.7</td><td>34.2</td><td>35.4</td></tr><tr><td><code>Centurio</code> Qwen</td><td>46.4</td><td>43.0</td><td>39.6</td><td>45.0</td><td>41.6</td><td>44.1</td><td>43.5</td><td>44.1</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents the results of the xMMMU (cross-lingual, multi-modal, multiple-choice visual question answering) task. It compares the performance of various large vision-language models (LVLMs) across different language tiers, including English and several multilingual models with varied English data composition in their training. The evaluation metrics likely includes accuracy scores for each language tier. It allows for analysis of how many languages can be included in model training, optimal language distributions in pre-training and instruction tuning, and strategies to enhance multilingual text understanding. The table likely shows the impact of various training strategies on performance across different languages.</p><details><summary>read the caption</summary>Table 29: xMMMU</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th></th><th>en</th><th>avg.</th><th>avg. Latin</th><th>avg. other</th><th>ar</th><th>de</th><th>hi</th><th>id</th><th>it</th><th>ko</th><th>ru</th><th>th</th><th>zh</th><th>zu</th></tr></thead><tbody><tr><td>Phi 3.5 - English</td><td>65.8</td><td>55.8</td><td>62.3</td><td>51.5</td><td>50.2</td><td>63.5</td><td>58.5</td><td>61.4</td><td>64.0</td><td>49.0</td><td>52.1</td><td>49.1</td><td>49.8</td><td>60.2</td></tr><tr><td>Phi 3.5 - T5 50</td><td>75.2</td><td>60.2</td><td>70.9</td><td>53.1</td><td>50.2</td><td>70.8</td><td>65.4</td><td>71.8</td><td>71.6</td><td>49.8</td><td>54.1</td><td>51.0</td><td>48.0</td><td>69.4</td></tr><tr><td>Phi 3.5 - T5-4 50</td><td>74.2</td><td>60.8</td><td>71.4</td><td>53.7</td><td>52.2</td><td>71.5</td><td>65.5</td><td>72.8</td><td>73.1</td><td>51.1</td><td>53.9</td><td>49.6</td><td>49.6</td><td>68.4</td></tr><tr><td>Phi 3.5 - T5-3 50</td><td>70.4</td><td>58.7</td><td>67.7</td><td>52.8</td><td>51.6</td><td>66.9</td><td>61.0</td><td>69.6</td><td>67.2</td><td>50.0</td><td>53.6</td><td>48.9</td><td>51.4</td><td>67.0</td></tr><tr><td>Phi 3.5 - T5-2 50</td><td>68.4</td><td>56.2</td><td>64.2</td><td>50.8</td><td>49.5</td><td>64.5</td><td>58.4</td><td>65.4</td><td>64.9</td><td>50.0</td><td>50.5</td><td>48.8</td><td>47.9</td><td>62.0</td></tr><tr><td>Phi 3.5 - L100 50</td><td>69.6</td><td>58.0</td><td>67.2</td><td>51.9</td><td>49.9</td><td>68.0</td><td>62.4</td><td>69.0</td><td>67.9</td><td>48.6</td><td>52.5</td><td>49.6</td><td>48.4</td><td>64.1</td></tr><tr><td>Llama 3 - English</td><td>72.0</td><td>60.5</td><td>69.6</td><td>54.4</td><td>53.5</td><td>69.9</td><td>67.2</td><td>71.1</td><td>70.9</td><td>48.9</td><td>57.5</td><td>50.1</td><td>49.4</td><td>66.5</td></tr><tr><td>Llama 3 - T5 50</td><td>73.4</td><td>62.2</td><td>72.5</td><td>55.4</td><td>54.5</td><td>72.2</td><td>67.1</td><td>74.4</td><td>71.5</td><td>50.5</td><td>56.6</td><td>51.6</td><td>51.9</td><td>72.0</td></tr><tr><td>Llama 3 - L100 50</td><td>72.0</td><td>58.4</td><td>67.9</td><td>52.1</td><td>51.6</td><td>69.6</td><td>62.0</td><td>65.9</td><td>70.4</td><td>49.9</td><td>52.0</td><td>48.8</td><td>48.4</td><td>65.6</td></tr><tr><td>Phi 3.5 - L100 1</td><td>58.4</td><td>52.6</td><td>55.7</td><td>50.5</td><td>50.2</td><td>55.2</td><td>53.5</td><td>57.5</td><td>56.4</td><td>49.6</td><td>50.9</td><td>48.2</td><td>50.5</td><td>53.5</td></tr><tr><td>Phi 3.5 - L100 10</td><td>56.9</td><td>51.6</td><td>54.9</td><td>49.4</td><td>48.5</td><td>54.8</td><td>49.6</td><td>55.1</td><td>56.8</td><td>50.5</td><td>48.2</td><td>49.6</td><td>50.0</td><td>52.9</td></tr><tr><td>Phi 3.5 - L100 24</td><td>60.4</td><td>54.0</td><td>58.8</td><td>50.8</td><td>51.8</td><td>58.9</td><td>54.5</td><td>58.1</td><td>60.0</td><td>50.0</td><td>51.1</td><td>48.4</td><td>49.2</td><td>58.0</td></tr><tr><td>Phi 3.5 - L100 50</td><td>69.6</td><td>58.0</td><td>67.2</td><td>51.9</td><td>49.9</td><td>68.0</td><td>62.4</td><td>69.0</td><td>67.9</td><td>48.6</td><td>52.5</td><td>49.6</td><td>48.4</td><td>64.1</td></tr><tr><td>Phi 3.5 - L100 75</td><td>74.5</td><td>61.2</td><td>71.6</td><td>54.2</td><td>53.2</td><td>71.2</td><td>63.8</td><td>74.0</td><td>70.5</td><td>50.5</td><td>54.2</td><td>51.9</td><td>51.8</td><td>70.6</td></tr><tr><td>Phi 3.5 - L100 90</td><td>71.6</td><td>59.4</td><td>69.2</td><td>52.9</td><td>51.0</td><td>70.2</td><td>60.5</td><td>69.4</td><td>71.2</td><td>49.6</td><td>54.1</td><td>50.4</td><td>51.8</td><td>66.1</td></tr><tr><td>Llama 3 - L100 10</td><td>65.9</td><td>56.6</td><td>62.6</td><td>52.6</td><td>51.5</td><td>62.1</td><td>59.5</td><td>62.6</td><td>65.8</td><td>50.8</td><td>54.5</td><td>50.4</td><td>48.8</td><td>59.9</td></tr><tr><td>Llama 3 - L100 50</td><td>72.0</td><td>58.4</td><td>67.9</td><td>52.1</td><td>51.6</td><td>69.6</td><td>62.0</td><td>65.9</td><td>70.4</td><td>49.9</td><td>52.0</td><td>48.8</td><td>48.4</td><td>65.6</td></tr><tr><td>Llama 3 - L100 90</td><td>73.1</td><td>59.4</td><td>68.4</td><td>53.3</td><td>51.0</td><td>67.4</td><td>65.8</td><td>71.0</td><td>69.0</td><td>50.6</td><td>52.6</td><td>49.9</td><td>50.1</td><td>66.2</td></tr><tr><td>Phi 3.5 - L100 50</td><td>69.6</td><td>58.0</td><td>67.2</td><td>51.9</td><td>49.9</td><td>68.0</td><td>62.4</td><td>69.0</td><td>67.9</td><td>48.6</td><td>52.5</td><td>49.6</td><td>48.4</td><td>64.1</td></tr><tr><td>Phi 3.5 - PT 100</td><td>79.5</td><td>63.3</td><td>74.8</td><td>55.6</td><td>52.8</td><td>75.8</td><td>68.5</td><td>76.2</td><td>76.5</td><td>50.8</td><td>59.6</td><td>50.9</td><td>51.0</td><td>70.8</td></tr><tr><td>Phi 3.5 - PT 50</td><td>76.1</td><td>62.4</td><td>73.0</td><td>55.3</td><td>52.4</td><td>72.2</td><td>69.6</td><td>73.6</td><td>73.8</td><td>49.2</td><td>59.9</td><td>50.0</td><td>50.6</td><td>72.2</td></tr><tr><td>Phi 3.5 - PT 1</td><td>78.1</td><td>64.5</td><td>74.5</td><td>57.7</td><td>57.0</td><td>74.0</td><td>72.8</td><td>76.8</td><td>75.0</td><td>52.8</td><td>62.2</td><td>51.4</td><td>50.2</td><td>72.4</td></tr><tr><td>Llama 3 - L100 50</td><td>72.0</td><td>58.4</td><td>67.9</td><td>52.1</td><td>51.6</td><td>69.6</td><td>62.0</td><td>65.9</td><td>70.4</td><td>49.9</td><td>52.0</td><td>48.8</td><td>48.4</td><td>65.6</td></tr><tr><td>Llama 3 - PT 1</td><td>76.9</td><td>65.1</td><td>74.4</td><td>58.9</td><td>55.0</td><td>74.8</td><td>73.0</td><td>75.5</td><td>74.4</td><td>53.4</td><td>65.9</td><td>52.5</td><td>53.8</td><td>72.9</td></tr><tr><td>Llama 3 - PT 100</td><td>79.9</td><td>65.2</td><td>77.4</td><td>57.0</td><td>52.6</td><td>77.6</td><td>73.4</td><td>78.1</td><td>78.2</td><td>51.0</td><td>64.0</td><td>49.1</td><td>51.8</td><td>75.8</td></tr><tr><td>Phi 3.5 - OCR English</td><td>78.4</td><td>64.6</td><td>74.7</td><td>57.9</td><td>59.1</td><td>77.1</td><td>70.9</td><td>73.6</td><td>74.5</td><td>50.6</td><td>66.5</td><td>51.1</td><td>49.0</td><td>73.6</td></tr><tr><td>Phi 3.5 - OCR 50</td><td>81.2</td><td>66.7</td><td>76.7</td><td>60.0</td><td>61.4</td><td>78.6</td><td>72.1</td><td>76.0</td><td>77.1</td><td>51.5</td><td>71.5</td><td>52.1</td><td>51.6</td><td>75.0</td></tr><tr><td>Phi 3.5 - OCR 1</td><td>81.0</td><td>69.8</td><td>78.3</td><td>64.1</td><td>66.8</td><td>78.0</td><td>76.8</td><td>78.5</td><td>79.1</td><td>56.9</td><td>73.2</td><td>58.6</td><td>52.4</td><td>77.6</td></tr><tr><td>Phi 3.5 - OCR Latin-down</td><td>78.9</td><td>65.4</td><td>74.2</td><td>59.5</td><td>57.8</td><td>75.5</td><td>67.6</td><td>75.0</td><td>75.0</td><td>56.4</td><td>67.8</td><td>55.0</td><td>52.5</td><td>71.1</td></tr><tr><td>Phi 3.5 - OCR 50 (frozen)</td><td>76.1</td><td>62.1</td><td>70.8</td><td>56.3</td><td>59.2</td><td>73.2</td><td>63.2</td><td>66.2</td><td>76.1</td><td>50.0</td><td>68.0</td><td>47.8</td><td>49.8</td><td>67.8</td></tr><tr><td>Gemma 2 - L100 50</td><td>59.9</td><td>53.5</td><td>57.1</td><td>51.1</td><td>49.6</td><td>59.1</td><td>56.5</td><td>56.8</td><td>58.9</td><td>49.9</td><td>51.0</td><td>50.6</td><td>49.2</td><td>53.6</td></tr><tr><td>Llama 3 - L100 50</td><td>72.0</td><td>58.4</td><td>67.9</td><td>52.1</td><td>51.6</td><td>69.6</td><td>62.0</td><td>65.9</td><td>70.4</td><td>49.9</td><td>52.0</td><td>48.8</td><td>48.4</td><td>65.6</td></tr><tr><td>Qwen 2.5 - L100 50</td><td>82.8</td><td>62.5</td><td>75.1</td><td>54.0</td><td>51.5</td><td>76.4</td><td>66.5</td><td>76.5</td><td>76.5</td><td>50.1</td><td>55.2</td><td>51.0</td><td>49.8</td><td>71.1</td></tr><tr><td>Aya-Expanse - L100 50</td><td>79.1</td><td>63.5</td><td>75.2</td><td>55.7</td><td>53.9</td><td>77.2</td><td>71.4</td><td>75.6</td><td>75.0</td><td>50.6</td><td>56.0</td><td>51.1</td><td>51.0</td><td>73.1</td></tr><tr><td>modelname Aya</td><td>83.1</td><td>74.2</td><td>80.9</td><td>69.7</td><td>75.9</td><td>82.1</td><td>80.1</td><td>81.4</td><td>80.6</td><td>68.8</td><td>73.5</td><td>66.5</td><td>53.4</td><td>79.5</td></tr><tr><td>modelname Qwen</td><td>84.8</td><td>76.1</td><td>82.7</td><td>71.8</td><td>76.9</td><td>83.5</td><td>82.4</td><td>83.8</td><td>83.1</td><td>72.4</td><td>75.6</td><td>64.4</td><td>58.9</td><td>80.2</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents the results of the SMPQA Grounding task, a novel benchmark for evaluating multilingual OCR capabilities in images. The task assesses a model&rsquo;s ability to correctly identify if a given textual label in a prompt corresponds to a specific section (bar or pie chart slice) in an image. The table systematically evaluates various multilingual language models&rsquo; performance across different languages and training strategies. The rows represent different experimental configurations including the use of various language models, different numbers of languages trained on, and data augmentation approaches such as varying English/multilingual data ratios and including synthetic OCR data. The columns represent the accuracy score for each language tested. This provides insight into the impact of multilingual training and data composition on multilingual image text understanding.</p><details><summary>read the caption</summary>Table 30: SMPQA Ground</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Model</th><th>en</th><th>avg.</th><th>avg. Latin</th><th>avg. other</th><th>ar</th><th>de</th><th>hi</th><th>id</th><th>it</th><th>ko</th><th>ru</th><th>th</th><th>zh</th><th>zu</th></tr></thead><tbody><tr><td>Phi 3.5 - English</td><td>36.2</td><td>5.0</td><td>12.4</td><td>0.0</td><td>0.0</td><td>17.4</td><td>0.0</td><td>12.6</td><td>15.2</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>4.4</td></tr><tr><td>Phi 3.5 - T5 50</td><td>36.4</td><td>5.4</td><td>13.6</td><td>0.0</td><td>0.0</td><td>21.2</td><td>0.0</td><td>13.2</td><td>16.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>3.8</td></tr><tr><td>Phi 3.5 - T5-4 50</td><td>35.0</td><td>5.8</td><td>14.4</td><td>0.0</td><td>0.0</td><td>20.0</td><td>0.0</td><td>14.6</td><td>16.6</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>6.4</td></tr><tr><td>Phi 3.5 - T5-3 50</td><td>34.6</td><td>5.8</td><td>14.4</td><td>0.0</td><td>0.0</td><td>16.0</td><td>0.0</td><td>16.6</td><td>20.4</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>4.8</td></tr><tr><td>Phi 3.5 - T5-2 50</td><td>35.8</td><td>5.8</td><td>14.5</td><td>0.0</td><td>0.0</td><td>18.0</td><td>0.0</td><td>14.8</td><td>19.6</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>5.6</td></tr><tr><td>Phi 3.5 - L100 50</td><td>33.4</td><td>5.2</td><td>12.8</td><td>0.1</td><td>0.0</td><td>17.4</td><td>0.0</td><td>14.0</td><td>14.6</td><td>0.0</td><td>0.2</td><td>0.2</td><td>0.0</td><td>5.2</td></tr><tr><td>Llama 3 - English</td><td>41.0</td><td>8.5</td><td>21.1</td><td>0.0</td><td>0.0</td><td>24.4</td><td>0.0</td><td>21.6</td><td>23.8</td><td>0.0</td><td>0.0</td><td>0.2</td><td>0.0</td><td>14.8</td></tr><tr><td>Llama 3 - T5 50</td><td>41.4</td><td>8.2</td><td>20.4</td><td>0.0</td><td>0.0</td><td>25.2</td><td>0.0</td><td>21.8</td><td>23.4</td><td>0.0</td><td>0.0</td><td>0.2</td><td>0.0</td><td>11.2</td></tr><tr><td>Llama 3 - L100 50</td><td>39.2</td><td>7.3</td><td>18.2</td><td>0.0</td><td>0.0</td><td>21.6</td><td>0.0</td><td>18.8</td><td>21.6</td><td>0.0</td><td>0.0</td><td>0.2</td><td>0.0</td><td>10.8</td></tr><tr><td>Phi 3.5 - L100 1</td><td>22.0</td><td>4.0</td><td>10.1</td><td>0.0</td><td>0.0</td><td>12.0</td><td>0.0</td><td>9.0</td><td>14.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>5.2</td></tr><tr><td>Phi 3.5 - L100 10</td><td>24.6</td><td>4.1</td><td>10.3</td><td>0.0</td><td>0.0</td><td>11.6</td><td>0.0</td><td>10.0</td><td>14.2</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>5.4</td></tr><tr><td>Phi 3.5 - L100 24</td><td>26.0</td><td>3.8</td><td>9.5</td><td>0.1</td><td>0.0</td><td>12.2</td><td>0.0</td><td>8.4</td><td>12.6</td><td>0.0</td><td>0.0</td><td>0.4</td><td>0.0</td><td>4.8</td></tr><tr><td>Phi 3.5 - L100 50</td><td>33.4</td><td>5.2</td><td>12.8</td><td>0.1</td><td>0.0</td><td>17.4</td><td>0.0</td><td>14.0</td><td>14.6</td><td>0.0</td><td>0.2</td><td>0.2</td><td>0.0</td><td>5.2</td></tr><tr><td>Phi 3.5 - L100 75</td><td>38.4</td><td>6.0</td><td>15.1</td><td>0.0</td><td>0.0</td><td>21.0</td><td>0.0</td><td>14.8</td><td>18.6</td><td>0.0</td><td>0.2</td><td>0.0</td><td>0.0</td><td>5.8</td></tr><tr><td>Phi 3.5 - L100 90</td><td>39.8</td><td>6.5</td><td>16.1</td><td>0.0</td><td>0.0</td><td>21.0</td><td>0.0</td><td>17.0</td><td>21.8</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>4.8</td></tr><tr><td>Llama 3 - L100 10</td><td>32.0</td><td>6.3</td><td>15.6</td><td>0.1</td><td>0.0</td><td>17.8</td><td>0.0</td><td>15.8</td><td>19.2</td><td>0.0</td><td>0.0</td><td>0.4</td><td>0.0</td><td>9.6</td></tr><tr><td>Llama 3 - L100 50</td><td>39.2</td><td>7.3</td><td>18.2</td><td>0.0</td><td>0.0</td><td>21.6</td><td>0.0</td><td>18.8</td><td>21.6</td><td>0.0</td><td>0.0</td><td>0.2</td><td>0.0</td><td>10.8</td></tr><tr><td>Llama 3 - L100 90</td><td>40.0</td><td>7.5</td><td>18.8</td><td>0.0</td><td>0.0</td><td>21.2</td><td>0.0</td><td>21.0</td><td>20.4</td><td>0.0</td><td>0.0</td><td>0.2</td><td>0.0</td><td>12.6</td></tr><tr><td>Phi 3.5 - L100 50</td><td>33.4</td><td>5.2</td><td>12.8</td><td>0.1</td><td>0.0</td><td>17.4</td><td>0.0</td><td>14.0</td><td>14.6</td><td>0.0</td><td>0.2</td><td>0.2</td><td>0.0</td><td>5.2</td></tr><tr><td>Phi 3.5 - PT 100</td><td>44.0</td><td>9.9</td><td>24.5</td><td>0.2</td><td>0.0</td><td>31.4</td><td>0.0</td><td>25.6</td><td>26.8</td><td>0.0</td><td>1.2</td><td>0.2</td><td>0.0</td><td>14.0</td></tr><tr><td>Phi 3.5 - PT 50</td><td>41.8</td><td>9.4</td><td>23.1</td><td>0.2</td><td>0.0</td><td>27.8</td><td>0.0</td><td>24.4</td><td>25.0</td><td>0.0</td><td>1.2</td><td>0.2</td><td>0.0</td><td>15.0</td></tr><tr><td>Phi 3.5 - PT 1</td><td>42.2</td><td>9.5</td><td>23.7</td><td>0.1</td><td>0.0</td><td>27.2</td><td>0.0</td><td>24.4</td><td>29.0</td><td>0.0</td><td>0.4</td><td>0.0</td><td>0.0</td><td>14.0</td></tr><tr><td>Llama 3 - L100 50</td><td>39.2</td><td>7.3</td><td>18.2</td><td>0.0</td><td>0.0</td><td>21.6</td><td>0.0</td><td>18.8</td><td>21.6</td><td>0.0</td><td>0.0</td><td>0.2</td><td>0.0</td><td>10.8</td></tr><tr><td>Llama 3 - PT 1</td><td>48.4</td><td>11.4</td><td>27.9</td><td>0.4</td><td>0.0</td><td>29.6</td><td>0.2</td><td>30.6</td><td>30.6</td><td>0.0</td><td>1.6</td><td>0.4</td><td>0.0</td><td>20.6</td></tr><tr><td>Llama 3 - PT 100</td><td>48.8</td><td>10.5</td><td>25.0</td><td>0.8</td><td>0.0</td><td>28.8</td><td>2.6</td><td>26.2</td><td>28.4</td><td>0.2</td><td>1.8</td><td>0.4</td><td>0.0</td><td>16.6</td></tr><tr><td>Phi 3.5 - OCR English</td><td>55.8</td><td>18.3</td><td>39.9</td><td>3.9</td><td>5.2</td><td>38.6</td><td>2.4</td><td>43.2</td><td>41.6</td><td>0.0</td><td>15.2</td><td>0.4</td><td>0.0</td><td>36.4</td></tr><tr><td>Phi 3.5 - OCR 50</td><td>53.8</td><td>21.0</td><td>41.8</td><td>7.1</td><td>14.4</td><td>42.2</td><td>6.4</td><td>45.8</td><td>42.6</td><td>0.2</td><td>21.2</td><td>0.6</td><td>0.0</td><td>36.4</td></tr><tr><td>Phi 3.5 - OCR 1</td><td>54.8</td><td>22.2</td><td>43.5</td><td>8.0</td><td>17.2</td><td>43.8</td><td>6.2</td><td>46.4</td><td>42.8</td><td>1.2</td><td>21.4</td><td>1.8</td><td>0.0</td><td>40.8</td></tr><tr><td>Phi 3.5 - OCR Latin-down</td><td>54.6</td><td>22.4</td><td>41.0</td><td>9.9</td><td>20.2</td><td>41.6</td><td>7.0</td><td>42.6</td><td>43.0</td><td>2.8</td><td>25.6</td><td>3.4</td><td>0.6</td><td>36.8</td></tr><tr><td>Phi 3.5 - OCR 50 (frozen)</td><td>47.2</td><td>15.7</td><td>34.1</td><td>3.5</td><td>5.2</td><td>36.4</td><td>3.8</td><td>37.2</td><td>33.0</td><td>0.0</td><td>11.8</td><td>0.2</td><td>0.0</td><td>29.6</td></tr><tr><td>Gemma 2 - L100 50</td><td>28.6</td><td>3.8</td><td>9.4</td><td>0.1</td><td>0.0</td><td>13.8</td><td>0.0</td><td>10.4</td><td>8.4</td><td>0.0</td><td>0.0</td><td>0.4</td><td>0.0</td><td>5.0</td></tr><tr><td>Llama 3 - L100 50</td><td>39.2</td><td>7.3</td><td>18.2</td><td>0.0</td><td>0.0</td><td>21.6</td><td>0.0</td><td>18.8</td><td>21.6</td><td>0.0</td><td>0.0</td><td>0.2</td><td>0.0</td><td>10.8</td></tr><tr><td>Qwen 2.5 - L100 50</td><td>48.8</td><td>10.1</td><td>25.1</td><td>0.1</td><td>0.0</td><td>32.0</td><td>0.0</td><td>23.8</td><td>29.0</td><td>0.0</td><td>0.2</td><td>0.2</td><td>0.0</td><td>15.6</td></tr><tr><td>Aya-Expanse - L100 50</td><td>46.6</td><td>10.2</td><td>25.4</td><td>0.1</td><td>0.0</td><td>27.4</td><td>0.0</td><td>28.8</td><td>27.4</td><td>0.0</td><td>0.0</td><td>0.4</td><td>0.0</td><td>18.0</td></tr><tr><td>modelname Aya</td><td>60.0</td><td>30.1</td><td>49.8</td><td>17.0</td><td>29.2</td><td>50.2</td><td>17.6</td><td>52.6</td><td>51.2</td><td>11.2</td><td>38.2</td><td>4.8</td><td>0.8</td><td>45.2</td></tr><tr><td>modelname Qwen</td><td>65.2</td><td>31.7</td><td>54.3</td><td>16.6</td><td>21.4</td><td>53.2</td><td>21.4</td><td>55.4</td><td>56.6</td><td>16.2</td><td>34.8</td><td>5.2</td><td>0.6</td><td>52.2</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents the results of the SMPQA-Name task, a new benchmark introduced in the paper to evaluate the multilingual text-in-image understanding capabilities of Large Vision-Language Models (LVLMs). The task focuses on the model&rsquo;s ability to accurately read and identify textual content within images. The table shows the performance of various models, including different configurations of the Centurio model (with varying numbers of training languages and data distributions), across multiple languages. The scores reflect the models&rsquo; accuracy in recognizing and identifying text in images.</p><details><summary>read the caption</summary>Table 31: SMPQA Name</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th></th><th>en</th><th>avg.</th><th>af</th><th>am</th><th>cs</th><th>el</th><th>es</th><th>fa</th><th>fi</th><th>ha</th><th>hr</th><th>hu</th><th>ja</th><th>mi</th><th>nl</th><th>no</th><th>pl</th><th>ro</th><th>ta</th><th>te</th><th>zu</th></tr></thead><tbody><tr><td>Centurio Aya</td><td>69.7</td><td>54.7</td><td>63.6</td><td>29.4</td><td>66.2</td><td>67.8</td><td>65.1</td><td>60.0</td><td>43.3</td><td>37.5</td><td>63.6</td><td>49.8</td><td>66.7</td><td>37.0</td><td>62.4</td><td>59.1</td><td>62.6</td><td>64.0</td><td>46.9</td><td>50.9</td><td>42.6</td></tr><tr><td>Centurio Qwen</td><td>72.7</td><td>56.2</td><td>65.3</td><td>47.4</td><td>62.2</td><td>56.7</td><td>67.0</td><td>53.6</td><td>48.8</td><td>36.7</td><td>65.4</td><td>54.1</td><td>67.6</td><td>39.1</td><td>63.7</td><td>63.6</td><td>60.4</td><td>58.5</td><td>45.2</td><td>63.4</td><td>49.5</td></tr><tr><td>Parrot</td><td>30.5</td><td>25.7</td><td>26.0</td><td>22.8</td><td>26.1</td><td>25.5</td><td>27.3</td><td>25.9</td><td>26.4</td><td>23.7</td><td>25.3</td><td>25.6</td><td>26.7</td><td>25.4</td><td>28.0</td><td>26.6</td><td>26.5</td><td>26.8</td><td>25.5</td><td>23.9</td><td>24.0</td></tr><tr><td>PALO 13B</td><td>61.4</td><td>41.1</td><td>48.4</td><td>25.9</td><td>47.9</td><td>35.8</td><td>53.2</td><td>37.5</td><td>42.7</td><td>26.1</td><td>52.3</td><td>47.9</td><td>49.1</td><td>31.0</td><td>48.9</td><td>51.2</td><td>46.1</td><td>46.5</td><td>28.9</td><td>32.2</td><td>28.3</td></tr><tr><td>PALO 7B</td><td>58.7</td><td>38.6</td><td>44.2</td><td>28.4</td><td>43.6</td><td>33.5</td><td>49.9</td><td>36.9</td><td>39.1</td><td>24.5</td><td>49.6</td><td>45.4</td><td>48.8</td><td>27.8</td><td>45.1</td><td>45.8</td><td>42.0</td><td>44.0</td><td>26.7</td><td>30.1</td><td>28.3</td></tr><tr><td>InternVL 2.5 4B</td><td>68.4</td><td>45.4</td><td>53.2</td><td>31.3</td><td>53.2</td><td>42.3</td><td>60.8</td><td>45.4</td><td>38.3</td><td>26.3</td><td>55.2</td><td>42.1</td><td>60.5</td><td>29.5</td><td>56.6</td><td>53.7</td><td>53.1</td><td>49.7</td><td>35.3</td><td>50.1</td><td>26.5</td></tr><tr><td>InternVL 2.5 8B</td><td>70.3</td><td>44.2</td><td>54.4</td><td>29.1</td><td>52.8</td><td>43.3</td><td>57.8</td><td>40.5</td><td>41.3</td><td>25.8</td><td>55.6</td><td>44.9</td><td>57.3</td><td>30.0</td><td>51.8</td><td>54.8</td><td>50.3</td><td>48.9</td><td>33.2</td><td>41.2</td><td>27.3</td></tr><tr><td>Qwen2-VL 2B</td><td>78.2</td><td>47.2</td><td>56.6</td><td>30.3</td><td>56.7</td><td>47.2</td><td>64.0</td><td>48.7</td><td>41.7</td><td>26.1</td><td>57.1</td><td>48.0</td><td>62.2</td><td>30.0</td><td>59.2</td><td>57.8</td><td>54.6</td><td>54.5</td><td>31.9</td><td>43.4</td><td>27.6</td></tr><tr><td>Qwen2-VL 7B</td><td>80.7</td><td>57.5</td><td>68.9</td><td>37.2</td><td>68.5</td><td>62.2</td><td>72.6</td><td>59.8</td><td>55.1</td><td>27.1</td><td>72.2</td><td>61.8</td><td>71.8</td><td>29.5</td><td>69.5</td><td>69.6</td><td>67.5</td><td>65.6</td><td>42.7</td><td>62.3</td><td>29.3</td></tr><tr><td>Maya</td><td>54.0</td><td>43.2</td><td>50.6</td><td>27.1</td><td>53.3</td><td>53.6</td><td>52.7</td><td>48.7</td><td>35.3</td><td>23.7</td><td>50.5</td><td>39.3</td><td>55.2</td><td>28.6</td><td>51.4</td><td>46.4</td><td>50.0</td><td>51.3</td><td>31.9</td><td>36.9</td><td>33.4</td></tr><tr><td>Llama-Vision</td><td>75.6</td><td>50.8</td><td>65.1</td><td>30.6</td><td>61.3</td><td>42.9</td><td>65.1</td><td>49.9</td><td>51.5</td><td>31.1</td><td>60.9</td><td>65.0</td><td>46.3</td><td>32.8</td><td>61.5</td><td>61.8</td><td>55.7</td><td>57.3</td><td>42.0</td><td>51.6</td><td>31.9</td></tr><tr><td>Phi 3.5 Vision</td><td>63.1</td><td>36.8</td><td>40.9</td><td>28.7</td><td>41.0</td><td>34.7</td><td>52.7</td><td>33.5</td><td>34.9</td><td>27.1</td><td>40.5</td><td>36.8</td><td>45.9</td><td>28.2</td><td>43.6</td><td>44.4</td><td>38.5</td><td>39.8</td><td>30.9</td><td>28.1</td><td>28.1</td></tr><tr><td>Pixtral 12B</td><td>71.0</td><td>54.2</td><td>62.3</td><td>34.3</td><td>61.6</td><td>58.3</td><td>66.1</td><td>57.3</td><td>52.0</td><td>27.7</td><td>67.1</td><td>60.4</td><td>64.8</td><td>31.9</td><td>58.6</td><td>62.1</td><td>59.8</td><td>59.0</td><td>56.7</td><td>64.5</td><td>25.0</td></tr><tr><td>Pangea</td><td>70.3</td><td>52.1</td><td>61.4</td><td>34.3</td><td>59.6</td><td>54.2</td><td>64.4</td><td>54.9</td><td>45.4</td><td>27.9</td><td>63.0</td><td>49.8</td><td>65.5</td><td>29.6</td><td>61.0</td><td>64.1</td><td>59.5</td><td>60.6</td><td>42.4</td><td>62.7</td><td>29.3</td></tr><tr><td>MiniCPM 2.6</td><td>72.6</td><td>47.4</td><td>56.0</td><td>29.9</td><td>55.1</td><td>46.6</td><td>62.1</td><td>48.5</td><td>41.8</td><td>22.9</td><td>59.5</td><td>44.9</td><td>62.9</td><td>29.0</td><td>57.8</td><td>55.2</td><td>54.7</td><td>52.7</td><td>34.5</td><td>53.9</td><td>33.4</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents the results of the BIN-MC (Babel-ImageNet Multiple Choice) task. BIN-MC is a visual question answering task where the goal is to identify the correct label for images from the Babel-ImageNet dataset, which contains translations of ImageNet labels into multiple languages. The table shows the performance of different models, varying in the number of languages included in training and the proportion of English data in the training set. The performance metric is the exact accuracy, showing how well the models predict the correct label for each image, categorized by language tiers (from high-resource to low-resource) and aggregated as multilingual average. The results are analyzed based on different training configurations, including various mixes of English and multilingual data and pre-training approaches. This allows researchers to examine the influence of training language distribution and the effect of pre-training on multilingual capabilities.</p><details><summary>read the caption</summary>Table 32: BIN-MC</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th style=text-align:left></th><th style=text-align:left>en</th><th style=text-align:left>avg.</th><th style=text-align:left>af</th><th style=text-align:left>zh</th><th style=text-align:left>it</th><th style=text-align:left>pt</th><th style=text-align:left>th</th><th style=text-align:left>vi</th></tr></thead><tbody><tr><td style=text-align:left>Centurio Aya</td><td style=text-align:left>53.0</td><td style=text-align:left>41.2</td><td style=text-align:left>52.8</td><td style=text-align:left>51.4</td><td style=text-align:left>47.7</td><td style=text-align:left>27.4</td><td style=text-align:left>27.8</td><td style=text-align:left>40.3</td></tr><tr><td style=text-align:left>Centurio Qwen</td><td style=text-align:left>61.2</td><td style=text-align:left>46.9</td><td style=text-align:left>50.9</td><td style=text-align:left>55.6</td><td style=text-align:left>49.0</td><td style=text-align:left>31.9</td><td style=text-align:left>29.6</td><td style=text-align:left>64.1</td></tr><tr><td style=text-align:left>Parrot</td><td style=text-align:left>46.6</td><td style=text-align:left>36.2</td><td style=text-align:left>38.0</td><td style=text-align:left>37.8</td><td style=text-align:left>36.8</td><td style=text-align:left>25.9</td><td style=text-align:left>23.5</td><td style=text-align:left>55.1</td></tr><tr><td style=text-align:left>PALO 13B</td><td style=text-align:left>45.2</td><td style=text-align:left>28.3</td><td style=text-align:left>33.1</td><td style=text-align:left>31.3</td><td style=text-align:left>36.5</td><td style=text-align:left>19.3</td><td style=text-align:left>20.2</td><td style=text-align:left>29.2</td></tr><tr><td style=text-align:left>PALO 7B</td><td style=text-align:left>41.0</td><td style=text-align:left>29.1</td><td style=text-align:left>34.4</td><td style=text-align:left>31.5</td><td style=text-align:left>32.7</td><td style=text-align:left>21.8</td><td style=text-align:left>21.1</td><td style=text-align:left>33.4</td></tr><tr><td style=text-align:left>InternVL 2.5 4B</td><td style=text-align:left>63.2</td><td style=text-align:left>50.3</td><td style=text-align:left>46.0</td><td style=text-align:left>60.9</td><td style=text-align:left>50.3</td><td style=text-align:left>34.9</td><td style=text-align:left>39.1</td><td style=text-align:left>70.4</td></tr><tr><td style=text-align:left>InternVL 2.5 8B</td><td style=text-align:left>67.0</td><td style=text-align:left>53.3</td><td style=text-align:left>57.7</td><td style=text-align:left>61.7</td><td style=text-align:left>53.2</td><td style=text-align:left>33.0</td><td style=text-align:left>39.1</td><td style=text-align:left>75.2</td></tr><tr><td style=text-align:left>Qwen2-VL 2B</td><td style=text-align:left>47.9</td><td style=text-align:left>40.5</td><td style=text-align:left>38.0</td><td style=text-align:left>51.6</td><td style=text-align:left>36.4</td><td style=text-align:left>36.2</td><td style=text-align:left>26.1</td><td style=text-align:left>54.9</td></tr><tr><td style=text-align:left>Qwen2-VL 7B</td><td style=text-align:left>56.1</td><td style=text-align:left>49.7</td><td style=text-align:left>50.9</td><td style=text-align:left>58.6</td><td style=text-align:left>46.8</td><td style=text-align:left>34.7</td><td style=text-align:left>38.3</td><td style=text-align:left>69.0</td></tr><tr><td style=text-align:left>Maya</td><td style=text-align:left>49.2</td><td style=text-align:left>36.3</td><td style=text-align:left>48.5</td><td style=text-align:left>46.4</td><td style=text-align:left>36.6</td><td style=text-align:left>25.9</td><td style=text-align:left>20.0</td><td style=text-align:left>40.3</td></tr><tr><td style=text-align:left>Phi 3.5 Vision</td><td style=text-align:left>56.3</td><td style=text-align:left>40.7</td><td style=text-align:left>51.5</td><td style=text-align:left>54.4</td><td style=text-align:left>44.1</td><td style=text-align:left>25.2</td><td style=text-align:left>24.3</td><td style=text-align:left>44.4</td></tr><tr><td style=text-align:left>Pixtral 12B</td><td style=text-align:left>49.4</td><td style=text-align:left>33.7</td><td style=text-align:left>39.9</td><td style=text-align:left>53.6</td><td style=text-align:left>34.4</td><td style=text-align:left>19.5</td><td style=text-align:left>7.0</td><td style=text-align:left>47.7</td></tr><tr><td style=text-align:left>Pangea</td><td style=text-align:left>58.0</td><td style=text-align:left>45.5</td><td style=text-align:left>50.3</td><td style=text-align:left>58.6</td><td style=text-align:left>49.0</td><td style=text-align:left>32.2</td><td style=text-align:left>27.8</td><td style=text-align:left>55.3</td></tr><tr><td style=text-align:left>MiniCPM 2.6</td><td style=text-align:left>55.0</td><td style=text-align:left>48.2</td><td style=text-align:left>44.2</td><td style=text-align:left>54.6</td><td style=text-align:left>44.3</td><td style=text-align:left>36.9</td><td style=text-align:left>38.3</td><td style=text-align:left>70.8</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents the results of the M3Exam task, a multiple-choice visual question answering task, across various language models. The results are broken down by language tier (T1-T5), showcasing performance on English and multilingual data. The models&rsquo; performances are measured and compared using accuracy scores and it includes the results for different training setups including varying the number of training languages and the ratio of English to multilingual data.</p><details><summary>read the caption</summary>Table 33: M3Exam</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th style=text-align:left></th><th style=text-align:center>en</th><th style=text-align:center>avg.</th><th style=text-align:center>am</th><th style=text-align:center>ber</th><th style=text-align:center>bn</th><th style=text-align:center>de</th><th style=text-align:center>fil</th><th style=text-align:center>ha</th><th style=text-align:center>hi</th><th style=text-align:center>ru</th><th style=text-align:center>sw</th><th style=text-align:center>th</th><th style=text-align:center>zu</th></tr></thead><tbody><tr><td style=text-align:left>Centurio Aya</td><td style=text-align:center>82.5</td><td style=text-align:center>66.8</td><td style=text-align:center>71.7</td><td style=text-align:center>54.2</td><td style=text-align:center>59.3</td><td style=text-align:center>73.3</td><td style=text-align:center>59.2</td><td style=text-align:center>65.0</td><td style=text-align:center>71.2</td><td style=text-align:center>75.8</td><td style=text-align:center>67.5</td><td style=text-align:center>72.5</td><td style=text-align:center>65.5</td></tr><tr><td style=text-align:left>Centurio Qwen</td><td style=text-align:center>87.5</td><td style=text-align:center>73.1</td><td style=text-align:center>77.5</td><td style=text-align:center>49.2</td><td style=text-align:center>62.7</td><td style=text-align:center>80.8</td><td style=text-align:center>78.3</td><td style=text-align:center>76.7</td><td style=text-align:center>72.9</td><td style=text-align:center>85.0</td><td style=text-align:center>70.0</td><td style=text-align:center>81.7</td><td style=text-align:center>69.0</td></tr><tr><td style=text-align:left>Parrot</td><td style=text-align:center>59.2</td><td style=text-align:center>52.9</td><td style=text-align:center>45.0</td><td style=text-align:center>64.2</td><td style=text-align:center>53.4</td><td style=text-align:center>63.3</td><td style=text-align:center>49.2</td><td style=text-align:center>41.7</td><td style=text-align:center>62.7</td><td style=text-align:center>62.5</td><td style=text-align:center>35.8</td><td style=text-align:center>67.5</td><td style=text-align:center>36.2</td></tr><tr><td style=text-align:left>PALO 13B</td><td style=text-align:center>63.3</td><td style=text-align:center>26.2</td><td style=text-align:center>25.0</td><td style=text-align:center>55.0</td><td style=text-align:center>0.8</td><td style=text-align:center>44.2</td><td style=text-align:center>47.5</td><td style=text-align:center>40.0</td><td style=text-align:center>0.0</td><td style=text-align:center>5.8</td><td style=text-align:center>32.5</td><td style=text-align:center>0.0</td><td style=text-align:center>37.1</td></tr><tr><td style=text-align:left>PALO 7B</td><td style=text-align:center>48.3</td><td style=text-align:center>25.6</td><td style=text-align:center>40.8</td><td style=text-align:center>75.0</td><td style=text-align:center>0.0</td><td style=text-align:center>0.0</td><td style=text-align:center>49.2</td><td style=text-align:center>40.0</td><td style=text-align:center>0.0</td><td style=text-align:center>0.0</td><td style=text-align:center>39.2</td><td style=text-align:center>0.0</td><td style=text-align:center>37.9</td></tr><tr><td style=text-align:left>InternVL 2.5 4B</td><td style=text-align:center>72.5</td><td style=text-align:center>49.7</td><td style=text-align:center>43.3</td><td style=text-align:center>50.0</td><td style=text-align:center>40.7</td><td style=text-align:center>62.5</td><td style=text-align:center>56.7</td><td style=text-align:center>41.7</td><td style=text-align:center>42.4</td><td style=text-align:center>63.3</td><td style=text-align:center>35.8</td><td style=text-align:center>74.2</td><td style=text-align:center>36.2</td></tr><tr><td style=text-align:left>InternVL 2.5 8B</td><td style=text-align:center>87.5</td><td style=text-align:center>51.6</td><td style=text-align:center>43.3</td><td style=text-align:center>50.0</td><td style=text-align:center>41.5</td><td style=text-align:center>64.2</td><td style=text-align:center>49.2</td><td style=text-align:center>41.7</td><td style=text-align:center>59.3</td><td style=text-align:center>75.8</td><td style=text-align:center>36.7</td><td style=text-align:center>68.3</td><td style=text-align:center>37.1</td></tr><tr><td style=text-align:left>Qwen2-VL 2B</td><td style=text-align:center>61.7</td><td style=text-align:center>50.5</td><td style=text-align:center>44.2</td><td style=text-align:center>50.0</td><td style=text-align:center>43.2</td><td style=text-align:center>65.0</td><td style=text-align:center>53.3</td><td style=text-align:center>41.7</td><td style=text-align:center>61.0</td><td style=text-align:center>52.5</td><td style=text-align:center>38.3</td><td style=text-align:center>67.5</td><td style=text-align:center>38.8</td></tr><tr><td style=text-align:left>Qwen2-VL 7B</td><td style=text-align:center>60.0</td><td style=text-align:center>52.9</td><td style=text-align:center>48.3</td><td style=text-align:center>50.0</td><td style=text-align:center>46.6</td><td style=text-align:center>60.0</td><td style=text-align:center>50.0</td><td style=text-align:center>46.7</td><td style=text-align:center>48.3</td><td style=text-align:center>63.3</td><td style=text-align:center>58.3</td><td style=text-align:center>60.8</td><td style=text-align:center>49.1</td></tr><tr><td style=text-align:left>Maya</td><td style=text-align:center>46.7</td><td style=text-align:center>42.3</td><td style=text-align:center>43.3</td><td style=text-align:center>48.3</td><td style=text-align:center>33.9</td><td style=text-align:center>50.8</td><td style=text-align:center>51.7</td><td style=text-align:center>40.8</td><td style=text-align:center>42.4</td><td style=text-align:center>45.8</td><td style=text-align:center>34.2</td><td style=text-align:center>38.3</td><td style=text-align:center>35.3</td></tr><tr><td style=text-align:left>Phi 3.5 Vision</td><td style=text-align:center>81.7</td><td style=text-align:center>50.3</td><td style=text-align:center>45.8</td><td style=text-align:center>49.2</td><td style=text-align:center>56.8</td><td style=text-align:center>73.3</td><td style=text-align:center>54.2</td><td style=text-align:center>41.7</td><td style=text-align:center>56.8</td><td style=text-align:center>85.8</td><td style=text-align:center>38.3</td><td style=text-align:center>15.0</td><td style=text-align:center>36.2</td></tr><tr><td style=text-align:left>Pixtral 12B</td><td style=text-align:center>55.8</td><td style=text-align:center>47.7</td><td style=text-align:center>51.7</td><td style=text-align:center>32.5</td><td style=text-align:center>47.5</td><td style=text-align:center>63.3</td><td style=text-align:center>51.7</td><td style=text-align:center>44.2</td><td style=text-align:center>16.1</td><td style=text-align:center>54.2</td><td style=text-align:center>65.8</td><td style=text-align:center>53.3</td><td style=text-align:center>44.0</td></tr><tr><td style=text-align:left>Pangea</td><td style=text-align:center>69.2</td><td style=text-align:center>58.9</td><td style=text-align:center>45.8</td><td style=text-align:center>90.0</td><td style=text-align:center>53.4</td><td style=text-align:center>61.7</td><td style=text-align:center>55.0</td><td style=text-align:center>41.7</td><td style=text-align:center>60.2</td><td style=text-align:center>74.2</td><td style=text-align:center>54.2</td><td style=text-align:center>75.8</td><td style=text-align:center>36.2</td></tr><tr><td style=text-align:left>MiniCPM 2.6</td><td style=text-align:center>52.5</td><td style=text-align:center>49.1</td><td style=text-align:center>45.0</td><td style=text-align:center>55.8</td><td style=text-align:center>49.2</td><td style=text-align:center>45.8</td><td style=text-align:center>48.3</td><td style=text-align:center>40.8</td><td style=text-align:center>44.1</td><td style=text-align:center>59.2</td><td style=text-align:center>48.3</td><td style=text-align:center>65.8</td><td style=text-align:center>37.9</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents a comparison of various Large Vision-Language Models (LVLMs) on the Visually Grounded Reasoning (VGR) task. The models are evaluated across multiple languages, and the results show the accuracy of each model in predicting whether a given textual hypothesis is true or false based on a pair of images. The table allows for a comparative analysis of the models&rsquo; performance on this specific task, highlighting their strengths and weaknesses in understanding and reasoning with visual and linguistic information across different languages.</p><details><summary>read the caption</summary>Table 34: VGR</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Model</th><th>en</th><th>avg.</th><th>am</th><th>ber</th><th>bn</th><th>de</th><th>fil</th><th>ha</th><th>hi</th><th>ru</th><th>sw</th><th>th</th><th>zu</th><th></th></tr></thead><tbody><tr><td>Centurio Aya</td><td>12.5</td><td>20.7</td><td>18.3</td><td>21.7</td><td>20.0</td><td>11.7</td><td>24.2</td><td>29.2</td><td>15.2</td><td>10.8</td><td>28.6</td><td>29.2</td><td>19.5</td><td></td></tr><tr><td>Centurio Qwen</td><td>28.3</td><td>27.0</td><td>18.3</td><td>20.0</td><td>33.3</td><td>32.5</td><td>29.2</td><td>22.5</td><td>25.0</td><td>22.5</td><td>30.4</td><td>30.0</td><td>33.1</td><td></td></tr><tr><td>Parrot</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td></td></tr><tr><td>PALO 13B</td><td>2.5</td><td>4.9</td><td>6.7</td><td>5.0</td><td>6.7</td><td>5.0</td><td>5.8</td><td>2.5</td><td>3.6</td><td>4.2</td><td>5.4</td><td>5.0</td><td>4.2</td><td></td></tr><tr><td>PALO 7B</td><td>5.8</td><td>6.8</td><td>8.3</td><td>9.2</td><td>10.0</td><td>5.8</td><td>6.7</td><td>4.2</td><td>9.8</td><td>5.0</td><td>4.5</td><td>5.8</td><td>5.1</td><td></td></tr><tr><td>InternVL 2.5 4B</td><td>24.2</td><td>21.0</td><td>18.3</td><td>26.7</td><td>17.5</td><td>20.8</td><td>20.0</td><td>23.3</td><td>22.3</td><td>20.0</td><td>23.2</td><td>20.0</td><td>18.6</td><td></td></tr><tr><td>InternVL 2.5 8B</td><td>57.5</td><td>29.0</td><td>25.0</td><td>22.5</td><td>25.8</td><td>38.3</td><td>36.7</td><td>25.8</td><td>41.1</td><td>35.8</td><td>15.2</td><td>30.0</td><td>22.9</td><td></td></tr><tr><td>Qwen2-VL 2B</td><td>22.5</td><td>20.4</td><td>17.5</td><td>20.0</td><td>13.3</td><td>26.7</td><td>25.0</td><td>24.2</td><td>20.5</td><td>16.7</td><td>21.4</td><td>15.8</td><td>23.7</td><td></td></tr><tr><td>Qwen2-VL 7B</td><td>5.8</td><td>13.2</td><td>14.2</td><td>15.8</td><td>13.3</td><td>11.7</td><td>10.0</td><td>15.0</td><td>12.5</td><td>12.5</td><td>13.4</td><td>13.3</td><td>13.6</td><td></td></tr><tr><td>Maya</td><td>20.0</td><td>20.1</td><td>20.0</td><td>25.8</td><td>19.2</td><td>20.8</td><td>15.0</td><td>25.8</td><td>17.9</td><td>23.3</td><td>21.4</td><td>15.8</td><td>16.1</td><td></td></tr><tr><td>Phi 3.5 Vision</td><td>45.8</td><td>31.5</td><td>27.5</td><td>29.2</td><td>23.3</td><td>36.7</td><td>30.0</td><td>31.7</td><td>33.9</td><td>29.2</td><td>37.5</td><td>35.8</td><td>31.4</td><td></td></tr><tr><td>Pixtral 12B</td><td>9.2</td><td>12.4</td><td>17.5</td><td>13.3</td><td>10.0</td><td>16.7</td><td>10.0</td><td>16.7</td><td>3.6</td><td>14.2</td><td>8.9</td><td>12.5</td><td>13.6</td><td></td></tr><tr><td>Pangea</td><td>0.0</td><td>6.7</td><td>0.0</td><td>0.8</td><td>0.0</td><td>20.8</td><td>24.2</td><td>15.8</td><td>6.2</td><td>0.8</td><td>3.6</td><td>0.8</td><td>0.8</td><td></td></tr><tr><td>MiniCPM 2.6</td><td>9.2</td><td>14.6</td><td>11.7</td><td>19.2</td><td>12.5</td><td>10.8</td><td>10.0</td><td>22.5</td><td>10.7</td><td>12.5</td><td>19.6</td><td>11.7</td><td>19.5</td><td></td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents a comparison of various large vision-language models (LVLMs) on the Visio-Linguistic Outlier Detection (VLOD) task. The VLOD task requires identifying the image that doesn&rsquo;t fit a given textual description within a set of images. The table shows the performance of different models across various languages, indicating their accuracy in this task. The models include Centurio (two versions), Parrot, PALO (two sizes), InternVL (two sizes), Qwen2-VL (two sizes), Maya, Phi-3.5 Vision, Pixtral, Pangea, and MiniCPM. The results are presented as percentages, likely representing accuracy rates for each model in each language.</p><details><summary>read the caption</summary>Table 35: VLOD</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Model</th><th>en</th><th>avg.</th><th>id</th><th>sw</th><th>ta</th><th>tr</th><th>zh</th></tr></thead><tbody><tr><td><code>Centurio</code> Aya</td><td>85.0</td><td>77.9</td><td>79.5</td><td>70.9</td><td>73.4</td><td>83.4</td><td>82.4</td></tr><tr><td><code>Centurio</code> Qwen</td><td>89.6</td><td>81.7</td><td>85.0</td><td>76.8</td><td>76.0</td><td>84.2</td><td>86.7</td></tr><tr><td>Parrot</td><td>63.5</td><td>55.1</td><td>56.6</td><td>51.2</td><td>50.7</td><td>58.6</td><td>58.2</td></tr><tr><td>PALO 13B</td><td>63.8</td><td>33.1</td><td>58.7</td><td>50.9</td><td>2.6</td><td>53.1</td><td>0.2</td></tr><tr><td>PALO 7B</td><td>62.7</td><td>24.1</td><td>33.6</td><td>47.8</td><td>0.4</td><td>38.5</td><td>0.0</td></tr><tr><td>InternVL 2.5 4B</td><td>74.9</td><td>59.0</td><td>65.7</td><td>50.7</td><td>50.9</td><td>64.2</td><td>63.5</td></tr><tr><td>InternVL 2.5 8B</td><td>83.0</td><td>63.3</td><td>63.2</td><td>51.4</td><td>54.6</td><td>67.2</td><td>79.9</td></tr><tr><td>Qwen2-VL 2B</td><td>67.9</td><td>55.9</td><td>60.9</td><td>51.8</td><td>52.2</td><td>59.0</td><td>55.8</td></tr><tr><td>Qwen2-VL 7B</td><td>69.8</td><td>60.2</td><td>61.1</td><td>53.1</td><td>60.9</td><td>65.3</td><td>60.7</td></tr><tr><td>Maya</td><td>60.3</td><td>56.3</td><td>60.3</td><td>50.7</td><td>50.6</td><td>58.9</td><td>61.2</td></tr><tr><td>Phi 3.5 Vision</td><td>73.4</td><td>46.4</td><td>56.4</td><td>51.3</td><td>50.8</td><td>58.0</td><td>15.7</td></tr><tr><td>Pixtral 12B</td><td>67.7</td><td>60.7</td><td>62.5</td><td>54.4</td><td>61.8</td><td>65.5</td><td>59.1</td></tr><tr><td>Pangea</td><td>75.8</td><td>70.5</td><td>74.3</td><td>70.9</td><td>66.6</td><td>71.1</td><td>69.6</td></tr><tr><td>MiniCPM 2.6</td><td>70.2</td><td>57.9</td><td>57.8</td><td>54.2</td><td>57.2</td><td>63.3</td><td>57.2</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents a comparison of the performance of Centurio and thirteen other large vision-language models (LVLMs) on the MaRVL (Multicultural Reasoning over Vision and Language) dataset. MaRVL is a benchmark designed to evaluate the ability of models to perform multicultural reasoning tasks, incorporating both visual and linguistic aspects. The table shows the accuracy of each model across various language tiers (T1-T5), providing a detailed breakdown of performance across different linguistic groups. This helps assess how well the models generalize to diverse languages and cultural contexts. The results are presented as percentages and also include a breakdown of performance for English and non-English components. This comprehensive comparison allows for a detailed understanding of each model&rsquo;s strengths and weaknesses in handling multilingual and multicultural data.</p><details><summary>read the caption</summary>Table 36: MaRVL</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th style=text-align:left></th><th style=text-align:left>en</th><th style=text-align:left>avg.</th><th style=text-align:left>fr</th><th style=text-align:left>hi</th><th style=text-align:left>he</th><th style=text-align:left>ro</th><th style=text-align:left>th</th><th style=text-align:left>zh</th></tr></thead><tbody><tr><td style=text-align:left>Centurio Aya</td><td style=text-align:left>55.7</td><td style=text-align:left>49.3</td><td style=text-align:left>45.1</td><td style=text-align:left>58.7</td><td style=text-align:left>62.9</td><td style=text-align:left>51.1</td><td style=text-align:left>46.7</td><td style=text-align:left>31.6</td></tr><tr><td style=text-align:left>Centurio Qwen</td><td style=text-align:left>60.1</td><td style=text-align:left>47.7</td><td style=text-align:left>47.1</td><td style=text-align:left>45.1</td><td style=text-align:left>56.8</td><td style=text-align:left>47.7</td><td style=text-align:left>57.0</td><td style=text-align:left>32.2</td></tr><tr><td style=text-align:left>Parrot</td><td style=text-align:left>28.2</td><td style=text-align:left>3.6</td><td style=text-align:left>2.7</td><td style=text-align:left>2.9</td><td style=text-align:left>1.4</td><td style=text-align:left>1.2</td><td style=text-align:left>3.0</td><td style=text-align:left>10.7</td></tr><tr><td style=text-align:left>PALO 13B</td><td style=text-align:left>51.7</td><td style=text-align:left>33.1</td><td style=text-align:left>42.0</td><td style=text-align:left>17.5</td><td style=text-align:left>53.4</td><td style=text-align:left>34.2</td><td style=text-align:left>20.9</td><td style=text-align:left>30.6</td></tr><tr><td style=text-align:left>PALO 7B</td><td style=text-align:left>54.0</td><td style=text-align:left>22.5</td><td style=text-align:left>39.9</td><td style=text-align:left>9.2</td><td style=text-align:left>30.6</td><td style=text-align:left>16.8</td><td style=text-align:left>12.3</td><td style=text-align:left>26.4</td></tr><tr><td style=text-align:left>InternVL 2.5 4B</td><td style=text-align:left>46.0</td><td style=text-align:left>42.5</td><td style=text-align:left>45.7</td><td style=text-align:left>37.1</td><td style=text-align:left>38.8</td><td style=text-align:left>31.5</td><td style=text-align:left>51.0</td><td style=text-align:left>50.8</td></tr><tr><td style=text-align:left>InternVL 2.5 8B</td><td style=text-align:left>45.6</td><td style=text-align:left>38.2</td><td style=text-align:left>51.2</td><td style=text-align:left>27.9</td><td style=text-align:left>24.5</td><td style=text-align:left>35.7</td><td style=text-align:left>36.4</td><td style=text-align:left>53.4</td></tr><tr><td style=text-align:left>Qwen2-VL 2B</td><td style=text-align:left>53.7</td><td style=text-align:left>26.5</td><td style=text-align:left>40.3</td><td style=text-align:left>10.8</td><td style=text-align:left>9.5</td><td style=text-align:left>15.6</td><td style=text-align:left>38.1</td><td style=text-align:left>44.6</td></tr><tr><td style=text-align:left>Qwen2-VL 7B</td><td style=text-align:left>54.7</td><td style=text-align:left>31.2</td><td style=text-align:left>38.6</td><td style=text-align:left>18.7</td><td style=text-align:left>13.9</td><td style=text-align:left>37.2</td><td style=text-align:left>42.1</td><td style=text-align:left>36.8</td></tr><tr><td style=text-align:left>Maya</td><td style=text-align:left>55.4</td><td style=text-align:left>17.3</td><td style=text-align:left>19.1</td><td style=text-align:left>13.0</td><td style=text-align:left>21.1</td><td style=text-align:left>18.0</td><td style=text-align:left>11.6</td><td style=text-align:left>20.8</td></tr><tr><td style=text-align:left>Llama-Vision</td><td style=text-align:left>0.0</td><td style=text-align:left>4.7</td><td style=text-align:left>0.0</td><td style=text-align:left>0.6</td><td style=text-align:left>2.4</td><td style=text-align:left>0.3</td><td style=text-align:left>24.8</td><td style=text-align:left>0.0</td></tr><tr><td style=text-align:left>Phi 3.5 Vision</td><td style=text-align:left>43.6</td><td style=text-align:left>17.9</td><td style=text-align:left>23.5</td><td style=text-align:left>12.1</td><td style=text-align:left>16.3</td><td style=text-align:left>7.8</td><td style=text-align:left>20.9</td><td style=text-align:left>27.0</td></tr><tr><td style=text-align:left>Pixtral 12B</td><td style=text-align:left>59.4</td><td style=text-align:left>43.4</td><td style=text-align:left>46.8</td><td style=text-align:left>31.7</td><td style=text-align:left>54.4</td><td style=text-align:left>44.1</td><td style=text-align:left>44.4</td><td style=text-align:left>39.1</td></tr><tr><td style=text-align:left>Pangea</td><td style=text-align:left>61.4</td><td style=text-align:left>55.0</td><td style=text-align:left>47.4</td><td style=text-align:left>61.0</td><td style=text-align:left>53.7</td><td style=text-align:left>52.9</td><td style=text-align:left>67.2</td><td style=text-align:left>47.9</td></tr><tr><td style=text-align:left>MiniCPM 2.6</td><td style=text-align:left>53.4</td><td style=text-align:left>22.3</td><td style=text-align:left>14.3</td><td style=text-align:left>12.1</td><td style=text-align:left>5.1</td><td style=text-align:left>19.5</td><td style=text-align:left>53.6</td><td style=text-align:left>29.3</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents a comparison of the performance of various Large Vision-Language Models (LVLMs) on the MaXM dataset. MaXM is a multilingual visual question answering dataset, designed to test the models&rsquo; ability to understand and answer questions about images in various languages. The table shows the accuracy of each model across different language tiers (T1-T5) and overall (avg), providing a detailed breakdown of performance across a range of multilingual capabilities. The models compared include several popular open-source and closed-source LVLMs, as well as the Centurio model, the focus of the paper.</p><details><summary>read the caption</summary>Table 37: MaXM</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th style=text-align:left></th><th style=text-align:center>avg.</th><th style=text-align:center>ar</th><th style=text-align:center>de</th><th style=text-align:center>fr</th><th style=text-align:center>it</th><th style=text-align:center>ja</th><th style=text-align:center>ko</th><th style=text-align:center>ru</th><th style=text-align:center>th</th><th style=text-align:center>vi</th></tr></thead><tbody><tr><td style=text-align:left>Centurio Aya</td><td style=text-align:center>11.1</td><td style=text-align:center>6.7</td><td style=text-align:center>19.9</td><td style=text-align:center>22.5</td><td style=text-align:center>16.7</td><td style=text-align:center>5.0</td><td style=text-align:center>9.0</td><td style=text-align:center>5.2</td><td style=text-align:center>5.2</td><td style=text-align:center>9.7</td></tr><tr><td style=text-align:left>Centurio Qwen</td><td style=text-align:center>11.9</td><td style=text-align:center>4.6</td><td style=text-align:center>22.7</td><td style=text-align:center>26.5</td><td style=text-align:center>18.6</td><td style=text-align:center>5.9</td><td style=text-align:center>9.9</td><td style=text-align:center>5.0</td><td style=text-align:center>5.2</td><td style=text-align:center>8.9</td></tr><tr><td style=text-align:left>Parrot</td><td style=text-align:center>2.0</td><td style=text-align:center>1.4</td><td style=text-align:center>1.9</td><td style=text-align:center>0.9</td><td style=text-align:center>1.6</td><td style=text-align:center>1.6</td><td style=text-align:center>2.7</td><td style=text-align:center>2.0</td><td style=text-align:center>5.2</td><td style=text-align:center>0.9</td></tr><tr><td style=text-align:left>PALO 13B</td><td style=text-align:center>6.3</td><td style=text-align:center>2.6</td><td style=text-align:center>15.6</td><td style=text-align:center>12.1</td><td style=text-align:center>10.4</td><td style=text-align:center>4.0</td><td style=text-align:center>4.3</td><td style=text-align:center>4.0</td><td style=text-align:center>0.0</td><td style=text-align:center>4.2</td></tr><tr><td style=text-align:left>PALO 7B</td><td style=text-align:center>5.8</td><td style=text-align:center>1.8</td><td style=text-align:center>14.3</td><td style=text-align:center>13.3</td><td style=text-align:center>8.3</td><td style=text-align:center>3.4</td><td style=text-align:center>3.2</td><td style=text-align:center>3.6</td><td style=text-align:center>0.4</td><td style=text-align:center>4.1</td></tr><tr><td style=text-align:left>InternVL 2.5 4B</td><td style=text-align:center>25.1</td><td style=text-align:center>11.2</td><td style=text-align:center>34.4</td><td style=text-align:center>38.4</td><td style=text-align:center>33.5</td><td style=text-align:center>18.4</td><td style=text-align:center>29.0</td><td style=text-align:center>9.8</td><td style=text-align:center>16.5</td><td style=text-align:center>34.6</td></tr><tr><td style=text-align:left>InternVL 2.5 8B</td><td style=text-align:center>25.0</td><td style=text-align:center>11.5</td><td style=text-align:center>33.8</td><td style=text-align:center>37.4</td><td style=text-align:center>35.3</td><td style=text-align:center>19.7</td><td style=text-align:center>30.3</td><td style=text-align:center>10.4</td><td style=text-align:center>16.5</td><td style=text-align:center>30.4</td></tr><tr><td style=text-align:left>Qwen2-VL 2B</td><td style=text-align:center>19.0</td><td style=text-align:center>6.1</td><td style=text-align:center>26.8</td><td style=text-align:center>30.9</td><td style=text-align:center>30.7</td><td style=text-align:center>13.5</td><td style=text-align:center>21.1</td><td style=text-align:center>9.3</td><td style=text-align:center>10.0</td><td style=text-align:center>22.4</td></tr><tr><td style=text-align:left>Qwen2-VL 7B</td><td style=text-align:center>23.2</td><td style=text-align:center>16.9</td><td style=text-align:center>27.3</td><td style=text-align:center>31.7</td><td style=text-align:center>35.2</td><td style=text-align:center>16.1</td><td style=text-align:center>24.6</td><td style=text-align:center>10.8</td><td style=text-align:center>15.6</td><td style=text-align:center>30.7</td></tr><tr><td style=text-align:left>Maya</td><td style=text-align:center>5.3</td><td style=text-align:center>2.8</td><td style=text-align:center>13.1</td><td style=text-align:center>12.2</td><td style=text-align:center>6.6</td><td style=text-align:center>2.8</td><td style=text-align:center>4.8</td><td style=text-align:center>2.9</td><td style=text-align:center>0.4</td><td style=text-align:center>2.3</td></tr><tr><td style=text-align:left>Llama-Vision</td><td style=text-align:center>15.2</td><td style=text-align:center>7.4</td><td style=text-align:center>24.0</td><td style=text-align:center>18.7</td><td style=text-align:center>25.3</td><td style=text-align:center>9.4</td><td style=text-align:center>14.5</td><td style=text-align:center>6.1</td><td style=text-align:center>15.2</td><td style=text-align:center>15.8</td></tr><tr><td style=text-align:left>Phi 3.5 Vision</td><td style=text-align:center>11.1</td><td style=text-align:center>3.3</td><td style=text-align:center>18.2</td><td style=text-align:center>20.2</td><td style=text-align:center>25.2</td><td style=text-align:center>5.6</td><td style=text-align:center>8.8</td><td style=text-align:center>5.4</td><td style=text-align:center>3.0</td><td style=text-align:center>10.5</td></tr><tr><td style=text-align:left>Pixtral 12B</td><td style=text-align:center>14.1</td><td style=text-align:center>4.3</td><td style=text-align:center>25.7</td><td style=text-align:center>27.3</td><td style=text-align:center>25.2</td><td style=text-align:center>5.9</td><td style=text-align:center>9.1</td><td style=text-align:center>7.5</td><td style=text-align:center>5.2</td><td style=text-align:center>16.6</td></tr><tr><td style=text-align:left>Pangea</td><td style=text-align:center>19.3</td><td style=text-align:center>8.3</td><td style=text-align:center>29.5</td><td style=text-align:center>35.2</td><td style=text-align:center>29.2</td><td style=text-align:center>9.3</td><td style=text-align:center>14.5</td><td style=text-align:center>7.4</td><td style=text-align:center>10.8</td><td style=text-align:center>29.2</td></tr><tr><td style=text-align:left>MiniCPM 2.6</td><td style=text-align:center>16.1</td><td style=text-align:center>2.3</td><td style=text-align:center>23.9</td><td style=text-align:center>27.5</td><td style=text-align:center>32.7</td><td style=text-align:center>11.7</td><td style=text-align:center>12.7</td><td style=text-align:center>7.3</td><td style=text-align:center>10.0</td><td style=text-align:center>16.5</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents the results of the MTVQA (Multilingual Text-heavy Visual Question Answering) task, comparing various multilingual vision-language models. It shows the accuracy of each model across different languages, evaluating their ability to understand and answer questions about images containing text primarily in the language of the question. The results are broken down for different training strategies, including different multilingual data ratios and instruction tuning approaches. This allows analysis of the impact of training strategies on multilingual performance of the model.</p><details><summary>read the caption</summary>Table 38: MTVQA</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th></th><th>en</th><th>avg.</th><th>bn</th><th>de</th><th>id</th><th>ko</th><th>pt</th><th>ru</th><th>zh</th></tr></thead><tbody><tr><td>Parrot</td><td>37.7</td><td>21.2</td><td>20.2</td><td>23.2</td><td>19.8</td><td>22.8</td><td>21.7</td><td>19.7</td><td>21.2</td></tr><tr><td>PALO 13B</td><td>58.0</td><td>27.8</td><td>26.3</td><td>14.7</td><td>29.6</td><td>30.9</td><td>17.8</td><td>30.9</td><td>44.1</td></tr><tr><td>PALO 7B</td><td>59.1</td><td>36.6</td><td>42.8</td><td>34.5</td><td>30.0</td><td>40.8</td><td>27.7</td><td>32.2</td><td>47.9</td></tr><tr><td>InternVL 2.5 4B</td><td>63.6</td><td>28.0</td><td>28.1</td><td>29.2</td><td>15.4</td><td>38.3</td><td>27.2</td><td>31.5</td><td>25.9</td></tr><tr><td>InternVL 2.5 8B</td><td>63.4</td><td>32.0</td><td>17.4</td><td>23.8</td><td>25.0</td><td>38.2</td><td>27.6</td><td>36.4</td><td>55.2</td></tr><tr><td>Qwen2-VL 2B</td><td>60.5</td><td>38.2</td><td>18.6</td><td>43.2</td><td>32.6</td><td>39.0</td><td>39.9</td><td>44.1</td><td>50.3</td></tr><tr><td>Qwen2-VL 7B</td><td>62.5</td><td>49.3</td><td>37.4</td><td>51.1</td><td>48.4</td><td>50.3</td><td>51.8</td><td>52.1</td><td>54.1</td></tr><tr><td>Maya</td><td>58.2</td><td>49.1</td><td>40.1</td><td>53.2</td><td>49.7</td><td>47.2</td><td>52.5</td><td>50.6</td><td>50.1</td></tr><tr><td>Llama-Vision</td><td>39.3</td><td>27.6</td><td>26.0</td><td>29.2</td><td>26.8</td><td>24.9</td><td>27.9</td><td>30.7</td><td>27.9</td></tr><tr><td>Phi 3.5 Vision</td><td>65.2</td><td>38.0</td><td>5.0</td><td>51.9</td><td>37.3</td><td>35.6</td><td>50.6</td><td>45.9</td><td>39.5</td></tr><tr><td>Pixtral 12B</td><td>59.9</td><td>3.8</td><td>0.7</td><td>5.4</td><td>14.0</td><td>0.3</td><td>3.6</td><td>0.4</td><td>1.9</td></tr><tr><td>Pangea</td><td>64.6</td><td>60.4</td><td>59.1</td><td>61.6</td><td>60.7</td><td>58.8</td><td>62.1</td><td>60.7</td><td>59.6</td></tr><tr><td>MiniCPM 2.6</td><td>57.9</td><td>45.7</td><td>33.9</td><td>49.0</td><td>46.3</td><td>42.1</td><td>51.0</td><td>48.7</td><td>48.6</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents the results of the xGQA (cross-lingual visual question answering) task. It shows the performance of various large vision-language models (LVLMs) across different language groups and settings. The models are evaluated on their ability to correctly answer questions about images, assessing their multilingual capabilities. Different training strategies are tested, varying the number of languages included in the training data (English only, 50% English and 50% multilingual, and 100 languages) and training method (instruction-tuning and pre-training). The results are given as accuracy scores (%), allowing for a comparison of how different approaches and configurations affect the models&rsquo; performance across languages.</p><details><summary>read the caption</summary>Table 39: xGQA</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption>Model Name|en|avg.|ar|bn|cs|da|de|el|es|fa|fi|fil|fr|he|hi|hr|hu|id|it|ja|ko|mi|nl|no|pl|pt|quz|ro|ru|sv|sw|te|th|tr|uk|vi|zh|
&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;
Centurio Aya|78.4|<strong>39.2</strong>|<strong>40.4</strong>|18.5|<strong>33.9</strong>|<strong>40.0</strong>|<strong>38.6</strong>|<strong>35.3</strong>|<strong>69.7</strong>|<strong>55.8</strong>|<strong>11.0</strong>|<strong>34.0</strong>|<strong>71.3</strong>|<strong>47.1</strong>|<strong>26.3</strong>|<strong>24.9</strong>|<strong>19.6</strong>|<strong>58.3</strong>|<strong>60.4</strong>|<strong>49.1</strong>|<strong>21.3</strong>|<strong>33.7</strong>|<strong>61.7</strong>|<strong>42.5</strong>|<strong>37.9</strong>|59.3|<strong>1.7</strong>|<strong>34.6</strong>|38.0|<strong>45.9</strong>|29.9|<strong>15.1</strong>|26.0|<strong>30.6</strong>|<strong>30.6</strong>|<strong>72.7</strong>|<strong>56.9</strong>
Centurio Qwen|<strong>79.1</strong>|34.4|<strong>36.6</strong>|17.1|<strong>29.7</strong>|<strong>43.1</strong>|32.0|<strong>19.2</strong>|<strong>69.2</strong>|<strong>31.2</strong>|<strong>12.0</strong>|<strong>33.6</strong>|67.6|27.6|20.3|<strong>22.0</strong>|<strong>18.7</strong>|50.4|<strong>53.7</strong>|43.5|13.4|<strong>34.9</strong>|<strong>56.2</strong>|41.4|30.0|<strong>59.9</strong>|<strong>2.1</strong>|23.4|<strong>39.2</strong>|42.7|<strong>30.2</strong>|13.5|<strong>42.3</strong>|23.3|20.3|69.4|33.8
Parrot|5.6|0.4|0.6|0.0|0.2|0.0|0.0|0.0|3.3|2.3|0.0|0.0|0.3|0.0|0.0|0.0|0.0|0.0|0.2|0.0|0.0|0.0|1.6|0.0|0.0|4.0|0.2|0.4|0.4|0.0|0.0|0.8|0.0|1.0|0.0|0.0|0.0
PALO 13B|67.3|17.0|23.5|<strong>22.9</strong>|7.9|30.3|32.4|0.2|57.0|1.5|6.6|8.4|66.2|0.6|<strong>25.0</strong>|9.9|2.7|22.7|40.4|19.7|0.2|0.3|36.5|31.0|9.1|13.8|0.8|14.5|21.3|33.9|0.8|0.0|0.5|0.6|2.6|15.6|37.0
PALO 7B|65.9|13.5|17.3|18.8|5.8|18.5|23.3|0.1|48.3|1.5|4.0|2.7|59.1|0.2|21.2|2.8|6.3|20.2|31.0|29.8|2.4|0.3|29.8|16.5|8.4|8.9|0.5|2.6|19.7|23.3|0.0|0.0|0.0|0.5|0.1|17.4|29.7
InternVL 2.5 4B|38.9|17.5|12.1|3.7|9.4|13.6|28.0|2.0|39.7|10.1|2.6|6.2|49.2|8.5|5.4|5.6|3.8|39.9|33.1|33.1|8.9|0.8|29.3|14.2|12.9|39.0|0.2|9.9|23.4|17.1|0.6|1.1|27.9|7.8|7.1|61.3|44.1
InternVL 2.5 8B|38.3|15.7|7.9|4.0|10.7|19.2|27.8|2.9|35.0|8.7|5.0|10.9|47.0|8.2|8.3|7.9|5.3|24.7|27.5|22.0|6.7|0.9|26.8|16.6|12.0|35.0|0.8|12.0|22.6|20.5|1.0|2.6|7.2|9.3|4.7|46.2|40.1
Qwen2-VL 2B|68.8|5.2|0.8|0.0|1.7|7.2|7.0|0.2|5.1|9.0|1.2|2.9|9.4|0.4|0.0|1.4|2.1|8.9|5.9|8.4|1.0|0.3|2.9|5.2|1.5|21.0|1.0|3.7|1.1|13.0|1.1|0.0|1.3|0.9|0.6|7.9|49.5
Qwen2-VL 7B|50.3|24.6|17.9|11.5|23.8|32.3|36.1|13.5|38.9|23.6|8.0|8.3|50.6|13.7|6.7|11.6|15.5|45.4|38.7|32.0|9.1|0.9|39.1|35.7|<strong>30.1</strong>|48.8|0.9|19.0|37.9|<strong>43.1</strong>|2.4|3.9|31.2|15.8|16.6|55.6|41.8
Maya|55.9|14.6|20.6|18.4|11.4|10.6|23.6|10.7|38.2|1.5|0.5|2.1|47.3|18.9|15.0|2.0|0.9|19.4|34.4|26.3|8.9|0.3|28.8|9.4|15.8|16.4|0.6|22.0|19.9|11.4|0.5|0.0|0.2|13.5|1.5|31.8|26.9
Llama-Vision|35.9|7.2|0.0|0.0|0.9|15.5|22.4|0.5|14.7|0.0|4.0|13.1|32.1|0.0|0.0|2.9|13.2|2.2|33.5|0.2|0.1|0.8|30.1|2.8|2.4|15.7|0.2|23.4|0.3|11.2|6.8|0.0|1.2|0.6|0.1|0.8|0.0
Phi 3.5 Vision|32.3|6.3|2.8|0.0|0.6|10.5|21.3|0.1|21.9|0.1|0.9|2.5|32.5|1.0|0.1|1.5|2.6|4.2|23.6|8.0|0.3|0.2|19.8|10.7|1.7|25.8|0.4|3.0|0.5|10.2|0.5|0.0|1.0|1.7|0.1|2.6|8.1
Pixtral 12B|26.5|22.1|18.6|9.6|16.8|24.4|33.2|8.9|36.5|20.5|10.4|15.3|47.8|18.0|6.3|18.7|15.6|44.6|32.8|21.8|12.0|5.9|29.7|26.0|19.6|42.4|1.0|20.2|33.8|30.0|10.4|6.2|23.8|14.9|18.4|51.7|28.1
Pangea|70.1|<strong>34.6</strong>|33.3|<strong>30.8</strong>|19.4|25.2|<strong>39.4</strong>|13.0|61.4|25.4|4.2|6.7|<strong>69.7</strong>|<strong>42.7</strong>|21.5|9.5|3.6|<strong>70.9</strong>|53.5|<strong>63.3</strong>|<strong>20.3</strong>|0.3|44.9|<strong>48.5</strong>|24.1|<strong>64.6</strong>|1.7|<strong>38.7</strong>|<strong>47.3</strong>|20.1|<strong>40.7</strong>|<strong>21.8</strong>|<strong>61.4</strong>|<strong>30.2</strong>|<strong>20.7</strong>|<strong>81.3</strong>|<strong>50.7</strong>
MiniCPM 2.6|<strong>87.5</strong>|14.2|6.7|3.3|8.5|8.7|27.5|1.7|44.0|5.8|3.2|5.0|52.1|1.5|3.0|6.1|5.8|24.6|24.6|18.6|4.4|2.2|27.8|12.0|12.0|36.0|0.2|10.0|20.0|17.0|1.5|0.5|20.9|8.0|7.5|25.8|39.4</table></figure><blockquote><p>üîº This table presents a comprehensive evaluation of various multilingual vision-language models on the XM3600 dataset, a large-scale multilingual image captioning benchmark. The results are detailed for multiple language groups and model configurations. Each row represents a different model, with English performance and average multilingual performance reported separately (en & mul). The evaluation considers various languages (represented by ISO codes), showcasing the performance differences between language groups. In addition to overall performance, language fidelity (i.e., how well the model generates text in the target language) is assessed for some models, providing insights into the models&rsquo; multilingual capabilities and limitations.</p><details><summary>read the caption</summary>Table 40: XM3600</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th>Model Name</th><th>en</th><th>avg.</th><th>ar</th><th>bn</th><th>cs</th><th>da</th><th>de</th><th>el</th><th>es</th><th>fa</th><th>fi</th><th>fil</th><th>fr</th><th>he</th><th>hi</th><th>hr</th><th>hu</th><th>id</th><th>it</th><th>ja</th><th>ko</th><th>mi</th><th>nl</th><th>no</th><th>pl</th><th>pt</th><th>quz</th><th>ro</th><th>ru</th><th>sv</th><th>sw</th><th>te</th><th>th</th><th>tr</th><th>uk</th><th>vi</th><th>zh</th></tr></thead><tbody><tr><td>Centurio Aya</td><td>100.0</td><td>95.7</td><td>93.6</td><td>100.0</td><td>97.7</td><td>96.7</td><td>100.0</td><td>100.0</td><td>99.8</td><td>100.0</td><td>99.8</td><td>100.0</td><td>100.0</td><td>99.8</td><td>99.6</td><td>84.6</td><td>99.8</td><td>99.2</td><td>99.6</td><td>98.8</td><td>100.0</td><td>98.8</td><td>100.0</td><td>97.3</td><td>99.8</td><td>100.0</td><td>1.8</td><td>100.0</td><td>99.6</td><td>98.8</td><td>90.6</td><td>100.0</td><td>99.6</td><td>100.0</td><td>99.8</td><td>100.0</td><td>89.6</td></tr><tr><td>Centurio Qwen</td><td>99.8</td><td>95.2</td><td>95.1</td><td>100.0</td><td>98.6</td><td>93.9</td><td>100.0</td><td>100.0</td><td>99.4</td><td>100.0</td><td>100.0</td><td>100.0</td><td>100.0</td><td>98.8</td><td>99.0</td><td>80.9</td><td>100.0</td><td>96.7</td><td>99.8</td><td>98.8</td><td>100.0</td><td>100.0</td><td>100.0</td><td>95.7</td><td>99.6</td><td>99.4</td><td>3.7</td><td>100.0</td><td>99.4</td><td>98.2</td><td>86.5</td><td>100.0</td><td>99.8</td><td>99.6</td><td>99.2</td><td>100.0</td><td>86.5</td></tr><tr><td>Parrot</td><td>100.0</td><td>25.0</td><td>100.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>100.0</td><td>100.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>100.0</td><td>0.0</td><td>0.0</td><td>100.0</td><td>0.0</td><td>0.0</td><td>100.0</td><td>0.0</td><td>0.0</td><td>100.0</td><td>0.0</td><td>100.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>PALO 13B</td><td>100.0</td><td>60.1</td><td>98.6</td><td>93.9</td><td>47.1</td><td>87.5</td><td>100.0</td><td>60.7</td><td>99.6</td><td>0.0</td><td>74.0</td><td>71.5</td><td>99.8</td><td>35.4</td><td>98.2</td><td>70.1</td><td>9.0</td><td>66.4</td><td>99.2</td><td>61.5</td><td>9.8</td><td>0.0</td><td>99.8</td><td>92.2</td><td>41.2</td><td>27.5</td><td>0.0</td><td>95.5</td><td>68.2</td><td>94.9</td><td>1.6</td><td>26.4</td><td>68.0</td><td>9.4</td><td>11.3</td><td>54.7</td><td>88.9</td></tr><tr><td>PALO 7B</td><td>100.0</td><td>72.0</td><td>99.6</td><td>98.8</td><td>47.5</td><td>93.4</td><td>100.0</td><td>58.2</td><td>99.8</td><td>0.0</td><td>91.8</td><td>52.7</td><td>100.0</td><td>30.7</td><td>98.8</td><td>27.0</td><td>90.8</td><td>96.9</td><td>99.4</td><td>99.2</td><td>91.6</td><td>0.0</td><td>99.4</td><td>95.1</td><td>95.5</td><td>27.0</td><td>0.0</td><td>69.1</td><td>100.0</td><td>96.9</td><td>0.0</td><td>56.8</td><td>91.6</td><td>84.4</td><td>0.0</td><td>99.6</td><td>99.8</td></tr><tr><td>InternVL 2.5 4B</td><td>100.0</td><td>91.0</td><td>96.7</td><td>93.9</td><td>97.1</td><td>82.8</td><td>100.0</td><td>99.0</td><td>99.8</td><td>98.8</td><td>96.1</td><td>95.3</td><td>100.0</td><td>96.7</td><td>91.4</td><td>96.1</td><td>96.9</td><td>99.6</td><td>100.0</td><td>99.2</td><td>48.2</td><td>99.6</td><td>83.0</td><td>99.6</td><td>100.0</td><td>7.0</td><td>98.8</td><td>99.2</td><td>97.7</td><td>34.6</td><td>90.8</td><td>98.2</td><td>97.7</td><td>95.7</td><td>96.1</td><td>99.8</td><td></td></tr><tr><td>InternVL 2.5 8B</td><td>100.0</td><td>91.1</td><td>99.4</td><td>95.3</td><td>97.7</td><td>82.8</td><td>100.0</td><td>100.0</td><td>99.4</td><td>97.9</td><td>98.2</td><td>96.3</td><td>100.0</td><td>98.4</td><td>95.1</td><td>83.2</td><td>98.2</td><td>96.7</td><td>100.0</td><td>99.8</td><td>99.2</td><td>66.8</td><td>99.2</td><td>86.5</td><td>99.6</td><td>99.8</td><td>1.2</td><td>99.8</td><td>99.8</td><td>98.2</td><td>54.7</td><td>99.4</td><td>99.2</td><td>98.4</td><td>40.6</td><td>100.0</td><td>99.8</td></tr><tr><td>Qwen2-VL 2B</td><td>100.0</td><td>13.2</td><td>8.2</td><td>0.0</td><td>0.0</td><td>9.6</td><td>12.9</td><td>0.2</td><td>5.9</td><td>58.4</td><td>0.2</td><td>10.9</td><td>10.0</td><td>4.5</td><td>0.0</td><td>3.1</td><td>0.2</td><td>12.7</td><td>3.9</td><td>19.3</td><td>18.0</td><td>0.2</td><td>0.0</td><td>5.3</td><td>0.0</td><td>34.0</td><td>0.0</td><td>15.4</td><td>0.0</td><td>23.6</td><td>4.5</td><td>0.0</td><td>1.0</td><td>1.0</td><td>0.8</td><td>12.7</td><td>98.8</td></tr><tr><td>Qwen2-VL 7B</td><td>100.0</td><td>90.0</td><td>96.5</td><td>98.2</td><td>93.9</td><td>86.1</td><td>99.8</td><td>99.4</td><td>99.2</td><td>99.2</td><td>95.7</td><td>96.3</td><td>98.2</td><td>98.2</td><td>60.2</td><td>79.1</td><td>75.4</td><td>86.9</td><td>99.0</td><td>98.8</td><td>99.0</td><td>64.5</td><td>99.2</td><td>94.1</td><td>95.7</td><td>95.3</td><td>0.2</td><td>98.2</td><td>99.4</td><td>97.9</td><td>72.1</td><td>95.1</td><td>98.8</td><td>89.8</td><td>83.2</td><td>98.2</td><td>99.0</td></tr><tr><td>Maya</td><td>100.0</td><td>65.7</td><td>99.0</td><td>96.1</td><td>67.6</td><td>85.5</td><td>98.6</td><td>92.0</td><td>99.8</td><td>0.2</td><td>12.1</td><td>1.0</td><td>100.0</td><td>77.0</td><td>98.4</td><td>20.7</td><td>60.7</td><td>40.6</td><td>99.6</td><td>99.8</td><td>91.4</td><td>0.0</td><td>99.8</td><td>80.1</td><td>92.0</td><td>43.9</td><td>0.0</td><td>95.7</td><td>100.0</td><td>96.7</td><td>1.6</td><td>0.0</td><td>47.9</td><td>96.3</td><td>7.2</td><td>62.5</td><td>99.8</td></tr><tr><td>Llama-Vision</td><td>100.0</td><td>33.3</td><td>0.0</td><td>0.0</td><td>4.9</td><td>68.8</td><td>95.5</td><td>7.0</td><td>52.7</td><td>0.0</td><td>35.0</td><td>80.7</td><td>88.3</td><td>0.0</td><td>0.0</td><td>17.0</td><td>91.0</td><td>1.6</td><td>94.7</td><td>0.0</td><td>0.0</td><td>93.0</td><td>99.6</td><td>9.0</td><td>9.2</td><td>48.2</td><td>0.8</td><td>92.6</td><td>0.2</td><td>33.8</td><td>73.6</td><td>0.0</td><td>2.3</td><td>0.2</td><td>0.0</td><td>0.2</td><td>0.0</td></tr><tr><td>Phi 3.5 Vision</td><td>100.0</td><td>40.8</td><td>58.4</td><td>0.6</td><td>1.4</td><td>85.4</td><td>99.2</td><td>16.2</td><td>99.4</td><td>0.0</td><td>15.2</td><td>30.1</td><td>99.8</td><td>14.8</td><td>4.7</td><td>25.2</td><td>56.4</td><td>31.6</td><td>99.0</td><td>58.6</td><td>9.0</td><td>1.0</td><td>93.6</td><td>89.8</td><td>27.3</td><td>63.9</td><td>0.0</td><td>56.8</td><td>0.0</td><td>85.0</td><td>2.9</td><td>13.7</td><td>3.5</td><td>38.1</td><td>0.0</td><td>51.8</td><td>37.9</td></tr><tr><td>Pixtral 12B</td><td>100.0</td><td>96.8</td><td>99.8</td><td>99.6</td><td>98.8</td><td>95.9</td><td>100.0</td><td>99.4</td><td>100.0</td><td>100.0</td><td>99.8</td><td>99.8</td><td>100.0</td><td>99.8</td><td>100.0</td><td>100.0</td><td>93.4</td><td>100.0</td><td>99.6</td><td>100.0</td><td>100.0</td><td>99.6</td><td>100.0</td><td>95.5</td><td>100.0</td><td>100.0</td><td>9.4</td><td>99.6</td><td>100.0</td><td>100.0</td><td>95.9</td><td>99.4</td><td>100.0</td><td>99.8</td><td>100.0</td><td>100.0</td><td>99.8</td></tr><tr><td>Pangea</td><td>99.8</td><td>87.9</td><td>98.8</td><td>99.0</td><td>97.9</td><td>19.1</td><td>99.6</td><td>99.8</td><td>99.2</td><td>98.4</td><td>91.6</td><td>68.9</td><td>100.0</td><td>100.0</td><td>98.2</td><td>67.8</td><td>93.6</td><td>97.9</td><td>100.0</td><td>99.4</td><td>100.0</td><td>0.8</td><td>99.6</td><td>95.7</td><td>99.6</td><td>100.0</td><td>0.0</td><td>100.0</td><td>99.6</td><td>67.0</td><td>82.4</td><td>99.8</td><td>100.0</td><td>99.8</td><td>91.0</td><td>99.8</td><td>99.0</td></tr><tr><td>MiniCPM 2.6</td><td>99.8</td><td>92.3</td><td>94.7</td><td>96.5</td><td>95.5</td><td>96.3</td><td>100.0</td><td>99.8</td><td>99.8</td><td>99.0</td><td>98.4</td><td>97.9</td><td>100.0</td><td>62.9</td><td>92.6</td><td>77.3</td><td>94.5</td><td>93.6</td><td>100.0</td><td>98.4</td><td>99.2</td><td>99.2</td><td>99.6</td><td>95.5</td><td>96.5</td><td>99.8</td><td>10.0</td><td>98.2</td><td>97.3</td><td>99.4</td><td>66.4</td><td>85.5</td><td>96.3</td><td>95.1</td><td>99.2</td><td>90.6</td><td>97.1</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents the language fidelity results for the XM3600 dataset. Language fidelity refers to the model&rsquo;s ability to generate output in the correct target language. The table shows the performance for different language groups (T1-T5, representing various levels of language resource availability) and different model configurations (English-only, multilingual with various English-multilingual ratios, and pre-training configurations). Each cell displays a percentage, indicating the accuracy of language fidelity for that specific model and language group. This table helps analyze how different data compositions and pre-training strategies affect the model&rsquo;s multilingual capabilities.</p><details><summary>read the caption</summary>Table 41: XM3600 Language Fidelity</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th></th><th>en</th><th>avg.</th><th>ar</th><th>es</th><th>fr</th><th>ru</th></tr></thead><tbody><tr><td>Centurio Aya</td><td>65.0</td><td>62.4</td><td>61.7</td><td>61.0</td><td>64.3</td><td>62.7</td></tr><tr><td>Centurio Qwen</td><td>75.4</td><td>70.2</td><td>68.8</td><td>70.9</td><td>70.5</td><td>70.8</td></tr><tr><td>Parrot</td><td>28.7</td><td>31.4</td><td>34.0</td><td>24.3</td><td>30.0</td><td>37.4</td></tr><tr><td>PALO 13B</td><td>56.6</td><td>53.6</td><td>51.8</td><td>52.7</td><td>54.9</td><td>55.0</td></tr><tr><td>PALO 7B</td><td>58.0</td><td>53.4</td><td>52.5</td><td>52.3</td><td>53.7</td><td>55.1</td></tr><tr><td>InternVL 2.5 4B</td><td>69.0</td><td>58.7</td><td>55.7</td><td>58.8</td><td>61.4</td><td>59.0</td></tr><tr><td>InternVL 2.5 8B</td><td>73.5</td><td>66.4</td><td>61.8</td><td>68.0</td><td>68.4</td><td>67.3</td></tr><tr><td>Qwen2-VL 2B</td><td>61.9</td><td>56.2</td><td>52.9</td><td>55.3</td><td>58.6</td><td>57.9</td></tr><tr><td>Qwen2-VL 7B</td><td>62.1</td><td>59.6</td><td>59.2</td><td>58.9</td><td>60.0</td><td>60.3</td></tr><tr><td>Maya</td><td>50.1</td><td>43.9</td><td>45.3</td><td>42.7</td><td>45.8</td><td>41.8</td></tr><tr><td>Phi 3.5 Vision</td><td>58.9</td><td>53.3</td><td>49.7</td><td>52.7</td><td>56.4</td><td>54.3</td></tr><tr><td>Pixtral 12B</td><td>60.9</td><td>52.7</td><td>36.0</td><td>57.9</td><td>59.0</td><td>58.1</td></tr><tr><td>Pangea</td><td>69.0</td><td>65.2</td><td>64.5</td><td>64.3</td><td>66.3</td><td>65.7</td></tr><tr><td>MiniCPM 2.6</td><td>71.9</td><td>65.4</td><td>61.1</td><td>67.5</td><td>67.0</td><td>66.1</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents a comparison of various multilingual vision-language models (LVLMs) on the Cross-lingual Visual Natural Language Inference (XVNLI) task. The models are evaluated across multiple languages (English, Arabic, Spanish, French, Russian), and the results are expressed as percentages, indicating the model&rsquo;s accuracy on the task. The table allows for a comparison of the models&rsquo; performance in handling diverse languages and offers insight into the effectiveness of different training strategies and model architectures for cross-lingual understanding in a vision-language context.</p><details><summary>read the caption</summary>Table 42: XVNLI</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th></th><th>en</th><th>avg.</th><th>ar</th><th>fr</th><th>hi</th><th>id</th><th>ja</th><th>pt</th></tr></thead><tbody><tr><td>Centurio Aya</td><td>37.6</td><td>37.2</td><td>36.2</td><td>38.9</td><td>38.8</td><td>39.7</td><td>34.2</td><td>35.4</td></tr><tr><td>Centurio Qwen</td><td>46.4</td><td>43.0</td><td>39.6</td><td>45.0</td><td>41.6</td><td>44.1</td><td>43.5</td><td>44.1</td></tr><tr><td>Parrot</td><td>35.3</td><td>32.4</td><td>31.9</td><td>34.9</td><td>26.1</td><td>31.3</td><td>34.9</td><td>35.4</td></tr><tr><td>PALO 13B</td><td>32.4</td><td>28.9</td><td>24.2</td><td>34.9</td><td>24.2</td><td>31.6</td><td>26.4</td><td>32.3</td></tr><tr><td>PALO 7B</td><td>31.8</td><td>30.9</td><td>28.2</td><td>33.6</td><td>27.3</td><td>30.6</td><td>32.3</td><td>33.3</td></tr><tr><td>InternVL 2.5 4B</td><td>49.2</td><td>42.7</td><td>41.6</td><td>45.6</td><td>33.7</td><td>43.4</td><td>44.2</td><td>47.8</td></tr><tr><td>InternVL 2.5 8B</td><td>50.7</td><td>45.2</td><td>40.3</td><td>48.7</td><td>41.2</td><td>43.1</td><td>47.6</td><td>50.2</td></tr><tr><td>Qwen2-VL 2B</td><td>36.8</td><td>35.5</td><td>31.5</td><td>41.3</td><td>30.2</td><td>36.7</td><td>36.1</td><td>37.0</td></tr><tr><td>Qwen2-VL 7B</td><td>43.0</td><td>40.7</td><td>36.9</td><td>42.6</td><td>38.5</td><td>41.1</td><td>41.3</td><td>43.8</td></tr><tr><td>Maya</td><td>37.9</td><td>33.3</td><td>32.6</td><td>36.6</td><td>31.3</td><td>31.6</td><td>32.0</td><td>36.0</td></tr><tr><td>Phi 3.5 Vision</td><td>41.7</td><td>37.4</td><td>34.9</td><td>44.3</td><td>29.2</td><td>37.7</td><td>35.7</td><td>42.4</td></tr><tr><td>Pixtral 12B</td><td>30.3</td><td>26.2</td><td>19.1</td><td>28.5</td><td>19.2</td><td>27.3</td><td>28.6</td><td>34.7</td></tr><tr><td>Pangea</td><td>43.1</td><td>42.0</td><td>37.6</td><td>43.0</td><td>38.5</td><td>46.8</td><td>41.6</td><td>44.8</td></tr><tr><td>MiniCPM 2.6</td><td>39.1</td><td>36.5</td><td>30.5</td><td>38.9</td><td>33.7</td><td>37.7</td><td>37.2</td><td>40.7</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents a detailed comparison of various multilingual vision-language models&rsquo; performance on the xMMMU (cross-lingual, multi-modal, multiple-choice visual question answering) dataset. It breaks down the accuracy scores (using exact match) achieved by different models across various language groups (T1-T5 tiers representing low to high resource languages), showing the average accuracy across all languages and the performance for each language individually. This allows for a nuanced understanding of each model&rsquo;s strengths and weaknesses in handling multilingual and multimodal data. The models included represent a mix of architectures and training strategies, enabling an analysis of the influence of factors such as model size, training data composition, and multilingual training techniques.</p><details><summary>read the caption</summary>Table 43: xMMMU</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption>Model Name | avg. | amh-ethiopia | arz-egypt | ben-india | bre-france | bul-bulgaria | fil-philippines | gle-ireland | hin-india | ibo-nigeria | ind-indonesia | jav-indonesia | jpn-japan | kin-rwanda | kor-south korea | mar-india | min-indonesia | mon-mongolia | msa-malaysia | nor-norway | orm-ethiopia | por-brazil | ron-romania | rus-russia | sin-sri lanka | spa-argentina | spa-chile | spa-colombia | spa-ecuador | spa-mexico | spa-spain | spa-uruguay | sun-indonesia | swa-kenya | tam-india | tel-india | urd-india | urd-pakistan | zho-china | zho-singapore
&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;|&mdash;
Centurio Aya | 49.4 | 32.1 | 52.7 | 45.8 | 30.4 | 50.1 | 48.8 | 41.7 | 67.2 | 31.0 | 53.6 | 41.1 | 44.8 | 32.8 | 61.7 | 56.9 | 42.6 | 29.2 | 52.7 | 55.5 | 36.4 | 65.1 | 61.6 | 65.5 | 28.9 | 60.0 | 58.5 | 56.0 | 55.2 | 52.6 | 68.2 | 40.3 | 42.0 | 50.2 | 49.5 | 37.0 | 47.3 | 50.5 | 64.6 | 65.6
Centurio Qwen | 52.9 | 38.0 | 52.7 | 54.9 | 30.6 | 49.6 | 51.7 | 48.5 | 65.7 | 30.5 | 54.9 | 46.1 | 44.8 | 42.1 | 66.2 | 55.9 | 41.8 | 33.3 | 55.9 | 58.2 | 35.0 | 70.8 | 57.0 | 67.5 | 42.7 | 63.8 | 66.2 | 59.8 | 58.8 | 61.9 | 70.4 | 42.5 | 41.5 | 56.4 | 43.0 | 50.5 | 52.7 | 56.9 | 71.1 | 73.1
Parrot | 41.1 | 31.6 | 35.5 | 33.9 | 31.1 | 38.8 | 45.3 | 45.1 | 41.8 | 35.5 | 43.4 | 37.7 | 34.5 | 32.8 | 47.6 | 32.2 | 36.3 | 34.3 | 42.9 | 49.5 | 34.6 | 60.9 | 44.0 | 45.0 | 28.0 | 47.9 | 52.1 | 48.5 | 43.9 | 44.3 | 65.7 | 36.2 | 31.5 | 40.7 | 36.9 | 29.5 | 31.8 | 32.9 | 64.3 | 55.2
PALO 13B | 39.6 | 26.1 | 31.5 | 33.9 | 31.6 | 35.3 | 45.3 | 41.1 | 40.3 | 32.5 | 41.0 | 35.4 | 33.0 | 28.9 | 42.8 | 37.6 | 37.8 | 26.3 | 44.1 | 54.5 | 29.4 | 53.9 | 52.6 | 44.0 | 24.9 | 47.5 | 50.0 | 47.3 | 47.8 | 44.9 | 63.2 | 39.4 | 37.5 | 39.6 | 31.3 | 29.5 | 35.9 | 40.3 | 46.6 | 39.6
PALO 7B | 37.1 | 20.5 | 25.1 | 30.1 | 27.7 | 32.6 | 43.3 | 37.1 | 43.3 | 31.0 | 37.9 | 33.7 | 29.1 | 29.4 | 42.4 | 33.7 | 31.1 | 25.6 | 36.8 | 47.5 | 33.6 | 51.1 | 49.3 | 45.5 | 24.0 | 47.5 | 49.1 | 45.2 | 45.9 | 42.4 | 57.5 | 36.2 | 31.5 | 31.1 | 29.0 | 28.5 | 34.1 | 40.3 | 46.3 | 42.0
InternVL 2.5 4B | 48.1 | 36.3 | 38.9 | 42.7 | 33.1 | 42.0 | 47.8 | 45.7 | 51.7 | 29.0 | 54.6 | 44.4 | 39.4 | 34.9 | 65.9 | 48.0 | 41.0 | 27.6 | 53.0 | 52.8 | 34.6 | 66.5 | 49.7 | 65.5 | 34.7 | 60.4 | 59.8 | 54.4 | 56.6 | 53.9 | 68.2 | 44.8 | 44.0 | 45.8 | 35.5 | 41.0 | 47.7 | 41.7 | 74.6 | 66.5
InternVL 2.5 8B | 48.6 | 29.5 | 41.4 | 42.3 | 29.4 | 47.4 | 46.8 | 47.5 | 50.2 | 33.5 | 54.9 | 44.8 | 41.4 | 32.8 | 56.9 | 43.6 | 44.6 | 33.0 | 54.3 | 55.2 | 32.2 | 64.4 | 60.3 | 62.5 | 29.8 | 60.4 | 65.4 | 56.4 | 59.7 | 57.0 | 72.3 | 44.8 | 41.5 | 50.9 | 35.5 | 39.5 | 44.1 | 39.8 | 78.5 | 72.6
Qwen2-VL 2B | 33.6 | 27.4 | 31.0 | 33.6 | 25.9 | 32.9 | 32.0 | 31.3 | 34.8 | 35.5 | 36.9 | 31.6 | 25.1 | 31.5 | 37.6 | 24.3 | 27.9 | 31.1 | 32.1 | 40.5 | 33.2 | 39.8 | 32.5 | 33.0 | 24.9 | 40.0 | 40.2 | 40.7 | 39.0 | 35.0 | 42.1 | 34.6 | 33.5 | 37.7 | 25.2 | 27.0 | 30.5 | 31.9 | 44.1 | 41.5
Qwen2-VL 7B | 37.6 | 31.2 | 35.5 | 31.5 | 31.1 | 35.6 | 40.9 | 37.1 | 39.3 | 31.0 | 40.8 | 32.3 | 36.0 | 30.2 | 43.4 | 31.2 | 33.5 | 34.0 | 43.5 | 42.8 | 37.4 | 47.9 | 34.8 | 47.5 | 30.7 | 44.5 | 47.9 | 40.7 | 42.3 | 40.2 | 47.8 | 41.6 | 31.5 | 34.1 | 26.6 | 26.5 | 37.3 | 31.0 | 51.4 | 43.4
Maya | 39.8 | 30.3 | 41.9 | 38.8 | 30.6 | 36.7 | 35.0 | 34.4 | 46.8 | 31.0 | 36.2 | 34.7 | 29.1 | 31.5 | 50.0 | 42.6 | 33.9 | 31.1 | 44.4 | 47.5 | 29.9 | 53.5 | 51.3 | 42.0 | 30.2 | 44.9 | 47.4 | 45.2 | 45.6 | 39.3 | 55.7 | 34.3 | 33.5 | 38.8 | 32.2 | 29.0 | 43.2 | 48.1 | 50.5 | 50.9
Llama-Vision | 38.8 | 32.1 | 5.4 | 60.1 | 13.6 | 22.4 | 43.8 | 35.9 | 46.3 | 28.5 | 42.0 | 34.0 | 25.1 | 18.3 | 45.9 | 38.1 | 34.3 | 27.9 | 40.6 | 48.5 | 23.4 | 38.7 | 47.7 | 52.0 | 48.4 | 47.9 | 55.1 | 51.0 | 48.1 | 48.3 | 70.1 | 37.1 | 29.5 | 52.0 | 60.7 | 62.5 | 24.5 | 15.3 | 36.3 | 23.1
Phi 3.5 Vision | 40.9 | 28.6 | 38.4 | 28.7 | 28.9 | 33.7 | 45.3 | 40.2 | 42.8 | 38.0 | 40.8 | 35.4 | 36.9 | 36.2 | 44.1 | 33.7 | 39.0 | 35.3 | 41.3 | 48.2 | 33.6 | 62.0 | 43.7 | 45.0 | 29.3 | 55.1 | 59.0 | 51.9 | 52.8 | 48.0 | 64.2 | 45.1 | 32.0 | 46.5 | 29.4 | 32.5 | 29.5 | 26.9 | 51.1 | 40.6
Pixtral 12B | 33.5 | 22.6 | 27.1 | 21.7 | 24.9 | 30.5 | 35.5 | 38.7 | 41.3 | 26.5 | 36.9 | 32.3 | 27.6 | 25.1 | 39.7 | 24.3 | 28.3 | 24.7 | 36.8 | 32.1 | 21.5 | 41.2 | 28.8 | 40.5 | 23.6 | 49.1 | 54.3 | 43.6 | 48.3 | 40.6 | 48.4 | 40.3 | 27.0 | 42.1 | 22.9 | 19.0 | 27.7 | 23.6 | 47.3 | 39.6
Pangea | 55.2 | 35.5 | 49.3 | 53.5 | 33.1 | 52.3 | 56.7 | 53.1 | 66.2 | 40.0 | 60.0 | 50.5 | 42.9 | 33.6 | 68.3 | 57.9 | 48.2 | 40.7 | 60.3 | 58.2 | 36.4 | 69.7 | 62.9 | 73.5 | 36.0 | 63.8 | 67.1 | 61.4 | 62.4 | 60.7 | 73.0 | 44.8 | 49.0 | 65.6 | 46.7 | 55.0 | 57.7 | 65.7 | 71.7 | 68.4
MiniCPM 2.6 | 34.1 | 26.9 | 31.5 | 27.6 | 25.9 | 32.6 | 31.5 | 37.1 | 32.3 | 36.0 | 36.2 | 31.6 | 33.5 | 30.6 | 31.0 | 32.2 | 29.9 | 29.2 | 34.6 | 40.5 | 36.0 | 45.1 | 33.4 | 37.0 | 26.7 | 37.7 | 44.4 | 40.7 | 38.7 | 35.9 | 42.5 | 36.2 | 29.0 | 34.1 | 24.3 | 28.5 | 32.7 | 22.7 | 48.2 | 46.2</table></figure><blockquote><p>üîº This table presents a comparison of various Large Vision-Language Models (LVLMs) on the CVQA (Cross-lingual Visual Question Answering) task. It shows the performance of each model across multiple languages, highlighting the performance of Centurio, the model introduced in the paper, against other state-of-the-art models. The metrics used likely reflect accuracy, potentially broken down by language.</p><details><summary>read the caption</summary>Table 44: CVQA</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th></th><th>en</th><th>avg.</th><th>avg. Latin</th><th>avg. other</th><th>ar</th><th>de</th><th>hi</th><th>id</th><th>it</th><th>ko</th><th>ru</th><th>th</th><th>zh</th><th>zu</th></tr></thead><tbody><tr><td>Centurio Aya</td><td>83.1</td><td>74.2</td><td>80.9</td><td>69.7</td><td>75.9</td><td>82.1</td><td>80.1</td><td>81.4</td><td>80.6</td><td>68.8</td><td>73.5</td><td>66.5</td><td>53.4</td><td>79.5</td></tr><tr><td>Centurio Qwen</td><td>84.8</td><td>76.1</td><td>82.7</td><td>71.8</td><td>76.9</td><td>83.5</td><td>82.4</td><td>83.8</td><td>83.1</td><td>72.4</td><td>75.6</td><td>64.4</td><td>58.9</td><td>80.2</td></tr><tr><td>Parrot</td><td>51.0</td><td>49.9</td><td>50.5</td><td>49.5</td><td>50.4</td><td>51.6</td><td>49.6</td><td>51.0</td><td>49.8</td><td>50.4</td><td>50.5</td><td>48.2</td><td>47.8</td><td>49.5</td></tr><tr><td>PALO 13B</td><td>54.0</td><td>51.5</td><td>52.7</td><td>50.7</td><td>50.9</td><td>53.2</td><td>51.2</td><td>52.5</td><td>52.8</td><td>51.0</td><td>49.5</td><td>51.0</td><td>50.7</td><td>52.1</td></tr><tr><td>PALO 7B</td><td>55.5</td><td>52.8</td><td>55.4</td><td>51.0</td><td>50.4</td><td>56.9</td><td>51.0</td><td>55.0</td><td>54.1</td><td>51.6</td><td>51.1</td><td>51.4</td><td>50.2</td><td>55.8</td></tr><tr><td>InternVL 2.5 4B</td><td>87.0</td><td>78.3</td><td>86.9</td><td>72.6</td><td>54.9</td><td>87.6</td><td>59.8</td><td>87.0</td><td>88.2</td><td>89.4</td><td>86.4</td><td>55.1</td><td>90.4</td><td>84.8</td></tr><tr><td>InternVL 2.5 8B</td><td>91.0</td><td>79.2</td><td>88.7</td><td>72.8</td><td>55.8</td><td>89.8</td><td>54.9</td><td>89.1</td><td>89.1</td><td>92.5</td><td>86.9</td><td>53.1</td><td>93.6</td><td>86.9</td></tr><tr><td>Qwen2-VL 2B</td><td>85.0</td><td>83.5</td><td>83.4</td><td>83.5</td><td>70.6</td><td>84.4</td><td>86.5</td><td>84.1</td><td>83.5</td><td>88.1</td><td>78.8</td><td>86.4</td><td>90.4</td><td>81.8</td></tr><tr><td>Qwen2-VL 7B</td><td>91.2</td><td>90.9</td><td>90.1</td><td>91.4</td><td>83.4</td><td>90.5</td><td>94.8</td><td>91.0</td><td>90.8</td><td>93.8</td><td>87.5</td><td>94.1</td><td>94.9</td><td>88.2</td></tr><tr><td>Maya</td><td>51.4</td><td>50.9</td><td>51.6</td><td>50.4</td><td>50.4</td><td>53.4</td><td>50.1</td><td>51.5</td><td>50.0</td><td>49.9</td><td>49.5</td><td>51.1</td><td>51.6</td><td>51.6</td></tr><tr><td>Llama-Vision</td><td>91.1</td><td>84.8</td><td>89.9</td><td>81.5</td><td>63.2</td><td>90.1</td><td>91.1</td><td>89.5</td><td>91.9</td><td>87.4</td><td>83.0</td><td>84.8</td><td>79.5</td><td>88.0</td></tr><tr><td>Phi 3.5 Vision</td><td>92.2</td><td>79.4</td><td>90.2</td><td>72.2</td><td>53.1</td><td>91.9</td><td>83.8</td><td>89.2</td><td>90.9</td><td>77.9</td><td>86.6</td><td>55.5</td><td>76.5</td><td>88.8</td></tr><tr><td>Pixtral 12B</td><td>91.1</td><td>71.0</td><td>90.5</td><td>58.0</td><td>50.4</td><td>91.5</td><td>53.6</td><td>91.1</td><td>90.9</td><td>49.5</td><td>88.2</td><td>52.9</td><td>53.4</td><td>88.4</td></tr><tr><td>Pangea</td><td>87.2</td><td>72.2</td><td>85.7</td><td>63.1</td><td>51.5</td><td>86.6</td><td>69.4</td><td>86.2</td><td>87.1</td><td>71.4</td><td>79.2</td><td>54.4</td><td>52.9</td><td>82.9</td></tr><tr><td>MiniCPM 2.6</td><td>89.0</td><td>74.3</td><td>88.0</td><td>65.2</td><td>52.0</td><td>89.0</td><td>53.1</td><td>87.9</td><td>89.0</td><td>54.8</td><td>84.0</td><td>53.1</td><td>94.5</td><td>86.0</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents the results of the SMPQA Grounding task, a new benchmark evaluating multilingual OCR capabilities. The results are broken down by language, model, and training strategy. It showcases the performance of several models across multiple languages, highlighting the impact of different training strategies on multilingual text-in-image understanding.</p><details><summary>read the caption</summary>Table 45: SMPQA Ground</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table><thead><tr><th style=text-align:left></th><th style=text-align:center>en</th><th style=text-align:center>avg.</th><th style=text-align:center>avg. Latin</th><th style=text-align:center>avg. other</th><th style=text-align:center>ar</th><th style=text-align:center>de</th><th style=text-align:center>hi</th><th style=text-align:center>id</th><th style=text-align:center>it</th><th style=text-align:center>ko</th><th style=text-align:center>ru</th><th style=text-align:center>th</th><th style=text-align:center>zh</th><th style=text-align:center>zu</th></tr></thead><tbody><tr><td style=text-align:left>Centurio Aya</td><td style=text-align:center>60.0</td><td style=text-align:center>30.1</td><td style=text-align:center>49.8</td><td style=text-align:center>17.0</td><td style=text-align:center>29.2</td><td style=text-align:center>50.2</td><td style=text-align:center>17.6</td><td style=text-align:center>52.6</td><td style=text-align:center>51.2</td><td style=text-align:center>11.2</td><td style=text-align:center>38.2</td><td style=text-align:center>4.8</td><td style=text-align:center>0.8</td><td style=text-align:center>45.2</td></tr><tr><td style=text-align:left>Centurio Qwen</td><td style=text-align:center>65.2</td><td style=text-align:center>31.7</td><td style=text-align:center>54.3</td><td style=text-align:center>16.6</td><td style=text-align:center>21.4</td><td style=text-align:center>53.2</td><td style=text-align:center>21.4</td><td style=text-align:center>55.4</td><td style=text-align:center>56.6</td><td style=text-align:center>16.2</td><td style=text-align:center>34.8</td><td style=text-align:center>5.2</td><td style=text-align:center>0.6</td><td style=text-align:center>52.2</td></tr><tr><td style=text-align:left>Parrot</td><td style=text-align:center>0.0</td><td style=text-align:center>0.0</td><td style=text-align:center>0.0</td><td style=text-align:center>0.1</td><td style=text-align:center>0.0</td><td style=text-align:center>0.0</td><td style=text-align:center>0.0</td><td style=text-align:center>0.0</td><td style=text-align:center>0.0</td><td style=text-align:center>0.4</td><td style=text-align:center>0.0</td><td style=text-align:center>0.0</td><td style=text-align:center>0.0</td><td style=text-align:center>0.0</td></tr><tr><td style=text-align:left>PALO 13B</td><td style=text-align:center>25.6</td><td style=text-align:center>4.0</td><td style=text-align:center>9.9</td><td style=text-align:center>0.1</td><td style=text-align:center>0.0</td><td style=text-align:center>12.0</td><td style=text-align:center>0.0</td><td style=text-align:center>10.2</td><td style=text-align:center>12.4</td><td style=text-align:center>0.4</td><td style=text-align:center>0.0</td><td style=text-align:center>0.0</td><td style=text-align:center>0.0</td><td style=text-align:center>5.0</td></tr><tr><td style=text-align:left>PALO 7B</td><td style=text-align:center>22.4</td><td style=text-align:center>2.7</td><td style=text-align:center>6.7</td><td style=text-align:center>0.1</td><td style=text-align:center>0.0</td><td style=text-align:center>8.4</td><td style=text-align:center>0.0</td><td style=text-align:center>7.0</td><td style=text-align:center>7.0</td><td style=text-align:center>0.4</td><td style=text-align:center>0.0</td><td style=text-align:center>0.0</td><td style=text-align:center>0.0</td><td style=text-align:center>4.4</td></tr><tr><td style=text-align:left>InternVL 2.5 4B</td><td style=text-align:center>77.8</td><td style=text-align:center>47.5</td><td style=text-align:center>67.7</td><td style=text-align:center>34.0</td><td style=text-align:center>0.0</td><td style=text-align:center>71.0</td><td style=text-align:center>0.0</td><td style=text-align:center>69.8</td><td style=text-align:center>69.6</td><td style=text-align:center>69.0</td><td style=text-align:center>54.4</td><td style=text-align:center>0.2</td><td style=text-align:center>80.2</td><td style=text-align:center>60.4</td></tr><tr><td style=text-align:left>InternVL 2.5 8B</td><td style=text-align:center>80.6</td><td style=text-align:center>48.2</td><td style=text-align:center>68.1</td><td style=text-align:center>34.9</td><td style=text-align:center>0.0</td><td style=text-align:center>69.2</td><td style=text-align:center>0.0</td><td style=text-align:center>70.4</td><td style=text-align:center>70.8</td><td style=text-align:center>67.2</td><td style=text-align:center>61.2</td><td style=text-align:center>0.2</td><td style=text-align:center>80.8</td><td style=text-align:center>62.2</td></tr><tr><td style=text-align:left>Qwen2-VL 2B</td><td style=text-align:center>68.8</td><td style=text-align:center>47.4</td><td style=text-align:center>60.0</td><td style=text-align:center>39.0</td><td style=text-align:center>0.2</td><td style=text-align:center>61.2</td><td style=text-align:center>24.8</td><td style=text-align:center>59.4</td><td style=text-align:center>61.2</td><td style=text-align:center>66.0</td><td style=text-align:center>46.8</td><td style=text-align:center>24.0</td><td style=text-align:center>72.0</td><td style=text-align:center>58.2</td></tr><tr><td style=text-align:left>Qwen2-VL 7B</td><td style=text-align:center>85.0</td><td style=text-align:center>64.9</td><td style=text-align:center>76.2</td><td style=text-align:center>57.4</td><td style=text-align:center>1.8</td><td style=text-align:center>80.6</td><td style=text-align:center>58.6</td><td style=text-align:center>75.8</td><td style=text-align:center>79.2</td><td style=text-align:center>77.6</td><td style=text-align:center>70.6</td><td style=text-align:center>43.8</td><td style=text-align:center>92.0</td><td style=text-align:center>69.2</td></tr><tr><td style=text-align:left>Maya</td><td style=text-align:center>14.6</td><td style=text-align:center>1.8</td><td style=text-align:center>4.3</td><td style=text-align:center>0.1</td><td style=text-align:center>0.0</td><td style=text-align:center>8.2</td><td style=text-align:center>0.0</td><td style=text-align:center>3.6</td><td style=text-align:center>4.6</td><td style=text-align:center>0.4</td><td style=text-align:center>0.0</td><td style=text-align:center>0.0</td><td style=text-align:center>0.0</td><td style=text-align:center>0.8</td></tr><tr><td style=text-align:left>Llama-Vision</td><td style=text-align:center>58.4</td><td style=text-align:center>22.8</td><td style=text-align:center>46.6</td><td style=text-align:center>6.9</td><td style=text-align:center>0.0</td><td style=text-align:center>55.4</td><td style=text-align:center>2.4</td><td style=text-align:center>38.4</td><td style=text-align:center>37.2</td><td style=text-align:center>8.4</td><td style=text-align:center>13.0</td><td style=text-align:center>6.0</td><td style=text-align:center>11.8</td><td style=text-align:center>55.4</td></tr><tr><td style=text-align:left>Phi 3.5 Vision</td><td style=text-align:center>84.8</td><td style=text-align:center>35.9</td><td style=text-align:center>69.4</td><td style=text-align:center>13.5</td><td style=text-align:center>0.2</td><td style=text-align:center>70.8</td><td style=text-align:center>12.0</td><td style=text-align:center>69.4</td><td style=text-align:center>76.6</td><td style=text-align:center>15.4</td><td style=text-align:center>40.4</td><td style=text-align:center>0.2</td><td style=text-align:center>12.8</td><td style=text-align:center>61.0</td></tr><tr><td style=text-align:left>Pixtral 12B</td><td style=text-align:center>85.0</td><td style=text-align:center>35.9</td><td style=text-align:center>73.3</td><td style=text-align:center>10.9</td><td style=text-align:center>0.0</td><td style=text-align:center>71.8</td><td style=text-align:center>0.0</td><td style=text-align:center>75.4</td><td style=text-align:center>81.6</td><td style=text-align:center>0.4</td><td style=text-align:center>64.6</td><td style=text-align:center>0.4</td><td style=text-align:center>0.0</td><td style=text-align:center>64.6</td></tr><tr><td style=text-align:left>Pangea</td><td style=text-align:center>72.0</td><td style=text-align:center>23.8</td><td style=text-align:center>54.4</td><td style=text-align:center>3.4</td><td style=text-align:center>0.0</td><td style=text-align:center>58.6</td><td style=text-align:center>0.2</td><td style=text-align:center>57.2</td><td style=text-align:center>64.4</td><td style=text-align:center>0.4</td><td style=text-align:center>19.2</td><td style=text-align:center>0.4</td><td style=text-align:center>0.0</td><td style=text-align:center>37.4</td></tr><tr><td style=text-align:left>MiniCPM 2.6</td><td style=text-align:center>80.8</td><td style=text-align:center>39.3</td><td style=text-align:center>67.5</td><td style=text-align:center>20.6</td><td style=text-align:center>0.0</td><td style=text-align:center>67.2</td><td style=text-align:center>0.0</td><td style=text-align:center>69.8</td><td style=text-align:center>71.4</td><td style=text-align:center>1.0</td><td style=text-align:center>38.4</td><td style=text-align:center>0.4</td><td style=text-align:center>83.6</td><td style=text-align:center>61.6</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents the results of the SMPQA-Name task, a new benchmark for evaluating multilingual OCR capabilities in images. It shows the performance of various models across different languages, categorized by script type (Latin or other) and resource level. The metrics used are likely accuracy scores, possibly separated for Latin-script and other-script languages, demonstrating each model&rsquo;s proficiency in reading text from images containing various scripts and language families.</p><details><summary>read the caption</summary>Table 46: SMPQA Name</details></blockquote></details><h3 class="relative group">Full paper<div id=full-paper class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#full-paper aria-label=Anchor>#</a></span></h3><div id=gallery-85f60bee833f52b6deb8fa0e36c6ad3c class=gallery><img src=https://ai-paper-reviewer.com/2501.05122/1.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.05122/2.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.05122/3.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.05122/4.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.05122/5.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.05122/6.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.05122/7.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.05122/8.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.05122/9.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.05122/10.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.05122/11.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.05122/12.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.05122/13.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.05122/14.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.05122/15.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.05122/16.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.05122/17.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.05122/18.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.05122/19.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2501.05122/20.png class="grid-w50 md:grid-w33 xl:grid-w25"></div></div><section class="flex flex-row flex-wrap justify-center pt-4 text-xl"><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.05122/&amp;title=Centurio:%20On%20Drivers%20of%20Multilingual%20Ability%20of%20Large%20Vision-Language%20Model" title="Share on LinkedIn" aria-label="Share on LinkedIn"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://twitter.com/intent/tweet/?url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.05122/&amp;text=Centurio:%20On%20Drivers%20of%20Multilingual%20Ability%20of%20Large%20Vision-Language%20Model" title="Tweet on Twitter" aria-label="Tweet on Twitter"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="mailto:?body=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.05122/&amp;subject=Centurio:%20On%20Drivers%20of%20Multilingual%20Ability%20of%20Large%20Vision-Language%20Model" title="Send via email" aria-label="Send via email"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1-27.64 140.9 68.65 266.2 199.1 285.1 19.01 2.888 36.17-12.26 36.17-31.49l1e-4-.6631c0-15.74-11.44-28.88-26.84-31.24-84.35-12.98-149.2-86.13-149.2-174.2.0-102.9 88.61-185.5 193.4-175.4 91.54 8.869 158.6 91.25 158.6 183.2v16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98.0036c-7.299.0-13.2 4.992-15.12 11.68-24.85-12.15-54.24-16.38-86.06-5.106-38.75 13.73-68.12 48.91-73.72 89.64-9.483 69.01 43.81 128 110.9 128 26.44.0 50.43-9.544 69.59-24.88 24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3c0-149.2-133.9-265.632-287.3-235.57zM239.1 304.3c-26.47.0-48-21.56-48-48.05s21.53-48.05 48-48.05 48 21.56 48 48.05-20.6 48.05-48 48.05z"/></svg></span></a></section></div><script>var oid="views_paper-reviews/2501.05122/index.md",oid_likes="likes_paper-reviews/2501.05122/index.md"</script><script type=text/javascript src=/ai-paper-reviewer/js/page.min.0860cf4e04fa2d72cc33ddba263083464d48f67de06114529043cb4623319efed4f484fd7f1730df5abea0e2da6f3538855634081d02f2d6e920b956f063e823.js integrity="sha512-CGDPTgT6LXLMM926JjCDRk1I9n3gYRRSkEPLRiMxnv7U9IT9fxcw31q+oOLabzU4hVY0CB0C8tbpILlW8GPoIw=="></script></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="flex group mr-3" href=/ai-paper-reviewer/paper-reviews/2501.05441/><span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&larr;</span>
<span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">The GAN is dead; long live the GAN! A Modern GAN Baseline</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2025-01-09T00:00:00+00:00>9 January 2025</time>
</span></span></a></span><span><a class="flex text-right group ml-3" href=/ai-paper-reviewer/paper-reviews/2501.05453/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">An Empirical Study of Autoregressive Pre-training from Videos</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2025-01-09T00:00:00+00:00>9 January 2025</time>
</span></span><span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&larr;</span></a></span></div></div><div class=pt-3><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class=pt-3><script src=https://utteranc.es/client.js repo=pmnxis/pmnxis.github.io issue-term=pathname label=Comment theme=dark-blue crossorigin=anonymous async></script></div></div></footer></article><div id=top-scroller class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer id=site-footer class="py-10 print:hidden"><nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400"><ul class="flex flex-col list-none sm:flex-row"><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=/ai-paper-reviewer/tags/ title=Tags>Tags</a></li><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=https://deep-diver.github.io/neurips2024/ title>NeurIPS2024</a></li></ul></nav><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2025
AI Paper Reviews by AI</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a></p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/ai-paper-reviewer/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh+sCQ0E53ghYrxgYqw+0GCRyIEpA=="></script></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://deep-diver.github.io/ai-paper-reviewer/ style=z-index:500><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body><script data-name=BMC-Widget data-cfasync=false src=https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js data-id=chansung data-description="Support me on Buy me a coffee!" data-message data-color=#FFDD00 data-position=Left data-x_margin=18 data-y_margin=18></script></html>