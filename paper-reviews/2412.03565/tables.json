[{"content": "| Method | LLM | Vision Encoder | AI2D [25] | MMMU [87] | POPE [34] | GQA [24] | MM-Vet [85] |\n|---|---|---|---|---|---|---|---| \n| LLaVA-1.5 [40] | Vicuna-7B [15] | CLIP-ViT-Large [61] | 54.8 | 35.3 | 85.9 | 62.0 | 30.5 |\n| LLaVA-Next [41] | Vicuna-7B [15] | CLIP-ViT-Large [61] | 66.6 | 35.1 | 86.4 | 64.2 | 44.1 |\n| DeepStack-L [54] | Vicuna-7B [15] | CLIP-ViT-Large [61] | - | 35.7 | 86.7 | 63.1 | 29.9 |\n| DeepStack-L-HD [54] | Vicuna-7B [15] | CLIP-ViT-Large [61] | - | 35.6 | 86.5 | 65.2 | 37.5 |\n| VILA [38] | Vicuna-7B [15] | CLIP-ViT-Large [61] | - | - | 85.5 | 62.3 | 34.9 |\n| ShareGPT4V [10] | Vicuna-7B [15] | CLIP-ViT-Large [61] |  | - | - | - | 37.6 |\n| MM-1.5 [91] | MM-LLM-7B [91] | MM-CLIP [91] | 72.0 | 41.8 | 88.6 | - | 42.2 |\n| InternVL2 [12] | InternLM-7B [68] | InternViT-300M [12] | 83.8 | 49.3 | - | - | 60.0 |\n| LLaVA-OV (SI) [29] | Qwen2-7B [80] | SigLIP-SO400M [89] | 81.6 | 47.3 | - | - | 58.8 |\n| LLaVA-OV [29] | Qwen2-7B [80] | SigLIP-SO400M [89] | 81.4 | 48.8 | - | - | 57.5 |\n| Qwen2-VL-Instruct [74] | Qwen2-7B [80] | DFN-CLIP-H [20] | 83.0 | 54.1 | - | - | 62.0 |\n| LLaVA-Next-Inst-IT | Vicuna-7B [15] | CLIP-ViT-Large [61] | 71.0 | 37.4 | 87.2 | 65.9 | 38.1 |\n| LLaVA-Next-Inst-IT | Qwen2-7B [80] | SigLIP-SO400 [89] | 78.7 | 42.7 | 87.6 | 65.5 | 44.7 |", "caption": "Table 1: Performance of LMMs on Inst-IT\u00a0Bench. We conduct extensive evaluations on Inst-IT\u00a0Bench, including state-of-the-art open-source image models, video models, and cutting-edge proprietary models. #IT indicates the number of training samples used during the instruction-tuning stage. N/A indicates that the number of training samples is unknown.", "description": "This table presents a comprehensive evaluation of various Large Multimodal Models (LMMs) on the INST-IT Bench, a newly proposed benchmark specifically designed to assess instance-level understanding capabilities in both images and videos.  The models evaluated encompass a range of open-source models (including those specialized for image or video understanding) and cutting-edge proprietary models.  The results are broken down by model, indicating the underlying Large Language Model (LLM) used, the amount of instruction-tuning data (#IT) employed during training, and the performance metrics achieved on both open-ended and multiple-choice question-answering tasks for image and video data, separately.  The '#IT' column highlights the varying amounts of instruction-tuning data used for each model, and an 'N/A' entry signifies that the exact amount of training data is unavailable.", "section": "3. INST-IT"}, {"content": "| Method | LLM | Vision Encoder | ANet-QA [86] | EgoSchema [47] | Next-QA [77] | VideoMME [21] | TempCompass [42] |\n|---|---|---|---|---|---|---|---| \n|  |  |  | (open-ended) | (subset) | (val) | (w/o subs) | (3 avg) |\n| DeepStack-L [54] | Vicuna-7B [15] | CLIP-ViT-Large [61] | 49.3 | 38.4 | 61.0 | - | - |\n| IG-VLM [26] | Vicuna-7B [15] | CLIP-ViT-Large [61] | 54.3 | 35.8 | 63.1 | - | - |\n| LLaVA-Next [41] | Vicuna-7B [15] | CLIP-ViT-Large [61] | 53.8 | - | - | - | - |\n| SF-LLaVA [78] | Vicuna-7B [15] | CLIP-ViT-Large [61] | 55.5 | 47.2 | 64.2 | - |  |\n| Video-ChatGPT [46] | Vicuna-7B [15] | CLIP-ViT-Large [61] | 35.2 | 47.3 | - | - | - |\n| VideoLLaMA2 [13] | Vicuna-7B [15] | CLIP-ViT-Large [61] | 50.2 | - | 51.7 | - | - |\n| LLaVA-Next-Video [96] | Vicuna-7B [15] | CLIP-ViT-Large [61] | 53.5 | 43.9 | - | 46.5 | - |\n| LLaVA-Next-Video-DPO [96] | Vicuna-7B [15] | CLIP-ViT-Large [61] | 60.2 |  | - | - | 58.3 |\n| LongVA [93] | Qwen2-7B [80] | CLIP-ViT-Large [61] | - |  | 68.3 | 52.4 | 61.3 |\n| MM-1.5-Video-SFT [91] | MM-LLM-7B [91] | MM-CLIP [91] | 60.9 | 57.2 | 76.8 | 53.5 | - |\n| InternVL2 [12] | InternLM-7B [68] | InternViT-300M [12] | - | - | - | 54.0 | - |\n| LLaVA-OV [29] | Qwen2-7B [80] | SigLIP-SO400M [89] | 56.6 | 60.1 | 79.4 | 58.2 | 69.4 |\n| LLaVA-Video [97] | Qwen2-7B [80] | SigLIP-SO400M [89] | 56.5 | 57.3 | 83.2 | 63.3 | - |\n| Qwen2-VL-Instruct [74] | Qwen2-7B [80] | DFN-CLIP-H [20] | - | 66.7 | - | 63.3 | 72.9 |\n| LLaVA-Next-Inst-IT | Vicuna-7B [15] | CLIP-ViT-Large [61] | 53.7 | 57.8 | 70.2 | 44.3 | 59.8 |\n| LLaVA-Next-Inst-IT | Qwen2-7B [80] | SigLIP-SO400 [89] | 55.2 | 50.4 | 73.0 | 54.0 | 63.9 |", "caption": "Table 2: Main results on image benchmarks.", "description": "This table presents the performance comparison of various Large Multimodal Models (LMMs) on several image understanding benchmarks.  It shows the quantitative results of each model on benchmarks such as AI2D, MMMU, POPE, GQA, and MM-VET, indicating their capabilities in understanding and reasoning about images.  The models' performances are compared in terms of accuracy or F1 scores, providing a quantitative overview of their strengths and weaknesses in handling various image-related tasks.", "section": "4. Experiments"}, {"content": "| CL | Tune Enc | Data comb | AI2D | MMMU | POPE | GQA | Inst-IT-I | Next-QA | VideoMME | Inst-IT-V |\n| :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |\n|  | Tune Enc | Data comb | AI2D | MMMU | POPE | GQA | Inst-IT-I | Next-QA | VideoMME | Inst-IT-V |\n|  |  |  | (test) | (val) | (test F1) | (val) | (mc) | (mc) | (w/o subt) | (mc) |\n|  | All layers | LLaVA-Next | 61.1 | 35.9 | 86.9 | 61.4 | 45.3 | 56.6 | 45.7 | 31.3 |\n|  | All layers | LLaVA-Next & Inst-IT Dataset<sub>video</sub> | 60.7 | 34.7 | 86.1 | 61.2 | 60.7 | 59.7 | 47.1 | 43.0 |\n| \u2713 | All layers | LLaVA-Next & Inst-IT Dataset<sub>video</sub> | 62.3 | 35.7 | 86.7 | 62.9 | 61.8 | 62.4 | 46.7 | 44.4 |\n| \u2713 | None | LLaVA-Next & Inst-IT Dataset<sub>video</sub> | 63.1 | 35.0 | 86.9 | 62.5 | 60.2 | 63.2 | 47.2 | 44.3 |\n| \u2713 | Last 12 | LLaVA-Next & Inst-IT Dataset<sub>video</sub> | 63.2 | 34.9 | 87.0 | 62.5 | 60.1 | 63.3 | 47.2 | 44.0 |\n| \u2713 | Last 12 | LLaVA-Next & Inst-IT Dataset (img+vid) | 63.0 | 36.1 | 87.2 | 62.7 | 59.6 | 64.3 | 46.6 | 43.7 |", "caption": "Table 3: Main results on video benchmarks. We report the average of 3 parts (MCQA, Y/N, Caption Match) of TempCompass for determinism results.", "description": "This table presents the performance of various Large Multimodal Models (LMMs) on a range of video understanding benchmarks.  The benchmarks assess different aspects of video comprehension, including open-ended question answering, yes/no question answering, and caption matching. For the TempCompass benchmark, the average score across three sub-tasks is reported to provide a more comprehensive evaluation of temporal understanding. The results demonstrate the relative strengths and weaknesses of each LLM in handling various video understanding tasks.", "section": "4. Experiments"}, {"content": "| # | Data Combination | AI2D | MMMU | POPE | GQA | Inst-IT-I | Next-QA | VideoMME | Inst-IT-V |\n|---|---|---|---|---|---|---|---|---|---| \n|  |  | (test) | (val) | (F1) | (val) | (mc) | (mc) | (w/o subt) | (mc) |\n| 0 | LLaVA-Next | 61.1 | 35.9 | 86.9 | 61.4 | 45.3 | 56.6 | 45.7 | 31.3 |\n| 1 | + inst-cap & img-cap | 63.0 | 35.1 | 86.1 | 62.7 | 58.9 | 62.4 | 46.0 | 33.8 |\n| 2 | + temporal diff | 63.0 | 35.6 | 87.1 | 62.7 | 59.6 | 64.2 | 45.6 | 36.9 |\n| 3 | + video-description & qa | 63.2 | 34.9 | 87.0 | 62.5 | 60.1 | 63.3 | 47.2 | 44.0 |\n| 4 | + Inst-IT Dataset<sub>image</sub> | 63.0 | 36.1 | 87.2 | 62.7 | 59.6 | 64.3 | 46.6 | 43.7 |", "caption": "Table 4: Ablation on data training recipe. We utilize LLaVA-Qwen2-1.5B-CLIP336 as the baseline model. Inst-IT-I and Inst-IT-V indicate the multi-choice splits of the image and video part of our Inst-IT\u00a0Bench, separately.", "description": "This table presents an ablation study on the data training recipe used in the paper.  The researchers use LLaVA-Qwen2-1.5B-CLIP336 as a baseline model and evaluate the performance of different training data combinations on the Inst-IT Bench.  Inst-IT-I and Inst-IT-V represent the image and video subsets respectively of the Inst-IT Bench's multiple choice questions. The table shows how different configurations (e.g., inclusion of the Inst-IT dataset, fine-tuning all layers or only the last 12 layers of the model) impact model performance on image and video understanding tasks, as measured by the Inst-IT Bench. This analysis helps determine the optimal training recipe that balances the Inst-IT-specific fine-tuning with general multimodal instruction tuning.", "section": "3. INST-IT"}, {"content": "| Model | Rec | OCR | Know | Math | Rel | Lang | All | Rec | OCR | Know | Math | Rel | Lang | All |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| GPT-4V-turbo-detail:high [1] | 58.1 | 69.8 | 59.5 | 71.0 | 61.4 | 51.9 | 60.7 | 56.9 | 69.7 | 63.7 | 80.6 | 61.1 | 45.6 | 59.9 |\n| GPT-4V-turbo-detail:low [1] | 53.2 | 50.3 | 55.6 | 67.7 | 57.5 | 57.5 | 52.8 | 51.7 | 50.3 | 59.3 | 60.3 | 55.0 | 43.8 | 51.4 |\n| InstructBLIP-7B [16] | 36.9 | 16.3 | 34.2 | 22.3 | 26.8 | 7.5 | 31.7 | 38.9 | 17 | 35.4 | 9.7 | 29.3 | 17.5 | 33.3 |\n| Shikra-7B [9] | 40.2 | 10.0 | 28.0 | 3.5 | 18.9 | 20.6 | 33.7 | \u2013 | \u2013 | \u2013 | \u2013 | \u2013 | \u2013 | \u2013 |\n| GPT4ROI-7B [94] | 35.6 | 16.7 | 29.7 | 9.7 | 32.5 | 13.8 | 35.1 | \u2013 | \u2013 | \u2013 | \u2013 | \u2013 | \u2013 | \u2013 |\n| Kosmos-2 [58] | 29.5 | 14.2 | 18.5 | 9.7 | 7.5 | 21.9 | 26.9 | \u2013 | \u2013 | \u2013 | \u2013 | \u2013 | \u2013 | \u2013 |\n| LLaVA-1.5-7B [40] | 50.8 | 12.4 | 49.2 | 6.5 | 51.8 | 23.8 | 41.6 | 49.1 | 13.0 | 42.9 | 9.7 | 50.0 | 27.5 | 40.2 |\n| Qwen-VL-Chat [4] | 43.0 | 30.4 | 40.2 | 9.7 | 25.7 | 28.7 | 39.2 | 48.7 | 22.1 | 41.2 | 6.5 | 48.2 | 25.0 | 41.7 |\n| ViP-LLaVA-7B [5] | 54.8 | 18.8 | 52.9 | 9.7 | 53.9 | 42.5 | 45.5 | 55.3 | 17.6 | 45.9 | 8.1 | 44.6 | 33.1 | 46.8 |\n| LLaVA-Next-Inst-IT-Vicuna-7B | 51.3 | 23.7 | 54.2 | 12.9 | 64.3 | 46.2 | 45.1 | 55.0 | 21.3 | 52.5 | 16.1 | 57.5 | 40.6 | 48.2 |\n| LLaVA-Next-Inst-IT-Qwen2-7B | 58.9 | 24.5 | 48.5 | 12.9 | 48.2 | 46.3 | 50.5 | 57.7 | 22.5 | 53.2 | 19.4 | 53.6 | 45.0 | 49.0 |", "caption": "Table 5: Ablation on detailed data combination. The dataset combination in line #3 corresponds to the video part of Inst-IT\u00a0Dataset, while line #4 represents the complete Inst-IT\u00a0Dataset by incorporating the image part into line #3.", "description": "This ablation study investigates the impact of different components within the INST-IT dataset on the model's performance.  It starts with a baseline model (line #0) and progressively adds components of the INST-IT dataset: line #1 adds instance and image captions, line #2 adds information about temporal changes, line #3 incorporates all video-related annotations from INST-IT, and finally, line #4 adds image annotations to create the complete INST-IT dataset. The table shows the results of this incremental approach, demonstrating the contribution of each data aspect on various image and video benchmarks. ", "section": "3. INST-IT"}, {"content": "| Dataset Name | Ann. Type | Split | Sample Num. |\n|---|---|---|---|\n| **Video Instance Segmentation** |  |  |  |\n| BRUST [3] | mask | training | 500 |\n| UVO [75] | mask | training | 5,135 |\n| OVIS [59] | mask | training | 599 |\n| LVVIS [71] | mask | training | 3,057 |\n| YoutubeVIS [82] | mask | training | 2,897 |\n| **Video Object Tracking** |  |  |  |\n| BenSMOT [35] | box | training | 2,261 |\n| VidOR [67] | box | training | 6,969 |\n| **Image** |  |  |  |\n| SA-1B [59] | none | 1-5 | 51,101 |", "caption": "Table 6: ViP-Bench Evaluation Results. We direct perform evaluation with our Inst-IT models.", "description": "Table 6 presents the results of evaluating the Inst-IT models on the ViP-Bench benchmark.  ViP-Bench focuses on instance-level understanding, and this table shows how the Inst-IT models perform compared to other state-of-the-art models on various subtasks within ViP-Bench.  The results are broken down across several metrics (Rec, OCR, Know, Math, Rel, Lang, All) for various models and different types of prompts (synthesized visual prompts with tight bounding boxes and visual prompts from humans, e.g., arrows, circles). This allows for a comprehensive comparison of performance across different models and prompting strategies, highlighting the strengths and weaknesses of each approach on this specific benchmark focused on instance understanding.", "section": "4. Experiments"}, {"content": "| Frame | Instance-level Captions | Image-level Captions | Temporal Differences |\n|---|---|---|---|\n| <img src=\"https://arxiv.org/html/2412.03565/1.png\" width=\"124\" height=\"81\"> timestamp&lt;1&gt; | 1: Wearing a light gray suit with a white shirt, standing indoors. 2: Wearing a sleeveless white lace dress, holding an object in the hand. 3: Wearing a dark floral-patterned dress with long wavy hair. | [1] [2] [3] are standing closely together in an indoor setting. [1] is on the left side wearing a formal, light gray suit with a white shirt. [2], in the middle, is wearing a sleeveless white lace dress, holding something in their hand. [3] is on the right side in a dark floral-patterned dress with long, wavy hair. They appear to be in a room with wooden paneling and some framed art on the wall. |  |\n| <img src=\"https://arxiv.org/html/2412.03565/2.png\" width=\"124\" height=\"82\"> timestamp&lt;2&gt; | 1: A person wearing a gray suit with a white shirt, short hair. 2: A person in a white, sleeveless dress with long dark hair. 3: A person wearing a dark floral dress with long dark hair. 5: A person wearing red, partially visible in the background. 6: A small black cellphone held in a hand. | The scene appears to be in an office setting with a wooden table at the foreground. [1] is standing to the left, facing [2], and appears to be holding [2]\u2019s finger or hand. [2] stands slightly to the right, returning focus with [1]. [3] is to the right of [2], slightly in the background, smiling and looking forward. A bouquet of white flowers lies on the table near [2]. [5] is partially visible in the background on the right, seated and wearing red. [6] is a cellphone held by [5]. Background shows a wooden wall and a reflection in a window. | [1] has moved closer to [2] and is now in contact with [2]\u2019s hand. [2] has turned slightly towards [1] compared to the previous frame. [3] remains in a similar position, but the expression suggests more engagement with the scene. [5] and [6] have appeared in the frame; [5] is visible in the background holding [6]. The table with a bouquet of flowers is now visible, indicating a shift in camera angle slightly to include more of the right side of the room. |\n| <img src=\"https://arxiv.org/html/2412.03565/3.png\" width=\"124\" height=\"82\"> timestamp&lt;3&gt; | 1: Wearing a grey suit, standing beside [2] and slightly turned towards them. 2: Wearing a white, sleeveless dress with floral textures. Holding a bouquet of white flowers. 3: Wearing a dark patterned dress, standing slightly behind [2]. 4: Partially visible, wearing dark clothing, located at the edge of the left side of the frame. 5: Seated, wearing a red outfit. Holding a white object above their head, possibly obscuring their face. | The scene shows [1] [2] [3] near a wooden conference table in a professional setting, possibly an office. [1] wears a grey suit and is standing to the left, engaged with [2] who is wearing a white dress and holding flowers. [3], who is in a patterned dress, stands closely behind [2]. The newly appeared [4] is seated to the far left, partially visible at the edge of the frame. [5] is seated on the right side, holding an object above their head, possibly obscuring their face. The room has wooden walls and a framed picture hanging on the wall. | Object [5] has lifted an object above their head, possibly a piece of paper. Object [4] has appeared in the scene, seated on the left side of the frame, which was not visible earlier. The positions of objects [1], [2], and [3] remain unchanged, as does the background and setting of the room. Overall, no significant movement is noticed in terms of camera angle or position for objects [1] [2] [3]. |\n| <img src=\"https://arxiv.org/html/2412.03565/4.png\" width=\"124\" height=\"82\"> timestamp&lt;4&gt; | 1: Wearing a light gray suit jacket, white dress shirt, and dark pants. 2: Wearing a white dress with a lace overlay, fitted at the waist. 3: Wearing a patterned dress with a floral design, strapless. 4: Visible part of a person wearing a dark shirt, seated or standing near the table. | The setting appears to be indoors, with [1] [2] and [3] standing together around a table with a bouquet of flowers on it. [1] is interacting with [2], who is at the center, and they are possibly holding hands or engaged in some form of exchange. [3] is standing beside [2] and looking on, slightly leaning towards her. The room has wooden walls and a large framed picture in the background. The setting suggests a formal or ceremonial atmosphere, possibly a wedding or an official gathering. The camera angle is focused on this group, highlighting their interaction. | [1] has moved slightly closer to [2], and they appear to be holding hands or exchanging something. [5] is no longer visible in the frame, possibly due to a change in camera angle or positioning of the individuals. |\n| <img src=\"https://arxiv.org/html/2412.03565/5.png\" width=\"124\" height=\"82\"> timestamp&lt;5&gt; | 1: An adult wearing a light gray suit with button details and a white shirt. The expression and stance suggest focus and engagement. 2: An adult in a white, lacy dress with thin straps. The person has long dark hair and appears to be smiling, holding hands with [1]. 3: An adult wearing a multicolored, patterned dress. The person has long, wavy hair and is smiling while observing [1] and [2]. | The current frame captures a moment in an interior setting with [1] wearing a light gray suit, [2] in a white lace dress, and [3] in a patterned dress. [1] and [2] are engaged, with [1] facing [2] and holding their hand, suggesting an exchange, possibly a ring. [2] smiles, indicating a moment of happiness. [3] stands to the right, smiling and observing the interaction, detached but engaged with the scene. The background shows a wooden wall and framed picture, reflecting a formal environment possibly used for ceremonies. A bouquet of flowers rests on the table in front of the group. | Between the previous and the current frame, [1] and [2] have shifted slightly closer, with [1] now directly holding [2]\u2019s hand, indicating a progression in their interaction, possibly the continuation or conclusion of an exchange, such as the placing of a ring. [3] remains in a similar position but continues to observe [1] and [2], emphasizing their passive role in the interaction. There is no notable change in the background or environment. |", "caption": "Table 7: Data sources. We use 7 video datasets and 1 image dataset as our data sources. We demonstrate their annotation formats, splits we used, and the number of samples from each dataset.", "description": "Table 7 details the data sources used in the INST-IT dataset.  It lists seven video datasets and one image dataset, specifying the type of annotation provided (mask or bounding box), the data split used (training), and the total number of samples available for each dataset. This table is crucial for understanding the scale and composition of the data used in the training and evaluation of the models presented in the paper. The information provided facilitates the reproducibility and interpretation of the research.", "section": "3. INST-IT"}, {"content": "| Frame | Instance-level Captions | Image-level Captions | Temporal Differences |\n|---|---|---|---| \n| <img src=\"https://arxiv.org/html/2412.03565/6.png\" width=\"124\" height=\"82\"> timestamp &lt; 6 &gt; | 1: [1] is wearing a grey suit with a white shirt, looking forward, standing upright and smiling slightly. 2: [2] is wearing a white sleeveless dress, with hair tied back, and is standing with a calm expression. 3: [3] is wearing a floral dress with an energetic expression, standing with arms slightly bent. | The image depicts a formal setting with a group of three adults, [1], [2], and [3], standing closely together. The background features a wooden paneled wall and a framed picture. [1] and [2] are positioned in the center, both facing forward, suggesting they are the focus of the occasion. [1] is on the left, wearing a grey suit, and [2] is to the right of [1] in a white dress. They appear to be engaged in a ceremony or formal event. [3] is to the right of [2], wearing a floral dress, and displays a cheerful demeanor. The lighting is bright, illuminating their faces and creating a formal, celebratory atmosphere. | Between the frames, there is a noticeable shift in the poses and expressions of [1] and [2]. In the current frame, [1] is now standing upright with a slight smile, while previously [1] was leaning towards [2], holding [2]\u2019s hand, suggesting a shift from interaction to posing. [2], who was previously looking at [1], is now facing forward with a calm expression, indicating a change from an interactive pose to a more neutral one. Both [1] and [2] have adjusted their posture to face the camera more directly. [3] remains in similar positioning as before but has moved slightly closer to [2] and is displaying a more energetic expression, emphasizing the cheerful atmosphere. The objects on the table in the foreground, visible in the previous frame, are no longer the focal point, showing that the primary focus is now the individuals standing together. |\n| <img src=\"https://arxiv.org/html/2412.03565/7.png\" width=\"124\" height=\"82\"> timestamp &lt; 7 &gt; | 1: [1] is dressed in a grey suit with a white shirt, looking formal and neat. 2: [2] is wearing a white, sleeveless dress with a lightly patterned texture. 4: [4] is dressed in a dark outfit, including a dark scarf or similar accessory. | In the current frame, [1] is positioned in the center, wearing a grey suit and a white shirt. [2] is to the right of [1], dressed in a white sleeveless dress. [4] appears on the left side of the image, wearing a dark outfit, which includes a scarf, giving a formal look. The environment is a room with wooden walls, and a large map or blueprint hangs on the wall in the background. The lighting highlights the three individuals, [1] [2] [4], and the focus is on them standing in a formal setting. [1] and [2] appear to be closer together, engaged in the setting\u2019s activity, with [4] seeming to join or rejoin the group. | [3] is no longer visible in the current frame. [4] has appeared, standing to the left side of [1] and [2]. [1] and [2] remain in similar positions as in the previous frame, but the group now includes [4]. |\n| <img src=\"https://arxiv.org/html/2412.03565/8.png\" width=\"124\" height=\"82\"> timestamp &lt; 8 &gt; | 1: Person in a gray suit with a white shirt underneath. 2: Person wearing a white dress with long dark hair. 3: Person with long hair wearing a patterned dress, standing in the background. | The current frame shows a group of three individuals indoors, with [1] on the left in a gray suit and white shirt, facing slightly towards [2], who is dressed in a white dress with long dark hair. [2] is looking at [1], suggesting an interaction or communication between them. [3] is slightly behind [2] and smiling, indicating a positive mood. The environment appears to be an office or meeting room with a large map or artwork on the wall in the background and a wooden wall, suggesting a formal or semi-formal setting. The lighting is bright, coming from the windows in the background, creating a clear but slightly shadowed detail on the individuals. | From the previous frame to the current one, [1] and [2] appear to have shifted slightly closer to each other, with [2]\u2019s head turned towards [1] indicating interaction. [3] is now visible in the scene, having entered from the right, which suggests a new addition to the group. [4] from the previous frame is no longer visible, indicating they may have exited the frame or moved out of view. The overall composition suggests a change in group dynamics as [3] enters and [1] and [2] interact more closely. |", "caption": "Table 8: Inst-IT\u00a0Dataset Frame-level Annotation, Part I (frame 1-5). Please zoom in to view the instance ID labels.", "description": "This table presents a detailed, frame-by-frame annotation of a video segment (frames 1-5) from the INST-IT dataset.  Each frame's annotation includes instance-level captions (describing individual objects by their ID and attributes), image-level captions (describing the overall scene and object interactions), and a description of temporal changes between consecutive frames. The table is designed to showcase the level of detail provided in the dataset for instance-level understanding.", "section": "3.1 Instance-centric Annotation Pipeline"}, {"content": "| Frame | Instance-level Captions | Image-level Captions | Temporal Differences |\n|---|---|---|---| \n| <img src=\"https://arxiv.org/html/2412.03565/9.png\" width=\"124\" height=\"82\"> timestamp &lt; 9 &gt; | 1: Wearing a light gray suit with a white shirt, standing with arms relaxed at the sides. 2: Wearing a sleeveless white dress, with black hair visible, standing sideways. 3: Clapping hands, wearing a dark, sleeveless floral-patterned dress. 4: Visible hands clapping, appearing on the left side of the frame. | In the current frame, [1] is standing next to [2], both are positioned near a wooden wall, with a large framed picture or window in the background. [2] is wearing a white dress and stands slightly leaning towards [1], who is dressed in a gray suit. [3] is to the right, wearing a patterned dress and clapping her hands. On the left side of the frame, [4]\u2019s hands are visible, indicating a clapping gesture. The environment appears to be well-lit, possibly indicating a celebratory or formal gathering. | [4] has appeared in the current frame, clapping, which was not present in the previous frame. [1] and [2] have slightly shifted positions, indicating a minor adjustment in posture. The lighting in the room appears brighter in the current frame. |\n| <img src=\"https://arxiv.org/html/2412.03565/10.png\" width=\"124\" height=\"82\"> timestamp &lt; 10 &gt; | 1: [1] is wearing a grey suit with a white shirt. The person\u2019s expression is neutral. 2: [2] is wearing a white dress, has long dark hair, and is smiling. 3: [3] is wearing a dark patterned dress, has long dark hair, and is smiling. 4: [4] is partially visible, clapping hands, wearing a long sleeve. | In the current frame, [1] stands on the left wearing a grey suit and appears slightly more composed than before. [2], next to [1], in a white dress, continues smiling, directed towards [1]. [3] stands behind [2] with a continuous smile and hands still positioned as if clapping, indicating a joyous or celebratory mood. [4] is partially visible on the edge, with both hands shown as if engaged in clapping. The background remains the same, with wall decor and a wooden frame, suggesting an indoor setting. The lighting is consistent, highlighting a positive atmosphere. | Between the previous and current frames, [1] has shifted from smiling to a neutral expression. [2]\u2019s expression remains unchanged, still smiling. [3] continues to smile, maintaining the same engagement level. [4] shows hands in clapping motion slightly more forward than before. The physical positions of all individuals are largely the same, with slight adjustments in posture, possibly due to motion between shots. |\n| <img src=\"https://arxiv.org/html/2412.03565/11.png\" width=\"124\" height=\"82\"> timestamp &lt; 11 &gt; | 1: Individual in a grey suit with a light-colored shirt underneath. 2: Individual in a white dress with a flower in their hair. 3: Individual in a dark floral dress with bare shoulders. 4: Visible hand, partially in the frame, with a watch on the wrist. | The current frame captures four adults in what appears to be an intimate celebration setting, inside a room with a wooden backdrop and a framed picture on the wall. [1] and [2] are the main focus, engaged in a kiss. Both are facing each other, with [1] in a grey suit and [2] in a white dress. [3] stands to the side, clapping, and appears joyous, indicating approval or celebration. The environment is that of a seemingly formal setting with elements suggesting a personal or official celebration. [4] is partially visible, with just a hand showing, suggesting a congratulatory gesture. | Between the previous and current frames, [1] and [2] have moved from standing side by side to facing each other and kissing, indicating a change from a neutral to an intimate interaction. [3] continues to display a supportive gesture by clapping, suggesting this action started in the previous frame and continued into the current one. The position of [4] indicates movement from a neutral position to a congratulatory gesture, seen by the positioning of the arm and hand. The overall increase in physical interaction between [1] and [2] and the supportive gestures by [3] and [4] contribute to a more emotionally engaging scene in the current frame. |\n| <img src=\"https://arxiv.org/html/2412.03565/12.png\" width=\"124\" height=\"82\"> timestamp &lt; 12 &gt; | 1: Adult wearing a light grey suit with a white shirt. Short dark hair, clean-shaven, and standing upright. 2: Adult in a white, sleeveless dress. Long dark hair pulled back. Appears to be smiling with eyes partially closed. 3: Adult in a dark floral dress with a sleeveless design. Long dark hair down and clapping. | In the current frame, [1] and [2] stand close together in the center of the image. [1] is wearing a grey suit with a white shirt and appears to be speaking or smiling. [2], dressed in a white dress, is leaning slightly towards [1] with a content expression. [3] is on the right, wearing a dark floral dress and clapping, seemingly celebrating with [1] and [2]. The environment is indoors with a wooden wall and a large framed picture in the background. The overall mood is celebratory, suggesting an event or occasion has taken place. | Compared to the previous frame, [1] and [2] were previously kissing, but now they are standing apart, with [2] leaning slightly towards [1]. [1] has shifted from facing [2] to facing slightly outward and appears to be speaking or smiling. [3] remains in the same position but continues clapping, indicating ongoing celebration. The celebratory mood persists, reflecting a continuation of the event captured in the previous frame. |", "caption": "Table 9: Inst-IT\u00a0Dataset Frame-level Annotation, Part II (frame 6-8). Please zoom in to view the instance ID labels.", "description": "This table presents a detailed, frame-by-frame annotation of a video segment (frames 6-8) from the INST-IT dataset.  Each frame's annotation includes instance-level captions describing the appearance and actions of individual objects, image-level captions providing a comprehensive overview of the entire scene, and temporal difference descriptions highlighting changes between consecutive frames. The table is designed to illustrate the rich and nuanced annotations characteristic of the INST-IT dataset, which focuses on instance-level understanding of both image and video data.", "section": "3.1 Instance-centric Annotation Pipeline"}, {"content": "| Question | Answer |\n|---|---| \n| What change occurs with [1]\u2019s expression between &lt;10&gt; and the previous frame? | [1] changes from smiling to a neutral expression. |\n| What activity are [1] and [2] involved in at &lt;11&gt;? | [1] and [2] are engaged in a kiss. |\n| What is the overall mood during &lt;11&gt; as suggested by [3]\u2019s actions? | A celebratory or joyous event. |\n| What interaction occurs between [1] and [2] at &lt;5&gt;? | [1] holds [2]\u2019s hand, suggesting an intimate gesture or exchange, likely a ring. |\n| Who joins [1] and [2] in the frame at &lt;7&gt;? | [4] appears in the frame, joining [1] and [2]. |\n| What changes in the group\u2019s composition between &lt;7&gt; and &lt;8&gt;? | [3] reappears, and [4] is no longer visible. |\n| What common setting element is seen throughout the frames &lt;1&gt; to &lt;12&gt;? | The scene is in an indoor setting with wooden paneling and framed art. |\n| What type of event is likely taking place based on the atmosphere in &lt;4&gt; and &lt;6&gt;? | A formal event, possibly a wedding or official gathering. |\n| What new elements are introduced in the scene at &lt;2&gt;? | [5] holds a cellphone in the background, partially visible. |\n| What is the mood and lighting like at &lt;6&gt;? | The mood is formal and celebratory, with bright lighting enhancing this atmosphere. |\n| What new background element appears at &lt;7&gt;? | There is a map or blueprint on the wall. |\n| What is notable about [5]\u2019s actions at &lt;3&gt;? | [5] is lifting an object above their head, possibly a piece of paper. |\n| What is the setting like in &lt;3&gt;? | The group is gathered near a wooden conference table in a formal setting. |\n| How are [1] and [2] interacting at &lt;8&gt;? | They are engaged in conversation or communication, indicated by body language and focus. |\n| What does [1]\u2019s expression suggest at &lt;12&gt;? | [1] speaks or smiles, suggesting engagement with [2] or others. |\n| What shift occurs in the focus of the camera between &lt;5&gt; and &lt;6&gt;? | The camera focuses more on individuals standing together, reducing focus on the foreground objects. |\n| What are [3] and [4] doing at &lt;9&gt;? | They clapping their hands in celebration. |\n| What decorative element is visible at &lt;2&gt;? | A bouquet of flowers lies on the table near [2]. |\n| How has the posture of [1] and [2] changed by &lt;6&gt;? | [1] and [2] face slightly outward, suggesting a pose for a photograph or audience. |\n| What overall physical change occurs between [1] and [2] from &lt;10&gt; to &lt;11&gt;? | There\u2019s a noticeable increase in their physical interaction, enhancing emotional engagement. |", "caption": "Table 10: Inst-IT\u00a0Dataset Frame-level Annotation, Part III (frame 9-12). Please zoom in to view the instance ID labels.", "description": "This table presents a detailed breakdown of frame-level annotations for frames 9 through 12 of a video in the INST-IT dataset.  Each frame's annotation includes instance-level captions (descriptions of individual objects, including their attributes and actions), image-level captions (a holistic description of the entire scene, including object interactions and environmental details), and temporal difference descriptions (capturing changes between consecutive frames). This level of detail highlights the dataset's emphasis on instance-level understanding and provides a rich source of data for training and evaluating multimodal models.", "section": "3.1 Instance-centric Annotation Pipeline"}]