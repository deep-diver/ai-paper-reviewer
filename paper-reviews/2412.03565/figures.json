[{"figure_path": "https://arxiv.org/html/2412.03565/x1.png", "caption": "Figure 1: Current LMMs struggle with instance-level understanding, failing to capture the nuanced details about specific instances.\nTo address this, we create a large-scale instance-specific instruction tuning dataset and train a multimodal model on it. Compared to existing models, our model show better performance in instance-level understanding.", "description": "The figure illustrates the limitations of current Large Multimodal Models (LMMs) in understanding visual content at the instance level.  Existing LMMs fail to capture the detailed, nuanced information about specific elements within an image or video.  To address this weakness, the authors created a large-scale dataset specifically focused on instance-level understanding, training a multimodal model on this data.  The results demonstrate that their model significantly outperforms existing models in the accuracy and detail of instance-level comprehension.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2412.03565/x2.png", "caption": "Figure 2: The automated data generation pipeline.\nWe process the video frames sequentially. At each timestamp t\ud835\udc61titalic_t, GPT-4o is prompted to create a frame-level annotation Ytfsuperscriptsubscript\ud835\udc4c\ud835\udc61\ud835\udc53Y_{t}^{f}italic_Y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_f end_POSTSUPERSCRIPT based on the current frame Xtsubscript\ud835\udc4b\ud835\udc61X_{t}italic_X start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT and the previous frame Xt\u2062-\u20621subscript\ud835\udc4b\ud835\udc61-1X_{t\\text{-}1}italic_X start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT. Then, all the frame-level annotations are aggregated to produce a video-level description Yv\u2062i\u2062dsuperscript\ud835\udc4c\ud835\udc63\ud835\udc56\ud835\udc51Y^{vid}italic_Y start_POSTSUPERSCRIPT italic_v italic_i italic_d end_POSTSUPERSCRIPT and a set of open-ended question-answer pairs Yq\u2062asuperscript\ud835\udc4c\ud835\udc5e\ud835\udc4eY^{qa}italic_Y start_POSTSUPERSCRIPT italic_q italic_a end_POSTSUPERSCRIPT.", "description": "This figure illustrates the automated data annotation pipeline used in the paper.  The pipeline processes video frames sequentially. For each frame, GPT-4 is given the current frame and the preceding frame as input. Based on this input, GPT-4 generates a frame-level annotation that includes instance-level captions, an image-level caption, and descriptions of temporal changes. These frame-level annotations are then aggregated to create a video-level summary and a set of open-ended question-answer pairs. This pipeline is key to creating the dataset used for training the model.", "section": "3.1 Instance-centric Annotation Pipeline"}, {"figure_path": "https://arxiv.org/html/2412.03565/x3.png", "caption": "Figure 3: Visualization of an data example from our Inst-IT\u00a0Dataset.\nFor each video, we provide (a) frame-level annotations, (b) a video-level description, and (c) open-ended question-answer pairs. Each frame-level annotation consists of three parts: captions for individual instances, a caption for the entire scene, and captions describing the temporal changes involving specific instances. A key feature of our dataset is its emphasis on instances of interest, including their state in each frame, how they change between frames, and questions and answers focused on their specific details throughout the video. The contours of instances in this example are deliberately highlighted for better visualization. A complete data example can be found in\u00a0Sec.\u00a0C.2.", "description": "This figure visualizes a sample from the INST-IT dataset, showcasing its multi-level annotation approach.  It highlights three key aspects: (a) Frame-level annotations, providing individual instance captions, a holistic scene caption, and a description of changes between frames; (b) A comprehensive video-level description summarizing the entire video; (c) Open-ended question-answer pairs focusing on specific instances and their interactions.  The dataset emphasizes individual \"instances of interest,\" meticulously detailing their state in each frame, their changes over time, and generating questions specifically about those details.  Instance boundaries are highlighted for clarity.", "section": "3. INST-IT"}, {"figure_path": "https://arxiv.org/html/2412.03565/x4.png", "caption": "Figure 4: Set-of-Marks visual prompting on the original videos. Each instance is assigned a unique numeric ID, which remains consistent across all frames.", "description": "The figure showcases the application of Set-of-Marks (SoM) visual prompting, a technique used to enhance instance-level understanding in videos.  In the original video frames, each individual instance (object or person of interest) is overlaid with a unique numerical ID. This ID remains consistent throughout the video sequence, allowing for easy tracking and identification of specific instances across different frames. This method simplifies the annotation process and enables the model to better understand individual instances and their interactions within a video.", "section": "3.1 Instance-centric Annotation Pipeline"}, {"figure_path": "https://arxiv.org/html/2412.03565/x5.png", "caption": "Figure 5: GPT-4o-based open-ended question answering correctness assessment.\nThe underlined parts in the figure are included only when evaluating the video split, while the italicized parts will be replaced by the actual sample for scoring.", "description": "Figure 5 illustrates the process of evaluating the correctness of open-ended question answering using GPT-4.  The evaluator is provided with a question, the ground truth answer, and a response from a model. The evaluator then scores the response on a scale of 0 to 1, with 1 representing complete correctness.  For video evaluations, additional contextual information (underlined in the figure) is provided, such as time-related details.  Placeholders (italicized in the figure) are used to insert the actual question, ground truth, and model answer during evaluation.", "section": "3.1 Instance-centric Annotation Pipeline"}, {"figure_path": "https://arxiv.org/html/2412.03565/x6.png", "caption": "Figure 6: Frame-level annotation task prompt, the italicized part are placeholders for the actual inputs.", "description": "This figure shows the prompt template used to instruct GPT-4 to generate frame-level annotations for images and videos.  The prompt guides GPT-4 on how to describe each marked object's visual attributes (color, shape, size, text) and the overall scene (objects, actions, background, etc.). Additionally, it specifies requirements for describing changes between consecutive frames (object movements, interactions, environmental shifts, and inferred causes). The italicized sections within the prompt are placeholders for the actual image/video data fed to the model.", "section": "3.1 Instance-centric Annotation Pipeline"}, {"figure_path": "https://arxiv.org/html/2412.03565/x7.png", "caption": "Figure 7: Video-level annotation task prompt, the italicized part are placeholders for the actual inputs.", "description": "This figure shows the prompt used for instructing GPT-40 to generate video-level annotations.  The prompt guides GPT-40 to aggregate frame-level descriptions into a coherent summary of the entire video while adhering to specific guidelines, including using chronological order, object IDs, and proper timestamp formatting.  The prompt ensures the annotation accurately reflects the video content without speculation or inference.  Placeholders within the prompt are denoted by italicized text to indicate where actual video data is inputted for annotation.", "section": "3.1 Instance-centric Annotation Pipeline"}, {"figure_path": "https://arxiv.org/html/2412.03565/x8.png", "caption": "Figure 8: Open-ended question-answer pairs generation task prompt, the italicized part are placeholders for the actual inputs.", "description": "This figure shows the prompt used to instruct GPT-4 to generate open-ended question-answer pairs from video frame descriptions.  The prompt provides guidelines for question quality and diversity, specifying the format for referencing objects and timestamps within the video.  It also includes instructions on structuring the output as a list of question-answer pairs. The italicized sections are placeholders for the actual video data inputted to the model during the data annotation process.  This detailed prompt is crucial for ensuring the quality and consistency of the generated questions and answers used in the INST-IT dataset.", "section": "3.1 Instance-centric Annotation Pipeline"}, {"figure_path": "https://arxiv.org/html/2412.03565/x9.png", "caption": "Figure 9: A data example from Inst-IT\u00a0Bench. Each test sample includes both open-ended QA and multiple-choice QA, focusing on specific instances or the relationships and interactions between instances.", "description": "Figure 9 shows an example from the INST-IT Bench benchmark dataset, which evaluates multimodal models' instance-level understanding. Each test sample includes both open-ended and multiple-choice question-answer pairs. The questions are designed to assess understanding of individual instances within an image or video, as well as the relationships and interactions between them. This detailed approach contrasts with benchmarks that primarily evaluate holistic image or video comprehension.  The figure visually illustrates this by showing the example questions and answers.", "section": "3.2 INST-IT Bench"}]