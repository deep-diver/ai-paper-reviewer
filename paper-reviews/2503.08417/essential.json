{"importance": "This paper introduces a novel motion in-betweening technique applicable to diverse characters without extensive data. It overcomes data scarcity issues, offering new avenues for 3D character animation and video generation research by **reducing reliance on manual processes** and enabling more creativity.", "summary": "AnyMoLe: Generate character motion in-between frames for diverse characters by video diffusion models without external data. Code: project page.", "takeaways": ["AnyMoLe reduces data dependency in motion in-betweening by leveraging video diffusion models.", "ICAdapt bridges the gap between real-world videos and rendered scenes, improving the quality of generated motions.", "A novel motion-video mimicking optimization technique enables seamless motion generation for characters with arbitrary joint structures."], "tldr": "Motion in-betweening, crucial for animation, traditionally demands character-specific datasets. Existing methods fall short when data is scarce. This work addresses this limitation by introducing AnyMoLe, a method leveraging video diffusion models. AnyMoLe generates motion in-between frames for diverse characters without external data, overcoming the dependence on extensive character-specific datasets. \n\nAnyMoLe employs a two-stage frame generation to enhance contextual understanding and introduces ICAdapt, a fine-tuning technique to bridge the gap between real-world and rendered character animations. Additionally, a \"motion-video mimicking\" optimization enables seamless motion generation for characters with arbitrary joint structures using 2D and 3D-aware features. This drastically **reduces data dependency while maintaining realistic transitions**.", "affiliation": "KAIST, Visual Media Lab", "categories": {"main_category": "Computer Vision", "sub_category": "Video Understanding"}, "podcast_path": "2503.08417/podcast.wav"}