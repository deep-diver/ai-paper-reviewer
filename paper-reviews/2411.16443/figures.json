[{"figure_path": "https://arxiv.org/html/2411.16443/x2.png", "caption": "Figure 1: SplatFlow for 3D Gaussian Splatting synthesis and its training-free applications. (a) Examples of direct 3D Gaussian Splatting (3DGS) generation only from text prompts, (b) Training-free applications, including 3DGS object editing, camera pose estimation, and novel view synthesis. SplatFlow seamlessly integrates these capabilities, showcasing its versatility in generating and editing complex 3D content.", "description": "This figure showcases the capabilities of SplatFlow, a novel framework for 3D Gaussian Splatting (3DGS) synthesis.  Panel (a) demonstrates the direct generation of high-fidelity 3D scenes solely from text prompts, highlighting SplatFlow's ability to create diverse and complex 3D content from simple textual descriptions.  Panel (b) illustrates several training-free applications enabled by SplatFlow, including object editing (replacing objects within a scene), camera pose estimation (determining camera position and orientation), and novel view synthesis (generating realistic views from unseen angles). These applications are seamlessly integrated within the SplatFlow framework, underlining its efficiency and versatility for both content creation and manipulation.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2411.16443/x3.png", "caption": "Figure 2: Overview of SplatFlow. SplatFlow consists of two main components: a multi-view Rectified Flow (RF) model and a Gaussian Splat Decoder (GSDecoder). Conditioned on text prompts, the RF model generates multi-view latents\u2014including image, depth, and Pl\u00fccker ray coordinates. After an optimization process to estimate camera poses, the GSDecoder decodes these latents into pixel-aligned 3DGS.", "description": "SplatFlow, a novel 3D scene generation model, is composed of two main components: a multi-view Rectified Flow (RF) model and a Gaussian Splat Decoder (GSDecoder). The RF model processes text prompts to generate multi-view latent representations, including images, depth maps, and Pl\u00fccker ray coordinates.  These latent representations are then used by the GSDecoder to generate pixel-aligned 3D Gaussian Splatting (3DGS) data.  Camera poses are estimated through an optimization process, ensuring accurate reconstruction of the 3D scene.", "section": "4. Methodology"}, {"figure_path": "https://arxiv.org/html/2411.16443/extracted/6023046/figure/asset/gsdecoder_qual/wdlatent/novel_view_1.png", "caption": "Figure 3: Qualitative results in text-to-3DGS generation on MVImgNet and DL3DV validation sets. The first two rows are rendered scenes from the MVImgNet dataset, while the last two rows are from the DL3DV dataset. Our SplatFlow produces cohesive and realistic scenes with sharp details, accurately capturing the intricacies of real-world environments and accommodating diverse camera trajectories.", "description": "This figure showcases qualitative results of text-to-3DGS generation using the SplatFlow model.  The top two rows display example scenes generated from the MVImgNet dataset; the bottom two rows show scenes generated from the DL3DV dataset.  The results highlight SplatFlow's ability to create realistic, detailed 3D scenes from text prompts, showcasing the model's capacity to handle complex real-world scenes with varied camera angles and perspectives.", "section": "5. Experimental Results"}, {"figure_path": "https://arxiv.org/html/2411.16443/extracted/6023046/figure/asset/gsdecoder_qual/wdlatent/novel_view_1.png", "caption": "Figure 4: Qualitative results in 3D editing with MVInpainter\u00a0[4] and DGE\u00a0[9]. We show rendered scenes except for MVInpainter.", "description": "This figure showcases a comparison of 3D object editing results using three different methods: SplatFlow, MVInpainter, and DGE.  The images demonstrate the ability of each method to replace objects within 3D scenes according to textual prompts.  SplatFlow's results are presented as rendered 3D scenes, highlighting the realistic replacement of the objects and preservation of scene context.  In contrast, MVInpainter's results are not directly shown (only mentioned in caption), and DGE's results are also shown as rendered 3D scenes, but potentially with less fidelity or realistic integration than the results achieved by SplatFlow. The comparison highlights the effectiveness of SplatFlow in performing precise 3D editing tasks compared to existing methods.", "section": "5.3. Result on 3DGS Editing"}, {"figure_path": "https://arxiv.org/html/2411.16443/extracted/6023046/figure/asset/gsdecoder_qual/wogan/novel_view_1.png", "caption": "Figure 5: Qualitative results for camera pose estimation. Camera poses are estimated from multi-view images. Image border colors match each camera, with black cameras indicating GT poses.", "description": "This figure showcases the qualitative results of camera pose estimation. Multiple input images are used to estimate camera poses, which are then compared to ground truth poses. The image borders are color-coded to match each camera, making it easy to identify which poses are estimates and which are ground truth. Black borders denote the ground truth poses. This visualization helps assess the accuracy and precision of the camera pose estimation model.", "section": "5. Experimental Results"}, {"figure_path": "https://arxiv.org/html/2411.16443/extracted/6023046/figure/asset/gsdecoder_qual/wgan/novel_view_1.png", "caption": "Figure 6: Qualitative results for novel view synthesis. Novel view synthesis is performed from the red-box images and depths.", "description": "This figure displays the results of novel view synthesis, a key task achieved by the SplatFlow model.  The red boxes highlight the input views (images and corresponding depth maps) used to generate the novel views. The figure showcases the model's ability to generate realistic and high-quality images from new perspectives, demonstrating the effectiveness of the multi-view rectified flow model in capturing and representing 3D scene structure.", "section": "5. Experimental Results"}, {"figure_path": "https://arxiv.org/html/2411.16443/extracted/6023046/figure/asset/gsdecoder_qual/gt_view_1.png", "caption": "(a) w/o Depth \n(200K iter)", "description": "This figure shows an ablation study on the design choices of the GSDecoder, focusing on the impact of incorporating depth latents and the vision-aided GAN loss. The image on the left represents the results when training the GSDecoder without depth latents for 200k iterations; the results in the middle are for the GSDecoder trained with depth latents for 200k iterations. The two images on the right show the results for 400k iterations, with and without the Vision-aided GAN loss, respectively.", "section": "C.1 Ablation on GSDecoder Design Choice"}, {"figure_path": "https://arxiv.org/html/2411.16443/extracted/6023046/figure/asset/gsdecoder_qual/wdlatent/output_image.jpg", "caption": "(b) w/ Depth \n(200K iter)", "description": "This figure shows the results of training a Gaussian Splatting Decoder (GSDecoder) with depth information for 200K iterations.  It is part of an ablation study evaluating the impact of adding depth latents and a vision-aided GAN loss on the quality of 3D Gaussian Splatting generation. The image likely showcases generated 3D scenes, potentially highlighting visual improvements achieved by including depth information during training.", "section": "C.1. Ablation on GSDecoder Design Choice"}, {"figure_path": "https://arxiv.org/html/2411.16443/extracted/6023046/figure/asset/gsdecoder_qual/wdlatent/output_image.jpg", "caption": "(c) w/o GAN Loss \n(400K iter)", "description": "This figure shows the qualitative results of an ablation study on the GSDecoder design choices. Specifically, it presents a comparison of 3D Gaussian Splatting (3DGS) generated images with and without a vision-aided GAN loss, trained for 400K iterations. The comparison aims to highlight the impact of the vision-aided GAN loss on the quality of generated 3D scenes. The results are shown as zoomed-in details to better visualize the differences in the generated images.", "section": "C.1 Ablation on GSDecoder Design Choice"}, {"figure_path": "https://arxiv.org/html/2411.16443/extracted/6023046/figure/asset/gsdecoder_qual/wogan/output_image.jpg", "caption": "(d) w/ GAN Loss\n(400K iter)", "description": "This figure shows the qualitative results of an ablation study on the GSDecoder design choices. Specifically, it compares the visual quality of 3D Gaussian Splatting (3DGS) generated by four different GSDecoder variants: 1) without depth latents and trained for 200K iterations; 2) with depth latents and trained for 200K iterations; 3) with depth latents and trained for 400K iterations; and 4) with depth latents, trained for 400K iterations, and incorporating the vision-aided GAN loss. The zoomed-in images highlight the improvements in detail and realism achieved by including depth latents and vision-aided GAN loss. ", "section": "C.1. Ablation on GSDecoder Design Choice"}, {"figure_path": "https://arxiv.org/html/2411.16443/extracted/6023046/figure/asset/gsdecoder_qual/wgan/output_image.jpg", "caption": "(e) Target\nView", "description": "This figure shows the target view used as the ground truth for evaluating the quality of the generated 3D Gaussian Splatting (3DGS) scenes. This target view serves as a reference image against which the generated views are compared to assess the accuracy and realism of the 3DGS reconstruction.  The figure specifically visualizes a high-quality, real-world image of a scene, providing a clear benchmark for the model's performance.", "section": "C. Additional Experimental Results"}, {"figure_path": "https://arxiv.org/html/2411.16443/extracted/6023046/figure/asset/gsdecoder_qual/output_image.jpg", "caption": "(f) Zoom:\nw/o Depth\n(200K iter)", "description": "This figure shows a zoomed-in comparison of 3D Gaussian splatting (3DGS) generation results from four different GSDecoder configurations.  It demonstrates the impact of incorporating depth latents and the vision-aided GAN loss on the quality of the generated 3D scenes. The four configurations compared are: (1) without depth latents trained for 200K iterations, (2) with depth latents trained for 200K iterations, (3) without the vision-aided GAN loss trained for 400K iterations (with depth latents), and (4) with the vision-aided GAN loss trained for 400K iterations (with depth latents). The zoomed-in views highlight the improved detail, realism, and overall quality achieved by adding depth latents and the vision-aided GAN loss.", "section": "C.1. Ablation on GSDecoder Design Choice"}]