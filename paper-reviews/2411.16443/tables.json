[{"content": "| Description | DreamScene [46] | LucidDreamer [15] | Director3D [49] | Ours |\n|---|---|---|---|---|\n| Stack of fries on a checkered surface |  |  |  |  |\n|  |  |  |  |  |\n| Broom with straw bristles, leaning against a sign. |  |  |  |  |\n|  |  |  |  |  |\n| A mannequin dressed in a red gown with yellow accents stands amidst an array of vibrant flowers and lush greenery. |  |  |  |  |\n|  |  |  |  |  |\n| An overgrown and neglected area with a large bush or small tree covered in ivy, a wooden fence, and a paved path. |  |  |  |  |\n|  |  |  |  |  |\n| Text | DreamScene [46] | LucidDreamer [15] | Director3D [49] | Ours |", "caption": "Table 1: Quantitative results in text-to-3DGS generation on the MVImgNet and DL3DV datasets. We compared our SplatFlow with and without the SDS++\u00a0[49], against Director3D\u00a0[49].", "description": "This table presents a quantitative comparison of different methods for generating 3D Gaussian Splatting (3DGS) scenes from text descriptions, using the FID and CLIP scores to evaluate the quality and text-image alignment.  The methods compared are Director3D [49] (a state-of-the-art approach), and the authors' proposed SplatFlow method, both with and without the additional SDS++ [49] refinement technique. The evaluation uses two datasets, MVImgNet and DL3DV, showing the performance of each method on real-world scenes with various scales and camera trajectories.", "section": "5. Experimental Results"}, {"content": "| Method | MVImgNet [117] |  | DL3DV [54] |  | \n|---|---|---|---|---|---| \n|  | FID-10K \u2193 | CLIPScore \u2191 | FID-2.4K \u2193 | CLIPScore \u2191 |  | \n|---|---|---|---|---|---| \n| Director3D [49] | 39.55 | 30.48 | 88.44 | 30.04 |  | \n| Director3D (w/ SDS++) [49] | 41.80 | 31.00 | 95.88 | 31.68 |  | \n| **SplatFlow** | **34.85** | 31.43 | **79.91** | 30.06 |  | \n| **SplatFlow (w/ SDS++)** | 35.46 | **32.30** | 85.31 | **31.90** |  | ", "caption": "Table 2: 3D object replacement.", "description": "This table presents a quantitative comparison of different methods for 3D object replacement.  It shows the CLIP score (a measure of how well the generated image aligns with the text prompt) and the CLIP D-sim (a measure of directional similarity between the original and edited images) for several methods, including the proposed SplatFlow with and without the SDS++ refinement technique.  Higher scores indicate better performance. The results demonstrate the effectiveness of SplatFlow in achieving high-quality 3D object replacement compared to existing approaches.", "section": "5.3. Result on 3DGS Editing"}, {"content": "| Method | CLIPScore \u2191 | CLIP D-sim \u2191 |\n|---|---|---|\n| DGE [9] | 27.43 | 0.102 |\n| SplatFlow | 28.47 | 0.169 |\n| **+) SDS++** | **31.30** | **0.224** |", "caption": "Table 3: Results in camera pose estimation on MVImgNet validation set. @Q\ud835\udc44Qitalic_Q represents the accuracy threshold for rotations (degrees) and camera centers (units).", "description": "This table presents a quantitative evaluation of camera pose estimation performance on the MVImgNet validation set.  It compares the accuracy of different methods in estimating camera rotation and center position.  The accuracy is measured using thresholds (@Q) for both rotation (in degrees) and camera center position (in units). Higher values generally indicate better performance.  Specifically, the table shows the percentage of estimated poses falling within various ranges of accuracy for rotations and translation (camera center).", "section": "5. Experimental Results"}, {"content": "| Method | Rotation\u2191 @5 | Rotation\u2191 @10 | Rotation\u2191 @15 | Camera Center\u2191 @0.05 | Camera Center\u2191 @0.1 | Camera Center\u2191 @0.2 |\n|---|---|---|---|---|---|---|\n| RelPose++ [51] | 19.4 | 37.7 | 51.4 | 0.6 | 12.5 | 55.0 |\n| Ray Regression [120] | 10.4 | 25.6 | 50.1 | 15.3 | 47.9 | 82.9 |\n| Ray Diffusion [120] | 17.5 | 38.7 | 59.6 | 24.1 | 60.9 | 87.6 |\n| **SplatFlow (w/ depth)** | **26.8** | **52.6** | **59.3** | **62.3** | **91.6** | **99.4** |\n| **SplatFlow (w/o depth)** | **28.8** | **54.5** | **63.9** | **64.9** | **94.0** | **99.7** |", "caption": "Table 4: Novel view synthesis results on MVImgNet. We use N\ud835\udc41Nitalic_N input views to synthesize K\u2212N\ud835\udc3e\ud835\udc41K-Nitalic_K - italic_N novel views, with uniformly sampled views for interpolation and central views for extrapolation.", "description": "This table presents the quantitative results of novel view synthesis experiments conducted on the MVImgNet dataset.  The method evaluates the model's ability to generate new viewpoints of a scene given a subset of existing views.  The experiment is divided into two categories: interpolation and extrapolation.  Interpolation involves generating new views from uniformly sampled input views. Extrapolation involves generating new views from views centrally located in the scene. The results are evaluated using metrics such as Peak Signal-to-Noise Ratio (PSNR), Structural SIMilarity index (SSIM), Learned Perceptual Image Patch Similarity (LPIPS), and Absolute Relative error (AbsRel) to assess image quality and accuracy.  The number of input views used (N) and the number of novel views synthesized (K-N) are also specified in the table.", "section": "5. Experimental Results"}, {"content": "| Input Images | Camera Poses |\n|---|---|", "caption": "Table 5: Ablation study on GSDecoder design choices. Evaluations are performed using PSNR, LPIPS, SSIM, and FID, highlighting the impact of incorporating depth latents and vision-aided GAN loss in improving 3DGS quality.", "description": "This table presents the results of an ablation study conducted on the GSDecoder, a key component of the SplatFlow model. The study investigates the impact of two design choices: the incorporation of depth latents and the use of a vision-aided GAN loss.  Four different variants of the GSDecoder were evaluated, each differing in the presence or absence of these design choices. The performance of each variant was assessed using four metrics: Peak Signal-to-Noise Ratio (PSNR), Learned Perceptual Image Patch Similarity (LPIPS), Structural Similarity Index (SSIM), and Fr\u00e9chet Inception Distance (FID).  The results illustrate the relative contribution of depth latents and the vision-aided GAN loss in improving the overall quality of 3D Gaussian Splatting (3DGS) generation.  Higher PSNR, SSIM, and lower LPIPS and FID scores indicate better image quality and alignment with the target view.", "section": "A.1. Gaussian Splatting Decoder (GSDecoder)"}, {"content": "| Type | RGB PSNR \u2191 | RGB SSIM \u2191 | RGB LPIPS \u2193 | Depth AbsRel \u2193 | Depth \u03b4\u2081 \u2191 |\n|---|---|---|---|---|---| \n| Interpolation (N=2) | 14.73 | 0.571 | 0.648 | 0.588 | 0.731 |\n| Interpolation (N=4) | 17.05 | 0.590 | 0.551 | 0.498 | 0.761 |\n| Interpolation (N=6) | 18.82 | 0.626 | 0.483 | 0.415 | 0.775 |\n| Extrapolation (N=2) | 15.15 | 0.577 | 0.627 | 0.771 | 0.715 |\n| Extrapolation (N=4) | 16.80 | 0.595 | 0.554 | 0.679 | 0.727 |\n| Extrapolation (N=6) | 17.96 | 0.613 | 0.503 | 0.602 | 0.747 |", "caption": "Table 6: Impact of the Stop-Ray modification. Evaluations are conducted using FID-10K and CLIPScore metrics to assess the effectiveness of stopping camera ray updates at different timesteps in the sampling process.", "description": "This table presents an ablation study on the effect of early stopping during the camera pose update process within the SplatFlow model's sampling process.  Different stopping timesteps (\"tstop\") are tested: 100, 50, 0 (no early stopping), and the default 150. The results show the Fr\u00e9chet Inception Distance (FID-10K) and CLIPScore metrics for each configuration. The metrics assess the quality of the generated images and how well they align with the given textual prompts.  This analysis helps determine the optimal time to halt camera pose updates in order to balance image quality and efficiency during generation.", "section": "C.2. Ablation on Sampling Process"}, {"content": "|---|---|---|---|---|---|---|---| \n|  |  |  |  |  |  |  |  | \n|  |  |  |  |  |  |  |  | \n|  |  |  |  |  |  |  |  | \n|  |  |  |  |  |  |  |  | \n|  |  |  |  |  |  |  |  | \n|  |  |  |  |  |  |  |  |", "caption": "Table 7: Impact of Stable Diffusion 3 Guidance. The table compares the FID-10K and CLIPScore metrics for SplatFlow with and without SD3 guidance.", "description": "This table presents a quantitative comparison of the performance of the SplatFlow model with and without the use of Stable Diffusion 3 (SD3) guidance.  It assesses the impact of integrating SD3 guidance on the model's ability to generate 3D scenes from text prompts by comparing two key metrics: Fr\u00e9chet Inception Distance (FID-10K), measuring the quality of generated images, and CLIPScore, evaluating how well the generated images align with the input text.  Lower FID-10K scores indicate better image quality, while higher CLIPScores reflect stronger alignment between images and text descriptions. The results reveal the contribution of SD3 guidance to the overall quality and alignment of the generated scenes. ", "section": "C.2. Ablation on Sampling Process"}, {"content": "| Method | PSNR \u2191 | LPIPS \u2193 | SSIM \u2191 | FID-50K \u2193 |\n|---|---|---|---|---|\n| w/o Depth Latent (200K Iterations) | 25.64 | 0.2507 | 0.7993 | 16.29 |\n| w/ Depth Latent (200K Iterations) | 26.19 | 0.2260 | 0.8169 | 11.92 |\n| w/ Depth Latent (400K Iterations) | 26.68 | 0.2129 | 0.8251 | 8.80 |\n| + Vision-Aided GAN Loss | **26.84** | **0.2048** | **0.8256** | **5.81** |", "caption": "Table 8: Quantitative results in Single-Object-with-Surrounding set of T3Bench\u00a0[26]. For the CLIPScore, we report our reproduced score due to an error in the measurement of Director3D\u00a0[49].", "description": "This table presents a quantitative comparison of different 3D scene generation methods on the 'Single-Object-with-Surrounding' subset of the T3Bench benchmark [26].  The metrics used are BRISQUE, NIQE, and CLIP score.  The CLIP score for Director3D [49] was reproduced due to an error in the original measurement reported in the paper.", "section": "5.2. Results on Text-to-3DGS Generation"}]