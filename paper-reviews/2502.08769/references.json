{"references": [{"fullname_first_author": "Mathilde Caron", "paper_title": "Emerging properties in self-supervised vision transformers", "publication_date": "2021-10-27", "reason": "This paper introduces DINO, a self-supervised learning method that uses a teacher-student framework and online clustering, which heavily inspired the methodology in the current paper."}, {"fullname_first_author": "Mahmoud Assran", "paper_title": "Self-supervised learning from images with a joint-embedding predictive architecture", "publication_date": "2023-06-01", "reason": "This paper introduces I-JEPA, a masked image modeling method using a split architecture and online latent reconstruction, which is compared extensively against in the current paper."}, {"fullname_first_author": "Kaiming He", "paper_title": "Masked autoencoders are scalable vision learners", "publication_date": "2022-06-01", "reason": "This paper introduces MAE, a widely influential masked image modeling method, serving as a major baseline and comparison point for the current work."}, {"fullname_first_author": "Alexei Baevski", "paper_title": "data2vec: A general framework for self-supervised learning in speech, vision and language", "publication_date": "2022-07-11", "reason": "This paper introduces data2vec, another significant self-supervised learning framework that influenced the design choices and comparisons made in the current research."}, {"fullname_first_author": "Hangbo Bao", "paper_title": "BEiT: BERT pre-training of image transformers", "publication_date": "2021-06-01", "reason": "This paper introduces BEiT, an early masked image modeling approach using a transformer architecture, which forms a key part of the historical background and comparison set for the current study."}]}