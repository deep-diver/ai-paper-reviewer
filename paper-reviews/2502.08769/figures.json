[{"figure_path": "https://arxiv.org/html/2502.08769/x1.png", "caption": "Figure 1: \nCAPI Method overview:\nimage patches embedded by a teacher are grouped into clusters.\nTheir assignments are then used as the training signal for the student.\nThe teacher and the student are jointly learned via self-distillation.\nThe loss is purely about predicting the content of missing patches and does not rely on augmentations or a contrastive loss.\nEvaluation scores:\nwe evaluate frozen representations on ADE20K segmentation with a k\ud835\udc58kitalic_k-nn and linear probe and on ImageNet-1k classification with an attentive probe.\nWe compare to MAE, data2vec 2.0, I-JEPA, and AIM.\nCompared to other masked image models, CAPI achieves higher performance with fewer FLOP, scaling well with model size, and approaches the scores of DINOv2+reg.", "description": "This figure illustrates the CAPI method, a novel approach to masked image modeling.  Image patches are processed by a teacher network, which then groups them into clusters based on latent representations. These cluster assignments serve as the training signal for a student network, trained via self-distillation with the teacher. Notably, the loss function focuses solely on predicting the content of masked image patches, without relying on augmentations or contrastive learning.  The figure then shows performance comparisons against existing methods (MAE, data2vec 2.0, I-JEPA, and AIM) on ADE20K segmentation and ImageNet-1k classification tasks using linear and attentive probes, respectively.  CAPI demonstrates superior performance with fewer FLOPs and improved scalability compared to other masked image models, approaching the performance of DINOv2+reg.", "section": "3 Approach"}, {"figure_path": "https://arxiv.org/html/2502.08769/x2.png", "caption": "Figure 2: \nOverview of the components of a reconstruction-based model.\nWe identify three main choices involved in designing a masked image model:\nthe choice of targets (fig.\u00a03), the loss function (Section\u00a03.1, fig.\u00a04) and the architecture of the predictor (Section\u00a03.2, fig.\u00a05).", "description": "Figure 2 illustrates the key design choices in building a masked image modeling (MIM) system.  The diagram highlights that three major decisions shape the effectiveness of a reconstruction-based MIM model:  First, selecting the target representation used during training (further detailed in Figure 3). Second, choosing the appropriate loss function to optimize the model (explained in Section 3.1 and Figure 4).  Third, defining the architecture of the predictor network responsible for reconstructing the masked image regions (discussed in Section 3.2 and Figure 5).", "section": "3 Approach"}, {"figure_path": "https://arxiv.org/html/2502.08769/x3.png", "caption": "(a) Pixel targets\n(iGPT, MAE, AIM)", "description": "This figure illustrates different target representations used in masked image modeling (MIM).  Specifically, it highlights the use of pixel-level targets, a common approach in early MIM methods like iGPT, MAE, and AIM. In these methods, the model is trained to directly reconstruct the masked pixel values, making the target representation identical to the raw pixel data.", "section": "3 Approach"}, {"figure_path": "https://arxiv.org/html/2502.08769/x4.png", "caption": "(b) Frozen teacher\n(BeiT,PeCo,EVA)", "description": "This figure shows different target representations used in Masked Image Modeling (MIM).  Specifically, it highlights the approach where a frozen teacher network provides the target representation for the student network to learn from.  The teacher network's weights are fixed, and its output is used as the target for the MIM task. This contrasts with methods that use an online teacher (EMA) or pixel targets for training.", "section": "3 Approach"}, {"figure_path": "https://arxiv.org/html/2502.08769/x5.png", "caption": "(c) EMA teacher\n(iBOT, Data2Vec, I-JEPA, Ours)", "description": "This figure shows the different target representations used in masked image modeling (MIM).  The figure illustrates three approaches to selecting the target representation during training: (a) Pixel targets, (b) Frozen teacher, and (c) EMA (exponential moving average) teacher.  The EMA teacher approach, which is used in CAPI, Data2Vec, I-JEPA, and other methods, updates the teacher representation using an exponential moving average of the student's representation. This method provides a more stable training signal and often leads to better results.", "section": "3 Approach"}, {"figure_path": "https://arxiv.org/html/2502.08769/x6.png", "caption": "Figure 3: \nThe target representations commonly used in MIM.\nWe focus on the EMA representations.", "description": "This figure illustrates different target representation strategies employed in Masked Image Modeling (MIM).  It compares three main approaches: using pixel-level targets (as in iGPT, MAE, and AIM), using the frozen representation of a teacher network (as in BeiT, PeCo, and EVA), and using the Exponential Moving Average (EMA) of a teacher network's representation (as in iBOT, Data2Vec, I-JEPA, and the authors' proposed CAPI method). The figure highlights the authors' focus on EMA-based representations, emphasizing their advantages in MIM.", "section": "3 Approach"}, {"figure_path": "https://arxiv.org/html/2502.08769/x7.png", "caption": "(a) Direct loss\n(MAE, I-JEPA)", "description": "This figure illustrates the direct loss function used in masked image modeling.  The direct loss calculates the difference between the predicted values and target values directly, without any intermediate steps. This type of loss is used in methods like MAE and I-JEPA. The figure likely shows a visual representation of this loss calculation, perhaps illustrating the flow of gradients used during backpropagation. This direct approach contrasts with other loss formulations shown later in the paper that involve clustering or other intermediate steps before comparing predictions to targets.", "section": "3.1 Clustering-based loss formulation"}, {"figure_path": "https://arxiv.org/html/2502.08769/x8.png", "caption": "(b) DINO loss\n(iBOT, DINOv2)", "description": "This figure illustrates the DINO loss function, used in masked image modeling methods such as iBOT and DINOv2.  The DINO loss leverages a teacher-student framework. The teacher network produces embeddings that are used as targets.  The student network tries to predict these targets.  The loss function measures the discrepancy between student predictions and teacher-produced embeddings.  The EMA (Exponential Moving Average) of the teacher's output is used in this process for stability and to improve the learned features.", "section": "3.1 Clustering-based loss formulation"}, {"figure_path": "https://arxiv.org/html/2502.08769/x9.png", "caption": "(c) Clustering\n(proposed)", "description": "This figure illustrates the clustering-based loss function proposed by the authors. Unlike previous methods that rely on cross-entropy between student and teacher output distributions (often using an MLP head), this approach directly uses soft cluster assignments (obtained via online clustering of teacher embeddings) as the target signal.  The gradient flow is depicted in red, highlighting that the loss is solely focused on predicting the correct cluster assignment for masked image patches and not reliant on any additional objectives such as contrastive loss or reconstruction loss. This design choice improves stability during training, a key improvement over prior methods.", "section": "3.1 Clustering-based loss formulation"}, {"figure_path": "https://arxiv.org/html/2502.08769/x10.png", "caption": "Figure 4: \nThe different loss formulations considered here.\nWe depict in red the flow of the gradient.", "description": "This figure compares three different loss formulations used in masked image modeling.  The 'Direct loss' directly computes the loss between the predicted and target representations. The 'DINO loss' uses a cross-entropy loss between the student and teacher's output distributions, which are considered as soft cluster memberships. The 'Clustering' approach employs a clustering-based loss that leverages the prediction of latent clusterings. The red arrows in the diagram illustrate the gradient flow for each method.", "section": "3. Approach"}, {"figure_path": "https://arxiv.org/html/2502.08769/x11.png", "caption": "(a) Fused predictor\n(BeiT, iBOT)", "description": "This figure illustrates the \"fused predictor\" architecture used in some masked image modeling methods.  In this design, a single transformer processes both the visible image patches and the special mask tokens representing missing regions. The model directly predicts pixel values or latent representations from this fused input.  This is in contrast to other designs where a separate predictor processes only the mask tokens, or where reconstruction occurs in a different feature space.  The examples cited here, BeiT and iBOT, demonstrate this architectural approach.", "section": "3 Approach"}, {"figure_path": "https://arxiv.org/html/2502.08769/x12.png", "caption": "(b) Self-att. predictor\n(MAE, I-JEPA)", "description": "This figure shows a self-attention predictor architecture used in masked image modeling.  The architecture consists of an encoder and a predictor, both implemented as transformers.  Unlike a fused architecture (where encoding and prediction happen in a single transformer), this split architecture separates the encoding of visible image patches from the prediction of masked patches. The predictor uses self-attention to focus on the encoded features of the visible patches to predict the content of the missing ones. This approach differs from other architectures like fused predictors which process both visible and masked tokens in one transformer or cross-attention based predictors which use cross-attention to combine the masked tokens with other context.", "section": "3.2 Predictor architecture"}, {"figure_path": "https://arxiv.org/html/2502.08769/x14.png", "caption": "(c) Cross-att. predictor\n(CrossMAE, ours)", "description": "This figure illustrates the cross-attention predictor architecture used in the CAPI model. Unlike fused or self-attention predictor architectures, the cross-attention predictor maintains separate encoder and predictor components. The encoder processes the visible image patches, while the predictor, using cross-attention, predicts the features of the masked patches based on the encoder's output. This design enhances efficiency and stability, as each prediction is independent of other positions, avoiding the need for repeated predictor passes. This architecture is inspired by CrossMAE and distinguishes CAPI from other masked image modeling methods.", "section": "3 Approach"}, {"figure_path": "https://arxiv.org/html/2502.08769/x15.png", "caption": "Figure 5: \nThe different predictor architectures discussed in the paper.\nHere, the boxes each represent a transformer.\nThe black lines represent the residual stream for a token.", "description": "Figure 5 illustrates three different architectures for the predictor component in a masked image modeling system.  Each architecture uses transformers, represented by boxes. The black lines show the residual connections (the path a token takes) between the different transformer layers within each architecture. The three architectures are: (a) A fused predictor, where the encoder and predictor are a single transformer; (b) A self-attention predictor where the encoder and the predictor are separate transformers and the predictor utilizes self-attention; and (c) A cross-attention predictor, also with separate encoder and predictor transformers, where the predictor uses cross-attention to access information from the encoder.", "section": "3 Approach"}, {"figure_path": "https://arxiv.org/html/2502.08769/x16.png", "caption": "Figure 6: \nAdditional ablation experiments.\n(Left) Influence of the number of prototypes.\n(center) Influence of the training length.\nEach point here is an independent training.\n(right) Influence of the training dataset.", "description": "Figure 6 presents the results of ablation studies examining the impact of various parameters on the model's performance. The left panel shows how the number of prototypes used in the clustering-based loss function affects the model's performance on ImageNet and ADE20K. The center panel illustrates the model's performance as a function of training duration. The right panel compares model performance when trained on different datasets: ImageNet-1k, ImageNet-22k, and LVD-142M.", "section": "4.2 Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2502.08769/extracted/6198836/resources/plot_distillation.png", "caption": "Figure 7: \nVisualization of the features of CAPI and baseline models.\nWe apply a PCA to the features and map the three first components to RGB.\nThe features produced by CAPI are discriminative and smooth.", "description": "This figure visualizes the features learned by the CAPI model and several baseline models. Principal Component Analysis (PCA) was used to reduce the dimensionality of the feature maps to three principal components, which were then mapped to the red, green, and blue channels of an RGB image.  The resulting images provide a visual representation of the learned features. The caption highlights that the features produced by CAPI are distinctly different and smoother compared to the baseline models, suggesting superior quality and potentially better performance.", "section": "4.3 Results"}, {"figure_path": "https://arxiv.org/html/2502.08769/extracted/6198836/resources/sinkhorn_standard.png", "caption": "Figure 8: Detailed overview of our method with reference tensor sizes for a CAPI ViT-L/14 model.\nWe denote in red the parts that are trained by the main loss, in purple the parts that are trained with the clustering loss, and in blue the parts that are updated by the EMA.", "description": "Figure 8 provides a detailed visualization of the CAPI model architecture, highlighting the flow of data and the different training mechanisms.  The diagram shows the teacher and student networks (both ViT-L/14), emphasizing the role of the exponential moving average (EMA) in updating the teacher. It also illustrates the clustering process used to generate the training signal (soft assignments) for the student, which predicts the missing image patches.  The color-coding helps distinguish components trained by the main loss (red), the clustering loss (purple), and the EMA updates (blue). Tensor sizes are included for a better understanding of the model's dimensions.", "section": "3 Approach"}, {"figure_path": "https://arxiv.org/html/2502.08769/extracted/6198836/resources/sinkhorn_modified.png", "caption": "Figure 9: The loss curve of our CAPI ViT-L during training.", "description": "This figure shows the training loss curve for the CAPI ViT-L model. The x-axis represents the training iteration, and the y-axis represents the loss value.  The curve demonstrates a smooth downward trend, indicating stable training and the absence of plateaus or significant instability.", "section": "4 Experiments"}, {"figure_path": "https://arxiv.org/html/2502.08769/x17.png", "caption": "Figure 10: Comparative downstream scores of the teacher model and the student model throughout training.", "description": "This figure displays the performance of both the teacher and student models throughout the training process.  The x-axis represents the training iteration, while the y-axis shows the accuracy.  Two sets of curves are presented: top-1 accuracy on ImageNet and mIoU scores on ADE20K, both for the teacher and student models. The purpose is to demonstrate the effectiveness of self-distillation, where the more advanced teacher model guides the student's learning, and to show the stability and improvement over time.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2502.08769/extracted/6198836/pca/241106_1910_stableeveuh_master_lower_lr/1__8_256_1024_24_12_16_4_0.2_xFSDP_SHARD_GRAD_OP_true_500_50000_0.001_0.0005_0.999/eval/training_499999_teacher/pca_visualization/pca_mypics_560px/artifacts/0000022_pca_joint.000.png", "caption": "(a) Standard SK", "description": "This figure shows the pseudo-code for the standard Sinkhorn-Knopp algorithm.  It details the iterative normalization process used to obtain a near-uniform distribution of tokens over clusters. The algorithm takes a tensor M as input and iteratively normalizes it along rows and columns. The normalization is done by dividing each element by the sum of elements in its row and then by the sum of elements in its column. This process is repeated until convergence, producing the output tensor M.", "section": "3.1 Clustering-based loss formulation"}, {"figure_path": "https://arxiv.org/html/2502.08769/extracted/6198836/pca/241106_1910_stableeveuh_master_lower_lr/1__8_256_1024_24_12_16_4_0.2_xFSDP_SHARD_GRAD_OP_true_500_50000_0.001_0.0005_0.999/eval/training_499999_teacher/pca_visualization/pca_mypics_560px/artifacts/0000022_pca_joint.001.png", "caption": "(b) Modified algorithm", "description": "This figure presents the pseudo-code for a modified Sinkhorn-Knopp algorithm.  The original algorithm (shown in (a)) and the modification are compared. The modification involves normalizing by the sum of tokens for every position rather than normalizing across all positions, which addresses positional collapse in masked image modeling.", "section": "3.1 Clustering-based loss formulation"}, {"figure_path": "https://arxiv.org/html/2502.08769/extracted/6198836/pca/241106_1910_stableeveuh_master_lower_lr/1__8_256_1024_24_12_16_4_0.2_xFSDP_SHARD_GRAD_OP_true_500_50000_0.001_0.0005_0.999/eval/training_499999_teacher/pca_visualization/pca_mypics_560px/artifacts/0000022_pca_joint.002.png", "caption": "Figure 11: \nPyTorch pseudo-code for the proposed modified Sinkhorn-Knopp algorithm.\nWe normalize by the sum of the tokens for every given position, instead of normalizing across all positions.", "description": "This figure presents the PyTorch pseudo-code for the modified Sinkhorn-Knopp algorithm used in the paper.  The Sinkhorn-Knopp algorithm is an iterative procedure to find a doubly stochastic matrix. This modification involves normalizing the sum of tokens for each position instead of normalizing across all positions in each iteration, leading to a more stable and efficient computation for the online clustering task. This approach enhances the clustering process and contributes to the overall improved performance of the masked image modeling technique.", "section": "3.1 Clustering-based loss formulation"}, {"figure_path": "https://arxiv.org/html/2502.08769/extracted/6198836/pca/241106_1910_stableeveuh_master_lower_lr/1__8_256_1024_24_12_16_4_0.2_xFSDP_SHARD_GRAD_OP_true_500_50000_0.001_0.0005_0.999/eval/training_499999_teacher/pca_visualization/pca_mypics_560px/artifacts/0000022_pca_joint.003.png", "caption": "Figure 12: Visualization of the features produced by CAPI and other vision models at various resolutions:\nCAPI ViT-L/14,\nDINOv2+reg ViT-g/14 (Darcet et\u00a0al., 2024),\nBEiT ViT-L/16 (Bao et\u00a0al., 2021),\nAIM ViT-3B/14 (El-Nouby et\u00a0al., 2024),\nMAE ViT-H/14 (El-Nouby et\u00a0al., 2024),\nI-JEPA ViT-H/14 (Assran et\u00a0al., 2023),\nand data2vec2 ViT-L/16 (Baevski et\u00a0al., 2022).\nWe apply a PCA decomposition to the dense outputs produced by each model for each image individually, and rescale the three first components to the RGB range for visualization.", "description": "This figure visualizes the feature maps generated by various self-supervised vision models, including CAPI, DINOv2+reg, BEiT, AIM, MAE, I-JEPA, and data2vec2, each with different model sizes and resolutions.  PCA (Principal Component Analysis) is used to reduce the dimensionality of the dense feature maps for each image.  The top three principal components are then mapped to the red, green, and blue channels of the RGB color space, allowing for visual comparison of the feature representations. This provides insights into how different models capture and represent visual information.", "section": "Results"}, {"figure_path": "https://arxiv.org/html/2502.08769/extracted/6198836/pca/241106_1910_stableeveuh_master_lower_lr/1__8_256_1024_24_12_16_4_0.2_xFSDP_SHARD_GRAD_OP_true_500_50000_0.001_0.0005_0.999/eval/training_499999_teacher/pca_visualization/pca_mypics_560px/artifacts/0000022_pca_joint.004.png", "caption": "Input", "description": "This figure visualizes the features produced by the CAPI ViT-L/14 model applied to images at 560 pixel resolution.  A Principal Component Analysis (PCA) is performed on the dense feature outputs, and the results are displayed. The first column shows the top three principal components mapped to RGB. The remaining columns depict the first eight channels of the PCA, each visualized individually using a coolwarm colormap.", "section": "Qualitative feature analysis"}, {"figure_path": "https://arxiv.org/html/2502.08769/extracted/6198836/pca/241106_1910_stableeveuh_master_lower_lr/1__8_256_1024_24_12_16_4_0.2_xFSDP_SHARD_GRAD_OP_true_500_50000_0.001_0.0005_0.999/eval/training_499999_teacher/pca_visualization/pca_mypics_560px/artifacts/0000022_pca_joint.005.png", "caption": "PCA", "description": "This figure visualizes the features produced by the CAPI ViT-L/14 model applied to images at 560 pixel resolution.  A principal component analysis (PCA) is performed on the dense feature outputs from the model across all images. The first column displays the top three principal components mapped to RGB colors. The subsequent eight columns show the first eight individual channels using a coolwarm colormap.  This provides a visual representation of the learned features, highlighting their spatial organization and potential discriminative qualities.", "section": "Visualisations"}, {"figure_path": "https://arxiv.org/html/2502.08769/extracted/6198836/pca/241106_1910_stableeveuh_master_lower_lr/1__8_256_1024_24_12_16_4_0.2_xFSDP_SHARD_GRAD_OP_true_500_50000_0.001_0.0005_0.999/eval/training_499999_teacher/pca_visualization/pca_mypics_560px/artifacts/0000022_pca_joint.006.png", "caption": "Channel 0", "description": "This figure visualizes the features produced by the CAPI ViT-L/14 model applied to images at 560 pixel resolution.  A principal component analysis (PCA) is performed on the dense feature outputs. The first column shows the first three principal components mapped to RGB channels, offering a visual representation of the main feature variations.  The subsequent eight columns display channels 0 through 7 individually, using a coolwarm colormap for better visualization of feature intensity and distribution. This visualization helps in understanding the nature and distribution of learned features within the CAPI model.", "section": "Visualisations"}, {"figure_path": "https://arxiv.org/html/2502.08769/extracted/6198836/pca/241106_1910_stableeveuh_master_lower_lr/1__8_256_1024_24_12_16_4_0.2_xFSDP_SHARD_GRAD_OP_true_500_50000_0.001_0.0005_0.999/eval/training_499999_teacher/pca_visualization/pca_mypics_560px/artifacts/0000022_pca_joint.007.png", "caption": "Channel 1", "description": "This figure visualizes the features produced by the CAPI ViT-L/14 model applied to images at 560 pixel resolution.  A Principal Component Analysis (PCA) was performed on the dense feature outputs, resulting in a dimensionality reduction. The first column displays the first three principal components mapped to RGB colors for visualization. The subsequent eight columns showcase the next eight channels of the PCA individually, using a coolwarm colormap (from Matplotlib). This visualization helps to understand the nature of the learned features, demonstrating the model's ability to capture different aspects of the images.", "section": "Visualisations"}, {"figure_path": "https://arxiv.org/html/2502.08769/extracted/6198836/pca/241106_1910_stableeveuh_master_lower_lr/1__8_256_1024_24_12_16_4_0.2_xFSDP_SHARD_GRAD_OP_true_500_50000_0.001_0.0005_0.999/eval/training_499999_teacher/pca_visualization/pca_mypics_560px/artifacts/0000022_pca_joint.008.png", "caption": "Channel 2", "description": "This figure visualizes the features extracted from the CAPI ViT-L/14 model applied to images at 560 pixel resolution.  It shows the second principal component (Channel 2) resulting from a Principal Component Analysis (PCA) of the features. The image showcases the features' spatial distribution and how they capture different aspects of the image content.  The visualization helps understand how well the model's features can discriminate and capture relevant semantic information from the image.", "section": "Visualisations"}, {"figure_path": "https://arxiv.org/html/2502.08769/extracted/6198836/pca/241106_1910_stableeveuh_master_lower_lr/1__8_256_1024_24_12_16_4_0.2_xFSDP_SHARD_GRAD_OP_true_500_50000_0.001_0.0005_0.999/eval/training_499999_teacher/pca_visualization/pca_mypics_560px/artifacts/0000022_pca_joint.009.png", "caption": "Channel 3", "description": "This figure visualizes the features produced by the CAPI ViT-L/14 model applied to images at 560 pixel resolution.  A Principal Component Analysis (PCA) is performed on the dense feature outputs across all images. The visualization shows the principal components of the features as RGB and the first eight channels individually using a colormap.", "section": "Visualisations"}, {"figure_path": "https://arxiv.org/html/2502.08769/extracted/6198836/pca/241106_1910_stableeveuh_master_lower_lr/1__8_256_1024_24_12_16_4_0.2_xFSDP_SHARD_GRAD_OP_true_500_50000_0.001_0.0005_0.999/eval/training_499999_teacher/pca_visualization/pca_mypics_560px/artifacts/0000023_pca_joint.000.png", "caption": "Channel 4", "description": "This figure visualizes the features produced by the CAPI ViT-L/14 model applied to images at 560 pixel resolution.  A Principal Component Analysis (PCA) is performed on the dense feature outputs. The first column shows the first three principal components mapped to RGB values. The remaining eight columns display the first eight channels of the features individually, using a coolwarm colormap.", "section": "Visualisations"}, {"figure_path": "https://arxiv.org/html/2502.08769/extracted/6198836/pca/241106_1910_stableeveuh_master_lower_lr/1__8_256_1024_24_12_16_4_0.2_xFSDP_SHARD_GRAD_OP_true_500_50000_0.001_0.0005_0.999/eval/training_499999_teacher/pca_visualization/pca_mypics_560px/artifacts/0000023_pca_joint.001.png", "caption": "Channel 5", "description": "This figure visualizes the features produced by the CAPI ViT-L/14 model applied to images at 560 pixel resolution.  It shows the features using Principal Component Analysis (PCA). The first column displays the first three principal components mapped to RGB colors. The subsequent columns display channels 0 through 7 individually using a coolwarm colormap.", "section": "Visualisations"}, {"figure_path": "https://arxiv.org/html/2502.08769/extracted/6198836/pca/241106_1910_stableeveuh_master_lower_lr/1__8_256_1024_24_12_16_4_0.2_xFSDP_SHARD_GRAD_OP_true_500_50000_0.001_0.0005_0.999/eval/training_499999_teacher/pca_visualization/pca_mypics_560px/artifacts/0000023_pca_joint.002.png", "caption": "Channel 6", "description": "This figure visualizes the features extracted by the CAPI ViT-L/14 model applied to images at 560 pixel resolution.  A Principal Component Analysis (PCA) is performed on the dense feature outputs from the model, and the visualization shows the first eight principal components individually. Each component is represented using a coolwarm colormap, providing a visual representation of the model's learned features across various image regions.", "section": "Visualisations"}, {"figure_path": "https://arxiv.org/html/2502.08769/extracted/6198836/pca/241106_1910_stableeveuh_master_lower_lr/1__8_256_1024_24_12_16_4_0.2_xFSDP_SHARD_GRAD_OP_true_500_50000_0.001_0.0005_0.999/eval/training_499999_teacher/pca_visualization/pca_mypics_560px/artifacts/0000023_pca_joint.003.png", "caption": "Channel 7", "description": "This figure visualizes the features produced by the CAPI ViT-L/14 model applied to images at 560 pixel resolution.  It uses Principal Component Analysis (PCA) to reduce the dimensionality of the features, then displays the results. The first column shows the top three principal components mapped to the RGB color space. The subsequent eight columns show channels 0 through 7 individually, using a coolwarm colormap.", "section": "Visualisations"}, {"figure_path": "https://arxiv.org/html/2502.08769/extracted/6198836/pca/241106_1910_stableeveuh_master_lower_lr/1__8_256_1024_24_12_16_4_0.2_xFSDP_SHARD_GRAD_OP_true_500_50000_0.001_0.0005_0.999/eval/training_499999_teacher/pca_visualization/pca_mypics_560px/artifacts/0000023_pca_joint.004.png", "caption": "Figure 13: Visualization of the features produced by CAPI ViT-L/14 applied to images at 560 pixel resolution.\nWe apply a PCA decomposition to the dense outputs produced by the model across all images.\nThe first column shows the first 3 components as RGB.\nThe next eight columns show the first eight channels individually using a coolwarm colormap from Matplotlib\u00a0(Hunter, 2007).", "description": "This figure visualizes the features extracted by the CAPI ViT-L/14 model when processing images at a resolution of 560 pixels.  A principal component analysis (PCA) was performed on the model's dense output across multiple images to reduce dimensionality. The resulting principal components are then displayed. The first column shows the first three principal components mapped to the red, green, and blue color channels (RGB), providing a visual representation of the most significant variations in the data. The following eight columns represent the next eight principal components, visualized individually using a 'coolwarm' colormap from the Matplotlib library, showcasing additional feature variations in the dataset.", "section": "4.3 Results"}]