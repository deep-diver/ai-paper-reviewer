[{"figure_path": "https://arxiv.org/html/2411.17116/x1.png", "caption": "(a) Phase 1: Local Context Encoding with Anchor Blocks", "description": "This figure illustrates the first phase of the Star Attention algorithm, called \"Context Encoding.\"  The long input context is divided into multiple blocks and distributed across several different hosts.  Each host processes only its assigned block. Notably, each block (except the first) is prepended with a copy of the first block, which acts as an 'anchor block'. This anchor block helps to maintain consistent attention patterns across blocks, preventing the formation of multiple attention 'sinks' which could reduce performance. The parallel processing of the blocks reduces computational complexity.", "section": "2 STAR ATTENTION ALGORITHM"}, {"figure_path": "https://arxiv.org/html/2411.17116/x2.png", "caption": "(b) Phase 2: Query Encoding and Output Generation with Global Attention", "description": "This phase illustrates the process of query encoding and token generation using global attention. The query is replicated across all hosts and attends to the KV cache of each host. Then global attention is computed by aggregating the results at the query host. Only the query host updates its KV cache during this phase.", "section": "2 STAR ATTENTION ALGORITHM"}, {"figure_path": "https://arxiv.org/html/2411.17116/x3.png", "caption": "Figure 1: Star Attention inference flow. All devices in the system are grouped into hosts where one of the hosts is labeled as the \u201cquery\u201d host. The input sequence is processed in two phases. Phase 1 - context encoding. The context portion of the input is partitioned into smaller blocks and distributed across hosts. All blocks, except the first, are prefixed with the initial block, called the \u201canchor\u201d block. Each host processes its assigned block and stores the non-anchor portion of the KV cache. Phase 2 - query encoding and token generation. The input query is broadcast to all the hosts, where in each host, it first attends to the local KV cache computed during phase one. Then the \u201cquery\u201d host computes global attention by aggregating the softmax normalization statistics from all the hosts. This process is repeated for each generated token.", "description": "Star Attention's two-phase inference process is illustrated. Phase 1 involves dividing the context into blocks, distributing them across hosts, and computing local attention within each block (prefixed by an 'anchor' block for improved accuracy). Phase 2 involves broadcasting the query, computing local attention on each host using the local KV cache, and then a designated 'query' host aggregates softmax statistics for global attention computation. This process repeats for each generated token.", "section": "2 STAR ATTENTION ALGORITHM"}, {"figure_path": "https://arxiv.org/html/2411.17116/x4.png", "caption": "Figure 2: Block sparsity pattern for a sequence partitioned into 5 context blocks cisubscript\ud835\udc50\ud835\udc56c_{i}italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and a query block q\ud835\udc5eqitalic_q. Each context block attends only to itself and the \u201canchor block\u201d whereas the query attends to the entire input.", "description": "This figure illustrates the block sparsity pattern used in Star Attention.  The input sequence is divided into five context blocks (c1 to c5) and a single query block (q).  Each context block only attends to its own tokens and the tokens in the 'anchor block' (c1), resulting in blockwise sparsity. This is a localized computation. However, the query block attends to all tokens in the input sequence (both context and query), enabling it to capture global context. The pattern reflects Star Attention's two-phase approach: localized context processing and global query-based attention.", "section": "2 Star Attention Algorithm"}, {"figure_path": "https://arxiv.org/html/2411.17116/x5.png", "caption": "(a) Global Attention", "description": "This figure displays the attention distribution along the sequence length for a context encoded using global attention. The x-axis represents the position IDs, and the y-axis represents the average attention score. The plot shows a clear spike or attention sink at the beginning of the sequence.", "section": "2 STAR ATTENTION ALGORITHM"}, {"figure_path": "https://arxiv.org/html/2411.17116/x6.png", "caption": "(b) Blockwise Context Encoding", "description": "This figure shows the attention distribution along the sequence length when context is encoded using blockwise attention without anchor blocks (Phase 1 of Star Attention).  It illustrates the attention distribution's deviation from the global attention pattern.  Multiple attention sinks are observed at the start of each block, demonstrating the suboptimal behavior of blockwise attention without the anchor block mechanism.", "section": "2 STAR ATTENTION ALGORITHM"}, {"figure_path": "https://arxiv.org/html/2411.17116/x7.png", "caption": "(c) Blockwise Context Encoding with Anchor Blocks", "description": "This figure shows the attention distribution along the sequence length for context encoded with Star Attention's blockwise approach and the use of anchor blocks.  It demonstrates how incorporating anchor blocks (the first block of the sequence, prepended to subsequent blocks) shifts attention sinks (points of high attention concentration) away from the start of each block to the beginning of the entire sequence, thus better approximating the attention pattern of global attention.", "section": "2 STAR ATTENTION ALGORITHM"}, {"figure_path": "https://arxiv.org/html/2411.17116/x8.png", "caption": "Figure 3: Attention distribution along the sequence length for context encoded with different strategies in phase 1 of Star Attention. (a) Global attention shows a spike at the start, corresponding to the attention sink. (b) Star Attention without anchor blocks shows several attention sinks present at the beginning of each block. (c) Star Attention with anchor blocks shifts sinks to anchor tokens, resulting in an attention distribution approximating global attention. In the plot, the input sequence (4K tokens) is divided into 512-token chunks.", "description": "Figure 3 illustrates the impact of different context encoding strategies on attention distribution within Star Attention's first phase. Panel (a) displays global attention, showing a single, sharp attention spike at the sequence beginning, characteristic of an attention sink.  Panel (b) depicts Star Attention without anchor blocks; here, multiple attention sinks emerge at the start of each block, indicating a less efficient attention pattern. This inefficiency is resolved in Panel (c), where Star Attention with anchor blocks is shown.  The anchor blocks effectively consolidate the attention sinks near the beginning of each block, which results in an attention distribution more closely resembling the global attention pattern seen in (a).  In all panels, a 4000-token input sequence is divided into 512-token chunks for analysis.", "section": "2 STAR ATTENTION ALGORITHM"}, {"figure_path": "https://arxiv.org/html/2411.17116/x9.png", "caption": "Figure 4: Accuracy (%) of Star Attention on RULER and BABILong evaluated on sequence lengths of 16K, 32K, 64K, and 128K. In all experiments, the block size and anchor block size are set to one-quarter of the total sequence length. Results using the Llama-3-8B-Instruct-262k, Llama-3.1-8B-Instruct and Llama-3.1-8B-Base models demonstrate that Star Attention retains 95-100% of the accuracy of global attention, and in some cases, even outperform it.", "description": "Figure 4 presents a comparison of the accuracy achieved by Star Attention against global attention across different sequence lengths (16K, 32K, 64K, and 128K) on the RULER and BABILong benchmarks.  The experiment used three Llama models: Llama-3-8B-Instruct-262k, Llama-3.1-8B-Instruct, and Llama-3.1-8B-Base.  For each sequence length, the block size and anchor block size for Star Attention were set to one-quarter of the total sequence length. The results show that Star Attention consistently maintains accuracy within 95-100% of global attention, and in some instances even surpasses it.", "section": "3 Experiments"}, {"figure_path": "https://arxiv.org/html/2411.17116/x10.png", "caption": "Figure 5: Effect of block size on the accuracy of Star Attention on the RULER benchmark with block sizes ranging from 4K to 32K tokens for Llama-3.1-8B instruct model at sequence length of 128K. In each setting, the anchor block size matches the context block size. The results indicate that larger context block sizes are positively correlated with improved accuracy.", "description": "This figure illustrates the relationship between the size of the context blocks used in Star Attention and the model's accuracy on the RULER benchmark.  The experiment was conducted using the Llama-3.1-8B instruct model with a sequence length of 128K tokens.  The x-axis represents the size of the context blocks (in thousands of tokens), ranging from 4K to 32K.  The y-axis represents the accuracy achieved by the model.  Crucially, the anchor block size was always set equal to the context block size. The graph shows a clear positive correlation: as the context block size increases, the model's accuracy improves. This suggests that using larger context blocks allows Star Attention to capture more contextual information, leading to better performance.", "section": "3.3 Trade-off between Accuracy and Speed"}]