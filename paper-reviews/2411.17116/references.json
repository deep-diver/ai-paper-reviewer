{"references": [{"fullname_first_author": "Tri Dao", "paper_title": "FlashAttention: Fast and memory-efficient exact attention with IO-awareness", "publication_date": "2022-12-01", "reason": "This paper introduces FlashAttention, an efficient attention mechanism that significantly reduces memory usage and improves inference speed, which is directly relevant to the Star Attention algorithm's goals."}, {"fullname_first_author": "Iz Beltagy", "paper_title": "Longformer: The long-document Transformer", "publication_date": "2020-04-01", "reason": "This paper presents Longformer, a model designed for handling long sequences, providing a foundation for approaches that address the challenges of long-context inference, which Star Attention also addresses."}, {"fullname_first_author": "Hao Liu", "paper_title": "Ringattention with blockwise transformers for near-infinite context", "publication_date": "2024-01-01", "reason": "The Ring Attention method, proposed in this paper, is directly compared against in the Star Attention paper, serving as a baseline and key point of comparison for evaluating the efficiency improvements."}, {"fullname_first_author": "Cheng-Ping Hsieh", "paper_title": "RULER: What's the real context size of your long-context language models?", "publication_date": "2024-01-01", "reason": "The RULER benchmark, introduced in this paper, is the primary evaluation suite used to assess the performance of Star Attention, making it crucial for understanding the algorithm's capabilities and limitations."}, {"fullname_first_author": "Yuri Kuratov", "paper_title": "BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack", "publication_date": "2024-06-01", "reason": "The BABILong benchmark, introduced here, provides an additional evaluation dataset, offering a broader perspective on the effectiveness of Star Attention in diverse long-context tasks."}]}