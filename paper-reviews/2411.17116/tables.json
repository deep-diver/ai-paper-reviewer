[{"content": "| Model | Seq. Len. (K) | Block Size (K) | Ring-Attn | Star-Attn Acc.(%) | \u0394 Acc. | \u0394 Speedup |\n|---|---|---|---|---|---|---|\n|  | 16 | 4 | 86.12 | +2.47% | 1.1x |\n|  | 32 | 8 | 82.52 | +1.54% | 1.2x |\n|  | 64 | 16 | 79.05 | +1.28% | 1.8x |\n| Llama-3-8B-Instruct, 1048K <br> Gradient.ai (2024) | 128 | 32 | 77.39 | +1.23% | 2.7x |\n|  | 16 | 4 | 95.09 | -2.85% | 1.7x |\n|  | 32 | 8 | 94.61 | -2.70% | 2.0x |\n| Llama-3.1-70B-Instruct, 128K <br> Meta-AI (2024) | 64 | 16 | 88.54 | -1.63% | 4.7x |", "caption": "Table 1: Star Attention vs Ring Attention (baseline) accuracy and relative inference speed-up. The \u0394\u0394\\Deltaroman_\u0394 for Star Attention shows the relative accuracy improvement (+) or degradation (-). We set block size to one-quarter of the sequence length. Star Attention achieves significant speedup over Ring Attention while maintaining the accuracy. For larger models, the speedup of Star Attention is even more pronounced.", "description": "This table compares the performance of Star Attention against Ring Attention (the baseline) across different LLMs and sequence lengths.  It shows the accuracy achieved by each method and calculates the relative speedup provided by Star Attention.  The change in accuracy (\"\u0394 Acc.\") indicates whether Star Attention improved or decreased accuracy compared to Ring Attention. The block size used for Star Attention was set to one-quarter of the sequence length. The results highlight that Star Attention significantly speeds up inference time while maintaining similar accuracy, and this improvement is even more significant for larger LLMs.", "section": "3 Experiments"}, {"content": "| Model | Seq. Len. (K) | Block Size (K) | Ring-Attn | Star-Attn Acc. (%) | \n\u0394 Acc. | \n\u0394 Speedup |\n|---|---|---|---|---|---|---|\n|  | 128 | 32 | 77.39 | +1.23% | 2.7x |\n|  | 256 | 32 | 74.44 | -1.04% | 10.8x |\n|  | 512 | 32 | 69.30 | -9.71% | 16.2x |\n| Llama3-8B-Instruct, 1048K <br> Gradient.ai (2024) | 1024 | 32 | 63.70 | -8.36% | 16.9x |\n|  | 64 | 16 | 88.54 | -1.63% | 4.7x |\n| Llama-3.1-70B-Instruct, 128K <br> Meta-AI (2024) | 128 | 16 | 65.29 | -11.44% | 8.7x |", "caption": "Table 2: Accuracy versus speed trade-off for Star Attention compared to Ring (Global) Attention on RULER. The \u0394\u0394\\Deltaroman_\u0394 for star attention shows the relative accuracy degradation and the relative speedup compared to global attention. When the block size remains fixed and the as sequence length increases, Star Attention achieves exponential speedup over Ring (Global) Attention at the cost of slightly more accuracy degradation. It is upto the user to decided how much accuracy they want to trade-off for speed by setting the block size.", "description": "This table presents a comparison of Star Attention and Ring Attention (the baseline) in terms of accuracy and inference speed on the RULER benchmark.  It shows the relative accuracy change (positive indicates improvement, negative indicates degradation) and speedup achieved by Star Attention compared to Ring Attention. The results are presented for various sequence lengths and a fixed block size.  As the sequence length increases, Star Attention demonstrates an exponential speedup, but with a slight decrease in accuracy. The table highlights the trade-off between speed and accuracy, allowing users to choose the block size that balances these factors according to their needs.", "section": "3 Experiments"}, {"content": "| Experiments | RULER-NIAH (%) | RULER-NIAH (%) | RULER-NIAH (%) | RULER-NIAH (%) |\n|---|---|---|---|---|\n|  | 64K | \u039464k | 128k | \u0394128k |\n| Global attention | 99.50 | - | 98.49 | - |\n| No anchor block | 60.11 | -39.59% | 73.75 | -25.12% |\n| Content set to first-block, position IDs are: |  |  |  |  |\n| \u00a0randomly sampled from [0, current_block) | 96.79 | -2.72% | 97.16 | -1.35% |\n| \u00a0same as previous block | 97.35 | -2.16% | 96.80 | -1.71% |\n| \u00a0**same as first block** | 97.61 | -1.90% | 97.54 | -0.96% |\n| Position IDs set to first-block, content is: |  |  |  |  |\n| \u00a0constant token (ex: \u2018 \u2019 or \u2018 the\u2019 or \u2018.\u2019 ) | 0.00 | -100.00% | 0 | -100.00% |\n| \u00a0random tokens | 90.55 | -8.99% | 82.63 | -10.15% |\n| \u00a0shuffled first block tokens | 92.96 | -6.57% | 90.76 | -3.26% |\n| \u00a0**first block tokens** | 97.61 | -1.90% | 94.94 | -0.96% |\n| Previous-block used as anchor | 94.20 | -5.33% | 96.13 | -2.40% |", "caption": "Table 3: Experiments on analyzing the impact of varying the position and content of the anchor block with the LLaMA-3.1-8B-Instruct model, with a block size of 16K for 64K sequence length, and 32K for 128K sequence lengths. In each setting, the size of the anchor block matches the context block size. The \u0394\u0394\\Deltaroman_\u0394 for star attention shows the relative accuracy degradation compared to global attention. The experiments are categorized into 4 groups: (i) absence of anchor block; (ii) varying the position IDs; (iii) varying the content; (iv) varying both the position and the content. Results indicate that while the anchor block\u2019s position is not critical, its content is essential for optimal performance.", "description": "This table presents an ablation study on the Star Attention model, specifically focusing on the impact of the anchor block's position and content on the model's performance.  The study uses the LLaMA-3.1-8B-Instruct model with two different sequence lengths (64K and 128K) and corresponding block sizes (16K and 32K respectively). The table compares the accuracy of Star Attention against global attention under four different conditions: (1) No anchor block, (2) Varying the position IDs of the anchor block, (3) Varying the content of the anchor block, and (4) Varying both position and content. The relative accuracy degradation (\u0394) compared to the global attention is calculated for Star Attention in each scenario. The results demonstrate that while the anchor block's position is less crucial, its content is vital for achieving optimal performance with Star Attention.", "section": "4 ABLATION STUDY"}, {"content": "| Task | Haystack | Keys Type | Keys # | Values Type | Values # | # Outputs |\n|---|---|---|---|---|---|---|\n| Category Name | Type |  |  |  |  |  |\n| Single 1 | noise | words | 1 | numbers | 1 | 1 |\n| Single 2 | book | words | 1 | numbers | 1 | 1 |\n| Single 3 | book | words | 1 | uuids | 1 | 1 |\n| MultiKey 1 | book | words | 4 | numbers | 1 | 1 |\n| MultiKey 2 | line | words | \u221e | numbers | 1 | 1 |\n| MultiKey 3 | kv | uuids | \u221e | uuids | 1 | 1 |\n| MultiValue | book | words | 1 | numbers | 4 | 1 |\n| NIAH (Retrieval) MultiQuery | book | words | 4 | numbers | 1 | 4 |\n| Multi-Hop Tracing Variable Tracking |  |  |  |  |  |  \u2013 |\n| Common Words Extraction |  |  |  |  |  |  \u2013 |\n| Aggregation Frequent Words Extraction |  |  |  |  |  |  \u2013 |\n| Question Answering QA 1 (squad) |  |  |  |  |  |  \u2013 |\n| Answering QA 2 (hotpotqa) |  |  |  |  |  |  \u2013 |", "caption": "Table 4: Configuration of RULER tasks", "description": "This table details the configuration of the tasks used in the RULER benchmark.  For each task, it shows the category (Retrieval, Multi-Hop Tracing, Aggregation, Question Answering), the task name, the type of haystack (the long text), the type of keys and values used, the number of keys and values, and the number of outputs. This provides a comprehensive overview of the various challenges presented by the RULER benchmark dataset.", "section": "Evaluation Benchmarks"}, {"content": "| Multi-Hop |\n|---|---| \n| Tracing |", "caption": "Table 5: Configuration of tasks in BABILong", "description": "The BABILong benchmark consists of 5 tasks, each involving a different number of supporting facts.  The tasks simulate scenarios with characters and objects moving and interacting in various locations.  Each interaction is described by a factual statement.  The goal is to answer questions based on these facts, testing the model's ability to reason across multiple facts and complex scenarios.  The table details the number of facts per task, distinguishing between single, double, and triple fact scenarios, as well as tasks involving two or three argument relations.  This allows for a comprehensive evaluation of the model's ability to handle varying levels of complexity in long-context reasoning.", "section": "3.2 RESULTS"}, {"content": "| Task | Name | # Facts per task |\n|---|---|---|\n| qa1 | single supporting fact | 2 - 10 |\n| qa2 | two supporting facts | 2 - 68 |\n| qa3 | three supporting facts | 4 - 32 |\n| qa4 | two arg relations | 2 |\n| qa5 | three arg relations | 2 - 126 |", "caption": "Table 6: Time per sample (seconds) for Llama3.1-8B-Instruct model with vanilla (global) inference, ring (global) and star attention, using 8 A100 GPUs. Vanilla autoregressive generation encounters out-of-memory (OOM) at 128K sequence length. It performs best in short context scenarios (i.e. sequences upto 32K tokens) but in long context scenarios, star attention demonstrates significant speedup.", "description": "This table compares the inference time per sample for three different methods: vanilla autoregressive generation, Ring Attention, and Star Attention.  The experiment uses the Llama3.1-8B-Instruct model and 8 A100 GPUs.  The results show that vanilla autoregressive generation runs out of memory (OOM) when processing sequences longer than 64K tokens. For shorter sequences (up to 32K tokens), vanilla generation is fastest. However, for longer sequences, Star Attention demonstrates significantly faster inference times than both vanilla generation and Ring Attention.", "section": "3 Experiments"}, {"content": "| Seq. Length | Time Per Sample (s) |\n|---|---|---|---|\n| (K) | Vanilla | Ring | Star |\n| 16 | 7 | 10 | 9 |\n| 32 | 10 | 12 | 10 |\n| 64 | 18 | 22 | 12 |\n| 128 | OOM | 53 | 20 |", "caption": "Table 7: Resources used for the speedup experiments", "description": "This table details the computational resources utilized in the speedup experiments comparing Star Attention and Ring Attention.  Specifically, it shows the model size (in terms of parameters), the sequence length processed, the number of GPUs used, and the number of parallel workers employed for each experiment configuration. This information is crucial for understanding the scalability and efficiency of the proposed Star Attention method relative to the baseline Ring Attention approach.", "section": "C Experiment Details"}, {"content": "| Model Size | Seq. Length | # GPUs | # Workers |\n|---|---|---|---| \n| 8B | 16K - 128K | 8 | 4 |\n|  | 256K - 512K | 16 | 8 |\n|  | 1M | 32 | 16 |\n| 70B | 16K - 32K | 8 | 4 |\n|  | 64K | 16 | 4 |\n|  | 128K | 32 | 8 |", "caption": "Table 8: Accuracy (%) of star attention on RULER and BABILONG evaluated on sequence lengths of 16K, 32K, 64K, and 128K. In all experiments, the block size and anchor block size are set to one-quarter of the total sequence length. Results using the Llama-3-8B-Instruct-262k, Llama-3.1-8B-Instruct and Llama-3.1-8B-Base models demonstrate that star attention retains 95-100% of the accuracy of global attention, and in some cases, even outperform it.", "description": "This table presents a comparison of the accuracy achieved by Star Attention against global attention across different sequence lengths (16K, 32K, 64K, and 128K) on two benchmark datasets: RULER and BABILONG.  Three different Llama-based language models were used in the evaluation: Llama-3-8B-Instruct-262k, Llama-3.1-8B-Instruct, and Llama-3.1-8B-Base. For each model and sequence length, the accuracy of Star Attention is reported as a percentage, along with the percentage difference compared to the accuracy achieved with global attention.  The results show that Star Attention generally maintains a high level of accuracy (95-100%) compared to global attention, while sometimes even slightly surpassing global attention's performance.", "section": "3 Experiments"}, {"content": "| Model | Seq. length | Block size | RULER (%) Global | RULER (%) Star | RULER (%) \u0394 | BABILONG (%) Global | BABILONG (%) Star | BABILONG (%) \u0394 | \n|---|---|---|---|---|---|---|---|---|\n| GradientAI Llama-3-8B -Instruct-262k | 16K | 4K | 88.92 | 89.48 | +0.63% | 43.60 | 43.40 | -0.46% | \n|  | 32K | 8K | 85.25 | 85.74 | +0.58% | 40.00 | 39.40 | -1.50% | \n|  | 64K | 16K | 83.17 | 82.30 | -1.05% | 40.40 | 39.00 | -3.47% | \n|  | 128K | 32K | 79.25 | 77.79 | -1.83% | 30.80 | 33.20 | +7.79% | \n| Meta Llama-3.1-8B -Instruct | 16K | 4K | 99.78 | 91.27 | -1.02% | 59.60 | 59.80 | +0.34% | \n|  | 32K | 8K | 99.66 | 88.70 | +1.34% | 54.60 | 54.00 | -1.10% | \n|  | 64K | 16K | 98.72 | 83.37 | -1.67% | 49.20 | 46.60 | -5.28% | \n|  | 128K | 32K | 92.54 | 74.41 | -2.49% | 40.00 | 38.60 | -3.50% | \n| Meta Llama-3.1-8B -Base | 16K | 4K | 77.18 | 78.64 | +1.9% | 22.00 | 25.20 | +14.55% | \n|  | 32K | 8K | 74.76 | 76.91 | +2.88% | 22.60 | 24.00 | +6.19% | \n|  | 64K | 16K | 70.01 | 69.09 | -1.32% | 26.80 | 27.20 | +1.49% | \n|  | 128K | 32K | 64.68 | 69.58 | +7.58% | 31.00 | 26.40 | -14.84% | ", "caption": "Table 9: Accuracy of Llama-3.1-8B-Instruct on retrieval tasks in RULER with Global Attention and Star Attention", "description": "This table presents a detailed comparison of the accuracy achieved by the Llama-3.1-8B-Instruct language model on various retrieval tasks within the RULER benchmark.  It contrasts the performance of the model using standard global attention against its performance using the Star Attention method proposed in the paper.  Results are broken down by the size of context blocks (4K, 8K, 16K, 32K) and sequence length (16K, 32K, 64K, 128K). Each row showcases accuracy for a specific block size and sequence length, providing a comprehensive assessment of the effect of Star Attention on accuracy across a range of settings.", "section": "3.2 RESULTS"}, {"content": "| Model | Block Size (K) | Seq. Len. (K) | Retrieval (NIAH) Single 1 | Retrieval (NIAH) Single 2 | Retrieval (NIAH) Single 3 | Retrieval (NIAH) Multi-Key 1 | Retrieval (NIAH) Multi-Key 2 | Retrieval (NIAH) Multi-Key 3 | Retrieval (NIAH) Multi-Value | Retrieval (NIAH) Multi-Query |\n|---|---|---|---|---|---|---|---|---|---|---|\n| Llama-3.1-8B-Instruct | Global Attn. |  |  |  |  |  |  |  |  |  |\n|  | 16 | 100 | 100 | 100 | 100 | 99.8 | 100 | 99 | 99.9 | 99.5 |\n|  | 32 | 100 | 100 | 100 | 100 | 99.8 | 99.8 | 99.6 | 99 | 99.05 |\n|  | 64 | 100 | 100 | 100 | 100 | 99.4 | 99.2 | 96.8 | 95.15 | 99.2 |\n|  | 128 | 100 | 99.6 | 99.8 | 97.6 | 87.2 | 66.8 | 91.55 | 97.8 |\n|  | 4 | 16 | 100 | 99.4 | 100 | 98 | 98.8 | 99 | 91.1 | 98.25 |\n|  | 8 | 32 | 100 | 100 | 100 | 99.2 | 99.4 | 98.2 | 94 | 98.3 |\n|  | 16 | 64 | 100 | 100 | 100 | 99.2 | 98 | 90 | 85.35 | 97.9 |\n|  | 32 | 128 | 100 | 100 | 99.6 | 96.4 | 84.8 | 59 | 82.7 | 96.55 |", "caption": "Table 10: Accuracy of Llama-3.1-8B-Instruct on multi-hop, aggregation, and question answering tasks in RULER with Global Attention and Star Attention", "description": "This table presents the accuracy results for the Llama-3.1-8B-Instruct model on various long-context tasks within the RULER benchmark.  It compares the performance of the model using global attention against Star Attention, a novel method introduced in the paper for efficient LLM inference.  The tasks evaluated include multi-hop reasoning, aggregation, and question answering, and the results are presented for different sequence lengths and block sizes.  The table demonstrates the impact of Star Attention on the accuracy of these tasks.", "section": "3 Experiments"}, {"content": "| Multi-| Key 1|\n|-|-|\n", "caption": "Table 11: Accuracy of Llama-3.1-8B-Base on retrieval tasks in RULER with Global Attention and Star Attention", "description": "This table presents the accuracy comparison between the global attention mechanism and the Star Attention method on various retrieval tasks within the RULER benchmark, specifically using the Llama-3.1-8B-Base language model.  It shows the accuracy achieved by each method across different sequence lengths (16K, 32K, 64K, and 128K) and varying block sizes. Each row represents a different experimental setup, and multiple columns show the accuracy for distinct subtasks within the retrieval category (Single NIAH, Multi-Key NIAH, Multi-Value, etc.) providing a granular view of performance.", "section": "3.2 Results"}, {"content": "| Multi-| Key 2|\n|---|---|", "caption": "Table 12: Accuracy of Llama-3.1-8B-Base on multi-hop, aggregation, and question answering tasks in RULER with Global Attention and Star Attention", "description": "This table presents a comparison of the accuracy achieved by Llama-3.1-8B-Base model on multi-hop, aggregation, and question answering tasks within the RULER benchmark, using both global attention and Star Attention mechanisms.  It shows the performance variation across different sequence lengths (16K, 32K, 64K, and 128K) and varying block sizes (4K, 8K, 16K, and 32K). The results highlight the effectiveness of Star Attention, especially at longer sequences, while also providing insights into the performance trade-off between accuracy and speed as influenced by block sizes.", "section": "3 EXPERIMENTS"}, {"content": "| Multi- |\n|---|---| \n| Key 3 |", "caption": "Table 13: Accuracy of Llama-3.1-70B-Instruct on retrieval tasks in RULER with Global Attention and Star Attention", "description": "This table presents the accuracy results for the Llama-3.1-70B-Instruct language model on a subset of retrieval tasks from the RULER benchmark.  It compares the performance of the model using two different attention mechanisms: global attention (the standard approach) and Star Attention (the novel method proposed in the paper). The table shows the accuracy achieved by each method for different sequence lengths and block sizes, allowing for a direct comparison of their performance.  Block size refers to the size of the context chunks used during the context encoding phase of Star Attention. The results highlight how Star Attention affects accuracy on different sequence lengths and various subtasks within the broader RULER benchmark.", "section": "3 Experiments"}, {"content": "| Multi-Value |\n|---|---|", "caption": "Table 14: Accuracy of Llama-3.1-70B-Instruct on multi-hop, aggregation, and question answering tasks in RULER with Global Attention and Star Attention", "description": "This table presents the accuracy results of the Llama-3.1-70B-Instruct language model on three task categories from the RULER benchmark: Multi-hop, Aggregation, and Question Answering.  It compares the performance of the model using global attention (the standard approach) against the performance using Star Attention (the proposed method). The results are shown for different sequence lengths and various block sizes, providing insights into the impact of Star Attention on the accuracy of the model across different task complexities and sequence lengths.", "section": "3 Experiments"}, {"content": "| Multi- | Query |\n|---|---|", "caption": "Table 15: Accuracy of GradientAI Llama-3-8B-Instruct-262K on retrieval tasks in RULER with Global Attention and Star Attention", "description": "This table presents the accuracy results of the GradientAI Llama-3-8B-Instruct-262K language model on the retrieval tasks within the RULER benchmark.  It compares the model's performance using two different attention mechanisms: Global Attention (the standard approach) and Star Attention (the novel method proposed in the paper). The results are broken down by different sequence lengths (from 16K to 256K tokens) and block sizes, allowing for an analysis of the trade-off between accuracy and efficiency of Star Attention at various scales.", "section": "3 EXPERIMENTS"}, {"content": "| **Llama-3.1-8B-Instruct** |  |  | **Multi-Hop** | **Aggregation** | **Question Answering** |  |  |\n|---|---|---|---|---|---|---|---|\n| **Block Size (K)** | **Seq. Len. (K)** | **VT** | **CWE** | **FWE** | **QA 1** | **QA 2** |  |  |\n| Global Attn. | 16 | 99.56 | 75 | 88.87 | 80.8 | 56.4 |  |  |\n|  | 32 | 99.2 | 14.7 | 93.93 | 78.8 | 54 |  |  |\n|  | 64 | 95.44 | 1.96 | 85.13 | 78.8 | 51.2 |  |  |\n|  | 128 | 61.76 | 0.04 | 72.33 | 76 | 41.6 |  |  |\n| 4 | 16 | 91.96 | 85.72 | 89.73 | 80.2 | 54.4 |  |  |\n| 8 | 32 | 92.68 | 45.66 | 95.27 | 78.6 | 51.8 |  |  |\n| 16 | 64 | 92.32 | 5.78 | 86.47 | 78.4 | 50.4 |  |  |\n| 32 | 128 | 62.8 | 0.04 | 75.87 | 68 | 41.6 |  |  |", "caption": "Table 16: Accuracy of GradientAI Llama-3-8B-Instruct-262K on multi-hop, aggregation, and question answering tasks in RULER with Global Attention and Star Attention", "description": "This table presents the accuracy results of the GradientAI Llama-3-8B-Instruct-262K model on various question answering tasks from the RULER benchmark.  It compares the model's performance using global attention against its performance when using Star Attention, a novel method introduced in the paper for efficient LLM inference over long sequences.  The results are broken down by several factors, including the sequence length, the size of the blocks used in the Star Attention method, and the specific sub-task within the RULER benchmark (e.g., multi-hop reasoning, aggregation, different question answering tasks). This allows for a comparison of accuracy trade-offs between global attention and Star Attention under different conditions. The goal is to show how Star Attention impacts accuracy on various types of tasks and sequence lengths compared to using full global attention.", "section": "3 Experiments"}, {"content": "| Model | Block Size (K) | Seq. Len. (K) | Retrieval (NIAH) Single 1 | Retrieval (NIAH) Single 2 | Retrieval (NIAH) Single 3 | Retrieval (NIAH) Multi-Key 1 | Retrieval (NIAH) Multi-Key 2 | Retrieval (NIAH) Multi-Key 3 | Retrieval (NIAH) Multi-Value | Retrieval (NIAH) Multi-Query |\n|---|---|---|---|---|---|---|---|---|---|---|\n| Llama-3.1-8B-Base | Global Attn. 16 | 16 | 100 | 100 | 100 | 100 | 99.2 | 100 | 99.4 | 99.45 | 99.85 |\n|  | 32 | 32 | 100 | 100 | 100 | 99 | 99.4 | 99.4 | 99.55 | 99.4 |\n|  | 64 | 64 | 100 | 100 | 100 | 98.8 | 86.2 | 95.4 | 96.8 | 97.55 |\n|  | 128 | 128 | 100 | 100 | 98 | 93.8 | 53.6 | 64 | 80.9 | 85.3 |\n|  | 4 | 4 | 16 | 100 | 100 | 100 | 97.4 | 99.2 | 99 | 98.4 | 99.15 |\n|  | 8 | 8 | 32 | 100 | 100 | 100 | 96.2 | 98.2 | 99.2 | 98.55 | 98.7 |\n|  | 16 | 16 | 64 | 100 | 100 | 100 | 96.6 | 90.6 | 85.6 | 94.9 | 96.15 |\n|  | 32 | 32 | 128 | 100 | 100 | 98.2 | 88.8 | 67 | 47.6 | 72.75 | 77.55 |", "caption": "Table 17: Accuracy of GradientAI Llama-3-8B-Instruct-1048K on retrieval tasks in RULER with Global Attention and Star Attention", "description": "This table presents the accuracy comparison between global attention and Star Attention on retrieval tasks from the RULER benchmark, using the GradientAI Llama-3-8B-Instruct-1048K model.  It shows the accuracy for various sequence lengths (16K, 32K, 64K, 128K, 256K, 512K, and 1024K tokens) and different block sizes (4K, 8K, 16K, and 32K tokens) for each of the five sub-tasks within the Needle-in-a-Haystack (NIAH) category of RULER: Single 1, Single 2, Single 3, Multi-Key 1, Multi-Key 2, Multi-Key 3, Multi-Value, and Multi-Query.  The results illustrate the impact of both sequence length and block size on model accuracy and the effectiveness of Star Attention in approximating global attention.", "section": "3.2 RESULTS"}, {"content": "| Multi- |\n|---|---| \n| Key 1 |", "caption": "Table 18: Accuracy of GradientAI Llama-3-8B-Instruct-1048K on multi-hop, aggregation, and question answering tasks in RULER with Global Attention and Star Attention", "description": "This table presents the performance comparison between Global Attention and Star Attention on the GradientAI Llama-3-8B-Instruct-1048K model for multi-hop reasoning, aggregation, and question answering tasks within the RULER benchmark.  It shows the accuracy achieved by each method across various sequence lengths and block sizes, offering insights into Star Attention's effectiveness and trade-offs with different configurations.", "section": "3 Experiments"}]