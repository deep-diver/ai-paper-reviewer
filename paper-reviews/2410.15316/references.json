{"references": [{" publication_date": "2019", "fullname_first_author": "Alec Radford", "paper_title": "Language models are unsupervised multitask learners", "reason": "This foundational paper established the effectiveness of large language models (LLMs) as a powerful paradigm for various NLP tasks, laying the groundwork for the development of Ichigo, which leverages pre-trained LLMs for its architecture.", "section_number": 6}, {" publication_date": "2020", "fullname_first_author": "Colin Raffel", "paper_title": "Exploring the limits of transfer learning with a unified text-to-text transformer", "reason": "This paper introduced a unified text-to-text transformer approach which is highly relevant to the Ichigo model's early fusion architecture and use of a uniform transformer for both speech and text data.", "section_number": 6}, {" publication_date": "2021", "fullname_first_author": "Kushal Lakhotia", "paper_title": "On generative spoken language modeling from raw audio", "reason": "This paper focused on generative spoken language modeling from raw audio, which directly relates to the Ichigo's design of quantizing speech into discrete tokens for processing within its unified model.", "section_number": 6}, {" publication_date": "2022", "fullname_first_author": "Aditya Ramesh", "paper_title": "Hierarchical text-conditional image generation with clip latents", "reason": "This is a pioneering paper in multimodal model design that integrates text and image modalities. The core idea of combining different modalities in a single framework is relevant to Ichigo's mixed-modal approach, although applied to audio and text rather than images.", "section_number": 6}, {" publication_date": "2023", "fullname_first_author": "Yu Shu", "paper_title": "LLASM: Large language and speech model", "reason": "This paper explored integrating speech capabilities into large language models, directly addressing the core task of Ichigo. It is a key comparative study showing that Ichigo outperforms existing models.", "section_number": 6}, {" publication_date": "2023", "fullname_first_author": "Tianrui Wang", "paper_title": "Viola: Unified codec language models for speech recognition, synthesis, and translation", "reason": "This paper also focused on creating a unified language model for speech-related tasks. Ichigo shares similarities in using a unified model for processing speech and text but differentiates in the early fusion approach.", "section_number": 6}, {" publication_date": "2023", "fullname_first_author": "Soham Deshmukh", "paper_title": "Pengi: An audio language model for audio tasks", "reason": "This paper explored audio language models, focusing on audio tasks, which relates to Ichigo's incorporation of speech understanding capabilities. Comparing Ichigo's performance with Pengi provides valuable insight into the effectiveness of the tokenized early fusion architecture.", "section_number": 6}, {" publication_date": "2024", "fullname_first_author": "Yunfei Chu", "paper_title": "Qwen2-Audio technical report", "reason": "This is another state-of-the-art model in speech-language tasks. Ichigo's performance is directly compared against it, demonstrating Ichigo's superior performance in terms of various benchmarks.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Qingkai Fang", "paper_title": "Llama-omni: Seamless speech interaction with large language models", "reason": "This paper explored seamless speech interaction using LLMs.  The approach shares similarities to Ichigo's integration of speech capabilities but differs in its architecture and fusion method. Comparing these two approaches provides a deeper understanding of the design space.", "section_number": 6}, {" publication_date": "2024", "fullname_first_author": "Abhimanyu Dubey", "paper_title": "The llama 3 herd of models", "reason": "This paper details the Llama 3 series of models, which serves as the backbone for Ichigo. Understanding the properties of this base model is crucial for evaluating the improvements and innovations introduced by Ichigo's design.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Alexandre D\u00e9fossez", "paper_title": "Moshi: a speech-text foundation model for real-time dialogue", "reason": "This paper introduced Moshi, another real-time multimodal foundation model. Comparing Ichigo with Moshi highlights the different approaches used to achieve real-time interactions and the trade-offs between their design choices.", "section_number": 6}, {" publication_date": "2024", "fullname_first_author": "Zhangchen Xu", "paper_title": "Magpie: Alignment data synthesis from scratch by prompting aligned llms with nothing", "reason": "This paper focuses on data generation techniques, which is relevant to Ichigo's synthetic data generation pipeline. It informs the discussion of data quality and its impact on model performance.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Jun Zhan", "paper_title": "AnyGPT: Unified multimodal llm with discrete sequence modeling", "reason": "This paper introduced a multimodal LLM with a focus on discrete sequence modeling for various modalities, a design principle highly relevant to Ichigo's approach of processing both speech and text tokens using a uniform transformer architecture.", "section_number": 6}, {" publication_date": "2020", "fullname_first_author": "Vineel Pratap", "paper_title": "MLS: A large-scale multilingual dataset for speech research", "reason": "This paper introduced a large-scale multilingual dataset for speech research, providing a key dataset for pre-training Ichigo. Understanding the properties of this dataset and its relevance for multilingual speech recognition is crucial to understanding the pre-training phase of Ichigo.", "section_number": 3}, {" publication_date": "2020", "fullname_first_author": "Tom B Brown", "paper_title": "Language models are few-shot learners", "reason": "This paper established the capability of LLMs to perform well on various tasks with minimal training data, which underpins Ichigo's fine-tuning methodology on limited instruction datasets.", "section_number": 1}, {" publication_date": "2017", "fullname_first_author": "Piotr Bojanowski", "paper_title": "Enriching word vectors with subword information", "reason": "This paper detailed a method for enriching word vectors with subword information, a technique that's implicitly used in processing text data in Ichigo, which involves tokenization and embedding of words.", "section_number": 3}, {" publication_date": "2015", "fullname_first_author": "Vassil Panayotov", "paper_title": "Librispeech: an asr corpus based on public domain audio books", "reason": "This paper introduced the Librispeech corpus, a significant dataset used for pre-training Ichigo. The properties of this dataset, its size, and the nature of its audio content are directly relevant to Ichigo's pre-training methodology.", "section_number": 3}, {" publication_date": "2021", "fullname_first_author": "John Hewitt", "paper_title": "Initializing new word embeddings for pretrained language models", "reason": "This paper directly addresses the specific problem of initializing new word embeddings for pretrained language models, an important consideration for expanding the vocabulary of Llama-3.1-8B-Instruct to include speech tokens in Ichigo.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Edward J Hu", "paper_title": "Lora: Low-rank adaptation of large language models", "reason": "This paper introduced a low-rank adaptation technique for large language models that could be potentially applied to Ichigo's adaptation process for improving efficiency and resource utilization.", "section_number": 6}, {" publication_date": "2024", "fullname_first_author": "Karl Cobbe", "paper_title": "Training verifiers to solve math word problems", "reason": "This paper introduced a dataset for evaluating reasoning capabilities in LLMs, particularly in the context of mathematics.  While not directly related to Ichigo's primary function, it's used in the evaluation process to assess its performance on general reasoning tasks.", "section_number": 5}]}