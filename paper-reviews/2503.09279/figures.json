[{"figure_path": "https://arxiv.org/html/2503.09279/x2.png", "caption": "Figure 1: \nCockatiel, a three-stage training pipeline that ensembles synthetic and human-aligned training.\nCockatiel-13B are capable to generate detailed captions that consistently aligns with every visual element in the input video (top left).\nFurthermore, Cockatiel-13B achieves new state-of-the-art and considerable dimension-balanced performance on VDCSCORE while consistently voted as the most human-aligned models compared to baselines (right).\nThe key contributor to these capabilities is the ensembling synthetic and human preferenced training, which infuses Cockatiel-13B with diverse strengths of leading VDC models and human preferences (bottom left).", "description": "This figure illustrates the Cockatiel framework, a three-stage training pipeline designed to generate detailed video captions. Stage 1 involves curating a dataset by selecting high-quality synthetic captions that align with human preferences, using a human-aligned caption quality scorer. Stage 2 utilizes this curated dataset to train the Cockatiel-13B model, incorporating the strengths of various existing models. Stage 3 distills a smaller, more efficient model (Cockatiel-8B) from the Cockatiel-13B model. The figure showcases an example of a detailed video caption generated by Cockatiel-13B, highlighting its ability to accurately describe the various visual elements in a video clip. It also presents the improved performance of Cockatiel-13B compared to other state-of-the-art models on the VDCSCORE benchmark, demonstrating its enhanced accuracy and alignment with human preferences. The superior performance is attributed to Cockatiel's ensemble approach which leverages both synthetic and human-aligned training.", "section": "3. The Proposed Cockatiel"}, {"figure_path": "https://arxiv.org/html/2503.09279/x3.png", "caption": "Figure 2: \nOverall pipeline of our proposed Cockatiel.\nOur training pipeline successfully ensemble both the advantages of base models and human preferences, yielding our Cockatiel captioner series.\nThrough the ensembling synthetic and human preferenced training, Cockatiel-13B achieves significant VDC performance while being preferred by humans.", "description": "The figure illustrates the three-stage training pipeline of the Cockatiel model for detailed video captioning. Stage 1 involves human-aligned data curation, using a human-aligned scorer to select high-quality synthetic captions based on various aspects such as main object, background, and camera movement.  The selected captions are then used in Stage 2 for ensemble synthesized training of the Cockatiel-13B model. This stage combines the strengths of multiple existing models and incorporates human preferences. Finally, Stage 3 involves distilling a smaller, more efficient Cockatiel-8B model from the larger Cockatiel-13B model. This process results in a model that achieves state-of-the-art video detailed captioning (VDC) performance while also aligning better with human preferences.", "section": "3. The Proposed Cockatiel"}, {"figure_path": "https://arxiv.org/html/2503.09279/x4.png", "caption": "Figure 3: \nQualitative comparison between Cockatiel-13B and the current sota VDC models.\nFor a detailed comparison between Cockatiel-13B and all leading VDC models, please refer to the supplementary files.\nThe caption content that is exclusively captured by our model, captured by our model and other baselines, or misaligned with the detailed visual elements in the videos are emphasized using red, yellow and green backgrounds.", "description": "This figure presents a qualitative comparison of video detailed captions generated by the Cockatiel-13B model and state-of-the-art (SOTA) models.  Specific examples highlight the model's ability to generate captions aligning with fine-grained details in the video, including objects, actions, and background elements. Captions exclusively generated by Cockatiel-13B, those shared with other models, and those misaligned with the video content are color-coded in red, yellow, and green, respectively. For a more extensive comparison, refer to the supplementary materials.", "section": "4. Results & Analysis"}, {"figure_path": "https://arxiv.org/html/2503.09279/x5.png", "caption": "Figure 4: \nAblation studies on the LoRA rank (left), training dataset size (middle), and the quality score threshold (right).\nFor brevity, we report only the average accuracy on VDCSCORE; more comprehensive results are provided in the supplementary materials.\nThe hyper-parameter settings are consistent across all the ablation studies, except the ablated one.\nSpecifically, the default settings are as follows: LoRA rank is set to 256, the training dataset size is 20k, and the threshold for the quality score is 3.5, the selection policy is the scorer-based selection policy with threshold on quality score.", "description": "This figure presents the results of ablation studies conducted on three hyperparameters: LoRA rank, training dataset size, and quality score threshold.  The leftmost graph shows the impact of varying LoRA rank on model accuracy, the middle graph displays how accuracy changes with different training dataset sizes, and the rightmost graph illustrates the effect of altering the quality score threshold.  Only the average accuracy, as measured by the VDCSCORE metric, is shown in the main figure for brevity;  more detailed results are available in the supplementary materials.  For all experiments, only one hyperparameter was changed at a time, while the others were held constant at their default values: LoRA rank = 256, training dataset size = 20k, and quality score threshold = 3.5.  The selection policy used was a scorer-based approach with a threshold applied to the quality score.", "section": "3.2. Ensemble Multi-models' Advantages and Human Preference"}, {"figure_path": "https://arxiv.org/html/2503.09279/x6.png", "caption": "Figure 5: \nHuman evaluation results. Our method, Cockatiel-13B, is obviously more human-preferred compared to baselines.", "description": "This figure presents the results of a human evaluation comparing Cockatiel-13B to other state-of-the-art video detailed captioning models.  The evaluation focused on human preference, assessing which model's captions were deemed more aligned with human expectations.  The results clearly show that Cockatiel-13B outperforms the other models, indicating its superior ability to generate human-preferred captions.", "section": "4. Results & Analysis"}, {"figure_path": "https://arxiv.org/html/2503.09279/x7.png", "caption": "Figure 6: \nA snapshot of our annotation user interface, where the whole annotation procedure is carried on.\nIn each annotation task, the annotators are present with a video on the left, a caption supposed to align with it on the top, and a question related to the detailed video-caption alignment on the right of the user interface.\n\u201cO\u201d, \u201cF\u201d, \u201cA\u201d, \u201cC\u201d, \u201cB\u201d are abbreviations for \u201cObject\u201d, \u201cobject Feature\u201d, \u201cobject Action\u201d, \u201cCamera\u201d and \u201cBackground\u201d, respectively.\nNotably, our user interface is highly praised by our annotators for its user-friendly and intuitive design, which ensures both the quality and quantity of the annotated data.", "description": "This figure shows the user interface used for annotating video captions.  The interface presents annotators with a video on the left, a caption to evaluate on top, and a question about the alignment of a specific visual element (object, object feature, object action, camera movement, or background) on the right.  The visual element is indicated by a single letter code: O, F, A, C, or B. The annotators select the most appropriate score to rate how well the caption describes that element in the video. The design of the interface was praised for being user-friendly and intuitive, leading to high-quality annotations.", "section": "3.1 Human-alignment Caption Scorer & Selector"}, {"figure_path": "https://arxiv.org/html/2503.09279/x8.png", "caption": "Figure 7: \nThe training pipeline of our scorer.\nOur selection policy is critical as its performance determines which knowledge and strengths is ensembled into Cockatiel-13B.\nAs a consequence, since our aim is to infuse Cockatiel-13B all the strengths of the base models and human preferences, we devise a selection policy with threshold setting based on our human-aligned caption quality scorer.\nTo obtain our caption quality scorer, we need human-annotated data on it or off-the-shelf models.\nSince no publicly available dataset nor model suits this need, we build them on our own, as demonstrated in this figure.", "description": "This figure illustrates the three-stage training pipeline for creating the human-aligned caption quality scorer used in the Cockatiel model.  The first stage involves human annotators scoring the quality of captions generated by three different base models (VILA-v1.5-13B, LLaVA-Video-7B, and Aria-3.5Bx8) across five visual aspects: object, object feature, object action, camera movement, and background.  The scores are used to train a scorer model, which is then used in the second stage for a selection policy. This policy chooses high-quality captions to train Cockatiel-13B, ensuring that the model learns from both the base models' strengths and human preferences. The final stage involves distilling a smaller, faster version of the Cockatiel model (Cockatiel-8B) from the larger Cockatiel-13B model.  The process emphasizes the creation of a human-aligned training dataset because no suitable pre-existing datasets or models were available.", "section": "3.1 Human-alignment Caption Scorer & Selector"}, {"figure_path": "https://arxiv.org/html/2503.09279/x9.png", "caption": "Figure 8: \nThe first qualitative case compared with three base models in the main content.\nThe caption content that is exclusively captured by our model, captured by our model and other baselines, or misaligned with the detailed visual elements in the videos are emphasized using red, yellow and green backgrounds.", "description": "This figure presents a qualitative comparison of video captions generated by the Cockatiel-13B model and three other state-of-the-art models (VILA-1.5-13B, LLaVA-Video-7B, and Aria-3.5Bx8).  The comparison focuses on a specific video depicting a scenic valley with a train.  Different color highlights are used to show which parts of the caption are uniquely generated by Cockatiel-13B, which parts are common to Cockatiel-13B and at least one other model, and which parts show misalignment with the video's detailed visual elements. This allows for a visual assessment of the model's ability to generate accurate and detailed video captions compared to existing models.", "section": "4. Results & Analysis"}]