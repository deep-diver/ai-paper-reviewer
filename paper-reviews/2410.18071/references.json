{"references": [{" publication_date": "2023", "fullname_first_author": "Josh Achiam", "paper_title": "GPT-4 technical report", "reason": "This paper is foundational as it presents GPT-4, a highly influential multimodal large language model (MLLM). Its technical report provides insights into the model's architecture and capabilities, which are relevant to the challenges and solutions discussed in the target paper regarding MLLM evaluation.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Zhe Chen", "paper_title": "How far are we to gpt-4v? Closing the gap to commercial multimodal models with open-source suites", "reason": "This work focuses on open-source multimodal LLMs, directly relevant to the paper's goal of improving MLLM evaluation methodologies. The advancements in open-source models impact the development and evaluation of new techniques.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Haodong Duan", "paper_title": "VLMEvalKit: An open-source toolkit for evaluating large multi-modality models", "reason": "As a toolkit for evaluating multi-modality models, VLMEvalKit provides a practical tool to aid the evaluation process.  It helps researchers efficiently analyze and compare the performance of various models, supporting the paper's goal to improve MLLM evaluation.", "section_number": 5}, {" publication_date": "2019", "fullname_first_author": "Jacob Devlin", "paper_title": "Bert: Pre-training of deep bidirectional transformers for language understanding", "reason": "BERT is a highly influential model in NLP, providing a critical foundation for many of the techniques used in prompt engineering and optimization. Understanding BERT's capabilities and limitations is essential to designing effective prompt optimization strategies.", "section_number": 4}, {" publication_date": "2021", "fullname_first_author": "Brian Lester", "paper_title": "The power of scale for parameter-efficient prompt tuning", "reason": "This paper explores parameter-efficient prompt tuning, a relevant technique in the context of optimizing prompts for LLMs.  Understanding parameter efficiency is crucial for developing computationally feasible prompt optimization methods for multimodal tasks.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Bohao Li", "paper_title": "Seed-Bench: Benchmarking multimodal LLMs with generative comprehension", "reason": "Seed-Bench provides a benchmark dataset for evaluating multimodal LLMs, offering a direct comparison point for the proposed TP-Eval framework.  Understanding the strengths and weaknesses of existing benchmarks is vital for developing new ones.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Xiang Lisa Li", "paper_title": "Prefix-tuning: Optimizing continuous prompts for generation", "reason": "This paper introduces prefix-tuning, a method for optimizing continuous prompts for generation in LLMs.  It is relevant to the discussion on prompt optimization techniques and helps contextualize the authors' novel approach.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Haotian Liu", "paper_title": "Mmbench: Is your multi-modal model an all-around player?", "reason": "MMBench is a significant benchmark for evaluating multimodal LLMs, providing a direct context for evaluating the performance of the TP-Eval framework. It serves as a comparative benchmark for demonstrating TP-Eval's effectiveness.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Haoyu Lu", "paper_title": "Deepseek-VL: Towards real-world vision-language understanding", "reason": "DeepSeek-VL is a prominent MLLM used in the experimental evaluation. Understanding its strengths and weaknesses helps evaluate TP-Eval's effectiveness and provides a real-world context.", "section_number": 5}, {" publication_date": "2022", "fullname_first_author": "Archiki Prasad", "paper_title": "Grips: Gradient-free, edit-based instruction search for prompting large language models", "reason": "This paper focuses on gradient-free prompt optimization, providing an alternative to traditional gradient-based methods. Exploring different prompt optimization techniques is crucial for developing effective and computationally efficient methods.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Reid Pryzant", "paper_title": "Automatic prompt optimization with \"gradient descent\" and beam search", "reason": "This work utilizes gradient-based methods for prompt optimization, a common approach in the field. It provides a baseline method to compare against the proposed method and contextualize the approach used in the target paper.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Melanie Sclar", "paper_title": "Quantifying language models' sensitivity to spurious features in prompt design or: How i learned to start worrying about prompt formatting", "reason": "This research directly addresses the issue of prompt sensitivity in LLMs, providing empirical evidence supporting the argument for prompt optimization. The findings strengthen the case for the TP-Eval framework.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Xinyu Tang", "paper_title": "Unleashing the potential of large language models as prompt optimizers: An analogical analysis with gradient-based model optimizers", "reason": "This work proposes using LLMs as prompt optimizers, a novel approach relevant to the paper's methodology. The analogical analysis of LLMs with gradient-based optimizers provides insights into their capabilities and limitations in this task.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Chengrun Yang", "paper_title": "Large Language Models as Optimizers", "reason": "This paper explores using large language models as optimizers for prompt generation and other tasks. This is directly relevant to the method used in the target paper where LLMs are used for prompt optimization.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Heng Yang", "paper_title": "Instoptima: Evolutionary multi-objective instruction optimization via large language model-based instruction operators", "reason": "This paper explores evolutionary methods for prompt optimization, offering a different approach compared to gradient-based methods.  Exploring various optimization strategies is important for developing robust methods.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Kaining Ying", "paper_title": "MMT-Bench: A comprehensive multimodal benchmark for evaluating large vision-language models towards multitask AGI", "reason": "MMT-Bench is a crucial benchmark used for evaluating multimodal LLMs. It provides a concrete dataset for demonstrating the effectiveness of the proposed TP-Eval framework and directly supports the claims made about the limitations of existing methods.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Weihao Yu", "paper_title": "MM-Vet: Evaluating large multimodal models for integrated capabilities", "reason": "MM-Vet offers another benchmark for evaluating multimodal LLMs. Its inclusion provides a broader context for assessing the performance of the proposed framework and highlights the variety of evaluation methodologies currently employed.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Xiang Yue", "paper_title": "MMMU: A massive multi-discipline multimodal understanding and reasoning benchmark for expert AGI", "reason": "MMMU is a significant benchmark for evaluating multimodal LLMs, offering a diverse range of tasks beyond those in MMT-Bench. Using this benchmark further validates the proposed method's effectiveness across diverse tasks and domains.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Pengwei Zhan", "paper_title": "Mitigating the inconsistency between word saliency and model confidence with pathological contrastive training", "reason": "This paper addresses issues related to the reliability and consistency of LLM outputs.  Addressing these issues is indirectly relevant to the paper's goal of improving evaluation methodologies, as unreliable outputs hamper accurate evaluation.", "section_number": 3}, {" publication_date": "2022", "fullname_first_author": "Tianjun Zhang", "paper_title": "Temper: Test-time prompting via reinforcement learning", "reason": "This work focuses on test-time prompting using reinforcement learning, a technique related to prompt optimization.  The insights from this research could inform the development of more effective prompt customization methods.", "section_number": 3}]}