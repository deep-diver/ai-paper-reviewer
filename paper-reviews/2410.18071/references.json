{"references": [{" publication_date": "2023", "fullname_first_author": "Josh Achiam", "paper_title": "GPT-4 technical report", "reason": "This paper is foundational as it presents the technical details of GPT-4, a highly influential multimodal large language model (MLLM).  Understanding GPT-4's capabilities and limitations is crucial for evaluating the effectiveness of any MLLM evaluation framework, such as TP-Eval.", "section_number": 3}, {" publication_date": "2024a", "fullname_first_author": "Haotian Liu", "paper_title": "Visual instruction tuning", "reason": "This paper is highly relevant because it focuses on improving MLLM performance through visual instruction tuning.  The findings and techniques presented here are directly applicable to improving the performance of models within the TP-Eval framework.", "section_number": 3}, {" publication_date": "2024b", "fullname_first_author": "Yuan Liu", "paper_title": "Mmbench: Is your multi-modal model an all-around player?", "reason": "This paper is crucial as it presents MMT-bench, a benchmark dataset heavily referenced and used in the TP-Eval paper.  The evaluation and analysis of MMT-bench within the context of TP-Eval highlights the critical need for improved prompt engineering techniques in MLLM evaluation.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Haoyu Lu", "paper_title": "Deepseek-vl: Towards real-world vision-language understanding", "reason": "This paper introduces DeepSeek-VL, one of the MLLMs used for experiments in TP-Eval.  Understanding DeepSeek-VL's capabilities and characteristics is important for interpreting the results and demonstrating TP-Eval's effectiveness.", "section_number": 5}, {" publication_date": "2022", "fullname_first_author": "Archiki Prasad", "paper_title": "Grips: Gradient-free, edit-based instruction search for prompting large language models", "reason": "This paper discusses novel techniques for prompt optimization which directly relates to the core methodology of TP-Eval.  The concepts presented, especially gradient-free optimization, help inform the choices made in the TP-Eval framework.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Reid Pryzant", "paper_title": "Automatic prompt optimization with\" gradient descent\" and beam search", "reason": "This work is vital as it proposes methods for automatic prompt optimization, a central component of TP-Eval.  The techniques presented in this paper significantly improve the efficiency and effectiveness of generating optimized prompts for MLLMs.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Melanie Sclar", "paper_title": "Quantifying language models' sensitivity to spurious features in prompt design or: How i learned to start worrying about prompt formatting", "reason": "This paper directly addresses prompt sensitivity, which is a core concern of TP-Eval.  Its findings on the impact of prompt formatting reinforce the importance of the TP-Eval framework in mitigating prompt-related biases.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Xinyu Tang", "paper_title": "Unleashing the potential of large language models as prompt optimizers: An analogical analysis with gradient-based model optimizers", "reason": "This paper offers a novel approach using LLMs as prompt optimizers.  This technique is closely related to TP-Eval's approach, making this paper highly relevant to the understanding of the proposed method.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Chengrun Yang", "paper_title": "Large Language Models as Optimizers", "reason": "This research directly addresses prompt optimization using LLMs, a critical aspect of TP-Eval's methodology.  Understanding this approach is essential to evaluating the novelty and innovation of TP-Eval.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Heng Yang", "paper_title": "Instoptima: Evolutionary multi-objective instruction optimization via large language model-based instruction operators", "reason": "This paper contributes to the body of knowledge on prompt optimization, which is central to TP-Eval.  Exploring different optimization techniques and their advantages/disadvantages helps understand the design choices made in the TP-Eval framework.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Kaining Ying", "paper_title": "MMT-bench: A comprehensive multimodal benchmark for evaluating large vision-language models towards multitask agi", "reason": "This work is vital as it introduces MMT-Bench, a critical benchmark dataset used in the TP-Eval paper. Understanding the structure, limitations, and challenges of MMT-Bench is fundamental for comprehending the contributions of TP-Eval.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Weihao Yu", "paper_title": "MM-Vet: Evaluating large multimodal models for integrated capabilities", "reason": "This paper introduces MM-Vet, another benchmark dataset relevant to MLLM evaluation. Comparing MM-Vet with MMT-Bench clarifies the scope and specific contributions of TP-Eval.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Xiang Yue", "paper_title": "MMMU: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi", "reason": "This work introduces MMMU, a benchmark dataset used in the TP-Eval experiments. Understanding the characteristics of MMMU helps analyze the generalizability and performance of TP-Eval across diverse MLLM evaluation tasks.", "section_number": 5}, {" publication_date": "2022", "fullname_first_author": "Pengwei Zhan", "paper_title": "Mitigating the inconsistency between word saliency and model confidence with pathological contrastive training", "reason": "This paper addresses issues related to prompt engineering and model reliability, which are indirectly related to the goals of TP-Eval.  Understanding these issues provides a more complete context for the challenges addressed by TP-Eval.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Pengwei Zhan", "paper_title": "Contrastive learning with adversarial examples for alleviating pathology of language model", "reason": "This work investigates the pathology of language models, which is relevant to understanding the challenges in accurately evaluating MLLMs.  These insights contribute to understanding the complexities and limitations of existing benchmarks.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Pengwei Zhan", "paper_title": "Rethinking word-level adversarial attack: The trade-off between efficiency, effectiveness, and imperceptibility", "reason": "This paper explores adversarial attacks on language models, which is related to the robustness and reliability of MLLM evaluations.  It provides a deeper understanding of the nuances of model behavior in response to various inputs and prompts.", "section_number": 3}, {" publication_date": "2021", "fullname_first_author": "Brian Lester", "paper_title": "The power of scale for parameter-efficient prompt tuning", "reason": "This paper explores techniques for parameter-efficient prompt tuning, which relates to the optimization aspect of TP-Eval. This enhances understanding of the methods used to optimize prompts for improved performance.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Bohao Li", "paper_title": "Seed-bench: Benchmarking multimodal llms with generative comprehension", "reason": "This paper introduces another benchmark dataset, Seed-Bench, relevant to MLLM evaluation.  Understanding Seed-Bench further clarifies the specific contributions of TP-Eval compared to other existing benchmarks.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Xiang Lisa Li", "paper_title": "Prefix-tuning: Optimizing continuous prompts for generation", "reason": "This paper presents the prefix-tuning technique for prompt optimization.  The ideas and techniques presented are relevant to the prompt optimization techniques used in TP-Eval.", "section_number": 3}, {" publication_date": "2022", "fullname_first_author": "Tianjun Zhang", "paper_title": "Test-time prompting via reinforcement learning", "reason": "This paper discusses reinforcement learning techniques for prompt optimization.  Understanding reinforcement learning approaches to prompt tuning helps contextualize the method used in TP-Eval.", "section_number": 3}]}