{"references": [{" publication_date": "2023", "fullname_first_author": "Josh Achiam", "paper_title": "GPT-4 technical report", "reason": "This paper introduces GPT-4, a highly influential multimodal large language model (MLLM).  Its technical report provides valuable insights into the model's architecture and capabilities, which are directly relevant to the evaluation methods discussed in the target paper.  The paper's impact on the field and its detailed description of a leading MLLM make it crucial for understanding the context of the research.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Zhe Chen", "paper_title": "How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites", "reason": "This paper focuses on the evaluation of MLLMs using open-source suites. Its findings on evaluation methodologies and comparisons of commercial and open-source models directly relate to the core issues addressed in the target paper.  The focus on open-source options adds a relevant practical aspect to the evaluation discussions.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Haodong Duan", "paper_title": "VLMEvalKit: An open-source toolkit for evaluating large multi-modality models", "reason": "This paper introduces a valuable tool for evaluating multi-modality models, specifically addressing the practical challenges of automated prompt customization for the assessment of models.  The open-source nature of this toolkit makes it directly relevant to the reproducibility and potential usability of the methods proposed by the target paper.", "section_number": 5}, {" publication_date": "2019", "fullname_first_author": "Jacob Devlin", "paper_title": "Bert: Pre-training of deep bidirectional transformers for language understanding", "reason": "BERT is a foundational model in NLP, with significant impacts on subsequent multimodal models' performance.  The target paper uses BERT for semantic similarity calculations; understanding BERT's architecture is key to evaluating this approach.  Its widespread use and impact make it essential for the methodological discussion.", "section_number": 4}, {" publication_date": "2021", "fullname_first_author": "Brian Lester", "paper_title": "The power of scale for parameter-efficient prompt tuning", "reason": "This paper explores parameter-efficient prompt tuning, directly addressing the problem of prompt optimization.  Understanding efficient prompt tuning is critical for evaluating the effectiveness of the proposed methods and their potential for broad application, especially given the computational demands of MLLM optimization.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Bohao Li", "paper_title": "SEED-Bench: Benchmarking multimodal LLMs with generative comprehension", "reason": "This paper focuses on a benchmark for multimodal LLMs that is crucial for the comparison of different prompt engineering methods. It provides a basis to contextualize the proposed TP-Eval's approach and assess its benefits in relation to existing benchmarks.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Xiang Lisa Li", "paper_title": "Prefix-tuning: Optimizing continuous prompts for generation", "reason": "This work is highly relevant as it presents a method of continuous prompt optimization.  It is vital to contrast the presented method with existing continuous methods for evaluating its novelty and effectiveness in the context of multi-modal evaluations.  Its approach to optimizing prompts is a key comparison point for TP-Eval.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Haotian Liu", "paper_title": "Mmbench: Is your multi-modal model an all-around player?", "reason": "This paper introduces MMT-Bench, a multimodal benchmark dataset, which the target paper uses for evaluating model capabilities. Understanding the design and limitations of this benchmark is crucial for interpreting the target paper's findings and the effectiveness of the TP-Eval method.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Yuan Liu", "paper_title": "Visual instruction tuning", "reason": "This paper on visual instruction tuning directly addresses the interaction between visual data and model prompts.  The techniques discussed are directly relevant to the prompt customization approach of the target paper, and their impact on MLLM performance is a crucial point of comparison.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Haoyu Lu", "paper_title": "Deepseek-VL: Towards real-world vision-language understanding", "reason": "This paper introduces DeepSeek, a specific MLLM that the target paper evaluates in its experiments.  Understanding the specifics of DeepSeek, including its capabilities and weaknesses, is crucial for evaluating the effectiveness and impact of the proposed TP-Eval framework.", "section_number": 5}, {" publication_date": "2022", "fullname_first_author": "Archiki Prasad", "paper_title": "Grips: Gradient-free, edit-based instruction search for prompting large language models", "reason": "This paper proposes Grips, an instruction search method. It directly addresses the prompt engineering challenges of LLMs, providing a valuable comparison point for understanding the target paper's approach to prompt customization. This paper's focus on instruction search informs the understanding of the optimization techniques employed.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Reid Pryzant", "paper_title": "Automatic prompt optimization with \"gradient descent\" and beam search", "reason": "This paper uses gradient descent for prompt optimization, a method that is relevant to the optimization procedures discussed in the target paper.  Comparing and contrasting these approaches adds valuable context to the overall methodology, highlighting the novelty and potential advantages of TP-Eval.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Melanie Sclar", "paper_title": "Quantifying language models' sensitivity to spurious features in prompt design or: How i learned to start worrying about prompt formatting", "reason": "This research directly addresses the impact of prompt variations on language model performance.  It provides a theoretical foundation for the concerns regarding prompt sensitivity that the target paper addresses, thereby enhancing the overall context and significance of the research.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Kaining Ying", "paper_title": "MMT-bench: A comprehensive multimodal benchmark for evaluating large vision-language models towards multitask AGI", "reason": "MMT-Bench is a significant benchmark dataset used in the experiments of the target paper.  Understanding its design, scope, and limitations is crucial for interpreting the results and evaluating the impact of TP-Eval on MLLM evaluation.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Weihao Yu", "paper_title": "MM-VET: Evaluating large multimodal models for integrated capabilities", "reason": "This paper introduces MM-VET, another relevant benchmark for evaluating multimodal models. Its focus on integrated capabilities highlights a different aspect of MLLM evaluation, providing additional context for understanding the scope and limitations of various approaches to MLLM evaluation.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Xiang Yue", "paper_title": "MMMU: A massive multi-discipline multimodal understanding and reasoning benchmark for expert AGI", "reason": "The paper introduces MMMU, a key benchmark dataset used in the target paper's experiments.  MMMU's focus on multi-disciplinary tasks provides additional context for understanding the broader implications of the proposed TP-Eval framework and its impact on evaluation methodologies.", "section_number": 5}, {" publication_date": "2022", "fullname_first_author": "Pengwei Zhan", "paper_title": "Mitigating the inconsistency between word saliency and model confidence with pathological contrastive training", "reason": "This paper focuses on the inconsistencies between word saliency and model confidence.  Understanding the challenges associated with model reliability and its relationship to prompt engineering is vital for evaluating the effectiveness of the proposed TP-Eval framework.  It addresses a crucial underlying issue related to prompt interpretation by LLMs.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Pengwei Zhan", "paper_title": "Contrastive learning with adversarial examples for alleviating pathology of language model", "reason": "This work explores the use of adversarial examples in improving language model performance.  This addresses a challenge that is indirectly relevant to the target paper, showcasing similar challenges in improving the model's ability to generate responses based on prompts.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Pengwei Zhan", "paper_title": "Rethinking word-level adversarial attack: The trade-off between efficiency, effectiveness, and imperceptibility", "reason": "This paper explores the challenges in adversarial attacks, a topic relevant to the target paper's focus on improving the robustness and reliability of MLLM evaluation methods.  The considerations of efficiency and imperceptibility are also directly relevant to the prompt engineering challenge addressed.", "section_number": 3}, {" publication_date": "2022", "fullname_first_author": "Tianjun Zhang", "paper_title": "TemperA: Test-time prompting via reinforcement learning", "reason": "This paper explores test-time prompting using reinforcement learning, a technique directly relevant to the automatic prompt customization method in TP-Eval.  The use of reinforcement learning for prompt optimization provides a crucial point of comparison for the target paper's approach, demonstrating potential avenues for improvement and highlighting related challenges.", "section_number": 3}]}