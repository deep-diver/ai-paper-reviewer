[{"figure_path": "2410.18071/tables/table_3_0.html", "caption": "Table 1: Similar prompt changes have different effects on two models for helmet anomaly detection task in MMT-Bench.", "description": "The table shows that small changes in prompt phrasing can significantly impact the performance of different multimodal large language models (MLLMs) on the same task, highlighting the issue of prompt sensitivity.", "section": "2.1 ANALYSIS FOR EXISTING BENCHMARKS"}, {"figure_path": "2410.18071/tables/table_7_0.html", "caption": "Table 2: Overall result for MMT-S. All three models exhibited significant performance improvements across a substantial number of tasks following prompt customization.", "description": "Table 2 presents the overall performance of three models on the MMT-S benchmark before and after prompt customization using TP-Eval, showing significant improvements in several tasks.", "section": "5.2 MAIN RESULTS"}, {"figure_path": "2410.18071/tables/table_10_0.html", "caption": "Table 3: Zero-shot prompt optimization utilizing In-context Learning.", "description": "The table shows the original prompt accuracy and the accuracy after zero-shot and few-shot prompt optimization for three tasks from the MMT-S benchmark using In-context Learning.", "section": "5.4 ZERO-SHOT EXPLORATION"}]