[{"figure_path": "2410.18071/tables/table_3_0.html", "caption": "Table 1: Similar prompt changes have different effects on two models for helmet anomaly detection task in MMT-Bench.", "description": "The table shows how similar prompt changes yield different accuracy results for two different models (LLaVA and DeepSeek) on the same task within the MMT-Bench benchmark.", "section": "2 MULTIMODAL LARGE LANGUAGE MODEL EVALUATION"}, {"figure_path": "2410.18071/tables/table_7_0.html", "caption": "Table 2: Overall result for MMT-S. All three models exhibited significant performance improvements across a substantial number of tasks following prompt customization.", "description": "Table 2 presents the overall performance of three models on the MMT-S benchmark before and after prompt customization, showing significant improvements across many tasks.", "section": "5.2 MAIN RESULTS"}, {"figure_path": "2410.18071/tables/table_10_0.html", "caption": "Table 3: Zero-shot prompt optimization utilizing In-context Learning.", "description": "Table 3 shows the performance of zero-shot prompt optimization using in-context learning on three tasks from MMT-S for LLaVA, comparing the original prompt, zero-shot optimized prompt and few-shot optimized prompt.", "section": "5.4 ZERO-SHOT EXPLORATION"}]