{"importance": "This paper is crucial for researchers working on multimodal large language models (MLLMs). It highlights a critical issue of prompt sensitivity in current MLLM benchmarks, which often leads to underestimation of model capabilities. The proposed TP-Eval framework offers a novel solution by customizing prompts for different models, leading to more accurate and reliable evaluations. This opens new avenues for research in MLLM evaluation, paving the way for more comprehensive and robust benchmarks.", "summary": "TP-Eval unveils the hidden potential of MLLMs by customizing prompts to mitigate evaluation bias caused by prompt sensitivity, leading to a more accurate assessment of model capabilities.", "takeaways": ["Current MLLM benchmarks suffer from prompt sensitivity, leading to underestimation of model capabilities.", "TP-Eval, a novel evaluation framework, addresses this by customizing prompts for individual models.", "Experiments demonstrate TP-Eval's effectiveness in uncovering models' true potential and mitigating evaluation bias."], "tldr": "Many current methods for evaluating Multimodal Large Language Models (MLLMs) are flawed because small changes to the way questions are phrased (the 'prompt') can significantly impact the results.  This paper points out this \"prompt sensitivity\" problem.  It then introduces TP-Eval, a new evaluation system that solves this by automatically creating customized prompts for each model.  The idea is that different models respond better to different question styles. By tailoring the prompts, TP-Eval provides a more accurate and fair way to compare how well different MLLMs perform. Experiments show TP-Eval is successful at uncovering models' true abilities, providing more reliable benchmark scores and highlighting the importance of prompt engineering in MLLM evaluation."}