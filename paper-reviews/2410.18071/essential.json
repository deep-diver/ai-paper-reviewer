{"reason": "TP-Eval is a novel evaluation framework for multimodal large language models (MLLMs) that customizes prompts to reduce evaluation bias and uncover models' true capabilities.", "summary": "TP-Eval enhances MLLM evaluation by customizing prompts for each model, reducing bias and revealing true capabilities previously hidden by prompt sensitivity.", "takeaways": ["Existing MLLM benchmarks suffer from prompt sensitivity, leading to underestimation of model performance.", "TP-Eval introduces a prompt customization method to mitigate evaluation bias and improve accuracy.", "Experiments on various MLLMs and benchmarks demonstrate TP-Eval's effectiveness in uncovering models' true potential."], "tldr": "Multimodal Large Language Models (MLLMs) are powerful, but their evaluation is hindered by prompt sensitivity.  Small changes in the questions asked can dramatically affect an MLLM's performance, masking its true capabilities and introducing bias if the same prompt is used for all models. This paper introduces TP-Eval, a new evaluation framework that addresses this problem.  TP-Eval uses a customized prompt for each MLLM, generating optimal prompts through an automated optimization process that considers both text and images. Experiments show that TP-Eval significantly improves the accuracy and fairness of MLLM evaluation, revealing hidden capabilities and reducing bias.  This approach is particularly useful for benchmarks with limited data, a common characteristic of MLLM evaluations. The findings are relevant to researchers working on MLLM development and evaluation, offering a more robust and reliable method for assessing model performance and highlighting the importance of prompt engineering in the field."}