[{"figure_path": "2410.18071/charts/charts_7_0.png", "caption": "Figure 3: Results of different models on MMT-S (L2-category). Accuracy improvement is calculated by accuracy using the optimized prompt divided by accuracy using the original prompt. Three models showed varying improvement across different task types, while performance gains differ between models, highlighting the underestimation and bias introduced by original prompts and the effectiveness of our method.", "description": "The chart displays the accuracy improvement percentage for three different models across various tasks in the MMT-S benchmark after prompt optimization.", "section": "5.2 MAIN RESULTS"}, {"figure_path": "2410.18071/charts/charts_8_0.png", "caption": "Figure 4: Overall performance with different prompt methods on MMMU with LLaVA. In most cases, the results after optimization surpass those achieved with the initial prompts, and they generally outperform the original questions as well.", "description": "The chart compares the overall performance of LLaVA on MMMU using original questions, initial prefix prompts, and optimized prefix prompts, showing improved accuracy with optimized prompts across different disciplines.", "section": "5.2 Main Results"}, {"figure_path": "2410.18071/charts/charts_8_1.png", "caption": "Figure 5: Result of applying optimized prompts to other models. Applying customized prompts from one model to another yields performance changes that differ from each model\u2019s inherent characteristics.", "description": "Figure 5 is a heatmap showing the performance changes when applying prompts optimized for one model to other models, highlighting the model-specific nature of optimal prompts.", "section": "5.2.2 OPTIMALITY ANALYSIS"}, {"figure_path": "2410.18071/charts/charts_9_0.png", "caption": "Figure 6: Performance on whether to use introspection or not.", "description": "The chart compares the performance of three different prompt optimization methods (original, no introspection, and the proposed method) on three tasks from the MMT-S benchmark.", "section": "5.3 ABLATION STUDY"}, {"figure_path": "2410.18071/charts/charts_9_1.png", "caption": "Figure 7: Influence of re-ranking. Both excessively high and low a* can lead to a reduction in performance, and each model achieves optimal performance with a* \u2208 [0.5, 0.6].", "description": "The chart displays the effect of the re-ranking parameter (a*) on the accuracy of three different MLLMs.", "section": "5.3 ABLATION STUDY"}]