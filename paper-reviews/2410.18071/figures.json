[{"figure_path": "2410.18071/figures/figures_1_0.png", "caption": "Figure 1: (a) shows underestimation caused by unsuitable prompts in MMT-Bench, (b) shows our proposed evaluation framework resolving this by customizing prompts.", "description": "Figure 1 illustrates the prompt sensitivity problem in existing MLLM benchmarks and the proposed TP-Eval framework that addresses this by customizing prompts for different models.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.18071/figures/figures_5_0.png", "caption": "Figure 2: The overview of our automatic prompt customization structure.", "description": "The figure illustrates the overall framework of TP-Eval's automatic prompt customization method, showing the interaction between the optimizer, scorer, and answer analyzer modules to generate optimal prompts for different MLLMs.", "section": "4 METHOD"}]