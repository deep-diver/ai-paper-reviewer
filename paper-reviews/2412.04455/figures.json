[{"figure_path": "https://arxiv.org/html/2412.04455/x2.png", "caption": "Figure 1: \nFor the task \u201cMove the pan with lobster to the stove without losing the lobster\u201d, (a) reactive failure detection identifies failures after they occur, and (b) proactive failure detection prevents foreseeable failures.\nIn (a), at t4Rsubscriptsuperscript\ud835\udc61\ud835\udc454t^{R}_{4}italic_t start_POSTSUPERSCRIPT italic_R end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT, the robot detects the failure after the lobster unpredictably jumps out due to the heat.\nIn (b), pan tilting is detected at t3Psubscriptsuperscript\ud835\udc61\ud835\udc433t^{P}_{3}italic_t start_POSTSUPERSCRIPT italic_P end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT and corrected it at t3P\u2032subscriptsuperscript\ud835\udc61superscript\ud835\udc43\u20323t^{P^{\\prime}}_{3}italic_t start_POSTSUPERSCRIPT italic_P start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT, requiring real-time precision.\nWe formulate both tasks as spatio-temporal constraint satisfaction problems, leveraging our proposed constraint elements for precise, real-time checking.\nFor example, in (a), a large relative distance between pan and lobster indicates failure; in (b), a large angle between the pan and the horizontal plane needs correction.\n(c) shows that our method combined with an open-loop policy forms a closed-loop system, enabling proactive (e.g., detecting moving glass during grasping) and reactive (e.g., removing toy after grasping) failure detection in cluttered scenes.", "description": "Figure 1 demonstrates different failure detection methods for a robotic task: moving a pan with a lobster to a stove without dropping the lobster.  (a) shows reactive detection where a failure (lobster jumping out) is identified after it occurs. (b) illustrates proactive detection, where a potential failure (pan tilting) is detected and corrected before it causes a problem. Both scenarios use spatio-temporal constraints to identify and resolve issues.  (c) shows the method integrated into a closed-loop system, which allows both proactive and reactive failure detection even in complex scenarios.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2412.04455/x3.png", "caption": "Figure 2: \nOverview of Code-as-Monitor. Given task instructions and prior information, the Constraint Generator derives the next subgoal and corresponding textual constraints based on multi-view observations.\nThe Painter maps these constraints onto images as constraint elements.\nThe Monitor generates monitor code from these images and tracks them for real-time monitoring.\nIf any constraint is violated, it outputs the reason for failure and triggers re-planning.\nThis framework unifies reactive and proactive failure detection via constraints, more generally abstracts relevant entities/parts through constraint elements, and ensures precise and real-time monitoring via code evaluation.", "description": "The figure illustrates the Code-as-Monitor (CaM) framework, which unifies reactive and proactive failure detection in robotics.  Given a task instruction (e.g., \"Move the pan with the bread to the stove without dropping the bread\"), the Constraint Generator analyzes multi-view observations and determines the next subgoal and its associated textual constraints. The Painter module converts these textual constraints into visual representations (constraint elements) overlaid on the images.  The Monitor module then uses these visual elements to generate and execute code that performs real-time monitoring for constraint satisfaction. If a constraint is violated, the Monitor outputs the reason for failure, triggering re-planning.  The use of constraint elements simplifies the monitoring process and enhances the generality and real-time performance of the system.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2412.04455/x4.png", "caption": "Figure 3: Constraint Element Pipeline. Given a constraint, our model ConSeg generates instance-level and part-level masks across multiple views, which are projected into 3D space.\nThrough a series of heuristics, the desired elements are produced. Once all elements are obtained, they are annotated onto the original multi-view images. Here we display the annotation result of one element.", "description": "The figure illustrates the process of generating constraint elements from multi-view images.  First, the ConSeg model generates instance-level and part-level masks from multiple camera views. These masks are then projected into 3D space. A series of heuristics are applied to the 3D point cloud to identify and extract the desired constraint elements. Finally, these elements are annotated back onto the original multi-view images for visualization and use in subsequent monitoring tasks.  The figure shows the annotation result for a single constraint element.", "section": "3.2. Constraint Element"}, {"figure_path": "https://arxiv.org/html/2412.04455/x5.png", "caption": "Figure 4: ConSeg architecture. Here we display the part-level segmentation, which will output the desired element type and mask.", "description": "The figure showcases the architecture of ConSeg, a model designed for constraint-aware part-level segmentation.  It takes as input a textual constraint describing the desired object part and a corresponding image.  The model processes the image via a vision encoder and uses a vision-language model (VLM) to generate an embedding based on the textual constraint and the visual features. This embedding is used by a decoder to produce the segmentation mask identifying the specific part of the object that satisfies the constraint. The output includes both the mask and a description (element type) of the segmented part. This method ensures precise segmentation of only the relevant parts required for constraint satisfaction, aiding in visual programming.", "section": "3.2 Constraint Element"}, {"figure_path": "https://arxiv.org/html/2412.04455/x6.png", "caption": "Figure 5: Example of Real-world Evaluation.\nThe red bounding box shows the current grasp target, which may shift due to environmental changes. CaM monitors and adapts to these changes in real-time, resulting in a closed-loop system with an open-loop policy.", "description": "The figure showcases a real-world robotic manipulation task where the robot's grasp target dynamically changes due to environmental factors.  The red bounding box highlights this shifting target.  The Code-as-Monitor (CaM) system actively tracks these changes in real-time and adapts accordingly. This creates a closed-loop system, effectively combining CaM's reactive and proactive failure detection with an existing open-loop control policy.", "section": "4.3. Main Results in Real World"}, {"figure_path": "https://arxiv.org/html/2412.04455/x7.png", "caption": "Figure 6: Visual comparison between our ConSeg and LISA\u00a0[28] at instance and part level. The red masks are the segmentation results.", "description": "Figure 6 presents a detailed comparison of the segmentation results produced by the proposed ConSeg model and the LISA [28] model.  It showcases both instance-level and part-level segmentations.  The visual comparison allows for a direct assessment of the accuracy and granularity of each model's output in identifying objects and their constituent parts within the image.  Red masks highlight the segmentation results generated by both methods, enabling an easy side-by-side comparison of their performance. This comparative analysis highlights the advantages of the proposed ConSeg model in terms of accuracy and detailed part segmentation.", "section": "4.4. Main Results of Segmentation"}, {"figure_path": "https://arxiv.org/html/2412.04455/x8.png", "caption": "Figure 7: Dataset Collection Pipeline. Our data is sourced from BridgeData V2\u00a0[32]. The data collection process consists of three steps: (1) Using GPT-4o\u00a0[1] to decompose the task instruction based on the initial observation from the first frame of the trajectory, generating subgoals along with two types of constraints for each subgoal (i.e., constraints during execution and upon completion) and object-part associations. (2) Utilizing external references (e.g., gripper open/close states) to assign subgoals, constraints, and object-part associations to each frame. (3) Leveraging off-the-shelf models (e.g., Grounded SAM\u00a0[51], Semantic SAM\u00a0[33]) to generate instance- and part-level masks\u00a0(blue mask in this figure) automatically, followed by manual filtering to curate the final dataset.", "description": "This figure illustrates the three-step process for collecting the dataset used in the paper.  First, GPT-4 is used to analyze each trajectory from the BridgeData V2 dataset [32], breaking down the task instructions into individual subgoals, identifying constraints (both during execution and upon completion), and defining the object-part relationships involved in each subgoal. Second, using external information, such as gripper states, each frame of the trajectory is linked to the corresponding subgoal, constraints, and object-part associations. Finally, pre-trained models (Grounded SAM [51] and Semantic SAM [33]) automatically generate instance and part-level segmentation masks. Manual review and filtering are then used to refine the dataset, removing any errors or low-quality annotations.", "section": "A. Constraint Painter"}, {"figure_path": "https://arxiv.org/html/2412.04455/x9.png", "caption": "Figure 8: Environmental Setup of three simulators and one real-world setting.\nFor CLIPort\u00a0[56] and OmniGibson\u00a0[32], we provide third-person front and top views and the robot platforms are the UR5 arm and Fetch, respectively.\nRLBench\u00a0[24] offers four camera views, including front left shoulder, right shoulder, and wrist views, with the robot platform being Franka equipped with a gripper.\nWe provide a wrist and a third-person front view for the real-world setting, utilizing a UR5 robot equipped with a Leap Hand\u00a0[53].", "description": "Figure 8 illustrates the experimental setups used in the paper, encompassing three simulation environments (CLIPort, OmniGibson, RLBench) and a real-world setting.  The simulation environments are depicted with their respective robot arms (UR5 for CLIPort and OmniGibson, Franka for RLBench), camera views (third-person front and top for CLIPort and OmniGibson; front left shoulder, right shoulder, wrist for RLBench), and the objects being manipulated. The real-world setup shows a UR5 arm with a Leap Hand, utilizing both wrist and third-person front views. This figure highlights the consistency of the experimental methodology across simulations and real-world testing.", "section": "4. Experimental Setup"}, {"figure_path": "https://arxiv.org/html/2412.04455/x10.png", "caption": "Figure 9: \nCLIPort task demonstration. we present three types of tasks in our experiments, including the starting and ending frames.", "description": "This figure shows example image sequences from three different tasks in the CLIPort environment.  Each task demonstrates the robot performing a manipulation action from start to finish. The purpose is to illustrate the variety of manipulation tasks used to evaluate the proposed approach. The three tasks are: (a) Stack in Order, (b) Sweep Half the Blocks, and (c) Use Rope to Close the Opening Square.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.04455/x11.png", "caption": "Figure 10: \nOmnigibson task demonstration. we present three types of tasks in our experiments, including the starting and ending frames.", "description": "This figure showcases three distinct robotic manipulation tasks from the Omnigibson simulator used in the paper's experiments.  Each task is visually represented with a sequence of images showing the initial and final states. The tasks demonstrate the diversity of manipulation challenges addressed by the proposed method.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.04455/x12.png", "caption": "Figure 11: \nRLBench task demonstration. we present six types of tasks in our experiments, including the starting and ending frames.", "description": "This figure showcases six diverse manipulation tasks from the RLBench benchmark, illustrating the types of challenges addressed in the paper. Each task includes a starting and ending frame, providing a visual representation of the robot's actions and the complexity involved. The tasks encompass a range of manipulation skills, including articulated object interactions (e.g., opening drawers, inserting objects), rotational manipulations (e.g., turning taps, screwing bulbs), and tool use (e.g., dragging with a stick, sweeping with a dustpan).", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.04455/x13.png", "caption": "Figure 12: \nDemonstration of \u201cStack in Order\u201d. We show how our framework detects failures and assists in recovery when the placement positions predicted by the policy for the \u201cStack in Order\u201d task are subject to a uniform [0,q]0\ud835\udc5e[0,q][ 0 , italic_q ] cm interference. Red boxes indicate the occurrence of failures, while green boxes signify successful task execution.", "description": "This figure demonstrates the 'Stack in Order' task, where the robot stacks blocks in a specific order (red, green, blue).  The experiment introduces a disturbance: the placement position of each block is randomly perturbed by a small amount (up to q cm). The figure showcases how the proposed framework detects when a block is not placed correctly (reactive failure detection). Red boxes highlight these failures, indicating that the robot failed to satisfy the specified spatial constraints.  The framework's ability to recover from these failures is also demonstrated (proactive failure detection), with green boxes marking successful placements after correction. This illustrates the system's ability to address errors and ensure successful task completion.", "section": "Main Results in Simulator"}, {"figure_path": "https://arxiv.org/html/2412.04455/x14.png", "caption": "Figure 13: \nDemonstration of \u201cStack in Order\u201d. We show how our framework performs failure detection and aids in recovery when, in the \u201cStack in Order\u201d task, there is a probability p\ud835\udc5dpitalic_p that blocks will fall due to being released by the robot\u2019s suction cup at each step. Red boxes indicate the occurrence of failures, while green boxes signify successful task execution.", "description": "This figure demonstrates the \"Stack in Order\" task, where a robot stacks blocks.  Failures are simulated by randomly releasing blocks (with probability p) from the robot's suction cup. The figure shows the robot's actions, highlighting failures (red boxes) and successful steps (green boxes). The framework's ability to detect failures and recover is showcased.", "section": "Main Results in Simulator"}, {"figure_path": "https://arxiv.org/html/2412.04455/x15.png", "caption": "Figure 14: \nDemonstration of \u201cSweep Half the Blocks\u201d and comparison to baseline. We show our framework can precisely count the blocks within a specified area and timely halts the policy execution to complete the task. In contrast, DoReMi\u00a0[16] fails to stop the policy execution in time, leading to task failure. Red boxes indicate the occurrence of failures, while green boxes signify successful task execution.", "description": "The figure shows a comparison of the proposed method and a baseline method (DoReMi) for the task of sweeping half of the blocks into a specified area. The proposed method accurately counts the number of blocks in the area and stops the robot when the task is completed, preventing task failure. In contrast, the baseline method fails to stop the robot in time, causing it to continue sweeping blocks beyond the specified number, which results in task failure. Red boxes highlight instances of failure, whereas green boxes indicate successful task execution.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.04455/x16.png", "caption": "Figure 15: \nDemonstration of \u201cUse Rope to Close the Opening Square\u201d and comparison to baseline. We show that our framework effectively detects when the rope closes the opening square and promptly stops the policy execution to complete the task successfully. Conversely, DoReMi fails to halt the policy execution on time; although it eventually succeeds in closing the opening, the excessive execution time results in task failure. Red boxes indicate the occurrence of failures, while green boxes signify successful task execution.", "description": "The figure demonstrates a comparison between the proposed Code-as-Monitor (CaM) framework and the DoReMi baseline in performing the \"Use Rope to Close the Opening Square\" task.  CaM effectively detects when the rope successfully closes the square and stops the robot's actions promptly, completing the task efficiently.  In contrast, DoReMi, while eventually successful, fails to stop the robot in a timely manner, resulting in a significant increase in execution time. The use of red and green boxes in the figure highlight task failures and successes, respectively, for a clear visual comparison of the two methods.", "section": "Main Results in Simulator"}, {"figure_path": "https://arxiv.org/html/2412.04455/x17.png", "caption": "Figure 16: \nDemonstration of \u201cSlot Pen\u201d. We show how our framework detects failures and assists in recovery when facing point-level disturbances. Red boxes indicate the occurrence of failures, light green indicates the recovery with subgoal success and dark green boxes signify successful task execution.", "description": "This figure demonstrates the \"Slot Pen\" task, showing how the Code-as-Monitor framework handles point-level disturbances.  The sequence of images illustrates the robot's actions, highlighting failures (red boxes), successful recovery after a failure (light green boxes), and successful task completion (dark green boxes). Point-level disturbances include actions such as the pen moving unexpectedly or dropping during manipulation. The figure showcases how the system identifies these failures and takes corrective actions to successfully complete the task.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.04455/x18.png", "caption": "Figure 17: \nDemonstration of \u201cStow Book\u201d. We show how our framework detects failures and assists in recovery when facing line-level disturbances. Red boxes indicate the occurrence of failures, light green indicates the recovery with subgoal success and dark green boxes signify successful task execution.", "description": "The figure showcases the framework's ability to detect and handle failures during the \"Stow Book\" task, specifically when line-level disturbances occur.  The process is illustrated step-by-step, highlighting the different stages.  Red boxes highlight instances where the system identifies a failure; light green boxes indicate successful recovery after a failure; dark green boxes indicate successful task completion without encountering any failures.", "section": "4.2.2 Results in Omnigibson"}, {"figure_path": "https://arxiv.org/html/2412.04455/x19.png", "caption": "Figure 18: \nDemonstration of \u201cPour Tea\u201d. We show how our framework detects failures and assists in recovery when facing surface-level disturbances. Red boxes indicate the occurrence of failures, light green indicates the recovery with subgoal success and dark green boxes signify successful task execution.", "description": "The figure showcases the different stages involved in the \u201cPour Tea\u201d task, highlighting the capabilities of the proposed framework in handling surface-level disturbances. The initial state shows the robot and the objects involved. Subsequent steps demonstrate the robot's actions, with red boxes indicating instances where failures were detected.  The light green boxes illustrate successful recovery after detecting failures, whereas the dark green boxes represent successful task completion without any failure detection.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.04455/x20.png", "caption": "Figure 19: \nDemonstration of \u201cClear all objects on the table except for animals\u201d. We show that our framework achieves both reactive failure detection (e.g., detecting unexpected failures when humans remove objects from the robot\u2019s grasp) and proactive failure detection (e.g., identifying target object movement during grasping to prevent foreseeable failures).\nThis effectively enhances the task success rate and reduces the execution time.", "description": "The figure demonstrates the robot performing the task of clearing all objects from a table except for the animals.  The robot uses a combination of reactive and proactive failure detection. Reactive detection addresses unexpected issues, like a human removing an object during the robot's grasp.  Proactive detection anticipates potential problems, such as a human moving an object while the robot is trying to grasp it. By anticipating and correcting for these issues, the system improves the task's success rate and reduces the time needed for completion.", "section": "4.3. Main Results in Real World"}, {"figure_path": "https://arxiv.org/html/2412.04455/x21.png", "caption": "Figure 20: \nVisualization of constraint-aware segmentation for the RoboFail Dataset\u00a0[41]. This dataset is not included in the training data.", "description": "This figure visualizes the results of applying constraint-aware image segmentation to the RoboFail dataset [41].  The RoboFail dataset contains images of robotic manipulation tasks with various failure scenarios.  The constraint-aware segmentation model, a key component of the Code-as-Monitor system presented in the paper, is designed to identify and segment specific regions (or elements) in the image that are relevant to the fulfillment of task constraints. Because the RoboFail dataset was not used in training the model, this visualization demonstrates the model's ability to generalize to unseen data and perform accurate segmentation of task-relevant elements even in novel scenarios.", "section": "3.2 Constraint Element"}, {"figure_path": "https://arxiv.org/html/2412.04455/x22.png", "caption": "Figure 21: \nVisualization of constraint-aware segmentation for the Open6DOF\u00a0[11]. This dataset is not included in the training data.", "description": "Figure 21 visualizes the results of applying constraint-aware segmentation to the Open6DOF benchmark dataset [11], which was not part of the model's training data. The figure showcases how the model identifies and segments relevant entities and parts of objects for various tasks (e.g., grasping, placing), highlighting its ability to generalize to unseen data and situations.", "section": "3.2 Constraint Element"}, {"figure_path": "https://arxiv.org/html/2412.04455/x23.png", "caption": "Figure 22: \nVisualization of constraint-aware segmentation for the RT-1 dataset\u00a0[3]. This dataset is not included in the training data.", "description": "This figure visualizes the results of applying constraint-aware segmentation to images from the RT-1 dataset [3], which is a real-world robotic manipulation dataset. The RT-1 dataset was not used during the training of the constraint-aware segmentation model.  The visualization shows the model's ability to identify and segment relevant objects and parts in images that were not seen during the model's training, demonstrating its generalization capabilities to new, unseen data. Each sub-figure presents a task from the dataset and the resulting segmentation masks generated by the model, highlighting the model's ability to accurately identify and isolate objects and parts based on the given constraints.", "section": "3.2 Constraint Element"}, {"figure_path": "https://arxiv.org/html/2412.04455/x24.png", "caption": "Figure 23: \nVisualization of constraint-aware segmentation for the Omnigibsom simulator.", "description": "The figure displays the results of applying constraint-aware segmentation to the Omnigibson simulator.  It shows the instance-level and part-level segmentations for various tasks within the simulator. Each task is decomposed into subgoals and constraints, and the visualizations highlight how the model identifies and segments the relevant objects and their parts according to those constraints.", "section": "3.2 Constraint Element"}]