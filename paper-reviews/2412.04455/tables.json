[{"content": "| Tasks with | Success Rate(%) \u2191 | Execution Time(s) \u2193 |\n|---|---|---|\n| disturbance | CP | +IM | +DRM | +Ours | +IM | +DRM | +Ours |\n| Stack in p=0.0 | 100.0 | 100.0 | 100.0 | 100.0 | 13.4 | 13.4 | 13.4 |\n| order with p=0.15 | 56.67 | 81.67 | 83.33 | **95.00** | 34.8 | 26.00 | **21.0** |\n| drop p=0.3 | 21.67 | 75.00 | 76.67 | **88.33** | 42.8 | 34.20 | **25.4** |\n| Stack in q=1 | 90.00 | 90.00 | 96.67 | **98.33** | 24.8 | 24.6 | **24.2** |\n| order with q=2 | 41.67 | 71.67 | 75.00 | **83.33** | 39.4 | 37.0 | **29.2** |\n| noise q=3 | 15.00 | 40.00 | 40.00 | **63.33** | 58.2 | 54.2 | **36.8** |\n| Sweep Half the Blocks | 0.00 | 18.33 | 16.67 | **75.00** | 22.0 | 16.6 | **16.4** |", "caption": "Table 1: Performance in CLIPort. We report the success rate and execution time, compared to CLIPort\u00a0(CP)\u00a0[56], with Inner Monologue\u00a0(IM)\u00a0[22] and DoReMi\u00a0(DRM)\u00a0[16].", "description": "This table presents a comparison of the success rates and execution times for three manipulation tasks in the CLIPort simulator, achieved by four different methods: the original CLIPort method, CLIPort enhanced with Inner Monologue, CLIPort with DoReMi, and the proposed Code-as-Monitor method.  The tasks involve stacking blocks, sweeping blocks, and using a rope to close a square opening, and each task is tested under varying levels of difficulty using different levels of introduced disturbances (probability of block dropping, noise level in block placement, etc.).  The results highlight the performance improvements achieved by Code-as-Monitor in terms of both higher success rates and faster execution times.", "section": "4.2 Main Results in Simulator"}, {"content": "| Method | Solt Pen (Point-level Disturbances) |  |  |  |  |  | Stow Book (Line-level Disturbances) |  |  |  |  |  | Pour Tea (Surface-level Disturbances) |  |  |  |  |  |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|  | SR with Disturbance(%) \u2191 |  |  |  | Time | Token | SR with Disturbance(%) \u2191 |  |  |  | Time | Token | SR with Disturbance(%) \u2191 |  |  |  | Time | Token |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| ReKep [23] | 30 | 20 | 10 | 10 | - | - | 40 | 30 | 30 | 20 | - | - | 20 | 20 | 20 | 10 | - | - |\n| +DRM | 40 | 10 | 20 | 20 | 177.84 | 54.54 | 50 | 40 | 20 | 40 | 127.17 | 38.67 | 0 | 0 | 0 | 0 | - | - |\n| +Ours | 60 | 50 | 40 | 40 | 101.85 | 25.82 | 70 | 60 | 70 | 60 | 93.08 | 18.67 | 50 | 40 | 40 | 30 | 174.55 | 44.19 |", "caption": "Table 2: Performance in Omnigibson. We report the success rate\u00a0(SR), execution time, and token usage, compared to DoReMi\u00a0(DRM)\u00a0[16].", "description": "This table presents a comparison of the performance of the Code-as-Monitor (CaM) system against a baseline method (DoReMi) [16] on three manipulation tasks within the Omnigibson simulator.  The tasks are designed to evaluate the system's ability to handle failures in a variety of situations, including point-level, line-level and surface-level disturbances.  For each task and disturbance condition, the table shows the success rate (SR), execution time, and the number of tokens used by each system. The success rate indicates the percentage of trials in which the task was successfully completed.  The execution time measures how long it took for the robot to complete the task.  Token usage is a measure of the computational cost associated with running each system.", "section": "4.2.2 Results in Omnigibson"}, {"content": "| Tasks with | Object | Success Rate(%) \u2191 | Success Rate(%) \u2191 | Success Rate(%) \u2191 | Execution Time(s) \u2193 | Execution Time(s) \u2193 |\n|---|---|---|---|---|---|---|\n| disturbance | types | DGN | +DRM | +**Ours** | +DRM | +**Ours** |\n| Pick & Place | Deformable | 0.00 | 83.33 | **96.67** | 61.8 | **46.3** |\n| with the | Transparent | 0.00 | 66.67 | **93.33** | 72.6 | **48.1** |\n| objects being | Small Rigid | 0.00 | 80.0 | **96.67** | 65.7 | **45.4** |\n| moved | Large Geometric | 0.00 | 86.67 | **96.67** | 68.9 | **45.3** |\n| Pick & Place | Deformable | 0.00 | 76.67 | **93.33** | 68.7 | **62.5** |\n| with the | Transparent | 0.00 | 60.00 | **90.00** | 77.7 | **62.7** |\n| objects being | Small Rigid | 0.00 | 63.33 | **93.33** | 69.8 | **60.5** |\n| removed | Large Geometric | 0.00 | 76.67 | **96.67** | 72.3 | **60.3** |", "caption": "Table 3: Performance of Single Pick & Place. We report the success rate and execution time. DGN donates DexGraspNet 2.0\u00a0[71].", "description": "This table presents the success rate and execution time of a single pick-and-place task in a real-world setting.  The task involves picking and placing objects of various types (deformable, transparent, small rigid, and large geometric) using a robot arm equipped with a dexterous hand.  DexGraspNet 2.0 [71] is used as the low-level control policy.  The results are broken down by object type and whether disturbances (moving the object during grasping, or removing the object from the hand during movement) were present.", "section": "4.3 Main Results in Real World"}, {"content": "| Tasks | w/ CE | DGN | +DRM | +Ours |\n|---|---|---|---|---|\n| Clear all objects on table except for animals | \u2713 | 0.00 | 10.00 | **60.00** |\n|  | \u2717 | 0.00 | 10.00 | **20.00** |\n| Grasp the animals according to their distances to fruits, from nearest to farthest | \u2713 | 0.00 | 0.00 | **90.00** |", "caption": "Table 4: Performance of Reasoning Pick & Place in cluttered scene. We report the success rate.\nThe robot is controlled by an open-loop policy named DexGraspNet 2.0\u00a0(DGN)\u00a0[71].\nw/ CE with \u2713\u2713\\checkmark\u2713 indicates using constraint elements; otherwise, constraint-aware entities or parts are used for tracking and code computation.", "description": "This table presents the success rates of a reasoning pick-and-place task in a cluttered scene.  The experiment uses the DexGraspNet 2.0 (DGN) open-loop control policy. Two conditions are compared: one where constraint elements are used for tracking and code generation, and another where constraint-aware entities or parts are used instead. The results show the impact of using constraint elements on task success.", "section": "4.3 Main Results in Real World"}, {"content": "| Method | ReasonSeg Instance-level |  | ConstraintSeg Instance-level |  | Part-level |  | \n|---|---|---|---|---|---|---|\n| gIoU | cIoU | gIoU | cIoU | gIoU | cIoU |\n| OVSeg [35] | 28.5 | 18.6 | 32.9 | 31.4 | 20.2 | 21.5 |\n| GRES [37] | 22.4 | 19.9 | 28.6 | 26.4 | 22.7 | 22.6 |\n| X-Decoder [78] | 22.6 | 17.9 | 27.8 | 28.0 | 23.5 | 25.1 |\n| SEEM [79] | 25.5 | 21.2 | 29.4 | 28.0 | 23.1 | 24.8 |\n| PixelLM [52] | 56.0 | 61.4 | 44.4 | 43.2 | 24.1 | 22.6 |\n| LISA-13B [28] | 56.6 | 65.1 | 42.1 | 38.9 | 23.4 | 24.3 |\n| FMC | 51.2 | 53.3 | 49.3 | 49.6 | 40.8 | 39.3 |\n| **ConSeg-13B** | 55.7 | 63.9 | **62.1** | **68.7** | **60.2** | **65.3** |", "caption": "Table 5: Performance of reasoning and constraint-aware segmentation. FMC denotes the foundation model combination baseline.", "description": "This table presents a comparison of the performance of different methods for reasoning and constraint-aware image segmentation.  The methods compared include a foundation model combination baseline (FMC) and the proposed ConSeg model. The evaluation metrics used are likely IoU (Intersection over Union) and its various forms, assessing the accuracy of the segmentation results at both the instance and part levels.", "section": "4.4. Main Results of Segmentation"}, {"content": "| MV | CS | CP | None | Dist.(a) | Dist.(b) | Dist.(c) | Avg |\n|---|---|---|---|---|---|---|---|---|\n| \u2717 | \u2713 | \u2713 | 40.0 | 40.0 | 30.0 | 50.0 | 40.0 |\n| \u2713 | \u2717 | \u2713 | 50.0 | 40.0 | 40.0 | 40.0 | 42.5 |\n| \u2713 | \u2713 | \u2717 | 60.0 | 50.0 | 60.0 | 50.0 | 55.0 |\n| \u2713 | \u2713 | \u2713 | 70.0 | 60.0 | 70.0 | 60.0 | 65.0 |", "caption": "Table 6: \nAblation studies in Omnigibson\u2019s \u201cStow Book\u201d, assessing the impact of Multi-View (MV), Constraint-aware Segmentation (CS) for elements, and Connect Points (CP) for element formation.", "description": "This table presents the results of ablation studies conducted on the \"Stow Book\" task within the Omnigibson simulator.  The study investigates the individual and combined effects of three key components of the Code-as-Monitor framework: Multi-View (MV) processing, Constraint-aware Segmentation (CS) for generating constraint elements, and the Connect Points (CP) algorithm used for creating constraint elements.  The table shows how the success rate of the task varies depending on the presence or absence of each component, providing insights into their relative contributions to overall performance.", "section": "4.5. Ablation Study"}, {"content": "| SemanticSeg | ReferSeg | VQA | ReasonSeg | ConstraintSeg |  | Ins-level |  | Part-level |  |\n|---|---|---|---|---|---|---|---|---|---| \n|  |  |  |  | Ins | Part | gIoU | cIoU | gIoU | cIoU |\n| \u2713 | \u2713 | \u2713 | \u2713 | \u2717 | \u2717 | 42.1 | 38.9 | 23.4 | 24.3 |\n| \u2713 | \u2713 | \u2713 | \u2713 | \u2713 | \u2717 | 60.7 | 65.9 | 40.4 | 45.6 |\n| \u2713 | \u2713 | \u2713 | \u2713 | \u2717 | \u2713 | 51.5 | 50.6 | 56.5 | 61.7 |\n| \u2713 | \u2713 | \u2713 | \u2713 | \u2713 | \u2713 | **62.1** | **68.7** | **60.2** | **65.3** |", "caption": "Table 7: Ablation study on training data. SemanticSeg includes ADE20K\u00a0[76], COCO-Stuff\u00a0[5], PACO-LVIS\u00a0[50] and PASCAL-Part\u00a0[6]. ReferSeg includes refCLEF, refCOCO, refCOCO+\u00a0[26] and refCOCOg\u00a0[42]. VQA indicates LLaVAInstruct-150k\u00a0[39].", "description": "This table presents the results of an ablation study investigating the impact of different training data components on the performance of the constraint-aware segmentation model, ConSeg.  The study specifically examines the contribution of instance-level and part-level segmentation data, along with the inclusion of various datasets for semantic segmentation (SemanticSeg), referring expression segmentation (ReferSeg), and visual question answering (VQA).  The goal is to determine which data sources are most crucial for achieving high performance in ConSeg.", "section": "4.5. Ablation Study"}, {"content": "| Tasks with | Success Rate(%) \u2191 | Execution Time(s) \u2193 |\n|---|---|---|\n| disturbance | CLIPort | +Inner Monologue | +DoReMi | +Ours | +Inner Monologue | +DoReMi | +Ours |\n| Stack in p=0.0 | p=0.0 | 100.00 \u00b1 0.00 | 100.00 \u00b1 0.00 | 100.00 \u00b1 0.00 | 100.00 \u00b1 0.00 | 13.40 \u00b1 1.82 | 13.40 \u00b1 1.82 | 13.40 \u00b1 1.82 |\n| order with p=0.15 | p=0.15 | 56.67 \u00b1 6.11 | 81.67 \u00b1 6.11 | 83.33 \u00b1 5.17 | **95.00 \u00b1 4.00** | 34.80 \u00b1 3.12 | 26.00 \u00b1 2.77 | **21.00 \u00b1 1.75** |\n| drop p | p=0.3 | 21.67 \u00b1 8.33 | 75.00 \u00b1 8.95 | 76.67 \u00b1 9.52 | **88.33 \u00b1 6.53** | 42.80 \u00b1 3.18 | 34.20 \u00b1 2.73 | **25.40 \u00b1 2.95** |\n| Stack in q=1 | q=1 | 90.00 \u00b1 6.11 | 90.00 \u00b1 6.11 | 96.67 \u00b1 4.00 | **98.33 \u00b1 3.27** | 24.80 \u00b1 4.08 | 24.60 \u00b1 4.66 | **24.20 \u00b1 4.65** |\n| order with q=2 | q=2 | 41.67 \u00b1 7.30 | 71.67 \u00b1 8.33 | 75.00 \u00b1 5.17 | **83.33 \u00b1 5.17** | 39.40 \u00b1 5.87 | 37.00 \u00b1 6.29 | **29.20 \u00b1 4.61** |\n| noise q | q=3 | 15.00 \u00b1 6.11 | 40.00 \u00b1 8.00 | 40.00 \u00b1 6.11 | **63.33 \u00b1 8.33** | 58.20 \u00b1 4.74 | 54.20 \u00b1 6.02 | **36.80 \u00b1 4.61** |\n| Sweep Half the Blocks |  | 0.00 \u00b1 0.00 | 18.33 \u00b1 6.11 | 16.67 \u00b1 8.95 | **75.00 \u00b1 11.55** | 22.00 \u00b1 2.91 | 16.60 \u00b1 1.33 | **16.40 \u00b1 1.00** |\n| Use Rope to Close the Opening Square |  | 0.00 \u00b1 0.00 | 68.33 \u00b1 9.52 | 58.33 \u00b1 18.62 | **76.67 \u00b1 6.11** | 41.60 \u00b1 6.34 | 65.80 \u00b1 7.40 | **34.60 \u00b1 2.81** |", "caption": "Table 8: Detailed Performance in CLIPort. We report the success rate and execution time for three tasks, compared to baseline methods.", "description": "This table presents a detailed comparison of the performance of four different methods on three robotic manipulation tasks within the CLIPort simulator.  The methods compared are the original CLIPort method, CLIPort enhanced with Inner Monologue, CLIPort with DoReMi, and the proposed Code-as-Monitor (CaM) method.  For each task and method, the table reports the success rate (percentage of successful task completions) and the average execution time (in seconds) required to complete the task.  The tasks include stacking blocks in a specific order (with variations in the probability of blocks dropping and noise in placement positions), sweeping half of a set of blocks into a designated area, and using a rope to enclose an open square. The results allow for a quantitative assessment of CaM's performance against established baselines, highlighting its improvements in terms of both success rate and efficiency.", "section": "4.2 Main Results in Simulator"}, {"content": "| Method | Avg. Success Rate (%) \u2191 | Articulated Object Open Drawer | Articulated Object Put in Drawer | Tool-Use Screw Bulb | Tool-Use Turn Tap | Tool-Use Drag Stick | Tool-Use Sweep to Dustpan |\n|---|---|---|---|---|---|---|---| \n| RVT2 [15] | 89.83 | 90.3 | 97.6 | 86.6 | 91.0 | 93.8 | 79.7 |\n| ARP [72] | 91.27 | 93.9 | 91.0 | 86.4 | 96.6 | 88.1 | 91.6 |\n| +DRM [16] | 87.97 | 90.6 | 87.7 | 83.1 | 93.3 | 84.8 | 88.3 |\n| +Ours | **97.08** | **98.1** | **98.3** | **97.5** | **97.9** | **95.6** | **94.0** |", "caption": "Table 9: Performance in RLBench. We report the success rate, compared to baseline methods.", "description": "This table presents a comparison of success rates achieved by different methods on various tasks within the RLBench environment.  The methods include two baseline methods,  DoReMi and the original RLBench method, and the proposed 'Code-as-Monitor' approach.  The tasks are categorized into articulated object interaction, rotational manipulation, and tool use, offering a comprehensive evaluation across diverse robotic manipulation scenarios.", "section": "4. Experiments"}, {"content": "Tasks with|Object|Object|Success Rate(%) \u2191|DGN|+DoReMi|+**Ours**|+DoReMi|+**Ours**\ndisturbance|types|Name| | | | | \nPick & Place with|Deformable|Toy Loopy|0.00|80.00|**100.00**|64.91 \u00b1 2.83|**46.02 \u00b1 3.11**\nthe objects being|Toy Dog|0.00|80.00|**100.00**|60.68 \u00b1 4.00|**47.06 \u00b1 3.24**\nmoved during|Toy Rabbit|0.00|90.00|**90.00**|59.83 \u00b1 1.82|**45.77 \u00b1 2.03**\ngrasping|Transparent|Beverage Bottle|0.00|60.00|**100.00**|69.97 \u00b1 7.89|**47.61 \u00b1 2.58**\n |Glass Cup|0.00|70.00|**90.00**|76.99 \u00b1 4.60|**48.32 \u00b1 3.22**\n |Shampoo Bottle|0.00|70.00|**90.00**|70.91 \u00b1 5.68|**48.31 \u00b1 3.08**\n |Small Rigid|Apple Model|0.00|80.00|**100.00**|64.65 \u00b1 4.34|**45.39 \u00b1 0.71**\n |Pear Model|0.00|90.00|**90.00**|67.11 \u00b1 1.10|**45.48 \u00b1 1.01**\n |Peach Model|0.00|70.00|**90.00**|65.48 \u00b1 2.90|**45.37 \u00b1 0.64**\n |Large Geometric|Plate|0.00|80.00|**100.00**|69.86 \u00b1 2.64|**45.18 \u00b1 0.65**\n |Ball|0.00|90.00|**100.00**|67.43 \u00b1 2.63|**45.37 \u00b1 0.70**\n |Pyramid|0.00|90.00|**90.00**|69.14 \u00b1 3.32|**45.42 \u00b1 0.72**\nPick & Place with|Deformable|Toy Loopy|0.00|80.00|**90.00**|69.29 \u00b1 4.87|**60.86 \u00b1 3.41**\nthe objects being|Toy Dog|0.00|70.00|**100.00**|66.09 \u00b1 2.99|**63.12 \u00b1 3.75**\nremoved during|Toy Rabbit|0.00|80.00|**90.00**|70.86 \u00b1 4.56|**63.40 \u00b1 3.88**\nmovement|Transparent|Beverage Bottle|0.00|50.00|**90.00**|77.90 \u00b1 2.89|**61.97 \u00b1 3.90**\n |Glass Cup|0.00|70.00|**90.00**|70.00 \u00b1 3.46|**63.22 \u00b1 4.35**\n |Shampoo Bottle|0.00|60.00|**90.00**|60 \u00b1 4.28|**63.00 \u00b1 3.81**\n |Small Rigid|Apple Model|0.00|70.00|**90.00**|70.21 \u00b1 4.30|**63.71 \u00b1 3.91**\n |Pear Model|0.00|60.00|**100.00**|72.70 \u00b1 4.84|**58.61 \u00b1 2.41**\n |Peach Model|0.00|60.00|**90.00**|66.48 \u00b1 3.32|**59.19 \u00b1 2.59**\n |Large Geometric|Plate|0.00|90.00|**100.00**|72.00 \u00b1 2.77|**59.21 \u00b1 2.61**\n |Ball|0.00|70.00|**100.00**|70.92 \u00b1 3.37|**61.57 \u00b1 3.80**\n |Pyramid|0.00|70.00|**90.00**|73.83 \u00b1 2.82|**60.25 \u00b1 3.25**", "caption": "Table 10: Detailed Performance of Single Pick & Place. We report the success rate and execution time. DGN is DexGraspNet 2.0\u00a0[71].", "description": "This table presents a detailed breakdown of the success rates and execution times for a single pick-and-place task across various object types, using DexGraspNet 2.0 [71] as a baseline method.  The object types are categorized as deformable, transparent, small rigid, and large geometric.  For each object type, the table shows the success rate and execution time for two scenarios: (1) objects are moved while being grasped, and (2) objects are removed from the robot hand during movement.  This allows for a comprehensive analysis of the performance differences across various object types and disturbance conditions.", "section": "4.3 Main Results in Real World"}]