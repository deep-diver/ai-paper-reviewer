[{"figure_path": "https://arxiv.org/html/2411.15138/extracted/6014333/Sections/Figures/pipeline.jpg", "caption": "Figure 1: Material Anything: A feed-forward PBR material generation model applicable to a diverse range of 3D meshes across varying texture and lighting conditions, including texture-less, albedo-only, generated, and scanned objects.", "description": "Figure 1 showcases Material Anything, a novel feed-forward model designed to generate physically-based rendering (PBR) materials for 3D objects. The figure highlights the model's versatility by demonstrating its application to a diverse range of 3D meshes under various conditions, including those with no texture, only albedo data, computer-generated textures, and those obtained from 3D scans. Each row represents a different object with its corresponding albedo and roughness values shown, followed by the generated object with its applied PBR material.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2411.15138/extracted/6014333/Sections/Figures/material_estimator.jpg", "caption": "Figure 2: Overview of Material Anything. For texture-less objects, we first generate coarse textures using image diffusion models, similar to the texture generation method\u00a0[6]. For objects with pre-existing textures, we directly process them. Next, a material estimator progressively estimates materials for each view from a rendered image, normal, and confidence mask. The confidence mask serves as additional guidance for illuminance uncertainty, addressing lighting variations in the input image and enhancing consistency across generated multi-view materials. These materials are then unwrapped into UV space and refined by a material refiner. The final material maps are integrated with the mesh, enabling the object for downstream applications.", "description": "Material Anything processes 3D objects to generate physically based render (PBR) material maps.  For objects lacking textures, it begins by generating coarse textures using an image diffusion model.  For textured objects, it uses the existing textures. Then, a material estimator processes each view of the object, using a rendered image, normal map, and confidence mask to progressively estimate material properties. The confidence mask helps to account for varying lighting conditions and improve consistency across views. The estimated materials are unwrapped into UV space and further refined using a UV-space material refiner.  The resulting, consistent UV material maps are combined with the mesh to create a fully textured 3D model ready for downstream use.", "section": "3. Approach"}, {"figure_path": "https://arxiv.org/html/2411.15138/extracted/6014333/Sections/Figures/progressive_generation.jpg", "caption": "Figure 3: Architectural design of material estimator and refiner. Both employ a triple-head U-Net, generating albedo, roughness-metallic, and bump maps via separate branches.", "description": "This figure illustrates the architecture of both the material estimator and refiner networks.  Each utilizes a triple-head U-Net.  This means the network has three separate branches that independently generate the albedo, roughness/metallic, and bump maps. These maps are then combined to produce a complete, physically-based rendering (PBR) material representation for a 3D object. The use of three separate heads helps avoid interference between the generation of the different material properties, ensuring greater accuracy and consistency.", "section": "3. Approach"}, {"figure_path": "https://arxiv.org/html/2411.15138/extracted/6014333/Sections/Figures/comp_learning.jpg", "caption": "Figure 4: Progressive material generation process for a texture-less object. \u201cProject\u201d denotes projecting known regions for the latent initialization of the next view. \u201cSD\u201d denotes the pre-trained stable diffusion model\u00a0[30] with depth ControlNet\u00a0[49]", "description": "This figure illustrates the progressive material generation process for a texture-less 3D object.  The process starts by using a pre-trained stable diffusion model ([30]) with depth ControlNet ([49]) to generate the material for the first view. For subsequent views, the model leverages information from previously generated views. Specifically, 'known regions' (areas where materials have already been estimated) are projected onto the latent space representation of the new view. This ensures consistency across views and avoids unnecessary recalculation. The progressive generation continues until materials for all views are estimated.  This approach effectively handles texture-less objects by building up a coherent material representation view-by-view.", "section": "3.2. Materials Generation for 3D Object"}, {"figure_path": "https://arxiv.org/html/2411.15138/extracted/6014333/Sections/Figures/comp_opt.jpg", "caption": "Figure 5: Comparisons with texture generation methods. These methods directly paint texture-less objects using image diffusion models but fail to generate the corresponding material properties.", "description": "Figure 5 presents a comparison of Material Anything against three other texture generation methods (Text2Tex, SyncMVD, and Paint3D).  All methods start with texture-less 3D models of chairs and a faucet. The comparison highlights that while the other methods can generate textures using image diffusion models, they fail to accurately produce the corresponding physically-based rendering (PBR) material properties such as albedo, roughness, metallic, and bump maps.  Material Anything, in contrast, successfully generates both realistic textures and accurate PBR material properties, resulting in more visually compelling and physically accurate 3D models.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2411.15138/extracted/6014333/Sections/Figures/comp_retrieve.jpg", "caption": "Figure 6: Comparisons with optimization methods. NvDiffRec\u00a0[26] estimates materials using the textured model by SyncMVD\u00a0[24] as input. The materials include albedo (top left); roughness (top right); metallic (bottom left); bump (bottom right).", "description": "Figure 6 compares Material Anything's material generation capabilities with optimization-based methods, specifically NvDiffRec [26] and DreamMat [52].  NvDiffRec uses textured models generated by SyncMVD [24] as input. The figure displays the generated material maps (albedo, roughness, metallic, and bump) for three different 3D objects, demonstrating Material Anything's superior performance in generating realistic and diverse material maps. Material Anything's results show significantly more detail and realism compared to the other methods. The comparison highlights Material Anything's efficiency and accuracy in material generation.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2411.15138/extracted/6014333/Sections/Figures/comp_others.jpg", "caption": "Figure 7: Comparisons with retrieval methods. The inputs are textured objects, including an albedo-only object and a scanned object. The materials include albedo (top left); roughness (top right); metallic (bottom left); bump (bottom right).", "description": "Figure 7 presents a comparison of Material Anything with retrieval-based methods. The input consists of textured 3D objects, specifically an albedo-only object and a scanned object.  The generated material maps are displayed for each object, broken down into four key PBR (Physically Based Rendering) components: albedo (reflectance), roughness (surface texture), metallic (metallicity), and bump (height variation). Each component is shown in a separate image, arranged in a 2x2 grid for easy comparison between the different methods and the ground truth.", "section": "4. Qualitative Evaluation"}, {"figure_path": "https://arxiv.org/html/2411.15138/extracted/6014333/Sections/Figures/ablations_view.jpg", "caption": "Figure 8: Comparisons with Rodin Gen-1 and Tripo3D. Rodin Gen-1 and Tripo3D are two closed-source methods. Our approach uses significantly less data, yet produces comparable results.", "description": "Figure 8 compares the material generation results of Material Anything with two closed-source methods, Rodin Gen-1 and Tripo3D.  The figure visually demonstrates that despite using significantly less training data, Material Anything achieves comparable material generation quality to these established methods. This highlights the efficiency and effectiveness of the proposed approach.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2411.15138/extracted/6014333/Sections/Figures/ablation_mask.jpg", "caption": "Figure 9: Effectiveness of triple-head U-Net and rendering loss. In both ablation experiments, the confidence mask is set to 1.", "description": "This figure demonstrates the impact of using a triple-head U-Net architecture and a rendering loss on material generation quality.  The triple-head network separates the generation of albedo, roughness/metallic, and bump maps into individual branches, preventing interference and improving stability. The rendering loss further refines the generated materials by comparing rendered images of the generated maps with ground truth renderings. Both ablations (removing the triple-head architecture and removing the rendering loss) are shown alongside the full model, with the confidence mask set to 1 for all to ensure consistent lighting conditions. The results show clear improvements from the combined techniques.", "section": "4.3. Ablation Study"}, {"figure_path": "https://arxiv.org/html/2411.15138/extracted/6014333/Sections/Figures/ablations_multi_view.jpg", "caption": "Figure 10: Effectiveness of confidence mask for various lighting conditions. \u201cW/O confidence mask\u201d indicates results from the material estimator without the confidence mask as input.", "description": "This figure shows the impact of using a confidence mask during material generation.  The confidence mask helps the model to differentiate between situations with reliable lighting cues (allowing use of those cues for material estimation) and those without (requiring reliance on prompt and semantic information).  The results demonstrate that using the confidence mask leads to superior material generation, especially in cases with unreliable or missing lighting information.  The 'W/O confidence mask' section shows the inferior results obtained when the confidence mask is not utilized. This highlights the importance of the confidence mask in handling the diversity of lighting conditions encountered in real-world 3D objects.", "section": "3.1 Image-based Material Diffusion"}, {"figure_path": "https://arxiv.org/html/2411.15138/extracted/6014333/Sections/Figures/ablations_uv.jpg", "caption": "Figure 11: Effectiveness of strategies for material consistency.", "description": "This figure demonstrates the impact of different strategies on achieving material consistency across multiple views of a 3D object.  It visually compares the results of generating materials using various methods, showing the differences in terms of consistency and visual quality. The methods illustrated involve different combinations of confidence masks (to account for variations in lighting) and a UV-space material refiner. The goal is to highlight how these techniques improve the consistency of materials across multiple viewpoints of a 3D model.", "section": "3.2. Materials Generation for 3D Object"}, {"figure_path": "https://arxiv.org/html/2411.15138/extracted/6014333/Sections/Figures/training_case.jpg", "caption": "Figure 12: Effectiveness of the UV-space material refiner. The material refiner effectively fills in holes caused by occlusions.", "description": "The figure demonstrates the effectiveness of the UV-space material refiner in Material Anything.  The left side shows material maps with holes and inconsistencies created by the image-space material generation process. These imperfections are due to self-occlusions in the 3D model, where parts of the object are hidden from view during rendering. The right side of the figure displays the refined material maps after processing by the UV-space material refiner.  The refiner successfully fills in the missing areas and smooths out inconsistencies, producing cleaner, more consistent material maps ready for use in downstream applications such as video game development or virtual reality.", "section": "3.2. Materials Generation for 3D Object"}, {"figure_path": "https://arxiv.org/html/2411.15138/extracted/6014333/Sections/Figures/camera.jpg", "caption": "Figure 13: The virtualization of our training data. We apply various degradations and simulate inconsistent lighting effects in the inputs to enhance the robustness of our method.", "description": "This figure visualizes the training data augmentation techniques used to improve the robustness of the Material Anything model.  The top row shows examples of input images that have undergone various degradations (e.g., blurring, color shifts) to simulate real-world scenarios where lighting might be inconsistent. A confidence mask indicates regions affected by degradation, which guides the model to learn how to effectively handle uncertain or unreliable lighting cues.  The bottom row shows examples of UV-space input data with masked regions and added canonical coordinate maps (CCM), which help refine the material generation process.", "section": "A. Material3D Dataset"}, {"figure_path": "https://arxiv.org/html/2411.15138/extracted/6014333/Sections/Figures/materials_editing.jpg", "caption": "Figure 14: The camera poses for progressive material generation and building training data.", "description": "This figure illustrates the camera viewpoints used in the progressive material generation process.  For each object, multiple views are rendered to capture comprehensive material properties.  These views are then used in a progressive manner to estimate materials and refine the results. The images show example camera positions, highlighting how the process of generating materials from different views contributes to creating a more complete and accurate material representation.", "section": "3. Approach"}, {"figure_path": "https://arxiv.org/html/2411.15138/extracted/6014333/Sections/Figures/relighting.jpg", "caption": "Figure 15: Material editing with prompts. Material Anything enables flexible editing and customization of materials for texture-less 3D objects by simply adjusting the input prompt.", "description": "This figure demonstrates the capability of Material Anything to edit and customize materials of texture-less 3D objects by modifying the text prompt.  The images show several examples of a 3D object rendered with different materials, all generated from the same object model but with varying text prompts specifying the desired material (e.g., wood, metal, stone). This highlights the method's flexibility and ease of use for material modification.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2411.15138/extracted/6014333/Sections/Figures/limitations.jpg", "caption": "Figure 16: Relighting results by Material Anything under various HDR environment maps. The left column displays the input texture-less meshes, while the top row presents the HDR environment maps used.", "description": "This figure demonstrates the ability of the Material Anything model to generate realistic material properties for 3D objects under various lighting conditions.  The left column shows the initial texture-less 3D models (a bed, a toilet, and a guitar). The top row displays the different HDR environment maps used for relighting.  The remaining cells show the results of applying each HDR environment map to each object, demonstrating the generated materials' consistent appearance across varying lighting scenarios. This highlights the model's robustness and accuracy in generating physically-based rendering (PBR) materials.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2411.15138/extracted/6014333/Sections/Figures/2D_results.jpg", "caption": "Figure 17: Failure Cases by Material Anything.", "description": "This figure showcases instances where Material Anything, the proposed material generation model, fails to accurately generate material properties.  The examples highlight limitations in handling fine surface details (as seen in the elephant's textured skin), and in differentiating between materials and artifacts within the input (as shown in the apple, where a highlight is mistakenly interpreted as part of the fruit's texture). These examples illustrate challenges the model faces in accurately segmenting and representing complex object details and subtle lighting effects.", "section": "D. Limitations and Failure Cases"}, {"figure_path": "https://arxiv.org/html/2411.15138/extracted/6014333/Sections/Figures/more_materials_uv.jpg", "caption": "Figure 18: Results by our material estimator on 2D renderings from Objaverse.", "description": "This figure displays the results of the material estimator, a core component of the Material Anything model, when applied to 2D renderings sourced from the Objaverse dataset.  It showcases the model's ability to estimate the albedo, roughness, metallic, and bump maps for different 3D objects. Each row represents a single object, with the 'GT' column displaying the ground truth material maps and the 'Ours' column displaying the model's estimations.  The visual comparison allows for a direct assessment of the accuracy and effectiveness of the material estimation process. The objects shown represent a variety of shapes and textures.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2411.15138/extracted/6014333/Sections/Figures/more_texture_objects.jpg", "caption": "Figure 19: Additional results by Material Anything on texture-less 3D objects. The generated UV material maps are provided.", "description": "This figure showcases additional examples generated by the Material Anything model.  It demonstrates the model's ability to generate realistic material textures and maps for 3D objects that initially lacked any surface texture. Three different textureless 3D objects (a bagel, a crown, and a laptop) are used as inputs. For each object, the figure displays the original texture-less model, followed by the generated albedo, roughness, metallic, bump maps, and a final rendering of the object with the generated materials applied.  The generated UV material maps are also provided, demonstrating the model's output in a format suitable for use in 3D modeling applications.", "section": "4. Experiments"}]