[{"content": "| Method | Type | Input Mesh | FID \u2193 | CLIP Score \u2191 |\n|---|---|---|---|---|\n| Text2Tex [6] | Learning | Texture-less | 116.41 | 30.33 |\n| SyncMVD [24] | Learning | Texture-less | 118.46 | 30.66 |\n| Paint3D [47] | Learning | Texture-less | 153.20 | 28.40 |\n| NvDiffRec [26] | Optimization | Texture-less | 103.81 | 30.14 |\n| DreamMat [52] | Optimization | Texture-less | 113.34 | 30.64 |\n| Ours | Learning | Texture-less | **100.63** | **31.06** |\n| Make-it-Real [12] | Retrieval | Textured | 104.38 | 88.62 |\n| Ours | Learning | Textured | **101.19** | **89.70** |", "caption": "Table 1: Quantitative comparisons. FID and CLIP scores (similarity between rendered views and text prompts) are computed on 1,200 images from 20 textured objects. For comparison with Make-it-Real, the CLIP score is calculated between rendered images from generated textures and those in Objaverse.", "description": "This table presents a quantitative comparison of different methods for generating 3D object textures, focusing on two key metrics: Fr\u00e9chet Inception Distance (FID) and CLIP score.  FID measures the visual similarity between generated images and real images, with lower scores indicating higher similarity. The CLIP score assesses the semantic alignment between generated images and textual descriptions, with higher scores representing better agreement.  The results are based on 1200 images from 20 textured objects.  A special note is made for the comparison with the Make-it-Real method, where the CLIP score is calculated using rendered images from generated textures compared against the Objaverse dataset rather than directly comparing rendered images.", "section": "4. Experiments"}, {"content": "| Materials | W/O Triple-head | W/O Rendering Loss | Full |\n|---|---|---|---|\n| Albedo | 0.0800 | 0.1442 | **0.0604** |\n| Roughness | 0.1196 | 0.1943 | **0.0877** |\n| Metallic | 0.1584 | 0.2594 | **0.1193** |\n| Bump | 0.0824 | 0.0716 | **0.0313** |", "caption": "Table 2: Ablation study for triple-head U-Net and rendering loss. RMSE is calculated for the materials across the views from 1,000 Objaverse objects.", "description": "This table presents the results of an ablation study evaluating the impact of two key components in the Material Anything model: the triple-head U-Net architecture and the rendering loss function.  The study uses 1000 3D objects from the Objaverse dataset.  For each object, the model generates material maps (albedo, roughness, metallic, and bump) for multiple views.  The root mean squared error (RMSE) is calculated for each material type across all views to quantify the impact of removing each component individually.  The table allows for a comparison of the model's performance with both components included against the results when one or the other is excluded, showcasing their relative contributions to overall model accuracy.", "section": "4.3 Ablation Study"}, {"content": "|---|---|---|---|---|\n|  | Light-less | Realistic | Unrealistic | Mean |\n| W/O Confidence | 0.1521 | 0.1074 | 0.1111 | 0.1235 |\n| Full | **0.1102** | **0.0747** | **0.0847** | **0.0899** |", "caption": "Table 3: Ablation study for confidence masks. Mean RMSE is calculated for materials from 1,000 Objaverse objects with different simulated lighting conditions, including light-less (albedo-only), realistic (scanned), and unrealistic light (generated).", "description": "This ablation study analyzes the impact of confidence masks on Material Anything's performance across various lighting conditions.  The study uses 1000 Objaverse objects, categorized into three groups based on lighting: light-less (albedo-only, meaning no lighting information is present), realistic (scanned, representing real-world lighting conditions), and unrealistic (generated, representing synthetically created lighting).  For each lighting category, the mean Root Mean Square Error (RMSE) is calculated for the generated albedo, roughness, metallic, and bump maps, to assess the model's accuracy under different illumination scenarios. The results show how well Material Anything handles the task with and without confidence masks, demonstrating the effectiveness of the confidence mask in improving accuracy across different lighting conditions.", "section": "4.3. Ablation Study"}]