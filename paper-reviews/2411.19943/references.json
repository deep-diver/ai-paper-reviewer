{"references": [{"fullname_first_author": "Rafailov", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "publication_date": "2024", "reason": "This paper introduces Direct Preference Optimization (DPO), a foundational algorithm for the paper's proposed method, which is extended and improved upon."}, {"fullname_first_author": "Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022", "reason": "This is a highly influential paper in the field of LLM alignment, providing a crucial foundation for the work on aligning LLMs with human preferences."}, {"fullname_first_author": "Li", "paper_title": "Contrastive decoding: Open-ended text generation as optimization", "publication_date": "2023", "reason": "This paper introduces contrastive decoding, a technique directly related to the proposed contrastive estimation method for identifying critical tokens."}, {"fullname_first_author": "Amini", "paper_title": "Direct preference optimization with an offset", "publication_date": "2024", "reason": "This paper addresses a limitation of DPO by introducing an offset, which is relevant to the paper's focus on improving the accuracy of reasoning tasks."}, {"fullname_first_author": "Pang", "paper_title": "Iterative reasoning preference optimization", "publication_date": "2024", "reason": "This paper tackles the challenge of applying DPO to reasoning tasks, which is directly addressed by the proposed method."}]}