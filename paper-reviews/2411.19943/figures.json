[{"figure_path": "https://arxiv.org/html/2411.19943/x1.png", "caption": "Figure 1: Impact of critical tokens on reasoning accuracy. The \u201cWith Critical Token\u201d line shows that, without intervention, repeated sampling from the original trajectory consistently fails to produce correct trajectories. In contrast, the \u201cWithout Critical Token\u201d line demonstrates that replacing the identified critical token with an alternative drastically increases the likelihood of correct reasoning outcomes. This underscores the significant role of critical tokens in incorrect reasoning trajectories.", "description": "This figure illustrates the effect of critical tokens on the accuracy of reasoning in large language models (LLMs). The blue line (\"With Critical Token\") shows that repeatedly sampling from a reasoning trajectory containing a critical token results in consistently low accuracy, rarely producing correct answers.  In contrast, the orange line (\"Without Critical Token\") shows that if the critical token is replaced with another, the accuracy increases dramatically. This demonstrates that these critical tokens are a significant factor leading to incorrect reasoning outcomes in LLMs.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2411.19943/x2.png", "caption": "Figure 2: Illustration of the impact of critical tokens on reasoning trajectories. The token \u201cowed\u201d leads to incorrect logical deductions, resulting in erroneous answers. In contrast, decoding alternative tokens like \u201cpaid\u201d significantly improves reasoning accuracy, enabling the model to produce correct answers.", "description": "Figure 2 shows examples of reasoning trajectories with different tokens.  The top example uses the token \"owed\" and leads to an incorrect answer (93.75 instead of 500).  The model's reasoning is flawed due to the use of this token. The subsequent examples show that replacing \"owed\" with alternative tokens like \"paid\" significantly improves the accuracy of the model's answer, leading to the correct answer of 500. This highlights how a single token can drastically affect the outcome of a reasoning problem.", "section": "2. Estimating Critical Tokens"}, {"figure_path": "https://arxiv.org/html/2411.19943/x3.png", "caption": "Figure 3: Contrastive estimation identifies critical tokens. This figure illustrates how contrastive estimation identifies critical tokens in incorrect trajectories by comparing the likelihoods of tokens generated by positive model and negative model.", "description": "This figure shows how contrastive estimation is used to identify 'critical tokens' in incorrect reasoning trajectories.  A positive model (trained on correct reasoning trajectories) and a negative model (trained on incorrect trajectories) are used to generate likelihoods for each token. By comparing the likelihoods produced by the two models, tokens that significantly contribute to the incorrectness of a trajectory (the critical tokens) are identified. The difference in likelihoods serves as an indicator of the token's criticality.  Tokens with a large difference in likelihood between the positive and negative models are highlighted as critical.", "section": "Estimating Critical Tokens"}, {"figure_path": "https://arxiv.org/html/2411.19943/x4.png", "caption": "Figure 4: Overview of aligning LLMs with critical tokens. The pipeline consists of two steps: (1) fine-tuning positive and negative models on correct and incorrect reasoning trajectories, and (2) applying contrastive estimation for cDPO.", "description": "This figure illustrates the cDPO (Contrastive Direct Preference Optimization) process for aligning LLMs with critical tokens.  The process is broken down into two steps. Step 1 involves training two separate models: a positive model trained on correct reasoning trajectories and a negative model trained on incorrect reasoning trajectories.  This allows the models to learn distinct patterns associated with correct and incorrect reasoning. Step 2 applies contrastive estimation, comparing the likelihoods of token generation from both the positive and negative models. This comparison helps to automatically identify the \"critical tokens\" within incorrect trajectories which are highly influential in producing erroneous results. The output of this contrastive estimation informs the cDPO algorithm which utilizes these insights for effective model optimization.", "section": "3. CDPO"}, {"figure_path": "https://arxiv.org/html/2411.19943/x5.png", "caption": "Figure 5: The accuracy across models on GSM8K for critical tokens identified by contrastive estimation. The results highlight the effectiveness of contrastive estimation in identifying critical tokens and demonstrate that cDPO achieves the highest performance by leveraging token-level signal from Contrastive Estimation.", "description": "Figure 5 presents a bar chart comparing the accuracy of three different LLMs (Llama-3 8B, Llama-3 70B, and DeepSeek math-7B) on the GSM8K benchmark.  Three scenarios are compared: using the original model with critical tokens, a modified version where critical tokens are excluded, and the proposed cDPO method. The chart clearly demonstrates that removing critical tokens improves accuracy across all models, and that cDPO achieves the best performance. This highlights the effectiveness of contrastive estimation for identifying and mitigating the negative impact of critical tokens during LLM reasoning.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2411.19943/x6.png", "caption": "Figure 6: Log probabilities of chosen and rejected sequences during training on the GSM8K dataset using DPO, RPO, and cDPO. The solid lines represent chosen sequences, while the dashed lines represent rejected sequences. The figure demonstrates how cDPO achieves a greater separation between chosen and rejected sequences compared to DPO and RPO.", "description": "Figure 6 illustrates the training dynamics of three different preference optimization methods: DPO, RPO, and cDPO, on the GSM8K dataset.  The graph plots the log probabilities of sequences chosen and rejected during training for each method.  Solid lines represent the log probabilities of chosen sequences, while dashed lines represent the log probabilities of rejected sequences.  The key observation is that cDPO achieves a significantly larger gap between the log probabilities of chosen and rejected sequences, compared to DPO and RPO. This demonstrates cDPO's superior ability to distinguish between high-quality and low-quality sequences during training, resulting in more effective preference optimization.", "section": "4.3. Analysis"}]