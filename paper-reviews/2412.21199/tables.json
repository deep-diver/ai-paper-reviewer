[{"content": "| Iteration | HumanEval Pro (%) | MBPP Pro (%) |\n|---|---|---|\n| Round 1 | 64.0 | 84.7 |\n| Round 2 | 98.8 | 99.7 |\n| Round 3 | 100.0 | 100.0 |", "caption": "Table 1: Pass@1 (%) of candidate solutions across different iteration rounds for canonical solution and test case generation with human manual review.", "description": "This table shows the success rate of generating canonical solutions and test cases for HumanEval Pro and MBPP Pro benchmarks across three iteration rounds. Each round involved manual review and refinement of the solutions and test cases.  The pass@1 metric indicates the percentage of times the model generated a correct solution in the first attempt.", "section": "3 Benchmark Construction"}, {"content": "| Model | Params | HumanEval (+) | HumanEval Pro (0-shot) | HumanEval Pro (1-shot) | MBPP (+) | MBPP Pro (0-shot) | MBPP Pro (1-shot) |\n|---|---|---|---|---|---|---|---| \n| **Proprietary Models** |  |  |  |  |  |  |  |\n| o1-mini | - | 97.6 (90.2) | 76.2 | 84.8 | 93.9 (78.3) | 68.3 | 81.2 |\n| GPT-4o | - | 90.2 (86.0) | 75.0 | 77.4 | 86.8 (72.5) | 70.9 | 80.2 |\n| GPT-4-Turbo | - | 90.2 (86.6) | 72.0 | 76.2 | 85.7 (73.3) | 69.3 | 73.3 |\n| Claude-3.5-sonnet | - | 92.1 (86.0) | 72.6 | 79.9 | 91.0 (74.6) | 66.4 | 76.2 |\n| **Open-source Models** |  |  |  |  |  |  |  |\n| Deepseek-V2.5 | - | 90.2 (83.5) | 73.8 | 76.8 | 87.6 (74.1) | 71.2 | 77.5 |\n| DeepseekCoder-V2-instruct | 21/236B | 90.2 (84.8) | 77.4 | 82.3 | 89.4 (76.2) | 71.4 | 76.5 |\n| Qwen2.5-Coder-1.5B-base | 1.5B | 43.9 (36.6) | 37.2 | 39.6 | 69.2 (58.6) | 48.4 | 51.3 |\n| Qwen2.5-Coder-1.5B-instruct | 1.5B | 70.7 (66.5) | 33.5 | 37.8 | 69.2 (59.4) | 42.1 | 43.7 |\n| DeepseekCoder-6.7B-base | 6.7B | 49.4 (39.6) | 35.4 | 36.6 | 70.2 (51.6) | 50.5 | 55.0 |\n| DeepseekCoder-6.7B-instruct | 6.7B | 78.6 (71.3) | 55.5 | 61.6 | 74.9 (65.6) | 57.1 | 58.2 |\n| Magicoder-S-DS-6.7B | 6.7B | 76.8 (70.7) | 54.3 | 56.7 | 75.7 (64.4) | 58.7 | 64.6 |\n| WaveCoder-Ultra-6.7B | 6.7B | 78.6 (69.5) | 54.9 | 59.8 | 74.9 (63.5) | 60.1 | 64.6 |\n| Qwen2.5-Coder-7B-base | 7B | 61.6 (53.0) | 54.9 | 56.1 | 76.9 (62.9) | 61.4 | 68.0 |\n| Qwen2.5-Coder-7B-instruct | 7B | 88.4 (84.1) | 65.9 | 67.1 | 83.5 (71.7) | 64.8 | 69.8 |\n| OpenCoder-8B-base | 8B | 66.5 (63.4) | 39.0 | 42.1 | 79.9 (70.4) | 52.4 | 53.7 |\n| OpenCoder-8B-instruct | 8B | 83.5 (78.7) | 59.1 | 54.9 | 79.1 (69.0) | 57.9 | 61.4 |\n| Yi-Coder-9B-base | 9B | 53.7 (46.3) | 42.7 | 50.0 | 78.3 (64.6) | 60.3 | 61.4 |\n| Yi-Coder-9B-chat | 9B | 85.4 (74.4) | 59.8 | 64.0 | 81.5 (69.3) | 64.8 | 71.7 |\n| Codestral-22B-v0.1 | 22B | 81.1 (73.2) | 59.1 | 65.9 | 78.2 (62.2) | 63.8 | 71.2 |\n| DeepseekCoder-33B-base | 33B | 56.1 (47.6) | 49.4 | 49.4 | 74.2 (60.7) | 59.0 | 65.1 |\n| DeepseekCoder-33B-instruct | 33B | 79.3 (75.0) | 56.7 | 62.8 | 80.4 (70.1) | 64.0 | 68.3 |\n| Qwen2.5-Coder-32B-base | 32B | 65.9 (60.4) | 61.6 | 67.1 | 83.0 (68.2) | 67.7 | 73.3 |\n| Qwen2.5-Coder-32B-instruct | 32B | 92.7 (87.2) | 70.1 | 80.5 | 90.2 (75.1) | 69.8 | 77.5 |\n| LLaMA3-70B-instruct | 70B | 81.7 (72.0) | 60.4 | 64.6 | 82.3 (69.0) | 63.5 | 70.4 |", "caption": "Table 2: Main result of different models on HumanEval Pro and MBPP Pro. More results is shown in Appendix\u00a0A.", "description": "This table presents the performance of various large language models (LLMs) on HumanEval Pro and MBPP Pro, which are benchmarks designed to evaluate the progressive reasoning and problem-solving capabilities of LLMs.  The results show the pass@1 scores (percentage of problems solved correctly on the first attempt) for each model on both benchmarks.  The table includes both proprietary and open-source models, allowing for a comparison of performance across different model architectures and training approaches.  Further detailed results are available in Appendix A.", "section": "4 Experiments"}, {"content": "| Error Type | Description | Examples |\n|---|---|---|\n| AssertionError | Failing to pass the test cases. | Examples in Section G.1 |\n| NameError | The code includes undefined variables. | Examples in Section G.2 |\n| ValueError | Unaware of the value of variables | Examples in Section G.3 |\n| IndexError | Array out of bounds | Examples in Section G.4 |\n| TypeError | Incorrect variable type usage. | Examples in Section G.5 |\n| Other Errors | KeyError, SyntaxError, ZeroDivisionError, IndentationError, etc. | \u2013 |", "caption": "Table 3: The execution error types and their descriptions in our evaluation results.", "description": "This table lists the different types of errors encountered during the execution of the code generated by various large language models (LLMs) in the HumanEval Pro and MBPP Pro benchmarks. For each error type, a short description is provided along with examples in the appendix.  The error types are categorized to help understand common failure modes of LLMs during code generation tasks.", "section": "3.3 Test Cases Generation"}, {"content": "| Model | CoT | HE Pro | MBPP Pro |\n|---|---|---|---|\n| GPT-4o | \u2718 | 75.0 | 70.9 |\n| GPT-4o | \u2714 | 78.0 | 70.9 |\n| DeepseekV2.5 | \u2718 | 73.8 | 71.2 |\n| DeepseekV2.5 | \u2714 | 74.4 | 71.4 |\n| Qwen2.5-Coder-32B-ins | \u2718 | 70.1 | 69.8 |\n| Qwen2.5-Coder-32B-ins | \u2714 | 72.0 | 70.1 |\n| Qwen2.5-Coder-7B-ins | \u2718 | 65.9 | 64.8 |\n| Qwen2.5-Coder-7B-ins | \u2714 | 71.3 | 64.8 |", "caption": "Table 4: The execution error types and their descriptions in our evaluation results.", "description": "This table presents a breakdown of the different types of errors encountered during the execution of code generated by various Large Language Models (LLMs) on the HumanEval Pro and MBPP Pro benchmarks.  Each error type is described, and examples can be found in the appendix. This analysis helps to identify failure modes and areas for future improvement in LLM code generation.", "section": "3.3 Test Cases Generation"}, {"content": "| Model | BCB-Lite | Pro (%) |\n|---|---|---|\n| GPT-4o | 64.9 | 52.6 |\n| GPT4-Turbo | 61.4 | 52.6 |\n| Claude-3.5-sonnet | 73.7 | 50.9 |\n| DeepseekV2.5 | 80.7 | 50.9 |\n| Qwen2.5Coder-1.5B-base | 50.9 | 15.8 |\n| Qwen2.5Coder-1.5B-instruct | 50.9 | 10.5 |\n| OpenCoder-8B-base | 56.1 | 10.5 |\n| OpenCoder-8B-instruct | 75.4 | 22.8 |\n| DeepseekCoder-6.7B-base | 59.6 | 35.1 |\n| DeepseekCoder-6.7B-instruct | 56.1 | 35.1 |\n| WaveCoder-Ultra-6.7B | 61.4 | 26.3 |\n| Magicoder-S-DS-6.7B | 50.9 | 33.3 |\n| Yi-Coder-9B | 57.9 | 21.1 |\n| Yi-Coder-9B-Chat | 66.7 | 31.6 |\n| Qwen2.5Coder-7B-base | 59.6 | 38.6 |\n| Qwen2.5Coder-7B-instruct | 64.9 | 35.1 |\n| DeepseekCoder-33B-base | 71.9 | 38.6 |\n| DeepseekCoder-33B-instruct | 80.7 | 43.9 |\n| Qwen2.5Coder-32B-base | 68.4 | 49.1 |\n| Qwen2.5Coder-32B-instruct | 80.7 | 52.6 |\n| Codestral-22B | 78.9 | 54.4 |\n| QwQ-32B-preview | 86.0 | 59.6 |", "caption": "Table 5: Passing rate (%) of LLMs on BigCodeBench (BCB)-Lite and BCB-Lite-Pro. A dataset example of BCB-Lite-Pro is shown in Section\u00a0G.6.", "description": "This table presents the performance of various Large Language Models (LLMs) on two code generation benchmarks: BigCodeBench-Lite and its extended self-invoking version, BigCodeBench-Lite Pro.  The passing rate, expressed as a percentage, indicates the success rate of each LLM in correctly solving the code generation problems within each benchmark.  The self-invoking nature of BigCodeBench-Lite Pro adds a layer of complexity compared to the original BigCodeBench-Lite, requiring LLMs to not only generate code but also effectively utilize previously generated functions to solve more intricate problems.  Section G.6 provides an example of a problem from the BigCodeBench-Lite Pro dataset.", "section": "Results Analysis"}, {"content": "| Model | HumanEval Pro (0-shot) | MBPP Pro (0-shot) |\n|---|---|---|\n| LLaMA-3.1-8B-base | 25.0 | 36.5 |\n| LLaMA-3.1-8B-instruct | 45.7 | 53.7 |\n| LLaMA-3.1-70B-base | 40.9 | 57.4 |\n| LLaMA-3.1-70B-instruct | 60.4 | 63.8 |\n| Qwen-2.5-72B-base | 62.2 | 65.3 |\n| Qwen-2.5-72B-instruct | 68.9 | 68.8 |\n| QwQ-32B-preview | 72.0 | 67.5 |\n| LLaMA-3.3-70B-instruct | 67.1 | 64.6 |\n| Mistral-Large-instruct-2411 | 75.0 | 69.3 |", "caption": "Table 6: Results of Other LLMs on HumanEval Pro and MBPP Pro (greedy decoding).", "description": "This table presents the results of various Large Language Models (LLMs) on HumanEval Pro and MBPP Pro benchmarks using greedy decoding.  It shows the pass rate for each model on these two benchmarks, offering a comparison of performance across different LLMs and highlighting the challenges posed by these more complex, self-invoking code generation tasks.", "section": "4 Experiments"}, {"content": "| Model | HumanEval Pro |  |  | MBPP Pro |  |  |\n|---|---|---|---|---|---|---|\n|  | pass@1 | pass@5 | pass@10 | pass@1 | pass@5 | pass@10 |\n|---|---|---|---|---|---|---|\n| DeepseekCoder-6.7B-base | 38.0 | 50.9 | 54.7 | 51.6 | 60.4 | 63.1 |\n| DeepseekCoder-6.7B-instruct | 55.9 | 64.1 | 66.5 | 55.2 | 62.6 | 64.9 |\n| Magicoder-S-DS-6.7B | 55.1 | 62.7 | 65.1 | 57.7 | 64.9 | 67.2 |\n| WaveCoder-Ultra-6.7B | 55.7 | 61.4 | 63.0 | 58.2 | 64.4 | 66.3 |\n| DeepseekCoder-33B-base | 49.4 | 60.8 | 65.2 | 59.1 | 67.2 | 69.3 |\n| DeepseekCoder-33B-instruct | 59.1 | 68.6 | 71.3 | 63.4 | 70.6 | 72.9 |\n| Qwen2.5-Coder-7B-base | 51.8 | 62.1 | 66.2 | 61.3 | 69.9 | 72.3 |\n| Qwen2.5-Coder-7B-instruct | 65.7 | 72.5 | 75.0 | 64.2 | 70.5 | 72.6 |\n| OpenCoder-9B-base | 44.5 | 56.2 | 59.9 | 54.8 | 62.9 | 65.0 |\n| OpenCoder-9B-instruct | 59.8 | 68.5 | 70.8 | 58.1 | 63.7 | 65.1 |\n| Yi-Coder-9B-base | 47.9 | 59.0 | 61.9 | 59.6 | 67.7 | 69.7 |\n| Yi-Coder-9B-chat | 59.7 | 66.4 | 67.9 | 65.0 | 69.8 | 71.2 |\n| Codestral-22B | 59.5 | 66.2 | 67.7 | 63.2 | 67.7 | 68.9 |\n| Qwen2.5-Coder-32B-base | 62.4 | 70.3 | 72.2 | 67.6 | 75.0 | 76.9 |\n| Qwen2.5-Coder-32B-instruct | 69.2 | 72.3 | 73.3 | 70.6 | 74.7 | 76.0 |\n| QwQ-32B-preview | 70.9 | 77.7 | 79.5 | 67.0 | 73.0 | 74.5 |", "caption": "Table 7: The results of different models on HumanEval Pro and MBPP Pro . We generate 20 samples for each problems with random sampling strategy where temperature is set to 0.2 and top_p is set to 0.95.", "description": "This table presents the performance of various large language models (LLMs) on HumanEval Pro and MBPP Pro, which are more challenging benchmarks designed to assess the progressive reasoning and problem-solving capabilities of LLMs through self-invoking code generation tasks.  The results show the pass rate (@1, @5, @10) for each model, indicating the percentage of times the model generated a correct solution within the top 1, 5, or 10 predictions. The evaluation methodology involves generating 20 samples per problem using a random sampling strategy with a temperature of 0.2 and top_p of 0.95 to ensure diversity and robustness in the results.", "section": "4 Experiments"}, {"content": "| Model Name | API Name |\n|---|---| \n| O1-mini | `o1-mini-2024-09-12` |\n| GPT-4o | `gpt-4o-2024-08-06` |\n| GPT-4-Turbo | `gpt-4-turbo-2024-04-09` |\n| Claude-3.5-sonnet | `claude-3-5-sonnet-20241022` |\n| Deepseek-V2.5 | `deepseek-chat` |", "caption": "Table 8: The corresponding API names and HuggingFace model URLs for the evaluated models are listed in Table\u00a02.", "description": "This table provides a mapping between the model names used in the paper and their corresponding API names and Hugging Face model URLs. This allows for easy reference and reproducibility of the experiments, as readers can directly access the models using the provided URLs.", "section": "3 Benchmark Construction"}, {"content": "| Model Name | HuggingFace URL |\n|---|---| \n| DeepseekCoder-V2-instruct | https://huggingface.co/deepseek-ai/DeepSeek-Coder-V2-Instruct |\n| Qwen2.5-Coder-1.5B-base | https://huggingface.co/Qwen/Qwen2.5-Coder-1.5B |\n| Qwen2.5-Coder-1.5B-instruct | https://huggingface.co/Qwen/Qwen2.5-Coder-1.5B-Instruct |\n| DeepseekCoder-6.7B-base | https://huggingface.co/deepseek-ai/deepseek-coder-6.7b-base |\n| DeepseekCoder-6.7B-instruct | https://huggingface.co/deepseek-ai/deepseek-coder-6.7b-instruct |\n| Magicoder-S-DS-6.7B | https://huggingface.co/ise-uiuc/Magicoder-S-DS-6.7B |\n| WaveCoder-Ultra-6.7B | https://huggingface.co/microsoft/wavecoder-ultra-6.7b |\n| Qwen2.5-Coder-7B-base | https://huggingface.co/Qwen/Qwen2.5-Coder-7B |\n| Qwen2.5-Coder-7B-instruct | https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct |\n| OpenCoder-8B-base | https://huggingface.co/infly/OpenCoder-8B-Base |\n| OpenCoder-8B-instruct | https://huggingface.co/infly/OpenCoder-8B-Instruct |\n| Yi-Coder-9B-base | https://huggingface.co/01-ai/Yi-Coder-9B |\n| Yi-Coder-9B-chat | https://huggingface.co/01-ai/Yi-Coder-9B-Chat |\n| Codestral-22B-v0.1 | https://huggingface.co/mistralai/Codestral-22B-v0.1 |\n| DeepseekCoder-33B-base | https://huggingface.co/deepseek-ai/deepseek-coder-33b-base |\n| DeepseekCoder-33B-instruct | https://huggingface.co/deepseek-ai/deepseek-coder-33b-instruct |\n| Qwen2.5-Coder-32B-base | https://huggingface.co/Qwen/Qwen2.5-Coder-32B |\n| Qwen2.5-Coder-32B-instruct | https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct |\n| LLaMA3-70B-instruct | https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct |\n| QwQ-32B-Preview | https://huggingface.co/Qwen/QwQ-32B-Preview |\n| LLaMA3.1-8B-base | https://huggingface.co/meta-llama/Llama-3.1-8B |\n| LLaMA3.1-8B-instruct | https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct |\n| LLaMA3.1-70B-base | https://huggingface.co/meta-llama/Llama-3.1-70B |\n| LLaMA3.1-70B-instruct | https://huggingface.co/meta-llama/Llama-3.1-70B-Instruct |\n| Qwen2.5-72B-base | https://huggingface.co/Qwen/Qwen2.5-72B |\n| Qwen2.5-72B-instruct | https://huggingface.co/Qwen/Qwen2.5-72B-Instruct |", "caption": "Table 9: Error type of Different Models on HumanEval Pro and MBPP Pro.", "description": "This table presents a detailed breakdown of the different types of errors encountered by various large language models (LLMs) while attempting to solve problems in the HumanEval Pro and MBPP Pro benchmarks.  The error types include AssertionErrors (test failures), NameErrors (undefined variable issues), ValueErrors (incorrect values), IndexErrors (array out-of-bounds errors), TypeErrors (type mismatch problems), and Other Errors (other types of errors). The table allows for a comparison of error frequencies among different LLMs and across the two benchmarks. This provides insights into the specific challenges and failure modes of LLMs in the self-invoking code generation task.", "section": "Error Statistics across Different Models"}]