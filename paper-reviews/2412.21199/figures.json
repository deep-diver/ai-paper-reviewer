[{"figure_path": "https://arxiv.org/html/2412.21199/x1.png", "caption": "Figure 1: The overview of self-invoking code generation in HumanEval Pro and MBPP Pro. Given a base problem and a related, more complex problem, they are required to solve the base problem and use its solution to address the complex problems.", "description": "The figure illustrates the self-invoking code generation task.  Given a simple base problem and a more complex problem that relies on the solution of the base problem, the model must solve the base problem first and then use its solution (or a function from its solution) to address the second, more complex problem. This tests the model's ability to not only generate code but also to use previously-generated code in a progressive problem-solving manner.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2412.21199/x2.png", "caption": "Figure 2: The overview of benchmark construction. An example is shown in Figure\u00a08. We summarize the entire benchmark construction process as follows:\n(1) Self-invoking problem Generation: We use Deepseek-V2.5 to generate the self-invoking problems, as well as their candidate solutions and test inputs.\n(2) Solutions Generation: We execute the generated solution with the test inputs in a controlled Python environment to obtain ground truth outputs.\n(3) Test Cases Generation:\nWe employ an iterative method involving Python execution check and manual review to ensure that all test cases pass successfully.\nThe final execution results are then used to construct complete test cases with assert command.", "description": "This figure illustrates the benchmark construction process, which consists of three main stages: self-invoking problem generation using Deepseek-V2.5, solution generation by executing the code in a controlled environment, and test case generation through an iterative process of Python execution checks and manual review. The final step involves using the assert command to create comprehensive test cases that ensure the reliability of the benchmark.", "section": "3 Benchmark Construction"}, {"figure_path": "https://arxiv.org/html/2412.21199/x3.png", "caption": "Figure 3: Performance Comparison: HumanEval Pro (and MBPP Pro) vs. HumanEval (and MBPP).", "description": "This figure compares the performance of various large language models (LLMs) on two code generation benchmarks: HumanEval and MBPP.  It shows the pass@1 scores (the percentage of times the model generated a correct solution on the first attempt) for each model on the original benchmarks (HumanEval and MBPP) and their more challenging, self-invoking counterparts (HumanEval Pro and MBPP Pro). This comparison highlights the relative difficulty of self-invoking code generation tasks for LLMs.", "section": "4 Experiments"}, {"figure_path": "https://arxiv.org/html/2412.21199/x4.png", "caption": "Figure 4: HumanEval (or MBPP) scores against the results on HumanEval Pro and MBPP Pro (HumanEval+ and MBPP+). We presents the comparison between base model and instruct model.", "description": "This figure displays a comparison of the performance of base language models and instruction-tuned models on the HumanEval and MBPP benchmarks, as well as their more challenging counterparts, HumanEval Pro and MBPP Pro.  Each point represents a specific model. The x-axis shows the model's performance on the original benchmark (HumanEval or MBPP), and the y-axis shows the model's performance on the corresponding Pro benchmark (HumanEval Pro or MBPP Pro). This allows for a visual assessment of how well the models generalize from simpler to more complex, self-invoking code generation tasks, highlighting the differences between base and instruction-tuned model performance.", "section": "5 Analysis"}, {"figure_path": "https://arxiv.org/html/2412.21199/x5.png", "caption": "(a) Qwen2.5-Coder-7B-base", "description": "This confusion matrix shows the performance of the Qwen2.5-Coder-7B-base model on HumanEval and HumanEval Pro, as well as MBPP and MBPP Pro.  The matrix breaks down the number of problems correctly solved (Passed) and incorrectly solved (Failed) in each benchmark.  It highlights the discrepancies between the performance on the original and the self-invoking benchmarks. The numbers within each quadrant represent the count of problems in each category, with percentages indicating the proportion of the total samples.", "section": "5.2 Confusion Matrix Correlation for Different Models"}, {"figure_path": "https://arxiv.org/html/2412.21199/x6.png", "caption": "(b) Qwen2.5-Coder-32B-base", "description": "This figure shows the confusion matrix for the Qwen2.5-Coder-32B-base model.  The confusion matrix visualizes the model's performance on both the HumanEval and MBPP benchmarks, and their self-invoking counterparts (HumanEval Pro and MBPP Pro).  Each cell in the matrix represents the count of problems that fall into specific categories,  For example, how many problems the model passed both in the original benchmark and the self-invoking version, and how many it passed in the original benchmark but failed in the self-invoking version. This analysis helps to understand the types of errors the model makes and where the self-invoking aspect presents additional difficulties.", "section": "5 Analysis"}, {"figure_path": "https://arxiv.org/html/2412.21199/x7.png", "caption": "(c) Qwen2.5-Coder-7B-instruct", "description": "The confusion matrix for the Qwen2.5-Coder-7B-instruct model on both HumanEval and HumanEval Pro, and MBPP and MBPP Pro.  The matrix shows the counts of problems that passed or failed in both the original and self-invoking benchmarks. This visualization helps understand the model's performance differences between simpler (original) and more complex (self-invoking) code generation tasks. The numbers within each cell represent the count of problems, allowing for a detailed analysis of the model's behavior in various scenarios.", "section": "5. Analysis"}, {"figure_path": "https://arxiv.org/html/2412.21199/x8.png", "caption": "(d) Qwen2.5-Coder-32B-instruct", "description": "This confusion matrix shows the performance of the Qwen2.5-Coder-32B-instruct model on HumanEval and HumanEval Pro, as well as MBPP and MBPP Pro.  The rows represent the results on the original benchmarks (HumanEval and MBPP), while the columns show the results on the self-invoking code generation benchmarks (HumanEval Pro and MBPP Pro). Each cell displays the number of problems that fall into a particular category. For instance, the top-left cell shows the number of problems that passed both HumanEval and HumanEval Pro. The bottom-right cell shows how many failed both.  Analyzing this matrix helps understand the model's ability to transfer its code generation skills from simpler tasks to more complex, self-invoking ones.", "section": "5.2 Confusion Matrix Correlation for Different Models"}, {"figure_path": "https://arxiv.org/html/2412.21199/x9.png", "caption": "Figure 5: The confusion matrix of different models. We use (Failed, Passed) to indicate samples that fail in HumanEval Pro (or MBPP Pro) but pass in HumanEval (or MBPP).", "description": "This figure presents a series of confusion matrices that visually represent the performance of various LLMs (Large Language Models) on code generation tasks.  Each matrix compares the model's performance on a standard code generation benchmark (HumanEval or MBPP) against its performance on a more challenging, self-invoking variant (HumanEval Pro or MBPP Pro). The (Failed, Passed) notation signifies instances where the model failed the self-invoking task but succeeded on the original task. This allows for a detailed analysis of the model's ability to leverage previously generated code to solve more complex problems, highlighting potential weaknesses in their progressive reasoning capabilities. The matrices provide a visual way to compare the error patterns across different LLMs.", "section": "Analysis"}, {"figure_path": "https://arxiv.org/html/2412.21199/x10.png", "caption": "Figure 6: Error types of GPT-4o with and without CoT reasoning on HumanEval Pro.", "description": "Figure 6 shows a bar chart comparing the types and counts of errors generated by GPT-40 on HumanEval Pro, with and without the Chain-of-Thought (CoT) prompting technique.  The chart visually represents the impact of CoT prompting on reducing specific error types such as AssertionError and NameError in the generated code, offering insights into how the reasoning capability of LLMs affects code generation quality.", "section": "5.3 Chain-of-Thought Prompting"}, {"figure_path": "https://arxiv.org/html/2412.21199/x11.png", "caption": "Figure 7: Statistics of error type across different LLMs on HumanEval Pro and MBPP Pro. We sum up all kinds of errors on the two benchmarks. Exact number is shown in Table\u00a09.", "description": "This figure displays a bar chart visualizing the frequency of different error types generated by various Large Language Models (LLMs) when completing HumanEval Pro and MBPP Pro tasks. Each bar represents an LLM, and its height reflects the total number of errors encountered across both benchmarks.  The chart categorizes the errors into six types: AssertionError, NameError, ValueError, IndexError, TypeError, and Other Errors. This allows for a comparison of the relative strengths and weaknesses of different LLMs in handling various types of coding errors within the context of self-invoking code generation.", "section": "5.4 Error Analysis"}]