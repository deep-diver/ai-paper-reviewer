{"importance": "This paper is important because **it introduces a novel evaluation task for large language models (LLMs)**, addressing limitations of existing benchmarks.  It highlights the gap between LLMs' ability to generate isolated code and their capacity to utilize self-generated code for complex problem-solving. This finding paves the way for **future research focused on improving the progressive reasoning and problem-solving capabilities of LLMs**. The proposed benchmarks and analysis provide valuable insights for researchers developing and evaluating LLMs.", "summary": "New benchmarks, HumanEval Pro and MBPP Pro, reveal LLMs struggle with self-invoking code generation, highlighting a critical gap in current code reasoning capabilities.", "takeaways": ["LLMs excel in standard code generation but fail significantly in self-invoking tasks requiring progressive reasoning.", "Instruction-tuned models show only marginal improvement over base models in self-invoking code generation.", "New benchmarks (HumanEval Pro, MBPP Pro, BigCodeBench-Lite Pro) provide a more realistic evaluation of LLMs' code reasoning abilities."], "tldr": "Current benchmarks for large language models (LLMs) primarily focus on isolated code generation tasks. This approach doesn't accurately reflect real-world coding scenarios that often demand complex reasoning and the ability to utilize previously generated code to tackle more complex issues. This paper introduces three new benchmarks, HumanEval Pro, MBPP Pro, and BigCodeBench-Lite Pro, which are designed to evaluate LLMs on a new task called \"self-invoking code generation\".  In this task, LLMs must solve a simple problem and then use its solution to address a related, more complex problem. \nThe study evaluates twenty LLMs across these benchmarks. The results reveal that while LLMs often perform well on the standard code generation benchmarks, their performance significantly drops in the self-invoking tasks. Additionally, the researchers find that instruction-tuned models, which are specifically trained to follow instructions, exhibit only marginal improvements compared to their base models in the self-invoking code generation task. The failure analysis reveals that difficulties in utilizing self-generated code and handling logical inconsistencies are major factors leading to the performance drop.  The paper's findings underscore the limitations of existing benchmarks and point towards the need for improvements in LLM training methods.", "affiliation": "Tsinghua University", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}, "podcast_path": "2412.21199/podcast.wav"}