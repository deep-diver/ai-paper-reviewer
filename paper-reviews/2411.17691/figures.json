[{"figure_path": "https://arxiv.org/html/2411.17691/x1.png", "caption": "Figure 1: Scaling laws for predicting Quantization-induced Degradation (QiD, denoted as \u0394q\u2062L\u2062o\u2062s\u2062ssubscript\u0394\ud835\udc5e\ud835\udc3f\ud835\udc5c\ud835\udc60\ud835\udc60\\Delta_{q}Lossroman_\u0394 start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT italic_L italic_o italic_s italic_s) in 7B, 70B, and 405B models trained on up to 100 trillion (1014superscript101410^{14}10 start_POSTSUPERSCRIPT 14 end_POSTSUPERSCRIPT) tokens. While low-bit quantization yields acceptable QiD for undertrained LLMs (trained with \u22641012absentsuperscript1012\\leq 10^{12}\u2264 10 start_POSTSUPERSCRIPT 12 end_POSTSUPERSCRIPT tokens), it becomes undesirable when applied to fully trained LLMs (e.g., trained with 100 trillion tokens, a milestone expected to be reached in the next few years), particularly for smaller models. Note that the gray areas in this figure indicate levels of QiD that degrade the model\u2019s predictions to a level worse than random guessing.", "description": "This figure presents scaling laws that predict the impact of quantization on large language models (LLMs).  The x-axis represents the number of training tokens used to train the model. The y-axis represents the quantization-induced degradation (QiD), measured as the increase in loss after applying low-bit quantization.  Three different model sizes (7B, 70B, and 405B parameters) are shown.  The figure demonstrates that low-bit quantization is acceptable for undertrained LLMs (trained on fewer tokens), but that the QiD increases significantly as the number of training tokens increases, especially for smaller models. The gray shaded regions highlight QiD levels that result in performance worse than random guessing.", "section": "3 Scaling Laws for Low-bit Quantization"}, {"figure_path": "https://arxiv.org/html/2411.17691/x2.png", "caption": "Figure 2: Performance of LLMs after low-bit quantization at different sizes and training levels. It is obvious that the models which are smaller or trained with more tokens suffer from greater quantization-induced degradation.", "description": "This figure displays the performance of Large Language Models (LLMs) after applying low-bit quantization.  It compares LLMs of different sizes (160M, 1B, and 12B parameters) and training levels (varying number of training tokens). The results show a clear trend: smaller models and those trained on larger datasets exhibit significantly more performance degradation due to quantization compared to larger models trained on smaller datasets. This suggests that low-bit quantization is more effective for undertrained LLMs.", "section": "3 Scaling Laws for Low-bit Quantization"}, {"figure_path": "https://arxiv.org/html/2411.17691/x3.png", "caption": "Figure 3: The fitted scaling law of QiD with respect to the number of training tokens in the form of Eq (5), where \u03b2\ud835\udefd\\betaitalic_\u03b2 is fitted to be 0.5316.", "description": "Figure 3 shows the relationship between quantization-induced degradation (QiD) and the number of training tokens.  The graph displays fitted curves for 2-bit, 3-bit, and 4-bit quantization, illustrating how QiD increases with the number of training tokens.  The equation used to fit the curves is specified as Eq (5),  with the scaling exponent (\u03b2) calculated to be 0.5316. This indicates that QiD grows sublinearly with respect to the number of training tokens. Different curves represent different model sizes, demonstrating that smaller models exhibit more significant QiD.", "section": "3 Scaling Laws for Low-bit Quantization"}, {"figure_path": "https://arxiv.org/html/2411.17691/x4.png", "caption": "Figure 4: The fitted scaling law of QiD with respect to the model size (i.e., the number of non-embedding parameters) in the form of Eq (6), where \u03b1\ud835\udefc\\alphaitalic_\u03b1 is fitted to be 0.2276.", "description": "Figure 4 presents the relationship between quantization-induced degradation (QiD) and model size in the context of low-bit quantization.  It shows how the amount of QiD decreases as the number of non-embedding parameters in the model increases.  The figure displays fitted curves derived from experimental data, illustrating the scaling law formulated in Equation (6).  The fitted scaling parameter \u03b1 is found to be 0.2276. This indicates that larger models exhibit less QiD than smaller models under low-bit quantization.", "section": "3 Scaling Laws for Low-bit Quantization"}, {"figure_path": "https://arxiv.org/html/2411.17691/x5.png", "caption": "Figure 5: The fitted scaling law of QiD with respect to the bit width in the form of Eq (7), where \u03b3\ud835\udefe\\gammaitalic_\u03b3 is fitted to be 5.4812.", "description": "Figure 5 presents the relationship between quantization-induced degradation (QiD) and the bit-width used for quantization. The figure shows that as the bit-width decreases, the QiD increases.  Specifically, it shows how the fitted scaling law, represented by the equation AqLoss(P) \u2248 P\u03b3, effectively models this relationship. The exponent \u03b3 (gamma) in the equation is determined through a fitting process and is found to be 5.4812. This implies that the degradation in performance due to quantization increases significantly with decreasing bit width.", "section": "3.4 Bit Width"}, {"figure_path": "https://arxiv.org/html/2411.17691/x6.png", "caption": "Figure 6: The unified scaling law we fit based on Eq (8) with the GPTQ-quantized LLMs from the Pythia suite: \u0394q\u2062L\u2062o\u2062s\u2062s\u2062(N,D,P)=0.017\u2062D0.5251/(N0.2261\u22c5P5.4967)subscript\u0394\ud835\udc5e\ud835\udc3f\ud835\udc5c\ud835\udc60\ud835\udc60\ud835\udc41\ud835\udc37\ud835\udc430.017superscript\ud835\udc370.5251\u22c5superscript\ud835\udc410.2261superscript\ud835\udc435.4967\\Delta_{q}Loss(N,D,P)=0.017D^{0.5251}/(N^{0.2261}\\cdot P^{5.4967})roman_\u0394 start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT italic_L italic_o italic_s italic_s ( italic_N , italic_D , italic_P ) = 0.017 italic_D start_POSTSUPERSCRIPT 0.5251 end_POSTSUPERSCRIPT / ( italic_N start_POSTSUPERSCRIPT 0.2261 end_POSTSUPERSCRIPT \u22c5 italic_P start_POSTSUPERSCRIPT 5.4967 end_POSTSUPERSCRIPT )", "description": "Figure 6 presents a unified scaling law that combines three factors\u2014model size (N), number of training tokens (D), and bit width (P)\u2014to predict quantization-induced degradation (\u0394qLoss) in low-bit quantized LLMs. The formula \u0394qLoss(N,D,P) = 0.017D^0.5251/(N^0.2261\u22c5P^5.4967) shows that QiD decreases as model size (N) increases and as bit-width (P) increases.  Conversely, it increases with the number of training tokens (D). The figure visually represents this relationship using fitted curves for different bit widths, illustrating how \u0394qLoss changes with model size and training tokens for each bit width.", "section": "3 Scaling Laws for Low-bit Quantization"}, {"figure_path": "https://arxiv.org/html/2411.17691/x7.png", "caption": "Figure 7: We can predict the performance of a quantized LLM as L\u2062o\u2062s\u2062sq=L\u2062o\u2062s\u2062s16-bit+\u0394q\u2062L\u2062o\u2062s\u2062s\ud835\udc3f\ud835\udc5c\ud835\udc60subscript\ud835\udc60\ud835\udc5e\ud835\udc3f\ud835\udc5c\ud835\udc60subscript\ud835\udc6016-bitsubscript\u0394\ud835\udc5e\ud835\udc3f\ud835\udc5c\ud835\udc60\ud835\udc60Loss_{q}=Loss_{\\textrm{16-bit}}+\\Delta_{q}Lossitalic_L italic_o italic_s italic_s start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT = italic_L italic_o italic_s italic_s start_POSTSUBSCRIPT 16-bit end_POSTSUBSCRIPT + roman_\u0394 start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT italic_L italic_o italic_s italic_s, where L\u2062o\u2062s\u2062s16-bit\ud835\udc3f\ud835\udc5c\ud835\udc60subscript\ud835\udc6016-bitLoss_{\\textrm{16-bit}}italic_L italic_o italic_s italic_s start_POSTSUBSCRIPT 16-bit end_POSTSUBSCRIPT can be predicted by the conventional LLM\u2019s scaling law which is fitted based on the function form of Eq (3) with the LLMs in the Pythia suite as L\u2062o\u2062s\u2062s16-bit=[(4.74\u2062e19/N)(0.045/0.399)+7.63\u2062e10/D]0.399\ud835\udc3f\ud835\udc5c\ud835\udc60subscript\ud835\udc6016-bitsuperscriptdelimited-[]superscript4.74superscript\ud835\udc5219\ud835\udc410.0450.3997.63superscript\ud835\udc5210\ud835\udc370.399Loss_{\\textrm{16-bit}}=[(4.74e^{19}/N)^{(0.045/0.399)}+7.63e^{10}/D]^{0.399}italic_L italic_o italic_s italic_s start_POSTSUBSCRIPT 16-bit end_POSTSUBSCRIPT = [ ( 4.74 italic_e start_POSTSUPERSCRIPT 19 end_POSTSUPERSCRIPT / italic_N ) start_POSTSUPERSCRIPT ( 0.045 / 0.399 ) end_POSTSUPERSCRIPT + 7.63 italic_e start_POSTSUPERSCRIPT 10 end_POSTSUPERSCRIPT / italic_D ] start_POSTSUPERSCRIPT 0.399 end_POSTSUPERSCRIPT.", "description": "Figure 7 illustrates how to predict the performance of a quantized large language model (LLM). It uses the equation Lossq = Loss16-bit + \u0394qLoss, where Lossq represents the cross-entropy loss of the quantized LLM, Loss16-bit represents the cross-entropy loss of the original LLM (16-bit precision), and \u0394qLoss represents the quantization-induced degradation. The figure shows that Loss16-bit can be predicted using a conventional LLM scaling law, specifically the one fitted using the LLMs from the Pythia suite with the formula: [ (4.74e^19/N)^(0.045/0.399) + 7.63e^10/D]^0.399.  The plot visually demonstrates the agreement between the predicted and observed loss values for different LLM sizes and training tokens.", "section": "3.5 Unified Scaling Law"}, {"figure_path": "https://arxiv.org/html/2411.17691/x8.png", "caption": "Figure 8: QiD results evaluated on RefinedWeb and Wikitext-2 with the 12B Pythia model.", "description": "This figure compares the quantization-induced degradation (QiD) results obtained using two different test datasets, RefinedWeb and Wikitext-2, with the 12B parameter Pythia language model. It shows that the QiD trends are largely independent of the test dataset used, indicating a robustness and generalizability of the results.", "section": "3.6 Validation with Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2411.17691/x9.png", "caption": "Figure 9: QiD results and fitted scaling laws for different quantization methods. Note that the GPTQ function here differs slightly from that in Figure 6, as it is fitted exclusively with 4-bit quantized Pythia checkpoints, whereas the function in Figure 6 is fitted using all quantized Pythia checkpoints.", "description": "Figure 9 presents a comparison of Quantization-Induced Degradation (QiD) across three different quantization methods: GPTQ, AWQ, and bitsandbytes.  The results are shown for 4-bit quantization applied to Pythia language models of various sizes and training levels.  Importantly, the GPTQ results in Figure 9 are specifically fitted using *only* the 4-bit quantized data, unlike Figure 6 where all bit-width data for GPTQ was used. This distinction highlights how the choice of data used to fit the scaling laws impacts the resulting curves.  The figure demonstrates that while the QiD trends are similar across the three quantization methods, the precise scaling laws (formulas describing the relationship between QiD and various factors) differ slightly depending on the chosen method.", "section": "3.6.2 Quantization Methods"}, {"figure_path": "https://arxiv.org/html/2411.17691/x10.png", "caption": "Figure 10: Left: Scaling laws for low-bit quantization, fitted on the LLM checkpoints of the Spectra suite, which are all trained with 300B tokens; Right: Actual \u0394q\u2062L\u2062o\u2062s\u2062ssubscript\u0394\ud835\udc5e\ud835\udc3f\ud835\udc5c\ud835\udc60\ud835\udc60\\Delta_{q}Lossroman_\u0394 start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT italic_L italic_o italic_s italic_s VS Predicted \u0394q\u2062L\u2062o\u2062s\u2062ssubscript\u0394\ud835\udc5e\ud835\udc3f\ud835\udc5c\ud835\udc60\ud835\udc60\\Delta_{q}Lossroman_\u0394 start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT italic_L italic_o italic_s italic_s that is computed based on the scaling laws fitted on Llama and Qwen.", "description": "Figure 10 presents a two-part visualization evaluating the accuracy of the proposed scaling laws for low-bit quantization.  The left panel displays the fitted scaling laws on the Spectra suite of LLMs, which share a consistent training size of 300 billion tokens. This demonstrates the application of the scaling laws to a different set of LLMs than the primary dataset used for deriving the laws. The right panel shows a comparison between the actual and predicted quantization-induced degradation (\u0394qLoss) for various LLMs (Llama and Qwen). This scatter plot visually assesses the predictive power of the scaling laws by comparing predicted \u0394qLoss values to the observed \u0394qLoss values for these models, providing a validation of their generalizability across different LLM architectures.", "section": "3.6 Validation with Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2411.17691/x11.png", "caption": "Figure 11: Fully trained LLMs suffer from much greater QiD (i.e., \u0394q\u2062L\u2062o\u2062s\u2062ssubscript\u0394\ud835\udc5e\ud835\udc3f\ud835\udc5c\ud835\udc60\ud835\udc60\\Delta_{q}Lossroman_\u0394 start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT italic_L italic_o italic_s italic_s) than undertrained LLMs.", "description": "This figure visualizes the relationship between quantization-induced degradation (QiD), model size, and the number of training tokens for large language models (LLMs).  The x and y axes represent model size (number of parameters) and number of training tokens, respectively. Each point represents a specific LLM checkpoint, colored according to the bit-width used for quantization (2, 3, or 4 bits). The color intensity of the points represents the magnitude of QiD (\u0394qLoss), with darker colors indicating higher QiD values. The plot shows a clear trend: smaller models trained with more tokens (fully trained) exhibit significantly higher QiD than larger models trained with fewer tokens (undertrained). This visual supports the paper's claim that low-bit quantization is more effective for undertrained LLMs.", "section": "4 Discussion: Low-bit Quantization Favors Undertrained LLMs"}, {"figure_path": "https://arxiv.org/html/2411.17691/x12.png", "caption": "Figure 12: Changes in model weights between adjacent checkpoints. Early (undertrained) checkpoints exhibit significant weight fluctuations during training, making the model relatively robust to weight variations. Therefore, small changes introduced by quantization have a limited impact on the model\u2019s performance. In contrast, fully trained checkpoints demonstrate very little weight fluctuations during training. As a result, low-bit quantization is likely to push weights beyond the narrow range of recent variations, leading to performance degradation or even model collapse.", "description": "This figure displays boxplots visualizing the change in model weights between consecutive checkpoints during training for both undertrained and fully trained LLMs.  The boxplots show that undertrained models exhibit significant weight fluctuations throughout training, meaning the model is tolerant of variations in weight values.  Consequently, the small weight changes introduced by low-bit quantization have minimal impact on performance. In contrast, fully-trained models display very little weight fluctuation during the later stages of training.  Low-bit quantization is more likely to shift weights outside this narrow range of recent variations, causing potential performance degradation or even model failure.  The figure provides visual evidence supporting the paper's claim that low-bit quantization is more effective for undertrained LLMs.", "section": "4 Discussion: Low-bit Quantization Favors Undertrained LLMs"}, {"figure_path": "https://arxiv.org/html/2411.17691/x13.png", "caption": "Figure 13: The number of training tokens for the state-of-the-art 7B-scale LLMs increase by nearly 50\u00d750\\times50 \u00d7 over the past 4 years. According to this trend, it is expected that the future models will have much more training tokens.", "description": "Figure 13 illustrates the rapid growth in the number of training tokens used for training state-of-the-art 7B parameter Large Language Models (LLMs) over the past four years.  The figure shows a nearly 50-fold increase in training tokens.  The x-axis represents the year (2020-2024), and the y-axis represents the number of training tokens in trillions.  Each bar represents a specific LLM and its corresponding training tokens. This trend strongly suggests that future LLMs will utilize significantly larger amounts of training data.", "section": "4.3 QiD Prediction When Scaling to 100 Trillion Training Tokens"}, {"figure_path": "https://arxiv.org/html/2411.17691/x14.png", "caption": "Figure 14: Training losses of BitNet and its 16-bit counterparts show a trend similar to that of low-bit quantization \u2013 they tend to perform well when undertrained but struggle to match the performance of fully trained LLMs.", "description": "Figure 14 presents a comparison of training loss curves for 120M and 1.2B parameter decoder-only language models trained using both standard 16-bit precision (bf16) and the 1-bit BitNet method.  The plot shows that in the early stages of training, BitNet's performance is comparable or even superior to the 16-bit models, primarily due to BitNet employing a higher learning rate.  However, as training progresses, the 16-bit models pull ahead, demonstrating the performance gap that widens further with larger models. This divergence highlights the limitations of low-bit, 1-bit training, particularly in later training stages, where the benefit of higher precision becomes increasingly crucial for achieving optimal model performance.  It provides visual evidence supporting the claim that native low-bit LLMs also favor undertrained models in a manner similar to post-training quantization.", "section": "4 From Low-bit Quantization to Low-bit LLMs"}]