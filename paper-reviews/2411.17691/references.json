{"references": [{"fullname_first_author": "Jared Kaplan", "paper_title": "Scaling laws for neural language models", "publication_date": "2020-01-28", "reason": "This paper introduces scaling laws for neural language models, which are foundational to understanding the relationship between model size, training data, and performance, a core concept in the current work."}, {"fullname_first_author": "Stella Biderman", "paper_title": "Pythia: A suite for analyzing large language models across training and scaling", "publication_date": "2023-07-01", "reason": "The Pythia suite of models is the primary dataset used in the experiments, providing a controlled setting for analyzing the impact of low-bit quantization across various model sizes and training levels."}, {"fullname_first_author": "Elias Frantar", "paper_title": "GPTQ: Accurate post-training quantization for generative pre-trained transformers", "publication_date": "2022-10-26", "reason": "GPTQ is the primary quantization technique used in the paper, and understanding its properties and performance is critical for interpreting the results."}, {"fullname_first_author": "Jordan Hoffmann", "paper_title": "Training compute-optimal large language models", "publication_date": "2022-03-24", "reason": "This paper provides insights into compute-optimal training of LLMs, which is relevant to understanding the scaling considerations of training increasingly larger models as mentioned in the current work."}, {"fullname_first_author": "Shuming Ma", "paper_title": "The era of 1-bit LLMs: All large language models are in 1.58 bits", "publication_date": "2024-02-15", "reason": "This paper explores the potential of 1-bit LLMs and provides a comparative analysis with higher-bit LLMs, showing the trend of moving toward lower-bit models for efficiency"}]}