[{"content": "| Model Size | $\n\\Delta_{q}\n$Loss = 0.2 | $\n\\Delta_{q}\n$Loss = 0.3 | $\n\\Delta_{q}\n$Loss = 0.4 | $\n\\Delta_{q}\n$Loss = 0.5 |  |  |  |  |  |  |  |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|  | 2 bits | 3 bits | 4 bits | 2 bits | 3 bits | 4 bits | 2 bits | 3 bits | 4 bits | 2 bits | 3 bits | 4 bits |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| 1B | 0.0011 | 0.1089 | 1.4424 | 0.0025 | 0.1990 | 2.6786 | 0.0043 | 0.3051 | 4.1556 | 0.0066 | 0.4251 | 5.8422 |\n| 7B | 0.0026 | 0.3038 | 4.5066 | 0.0057 | 0.5550 | 8.3689 | 0.0099 | 0.8512 | 12.9836 | 0.0152 | 1.1860 | 18.2531 |\n| 70B | 0.0071 | 1.0228 | **17.3499** | 0.0154 | 1.8687 | 32.2192 | 0.0267 | 2.8659 | 49.9854 | 0.0409 | 3.9932 | 70.2723 |\n| 405B | 0.0151 | 2.5807 | **48.4861** | 0.0328 | 4.7151 | 90.0398 | 0.0567 | 7.2311 | 139.6892 | 0.0868 | 10.0754 | 196.3829 |", "caption": "Table 1: Prediction of the number of training tokens (in trillion) needed to achieve a given training level measured by \u0394q\u2062L\u2062o\u2062s\u2062ssubscript\u0394\ud835\udc5e\ud835\udc3f\ud835\udc5c\ud835\udc60\ud835\udc60\\Delta_{q}Lossroman_\u0394 start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT italic_L italic_o italic_s italic_s for different model sizes and bit widths. Note that \u0394q\u2062L\u2062o\u2062s\u2062s=0.2subscript\u0394\ud835\udc5e\ud835\udc3f\ud835\udc5c\ud835\udc60\ud835\udc600.2\\Delta_{q}Loss=0.2roman_\u0394 start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT italic_L italic_o italic_s italic_s = 0.2 means the likelihood is reduced to 80% of its original value (e\u22120.2\u22480.8superscript\ud835\udc520.20.8e^{-0.2}\\approx 0.8italic_e start_POSTSUPERSCRIPT - 0.2 end_POSTSUPERSCRIPT \u2248 0.8), while \u0394q\u2062L\u2062o\u2062s\u2062s=0.5subscript\u0394\ud835\udc5e\ud835\udc3f\ud835\udc5c\ud835\udc60\ud835\udc600.5\\Delta_{q}Loss=0.5roman_\u0394 start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT italic_L italic_o italic_s italic_s = 0.5 means the likelihood is reduced to 60% (e\u22120.5\u22480.6superscript\ud835\udc520.50.6e^{-0.5}\\approx 0.6italic_e start_POSTSUPERSCRIPT - 0.5 end_POSTSUPERSCRIPT \u2248 0.6).", "description": "This table presents estimations of the number of training tokens (in trillions) needed to reach specific training levels for different model sizes and bit-widths using low-bit quantization. The training level is measured by the quantization-induced degradation (QiD), denoted as \u0394qLoss.  A \u0394qLoss of 0.2 signifies an 80% reduction in likelihood (e\u207b\u2070\u00b7\u00b2 \u2248 0.8), while a \u0394qLoss of 0.5 indicates a 60% reduction (e\u207b\u2070\u00b7\u2075 \u2248 0.6). The table shows these token estimations for four different QiD values (0.2, 0.3, 0.4, 0.5) across various model sizes (1B, 7B, 70B, 405B parameters) and bit-widths (2, 3, and 4 bits).", "section": "4.2 QiD: A Signal that Measures an LLM's Training Level"}]