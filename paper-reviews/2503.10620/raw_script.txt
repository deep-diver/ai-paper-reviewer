[{"Alex": "Hey podcast listeners, get ready to have your minds blown! We're diving deep into AI territory today, exploring how we can teach computers to not only read but also\u2026 hear! It\u2019s all about bridging the gap between text and speech, and it's way cooler than it sounds. Joining me is Jamie, ready to unearth some AI secrets.", "Jamie": "Hey Alex, I am excited to learn more. Hearing computers, it is really cool!"}, {"Alex": "Absolutely! At its core, this research is about expanding what large language models, or LLMs, can do. We're taking these AI giants, which are typically trained on text, and giving them the ability to understand spoken language, too.", "Jamie": "Okay, so like, turning a text-only AI into something that can understand spoken words? How is this achieved?"}, {"Alex": "Exactly! The key lies in something called 'speech discretization.' Think of it like converting speech into a series of distinct units, similar to how words are broken down into letters. This allows us to treat speech as another language the AI can learn.", "Jamie": "Hmm, like teaching an AI that already knows English, Spanish, and French, a whole new language, but it\u2019s actually sound? Interesting."}, {"Alex": "Spot on, Jamie! And this allows us to integrate speech without sacrificing the AI's existing text-based skills. In fact, we build upon an already multilingual model called TOWER.", "Jamie": "You mentioned the AI model TOWER. What exactly is this, and why was it the perfect base for integrating a speech component?"}, {"Alex": "TOWER is an open-source multilingual LLM and its pre-training setup is designed to handle multiple languages. Thus, we treat 'discretized speech' as just another language for TOWER to master, which is crucial because it allows TOWER to translate and transcribe existing text while handling audio inputs. So, we aren't breaking down its current skillset to build something new, but just expanding on it.", "Jamie": "Okay, so it\u2019s like, you\u2019re not replacing any parts, just adding on? What did you name the resulting model?"}, {"Alex": "Exactly! The resulting open-source model we created is called SPIRE! SPIRE can process English speech, perform ASR and speech translation, and maintain TOWER's original performance on translation tasks, without sacrificing its existing strengths.", "Jamie": "That\u2019s amazing. So, SPIRE isn't just understanding speech; it's also translating it and, like, keeping up with its original language skills? What specific benefits does this approach offer?"}, {"Alex": "Precisely! SPIRE leverages pre-existing language abilities with speech processing by utilizing the LLM's superior language modeling capabilities for disambiguating transcripts. So, it is also more effective than simply plugging an ASR output into a text-based LLM.", "Jamie": "So, the model is more accurate since it processes the speech directly rather than using a third-party ASR system? You mentioned there are existing speech-to-text solutions, so how does SPIRE stack up against those methods?"}, {"Alex": "That's right. Traditional methods often involve complex setups with separate speech processing components. SPIRE simplifies this, streamlines training, and cuts costs since we can avoid needing huge amounts of raw speech data for training the projector module.", "Jamie": "Interesting, it sounds way more efficient, but are there any tradeoffs to this? Is there any downside to using discretized speech units?"}, {"Alex": "That's a really insightful question, Jamie. Absolutely, there are some things to consider. Discretization can lead to some information loss compared to working directly with raw audio waveforms. The trick is to find the right balance and minimize that loss.", "Jamie": "Umm, that makes sense. The devil is in the details, as usual! So, if I understand correctly, SPIRE is like TOWER but with the ability to understand and translate speech without losing any of its text skills?"}, {"Alex": "You got it. So we put SPIRE through a rigorous training process with two key stages: continued pre-training (CPT) and instruction tuning (IT). The CPT stage mixed text and ASR data to extend the models recognition and comprehension, then we utilize IT for tasks such as speech to text and speech translation.", "Jamie": "So, it's like giving SPIRE a broad foundation and then teaching it specific skills? And what kind of data was used for this training? I am curious."}, {"Alex": "For pre-training, we used a massive dataset combining text from sources like the mC4 web crawl and speech data from sources like the SPGI Speech and GigaSpeech datasets. For the instruction tuning phase, we used TowerBlocks along with speech data translated to all nine target languages of TOWER.", "Jamie": "That sounds like a whole lot of data! How long did it take to actually train SPIRE with all of this information?"}, {"Alex": "The initial CPT stage took about six days using eight high-powered GPUs. The instruction tuning stage was a bit faster, taking about two and a half days with four GPUs, so it was a time commitment for sure.", "Jamie": "Wow, that\u2019s some serious processing power! Did you evaluate whether incorporating text tasks during the IT phase impacted the ASR performance of SPIRE?"}, {"Alex": "Great question! In fact, we ran experiments to investigate that directly. We found that incorporating the text tasks during IT didn't negatively impact ASR performance. So, SPIRE could still \u2018hear\u2019 just as well, even after being trained on more text-focused tasks.", "Jamie": "That is good to know. It would defeat the purpose if the translation tasks nerfed the voice translations."}, {"Alex": "Exactly, which is why it's one of our key findings. Speaking of the results, SPIRE comes pretty close to dedicated speech models and even beats some others like the HuBERT baseline, while outperforming SeamlessM4T on MT.", "Jamie": "Wow, so it\u2019s not just good at understanding speech, it\u2019s actually translating it better than some specialized models? How do you measure the success of the model? Like, is it just about accuracy?"}, {"Alex": "For ASR, we used Word Error Rate, which measures the difference between the AI's transcription and the actual words spoken. For translation, we used COMET-22 and something called spBLEU, which measure the quality and fluency of the translation.", "Jamie": "Okay, so pretty standard metrics for this kind of thing. Now, if someone wanted to use SPIRE, how accessible is it? Is it something that other researchers or developers can easily build upon?"}, {"Alex": "Absolutely! We are committed to open research. Our code, the trained models, and even the datasets we used are all publicly available. Anyone can download them and start experimenting.", "Jamie": "That's fantastic! Making it open-source will definitely help others build on your work. What do you see as the next steps for this research?"}, {"Alex": "That's what we're hoping for! One exciting direction is expanding this approach to more languages by replacing our English HuBERT component with multilingual speech encoders like mHuBERT-147. We can also explore adding more text based skills.", "Jamie": "Oh, so then SPIRE could understand even more languages? I have another question but first, what is HuBERT?"}, {"Alex": "mHuBERT-147 is another pre-trained speech model, much like TOWER. HuBERT is self-supervised model for speech pre-training, and we are thinking of expanding this approach. Ultimately, we want to build AI that can seamlessly understand and translate across languages and modalities.", "Jamie": "Okay! I can see where this could go, with near-perfect ASR and live interpretations. Are there limitations on the current model?"}, {"Alex": "Of course. As of current, SPIRE only handles speech and text on the input, so only handles text on the generation side. We also only looked at a narrow set of tasks with ASR/ST and MT tasks, so we are limited from getting the whole picture.", "Jamie": "Well, that makes sense, still sounds like a massive breakthrough. If you had to summarise, what is the key impact of your work?"}, {"Alex": "In a nutshell, our work demonstrates a simple and efficient way to equip text-based LLMs with speech understanding capabilities without sacrificing their existing strengths. It also underscores the potential of discretized speech units as a bridge between language and audio, opening new avenues for multimodal AI. Thanks for joining me, Jamie!", "Jamie": "Thanks, Alex!"}]