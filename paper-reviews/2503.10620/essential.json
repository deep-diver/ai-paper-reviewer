{"importance": "This paper is important because it provides a recipe to add speech to LLMs, enabling multimodal capabilities, which is a growing field. It offers insights into the trade-offs and best practices for integrating speech w/o compromising existing text capabilities. **The reproducible pipeline and open-source models facilitate further research and development in multilingual speech processing and multimodal LLMs.**", "summary": "SPIRE: Adds speech to text-only LLMs, maintaining text performance via discretized speech and continued pre-training.", "takeaways": ["Speech can be integrated as a translation language.", "Continued pre-training and instruction tuning are essential for optimal speech task performance.", "Speech integration can be achieved without sacrificing the LLM's original textual task capabilities."], "tldr": "Large Language Models have excelled in various tasks, leading to interest in multimodal integration, like speech. Existing methods link ASR to LLMs, missing disambiguation, or use modality projection with high costs. Speech discretization transforms speech into discrete units, simplifying training. However, development focuses on speech tasks, neglecting textual performance preservation.\n\nThis paper introduces **SPIRE, a speech-augmented LLM** built atop TOWER.  It transcribes and translates English speech, maintaining original translation capabilities. Using HuBERT-based k-means, SPIRE undergoes continued pre-training with ASR data and instruction tuning. Code and models are available, showing effective speech input integration as an additional language during adaptation.", "affiliation": "Paris-Saclay University", "categories": {"main_category": "Multimodal Learning", "sub_category": "Vision-Language Models"}, "podcast_path": "2503.10620/podcast.wav"}