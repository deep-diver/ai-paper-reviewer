[{"figure_path": "2410.18785/tables/table_22_0.md", "caption": "Table 1: Evaluation results of GPT2-XL. experiments are conducted on a sever with 8 RTX 4090 GPUs.", "description": "Table 1 presents the evaluation results for the GPT-2-XL language model using various model editing methods.  The table shows the performance (measured using MMLU, GSM8K, BBH, and CSQA benchmarks) of the unedited model and models edited using PMET, MEND, and KN methods across different numbers of edits (10, 20, 50, 100, 500, 1000). Each row represents a specific editing method and the number of edits applied, while the columns display the performance scores on the four different benchmarks. The scores are likely represented as some kind of accuracy or other relevant metric, though the specific type of metric isn't specified in the provided caption.", "section": "C.1 RQ1: Impact of the Number of Edits"}, {"figure_path": "2410.18785/tables/table_24_0.md", "caption": "Table 2: Results on evaluating the impact of different editing methods and numbers of edits on edited language models (base model). All editing is conducted on COUNTERFACT dataset with a fixed seed for a fair comparison. For all 4 tasks in this table, the higher score indicates a better performance. MEND and GRACE are not available for Mistral-7B.", "description": "This table presents the quantitative evaluation results of different model editing methods on two base language models, Llama2-7B and Mistral-7B, across various numbers of edits.  The performance of each model is measured on four tasks (MMLU, GSM8K, BBH, CSQA) before and after editing with different methods (ROME, MEMIT, PMET, GRACE, MEND, KN).  The results show the scores achieved on each task, indicating the impact of the number of edits and the editing method on the model's performance. Notably, MEND and GRACE methods are not applied to the Mistral-7B model in this experiment. The table allows for a comparison of the effectiveness of various editing methods in maintaining model capabilities with increasing numbers of edits.", "section": "C.1 RQ1: Impact of the Number of Edits"}, {"figure_path": "2410.18785/tables/table_25_0.md", "caption": "Table 2: Results on evaluating the impact of different editing methods and numbers of edits on edited language models (base model). All editing is conducted on COUNTERFACT dataset with a fixed seed for a fair comparison. For all 4 tasks in this table, the higher score indicates a better performance. MEND and GRACE are not available for Mistral-7B.", "description": "This table presents the results of evaluating the impact of different editing methods and the number of edits on the general abilities of base language models.  It includes results for Llama2-7B and Mistral-7B models, across four tasks (MMLU, GSM8K, BBH, CSQA) and six editing methods (ROME, MEMIT, PMET, GRACE, MEND, KN). Each method is evaluated at various numbers of edits (1, 5, 10, 20, 50, 100, 500, 1000). The table shows the performance (higher scores indicate better performance) on each task for each method and number of edits, with the \"w/o Edit\" row representing the performance of the unedited model.  Note that MEND and GRACE methods are unavailable for Mistral-7B model.", "section": "C.1 RQ1: Impact of the Number of Edits"}, {"figure_path": "2410.18785/tables/table_26_0.md", "caption": "Table 2: Results on evaluating the impact of different editing methods and numbers of edits on edited language models (base model). All editing is conducted on COUNTERFACT dataset with a fixed seed for a fair comparison. For all 4 tasks in this table, the higher score indicates a better performance. MEND and GRACE are not available for Mistral-7B.", "description": "This table presents the results of evaluating the impact of different model editing methods and varying numbers of edits on the general capabilities of several base language models.  It includes results for Llama2-7B and Mistral-7B models across four benchmarks: MMLU, GSM8K, BBH, and CSQA, measuring performance using various editing methods (ROME, MEMIT, PMET, GRACE, MEND, KN) with different edit numbers (1, 5, 10, 20, 50, 100, 500, 1000). The table shows that the performance of the models varies considerably depending on the editing method and the number of edits applied.  Note that MEND and GRACE methods were not available for Mistral-7B.", "section": "4.1 RQ1: Impact of the Number of Edits"}, {"figure_path": "2410.18785/tables/table_27_0.md", "caption": "Table 2: Results on evaluating the impact of different editing methods and numbers of edits on edited language models (base model). All editing is conducted on COUNTERFACT dataset with a fixed seed for a fair comparison. For all 4 tasks in this table, the higher score indicates a better performance. MEND and GRACE are not available for Mistral-7B.", "description": "Table 2 presents a quantitative evaluation of the impact of various model editing methods and the number of edits on the performance of base language models (not instruction-tuned).  The table compares the performance of unedited models (w/o Edit) against models edited using six different methods (ROME, MEMIT, PMET, GRACE, MEND, KN) across four benchmarks (MMLU, GSM8K, BBH, CSQA) for varying numbers of edits (1, 5, 10, 20, 50, 100, 500, 1000).  The results for Llama2-7B and Mistral-7B models are shown.  Higher scores indicate better performance on each benchmark.", "section": "RQ1: Impact of the Number of Edits"}, {"figure_path": "2410.18785/tables/table_27_1.md", "caption": "Table 2: Results on evaluating the impact of different editing methods and numbers of edits on edited language models (base model). All editing is conducted on COUNTERFACT dataset with a fixed seed for a fair comparison. For all 4 tasks in this table, the higher score indicates a better performance. MEND and GRACE are not available for Mistral-7B.", "description": "Table 2 presents a quantitative analysis of the impact of various model editing methods and the number of edits on the performance of base language models.  It compares the performance of unedited models (w/o Edit) against models edited using six different methods (ROME, MEMIT, PMET, GRACE, MEND, and KN) across four benchmark tasks (MMLU, GSM8K, BBH, and CSQA) and varying numbers of edits (1, 5, 10, 20, 50, 100, 500, and 1000). The results are shown separately for Llama2-7B and Mistral-7B models, indicating the performance (higher score representing better performance) on each task for each editing method and number of edits. Note that MEND and GRACE methods are unavailable for Mistral-7B.", "section": "4.1 RQ1: Impact of the Number of Edits"}, {"figure_path": "2410.18785/tables/table_28_0.md", "caption": "Table 2: Results on evaluating the impact of different editing methods and numbers of edits on edited language models (base model). All editing is conducted on COUNTERFACT dataset with a fixed seed for a fair comparison. For all 4 tasks in this table, the higher score indicates a better performance. MEND and GRACE are not available for Mistral-7B.", "description": "Table 2 presents a quantitative analysis of the impact of various model editing methods and the number of edits on the performance of base language models (excluding instruction-tuned models).  The table compares the performance of unedited and edited Llama2-7B and Mistral-7B models across four evaluation benchmarks (MMLU, GSM8K, BBH, and CSQA) after applying six different editing methods (ROME, MEMIT, PMET, GRACE, MEND, and KN).  Results are shown for different numbers of edits (1, 5, 10, 20, 50, 100, 500, 1000).  The table highlights the performance variations of each language model under the influence of different editing approaches and edit counts, allowing for a comparison of their relative effectiveness.", "section": "C.1 RQ1: Impact of the Number of Edits"}]