[{"figure_path": "2410.18785/tables/table_22_0.html", "caption": "Table 1: Evaluation results of GPT2-XL. experiments are conducted on a sever with 8 RTX 4090 GPUs.", "description": "Table 1 presents the evaluation results of GPT2-XL model with different editing methods and various numbers of edits on MMLU, GSM8K, BBH, and CSQA benchmarks.", "section": "C.1 RQ1: Impact of the Number of Edits"}, {"figure_path": "2410.18785/tables/table_25_0.html", "caption": "Table 2: Results on evaluating the impact of different editing methods and numbers of edits on edited language models (base model). All editing is conducted on COUNTERFACT dataset with a fixed seed for a fair comparison. For all 4 tasks in this table, the higher score indicates a better performance. MEND and GRACE are not available for Mistral-7B.", "description": "This table presents the results of evaluating the impact of different model editing methods and numbers of edits on the general abilities of base language models across various benchmarks.", "section": "C.1 RQ1: Impact of the Number of Edits"}, {"figure_path": "2410.18785/tables/table_26_0.html", "caption": "Table 2: Results on evaluating the impact of different editing methods and numbers of edits on edited language models (base model). All editing is conducted on COUNTERFACT dataset with a fixed seed for a fair comparison. For all 4 tasks in this table, the higher score indicates a better performance. MEND and GRACE are not available for Mistral-7B.", "description": "Table 2 presents the results of evaluating the impact of various model editing methods and different numbers of edits on the general abilities of base language models across multiple benchmarks.", "section": "C Detailed Experimental Results"}, {"figure_path": "2410.18785/tables/table_27_0.html", "caption": "Table 2: Results on evaluating the impact of different editing methods and numbers of edits on edited language models (base model). All editing is conducted on COUNTERFACT dataset with a fixed seed for a fair comparison. For all 4 tasks in this table, the higher score indicates a better performance. MEND and GRACE are not available for Mistral-7B.", "description": "Table 2 presents the performance of various language models (base models) after applying different editing methods with varying numbers of edits, evaluated across four benchmarks.", "section": "C Detailed Experimental Results"}, {"figure_path": "2410.18785/tables/table_27_1.html", "caption": "Table 7: Comparison of time costs for different benchmarks with and without vLLM using the Llama2-7B model. The unit is minutes. The table demonstrates that using vLLM significantly reduces the time costs across all benchmarks.", "description": "Table 7 compares the time costs of running benchmarks with and without the vLLM inference framework to show that using vLLM significantly reduces the time costs.", "section": "D.2 Evaluation Efficiency"}, {"figure_path": "2410.18785/tables/table_28_0.html", "caption": "Table 2: Results on evaluating the impact of different editing methods and numbers of edits on edited language models (base model). All editing is conducted on COUNTERFACT dataset with a fixed seed for a fair comparison. For all 4 tasks in this table, the higher score indicates a better performance. MEND and GRACE are not available for Mistral-7B.", "description": "Table 2 presents a quantitative evaluation of different model editing methods' impact on the general abilities of base language models (Llama2-7B and Mistral-7B) across various numbers of edits.", "section": "C Detailed Experimental Results"}]