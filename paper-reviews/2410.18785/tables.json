[{"figure_path": "2410.18785/tables/table_22_0.html", "caption": "Table 1: Evaluation results of GPT2-XL. experiments are conducted on a sever with 8 RTX 4090 GPUs.", "description": "Table 1 presents the evaluation results of the GPT2-XL language model after editing using various methods and a different number of edits.", "section": "C.1 RQ1: Impact of the Number of Edits"}, {"figure_path": "2410.18785/tables/table_24_0.html", "caption": "Table 2: Results on evaluating the impact of different editing methods and numbers of edits on edited language models (base model). All editing is conducted on COUNTERFACT dataset with a fixed seed for a fair comparison. For all 4 tasks in this table, the higher score indicates a better performance. MEND and GRACE are not available for Mistral-7B.", "description": "Table 2 presents a quantitative analysis of the impact of different editing methods and varying numbers of edits on the general abilities of base language models across four benchmark tasks.", "section": "C.1 RQ1: Impact of the Number of Edits"}, {"figure_path": "2410.18785/tables/table_25_0.html", "caption": "Table 2: Results on evaluating the impact of different editing methods and numbers of edits on edited language models (base model). All editing is conducted on COUNTERFACT dataset with a fixed seed for a fair comparison. For all 4 tasks in this table, the higher score indicates a better performance. MEND and GRACE are not available for Mistral-7B.", "description": "Table 2 presents a quantitative evaluation of the impact of various model editing methods and the number of edits on the general capabilities of base language models across different benchmarks.", "section": "C.1 RQ1: Impact of the Number of Edits"}, {"figure_path": "2410.18785/tables/table_26_0.html", "caption": "Table 2: Results on evaluating the impact of different editing methods and numbers of edits on edited language models (base model). All editing is conducted on COUNTERFACT dataset with a fixed seed for a fair comparison. For all 4 tasks in this table, the higher score indicates a better performance. MEND and GRACE are not available for Mistral-7B.", "description": "Table 2 presents a quantitative evaluation of the impact of different model editing methods and varying numbers of edits on the general abilities of several base language models across four distinct benchmark tasks.", "section": "C.1 RQ1: Impact of the Number of Edits"}, {"figure_path": "2410.18785/tables/table_27_0.html", "caption": "Table 2: Results on evaluating the impact of different editing methods and numbers of edits on edited language models (base model). All editing is conducted on COUNTERFACT dataset with a fixed seed for a fair comparison. For all 4 tasks in this table, the higher score indicates a better performance. MEND and GRACE are not available for Mistral-7B.", "description": "Table 2 presents the results of evaluating the impact of different model editing methods and various numbers of edits on the general abilities of base language models across four benchmark tasks.", "section": "C.1 RQ1: Impact of the Number of Edits"}, {"figure_path": "2410.18785/tables/table_27_1.html", "caption": "Table 7: Comparison of time costs for different benchmarks with and without vLLM using the Llama2-7B model. The unit is minutes. The table demonstrates that using vLLM significantly reduces the time costs across all benchmarks.", "description": "Table 7 compares the time costs of running benchmarks with and without vLLM, demonstrating the significant time reduction achieved by using vLLM.", "section": "D.2 Evaluation Efficiency"}, {"figure_path": "2410.18785/tables/table_28_0.html", "caption": "Table 2: Results on evaluating the impact of different editing methods and numbers of edits on edited language models (base model). All editing is conducted on COUNTERFACT dataset with a fixed seed for a fair comparison. For all 4 tasks in this table, the higher score indicates a better performance. MEND and GRACE are not available for Mistral-7B.", "description": "Table 2 presents the performance of different model editing methods on various language models (base models) with different numbers of edits, evaluated across multiple benchmarks.", "section": "C.1 RQ1: Impact of the Number of Edits"}]