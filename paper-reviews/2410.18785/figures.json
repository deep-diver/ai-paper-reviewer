[{"figure_path": "2410.18785/figures/figures_2_0.png", "caption": "Illustration about the model editing and its pitfalls in retaining edited knowledge. Left panel: model editing methods can efficiently update knowledge within language models; Right panel: when scaling editing to thousands, the model can't retain edited knowledge, see [16] for details.", "description": "The figure is composed of two parts. The left panel illustrates the general concept of model editing, where an unedited LLM is modified through an editing process to produce an edited LLM.  The right panel presents a graph showing the performance of various model editing methods (MEND, MEMIT, PMET, ROME) on Llama2-7B across four metrics (Score, Efficacy, Generalization, Specificity) as the number of edits increases.  The graph shows that the performance generally degrades as the number of edits increases, indicating that model editing methods are not effective when scaling to thousands of edits.", "section": "RQ1: Impact of the Number of Edits"}]