{"references": [{" publication_date": "2022", "fullname_first_author": "Kevin Meng", "paper_title": "Locating and editing factual associations in GPT", "reason": "This paper is highly relevant because it introduces the ROME method, a significant model editing technique used and analyzed extensively in this study.  The paper's focus on precisely locating and modifying factual associations within LLMs directly relates to the central theme of how editing methods affect the general abilities of these models, making it crucial for understanding the context and comparison of the results in this paper.", "section_number": 1}, {" publication_date": "2021", "fullname_first_author": "Eric Mitchell", "paper_title": "Fast model editing at scale", "reason": "This paper is highly relevant due to its introduction of MEND, a key model editing method used and analyzed in the current study.  Its focus on fast and efficient model editing is essential for this paper's investigation into the scalability and potential limitations of model editing methods when applied sequentially at large scales.  The efficiency and accuracy of MEND, compared with other methods, play a central role in the overall conclusions drawn.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Eric Mitchell", "paper_title": "Memory-based model editing at scale", "reason": "This paper introduces MEMIT, another crucial model editing technique analyzed in this study. Its memory-based approach and potential benefits in terms of efficiency and scalability are important for the current study's investigation into how different editing approaches affect the general capabilities of large language models, and their safety implications.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Hugo Touvron", "paper_title": "Llama: Open and efficient foundation language models", "reason": "This paper introduces the Llama model, a significant LLM architecture used in the present study, making it a fundamental reference for understanding the context and limitations of the base model used for the editing experiments.  The Llama model's performance and characteristics are directly relevant to the analysis of how various model editing methods impact its overall capabilities and the extent of changes induced by those editing methods.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Hugo Touvron", "paper_title": "Llama 2: Open foundation and fine-tuned chat models", "reason": "This paper describes Llama 2, one of the primary LLMs used in the experiments. The introduction of Llama 2 is crucial for replicating the study's methodology and understanding the characteristics of the model, especially its performance and capabilities when subjected to various model editing techniques.  The comparisons between Llama 2 and other models are relevant for analyzing the effects of editing on different LLM architectures and sizes.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Kevin Meng", "paper_title": "Mass-editing memory in a transformer", "reason": "This paper presents a method for efficiently editing facts within large language models (LLMs). The methods presented in this paper directly support the study's experimental setup, providing a necessary foundation for replicating and validating the current study's results. The focus on efficient editing is particularly relevant to this study\u2019s analysis of the general abilities of LLMs post-editing.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Eric Mitchell", "paper_title": "Fast model editing at scale", "reason": "This paper is crucial as it introduces MEND, a model editing technique used and extensively analyzed in this study. The focus on fast and efficient model editing is central to this paper's investigation of the scalability and limitations of model editing methods, especially when applied sequentially at large scales. Comparing MEND's performance against other methods is critical for understanding the results and drawing meaningful conclusions.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Eric Mitchell", "paper_title": "Memory-based model editing at scale", "reason": "The introduction of MEMIT is crucial for this paper as MEMIT is a model editing technique investigated in the study. Its memory-based approach and scalability are significant for analyzing the effects of various editing techniques on LLMs\u2019 general capabilities. The contrast between MEMIT and other methods, especially concerning efficiency and scalability, contributes to the paper's overall conclusions and the implications for real-world applications of model editing.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Kevin Meng", "paper_title": "Locating and editing factual associations in GPT", "reason": "This paper is highly relevant to this study because it introduces ROME, a model editing technique used and analyzed extensively.  The focus on precise manipulation of factual associations is crucial to the current study's understanding of how model editing affects LLMs\u2019 overall capabilities and safety.  Comparing the performance of ROME with other methods is essential for the analysis and conclusions.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Ce Zheng", "paper_title": "Can we edit factual knowledge by in-context learning?", "reason": "This paper is important because it explores the potential of in-context learning for model editing, providing an alternative perspective and potential comparison point for the methods explicitly investigated in this study. The paper's exploration of editing knowledge via in-context learning complements the study's direct examination of other model editing methods and adds valuable context to the broader field.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Longteng Zhang", "paper_title": "Dissecting the runtime performance of the training, fine-tuning, and inference of large language models", "reason": "This paper is highly relevant because it discusses the computational resource demands of training, fine-tuning, and inference of LLMs, which is directly pertinent to the efficiency analysis of model editing methods in this study.  The resource implications are critical for evaluating the practical applicability of model editing at scale and for comparing it with the more computationally expensive approach of full model retraining.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "Dan Hendrycks", "paper_title": "Measuring massive multitask language understanding", "reason": "This paper introduces the MMLU benchmark, a crucial evaluation metric used in the present study.  The importance of MMLU stems from its assessment of world knowledge across various tasks, which makes it central to understanding the impact of model editing on the general abilities of LLMs.  The results of the edited models on MMLU are a key element in the paper's analysis and conclusions.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Stella Biderman", "paper_title": "Pythia: A suite for analyzing large language models across training and scaling", "reason": "This paper introduces the Pythia family of models, a set of LLMs used in this study.  The use of Pythia models with varying parameter scales is crucial for investigating the impact of model size on the effectiveness of model editing.  By using Pythia models, the researchers can analyze how scaling impacts the robustness of editing methods and the general capabilities of LLMs post-editing.", "section_number": 3}, {" publication_date": "2019", "fullname_first_author": "Fabio Petroni", "paper_title": "Language models as knowledge bases?", "reason": "This foundational paper explores using language models as knowledge bases, which is directly relevant to this study's investigation into model editing as a knowledge update mechanism. This paper's concept of knowledge representation within LLMs helps set the theoretical foundation for understanding the challenges of modifying and updating that knowledge, which directly impacts this study\u2019s analysis of editing methods.", "section_number": 3}, {" publication_date": "2022", "fullname_first_author": "Xiaopeng Li", "paper_title": "PMET: Precise model editing in a transformer", "reason": "This paper introduces the PMET model editing technique, which is essential to this study. The PMET method's capabilities and limitations, especially when used in sequential editing, are important for the analysis and conclusions of this study.  Comparing PMET's performance with other methods helps to understand the relative strengths and weaknesses of various techniques for updating LLMs.", "section_number": 3}, {" publication_date": "2022", "fullname_first_author": "Damai Dai", "paper_title": "Knowledge neurons in pretrained transformers", "reason": "This work explores the concept of knowledge neurons within transformer models, which provides a valuable theoretical context for understanding how model editing works and affects the underlying knowledge representation. This paper's insights into the neural architecture of LLMs are helpful for interpreting the experimental findings regarding the effects of model editing on different aspects of the LLM's capabilities.", "section_number": 3}, {" publication_date": "2018", "fullname_first_author": "Alon Talmor", "paper_title": "CommonsenseQA: A question answering challenge targeting commonsense knowledge", "reason": "This paper introduces the CommonsenseQA benchmark, which directly supports the experimental setup of this study.  CommonsenseQA is a benchmark used to evaluate the impact of model editing on the commonsense reasoning capabilities of LLMs. The results obtained on CommonsenseQA help analyze the effects of editing on specific aspects of LLM abilities, contributing to the paper's overall evaluation and conclusions.", "section_number": 3}, {" publication_date": "2020", "fullname_first_author": "Karl Cobbe", "paper_title": "Training verifiers to solve math word problems", "reason": "This work introduces the GSM8K benchmark, which is used to evaluate the edited models\u2019 mathematical reasoning abilities. The GSM8K benchmark's focus on arithmetic problems is particularly relevant to this study's examination of how model editing impacts specific cognitive abilities of LLMs. Results on GSM8K contribute to the analysis of the general capabilities and potential limitations of model editing.", "section_number": 3}, {" publication_date": "2020", "fullname_first_author": "Mirac Suzgun", "paper_title": "Challenging big-bench tasks and whether chain-of-thought can solve them", "reason": "This paper introduces the Big-Bench Hard benchmark, another crucial component of this study's experimental design. The Big-Bench Hard benchmark is used to evaluate the effect of model editing on the general knowledge and reasoning capabilities of LLMs, which complements the assessment using MMLU and other benchmarks. The comprehensive nature of Big-Bench Hard enhances the validity and reliability of the study's results.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Zhoubo Li", "paper_title": "Unveiling the pitfalls of knowledge editing for large language models", "reason": "This paper is highly relevant as it directly addresses the pitfalls of knowledge editing, a central theme of this study. The paper's examination of knowledge distortion and other limitations caused by model editing is crucial for understanding and interpreting the experimental results. It adds valuable context and reinforces the need for further research on safer and more reliable editing methods.", "section_number": 4}]}