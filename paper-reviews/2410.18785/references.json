{"references": [{" publication_date": "2022", "fullname_first_author": "OpenAI", "paper_title": "Introducing chatgpt", "reason": "This paper introduces ChatGPT, a foundational large language model that has significantly impacted the field and serves as a key model for comparison in this study's evaluation of model editing techniques.", "section_number": 1}, {" publication_date": "March 2024", "fullname_first_author": "Anthropic", "paper_title": "The Claude 3 Model Family: Opus, Sonnet, Haiku", "reason": "This paper introduces the Claude family of LLMs, providing another important baseline model against which the impacts of model editing are evaluated in this study.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Hugo Touvron", "paper_title": "Llama: Open and efficient foundation language models", "reason": "This paper introduces the Llama LLM, a key model used in this study to evaluate the effectiveness of various model editing techniques.  Its open nature and efficient design make it a significant model for this research.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Hugo Touvron", "paper_title": "Llama 2: Open foundation and fine-tuned chat models", "reason": "This paper introduces Llama 2, another crucial LLM in this study, offering both base and fine-tuned variants. The fine-tuned model allows the authors to study the impacts of instruction tuning on editing.", "section_number": 1}, {" publication_date": "2019", "fullname_first_author": "Fabio Petroni", "paper_title": "Language models as knowledge bases?", "reason": "This foundational paper explores the potential of using language models as knowledge bases, providing a conceptual background relevant to the use of LLMs in this study.  The concept of LLMs as knowledge repositories is central to the idea of editing knowledge within these models.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Shirui Pan", "paper_title": "Unifying large language models and knowledge graphs: A roadmap", "reason": "This paper provides a valuable roadmap for integrating LLMs and knowledge graphs, a relevant consideration when analyzing knowledge editing techniques that affect the relationships between knowledge in LLMs.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Karan Singhal", "paper_title": "Large language models encode clinical knowledge", "reason": "This paper highlights the potential for errors, harm, and obsolescence in LLMs\u2019 knowledge, directly motivating the need for model editing techniques investigated in this study.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Longteng Zhang", "paper_title": "Dissecting the runtime performance of the training, fine-tuning, and inference of large language models", "reason": "This paper explores the computational costs of training, fine-tuning, and inference for large language models, offering a relevant context for discussing the resource constraints often associated with directly fine-tuning LLMs \u2013 a key motivation behind the rise of model editing.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Zhenheng Tang", "paper_title": "Fusionai: Decentralized training and deploying llms with massive consumer-level gpus", "reason": "This paper showcases the distributed training techniques that could potentially mitigate the hardware limitations often cited as a barrier to directly fine-tuning large language models, underscoring the importance of efficient model editing methods like the ones explored in this study.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Zhenheng Tang", "paper_title": "Fusionllm: A decentralized llm training system on geo-distributed gpus with adaptive compression", "reason": "Similar to the previous citation, this paper focuses on distributed training for LLMs, which underscores the motivation for efficient model updating techniques such as model editing.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Song Wang", "paper_title": "Knowledge editing for large language models: A survey", "reason": "This survey provides a broad overview of the existing model editing methods and is crucial for contextualizing the authors\u2019 work within the larger field of research on model editing. It helps to situate the authors' contributions in relation to existing literature.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Vittorio Mazzia", "paper_title": "A survey on knowledge editing of neural networks", "reason": "This survey provides a broad overview of knowledge editing methods for neural networks. This offers a wider perspective beyond Language Models, useful for understanding the broader context and challenges in knowledge editing.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Ningyu Zhang", "paper_title": "A comprehensive study of knowledge editing for large language models", "reason": "This paper provides a comprehensive analysis of knowledge editing for LLMs, covering many aspects including efficiency, scalability and safety, making it highly relevant to this study which also evaluates the effectiveness of knowledge editing across multiple factors.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Ce Zheng", "paper_title": "Can we edit factual knowledge by in-context learning?", "reason": "This study explores the potential of in-context learning for editing factual knowledge, providing a related approach that can be compared and contrasted to the methods investigated in this study.", "section_number": 1}, {" publication_date": "2021", "fullname_first_author": "Eric Mitchell", "paper_title": "Fast model editing at scale", "reason": "This paper introduces MEND, a significant model editing method that is included and analyzed in this study.  The method is central to the experimental evaluations and forms a key comparison point for other approaches.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Kevin Meng", "paper_title": "Locating and editing factual associations in gpt", "reason": "This study introduces the ROME method, another key editing technique included in this study. Understanding its strengths and weaknesses is crucial to the comparative analysis performed.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Eric Mitchell", "paper_title": "Memory-based model editing at scale", "reason": "This study introduces MEMIT, an important model editing method that is evaluated in this research.  The paper's methods and findings are directly relevant to the comparative analysis performed here.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Yunzhi Yao", "paper_title": "Editing large language models: Problems, methods, and opportunities", "reason": "This paper reviews various methods for editing LLMs and identifies challenges and opportunities.  Its overview of the field is critical for placing this study's contributions into the broader research landscape.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Kevin Meng", "paper_title": "Mass-editing memory in a transformer", "reason": "This paper introduces PMET, another model editing method that forms a part of the experimental evaluation conducted in the present study. Understanding its performance is crucial for drawing comprehensive conclusions.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Qi Li", "paper_title": "Can we continually edit language models?", "reason": "This study explores the impact of knowledge attenuation during sequential editing, addressing a critical limitation of model editing methods.  It is directly relevant to this study because it emphasizes the importance of understanding the long-term effects of edits on model behavior.", "section_number": 1}]}