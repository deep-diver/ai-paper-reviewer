{"reason": "This research paper investigates the impact of model editing techniques on the general capabilities of large language models (LLMs). The authors perform a comprehensive evaluation across various editing methods, LLM sizes, and datasets, revealing limitations in existing methods, especially when scaling edits beyond a certain threshold.", "summary": "Model editing boosts LLMs, but scaling edits hurts performance and safety!", "takeaways": ["Existing model editing methods cause inevitable performance degradation on general benchmarks as the number of edits increases.", "Instruction-tuned LLMs are more robust to editing than base models, exhibiting less performance drop.", "Current editing methods are only suitable for small-scale knowledge updates, highlighting the need for more practical and reliable editing methods."], "tldr": "This paper evaluates the effectiveness and limitations of current Large Language Model (LLM) editing methods.  The researchers tested multiple editing techniques across various LLMs and datasets.  They found that while LLM editing can successfully update specific knowledge, scaling the number of edits leads to significant performance drops across various benchmarks.  Instruction-tuned models performed better after editing than their base counterparts, showing more resistance to performance decline.  Larger models also showed greater resistance to edit-related issues.  Critically, even safety-focused models exhibited reduced safety after editing. The study concludes that current LLM editing methods are only appropriate for very small-scale knowledge updates, indicating a critical need for improved methods that can perform large-scale editing without compromising performance or safety."}