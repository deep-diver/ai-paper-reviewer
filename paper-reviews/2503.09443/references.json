{"references": [{"fullname_first_author": "Desmond Elliott", "paper_title": "Multi30K: Multilingual English-German image descriptions.", "publication_date": "2016-01-01", "reason": "Multi30K is a key dataset used for multimodal machine translation and image description, serving as a benchmark for evaluating the models in the paper."}, {"fullname_first_author": "Marta R. Costa-juss\u00e0", "paper_title": "No Language Left Behind: Scaling human-centered machine translation.", "publication_date": "2022-07-05", "reason": "NLLB is a significant multilingual machine translation model that provides a strong baseline and a component for some of the models compared against in the paper."}, {"fullname_first_author": "Jacob Devlin", "paper_title": "BERT: Pre-training of deep bidirectional transformers for language understanding.", "publication_date": "2019-01-01", "reason": "BERT is a foundational pre-trained language model that has impacted many NLP tasks and is important for adapting models to new languages."}, {"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision.", "publication_date": "2021-01-01", "reason": "CLIP's contrastive learning approach and its ability to transfer visual knowledge from language supervision are influential in multimodal learning and are used for aligning text and images in this paper's dataset creation."}, {"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need.", "publication_date": "2017-01-01", "reason": "The Transformer architecture is fundamental to the models used in this work, as it forms the basis for both the encoder and decoder components."}]}