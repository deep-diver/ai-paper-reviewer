[{"figure_path": "https://arxiv.org/html/2503.09443/x1.png", "caption": "Figure 1: Florenz (middle) is a vision-language model trained on an incomplete dataset (left) that covers the tasks image captioning (blue) and multimodal machine translation (orange). The incompleteness concerns the availability of languages for the individual tasks, where captioning data is only available for English and German while En\u2192\u2192\\rightarrow\u2192X translation is available in all languages. Florenz generalizes to the missing captioning-language pairs during inference with sufficient scale (right).", "description": "Figure 1 illustrates the architecture and training process of the Florenz vision-language model.  The diagram shows three parts: (left) an incomplete training dataset, (middle) the Florenz model architecture, and (right) the model's inference process. The training dataset includes image captioning tasks (in blue) and multimodal machine translation tasks (in orange).  Critically, the training data for image captioning is only available in English and German, while the machine translation data covers multiple languages (English to other languages). The Florenz model, designed as a monolingual encoder-decoder VLM, leverages pre-trained monolingual models.  The diagram demonstrates that despite limited training data for image captioning, the Florenz model generalizes to other languages (unseen languages) at inference time. The model's ability to do this is linked to a scaling law, which means that a larger model with increased parameter count and training data results in better cross-lingual generalization.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.09443/x2.png", "caption": "Figure 2: Dataset generation pipeline. Input is an image dataset with short captions or alt texts and a translation dataset with bitexts in English and the target language German. (1) Image and short caption are fed into a VLM to generate a detailed English description, which is (2) translated into the target language. (3) The image and (4) all English sentences of the translation dataset are embedded in a shared vector space. (5) Cosine similarity is calculated and (6) top-N matching pairs the most similar images and translations.", "description": "This figure illustrates the process of creating a multilingual multimodal dataset.  The input consists of an image dataset with short captions or alt text, and a parallel translation dataset (bitexts) in English and German.  The pipeline involves these steps:\n\n1. **Detailed Description Generation:** An image and its short caption are fed into a Vision-Language Model (VLM) to generate a more detailed description in English.\n2. **Translation:** The detailed English description is then translated into the target language (German).\n3. **Embedding:** Both the image (using image embeddings) and all English sentences from the parallel translation dataset (using text embeddings) are converted into vector representations in a shared vector space.\n4. **Cosine Similarity Calculation:**  Cosine similarity is computed to measure the similarity between the image embedding and each English sentence embedding.\n5. **Top-N Matching:** The top N most similar image-translation pairs are selected, creating a new dataset with matched image and translated descriptions.", "section": "4. Multimodal Multilingual Dataset Creation"}, {"figure_path": "https://arxiv.org/html/2503.09443/x3.png", "caption": "Figure 3: Test cross-entropy loss (CE) for various training compute budgets (GMACs, Giga Multiply-Accumulate operations), showing the effects of different model sizes and seen training samples. We show results for the test splits (see Sec.\u00a04.1) for unseen captioning (UC) in Spanish (Es) and Chinese (Zh), seen translation (ST) in these languages, and seen captioning (SC) in English (En) and German (De). The models F-0.4B, F-1.0B, F-3.5B, and F-11.2B are trained for 500, 2k, 5k, and 10k steps, respectively, resulting in 0.5M to 10M seen samples with a batch size of 1024. Eq.\u00a01 is fitted to the points on the Pareto frontier (gray staircase graph). Higher compute budgets improve CE for UC (left), ST (middle), and SC (right). This suggests that translation facilitates generalization in captioning.", "description": "This figure displays the relationship between training compute (measured in Giga Multiply-Accumulate operations or GMACs), model size, and the cross-entropy loss (CE) for various tasks.  Three different test sets are included: unseen captioning (UC) in Spanish and Chinese, seen translation (ST) in the same languages, and seen captioning (SC) in English and German.  The results illustrate how increasing the compute budget and model size impacts the performance of each task.  The gray staircase lines in each plot depict the Pareto frontier \u2013 the optimal tradeoff between compute and loss.  The results suggest that having seen translation data helps the model generalize to unseen captioning tasks in new languages.", "section": "6. Scaling Laws"}, {"figure_path": "https://arxiv.org/html/2503.09443/x4.png", "caption": "Figure 4: Predicted test cross-entropy loss (CE) as function of model parameters in billion (B) with confidence intervals (CI) and prediction intervals (PI) for unseen captioning in Spanish and Chinese (UC, blue), seen translation in Spanish and Chinese (ST, orange), and seen captioning in English and German (SC, green). The number of seen training samples is fixed to 10M, respective measurements are shown as dots. Extrapolation is drawn dashed.", "description": "Figure 4 illustrates the scaling law relationship between model size and cross-entropy loss for three distinct scenarios.  The y-axis shows the cross-entropy loss (a measure of model error), while the x-axis represents model size in billions of parameters. The three scenarios are: (1) unseen captioning tasks in Spanish and Chinese (UC, blue), representing the ability of a model to generate image captions in languages not encountered during training; (2) seen translation tasks in Spanish and Chinese (ST, orange), where the model has already seen translation tasks involving these languages during training; and (3) seen captioning tasks in English and German (SC, green), representing the model's performance in languages it has seen extensively during training.  The number of training samples was held constant at 10 million for all scenarios. The plot includes confidence intervals (CI) and prediction intervals (PI) to indicate the uncertainty associated with the estimates.  The dashed lines represent extrapolations to larger model sizes.", "section": "6. Scaling Laws"}, {"figure_path": "https://arxiv.org/html/2503.09443/x5.png", "caption": "Figure 5: Effect of adding a prefix (Fr: \u201dLa photo montre\u201d, Es: \u201dLa imagen muestra\u201d, etc.) to the decoder input to unlock zero-shot captioning. Tested on the image captioning dataset XM3600\u00a0[62] in the unseen languages Fr, Es, Ru, and Zh. The mean CIDEr\u00a0[64] over unseen languages significantly improves with the prefix.", "description": "This figure demonstrates how adding a language-specific prefix to the decoder input of a vision-language model (VLM) significantly improves its zero-shot image captioning capabilities in unseen languages.  The experiment uses the XM3600 dataset [62], testing the model's performance in French (Fr), Spanish (Es), Russian (Ru), and Chinese (Zh) \u2013 languages the model was not trained on for image captioning. The results, measured by CIDEr score [64], show a substantial increase when using prefixes, highlighting the effectiveness of this simple technique to enhance cross-lingual generalization in VLMs.", "section": "6. Results"}, {"figure_path": "https://arxiv.org/html/2503.09443/x6.png", "caption": "Figure 6: Scaling laws for fine-tuned models on different downstream tasks. First row: Multi30K translation to De and Fr measured in BLEU (Task 1; mean over Test2016, Test2017 and AmbiguousCOCO splits), CoMMuTE translation and disambiguation for En\u2192\u2192\\rightarrow\u2192De and En\u2192\u2192\\rightarrow\u2192{De, Fr, Ru, Zh} measured in BLEU and accuracy, respectively. Second row: Captioning tasks measured with CIDEr: COCO Karpathy (En), Multi30K (En, De) (Task 2, Test2016), and XM3600 for En, De and unseen languages (Fr, Es, Ru, Zh). UC and ST exhibit stronger scaling laws than tasks for languages already known to the pre-trained LLM (En) or those with complete task coverage (De).", "description": "This figure displays the scaling laws observed when fine-tuning various sized vision-language models on multiple downstream tasks.  The top row shows results for translation tasks from English to German (De) and French (Fr) using the Multi30K dataset, and for translation and disambiguation tasks using the CoMMuTE dataset (English to German, and English to German, French, Russian, and Chinese).  The metric used is BLEU score for translation. The bottom row presents results for image captioning tasks, utilizing the CIDEr metric. Datasets include COCO Karpathy (English only), Multi30K (English and German), and XM3600 (English, German, and four other unseen languages: French, Spanish, Russian, and Chinese). The figure highlights that the scaling laws are stronger for tasks and languages that were either already familiar to the pre-trained language model (English) or had complete training data coverage (German), compared to those with limited data or unseen languages.", "section": "6. Scaling Laws"}, {"figure_path": "https://arxiv.org/html/2503.09443/x7.png", "caption": "Figure 7: Distribution of caption and translation data with language coverage in our pre-training dataset.", "description": "This figure shows a breakdown of the language coverage in the pre-training dataset used to train the Florenz model. It visually represents the proportions of caption and translation data available for each language, highlighting the imbalance in data distribution across different languages.", "section": "4. Multimodal Multilingual Dataset Creation"}, {"figure_path": "https://arxiv.org/html/2503.09443/x8.png", "caption": "Figure 8: Distribution of caption and translation data with language coverage in our fine-tuning dataset.", "description": "This figure shows a breakdown of the language coverage in the fine-tuning dataset used for the Florenz model.  It visually represents the proportion of caption and translation data available for each language, illustrating which languages have more extensive data for both captioning and translation tasks, and which languages have a more limited dataset.", "section": "4.2. Fine-tuning Dataset"}, {"figure_path": "https://arxiv.org/html/2503.09443/x9.png", "caption": "Figure 9: Captions generated with F-11.2B by prompting with the underlined prefix. F-11.2B has not seen captioning data for Fr, Es, Ru, and Zh. English references are created with DeepL. Photo by Jeanette Dickson.", "description": "Figure 9 shows example captions generated by the F-11.2B model (a large vision-language model) in four different languages: French (Fr), Spanish (Es), Russian (Ru), and Chinese (Zh).  Crucially, this model had not received any training data for image captioning in these languages.  The captions were generated by providing the model with a short prompt (the underlined prefix in the figure). The English translations of the generated captions are also provided for comparison and were created using the DeepL translation service.  The image used is a photo of three pelicans on a dock.", "section": "6. Results"}, {"figure_path": "https://arxiv.org/html/2503.09443/x10.png", "caption": "Figure 10: Captions generated with F-11.2B by prompting with the underlined prefix. F-11.2B has not seen captioning data for French (Fr), Spanish (Es), Russian (Ru), and Chinese (Zh). English references are created with DeepL. Photo by Reinaldo Simoes.", "description": "Figure 10 shows example captions generated by the F-11.2B model (one of the Florenz models) for an image depicting a person at a train station.  Crucially, the model had *not* been trained on captioning data for French, Spanish, Russian, or Chinese.  The model was prompted with a prefix in each language (underlined in the figure) before generating the caption in that language. The English captions provided are machine translations created with DeepL.", "section": "3. Preliminary Analysis"}, {"figure_path": "https://arxiv.org/html/2503.09443/x11.png", "caption": "Figure 11: Detailed caption generated with F-11.2B 30K ft for German (De). English reference is created with DeepL. Photo by Jeanette Dickson.", "description": "Figure 11 shows a detailed caption generated by the F-11.2B 30K ft model for a picture of three pelicans standing on a dock.  The caption is in German.  An English translation, created using DeepL, is also provided for comparison and better understanding. The photo itself was taken by Jeanette Dickson.", "section": "4. Multimodal Multilingual Dataset Creation"}, {"figure_path": "https://arxiv.org/html/2503.09443/x12.png", "caption": "Figure 12: Detailed caption generated with F-11.2B 30K ft for German (De). English reference is created with DeepL. Photo by Reinaldo Simoes.", "description": "Figure 12 shows a detailed caption generated by the F-11.2B 30K ft model for a German-language image.  The image depicts a person at a train station.  The model generated a lengthy, highly descriptive caption of the scene in German. An English translation is included for comparison, generated using the DeepL translation service. The photograph is credited to Reinaldo Simoes.", "section": "4. Multimodal Multilingual Dataset Creation"}]