{"importance": "This paper introduces **Q-Eval-100K**, a large-scale dataset crucial for evaluating and improving text-to-vision models. The work provides a new benchmark, facilitating more reliable assessment of AI-generated content quality and alignment with textual prompts, fostering advancements in generative AI research.", "summary": "Q-Eval-100K: A new, large dataset for evaluating visual quality and text alignment in AI-generated content.", "takeaways": ["Q-Eval-100K dataset offers a large collection of human-labeled Mean Opinion Scores (MOS) for evaluating visual quality and alignment.", "The study introduces Q-Eval-Score, a unified evaluation model improving long-text prompt alignment handling.", "Experiments show Q-Eval-Score's superior performance and strong generalization across benchmarks."], "tldr": "Evaluating AI-generated visuals hinges on quality and text alignment. Current models rely on high-quality, large-scale human annotations, aligning with Scaling Law principles. Existing datasets often miss systematic capture of key dimensions or fail to disentangle quality from alignment, limiting LMM potential in real scenarios.\n\nThis paper introduces **Q-Eval-100K**, a dataset designed to evaluate visual quality and alignment level featuring 100K instances with 960K human annotations. Also, it presents **Q-Eval-Score**, a framework for assessing both dimensions and offers Vague-to-Specific strategy for long prompts.", "affiliation": "Shanghai Jiao Tong University", "categories": {"main_category": "Computer Vision", "sub_category": "Image Generation"}, "podcast_path": "2503.02357/podcast.wav"}