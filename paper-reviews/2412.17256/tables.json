[{"content": "| Methods | GSM 8K P@1 | GSM 8K P@32 | GSM 8K P@32-4 | MATH P@1 | MATH P@32 | MATH P@32-4 | APPS P@1 | APPS P@32 | APPS P@32-4 | ARC-C P@1 |\n|---|---|---|---|---|---|---|---|---|---|---|\n| SFT | 36.6 | 88.5 | 62.2 | 17.0 | 60.8 | 31.2 | 9.3 | 43.5 | 25.5 | \u2014 |\n| Rest-EM (w/o RM) | 40.5 | 89.9 | 69.8 | 22.8 | 60.0 | 33.6 | 14.5 | 43.9 | 28.2 | 70.7 |\n| Rest-EM (w/ RM) | 46.3 | 90.7 | 72.2 | 24.2 | 62.8 | 37.4 | \u2014 | \u2014 | \u2014 | \u2014 |\n| Iterative RFT (w/o RM) | 42.8 | 88.9 | 71.3 | 24.2 | 63.4 | 38.2 | 15.2 | 44.3 | 28.0 | 70.3 |\n| Iterative RFT (w/ RM) | 46.6 | 90.2 | 74.9 | 24.4 | 62.6 | 39.0 | \u2014 | \u2014 | \u2014 | \u2014 |\n| Online RFT (w/o RM) | 44.0 | 88.1 | 69.7 | 23.0 | 57.2 | 38.2 | 17.3 | 45.8 | 27.8 | 71.2 |\n| Online RFT (w/ RM) | 46.8 | 91.4 | 76.5 | 23.2 | 62.6 | 39.2 | \u2014 | \u2014 | \u2014 | \u2014 |\n| **B-STaR** | **53.8** | **93.6** | **81.0** | **27.8** | **67.2** | **42.2** | **19.6** | **49.3** | **30.7** | **73.0** |", "caption": "Table 1: Comparison of self-improvement methods across MATH, GSM8K, APPS and ARC-Challenge. Methods include variants with and without a reward model (\"w/ RM\" and \"w/o RM\"). The results are based on the Mistral-7B model except for APPS that is from Llama-3-8B.", "description": "This table compares the performance of several self-improvement methods on four different datasets: MATH, GSM8K, APPS, and ARC-Challenge.  The methods are tested with and without the use of a reward model.  The results show the Pass@1, Pass@32, and Pass@32-4 scores, which measure the accuracy of the top 1, 32, and 32 responses, respectively.  The Mistral-7B language model was used for all datasets except APPS, where Llama-3-8B was used.", "section": "Main Experiments"}, {"content": "| Step | 500 | 1000 | 1500 | 2000 | 2500 | 3000 | 3500 | 4000 | 4500 |\n|---|---|---|---|---|---|---|---|---|---| \n| Temperature | 0.5 | 0.8 | 0.9 | 1 | 1.1 | 1.1 | 0.9 | 1.1 | 1.1 |\n| Reward threshold | 0 | -0.1 | -0.1 | -0.1 | -0.1 | -0.1 | -0.1 | -0.1 | -0.1 |\n| Balance Score | 0.470 | 0.538 | 0.589 | 0.621 | 0.646 | 0.660 | 0.673 | 0.678 | 0.679 |", "caption": "Table 2: Dynamic configuration adjustments by B-STaR in mathematical problem-solving. The temperature increment and reward threshold increment are both set to 0.1. Additionally, finer-grained increments for these parameters are explored in detail in Appendix\u00a0D and summarized in Table\u00a05.", "description": "This table details how B-STaR dynamically adjusts its hyperparameters (temperature and reward threshold) during the training process for mathematical problem-solving.  Each row represents a training step (every 500 steps), showing the chosen temperature and reward threshold at that point, along with the resulting balance score (a metric combining exploration and exploitation). The temperature and reward threshold are increased incrementally (by 0.1 in this case), but the Appendix D provides a more detailed analysis with finer-grained adjustments (summarized in Table 5).  The balance score indicates the effectiveness of the configuration in balancing exploration and exploitation, aiming for a value of 1.", "section": "3.2 Dynamic or Static - On the Configurations of Exploration and Exploitation"}, {"content": "| Methods | GSM 8K | MATH |\n|---|---|---|\n| Online RFT | 46.8 | 23.2 |\n| B-STaR (Temperature Adjustment Only) | 53.1 | 25.0 |\n| B-STaR (Reward Threshold Adjustment Only) | 49.1 | 24.6 |\n| B-STaR (Temperature + Reward Threshold) | 53.8 | 27.8 |", "caption": "Table 3: Ablation study on dynamic adjustment in mathematical problem-solving, including temperature adjustment only and reward threshold adjustment only.", "description": "This table presents the results of an ablation study conducted to evaluate the impact of dynamically adjusting the temperature and reward threshold during the self-improvement process for mathematical problem-solving.  It compares the performance of three variations: one where only the temperature is dynamically adjusted, one where only the reward threshold is dynamically adjusted, and finally, the full B-STAR model where both are adjusted. This allows for a quantitative assessment of the individual contribution of each hyperparameter in optimizing the self-improvement process and achieving a balance between exploration and exploitation.", "section": "4.2 Results"}, {"content": "| Methods | GSM8K | MATH | APPS | ARC-C |\n|---|---|---|---|---|\n| SFT | 49.4 | 18.8 | 15.6 | 78.8 |\n| Rest-EM (w/RM) | 60.2 | 28.2 | 16.4 | 85.5 |\n| Iterative RFT (w/RM) | 55.3 | 27.2 | 17.1 | 85.2 |\n| Online RFT (w/RM) | 59.7 | 27.8 | 16.9 | 85.2 |\n| B-STaR | **61.6** | **29.2** | **18.1** | **86.3** |", "caption": "Table 4: A comparison of self-improvement methods trained on Llama-3.1-8B across MATH, GSM8K, APPS, and ARC-Challenge, showing the highest Pass@1 results. For ARC-Challenge, we start from Llama-3.1-8B-Instruct and omit the SFT stage due to the absence of CoT data for this dataset.", "description": "This table compares the performance of several self-improvement methods on four different datasets: MATH, GSM8K, APPS, and ARC-Challenge.  The methods are trained using the Llama-3.1-8B language model. The table highlights the Pass@1 accuracy (the percentage of times the top-ranked answer is correct) for each method and dataset.  Note that for the ARC-Challenge dataset, the standard supervised fine-tuning (SFT) stage was omitted because the data required for this step (Chain of Thought, CoT) was not available.", "section": "Main Experiments"}, {"content": "| Step | 500 | 1000 | 1500 | 2000 | 2500 | 3000 | 3500 | 4000 | 4500 |\n|---|---|---|---|---|---|---|---|---|---| \n| Temperature | 0.65 | 0.75 | 1.05 | 0.95 | 1.05 | 0.85 | 1.05 | 1.15 | 1.05 |\n| Reward Thresholds | -0.02 | -0.04 | -0.09 | -0.09 | -0.14 | -0.14 | -0.14 | -0.15 | -0.06 |\n| Balance Score | 0.500 | 0.557 | 0.591 | 0.626 | 0.652 | 0.665 | 0.679 | 0.682 | 0.684 |", "caption": "Table 5: Finer-grained dynamic configuration adjustments by B-STaR in mathematical problem-solving.", "description": "This table shows how the B-STAR model dynamically adjusts its temperature and reward threshold parameters during the mathematical problem-solving process.  It presents the values for these parameters at various training steps (500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500), demonstrating the finer-grained adjustments made by the model compared to the previous Table 2. The table also includes the resulting balance score for each step, reflecting the effectiveness of the chosen configuration in balancing exploration and exploitation.", "section": "3.2 Dynamic or Static \u2013 On the Configurations of Exploration and Exploitation"}, {"content": "| Configuration | GSM 8K | MATH |\n|---|---|---|\n| Temp = 1.0; Threshold = 0.0 | 46.8 | 23.2 |\n| Temp = 1.1; Threshold = -0.1 | 40.4 | 18.2 |\n| B-STaR | 53.1 | 27.8 |", "caption": "Table 6: Comparison of Online RFT using specific configurations and B-STaR Performance. This table reports the results with the stable hyperparameter combinations we found in our B-STaR experiments (Temperature = 1.1, Reward thresholds = -0.1)\n(Table\u00a02).", "description": "This table compares the performance of Online RFT using two specific hyperparameter configurations with that of B-STAR. The Online RFT results are obtained using the stable hyperparameter settings identified in the B-STAR experiments (temperature = 1.1 and reward threshold = -0.1), as detailed in Table 2 of the paper.  The comparison highlights the performance gains achieved by B-STAR's dynamic hyperparameter adjustment strategy over the static configurations of Online RFT.", "section": "4 Main Experiments"}]