[{"content": "| Model | MTC | RS | RG | Rad | Oph | Path | Micro | LLM+VLM | Bil (Ar) |\n|---|---|---|---|---|---|---|---|---|---|\n| **Meditron (Chen et\u00a0al. (2023))** | \\usym2717 | \\usym2717 | \\usym2717 | \\usym2717 | \\usym2717 | \\usym2717 | \\usym2717 | \\usym2717 | \\usym2717 |\n| **Med42 (Christophe et\u00a0al. (2024))** | \\usym2713 | \\usym2713 | \\usym2717 | \\usym2717 | \\usym2717 | \\usym2717 | \\usym2717 | \\usym2717 | \\usym2717 |\n| **OpenBioLLM (Ankit\u00a0Pal (2024))** | \\usym2713 | \\usym2713 | \\usym2717 | \\usym2717 | \\usym2717 | \\usym2717 | \\usym2717 | \\usym2717 | \\usym2717 |\n| **Llama3.1 (Meta (2024))** | \\usym2713 | \\usym2713 | \\usym2717 | \\usym2717 | \\usym2717 | \\usym2717 | \\usym2717 | \\usym2717 | \\usym2717 |\n| **BiMediXv1 (Pieri et\u00a0al. (2024))** | \\usym2713 | \\usym2713 | \\usym2717 | \\usym2717 | \\usym2717 | \\usym2717 | \\usym2717 | \\usym2717 | \\usym2713 |", "caption": "Table 1: Comparison of Recent Medical LLMs and VLMs. Abbreviations: MTC (Multi-turn conversation), RS (Report Summarization), RG (Report Generation), Rad (Radiology), Oph (Ophthalmology), Path (Pathology), Micro (Microscopic), UM (Unified Model: Single model checkpoint for all downstream tasks), LLM+VLM (Unified LLM + VLM), Bil (Ar) (Bilingual Arabic capabilities).", "description": "This table provides a comparison of recent Medical Large Language Models (LLMs) and Vision-Language Models (VLMs), highlighting their capabilities across various medical tasks and modalities.  The table uses abbreviations like MTC for Multi-turn conversation, RS for Report Summarization, and others as explained in the caption. It also indicates whether these models offer a Unified Model (UM) approach (single checkpoint for all tasks) or a combined LLM+VLM architecture, and whether they have Bilingual (Arabic) capabilities.", "section": "CONTRIBUTIONS"}, {"content": "| Model | MTC | RS | RG | Rad | Oph | Path | Micro | UM | LLM+VLM | Bil (Ar) |\n|---|---|---|---|---|---|---|---|---|---|---| \n| **LLaVA-pp (Rasheed et\u00a0al. (2024))** | 2713 | 2713 | 2717 | 2717 | 2717 | 2717 | 2717 | 2713 | 2717 | 2717 |\n| **MiniGPT-Med (Alkhaldi et\u00a0al. (2024))** | 2717 | 2713 | 2713 | 2713 | 2717 | 2717 | 2717 | 2713 | 2717 | 2717 |\n| **BioMedGPT (Zhang et\u00a0al. (2024))** | 2717 | 2713 | 2713 | 2713 | 2713 | 2713 | 2713 | 2717 | 2717 | 2717 |\n| **LLaVA-Med (Li et\u00a0al. (2023))** | 2713 | 2713 | 2713 | 2713 | 2713 | 2713 | 2713 | 2713 | 2717 | 2717 |\n| **Dragonfly VLM (Chen et\u00a0al. (2024))** | 2717 | 2713 | 2713 | 2713 | 2713 | 2713 | 2713 | 2713 | 2717 | 2717 |\n| **BiMediX2** | 2713 | 2713 | 2713 | 2713 | 2713 | 2713 | 2713 | 2713 | 2713 | 2713 |", "caption": "Table 2: Clinical LLM Evaluation Benchmark", "description": "This table presents a comparison of various large language models (LLMs) on a set of clinical benchmarks.  These benchmarks cover various medical question answering tasks like MedMCQA, MedQA, USMLE, and PubmedQA, as well as general medical knowledge and reasoning across multiple domains like Clinical Knowledge, College Biology, and others. The models are evaluated on these benchmarks, and their performance is represented by scores (e.g., accuracy, F1, etc.). This comparison allows researchers to assess the strengths and weaknesses of different LLMs in handling medical information, and BiMediX2 models in different sizes are compared against other existing models.", "section": "3 EXPERIMENTS"}, {"content": "| Model | Cli-KG | C-Bio | C-Med | Med-Gen | Pro-Med | Ana | MedMCQA | MedQA | USMLE | PubmedQA | Average |\n|---|---|---|---|---|---|---|---|---|---|---|---|\n| **BioMedGPT-LM-7B** | 49.4 | 43.1 | 41.4 | 45.0 | 51.0 | 45.2 | 34.8 | 33.2 | 31.7 | 74.0 | 44.9 |\n| **BiMediX2 4B** | 55.1 | 63.9 | 47.4 | 55.0 | 36.0 | 52.6 | 38.1 | 37.9 | 47.1 | 72.2 | 50.5 |\n| **LLaVA-Med** | 59.6 | 59.7 | 50.9 | 59.0 | 51.5 | 51.9 | 44.5 | 35.7 | 36.9 | 74.0 | 52.4 |\n| **Dragonfly-Med** | 65.6 | 69.4 | 56.6 | 69.0 | 58.4 | 57.0 | 49.9 | 42.8 | 46.1 | 75.4 | 59.0 |\n| **GPT 3.5** | 69.8 | 72.2 | 61.3 | 70.0 | 70.2 | 56.3 | 50.1 | 50.8 | 49.1 | 71.6 | 62.1 |\n| **Meditron 70B** | 68.3 | 77.8 | 63.6 | 75.0 | 74.6 | 56.3 | 48.4 | 53.1 | 55.4 | 76.2 | 64.9 |\n| **BiMediX2 8B** | 77.7 | 79.2 | 68.8 | 82.0 | 74.3 | 65.9 | 58.0 | 57.0 | 68.6 | 72.4 | 70.4 |\n| **GPT 4** | 86.0 | 95.1 | 76.9 | 91.0 | 93.0 | 80.0 | 69.5 | 78.9 | 83.8 | 75.2 | 82.9 |\n| **Llama3-Med42-70B** | 84.2 | 93.1 | 79.8 | 91.0 | 90.1 | 80.7 | 72.5 | 73.8 | 84.3 | 80.6 | 83.0 |\n| **OpenBioLLM-70B** | 92.5 | 93.8 | 85.6 | 93.0 | 93.4 | 83.7 | 74.1 | 68.9 | 72.0 | 78.0 | 83.5 |\n| **Llama 3.1 70B** | 83.4 | 95.1 | 79.2 | 93.0 | 91.5 | 80.7 | 71.7 | 73.8 | 92.0 | 77.6 | 83.8 |\n| **BiMediX2 70B** | **86.8** | **95.1** | **79.8** | **94.0** | **91.5** | **82.2** | **70.5** | **74.3** | **92.3** | **79.0** | **84.6** |", "caption": "Table 3: BiMed-MBench English Evaluation", "description": "This table presents the evaluation results of BiMediX2 and other large multimodal models on the English portion of the BiMed-MBench dataset. The table includes performance metrics for different categories (Conversation, Description, CXR, MRI, Histology, Gross, CT, and Overall) and allows for comparison across models like BiomedGPT, LLaVA, MiniGPT-Med, Dragonfly, and versions of BiMediX2.", "section": "4 Results"}, {"content": "| Model | Conversation | Description | CXR | MRI | Histology | Gross | CT | Overall |\n|---|---|---|---|---|---|---|---|---| \n| **BiomedGPT** | 15.3 | 13.3 | 16.4 | 13.0 | 14.1 | 14.9 | 15.8 | 14.8 |\n| **LLaVA-pp** | 34.3 | 36.6 | 44.7 | 33.3 | 34.7 | 30.2 | 31.5 | 34.9 |\n| **MiniGPT-Med** | 37.5 | 29.6 | 47.6 | 32.5 | 36.3 | 31.8 | 29.1 | 35.4 |\n| **LLaVA-Med** | 55.6 | 43.3 | 59.5 | 43.4 | 54.4 | 53.9 | 51.0 | 52.4 |\n| **Dragonfly-Med** | 59.2 | 34.2 | 67.0 | 51.2 | 53.7 | 42.6 | 48.3 | 52.7 |\n| **BiMediX2 8B** | **64.9** | **54.5** | **71.7** | **56.8** | **62.5** | **61.4** | **58.9** | **62.2** |", "caption": "Table 4: BiMed-MBench Arabic Evaluation", "description": "BiMediX2's performance on the Arabic portion of the BiMed-MBench benchmark, broken down by categories (Conversation, Description, CXR, MRI, Histology, Gross, CT) and overall score. The table shows BiMediX2 outperforms other models (BiomedGPT, MiniGPT-Med, LLaVA-Med, LLaVA-pp, Dragonfly-Med) in most categories and overall.", "section": "4 RESULTS"}, {"content": "| Model | Conversation | Description | CXR | MRI | Histology | Gross | CT | Overall |\n|---|---|---|---|---|---|---|---|---| \n| **BiomedGPT** | 11.1 | 11.2 | 11.4 | 10.8 | 11.5 | 11.3 | 11.1 | 11.2 |\n| **MiniGPT-Med** | 21.6 | 12.6 | 23.7 | 12.7 | 32.0 | 15.8 | 14.9 | 20.2 |\n| **LLaVA-Med** | 23.9 | 29.4 | 31.2 | 25.3 | 24.8 | 23.4 | 26.4 | 26.2 |\n| **LLaVA-pp** | 29.0 | 27.8 | 33.2 | 25.0 | 33.0 | 25.8 | 25.8 | 28.7 |\n| **Dragonfly-Med** | 32.8 | 19.9 | 31.9 | 25.7 | 33.0 | 24.0 | 31.7 | 29.5 |\n| **BiMediX2 8B** | **54.3** | **36.2** | **61.4** | **44.6** | **51.5** | **43.5** | **50.8** | **50.5** |", "caption": "Table 5: Medical VQA Benchmark (MultiMedEval\u00a0Royer et\u00a0al. (2024))", "description": "This table presents a benchmark comparison of different medical Vision-Language Answering (VQA) models using the MultiMedEval toolkit.  Performance metrics including BLEU-1, closed question accuracy, open question recall, overall recall, open question accuracy, and F1 score are reported for models including RadFM, LLaVA-Med, BioMedGPT, MiniGPT-Med, Phi-3.5V, and two versions of BiMediX2 (4B and 8B) across three VQA datasets: Rad-VQA, Slake-VQA, and Path-VQA. The average performance across all datasets is also provided for each model.", "section": "4 Results"}, {"content": "| Dataset | Metric | RadFM | LLaVA Med | BioMedGPT | MiniGPT-Med | Phi-3.5 V | BiMediX2 4B | BiMediX2 8B |\n|---|---|---|---|---|---|---|---|---| \n| Rad-VQA | BLEU-1\u2191 | 0.475 | 0.033 | 0.044 | 0.662 | 0.377 | 0.501 | 0.552 |\n|  | closed Q accuracy\u2191 | 0.577 | 0.545 | 0.203 | 0.829 | 0.618 | 0.685 | 0.725 |\n|  | open Q recall\u2191 | 0.407 | 0.246 | 0.199 | 0.546 | 0.295 | 0.292 | 0.363 |\n|  | recall\u2191 | 0.438 | 0.372 | 0.199 | 0.703 | 0.475 | 0.511 | 0.565 |\n|  | open Q accuracy\u2191 | 0.335 | 0.140 | 0.150 | 0.490 | 0.200 | 0.225 | 0.305 |\n|  | F1 \u2191 | 0.442 | 0.069 | 0.064 | 0.675 | 0.391 | 0.516 | 0.569 |\n| Slake-VQA | BLEU-1\u2191 | 0.746 | 0.036 | 0.175 | 0.337 | 0.089 | 0.625 | 0.778 |\n|  | closed Q accuracy\u2191 | 0.752 | 0.512 | 0.248 | 0.572 | 0.535 | 0.744 | 0.831 |\n|  | open Q recall\u2191 | 0.758 | 0.429 | 0.293 | 0.308 | 0.377 | 0.624 | 0.763 |\n|  | recall\u2191 | 0.695 | 0.443 | 0.260 | 0.396 | 0.404 | 0.664 | 0.786 |\n|  | open Q accuracy\u2191 | 0.725 | 0.362 | 0.259 | 0.278 | 0.329 | 0.567 | 0.729 |\n|  | F1 \u2191 | 0.714 | 0.075 | 0.192 | 0.349 | 0.129 | 0.641 | 0.787 |\n| Path-VQA | BLEU-1\u2191 | 0.257 | 0.021 | 0.145 | 0.296 | 0.283 | 0.469 | 0.587 |\n|  | closed Q accuracy\u2191 | 0.505 | 0.512 | 0.260 | 0.581 | 0.553 | 0.708 | 0.872 |\n|  | open Q recall\u2191 | 0.020 | 0.116 | 0.093 | 0.040 | 0.063 | 0.239 | 0.314 |\n|  | recall\u2191 | 0.221 | 0.287 | 0.176 | 0.311 | 0.308 | 0.474 | 0.593 |\n|  | open Q accuracy\u2191 | 0.005 | 0.053 | 0.077 | 0.019 | 0.027 | 0.210 | 0.282 |\n|  | F1 \u2191 | 0.232 | 0.052 | 0.154 | 0.299 | 0.287 | 0.475 | 0.595 |\n| **Average** |  | **0.461** | **0.239** | **0.177** | **0.427** | **0.319** | **0.509** | **0.611** |", "caption": "Table 6: Report Summarization (MultiMedEval\u00a0Royer et\u00a0al. (2024))", "description": "This table presents a benchmark evaluation of different medical Large Language Models (LLMs) on a report summarization task using the MIMIC-III dataset. The models are evaluated on their ability to generate concise and accurate summaries of medical reports based on their 'findings' sections, using metrics such as ROUGE-L, BLEU-1, BLEU-4*, F1-RadGraph, RadCliQ+*, CheXbert vector, and METEOR.  Higher scores indicate better performance.  The table compares the performance of several LLMs, including LLaVA-Med, Dragonfly-Med, and two versions of BiMediX2 (4B and 8B).  The results show that BiMediX2 8B achieves the highest average score across all the metrics.", "section": "4 RESULTS"}, {"content": "| Dataset | Metric | LLaVA Med | Dragonfly-Med | BiMediX2 4B | BiMediX2 8B |\n|---|---|---|---|---|---| \n| **MIMIC-III** | ROUGE-L\u2191 | 0.185 | 0.072 | 0.209 | 0.205 |\n| | BLEU-1\u2191 | 0.192 | 0.062 | 0.153 | 0.178 |\n| | BLEU-4\u2191* | 0.520 | 0.000 | 0.410 | 0.449 |\n| | F1-RadGraph\u2191 | 0.232 | 0.000 | 0.222 | 0.230 |\n| | RadCliQ\u2191* | 0.753 | 0.247 | 0.923 | 0.918 |\n| | CheXbert vector\u2191 | 0.600 | 0.326 | 0.633 | 0.593 |\n| | METEOR\u2191 | 0.303 | 0.060 | 0.264 | 0.339 |\n| **Average** |  | **0.398** | **0.110** | **0.402** | **0.416** |", "caption": "Table 7: Report Generation (MultiMedEval\u00a0Royer et\u00a0al. (2024))", "description": "This table presents the results of report generation on the MIMIC-CXR dataset, using metrics like F1-RadGraph, BLEU-1, BLEU-4*, ROUGE-L, RadCliQ*, CheXbert vector, and METEOR.  The average score, a unified metric derived by rescaling BLEU-4* and RadCliQ*, is also provided for each model.", "section": "4 RESULTS"}]