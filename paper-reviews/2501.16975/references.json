{"references": [{"fullname_first_author": "J. Kaplan", "paper_title": "Scaling laws for neural language models", "publication_date": "2020-01-08", "reason": "This paper introduces scaling laws that provide guidance on how to efficiently scale neural language models, and its findings are frequently cited in the paper as a basis for understanding the scaling behavior of language models."}, {"fullname_first_author": "A. Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-01", "reason": "This paper introduces the Transformer architecture, a fundamental component of modern large language models, which is extensively discussed in this paper."}, {"fullname_first_author": "T. Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper demonstrates the capability of large language models to perform well on a wide range of downstream tasks with limited fine-tuning, which is relevant to this paper's focus on efficient model scaling."}, {"fullname_first_author": "R. Radford", "paper_title": "Language models are unsupervised multitask learners", "publication_date": "2019-01-01", "reason": "This paper introduces a fundamental pre-trained large language model that is used extensively in modern research and is directly relevant to the methods and experiments described in this paper."}, {"fullname_first_author": "C. Tao", "paper_title": "Scaling laws with vocabulary: Larger models deserve larger vocabularies", "publication_date": "2024-07-13", "reason": "This paper directly addresses the relationship between vocabulary size and model performance, providing a theoretical basis for the research presented in this paper and sharing many of its findings."}]}