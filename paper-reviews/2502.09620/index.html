<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><title>Exploring the Potential of Encoder-free Architectures in 3D LMMs &#183; HF Daily Paper Reviews by AI</title>
<meta name=title content="Exploring the Potential of Encoder-free Architectures in 3D LMMs &#183; HF Daily Paper Reviews by AI"><meta name=description content="Encoder-free 3D LMMs outperform state-of-the-art, achieving comparable results to significantly larger models."><meta name=keywords content="Multimodal Learning,Vision-Language Models,üè¢ Northwestern Polytechnical University,"><link rel=canonical href=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.09620/><link type=text/css rel=stylesheet href=/ai-paper-reviewer/css/main.bundle.min.595affd4445a931ea6d6e3a5a3c709930fa52a60be10b21c6f81fdb8fecaacea33aacedf80cdc88be45f189be14ed4ce53ea74a1e1406fad9cbf90c5ed409173.css integrity="sha512-WVr/1ERakx6m1uOlo8cJkw+lKmC+ELIcb4H9uP7KrOozqs7fgM3Ii+RfGJvhTtTOU+p0oeFAb62cv5DF7UCRcw=="><script type=text/javascript src=/ai-paper-reviewer/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script><script defer type=text/javascript id=script-bundle src=/ai-paper-reviewer/js/main.bundle.min.efbf3b6b987689fffaf2d7b73173d2690c0279a04d444b0537a77d7f4ff6e6d493445400cb0cf56bc0f0f123e19f15394e63cae34e67f069bd013dd5c73df56e.js integrity="sha512-7787a5h2if/68te3MXPSaQwCeaBNREsFN6d9f0/25tSTRFQAywz1a8Dw8SPhnxU5TmPK405n8Gm9AT3Vxz31bg==" data-copy data-copied></script><script src=/ai-paper-reviewer/lib/zoom/zoom.min.37d2094687372da3f7343a221a470f6b8806f7891aa46a5a03966af7f0ebd38b9fe536cb154e6ad28f006d184b294525a7c4054b6bbb4be62d8b453b42db99bd.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ=="></script><link rel=apple-touch-icon sizes=180x180 href=/ai-paper-reviewer/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/ai-paper-reviewer/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/ai-paper-reviewer/favicon-16x16.png><link rel=manifest href=/ai-paper-reviewer/site.webmanifest><meta property="og:url" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.09620/"><meta property="og:site_name" content="HF Daily Paper Reviews by AI"><meta property="og:title" content="Exploring the Potential of Encoder-free Architectures in 3D LMMs"><meta property="og:description" content="Encoder-free 3D LMMs outperform state-of-the-art, achieving comparable results to significantly larger models."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="paper-reviews"><meta property="article:published_time" content="2025-02-13T00:00:00+00:00"><meta property="article:modified_time" content="2025-02-13T00:00:00+00:00"><meta property="article:tag" content="Multimodal Learning"><meta property="article:tag" content="Vision-Language Models"><meta property="article:tag" content="üè¢ Northwestern Polytechnical University"><meta property="og:image" content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.09620/cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.09620/cover.png"><meta name=twitter:title content="Exploring the Potential of Encoder-free Architectures in 3D LMMs"><meta name=twitter:description content="Encoder-free 3D LMMs outperform state-of-the-art, achieving comparable results to significantly larger models."><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"Paper Reviews by AI","name":"Exploring the Potential of Encoder-free Architectures in 3D LMMs","headline":"Exploring the Potential of Encoder-free Architectures in 3D LMMs","abstract":"Encoder-free 3D LMMs outperform state-of-the-art, achieving comparable results to significantly larger models.","inLanguage":"en","url":"https:\/\/deep-diver.github.io\/ai-paper-reviewer\/paper-reviews\/2502.09620\/","author":{"@type":"Person","name":"Hugging Face Daily Papers"},"copyrightYear":"2025","dateCreated":"2025-02-13T00:00:00\u002b00:00","datePublished":"2025-02-13T00:00:00\u002b00:00","dateModified":"2025-02-13T00:00:00\u002b00:00","keywords":["Multimodal Learning","Vision-Language Models","üè¢ Northwestern Polytechnical University"],"mainEntityOfPage":"true","wordCount":"3414"}]</script><meta name=author content="Hugging Face Daily Papers"><link href=https://github.com/deep-diver/paper-reviewer/ rel=me><link href=https://twitter.com/algo_diver/ rel=me><script src=/ai-paper-reviewer/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj+KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script><script defer src=/ai-paper-reviewer/lib/typeit/typeit.umd.1b3200cb448f5cd1f548f2781452643d3511a43584b377b82c03a58055da4fdb7bc8f6c6c2ce846480c7677ff25bfd0d75f15823c09443ab18e0fd2cad792587.js integrity="sha512-GzIAy0SPXNH1SPJ4FFJkPTURpDWEs3e4LAOlgFXaT9t7yPbGws6EZIDHZ3/yW/0NdfFYI8CUQ6sY4P0srXklhw=="></script><script defer src=/ai-paper-reviewer/lib/packery/packery.pkgd.min.js integrity></script><script type=text/javascript src=/ai-paper-reviewer/js/shortcodes/gallery.min.9b4cb28f931ed922c26fb9b2510c2debb370f6a63305050c2af81740b2919883715e24efbbdf3a081496718ec751df3a72729d4d0bc71d6071297563a97ce1ee.js integrity="sha512-m0yyj5Me2SLCb7myUQwt67Nw9qYzBQUMKvgXQLKRmINxXiTvu986CBSWcY7HUd86cnKdTQvHHWBxKXVjqXzh7g=="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-KX0S6Q55Y7"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-KX0S6Q55Y7")</script><meta name=theme-color><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-app.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-firestore.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-auth.js></script><script>const firebaseConfig={apiKey:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",authDomain:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",projectId:"neurips2024-f3065",storageBucket:"neurips2024-f3065.firebasestorage.app",messagingSenderId:"982475958898",appId:"1:982475958898:web:2147e5d7753d6ac091f0eb",measurementId:"G-YQ46HXQ9JS"};var app=firebase.initializeApp(firebaseConfig),db=firebase.firestore(),auth=firebase.auth()</script></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a></div><div class=min-h-[148px]></div><div class="fixed inset-x-0 pl-[24px] pr-[24px]" style=z-index:100><div id=menu-blur class="absolute opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl shadow-2xl"></div><div class="relative max-w-[64rem] ml-auto mr-auto"><div style=padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start space-x-3"><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/ai-paper-reviewer/ class="text-base font-medium text-gray-500 hover:text-gray-900">HF Daily Paper Reviews by AI</a></nav><nav class="hidden md:flex items-center space-x-5 md:ml-12 h-12"><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>About</p></a><a href=/ai-paper-reviewer/2025-03-25/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>2025-03-25</p></a><a href=/ai-paper-reviewer/2025-03-26/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>2025-03-26</p></a><a href=/ai-paper-reviewer/2025-03-27/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>2025-03-27</p></a><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Archive</p></a><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Tags</p></a><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></span><p class="text-base font-medium" title></p></a><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span><p class="text-base font-medium" title></p></a><button id=search-button aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button><div class="ltr:mr-14 rtl:ml-14 flex items-center"><button id=appearance-switcher aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></nav><div class="flex md:hidden items-center space-x-5 md:ml-12 h-12"><span></span>
<button id=search-button-mobile aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></button>
<button id=appearance-switcher-mobile aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400" style=margin-right:5px><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div><div class="-my-2 -mr-2 md:hidden"><label id=menu-button class=block><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper style=padding-top:5px class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li id=menu-close-button><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>About</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-03-25/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>2025-03-25</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-03-26/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>2025-03-26</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-03-27/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>2025-03-27</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Archive</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Tags</p></a></li><li class=mt-1><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li><li class=mt-1><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li></ul></div></label></div></div></div></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("menu-blur");n.style.opacity=t/300})</script><div class="relative flex flex-col grow"><main id=main-content class=grow><article><div id=hero class="h-[150px] md:h-[200px]"></div><div class="fixed inset-x-0 top-0 h-[800px] single_hero_background nozoom" style=background-image:url(/ai-paper-reviewer/paper-reviews/2502.09620/cover_hu8445942732992907315.png)><div class="absolute inset-0 bg-gradient-to-t from-neutral dark:from-neutral-800 to-transparent mix-blend-normal"></div><div class="absolute inset-0 opacity-60 bg-gradient-to-t from-neutral dark:from-neutral-800 to-neutral-100 dark:to-neutral-800 mix-blend-normal"></div></div><div id=background-blur class="fixed opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl"></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("background-blur");n.style.opacity=t/300})</script><header id=single_header class="mt-5 max-w-prose"><ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden"><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/>HF Daily Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class=inline><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/>Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/paper-reviews/2502.09620/>Exploring the Potential of Encoder-free Architectures in 3D LMMs</a><span class="px-1 text-primary-500">/</span></li></ol><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">Exploring the Potential of Encoder-free Architectures in 3D LMMs</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime=2025-02-13T00:00:00+00:00>13 February 2025</time><span class="px-2 text-primary-500">&#183;</span><span>3414 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">17 mins</span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=views_paper-reviews/2502.09620/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=views>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 576 512"><path fill="currentcolor" d="M288 32c-80.8.0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7.0 24.6C17.3 304 48.6 356 95.4 399.4 142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1 3.3-7.9 3.3-16.7.0-24.6-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144 64.5-144 144-144 144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64-11.5.0-22.3-3-31.6-8.4-.2 2.8-.4 5.5-.4 8.4.0 53 43 96 96 96s96-43 96-96-43-96-96-96c-2.8.0-5.6.1-8.4.4 5.3 9.3 8.4 20.1 8.4 31.6z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=likes_paper-reviews/2502.09620/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=likes>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<button id=button_likes class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400" onclick=process_article()>
<span id=button_likes_heart style=display:none class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span><span id=button_likes_emtpty_heart class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M244 84l11.1 12 12-11.98C300.6 51.37 347 36.51 392.6 44.1 461.5 55.58 512 115.2 512 185.1V190.9c0 41.5-17.2 81.2-47.6 109.5L283.7 469.1c-7.5 7-17.4 10.9-27.7 10.9S235.8 476.1 228.3 469.1L47.59 300.4C17.23 272.1.0 232.4.0 190.9V185.1c0-69.9 50.52-129.52 119.4-141 44.7-7.59 92 7.27 124.6 39.9C243.1 84 244 84.01 244 84zm11.1 79.9-45-46.8c-21.7-20.82-52.5-30.7-82.8-25.66C81.55 99.07 48 138.7 48 185.1V190.9c0 28.2 11.71 55.2 32.34 74.4L256 429.3l175.7-164c20.6-19.2 32.3-46.2 32.3-74.4V185.1c0-46.4-33.6-86.03-79.3-93.66C354.4 86.4 323.6 96.28 301.9 117.1l-46.8 46.8z"/></svg>
</span></span><span id=button_likes_text>&nbsp;Like</span></button></span></div><div class="flex flex-row flex-wrap items-center"><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/ai-generated/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">AI Generated
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/-daily-papers/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">ü§ó Daily Papers
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/multimodal-learning/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Multimodal Learning
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/vision-language-models/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Vision-Language Models
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/-northwestern-polytechnical-university/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">üè¢ Northwestern Polytechnical University</span></span></span></div></div><div class="flex author"><img class="!mt-0 !mb-0 h-24 w-24 rounded-full ltr:mr-4 rtl:ml-4" width=96 height=96 alt="Hugging Face Daily Papers" src=/ai-paper-reviewer/img/avatar_hu1570846118988919414.png><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">Author</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">Hugging Face Daily Papers</div><div class="text-sm text-neutral-700 dark:text-neutral-400">I am AI, and I review papers on HF Daily Papers</div><div class="text-2xl sm:text-lg"><div class="flex flex-wrap text-neutral-400 dark:text-neutral-500"><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://github.com/deep-diver/paper-reviewer/ target=_blank aria-label=Github rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://twitter.com/algo_diver/ target=_blank aria-label=Twitter rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span></a></div></div></div></div><div class=mb-5></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first lg:ml-auto px-0 lg:order-last ltr:lg:pl-8 rtl:lg:pr-8"><div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-[140px]"><details open id=TOCView class="toc-right mt-0 overflow-y-scroll overscroll-contain scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600 rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 hidden lg:block"><summary class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="min-w-[220px] py-2 border-dotted ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#encoder-free-3d-llms>Encoder-Free 3D LLMs</a></li><li><a href=#semantic-encoding>Semantic Encoding</a></li><li><a href=#geometric-aggregation>Geometric Aggregation</a></li><li><a href=#enel-model-results>ENEL Model Results</a></li><li><a href=#future-of-encoder-free>Future of Encoder-Free</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><details class="toc-inside mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 lg:hidden"><summary class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#encoder-free-3d-llms>Encoder-Free 3D LLMs</a></li><li><a href=#semantic-encoding>Semantic Encoding</a></li><li><a href=#geometric-aggregation>Geometric Aggregation</a></li><li><a href=#enel-model-results>ENEL Model Results</a></li><li><a href=#future-of-encoder-free>Future of Encoder-Free</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><script>var margin=200,marginError=50;(function(){var t=$(window),e=$("#TOCView"),s=e.height();function n(){var n=t.height()-margin;s>=n?(e.css("overflow-y","scroll"),e.css("max-height",n+marginError+"px")):(e.css("overflow-y","hidden"),e.css("max-height","9999999px"))}t.on("resize",n),$(document).ready(n)})()</script></div></div><div class="min-w-0 min-h-0 max-w-fit"><div class="article-content max-w-prose mb-20"><br><div class="flex flex-row flex-wrap items-center space-x-2"><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 48 48" fill="none"><rect width="48" height="48" fill="#fff" fill-opacity=".01"/><path d="M18 43V22c0-3.3137 2.6863-6 6-6s6 2.6863 6 6V43" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M12 40V22c0-6.6274 5.3726-12 12-12s12 5.3726 12 12V40" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M6 35V22C6 12.0589 14.0589 4 24 4s18 8.0589 18 18V35" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 44V31" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 24.625v-2.75" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/></svg>
</span></span><span>2502.09620</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg fill="#000" height="800" width="800" id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 511.999 511.999"><g><g><path d="M421.578 190.264l-99.847-99.847c-2.439-2.439-6.391-2.439-8.829.0L82.824 320.495c-2.439 2.439-2.439 6.392.0 8.829l99.847 99.847c2.439 2.439 6.391 2.439 8.829.0l230.078-230.078C424.017 196.655 424.017 192.703 421.578 190.264z"/></g></g><g><g><path d="M506.511 87.672 424.323 5.484c-7.308-7.31-19.175-7.315-26.488.0L348.219 55.1c-2.439 2.439-2.439 6.391.0 8.829l99.847 99.847c2.439 2.437 6.391 2.437 8.829.0l49.616-49.616C513.826 106.847 513.826 94.987 506.511 87.672z"/></g></g><g><g><path d="M508.133 491.11c-1.054-9.556-9.489-16.599-19.104-16.599H111.633l36.058-15.163c4.088-1.719 5.131-7.034 1.994-10.17l-86.854-86.854c-3.137-3.135-8.451-2.094-10.17 1.994C52.224 365.359 2.052 484.66 1.627 485.707c-5.815 13.208 4.855 27.01 18.107 26.263H489.52C500.566 511.97 509.379 502.408 508.133 491.11z"/></g></g></svg>
</span></span><span>Yiwen Tang et el.</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span>ü§ó 2025-02-14</span></span></span></div></div><p><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://arxiv.org/abs/2502.09620 target=_self role=button>‚Üó arXiv
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://huggingface.co/papers/2502.09620 target=_self role=button>‚Üó Hugging Face</a></p><audio controls><source src=https://ai-paper-reviewer.com/2502.09620/podcast.wav type=audio/wav>Your browser does not support the audio element.</audio><h3 class="relative group">TL;DR<div id=tldr class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#tldr aria-label=Anchor>#</a></span></h3><div class="lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl"><p>Current encoder-based 3D Large Multimodal Models (LMMs) face challenges adapting to varying point cloud resolutions and aligning encoder features with LLMs&rsquo; semantic needs. These limitations hinder the advancement of multimodal understanding. This often leads to performance degradation and inefficient model usage.</p><p>This paper introduces ENEL, the first comprehensive study of encoder-free architectures for 3D LMMs. It proposes two key strategies to overcome these limitations: LLM-embedded Semantic Encoding and Hierarchical Geometry Aggregation. The paper presents a novel Hybrid Semantic Loss for more effective high-level semantic extraction during pre-training. ENEL achieves state-of-the-art results in 3D classification, captioning, and visual question answering tasks, demonstrating the potential of encoder-free architectures for efficient and adaptive 3D LMMs.</p></div><h4 class="relative group">Key Takeaways<div id=key-takeaways class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#key-takeaways aria-label=Anchor>#</a></span></h4><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-f8c332c4a10778ea2676bb9f0e11a735></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-f8c332c4a10778ea2676bb9f0e11a735",{strings:[" Encoder-free 3D LMMs are feasible and achieve state-of-the-art performance. "],speed:10,lifeLike:!0,startDelay:0,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-12a02b746180af4169e903e14537224c></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-12a02b746180af4169e903e14537224c",{strings:[" LLM-embedded semantic encoding and hierarchical geometry aggregation effectively replace the traditional 3D encoder. "],speed:10,lifeLike:!0,startDelay:1e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-2d234eef14ef49f757c935d718201f86></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-2d234eef14ef49f757c935d718201f86",{strings:[" The proposed Hybrid Semantic Loss and the ENEL model offer a more efficient and adaptable architecture for 3D LMMs. "],speed:10,lifeLike:!0,startDelay:2e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><h4 class="relative group">Why does it matter?<div id=why-does-it-matter class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#why-does-it-matter aria-label=Anchor>#</a></span></h4><p>This paper is crucial because <strong>it challenges the conventional encoder-based approach in 3D Large Multimodal Models (LMMs)</strong>, a dominant paradigm in the field. By demonstrating a viable encoder-free alternative, it <strong>opens new avenues for research</strong>, particularly in handling varying point cloud resolutions and aligning encoder features with the semantic needs of LLMs. This work directly contributes to improving the efficiency and adaptability of 3D LMMs, impacting various downstream applications. Its findings encourage future exploration into improving LLM efficiency and reducing the computational burden of current encoder-based 3D LMMs.</p><hr><h4 class="relative group">Visual Insights<div id=visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#visual-insights aria-label=Anchor>#</a></span></h4><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.09620/extracted/6201996/intro3.png alt></figure></p><blockquote><p>üîº Figure 1 demonstrates the limitations of encoder-based 3D Large Multimodal Models (LMMs). Part (a) shows the impact of varying point cloud resolutions. During training, a fixed resolution (8192 points, 512 tokens) is used, but during inference, the resolution changes (2K-16K points, 128-2048 tokens). This mismatch causes a loss of information, significantly impacting performance as evaluated on the Objaverse captioning benchmark using GPT-4 scores. Part (b) illustrates the semantic mismatch between encoder embeddings and the LLM&rsquo;s needs. By visualizing attention weights (red indicates stronger attention), it shows that an encoder-free architecture produces point tokens with stronger semantic relevance for the language model.</p><details><summary>read the caption</summary>Figure 1: Issues of encoder-based 3D LMMs. (a) Point Cloud Resolution Limitation. During training, the point cloud size (P.T. size) and point token size (P.T. size) are fixed at 8192 and 512, respectively. And we adjust these two sizes during inference, point cloud size from 2K to 16K and the corresponding point token size from 128 to 2048. We evaluate them on the captioning task of the Objaverse benchmark using GPT-4 scores as the evaluation metric. (b) Embedding Semantic Discrepancy. We visualize the attention scores of the average text token to the point tokens, where red indicates higher values. The point tokens in the encoder-free architecture exhibit stronger textual semantic relevance needed for the LLM.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_centering ltx_align_middle" id=S2.T1.3><tr class=ltx_tr id=S2.T1.3.1><td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id=S2.T1.3.1.1 rowspan=2><span class="ltx_text ltx_font_bold" id=S2.T1.3.1.1.1>Method</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id=S2.T1.3.1.2><span class="ltx_text ltx_font_bold" id=S2.T1.3.1.2.1>Cls</span></td><td class="ltx_td ltx_align_center ltx_border_tt" colspan=2 id=S2.T1.3.1.3><span class="ltx_text ltx_font_bold" id=S2.T1.3.1.3.1>Cap</span></td></tr><tr class=ltx_tr id=S2.T1.3.2><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S2.T1.3.2.1><span class="ltx_text ltx_font_bold" id=S2.T1.3.2.1.1>GPT-4</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S2.T1.3.2.2><span class="ltx_text ltx_font_bold" id=S2.T1.3.2.2.1>GPT-4</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S2.T1.3.2.3><span class="ltx_text ltx_font_bold" id=S2.T1.3.2.3.1>S-BERT</span></td></tr><tr class=ltx_tr id=S2.T1.3.3><td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id=S2.T1.3.3.1>PointLLM-7B</td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S2.T1.3.3.2>53.00</td><td class="ltx_td ltx_align_center ltx_border_t" id=S2.T1.3.3.3>44.85</td><td class="ltx_td ltx_align_center ltx_border_t" id=S2.T1.3.3.4>47.47</td></tr><tr class=ltx_tr id=S2.T1.3.4><td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id=S2.T1.3.4.1>- Encoder</td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S2.T1.3.4.2>35.50</td><td class="ltx_td ltx_align_center ltx_border_t" id=S2.T1.3.4.3>33.37</td><td class="ltx_td ltx_align_center ltx_border_t" id=S2.T1.3.4.4>41.19</td></tr><tr class=ltx_tr id=S2.T1.3.5><td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id=S2.T1.3.5.1>+ 2-layer T.E.</td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S2.T1.3.5.2>42.50</td><td class="ltx_td ltx_align_center ltx_border_t" id=S2.T1.3.5.3>41.35</td><td class="ltx_td ltx_align_center ltx_border_t" id=S2.T1.3.5.4>44.25</td></tr><tr class=ltx_tr id=S2.T1.3.6><td class="ltx_td ltx_align_left ltx_border_r" id=S2.T1.3.6.1><span class="ltx_text ltx_font_bold" id=S2.T1.3.6.1.1>+ 3-layer T.E.</span></td><td class="ltx_td ltx_align_center ltx_border_r" id=S2.T1.3.6.2><span class="ltx_text ltx_font_bold" id=S2.T1.3.6.2.1>47.31</span></td><td class="ltx_td ltx_align_center" id=S2.T1.3.6.3><span class="ltx_text ltx_font_bold" id=S2.T1.3.6.3.1>43.86</span></td><td class="ltx_td ltx_align_center" id=S2.T1.3.6.4><span class="ltx_text ltx_font_bold" id=S2.T1.3.6.4.1>45.89</span></td></tr><tr class=ltx_tr id=S2.T1.3.7><td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id=S2.T1.3.7.1>+ 4-layer T.E.</td><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id=S2.T1.3.7.2>45.00</td><td class="ltx_td ltx_align_center ltx_border_bb" id=S2.T1.3.7.3>42.99</td><td class="ltx_td ltx_align_center ltx_border_bb" id=S2.T1.3.7.4>44.51</td></tr></table></table></figure><blockquote><p>üîº This table presents a comparison of different token embedding methods for 3D Large Multimodal Models (LMMs) on the Objaverse benchmark. The baseline model used is PointLLM-7B. The performance is measured using two tasks: classification (&lsquo;Cls&rsquo;) and captioning (&lsquo;Cap&rsquo;). The results are evaluated using three metrics: GPT-4 scores, Sentence-BERT (S-BERT) scores, and a custom score. The table compares the baseline PointLLM-7B model with various versions that incorporate a custom designed token embedding module (&lsquo;T.E.&rsquo;) with varying numbers of layers.</p><details><summary>read the caption</summary>Table 1: Token Embedding. We evaluate the performance on the Objaverse benchmark and adopt PointLLM-7B as the baseline model. ‚ÄôCls‚Äô and ‚ÄôCap‚Äô represent classification and captioning tasks, respectively. S-BERT refers to the Sentence-BERT. T.E. stands for our designed token embedding module.</details></blockquote><h3 class="relative group">In-depth insights<div id=in-depth-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#in-depth-insights aria-label=Anchor>#</a></span></h3><h4 class="relative group">Encoder-Free 3D LLMs<div id=encoder-free-3d-llms class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#encoder-free-3d-llms aria-label=Anchor>#</a></span></h4><p>Encoder-free 3D LLMs represent a significant departure from traditional encoder-based architectures. <strong>The core idea is to eliminate the need for a separate 3D encoder</strong>, which typically processes point cloud data into embeddings before feeding them to the Large Language Model (LLM). This approach directly integrates the 3D encoding function within the LLM, aiming to improve adaptability to varying point cloud resolutions and alleviate the semantic gap between encoder outputs and LLM requirements. <strong>Key challenges in realizing this include compensating for the loss of high-level 3D semantics</strong> usually extracted by encoders and <strong>integrating inductive biases</strong> for efficient 3D structure perception directly into the LLM. Solutions proposed often involve novel pre-training strategies, such as incorporating self-supervised losses that focus on semantic understanding within the LLM, and fine-tuning methodologies that introduce hierarchical geometry aggregation to capture local details. The potential benefits are substantial, including improved efficiency and flexibility, but the success heavily depends on effectively handling the aforementioned challenges.</p><h4 class="relative group">Semantic Encoding<div id=semantic-encoding class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#semantic-encoding aria-label=Anchor>#</a></span></h4><p>The concept of semantic encoding in the context of 3D Large Multimodal Models (LMMs) is crucial for bridging the gap between raw point cloud data and the high-level semantic understanding required by Large Language Models (LLMs). <strong>Effective semantic encoding is paramount for enabling LLMs to interpret and reason about 3D scenes</strong>. The paper explores strategies to achieve this without relying on traditional 3D encoders, which often introduce limitations in terms of point cloud resolution adaptability and semantic alignment with LLMs. <strong>The proposed LLM-embedded Semantic Encoding strategy directly embeds semantic information within the LLM</strong>, leveraging self-supervised learning techniques to guide the LLM&rsquo;s learning process. This innovative approach attempts to replace the role of the traditional 3D encoder, <strong>allowing the LLM itself to learn and extract meaningful 3D semantics</strong>. The study&rsquo;s experiments show promising results, demonstrating the feasibility and potential of this encoder-free approach to improve the performance of 3D LMMs.</p><h4 class="relative group">Geometric Aggregation<div id=geometric-aggregation class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#geometric-aggregation aria-label=Anchor>#</a></span></h4><p>The concept of &lsquo;Geometric Aggregation&rsquo; in the context of 3D Large Multimodal Models (LMMs) addresses the challenge of incorporating inductive bias into LLMs for better 3D geometric structure perception. Traditional 3D encoders often embed this bias explicitly, but LLMs lack such inherent structure. <strong>The proposed strategy aims to compensate by introducing a hierarchical aggregation mechanism in the early LLM layers.</strong> This involves using techniques like farthest point sampling (FPS) and k-Nearest Neighbors (k-NN) to aggregate tokens based on geometric proximity, thereby mimicking the multi-level processing of traditional encoders. <strong>The integration of gated self-attention further enhances the process by adaptively focusing on relevant information.</strong> This hierarchical approach helps the LLM capture both local details and global relationships within the 3D point cloud, enabling more nuanced understanding. <strong>Experimental results showcase the effectiveness of this strategy, demonstrating improved performance on tasks requiring detailed 3D understanding.</strong> However, the optimal level of hierarchy requires careful tuning; excessive aggregation can lead to information loss, highlighting the need for a balanced approach that preserves both local and global contextual information.</p><h4 class="relative group">ENEL Model Results<div id=enel-model-results class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#enel-model-results aria-label=Anchor>#</a></span></h4><p>An &lsquo;ENEL Model Results&rsquo; section would ideally present a detailed analysis of the encoder-free 3D Large Multimodal Model&rsquo;s performance across various tasks. It should begin by comparing ENEL&rsquo;s results to existing state-of-the-art encoder-based models, highlighting any <strong>significant performance improvements or shortcomings</strong>. Key metrics such as accuracy, precision, recall, and F1-score for tasks like 3D classification, captioning, and visual question answering (VQA) should be meticulously reported. Furthermore, an ablation study demonstrating the impact of individual components, like the LLM-embedded semantic encoding strategy or hierarchical geometry aggregation, is crucial to validate the design choices. <strong>Error analysis</strong> should also be included, identifying specific types of inputs or tasks where ENEL struggles and suggesting potential areas for improvement. Finally, a discussion on the <strong>efficiency and scalability</strong> of ENEL compared to encoder-based methods would provide a complete picture of the model&rsquo;s strengths and weaknesses, paving the way for future developments.</p><h4 class="relative group">Future of Encoder-Free<div id=future-of-encoder-free class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#future-of-encoder-free aria-label=Anchor>#</a></span></h4><p>The &ldquo;Future of Encoder-Free&rdquo; architectures in 3D Large Multimodal Models (LMMs) is promising, but faces challenges. <strong>Encoder-free approaches offer potential advantages in handling varying point cloud resolutions and aligning embedding semantics with LLMs&rsquo; needs.</strong> However, successfully replacing encoders requires overcoming the inherent difficulty of capturing high-level 3D semantics and geometric structures directly within the LLM. <strong>Future research should focus on developing more sophisticated self-supervised learning strategies and inductive bias mechanisms integrated into LLMs.</strong> This might involve exploring novel loss functions tailored to 3D data or incorporating architectural modifications within the LLM to better process spatial information. <strong>Further investigation into efficient token embedding modules is crucial</strong>, as they are the direct interface between raw point cloud data and the LLM. Ultimately, the success of encoder-free 3D LMMs depends on achieving comparable or superior performance to encoder-based models while maintaining the benefits of simplicity and flexibility.</p><h3 class="relative group">More visual insights<div id=more-visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#more-visual-insights aria-label=Anchor>#</a></span></h3><details><summary>More on figures</summary><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.09620/x1.png alt></figure></p><blockquote><p>üîº The figure illustrates the two-stage training pipeline of the ENEL model. Stage 1 (pre-training) focuses on embedding high-level 3D semantics into the LLM by making the first K layers learnable and utilizing the Hybrid Semantic Loss. Stage 2 (instruction tuning) employs the Hierarchical Geometric Aggregation strategy to enable the LLM to effectively capture the local geometric structures within the point cloud data.</p><details><summary>read the caption</summary>Figure 2: Overall Pipeline of Enel. The training is divided into two stages: the pre-training stage and the instruction tuning stage. In the first stage, we set the first KùêæKitalic_K layers to be learnable and apply the proposed Hybrid Semantic Loss to embed high-level semantics into the LLM. In the second stage, we adopt the Hierarchical Geometric Aggregation strategy to capture local structures of point clouds.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.09620/x2.png alt></figure></p><blockquote><p>üîº Figure 3 illustrates various self-supervised learning methods applied during the pre-training phase of an encoder-free 3D Large Multimodal Model (LMM). Subfigures (a) through (d) depict common approaches: Masked Modeling Loss, Reconstruction Loss, Contrastive Loss, and Knowledge Distillation Loss, respectively. Each method aims to learn high-level 3D semantic information from point cloud data without relying on a traditional 3D encoder. Subfigure (e) introduces a novel Hybrid Semantic Loss, specifically designed for this encoder-free architecture, combining aspects of the previous methods to achieve optimal performance.</p><details><summary>read the caption</summary>Figure 3: Point Cloud Self-Supervised Learning Losses. In the pre-training stage, we explore common self-supervised learning losses for the encoder-free 3D LMM: (a) Masked Modeling Loss, (b) Reconstruction Loss, (c) Contrastive Loss, and (d) Knowledge Distillation Loss. The (e) represents our proposed Hybrid Semantic Loss, specifically designed for the encoder-free architecture.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.09620/x3.png alt></figure></p><blockquote><p>üîº This figure illustrates the Hierarchical Geometry Aggregation strategy used in the instruction tuning stage of the ENEL model. The strategy aims to incorporate inductive bias into the LLM&rsquo;s early layers, allowing it to focus on local details within the point cloud data. This is achieved through a series of aggregation and propagation operations applied to the point tokens. Aggregation combines information from neighboring points, effectively capturing local geometric structures. Propagation then spreads this aggregated information back to the original point tokens, ensuring that local details are integrated into the higher-level semantic understanding of the point cloud by the LLM.</p><details><summary>read the caption</summary>Figure 4: Hierarchical Geometry Aggregation Strategy. In the instruction tuning stage, we apply aggregation and propagation operations to the point tokens to capture the local structural details.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.09620/x4.png alt></figure></p><blockquote><p>üîº Figure 5 visualizes the attention weights between the average word embedding and point cloud embeddings for encoder-based (PointLLM) and encoder-free (ENEL) models. The heatmaps show the attention scores, with redder colors indicating stronger attention. The figure demonstrates how the encoder-free model attends more directly to semantically relevant parts of the 3D object, whereas the encoder-based model&rsquo;s attention is more diffuse. Three object categories are shown: chairs (a), airplanes (b), and lamps (c), illustrating this difference in attention across various object types. The results support the claim that the encoder-free architecture achieves better semantic encoding.</p><details><summary>read the caption</summary>Figure 5: Difference in Semantic Encoding. By visualizing the attention scores of the average text token to the point tokens on the Objaverse dataset, we compare the semantic encoding potential of encoder-based and encoder-free architectures, where red indicates higher values. And (a) represents chairs, (b) represents airplanes, and (c) represents lamps.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.09620/extracted/6201996/loss2.png alt></figure></p><blockquote><p>üîº Figure 6 illustrates three variations of self-supervised learning strategies used for point cloud data in the context of encoder-free 3D large multimodal models (LMMs). Each subfigure shows a different loss function applied during pre-training to embed high-level semantics into the language model without explicit 3D encoders: (a) Masked Modeling Loss: A portion of the point tokens are masked, and the model attempts to predict their values. (b) Reconstruction Loss: The model reconstructs the original point cloud from a learned representation. (c) Hybrid Semantic Loss: Combines Masked Modeling and Reconstruction Losses, aiming to capture both high-level semantic information and fine-grained geometric details.</p><details><summary>read the caption</summary>Figure 6: Variants of Point Cloud Self-Supervised Learning Losses. (a) The Variant of Masked Modeling Loss, (b) The Variant of Reconstruction Loss, (c) The Variant of Hybrid Semantic Loss.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2502.09620/extracted/6201996/output3.png alt></figure></p><blockquote><p>üîº Figure 7 showcases several examples of ENEL&rsquo;s responses to various prompts involving 3D models. The prompts range from detailed descriptions to more specific questions requiring object recognition and property analysis. The examples demonstrate ENEL&rsquo;s ability to produce both precise and varied answers, highlighting its capability for nuanced understanding and generation within the context of 3D multimodal data.</p><details><summary>read the caption</summary>Figure 7: Enel Output Examples. We demonstrate that Enel provides precise and diverse responses when addressing different problems.</details></blockquote></details><details><summary>More on tables</summary><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_align_middle" id=S2.T2.3.1><tr class=ltx_tr id=S2.T2.3.1.1><td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id=S2.T2.3.1.1.1 rowspan=2><span class="ltx_text ltx_font_bold" id=S2.T2.3.1.1.1.1>Method</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id=S2.T2.3.1.1.2 rowspan=2><span class="ltx_text ltx_font_bold" id=S2.T2.3.1.1.2.1>LR</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id=S2.T2.3.1.1.3><span class="ltx_text ltx_font_bold" id=S2.T2.3.1.1.3.1>Cls</span></td><td class="ltx_td ltx_align_center ltx_border_tt" colspan=2 id=S2.T2.3.1.1.4><span class="ltx_text ltx_font_bold" id=S2.T2.3.1.1.4.1>Cap</span></td></tr><tr class=ltx_tr id=S2.T2.3.1.2><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S2.T2.3.1.2.1><span class="ltx_text ltx_font_bold" id=S2.T2.3.1.2.1.1>GPT-4</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S2.T2.3.1.2.2><span class="ltx_text ltx_font_bold" id=S2.T2.3.1.2.2.1>GPT-4</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S2.T2.3.1.2.3><span class="ltx_text ltx_font_bold" id=S2.T2.3.1.2.3.1>S-BERT</span></td></tr><tr class=ltx_tr id=S2.T2.3.1.3><td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id=S2.T2.3.1.3.1>PointLLM-7B</td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S2.T2.3.1.3.2>2e-3</td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S2.T2.3.1.3.3>53.00</td><td class="ltx_td ltx_align_center ltx_border_t" id=S2.T2.3.1.3.4>44.85</td><td class="ltx_td ltx_align_center ltx_border_t" id=S2.T2.3.1.3.5>47.47</td></tr><tr class=ltx_tr id=S2.T2.3.1.4><td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id=S2.T2.3.1.4.1 rowspan=2><span class=ltx_text id=S2.T2.3.1.4.1.1>+ 2 learnable layers</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S2.T2.3.1.4.2>2e-3</td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S2.T2.3.1.4.3>41.06</td><td class="ltx_td ltx_align_center ltx_border_t" id=S2.T2.3.1.4.4>42.23</td><td class="ltx_td ltx_align_center ltx_border_t" id=S2.T2.3.1.4.5>45.92</td></tr><tr class=ltx_tr id=S2.T2.3.1.5><td class="ltx_td ltx_align_center ltx_border_r" id=S2.T2.3.1.5.1>4e-4</td><td class="ltx_td ltx_align_center ltx_border_r" id=S2.T2.3.1.5.2>45.5</td><td class="ltx_td ltx_align_center" id=S2.T2.3.1.5.3>44.72</td><td class="ltx_td ltx_align_center" id=S2.T2.3.1.5.4>47.35</td></tr><tr class=ltx_tr id=S2.T2.3.1.6><td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id=S2.T2.3.1.6.1 rowspan=2><span class="ltx_text ltx_font_bold" id=S2.T2.3.1.6.1.1>+ 4 learnable layers</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S2.T2.3.1.6.2>2e-3</td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S2.T2.3.1.6.3>44.85</td><td class="ltx_td ltx_align_center ltx_border_t" id=S2.T2.3.1.6.4>41.53</td><td class="ltx_td ltx_align_center ltx_border_t" id=S2.T2.3.1.6.5>46.77</td></tr><tr class=ltx_tr id=S2.T2.3.1.7><td class="ltx_td ltx_align_center ltx_border_r" id=S2.T2.3.1.7.1><span class="ltx_text ltx_font_bold" id=S2.T2.3.1.7.1.1>4e-4</span></td><td class="ltx_td ltx_align_center ltx_border_r" id=S2.T2.3.1.7.2><span class="ltx_text ltx_font_bold" id=S2.T2.3.1.7.2.1>49.11</span></td><td class="ltx_td ltx_align_center" id=S2.T2.3.1.7.3><span class="ltx_text ltx_font_bold" id=S2.T2.3.1.7.3.1>45.39</span></td><td class="ltx_td ltx_align_center" id=S2.T2.3.1.7.4><span class="ltx_text ltx_font_bold" id=S2.T2.3.1.7.4.1>47.71</span></td></tr><tr class=ltx_tr id=S2.T2.3.1.8><td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t" id=S2.T2.3.1.8.1 rowspan=2><span class=ltx_text id=S2.T2.3.1.8.1.1>+ 8 learnable layers</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S2.T2.3.1.8.2>2e-3</td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S2.T2.3.1.8.3>43.76</td><td class="ltx_td ltx_align_center ltx_border_t" id=S2.T2.3.1.8.4>39.71</td><td class="ltx_td ltx_align_center ltx_border_t" id=S2.T2.3.1.8.5>42.38</td></tr><tr class=ltx_tr id=S2.T2.3.1.9><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id=S2.T2.3.1.9.1>4e-4</td><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id=S2.T2.3.1.9.2>48.00</td><td class="ltx_td ltx_align_center ltx_border_bb" id=S2.T2.3.1.9.3>44.49</td><td class="ltx_td ltx_align_center ltx_border_bb" id=S2.T2.3.1.9.4>47.21</td></tr></table></table></figure><blockquote><p>üîº This table presents ablation study results on further 3D encoding by making early layers of the LLM learnable. It shows the impact of varying the number of learnable layers (2, 4, or 8) and learning rates (2e-3 and 4e-4) during pre-training on the performance of the model in terms of GPT-4 scores for classification and captioning tasks, as well as S-BERT scores. The original learning rate used is 2e-3. The results demonstrate how making the LLM layers learnable, in combination with adjustments to the learning rate, contributes to improved performance, effectively emulating a 3D encoder&rsquo;s functionality within the LLM.</p><details><summary>read the caption</summary>Table 2: Further 3D Encoding. We set the LLM early layers to be learnable. LR represents the learning rate during the pre-training stage, with the original learning rate set to 2e-3.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_align_middle" id=S2.T3.28.12><tr class=ltx_tr id=S2.T3.28.12.13><td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id=S2.T3.28.12.13.1 rowspan=2><span class="ltx_text ltx_font_bold" id=S2.T3.28.12.13.1.1>Method</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id=S2.T3.28.12.13.2><span class="ltx_text ltx_font_bold" id=S2.T3.28.12.13.2.1>Cls</span></td><td class="ltx_td ltx_align_center ltx_border_tt" colspan=2 id=S2.T3.28.12.13.3><span class="ltx_text ltx_font_bold" id=S2.T3.28.12.13.3.1>Cap</span></td></tr><tr class=ltx_tr id=S2.T3.28.12.14><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S2.T3.28.12.14.1><span class="ltx_text ltx_font_bold" id=S2.T3.28.12.14.1.1>GPT-4</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S2.T3.28.12.14.2><span class="ltx_text ltx_font_bold" id=S2.T3.28.12.14.2.1>GPT-4</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S2.T3.28.12.14.3><span class="ltx_text ltx_font_bold" id=S2.T3.28.12.14.3.1>S-BERT</span></td></tr><tr class=ltx_tr id=S2.T3.28.12.15><td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id=S2.T3.28.12.15.1>PointLLM-7B</td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S2.T3.28.12.15.2>53.00</td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S2.T3.28.12.15.3>44.85</td><td class="ltx_td ltx_align_center ltx_border_t" id=S2.T3.28.12.15.4>47.47</td></tr><tr class=ltx_tr id=S2.T3.18.2.2><td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id=S2.T3.18.2.2.2>Masked Modeling Loss<math alttext="{}_{\text{patch}}" class="ltx_Math" display="inline" id="S2.T3.17.1.1.1.m1.1"><semantics id="S2.T3.17.1.1.1.m1.1a"><msub id="S2.T3.17.1.1.1.m1.1.1" xref="S2.T3.17.1.1.1.m1.1.1.cmml"><mi id="S2.T3.17.1.1.1.m1.1.1a" xref="S2.T3.17.1.1.1.m1.1.1.cmml"></mi><mtext id="S2.T3.17.1.1.1.m1.1.1.1" xref="S2.T3.17.1.1.1.m1.1.1.1a.cmml">patch</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.T3.17.1.1.1.m1.1b"><apply id="S2.T3.17.1.1.1.m1.1.1.cmml" xref="S2.T3.17.1.1.1.m1.1.1"><ci id="S2.T3.17.1.1.1.m1.1.1.1a.cmml" xref="S2.T3.17.1.1.1.m1.1.1.1"><mtext id="S2.T3.17.1.1.1.m1.1.1.1.cmml" mathsize="70%" xref="S2.T3.17.1.1.1.m1.1.1.1">patch</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T3.17.1.1.1.m1.1c">{}_{\text{patch}}</annotation><annotation encoding="application/x-llamapun" id="S2.T3.17.1.1.1.m1.1d">start_FLOATSUBSCRIPT patch end_FLOATSUBSCRIPT</annotation></semantics></math><sup class=ltx_sup id=S2.T3.18.2.2.2.1><span class="ltx_text ltx_font_italic" id=S2.T3.18.2.2.2.1.1>Œ®</span></sup></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S2.T3.18.2.2.3>48.50</td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S2.T3.18.2.2.4>45.34</td><td class="ltx_td ltx_align_center ltx_border_t" id=S2.T3.18.2.2.5>46.36</td></tr><tr class=ltx_tr id=S2.T3.20.4.4><td class="ltx_td ltx_align_left ltx_border_r" id=S2.T3.20.4.4.2>Masked Modeling Loss<math alttext="{}_{\text{patch}}" class="ltx_Math" display="inline" id="S2.T3.19.3.3.1.m1.1"><semantics id="S2.T3.19.3.3.1.m1.1a"><msub id="S2.T3.19.3.3.1.m1.1.1" xref="S2.T3.19.3.3.1.m1.1.1.cmml"><mi id="S2.T3.19.3.3.1.m1.1.1a" xref="S2.T3.19.3.3.1.m1.1.1.cmml"></mi><mtext id="S2.T3.19.3.3.1.m1.1.1.1" xref="S2.T3.19.3.3.1.m1.1.1.1a.cmml">patch</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.T3.19.3.3.1.m1.1b"><apply id="S2.T3.19.3.3.1.m1.1.1.cmml" xref="S2.T3.19.3.3.1.m1.1.1"><ci id="S2.T3.19.3.3.1.m1.1.1.1a.cmml" xref="S2.T3.19.3.3.1.m1.1.1.1"><mtext id="S2.T3.19.3.3.1.m1.1.1.1.cmml" mathsize="70%" xref="S2.T3.19.3.3.1.m1.1.1.1">patch</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T3.19.3.3.1.m1.1c">{}_{\text{patch}}</annotation><annotation encoding="application/x-llamapun" id="S2.T3.19.3.3.1.m1.1d">start_FLOATSUBSCRIPT patch end_FLOATSUBSCRIPT</annotation></semantics></math><sup class=ltx_sup id=S2.T3.20.4.4.2.1><span class="ltx_text ltx_font_italic" id=S2.T3.20.4.4.2.1.1>Œ¶</span></sup></td><td class="ltx_td ltx_align_center ltx_border_r" id=S2.T3.20.4.4.3>50.00</td><td class="ltx_td ltx_align_center ltx_border_r" id=S2.T3.20.4.4.4>46.80</td><td class="ltx_td ltx_align_center" id=S2.T3.20.4.4.5>47.29</td></tr><tr class=ltx_tr id=S2.T3.22.6.6><td class="ltx_td ltx_align_left ltx_border_r" id=S2.T3.22.6.6.2>Masked Modeling Loss<math alttext="{}_{\text{feat}}" class="ltx_Math" display="inline" id="S2.T3.21.5.5.1.m1.1"><semantics id="S2.T3.21.5.5.1.m1.1a"><msub id="S2.T3.21.5.5.1.m1.1.1" xref="S2.T3.21.5.5.1.m1.1.1.cmml"><mi id="S2.T3.21.5.5.1.m1.1.1a" xref="S2.T3.21.5.5.1.m1.1.1.cmml"></mi><mtext id="S2.T3.21.5.5.1.m1.1.1.1" xref="S2.T3.21.5.5.1.m1.1.1.1a.cmml">feat</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.T3.21.5.5.1.m1.1b"><apply id="S2.T3.21.5.5.1.m1.1.1.cmml" xref="S2.T3.21.5.5.1.m1.1.1"><ci id="S2.T3.21.5.5.1.m1.1.1.1a.cmml" xref="S2.T3.21.5.5.1.m1.1.1.1"><mtext id="S2.T3.21.5.5.1.m1.1.1.1.cmml" mathsize="70%" xref="S2.T3.21.5.5.1.m1.1.1.1">feat</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T3.21.5.5.1.m1.1c">{}_{\text{feat}}</annotation><annotation encoding="application/x-llamapun" id="S2.T3.21.5.5.1.m1.1d">start_FLOATSUBSCRIPT feat end_FLOATSUBSCRIPT</annotation></semantics></math><sup class=ltx_sup id=S2.T3.22.6.6.2.1><span class="ltx_text ltx_font_italic" id=S2.T3.22.6.6.2.1.1>Œ®</span></sup></td><td class="ltx_td ltx_align_center ltx_border_r" id=S2.T3.22.6.6.3>50.00</td><td class="ltx_td ltx_align_center ltx_border_r" id=S2.T3.22.6.6.4>45.80</td><td class="ltx_td ltx_align_center" id=S2.T3.22.6.6.5>46.29</td></tr><tr class=ltx_tr id=S2.T3.24.8.8><td class="ltx_td ltx_align_left ltx_border_r" id=S2.T3.24.8.8.2>Masked Modeling Loss<math alttext="{}_{\text{feat}}" class="ltx_Math" display="inline" id="S2.T3.23.7.7.1.m1.1"><semantics id="S2.T3.23.7.7.1.m1.1a"><msub id="S2.T3.23.7.7.1.m1.1.1" xref="S2.T3.23.7.7.1.m1.1.1.cmml"><mi id="S2.T3.23.7.7.1.m1.1.1a" xref="S2.T3.23.7.7.1.m1.1.1.cmml"></mi><mtext id="S2.T3.23.7.7.1.m1.1.1.1" xref="S2.T3.23.7.7.1.m1.1.1.1a.cmml">feat</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.T3.23.7.7.1.m1.1b"><apply id="S2.T3.23.7.7.1.m1.1.1.cmml" xref="S2.T3.23.7.7.1.m1.1.1"><ci id="S2.T3.23.7.7.1.m1.1.1.1a.cmml" xref="S2.T3.23.7.7.1.m1.1.1.1"><mtext id="S2.T3.23.7.7.1.m1.1.1.1.cmml" mathsize="70%" xref="S2.T3.23.7.7.1.m1.1.1.1">feat</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T3.23.7.7.1.m1.1c">{}_{\text{feat}}</annotation><annotation encoding="application/x-llamapun" id="S2.T3.23.7.7.1.m1.1d">start_FLOATSUBSCRIPT feat end_FLOATSUBSCRIPT</annotation></semantics></math><sup class=ltx_sup id=S2.T3.24.8.8.2.1><span class="ltx_text ltx_font_italic" id=S2.T3.24.8.8.2.1.1>Œ¶</span></sup></td><td class="ltx_td ltx_align_center ltx_border_r" id=S2.T3.24.8.8.3>49.50</td><td class="ltx_td ltx_align_center ltx_border_r" id=S2.T3.24.8.8.4>47.35</td><td class="ltx_td ltx_align_center" id=S2.T3.24.8.8.5>47.93</td></tr><tr class=ltx_tr id=S2.T3.25.9.9><td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id=S2.T3.25.9.9.1>Reconstruction Loss<math alttext="{}_{\text{patch}}" class="ltx_Math" display="inline" id="S2.T3.25.9.9.1.m1.1"><semantics id="S2.T3.25.9.9.1.m1.1a"><msub id="S2.T3.25.9.9.1.m1.1.1" xref="S2.T3.25.9.9.1.m1.1.1.cmml"><mi id="S2.T3.25.9.9.1.m1.1.1a" xref="S2.T3.25.9.9.1.m1.1.1.cmml"></mi><mtext id="S2.T3.25.9.9.1.m1.1.1.1" xref="S2.T3.25.9.9.1.m1.1.1.1a.cmml">patch</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.T3.25.9.9.1.m1.1b"><apply id="S2.T3.25.9.9.1.m1.1.1.cmml" xref="S2.T3.25.9.9.1.m1.1.1"><ci id="S2.T3.25.9.9.1.m1.1.1.1a.cmml" xref="S2.T3.25.9.9.1.m1.1.1.1"><mtext id="S2.T3.25.9.9.1.m1.1.1.1.cmml" mathsize="70%" xref="S2.T3.25.9.9.1.m1.1.1.1">patch</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T3.25.9.9.1.m1.1c">{}_{\text{patch}}</annotation><annotation encoding="application/x-llamapun" id="S2.T3.25.9.9.1.m1.1d">start_FLOATSUBSCRIPT patch end_FLOATSUBSCRIPT</annotation></semantics></math></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S2.T3.25.9.9.2>49.50</td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S2.T3.25.9.9.3>46.96</td><td class="ltx_td ltx_align_center ltx_border_t" id=S2.T3.25.9.9.4>47.33</td></tr><tr class=ltx_tr id=S2.T3.26.10.10><td class="ltx_td ltx_align_left ltx_border_r" id=S2.T3.26.10.10.1>Reconstruction Loss<math alttext="{}_{\text{feat}}" class="ltx_Math" display="inline" id="S2.T3.26.10.10.1.m1.1"><semantics id="S2.T3.26.10.10.1.m1.1a"><msub id="S2.T3.26.10.10.1.m1.1.1" xref="S2.T3.26.10.10.1.m1.1.1.cmml"><mi id="S2.T3.26.10.10.1.m1.1.1a" xref="S2.T3.26.10.10.1.m1.1.1.cmml"></mi><mtext id="S2.T3.26.10.10.1.m1.1.1.1" xref="S2.T3.26.10.10.1.m1.1.1.1a.cmml">feat</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.T3.26.10.10.1.m1.1b"><apply id="S2.T3.26.10.10.1.m1.1.1.cmml" xref="S2.T3.26.10.10.1.m1.1.1"><ci id="S2.T3.26.10.10.1.m1.1.1.1a.cmml" xref="S2.T3.26.10.10.1.m1.1.1.1"><mtext id="S2.T3.26.10.10.1.m1.1.1.1.cmml" mathsize="70%" xref="S2.T3.26.10.10.1.m1.1.1.1">feat</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T3.26.10.10.1.m1.1c">{}_{\text{feat}}</annotation><annotation encoding="application/x-llamapun" id="S2.T3.26.10.10.1.m1.1d">start_FLOATSUBSCRIPT feat end_FLOATSUBSCRIPT</annotation></semantics></math></td><td class="ltx_td ltx_align_center ltx_border_r" id=S2.T3.26.10.10.2>48.50</td><td class="ltx_td ltx_align_center ltx_border_r" id=S2.T3.26.10.10.3>45.95</td><td class="ltx_td ltx_align_center" id=S2.T3.26.10.10.4>47.18</td></tr><tr class=ltx_tr id=S2.T3.28.12.16><td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id=S2.T3.28.12.16.1>Contrastive Loss</td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S2.T3.28.12.16.2>43.50</td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S2.T3.28.12.16.3>42.91</td><td class="ltx_td ltx_align_center ltx_border_t" id=S2.T3.28.12.16.4>44.77</td></tr><tr class=ltx_tr id=S2.T3.28.12.17><td class="ltx_td ltx_align_left ltx_border_r" id=S2.T3.28.12.17.1>Knowledge Distillation Loss</td><td class="ltx_td ltx_align_center ltx_border_r" id=S2.T3.28.12.17.2>49.50</td><td class="ltx_td ltx_align_center ltx_border_r" id=S2.T3.28.12.17.3>45.43</td><td class="ltx_td ltx_align_center" id=S2.T3.28.12.17.4>47.09</td></tr><tr class=ltx_tr id=S2.T3.27.11.11><td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id=S2.T3.27.11.11.1>Hybrid Semantic Loss<math alttext="{}_{\text{patch}}" class="ltx_Math" display="inline" id="S2.T3.27.11.11.1.m1.1"><semantics id="S2.T3.27.11.11.1.m1.1a"><msub id="S2.T3.27.11.11.1.m1.1.1" xref="S2.T3.27.11.11.1.m1.1.1.cmml"><mi id="S2.T3.27.11.11.1.m1.1.1a" xref="S2.T3.27.11.11.1.m1.1.1.cmml"></mi><mtext id="S2.T3.27.11.11.1.m1.1.1.1" xref="S2.T3.27.11.11.1.m1.1.1.1a.cmml">patch</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.T3.27.11.11.1.m1.1b"><apply id="S2.T3.27.11.11.1.m1.1.1.cmml" xref="S2.T3.27.11.11.1.m1.1.1"><ci id="S2.T3.27.11.11.1.m1.1.1.1a.cmml" xref="S2.T3.27.11.11.1.m1.1.1.1"><mtext id="S2.T3.27.11.11.1.m1.1.1.1.cmml" mathsize="70%" xref="S2.T3.27.11.11.1.m1.1.1.1">patch</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T3.27.11.11.1.m1.1c">{}_{\text{patch}}</annotation><annotation encoding="application/x-llamapun" id="S2.T3.27.11.11.1.m1.1d">start_FLOATSUBSCRIPT patch end_FLOATSUBSCRIPT</annotation></semantics></math></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S2.T3.27.11.11.2>50.50</td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S2.T3.27.11.11.3>46.84</td><td class="ltx_td ltx_align_center ltx_border_t" id=S2.T3.27.11.11.4>47.59</td></tr><tr class=ltx_tr id=S2.T3.28.12.12><td class="ltx_td ltx_align_left ltx_border_r" id=S2.T3.28.12.12.1>Hybrid Semantic Loss<math alttext="{}_{\text{feat}}" class="ltx_Math" display="inline" id="S2.T3.28.12.12.1.m1.1"><semantics id="S2.T3.28.12.12.1.m1.1a"><msub id="S2.T3.28.12.12.1.m1.1.1" xref="S2.T3.28.12.12.1.m1.1.1.cmml"><mi id="S2.T3.28.12.12.1.m1.1.1a" xref="S2.T3.28.12.12.1.m1.1.1.cmml"></mi><mtext id="S2.T3.28.12.12.1.m1.1.1.1" xref="S2.T3.28.12.12.1.m1.1.1.1a.cmml">feat</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.T3.28.12.12.1.m1.1b"><apply id="S2.T3.28.12.12.1.m1.1.1.cmml" xref="S2.T3.28.12.12.1.m1.1.1"><ci id="S2.T3.28.12.12.1.m1.1.1.1a.cmml" xref="S2.T3.28.12.12.1.m1.1.1.1"><mtext id="S2.T3.28.12.12.1.m1.1.1.1.cmml" mathsize="70%" xref="S2.T3.28.12.12.1.m1.1.1.1">feat</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T3.28.12.12.1.m1.1c">{}_{\text{feat}}</annotation><annotation encoding="application/x-llamapun" id="S2.T3.28.12.12.1.m1.1d">start_FLOATSUBSCRIPT feat end_FLOATSUBSCRIPT</annotation></semantics></math></td><td class="ltx_td ltx_align_center ltx_border_r" id=S2.T3.28.12.12.2>52.00</td><td class="ltx_td ltx_align_center ltx_border_r" id=S2.T3.28.12.12.3>48.51</td><td class="ltx_td ltx_align_center" id=S2.T3.28.12.12.4>48.06</td></tr><tr class=ltx_tr id=S2.T3.28.12.18><td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id=S2.T3.28.12.18.1><span class="ltx_text ltx_font_bold" id=S2.T3.28.12.18.1.1>+ Position Embedding</span></td><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id=S2.T3.28.12.18.2><span class="ltx_text ltx_font_bold" id=S2.T3.28.12.18.2.1>53.00</span></td><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id=S2.T3.28.12.18.3><span class="ltx_text ltx_font_bold" id=S2.T3.28.12.18.3.1>48.85</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S2.T3.28.12.18.4><span class="ltx_text ltx_font_bold" id=S2.T3.28.12.18.4.1>48.00</span></td></tr></table></table></figure><blockquote><p>üîº This table presents results from experiments on different self-supervised learning methods used in the pre-training stage of the ENEL model. The goal was to determine how effectively these losses can help the language model learn high-level semantic representations of 3D point cloud data without an explicit encoder. The experiments involved variations of Masked Modeling Loss, Reconstruction Loss, Contrastive Loss, Knowledge Distillation Loss, and a novel Hybrid Semantic Loss. The results are shown in terms of the model&rsquo;s performance on classification and captioning tasks, using GPT-4 and other metrics for evaluation. The mask ratio (proportion of tokens masked) is varied (30% and 60%) to assess its impact on performance. The Hybrid Semantic Loss combines masked modeling and reconstruction losses, targeting both point tokens and patches.</p><details><summary>read the caption</summary>Table 3: LLM-embedded Semantic Encoding. In the pre-training stage, we explore the effects of various self-supervised learning losses targeting point tokens. Œ®Œ®\Psiroman_Œ® represents a mask ratio of 60%, while Œ¶Œ¶\Phiroman_Œ¶ represents a mask ratio of 30%. The subscript patch and feat represent the loss target. For Hybrid Semantic Loss, the subscript patch and feat represent the masked modeling target, while the reconstruction target is the corresponding feat and patch.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_align_middle" id=S2.T4.14.6><tr class=ltx_tr id=S2.T4.14.6.7><td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id=S2.T4.14.6.7.1 rowspan=2><span class="ltx_text ltx_font_bold" id=S2.T4.14.6.7.1.1>Method</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id=S2.T4.14.6.7.2><span class="ltx_text ltx_font_bold" id=S2.T4.14.6.7.2.1>Cls</span></td><td class="ltx_td ltx_align_center ltx_border_tt" colspan=2 id=S2.T4.14.6.7.3><span class="ltx_text ltx_font_bold" id=S2.T4.14.6.7.3.1>Cap</span></td></tr><tr class=ltx_tr id=S2.T4.14.6.8><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S2.T4.14.6.8.1><span class="ltx_text ltx_font_bold" id=S2.T4.14.6.8.1.1>GPT-4</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S2.T4.14.6.8.2><span class="ltx_text ltx_font_bold" id=S2.T4.14.6.8.2.1>GPT-4</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S2.T4.14.6.8.3><span class="ltx_text ltx_font_bold" id=S2.T4.14.6.8.3.1>S-BERT</span></td></tr><tr class=ltx_tr id=S2.T4.14.6.9><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S2.T4.14.6.9.1>PointLLM-7B</td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S2.T4.14.6.9.2>53.00</td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S2.T4.14.6.9.3>44.85</td><td class="ltx_td ltx_align_center ltx_border_t" id=S2.T4.14.6.9.4>47.47</td></tr><tr class=ltx_tr id=S2.T4.9.1.1><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S2.T4.9.1.1.1><math alttext="l" class="ltx_Math" display="inline" id="S2.T4.9.1.1.1.m1.1"><semantics id="S2.T4.9.1.1.1.m1.1a"><mi id="S2.T4.9.1.1.1.m1.1.1" xref="S2.T4.9.1.1.1.m1.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S2.T4.9.1.1.1.m1.1b"><ci id="S2.T4.9.1.1.1.m1.1.1.cmml" xref="S2.T4.9.1.1.1.m1.1.1">ùëô</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T4.9.1.1.1.m1.1c">l</annotation><annotation encoding="application/x-llamapun" id="S2.T4.9.1.1.1.m1.1d">italic_l</annotation></semantics></math>=1</td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S2.T4.9.1.1.2>52.50</td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S2.T4.9.1.1.3>48.86</td><td class="ltx_td ltx_align_center ltx_border_t" id=S2.T4.9.1.1.4>48.14</td></tr><tr class=ltx_tr id=S2.T4.10.2.2><td class="ltx_td ltx_align_center ltx_border_r" id=S2.T4.10.2.2.1><math alttext="l" class="ltx_Math" display="inline" id="S2.T4.10.2.2.1.m1.1"><semantics id="S2.T4.10.2.2.1.m1.1a"><mi id="S2.T4.10.2.2.1.m1.1.1" xref="S2.T4.10.2.2.1.m1.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S2.T4.10.2.2.1.m1.1b"><ci id="S2.T4.10.2.2.1.m1.1.1.cmml" xref="S2.T4.10.2.2.1.m1.1.1">ùëô</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T4.10.2.2.1.m1.1c">l</annotation><annotation encoding="application/x-llamapun" id="S2.T4.10.2.2.1.m1.1d">italic_l</annotation></semantics></math>=2</td><td class="ltx_td ltx_align_center ltx_border_r" id=S2.T4.10.2.2.2>50.00</td><td class="ltx_td ltx_align_center ltx_border_r" id=S2.T4.10.2.2.3>46.76</td><td class="ltx_td ltx_align_center" id=S2.T4.10.2.2.4>47.95</td></tr><tr class=ltx_tr id=S2.T4.11.3.3><td class="ltx_td ltx_align_center ltx_border_r" id=S2.T4.11.3.3.1><math alttext="l" class="ltx_Math" display="inline" id="S2.T4.11.3.3.1.m1.1"><semantics id="S2.T4.11.3.3.1.m1.1a"><mi id="S2.T4.11.3.3.1.m1.1.1" xref="S2.T4.11.3.3.1.m1.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S2.T4.11.3.3.1.m1.1b"><ci id="S2.T4.11.3.3.1.m1.1.1.cmml" xref="S2.T4.11.3.3.1.m1.1.1">ùëô</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T4.11.3.3.1.m1.1c">l</annotation><annotation encoding="application/x-llamapun" id="S2.T4.11.3.3.1.m1.1d">italic_l</annotation></semantics></math>=3</td><td class="ltx_td ltx_align_center ltx_border_r" id=S2.T4.11.3.3.2>48.00</td><td class="ltx_td ltx_align_center ltx_border_r" id=S2.T4.11.3.3.3>45.51</td><td class="ltx_td ltx_align_center" id=S2.T4.11.3.3.4>46.85</td></tr><tr class=ltx_tr id=S2.T4.12.4.4><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S2.T4.12.4.4.1><math alttext="H" class="ltx_Math" display="inline" id="S2.T4.12.4.4.1.m1.1"><semantics id="S2.T4.12.4.4.1.m1.1a"><mi id="S2.T4.12.4.4.1.m1.1.1" xref="S2.T4.12.4.4.1.m1.1.1.cmml">H</mi><annotation-xml encoding="MathML-Content" id="S2.T4.12.4.4.1.m1.1b"><ci id="S2.T4.12.4.4.1.m1.1.1.cmml" xref="S2.T4.12.4.4.1.m1.1.1">ùêª</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T4.12.4.4.1.m1.1c">H</annotation><annotation encoding="application/x-llamapun" id="S2.T4.12.4.4.1.m1.1d">italic_H</annotation></semantics></math>=2</td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S2.T4.12.4.4.2>53.50</td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S2.T4.12.4.4.3>49.13</td><td class="ltx_td ltx_align_center ltx_border_t" id=S2.T4.12.4.4.4>48.33</td></tr><tr class=ltx_tr id=S2.T4.13.5.5><td class="ltx_td ltx_align_center ltx_border_r" id=S2.T4.13.5.5.1><math alttext="H" class="ltx_Math" display="inline" id="S2.T4.13.5.5.1.m1.1"><semantics id="S2.T4.13.5.5.1.m1.1a"><mi id="S2.T4.13.5.5.1.m1.1.1" xref="S2.T4.13.5.5.1.m1.1.1.cmml">H</mi><annotation-xml encoding="MathML-Content" id="S2.T4.13.5.5.1.m1.1b"><ci id="S2.T4.13.5.5.1.m1.1.1.cmml" xref="S2.T4.13.5.5.1.m1.1.1">ùêª</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T4.13.5.5.1.m1.1c">H</annotation><annotation encoding="application/x-llamapun" id="S2.T4.13.5.5.1.m1.1d">italic_H</annotation></semantics></math>=4</td><td class="ltx_td ltx_align_center ltx_border_r" id=S2.T4.13.5.5.2>52.50</td><td class="ltx_td ltx_align_center ltx_border_r" id=S2.T4.13.5.5.3>48.39</td><td class="ltx_td ltx_align_center" id=S2.T4.13.5.5.4>47.75</td></tr><tr class=ltx_tr id=S2.T4.14.6.6><td class="ltx_td ltx_align_center ltx_border_r" id=S2.T4.14.6.6.1><math alttext="H" class="ltx_Math" display="inline" id="S2.T4.14.6.6.1.m1.1"><semantics id="S2.T4.14.6.6.1.m1.1a"><mi id="S2.T4.14.6.6.1.m1.1.1" xref="S2.T4.14.6.6.1.m1.1.1.cmml">H</mi><annotation-xml encoding="MathML-Content" id="S2.T4.14.6.6.1.m1.1b"><ci id="S2.T4.14.6.6.1.m1.1.1.cmml" xref="S2.T4.14.6.6.1.m1.1.1">ùêª</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T4.14.6.6.1.m1.1c">H</annotation><annotation encoding="application/x-llamapun" id="S2.T4.14.6.6.1.m1.1d">italic_H</annotation></semantics></math>=8</td><td class="ltx_td ltx_align_center ltx_border_r" id=S2.T4.14.6.6.2>51.00</td><td class="ltx_td ltx_align_center ltx_border_r" id=S2.T4.14.6.6.3>48.95</td><td class="ltx_td ltx_align_center" id=S2.T4.14.6.6.4>47.97</td></tr><tr class=ltx_tr id=S2.T4.14.6.10><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id=S2.T4.14.6.10.1><span class="ltx_text ltx_font_bold" id=S2.T4.14.6.10.1.1>+ Self-Attn.</span></td><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id=S2.T4.14.6.10.2><span class="ltx_text ltx_font_bold" id=S2.T4.14.6.10.2.1>55.00</span></td><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id=S2.T4.14.6.10.3><span class="ltx_text ltx_font_bold" id=S2.T4.14.6.10.3.1>50.92</span></td><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id=S2.T4.14.6.10.4><span class="ltx_text ltx_font_bold" id=S2.T4.14.6.10.4.1>48.61</span></td></tr></table></table></figure><blockquote><p>üîº This table presents ablation study results on the Hierarchical Geometry Aggregation strategy used in the instruction tuning stage of the ENEL model. It shows the impact of varying the number of aggregation and propagation operations (<em>l</em>), the number of LLM layers between these operations (<em>H</em>), and the inclusion of gated self-attention on the model&rsquo;s performance. The results are evaluated using GPT-4 scores for classification and captioning tasks, along with Sentence-BERT and SimCSE metrics. This allows for a comprehensive analysis of how different architectural choices in the Hierarchical Geometry Aggregation affect the model&rsquo;s ability to understand 3D geometric structures.</p><details><summary>read the caption</summary>Table 4: Hierarchical Geometry Aggregation. In the instruction tuning stage, we conduct the experiments of Hierarchical Geometry Aggregation strategy. lùëôlitalic_l represents the number of aggregation and propagation operations. HùêªHitalic_H refers to the LLM layers between lùëôlitalic_l aggregation and lùëôlitalic_l propagation operations. + Self-Attn. represents the incorporation of the gated self-attention in the aggregation.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_align_middle" id=S2.T5.3.1><tr class=ltx_tr id=S2.T5.3.1.1><td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id=S2.T5.3.1.1.1 rowspan=2 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_text ltx_font_bold" id=S2.T5.3.1.1.1.1>Model</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan=6 id=S2.T5.3.1.1.2 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_text ltx_font_bold" id=S2.T5.3.1.1.2.1>Cap</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id=S2.T5.3.1.1.3 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_text ltx_font_bold" id=S2.T5.3.1.1.3.1>Cls</span></td><td class="ltx_td ltx_align_center ltx_border_tt" id=S2.T5.3.1.1.4 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_text ltx_font_bold" id=S2.T5.3.1.1.4.1>QA</span></td></tr><tr class=ltx_tr id=S2.T5.3.1.2><td class="ltx_td ltx_align_center ltx_border_t" id=S2.T5.3.1.2.1 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_text ltx_font_bold" id=S2.T5.3.1.2.1.1>GPT-4</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S2.T5.3.1.2.2 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_text ltx_font_bold" id=S2.T5.3.1.2.2.1>Sentence-BERT</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S2.T5.3.1.2.3 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_text ltx_font_bold" id=S2.T5.3.1.2.3.1>SimCSE</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S2.T5.3.1.2.4 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_text ltx_font_bold" id=S2.T5.3.1.2.4.1>BLEU-1</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S2.T5.3.1.2.5 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_text ltx_font_bold" id=S2.T5.3.1.2.5.1>ROUGE-L</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S2.T5.3.1.2.6 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_text ltx_font_bold" id=S2.T5.3.1.2.6.1>METEOR</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S2.T5.3.1.2.7 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_text ltx_font_bold" id=S2.T5.3.1.2.7.1>GPT-4</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S2.T5.3.1.2.8 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_text ltx_font_bold" id=S2.T5.3.1.2.8.1>GPT-4</span></td></tr><tr class=ltx_tr id=S2.T5.3.1.3><td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id=S2.T5.3.1.3.1 style=padding-top:1pt;padding-bottom:1pt>InstructBLIP-7B¬†<cite class="ltx_cite ltx_citemacro_citep">(Dai et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2502.09620v1#bib.bib9 title>2023</a>)</cite></td><td class="ltx_td ltx_align_center ltx_border_t" id=S2.T5.3.1.3.2 style=padding-top:1pt;padding-bottom:1pt>45.34</td><td class="ltx_td ltx_align_center ltx_border_t" id=S2.T5.3.1.3.3 style=padding-top:1pt;padding-bottom:1pt>47.41</td><td class="ltx_td ltx_align_center ltx_border_t" id=S2.T5.3.1.3.4 style=padding-top:1pt;padding-bottom:1pt>48.48</td><td class="ltx_td ltx_align_center ltx_border_t" id=S2.T5.3.1.3.5 style=padding-top:1pt;padding-bottom:1pt>4.27</td><td class="ltx_td ltx_align_center ltx_border_t" id=S2.T5.3.1.3.6 style=padding-top:1pt;padding-bottom:1pt>8.28</td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S2.T5.3.1.3.7 style=padding-top:1pt;padding-bottom:1pt>12.99</td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S2.T5.3.1.3.8 style=padding-top:1pt;padding-bottom:1pt>43.50</td><td class="ltx_td ltx_align_center ltx_border_t" id=S2.T5.3.1.3.9 style=padding-top:1pt;padding-bottom:1pt>‚Äì</td></tr><tr class=ltx_tr id=S2.T5.3.1.4><td class="ltx_td ltx_align_left ltx_border_r" id=S2.T5.3.1.4.1 style=padding-top:1pt;padding-bottom:1pt>InstructBLIP-13B¬†<cite class="ltx_cite ltx_citemacro_citep">(Dai et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2502.09620v1#bib.bib9 title>2023</a>)</cite></td><td class="ltx_td ltx_align_center" id=S2.T5.3.1.4.2 style=padding-top:1pt;padding-bottom:1pt>44.97</td><td class="ltx_td ltx_align_center" id=S2.T5.3.1.4.3 style=padding-top:1pt;padding-bottom:1pt>45.90</td><td class="ltx_td ltx_align_center" id=S2.T5.3.1.4.4 style=padding-top:1pt;padding-bottom:1pt>48.86</td><td class="ltx_td ltx_align_center" id=S2.T5.3.1.4.5 style=padding-top:1pt;padding-bottom:1pt>4.65</td><td class="ltx_td ltx_align_center" id=S2.T5.3.1.4.6 style=padding-top:1pt;padding-bottom:1pt>8.85</td><td class="ltx_td ltx_align_center ltx_border_r" id=S2.T5.3.1.4.7 style=padding-top:1pt;padding-bottom:1pt>13.23</td><td class="ltx_td ltx_align_center ltx_border_r" id=S2.T5.3.1.4.8 style=padding-top:1pt;padding-bottom:1pt>34.25</td><td class="ltx_td ltx_align_center" id=S2.T5.3.1.4.9 style=padding-top:1pt;padding-bottom:1pt>‚Äì</td></tr><tr class=ltx_tr id=S2.T5.3.1.5><td class="ltx_td ltx_align_left ltx_border_r" id=S2.T5.3.1.5.1 style=padding-top:1pt;padding-bottom:1pt>LLaVA-7B¬†<cite class="ltx_cite ltx_citemacro_citep">(Liu et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2502.09620v1#bib.bib27 title>2024</a>)</cite></td><td class="ltx_td ltx_align_center" id=S2.T5.3.1.5.2 style=padding-top:1pt;padding-bottom:1pt>46.71</td><td class="ltx_td ltx_align_center" id=S2.T5.3.1.5.3 style=padding-top:1pt;padding-bottom:1pt>45.61</td><td class="ltx_td ltx_align_center" id=S2.T5.3.1.5.4 style=padding-top:1pt;padding-bottom:1pt>47.10</td><td class="ltx_td ltx_align_center" id=S2.T5.3.1.5.5 style=padding-top:1pt;padding-bottom:1pt>3.64</td><td class="ltx_td ltx_align_center" id=S2.T5.3.1.5.6 style=padding-top:1pt;padding-bottom:1pt>7.70</td><td class="ltx_td ltx_align_center ltx_border_r" id=S2.T5.3.1.5.7 style=padding-top:1pt;padding-bottom:1pt>12.14</td><td class="ltx_td ltx_align_center ltx_border_r" id=S2.T5.3.1.5.8 style=padding-top:1pt;padding-bottom:1pt>50.00</td><td class="ltx_td ltx_align_center" id=S2.T5.3.1.5.9 style=padding-top:1pt;padding-bottom:1pt>‚Äì</td></tr><tr class=ltx_tr id=S2.T5.3.1.6><td class="ltx_td ltx_align_left ltx_border_r" id=S2.T5.3.1.6.1 style=padding-top:1pt;padding-bottom:1pt>LLaVA-13B¬†<cite class="ltx_cite ltx_citemacro_citep">(Liu et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2502.09620v1#bib.bib27 title>2024</a>)</cite></td><td class="ltx_td ltx_align_center" id=S2.T5.3.1.6.2 style=padding-top:1pt;padding-bottom:1pt>38.28</td><td class="ltx_td ltx_align_center" id=S2.T5.3.1.6.3 style=padding-top:1pt;padding-bottom:1pt>46.37</td><td class="ltx_td ltx_align_center" id=S2.T5.3.1.6.4 style=padding-top:1pt;padding-bottom:1pt>45.90</td><td class="ltx_td ltx_align_center" id=S2.T5.3.1.6.5 style=padding-top:1pt;padding-bottom:1pt>4.02</td><td class="ltx_td ltx_align_center" id=S2.T5.3.1.6.6 style=padding-top:1pt;padding-bottom:1pt>8.15</td><td class="ltx_td ltx_align_center ltx_border_r" id=S2.T5.3.1.6.7 style=padding-top:1pt;padding-bottom:1pt>12.58</td><td class="ltx_td ltx_align_center ltx_border_r" id=S2.T5.3.1.6.8 style=padding-top:1pt;padding-bottom:1pt>51.75</td><td class="ltx_td ltx_align_center" id=S2.T5.3.1.6.9 style=padding-top:1pt;padding-bottom:1pt>47.90</td></tr><tr class=ltx_tr id=S2.T5.3.1.7><td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id=S2.T5.3.1.7.1 style=padding-top:1pt;padding-bottom:1pt>3D-LLM¬†<cite class="ltx_cite ltx_citemacro_citep">(Hong et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2502.09620v1#bib.bib19 title>2023</a>)</cite></td><td class="ltx_td ltx_align_center ltx_border_t" id=S2.T5.3.1.7.2 style=padding-top:1pt;padding-bottom:1pt>33.42</td><td class="ltx_td ltx_align_center ltx_border_t" id=S2.T5.3.1.7.3 style=padding-top:1pt;padding-bottom:1pt>44.48</td><td class="ltx_td ltx_align_center ltx_border_t" id=S2.T5.3.1.7.4 style=padding-top:1pt;padding-bottom:1pt>43.68</td><td class="ltx_td ltx_align_center ltx_border_t" id=S2.T5.3.1.7.5 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_text ltx_font_bold" id=S2.T5.3.1.7.5.1>16.91</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S2.T5.3.1.7.6 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_text ltx_font_bold" id=S2.T5.3.1.7.6.1>19.48</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S2.T5.3.1.7.7 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_text ltx_font_bold" id=S2.T5.3.1.7.7.1>19.73</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S2.T5.3.1.7.8 style=padding-top:1pt;padding-bottom:1pt>45.25</td><td class="ltx_td ltx_align_center ltx_border_t" id=S2.T5.3.1.7.9 style=padding-top:1pt;padding-bottom:1pt>‚Äì</td></tr><tr class=ltx_tr id=S2.T5.3.1.8><td class="ltx_td ltx_align_left ltx_border_r" id=S2.T5.3.1.8.1 style=padding-top:1pt;padding-bottom:1pt>PointLLM-7B¬†<cite class="ltx_cite ltx_citemacro_citep">(Xu et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2502.09620v1#bib.bib42 title>2023</a>)</cite></td><td class="ltx_td ltx_align_center" id=S2.T5.3.1.8.2 style=padding-top:1pt;padding-bottom:1pt>44.85</td><td class="ltx_td ltx_align_center" id=S2.T5.3.1.8.3 style=padding-top:1pt;padding-bottom:1pt>47.47</td><td class="ltx_td ltx_align_center" id=S2.T5.3.1.8.4 style=padding-top:1pt;padding-bottom:1pt>48.55</td><td class="ltx_td ltx_align_center" id=S2.T5.3.1.8.5 style=padding-top:1pt;padding-bottom:1pt>3.87</td><td class="ltx_td ltx_align_center" id=S2.T5.3.1.8.6 style=padding-top:1pt;padding-bottom:1pt>7.30</td><td class="ltx_td ltx_align_center ltx_border_r" id=S2.T5.3.1.8.7 style=padding-top:1pt;padding-bottom:1pt>11.92</td><td class="ltx_td ltx_align_center ltx_border_r" id=S2.T5.3.1.8.8 style=padding-top:1pt;padding-bottom:1pt>53.00</td><td class="ltx_td ltx_align_center" id=S2.T5.3.1.8.9 style=padding-top:1pt;padding-bottom:1pt>41.20</td></tr><tr class=ltx_tr id=S2.T5.3.1.9><td class="ltx_td ltx_align_left ltx_border_r" id=S2.T5.3.1.9.1 style=padding-top:1pt;padding-bottom:1pt>PointLLM-13B¬†<cite class="ltx_cite ltx_citemacro_citep">(Xu et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2502.09620v1#bib.bib42 title>2023</a>)</cite></td><td class="ltx_td ltx_align_center" id=S2.T5.3.1.9.2 style=padding-top:1pt;padding-bottom:1pt>48.15</td><td class="ltx_td ltx_align_center" id=S2.T5.3.1.9.3 style=padding-top:1pt;padding-bottom:1pt>47.91</td><td class="ltx_td ltx_align_center" id=S2.T5.3.1.9.4 style=padding-top:1pt;padding-bottom:1pt>49.12</td><td class="ltx_td ltx_align_center" id=S2.T5.3.1.9.5 style=padding-top:1pt;padding-bottom:1pt>3.83</td><td class="ltx_td ltx_align_center" id=S2.T5.3.1.9.6 style=padding-top:1pt;padding-bottom:1pt>7.23</td><td class="ltx_td ltx_align_center ltx_border_r" id=S2.T5.3.1.9.7 style=padding-top:1pt;padding-bottom:1pt>12.26</td><td class="ltx_td ltx_align_center ltx_border_r" id=S2.T5.3.1.9.8 style=padding-top:1pt;padding-bottom:1pt>54.00</td><td class="ltx_td ltx_align_center" id=S2.T5.3.1.9.9 style=padding-top:1pt;padding-bottom:1pt>46.60</td></tr><tr class=ltx_tr id=S2.T5.3.1.10><td class="ltx_td ltx_align_left ltx_border_r" id=S2.T5.3.1.10.1 style=padding-top:1pt;padding-bottom:1pt>ShapeLLM-7B¬†<cite class="ltx_cite ltx_citemacro_citep">(Qi et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2502.09620v1#bib.bib33 title>2024</a>)</cite></td><td class="ltx_td ltx_align_center" id=S2.T5.3.1.10.2 style=padding-top:1pt;padding-bottom:1pt>46.92</td><td class="ltx_td ltx_align_center" id=S2.T5.3.1.10.3 style=padding-top:1pt;padding-bottom:1pt>48.20</td><td class="ltx_td ltx_align_center" id=S2.T5.3.1.10.4 style=padding-top:1pt;padding-bottom:1pt>49.23</td><td class="ltx_td ltx_align_center" id=S2.T5.3.1.10.5 style=padding-top:1pt;padding-bottom:1pt>‚Äì</td><td class="ltx_td ltx_align_center" id=S2.T5.3.1.10.6 style=padding-top:1pt;padding-bottom:1pt>‚Äì</td><td class="ltx_td ltx_align_center ltx_border_r" id=S2.T5.3.1.10.7 style=padding-top:1pt;padding-bottom:1pt>‚Äì</td><td class="ltx_td ltx_align_center ltx_border_r" id=S2.T5.3.1.10.8 style=padding-top:1pt;padding-bottom:1pt>54.50</td><td class="ltx_td ltx_align_center" id=S2.T5.3.1.10.9 style=padding-top:1pt;padding-bottom:1pt>47.40</td></tr><tr class=ltx_tr id=S2.T5.3.1.11><td class="ltx_td ltx_align_left ltx_border_r" id=S2.T5.3.1.11.1 style=padding-top:1pt;padding-bottom:1pt>ShapeLLM-13B¬†<cite class="ltx_cite ltx_citemacro_citep">(Qi et¬†al., <a class=ltx_ref href=https://arxiv.org/html/2502.09620v1#bib.bib33 title>2024</a>)</cite></td><td class="ltx_td ltx_align_center" id=S2.T5.3.1.11.2 style=padding-top:1pt;padding-bottom:1pt>48.94</td><td class="ltx_td ltx_align_center" id=S2.T5.3.1.11.3 style=padding-top:1pt;padding-bottom:1pt>48.52</td><td class="ltx_td ltx_align_center" id=S2.T5.3.1.11.4 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_text ltx_font_bold" id=S2.T5.3.1.11.4.1>49.98</span></td><td class="ltx_td ltx_align_center" id=S2.T5.3.1.11.5 style=padding-top:1pt;padding-bottom:1pt>‚Äì</td><td class="ltx_td ltx_align_center" id=S2.T5.3.1.11.6 style=padding-top:1pt;padding-bottom:1pt>‚Äì</td><td class="ltx_td ltx_align_center ltx_border_r" id=S2.T5.3.1.11.7 style=padding-top:1pt;padding-bottom:1pt>‚Äì</td><td class="ltx_td ltx_align_center ltx_border_r" id=S2.T5.3.1.11.8 style=padding-top:1pt;padding-bottom:1pt>54.00</td><td class="ltx_td ltx_align_center" id=S2.T5.3.1.11.9 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_text ltx_font_bold" id=S2.T5.3.1.11.9.1>53.10</span></td></tr><tr class=ltx_tr id=S2.T5.3.1.12><td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id=S2.T5.3.1.12.1 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_text ltx_font_bold ltx_font_smallcaps" id=S2.T5.3.1.12.1.1>Enel<span class="ltx_text ltx_font_upright" id=S2.T5.3.1.12.1.1.1>-7B</span></span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S2.T5.3.1.12.2 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_text ltx_font_bold" id=S2.T5.3.1.12.2.1>50.92</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S2.T5.3.1.12.3 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_text ltx_font_bold" id=S2.T5.3.1.12.3.1>48.61</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S2.T5.3.1.12.4 style=padding-top:1pt;padding-bottom:1pt>49.31</td><td class="ltx_td ltx_align_center ltx_border_bb" id=S2.T5.3.1.12.5 style=padding-top:1pt;padding-bottom:1pt>3.88</td><td class="ltx_td ltx_align_center ltx_border_bb" id=S2.T5.3.1.12.6 style=padding-top:1pt;padding-bottom:1pt>7.20</td><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id=S2.T5.3.1.12.7 style=padding-top:1pt;padding-bottom:1pt>12.50</td><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id=S2.T5.3.1.12.8 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_text ltx_font_bold" id=S2.T5.3.1.12.8.1>55.00</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S2.T5.3.1.12.9 style=padding-top:1pt;padding-bottom:1pt>42.70</td></tr></table></table></figure><blockquote><p>üîº Table 5 presents a comparative analysis of various models&rsquo; performance on diverse 3D understanding tasks. The primary evaluation metric is GPT-4 scores, providing a comprehensive assessment of the models&rsquo; ability to understand and generate human-quality text descriptions and answers related to 3D data. To complement the GPT-4 scores and offer a more nuanced perspective, the table also includes data-driven metrics such as Sentence-BERT and SimCSE scores, providing additional quantitative insights into the models&rsquo; performance.</p><details><summary>read the caption</summary>Table 5: Comparison of different models on various 3D understanding tasks. A primary focus is placed on GPT-4 evaluation, along with data-driven metrics (Sentence-BERT and SimCSE).</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_align_middle" id=A2.T6.28.6><tr class=ltx_tr id=A2.T6.28.6.7><td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id=A2.T6.28.6.7.1 rowspan=2 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_text ltx_font_bold" id=A2.T6.28.6.7.1.1>Model</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan=6 id=A2.T6.28.6.7.2 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_text ltx_font_bold" id=A2.T6.28.6.7.2.1>Cap</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id=A2.T6.28.6.7.3 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_text ltx_font_bold" id=A2.T6.28.6.7.3.1>Cls</span></td></tr><tr class=ltx_tr id=A2.T6.28.6.8><td class="ltx_td ltx_align_center ltx_border_t" id=A2.T6.28.6.8.1 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_text ltx_font_bold" id=A2.T6.28.6.8.1.1>GPT-4</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=A2.T6.28.6.8.2 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_text ltx_font_bold" id=A2.T6.28.6.8.2.1>Sentence-BERT</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=A2.T6.28.6.8.3 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_text ltx_font_bold" id=A2.T6.28.6.8.3.1>SimCSE</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=A2.T6.28.6.8.4 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_text ltx_font_bold" id=A2.T6.28.6.8.4.1>BLEU-1</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=A2.T6.28.6.8.5 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_text ltx_font_bold" id=A2.T6.28.6.8.5.1>ROUGE-L</span></td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=A2.T6.28.6.8.6 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_text ltx_font_bold" id=A2.T6.28.6.8.6.1>METEOR</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=A2.T6.28.6.8.7 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_text ltx_font_bold" id=A2.T6.28.6.8.7.1>GPT-4</span></td></tr><tr class=ltx_tr id=A2.T6.28.6.9><td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id=A2.T6.28.6.9.1 style=padding-top:1pt;padding-bottom:1pt><span class="ltx_text ltx_font_bold ltx_font_smallcaps" id=A2.T6.28.6.9.1.1>Enel<span class="ltx_text ltx_font_upright" id=A2.T6.28.6.9.1.1.1>-7B</span></span></td><td class="ltx_td ltx_align_center ltx_border_t" id=A2.T6.28.6.9.2 style=padding-top:1pt;padding-bottom:1pt>50.92</td><td class="ltx_td ltx_align_center ltx_border_t" id=A2.T6.28.6.9.3 style=padding-top:1pt;padding-bottom:1pt>48.61</td><td class="ltx_td ltx_align_center ltx_border_t" id=A2.T6.28.6.9.4 style=padding-top:1pt;padding-bottom:1pt>49.31</td><td class="ltx_td ltx_align_center ltx_border_t" id=A2.T6.28.6.9.5 style=padding-top:1pt;padding-bottom:1pt>3.88</td><td class="ltx_td ltx_align_center ltx_border_t" id=A2.T6.28.6.9.6 style=padding-top:1pt;padding-bottom:1pt>7.20</td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=A2.T6.28.6.9.7 style=padding-top:1pt;padding-bottom:1pt>12.50</td><td class="ltx_td ltx_align_center ltx_border_t" id=A2.T6.28.6.9.8 style=padding-top:1pt;padding-bottom:1pt>55.00</td></tr><tr class=ltx_tr id=A2.T6.28.6.10><td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id=A2.T6.28.6.10.1 style=padding-top:1pt;padding-bottom:1pt>‚Äì Hybrid Semantic Loss</td><td class="ltx_td ltx_align_center ltx_border_t" id=A2.T6.28.6.10.2 style=padding-top:1pt;padding-bottom:1pt>47.19</td><td class="ltx_td ltx_align_center ltx_border_t" id=A2.T6.28.6.10.3 style=padding-top:1pt;padding-bottom:1pt>48.07</td><td class="ltx_td ltx_align_center ltx_border_t" id=A2.T6.28.6.10.4 style=padding-top:1pt;padding-bottom:1pt>48.31</td><td class="ltx_td ltx_align_center ltx_border_t" id=A2.T6.28.6.10.5 style=padding-top:1pt;padding-bottom:1pt>3.46</td><td class="ltx_td ltx_align_center ltx_border_t" id=A2.T6.28.6.10.6 style=padding-top:1pt;padding-bottom:1pt>7.41</td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=A2.T6.28.6.10.7 style=padding-top:1pt;padding-bottom:1pt>11.84</td><td class="ltx_td ltx_align_center ltx_border_t" id=A2.T6.28.6.10.8 style=padding-top:1pt;padding-bottom:1pt>50.61</td></tr><tr class=ltx_tr id=A2.T6.24.2.2><td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id=A2.T6.24.2.2.2 style=padding-top:1pt;padding-bottom:1pt>Hybrid Semantic Loss<sub class=ltx_sub id=A2.T6.24.2.2.2.1><span class="ltx_text ltx_font_italic" id=A2.T6.24.2.2.2.1.1>patch</span></sub><sup class=ltx_sup id=A2.T6.24.2.2.2.2><span class="ltx_text ltx_font_italic" id=A2.T6.24.2.2.2.2.1>Œ¶</span></sup></td><td class="ltx_td ltx_align_center ltx_border_t" id=A2.T6.24.2.2.3 style=padding-top:1pt;padding-bottom:1pt>49.05</td><td class="ltx_td ltx_align_center ltx_border_t" id=A2.T6.24.2.2.4 style=padding-top:1pt;padding-bottom:1pt>48.82</td><td class="ltx_td ltx_align_center ltx_border_t" id=A2.T6.24.2.2.5 style=padding-top:1pt;padding-bottom:1pt>49.20</td><td class="ltx_td ltx_align_center ltx_border_t" id=A2.T6.24.2.2.6 style=padding-top:1pt;padding-bottom:1pt>4.01</td><td class="ltx_td ltx_align_center ltx_border_t" id=A2.T6.24.2.2.7 style=padding-top:1pt;padding-bottom:1pt>7.25</td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=A2.T6.24.2.2.8 style=padding-top:1pt;padding-bottom:1pt>12.38</td><td class="ltx_td ltx_align_center ltx_border_t" id=A2.T6.24.2.2.9 style=padding-top:1pt;padding-bottom:1pt>52.20</td></tr><tr class=ltx_tr id=A2.T6.26.4.4><td class="ltx_td ltx_align_left ltx_border_r" id=A2.T6.26.4.4.2 style=padding-top:1pt;padding-bottom:1pt>Hybrid Semantic Loss<sub class=ltx_sub id=A2.T6.26.4.4.2.1><span class="ltx_text ltx_font_italic" id=A2.T6.26.4.4.2.1.1>patch</span></sub><sup class=ltx_sup id=A2.T6.26.4.4.2.2><span class="ltx_text ltx_font_italic" id=A2.T6.26.4.4.2.2.1>Œ®</span></sup></td><td class="ltx_td ltx_align_center" id=A2.T6.26.4.4.3 style=padding-top:1pt;padding-bottom:1pt>48.96</td><td class="ltx_td ltx_align_center" id=A2.T6.26.4.4.4 style=padding-top:1pt;padding-bottom:1pt>48.38</td><td class="ltx_td ltx_align_center" id=A2.T6.26.4.4.5 style=padding-top:1pt;padding-bottom:1pt>49.00</td><td class="ltx_td ltx_align_center" id=A2.T6.26.4.4.6 style=padding-top:1pt;padding-bottom:1pt>3.66</td><td class="ltx_td ltx_align_center" id=A2.T6.26.4.4.7 style=padding-top:1pt;padding-bottom:1pt>6.97</td><td class="ltx_td ltx_align_center ltx_border_r" id=A2.T6.26.4.4.8 style=padding-top:1pt;padding-bottom:1pt>11.98</td><td class="ltx_td ltx_align_center" id=A2.T6.26.4.4.9 style=padding-top:1pt;padding-bottom:1pt>52.00</td></tr><tr class=ltx_tr id=A2.T6.28.6.6><td class="ltx_td ltx_align_left ltx_border_r" id=A2.T6.28.6.6.2 style=padding-top:1pt;padding-bottom:1pt>Hybrid Semantic Loss<sub class=ltx_sub id=A2.T6.28.6.6.2.1><span class="ltx_text ltx_font_italic" id=A2.T6.28.6.6.2.1.1>feat</span></sub><sup class=ltx_sup id=A2.T6.28.6.6.2.2><span class="ltx_text ltx_font_italic" id=A2.T6.28.6.6.2.2.1>Œ®</span></sup></td><td class="ltx_td ltx_align_center" id=A2.T6.28.6.6.3 style=padding-top:1pt;padding-bottom:1pt>49.63</td><td class="ltx_td ltx_align_center" id=A2.T6.28.6.6.4 style=padding-top:1pt;padding-bottom:1pt>48.00</td><td class="ltx_td ltx_align_center" id=A2.T6.28.6.6.5 style=padding-top:1pt;padding-bottom:1pt>48.62</td><td class="ltx_td ltx_align_center" id=A2.T6.28.6.6.6 style=padding-top:1pt;padding-bottom:1pt>3.78</td><td class="ltx_td ltx_align_center" id=A2.T6.28.6.6.7 style=padding-top:1pt;padding-bottom:1pt>6.88</td><td class="ltx_td ltx_align_center ltx_border_r" id=A2.T6.28.6.6.8 style=padding-top:1pt;padding-bottom:1pt>12.33</td><td class="ltx_td ltx_align_center" id=A2.T6.28.6.6.9 style=padding-top:1pt;padding-bottom:1pt>51.50</td></tr><tr class=ltx_tr id=A2.T6.28.6.11><td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id=A2.T6.28.6.11.1 style=padding-top:1pt;padding-bottom:1pt>‚Äì gate mechanism</td><td class="ltx_td ltx_align_center ltx_border_t" id=A2.T6.28.6.11.2 style=padding-top:1pt;padding-bottom:1pt>49.26</td><td class="ltx_td ltx_align_center ltx_border_t" id=A2.T6.28.6.11.3 style=padding-top:1pt;padding-bottom:1pt>48.41</td><td class="ltx_td ltx_align_center ltx_border_t" id=A2.T6.28.6.11.4 style=padding-top:1pt;padding-bottom:1pt>48.93</td><td class="ltx_td ltx_align_center ltx_border_t" id=A2.T6.28.6.11.5 style=padding-top:1pt;padding-bottom:1pt>3.71</td><td class="ltx_td ltx_align_center ltx_border_t" id=A2.T6.28.6.11.6 style=padding-top:1pt;padding-bottom:1pt>7.12</td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=A2.T6.28.6.11.7 style=padding-top:1pt;padding-bottom:1pt>12.47</td><td class="ltx_td ltx_align_center ltx_border_t" id=A2.T6.28.6.11.8 style=padding-top:1pt;padding-bottom:1pt>53.50</td></tr><tr class=ltx_tr id=A2.T6.28.6.12><td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id=A2.T6.28.6.12.1 style=padding-top:1pt;padding-bottom:1pt>l=2,H=2,O=0</td><td class="ltx_td ltx_align_center ltx_border_t" id=A2.T6.28.6.12.2 style=padding-top:1pt;padding-bottom:1pt>48.81</td><td class="ltx_td ltx_align_center ltx_border_t" id=A2.T6.28.6.12.3 style=padding-top:1pt;padding-bottom:1pt>48.10</td><td class="ltx_td ltx_align_center ltx_border_t" id=A2.T6.28.6.12.4 style=padding-top:1pt;padding-bottom:1pt>48.57</td><td class="ltx_td ltx_align_center ltx_border_t" id=A2.T6.28.6.12.5 style=padding-top:1pt;padding-bottom:1pt>3.70</td><td class="ltx_td ltx_align_center ltx_border_t" id=A2.T6.28.6.12.6 style=padding-top:1pt;padding-bottom:1pt>6.99</td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=A2.T6.28.6.12.7 style=padding-top:1pt;padding-bottom:1pt>12.01</td><td class="ltx_td ltx_align_center ltx_border_t" id=A2.T6.28.6.12.8 style=padding-top:1pt;padding-bottom:1pt>51.50</td></tr><tr class=ltx_tr id=A2.T6.28.6.13><td class="ltx_td ltx_align_left ltx_border_r" id=A2.T6.28.6.13.1 style=padding-top:1pt;padding-bottom:1pt>l=2,H=4,O=0</td><td class="ltx_td ltx_align_center" id=A2.T6.28.6.13.2 style=padding-top:1pt;padding-bottom:1pt>49.02</td><td class="ltx_td ltx_align_center" id=A2.T6.28.6.13.3 style=padding-top:1pt;padding-bottom:1pt>48.47</td><td class="ltx_td ltx_align_center" id=A2.T6.28.6.13.4 style=padding-top:1pt;padding-bottom:1pt>48.61</td><td class="ltx_td ltx_align_center" id=A2.T6.28.6.13.5 style=padding-top:1pt;padding-bottom:1pt>3.65</td><td class="ltx_td ltx_align_center" id=A2.T6.28.6.13.6 style=padding-top:1pt;padding-bottom:1pt>7.10</td><td class="ltx_td ltx_align_center ltx_border_r" id=A2.T6.28.6.13.7 style=padding-top:1pt;padding-bottom:1pt>12.31</td><td class="ltx_td ltx_align_center" id=A2.T6.28.6.13.8 style=padding-top:1pt;padding-bottom:1pt>52.00</td></tr><tr class=ltx_tr id=A2.T6.28.6.14><td class="ltx_td ltx_align_left ltx_border_r" id=A2.T6.28.6.14.1 style=padding-top:1pt;padding-bottom:1pt>l=2,H=2,O=2</td><td class="ltx_td ltx_align_center" id=A2.T6.28.6.14.2 style=padding-top:1pt;padding-bottom:1pt>48.96</td><td class="ltx_td ltx_align_center" id=A2.T6.28.6.14.3 style=padding-top:1pt;padding-bottom:1pt>47.96</td><td class="ltx_td ltx_align_center" id=A2.T6.28.6.14.4 style=padding-top:1pt;padding-bottom:1pt>48.89</td><td class="ltx_td ltx_align_center" id=A2.T6.28.6.14.5 style=padding-top:1pt;padding-bottom:1pt>3.80</td><td class="ltx_td ltx_align_center" id=A2.T6.28.6.14.6 style=padding-top:1pt;padding-bottom:1pt>7.05</td><td class="ltx_td ltx_align_center ltx_border_r" id=A2.T6.28.6.14.7 style=padding-top:1pt;padding-bottom:1pt>12.55</td><td class="ltx_td ltx_align_center" id=A2.T6.28.6.14.8 style=padding-top:1pt;padding-bottom:1pt>52.00</td></tr><tr class=ltx_tr id=A2.T6.28.6.15><td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id=A2.T6.28.6.15.1 style=padding-top:1pt;padding-bottom:1pt>l=2,H=4,O=2</td><td class="ltx_td ltx_align_center ltx_border_bb" id=A2.T6.28.6.15.2 style=padding-top:1pt;padding-bottom:1pt>49.58</td><td class="ltx_td ltx_align_center ltx_border_bb" id=A2.T6.28.6.15.3 style=padding-top:1pt;padding-bottom:1pt>48.70</td><td class="ltx_td ltx_align_center ltx_border_bb" id=A2.T6.28.6.15.4 style=padding-top:1pt;padding-bottom:1pt>48.84</td><td class="ltx_td ltx_align_center ltx_border_bb" id=A2.T6.28.6.15.5 style=padding-top:1pt;padding-bottom:1pt>3.84</td><td class="ltx_td ltx_align_center ltx_border_bb" id=A2.T6.28.6.15.6 style=padding-top:1pt;padding-bottom:1pt>7.56</td><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id=A2.T6.28.6.15.7 style=padding-top:1pt;padding-bottom:1pt>12.76</td><td class="ltx_td ltx_align_center ltx_border_bb" id=A2.T6.28.6.15.8 style=padding-top:1pt;padding-bottom:1pt>53.00</td></tr></table></table></figure><blockquote><p>üîº This table presents the results of ablation experiments conducted on the ENEL model. The experiments systematically removed or altered components of the model to assess their individual contributions to the overall performance. Specifically, it investigates the impact of different mask ratios (30% and 60%) in the Hybrid Semantic Loss, the effect of using only the patch or feature reconstruction targets within the loss function, and the influence of varying the number of hierarchical geometry aggregation and propagation operations and the number of layers within the LLM between these operations. The performance metrics across different variants are evaluated using GPT-4 scores for classification and captioning tasks, along with sentence-BERT and SimCSE metrics.</p><details><summary>read the caption</summary>Table 6: Ablation Experiments. We begin the ablation experiments by changing the single configuration of the module from Enel. Œ®Œ®\Psiroman_Œ® represents a mask ratio of 60%, while Œ¶Œ¶\Phiroman_Œ¶ represents a mask ratio of 30%. For Hybrid Semantic Loss, the subscript p‚Å¢a‚Å¢t‚Å¢c‚Å¢hùëùùëéùë°ùëê‚Ñépatchitalic_p italic_a italic_t italic_c italic_h and f‚Å¢e‚Å¢a‚Å¢tùëìùëíùëéùë°featitalic_f italic_e italic_a italic_t represent the masked modeling target, while the reconstruction target is the corresponding f‚Å¢e‚Å¢a‚Å¢tùëìùëíùëéùë°featitalic_f italic_e italic_a italic_t and p‚Å¢a‚Å¢t‚Å¢c‚Å¢hùëùùëéùë°ùëê‚Ñépatchitalic_p italic_a italic_t italic_c italic_h. lùëôlitalic_l represents the number of aggregation and propagation operations. HùêªHitalic_H refers to the LLM layers between lùëôlitalic_l aggregation and lùëôlitalic_l propagation operations. OùëÇOitalic_O refers to the LLM layer between two individual aggregation or propagation operations.</details></blockquote></details><h3 class="relative group">Full paper<div id=full-paper class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#full-paper aria-label=Anchor>#</a></span></h3><div id=gallery-7b15693b6cb6aed5ded6ba87a8f6d87b class=gallery><img src=https://ai-paper-reviewer.com/2502.09620/1.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.09620/2.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.09620/3.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.09620/4.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.09620/5.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.09620/6.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.09620/7.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.09620/8.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.09620/9.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.09620/10.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.09620/11.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.09620/12.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.09620/13.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.09620/14.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2502.09620/15.png class="grid-w50 md:grid-w33 xl:grid-w25"></div></div><section class="flex flex-row flex-wrap justify-center pt-4 text-xl"><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.09620/&amp;title=Exploring%20the%20Potential%20of%20Encoder-free%20Architectures%20in%203D%20LMMs" title="Share on LinkedIn" aria-label="Share on LinkedIn"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://twitter.com/intent/tweet/?url=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.09620/&amp;text=Exploring%20the%20Potential%20of%20Encoder-free%20Architectures%20in%203D%20LMMs" title="Tweet on Twitter" aria-label="Tweet on Twitter"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="mailto:?body=https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.09620/&amp;subject=Exploring%20the%20Potential%20of%20Encoder-free%20Architectures%20in%203D%20LMMs" title="Send via email" aria-label="Send via email"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1-27.64 140.9 68.65 266.2 199.1 285.1 19.01 2.888 36.17-12.26 36.17-31.49l1e-4-.6631c0-15.74-11.44-28.88-26.84-31.24-84.35-12.98-149.2-86.13-149.2-174.2.0-102.9 88.61-185.5 193.4-175.4 91.54 8.869 158.6 91.25 158.6 183.2v16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98.0036c-7.299.0-13.2 4.992-15.12 11.68-24.85-12.15-54.24-16.38-86.06-5.106-38.75 13.73-68.12 48.91-73.72 89.64-9.483 69.01 43.81 128 110.9 128 26.44.0 50.43-9.544 69.59-24.88 24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3c0-149.2-133.9-265.632-287.3-235.57zM239.1 304.3c-26.47.0-48-21.56-48-48.05s21.53-48.05 48-48.05 48 21.56 48 48.05-20.6 48.05-48 48.05z"/></svg></span></a></section></div><script>var oid="views_paper-reviews/2502.09620/index.md",oid_likes="likes_paper-reviews/2502.09620/index.md"</script><script type=text/javascript src=/ai-paper-reviewer/js/page.min.0860cf4e04fa2d72cc33ddba263083464d48f67de06114529043cb4623319efed4f484fd7f1730df5abea0e2da6f3538855634081d02f2d6e920b956f063e823.js integrity="sha512-CGDPTgT6LXLMM926JjCDRk1I9n3gYRRSkEPLRiMxnv7U9IT9fxcw31q+oOLabzU4hVY0CB0C8tbpILlW8GPoIw=="></script></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="flex group mr-3" href=/ai-paper-reviewer/paper-reviews/2502.12170/><span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&larr;</span>
<span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">MUDDFormer: Breaking Residual Bottlenecks in Transformers via Multiway Dynamic Dense Connections</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2025-02-13T00:00:00+00:00>13 February 2025</time>
</span></span></a></span><span><a class="flex text-right group ml-3" href=/ai-paper-reviewer/paper-reviews/2502.09614/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">DexTrack: Towards Generalizable Neural Tracking Control for Dexterous Manipulation from Human References</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2025-02-13T00:00:00+00:00>13 February 2025</time>
</span></span><span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&larr;</span></a></span></div></div><div class=pt-3><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class=pt-3><script src=https://utteranc.es/client.js repo=pmnxis/pmnxis.github.io issue-term=pathname label=Comment theme=dark-blue crossorigin=anonymous async></script></div></div></footer></article><div id=top-scroller class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer id=site-footer class="py-10 print:hidden"><nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400"><ul class="flex flex-col list-none sm:flex-row"><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=/ai-paper-reviewer/tags/ title>Tags</a></li><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=https://deep-diver.github.io/neurips2024/ title>NeurIPS2024</a></li></ul></nav><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2025
Hugging Face Daily Papers</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a></p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/ai-paper-reviewer/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh+sCQ0E53ghYrxgYqw+0GCRyIEpA=="></script></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://deep-diver.github.io/ai-paper-reviewer/ style=z-index:500><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body><script data-name=BMC-Widget data-cfasync=false src=https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js data-id=chansung data-description="Support me on Buy me a coffee!" data-message data-color=#FFDD00 data-position=Left data-x_margin=18 data-y_margin=18></script></html>