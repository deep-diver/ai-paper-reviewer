[{"figure_path": "https://arxiv.org/html/2502.09620/extracted/6201996/intro3.png", "caption": "Figure 1: Issues of encoder-based 3D LMMs.\n(a) Point Cloud Resolution Limitation.\nDuring training, the point cloud size (P.T. size) and point token size (P.T. size) are fixed at 8192 and 512, respectively.\nAnd we adjust these two sizes during inference, point cloud size from 2K to 16K and the corresponding point token size from 128 to 2048.\nWe evaluate them on the captioning task of the Objaverse benchmark using GPT-4 scores as the evaluation metric.\n(b) Embedding Semantic Discrepancy.\nWe visualize the attention scores of the average text token to the point tokens, where red indicates higher values.\nThe point tokens in the encoder-free architecture exhibit stronger textual semantic relevance needed for the LLM.", "description": "Figure 1 demonstrates the limitations of encoder-based 3D Large Multimodal Models (LMMs).  Part (a) shows the impact of varying point cloud resolutions.  During training, a fixed resolution (8192 points, 512 tokens) is used, but during inference, the resolution changes (2K-16K points, 128-2048 tokens).  This mismatch causes a loss of information, significantly impacting performance as evaluated on the Objaverse captioning benchmark using GPT-4 scores. Part (b) illustrates the semantic mismatch between encoder embeddings and the LLM's needs. By visualizing attention weights (red indicates stronger attention), it shows that an encoder-free architecture produces point tokens with stronger semantic relevance for the language model.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2502.09620/x1.png", "caption": "Figure 2: Overall Pipeline of Enel.\nThe training is divided into two stages: the pre-training stage and the instruction tuning stage. In the first stage, we set the first K\ud835\udc3eKitalic_K layers to be learnable and apply the proposed Hybrid Semantic Loss to embed high-level semantics into the LLM. In the second stage, we adopt the Hierarchical Geometric Aggregation strategy to capture local structures of point clouds.", "description": "The figure illustrates the two-stage training pipeline of the ENEL model. Stage 1 (pre-training) focuses on embedding high-level 3D semantics into the LLM by making the first K layers learnable and utilizing the Hybrid Semantic Loss.  Stage 2 (instruction tuning) employs the Hierarchical Geometric Aggregation strategy to enable the LLM to effectively capture the local geometric structures within the point cloud data.", "section": "2. Investigation of Encoder-free 3D LMM"}, {"figure_path": "https://arxiv.org/html/2502.09620/x2.png", "caption": "Figure 3: Point Cloud Self-Supervised Learning Losses.\nIn the pre-training stage, we explore common self-supervised learning losses for the encoder-free 3D LMM: (a) Masked Modeling Loss, (b) Reconstruction Loss, (c) Contrastive Loss, and (d) Knowledge Distillation Loss.\nThe (e) represents our proposed Hybrid Semantic Loss, specifically designed for the encoder-free architecture.", "description": "Figure 3 illustrates various self-supervised learning methods applied during the pre-training phase of an encoder-free 3D Large Multimodal Model (LMM).  Subfigures (a) through (d) depict common approaches: Masked Modeling Loss, Reconstruction Loss, Contrastive Loss, and Knowledge Distillation Loss, respectively.  Each method aims to learn high-level 3D semantic information from point cloud data without relying on a traditional 3D encoder.  Subfigure (e) introduces a novel Hybrid Semantic Loss, specifically designed for this encoder-free architecture, combining aspects of the previous methods to achieve optimal performance.", "section": "2.2. LLM-embedded Semantic Encoding"}, {"figure_path": "https://arxiv.org/html/2502.09620/x3.png", "caption": "Figure 4: Hierarchical Geometry Aggregation Strategy.\nIn the instruction tuning stage, we apply aggregation and propagation operations to the point tokens to capture the local structural details.", "description": "This figure illustrates the Hierarchical Geometry Aggregation strategy used in the instruction tuning stage of the ENEL model.  The strategy aims to incorporate inductive bias into the LLM's early layers, allowing it to focus on local details within the point cloud data. This is achieved through a series of aggregation and propagation operations applied to the point tokens.  Aggregation combines information from neighboring points, effectively capturing local geometric structures. Propagation then spreads this aggregated information back to the original point tokens, ensuring that local details are integrated into the higher-level semantic understanding of the point cloud by the LLM.", "section": "2.3 Hierarchical Geometry Aggregation"}, {"figure_path": "https://arxiv.org/html/2502.09620/x4.png", "caption": "Figure 5: Difference in Semantic Encoding.\nBy visualizing the attention scores of the average text token to the point tokens on the Objaverse dataset, we compare the semantic encoding potential of encoder-based and encoder-free architectures, where red indicates higher values.\nAnd (a) represents chairs, (b) represents airplanes, and (c) represents lamps.", "description": "Figure 5 visualizes the attention weights between the average word embedding and point cloud embeddings for encoder-based (PointLLM) and encoder-free (ENEL) models.  The heatmaps show the attention scores, with redder colors indicating stronger attention. The figure demonstrates how the encoder-free model attends more directly to semantically relevant parts of the 3D object, whereas the encoder-based model's attention is more diffuse.  Three object categories are shown: chairs (a), airplanes (b), and lamps (c), illustrating this difference in attention across various object types.  The results support the claim that the encoder-free architecture achieves better semantic encoding.", "section": "3. Results and Visualization"}, {"figure_path": "https://arxiv.org/html/2502.09620/extracted/6201996/loss2.png", "caption": "Figure 6: Variants of Point Cloud Self-Supervised Learning Losses.\n(a) The Variant of Masked Modeling Loss, (b) The Variant of Reconstruction Loss, (c) The Variant of Hybrid Semantic Loss.", "description": "Figure 6 illustrates three variations of self-supervised learning strategies used for point cloud data in the context of encoder-free 3D large multimodal models (LMMs).  Each subfigure shows a different loss function applied during pre-training to embed high-level semantics into the language model without explicit 3D encoders:\n\n(a) Masked Modeling Loss:  A portion of the point tokens are masked, and the model attempts to predict their values.\n(b) Reconstruction Loss: The model reconstructs the original point cloud from a learned representation.\n(c) Hybrid Semantic Loss: Combines Masked Modeling and Reconstruction Losses, aiming to capture both high-level semantic information and fine-grained geometric details.", "section": "2.2. LLM-embedded Semantic Encoding"}, {"figure_path": "https://arxiv.org/html/2502.09620/extracted/6201996/output3.png", "caption": "Figure 7: Enel Output Examples.\nWe demonstrate that Enel provides precise and diverse responses when addressing different problems.", "description": "Figure 7 showcases several examples of ENEL's responses to various prompts involving 3D models.  The prompts range from detailed descriptions to more specific questions requiring object recognition and property analysis.  The examples demonstrate ENEL's ability to produce both precise and varied answers, highlighting its capability for nuanced understanding and generation within the context of 3D multimodal data.", "section": "3. Results and Visualization"}]