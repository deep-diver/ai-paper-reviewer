[{"content": "| MODEL | SETTING | MATHEMATICS | ARITHMETIC | ARITHMETIC | COMMONSENSE | AVERAGE |\n|---|---|---|---|---|---|---|\n|  |  | MATH | GSM8K | SVAMP | StrategyQA |  |\n| Qwen2.5-14B-instruct | Zero-shot CoT | 69.8 | 92.4 | 91.6 | 62.8 | 79.1 |\n|  | Few-shot CoT | 80.0 | 94.8 | 91.3 | 53.1 | 79.8 |\n|  | CoT+SC@4 | 76.2 | 94.0 | 91.0 | 69.7 | 82.7 |\n|  | Ours | **80.2** | **95.3** | **93.7** | **77.3** | **86.6** |\n| Qwen2.5-7B-instruct | Zero-shot CoT | 64.8 | 86.2 | 91.3 | 52.8 | 73.7 |\n|  | Few-shot CoT | 75.5 | 91.6 | 92.3 | 67.6 | 81.7 |\n|  | CoT+SC@4 | 76.4 | 92.0 | 92.3 | 73.2 | 83.4 |\n|  | Ours | **79.6** | **92.8** | **93.0** | **76.0** | **85.4** |\n| Qwen2-7B-instruct | Zero-shot CoT | 36.9 | 76.6 | 85.2 | 55.3 | 63.5 |\n|  | Few-shot CoT | 52.9 | 85.7 | 87.3 | 62.3 | 72.0 |\n|  | CoT+SC@4 | 55.6 | 87.7 | 90.3 | 65.5 | 74.8 |\n|  | Ours | **63.8** | **90.6** | **92.7** | **72.0** | **79.8** |\n| Yi-1.5-6B-Chat | Zero-shot CoT | 30.4 | 76.4 | 64.4 | 46.2 | 54.3 |\n|  | Few-shot CoT | 40.5 | 78.9 | 81.3 | 61.1 | 65.4 |\n|  | CoT+SC@4 | 42.2 | 79.4 | 87.6 | 65.2 | 68.6 |\n|  | Ours | **54.0** | **81.4** | **90.0** | **70.3** | **74.0** |\n| Llama-3-8B-Instruct | Zero-shot CoT | 5.8 | 68.3 | 70.9 | 57.2 | 50.5 |\n|  | Few-shot CoT | 17.8 | 74.5 | 81.0 | 68.4 | 60.4 |\n|  | CoT+SC@4 | 28.8 | 80.6 | 88.0 | 66.8 | 66.0 |\n|  | Ours | **43.2** | **89.6** | **92.7** | **73.0** | **74.6** |\n| Llama-3.1-8B-Instruct | Zero-shot CoT | 18.0 | 61.5 | 69.3 | 52.4 | 50.3 |\n|  | Few-shot CoT | 47.2 | 76.6 | 82.0 | 63.6 | 67.3 |\n|  | CoT+SC@4 | 44.2 | 80.5 | 85.6 | 69.8 | 70.0 |\n|  | Ours | **55.0** | **90.7** | **93.0** | **73.2** | **78.0** |", "caption": "Table 1: Evaluation of HiAR-ICL\u2019s reasoning capabilities against ICL methods across four reasoning benchmarks. The best results in each box are highlighted in bold. Our method, HiAR-ICL consistently achieves the best performance across models and datasets.", "description": "This table presents a comprehensive evaluation of the HiAR-ICL method's reasoning capabilities. It compares HiAR-ICL's performance against several state-of-the-art In-Context Learning (ICL) techniques across four distinct reasoning benchmarks: MATH, GSM8K, SVAMP, and StrategyQA.  The comparison includes both zero-shot and few-shot Chain-of-Thought (CoT) prompting methods, as well as CoT enhanced with a Self-Consistency (SC) approach. Results are shown for various large language models (LLMs), highlighting the consistent superior performance of HiAR-ICL across different models and datasets.  The best performance in each category is indicated in bold.", "section": "4. Experiments"}, {"content": "| MODEL | SETTING | MATH | GSM8K |\n|---|---|---|---| \n| Claude-3-Opus | CS | 60.1 | 95.0 |\n| Claude-3.5-Sonnet | CS | 71.1 | 96.4 |\n| GPT-3.5 | CS | 43.1 | 81.6 |\n| GPT-4 | CS | 64.5 | 94.2 |\n| GPT-4o | CS | 76.6 | 96.1 |\n| GPT-4o mini | CS | 70.2 | 93.2 |\n| Gemini-1.5-Pro | CS | 67.7 | 90.8 |\n| Llama-3.1-405B-Instruct | OS 405B | 73.8 | 96.8 |\n| Llama-3.1-70B-Instruct | OS 70B | 68.0 | 95.1 |\n| Llama-3-70B-Instruct | OS 70B | 50.4 | 93.0 |\n| Nemotron4-340B-Instruct | OS 340B | 41.1 | 92.3 |\n| Mixtral-large2-Instruct | OS 123B | 69.9 | 92.7 |\n| Mixtral-8x22B-Instruct | OS 141B | 54.1 | 88.2 |\n| NuminaMath-72B CoT | OS 72B | 66.7 | 90.8 |\n| Qwen2-72B-Instruct | OS 72B | 69.0 | 93.2 |\n| Yi-1.5-34B-Chat | OS 34B | 50.1 | 90.2 |\n| Qwen2.5-14B-instruct | Ours | 80.2 | 95.3 |\n| Qwen2.5-7B-instruct | Ours | 79.6 | 92.8 |\n| Qwen2-7B-instruct | Ours | 63.8 | 90.6 |\n| Yi-1.5-6B-Chat | Ours | 54.0 | 81.4 |\n| Llama-3-8B-Instruct | Ours | 43.2 | 89.6 |\n| Llama-3.1-8B-Instruct | Ours | 55.0 | 90.7 |", "caption": "Table 2: Comparison with leading closed-source LLMs. The best results in each box are highlighted in bold. Results for closed-source models are sourced from corresponding official websites. \u2018CS\u2019 and \u2018OS\u2019 represent closed-source and open-source LLMs, respectively. Notably, the 7B model, Qwen2.5-7b-instruct, surpasses all closed-source models, achieving SOTA performance.", "description": "Table 2 compares the performance of the proposed HiAR-ICL method with several leading closed-source Large Language Models (LLMs) and open-source LLMs on two benchmark datasets: MATH and GSM8K.  The table highlights the accuracy achieved by each model on each dataset.  'CS' denotes closed-source models and 'OS' denotes open-source models.  The best performance for each model/dataset combination is shown in bold.  Importantly, the table demonstrates that the 7B parameter Qwen2.5-7B-Instruct model, enhanced with HiAR-ICL, surpasses all the closed-source LLMs in performance, achieving state-of-the-art (SOTA) results.", "section": "4. Experiments"}, {"content": "| MODEL | METHOD | GSM8K | MATH |\n|---|---|---|---| \n| Yi-1.5-6B-Chat | BEATS | 76.1 | 51.2 |\n|  | Ours | **81.4** | **54.0** |\n| Qwen2-7B-instruct | BEATS | 83.0 | 61.5 |\n|  | Ours | **90.6** | **63.8** |\n| Llama-3-8B-Instruct | ToT | 69.0 | 13.6 |\n|  | RAP | 80.5 | 18.8 |\n|  | ReST-MCTS* | - | 34.2 |\n|  | LiteSearch | 82.3 | - |\n|  | LLaMA-Berry | 88.1 | 39.6 |\n|  | BEATS | 88.4 | 42.9 |\n|  | Ours | **89.6** | **43.2** |\n| Llama-3.1-8B-Instruct | LLaMA-Berry | 89.8 | 54.8 |\n|  | Ours | **90.7** | **55.0** |", "caption": "Table 3: Evaluation of HiAR-ICL\u2019s reasoning capabilities against state-of-the-art tree-based methods. Our results are marked in blue, with baseline results sourced from the original paper when accessible. The best results are highlighted in bold.", "description": "This table presents a comparison of HiAR-ICL's performance against other state-of-the-art tree-based reasoning methods on several benchmark datasets (GSM8K, MATH, and StrategyQA).  It highlights HiAR-ICL's accuracy and efficiency in solving complex reasoning problems compared to existing approaches such as ToT, RAP, ReST-MCTS*, LiteSearch, LLaMA-Berry, and rStar. The best performance for each model and dataset is shown in bold.  HiAR-ICL results are shown in blue, while the baseline results are taken from the original papers where accessible.", "section": "4. Experiments"}, {"content": "| MODEL | SETTING | GSM8K | MATH |\n|---|---|---|---| \n| Llama3-8B-Instruct | ORM | 86.4 | 38.6 |\n|  | PRM (min) | **89.6** | 42.8 |\n|  | PRM (product) | 87.9 | 41.4 |\n|  | SC | 89.0 | **43.2** |\n| Qwen2-7B-instruct | ORM | 88.7 | 56.6 |\n|  | PRM (min) | **90.6** | 62.2 |\n|  | PRM (product) | 90.4 | 61.4 |\n|  | SC | 90.4 | **63.8** |", "caption": "Table 4: The effect of verification method for HiAR-ICL. \u2018PRM\u2019, \u2018ORM\u2019, \u2019SC\u2019 denote process-reward model, output-reward model, and self-consistency, respectively.", "description": "This table presents the results of an ablation study on the HiAR-ICL model, evaluating the impact of different verification methods on its performance. The model uses three verification methods: Process Reward Model (PRM), Output Reward Model (ORM), and self-consistency (SC).  PRM assesses the quality of the reasoning steps, ORM assesses the final solution, and SC checks for consistency in the reasoning process. The table compares the performance of HiAR-ICL across different datasets (GSM8K and MATH) with each verification method used individually. This analysis helps understand which method, or combination of methods, contributes most to the model's accuracy and overall effectiveness.", "section": "4. Experiments"}, {"content": "| METHOD | 1 | 2 | 3 | 4 | 5 | AVERAGE |\n|---|---|---|---|---|---|---|\n| CoT | 83.7 | 83.3 | 82.8 | 59.4 | 37.3 | 64.8 |\n| CoT+SC | 95.3 | 90.0 | 91.4 | 73.4 | 52.2 | 76.4 |\n| HiAR-ICL | 97.7 | 94.5 | 92.3 | 80.5 | 53.0 | 79.6 |", "caption": "Table 5: Performance variations of Qwen2.5-7B-Instruct across different difficulty levels on MATH. We list the result of Zero-shot CoT, fewshot CoT+SC, and our method.", "description": "This table presents a detailed breakdown of the performance of the Qwen2.5-7B-Instruct model on the MATH benchmark dataset across various difficulty levels.  It compares the results obtained using three different approaches: Zero-shot Chain-of-Thought (CoT), Few-shot CoT with Self-Consistency (CoT+SC), and the authors' proposed HiAR-ICL method.  The table allows for a direct comparison of these three approaches, highlighting the impact of different prompting strategies and the effectiveness of the novel HiAR-ICL method in addressing the challenges posed by varying problem complexities.", "section": "4. Experiments"}]