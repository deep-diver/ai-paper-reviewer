{"references": [{"fullname_first_author": "Brown, T.", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper introduced the foundational concept of in-context learning (ICL), which is central to the current work."}, {"fullname_first_author": "Wei, J.", "paper_title": "Chain-of-thought prompting elicits reasoning in large language models", "publication_date": "2022-12-01", "reason": "This work introduced the Chain-of-Thought (CoT) prompting method which is a key technique being advanced upon."}, {"fullname_first_author": "Kojima, T.", "paper_title": "Large language models are zero-shot reasoners", "publication_date": "2022-12-01", "reason": "This paper demonstrated the surprising ability of LLMs to perform zero-shot reasoning, a phenomenon explored and expanded upon here."}, {"fullname_first_author": "Wang, X.", "paper_title": "Self-consistency improves chain of thought reasoning in language models", "publication_date": "2023-12-01", "reason": "This paper introduced the self-consistency method, which is used to improve reasoning and is directly relevant to the current work."}, {"fullname_first_author": "Zhao, W.", "paper_title": "A survey of large language models", "publication_date": "2023-12-01", "reason": "This paper provides a comprehensive overview of LLMs, which is the basis for the current work."}]}