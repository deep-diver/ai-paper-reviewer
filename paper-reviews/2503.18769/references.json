{"references": [{"fullname_first_author": "Alan Dao", "paper_title": "Alphamaze: Enhancing large language models' spatial intelligence via grpo", "publication_date": "2025-02-00", "reason": "This paper introduces AlphaMaze, a novel approach to enhance LLMs' visual spatial intelligence, specifically for maze navigation and serves as the foundational principle upon which AlphaSpace is built."}, {"fullname_first_author": "Shuai Bai", "paper_title": "Qwen2.5-vl technical report", "publication_date": "2025-02-00", "reason": "This paper details the Qwen model, which the current work extends to use as a base model (DeepSeek-R1-distil-Qwen-1.5B)."}, {"fullname_first_author": "Jin Kim", "paper_title": "Openvla: An open-source vision-language-action model", "publication_date": "2024-06-00", "reason": "This paper introduces OpenVLA, an open-source 7-billion-parameter vision-language-action model trained on 970,000 robot episodes from the Open X-Embodiment dataset."}, {"fullname_first_author": "Rui Yang", "paper_title": "Embodiedbench: Comprehensive benchmarking multi-modal large language models for vision-driven embodied agents", "publication_date": "2025-02-00", "reason": "This paper details EmbodiedBench, a widely used benchmarking framework for evaluating the performance of vision-language models on various vision and spatial reasoning tasks, which is used in the current work."}, {"fullname_first_author": "Kevin Black", "paper_title": "\u03c0: A vision-language-action flow model for general robot control", "publication_date": "2024-10-00", "reason": "This paper introduces the \u03c0\u03bf (pi-zero) model, representing a significant advancement in general-purpose robot foundation models."}]}