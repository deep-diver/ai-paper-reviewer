[{"content": "| Attention Mechanisms | Speed | Modeling | Global Attention | Block | Average Attention Distance |\n|---|---|---|---|---|---| \n| Full 3D Attention | Slow | Strong | All blocks | Equal | 1 |\n| 2+1D Attention | Fast | Weak | None block | Not Equal | 2 - (1/T + 1/HW) |\n| Skip + Window Attention | Middle | Weak | Half blocks | Not Equal | 2 - (1/k + k/THW) |\n| **Skiparse Attention** | Middle | Strong | All blocks | Equal | 2 - 2/k + 1/k<sup>2</sup>, 1 < k << THW |", "caption": "Table 1: Comparison of the different attention mechanisms. Across multiple comparison metrics, Skiparse Attention is closer to Full 3D Attention, giving it the best spatiotemporal modeling capability apart from Full 3D Attention.", "description": "This table compares different attention mechanisms used in video generation models, specifically focusing on their computational efficiency, modeling capability, and how closely they approximate the performance of the computationally expensive but highly effective Full 3D Attention.  The metrics considered include speed, the ability to model spatiotemporal relationships in videos, the proportion of the input data that is involved in attention calculations (Global Attention), and the average distance between tokens considered in attention calculations (Average Attention Distance). The results show that Skiparse Attention offers a good balance between computational efficiency and accuracy, achieving the best spatiotemporal modeling capability among alternatives except for the computationally expensive Full 3D Attention.", "section": "2.2 Joint Image-Video Skiparse Denoiser"}, {"content": "| Attention Mechanisms | AD<sub>avg</sub> |\n|---|---| \n| Full 3D Attention | **1.000** |\n| 2+1D Attention | 1.957 |\n| Skip + Window Attention (k=2) | 1.500 |\n| Skip + Window Attention (k=4) | 1.750 |\n| Skip + Window Attention (k=8) | 1.875 |\n| **Skiparse Attention (k=2)** | **1.250** |\n| **Skiparse Attention (k=4)** | 1.563 |\n| **Skiparse Attention (k=8)** | 1.766 |", "caption": "Table 2: The average attention distance ADavgsubscriptADavg\\mathrm{AD_{avg}}roman_AD start_POSTSUBSCRIPT roman_avg end_POSTSUBSCRIPT of different attention mechanisms. Results are calculated when the latent shape is 24\u00d732\u00d73224323224\\times 32\\times 3224 \u00d7 32 \u00d7 32.", "description": "This table presents a comparison of different attention mechanisms used in video generation models, specifically focusing on the concept of \"Average Attention Distance (ADavg)\".  ADavg quantifies how closely the attention patterns of a given mechanism resemble those of a full 3D attention mechanism, which is computationally expensive but provides the most comprehensive context.  Lower ADavg values generally indicate a more efficient mechanism that still captures significant spatiotemporal relationships, though with less computational cost. The results shown are calculated for a specific latent space size of 24x32x32, which is a parameter within the video generation model and impacts the attention calculations. Different attention mechanisms (Full 3D Attention, 2+1D Attention, Skip+Window Attention, and Skiparse Attention) are compared based on their ADavg, illustrating the trade-offs between computational cost and the comprehensiveness of attention.", "section": "2.2 Joint Image-Video Skiparse Denoiser"}, {"content": "| Source | Year | Length | Manual | # Num |\n|---|---|---|---|---|\n| COCO [lin2014microsoft] | 2014 | Short | Yes | 12k |\n| DiffusionDB [wang2022diffusiondb] | 2022 | Tags | Yes | 6k |\n| JourneyDB [sun2024journeydb] | 2023 | Medium | No | 3k |\n| Dense Captions (From Internet) | 2024 | Dense | Yes | 0.5k |", "caption": "Table 3: Overview of utilized datasets for fine-tuning prompt refiner.", "description": "This table details the datasets used to fine-tune the prompt refiner model. It lists the source of each dataset, the year it was published, the length of captions typically found within, whether the captions were manually created or automatically generated, and the total number of data points in each.", "section": "3.3 Prompt Refiner"}, {"content": "| Domain | Dataset | Source | Captioner | Data Available | Caption Available | # Num |\n|---|---|---|---|---|---|---|\n| Image | SAM | SAM | LLaVA | Yes | Yes | 11.1M |\n|  | Anytext | Anytext | InternVL2 | Yes | Yes | 1.8M |\n|  | Human | LAION | InternVL2 | Yes | Yes | 0.1M |\n|  | Internal | - | QWen2-VL | No | No | 5.0M |\n| Video | VIDAL | YouTube Shorts | Multi-model<sup>\u2217</sup> | Yes | Yes | 2.8M |\n|  | Panda70M | YouTube | QWen2-VL | Yes | Yes<sup>\u2020</sup> | 21.2M |\n|  |  |  | ShareGPT4Video |  |  |  |\n|  | StockVideo | Mixkit<sup>\u2021</sup> | QWen2-VL | Yes | Yes |  |\n|  |  | Pexels<sup>\u22cf</sup> | ShareGPT4Video |  |  | 0.8M |\n|  |  | Pixabay<sup>\u22ce</sup> |  |  |  |  |", "caption": "Table 4: Data card of Open-Sora Plan v1.3. \u201c*\u201d denotes that the original team employs multiple models, including OFA\u00a0[wang2022ofa], mPLUG-Owl\u00a0[ye2023mplug], and ChatGPT\u00a0[openai2023gpt4] to refine captions. \u201c\u2020\u2020{\\dagger}\u2020\u201d indicates that while we do not release captions generated with QWen2-VL and ShareGPT4Video, the original team has made their generated captions publicly available.", "description": "This table presents a detailed data card for Open-Sora Plan version 1.3, outlining the datasets used for training the model. It specifies the domain (image or video), the dataset name, the source of the data, the caption generator used, and the availability of both the data and captions.  Noteworthy is that the original creators employed multiple models (OFA, mPLUG-Owl, and ChatGPT) for caption refinement, and while the current work doesn't share captions generated using QWen2-VL and ShareGPT4Video, the original team's generated captions are publicly available.  The table provides a comprehensive overview of the data resources leveraged in the Open-Sora Plan model, detailing their origins and accessibility.", "section": "4 Data Curation Pipeline"}, {"content": "| Curation Step | Tools | Thresholds | Remaining |\n|---|---|---|---| \n| Video Slicing | - | Each video is clipped to 16s | 100% |\n| Jump Cut | LPIPS [Zhang_Isola_Efros_Shechtman_Wang_2018] | 32 \u2264 frames number \u2264 512 | 97% |\n| Motion Calculation | LPIPS [Zhang_Isola_Efros_Shechtman_Wang_2018] | 0.001 \u2264 motion score \u2264 0.3 | 89% |\n| OCR Cropping | EasyOCR* | 0.20 \u2264 edge | 89% |\n| Aesthetic Filtration | Laion Aesthetic Predictor v2\u2020 | 4.75 \u2264 aesthetic score | 49% |\n| Low-level Quality Filtration | DOVER [wu2023exploring] | 0 \u2264 technical score | 44% |\n| Motion Double-Checking | LPIPS [Zhang_Isola_Efros_Shechtman_Wang_2018] | 0.001 \u2264 motion score \u2264 0.3 | 42% |", "caption": "Table 5: Implementation details and discarded data number of different filtering steps.", "description": "This table details the data filtering process used in the Open-Sora Plan project. Each row represents a step in the curation pipeline, specifying the techniques used (like jump cut detection with LPIPS or aesthetic score filtering), the thresholds applied to determine whether to discard data, and the resulting dataset size after each filtering step.  It shows how the raw dataset is refined through multiple stages to improve data quality for training the video generation model. This process is crucial to the models performance.", "section": "4.2 Data Filtering Strategy"}, {"content": "| Channel | Model | T\u2191 | Mem.\u2193 | PSNR\u2191 | LPIPS\u2193 | rFVD\u2193 |\n|---|---|---|---|---|---|---|\n| 4 | CV-VAE | 1.85 | 25.00 | 30.76 | 0.0803 | 369.23 |\n| 4 | OD-VAE | 2.63 | 31.19 | 30.69 | 0.0553 | 255.92 |\n| 4 | Allegro | 0.71 | 54.35 | 32.18 | 0.0524 | 209.68 |\n| 4 | WF-VAE-S(Ours) | 11.11 | 4.70 | 31.39 | 0.0517 | 188.04 |\n| 4 | WF-VAE-L(Ours) | 5.55 | 7.00 | 32.32 | 0.0513 | 186.00 |\n| 16 | CogVideoX | 1.02 | 35.01 | 35.76 | 0.0277 | 59.83 |\n| 16 | WF-VAE-L(Ours) | 5.55 | 7.00 | 35.79 | 0.0230 | 54.36 |", "caption": "Table 6: \nQuantitative comparison with state-of-the-art VAEs on WebVid-10M dataset. Reconstruction metrics are evaluated on 33-frame videos at a resolution of 256\u00d7\\times\u00d7256. \u201cT\u201d and \u201cMem.\u201d denote encoding throughput and Memory cost (GB), assessed on 33-frame videos at a resolution of 512\u00d7\\times\u00d7512. The highest result is highlighted in bold, and the second highest result is underlined.", "description": "Table 6 presents a quantitative comparison of the Open-Sora Plan's Wavelet-Flow Variational Autoencoder (WF-VAE) against other state-of-the-art Variational Autoencoders (VAEs) using the WebVid-10M dataset.  The evaluation focuses on 33-frame video sequences at a resolution of 256x256 pixels. Key performance metrics include Peak Signal-to-Noise Ratio (PSNR), Learned Perceptual Image Patch Similarity (LPIPS), and Reconstruction Fr\u00e9chet Video Distance (rFVD), all indicating the quality of video reconstruction.  Additionally, the table shows the encoding throughput (T, measured in videos per second) and memory cost (Mem., in GB) when processing 33-frame videos at the higher resolution of 512x512 pixels. The highest value for each metric is shown in bold, and the second-highest is underlined, enabling easy comparison of the WF-VAE's performance.", "section": "2 Core Models of Open-Sora Plan"}, {"content": "| Channel | Method | BWI | PSNR\u2191 | LPIPS\u2193 |\n|---|---|---|---|---|\n| 4 | OD-VAE | \n\n<div style=\"text-align:center\">\n  <img src=\"https://arxiv.org/html/2412.00131/ding55.png\">\n</div>\n\n | 30.31 | 0.0439 |\n|  |  | \n<div style=\"text-align:center\">\n  <img src=\"https://arxiv.org/html/2412.00131/ding51.png\">\n</div>\n\n | 28.51 (-1.80) | 0.0552 (+0.011) |\n| 4 | WF-VAE-L (Ours) | \n<div style=\"text-align:center\">\n  <img src=\"https://arxiv.org/html/2412.00131/ding55.png\">\n</div>\n\n | 32.10 | 0.0411 |\n|  |  | \n<div style=\"text-align:center\">\n  <img src=\"https://arxiv.org/html/2412.00131/ding51.png\">\n</div>\n\n | 32.10 (-0.00) | 0.0411 (-0.000) |\n| 16 | CogVideoX | \n<div style=\"text-align:center\">\n  <img src=\"https://arxiv.org/html/2412.00131/ding55.png\">\n</div>\n\n | 35.79 | 0.0198 |\n|  |  | \n<div style=\"text-align:center\">\n  <img src=\"https://arxiv.org/html/2412.00131/ding51.png\">\n</div>\n\n | 35.41 (-0.38) | 0.0218 (+0.002) |\n| 16 | WF-VAE-L (Ours) | \n<div style=\"text-align:center\">\n  <img src=\"https://arxiv.org/html/2412.00131/ding55.png\">\n</div>\n\n | 35.87 | 0.0175 |\n|  |  | \n<div style=\"text-align:center\">\n  <img src=\"https://arxiv.org/html/2412.00131/ding51.png\">\n</div>\n\n | 35.87 (-0.00) | 0.0175 (-0.000) |", "caption": "Table 7: Quantitative analysis of visual quality degradation induced by block-wise inference on Panda70M. BWI denotes Block-Wise Inference and experiments are conducted on 33 frames with 256\u00d7\\times\u00d7256 resolution. Values highlighted in red signify degradation in comparison to direct inference, whereas values highlighted in green indicate preservation of the quality.", "description": "This table presents a quantitative comparison of visual quality metrics (PSNR and LPIPS) for video reconstruction using different methods: direct inference versus block-wise inference (BWI). The experiments were performed using the Panda70M dataset on 33-frame videos with a resolution of 256x256 pixels. The results show the impact of block-wise inference on the quality of video reconstruction. Red highlighted values indicate a decrease in quality compared to direct inference, while green values show no quality degradation.", "section": "2.2 Joint Image-Video Skiparse Denoiser"}, {"content": "| Model | Size | Aesthetic Quality | Action | Object Class | Spatial | Scene | Multiple Objects | CH | GPT4o |\n|---|---|---|---|---|---|---|---|---|---|---|\n| OpenSora v1.2 | 1.2B | 56.18 | 85.8 | 83.37 | 67.51 | 42.47 | 58.41 | 51.87 | 2.50 |\n| CogVideoX-2B | 1.7B | 58.78 | 89.0 | 78.00 | 53.91 | 38.59 | 48.48 | 38.60 | 3.09 |\n| CogVideoX-5B | 5.6B | 56.46 | 77.2 | 76.85 | 45.89 | 41.44 | 46.43 | 48.45 | 3.36 |\n| Mochi-1 | 10.0B | 56.94 | 94.6 | 86.51 | 69.24 | 36.99 | 50.47 | 28.07 | 3.76 |\n| OpenSoraPlan v1.3 | 2.7B | 59.00 | 81.8 | 70.97 | 44.46 | 28.56 | 35.87 | 71.00 | 2.64 |\n| OpenSoraPlan v1.3\u2217 | 2.7B | 60.70 | 86.4 | 84.72 | 49.63 | 52.92 | 44.57 | 68.39 | 2.95 |", "caption": "Table 8: Quantitative comparison of Open-Sora Plan and other state-of-the-art methods. \u201c*\u201d donates we use our prompt refiner to get results.", "description": "This table presents a quantitative comparison of the video generation capabilities of Open-Sora Plan against several other state-of-the-art models.  The comparison uses metrics from the VBench benchmark, focusing on aspects like aesthetic quality, action accuracy, object class recognition, spatial scene understanding, multiple object recognition, and overall consistency.  A separate row shows results for Open-Sora Plan when using a prompt refiner (indicated by an asterisk). This allows assessment of the model's performance with both detailed prompts (typical for training) and shorter, more concise prompts (more common in real-world use cases). The table offers insights into how effectively different models generate various aspects of video content.", "section": "5 Results"}]