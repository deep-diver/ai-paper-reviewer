[{"figure_path": "2410.20305/tables/table_1_0.html", "caption": "Table 1: Comparison of training samples per second for different attention implementations. Relative speedups over FlashAttention-3 and FlexAttention, respectively, are shown for the FlexAttention with Prefix Sharing column. FlexAttention with prefix sharing consistently outperforms the baselines, with speedups ranging from 1.1-1.5x, with FlexAttention alone being slower than FA3. For the Prefix / Completion column, we report the median ratio. For high median overall lengths (> 500), the gains from prefix sharing are > 35%, with better gains for high prefix to completion ratios.", "description": "Table 1 compares the training throughput (samples per second) of different attention mechanisms (FlashAttention-3, FlexAttention, and FlexAttention with prefix sharing) across several preference optimization datasets, showing speedups gained from using prefix sharing.", "section": "4.3.1 Prefix Sharing Benchmarking"}, {"figure_path": "2410.20305/tables/table_7_0.html", "caption": "Table 1: Comparison of training samples per second for different attention implementations. Relative speedups over FlashAttention-3 and FlexAttention, respectively, are shown for the FlexAttention with Prefix Sharing column. FlexAttention with prefix sharing consistently outperforms the baselines, with speedups ranging from 1.1-1.5x, with FlexAttention alone being slower than FA3. For the Prefix / Completion column, we report the median ratio. For high median overall lengths (> 500), the gains from prefix sharing are > 35%, with better gains for high prefix to completion ratios.", "description": "Table 1 compares the training throughput (samples per second) of three different attention mechanisms across six datasets, showing the speedup achieved by using prefix sharing.", "section": "4.3.1 Prefix Sharing Benchmarking"}, {"figure_path": "2410.20305/tables/table_8_0.html", "caption": "Table 2: Comparison of training samples per second with sequence packing. For Flex Attn + Prefix Sharing + Packing, relative speedups over FA3 + Packing and Flex Attn + Packing are shown in parentheses, respectively. For the Prefix / Completion column, we report the median ratio. Our method (Prefix sharing + Packing) demonstrates at least a 30% increase in training throughput for most datasets. The impact of sequence packing is especially prominent for datasets like HH-RLHF and TLDR with shorter overall sequence lengths. Only Ultrafeedback, which has a extremely low prefix-to-completion ratio (0.3), shows a modest improvement of 21% over the FlexAttention baseline.", "description": "Table 2 presents the training throughput (samples per second) for different attention mechanisms with and without sequence packing, showing that prefix sharing with packing consistently improves training efficiency for most datasets.", "section": "4.3.2 Packing Benchmarking"}, {"figure_path": "2410.20305/tables/table_12_0.html", "caption": "Table 3: MT-Bench [37] scores for different packing and non-packing DPO training across different batch sizes. Models were trained with Ultrafeedback using hyperparameters from Zephyr [29].", "description": "Table 3 compares the MT-Bench scores achieved by models trained with packing and without packing for different batch sizes, showing that packing does not significantly harm downstream performance.", "section": "4.3.2 Packing Benchmarking"}]