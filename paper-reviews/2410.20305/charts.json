[{"figure_path": "2410.20305/charts/charts_5_0.png", "caption": "Figure 3: Microbenchmarking results of the MLP layer for Mistral 7B. Relative speedups of prefix sharing over normal paired data are shown in comparison to the ideal speedup (assuming linear runtime). We see that the MLP layer scales very closely to the ideal speedup and that increasing the prefix length helps push the speedup closer to the ideal for a given prefix to completion ratio.", "description": "The chart displays the relative speedup of using prefix sharing in the MLP layer of the Mistral-7B model compared to the normal paired format, for various prefix lengths and prefix-to-completion ratios, showing near-ideal linear speedup for longer prefix lengths.", "section": "4.2.1 MLP Micro-benchmarking"}, {"figure_path": "2410.20305/charts/charts_6_0.png", "caption": "Figure 4: Microbenchmarking results of the self-attention operation only for Mistral 7B. Relative speedups of FlexAttention with prefix sharing over FlashAttention-3 and FlexAttention are shown, along with the ideal speedup (assuming perfect quadratic scaling). We see that for high prefix lengths, FlexAttention with prefix sharing attains nearly ideal speedups over FlexAttention without prefix sharing, but overall it is still slower or similar in speed to FlashAttention-3.", "description": "Figure 4 shows the relative speedups of FlexAttention with prefix sharing compared to FlashAttention-3 and FlexAttention without prefix sharing for the self-attention operation of Mistral-7B,  considering various prefix and completion lengths.", "section": "4.2.2 Attention Micro-benchmarking"}, {"figure_path": "2410.20305/charts/charts_6_2.png", "caption": "Figure 5: Microbenchmarking results of the full self-attention layer (QKV projection + self-attention) for Mistral 7B. Relative speedups of FlexAttention with prefix sharing over FlashAttention-3 and FlexAttention are shown, along with the ideal speedup (assuming linear runtime). We see that although FlexAttention is slower than FlashAttention-3 for lower ratios between the prefix and completion length, as the ratio grows, FlexAttention with prefix sharing become faster.", "description": "Figure 5 shows the microbenchmarking results of the full self-attention layer for Mistral 7B, comparing the relative speedups of FlexAttention with prefix sharing against FlexAttention and FlashAttention-3, also showing the ideal speedup.", "section": "4.2.2 Attention Micro-benchmarking"}, {"figure_path": "2410.20305/charts/charts_6_3.png", "caption": "Figure 5: Microbenchmarking results of the full self-attention layer (QKV projection + self-attention) for Mistral 7B. Relative speedups of FlexAttention with prefix sharing over FlashAttention-3 and FlexAttention are shown, along with the ideal speedup (assuming linear runtime). We see that although FlexAttention is slower than FlashAttention-3 for lower ratios between the prefix and completion length, as the ratio grows, FlexAttention with prefix sharing become faster.", "description": "Figure 5 shows the microbenchmarking results of the full self-attention layer for Mistral 7B, comparing the relative speedups of FlexAttention with prefix sharing against FlashAttention-3 and FlexAttention without prefix sharing, also showing the ideal speedup assuming linear runtime.", "section": "4.2.2 Attention Micro-benchmarking"}]