{"references": [{" publication_date": "2024", "fullname_first_author": "Ben Athiwaratkun", "paper_title": "Bifurcated attention: Accelerating massively parallel decoding with shared prefixes in llms", "reason": "This paper is highly relevant because it explores prefix sharing in the context of inference, demonstrating its effectiveness in reducing memory consumption during decoding. This directly relates to the proposed method in the main paper, which leverages prefix sharing for training efficiency by reducing redundant computations.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Yuntao Bai", "paper_title": "Training a helpful and harmless assistant with reinforcement learning from human feedback", "reason": "This paper is highly relevant because it provides background on reinforcement learning from human feedback (RLHF), a technique often used in conjunction with preference optimization.  Understanding RLHF is critical for appreciating the value of DPO, which is computationally more efficient than traditional RL-based methods that involve training a reward model.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Tianle Cai", "paper_title": "Medusa: Simple llm inference acceleration framework with multiple decoding heads", "reason": "This paper is relevant due to its similarity to the proposed method. While it focuses on inference, the idea of using custom attention masks to simultaneously process multiple sequences with a shared prefix is directly analogous to the proposed prefix sharing for training efficiency.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Ganqu Cui", "paper_title": "Ultrafeedback: Boosting language models with scaled ai feedback", "reason": "This paper is important as it provides one of the datasets (Ultrafeedback) used in the experimental evaluation.  The performance on this dataset demonstrates the effectiveness of the proposed method in enhancing training efficiency even for data with low prefix-to-completion ratios.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Luigi Daniele", "paper_title": "Amplify-instruct: Synthetically generated diverse multi-turn conversations for efficient Ilm training", "reason": "This paper is important as it provides one of the datasets (Capybara) used in the experimental evaluation.  Capybara is a multi-turn chat dataset, making it suitable for evaluating the proposed method's efficiency in handling long-prompt tasks.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Ning Ding", "paper_title": "Enhancing chat language models by scaling high-quality instructional conversations", "reason": "This paper is important as it provides one of the datasets (UltraFeedback) used in the experimental evaluation.  The dataset provides insight into the relative efficiency gains under different prefix-to-completion ratios.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Hamish Ivison", "paper_title": "Unpacking DPO and PPO: Disentangling Best Practices for Learning from Preference Feedback", "reason": "This paper provides essential background on preference optimization techniques such as DPO and PPO, helping contextualize the significance of the proposed prefix sharing for DPO. The paper focuses on analyzing and comparing different approaches for preference learning and thus informs the significance of improving efficiency in DPO.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Jiwoo Hong", "paper_title": "Orpo: Monolithic preference optimization without reference model", "reason": "This paper is important as it introduces another preference optimization algorithm (ORPO) that is related to DPO, allowing the proposed method to be better situated within the broader landscape of preference tuning algorithms. It also helps to understand the potential for generalization of the prefix sharing technique beyond just DPO.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Hamish Ivison", "paper_title": "Camels in a changing climate: Enhancing Im adaptation with tulu 2", "reason": "This paper is important as it provides one of the datasets (Tulu-Helpsteer) used in the experimental evaluation.  The results on this dataset highlight the effectiveness of the proposed method across various dataset characteristics.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Albert Q Jiang", "paper_title": "Mistral 7b", "reason": "This paper introduces the Mistral-7B model, which is the LLM used in the micro-benchmarking experiments. This model is key to validating the performance improvement through careful micro-benchmarking and helps establish the context for the evaluation.", "section_number": 4}, {" publication_date": "2021", "fullname_first_author": "Mario Michael Krell", "paper_title": "Efficient sequence packing without cross-contamination: Accelerating large language models without impacting performance", "reason": "This paper introduces the concept of sequence packing, a crucial technique used in conjunction with prefix sharing to further enhance training efficiency. The understanding of sequence packing is necessary to understand the overall performance improvement achieved in the experiments.", "section_number": 3}, {" publication_date": "2019", "fullname_first_author": "Yinhan Liu", "paper_title": "Roberta: A robustly optimized bert pretraining approach", "reason": "This paper, while not directly related to preference optimization, provides background on efficient training techniques used in other large language model (LLM) contexts. Understanding these general efficient training approaches is crucial for better understanding and appreciating the work's contribution to the area of preference tuning.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Zimu Lu", "paper_title": "Step-controlled dpo: Leveraging stepwise error for enhanced mathematical reasoning", "reason": "This paper explores DPO in the context of mathematical reasoning. The experimental evaluation includes datasets such as MetaMath-DPO, highlighting the efficiency of the method across different task types.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Yu Meng", "paper_title": "Simpo: Simple preference optimization with a reference-free reward", "reason": "This paper introduces another relevant preference optimization algorithm, SimPO, providing a broader context for understanding the significance of the proposed prefix sharing method and its applicability to different preference tuning techniques.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "reason": "This paper is highly relevant because it provides background on RLHF, which often works with preference data. The paper's approach is compared and contrasted with DPO to highlight the efficiency improvements of the proposed method.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Arka Pal", "paper_title": "Smaug: Fixing failure modes of preference optimisation with dpo-positive", "reason": "This paper is highly relevant as it provides additional background on DPO, discussing its failure modes and potential solutions. Understanding these limitations helps better appreciate the value of the proposed method in addressing some of the computational inefficiencies of DPO.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Richard Yuanzhe Pang", "paper_title": "Iterative reasoning preference optimization", "reason": "This paper is relevant as it introduces another preference optimization algorithm, providing additional context and insight into related approaches. It also helps to highlight the specific contribution of the proposed prefix sharing method.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Pranav Putta", "paper_title": "Agent q: Advanced reasoning and learning for autonomous ai agents", "reason": "This paper is important as it discusses another area of LLM application (Agentic planning) and offers another dataset used in the experiments. The use of the proposed technique across different datasets and tasks underscores its potential broad applicability.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Rafael Rafailov", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "reason": "This paper is fundamental as it introduces DPO, the core preference optimization algorithm which the proposed work aims to improve.  Understanding DPO is essential for appreciating the contribution of prefix sharing to the field of LLM fine-tuning.", "section_number": 1}, {" publication_date": "2020", "fullname_first_author": "Jeff Rasley", "paper_title": "Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters", "reason": "This paper is important because it introduces DeepSpeed, a system optimization tool used to accelerate training in the experiments. It is crucial to understanding the experimental setup and ensuring reproducibility of the results.", "section_number": 4}]}