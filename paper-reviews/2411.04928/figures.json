[{"figure_path": "https://arxiv.org/html/2411.04928/x2.png", "caption": "Figure 1: With just a single image as input, our proposed DimensionX can generate highly realistic videos and 3D/4D environments that are aware of spatial and temporal dimensions.", "description": "Figure 1 showcases DimensionX's capabilities.  Given a single input image (top left), DimensionX generates a diverse range of outputs.  These include: videos with controlled camera movement or object motion (middle); full 3D scene renderings from novel viewpoints (bottom left); and 4D scene representations illustrating changes over time from new perspectives (bottom right).  This demonstrates the model's ability to understand and generate both spatial and temporal aspects of scenes from a single image.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2411.04928/x3.png", "caption": "Figure 2: Pipeline of DimensionX. Our framework is mainly divided into three parts. (a) Controllable Video Generation with ST-Director. We introduce ST-Director to decompose the spatial and temporal parameters in video diffusion models by learning dimension-aware LoRA on our collected dimension-variant datasets. (b) 3D Scene Generation with S-Director. Given one view, a high-quality 3D scene is recovered from the video frames generated by S-Director. (c) 4D Scene Generation with ST-Director. Given a single image, a temporal-variant video is produced by T-Director, from which a key frame is selected to generate a spatial-variant reference video. Guided by the reference video, per-frame spatial-variant videos are generated by S-Director, which are then combined into multi-view videos. Through the multi-loop refinement of T-Director, consistent multi-view videos are then passed to optimize the 4D scene.", "description": "DimensionX's framework is composed of three main stages: controllable video generation using ST-Director (decomposing spatial and temporal parameters via dimension-aware LoRAs); 3D scene generation from a single view's video frames using S-Director; and 4D scene generation by first generating a temporal-variant video with T-Director, selecting a keyframe to produce a spatial-variant reference video with S-Director, and refining this with multiple iterations of T-Director to create consistent multi-view videos for 4D scene optimization.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2411.04928/x4.png", "caption": "Figure 3: Visualization of Attention Map. The left row shows the attention maps during the denoising process of the original video diffusion model. The right row, from top to bottom, illustrates the attention map variation of S-Director and T-Director, respectively. Starting from step 0, the early denoising steps (before step 10 of total denoising step 50) have determined the outline and layouts of output videos. Specifically, the spatial component is recovered earlier than the temporal information during the denoising process.", "description": "This figure visualizes attention maps during the video denoising process. The left side shows the attention maps from the original video diffusion model, while the right side displays how the S-Director (spatial) and T-Director (temporal) affect the attention maps.  Analysis reveals that the spatial component of the video is defined earlier in the process than the temporal component.  The early steps of denoising (before step 10 out of 50) largely determine the overall structure and layout of the generated videos. This highlights how the model prioritizes spatial information before focusing on temporal details.", "section": "3.2 ST-Director for Controllable Video Generation"}, {"figure_path": "https://arxiv.org/html/2411.04928/x5.png", "caption": "Figure 4: Qualitative comparison in dimension-aware video generation. Given the same image and text prompt, the first row is the temporal-variant video generation (camera static), the second row is the spatial-variant video generation (camera zoom out), and the third row is the spatial- and temporal-variant video generation (camera orbit right).", "description": "This figure displays a qualitative comparison of DimensionX's dimension-aware video generation capabilities.  Three rows demonstrate different controlled video generations using the same image and text prompt. The first row shows a temporal-variant video where only the content changes, keeping the camera static. The second row illustrates spatial-variant video generation, with the camera zooming out while the content remains relatively unchanged. Finally, the third row showcases a combination of spatial and temporal variations in video generation, featuring a camera orbiting the subject. This figure highlights DimensionX's ability to control both the spatial and temporal aspects of video generation independently and together.", "section": "4. Experiment"}, {"figure_path": "https://arxiv.org/html/2411.04928/x6.png", "caption": "Figure 5: Qualitative Comparison in sparse-view 3D generation. Given two large-angle views, our approach obviously outperforms other baselines.", "description": "This figure compares the 3D reconstruction results of different methods using only two wide-angle input views.  DimensionX, the authors' method, is shown to produce significantly better results than the other baselines (ViewCrafter and InstantSplat) in terms of overall 3D scene quality and fidelity.", "section": "4.3 3D Scene Generation"}, {"figure_path": "https://arxiv.org/html/2411.04928/x7.png", "caption": "Figure 6: Qualitative results of 4D scene generation. Given a real-world or synthetic single image, our DimensionX produces coherent and intricate 4D scenes with rich features.", "description": "Figure 6 presents qualitative results demonstrating DimensionX's 4D scene generation capabilities.  Starting with a single real-world or synthetic image as input, the model generates a sequence of videos representing dynamic scenes with intricate details and coherent visual features. The figure showcases examples of these generated 4D scenes, highlighting the model's ability to produce complex, photorealistic outputs from minimal input.", "section": "4.4 4D Scene Generation"}, {"figure_path": "https://arxiv.org/html/2411.04928/x8.png", "caption": "Figure 7: Ablation study on the sparse-view 3D generation: The absence of S-Director results in lower reconstruction quality.", "description": "This ablation study analyzes the impact of the S-Director on sparse-view 3D scene generation.  Two sets of results are presented, one where the S-Director is included and one where it is excluded. The figures show input images, the ground truth 3D scene, and the generated 3D scenes.  The inclusion of the S-Director leads to a significant improvement in the quality of the reconstructed 3D scene.  The absence of the S-Director results in lower reconstruction quality, indicated by noticeable artifacts and reduced detail.  Quantitative metrics, such as PSNR, SSIM, and LPIPS, are provided to support these observations.", "section": "4.5. Ablation Study"}]