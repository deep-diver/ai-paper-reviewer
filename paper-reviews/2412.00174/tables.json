[{"content": "| Methods | FID \u2193 | Diversity \u2191 | PA-MPJPE \u2193 | Angle Error \u2193 | VC Similarity \u2191 | Context Relevance \u2191 | Character Consistency \u2191 | Inference \u2193 | Latency \u2193 |\n|---|---|---|---|---|---|---|---|---|---| \n| SynMSI Dataset | - | 9.136 | - | - | - | 4.888 | 4.893 | - | - |\n| LLM+Speech (Llama2) [69] | - | - | - | - | 0.818 | 3.527 | **3.859** | - | - |\n| AnyGPT (fine-tune) [81] | - | - | - | - | 0.819 | 3.502 | 3.803 | - | - |\n| DLP (MotionGPT) [17] | **4.254** | 8.259 | 165.053 | 0.495 | 0.812 | **3.577** | 3.785 | 5.518 |\n| SOLAMI (w/o pretrain) | 5.052 | **8.558** | **159.709** | **0.387** | **0.820** | 3.541 | 3.461 | 2.657 |\n| SOLAMI (LoRA) | 15.729 | 8.145 | 167.149 | 0.400 | 0.770 | 3.251 | 3.423 | 2.710 |\n| SOLAMI (full params) | **3.443** | **8.853** | **151.500** | **0.360** | **0.824** | **3.634** | **3.824** | **2.639** |", "caption": "Table 1: Quantitative results of baselines and SOLAMI. \u2018\u2191\u2191\\uparrow\u2191\u2019(\u2018\u2193\u2193\\downarrow\u2193\u2019) indicates that the values are better if the metrics are larger (smaller). We run all the evaluations 5 times and report the average metric. The best results are in bold and the second best results are underlined.", "description": "This table presents a quantitative comparison of SOLAMI against several baseline methods across various metrics related to motion and speech quality.  Lower values are better for FID and PA-MPJPE, while higher values are preferred for Diversity, Angle Error, VC Similarity, Context Relevance, and Character Consistency. Inference Latency is also compared. The evaluation was conducted five times for each method, and the table displays the average results. Bold values indicate the best performance, and underlined values represent the second-best performance.", "section": "6. Experiments"}, {"content": "| Dimension | Questions |\n|---|---| \n| Motion Coherence | Does the motion match the character\u2019s setting? |\n|  | Does the motion align well with speech? |\n| Motion Interaction | Does the character follow motion instructions correctly? |\n|  | Does the character understand user\u2019s motion? |\n| Speech Consistency | Does the speech match the character\u2019s setting? |\n|  | Is the speech relevant to the current topic? |\n| Overall Experience | How would you rate the overall experience? |", "caption": "Table 2: Questionnaire settings of our user study.", "description": "This table details the questionnaire used in the user study to evaluate the immersive VR experience.  It outlines the questions asked to assess different aspects of the user's experience, categorized into dimensions such as motion coherence (how well the character's motion aligned with the scene and dialogue), speech consistency (how well the speech matched the character and conversation), and overall experience. This provides a structured way to gather feedback on various aspects of the system's performance.", "section": "6.3 VR Interface User Study"}, {"content": "| ID | Body & Hand | Repre | Backbone | Token Interleaved | FID \u2193 | Diversity \u2191 | PA-MPJPE \u2193 | Pred Valid \u2191 |\n|---|---|---|---|---|---|---|---|---|\n| 1 | bind | joints | GPT-2 | - | 1.48 | 9.03 | 148.00 | 0.836 |\n| 2 | bind | rotation | GPT-2 | - | 3.44 | 12.94 | 143.70 | 0.813 |\n| 3 | separate | rotation | GPT-2 | Yes | 3.00 | 11.64 | 117.26 | 0.676 |\n| 4 | separate | rotation | GPT-2 | No | 2.72 | 14.05 | 112.53 | 0.638 |\n| 5 | separate | rotation | Llama2 | No | 1.82 | 10.40 | 110.23 | 0.999 |", "caption": "Table 3: Quantitative results of pre-training on text-to-motion task. \u2018\u2191\u2191\\uparrow\u2191\u2019(\u2018\u2193\u2193\\downarrow\u2193\u2019) indicates that the values are better if the metrics are larger (smaller). The best results are in bold and the second best results are underlined.", "description": "This table presents a quantitative comparison of different pre-training methods for a text-to-motion task.  The methods are evaluated using several metrics: FID (Frechet Inception Distance), Diversity, PA-MPJPE (Percentage of Average MPJPE), and Pred Valid.  Lower FID scores indicate better reconstruction quality, while higher Diversity, Pred Valid, and lower PA-MPJPE scores represent improved performance. The table shows results for different configurations: whether the body and hand motions are treated as separate sequences or combined, the type of backbone network used (GPT-2 or Llama2), and whether or not interleaved tokens were used.  This allows for an analysis of the effectiveness of various design choices in generating high-quality motion from textual descriptions.", "section": "3.2 Training"}, {"content": "| ID | Body & Hand | Repre | Motion Metrics |  PA-MPJPE \u2193 | FID \u2193 |\n|---|---|---|---|---|---| \n| 1 | separate | joints | 87 | **1.0** | \n| 2 | bind | joints | **80** | 1.3 | \n| 3 | separate | rotation | 88 | 1.88 | \n| 4 | bind | rotation | 113 | 2.34 |", "caption": "Table 4: Quantitative results of Motion VQVAE. \u2018\u2191\u2191\\uparrow\u2191\u2019(\u2018\u2193\u2193\\downarrow\u2193\u2019) indicates that the values are better if the metrics are larger (smaller). The best results are in bold.", "description": "This table presents a quantitative comparison of different motion vector quantization variational autoencoder (VQ-VAE) configurations.  It evaluates the performance of four variations based on two factors: whether the body and hand motions are represented separately or jointly ('bind' vs. 'separate') and whether a joints representation or rotation representation was used.  The evaluation metrics include FID (Fr\u00e9chet Inception Distance), which measures the quality of reconstructed motion, and PA-MPJPE (Pose-Aligned Mean Per-Joint Position Error), which assesses the accuracy of the representation. Lower FID and PA-MPJPE scores indicate better performance.  The table also shows the reconstruction success rate.", "section": "3. Social Vision-Language-Action Modeling"}, {"content": "| Methods | Input | Output |\n|---|---|---|\n| MoCap Human Motions from Internet Videos with SMPLer-X [16] | https://arxiv.org/html/2412.00174/figures/images/collection_methods/cc_input.png | https://arxiv.org/html/2412.00174/figures/images/collection_methods/cc_output.png |\n|  | https://arxiv.org/html/2412.00174/figures/images/collection_methods/mj_input.png | https://arxiv.org/html/2412.00174/figures/images/collection_methods/mj_output.png |\n| Motion Captioning on Internet Videos with GPT-4o [52] | https://arxiv.org/html/2412.00174/figures/images/collection_methods/xcjy_input.png | 1-3s: Turn head to the right and look straight ahead, with a neutral expression; 4-5s: Turn body and look sideways, with a serious expression, almost no movement; 6-8s: Turn to the left side, smiling while looking forward. |\n|  | https://arxiv.org/html/2412.00174/figures/images/collection_methods/panda_input.png | 1-2s: A panda in a combat stance, right hand raised in a fist, left hand extended, with a serious facial expression; 3s: Panda\u2019s body tilts to the left side, right hand clenched in a fist, left hand stretched forward, eyes looking to the right front; 4-5s: Panda raises both hands above the head, lifting one leg. |\n| Real Data Collection from VR Platforms | https://arxiv.org/html/2412.00174/figures/images/collection_methods/vr_input.png | https://arxiv.org/html/2412.00174/figures/images/collection_methods/vr_output.png |\n| Synthetic Data Generation from Existing Datasets | https://arxiv.org/html/2412.00174/figures/images/collection_methods/synmsi_input.png | https://arxiv.org/html/2412.00174/figures/images/collection_methods/synmsi_output.png |", "caption": "Table 5: Methods of collecting multimodal interaction data.", "description": "This table compares three different methods for collecting multimodal interaction data, which is crucial for training models like SOLAMI.  It outlines the input data used (e.g., MoCap data from internet videos, motion captioning from internet videos, real data collected via VR platforms) and shows the corresponding output format generated. The table highlights the challenges and complexities associated with each method, demonstrating the various ways researchers have attempted to gather this data for social VLA modeling.", "section": "C. More Details of Data Generation"}]