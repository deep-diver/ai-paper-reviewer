[{"figure_path": "https://arxiv.org/html/2412.00174/x2.png", "caption": "Figure 1: \nSOLAMI enables the user to interact with 3D autonomous characters through speech and body language in an immersive VR environment via an end-to-end social vision-language-action model, which is trained on our synthesized multimodal dataset SynMSI.", "description": "This figure illustrates the SOLAMI system, which allows users to interact with 3D autonomous characters in an immersive virtual reality (VR) environment using both speech and body language. The system employs an end-to-end social vision-language-action (VLA) model.  The user's speech and body language are captured as input through multi-modal capture. This input is processed by the VLA model, which generates corresponding character motion and speech as output.  The figure shows the various components of the system, including multi-modal capture, motion and speech tokenization, the social VLA modeling, character motion and speech generation, and interactive tasks.  The model is trained using a synthesized multimodal dataset called SynMSI.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2412.00174/x3.png", "caption": "Figure 2: \nTraining pipeline of SOLAMI. We train SOLAMI through a three-stage process. In the pre-training stage, we train the model with motion-text and speech-text related tasks to align the speech and motion modalities with language. During the instruction tuning stage, we train the model with social multimodal multi-round interaction data, enabling it to generate multimodal responses that align with the character settings and the context of the topic.", "description": "The figure illustrates the three-stage training pipeline for the SOLAMI model.  The pre-training stage focuses on aligning speech and motion modalities with language using motion-text and speech-text paired data. This stage helps the model learn the relationships between different modalities. The instruction tuning stage utilizes social multimodal multi-round interaction data to train the model to generate appropriate multimodal responses based on the character's setting and conversational context.  This stage teaches the model to engage in natural, context-aware interactions. The figure visually depicts these stages, showing the data used and the model's output.", "section": "3. Social Vision-Language-Action Modeling"}, {"figure_path": "https://arxiv.org/html/2412.00174/x4.png", "caption": "Figure 3: \nSynMSI dataset generation.\nOur synthesizing pipeline consists of 4 steps.\nBased on numerous character-relevant topics and state-of-the-art LLMs\u00a0[52], we generate text scripts for multimodal dialogues. Using a large-scale motion database\u00a0[76, 29, 17], we retrieve the most appropriate motions and refine the speech scripts accordingly. Finally, we employ TTS/voice cloning\u00a0[19] to generate character-specific speech. This approach enables us to create multimodal interaction data of various characters using only existing motion datasets.", "description": "This figure illustrates the four-step synthesis pipeline used to create the SynMSI dataset.  First, character-relevant topics are identified using LLMs to generate text scripts for multimodal dialogues. Second, a large-scale motion database is used to select the most suitable motions for the generated scripts, and the speech scripts are refined accordingly. Third, character-specific speech is generated using TTS or voice cloning technology. Finally, the combined data produces multimodal interaction data for various characters, leveraging only pre-existing motion datasets.", "section": "4. SynMSI Dataset"}, {"figure_path": "https://arxiv.org/html/2412.00174/x5.png", "caption": "Figure 4: \nVR interface architecture. Our VR project consists of a Quest 3 client and a server. The Quest client captures and transmits user body motion and speech to the server. The server then generates character\u2019s speech, body motion, and face blendshape parameters based on the selected methods. The response is then sent back to the Quest client to drive the character.", "description": "The VR system architecture comprises a Quest 3 client and a server.  The Quest 3 client captures user speech and body motion data, transmitting it to the server.  The server processes this data using chosen methods (e.g., SOLAMI) to generate a character's response including speech, body motion, and facial expressions (blendshapes). This generated response is then sent back to the Quest 3 client for real-time animation of the 3D character.", "section": "5. VR Interface"}, {"figure_path": "https://arxiv.org/html/2412.00174/x6.png", "caption": "Figure 5: \nResults of the user study with 95% confidence.", "description": "This figure displays the results of a user study evaluating the performance of SOLAMI and baseline models.  The study used a 5-point Likert scale to assess four dimensions of user experience: Motion Coherence (how well the character's motion matched the setting and dialogue), Motion Interaction (how well the character responded to user motion input), Speech Consistency (how well character speech matched the setting and topic), and Overall Experience (the overall user satisfaction).  Error bars show 95% confidence intervals.  The results visually demonstrate SOLAMI's superior performance compared to LLM+Speech and DLP (MotionGPT) across all four dimensions, indicating a more natural and engaging interactive experience for users.", "section": "6.3 VR Interface User Study"}, {"figure_path": "https://arxiv.org/html/2412.00174/extracted/6031754/figures/images/wordclouds/keywords_plot_assistant.png", "caption": "Figure 6: \nQualitative results of SOLAMI and baselines, and the user workflow for VR experience.\nOur social VLA model, trained in an end-to-end strategy on SynMSI dataset, can accurately perceive the semantic information embedded within users\u2019 speech and motion input, and subsequently generate natural and coherent responses.", "description": "Figure 6 presents a qualitative comparison of SOLAMI against two baselines (LLM+Speech and DLP) in terms of their ability to handle three key aspects of social interaction: understanding body language, executing motion commands, and engaging in interactive tasks.  The figure shows example user inputs and the corresponding responses from each method, visually demonstrating SOLAMI\u2019s superior performance in generating natural and coherent responses that accurately reflect the semantic meaning of the user's speech and motion. The workflow diagram illustrates the process: User input (speech and motion) is captured in the VR environment, processed by the chosen model, and the model's response (speech and motion) drives the 3D character's behavior, providing a visual representation of the overall system.", "section": "6. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.00174/extracted/6031754/figures/images/wordclouds/keywords_plot_11-45-G.png", "caption": "(a) Samantha", "description": "Figure 7 shows a word cloud visualization of keywords from the topics collected for different characters in the SynMSI dataset.  The topics are categorized into four groups: character-related topics, news-related topics, daily life topics, and topics that people are generally curious about.  These topics serve to provide a foundation for generating diverse and engaging multi-round dialogues in a natural conversational style for the characters within the virtual environment.  The word clouds visually represent the frequency of keywords associated with each character, offering insight into the specific thematic areas of conversation covered in the SynMSI dataset.", "section": "SynMSI Dataset"}, {"figure_path": "https://arxiv.org/html/2412.00174/extracted/6031754/figures/images/wordclouds/keywords_plot_Batman.png", "caption": "(b) K-VRC", "description": "Figure 7(b) shows a word cloud visualization of keywords related to the character K-VRC from the SynMSI dataset.  The word cloud highlights the topics and themes of conversations generated for this specific character, showcasing the variety of interactions encompassed within the dataset. The words' sizes reflect their frequency in the dataset, providing insights into the common themes discussed with K-VRC during the multi-modal interactions.", "section": "4. SynMSI Dataset"}, {"figure_path": "https://arxiv.org/html/2412.00174/extracted/6031754/figures/images/wordclouds/keywords_plot_Bananya.png", "caption": "(c) Batman", "description": "The figure shows a word cloud visualization of keywords related to Batman, a character used in the SynMSI dataset.  The word cloud highlights terms associated with Batman's persona, settings (Gotham, Batcave), and actions (criminals, attack, Batmobile), showcasing the diverse semantic information captured within the dataset.", "section": "4. SynMSI Dataset"}]