{"references": [{"fullname_first_author": "Tri Dao", "paper_title": "FlashAttention: Fast and memory-efficient exact attention with IO-awareness", "publication_date": "2022", "reason": "This paper introduced FlashAttention, a crucial technique for efficient attention mechanisms in LLMs, which is directly relevant to the memory optimization problem addressed in the current paper."}, {"fullname_first_author": "Samyam Rajbhandari", "paper_title": "ZeRO: Memory optimizations toward training trillion parameter models", "publication_date": "2020", "reason": "This paper presented ZeRO, a memory optimization technique for large-scale model training that is foundational to many modern LLM training practices and directly related to the memory challenges addressed in the current paper."}, {"fullname_first_author": "Nikita Kitaev", "paper_title": "Reformer: The efficient transformer", "publication_date": "2020", "reason": "This paper introduced the Reformer model, an efficient transformer architecture that addresses the quadratic complexity of self-attention, making it relevant to the memory challenges of LLMs discussed in the current paper."}, {"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017", "reason": "This paper introduced the Transformer architecture, which is fundamental to most modern LLMs, including those considered in this paper, and therefore is highly influential in the field."}, {"fullname_first_author": "Priya Goyal", "paper_title": "Accurate, large minibatch SGD: Training ImageNet in 1 hour", "publication_date": "2017", "reason": "This paper demonstrated efficient large mini-batch training techniques, a key element for scaling up LLM training and directly relevant to the optimization goals of the current paper."}]}