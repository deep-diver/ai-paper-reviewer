[{"content": "| Inputs: |  **\\(\\mathbf{{\\color[rgb]{0.953125,0.61328125,0.0703125}\\definecolor[named]{pgfstrokecolor}{rgb}{0.953125,0.61328125,0.0703125}E}}\\in\\mathbb{R}^{{D}\\times{N}}\\)**, **\\(\\mathbf{{\\color[rgb]{0.16015625,0.5,0.7265625}\\definecolor[named]{pgfstrokecolor}{rgb}{0.16015625,0.5,0.7265625}C}}\\in\\mathbb{R}^{{D}\\times{|V|}}\\)**, **\\(\\mathbf{x}\\in\\mathbb{R}^{N}\\)** | \n|---|---| \n|  | Block sizes **\\(N_{B}\\)** and **\\(D_{B}\\)**. | \n| Outputs: | **\\(\\mathbf{o}=(\\mathbf{{\\color[rgb]{0.16015625,0.5,0.7265625}\\definecolor[named]{pgfstrokecolor}{rgb}{0.16015625,0.5,0.7265625}C}}^{\\top}\\mathbf{{\\color[rgb]{0.953125,0.61328125,0.0703125}\\definecolor[named]{pgfstrokecolor}{rgb}{0.953125,0.61328125,0.0703125}E}})_{\\mathbf{x}}\\in\\mathbb{R}^{N}\\)** |", "caption": "Table 1: Peak memory footprint and time to compute the loss, its gradient, and their combination.\nNote that intermediate buffers can often (but not always) be reused between the loss and gradient computation, resulting in lower peak memory consumption than the sum of the parts. Batch of 8192819281928192 tokens with a vocabulary size of 256000256000256000256000 and hidden dimension 2304. Embedding and classifier matrix taken during Gemma 2 (2B) training on Alpaca. Measured on an A100-SXM4 GPU with \\qty80GB of RAM, PyTorch 2.4.1, CUDA 12.4, rounded to closest MB.\nSome numbers are multiples of 1000100010001000 due to dimensions chosen and PyTorch\u2019s allocation strategy.\n\u2018Lower bound\u2019 is the amount of memory required for the output buffer(s), i.e., \u2207\ud835\udc04\u2207\ud835\udc04\\nabla\\mathbf{{\\color[rgb]{0.953125,0.61328125,0.0703125}\\definecolor[named]{%\npgfstrokecolor}{rgb}{0.953125,0.61328125,0.0703125}E}}\u2207 bold_E and \u2207\ud835\udc02\u2207\ud835\udc02\\nabla\\mathbf{{\\color[rgb]{0.16015625,0.5,0.7265625}\\definecolor[named]{%\npgfstrokecolor}{rgb}{0.16015625,0.5,0.7265625}C}}\u2207 bold_C, this is the lower bound for the memory footprint of any method.", "description": "This table compares the peak memory usage and runtime for different methods of computing the cross-entropy loss and its gradient.  It includes a baseline PyTorch implementation, optimized versions using `torch.compile` and Torch Tune, the Liger Kernels approach, and the proposed Cut Cross-Entropy (CCE) method. The comparison considers memory usage for the loss computation, gradient calculation, and their combination.  The experiment used a batch size of 8192 tokens and a vocabulary size of 256,000, with a hidden dimension of 2304, running on an A100-SXM4 GPU with 80GB of RAM.  The lower bound represents the minimum memory needed for the output gradients. Note that memory reuse between loss and gradient computations can sometimes reduce the overall peak memory.", "section": "5.1 Runtime and Memory"}, {"content": "| Inputs: |  \\mathbf{\\color[rgb]{0.953125,0.61328125,0.0703125}\\definecolor[named]{pgfstrokecolor}{rgb}{0.953125,0.61328125,0.0703125}E} \\in \\mathbb{R}^{D \\times N} and \\mathbf{\\color[rgb]{0.16015625,0.5,0.7265625}\\definecolor[named]{pgfstrokecolor}{rgb}{0.16015625,0.5,0.7265625}C} \\in \\mathbb{R}^{D \\times |V|} | \n|---|---| \n|  | Block sizes \\(N_B\\), \\(M_B\\), and \\(D_B\\). | \n| Outputs: | \\mathbf{\\mathrm{\\color[rgb]{0.75390625,0.22265625,0.16796875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.75390625,0.22265625,0.16796875}LSE}} = \\log\\sum_j \\exp(\\mathbf{\\color[rgb]{0.16015625,0.5,0.7265625}\\definecolor[named]{pgfstrokecolor}{rgb}{0.16015625,0.5,0.7265625}C}_j^\\top \\mathbf{\\color[rgb]{0.953125,0.61328125,0.0703125}\\definecolor[named]{pgfstrokecolor}{rgb}{0.953125,0.61328125,0.0703125}E}) \\in \\mathbb{R}^N |", "caption": "Table A1: Table\u00a01 where all methods include a filter that removes tokens that are ignored in loss computation. This simple change represents large improvements in practice.", "description": "Table A1 presents a revised version of Table 1, incorporating a filter that excludes tokens not contributing to the loss calculation (e.g., padding tokens). This simple modification significantly improves the efficiency of all the methods evaluated, as shown by the runtime and memory consumption data.  The table provides a direct comparison of various cross-entropy loss computation methods, highlighting how effectively this pre-processing step reduces the memory footprint and computation time for each.", "section": "A Removing Ignored Tokens"}, {"content": "| Inputs: |  **E** \u2208 \u211d<sup>D\u00d7N</sup>, **C** \u2208 \u211d<sup>D\u00d7|V|</sup>, **LSE** \u2208 \u211d<sup>N</sup>, and \u2207**LSE** \u2208 \u211d<sup>N</sup> | \n|---|---| \n|  | Block sizes N<sub>B</sub>, M<sub>B</sub>, and D<sub>B</sub>. | \n|  | Accuracy threshold \u03b5. | \n| Outputs: | \u2207**E** \u2208 \u211d<sup>D\u00d7N</sup>, \u2207**C** \u2208 \u211d<sup>D\u00d7|V|</sup> | ", "caption": "Table A2: Memory usage and time of CCE, Liger Kernels, Torch Tune, torch.compile, and Baseline for additional models. Batch of 8192819281928192 tokens.", "description": "This table presents a comparison of the memory usage and runtime performance of different cross-entropy loss computation methods across various large language models.  The methods compared are Cut Cross-Entropy (CCE), Liger Kernels, Torch Tune, Torch Compile, and a baseline PyTorch implementation.  The models used include Gemma 2 (9B, 27B), Llama 3 (8B), Mistral NeMo, and Phi 3.5 Mini.  The experiment uses a batch size of 8,192 tokens for each model. For each method and model, the table shows the memory usage for loss computation, gradient calculation, and both together, along with the corresponding computation times. The results highlight CCE's superior memory efficiency compared to other methods, demonstrating significant reductions in memory consumption while maintaining competitive runtime performance.", "section": "5.1 Runtime and Memory"}, {"content": "| Method | Loss Memory | Loss Time | Gradient Memory | Gradient Time | Loss+Gradient Memory | Loss+Gradient Time |\n|---|---|---|---|---|---|---|\n| Lower bound | 0.004MB |  | 1161MB |  | 1161MB |  |\n| 1) CCE (Ours) | **1MB** | **43ms** | **1163MB** | 95ms | **1164MB** | **135ms** |\n| 2) Liger Kernels (Hsu et al., 2024)<sup>2</sup> | 1474MB | 302ms |  |  | 1474MB | 303ms |\n| 3) Torch Tune Team (2024) (8 chunks) | 8000MB | 55ms | 1630MB | 115ms | 9631MB | 170ms |\n| 4) `torch.compile` | 4000MB | 49ms | 12000MB | **92ms** | 16000MB | 143ms |\n| 5) Baseline | 24000MB | 82ms | 16000MB | 121ms | 28000MB | 207ms |\n| 6) CCE (No Vocab Sorting) | 0.09MB | 42ms | 1162MB | 104ms | 1162MB | 143ms |\n| 7) CCE (No Grad. Filter) | 0.09MB | 42ms | 1162MB | 324ms | 1162MB | 362ms |", "caption": "Table A3: Raw data for Fig.\u00a01. Memory usage calculated using a global batch size of 65536655366553665536.", "description": "This table provides the raw data used to generate Figure 1 in the paper.  It details the memory usage breakdown for various large language models (LLMs), categorized into log probabilities, activations, and weights/optimizer/gradients. The memory usage is calculated using a global batch size of 65,536 tokens.  For each model, the table shows the maximum batch size attainable before and after applying the Cut Cross-Entropy (CCE) optimization, along with the resulting increase in batch size.", "section": "Appendix"}]