[{"figure_path": "https://arxiv.org/html/2411.09009/x1.png", "caption": "(a) Regular cross-entropy", "description": "This figure shows a comparison of memory usage for different language models using regular cross-entropy and the proposed Cut Cross-Entropy (CCE).  Subfigure (a) displays memory usage for various models when using the standard cross-entropy loss computation. It breaks down the memory usage into different components: log probabilities, weights and optimizer states, and activation checkpoints.  The x-axis represents the maximum batch size in millions of tokens, while the y-axis represents the memory usage in gigabytes (GB). Each point represents a specific language model. The different colored parts of the points represent the memory consumption of each part of the model. The subfigure shows that the log probabilities of the cross-entropy loss consume a significant portion of the memory.", "section": "INTRODUCTION"}, {"figure_path": "https://arxiv.org/html/2411.09009/x2.png", "caption": "(b) Cut cross-entropy (ours)", "description": "This figure shows the memory usage and maximum attainable batch size for various language models when using the Cut Cross-Entropy (CCE) method. It demonstrates that CCE significantly reduces the memory footprint of the loss computation, thereby enabling the use of larger batch sizes without sacrificing training speed or convergence.  The chart visually compares CCE's performance to regular cross-entropy, showcasing the dramatic reduction in memory consumption achieved by CCE.", "section": "1 INTRODUCTION"}, {"figure_path": "https://arxiv.org/html/2411.09009/x3.png", "caption": "Figure 1: \nMemory use and maximum attainable batch size (in millions of tokens) for a variety of frontier models on a 16-GPU (80 GB each) fully-sharded data-parallel setup\u00a0(Rajbhandari et\u00a0al., 2020) with activation checkpointing\u00a0(Chen et\u00a0al., 2016) and a mixed-precision 16-bit (fp16/bf16) AdamW optimizer\u00a0(Kingma & Ba, 2015; Loshchilov & Hutter, 2019).\nFor each model, we break its memory use down into weights and optimizer states, activation checkpoints, and the log-probabilities computed by the cross-entropy loss layer.\nOur Cut Cross-Entropy (CCE) enables increasing the batch size by 1.5x (Llama 2 13B) to 10x (GPT 2, Gemma 2 2B), with no sacrifice in speed or convergence. Exact values in Table\u00a0A3.", "description": "Figure 1 is a comparison of memory usage and maximum batch size for various large language models (LLMs) under two different cross-entropy loss implementations: regular cross-entropy and the authors' proposed Cut Cross-Entropy (CCE). The models are trained using a 16-GPU setup with fully-sharded data parallelism, activation checkpointing, and a mixed-precision AdamW optimizer.  The figure shows that the memory consumption of the cross-entropy loss is significantly reduced by CCE. This allows for a substantial increase in the maximum batch size attainable during training (ranging from 1.5x to 10x), without affecting training speed or convergence. Memory usage is broken down by component (weights, optimizer states, activations, and log-probabilities).  Table A3 provides more details on the exact memory usage numbers.", "section": "1 INTRODUCTION"}, {"figure_path": "https://arxiv.org/html/2411.09009/x4.png", "caption": "(a) Indexed matmul\n(forward)", "description": "This figure illustrates the access patterns and computation involved in the indexed matrix multiplication during the forward pass of the Cut Cross-Entropy (CCE) algorithm.  It's a block diagram showing how the algorithm efficiently computes the dot product between network embeddings and classifier weights without materializing the entire logit matrix in global memory.  The inputs, including embeddings (E) and classifier weights (C), are divided into blocks, and the operations are performed blockwise to leverage GPU cache efficiently.  The result of the indexed matrix multiplication is written to global memory.", "section": "4.1 MEMORY-EFFICIENT INDEXED MATRIX MULTIPLICATION"}, {"figure_path": "https://arxiv.org/html/2411.09009/x5.png", "caption": "(b) Linear-log-sum-exp,\nforward pass", "description": "This figure shows the forward pass of the linear-log-sum-exp operation used in the Cut Cross-Entropy (CCE) method.  The linear-log-sum-exp computation is a crucial part of calculating the cross-entropy loss efficiently in CCE. The diagram illustrates the process of computing the log-sum-exp (LSE) values, which involves intermediate matrix multiplications and reduction operations performed on smaller blocks to optimize memory usage. The specific access patterns and computations are shown to highlight the efficiency of this approach.", "section": "4.2 MEMORY-EFFICIENT LINEAR-LOG-SUM-EXP, FORWARD PASS"}, {"figure_path": "https://arxiv.org/html/2411.09009/x6.png", "caption": "(c) Linear-log-sum-exp,\nbackward pass", "description": "This figure shows the backward pass of the linear-log-sum-exp operation. The backward pass is crucial for calculating gradients during training, allowing the model to adjust its weights and improve its accuracy.  The illustration details the computational steps and memory access patterns involved in this process, highlighting the efficiency and memory savings achieved by the proposed Cut Cross-Entropy (CCE) method. It shows how the CCE method efficiently handles large vocabularies while keeping memory consumption low.", "section": "4.2 MEMORY-EFFICIENT LINEAR-LOG-SUM-EXP, BACKWARD PASS"}, {"figure_path": "https://arxiv.org/html/2411.09009/x7.png", "caption": "Figure 2: Access patterns and computation of blockwise (a) indexed matrix multiplication, (b) linear-log-sum-exp forward pass, and (c) linear-log-sum-exp backward pass.\nSee Algorithms\u00a01, 2 and\u00a03 for the corresponding algorithms.", "description": "Figure 2 illustrates the computational steps and memory access patterns of three key operations within the Cut Cross-Entropy (CCE) method.  Panel (a) shows the blockwise indexed matrix multiplication, which efficiently computes the dot product of the classifier weights and embeddings, avoiding the need to store the entire logit matrix.  This is followed by (b) the linear-log-sum-exp forward pass, illustrating how the log-sum-exp operation is performed efficiently in a blockwise manner to prevent memory overflow. Finally, (c) displays the corresponding backward pass, outlining how gradients are calculated efficiently using the same blockwise approach. Algorithms 1, 2, and 3 in the paper provide detailed pseudocode for each of these operations.", "section": "4 CUT CROSS-ENTROPY"}, {"figure_path": "https://arxiv.org/html/2411.09009/x8.png", "caption": "Figure 3: Average probability for the i\ud835\udc56iitalic_ith most likely token, log-log plot. The probabilities very quickly vanish below numerical precision.", "description": "This log-log plot displays the average probability of the i-th most likely token in the vocabulary.  The y-axis represents the probability (on a logarithmic scale), and the x-axis represents the rank (also on a logarithmic scale).  The graph shows how rapidly the probabilities decrease as the token rank increases. This demonstrates that the probabilities for many less frequent tokens fall below the level of numerical precision, which has implications for memory efficiency in computing cross-entropy loss, as detailed in the paper.", "section": "5.2 Gradient Filtering"}, {"figure_path": "https://arxiv.org/html/2411.09009/x9.png", "caption": "(a) Gemma 2 2B", "description": "The figure shows training loss curves for the Gemma 2 2B model, comparing the performance of Cut Cross-Entropy (CCE) and Torch Compile Cross-Entropy.  Both methods exhibit nearly identical loss curves over the course of training, indicating that CCE's gradient filtering does not negatively impact convergence. The graph plots training loss against the number of gradient steps.  Confidence intervals are shown to illustrate the variability across multiple training runs.", "section": "5.3 Training Stability"}, {"figure_path": "https://arxiv.org/html/2411.09009/x10.png", "caption": "(b) Phi 3.5 Mini", "description": "This figure displays the training loss curves for the Phi 3.5 Mini language model.  The curves compare the performance of Cut Cross-Entropy (CCE) against a baseline method (torch.compile).  The near-identical curves demonstrate that CCE's gradient filtering technique does not negatively impact the model's convergence during training. Results are averaged over five separate training runs (seeds) for a more robust comparison.", "section": "5.3 Training Stability"}]