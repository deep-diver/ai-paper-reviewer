[{"figure_path": "https://arxiv.org/html/2412.18450/x1.png", "caption": "Figure 1: Proposed 3DGraphLLM approach leverages 3D semantic scene graph learnable representation supplied as input to an LLM to perform various 3D vision-language tasks.", "description": "This figure illustrates the 3DGraphLLM approach.  A 3D scene graph is generated, encoding the objects and their semantic relationships within a 3D scene.  This graph is then converted into a learnable representation, which is fed as input to a Large Language Model (LLM). The LLM then processes this information to perform a variety of 3D vision-language tasks, such as 3D referred object grounding, 3D dense scene captioning, and 3D question answering.  The output of the LLM is a response in natural language to a user query about the 3D scene.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2412.18450/x2.png", "caption": "Figure 2: \nThe overall architecture of our approach. 3DGraphLLM leverages pre-trained encoders for 3D object point clouds and semantic relationships between objects. We introduce trainable layers to map the extracted graph node and edge features into the token embedding space of a pre-trained LLM. The scene graph is flattened for input into the LLM, with each object represented by a subgraph of its k nearest neighbors. To further adapt the LLM to 3D vision-language tasks, we add new object tokens to the LLM\u2019s vocabulary and fine-tune it using LoRa.", "description": "The figure illustrates the architecture of 3DGraphLLM, a system that combines 3D scene graphs with large language models (LLMs) for 3D scene understanding.  The system processes 3D point cloud data, extracting features for objects and their semantic relationships using pre-trained encoders. These features are then projected into the embedding space of an LLM using trainable layers.  The scene graph is transformed into a flattened sequence for LLM input, with each object represented by its features and the features of its k nearest neighbors (k-NN). To enhance LLM performance, new object tokens are added to the LLM vocabulary, and the LLM itself is fine-tuned using LoRA (Low-Rank Adaptation). The output is a response to various vision-language tasks such as object grounding, scene captioning, and question answering.", "section": "3 Method"}, {"figure_path": "https://arxiv.org/html/2412.18450/x3.png", "caption": "Figure 3: \nQualitative examples of 3DGraphLLM performance on the ScanRefer dataset. For each query, we provide an RGB image from the ScanNet dataset showing the selected object, along with a visualization of the RGB point cloud. In the point cloud, green points indicate the points that 3DGraphLLM identified as corresponding to the object from the text query, while the green box highlights the ground truth (GT) box for the query.", "description": "This figure showcases qualitative examples of 3DGraphLLM's performance on the ScanRefer dataset.  Each example shows a user query, an RGB image of the ScanNet scene highlighting the identified object, and a 3D point cloud visualization.  Within the point cloud, green points pinpoint the points 3DGraphLLM identified as belonging to the object described in the query, while a green bounding box indicates the ground truth location of the object.", "section": "4.1 Experimental Results"}, {"figure_path": "https://arxiv.org/html/2412.18450/x4.png", "caption": "Figure 4: \nComparison of Uni3D object features and VL-SAT semantic edge features for the two nearest neighbors (NNs) based on ground-truth (GT) scene segmentation and Mask3D scene segmentation within the ScanNet training set.\n Left: Uni3D object features are relatively close for GT point clouds and Mask3D point clouds. Center: using the standard approach for selecting NNs to generate VL-SAT features, the features for pairs of Mask3D point clouds differ significantly from those of GT point clouds. Right: after applying a minimum neighbor distance filter for selecting NNs, the VL-SAT features for object pairs from Mask3D instance segmentation align more closely with those from GT instance segmentation.", "description": "Figure 4 presents a comparison of Uni3D object features and VL-SAT semantic edge features using both ground truth (GT) and Mask3D scene segmentations from the ScanNet training dataset.  The left panel shows that Uni3D object features are very similar for both GT and Mask3D point clouds. The center panel demonstrates that using the standard nearest neighbor (NN) selection method for VL-SAT feature generation results in significantly different features for Mask3D compared to GT. The right panel illustrates how applying a minimum distance filter during NN selection leads to much closer alignment between VL-SAT features derived from Mask3D and GT segmentations.", "section": "4.2 Ablation Studies. Role of Semantic Relations and Training Pipeline"}, {"figure_path": "https://arxiv.org/html/2412.18450/extracted/6093446/nn_ablation.png", "caption": "Figure 5: \nDependence of inference speed and visual grounding accuracy on the number of nearest neighbors in the object subgraph. This experiment utilizes the RioRefer dataset along with GT instance segmentation.", "description": "Figure 5 illustrates the relationship between the number of nearest neighbors considered when constructing object subgraphs and two key metrics: inference speed and visual grounding accuracy.  The experiment uses the RioRefer dataset and ground truth instance segmentation for a comprehensive evaluation.  Increasing the number of nearest neighbors generally improves visual grounding accuracy, but at the cost of increased inference time.  This demonstrates a trade-off between model performance and computational efficiency, highlighting the importance of optimizing the number of neighbors for optimal results.", "section": "4.3 Ablation Studies. 3D Scene Graph Representation"}, {"figure_path": "https://arxiv.org/html/2412.18450/x5.png", "caption": "Figure 6: \nCommon failure cases of 3DGraphLLM related to spatial relationships. Left: In the ScanQA dataset, 3DGraphLLM incorrectly identifies the front/back and left.right directions relative to the observer. Right: In the ScanRefer dataset, 3DGraphLLM confuses left and right. The GT object is highlighted in green, and the 3DGraphLLM prediction is highlighted in red.", "description": "This figure showcases instances where 3DGraphLLM struggles with spatial relationships. The left panel shows an example from the ScanQA dataset where the model misinterprets front/back and left/right directions relative to the viewer's perspective. The right panel illustrates a similar issue from the ScanRefer dataset, this time with the model confusing left and right.  Ground truth objects are highlighted in green, while the model's predictions are in red.  This highlights the model's limitations in accurately interpreting and applying spatial reasoning during scene understanding tasks.", "section": "A Common Failure Cases"}, {"figure_path": "https://arxiv.org/html/2412.18450/x6.png", "caption": "Figure 7: \nFunctional queries about the room and objects to the 3DGraphLLM. Left: 3DGraphLLM is capable of answering questions about functional properties of the room and its room type. Right: 3DGraphLLM is capable of answering questions about the functional properties of objects in a room.", "description": "Figure 7 demonstrates 3DGraphLLM's ability to answer functional queries about a room and its objects, showcasing its common sense reasoning capabilities.  The left panel shows 3DGraphLLM correctly identifying the room type ('hotel room') and its lack of cooking facilities based on the objects present, implying an understanding of functional properties. The right panel shows 3DGraphLLM successfully identifying an object suitable for homework ('laptop') given a query about functionality, highlighting its capacity to connect object properties to human actions and tasks.", "section": "Experiments"}]