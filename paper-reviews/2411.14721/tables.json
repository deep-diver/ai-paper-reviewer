[{"content": "| Method | BLEU-2 \u2191 | BLEU-4 \u2191 | ROUGE-1 \u2191 | ROUGE-2 \u2191 | ROUGE-L \u2191 | METEOR \u2191 |\n|---|---|---|---|---|---|---|\n| MolT5-large | 0.594 | 0.508 | 0.654 | 0.510 | 0.594 | 0.614 |\n| MolReGPT | 0.607 | 0.525 | 0.634 | 0.476 | 0.562 | 0.610 |\n| MolCA | 0.639 | 0.555 | 0.697 | 0.558 | 0.636 | 0.669 |\n| BioT5 | 0.635 | 0.556 | 0.692 | 0.559 | 0.633 | 0.656 |\n| ICMA | 0.651 | 0.581 | 0.686 | 0.550 | 0.625 | 0.661 |\n| **MolReFlect** | **0.676** | **0.608** | **0.703** | **0.571** | **0.644** | **0.680** |", "caption": "Table 1: Overall performance comparison for the Mol2Cap task on the ChEBI-20 dataset (Best, Second Best). Except for MolReGPT, all the other methods involve fine-tuning LLMs on the ChEBI-20 dataset.", "description": "This table presents a comprehensive comparison of the performance of various models on the molecule captioning task using the ChEBI-20 dataset.  It specifically evaluates the Mol2Cap (molecule-to-caption) subtask, comparing different methods' effectiveness in generating accurate and relevant captions for given molecular structures.  The models are evaluated based on several standard metrics, including BLEU scores (BLEU-2 and BLEU-4), ROUGE scores (ROUGE-1, ROUGE-2, and ROUGE-L), and METEOR score.  The table highlights the best and second-best performing models to facilitate easy comparison and identification of top-performing approaches.  Note that most methods involve fine-tuning LLMs on the ChEBI-20 dataset, except for MolReGPT.", "section": "5 Experiments"}, {"content": "| Method | BLEU\u2191 | EM\u2191 | Levenshtein\u2193 | MACCS FTS\u2191 | RDK FTS\u2191 | Morgan FTS\u2191 | Validity\u2191 |\n|---|---|---|---|---|---|---|---| \n| MolT5-large | 0.854 | 0.311 | 16.07 | 0.834 | 0.746 | 0.684 | 0.905 |\n| MolReGPT | 0.857 | 0.280 | 17.14 | 0.903 | 0.805 | 0.739 | 0.899 |\n| BioT5 | 0.867 | 0.413 | 15.10 | 0.886 | 0.801 | 0.734 | 1.000 |\n| ICMA | 0.855 | 0.460 | 18.73 | 0.916 | 0.837 | 0.789 | 0.958 |\n| **MolReFlect** | **0.903** | **0.510** | **11.84** | **0.929** | **0.860** | **0.813** | **0.977** |", "caption": "Table 2: Overall performance comparison for the Cap2Mol task on the ChEBI-20 dataset (Best, Second Best). Except for MolReGPT, all the other methods involve fine-tuning LLMs on the ChEBI-20 dataset.", "description": "This table presents a comprehensive comparison of various models' performance on the Cap2Mol task within the ChEBI-20 dataset.  The Cap2Mol task involves generating molecule structures (SMILES strings) from given captions describing molecules.  The table compares different models using several metrics, including BLEU score (measuring translation quality), Exact Match (the percentage of perfectly generated molecules), Levenshtein distance (measuring the edit distance between generated and ground truth SMILES), and three different types of molecular fingerprint scores (MACCS, RDK, Morgan), assessing the similarity of generated and reference molecule structures. Finally, a validity score is included indicating the proportion of valid SMILES strings generated by each model.  The models are compared to establish which ones perform best and the extent to which different approaches succeed in the task.  MolReFlect is shown to outperform other models across multiple metrics, showcasing the effectiveness of its approach.", "section": "5.2 Overall Performance Comparison"}, {"content": "| Method | BLEU-2\u2191 | BLEU-4\u2191 | ROUGE-1\u2191 | ROUGE-2\u2191 | ROUGE-L\u2191 | METEOR\u2191 |\n|---|---|---|---|---|---|---|\n| Mistral-7B(naive-SFT) | 0.566 | 0.478 | 0.614 | 0.449 | 0.547 | 0.572 |\n| **MolReFlect** | **0.676** | **0.608** | **0.703** | **0.571** | **0.644** | **0.680** |\n| w/o Context Examples | 0.617 | 0.539 | 0.657 | 0.510 | 0.593 | 0.623 |\n| w/o Fine-grained Alignments | 0.651 | 0.581 | 0.686 | 0.550 | 0.625 | 0.661 |\n| w/o In-Context Reflection | 0.648 | 0.580 | 0.700(8) | 0.568(3) | 0.640(7) | 0.678 |\n| w/o Selection | 0.672 | 0.604 | 0.701(1) | 0.568(1) | 0.640(9) | 0.677 |", "caption": "Table 3: Ablation analysis of MolReFlect for the Mol2Cap task performance (Mistral-7B-Instruct-v0.2 as backbone). Above: Mistral-7B(naive-SFT) and MolReFlect; Middle: Ablating Context Examples and Fine-grained Alignments; Below: Ablating In-Context Reflection and Selection.", "description": "This table presents an ablation study on the MolReFlect model for the molecule-to-caption (Mol2Cap) task. It shows the impact of different components of the model on performance, using Mistral-7B-Instruct-v0.2 as the base model. The top section compares the baseline naive supervised fine-tuning (naive-SFT) with the full MolReFlect model. The middle section demonstrates the effect of removing context examples and fine-grained alignments separately. The bottom section investigates the impact of removing the in-context reflection and selection stages of MolReFlect.  The results are evaluated using BLEU, ROUGE, and METEOR metrics.", "section": "5 Experiments"}, {"content": "| Method | BLEU\u2191 | EM\u2191 | Levenshtein\u2193 | MACCS FTS\u2191 | RDK FTS\u2191 | Morgan FTS\u2191 | Validity\u2191 |\n|---|---|---|---|---|---|---|---| \n| Mistral-7B(naive-SFT) | 0.767 | 0.234 | 27.39 | 0.852 | 0.718 | 0.649 | 0.918 |\n| **MolReFlect** | **0.903** | **0.510** | **11.84** | **0.929** | **0.860** | **0.813** | 0.977 |\n| w/o Context Examples | 0.886 | 0.430 | 13.99 | 0.916 | 0.828 | 0.775 | 0.981 |\n| w/o Fine-grained Alignments | 0.855 | 0.460 | 18.73 | 0.916 | 0.837 | 0.789 | 0.958 |\n| w/o In-Context Reflection | 0.900(3) | 0.502 | 11.94 | 0.926 | 0.855 | 0.807 | 0.979 |\n| w/o Selection | 0.900(1) | 0.496 | 12.86 | 0.927 | 0.858 | 0.808 | **0.980** |", "caption": "Table 4: Ablation analysis of MolReFlect for the Cap2Mol task performance (Mistral-7B-Instruct-v0.2 as backbone). Above: Mistral-7B(naive-SFT) and MolReFlect; Middle: Ablating Context Examples and Fine-grained Alignments; Below: Ablating In-Context Reflection and Selection.", "description": "This table presents an ablation study on the MolReFlect model for the Cap2Mol task, which involves generating molecules from captions. It compares the performance of Mistral-7B with naive supervised fine-tuning against MolReFlect's full model and several ablated versions. The ablations systematically remove components of MolReFlect, such as context examples, fine-grained alignments, in-context reflection, and selection, to isolate the impact of each component on the overall performance. The metrics used include BLEU, Exact Match, Levenshtein, MACCS, RDK, Morgan fingerprint scores, and Validity.", "section": "5 Experiments"}, {"content": "| Method | BLEU-2\u2191 | BLEU-4\u2191 | ROUGE-1\u2191 | ROUGE-2\u2191 | ROUGE-L\u2191 | METEOR\u2191 | AVG IMP |\n|---|---|---|---|---|---|---|---| \n| Direct Prompting | 0.071 | 0.038 | 0.220 | 0.093 | 0.192 | 0.139 | - |\n| Chain-of-Thought | 0.149 | 0.075 | 0.249 | 0.089 | 0.204 | 0.179 | 41.80% |\n| Few-shot Prompting | 0.457 | 0.389 | 0.556 | 0.399 | 0.492 | 0.481 | - |\n| Few-shot Chain-of-Thought | 0.474 | 0.382 | 0.523 | 0.349 | 0.449 | 0.476 | -4.41% |", "caption": "Table 5: Performance comparison of prompting strategies for the teacher LLM (Llama-3-70B-Instruct) to perform the Mol2Cap task independently.", "description": "This table compares the performance of different prompting strategies when using the Llama-3-70B-Instruct large language model (LLM) to perform the molecule-to-caption (Mol2Cap) task without any fine-tuning.  It shows how various prompting techniques\u2014direct prompting, chain-of-thought prompting, few-shot prompting, and few-shot chain-of-thought prompting\u2014affect the model's ability to generate accurate captions for molecules. The results are evaluated using several metrics including BLEU-2, BLEU-4, ROUGE-1, ROUGE-2, ROUGE-L, and METEOR scores, providing a comprehensive assessment of the generated captions' quality.", "section": "5 Experiments"}, {"content": "| Method | BLEU\u2191 | EM\u2191 | Levenshtein\u2193 | MACCS FTS\u2191 | RDK FTS\u2191 | Morgan FTS\u2191 | Validity\u2191 | AVG IMP |\n|---|---|---|---|---|---|---|---|---|\n| Direct Prompting | 0.417 | 0.032 | 46.91 | 0.711 | 0.474 | 0.411 | 0.666 | - |\n| Chain-of-Thought | 0.380 | 0.033 | 47.46 | 0.708 | 0.476 | 0.407 | 0.683 | -1.05% |\n| Few-shot Prompting | 0.773 | 0.134 | 22.53 | 0.869 | 0.748 | 0.679 | 0.751 | - |\n| Few-shot Chain-of-Thought | 0.759 | 0.129 | 23.13 | 0.872 | 0.752 | 0.679 | 0.766 | 0.74% |", "caption": "Table 6: Performance comparison of prompting strategies for the teacher LLM (Llama-3-70B-Instruct) to perform the Cap2Mol task independently.", "description": "This table compares the performance of different prompting strategies when using the Llama-3-70B-Instruct large language model (LLM) to perform the Cap2Mol task (generating molecules from captions) independently, without a smaller student LLM.  The strategies tested include direct prompting, chain-of-thought prompting, few-shot prompting, and few-shot chain-of-thought prompting.  The table shows the performance metrics (BLEU, Exact Match, Levenshtein distance, MACCS, RDK, Morgan fingerprint scores, and Validity) for each prompting strategy to assess which approach is most effective for this task when using only the large LLM.", "section": "5 Experiments"}, {"content": "| Item | Value |\n|---|---| \n| int4 | True |\n| temperature | 0.75 |\n| top_p | 0.85 |\n| top_k | 40 |\n| num_return_sequences | 1 |\n| max_new_tokens | 512 |\n| number-of-examples | 2 |", "caption": "Table 7: Hyper-parameters for the larger teacher LLM.", "description": "This table details the hyperparameters used for the larger teacher Language Model (LLM) in the MolReFlect framework.  These settings control aspects of the model's behavior during the zero-shot alignment extraction and in-context selective reflection stages.  Specific parameters include the quantization method (int4), temperature, top_p and top_k values for sampling, the number of return sequences, and the maximum number of new tokens to generate.", "section": "3 Preliminaries"}, {"content": "| Item | Value |\n|---|---| \n| macro batch size | 32 |\n| micro batch size | 1 |\n| steps | 8000 |\n| warm-up steps | 1000 |\n| cutoff length | 4096 |\n| number-of-examples | 2 |\n| learning rate | 2e-4 |\n| lora_r | 32 |\n| lora_alpha | 64 |\n| lora_dropout | 0.1 |\n| int8 | True |\n| fp16 | True |\n| temperature | 0.75 |\n| top_p | 0.85 |\n| top_k | 40 |\n| num_return_sequences | 1 |\n| max_new_tokens | 512 |", "caption": "Table 8: Hyper-parameters for the smaller student LLM.", "description": "This table lists the hyperparameters used for fine-tuning the smaller student Large Language Model (LLM) in the MolReFlect framework.  It includes settings for batch size (both macro and micro), training steps, learning rate, LoRA (Low-Rank Adaptation) parameters (r, alpha, and dropout), and other settings relevant to the generation process (integer type, precision, temperature, top_p, top_k, and max_new_tokens).", "section": "3 Preliminaries"}, {"content": "| Item | semantic similarity | perplexity |\n|---|---|---|\n| molecules | 0.2483 | 2.246 |\n| zero-shot alignments | 0.4983 | 2.066 |\n| in-context reflected alignments | 0.4985 | 2.070 |\n| fine-grained alignments | 0.5029 | 1.995 |", "caption": "Table 9: Average semantic similarity and perplexity scores of different alignments and the original molecules in the training set for the Mol2Cap task.", "description": "This table presents a quantitative analysis of the quality of different molecular alignments generated for the molecule captioning task (Mol2Cap). It compares the average semantic similarity and perplexity scores of four types of alignments: original molecules, zero-shot alignments, in-context reflected alignments, and fine-grained alignments.  The perplexity score measures how well a language model predicts the alignment, while semantic similarity assesses the closeness in meaning between the alignment and the original molecule. This comparison helps evaluate the effectiveness of different alignment methods in representing molecular structures and their associated textual descriptions.", "section": "B.1 Statistics of Fine-grained Alignments"}, {"content": "| Item | semantic similarity | perplexity |\n|---|---|---|\n| captions | 0.2483 | 2.758 |\n| zero-shot alignments | 0.2721 | 2.426 |\n| in-context reflected alignments | 0.2377 | 2.351 |\n| fine-grained alignments | 0.2524 | 2.230 |", "caption": "Table 10: Average semantic similarity and perplexity scores of different alignments and the original molecule captions in the training set for the Cap2Mol task.", "description": "This table presents a quantitative analysis of the quality of various molecule-caption alignments generated during the Cap2Mol task.  It compares the average semantic similarity and perplexity scores for four different alignment types: original molecule captions, zero-shot alignments, in-context reflected alignments, and the final fine-grained alignments. The semantic similarity score measures how well the alignment captures the meaning of the original caption, while the perplexity score reflects the uncertainty or randomness of the alignment.  By comparing these scores across the different alignment types, the table helps evaluate the effectiveness of the MolReFlect framework's different stages in refining the molecule-caption alignments.", "section": "5 Experiments"}, {"content": "| Tasks | BACE | BBBP |\n|---|---|---|\n| Mistral7B | 0.4926 | 0.4829 |\n| ICMA | 0.7995 | 0.6775 |\n| MolReFlect | 0.8795 | 0.8925 |", "caption": "Table 11: ROC-AUC (%) scores of MolReFlect on the BACE and BBBP task from the MoleculeNet dataset (Wu et\u00a0al., 2018) (Best, Second Best).", "description": "This table presents the area under the ROC curve (ROC-AUC) scores achieved by the MolReFlect model and several baseline models on two molecular property prediction tasks: the Binding Affinity to the Beta-Secretase 1 (BACE) and Blood-Brain Barrier Permeability (BBBP) tasks.  The MoleculeNet dataset (Wu et al., 2018) is used for this evaluation.  The scores indicate the performance of each model in predicting whether a molecule exhibits a certain property (e.g., binding affinity or BBBP permeability).  Higher scores represent better performance.", "section": "5 Experiments"}, {"content": "| Method | BLEU-2 \u2191 | BLEU-4 \u2191 | ROUGE-1 \u2191 | ROUGE-2 \u2191 | ROUGE-L \u2191 | METEOR \u2191 |\n|---|---|---|---|---|---|---|\n| Mistral-7B | 0.361 | 0.288 | 0.471 | 0.325 | 0.419 | 0.421 |\n| MolReFlect w/o CoT-ICMT | 0.369 | 0.297 | 0.482 | 0.342 | 0.433 | 0.431 |\n| **MolReFlect** | **0.414** | **0.343** | **0.511** | **0.374** | **0.458** | **0.470** |", "caption": "Table 12: Mol2Cap Performance of MolReFlect on the PubChem dataset (Best, Second Best). Here, Mistral-7B serves as the backbone LLM.", "description": "This table presents a comparison of the performance of different models on the Mol2Cap task using the PubChem dataset.  Specifically, it contrasts the performance of the Mistral-7B model (baseline) with MolReFlect, both with and without Chain-of-Thought In-Context Molecule Tuning (CoT-ICMT). The metrics used for evaluation include BLEU scores (BLEU-2, BLEU-4), ROUGE scores (ROUGE-1, ROUGE-2, ROUGE-L), and METEOR score.  The results showcase MolReFlect's performance improvement over the baseline model, highlighting the effectiveness of the fine-tuning method.", "section": "5.2 Overall Performance Comparison"}, {"content": "| Method | BLEU\u2191 | EM\u2191 | Levenshtein\u2193 | MACCS FTS\u2191 | RDK FTS\u2191 | Morgan FTS\u2191 | Validity\u2191 |\n|---|---|---|---|---|---|---|---| \n| Mistral-7B | 43.84 | 8.2 | 74.16 | 73.08 | 57.72 | 47.19 | 86.6 |\n| MolReFlect w/o CoT-ICMT | 74.39 | 14.45 | 30.23 | 79.87 | 66.24 | 56.02 | 95.5 |\n| **MolReFlect** | **76.32** | **17.15** | **27.69** | **80.6** | **67.76** | **57.65** | **96.2** |", "caption": "Table 13: Cap2Mol Performance of MolReFlect on the PubChem dataset (Best, Second Best). Here, Mistral-7B serves as the backbone LLM.", "description": "This table presents the results of the Cap2Mol task on the PubChem dataset using MolReFlect, a novel teacher-student framework for molecule-caption alignment.  The table compares the performance of MolReFlect against several baseline methods.  Metrics used for evaluation include BLEU score, exact match rate, Levenshtein distance, MACCS, RDK, and Morgan fingerprints, and the validity of generated SMILES strings.  Mistral-7B served as the base large language model for all methods in this comparison.", "section": "5.2 Overall Performance Comparison"}, {"content": "| Method | BLEU-2\u2191 | BLEU-4\u2191 | ROUGE-1\u2191 | ROUGE-2\u2191 | ROUGE-L\u2191 | METEOR\u2191 |\n|---|---|---|---|---|---|---|\n| **MolReFlect** | **0.672** | **0.605** | **0.703** | **0.571** | **0.644** | **0.678** |\n| w/o Context Examples | 0.617 | 0.540 | 0.661 | 0.515 | 0.598 | 0.622 |\n| w/o Fine-grained Alignments | 0.665 | 0.595 | 0.693 | 0.559 | 0.633 | 0.669 |", "caption": "Table 14: Mol2Cap Performance of MolReFlect when Llama-3-8B-Instruct serves as the student LLM (Best, Second Best). We also compare the performance by removing the context examples and fine-grained alignments for ablation purposes.", "description": "This table presents the performance of the MolReFlect model on the Mol2Cap task of the ChEBI-20 dataset using Llama-3-8B-Instruct as the student LLM.  It shows the performance metrics (BLEU-2, BLEU-4, ROUGE-1, ROUGE-2, ROUGE-L, METEOR) achieved by MolReFlect under different conditions: with all components intact, without context examples, and without fine-grained alignments.  This allows for an analysis of the contribution of each component to the model's overall performance.  The best and second-best results are indicated.", "section": "5.3 Ablation Study & Discussion"}, {"content": "| Method | BLEU\u2191 | EM\u2191 | Levenshtein\u2193 | MACCS FTS\u2191 | RDK FTS\u2191 | Morgan FTS\u2191 | Validity\u2191 |\n|---|---|---|---|---|---|---|---| \n| **MolReFlect** | **0.896** | **0.472** | **13.33** | **0.925** | **0.846** | **0.797** | **0.979** |\n| w/o Context Examples | 0.864 | 0.395 | 16.13 | 0.904 | 0.815 | 0.754 | 0.964 |\n| w/o Fine-grained Alignments | 0.851 | 0.445 | 19.27 | 0.915 | 0.836 | 0.785 | 0.958 |", "caption": "Table 15: Cap2Mol Performance of MolReFlect when Llama-3-8B-Instruct serves as the student LLM (Best, Second Best). We also compare the performance by removing the context examples and fine-grained alignments for ablation purposes.", "description": "This table presents the results of the Cap2Mol task using MolReFlect with Llama-3-8B-Instruct as the student LLM.  It shows the overall performance metrics (BLEU, Exact Match, Levenshtein distance, and various fingerprint scores) comparing MolReFlect's performance against two ablation studies. The ablation studies remove either context examples or fine-grained alignments to analyze the impact of these components on the model's ability to generate molecules from captions.", "section": "5.2 Overall Performance Comparison"}, {"content": "| Probing Test | MolT5-base ROUGE-2 | MolT5-base METEOR | Text+Chem T5-base ROUGE-2 | Text+Chem T5-base METEOR | MolT5-large ROUGE-2 | MolT5-large METEOR | Text+Chem T5-augm ROUGE-2 | Text+Chem T5-augm METEOR | MolReFlect ROUGE-2 | MolReFlect METEOR |\n|---|---|---|---|---|---|---|---|---|---|---|\n| original | 0.481 | 0.583 | 0.498 | 0.604 | 0.510 | 0.614 | 0.543 | 0.648 | 0.571 | 0.680 |\n| canonical | 0.315 | 0.450 | 0.381 | 0.515 | 0.390 | 0.532 | 0.377 | 0.514 | 0.416 | 0.543 |\n| hydrogen | 0.199 | 0.329 | 0.187 | 0.314 | 0.174 | 0.318 | 0.201 | 0.336 | 0.305 | 0.435 |\n| kekulization | 0.333 | 0.475 | 0.413 | 0.574 | 0.405 | 0.546 | 0.410 | 0.546 | 0.443 | 0.569 |\n| cycles | 0.417 | 0.540 | 0.483 | 0.600 | 0.566 | 0.603 | 0.4575 | 0.581 | 0.545 | 0.658 |", "caption": "Table 16: Results of robustness probing test. The performance on the original test set is labelled as \u201coriginal\". The best performance is bold and the second-best performance is underlined.", "description": "This table presents the results of a robustness probing test conducted on various molecule captioning models.  The test evaluates model performance under different transformations of SMILES strings (canonicalization, hydrogen addition, kekulization, and cycle modification).  The 'original' row shows performance on the original test set without SMILES string transformations.  The best performance for each metric (ROUGE-2 and METEOR) under each transformation is shown in bold, with the second-best performance underlined.  This allows for a comparison of how well different models maintain accuracy when faced with variations in input data representation.", "section": "B.6 Study of Model Robustness"}]