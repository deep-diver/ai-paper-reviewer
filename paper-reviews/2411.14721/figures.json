[{"figure_path": "https://arxiv.org/html/2411.14721/x1.png", "caption": "Figure 1: An illustration of the alignments between the molecular space and the language space. The sub-structure patterns are highlighted with colours, and their corresponding caption phrases are also coloured with the same colours to signify the alignments. Here, the molecule Dodecanoyl Dodecanoate (CCCCCCCCCCCC(=O)OC(=O)CCCCCCCCCCC)\nis the reaction production of two dodecanoic acids. Thus, it has an anhydride group, and there are 12 carbon atoms on each side of the central oxygen atom.", "description": "Figure 1 illustrates the alignment of molecular structures with their textual descriptions.  The figure shows Dodecanoyl Dodecanoate, a molecule formed by the condensation of two dodecanoic acid molecules.  Its chemical structure is represented, with key sub-structures (like the anhydride group and the dodecyl chains) color-coded and linked to corresponding phrases in its caption. This visual demonstrates the fine-grained alignment between molecular components and their linguistic representations, highlighting the central goal of the MolReFlect framework.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2411.14721/x2.png", "caption": "Figure 2: Comparisons of four different fine-tuning paradigms, including (a) Naive Supervised Fine-tuning (naive-SFT), (b) Instruction Tuning (Wei et\u00a0al., 2021), (c) In-Context Molecule Tuning (ICMT) (Li et\u00a0al., 2024a), and (d) our proposed Chain-of-Thought In-Context Molecule Tuning (CoT-ICMT).", "description": "This figure compares four different fine-tuning methods for language models, specifically focusing on their application in molecule-caption translation.  (a) shows the basic Naive Supervised Fine-tuning approach, where the model directly maps input to output without explicit instructions or contextual information. (b) illustrates Instruction Tuning, which adds task instructions to guide the model's learning. (c) depicts In-Context Molecule Tuning, incorporating similar examples as context for improved performance. Finally, (d) presents the authors' proposed Chain-of-Thought In-Context Molecule Tuning, which combines the benefits of instruction tuning and in-context learning by structuring the reasoning process in a chain-of-thought format, leading to more explainable and accurate results.", "section": "Preliminaries"}, {"figure_path": "https://arxiv.org/html/2411.14721/x3.png", "caption": "Figure 3: The overall framework of MolReFlect.", "description": "The figure illustrates the three main stages of the MolReFlect model: Zero-shot Alignment Extraction, In-Context Selective Reflection, and Chain-of-Thought In-Context Molecule Tuning (CoT-ICMT).  Zero-shot Alignment Extraction uses a large language model (LLM) to initially extract fine-grained alignments between molecules and their corresponding text descriptions.  In-Context Selective Reflection refines these alignments by retrieving similar examples and utilizing the teacher LLM to reflect on these examples. Finally, CoT-ICMT enhances learning by incorporating the fine-grained alignments and reasoning processes in a chain-of-thought format, helping a smaller student LLM refine its molecule-text alignment capabilities. The diagram visually represents the data flow and interaction between the teacher and student LLMs throughout this process.", "section": "4 MolReFlect"}, {"figure_path": "https://arxiv.org/html/2411.14721/x4.png", "caption": "Figure 4: Embedding distributions of molecules and captions.", "description": "This figure visualizes the embedding distributions of molecules and their corresponding captions using a dimensionality reduction technique like t-SNE or UMAP. The goal is to show how the embeddings of molecules (represented as SMILES or graph structures) and their textual descriptions cluster together in a low-dimensional space.  The proximity of molecules and captions in this space indicates the quality of the alignment between the molecular representation and the text.  Similar molecules and their related captions should be clustered close together, demonstrating the effectiveness of the model in generating accurate and semantically meaningful captions for molecules, and vice versa.", "section": "B Extensive Experiments"}, {"figure_path": "https://arxiv.org/html/2411.14721/x5.png", "caption": "Figure 5: Cases of Fine-grained Alignments. We could observe that the molecule structure and characteristics have already been mentioned and aligned by the fine-grained alignments, which will surely benefit the final generations.", "description": "Figure 5 presents two examples illustrating the concept of fine-grained alignments in the MolReFlect model.  The top panel shows how the model extracts key features and characteristics from a molecule caption (e.g., 'The molecule is an aldimine and a one-carbon compound') and aligns them to the corresponding substructures of the molecule's SMILES representation. This alignment process helps the model link textual descriptions to specific molecular components. The bottom panel reverses this process, showing how the model identifies key substructures and characteristics within the molecule's SMILES and links these to specific descriptive phrases in the caption (e.g., 'The molecule is an optically active form of alpha-aminobutyric acid having L-configuration.'). These aligned features are crucial for accurate and explainable molecule-caption generation.", "section": "C Case Studies"}, {"figure_path": "https://arxiv.org/html/2411.14721/x6.png", "caption": "Figure 6: Cases of Customized Examples for the Cap2Mol task. We follow the customized examples in Li et\u00a0al. (2023a). Obviously, MolReFlect generates correct molecules in general, matching the requirements mentioned in the customized cases, while MolT5 and ICMA fail to meet the requirements.", "description": "Figure 6 showcases customized examples for the Cap2Mol task, a molecule generation task from text descriptions.  The examples are designed to test the models' ability to generate molecules that precisely match specific descriptions.  The figure directly compares the outputs of three different models: MolReFlect, MolT5, and ICMA.  MolReFlect successfully produces molecules that align with the input descriptions in each case.  In contrast, MolT5 and ICMA fail to generate correct molecules, highlighting MolReFlect's superior performance in handling customized, complex descriptions.", "section": "4 MolReFlect"}, {"figure_path": "https://arxiv.org/html/2411.14721/x7.png", "caption": "Figure 7: Cases for the Mol2Cap task.", "description": "This figure showcases six examples of the Mol2Cap task, which involves generating molecule captions from SMILES strings.  For each example, it shows the SMILES string representation of a molecule, followed by the captions generated by three different models: MolT5-large, ICMA (using Mistral-7B), and MolReFlect (also using Mistral-7B). Finally, the ground truth caption for each molecule is provided. This allows for a direct comparison of the different models' performance in accurately and comprehensively describing molecules using natural language.", "section": "C.3 Mol2Cap Cases"}, {"figure_path": "https://arxiv.org/html/2411.14721/x8.png", "caption": "Figure 8: Cases for the Cap2Mol task.", "description": "This figure showcases six examples of the Cap2Mol task, demonstrating the model's ability to generate molecular structures from textual descriptions. Each example includes the input caption describing the molecule's properties, the corresponding molecule structures generated by MolT5, ICMA (Mistral-7B), and MolReFlect (Mistral-7B), and the ground truth structure. This visualization helps to compare the performance of different models on this task and highlight MolReFlect's superior accuracy in generating correct molecular structures based on the given captions.", "section": "C.4 Cap2Mol Cases"}, {"figure_path": "https://arxiv.org/html/2411.14721/x9.png", "caption": "Figure 9: Prompt templates for Zero-shot Alignment Extraction.", "description": "This figure displays the prompt templates utilized in the Zero-shot Alignment Extraction stage of the MolReFlect model.  It shows the specific instructions given to the Large Language Model (LLM) to extract fine-grained alignments from molecule structures (Mol2Cap) and molecule captions (Cap2Mol).  The prompts guide the LLM to identify key structural features and descriptive phrases to establish a correspondence between the molecular and textual representations.", "section": "4 MolReFlect"}, {"figure_path": "https://arxiv.org/html/2411.14721/x10.png", "caption": "Figure 10: Prompt templates for In-Context Selective Reflection.", "description": "This figure presents the prompt templates utilized in the In-Context Selective Reflection stage of the MolReFlect framework.  It shows how the model prompts the language model (LLM) to refine its zero-shot alignments by providing it with relevant example molecule-alignment pairs. Two templates are shown, one for the Mol2Cap task (molecule to caption) and another for the Cap2Mol task (caption to molecule).  The templates leverage a chat-based interaction format (System, User, Assistant roles) to guide the LLM through the refinement process.", "section": "4 MolReFlect"}, {"figure_path": "https://arxiv.org/html/2411.14721/x11.png", "caption": "Figure 11: Prompt templates for MolReFlect (w/o Fine-grained Alignments).", "description": "This figure displays the prompt templates utilized in the MolReFlect model when fine-grained alignments are not employed.  It shows the structure of the prompts used for both the Mol2Cap (molecule-to-caption) and Cap2Mol (caption-to-molecule) tasks. The prompts are formatted for a chat interface with roles for the system (instructions), user (input), and assistant (model response) to guide the language model in generating the desired outputs.", "section": "4 MolReFlect"}, {"figure_path": "https://arxiv.org/html/2411.14721/x12.png", "caption": "Figure 12: Prompt templates for Chain-of-Thought In-Context Molecule Tuning (CoT-ICMT).", "description": "This figure displays the prompt templates utilized in the Chain-of-Thought In-Context Molecule Tuning (CoT-ICMT) phase of the MolReFlect framework.  It details the structure of the prompts used to guide the Large Language Model (LLM) in generating fine-grained alignments between molecules and their corresponding text descriptions.  The prompts incorporate example molecule-alignment pairs to improve the LLM's understanding and performance in the task.  The templates are designed for the chat interface of LLMs, showing system, user, and assistant roles.", "section": "4 MolReFlect"}]