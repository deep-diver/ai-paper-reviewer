{"reason": "This paper introduces ResFormer and SVFormer, two novel Transformer architectures designed to mitigate attention concentration, a phenomenon where attention increasingly focuses on fewer tokens as the network deepens. ResFormer adds a residual connection from the first layer's values to subsequent layers, approximating cross-layer attention without high computational costs. SVFormer further improves efficiency by sharing the same value embeddings across all layers. Experiments show both models improve training efficiency and downstream task performance compared to standard Transformers and related methods.", "takeaways": ["ResFormer mitigates attention concentration in deep Transformers by adding a residual connection from the first layer's values, improving training efficiency and downstream performance.", "SVFormer significantly reduces the KV cache size (by ~50%) by using single-layer value embeddings, leading to faster training.", "Both ResFormer and SVFormer demonstrate performance gains on various downstream tasks, highlighting the effectiveness of the proposed value residual learning approach for improving Transformer efficiency and representation learning."], "tldr": "To address attention concentration in deep Transformers, this paper proposes ResFormer, which uses residual connections from the first layer's values, and SVFormer, which shares value embeddings across all layers. Both models improve training efficiency and downstream performance compared to vanilla Transformers and other related methods."}