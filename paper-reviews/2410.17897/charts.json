[{"figure_path": "2410.17897/charts/charts_2_0.png", "caption": "Figure 1: (Left) Illustration of the relative training loss (loss of target model - loss of vanilla Transformer) curve between different Transformer variants; model size is fixed to be 82M. (Middle) The average entropy of token importance across layers in ResFormer vs. the vanilla Transformer, where token importance is derived from the attention matrix. Lower entropy indicates more focused attention on specific tokens. More details can be found in Eqn. 11. (Right) The average entropy of token importance across layers in Llama (8B) (Dubey et al., 2024) and Mistral (7B) (Jiang et al., 2023).", "description": "The chart displays a comparison of relative training loss, average entropy of token importance across layers for various transformer models, highlighting the impact of ResFormer on attention concentration.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.17897/charts/charts_5_0.png", "caption": "Figure 3: Average token similarity between the outputs of different mapping methods and that of Eqn. 2.", "description": "The chart compares the average cosine similarity between the outputs of different mapping methods (current attention, identity mapping) and that of Equation 2, showing how well different methods approximate the proposed efficient cross-layer attention.", "section": "3.3 TRANSFORMER WITH RESIDUAL VALUE"}, {"figure_path": "2410.17897/charts/charts_5_1.png", "caption": "Figure 4: Ablation study on sharing keys or values in every two layers, with CLAttention denoting sharing both.", "description": "The chart shows the ablation study on sharing keys or values in every two layers, comparing different information sharing methods with the relative training loss.", "section": "4.3 ABLATION STUDY OF RESIDUAL CONNECTION"}, {"figure_path": "2410.17897/charts/charts_6_0.png", "caption": "Figure 5: (Left) The relative training curve between a 82M ResFormer and Transformer across different training sequence lengths. (Middle) Average training loss for the final 50 steps across different model sizes and the corresponding fitted curves. (Right) The relative training curve across different model size for a fixed 2,048 training sequence length.", "description": "The chart displays the relative training loss curves of ResFormer and Transformer models across various training sequence lengths and model sizes, illustrating ResFormer's superior performance and training efficiency.", "section": "3.5 SVFORMER: SINGLE-LAYER VALUE FOR HALF KV CACHE"}, {"figure_path": "2410.17897/charts/charts_7_0.png", "caption": "Figure 6: Ablation study of adding residual connection to queries or keys.", "description": "The chart displays the relative training loss curves for models with residual connections added to queries, keys, and values, respectively, showing that adding a residual connection to the values provides the most benefit.", "section": "4.3 ABLATION STUDY OF RESIDUAL CONNECTION"}, {"figure_path": "2410.17897/charts/charts_8_0.png", "caption": "Figure 9: Left: Distribution of eigenvalues for the value vectors in the first layer of ResFormer and Transformer. Right: Maximum eigenvalue for each layer of ResFormer and Transformer.", "description": "The chart visualizes the distribution and maximum values of eigenvalues for value vectors in the first layer and across all layers of ResFormer and Transformer models, comparing their representational capacity.", "section": "4.5 VISUALIZATION OF RESFORMER"}, {"figure_path": "2410.17897/charts/charts_9_0.png", "caption": "Figure 10: The relative training loss for SVFormer and other KV efficient model compared with vanilla attention. The numbers in parentheses represent the training sequence length. Left: Model with nearly 1/2 KV cache. Right: Model with nearly 1/8 KV cache.", "description": "The chart compares the relative training loss of SVFormer against other KV-efficient methods (GQA and CLA) with different training sequence lengths and KV cache sizes.", "section": "4.6 SVFORMER vs. GQA"}, {"figure_path": "2410.17897/charts/charts_9_1.png", "caption": "Figure 11: Left: The relative training loss for SVFormer under different sequence lengths with a fixed batch size of 2M tokens. Right: Analysis of critical point, and we predict it for length 64,000 using linear regression with the last 1,000 data points.", "description": "The chart shows the relative training loss of SVFormer for different sequence lengths and predicts the critical point (training steps exceeded) based on linear regression.", "section": "4.1.1 TRAINING DETAILS"}, {"figure_path": "2410.17897/charts/charts_10_0.png", "caption": "Figure 12: The relative training loss for SVFormer under different hyper-parameter setting.", "description": "The chart displays the relative training loss curves of SVFormer under various hyperparameter settings (learning rate, warmup steps, model size, and architecture).", "section": "4.8 ABLATION STUDY OF SVFORMER"}, {"figure_path": "2410.17897/charts/charts_10_1.png", "caption": "Figure 13: Ablation study of sharing first layer's query(key) across all layers.", "description": "The chart displays an ablation study comparing the relative training loss when sharing different components (value, query, or key) from the first layer across all layers in a transformer model.", "section": "4.8 ABLATION STUDY OF SVFORMER"}, {"figure_path": "2410.17897/charts/charts_14_0.png", "caption": "Figure 15: (Left) The average token similarity of hidden states across layers in ResFormer vs. the vanilla Transformer. (Right) The average token similarity of hidden states across layers in Llama (8B) (Dubey et al., 2024) and Mistral (7B) (Jiang et al., 2023).", "description": "The chart displays the average token similarity of hidden states across layers for various transformer models, illustrating the over-smoothing effect in deeper layers.", "section": "A.1 TOKEN SIMILARITY ANALYSIS"}]