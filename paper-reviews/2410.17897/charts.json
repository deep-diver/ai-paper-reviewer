[{"figure_path": "2410.17897/charts/charts_2_0.png", "caption": "Figure 1: (Left) Illustration of the relative training loss (loss of target model - loss of vanilla Transformer) curve between different Transformer variants; model size is fixed to be 82M. (Middle) The average entropy of token importance across layers in ResFormer vs. the vanilla Transformer, where token importance is derived from the attention matrix. Lower entropy indicates more focused attention on specific tokens. More details can be found in Eqn. 11. (Right) The average entropy of token importance across layers in Llama (8B) (Dubey et al., 2024) and Mistral (7B) (Jiang et al., 2023).", "description": "This figure presents a comparison of three different Transformer models: DenseFormer, NeuTRENO, and ResFormer, with and without NeuTRENO integration. The left panel displays the relative training loss curves of the three models compared to a vanilla Transformer, showing that ResFormer consistently outperforms the others with the added NeuTRENO integration yielding further improvement.  The middle panel illustrates the average entropy of token importance across different layers for ResFormer and the vanilla Transformer. Lower entropy suggests that the model pays more attention to fewer tokens, indicating attention concentration; ResFormer exhibits lower entropy indicating better attention distribution. The right panel shows the average entropy of token importance across layers in two large language models, Llama and Mistral, illustrating a similar trend of attention concentration with increasing depth.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.17897/charts/charts_5_0.png", "caption": "Figure 3: Average token similarity between the outputs of different mapping methods and that of Eqn. 2.", "description": "The chart is a line graph comparing the average cosine similarity between different mapping methods and the output of Equation 2 across seven layers.  The \"Current Attention\" line shows the similarity between the results of using current attention as a mapping matrix and the results from Equation 2. The \"Identity Mapping\" line presents the similarity between identity mapping and Equation 2. The graph visually demonstrates that the current attention method shows better approximation of Equation 2 compared to the identity mapping method, suggesting that the former is a superior approach.", "section": "3.3 TRANSFORMER WITH RESIDUAL VALUE"}, {"figure_path": "2410.17897/charts/charts_5_1.png", "caption": "Figure 4: Ablation study on sharing keys or values in every two layers, with CLAttention denoting sharing both.", "description": "The chart displays the relative training loss curves for three different methods of sharing information across layers in a transformer model, namely CLAttention (sharing both keys and values), sharing only keys, and sharing only values. The x-axis represents the training steps, and the y-axis represents the relative training loss, calculated as the difference between the loss of a specific method and the loss of a vanilla transformer.  The CLAttention method shows a consistently lower relative training loss than sharing only keys or values throughout the training process, indicating that sharing both keys and values is more effective than sharing only one. Sharing only values demonstrates a significant improvement in the relative loss initially, but then plateaus at a small positive value, while sharing only keys displays a higher relative loss. The horizontal dashed line at 0 represents the performance of the vanilla transformer.", "section": "4.3 ABLATION STUDY OF RESIDUAL CONNECTION"}, {"figure_path": "2410.17897/charts/charts_6_0.png", "caption": "Figure 5: (Left) The relative training curve between a 82M ResFormer and Transformer across different training sequence lengths. (Middle) Average training loss for the final 50 steps across different model sizes and the corresponding fitted curves. (Right) The relative training curve across different model size for a fixed 2,048 training sequence length.", "description": "This figure presents three subplots visualizing the relative training performance of ResFormer against a vanilla Transformer under varying conditions. The left subplot shows relative training loss curves for both models across different training sequence lengths (2048, 32000, 64000), demonstrating ResFormer's consistent advantage. The middle subplot displays the average training loss over the final 50 training steps for various model sizes (2M, 82M, 180M, 468M), with fitted curves highlighting the trend of improved performance in ResFormer as model size increases. The right subplot illustrates the relative training loss curves for all model sizes with a fixed sequence length of 2048, further reinforcing ResFormer's superiority.", "section": "3.3 TRANSFORMER WITH RESIDUAL VALUE"}, {"figure_path": "2410.17897/charts/charts_7_0.png", "caption": "Figure 6: Ablation study of adding residual connection to queries or keys.", "description": "This chart presents an ablation study comparing the effects of adding residual connections to different components of the attention mechanism: queries, keys, and values.  The x-axis represents the training step, and the y-axis shows the relative training loss (the difference in training loss between the model with the modification and the baseline vanilla Transformer model). Three lines are plotted, each corresponding to a different component receiving the residual connection. The chart allows for a visual comparison of how adding a residual connection to each component individually impacts the training loss, providing insights into which modification is most beneficial for improving model performance.", "section": "4.3 ABLATION STUDY OF RESIDUAL CONNECTION"}, {"figure_path": "2410.17897/charts/charts_8_0.png", "caption": "Figure 9: Left: Distribution of eigenvalues for the value vectors in the first layer of ResFormer and Transformer. Right: Maximum eigenvalue for each layer of ResFormer and Transformer.", "description": "This figure presents a comparison of eigenvalue distributions for value vectors in the first layer and the maximum eigenvalue across all layers of both ResFormer and the vanilla Transformer.  The left panel displays the distribution of eigenvalues for the first layer, showing that ResFormer models (82M, 180M, and 468M) have significantly higher eigenvalues than their vanilla Transformer counterparts of the same size. The right panel shows the maximum eigenvalue for each layer across models of different sizes. It demonstrates that ResFormers consistently exhibit higher maximum eigenvalues than their vanilla Transformer counterparts throughout all layers.  These findings support the claim that ResFormer improves the quality of value vector representations.", "section": "4.5 VISUALIZATION OF RESFORMER"}, {"figure_path": "2410.17897/charts/charts_9_0.png", "caption": "Figure 10: The relative training loss for SVFormer and other KV efficient model compared with vanilla attention. The numbers in parentheses represent the training sequence length. Left: Model with nearly 1/2 KV cache. Right: Model with nearly 1/8 KV cache.", "description": "This figure presents a comparison of the relative training loss curves for various transformer models with different KV cache efficiency. The left panel shows the performance of SVFormer, GQA2, and CLA2 models when the training sequence length is 2048, demonstrating that SVFormer demonstrates lower loss compared to other models. The right panel shows the same models but with a sequence length of 64000.  Here SVFormer combined with GQA4 achieves lower loss than other models. The results highlight the impact of sequence length on the performance of these models, particularly for SVFormer.", "section": "4.6 SVFORMER vs. GQA"}, {"figure_path": "2410.17897/charts/charts_9_1.png", "caption": "Figure 11: Left: The relative training loss for SVFormer under different sequence lengths with a fixed batch size of 2M tokens. Right: Analysis of critical point, and we predict it for length 64,000 using linear regression with the last 1,000 data points.", "description": "This figure is composed of two sub-figures. The left sub-figure is a line chart showing the relative training loss for SVFormer at different sequence lengths (512, 2048, 8192, 32000, and 64000) against the training steps. Each line represents a different sequence length, showing how the relative training loss changes over the training steps for each. The right sub-figure is a scatter plot showing the relationship between the training sequence length and the critical point step. The critical point step is defined as the number of training steps at which SVFormer's training loss surpasses that of the vanilla Transformer.  A trendline is fitted to the scatter plot to show the exponential relationship between sequence length and critical point step.", "section": "4.7 OTHER FACTORS INFLUENCING SVFORMER"}, {"figure_path": "2410.17897/charts/charts_10_0.png", "caption": "Figure 12: The relative training loss for SVFormer under different hyper-parameter setting.", "description": "This figure presents an ablation study on the hyperparameters of the SVFormer model, including learning rate, warmup steps, model size, and architecture.  It consists of four sub-figures, each showing the relative training loss curves for different settings of a specific hyperparameter. (a) Learning Rate shows curves for peak learning rates of 1e-4, 3e-4, and 6e-4. (b) Warmup Steps compares curves with warmup steps of 1200 and 120. (c) Model Size contrasts curves for model sizes of 2M, 82M, 180M, and 468M parameters.  (d) Architecture displays curves for Llama 82M and GPT2 78M architectures. The dashed red lines represent the average relative training loss for a vanilla transformer model for reference.", "section": "4.8 ABLATION STUDY OF SVFORMER"}, {"figure_path": "2410.17897/charts/charts_10_1.png", "caption": "Figure 13: Ablation study of sharing first layer's query(key) across all layers.", "description": "This ablation study investigates the impact of sharing different components (value, query, and key) from the first layer across all layers in the model. The chart displays the relative training loss curves for three scenarios: sharing only values, sharing only queries, and sharing only keys from the first layer.  The horizontal dashed line indicates the relative training loss of the baseline model (vanilla transformer). The results show that sharing values from the first layer has minimal negative effect on training loss, while sharing queries or keys results in significant performance degradation, suggesting that sharing values is a more effective approach.", "section": "4.8 ABLATION STUDY OF SVFORMER"}, {"figure_path": "2410.17897/charts/charts_14_0.png", "caption": "Figure 15: (Left) The average token similarity of hidden states across layers in ResFormer vs. the vanilla Transformer. (Right) The average token similarity of hidden states across layers in Llama (8B) (Dubey et al., 2024) and Mistral (7B) (Jiang et al., 2023).", "description": "This figure presents a comparison of average token similarity across different transformer models. The left panel shows the average token similarity across layers for ResFormer, a vanilla transformer, and NeuTRENO, all with 468M parameters.  The right panel displays the average token similarity for Llama 8B V3.1, Llama 8B Instruct V3.1, Mistral 7B V0.2, and Mistral 7B Instruct V0.2 models across their respective layers.  Both panels illustrate the trend of token similarity over the layers of various transformer models, highlighting differences in how the models' attention mechanisms impact the similarity of token representations across layers.", "section": "A.1 TOKEN SIMILARITY ANALYSIS"}]