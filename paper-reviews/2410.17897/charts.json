[{"figure_path": "2410.17897/charts/charts_2_0.png", "caption": "Figure 1: (Left) Illustration of the relative training loss (loss of target model - loss of vanilla Transformer) curve between different Transformer variants; model size is fixed to be 82M. (Middle) The average entropy of token importance across layers in ResFormer vs. the vanilla Transformer, where token importance is derived from the attention matrix. Lower entropy indicates more focused attention on specific tokens. More details can be found in Eqn. 11. (Right) The average entropy of token importance across layers in Llama (8B) (Dubey et al., 2024) and Mistral (7B) (Jiang et al., 2023).", "description": "The chart illustrates the relative training loss, average entropy of token importance across layers in ResFormers vs. vanilla Transformers, and average entropy of token importance across layers in LLMs.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.17897/charts/charts_5_0.png", "caption": "Figure 3: Average token similarity between the outputs of different mapping methods and that of Eqn. 2.", "description": "The chart displays the average cosine similarity between outputs generated using different mapping methods (current attention and identity mapping) and the output from Equation 2, showing how well the approximation method preserves the original attention mechanism.", "section": "3.3 TRANSFORMER WITH RESIDUAL VALUE"}, {"figure_path": "2410.17897/charts/charts_5_1.png", "caption": "Figure 4: Ablation study on sharing keys or values in every two layers, with CLAttention denoting sharing both.", "description": "The chart displays the relative training loss curves for different methods of sharing keys and values in a transformer model, showing the impact of various sharing strategies on model performance.", "section": "4.3 ABLATION STUDY OF RESIDUAL CONNECTION"}, {"figure_path": "2410.17897/charts/charts_6_0.png", "caption": "Figure 5: (Left) The relative training curve between a 82M ResFormer and Transformer across different training sequence lengths. (Middle) Average training loss for the final 50 steps across different model sizes and the corresponding fitted curves. (Right) The relative training curve across different model size for a fixed 2,048 training sequence length.", "description": "The chart displays the relative training loss curves of ResFormer and Transformer models with varying sequence lengths and model sizes, showing ResFormer's consistent performance advantage.", "section": "3.5 SVFORMER: SINGLE-LAYER VALUE FOR HALF KV CACHE"}, {"figure_path": "2410.17897/charts/charts_7_0.png", "caption": "Figure 6: Ablation study of adding residual connection to queries or keys.", "description": "The chart displays the relative training loss curves for three different variations of ResFormer, each adding a residual connection to either the queries, keys, or values, to demonstrate the impact of adding residual connections on model training performance.", "section": "4.3 ABLATION STUDY OF RESIDUAL CONNECTION"}, {"figure_path": "2410.17897/charts/charts_8_0.png", "caption": "Figure 9: Left: Distribution of eigenvalues for the value vectors in the first layer of ResFormer and Transformer. Right: Maximum eigenvalue for each layer of ResFormer and Transformer.", "description": "The chart displays the distribution and maximum values of eigenvalues for value vectors in the first layer of ResFormer and Transformer models, illustrating differences in their representational capacity across layers.", "section": "4.5 VISUALIZATION OF RESFORMER"}, {"figure_path": "2410.17897/charts/charts_9_0.png", "caption": "Figure 10: The relative training loss for SVFormer and other KV efficient model compared with vanilla attention. The numbers in parentheses represent the training sequence length. Left: Model with nearly 1/2 KV cache. Right: Model with nearly 1/8 KV cache.", "description": "The chart displays the relative training loss curves of SVFormer, GQA, and CLA, with and without combinations, at two different sequence lengths, illustrating their training efficiency and KV cache usage.", "section": "4.6 SVFORMER vs. GQA"}, {"figure_path": "2410.17897/charts/charts_9_1.png", "caption": "Figure 11: Left: The relative training loss for SVFormer under different sequence lengths with a fixed batch size of 2M tokens. Right: Analysis of critical point, and we predict it for length 64,000 using linear regression with the last 1,000 data points.", "description": "The chart shows the relative training loss of SVFormer under different sequence lengths and the relationship between the critical point (training steps exceeded) and sequence length.", "section": "4.7 OTHER FACTORS INFLUENCING SVFORMER"}, {"figure_path": "2410.17897/charts/charts_10_0.png", "caption": "Figure 12: The relative training loss for SVFormer under different hyper-parameter setting.", "description": "The chart displays the relative training loss curves of SVFormer under different hyperparameter settings, including learning rate, warmup steps, model size, and architecture.", "section": "4.8 ABLATION STUDY OF SVFORMER"}, {"figure_path": "2410.17897/charts/charts_10_1.png", "caption": "Figure 13: Ablation study of sharing first layer's query(key) across all layers.", "description": "The chart displays an ablation study showing the effects of sharing the first layer's queries or keys on model performance across all layers.", "section": "4.8 ABLATION STUDY OF SVFORMER"}, {"figure_path": "2410.17897/charts/charts_14_0.png", "caption": "Figure 15: (Left) The average token similarity of hidden states across layers in ResFormer vs. the vanilla Transformer. (Right) The average token similarity of hidden states across layers in Llama (8B) (Dubey et al., 2024) and Mistral (7B) (Jiang et al., 2023).", "description": "The chart displays the average token similarity of hidden states across layers in Resformer, vanilla Transformer, Llama, and Mistral, illustrating the over-smoothing effect in deep networks.", "section": "A.1 TOKEN SIMILARITY ANALYSIS"}]