[{"figure_path": "2410.17897/figures/figures_3_0.png", "caption": "Figure 2: Simplified illustration of the vanilla Transformer, NeuTRENO, DenseFormer, ResFormer, and SVFormer, with only three-layer structures and no operations other than attention. A\u00b2, Vi, and H\u00b2 denote the attention matrix, value vectors, and attention outputs at the i-th layer, respectively. \u2295, \u2212, and \u2297 represent standard matrix addition, subtraction, and multiplication, respectively.", "description": "Figure 2 simplifies the architecture of five different Transformer variants, showing the key differences in their information flow and operations.", "section": "METHOD"}]