[{"figure_path": "2410.17897/figures/figures_3_0.png", "caption": "Simplified illustration of the vanilla Transformer, NeuTRENO, DenseFormer, ResFormer, and SVFormer, with only three-layer structures and no operations other than attention. A\u00b2, Vi, and H\u00b2 denote the attention matrix, value vectors, and attention outputs at the i-th layer, respectively. \u2295, \u2212, and \u2297 represent standard matrix addition, subtraction, and multiplication, respectively.", "description": "The figure presents a simplified illustration comparing five different transformer architectures: vanilla Transformer, NeuTRENO, DenseFormer, ResFormer, and SVFormer. Each architecture is shown with only three layers to highlight the key differences in their designs.  The vanilla Transformer shows the standard attention mechanism where the query, key, and value vectors are processed independently within each layer. NeuTRENO incorporates the difference between the value vectors of the first layer and the current layer into the attention output. DenseFormer allows each layer to access hidden states from all preceding layers. ResFormer adds a residual connection from the values of the first layer to all subsequent layers to approximate cross-layer attention. Finally, SVFormer shares the value embedding from the first layer across all layers, reducing the KV cache.", "section": "Method"}]