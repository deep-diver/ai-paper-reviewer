[{"figure_path": "2410.17897/tables/table_8_0.html", "caption": "Table 1: Zero-shot accuracy on commonsense reasoning tasks.", "description": "Table 1 presents the zero-shot accuracy of different models on several commonsense reasoning tasks, comparing the vanilla Transformer and ResFormer with varying sequence lengths.", "section": "4.4 DOWNSTREAM EVALUATIONS"}, {"figure_path": "2410.17897/tables/table_14_0.html", "caption": "Table 2: The details of pre-train dataset.", "description": "Table 2 presents the data sources, proportions, and number of tokens used for pretraining the language model.", "section": "4 PRETRAIN EXPERIMENTS"}, {"figure_path": "2410.17897/tables/table_15_0.html", "caption": "Table 5: Validation loss on slimpajama.", "description": "Table 5 presents the validation loss for different models on the whole validation split of slimpajama.", "section": "A.4 VALIDATION LOSS ON SLIMPAJAMA"}, {"figure_path": "2410.17897/tables/table_15_1.html", "caption": "Table 4: Training details for models with different size.", "description": "This table shows the training details of the ResFormer and vanilla Transformer models with different sizes, including the number of layers, attention heads, hidden dimensions, FFN dimensions, and other hyperparameters.", "section": "4.1 SETTING"}, {"figure_path": "2410.17897/tables/table_15_2.html", "caption": "Table 5: Validation loss on slimpajama.", "description": "Table 5 presents the validation loss for different models on the whole validation split of slimpajama dataset, comparing vanilla transformer and resformer models of different sizes.", "section": "A.4 VALIDATION LOSS ON SLIMPAJAMA"}]