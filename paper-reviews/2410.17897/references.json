{"references": [{" publication_date": "2017", "fullname_first_author": "A Vaswani", "paper_title": "Attention is all you need", "reason": "This paper introduced the Transformer architecture, which is the foundation for the current work and a highly influential model in the field of deep learning.  Its impact is pervasive, making it a fundamental reference for any work involving Transformers.", "section_number": 1}, {" publication_date": "2021", "fullname_first_author": "Daquan Zhou", "paper_title": "Deepvit: Towards deeper vision transformer", "reason": "This paper directly addresses the problem of over-smoothing in deep Transformers, a central challenge that this work also tackles. It provides context and helps to explain the background of the problem being addressed.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Han Shi", "paper_title": "Revisiting over-smoothing in bert from the perspective of graph", "reason": "This paper contributes to the understanding of the over-smoothing phenomenon in deep Transformers, focusing on the impact of the self-attention mechanism. It provides valuable context for analyzing the phenomenon.", "section_number": 1}, {" publication_date": "2016", "fullname_first_author": "Kaiming He", "paper_title": "Deep residual learning for image recognition", "reason": "This paper introduced the ResNet architecture, which is foundational to many modern deep learning models, including the Transformer.  Its impact on the use of residual connections in deep architectures is significant and relevant to the methods proposed in this work.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Tam Nguyen", "paper_title": "Mitigating over-smoothing in transformers via regularized nonlocal functionals", "reason": "This work directly tackles the problem of over-smoothing, proposing methods to alleviate it. This makes it a crucial reference for understanding the challenges and existing solutions to over-smoothing.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Jordan Hoffmann", "paper_title": "Training compute-optimal large language models", "reason": "This paper examines scaling laws in large language models, which is directly relevant to the motivation behind this work. The study of scaling laws provides crucial context for understanding the challenges and opportunities in scaling Transformer models.", "section_number": 1}, {" publication_date": "2020", "fullname_first_author": "Jared Kaplan", "paper_title": "Scaling laws for neural language models", "reason": "This paper introduced scaling laws for large language models, which are relevant to the research context. Understanding scaling laws is crucial for making informed decisions about model architecture and training.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Jackson Petty", "paper_title": "The impact of depth on compositional generalization in transformer language models", "reason": "This paper investigates the effects of depth on the generalization ability of Transformers, which is crucial for evaluating methods designed to improve performance in deeper networks.  This work helps to establish the theoretical context around over-smoothing and its potential impact.", "section_number": 1}, {" publication_date": "2016", "fullname_first_author": "Gao Huang", "paper_title": "Deep networks with stochastic depth", "reason": "This work introduced stochastic depth, a regularization technique commonly used in deep learning to improve generalization and prevent overfitting.  This method is relevant to the problem of over-smoothing in Transformers.", "section_number": 2}, {" publication_date": "2017", "fullname_first_author": "Gao Huang", "paper_title": "Densely connected convolutional networks", "reason": "This paper introduced DenseNet, an architecture with dense connections that improve information flow in deep networks, relevant to the methods that aim to enhance information propagation in Transformers. The design philosophy of DenseNet, involving connections across layers, is comparable to the methods proposed in this paper.", "section_number": 2}, {" publication_date": "2016", "fullname_first_author": "Kaiming He", "paper_title": "Deep residual learning for image recognition", "reason": "This paper introduced ResNet, which has significantly impacted the field of deep learning by proposing residual connections to improve information flow in very deep networks.  This work is a crucial reference because of its contribution to the concept of shortcut connections which is also relevant to this research.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Matteo Pagliardini", "paper_title": "Enhancing information flow in transformers via depth weighted averaging", "reason": "This paper explores methods to enhance information flow within Transformers, directly addressing a challenge relevant to this research. Understanding existing solutions for information flow improvement helps to contextualize the proposed methods.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "William Brandon", "paper_title": "Reducing transformer key-value cache size with cross-layer attention", "reason": "This work addresses the KV cache compression problem, which is a significant concern in deploying large Transformer models.  It provides a detailed overview of relevant approaches and helps to contextualize the methods proposed in this work.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Abhimanyu Dubey", "paper_title": "The llama 3 herd of models", "reason": "This paper introduces several large language models that are relevant to the experiments in this paper. The use of these LLMs for comparison helps to establish benchmarks for the performance of the proposed methods and contextualize the findings.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Albert Q Jiang", "paper_title": "Mistral 7b", "reason": "This paper introduces the Mistral 7B language model, which is used in the experiments in this paper as a baseline for comparison.  It is directly relevant to the evaluation of the proposed methods.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Tianwen Wei", "paper_title": "Skywork: A more open bilingual foundation model", "reason": "This paper introduces a new bilingual foundation model, which is used in the experiments in this paper for comparison. The use of Skywork adds to the range of models considered, enriching the findings and their implications.", "section_number": 4}, {" publication_date": "2020", "fullname_first_author": "Tom B Brown", "paper_title": "Language models are few-shot learners", "reason": "This is a highly influential paper in the field of large language models, foundational to much of the current research on the subject. This paper\u2019s contribution helps establish the broader context of the current research.", "section_number": 4}, {" publication_date": "2018", "fullname_first_author": "Jacob Devlin", "paper_title": "Bert: Pre-training of deep bidirectional transformers for language understanding", "reason": "BERT is a highly influential and widely used model architecture which forms the basis for many of the current large language models. This makes this paper a seminal work in the field, providing key context and background information.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Yuhui Xu", "paper_title": "Think: Thinner key cache by query-driven pruning", "reason": "This paper explores methods for compressing the KV cache in Transformer models, an important optimization that is directly addressed by the proposed SVFormer method in this paper.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Daria Soboleva", "paper_title": "SlimPajama: A 627B token cleaned and deduplicated version of RedPajama", "reason": "This paper introduces the SlimPajama dataset, which is used in this work's pretraining experiments.  The description of the dataset and its properties is crucial for understanding the context and results of the experiments.", "section_number": 4}]}