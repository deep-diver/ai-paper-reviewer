{"references": [{" publication_date": "2017", "fullname_first_author": "A Vaswani", "paper_title": "Attention is all you need", "reason": "This paper is foundational to the entire field of Transformer models, introducing the self-attention mechanism that is central to the current work.  The paper's impact is immense, as it laid the groundwork for many subsequent advances in NLP and other domains.", "section_number": 1}, {" publication_date": "2018", "fullname_first_author": "Jacob Devlin", "paper_title": "Bert: Pre-training of deep bidirectional transformers for language understanding", "reason": "BERT is a highly influential model that significantly advanced NLP capabilities and demonstrated the effectiveness of pre-training large language models. Its techniques and architectures have impacted subsequent research, including this work's focus on mitigating over-smoothing problems in deep Transformers.", "section_number": 1}, {" publication_date": "2019", "fullname_first_author": "Z Lan", "paper_title": "Albert: A lite bert for self-supervised learning of language representations", "reason": "ALBERT is a significant model that improved upon BERT by reducing model size and training costs, addressing scalability challenges that are also relevant to this paper's focus on deeper Transformer models.", "section_number": 1}, {" publication_date": "2020", "fullname_first_author": "Tom B Brown", "paper_title": "Language models are few-shot learners", "reason": "This paper presented important scaling laws for language models and highlighted the impact of model size on performance, influencing current research on efficient large language models.", "section_number": 1}, {" publication_date": "2020", "fullname_first_author": "Jared Kaplan", "paper_title": "Scaling laws for neural language models", "reason": "This work is crucial in understanding the relationship between model size, training compute, and performance.  The scaling laws presented directly inform efforts to train efficient and large models.", "section_number": 1}, {" publication_date": "2021", "fullname_first_author": "Daquan Zhou", "paper_title": "Deepvit: Towards deeper vision transformer", "reason": "This paper directly addresses the problem of training deep Vision Transformers and informs the current work's goal of training more efficient and deeper Transformer models.  The paper's focus on the challenges of over-smoothing is particularly relevant.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Jordan Hoffmann", "paper_title": "Training compute-optimal large language models", "reason": "This work focuses on efficiently training large language models.  The findings and techniques discussed are particularly relevant to the current paper, which similarly addresses the challenges of training deep Transformer models.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Tam Nguyen", "paper_title": "Mitigating over-smoothing in transformers via regularized nonlocal functionals", "reason": "NeuTRENO is a key related method that directly tackles the problem of over-smoothing in Transformers. This paper's method is compared against NeuTRENO, highlighting its importance in the field.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Matteo Pagliardini", "paper_title": "Enhancing information flow in transformers via depth weighted averaging", "reason": "DenseFormer is another key related method that improves the flow of information across layers in Transformers.  The paper's method is compared against DenseFormer, showing its relevance and influence.", "section_number": 2}, {" publication_date": "2016", "fullname_first_author": "Kaiming He", "paper_title": "Deep residual learning for image recognition", "reason": "ResNet is foundational to the field of deep learning, introducing residual connections to mitigate the vanishing gradient problem in deep networks.  The current work's use of residual connections for efficient cross-layer attention builds upon this work.", "section_number": 2}, {" publication_date": "2016", "fullname_first_author": "Gao Huang", "paper_title": "Deep networks with stochastic depth", "reason": "Stochastic Depth is an important technique to prevent overfitting in very deep networks.  The idea of randomly dropping layers during training is implicitly related to the problem of over-smoothing in deep Transformers.", "section_number": 2}, {" publication_date": "2017", "fullname_first_author": "Gao Huang", "paper_title": "Densely connected convolutional networks", "reason": "DenseNet's approach to connecting layers directly is conceptually related to the proposed methods, which aim to enhance information flow in Transformers by promoting more efficient communication between layers.", "section_number": 2}, {" publication_date": "2019", "fullname_first_author": "Noam Shazeer", "paper_title": "Fast transformer decoding: One write-head is all you need", "reason": "This paper explores efficient techniques for transformer decoding, impacting the field of large-scale language model inference, which is implicitly relevant to this paper's focus on memory-efficient methods.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "Noam Shazeer", "paper_title": "Glu variants improve transformer", "reason": "This paper introduces the GLU activation function, which is used in the SlimPajama architecture used in the experiments of this work.", "section_number": 4}, {" publication_date": "2020", "fullname_first_author": "Jared Kaplan", "paper_title": "Scaling laws for neural language models", "reason": "This paper is highly influential in understanding the scaling properties of language models.  The presented scaling laws affect the design and training of current large language models.", "section_number": 4}, {" publication_date": "2021", "fullname_first_author": "Zihang Dai", "paper_title": "Transformer-xl: Attentive language models beyond a fixed-length context", "reason": "Transformer-XL is a significant model that addressed the limitations of standard transformers with respect to handling long sequences.  This relates to the current work's discussion of KV cache.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Joshua Ainslie", "paper_title": "Gqa: Training generalized multi-query transformer models from multi-head checkpoints", "reason": "GQA is a key method used for KV cache compression in Transformers, which is directly related to the efficiency focus of SVFormer.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Abhimanyu Dubey", "paper_title": "The llama 3 herd of models", "reason": "Llama 3 is a leading large language model whose architecture and training are relevant to the current work, especially in the context of efficiency and scaling.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "William Brandon", "paper_title": "Reducing transformer key-value cache size with cross-layer attention", "reason": "This paper proposes a method for reducing the size of the KV cache in Transformers, directly addressing a problem that SVFormer also tackles.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Jackson Petty", "paper_title": "The impact of depth on compositional generalization in transformer language models", "reason": "This paper is significant because it directly addresses the issues of compositional generalization, which is affected by the over-smoothing phenomenon addressed in the main paper. The findings here inform the discussion and motivate the need for the proposed methods.", "section_number": 2}]}