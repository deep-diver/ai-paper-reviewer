[{"content": "| #Steps | Index of the sampled timesteps | PSNR \u2191 | SSIM \u2191 | LPIPS \u2193 | NIQE \u2193 | PI \u2193 | CLIPIQA \u2191 | MUSIQ \u2191 |\n|---|---|---|---|---|---|---|---|---|\n| 9 |  |  |  |  |  |  |  |  |\n| 5 | {250, 200, 150, 100, 50} | 22.70 | 0.6412 | 0.2844 | 4.8757 | 3.4744 | 0.6733 | 69.8427 |\n| 3 | {250, 150, 50} | 22.92 | 0.6478 | 0.2762 | 4.7980 | 3.4002 | 0.6823 | 70.4688 |\n|  | {200, 100, 50} | 23.41 | 0.6609 | 0.2648 | 4.5089 | 3.2074 | 0.6851 | 70.7024 |\n|  | {150, 100, 50} | 23.84 | 0.6713 | 0.2575 | 4.2719 | 3.0527 | 0.6823 | 70.4569 |\n| 1 | {250} | 23.84 | 0.6713 | 0.2575 | 4.5287 | 3.1748 | 0.7132 | 72.5773 |\n|  | {200} | 24.14 | 0.6789 | 0.2517 | 4.3815 | 3.0866 | 0.7093 | 72.2909 |\n|  | {150} | 24.42 | 0.6851 | 0.2469 | 4.2194 | 2.9979 | 0.7019 | 71.7100 |\n|  | {100} | 24.66 | 0.6891 | 0.2450 | 4.0606 | 2.8951 | 0.6912 | 70.8251 |", "caption": "Table 1: Quantitative results of InvSR with various numbers of sampling steps ranging from one to five on the ImageNet-Test dataset.", "description": "This table presents a quantitative evaluation of the proposed InvSR (Arbitrary-steps Image Super-resolution via Diffusion Inversion) method.  It shows the performance of InvSR across different numbers of sampling steps (1, 3, 5) on the ImageNet-Test dataset.  For each sampling step configuration, the results are reported for several different subsets of timesteps used during the sampling process.  The metrics used to evaluate InvSR include Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index (SSIM), Learned Perceptual Image Patch Similarity (LPIPS), Natural Image Quality Evaluator (NIQE), Perceptual Index (PI),  CLIP-based Image Quality Assessment (CLIPIQA), and Multi-Scale Image Quality (MUSIQ).  Higher values for PSNR, SSIM, CLIPIQA, and MUSIQ indicate better performance, while lower values for LPIPS and NIQE are preferable.  This allows for a comprehensive analysis of InvSR's performance under different sampling strategies and provides insights into the trade-off between computational cost and image quality.", "section": "4. Experiments"}, {"content": " | Datasets | Methods | PSNR \u2191 | SSIM \u2191 | LPIPS \u2193 | NIQE \u2193 | PI \u2193 | CLIPIQA \u2191 | MUSIQ \u2191 | #Params (M) | \n|---|---|---|---|---|---|---|---|---|---|---|\n| ImageNet-Test | BSRGAN [73] | 27.05 | 0.7453 | 0.2437 | 4.5345 | 3.7111 | 0.5703 | 67.7195 | 16.70 |\n|  | RealESRGAN [55] | 26.62 | 0.7523 | 0.2303 | 4.4909 | 3.7234 | 0.5090 | 64.8186 | 16.70 |\n| ImageNet-Test | LDM-50 [43] | 27.19 | 0.7285 | 0.2286 | 5.2411 | 4.2554 | 0.5554 | 62.8776 | 113.60 |\n|  | StableSR-50 [53] | 24.77 | 0.6908 | 0.2591 | 4.5120 | 3.1473 | 0.7067 | 71.2811 | 152.70 |\n|  | DiffBIR-50 [30] | 25.72 | 0.6695 | 0.2795 | 4.5875 | 3.2260 | 0.6900 | 69.7089 | 385.43 |\n|  | SeeSR-50 [60] | 26.69 | 0.7422 | 0.2187 | 4.3825 | 3.4742 | 0.5868 | 71.2412 | 751.50 |\n| RealSR | BSRGAN [73] | 26.51 | 0.7746 | 0.2685 | 4.6501 | 4.4644 | 0.5439 | 63.5869 | 16.70 |\n|  | RealESRGAN [55] | 25.85 | 0.7734 | 0.2728 | 4.6766 | 4.4881 | 0.4898 | 59.6803 | 16.70 |\n|  | LDM-50 [43] | 26.75 | 0.7711 | 0.2945 | 4.8712 | 5.0025 | 0.4907 | 54.3910 | 113.60 |\n|  | StableSR-50 [53] | 26.27 | 0.7755 | 0.2671 | 5.1745 | 4.8209 | 0.5209 | 60.1758 | 152.70 |\n|  | DiffBIR-50 [30] | 24.83 | 0.6642 | 0.3864 | 3.7366 | 3.3661 | 0.6857 | 65.3934 | 385.43 |\n|  | SeeSR-50 [60] | 26.20 | 0.7555 | 0.2806 | 4.5358 | 4.1464 | 0.6824 | 66.3757 | 751.50 |\n|  | ResShift-4 [69] | 25.77 | 0.7453 | 0.3395 | 6.9113 | 5.4013 | 0.5994 | 57.5536 | 118.59 |\n|  | SinSR-1 [57] | 26.98 | 0.7304 | 0.2209 | 5.2623 | 3.8189 | 0.6618 | 67.7593 | 118.59 |\n|  | OSEDiff-1 [59] | 23.95 | 0.6756 | 0.2624 | 4.7157 | 3.3775 | 0.6818 | 70.3928 | 8.50 |\n|  | InvSR-1 (Ours) | 24.14 | 0.6789 | 0.2517 | 4.3815 | 3.0866 | 0.7093 | 72.2900 | 33.84 |\n|  | InvSR-1 (Ours) | 24.50 | 0.7262 | 0.2872 | 4.2189 | 3.7779 | 0.6918 | 67.4586 | 33.84 |", "caption": "Table 2: Quantitative comparisons of different methods on ImageNet-Test and RealSR. The number of sampling steps is marked in the format of \u201cMethod name-Steps\u201d for diffusion-based methods. The best and second-best results are highlighted in bold and underlined.", "description": "Table 2 presents a quantitative comparison of several image super-resolution (SR) methods on two datasets: ImageNet-Test and RealSR.  The table evaluates performance using a range of metrics, including PSNR, SSIM, LPIPS, NIQE, PI, CLIPIQA, and MUSIQ. For diffusion-based methods, the number of sampling steps used is indicated, allowing for a comparison of performance across various approaches with differing computational costs. The best and second-best results for each metric on each dataset are highlighted for easy identification.", "section": "4. Experiments"}, {"content": "| Methods | NIQE \u2193 | PI \u2193 | CLIPIQA \u2191 | MUSIQ \u2191 |\n|---|---|---|---|---|\n| 5 |  |  |  |  |\n| BSRGAN [73] | 4.4408 | 4.0276 | 0.6263 | 66.6288 |\n| RealESRGAN [55] | 4.1568 | 3.8852 | 0.6189 | 64.4957 |\n| LDM-50 [43] | 4.3248 | 4.2545 | 0.5511 | 55.8246 |\n| StableSR-50 [53] | 4.5593 | 4.0977 | 0.6214 | 62.7613 |\n| DiffBIR-50 [30] | **3.8630** | **3.2117** | **0.7404** | 67.9806 |\n| SeeSR-50 [60] | 4.3678 | 3.7429 | 0.7114 | **69.7658** |\n| ResShift-4 [69] | 5.9866 | 4.8318 | 0.6515 | 61.7967 |\n| SinSR-1 [57] | 5.6243 | 4.2830 | 0.7228 | 64.0573 |\n| OSEDiff-1 [59] | 4.3457 | 3.8219 | 0.7093 | 68.8202 |\n| *InvSR-1 (Ours)* | **4.0284** | **3.4666** | **0.7291** | 69.8055 |", "caption": "Table 3: Quantitative comparisons of various methods on RealSet80. The number of sampling steps is marked in the format of \u201cMethod name-Steps\u201d for diffusion-based methods. The best and second-best results are highlighted in bold and underlined.", "description": "Table 3 presents a quantitative comparison of different image super-resolution (SR) methods on the RealSet80 dataset.  The table shows performance metrics for various methods, including both traditional GAN-based methods (BSRGAN, RealESRGAN) and diffusion-based methods (LDM, StableSR, DiffBIR, SeeSR, ResShift, SinSR, OSEDiff, and InvSR).  For diffusion-based methods, the number of sampling steps used is explicitly stated in the method name (e.g., LDM-50 indicates 50 sampling steps).  The metrics used to evaluate the SR performance include several no-reference image quality metrics (NIQE, PI, CLIPIQA, MUSIQ). The best and second-best performing methods for each metric are highlighted in bold and underlined for easy comparison.  This table provides a quantitative assessment of the relative strengths and weaknesses of various SR approaches, specifically highlighting how the proposed InvSR method performs compared to other state-of-the-art techniques.", "section": "4. Experiments"}, {"content": "| Base models | #Steps | Index of the sampled timesteps | PSNR\u2191 | SSIM\u2191 | LPIPS\u2193 | NIQE\u2193 | PI\u2193 | CLIPIQA\u2191 | MUSIQ\u2191 |\n|---|---|---|---|---|---|---|---|---|---| \n| 10 |  |  |  |  |  |  |  |  |  |\n| SD-Turbo | 5 | {250, 200, 150, 100, 50} | 22.70 | 0.6412 | 0.2844 | 4.8757 | 3.4744 | 0.6733 | 69.8427 |\n| SD-2.0 |  |  | 21.40 | 0.6063 | 0.3274 | 5.1508 | 3.8709 | 0.6467 | 67.6056 |\n| SD-Turbo | 3 | {150, 100, 50} | 23.84 | 0.6713 | 0.2575 | 4.2719 | 3.0527 | 0.6823 | 70.4569 |\n| SD-2.0 |  |  | 23.13 | 0.6566 | 0.2776 | 4.2449 | 3.1467 | 0.6722 | 69.5178 |\n| SD-Turbo | 1 | {200} | 24.14 | 0.6789 | 0.2517 | 4.3815 | 3.0866 | 0.7093 | 72.2909 |\n| SD-2.0 |  |  | 23.36 | 0.6637 | 0.2647 | 4.3304 | 3.1545 | 0.6969 | 71.4974 |", "caption": "Table 4: Quantitative comparisons of the proposed InvSR equipped with two different based models, namely SD-2.0 and SD-Turbo, on the dataset of ImageNet-Test.", "description": "This table presents a quantitative comparison of the performance of the proposed InvSR model using two different pre-trained diffusion models as its base: Stable Diffusion 2.0 and Stable Diffusion Turbo. The comparison is done on the ImageNet-Test dataset and uses several metrics such as PSNR, SSIM, LPIPS, NIQE, PI, CLIPIQA, and MUSIQ to evaluate the image quality of super-resolution results.  The results are shown for different numbers of sampling steps to demonstrate the effect of this parameter on the final output.  This helps in understanding how the choice of base model and the number of sampling steps affect the performance of the InvSR model.", "section": "4. Experiments"}, {"content": "| Methods | #Steps | Index of the sampled timesteps | PSNR \u2191 | SSIM \u2191 | LPIPS \u2193 | NIQE \u2193 | PI \u2193 | CLIPIQA \u2191 | MUSIQ \u2191 |\n|---|---|---|---|---|---|---|---|---|---| \n| 10 |  |  |  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |  |  |  |\n| *InvSR* | 5 | {250, 200, 150, 100, 50} | 22.70 | 0.6412 | 0.2844 | 4.8757 | 3.4744 | 0.6733 | 69.8427 |\n| *InvSR-Int* |  |  | 22.70 | 0.6412 | 0.2844 | 4.8785 | 3.4718 | 0.6734 | 69.8466 |\n|  |  |  |  |  |  |  |  |  |  |", "caption": "Table 5: Quantitative comparisons of InvSR to the baseline method InvSR-Int that combines an additional noise predictor for the intermediate timesteps on the dataset of ImageNet-Test.", "description": "This table presents a quantitative comparison of two methods on the ImageNet-Test dataset: InvSR and InvSR-Int.  InvSR is the proposed method, while InvSR-Int is a modified version that incorporates an extra noise predictor for intermediate steps in the diffusion process. The comparison uses several metrics (PSNR, SSIM, LPIPS, NIQE, PI, CLIPIQA, and MUSIQ) to evaluate the performance of both methods, which helps assess the impact of adding the extra noise predictor for intermediate steps on the overall image quality.", "section": "4. Experiments"}, {"content": "| Methods |  \u03bb<sub>l</sub> (LPIPS loss) | \u03bb<sub>g</sub> (GAN loss) | PSNR \u2191 | SSIM \u2191 | LPIPS \u2193 | NIQE \u2193 | PI \u2193 | CLIPIQA \u2191 | MUSIQ \u2191 |\n|---|---|---|---|---|---|---|---|---|---| \n| 10 |  |  | 26.71 | 0.7365 | 0.2850 | 9.2792 | 6.4147 | 0.6168 | 64.6069 |\n| Baseline1 | 0.0 | 0.0 | 26.71 | 0.7365 | 0.2850 | 9.2792 | 6.4147 | 0.6168 | 64.6069 |\n| Baseline2 | 2.0 | 0.0 | 26.24 | 0.7274 | 0.2841 | 8.4367 | 5.7973 | 0.6501 | 66.1726 |\n| Baseline3 | 0.0 | 0.1 | 24.11 | 0.6809 | 0.2599 | 4.4518 | 3.1229 | 0.7078 | 72.5045 |\n| *InvSR-1* | 2.0 | 0.1 | 24.14 | 0.6789 | 0.2517 | 4.3815 | 3.0866 | 0.7093 | 72.2909 |", "caption": "Table 6: Quantitative ablation studies on the loss function in Eq.\u00a0(11), wherein the hyper-parameters \u03bblsubscript\ud835\udf06\ud835\udc59\\lambda_{l}italic_\u03bb start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT and \u03bbgsubscript\ud835\udf06\ud835\udc54\\lambda_{g}italic_\u03bb start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT control the weight importance of the LPIPS loss and the GAN loss, respectively.", "description": "This table presents an ablation study on the impact of different loss functions used in training the noise predictor model.  It shows the results of using different combinations of the LPIPS (Learned Perceptual Image Patch Similarity) loss and the GAN (Generative Adversarial Network) loss, quantified by their respective hyperparameters \u03bbl (lambda_l) and \u03bbg (lambda_g).  The study evaluates the effectiveness of each loss combination on several key metrics, such as PSNR, SSIM, LPIPS, NIQE, PI, CLIPIQA, and MUSIQ, to determine the optimal configuration for achieving high-quality image super-resolution.", "section": "3.2.3 Model Training"}, {"content": "| Metrics | Methods |  |  |  |  |  |  |  |  |\n|---|---|---|---|---|---|---|---|---|---| \n| 10 <span class=\"ltx_rule\" style=\"width:100%;height:0.4pt;background:black;display:inline-block;\">\u00a0</span> |  |  |  |  |  |  |  |  |  |\n|  | BSRGAN | RealESRGAN | StableSR-50 | DiffBIR-50 | SeeSR-50 | ResShift-4 | SinSR-1 | OSEDiff-1 | InvSR-1 |\n| #Params (M) | 16.70 | 16.70 | 152.70 | 385.43 | 751.50 | 118.59 | 118.59 | 8.50 | 33.84 |\n| Runtime (ms) | 65 | 65 | 3459 | 7937 | 6438 | 319 | 138 | 176 | 117 |", "caption": "Table 7: Efficiency comparisons of different methods on the x4 (128\u2192512\u2192128512128\\rightarrow 512128 \u2192 512) SR task, where the runtime results are tested on an NVIDIA A100 GPU with 40GB memory. For diffusion-based SR approaches, the number of sampling steps is annotated in the format of \u201cMethod name-Steps\u201d.", "description": "Table 7 presents a comparison of the computational efficiency of various image super-resolution (SR) methods.  The comparison focuses on the speed of the algorithms when performing a 4x upscaling task (increasing the resolution from 128x128 to 512x512 pixels). The runtime was measured using an NVIDIA A100 GPU with 40GB of memory. For methods that utilize diffusion models, the number of sampling steps employed is also specified. This table helps to illustrate the trade-off between model complexity (as indicated by the number of parameters), computational time, and the quality of super-resolution achieved.", "section": "4. Experiments"}]