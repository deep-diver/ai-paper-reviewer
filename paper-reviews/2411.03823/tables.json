[{"content": "| Model | ScienceQA Training Set |  |  |  | ScienceQA Test Set |  |  |  | MMStar Validation Set |  |  |  |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| Metric | CR | PCR | \u0394 | IL | CR | PCR | \u0394 | IL | CR | PCR | \u0394 | IL |\n| Open-source MLLMs |  |  |  |  |  |  |  |  |  |  |  |  |\n| LLaVA-1.5-7B | 59.7 | 58.6 | -1.1 | \u2013 | 60.3 | 61.6 | 1.3 | 10.5 | 38.9 | 41.7 | 2.8 | 11.0 |\n| VILA1.5-3B | 57.7 | 58.3 | 0.6 | **14.5** | 60.3 | 59.8 | -0.5 | 14.8 | 38.6 | 37.6 | -1.0 | \u2013 |\n| Qwen-VL-Chat | 58.4 | 60.8 | 2.5 | 13.3 | 60.3 | 60.4 | 0.1 | 13.7 | 40.9 | 44.2 | 3.3 | 13.2 |\n| fuyu-8b | 36.5 | 37.5 | 1.0 | 13.4 | 37.4 | 36.9 | -0.5 | **14.9** | 28.2 | 27.0 | **-1.2** | \u2013 |\n| idefics2-8b | 85.1 | 84.0 | -1.2 | \u2013 | 84.0 | 84.3 | 0.3 | 2.8 | 48.2 | 49.3 | 1.1 | 7.9 |\n| Phi-3-vision-128k-instruct | 90.5 | 90.4 | -0.1 | 4.6 | 88.4 | 89.1 | 0.7 | 3.9 | 48.7 | 51.9 | 3.2 | 7.2 |\n| Yi-VL-6B | 60.5 | 61.8 | 1.3 | 10.0 | 59.5 | 61.3 | 1.8 | 9.6 | 38.8 | 44.0 | 5.2 | 9.3 |\n| InternVL2-8B | 94.1 | 93.9 | -0.3 | 2.0 | 92.3 | 93.1 | 0.8 | 1.7 | 56.9 | 60.1 | 3.2 | 5.1 |\n| Proprietary MLLMs |  |  |  |  |  |  |  |  |  |  |  |  |\n| GPT-4o | 69.9 | 70.0 | 0.1 | 2.7 | 69.1 | 69.7 | 0.6 | 2.8 | 48.6 | 50.5 | 1.9 | 9.4 |\n| Gemini-1.5-Pro | 68.5 | 67.9 | -0.6 | 6.6 | 66.5 | 66.2 | -0.3 | 7.1 | 45.7 | 45.5 | -0.2 | 9.9 |\n| Claude-3.5-Sonnet | 70.3 | 65.0 | **-5.3** | \u2013 | 67.3 | 64.9 | **-2.4** | \u2013 | 36.3 | 36.4 | 0.1 | **15.9** |", "caption": "Table 1: Comparison of MLLMs\u2019 performance on different multi-choice datasets. Bold values indicate the most significant \u0394\u0394\\Deltaroman_\u0394 or I\u2062L\ud835\udc3c\ud835\udc3fILitalic_I italic_L. \u201c\u2013\u201d denotes that \u0394\u0394\\Deltaroman_\u0394 is significant so that I\u2062L\ud835\udc3c\ud835\udc3fILitalic_I italic_L will not be calculated.", "description": "This table presents a comparison of the performance of various Multimodal Large Language Models (MLLMs) on two different multi-choice datasets: ScienceQA and MMStar.  For each MLLM and dataset, the table shows the Correct Rate (CR) before and after applying a perturbation, the Perturbed Correct Rate (PCR), the difference between them (\u0394), and the Instance Leakage (IL).  The \u0394 value indicates the change in accuracy due to the perturbation, revealing the model's sensitivity to potential data contamination.  A large negative \u0394 value indicates a high level of contamination.  The IL metric, calculated only when the \u0394 value is not significant, represents the proportion of instances where the model was correct before perturbation but incorrect after. A higher IL value also points to contamination. The '-' symbol indicates that the \u0394 value was significant, making the IL calculation unnecessary. The table helps analyze the level of data contamination in different MLLMs and datasets.  The bold values indicate the most significant \u0394 or IL for each dataset.", "section": "4.2 Main Results"}, {"content": "| Model | COCO Validation Set |  |  |  | NoCaps Validation Set |  |  |  | Vintage Training Set |  |  |  |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| Metric | CR | PCR | \u0394 | IL | CR | PCR | \u0394 | IL | CR | PCR | \u0394 | IL |\n| Open-source MLLMs |  |  |  |  |  |  |  |  |  |  |  |  |\n| LLaVA-1.5-7B | 34.6 | 34.0 | -0.6 | 19.0 | 30.9 | 28.5 | -2.4 | \u2013 | 10.8 | 10.1 | -0.7 | 9.0 |\n| VILA-1.5-3B | 19.1 | 20.5 | 1.4 | 13.0 | 19.1 | 20.5 | 1.4 | 13.0 | 1.5 | 2.2 | 0.7 | 1.5 |\n| Qwen-VL-Chat | 32.2 | 30.3 | -1.9 | \u2013 | 28.7 | 27.3 | -1.4 | \u2013 | 15.1 | 15.4 | 0.3 | 12.4 |\n| fuyu-8b | 9.6 | 10.6 | 1.0 | 7.8 | 10.0 | 9.8 | -0.2 | 8.3 | 2.4 | 3.3 | 0.9 | 2.3 |\n| idefics2-8b | 43.5 | 42.3 | -1.2 | \u2013 | 42.6 | 37.5 | -5.1 | \u2013 | 18.5 | 17.0 | -1.5 | \u2013 |\n| Phi-3-vision-128k-instruct | 38.8 | 39.3 | 0.5 | 19.4 | 36.9 | 33.3 | -3.6 | \u2013 | 17.4 | 11.7 | -5.7 | \u2013 |\n| Yi-VL-6B | 43.9 | 43.3 | -0.6 | 19.4 | 37.2 | 36.1 | -1.1 | \u2013 | 3.3 | 4.2 | 0.9 | 2.8 |\n| InternVL2-8B | 53.3 | 51.9 | -1.4 | \u2013 | 48.0 | 46.2 | -1.8 | \u2013 | 28.0 | 28.7 | 0.7 | 18.8 |\n| Proprietary MLLMs |  |  |  |  |  |  |  |  |  |  |  |  |\n| GPT-4o | 58.1 | 54.4 | -3.7 | \u2013 | 54.2 | 55.1 | 0.9 | 19.4 | 36.3 | 38.4 | 2.1 | 20.1 |\n| Gemini-1.5-Pro | 57.5 | 55.3 | -2.2 | \u2013 | 51.2 | 52.0 | 0.8 | 18.7 | \u2013 | \u2013 | \u2013 | \u2013 |\n| Claude-3.5-Sonnet | 53.7 | 51.0 | -2.7 | \u2013 | 50.8 | 51.5 | 0.7 | 20.0 | 35.2 | 33.0 | -2.2 | 21.3 |", "caption": "Table 2: Comparison of MLLMs\u2019 performance on different caption datasets. We have not detected the contamination of Gemini-1.5-Pro on Vintage yet.", "description": "This table presents a comparison of the performance of various Multimodal Large Language Models (MLLMs) on three different image captioning datasets: COCO-Caption2017, NoCaps, and Vintage.  For each MLLM and dataset, the table shows the correct rate (CR) before and after applying a perturbation (PCR), the difference between those two rates (\u0394), and a contamination leakage metric (IL). The \u0394 value helps to determine how sensitive the model is to the perturbation, indicating the presence and extent of data contamination.  The IL metric provides a measure of instance-level contamination, showing if individual training examples from the benchmark datasets might have leaked into the model's training data.  Note that contamination for Gemini-1.5-Pro on the Vintage dataset was not detected.", "section": "4.2 Main Results"}, {"content": "| Models | ScienceQA Train Set |  |  | NoCaps Val. Set |  |  |\n|---|---|---|---|---|---|---|\n|  | CR | PCR | \u0394 | CR | PCR | \u0394 |\n| LLaVA-1.5-7B-cont | 72.9 | 67.9 | -5.0 | 38.2 | 32.8 | -5.4 |\n| LLaVA-1.5-7B-no-cont | 61.8 | 61.2 | -0.6 | 33.0 | 32.1 | -0.9 |", "caption": "Table 3: Detection results after actively contaminating the model with the ScienceQA training set and NoCaps validation set, showcasing the effectiveness of our method in accurately identifying contamination.", "description": "This table presents the results of an experiment designed to evaluate the effectiveness of the MM-Detect framework in identifying data contamination.  Two versions of the LLaVA-1.5-7B model were trained: one without contamination (LLaVA-1.5-7B-no-cont) and one with contamination introduced by incorporating data from the ScienceQA training set and the NoCaps validation set (LLaVA-1.5-7B-cont). The table displays the correct rate (CR), perturbed correct rate (PCR), and the difference between them (\u0394) for both models on the ScienceQA training set and the NoCaps validation set.  The results demonstrate the impact of contamination on model performance, and highlight MM-Detect's ability to detect these performance changes accurately.", "section": "5.1 MM-DETECT IS AN EFFECTIVE DETECTOR"}, {"content": "| Model | CR | PCR | \u0394 |\n|---|---|---|---| \n| LLaVA-1.5-7B-cont | 64.3 | 63.8 | -0.5 |\n| LLaVA-1.5-7B-no-cont | 61.4 | 61.5 | 0.01 |", "caption": "Table 6: Contamination rates of the LLMs used by multimodal models. ILM denotes the IL of the corresponding MLLMs.", "description": "Table 6 presents the contamination rates observed in various Large Language Models (LLMs) that serve as the foundation for several multimodal models.  The contamination rate indicates the percentage of image-related questions correctly answered by the LLM *without* the image being provided. A higher rate suggests a greater likelihood of the LLM having memorized information from the multimodal benchmark datasets during its pre-training phase.  The table also includes the instance leakage metric (ILM) for each corresponding multimodal model, which further quantifies the degree of contamination.", "section": "6 AT WHICH STAGE IS CONTAMINATION INTRODUCED?"}, {"content": "| Model | CR | PCR | \u0394 |\n|---|---|---|---| \n| LLaVA-1.5-7B-cont | 38.1 | 34.9 | -3.2 |\n| LLaVA-1.5-7B-no-coco | 32.5 | 31.9 | -0.6 |", "caption": "Table 7: Depiction of the overlap between the training data of MLLMs and the benchmarks, as well as the contamination degree \u0394\u0394\\Deltaroman_\u0394 of MLLMs on benchmarks.  Green signifies no overlap,  yellow suggests potential overlap, and  Red indicates partial or entire overlap.", "description": "Table 7 shows the degree of overlap between the training data used for various Multimodal Large Language Models (MLLMs) and three benchmark datasets: ScienceQA, COCO Captions, and NoCaps.  The table also presents the contamination degree (\u0394) for each MLLM on each benchmark dataset.  The color-coding helps visualize the level of overlap: green indicates no overlap, yellow suggests potential overlap, and red signifies a partial or complete overlap between the MLLM's training data and the benchmark dataset. This table helps to analyze the sources of contamination in MLLMs, indicating whether contamination might stem from the inclusion of benchmark data during the training process.", "section": "6.2 A REVIEW OF VISUAL INSTRUCTION TUNING DATA FOR CROSS-MODAL CONTAMINATION DETECTION"}, {"content": "| Model | ContRate | IL<sub>M</sub> | \n|---|---|---| \n| LLaMA2-7b (LLaVA-1.5 & VILA) | 25.6 | 11.0 | \n| Qwen-7B (Qwen-VL) | 13.2 | 13.2 | \n| InternLM2-7B (InternVL2) | 11.0 | 5.1 | \n| Mistral-7B-v0.1 (idefics2) | 10.7 | 7.9 | \n| Phi-3-small-128k-instruct (Phi-3-vision) | 6.1 | 7.2 | \n| Yi-6B (Yi-VL) | 3.4 | 9.3 |", "caption": "Table 8: Perplexity of LLaVA-1.5-13b on various multimodal benchmarks (100 samples randomly selected from each dataset).", "description": "Table 8 presents the perplexity scores achieved by the LLaVA-1.5-13b model on various multimodal benchmark datasets.  Perplexity is a measure of how well a probability model predicts a sample. Lower perplexity indicates better prediction accuracy. The table shows the perplexity for both the training and validation sets of four different datasets: ScienceQA, MMStar, COCO-Caption2017, and NoCaps.  Each dataset's perplexity score reflects the model's performance on that dataset. The results are based on 100 randomly selected samples from each dataset, providing a representative measure of the model's overall performance on each dataset.", "section": "A.1 Logits-base"}, {"content": "| Model | ScienceQA | COCO Caption | Nocaps |\n|---|---|---|---|\n| Phi-3-Vision | 0.7 | 0.5 | -3.6 |\n| VILA | -0.5 | 1.4 | 1.4 |\n| Idefics2 | 0.3 | -1.2 | -5.1 |\n| LLaVA-1.5 | 1.3 | -0.6 | -2.4 |\n| Yi-VL | 1.8 | -0.6 | -1.1 |\n| Qwen-VL-Chat | 0.1 | -1.9 | -1.4 |\n| InternVL2 | 0.8 | -1.4 | -1.8 |", "caption": "Table 9: Contamination detection of LLaVA-1.5-13b using TS-Guessing (question-based) on various multimodal benchmarks (100 samples randomly selected from each dataset).", "description": "This table presents the results of contamination detection experiments using the TS-Guessing method on the LLaVA-1.5-13b model.  TS-Guessing is a question-based approach to detecting contamination. The experiment involved evaluating the model's performance on three different multimodal benchmark datasets: COCO-Caption2017, NoCaps, and ScienceQA.  For each dataset, 100 samples were randomly selected to assess the model's ability to correctly answer questions after the order of options or keywords has been altered. The table displays the model's performance using Exact Match, ROUGE-L, and F1 scores for each dataset, providing insights into the level of contamination present.", "section": "A.2 MASKING-BASE"}, {"content": "| Dataset | Perplexity | Split |\n|---|---|---|\n| ScienceQA | 1.4498 | Training Set |\n| MMStar | 1.4359 | Validation Set |\n| COCO-Caption2017 | 1.7530 | Validation Set |\n| NoCaps | 1.8155 | Validation Set |", "caption": "Table 10: Contamination detection of LLaVA-1.5-13b using CDD (Contamination Detection via output Distribution) on various multimodal benchmarks (100 samples randomly selected from each dataset).", "description": "Table 10 presents the results of contamination detection performed on the LLaVA-1.5-13b model using the Contamination Detection via output Distribution (CDD) method.  The CDD method assesses contamination by comparing the similarity between a model's outputs and benchmark data. The table shows the contamination level detected (as a percentage) for three different multimodal benchmark datasets: COCO-Caption2017, NoCaps, and ScienceQA. For each dataset, 100 samples were randomly selected to perform the contamination detection.  This table highlights the challenges of using comparison-based methods for contamination detection in multimodal models.", "section": "A.3 COMPARISON-BASE"}]