[{"content": "| Input views | Target views | Real data | Synthetic data |\n|---|---|---|---| \n| Camera motion | Scene motion | Camera motion | Scene motion | \n| \u2705 | \u274c | \u2705 | \u274c | \n| CO3D [54], MVImgNet [82] | Re10K [93], MC4K [35] | Kubric [19], Objaverse [12] |\n| \u274c | \u2705 | \u274c | \u2705 | \n| static-view videos |\n| \u2705 | \u2705 | \u2705 | \u2705 | \n| - |\n| \u2705 | \u2705 | \u2705 | \u274c | \n| CO3D augmented with Lumiere [7] |\n| \u2705 | \u2705 | \u274c | \u2705 | \n| static-view videos augmented with CAT3D [17] |\n| \u2705 | \u2705 | \u274c | \u274c | \n| single image |", "caption": "Table 1: Our training datasets: Datasets grouped based on whether each has camera or scene motion in the input or target views. The \u201csingle image\u201d row corresponds to randomly (with 1% probability) setting all target views to be one of the input views when drawing samples from all above datasets. For Objaverse, we only use the filtered animated assets\u00a0[38]. See Sec.\u00a03.2 for details.", "description": "This table details the datasets used to train the multi-view video diffusion model.  It categorizes datasets based on whether they contain camera motion, scene motion, or both in their input and target views.  The table highlights the sources of data (real-world and synthetic), indicating the presence or absence of specific types of motion in the input and output views.  A special row notes that 1% of the time, all target views are randomly selected to be identical to one of the input views during sampling, enriching the dataset's diversity.  Specific details about the Objaverse dataset and the filtering method for video datasets are referenced for further information.", "section": "3.2. Dataset Curation"}, {"content": "| Method | Fixed Viewpoint PSNR | Fixed Viewpoint SSIM | Fixed Viewpoint LPIPS | Varying Viewpoint Fixed Time PSNR | Varying Viewpoint Fixed Time SSIM | Varying Viewpoint Fixed Time LPIPS | Varying Viewpoint Varying Time PSNR | Varying Viewpoint Varying Time SSIM | Varying Viewpoint Varying Time LPIPS |\n|---|---|---|---|---|---|---|---|---|\n| 4DiM [71] | 19.77 | 0.540 | 0.195 | 18.81 | 0.428 | 0.219 | 17.28 | 0.378 | 0.256 |\n| Ours | 21.97 | 0.683 | 0.121 | 21.68 | 0.588 | 0.105 | 19.73 | 0.533 | 0.155 |", "caption": "Table 2: Quantitative comparison, disentangled control: We compare with 4DiM\u00a0[71] on the NSFF dataset\u00a0[36], evaluating how well the time and viewpoint can be independently manipulated.", "description": "This table presents a quantitative comparison of CAT4D against the 4DiM model [71] on the NSFF dataset [36].  The comparison focuses on evaluating the models' ability to independently control time and viewpoint during video generation.  The results assess how well each model can manipulate temporal changes (scene dynamics) and camera perspectives without affecting the other.  Metrics such as PSNR, SSIM, and LPIPS are used to measure the quality of generated video frames under various controlled conditions.", "section": "4. Experiments"}, {"content": "| Method | PSNR \u2191 | SSIM \u2191 | LPIPS \u2193 |\n|---|---|---|---|\n| CAT3D-1cond [17] | 15.33 | 0.379 | 0.527 |\n| CAT3D-3cond [17] | 20.19 | 0.568 | 0.258 |\n| Ours | 20.79 | 0.576 | 0.160 |", "caption": "Table 3: Quantitative comparison, sparse-view \u201cbullet-time\u201d: We compare with CAT3D\u00a0[17] on the NSFF dataset\u00a0[36], evaluating the ability to reconstruct a consistent static 3D scene from input images containing scene motion.", "description": "Table 3 presents a quantitative comparison of CAT4D's performance against CAT3D [17] on the task of sparse-view \"bullet-time\" 3D reconstruction using the NSFF dataset [36].  The goal is to reconstruct a consistent static 3D scene from input images that contain motion.  The comparison focuses on the ability of each method to generate high-quality 3D reconstructions despite the presence of dynamic elements in the input images.  Metrics such as PSNR, SSIM, and LPIPS are used to evaluate the quality of the generated 3D reconstructions.", "section": "3.3 Sparse-View Bullet-Time 3D Reconstruction"}, {"content": "| Method | mPSNR \u2191 | mSSIM \u2191 | mLPIPS \u2193 |\n|---|---|---|---|\n| 4D-GS [72] | 16.54 | 0.594 | 0.347 |\n| Shape-of-Motion [70] | 16.72 | 0.630 | 0.450 |\n| Ours | 17.39 | 0.607 | 0.341 |\n| MoSca [33] \u2020 | 19.54 | 0.738 | 0.244 |\n| Ours \u2020 | 18.24 | 0.666 | 0.227 |", "caption": "Table 4: Quantitative comparison, 4D reconstruction: Following prior work \u00a0[16, 70, 33], we report co-visibility masked image metrics on the DyCheck dataset\u00a0[16]. \u2020 indicates methods trained on images at half the original resolution.", "description": "This table presents a quantitative comparison of different methods for 4D reconstruction on the DyCheck dataset.  The metrics used are co-visibility masked mean Peak Signal-to-Noise Ratio (mPSNR), mean Structural Similarity Index (mSSIM), and mean Learned Perceptual Image Patch Similarity (mLPIPS).  The results are compared against ground truth and show the performance of different methods. The symbol \u2020 indicates methods that were trained on images at half the original resolution, highlighting a difference in training methodology that affects comparison.", "section": "4. Experiments"}, {"content": "| Sampling strategy | PSNR \u2191 | SSIM \u2191 | LPIPS \u2193 |\n|---|---|---|---|\n| Independent multi-view | 20.27 | 0.525 | 0.136 |\n| Independent temporal | 21.63 | 0.615 | 0.130 |\n| Multi-view sampling | 22.34 | 0.609 | 0.217 |\n| Temporal sampling | 23.36 | 0.681 | 0.145 |\n| Alternating sampling | 22.15 | 0.633 | 0.108 |", "caption": "Table 5: Quantitative comparison, sampling strategies: We compare our sampling strategy to four simpler variants by comparing to ground-truth images from the NSFF dataset\u00a0[36].", "description": "This table presents a quantitative comparison of different sampling strategies used in the CAT4D model for generating multi-view videos.  The strategies compared include independent multi-view sampling, independent temporal sampling, multi-view sampling (with a sliding window overlap), temporal sampling (with a sliding window overlap), and the authors' proposed alternating sampling strategy. The evaluation metric used to assess the quality of generated videos is the combination of PSNR, SSIM, and LPIPS, compared to the ground-truth videos from the NSFF dataset [36]. This allows for an objective assessment of the efficacy of each sampling approach in generating videos that closely resemble the real ones.", "section": "3.4 Generating Consistent Multi-view Videos"}, {"content": "| Training Data | Fixed Viewpoint\\nVarying Time | PSNR | SSIM | LPIPS | Varying Viewpoint\\nFixed Time | PSNR | SSIM | LPIPS | Varying Viewpoint\\nVarying Time | PSNR | SSIM | LPIPS |\n|---|---|---|---|---|---|---|---|---|---|---|---| \n| Synthetic only | 22.19 | 0.745 | 0.123 | 21.41 | 0.547 | 0.123 | 19.50 | 0.523 | 0.173 |\n| No augmentation | 20.84 | 0.596 | 0.135 | 22.03 | 0.602 | 0.104 | 19.41 | 0.519 | 0.160 |\n| All datasets | 22.49 | 0.749 | 0.110 | 21.86 | 0.599 | 0.105 | 19.74 | 0.546 | 0.152 |", "caption": "Table 6: A ablation study of training data, evaluated on the NSFF dataset\u00a0[36]. All datasets: using all of our training datasets. No augmentation: dropping the two augmented datasets (CO3D augmented with Lumiere and static-view video data augmented with CAT3D). Synthetic only: dropping all real-world datasets and using only synthetic 4D data (Kubric and Objaverse).", "description": "This table presents the results of an ablation study investigating the impact of different training data compositions on the performance of the CAT4D model.  The study compares three scenarios: using all available datasets, removing the augmented datasets (CO3D data augmented with Lumiere and static videos augmented with CAT3D), and using only synthetic 4D data (Kubric and Objaverse). The performance is evaluated on the NSFF dataset [36] using metrics such as PSNR, SSIM, and LPIPS for various combinations of fixed/varying viewpoints and times.", "section": "4. Experiments"}]