[{"figure_path": "https://arxiv.org/html/2411.18613/x2.png", "caption": "Figure 1: CAT4D enables 4D scene creation from any number of real or generated images or video frames.", "description": "This figure showcases the versatility of CAT4D in generating 4D scenes from various sources.  It displays three examples:  (1) reconstructing a 4D scene from a sparsely sampled, dynamic capture of a real-world scene (sparse dynamic capture to 4D), (2) generating a 4D scene from a single, real-world video (real video to 4D), and (3) generating a 4D scene directly from text prompts (text-to-video-to-4D). Each example highlights CAT4D's ability to create a comprehensive 4D model from different input types, demonstrating its capacity for flexible and robust 4D scene creation.", "section": "Abstract"}, {"figure_path": "https://arxiv.org/html/2411.18613/x3.png", "caption": "Figure 2: Qualitative results: CAT4D can generate high-quality dynamic 3D scenes from a single input monocular video. For each example, we show four rendered images, varying in time along the vertical axis, and varying in viewpoint along the horizontal axis. We also show a depth map (bottom right) and a frame from the input video (top right) at the same timestamp as the second column of renders.", "description": "This figure showcases the results of CAT4D, demonstrating its ability to generate high-quality dynamic 3D scenes from just a single monocular video. Each example presents a 2x2 grid of rendered images.  The vertical axis represents the change in time within the video, while the horizontal axis shows shifts in viewpoint.  To help assess the results, a depth map (in the bottom right) and a corresponding frame from the input video (in the top right) are included for comparison.", "section": "Qualitative Results"}, {"figure_path": "https://arxiv.org/html/2411.18613/x4.png", "caption": "Figure 3: What is a multi-view video model? Given one or several input images (grey), different generative models have the ability to create novel images (orange) at certain collections of camera viewpoints and timestamps. Video models generate frames at all timestamps, but without control over camera. Multi-view models generate at controllable cameras but at a fixed timestamp. Camera-controlled video models enable the choice of camera per timestamp, but cannot generate multiple cameras per timestamp. Multi-view video models can generate all views at all timestamps.", "description": "This figure illustrates the capabilities of different generative models in creating images from various viewpoints and timestamps.  It compares video models (which can generate frames for all timestamps but lack camera control), multi-view models (which provide camera control but only at a fixed timestamp), camera-controlled video models (which allow camera control per timestamp but only one camera at a time), and multi-view video models (which offer full control over both camera viewpoint and timestamp, generating multiple views for each timestamp). The figure highlights the unique advantage of multi-view video models in generating comprehensive and dynamic 3D scenes.", "section": "2. Related Work"}, {"figure_path": "https://arxiv.org/html/2411.18613/x5.png", "caption": "Figure 4: Illustration of the method: Given a monocular video (top), we generate the missing frames (orange frames) of virtual stationary video cameras positioned at all input poses (gray circles) and novel poses (blue circles) using our multi-view video diffusion model. These frames are then used to reconstruct the dynamic 3D scene as deforming 3D Gaussians. Note that although the input trajectory is visualized with changing viewpoints, our method also works for fixed-viewpoint videos.", "description": "This figure illustrates the two-stage process of CAT4D, a method for creating 4D scenes from a monocular video.  The top shows the input, a single monocular video.  The method then uses a multi-view video diffusion model to generate missing frames from virtual, stationary cameras at both the original viewpoints (gray circles) and novel viewpoints (blue circles). These synthesized multi-view video frames are then used to reconstruct a dynamic 3D scene represented as deformable 3D Gaussians.  Importantly, while the example shows a video with camera motion, the authors emphasize that the method works equally well for videos recorded from a fixed viewpoint.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2411.18613/x6.png", "caption": "Figure 5: Illustrating our alternating sampling strategy: Given a diffusion model that generates N\ud835\udc41Nitalic_N output views (here, N=3\ud835\udc413N=3italic_N = 3), we use SDEdit\u00a0[44] to alternate between multi-view and temporal sampling to generate a grid of images at K\ud835\udc3eKitalic_K cameras and L\ud835\udc3fLitalic_L time steps (top, here K=4\ud835\udc3e4K=4italic_K = 4 and L=4\ud835\udc3f4L=4italic_L = 4). In multi-view sampling, we generate each sliding window of size 3333 for each column and take the median of the results (middle). Temporal sampling follows a similar process for rows (bottom). Generations for each column or row can be executed in parallel.", "description": "Figure 5 illustrates the alternating sampling strategy used to generate a grid of images from a diffusion model. The process begins with a diffusion model capable of generating N output views (in this case, N=3).  The goal is to produce a grid of images with K cameras and L timesteps (K=4, L=4 in the example). The strategy alternates between multi-view sampling and temporal sampling, using SDEdit for image generation. Multi-view sampling generates each column of the image grid by creating sliding windows of size 3 and taking the median, while temporal sampling similarly generates each row. This parallel process significantly speeds up generation.", "section": "3.4. Generating Consistent Multi-view Videos"}, {"figure_path": "https://arxiv.org/html/2411.18613/x7.png", "caption": "Figure 6: Qualitative comparison, disentangled control: The camera-time grid on the left shows the positions of three input images (gray cells, images visualized in top row) and output images (green) in three different target sampling settings. One frame from each setting (orange cell) is visualized in each row, comparing our model with 4DiM\u00a0[71] and ground truth.", "description": "Figure 6 compares the ability of different models to independently control camera viewpoint and time when generating images.  The left side displays a camera-time grid showing the positions of three input images (gray) and generated images (green) under three different sampling settings. Each row shows a frame from one sampling setting, comparing the output of the authors' model to 4DiM [71] and the ground truth. This demonstrates the model's ability to disentangle camera and time control, generating consistent images across viewpoints and time steps.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2411.18613/x8.png", "caption": "Figure 7: Qualitative comparison, sparse-view \u201cbullet-time\u201d 3D reconstruction: The three input images are shown on the top, where the first one is the target bullet-time frame.", "description": "This figure compares the results of three different methods for sparse-view \"bullet-time\" 3D reconstruction, a technique that creates a static 3D model of a dynamic scene from a limited number of input images.  The top row shows the three input images used for each reconstruction method, with the first image representing the target time for the \"bullet-time\" effect (creating a static 3D model at that specific moment). The remaining rows showcase the reconstruction results from CAT3D with one input condition, CAT3D with three input conditions, and the proposed CAT4D method. The results are qualitatively compared against ground truth. The visualization helps assess each method's ability to generate a consistent static representation of the scene at the target time while dealing with moving objects.", "section": "3.3 Sparse-View Bullet-Time 3D Reconstruction"}, {"figure_path": "https://arxiv.org/html/2411.18613/x9.png", "caption": "Figure 8: Qualitative comparison, sampling strategies: A comparison of different sampling strategies using space-time slices, where the vertical axis represents time and the horizontal axis shows a spatial slice of the image (red line). Our alternating sampling strategy best matches the ground truth motion.", "description": "Figure 8 compares different sampling strategies for generating multi-view videos used in 4D reconstruction.  Each strategy's output is visualized using space-time slices, where the vertical axis shows the time progression of the scene, and the horizontal axis displays a single spatial slice through the scene (indicated by the red line). The figure demonstrates that the alternating sampling approach, combining multi-view and temporal sampling, most accurately captures the ground truth motion compared to the other strategies, which show noticeable inconsistencies.", "section": "3.4 Generating Consistent Multi-view Videos"}, {"figure_path": "https://arxiv.org/html/2411.18613/x10.png", "caption": "Figure 9: Qualitative comparison, 4D reconstruction: We compare 4D reconstructions on the DyCheck dataset\u00a0[16]. The rightmost column shows the input frame, at the same timestamp as the desired target image. Evaluation-excluded co-visibility masks are highlighted green. The visual improvement of our rendering over 4D-GS\u00a0[72] (which our system leverages) demonstrates the value of our proposed multi-view video model. Renderings from Shape-of-Motion\u00a0[70] and MoSca\u00a0[33] were graciously provided by the authors.", "description": "Figure 9 presents a qualitative comparison of 4D reconstruction methods on the DyCheck dataset [16].  The figure visually compares the results of four different methods: 4D-GS [72], Shape-of-Motion [70], MoSca [33], and the proposed CAT4D model. Each row represents a different scene, showing the input frame (rightmost column), ground truth (far right column), and the reconstructions generated by each method.  Areas where co-visibility masks were used for evaluation are highlighted in green.  The improved visual quality of the CAT4D renderings compared to the baseline 4D-GS method highlights the effectiveness of the CAT4D's multi-view video model.  Reconstructions from Shape-of-Motion and MoSca were included for comparison purposes and were graciously provided by the authors.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2411.18613/x11.png", "caption": "Figure 10: Camera trajectories (where we generate novel views) for different types of input videos. Within each panel, we show the trajectories from two different viewpoints. The input views are colored red, and the anchoring sample views are colored blue with the remaining sample views are colored by their index. For videos with sufficient view coverage (a), we only generate anchor views picked from the input camera trajectory.", "description": "Figure 10 visualizes four distinct camera trajectories employed for generating novel views in videos of different types.  Each panel showcases trajectories from two viewpoints, with input views marked in red.  Anchoring sample views are in blue, and the remaining samples are color-coded by index. Panel (a) illustrates the strategy for videos with sufficient view coverage, wherein only anchor views are selected from the input trajectory. The other panels (b), (c), and (d) present variations tailored for forward-moving and static-viewpoint scenarios respectively, highlighting how trajectory selection adapts to diverse input video characteristics.", "section": "3. Method"}]