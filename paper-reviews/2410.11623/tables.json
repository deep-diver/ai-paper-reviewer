[{"figure_path": "2410.11623/tables/table_3_0.html", "caption": "Table 1: Comparison of recent evaluation benchmarks of multimodal large language models and our proposed benchmark VidEgoThink. VQA/HP/VG/RM indicate visual question answering, hierarchy planning, visual grounding, and reward modeling. Existing/Handcraft/Automatic denote the way of collecting data, including existing dataset, manual annotation, and automatic generation.", "description": "Table 1 compares existing multimodal large language model evaluation benchmarks against the proposed VidEgoThink benchmark, highlighting differences in capabilities, task types, data collection methods, and data size.", "section": "Related Work"}, {"figure_path": "2410.11623/tables/table_8_0.html", "caption": "Table 1: Comparison of recent evaluation benchmarks of multimodal large language models and our proposed benchmark VidEgoThink. VQA/HP/VG/RM indicate visual question answering, hierarchy planning, visual grounding, and reward modeling. Existing/Handcraft/Automatic denote the way of collecting data, including existing dataset, manual annotation, and automatic generation.", "description": "This table compares various recent egocentric video benchmarks for multimodal large language models (MLLMs) across several key tasks, highlighting their differences in data collection methods and dataset sizes, and introduces the VidEgoThink benchmark.", "section": "Related Work"}, {"figure_path": "2410.11623/tables/table_9_0.html", "caption": "Table 1: Comparison of recent evaluation benchmarks of multimodal large language models and our proposed benchmark VidEgoThink. VQA/HP/VG/RM indicate visual question answering, hierarchy planning, visual grounding, and reward modeling. Existing/Handcraft/Automatic denote the way of collecting data, including existing dataset, manual annotation, and automatic generation.", "description": "This table compares various existing egocentric video understanding benchmarks with the proposed VidEgoThink benchmark across key aspects such as task types, data collection methods, and data size.", "section": "Related Work"}, {"figure_path": "2410.11623/tables/table_11_0.html", "caption": "Table 1: Comparison of recent evaluation benchmarks of multimodal large language models and our proposed benchmark VidEgoThink. VQA/HP/VG/RM indicate visual question answering, hierarchy planning, visual grounding, and reward modeling. Existing/Handcraft/Automatic denote the way of collecting data, including existing dataset, manual annotation, and automatic generation.", "description": "Table 1 compares various existing egocentric video understanding benchmarks for multimodal large language models, highlighting their task types, data collection methods, and dataset sizes, alongside the proposed VidEgoThink benchmark.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.11623/tables/table_12_0.html", "caption": "Table 4: Experimental results of video question answering. OE, OO, OI, OC, OS, OP denote object existence, object order, object interaction, object count, object state, object prediction. AE, AS, AC indicates action existence, action sequence, action count. SE, ST, SP denote scene existence, scene transition, scene prediction. The bold font denotes the best performance and the underline font denotes the second-best performance.", "description": "Table 4 presents the experimental results of video question answering across different dimensions (object, action, scene) and models, highlighting the best and second-best performances.", "section": "5.2 RESULTS"}, {"figure_path": "2410.11623/tables/table_13_0.html", "caption": "Table 5: Experimental results of video question answering, hierarchy planning, visual grounding, and reward modeling tasks. The bold font denotes the best performance and the underline font denotes the second-best performance.", "description": "Table 5 presents a comparative analysis of the performance of various multimodal large language models across four tasks: video question answering, hierarchy planning, visual grounding, and reward modeling.", "section": "5.2 RESULTS"}, {"figure_path": "2410.11623/tables/table_28_0.html", "caption": "Table 1: Comparison of recent evaluation benchmarks of multimodal large language models and our proposed benchmark VidEgoThink. VQA/HP/VG/RM indicate visual question answering, hierarchy planning, visual grounding, and reward modeling. Existing/Handcraft/Automatic denote the way of collecting data, including existing dataset, manual annotation, and automatic generation.", "description": "Table 1 compares various existing egocentric video benchmarks for multimodal large language models (MLLMs)  with the proposed VidEgoThink benchmark, highlighting their capabilities, task types, data collection methods, and data size.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.11623/tables/table_29_0.html", "caption": "Table 1: Comparison of recent evaluation benchmarks of multimodal large language models and our proposed benchmark VidEgoThink. VQA/HP/VG/RM indicate visual question answering, hierarchy planning, visual grounding, and reward modeling. Existing/Handcraft/Automatic denote the way of collecting data, including existing dataset, manual annotation, and automatic generation.", "description": "Table 1 compares various recent egocentric video understanding benchmarks for multimodal large language models (MLLMs) against the proposed VidEgoThink benchmark, highlighting differences in capabilities, view, task types, data sources, and data collection methods.", "section": "Related Work"}]