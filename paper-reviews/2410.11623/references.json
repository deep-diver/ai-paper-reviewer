{"references": [{" publication_date": "2022", "fullname_first_author": "Josh Achiam", "paper_title": "Gpt-4 technical report", "reason": "This paper is foundational for understanding the capabilities and limitations of GPT-4, a leading large language model.  Its detailed technical report provides insights into the architecture and training data of GPT-4, which are crucial for understanding its performance in the VidEgoThink benchmark and for developing better models for egocentric video understanding.  It is referenced in Section 5, focusing on API-based models.", "section_number": 5}, {" publication_date": "2022", "fullname_first_author": "Michael Ahn", "paper_title": "Do as i can, not as i say: Grounding language in robotic affordances", "reason": "This paper directly addresses the core problem of grounding language in robotic affordances, a central theme in embodied AI and strongly related to the tasks in VidEgoThink (particularly visual grounding and hierarchical planning). Its relevance to the goals of VidEgoThink makes it a key reference for Section 3, which details the task types within the benchmark.", "section_number": 3}, {" publication_date": "2022", "fullname_first_author": "Jean-Baptiste Alayrac", "paper_title": "Flamingo: a visual language model for few-shot learning", "reason": "This paper is highly relevant because it describes Flamingo, a visual language model capable of few-shot learning.  This capability is directly relevant to the performance of MLLMs on the VidEgoThink benchmark, which is evaluated in Section 5.  The model's ability to quickly adapt to new tasks is a critical factor in egocentric video understanding.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "AI Anthropic", "paper_title": "The claude 3 model family: Opus, sonnet, haiku", "reason": "This paper describes the Claude 3 family of models, which are used as automatic evaluators in Section 3 of the paper.  The performance of these LLMs is crucial for ensuring the reliability and validity of the benchmark's evaluation metrics, particularly in video question answering where semantic similarity is assessed.", "section_number": 3}, {" publication_date": "2020", "fullname_first_author": "Jinze Bai", "paper_title": "Qwen technical report", "reason": "This paper describes Qwen, a multimodal large language model.  The experimental evaluation in Section 5 uses Qwen as one of the open-source LLMs to assess performance on the VidEgoThink benchmark.  Understanding Qwen's architecture and training data is crucial for interpreting its performance in the benchmark.", "section_number": 5}, {" publication_date": "2020", "fullname_first_author": "Baoxiong Jia", "paper_title": "Lemma: A multi-view dataset for learning multi-agent multi-task activities", "reason": "The LEMMA dataset is relevant because it contains data on goal-directed actions and multi-task situations, which are similar to the scenarios presented in the VidEgoThink benchmark, particularly in the context of hierarchy planning and reward modeling. Its mention in Section 3 and its relevance to embodied AI make it an important contribution to the field.", "section_number": 3}, {" publication_date": "2022", "fullname_first_author": "Baoxiong Jia", "paper_title": "Egotaskqa: Understanding human tasks in egocentric videos", "reason": "EgoTaskQA is a key existing benchmark that evaluates egocentric video understanding, providing a direct comparison for the VidEgoThink benchmark.   The paper's discussion in Section 2 explains its relevance and limitations, highlighting the need for a benchmark like VidEgoThink.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "reason": "This paper is a seminal work in the field of large language models, establishing the concept of few-shot learning.  This concept is central to the evaluation of MLLMs on the VidEgoThink benchmark, as the benchmark aims to evaluate the models' ability to generalize across various tasks with limited training data. Thus it is important for Section 2.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Keqin Chen", "paper_title": "Shikra: Unleashing multimodal llm's referential dialogue magic", "reason": "This paper is referenced in Section 5 in context of image-based MLLMs and its relevance to the research on using multimodal LLMs for understanding egocentric videos. This model's performance on VidEgoThink shows the relative strengths and limitations of image-based models compared to video-based models.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Yi Chen", "paper_title": "Egoplan-bench: Benchmarking egocentric embodied planning with multimodal large language models", "reason": "This paper introduces EgoPlan-Bench, another benchmark for evaluating egocentric embodied planning capabilities of MLLMs.   Its discussion in Section 2 highlights limitations in its task design, establishing the need for VidEgoThink to overcome the shortcomings and improve upon the state of the art.", "section_number": 2}, {" publication_date": "2018", "fullname_first_author": "Dima Damen", "paper_title": "Scaling egocentric vision: The epic-kitchens dataset", "reason": "The Epic-Kitchens dataset is a significant dataset in egocentric vision research, providing a large-scale resource for training and evaluating models. Its mention in Section 3 highlights its importance as a source of egocentric video data, and it serves as context for the data used in the VidEgoThink benchmark.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Danny Driess", "paper_title": "Palm-e: An embodied multimodal language model", "reason": "Palm-E is a highly relevant multimodal language model directly used in embodied AI applications.  Section 2 discusses the importance of embodied AI for understanding egocentric videos, which are crucial for downstream applications in robotics. Palm-E's capabilities are closely related to the tasks in the VidEgoThink benchmark.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Yifan Du", "paper_title": "A survey of vision-language pre-trained models", "reason": "This survey paper provides a broad overview of vision-language pre-trained models, establishing the context for the research on MLLMs and their application to egocentric video understanding.  Section 2 uses this work to set the stage for VidEgoThink, highlighting the gap in existing research on egocentric video understanding capabilities.", "section_number": 2}, {" publication_date": "2019", "fullname_first_author": "Chenyou Fan", "paper_title": "Egovqa-an egocentric video question answering benchmark dataset", "reason": "EgovQA is another existing benchmark in the field of egocentric video question answering, offering a comparison point for VidEgoThink. The paper's discussion in Section 2 provides context for VidEgoThink's development and points out the limitations of existing benchmarks.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Jensen Gao", "paper_title": "Physically grounded vision-language models for robotic manipulation", "reason": "This paper is directly relevant to the goals of VidEgoThink, as it explores physically grounded vision-language models for robotic manipulation, which are essential for embodied AI applications. Section 3 refers to this work regarding the Visual Grounding task in VidEgoThink.", "section_number": 3}, {" publication_date": "2022", "fullname_first_author": "Kristen Grauman", "paper_title": "Ego4d: Around the world in 3,000 hours of egocentric video", "reason": "Ego4D is the main dataset used for creating the VidEgoThink benchmark.  The data collection methods detailed in Section 4 rely heavily on Ego4D, making it a crucial reference for understanding the dataset's characteristics and its influence on the benchmark.", "section_number": 4}, {" publication_date": "2022", "fullname_first_author": "Wenlong Huang", "paper_title": "Language models as zero-shot planners: Extracting actionable knowledge for embodied agents", "reason": "This paper explores using language models as zero-shot planners for embodied agents, which is directly relevant to the hierarchy planning task in VidEgoThink. Section 3 explains this relevance in context of the hierarchy planning task within VidEgoThink.", "section_number": 3}, {" publication_date": "2022", "fullname_first_author": "Baoxiong Jia", "paper_title": "Egotaskqa: Understanding human tasks in egocentric videos", "reason": "This paper is highly relevant because it introduces EgoTaskQA, a benchmark for evaluating egocentric video understanding. It\u2019s a critical reference point for the VidEgoThink benchmark, discussed extensively in Section 2, highlighting the limitations of existing benchmarks and the need for a more comprehensive evaluation framework.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Shuhei Kurita", "paper_title": "Refego: Referring expression comprehension dataset from first-person perception of ego4d", "reason": "This paper introduces RefEgo, a dataset focusing on object tracking from a first-person perspective, which is directly relevant to the visual grounding task in VidEgoThink. Section 3 details the Visual Grounding task which focuses on object, frame, and temporal grounding, and the discussion in Section 2 explains the relevance and limitations of existing benchmarks that motivated the creation of VidEgoThink.", "section_number": 3}]}