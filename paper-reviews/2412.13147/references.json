{"references": [{"fullname_first_author": "Mark Chen", "paper_title": "Evaluating large language models trained on code", "publication_date": "2021-07-08", "reason": "This paper introduces the Pass@k metric, which is foundational to the G-Pass@k metric proposed in this work and widely used for evaluating code generation and reasoning capabilities."}, {"fullname_first_author": "Karl Cobbe", "paper_title": "Training verifiers to solve math word problems", "publication_date": "2021-10-29", "reason": "This work introduces GSM8K, a widely used benchmark for evaluating mathematical reasoning in large language models, and highlights the challenges of complex reasoning in this domain."}, {"fullname_first_author": "Dan Hendrycks", "paper_title": "Measuring mathematical problem solving with the MATH dataset", "publication_date": "2021-00-00", "reason": "This paper presents MATH, a challenging benchmark composed of competition-level math problems, which serves as a key evaluation dataset for mathematical reasoning in LLMs and is partially included in LiveMathBench."}, {"fullname_first_author": "Dan Hendrycks", "paper_title": "Measuring massive multitask language understanding", "publication_date": "2021-00-00", "reason": "This paper introduces the MMLU benchmark, a multi-subject evaluation that includes a dedicated mathematics section, providing insights into LLMs' broader reasoning capabilities beyond specialized mathematical domains."}, {"fullname_first_author": "Berk Atil", "paper_title": "LLM stability: A detailed analysis with some surprises", "publication_date": "2024-08-08", "reason": "This work directly addresses the issue of stability in LLM reasoning, providing analysis and metrics that highlight the discrepancy between benchmark performance and real-world application consistency, a key motivation for the presented research."}]}