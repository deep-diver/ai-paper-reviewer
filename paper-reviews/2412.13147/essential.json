{"importance": "**This paper aids researchers in two key ways:** 1) **It introduces a new metric (G-Pass@k) that provides a continuous evaluation of LLM performance and stability**, addressing a notable gap in existing evaluation methods which fail to capture consistency. 2) **It offers a new benchmark dataset, LiveMathBench, of challenging, up-to-date mathematical problems to evaluate these advanced reasoning capabilities and consistency.**  It opens new avenues for developing more robust evaluation methods and for enhancing the \u201crealistic\u201d reasoning capabilities in complex tasks like mathematical problem-solving.", "summary": "G-Pass@k & LiveMathBench: Evaluating the stability of LLMs.", "takeaways": ["LLMs, even specialized ones, show instability when solving complex reasoning problems across multiple attempts.", "Simply increasing model size does not guarantee increased stability in reasoning.", "There is a significant gap between a model\u2019s potential (best-case performance) and its actual, consistent performance in practice."], "tldr": "**Large Language Models (LLMs) excel in complex tasks but there's a difference between their performance on benchmarks and how well they do in real-world situations.**  Current evaluations often look at the best possible answer out of several tries (like choosing the best out of 10 attempts). This doesn't show **how consistent** an LLM is, which is crucial for real-world use where reliable, predictable outcomes matter.  Existing benchmarks might also be outdated or too easy due to data leakage (where the models has already seen some of the \u201ctest\u201d data). **This inconsistency poses challenges for real-world applications that need reliable and predictable performance.**\n**This work tackles these issues with a two-pronged approach.** First, they introduce **G-Pass@k**, a new evaluation metric.  Instead of just looking at peak performance, **G-Pass@k measures how often an LLM gives the right answer across multiple tries, showing both its best capability and its consistency.** Second, they present **LiveMathBench**, a challenging new collection of up-to-date math problems, **minimizing data leakage issues by incorporating latest problems from various mathematical competitions.**  Using G-Pass@k with LiveMathBench, the authors show that current LLMs, even the large ones, still have a long way to go in terms of consistent reasoning.", "affiliation": "Shanghai AI Laboratory", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}, "podcast_path": "2412.13147/podcast.wav"}