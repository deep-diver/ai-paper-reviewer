[{"content": "|                       | Basic Trajectory                                                                        | Difficult Trajectory                                                                   |\n| :-------------------- | :--------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------ |\n|                       | DUSt3R                     | VggSfM                     | ParticleSfM                | DUSt3R                     | VggSfM                     | ParticleSfM                |\n|                       | T-Err \u2193                    | R-Err \u2193                    | T-Err \u2193                    | R-Err \u2193                    | T-Err \u2193                    | R-Err \u2193                    |\n| CameraCtrl           | 0.090                      | 0.300                      | 1.405                      | 0.177                      | 2.277                      | 0.825                      |\n| MotionCtrl           | 0.057                      | 0.233                      | 1.324                      | 0.258                      | 1.811                      | 0.868                      |\n| **Ours**              | **0.041**                   | **0.159**                   | **1.036**                   | **0.125**                   | **1.648**                   | **0.685**                   |}", "caption": "Table 1: Quantitative comparisons\u00a0(Pose got by DUSt3R, VggSfM, and ParticleSfM). We compare against prior works on basic trajectory and random trajectory respectively. T-Err, R-Err represent translation error and rotation error.", "description": "This table presents a quantitative comparison of camera trajectory estimation methods.  It evaluates the accuracy of three different Structure-from-Motion (SfM) algorithms: DUSt3R, VggSfM, and ParticleSfM, in estimating camera poses. The comparison includes results from previous state-of-the-art methods (CameraCtrl and MotionCtrl), focusing on both basic (regularly sampled) and difficult (irregularly sampled) camera trajectories.  The evaluation metrics used are translation error (T-Err) and rotation error (R-Err), which measure the deviation between the estimated and ground truth camera poses. Lower values for T-Err and R-Err indicate better accuracy.", "section": "4. Experiments"}, {"content": "|                       | webvid                     |                     OpenVid                     |\n|-----------------------|-----------------------------|---------------------------------------------|\n|                       | LPIPS \u2193 | PSNR \u2191 | SSIM \u2191 | FID \u2193 | FVD \u2193 | LPIPS \u2193 | PSNR \u2191 | SSIM \u2191 | FID \u2193 | FVD \u2193 |\n| Motion-I2V             | 0.375          | 16.14         | 0.487         | 94.77         | 720          | 0.329          | 15.28         | 0.488         | 72.14         | 704          |\n| MOFA-Video            | 0.351          | 18.43         | 0.603         | 57.12         | 524          | 0.300          | 19.64         | 0.655         | 52.66         | 654          |\n| DynamiCrafter         | 0.268          | 18.56         | 0.532         | 63.73         | 685          | 0.393          | 13.83         | 0.402         | 59.61         | 751          |\n| CogVideoX+image       | 0.147          | 24.22         | 0.762         | 59.20         | 486          | 0.164          | 22.61         | 0.762         | 43.29         | 547          |\n| Pyramid-Flow          | 0.152          | 24.99         | 0.792         | 55.78         | 470          | 0.122          | 23.37         | 0.789         | 39.48         | 453          |\n| Open-Sora             | 0.179          | 23.21         | 0.725         | 58.33         | 552          | 0.117          | 22.78         | 0.760         | 44.48         | 512          |\n| Ours                   | 0.135          | 25.22         | 0.810         | 48.11         | 380          | 0.113          | 25.04         | 0.836         | 33.12         | 322          |", "caption": "Table 2: Video quality comparison.", "description": "This table presents a quantitative comparison of video generation quality using various metrics across different models. It compares the performance of several state-of-the-art video generation methods, including Motion-I2V, MOFA-Video, DynamiCrafter, CogVideoX+image, Pyramid-Flow, Open-Sora, and the proposed model, on two datasets: Webvid and OpenVid. The metrics used are LPIPS (Learned Perceptual Image Patch Similarity), PSNR (Peak Signal-to-Noise Ratio), SSIM (Structural Similarity Index), FID (Fr\u00e9chet Inception Distance), and FVD (Fr\u00e9chet Video Distance).  These metrics assess various aspects of video quality, such as perceptual similarity, noise level, and overall video coherence.", "section": "4. Experiments"}, {"content": "|               | webvid                       |               |               | OpenVid                      |               |               |\n| :------------ | :---------------------------- | :------------ | :------------ | :---------------------------- | :------------ | :------------ |\n|               | SubC \u2191                         | MoS \u2191          | Aesq \u2191         | SubC \u2191                         | MoS \u2191          | Aesq \u2191         |\n| DynamiCrafter | 0.832                          | 0.958         | 0.443         | 0.910                          | 0.964         | 0.536         |\n| CogVideoX+image | 0.855                          | 0.984         | 0.443         | 0.929                          | 0.987         | 0.567         |\n| Pyramid-Flow  | 0.906                          | **0.991**     | 0.438         | 0.941                          | 0.991         | 0.537         |\n| Open-Sora     | 0.897                          | 0.989         | 0.438         | 0.954                          | 0.990         | 0.524         |\n| Ours          | **0.928**                      | **0.991**     | **0.474**     | **0.971**                      | **0.993**     | **0.600**     |", "caption": "Table 3: Video consistency quality comparison. SubC: Subject Consistency; MoS: Motion Smoothness; AesQ: Aesthetic Quality.", "description": "This table presents a quantitative comparison of video consistency metrics across different video generation methods.  It evaluates three key aspects of video quality: Subject Consistency (SubC), which measures how well the subject's appearance and motion are maintained throughout the video; Motion Smoothness (MoS), which assesses the fluidity and naturalness of movement; and Aesthetic Quality (AesQ), which evaluates the overall visual appeal of the generated video.  Higher scores indicate better performance in each category.", "section": "4. Experiments"}, {"content": "|---|---|---|---|---|---|---|---|\n|  | Visual Quality |  Trajectory Alignment  |\n|  | LPIPS \u2193 | PSNR \u2191 | SSIM \u2191 | FID \u2193 | FVD \u2193 | TransErr \u2193 | RotErr \u2193 |\n| Camera embedding | 0.401 | 14.22 | 0.531 | 52.46 | 346 | 0.551 | 0.048 |\n| ControlNet-Like | 0.400 | 14.21 | 0.528 | 50.96 | 356 | 0.737 | 0.050 |\n| w/o FS | 0.241 | 17.88 | 0.615 | 46.85 | 311 | 0.671 | 0.059 |\n| w/o noise | 0.228 | 19.32 | 0.654 | 49.38 | 474 | 0.425 | 0.048 |\n| Full Model | **0.142** | **23.22** | **0.796** | **41.67** | **168** | **0.354** | **0.047** |", "caption": "Table 4: Ablation study.", "description": "This table presents the results of an ablation study conducted to evaluate the impact of different components of the AnimateAnything model on video generation quality and camera trajectory alignment.  The study examines the effect of removing or modifying key elements, such as the camera embedding, a ControlNet-like module, frequency stabilization (FS), and the addition of noise during training.  Quantitative metrics (LPIPS, PSNR, SSIM, FID, FVD, TransErr, RotErr) are used to assess the performance of each variant, providing insights into the contribution of each component to the overall system's effectiveness.", "section": "3.5. Training Strategy"}]