[{"figure_path": "https://arxiv.org/html/2411.10836/x2.png", "caption": "Figure 1: \nAnimate anything. Consistent and controllable animation for different kinds of control signals. Given a reference image and corresponding user prompts, our approach can animate arbitrary characters, generating clear stable videos while maintaining consistency with the appearance details of the reference object.", "description": "This figure showcases the capabilities of the 'AnimateAnything' approach.  It demonstrates consistent and controllable animation generation from various control signals (user prompts and a reference image). The animation maintains the appearance details of the reference object, producing clear, stable videos of animated characters, even with diverse control inputs.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2411.10836/x3.png", "caption": "Figure 2: The generated optical flow by our method with different condition signals. Given a specific image, from top to bottom are optical flows generated with camera trajectory, arrow-based motion annotation, and both conditions, respectively.", "description": "This figure demonstrates the optical flow generated by the AnimateAnything model under various control conditions.  The top row shows the optical flow generated when only camera trajectory is used as input. The middle row displays the optical flow resulting from arrow-based motion annotations alone. The bottom row illustrates the combined effect of both camera trajectory and arrow-based annotations on the generated optical flow. This highlights the model's ability to integrate multiple types of control signals to produce a unified representation of motion.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2411.10836/x4.png", "caption": "Figure 3: AnimateAnything Pipeline.\nThe pipeline consists of two stages:\n1) Unified Flow Generation, which creates a unified optical flow representation by leveraging visual control signals through two synchronized latent diffusion models, namely the Flow Generation Model\u00a0(FGM) and the Camera Reference Model\u00a0(CRM). The FGM accepts sparse or coarse optical flow derived from visual signals other than camera trajectory. The CRM inputs the encoded reference image and camera trajectory embedding to generate multi-level reference features. These features are fed into a reference attention layer to progressively guide the FGM\u2019s denoising process in each time step, producing a unified dense optical flow.\n2) Video Generation, which compresses the generated unified flow with a 3D VAE encoder and integrates it with video latents from the image encoder using a single ViT block. The final output is then combined with text embeddings to generate the final video using the DiT blocks.", "description": "AnimateAnything uses a two-stage pipeline for video generation.  Stage 1, Unified Flow Generation, combines various control signals (camera trajectory, motion annotations, etc.) into a unified optical flow representation using two synchronized latent diffusion models: the Flow Generation Model (FGM) and the Camera Reference Model (CRM). The FGM handles sparse/coarse optical flows from sources except camera trajectory, while the CRM processes the encoded reference image and camera trajectory to generate multi-level reference features that guide FGM's denoising process. Stage 2, Video Generation, takes this unified optical flow, compresses it with a 3D VAE encoder, integrates it with video latents from an image encoder via a ViT block, and combines it with text embeddings to generate the final video using DiT blocks.", "section": "3. Methods"}, {"figure_path": "https://arxiv.org/html/2411.10836/x5.png", "caption": "Figure 4: Video stabilization Module", "description": "This module enhances video stability by operating in the frequency domain.  It takes input features, applies a Fast Fourier Transform (FFT) to convert them to the frequency domain, modifies these features using a parameterized weight matrix and an inverse FFT (InvFFT) to return to the time domain. This process helps to suppress instability and flickering by adjusting temporal frequency components. The architecture diagram shows the FFT, frequency adaptors, inverse FFT, and the subsequent application of the modified features via pixel-wise multiplication.", "section": "3.4. Frequency Stabilization"}, {"figure_path": "https://arxiv.org/html/2411.10836/x6.png", "caption": "Figure 5: Camera trajectory comparison with other trajectory-based methods", "description": "Figure 5 presents a comparison of camera trajectory estimations produced by different methods, including CameraCtrl, MotionCtrl, and the proposed method. It visualizes how accurately each method reconstructs the camera path from video frames. The figure aims to showcase the superiority of the proposed approach in terms of precision and consistency in estimating camera movements, which is a crucial aspect for high-quality video generation.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2411.10836/x7.png", "caption": "Figure 6: Motion Transfer comparison with state-of-the-art methods.", "description": "Figure 6 demonstrates the capability of the proposed method in motion transfer tasks by comparing it with several state-of-the-art approaches.  The figure showcases examples of video generation guided by optical flow extracted from a reference video. It visually compares the results of the proposed method against those obtained using Motion-I2V, MOFA-Clone, and Motion-Videos, highlighting differences in motion consistency, style preservation, and artifact reduction. This comparison aims to show the superior performance of the proposed method in accurately transferring motion while maintaining the style and details of the original video.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2411.10836/x8.png", "caption": "Figure 7: Users drag animation comparison with other animation methods.", "description": "Figure 7 presents a comparison of animation results generated by different methods using user-provided drag annotations as input.  It demonstrates the capability of various approaches to interpret user-drawn motion cues and produce realistic-looking animations, enabling a qualitative assessment of the precision and consistency of the different techniques.  The figure likely showcases how accurately and smoothly each model translates the simplistic input of a drag to a more complex and nuanced animation. This comparison directly evaluates the effectiveness of different methods in handling user-specified motion control.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2411.10836/x9.png", "caption": "Figure 8: Human face animation with optical flow extracted from reference video", "description": "Figure 8 presents a comparison of human face animation results generated using different methods.  The key aspect highlighted is the use of optical flow extracted from a reference video to drive the animation. The figure showcases the effectiveness of the proposed method in generating consistent and realistic facial expressions and lip movements, even when the input optical flow may not be perfectly aligned with the target image.  This demonstrates the robustness and flexibility of the approach.", "section": "4.3 Ablation Study And Analysis"}]