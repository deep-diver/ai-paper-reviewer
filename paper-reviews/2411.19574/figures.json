[{"figure_path": "https://arxiv.org/html/2411.19574/extracted/6047733/image/induction.png", "caption": "(a) Various depth", "description": "The figure shows how the accuracy of induction varies among different models with different depths (number of layers).  The training step size is the only parameter that changes. The KV shifting attention (1-layer) achieves comparable performance to the vanilla model with 2 layers, and both significantly outperform the vanilla model with only 1 layer, demonstrating the effectiveness of KV shifting attention in reducing the depth requirement for induction head learning.", "section": "3.2 Great bias when learning induction heads"}, {"figure_path": "https://arxiv.org/html/2411.19574/extracted/6047733/image/presure_test.png", "caption": "(b) Various width", "description": "The figure shows how the accuracy of induction varies with different hidden sizes.  There are two layers in the vanilla model and one layer in the KV shifting attention model; therefore, the vanilla model has twice the number of parameters.  The results demonstrate that KV shifting attention achieves comparable accuracy to the two-layer vanilla model, even with a smaller width (fewer parameters), highlighting its efficiency in learning induction heads.", "section": "3.2 Great bias when learning induction heads"}, {"figure_path": "https://arxiv.org/html/2411.19574/extracted/6047733/image/ot0.png", "caption": "Figure 1: On the left, as the training step size increases, the accuracy of induction varies among different models. In this setting, the only difference between Vanilla and KV shifting attention is the calculation of key and value. The total parameters of Vanilla and KV shifting attention with one layers is the same. And the parameters of Vanilla with 2 layers is twice. On the right is the induction accuracy with different hidden size. There are two layers in Vanilla model, and one layer in KV shifting attention, which means Vanilla model has two times parameters than KV shifting attention.", "description": "This figure compares the performance of standard multi-layer transformers (Vanilla) and the proposed KV shifting attention mechanism in learning induction heads.  The left panel shows how accuracy in an induction task changes as the training progresses for one-layer and two-layer Vanilla models, and a one-layer KV shifting model. Note that the one-layer KV shifting model has the same number of parameters as the one-layer Vanilla model, while the two-layer Vanilla model has twice as many parameters. The right panel shows accuracy as a function of the hidden layer size. Again, the one-layer KV shifting model is compared to one-layer and two-layer Vanilla models.  The results demonstrate that KV shifting attention improves induction learning, even when comparing models with the same number of parameters.", "section": "3.2 Great bias when learning induction heads"}, {"figure_path": "https://arxiv.org/html/2411.19574/extracted/6047733/image/ot10.png", "caption": "(a) O\u2062(T)=0\ud835\udc42\ud835\udc470O(T)=0italic_O ( italic_T ) = 0", "description": "This figure shows contour lines and gradient descent directions of the loss function L for a simplified model used to analyze induction heads.  The loss function is simplified by treating O(T) as a constant and setting \u03b12 = 1 - \u03b11 and \u03b22 = 1 - \u03b21.  The plot shows how the contour lines and gradient directions change as O(T) increases from 0 to 10 to 100.  When O(T) is small, the gradient descent is non-monotonic, but as O(T) increases, it becomes more consistent and it becomes easier for the model to learn induction heads as indicated by the gradient moving toward the point (\u03b11, \u03b21) = (0,1), representing an ideal induction head.", "section": "3.2 Great bias when learning induction heads"}, {"figure_path": "https://arxiv.org/html/2411.19574/extracted/6047733/image/ot100.png", "caption": "(b) O\u2062(T)=10\ud835\udc42\ud835\udc4710O(T)=10italic_O ( italic_T ) = 10", "description": "This figure visualizes the contour lines and gradient descent direction of the loss function L in relation to the variables \u03b1\u2081 and \u03b2\u2081.  The loss function represents the optimization objective during the training of the KV shifting attention mechanism.  The contour lines show the level sets of the loss function, where points on the same line have the same loss value.  The gradient descent vectors indicate the direction of steepest descent, which guides the training process to minimize the loss function.  The parameter O(T) reflects the complexity of the task and controls how sparse the contour lines are (more sparse for larger O(T)).  The figure shows three cases (O(T) = 0, O(T) = 10, O(T) = 100) with different densities of contour lines, illustrating the effect of task complexity on the optimization landscape.", "section": "3.2 Great bias when learning induction heads"}, {"figure_path": "https://arxiv.org/html/2411.19574/extracted/6047733/image/4layers.png", "caption": "(c) O\u2062(T)=100\ud835\udc42\ud835\udc47100O(T)=100italic_O ( italic_T ) = 100", "description": "This figure shows contour lines and gradient descent directions for the loss function L when O(T) is set to 100.  The contour lines represent the values of L, and the arrows indicate the direction of gradient descent. The gradient descent is an optimization process used to find the minimum value of the loss function.  The sparseness of the contour lines indicates a slower convergence speed of the optimization algorithm.  This visualization is used to illustrate the impact of the O(T) term on the model's ability to learn induction heads.", "section": "3.2 Great bias when learning induction heads"}, {"figure_path": "https://arxiv.org/html/2411.19574/extracted/6047733/image/2layers.png", "caption": "Figure 2: Contour lines and gradient decent derection of L\ud835\udc3fLitalic_L. We simplified O\u2062(T)\ud835\udc42\ud835\udc47O(T)italic_O ( italic_T ) as a constant, and \u03b12=1\u2212\u03b11subscript\ud835\udefc21subscript\ud835\udefc1\\alpha_{2}=1-\\alpha_{1}italic_\u03b1 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 1 - italic_\u03b1 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and \u03b22=1\u2212\u03b21subscript\ud835\udefd21subscript\ud835\udefd1\\beta_{2}=1-\\beta_{1}italic_\u03b2 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 1 - italic_\u03b2 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT. Induction heads means (\u03b11,\u03b21)=(0,1)subscript\ud835\udefc1subscript\ud835\udefd101(\\alpha_{1},\\beta_{1})=(0,1)( italic_\u03b1 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_\u03b2 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) = ( 0 , 1 ).", "description": "This figure visualizes the contour lines and gradient descent directions of the loss function (L) involved in learning induction heads using KV shifting attention.  The analysis simplifies the complex loss function by treating O(T) (a term related to sequence length) as a constant.  The plot focuses on the relationship between two learnable parameters, \u03b11 and \u03b21, while \u03b12 and \u03b22 are defined as 1-\u03b11 and 1-\u03b21 respectively.  The point (\u03b11, \u03b21) = (0, 1) represents the ideal state of learning perfect induction heads.  The contour lines show how quickly the loss function changes with respect to these parameters.  The gradient descent directions indicate the direction the parameters are likely to move during training in order to minimize the loss, thus illustrating how effectively the KV shifting attention method facilitates the training process towards the ideal (0,1) state.  Different subplots show the effect of varying the complexity of the training data, represented by O(T).", "section": "3.2 Great bias when learning induction heads"}, {"figure_path": "https://arxiv.org/html/2411.19574/extracted/6047733/image/1layers.png", "caption": "(a) 50M", "description": "This figure shows the accuracy of learning 3-gram text using models of different sizes. The experiment includes three models: a 50M parameter model with 4 layers, a 0.4M parameter model with 2 layers, and a 0.8K parameter model with 1 layer.  The x-axis represents the number of training epochs, and the y-axis represents the accuracy of predicting the third token in a sequence given the first two tokens. The purpose is to compare the performance of standard transformers (Vanilla) against KV shifting attention in learning n-grams. The results show that KV shifting attention does not significantly improve or hinder the ability to learn 3-grams compared to standard transformers across different model sizes.", "section": "3.3 Can KV shifting attention learn n-gram better?"}, {"figure_path": "https://arxiv.org/html/2411.19574/extracted/6047733/image/loss_curve.png", "caption": "(b) 0.4M", "description": "This figure displays the accuracy of induction head learning across different model sizes and training steps.  The experiment uses a simplified setting, focusing only on the ability to learn induction heads.  The smaller model (0.4M parameters) is compared to a larger model (50M parameters), showing the influence of model size on induction head learning ability, measured by the accuracy of predicting subsequent tokens given a repeating pattern of preceding tokens.  The graph demonstrates how well each model learns to identify and utilize these repeating patterns to make accurate predictions. The x-axis represents the number of training steps, while the y-axis indicates the accuracy of the induction head mechanism.", "section": "3.2 Great bias when learning induction heads"}, {"figure_path": "https://arxiv.org/html/2411.19574/extracted/6047733/image/19b.png", "caption": "(c) 0.8K", "description": "This figure shows the accuracy of learning 3-gram text using a model with 0.8K parameters.  It compares the performance of a standard transformer (Vanilla) against the KV shifting attention model. The x-axis represents the training epochs, and the y-axis represents the accuracy. This experiment is designed to evaluate the models' ability to learn n-grams, a fundamental aspect of language modeling. The results show how the accuracy of learning 3-grams changes over time for both models.", "section": "3.3 Can KV shifting attention learn n-gram better?"}, {"figure_path": "https://arxiv.org/html/2411.19574/extracted/6047733/image/seed.png", "caption": "Figure 3: Accuracy of learning 3-gram text using models of different sizes. In this experiments, there are 50M parameters model with 4 layers, 0.4M parameters model with 2 layers, 0.8K parameters model with 1 layer.", "description": "This figure compares the accuracy of learning 3-gram text across models with varying numbers of parameters and layers.  The experiment demonstrates that even a single-layer model with a small number of parameters (0.8K) can achieve a relatively high accuracy. A model with 0.4M parameters and 2 layers performs better, and the largest model with 50M parameters and 4 layers achieves the highest accuracy. This highlights that more parameters and layers generally improve performance for this specific task. However, the single-layer model's performance is still noteworthy given its parameter efficiency.", "section": "3.3 Can KV shifting attention learn n-gram better?"}, {"figure_path": "https://arxiv.org/html/2411.19574/extracted/6047733/image/lr.png", "caption": "(a) 2.9B", "description": "The training loss curves for the 2.9B parameter model are plotted, comparing the vanilla model with KV shifting attention.  The x-axis represents the number of training tokens (in billions), and the y-axis shows the training loss.  The plot illustrates the convergence speed and overall loss values of both models during training, highlighting the difference in their performance.", "section": "Main Result"}, {"figure_path": "https://arxiv.org/html/2411.19574/extracted/6047733/image/lr1e-2.png", "caption": "(b) 19B", "description": "The figure shows the training loss curve for the 19B parameter model. The model was trained using 200B tokens. The blue line represents the training loss for the vanilla model, while the orange line represents the training loss for the model using KV shifting attention.  The graph indicates that the model with KV shifting attention converges faster and achieves a lower final loss compared to the vanilla model. This demonstrates the efficacy of KV shifting attention in enhancing training efficiency and improving model performance.", "section": "4.2 Main Result"}, {"figure_path": "https://arxiv.org/html/2411.19574/extracted/6047733/image/1d4b.png", "caption": "Figure 4: Training loss curve. We train 2.9B model with 500B tokens, and 19B models with 200B tokens.", "description": "This figure displays the training loss curves for two large language models: one with 2.9 billion parameters trained on 500 billion tokens and another with 19 billion parameters trained on 200 billion tokens.  The curves show the decrease in training loss over the course of training, illustrating the models' learning progress.  Comparing the two curves allows for assessing the impact of model size and training data volume on the learning process.", "section": "4.2 Main Result"}, {"figure_path": "https://arxiv.org/html/2411.19574/extracted/6047733/image/7b.png", "caption": "(a) Various Seeds", "description": "This figure displays the training loss curves for a 1.5B parameter model across five different random seeds.  Each curve represents a separate training run with different random initialization and data sampling. The purpose is to demonstrate the robustness of KV shifting attention compared to vanilla attention, showing that KV shifting attention consistently achieves lower training loss even under varied conditions.", "section": "3.2 Great bias when learning induction heads"}, {"figure_path": "https://arxiv.org/html/2411.19574/extracted/6047733/image/14b.png", "caption": "(b) Various LR", "description": "This figure shows the training loss curves for a 1.5B parameter model with different learning rates (LR).  It compares the performance of the vanilla attention mechanism against the KV shifting attention mechanism across various learning rates. The goal is to assess the robustness and stability of each approach under different optimization settings, demonstrating how the KV shifting attention mechanism generally maintains stability even when the vanilla attention mechanism diverges (e.g., at LR=1e-2).", "section": "3.2 Great bias when learning induction heads"}, {"figure_path": "https://arxiv.org/html/2411.19574/extracted/6047733/image/scaling_law.png", "caption": "(c) LR=1e-2", "description": "This figure displays the training loss curves for a 1.5B parameter model using different learning rates. Specifically, it shows the impact of a 1e-2 learning rate on the training loss of both Vanilla attention and KV shifting attention.  Note that the y-axis represents training loss and x-axis represents training steps.  The plot helps illustrate the robustness and stability of KV shifting attention, particularly its resilience to higher learning rates, as compared to the Vanilla attention mechanism.", "section": "3.2 Great bias when learning induction heads"}, {"figure_path": "https://arxiv.org/html/2411.19574/extracted/6047733/image/1d4b40b.png", "caption": "Figure 5: Training loss of 1.5B parameters model among random seeds and learning rate (LR).", "description": "This figure displays the training loss curves for a 1.5B parameter language model under various conditions.  Specifically, it illustrates how the training loss changes with the number of training steps across multiple runs using different random seeds for model initialization, and also shows how the loss varies when using different learning rates (LR).  This allows for an assessment of the model's robustness and stability during training across different random initializations and hyperparameter settings. The different lines represent separate training runs with different random seeds or learning rates.", "section": "3.2 Great bias when learning induction heads"}, {"figure_path": "https://arxiv.org/html/2411.19574/extracted/6047733/image/gate.png", "caption": "(a) 1.5B Parameters", "description": "Training loss curve for the 1.5B parameter model.  The plot shows the training loss over time (steps) for both a vanilla transformer and one utilizing KV shifting attention.  It illustrates the relative convergence speed and loss values of each model, providing insight into the effect of KV shifting attention on training efficiency.  Note the difference in scale between the vanilla and KV shifting models.", "section": "3.2 Great bias when learning induction heads"}, {"figure_path": "https://arxiv.org/html/2411.19574/extracted/6047733/image/aba_new.png", "caption": "(b) 6.7B Parameters", "description": "The figure shows the training loss curves for a 6.7B parameter language model.  The graph plots training loss against the number of training steps. Two lines are displayed, one for a model using standard attention and another for a model using KV shifting attention.  This visualization allows comparison of the training loss reduction efficiency for both attention mechanisms.", "section": "3.2 Great bias when learning induction heads"}, {"figure_path": "https://arxiv.org/html/2411.19574/extracted/6047733/image/p3_new.png", "caption": "(c) 13B Parameters", "description": "This figure shows the training loss curve for the 13B parameter model.  The loss is plotted against the number of training steps.  This plot is part of an experiment comparing the performance of a model using the KV shifting attention mechanism to a vanilla transformer model of the same size. This particular plot shows how the loss function changes over training for the larger model, giving insight into the convergence speed and overall performance of the KV-shifting attention.", "section": "3.2 Great bias when learning induction heads"}, {"figure_path": "https://arxiv.org/html/2411.19574/extracted/6047733/image/hop_vanilla.png", "caption": "Figure 6: Training loss comparison between different size. All models are trained on 10B tokens. The batch size for 1.5B and 6.7B model is 0.5M, for 13B is 1M, so the total steps of 13B model is half of others.", "description": "Figure 6 presents a comparison of training loss curves for transformer language models of varying sizes (1.5B, 6.7B, and 13B parameters).  All models were trained on the same amount of data (10 billion tokens). However, the batch size varied across models.  The 1.5B and 6.7B parameter models used a batch size of 0.5 million tokens while the 13B parameter model used a batch size of 1 million tokens.  This difference in batch size resulted in the 13B model completing approximately half the number of training steps compared to the other two models. The graphs illustrate the training loss over these steps, allowing for a comparison of training efficiency and convergence across model sizes.", "section": "4.5 Scaling Experiment"}, {"figure_path": "https://arxiv.org/html/2411.19574/extracted/6047733/image/hop_vanilla_ori.png", "caption": "(a) Scaling law", "description": "This figure presents the scaling law for both vanilla attention and KV shifting attention.  The x-axis represents the number of parameters (log scale), and the y-axis represents the validation loss (log scale). The plot shows how validation loss decreases as model size increases, illustrating the relationship between model scale and performance for the two different attention mechanisms. This allows comparison of the efficiency with which each attention mechanism utilizes increased model size to improve performance.", "section": "4.5 Scaling Experiment"}, {"figure_path": "https://arxiv.org/html/2411.19574/extracted/6047733/image/hop_kv.png", "caption": "(b) 1.5B Parameters", "description": "The figure shows the training loss curves for a 1.5 billion parameter language model.  Specifically, it compares the training loss of a model using standard attention ('Vanilla') versus a model using KV shifting attention. The plot shows the loss decreasing over training steps, with KV shifting attention demonstrating faster convergence and a lower final loss compared to the standard model. This illustrates the improved efficiency and effectiveness of the KV shifting attention mechanism for language model training.", "section": "3.2 Great bias when learning induction heads"}, {"figure_path": "https://arxiv.org/html/2411.19574/extracted/6047733/image/qkvshifting.png", "caption": "Figure 7: Validation loss across different size and training tokens. For scaling law, while others in this paper means the total parameters. models are trained on 10B tokens and calculate the final checkpoint\u2019s validation loss.", "description": "Figure 7 displays the validation loss for models of various sizes trained on 10 billion tokens.  The left panel (a) shows a scaling law where the x-axis represents the total number of parameters (excluding embedding parameters) and the y-axis represents the validation loss achieved at the final checkpoint of training.  The right panel (b) shows the training loss curve for the 1.5B parameter model, specifically focusing on the model's performance as the training progressed towards 30B tokens.  This highlights how the validation loss remains relatively stable for KV shifting attention, even with a large increase in training data, unlike the vanilla attention model.", "section": "4.5 Scaling Experiment"}, {"figure_path": "https://arxiv.org/html/2411.19574/extracted/6047733/image/mmlu.png", "caption": "(a) Variant", "description": "This figure displays the training loss curves for different variations of the KV shifting attention mechanism on a 1.5B parameter model trained with 10B tokens.  The variations include the baseline Vanilla attention, KV shifting attention with a gate mechanism (KV shifting Gate), KV shifting attention restricted to values between 0 and 1 (KV shifting 0 to 1), and standard KV shifting attention.  The purpose is to investigate the impact of these modifications on the model's learning process and performance.", "section": "Further experiments"}, {"figure_path": "https://arxiv.org/html/2411.19574/extracted/6047733/image/cmmlu.png", "caption": "(b) Ablation", "description": "This ablation study investigates the impact of removing key and value shifting components from the KV shifting attention mechanism.  The figure compares the training loss curves of the full KV shifting attention model against versions where either the key shifting or value shifting, or both, are removed. This allows for assessment of the relative contribution of each component to the model's overall performance and learning ability. Specifically, it shows the impact on the model's capacity to learn the induction heads mechanism that are central to the paper's focus on improving language modeling. The relative differences in training loss among these variants illustrate the importance of both key and value shifting for achieving the model's improved performance.", "section": "3.2 Great bias when learning induction heads"}]