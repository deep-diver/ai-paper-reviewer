[{"figure_path": "https://arxiv.org/html/2412.09585/x2.png", "caption": "Figure 1: Different Paradigms for Incorporating Visual Information into LLMs. (a, b) Existing approaches\u00a0[41, 1, 61] feed features from the visual encoder(s) into the LLM and train the model solely with natural language supervision, i.e., next (text) token prediction (NTP) objective to align the embedding space of the vision encoder(s) and the LLM. (c) We propose distilling target visual information into the intermediate representations of the LLM from a set of target encoders (\ud835\udc04targetsuperscript\ud835\udc04target\\mathbf{E}^{\\text{target}}bold_E start_POSTSUPERSCRIPT target end_POSTSUPERSCRIPT). We adopt a predictive embedding\u00a0[2, 4] optimization approach at selected LLM layers during training to minimize the embedding losses along with the next token prediction (NTP) objective loss function, resulting in a vision-centric approach to training the Multimodal Large Language Model. We only use a single base vision encoder during inference.", "description": "This figure illustrates three different methods for integrating visual information into large language models (LLMs).  (a) and (b) show existing methods that feed visual features directly into the LLM, relying solely on natural language training.  In contrast, (c) presents the proposed OLA-VLM approach, which distills visual information from multiple specialized encoders into the LLM's intermediate layers during training.  This allows for a more vision-centric training process, ultimately improving visual understanding without the need for multiple visual encoders during inference.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2412.09585/x3.png", "caption": "Figure 2: Probing reveals a relationship between representation quality and performance. (a) We observe that increasing the amount of data and training solely with the next-token prediction objective enhances the visual representation quality within the LLM, resulting in improved performance, underscoring the effectiveness of our probing setup. (b) Our OLA-VLM exhibits superior visual representations and performance compared to LLaVA-1.5\u00a0[41] under the same settings, demonstrating the effectiveness of minimizing the predictive embedding objective during training.", "description": "Figure 2 demonstrates the correlation between the quality of visual representations within a large language model (LLM) and its downstream performance.  Panel (a) shows that training an LLM with solely natural language supervision (next-token prediction) leads to improved visual representation quality as the training data size increases, which in turn boosts performance. This validates the probing methodology used in the study. Panel (b) compares the proposed OLA-VLM with the LLaVA-1.5 baseline. OLA-VLM, incorporating predictive embedding optimization, shows significantly superior visual representation quality and performance, highlighting the effectiveness of the proposed method.", "section": "3. Visually Probing Language Embeddings"}, {"figure_path": "https://arxiv.org/html/2412.09585/x4.png", "caption": "Figure 3: Probing Visual Representations across LLM layers in Multimodal Large Language Models. (1) As shown in the first row, the multi-encoder baseline has the best probing performance owing to the additional feature inputs. The performance of probes trained on our OLA-VLM falls between the two baselines, demonstrating the effectiveness of our embedding distillation approach in learning an improved projector while only using a single encoder during inference. (2) We observe that the probing performance for single encoder models trained solely with natural language supervision improves as the training data of the base MLLM increases, indicating that the LLM improves its visual representations of the world with more training data. In the last row, we observe that our OLA-VLM (base setting) outperforms the LLaVA-1.5 model trained with more data during the PT stage, demonstrating the effectiveness of our approach.", "description": "Figure 3 presents a comprehensive analysis of visual representation quality within different Multimodal Large Language Models (MLLMs) across various layers.  The top row contrasts three different MLLM approaches: a single-encoder model, a multi-encoder model, and the authors' proposed OLA-VLM model. It shows that the multi-encoder model achieves the best probing performance due to its access to more visual features. OLA-VLM, while using only a single encoder at inference time, demonstrates performance that falls between the single- and multi-encoder baselines, signifying the effectiveness of its knowledge distillation method for improving the projector's ability to handle visual inputs. The middle row demonstrates how the probing performance of a single encoder model improves solely based on the amount of natural language training data used, highlighting the model's ability to learn better visual representations with more data. Finally, the bottom row compares the base OLA-VLM against a LLaVA-1.5 model trained on a larger dataset, revealing that OLA-VLM still achieves better results, underscoring its ability to leverage a more efficient training approach.", "section": "3. Visually Probing Language Embeddings"}, {"figure_path": "https://arxiv.org/html/2412.09585/x5.png", "caption": "Figure 4: Architecture for OLA-VLM. During Pre-Training (PT), we optimize an embedding loss at specific layers for each target encoder: layers d\u2208\ud835\udd3b\ud835\udc51\ud835\udd3bd\\in\\mathbb{D}italic_d \u2208 blackboard_D, s\u2208\ud835\udd4a\ud835\udc60\ud835\udd4as\\in\\mathbb{S}italic_s \u2208 blackboard_S, and g\u2208\ud835\udd3e\ud835\udc54\ud835\udd3eg\\in\\mathbb{G}italic_g \u2208 blackboard_G for the depth, segmentation, and generation tasks, respectively. We use a resampler-based embedding predictor\u00a0[28], denoted as \ud835\udc0f{task}lsubscriptsuperscript\ud835\udc0f\ud835\udc59task\\mathbf{P}^{l}_{\\{\\text{task}\\}}bold_P start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT start_POSTSUBSCRIPT { task } end_POSTSUBSCRIPT at each layer l\ud835\udc59litalic_l, to output predictions. Each predictor takes in two inputs: a set of learnable queries (\ud835\udc10tasksuperscript\ud835\udc10task\\mathbf{Q}^{{\\text{task}}}bold_Q start_POSTSUPERSCRIPT task end_POSTSUPERSCRIPT) and the token sequence from layer l\ud835\udc59litalic_l, with special tokens for other tasks omitted. The final loss is the sum of embedding losses across all selected layers and the next-token prediction objective. During IFT, we train with only the next-token prediction objective while keeping the special tokens frozen so as not to affect their task-specific nature.", "description": "This figure illustrates the architecture of OLA-VLM, a multimodal large language model that incorporates visual information from multiple sources.  During pre-training (PT), the model optimizes embedding losses at specific layers for each task (depth, segmentation, generation).  These losses compare the model's output to target encodings from specialized visual encoders, using a resampler-based embedding predictor.  The predictor receives learnable queries and the model's token sequence as input. The total loss combines these embedding losses with the standard next-token prediction loss. During instruction fine-tuning (IFT), only the next-token prediction loss is used, while task-specific special tokens are kept frozen to maintain their individual nature. This method aims to improve the model's visual understanding by embedding task-specific information at various layers during pre-training, rather than solely relying on the natural language supervision.", "section": "4. Embedding Visual Information into LLM"}, {"figure_path": "https://arxiv.org/html/2412.09585/x6.png", "caption": "(a) depth embedding loss", "description": "This figure shows the ablation study on the choice of layers for computing the depth embedding loss in the OLA-VLM model.  The x-axis represents different layers in the LLM, and the y-axis represents the cosine similarity between the probe's prediction and the ground truth depth features. The plot shows that the optimal layer for computing the embedding loss is around layer 18, indicating the optimal layer to inject depth information from the teacher model into the LLM.", "section": "3. Visually Probing Language Embeddings"}, {"figure_path": "https://arxiv.org/html/2412.09585/x7.png", "caption": "(b) seg embedding loss", "description": "This figure shows the ablation study on the choice of layers for computing the segmentation embedding loss in the OLA-VLM model.  The x-axis represents the layer number in the LLM, and the y-axis represents the cosine similarity between the probe's output and the target segmentation features. The plot illustrates how different choices of layers for computing the embedding loss impact the quality of learned visual representations. The results indicate that choosing optimal layers for the loss calculation significantly enhances the performance of the model.", "section": "3. Visually Probing Language Embeddings"}, {"figure_path": "https://arxiv.org/html/2412.09585/x8.png", "caption": "(c) gen embedding loss", "description": "This figure shows the ablation study on the choice of layers for computing the generation embedding loss. It displays the performance on the CV-Bench benchmark for different layer selections, revealing the optimal layer choice for minimizing the generation embedding loss and achieving better performance.", "section": "5.3 Ablations"}, {"figure_path": "https://arxiv.org/html/2412.09585/x9.png", "caption": "Figure 5: Ablating choice of layers for \u2112embsubscript\u2112emb\\mathcal{L}_{\\text{emb}}caligraphic_L start_POSTSUBSCRIPT emb end_POSTSUBSCRIPT on CV-Bench.\nWe observe optimal performance by computing the embedding losses for depth, seg, and gen features at the 18thth{}^{\\text{th}}start_FLOATSUPERSCRIPT th end_FLOATSUPERSCRIPT, 18thth{}^{\\text{th}}start_FLOATSUPERSCRIPT th end_FLOATSUPERSCRIPT, and 20thth{}^{\\text{th}}start_FLOATSUPERSCRIPT th end_FLOATSUPERSCRIPT layer, respectively. We do not use any special tokens for this ablation study.", "description": "This figure displays ablation studies on the choice of layers for calculating embedding loss (\u2112emb) in the OLA-VLM model. The experiment was conducted using the CV-Bench benchmark.  The x-axis represents the layer number of the Large Language Model (LLM) and the y-axis shows the cosine similarity. Three different tasks (depth, segmentation, generation) are considered, each represented by a different colored line. The results indicate that calculating embedding losses at the 18th layer for depth and segmentation tasks, and the 20th layer for the generation task yields the optimal performance. Notably, this experiment was performed without using any special tokens.", "section": "5.3 Ablations"}, {"figure_path": "https://arxiv.org/html/2412.09585/x10.png", "caption": "Figure 6: Visualizing Embedding Predictor Outputs after the PT stage. The quality of the decoded representations indicates the effectiveness of our embedding optimization.", "description": "This figure visualizes the output of the embedding predictor models after the pre-training (PT) stage.  Specifically, it shows the results of decoding the embedding predictor's output using decoders from the target tasks (depth, segmentation, and generation). The visual quality of these decoded representations serves as a qualitative measure of the effectiveness of the proposed embedding optimization technique within OLA-VLM.  The comparison of the results across different layers of the model offers insights into the layer-wise impact of the optimization strategy. High-quality decoded images suggest successful embedding optimization.", "section": "3. Visually Probing Language Embeddings"}, {"figure_path": "https://arxiv.org/html/2412.09585/x11.png", "caption": "Figure I: Qualitative Examples for the Count task in CV-Bench. Our OLA-VLM can accurately predict the presence of one picture and one tree, unlike LLaVA-1.5\u00a0[41].", "description": "This figure presents two examples illustrating the performance of OLA-VLM and LLaVA-1.5 on the 'Count' task within the CV-Bench benchmark.  The 'Count' task assesses the model's ability to accurately count objects within an image. Each example shows an image alongside multiple-choice questions about the number of certain objects (e.g., pictures or trees).  The ground truth answers are provided, demonstrating that OLA-VLM correctly identifies the number of pictures and trees in both example images, while LLaVA-1.5 makes incorrect predictions in both cases.  This highlights OLA-VLM's improved visual understanding and object counting capabilities.", "section": "5. Qualitative Comparisons"}, {"figure_path": "https://arxiv.org/html/2412.09585/x12.png", "caption": "Figure II: Ground-truth outputs from the target models used for Probing MLLMs.", "description": "This figure displays the ground truth outputs generated by three different target models used in the probing experiments of the paper.  These models represent different visual tasks: image segmentation, depth estimation, and image generation.  Each model's output provides a visual benchmark against which the LLM's internal visual representations are compared to assess the quality of the LLM's visual understanding. The figure is crucial for evaluating how effectively the OLA-VLM approach improves the LLM's ability to capture and represent visual information, and offers a visual interpretation of the probing results.", "section": "3. Visually Probing Language Embeddings"}, {"figure_path": "https://arxiv.org/html/2412.09585/x13.png", "caption": "Figure III: Qualitative Examples for the Depth task in CV-Bench. Our OLA-VLM can accurately predict that the lamp and keyboard ar closer to the camera in the respective samples.", "description": "This figure showcases qualitative examples from the Depth task within the CV-Bench benchmark.  It directly compares the performance of the OLA-VLM model against the LLaVA-1.5 baseline.  Each example presents an image with two objects, one closer to the camera than the other. The caption highlights that OLA-VLM correctly identifies the closer object (lamp and keyboard) in both examples, while LLaVA-1.5 makes incorrect predictions.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.09585/x14.png", "caption": "Figure IV: Qualitative Examples for the Relation task in CV-Bench. Our OLA-VLM can accurately predict that the positions of the trees and the bottle in the respective samples.", "description": "Figure IV showcases examples from the CV-Bench dataset's Relation task, illustrating how OLA-VLM and LLaVA-1.5 models predict the spatial relationship between objects. The figure highlights OLA-VLM's improved accuracy in determining the relative positions of objects (e.g., above/below, left/right) compared to the LLaVA-1.5 model.", "section": "5. Qualitative Comparisons"}, {"figure_path": "https://arxiv.org/html/2412.09585/x15.png", "caption": "Figure V: Qualitative Examples for the Distance task in CV-Bench. Our OLA-VLM can accurately predict that the distances between the respective pair of objects.", "description": "Figure V shows examples from the CV-Bench dataset's Distance task, which evaluates the model's ability to estimate real-world distances between objects in an image.  Each example presents a scene with three objects, and the task is to identify which object is closest to a reference object.  The figure highlights how OLA-VLM more accurately predicts the correct distances compared to the LLaVA-1.5 baseline.  This demonstrates OLA-VLM's improved performance in understanding and representing spatial relationships between objects.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.09585/", "caption": "Figure VI: Layerwise visualizations for the depth probes. For LLaVA-1.5\u00a0[41], the probes generate blob-like outputs up to the eighth layer, with visualizations progressively improving in the middle layers, aligning with the findings presented in Sec. 3 of the main text. Notably, probes trained on OLA-VLM begin producing distinguishable object shapes and boundaries as early as the third layer, attributed to the enhanced projector design and the incorporation of embedding losses.", "description": "This figure displays layer-wise visualizations of depth probe outputs for both LLaVA-1.5 and OLA-VLM.  LLaVA-1.5 shows blob-like outputs for the first eight layers, gradually improving in the middle layers.  In contrast, OLA-VLM produces distinguishable object shapes and boundaries from as early as the third layer, demonstrating the effectiveness of its enhanced projector and embedding loss techniques.", "section": "3. Visually Probing Language Embeddings"}]