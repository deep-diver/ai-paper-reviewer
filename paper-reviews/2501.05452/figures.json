[{"figure_path": "https://arxiv.org/html/2501.05452/x2.png", "caption": "Figure 1: Overview of ReFocus. ReFocus performs visual chain of thought via input-image editing on an example data from TableVQA\u00a0[16]. Given an image and question pair, ReFocus equips GPT-4 with editing tools (details in \u00a73), and GPT-4 generates pseudo code if an edit action is needed. ReFocus then executes the editing actions, and feeds GPT-4 with the new image until an answer is reached. In the above example, mask_column and draw_row are performed.", "description": "This figure illustrates the ReFocus framework's operation using an example from the TableVQA dataset.  The process begins with an image and a question. ReFocus integrates GPT-4 with image editing tools. GPT-4 then generates Python code for these tools to modify the image based on its reasoning. The modified image is fed back to GPT-4 iteratively, refining the focus, until an answer is derived. This example demonstrates using the mask_column and draw_row tools.", "section": "3. REFOCUS"}, {"figure_path": "https://arxiv.org/html/2501.05452/x3.png", "caption": "Figure 2: Example of how ReFocus + GPT-4o solves previously unsolvable problem in ChartQA dataset\u00a0[24] through improved visual grounding. Given the original horizontal bar image (left), GPT-4o grounds to the wrong bars and thus gets the wrong answer. ReFocus eliminates such possibility through editing, guiding the model to the correct answer (right).", "description": "This figure demonstrates how the ReFocus framework improves the accuracy of GPT-4 in solving ChartQA problems.  The left panel shows the original horizontal bar chart and the incorrect answer generated by GPT-4 alone.  GPT-4 incorrectly identifies bars, leading to a wrong answer because it doesn't adequately ground its visual understanding.  The right panel illustrates how ReFocus uses visual editing (masking irrelevant portions of the chart) to guide GPT-4 toward the correct answer.  By selectively focusing the model's attention, ReFocus eliminates the misinterpretation and achieves the correct result.", "section": "3. REFOCUS"}, {"figure_path": "https://arxiv.org/html/2501.05452/x4.png", "caption": "Figure 3: ReFocus equips GPT-4 with selective attention. Above is an example of how ReFocus + GPT-4o solves previously unsolvable problem in ChartXiv dataset\u00a0[34]. Specifically, ReFocus edits upon the original image by masking out all irrelevant information \u2013 the other three subplots that could be distracting. As a result, GPT-4o is able to conduct better reasoning with the edited image, and reach the correct answer.", "description": "This figure demonstrates the REFOCUS framework's ability to improve visual reasoning in LLMs.  It shows an example from the ChartXiv dataset where GPT-4, when provided with the original image containing multiple subplots, fails to answer the question correctly. However, by using REFOCUS to mask irrelevant subplots and isolate the relevant information, the model successfully answers the question.  This highlights how REFOCUS enhances selective attention, improving accuracy by effectively guiding the model's focus to the most pertinent visual elements.", "section": "3. REFOCUS"}, {"figure_path": "https://arxiv.org/html/2501.05452/x5.png", "caption": "Figure 4: ReFocus unleashes better visual grounding and counting abilities for GPT-4 as in ChartQA\u00a0[24] Vertical Bar problems.", "description": "This figure demonstrates how ReFocus improves GPT-4's performance on a visual question answering task involving vertical bar charts from the ChartQA dataset.  The original image (left) shows a chart where GPT-4 struggles to accurately count the number of years with values below 10%, due to visual grounding issues.  After applying ReFocus, which modifies the image via visual editing actions (right), GPT-4 successfully counts the years below 10%. This showcases ReFocus's capability to improve visual reasoning by guiding the model's attention via intermediate visual edits.", "section": "3.2 Visual Editing Tools"}, {"figure_path": "https://arxiv.org/html/2501.05452/x6.png", "caption": "Figure 5: ReFocus unleashes better OCR for GPT-4. In this example from TableVQA\u00a0[16], ReFocus + GPT-4 conducts the edit action highlight_column. With this simple action, GPT-4 can focus more on the important subarea, and recognize the characters better.", "description": "The figure demonstrates how ReFocus improves the performance of GPT-4 on a table visual question answering task from the TableVQA dataset.  By applying the 'highlight_column' visual edit, ReFocus allows GPT-4 to focus more effectively on relevant parts of the table image. This improved focus leads to a significant improvement in the model's ability to perform optical character recognition (OCR), resulting in more accurate character recognition and ultimately a more accurate answer to the question.  The example shows how the visual edit improves the ability of the GPT-4 to accurately recognize the characters in the 'Aircraft' column, thereby improving overall performance on the task.", "section": "3.2 Visual Editing Tools"}, {"figure_path": "https://arxiv.org/html/2501.05452/x7.png", "caption": "Figure 6: Training set collection using ReFocus on ChartQA dataset.", "description": "This figure illustrates the process of creating a training dataset for multimodal large language models using the ReFocus framework and the ChartQA dataset.  It shows the stages involved: starting with an image and a question, generating code to focus on specific areas of the image, performing the visual editing using that code, obtaining the correct answer, and then using this entire sequence (image, question, code, edited image, answer) as a single training data point.  The dataset is then used for supervised finetuning of the model, which allows for more effective learning of visual reasoning processes.", "section": "5. Finetune with REFOCUS data"}, {"figure_path": "https://arxiv.org/html/2501.05452/x8.png", "caption": "Figure 7: Statistics of how often visual editing are performed.", "description": "This figure presents a bar chart visualizing the frequency of visual editing actions performed by the REFOCUS framework across different datasets. For each dataset (VWTQ, VWTQ_syn, VTabFact, CharXiv, Horizontal Bar, Vertical Bar), a bar represents the percentage of images where the underlying multimodal LLM (GPT-4 in this case) chose to perform visual editing.  The chart illustrates the varying degrees to which visual editing was deemed necessary by the model across different types of structured image data.", "section": "4. Experiments and Analyses"}, {"figure_path": "https://arxiv.org/html/2501.05452/x13.png", "caption": "Figure 8: Phi-3.5-vision finetuned with ReFocus visual chain of thought data outputs the areas to focus on. For illustration purposes, we draw these areas in red boxes, and compare with the ReFocus + GPT-4o prompting output.", "description": "This figure displays a comparison of how the Phi-3.5-vision model, fine-tuned with ReFocus visual chain of thought data, identifies areas of focus in images compared to the ReFocus + GPT-4o prompting method.  The Phi-3.5-vision model outputs are presented as text with bounding box coordinates for the areas to focus on. These areas are highlighted with red boxes in the figure for clarity.  Multiple examples from Horizontal Bar and Vertical Bar datasets are shown, illustrating the model's ability to pinpoint key visual elements for improved reasoning.  The comparison visually demonstrates the effectiveness of the ReFocus technique in guiding the model towards accurate interpretation of charts.", "section": "D. SFT Qualitative Examples"}]