[{"figure_path": "https://arxiv.org/html/2502.07316/x1.png", "caption": "Figure 1: Overview of our training data construction: Raw code files are gathered from various sources and converted into a unified format. Input-output pairs are then generated by executing the code, while natural language CoTs for predictions are collected from DeepSeek-V2.5. The verified CoTs can undergo optional revisions to further enhance reasoning chains.", "description": "The figure illustrates the process of creating the training dataset for CODEI/O.  It starts with gathering raw code files from diverse sources. These files are then processed and transformed into a standardized format.  Next, input and output pairs are generated by executing the code. Simultaneously, natural language Chain-of-Thought (CoT) reasoning is obtained from the DeepSeek-V2.5 model to explain these input-output predictions.  Finally, the CoTs are verified and optionally revised to improve the reasoning process. This iterative refinement step helps improve the quality of the training data.", "section": "2. CODEI/O"}, {"figure_path": "https://arxiv.org/html/2502.07316/x2.png", "caption": "Figure 2: Two examples for the collected responses for input and output prediction respectively.", "description": "This figure displays two examples from the CODEI/O dataset illustrating the model's input and output prediction capabilities. The left example shows a prediction for the output given a specified input.  A detailed chain of thought (CoT) is presented, demonstrating how the model arrives at its answer. The right example demonstrates the opposite: predicting the input that would result in a given output. Again, a CoT is shown, outlining the reasoning process.  These examples showcase the model's ability to reason using natural language CoTs, given the code and a query.", "section": "2. CODEI/O"}, {"figure_path": "https://arxiv.org/html/2502.07316/x3.png", "caption": "Figure 3: Average scores of Stage 1 training on CodeI/O, a 3.5M WebInstruct subset (WI) and an enhanced version distilled from DeepSeek-V2.5 Directly (WI-DS25).", "description": "This figure compares the performance of training a language model on three different datasets in the first stage of a two-stage training process.  The three datasets are: 1) CODEI/O, a new dataset created by the authors; 2) WI, a 3.5 million sample subset of the WebInstruct dataset, representing a strong existing instruction tuning dataset; and 3) WI-DS25, an enhanced version of the WebInstruct dataset created by directly distilling knowledge from the DeepSeek-V2.5 model. The figure shows the average scores achieved on various downstream benchmarks after training on these datasets.  This allows the reader to compare the effectiveness of the newly proposed CODEI/O dataset against strong existing baselines, showing its improved performance on diverse reasoning tasks. ", "section": "3. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.07316/x4.png", "caption": "(a) Size of randomly sampled subset.", "description": "This figure shows how the model's performance changes as the size of the training dataset varies.  The x-axis represents the size of a randomly selected subset of the CODEI/O training data, ranging from a small fraction of the full dataset to the complete dataset. The y-axis represents the average performance across multiple reasoning benchmarks. The plot illustrates the relationship between training data size and model performance, demonstrating the scaling behavior of the model.", "section": "4.3. Scaling Effect of CODEI/O"}, {"figure_path": "https://arxiv.org/html/2502.07316/x5.png", "caption": "(b) Ratio of testcases per sample compared to the full set.", "description": "This figure shows how the model's performance changes when varying the number of input/output test cases per training sample, while keeping the total number of training samples constant. The x-axis represents the ratio of testcases used compared to the full dataset, showing the scaling effect of the CODEI/O dataset.", "section": "4.3. Scaling Effect of CODEI/O"}, {"figure_path": "https://arxiv.org/html/2502.07316/x6.png", "caption": "Figure 4: The scaling effect of CodeI/O in the first stage training.", "description": "This figure demonstrates the impact of the size and quantity of training data derived from CodeI/O on model performance.  The plots show that increasing the size of the training dataset (Panel a) and the number of input/output examples per training sample (Panel b) leads to consistent improvements across various reasoning benchmarks.  Larger datasets and more input/output pairs result in better model generalization and higher performance. The results highlight the effectiveness and scalability of the CodeI/O training approach.", "section": "3. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.07316/x7.png", "caption": "Figure 5: Average benchmark scores from training on data from different turns of revision.", "description": "This figure displays the average benchmark scores achieved by training language models on data from different revision turns.  The x-axis represents the revision turn (0, 1, or 2), indicating whether the model's responses were used directly (Turn 0), corrected once (Turn 1), or corrected twice (Turn 2). The y-axis shows the average score across multiple benchmark tasks.  The different colored lines likely represent different language models or model sizes, allowing for comparison of performance changes across different models when training on progressively revised data.  The purpose is to illustrate the impact of iterative response refinement on model performance.", "section": "4. Analysis"}, {"figure_path": "https://arxiv.org/html/2502.07316/x8.png", "caption": "Figure 6: An example in the constructed Leetcode-O benchmark.", "description": "The figure shows an example problem from the LeetCode-O benchmark dataset.  This dataset is specifically designed to test the model's ability to predict the output of a code given a textual description of the problem, without providing the actual code itself. The example demonstrates a common type of coding problem requiring logical reasoning and algorithmic thinking. The user is given a problem involving coins and asked to determine the number of complete rows in a staircase built using those coins, with the instructions to provide the final answer in a specific JSON format. This illustrates the style and complexity of problems in the LeetCode-O benchmark.", "section": "3. Experiments"}]