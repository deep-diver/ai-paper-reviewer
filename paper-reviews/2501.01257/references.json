{"references": [{"fullname_first_author": "Dan Hendrycks", "paper_title": "Measuring coding challenge competence with apps", "publication_date": "2021-05-00", "reason": "This paper introduced APPS, a benchmark for evaluating coding abilities that the current paper compares against and improves upon."}, {"fullname_first_author": "Yujia Li", "paper_title": "Competition-level code generation with alphacode", "publication_date": "2022-00-00", "reason": "This paper is highly relevant as it introduces a benchmark for code generation that the current paper references and builds upon for its novel benchmark."}, {"fullname_first_author": "Jacob Austin", "paper_title": "Program synthesis with large language models", "publication_date": "2021-08-00", "reason": "This paper is important because it discusses the use of large language models for code generation, which is the topic of the current paper."}, {"fullname_first_author": "Mark Chen", "paper_title": "Evaluating large language models trained on code", "publication_date": "2021-07-00", "reason": "This paper is relevant to the current paper because it presents a benchmark for evaluating large language models, which the current paper also does."}, {"fullname_first_author": "Naman Jain", "paper_title": "LiveCodeBench: Holistic and contamination free evaluation of large language models for code", "publication_date": "2024-03-00", "reason": "This paper is important because it discusses another benchmark for code generation, which is compared with the current paper's benchmark"}]}