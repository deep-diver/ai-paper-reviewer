[{"content": "| Problem | Difficulty | Updates | Zero False | Positive? | Special | Judge? | Aligned | Execution | Environment? | Standardized | Elo Rating? |\n|---|---|---|---|---|---|---|---|---|---|---|---|\n| APPS | \u2605\u2605 | No updates | \u2717 | \u2717 | \u2717 | \u2717 | \u2717 | \u2717 |  | \u2717 |  |\n| CodeContests | \u2605\u2605\u2605 | No updates | \u2717 | \u2717 | \u2717 | \u2717 | \u2717 | \u2717 |  | \u2717 |  |\n| TACO | \u2605\u2605 | No updates | \u2717 | \u2717 | \u2717 | \u2717 | \u2717 | \u2717 |  | \u2717 |  |\n| xCodeEval | \u2605 | No updates | \u2717 | \u2717 | \u2717 | \u2717 | \u2717 | \u2717 |  | \u2717 |  |\n| USACO | \u2605\u2605 | Offline | \u2713 | \u2717 | \u2717 | \u2717 | \u2717 | \u2717 |  | \u2717 |  |\n| LiveCodeBench | \u2605 | Online | \u2717 | \u2717 | \u2717 | \u2717 | \u2717 | \u2717 |  | \u2717 |  |\n| CodeForces | \u2605\u2605\u2605 | Online | \u2713 | \u2713 | \u2713 | \u2713 | \u2713 | \u2713 |  | \u2713 |  |", "caption": "Table 1: Comparison between CodeForces and other competition code benchmarks.", "description": "This table compares CODEELO with other existing competitive programming benchmarks across several key features.  These features include whether the benchmark provides updates, ensures zero false positives in its evaluation, supports special judges (for problems with multiple valid solutions), offers an aligned execution environment to prevent inconsistencies, and whether it uses a standardized Elo rating system to rank and compare model performance.", "section": "2 Related Work"}, {"content": "| Problem | Diffculty |\n|---|---|", "caption": "Table 2: Basic statics of different contest divisions.", "description": "Table 2 presents a statistical summary of CodeForces contests categorized by difficulty division.  It shows the number of contests within each division (Div. 1, Div. 1+2, Div. 2, Div. 3, Div. 4), the average number of problems per contest within each division, the average problem rating across the problems in each division, and the approximate CodeForces rating range required to successfully solve problems in each division.", "section": "4.1 Experiment Setup"}, {"content": "| Zero False | Positive? |\n|---|---|", "caption": "Table 3: Main results of different LLMs on CodeForces. The number in parentheses after the overall Elo rating shows the percentile rank among human participants. The underlined numbers represent the best scores within the same model size range.", "description": "This table presents the performance of various Large Language Models (LLMs) on the CODEELO benchmark, a competition-level code generation benchmark based on CodeForces problems.  For each LLM, it shows the overall Elo rating (a measure of skill relative to human competitors), pass rates at different problem difficulty levels (Easy, Medium, Hard), and pass rates at various numbers of attempts (pass@1, pass@2, pass@4, pass@8).  The Elo rating is accompanied by the percentile rank the model achieved compared to human participants.  The table also categorizes LLMs by size (1B+, 6B+, 13B+, 30B+), highlighting the best performance for each size category.", "section": "4 Evaluation on Existing LLMs"}, {"content": "| Special | Judge? |\n|---|---|", "caption": "Table 4: Pass rate (pass@1111) on major problem categories that have at least 30 problems tested. The abbreviations \"Gr.\", \"Ma.\", \"Im.\", \"BF.\", \"DP\", \"DS.\", \"CA.\", \"BS.\", \"So.\", \"Gr.\", \"DFS\", \"NT.\", \"Tr.\", \"Co.\", \"TP.\", and \"Bi.\" stand for greedy, math, implementation, brute force, dp, data structures, constructive algorithms, binary search, sortings, graphs, dfs and similar, number theory, trees, combinatorics, two pointers, and bitmasks, respectively.", "description": "This table presents the success rate (pass@1) of various large language models (LLMs) across 16 common algorithm categories in solving CodeForces problems.  Each category contains at least 30 problems, and the table shows the percentage of problems each model correctly solved on its first attempt. The abbreviations used represent different algorithm types, such as 'Gr.' for greedy algorithms, 'Ma.' for math-based problems, 'Im.' for implementation problems, 'BF.' for brute force, 'DP' for dynamic programming, 'DS.' for data structures, 'CA.' for constructive algorithms, 'BS.' for binary search, 'So.' for sorting algorithms, 'Gr.' for graph algorithms, 'DFS' for Depth-First Search and similar graph traversal algorithms, 'NT.' for number theory problems, 'Tr.' for tree-based problems, 'Co.' for combinatorics, 'TP.' for two-pointer algorithms, and 'Bi.' for bitmask algorithms. This detailed breakdown helps analyze the strengths and weaknesses of different LLMs across various algorithm categories.", "section": "5.1 Performance Across Algorithms"}, {"content": "| Aligned | Execution | Environment? |\n|---|---|---|\n", "caption": "Table 5: Model cards.", "description": "This table lists all the Large Language Models (LLMs) used in the CODEELO benchmark, providing their short names, citations to their original papers, and HuggingFace endpoints where available.", "section": "A Model Cards"}, {"content": "| Standardized | Elo Rating? |\n|---|---|", "caption": "Table 6: Percentiles of ratings among all human participants, calculated based on publicly available user ratings from the CodeForces platform, collected in November, 2024.", "description": "This table shows the distribution of Elo ratings among all human participants on the CodeForces platform during November 2024.  Each row represents a percentile (e.g., the 26th percentile means that 26% of human participants had a rating equal to or lower than the rating listed in that row).  It illustrates the range of skill levels present within the CodeForces community and provides context for comparing the performance of large language models (LLMs) against human coders, as reported in the paper.", "section": "4 Evaluation on Existing LLMs"}]