{"importance": "This paper is important because it introduces **CODEELO**, a novel benchmark for evaluating large language models' code generation capabilities.  It addresses limitations of existing benchmarks by using a **standardized competition-level coding platform**, enabling more accurate and human-comparable evaluations. This provides crucial insights into LLM strengths and weaknesses in code generation, paving the way for improved model development and fairer comparisons.", "summary": "CODEELO benchmark uses CodeForces to fairly evaluate LLMs' coding abilities, providing human-comparable Elo ratings and addressing limitations of existing benchmarks.", "takeaways": ["CODEELO benchmark offers a standardized, competition-level evaluation of LLMs' code generation abilities, overcoming limitations of previous methods.", "The benchmark employs a unique judging method using the CodeForces platform, achieving zero false positives and supporting special judges, resulting in human-comparable Elo ratings.", "Analysis reveals that models excel in math and implementation problems but struggle with dynamic programming and tree algorithms; C++ outperforms Python in most models."], "tldr": "Current benchmarks for evaluating large language models' (LLMs) coding skills are inadequate, lacking comprehensive test suites, private test cases, and proper judge support, leading to inaccurate results and misaligned evaluation environments. This research introduces CODEELO, a new benchmark designed to address these issues.  It leverages the CodeForces competitive coding platform, ensuring accurate and standardized evaluations. \nCODEELO uses a unique judging system by directly submitting code to the CodeForces platform, guaranteeing reliable results.  The benchmark categorizes problems by difficulty and algorithm type, offering detailed performance analysis.  The results reveal that while some LLMs perform exceptionally well, others struggle even with basic tasks, highlighting areas for improvement. CODEELO also introduces a human-comparable Elo rating system, enabling more precise comparisons between LLMs and human coders.", "affiliation": "Alibaba Group", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}, "podcast_path": "2501.01257/podcast.wav"}