[{"figure_path": "2410.13639/charts/charts_6_0.png", "caption": "Figure 1: The statistics of different reasoning patterns on different benchmarks.", "description": "The chart displays the frequency of six different reasoning patterns (SA, MR, DC, SR, CI, EC) used by the OpenAI's o1 model across four different benchmark datasets (HotpotQA, Collie, AIME, USACO).", "section": "4.2 ANALYSIS OF THE REASONING PATTERN OF 01"}, {"figure_path": "2410.13639/charts/charts_6_1.png", "caption": "Figure 2: The statistics of reasoning patterns.", "description": "The bar chart displays the frequency of six different reasoning patterns (SA, MR, DC, SR, CI, EC) observed in the OpenAI's o1 model across various tasks.", "section": "4.2 ANALYSIS OF THE REASONING PATTERN OF 01"}, {"figure_path": "2410.13639/charts/charts_7_0.png", "caption": "Figure 3: The statistics of the number of o1's reasoning tokens on different tasks. 'ALL' represents the average length of reasoning tokens for all samples, while 'True' and 'False' show the averages for correctly and incorrectly answered samples, respectively. 'Input' refers to the average length of the input prompt.", "description": "Figure 3 is a bar chart that displays the average number of reasoning tokens used by the o1 model across different tasks, categorized by correct/incorrect answers and input prompt length.", "section": "4.4 THE NUMBER OF REASONING TOKENS ACROSS DIFFERENT TASKS FOR 01"}, {"figure_path": "2410.13639/charts/charts_8_0.png", "caption": "Figure 4: The results of BoN( GPT-40) using different reward models under N = 4 setting. The SRG represents the Skywork-Reward-Gemma-2-27B, the URM-LLaMa refers to the URM-LLaMa-3.1-8B.", "description": "The chart displays the performance of Best-of-N (BoN) using GPT-40 with different reward models (SRG, URM-LLaMa, GPT40, and Human) on HotpotQA and Collie datasets.", "section": "4.5 THE REWARD MODEL LIMITS ABILITIES OF SEARCHING METHODS"}, {"figure_path": "2410.13639/charts/charts_8_1.png", "caption": "Figure 5: The results of BoN under different search spaces (i.e., the N ranging from 1 to 16) on HotpotQA.", "description": "The chart displays the performance of the Best-of-N (BoN) method on the HotpotQA dataset using different base models (GPT-40, Qwen2.5, Llama3) with varying numbers of generated outputs (N).", "section": "4.6 THE SEARCH SPACE ALSO DETERMINES THE UPPER BOUNDARY OF LLMS"}, {"figure_path": "2410.13639/charts/charts_8_2.png", "caption": "Figure 6: The results of o1 model on AIME24, AIME23, and AIME22.", "description": "The chart displays the performance of three variants of the o1 model (o1-preview, o1-web, and o1-mini) on three different AIME datasets (AIME22, AIME23, and AIME24).", "section": "4.8 THE MATH ABILITY OF OPENAI'S 01"}, {"figure_path": "2410.13639/charts/charts_9_0.png", "caption": "Figure 8: The results of the LLMs on the raw bench and the filtered bench. On the left subfigure, we present LLMs' capabilities on the raw and filtered HotpotQA, and on the right subfigure, we provide the corresponding results on Collie.", "description": "The chart compares the performance of different LLMs (Qwen2.5-72B, Llama3-72B, GPT-40) on raw and filtered versions of HotpotQA and Collie datasets.", "section": "4.7 ANALYSIS ON DATA FILTER"}]