{"references": [{" publication_date": "2023", "fullname_first_author": "Josh Achiam", "paper_title": "Gpt-4 technical report", "reason": "This paper is a foundational technical report that details the architecture and capabilities of GPT-4, a highly influential large language model.  Understanding GPT-4's capabilities is crucial to understanding the context and advancements in the field of LLMs, which this paper directly contributes to, and because the current paper uses GPT-4 as a backbone for comparison.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Jinze Bai", "paper_title": "Qwen technical report", "reason": "This technical report details the architecture and performance of Qwen, a large language model, which is used as one of the baselines in this study.  Understanding Qwen's architecture and performance characteristics is essential for a proper comparative analysis in the context of LLMs and Test-time Compute methods.", "section_number": 3}, {" publication_date": "2020", "fullname_first_author": "Tom B Brown", "paper_title": "Language models are few-shot learners", "reason": "This seminal paper established the impressive few-shot learning capabilities of large language models, a key concept relevant to the current study because the o1 model leverages these capabilities.  The paper fundamentally changed the understanding and expectations for LLMs.", "section_number": 2}, {" publication_date": "2024a", "fullname_first_author": "Linzheng Chai", "paper_title": "Mceval: Massively multilingual code evaluation", "reason": "This paper introduces a benchmark for evaluating multilingual code generation in LLMs, which is directly related to the current study's focus on evaluating LLMs across various tasks, including coding. The evaluation methods and insights are directly relevant to the current research.", "section_number": 3}, {" publication_date": "2024b", "fullname_first_author": "Linzheng Chai", "paper_title": "xcot: Cross-lingual instruction tuning for cross-lingual chain-of-thought reasoning", "reason": "This work is significant because it explores the capabilities of chain-of-thought prompting in LLMs, a reasoning technique relevant to the current study which investigates reasoning patterns.  The study's results are directly applicable to the analysis of reasoning in this paper.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Abhimanyu Dubey", "paper_title": "The llama 3 herd of models", "reason": "This paper introduces Llama 3, a large language model used as a baseline in the current study's benchmarks. Understanding Llama 3's characteristics is essential for comparative analysis of different LLMs and their reasoning capabilities.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Luyu Gao", "paper_title": "Pal: Program-aided language models", "reason": "This paper introduces program-aided language models, providing an alternative approach to enhancing LLMs' capabilities.  Understanding this approach adds context to the current research exploring Test-Time Compute as an alternative for boosting LLM reasoning.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Sachin Goyal", "paper_title": "Think before you speak: Training language models with pause tokens", "reason": "This study explores enhancing LLMs by introducing pause tokens during training, an approach related to Test-time Compute methods explored in the current paper, improving the reasoning capabilities of LLMs. This directly supports the research by providing an additional technique for improvement.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "Tom Henighan", "paper_title": "Scaling laws for autoregressive generative modeling", "reason": "This highly influential paper established the scaling laws governing the performance of large language models.  Understanding these scaling laws provides the foundation for comprehending the limitations of simply scaling up model size, which is a central theme of this paper.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Naman Jain", "paper_title": "Livecodebench: Holistic and contamination free evaluation of large language models for code", "reason": "This paper introduces a benchmark for evaluating large language models' code generation capabilities, directly relevant to one of the domains explored in the current study. Understanding this benchmark's methodology and insights informs the current research's approach.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Nathan Lambert", "paper_title": "Rewardbench: Evaluating reward models for language modeling", "reason": "This paper is highly relevant because the current study uses reward models in some of its baseline methods.  Understanding reward model evaluation is critical for interpreting the results of methods like Best-of-N (BoN).", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Aman Madaan", "paper_title": "Self-refine: Iterative refinement with self-feedback", "reason": "This paper introduces Self-Refine, a Test-time Compute method that is directly compared in this study.  Understanding Self-Refine's mechanism and limitations provides a foundation for comparing it to OpenAI's o1 model and other methods.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Ankit Satpute", "paper_title": "Can llms master math? investigating large language models on math stack exchange", "reason": "This paper investigates LLMs' capabilities in solving mathematical problems.  The study's findings are directly relevant to evaluating mathematical reasoning, one of the main domains explored in the current paper, making it a key reference.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Quan Shi", "paper_title": "Can language models solve olympiad programming?", "reason": "This paper directly addresses the capabilities of LLMs in solving programming problems, a key area of focus in this paper's benchmarks.  The findings provide direct context for evaluating the o1 model's performance in the code generation domain.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Charlie Snell", "paper_title": "Scaling llm test-time compute optimally can be more effective than scaling model parameters", "reason": "This paper directly addresses the core motivation behind Test-time Compute methods, which is the focus of this paper. It provides theoretical justification for the research question and validates the overall approach.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Hugo Touvron", "paper_title": "Llama: Open and efficient foundation language models", "reason": "This paper introduces the Llama large language model family, including Llama 3, which serves as one of the baselines in the current study.  Understanding Llama's characteristics is crucial for a proper comparison and analysis of different models.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Ruocheng Wang", "paper_title": "Hypothesis search: Inductive reasoning with language models", "reason": "This paper explores hypothesis search as a method for inductive reasoning in LLMs, an approach related to the test-time compute methods explored in this paper. Understanding this method allows for a more comprehensive comparison and analysis of different reasoning techniques in LLMs.", "section_number": 2}, {" publication_date": "2017", "fullname_first_author": "A Vaswani", "paper_title": "Attention is all you need", "reason": "This highly influential paper introduced the Transformer architecture, which forms the basis of most modern large language models, including those used in this study. Understanding the Transformer architecture is fundamental to understanding the underlying mechanisms of LLMs.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "An Yang", "paper_title": "Qwen2 technical report", "reason": "This technical report details the architecture and capabilities of Qwen-2, which provides context for the current study, which uses Qwen as a baseline.  Understanding Qwen-2 allows for a better comparison and analysis of the reasoning capabilities of different LLMs.", "section_number": 3}, {" publication_date": "2018", "fullname_first_author": "Zhilin Yang", "paper_title": "Hotpotqa: A dataset for diverse, explainable multi-hop question answering", "reason": "This paper introduces the HotpotQA dataset, a benchmark used in the current study.  Understanding the characteristics of HotpotQA is crucial for interpreting the results and comparing the performance of different models on this specific commonsense reasoning task.", "section_number": 3}]}