[{"figure_path": "https://arxiv.org/html/2411.02319/x1.png", "caption": "Figure 1: Gen\ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic_XD is a unified model for high-quality 3D and 4D generation from any number of condition images. By controlling the motion strength and condition masks, Gen\ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic_XD can support various application without any modification. The condition images are shown with star icon and the time dimension is illustrated with dash line.", "description": "This figure showcases the GenXD model's capabilities in generating high-quality 3D and 4D scenes from various numbers of input images.  The model takes condition images (marked with a star icon) as input and can be controlled to generate outputs with different degrees of motion (indicated by dashed lines representing the time dimension). The significance lies in GenXD's ability to handle both 3D (static) and 4D (dynamic) generation tasks within a single unified framework, adapting seamlessly to diverse application needs without requiring any model adjustments. The four subfigures illustrate the model's performance across different scenarios: single-view 3D generation, multi-view 3D generation, single-view 4D generation, and multi-view 4D generation.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2411.02319/x2.png", "caption": "Figure 2: The pipeline for CamVid-30K data curation, including (a) camera pose estimation and (b) object motion estimation. We first leverage mask-based SfM (masks are overlayed to images in (a) for visualization) to estimate camera pose and reconstruct 3D point clouds of static parts. Then relative depth is aligned with the sparse depth and project the tracking keypoints to consecutive frame for object motion estimation.", "description": "This figure illustrates the data curation pipeline used to create the CamVid-30K dataset.  The pipeline consists of two main stages: camera pose estimation and object motion estimation. Camera pose estimation starts by using Structure-from-Motion (SfM) on masked images to reconstruct 3D point clouds from the static elements in the scene. This process leverages masks that highlight the static areas.  Next, relative depth is estimated, aligned with the sparse depth obtained from SfM, and used to project the tracking keypoints onto consecutive frames. Object motion estimation involves identifying moving objects, calculating their motion field in the 2D video frames using keypoint tracking. The motion field helps determine the true object motion, removing static scenes from the data, and finally resulting in the CamVid-30K dataset.", "section": "3 CAMVID-30K"}, {"figure_path": "https://arxiv.org/html/2411.02319/x3.png", "caption": "Figure 3: Examples for object motion estimation.\nThe motion strength is multiplied by 100. In the first example, the girl is dancing, together with the camera moving. In the second example, the camera is zooming in (red rectangle for better illustration) but the object is static. In this case, the motion strength is much smaller.", "description": "Figure 3 illustrates object motion estimation using motion strength, a metric multiplied by 100 for visualization.  The left panel shows a scenario where a girl is dancing while the camera also moves; this results in a relatively high motion strength value. The right panel presents a case where the camera zooms in on a static object. Here, the motion strength is significantly lower, as the object itself is not moving, despite camera movement.", "section": "3 CAMVID-30K"}, {"figure_path": "https://arxiv.org/html/2411.02319/x4.png", "caption": "Figure 4: The framework of Gen\ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic_XD.\nWe leverage mask latent conditioned diffusion model to generate 3D and 4D samples with both camera (colorful map) and image (binary map) conditions. In addition, multiview-temporal modules together with \u03b1\ud835\udefc\\alphaitalic_\u03b1-fusing are proposed to effectively disentangle and fuse multiview and temporal information.", "description": "This figure illustrates the architecture of the GenXD model, a unified framework for generating 3D and 4D scenes from various input conditions.  The core of the model is a masked latent conditioned diffusion model, which processes both camera pose information (represented as a colorful map) and image content (as a binary map) to produce 3D and 4D outputs.  The model incorporates multiview-temporal modules that effectively separate camera and object movements within the scene and combine the spatial and temporal information via alpha-fusing, allowing for consistent generation of dynamic scenes across multiple viewpoints.", "section": "4 GenXD"}, {"figure_path": "https://arxiv.org/html/2411.02319/x5.png", "caption": "Figure 5: Qualitative comparison with camera conditioned video generation methods. Gen\ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic_XD can generate video well-aligned with camera trajectory and containing realistic object motion. (Please refer to supplementary video for better illustration.)", "description": "Figure 5 presents a qualitative comparison of GenXD against other camera-conditioned video generation methods (CameraCtrl and MotionCtrl).  It showcases GenXD's ability to generate videos where the object motion is realistic and aligns well with the camera's trajectory.  The figure visually demonstrates GenXD's superior performance in handling both camera movement and object motion simultaneously, resulting in more natural and coherent video sequences compared to the other methods.  The caption encourages viewers to consult the supplementary video for a more detailed comparison.", "section": "5.2 4D Generation"}, {"figure_path": "https://arxiv.org/html/2411.02319/x6.png", "caption": "Table 2: 4D scene generation.", "description": "This table presents a quantitative comparison of different methods for 4D scene generation.  It shows the Fr\u00e9chet Inception Distance (FID) and Fr\u00e9chet Video Distance (FVD) scores for several methods, including MotionCtrl, CameraCtrl, Animate124, and GenXD (both single-view and multi-view). Lower FID and FVD scores indicate better performance. The results demonstrate the superior performance of GenXD, particularly in the multi-view setting, compared to existing state-of-the-art methods.", "section": "5.1 Experimental Setup"}, {"figure_path": "https://arxiv.org/html/2411.02319/x7.png", "caption": "Table 3: 4D object generation.", "description": "This table presents a quantitative comparison of different methods for 4D object generation.  It compares the methods across two metrics: generation time and CLIP-I (a measure of image quality). The methods being compared include several existing 4D object generation approaches as well as the authors' proposed method, GenXD, in both single-view and multi-view configurations. This allows for a quantitative assessment of GenXD's performance compared to state-of-the-art methods.", "section": "4D object generation"}, {"figure_path": "https://arxiv.org/html/2411.02319/x8.png", "caption": "Figure 6: Qualitative comparison of few-view 3D reconstruction.", "description": "This figure displays a qualitative comparison of 3D reconstruction results from various methods using only a few input views.  It visually demonstrates the differences in reconstruction quality achieved by different approaches, showcasing the impact of limited input data on the resulting 3D models. The image showcases several methods' performance on the task, illustrating how different techniques might handle the challenges of reconstructing a complete 3D scene from sparse viewpoints.", "section": "5.3 3D GENERATION"}, {"figure_path": "https://arxiv.org/html/2411.02319/x9.png", "caption": "Figure 7: Qualitative evaluation on the influence of motion strength. (Please refer to supplementary video for better illustration.)", "description": "This figure displays a qualitative evaluation of how the 'motion strength' parameter affects the results of 3D and 4D generation.  It showcases the effect of varying motion strength on the generated videos, demonstrating the controllability offered by this parameter.  Different levels of motion strength are compared across several generated video sequences, showing how the intensity of motion changes as motion strength increases.  The videos generated with varied motion strength show the varying degrees of movement of the objects within the scene, ranging from almost static to intense motion.  Reference to a supplementary video is provided for detailed visual understanding.", "section": "5.4 Ablation Study"}, {"figure_path": "https://arxiv.org/html/2411.02319/x10.png", "caption": "Figure 8: The visualization of the generated 4D videos. (Please refer to supplementary video for better illustration.)", "description": "Figure 8 presents a visualization of 4D videos generated using the GenXD model. The figure showcases several examples, each featuring a sequence of frames illustrating the dynamic evolution of a scene over time. Due to the limitations of a static image format, it is highly recommended to refer to the supplementary video provided in the paper for a complete and more effective illustration of the generated 4D videos. The supplementary video allows for dynamic viewing of the generated content.", "section": "A MORE QUALITATIVE RESULTS"}]