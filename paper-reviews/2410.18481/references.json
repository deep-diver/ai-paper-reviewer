{"references": [{" publication_date": "2023", "fullname_first_author": "S\u00e9bastien Bubeck", "paper_title": "Sparks of artificial general intelligence: Early experiments with GPT-4", "reason": "This paper is highly relevant as it discusses the advancements in large language models (LLMs), a key element influencing the development of conversational AI and the methodology used in the current study.  The discussion of GPT-4's capabilities and limitations helps to contextualize the current work, particularly concerning the potential and limitations of LLMs in handling complex dialog structures and generating accurate responses.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Pan Lu", "paper_title": "Learn to explain: Multimodal reasoning via thought chains for science question answering", "reason": "This paper discusses the use of LLMs for multimodal reasoning and knowledge extraction, providing context for the application of LLMs in the analysis and understanding of dialogue data.  The integration of multimodal context and reasoning is relevant to the task of workflow extraction, potentially contributing to more comprehensive and contextually-aware approaches.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Dan Hendrycks", "paper_title": "Measuring massive multitask language understanding", "reason": "This paper provides a framework and insights into evaluating LLM performance across various tasks.  This is directly relevant to the current study, which assesses the performance of different embedding models on related tasks such as dialog flow extraction and action classification.  The methodology used in this paper for benchmarking can inform the current study's design and analysis.", "section_number": 1}, {" publication_date": "2021", "fullname_first_author": "Dan Hendrycks", "paper_title": "Measuring mathematical problem solving with the math dataset", "reason": "This paper focuses on evaluating the capabilities of LLMs in solving mathematical problems, showcasing a methodology for evaluating performance in specific domains.  The current study also evaluates the performance of LLMs on a specific domain (dialog flow extraction), and the evaluation methodologies from this paper provide useful insights for rigorous assessment.", "section_number": 1}, {" publication_date": "2021", "fullname_first_author": "Karl Cobbe", "paper_title": "Training verifiers to solve math word problems", "reason": "This paper demonstrates how LLMs can be used to perform complex reasoning tasks.   The current work uses LLMs in a downstream task to improve the human readability and interpretability of the generated results.  The techniques presented in this paper could provide useful guidance on prompt engineering and the use of LLMs for this purpose.", "section_number": 1}, {" publication_date": "2018", "fullname_first_author": "Daniel Cer", "paper_title": "Universal sentence encoder", "reason": "This paper is a seminal work in sentence embeddings, introducing a widely used model.  The current study benchmarks its performance against newer models and highlights its contribution to sentence representation learning, forming a fundamental building block of the proposed method.", "section_number": 2}, {" publication_date": "2019", "fullname_first_author": "Nils Reimers", "paper_title": "Sentence-BERT: Sentence embeddings using Siamese BERT-networks", "reason": "Sentence-BERT is a widely used and highly effective sentence embedding model used as a baseline in this study.  Understanding Sentence-BERT's architecture and performance characteristics is critical for evaluating the proposed method's effectiveness relative to state-of-the-art techniques.", "section_number": 2}, {" publication_date": "2014", "fullname_first_author": "Jeffrey Pennington", "paper_title": "GloVe: Global vectors for word representation", "reason": "GloVe is a fundamental and well-established word embedding model, and its performance is benchmarked in the current study.  Understanding its strengths and weaknesses in the context of sentence embeddings helps to contextualize the improvements offered by the proposed approach.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "Chien-Sheng Wu", "paper_title": "TOD-BERT: Pre-trained natural language understanding for task-oriented dialogue", "reason": "TOD-BERT is a key dialog-specific embedding model used as a baseline.  Its design and performance in task-oriented dialogs provide a strong basis for comparison, helping to demonstrate the effectiveness of the proposed method relative to existing state-of-the-art approaches in the domain.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Che Liu", "paper_title": "DialogueCSE: Dialogue-based contrastive learning of sentence embeddings", "reason": "DialogueCSE is a key dialog-specific embedding model used as a baseline.  Its use of contrastive learning and focus on conversation-based similarity provide a critical comparison point for evaluating the performance and novelty of the proposed method in a similar context.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Zhihan Zhou", "paper_title": "Learning dialogue representations from consecutive utterances", "reason": "This paper introduces a dialog-specific sentence embedding model that serves as a strong baseline in this study.  Comparing the performance of the proposed method against DialogueCSE and DSE helps to evaluate its novelty and effectiveness in addressing the challenges of dialog representation learning.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "Ting Chen", "paper_title": "A simple framework for contrastive learning of visual representations", "reason": "This paper introduces a foundational framework for contrastive learning that is highly relevant to the core methodology of the current study.  The principles of contrastive learning are central to the proposed method, and understanding this foundational work helps to contextualize its application in the domain of dialog embeddings.", "section_number": 3}, {" publication_date": "2020", "fullname_first_author": "Kaiming He", "paper_title": "Momentum contrast for unsupervised visual representation learning", "reason": "This paper showcases the use of contrastive learning for unsupervised visual representation learning, which is closely related to the current study's approach. The techniques and insights discussed in this paper contribute directly to the understanding and application of contrastive learning for sentence embeddings in the context of dialog flows.", "section_number": 3}, {" publication_date": "2019", "fullname_first_author": "R Devon Hjelm", "paper_title": "Learning deep representations by mutual information estimation and maximization", "reason": "This paper presents a method for learning deep representations using mutual information estimation and maximization. The underlying principles of mutual information maximization are relevant to contrastive learning, and the techniques discussed in this paper provide further context for the current study's approach to learn meaningful representations of sentences in dialogs.", "section_number": 3}, {" publication_date": "2020", "fullname_first_author": "Prannay Khosla", "paper_title": "Supervised contrastive learning", "reason": "This paper introduces supervised contrastive learning, a key technique used in this study.  The method's principles and application in representation learning are crucial for understanding the proposed method's approach to learning action-based sentence embeddings.", "section_number": 4}, {" publication_date": "2020", "fullname_first_author": "Jianmo Ni", "paper_title": "Large dual encoders are generalizable retrievers", "reason": "This paper introduces a generalizable T5-based dense retriever, which serves as a strong baseline for comparison against other sentence embedding models. This detailed description enables a robust comparison, particularly against general-purpose methods, helping to demonstrate the advantages of the proposed D2F model and its capacity for generalized task-oriented dialogue.", "section_number": 6}, {" publication_date": "2024", "fullname_first_author": "OpenAI", "paper_title": "New embedding models and api updates", "reason": "This paper introduces the OpenAI text-embedding-3-large model, which is a recent and powerful sentence embedding model used as a baseline for comparison.  The model's performance characteristics help to contextualize the performance and improvements offered by the Dialog2Flow (D2F) embeddings.", "section_number": 6}, {" publication_date": "2020", "fullname_first_author": "Chien-Sheng Wu", "paper_title": "TOD-BERT: Pre-trained natural language understanding for task-oriented dialogue", "reason": "TOD-BERT is a prominent model specifically designed for task-oriented dialogues, offering a strong baseline for comparison. Understanding its architecture, pre-training strategy, and performance characteristics is crucial for assessing the proposed method's novel contributions in the domain of dialog representation learning.", "section_number": 6}, {" publication_date": "2022", "fullname_first_author": "Liang Qiu", "paper_title": "Structure extraction in task-oriented dialogues with slot clustering", "reason": "This paper focuses on structure extraction in task-oriented dialogues, using a slot clustering approach. The approach is directly relevant to this study's goal of automatically extracting dialog flows, and it offers a critical comparison point for evaluating the performance and novelty of the proposed Dialog2Flow (D2F) embeddings.", "section_number": 6}]}