{"references": [{" publication_date": "2024", "fullname_first_author": "Fengji Zhang", "paper_title": "HUMANEVAL-V: EVALUATING VISUAL UNDERSTANDING AND REASONING ABILITIES OF LARGE MULTIMODAL MODELS THROUGH CODING TASKS", "reason": "This is the main paper introducing the HumanEval-V benchmark, providing the methodology, experimental setup, and results.  It's the most important because it defines and presents the benchmark.", "section_number": 1}, {" publication_date": "2021", "fullname_first_author": "Mark Chen", "paper_title": "Evaluating large language models trained on code", "reason": "This paper introduces HumanEval, a well-established benchmark for evaluating large language models' code generation capabilities. HumanEval-V builds upon HumanEval's success, expanding it to include multimodal aspects.", "section_number": 1}, {" publication_date": "2021", "fullname_first_author": "Jacob Austin", "paper_title": "Program synthesis with large language models", "reason": "This paper explores program synthesis using large language models, a closely related task to code generation which is the focus of HumanEval-V. The insights from this work informs the design choices for HumanEval-V.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "S\u00e9bastien Bubeck", "paper_title": "Sparks of artificial general intelligence: Early experiments with GPT-4", "reason": "This paper analyzes the capabilities of GPT-4, a leading LLM, highlighting the importance of complex reasoning tasks such as coding in assessing general intelligence (AGI). This context is relevant to HumanEval-V which evaluates multimodal models' reasoning in coding.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Chaoyou Fu", "paper_title": "MME: A comprehensive evaluation benchmark for multimodal large language models", "reason": "This paper introduces MME, a multimodal benchmark assessing various capabilities, and this work is important because it is used as a comparison benchmark to show the unique aspects of HumanEval-V.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Yash Goyal", "paper_title": "Making the V in VQA matter: Elevating the role of image understanding in visual question answering", "reason": "This paper highlights the importance of image understanding in VQA, which is relevant to HumanEval-V as it focuses on the integration of visual information in solving coding problems. This work is important to the understanding of the benchmark.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Bo Li", "paper_title": "MMCode: Evaluating multi-modal code large language models with visually rich programming problems", "reason": "This introduces MMCode, a benchmark for evaluating coding abilities of LMMs.  It is important to note limitations in MMCode design, highlighting why HumanEval-V is necessary.", "section_number": 5}, {" publication_date": "2022", "fullname_first_author": "Aman Madaan", "paper_title": "Language models of code are few-shot commonsense learners", "reason": "This work connects code generation with commonsense reasoning, a cognitive aspect HumanEval-V implicitly tests in the multimodal coding tasks.  Understanding this relationship helps interpret the results in context.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Hugo Lauren\u00e7on", "paper_title": "Unlocking the conversion of web screenshots into HTML code with the Websight dataset", "reason": "This paper focuses on visual-to-code tasks. It's important to compare and contrast with HumanEval-V's approach and contributions.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Bohao Li", "paper_title": "SEEDBench: Benchmarking multimodal LLMs with generative comprehension", "reason": "SEEDBench is another multimodal benchmark; comparing it with HumanEval-V helps highlight the differences in methodology and evaluation criteria.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Shuo Liu", "paper_title": "ConvBench: A multi-turn conversation evaluation benchmark with hierarchical capability for large vision-language models", "reason": "This benchmark is closely related and offers another method to analyze conversational capabilities within multimodal settings. This work is important to compare and contrast methodologies.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Chaoyou Fu", "paper_title": "MME: A comprehensive evaluation benchmark for multimodal large language models", "reason": "This paper introduces a multimodal benchmark with a broader scope, which enables comparing the specific focus and challenges of HumanEval-V within the broader landscape of multimodal LLM evaluation.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Yuan Liu", "paper_title": "MMBench: Is your multi-modal model an all-around player?", "reason": "MMBench offers a comprehensive evaluation for LMMs. Comparing it helps highlight the unique strengths and limitations of HumanEval-V.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Weihao Yu", "paper_title": "MM-VET: Evaluating large multimodal models for integrated capabilities", "reason": "This work focuses on visual understanding, comparing with HumanEval-V's unique approach emphasizing both visual and coding aspects.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Kaining Ying", "paper_title": "MMT-Bench: A comprehensive multimodal benchmark for evaluating large vision-language models towards multitask AGI", "reason": "MMT-Bench focuses on multitask abilities. Comparing it against HumanEval-V's focus on visual reasoning and coding provides a clearer perspective of the benchmark's contributions.", "section_number": 5}, {" publication_date": "2022", "fullname_first_author": "Jason Wei", "paper_title": "Chain-of-thought prompting elicits reasoning in large language models", "reason": "Chain-of-thought prompting is a significant technique in prompting LLMs. It is important for understanding the effectiveness of this technique in solving coding problems with visual input.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Shunyu Yao", "paper_title": "Tree of thoughts: Deliberate problem solving with large language models", "reason": "This paper explores advanced reasoning techniques for LLMs, which are relevant to interpreting the results and limitations of current LMMs in solving HumanEval-V tasks.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Yuan Yao", "paper_title": "MiniCPM-V: A GPT-4V level MLLM on your phone", "reason": "This paper introduces MiniCPM-V, a high-performing open-weight model used for comparisons in evaluating the capabilities of open-weight models in HumanEval-V. Understanding the performance of this model helps contextualize the results.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Haodong Duan", "paper_title": "VLMEVALKIT: An open-source toolkit for evaluating large multi-modality models", "reason": "This toolkit is used for evaluating large multi-modality models and it's an important resource for evaluating the performance of LMMs.  Understanding the toolkit's capabilities helps contextualize the study's methodology.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Jiawei Liu", "paper_title": "Is your code generated by ChatGPT really correct? Rigorous evaluation of large language models for code generation", "reason": "This paper delves into rigorous evaluation of LLMs for code generation. It is crucial as HumanEval-V also focuses on code generation, and comparing methodologies is important for understanding the benchmark's contributions.", "section_number": 4}]}