{"importance": "This paper is crucial for researchers working on large multimodal models (LMMs) and visual reasoning.  It introduces a novel benchmark, HumanEval-V, addressing a critical gap in evaluating LMM coding abilities with visual context. The findings highlight significant challenges in current LMMs' visual reasoning and coding skills, opening new avenues for research and model improvement.", "summary": "HumanEval-V: A new benchmark rigorously evaluates large multimodal models' visual understanding and reasoning abilities through carefully designed coding tasks, revealing significant limitations in current models.", "takeaways": ["HumanEval-V, a novel benchmark, evaluates LMMs' visual reasoning and coding abilities via code generation tasks incorporating visual contexts.", "Current LMMs struggle significantly with visual reasoning in coding tasks, especially open-source models.", "The benchmark reveals a gap between proprietary and open-source LMMs in performance, highlighting areas for future research."], "tldr": "This research introduces HumanEval-V, a benchmark designed to rigorously test the visual understanding and reasoning capabilities of large multimodal models (LMMs) by using coding tasks.  These tasks require models to generate code solutions based on images and text prompts, emphasizing visual reasoning. The researchers carefully crafted 108 Python coding tasks, adapting existing problems and redrawing visuals to prevent data leakage.  They evaluated 19 state-of-the-art LMMs, including both proprietary and open-source models.  Results showed that even leading proprietary models struggled, achieving only around 13% accuracy on the first try and 36% accuracy after multiple attempts. Open-source models performed significantly worse.  The study also found that current LMMs struggle to integrate visual and textual information effectively, and often overfit to previous training data. This suggests that significant improvement is needed in visual reasoning and code generation capabilities of LMMs. The benchmark and code are publicly available."}