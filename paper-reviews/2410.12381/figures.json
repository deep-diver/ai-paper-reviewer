[{"figure_path": "2410.12381/figures/figures_3_0.png", "caption": "Figure 2: The HumanEval-V construction pipeline, with representative examples for each stage.", "description": "The figure illustrates the four-stage HumanEval-V benchmark construction pipeline, which includes data collection, adaptation, mutation, and validation.", "section": "2 BENCHMARK CONSTRUCTION"}, {"figure_path": "2410.12381/figures/figures_17_0.png", "caption": "Figure 2: The HumanEval-V construction pipeline, with representative examples for each stage.", "description": "The figure illustrates the four-stage pipeline (collect, adapt, mutate, and validate) used to construct the HumanEval-V benchmark, showing representative examples for each stage.", "section": "2 BENCHMARK CONSTRUCTION"}, {"figure_path": "2410.12381/figures/figures_22_0.png", "caption": "Figure 1: An example coding task in HumanEval-V that all LMMs evaluated in this work cannot solve, including GPT-40 and Claude 3.5 Sonnet. Related error analysis can be found in Appendix A.", "description": "The figure shows an example coding task from the HumanEval-V benchmark that requires visual reasoning to complete a Python function.", "section": "2 BENCHMARK CONSTRUCTION"}, {"figure_path": "2410.12381/figures/figures_23_0.png", "caption": "Figure 7: A curated selection of representative images in HumanEval-V, covering visual elements like trees, graphs, matrices, maps, grids, flowcharts, and more.", "description": "The figure shows a selection of diverse visual elements used in HumanEval-V coding tasks, showcasing the variety of visual contexts used.", "section": "2 BENCHMARK CONSTRUCTION"}, {"figure_path": "2410.12381/figures/figures_23_1.png", "caption": "Figure 2: The HumanEval-V construction pipeline, with representative examples for each stage.", "description": "The figure illustrates the HumanEval-V construction pipeline, showing the steps involved in collecting, adapting, mutating, and validating coding tasks.", "section": "2 BENCHMARK CONSTRUCTION"}, {"figure_path": "2410.12381/figures/figures_24_0.png", "caption": "Figure 1: An example coding task in HumanEval-V that all LMMs evaluated in this work cannot solve, including GPT-40 and Claude 3.5 Sonnet. Related error analysis can be found in Appendix A.", "description": "The figure shows an example coding task from HumanEval-V which involves determining if two line segments ultimately intersect, based on visual input and a Python function signature.", "section": "2 BENCHMARK CONSTRUCTION"}, {"figure_path": "2410.12381/figures/figures_25_0.png", "caption": "Figure 2: The HumanEval-V construction pipeline, with representative examples for each stage.", "description": "The figure illustrates the four-stage HumanEval-V benchmark construction pipeline: collecting, adapting, mutating, and validating coding tasks.", "section": "2 BENCHMARK CONSTRUCTION"}, {"figure_path": "2410.12381/figures/figures_26_0.png", "caption": "Figure 1: An example coding task in HumanEval-V that all LMMs evaluated in this work cannot solve, including GPT-40 and Claude 3.5 Sonnet. Related error analysis can be found in Appendix A.", "description": "An example coding task in HumanEval-V that emphasizes the need for visual reasoning and cannot be solved by state-of-the-art large multimodal models.", "section": "2 BENCHMARK CONSTRUCTION"}, {"figure_path": "2410.12381/figures/figures_27_0.png", "caption": "Figure 2: The HumanEval-V construction pipeline, with representative examples for each stage.", "description": "The figure illustrates the HumanEval-V construction pipeline, showing the four main stages: collection, adaptation, mutation, and validation, with representative examples of each stage.", "section": "2 BENCHMARK CONSTRUCTION"}, {"figure_path": "2410.12381/figures/figures_28_0.png", "caption": "Figure 1: An example coding task in HumanEval-V that all LMMs evaluated in this work cannot solve, including GPT-40 and Claude 3.5 Sonnet. Related error analysis can be found in Appendix A.", "description": "The figure shows an example coding task from the HumanEval-V benchmark, which requires LMMs to solve a geometric intersection problem using only visual information and a function signature.", "section": "2 BENCHMARK CONSTRUCTION"}, {"figure_path": "2410.12381/figures/figures_29_0.png", "caption": "Figure 1: An example coding task in HumanEval-V that all LMMs evaluated in this work cannot solve, including GPT-40 and Claude 3.5 Sonnet. Related error analysis can be found in Appendix A.", "description": "The figure shows an example coding task from the HumanEval-V benchmark that requires visual reasoning to solve a geometric intersection problem.", "section": "2 BENCHMARK CONSTRUCTION"}, {"figure_path": "2410.12381/figures/figures_30_0.png", "caption": "Figure 1: An example coding task in HumanEval-V that all LMMs evaluated in this work cannot solve, including GPT-40 and Claude 3.5 Sonnet. Related error analysis can be found in Appendix A.", "description": "The figure shows an example of a coding task from the HumanEval-V benchmark, which involves determining if two line segments intersect based on their coordinates shown in an image.", "section": "2 BENCHMARK CONSTRUCTION"}, {"figure_path": "2410.12381/figures/figures_31_0.png", "caption": "Figure 1: An example coding task in HumanEval-V that all LMMs evaluated in this work cannot solve, including GPT-40 and Claude 3.5 Sonnet. Related error analysis can be found in Appendix A.", "description": "The figure shows an example coding task from HumanEval-V which requires LMMs to determine if two line segments intersect based on a visual context and textual description.", "section": "2 BENCHMARK CONSTRUCTION"}]