{"references": [{" publication_date": "2023", "fullname_first_author": "Dillon C Adam", "paper_title": "Generative ai for infectious diseases: An evaluation of chatgpt for medical translation", "reason": "This paper is highly relevant to the context of evaluating LLMs in healthcare, specifically focusing on the use of ChatGPT for medical translation.  This is directly applicable to the goal of WorldMedQA-V, which aims to provide a more comprehensive evaluation of multilingual LLMs in medical settings. The study's focus on translation adds to the importance of its inclusion among the top references.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Mahyar Abbasian", "paper_title": "Foundation metrics for evaluating effectiveness of healthcare conversations powered by generative AI", "reason": "This paper is highly relevant because it directly addresses the need for robust benchmarks to assess the safety and efficacy of AI models in healthcare, which is a central theme of the introduced WorldMedQA-V dataset.  It emphasizes the importance of evaluating AI's performance in healthcare conversations, a context directly relevant to the applications of WorldMedQA-V.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Andrew M. Bean", "paper_title": "Exploring the landscape of large language models in medical question answering", "reason": "This paper provides a comprehensive overview of the current state of large language models in medical question answering, identifying key trends and challenges in the field. This contextual information is important to understand the current landscape before presenting WorldMedQA-V as a novel contribution.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Dana Brin", "paper_title": "Comparing Chat-GPT and GPT-4 performance in USMLE soft skill assessments", "reason": "This study directly compares two prominent large language models (LLMs), ChatGPT and GPT-4, on their performance in USMLE soft skill assessments. This is directly relevant because the paper introduces WorldMedQA-V as a means to evaluate the performance of various VLMs, including LLMs, in medical settings. Therefore, the comparison made in this study is useful for contextualizing the benchmarks and establishing the relative performance of these two prominent LLMs before testing the WorldMedQA-V.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Shan Chen", "paper_title": "Cross-care: Assessing the healthcare implications of pre-training data on language model bias", "reason": "This paper directly addresses the issue of bias in language models, a key challenge in evaluating AI models in healthcare. This aligns perfectly with the goals of WorldMedQA-V, which aims to provide a more fair and equitable evaluation of AI models across diverse healthcare contexts. By acknowledging and analyzing bias, WorldMedQA-V contributes to addressing systemic challenges and promoting fairer evaluation.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Jan Clusmann", "paper_title": "The future landscape of large language models in medicine", "reason": "This review article provides a broad overview of the current and future landscape of Large Language Models (LLMs) in medicine, which is highly relevant to the goals of WorldMedQA-V.  Understanding the existing trends and challenges in the field informs the rationale and contributions of the dataset.  The review\u2019s analysis of LLMs and their application in the medical field enhances the paper's value in the context of WorldMedQA-V.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Dillon C Adam", "paper_title": "Generative ai for infectious diseases: An evaluation of chatgpt for medical translation", "reason": "This paper directly relates to the multilingual aspect of WorldMedQA-V. The study's findings on the performance and limitations of ChatGPT for medical translation are significant in providing context for the dataset and its intended applications.  It strengthens the paper's argument by demonstrating the complexities of multilingual AI applications in healthcare, underscoring the necessity of a benchmark like WorldMedQA-V.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Jieyi Zhang", "paper_title": "Exams-v: A multi-discipline multi-lingual multi-modal exam benchmark for evaluating vision language models", "reason": "This paper introduces a benchmark dataset for evaluating vision-language models (VLMs) that is relevant to the WorldMedQA-V because it focuses on evaluating VLMs across multiple languages and disciplines.  The similarities in their approach to multilingual and multimodal evaluation make this a strong related work.  This paper helps to establish the current state of VLM benchmarks and highlights the need for more robust and diverse evaluations in healthcare.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Yiyi Wang", "paper_title": "Culturalvqa: A new frontier in vision and language understanding", "reason": "This study presents CulturalVQA, a benchmark dataset which evaluates VLMs across various cultures and languages.  It demonstrates performance discrepancies across different cultural and linguistic backgrounds, mirroring the focus of the WorldMedQA-V dataset on multilingual and multimodal assessments of AI in healthcare. The paper strengthens the motivation for WorldMedQA-V.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Haodong Duan", "paper_title": "Vlmevalkit: An open-source toolkit for evaluating large multi-modality models", "reason": "This paper introduces a novel open-source toolkit that is directly relevant to the evaluation methodology employed in the WorldMedQA-V study. The VLMEvalKit was utilized to assess the performance of ten different multimodal language models.  This toolkit provided a standardized and efficient way to evaluate these models, which is relevant to the methodology section of WorldMedQA-V.", "section_number": 3}, {" publication_date": "1968", "fullname_first_author": "Jacob Cohen", "paper_title": "Weighted kappa: Nominal scale agreement provision for scaled disagreement or partial credit", "reason": "This seminal paper describes Cohen's kappa, a statistical metric used extensively to assess inter-rater reliability and agreement.  The choice of Cohen's kappa as a key metric for evaluating model consistency in WorldMedQA-V establishes the importance of measuring agreement between original language responses and English translations, reinforcing its methodological rigor and enhancing the reliability of the results.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "T. H. Kung", "paper_title": "Performance of chatgpt on usmle: Potential for ai-assisted medical education using large language models", "reason": "This study provides empirical evidence of an LLM's (ChatGPT) performance on the USMLE, setting the stage for comparing the performance of other models, including VLMs, on different benchmarks such as WorldMedQA-V.  This provides a critical point of reference for evaluating the performance of VLMs in medical contexts.  By comparing the results obtained on WorldMedQA-V with this study's findings, the authors demonstrate how their dataset contributes to a more robust and comprehensive evaluation framework.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Travis Zack", "paper_title": "Assessing the potential of GPT-4 to perpetuate racial and gender biases in health care: a model evaluation study", "reason": "This paper highlights the risks of bias in large language models and the importance of evaluating them for fairness in healthcare contexts. This finding aligns directly with the goals of WorldMedQA-V, which addresses multilingual and multimodal evaluation of medical AI models.  WorldMedQA-V\u2019s acknowledgement of bias and steps towards more equitable AI evaluation makes this paper highly relevant to the work.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Arun James Thirunavukarasu", "paper_title": "Large language models in medicine", "reason": "This paper offers a broad review of the application of large language models in medicine, laying the groundwork for understanding the broader context in which WorldMedQA-V operates. The review helps to highlight the importance of robust benchmarking and evaluation in the adoption of these models for healthcare applications, providing valuable context and support for the development of WorldMedQA-V.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Andy K. Zhang", "paper_title": "Language model developers should report train-test overlap", "reason": "This paper is highly relevant to the concerns regarding data contamination in LLMs\u2019 training data, a critical issue in evaluating the performance of large language models and which is explicitly acknowledged as a limitation of existing benchmarks in this paper. This directly relates to WorldMedQA-V which carefully addresses this issue to mitigate its impact on evaluation.", "section_number": 6}, {" publication_date": "2024", "fullname_first_author": "Ahmet \u00dcst\u00fcn", "paper_title": "Aya model: An instruction finetuned open-access multilingual language model", "reason": "The study introduces a novel multilingual language model (Aya), highlighting the growing interest and development in this field.  This is relevant to WorldMedQA-V since the study focuses on evaluating multilingual VLMs and the existence of new multilingual models such as Aya can be seen as a potential user group of the introduced benchmark dataset.", "section_number": 6}, {" publication_date": "2021", "fullname_first_author": "Jing Li", "paper_title": "MLEC-QA: A Chinese Multi-Choice Biomedical Question Answering Dataset", "reason": "This paper introduces a substantial Chinese biomedical QA dataset (MLEC-QA), providing valuable context for the WorldMedQA-V project.  By comparing the size and scope of MLEC-QA to the proposed dataset, we gain insight into the scale and diversity of the WorldMedQA-V. The dataset\u2019s focus on a specific language, Chinese, enhances the relevance of the study to the broader goal of advancing multilingual evaluations of AI in healthcare.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "Di Jin", "paper_title": "What disease does this patient have?: A large-scale open domain question answering dataset from medical exams", "reason": "This paper introduces a significant dataset (MedQA) for evaluating question-answering systems in medicine and it's directly relevant to this work.  The paper sets the stage for WorldMedQA-V by highlighting the need for more comprehensive medical QA datasets that are not limited by language or scope.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Niclas Hertzberg", "paper_title": "MedQA-SWE a clinical question & answer dataset for Swedish", "reason": "This work is highly relevant because it provides a benchmark that aligns directly with WorldMedQA-V\u2019s goal. This study introduces a Swedish medical question answering dataset (MedQA-SWE), highlighting the importance of addressing linguistic diversity in medical AI evaluation. The similarities in the context and focus on improving multilingual AI evaluations in the healthcare setting make this a valuable reference for understanding the global implications of WorldMedQA-V.", "section_number": 2}]}