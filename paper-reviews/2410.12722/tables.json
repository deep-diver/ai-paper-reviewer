[{"figure_path": "2410.12722/tables/table_9_0.html", "caption": "Table 1: Summary of existing open-source Medical QA Datasets by Country.", "description": "Table 1 summarizes existing publicly-available medical examination QA datasets by country, including the number of questions and answers, languages, modalities, source, and years.", "section": "Appendix A.1"}, {"figure_path": "2410.12722/tables/table_11_0.html", "caption": "Table 2: WorldMedQA's data across countries and languages. In the curated dataset, each QA was associated with at least one image. Some images were present in more than one question. In the final subset for evaluation (rightmost column), each question had a single image and the correct option associated with it, resulting in fewer samples. The number of answer options per question (fourth column) refers to the original number of choices in the multiple-choice format (e.g., A-D for four options or A-E for five options). However, all questions after preprocessing results in 4 options only. In cases where this varies, such as in Brazil, the value represents a weighted average across questions. The total number of QAs and images does not immediately add up due to some questions sharing images or having multiple associated options.", "description": "Table 2 presents a summary of the WorldMedQA-V dataset's distribution across four countries, four languages, and includes the number of questions, images, and final evaluated samples.", "section": "A.3 Detailed Data Statistics"}, {"figure_path": "2410.12722/tables/table_13_0.html", "caption": "Table 3: Cohen's Kappa reflecting agreement between languages for the same models, countries, and testing setting. Values in bold highlight the model with highest kappa per country and testing mode. The two studied settings were text-only (T. only) and text and image (T. & I.).", "description": "Table 3 shows Cohen's Kappa reflecting the agreement between model outputs in original languages and their English translations, categorized by country and whether images were included.", "section": "A.5 Model output consistency across countries and test setting"}, {"figure_path": "2410.12722/tables/table_14_0.html", "caption": "Table 4: Accuracy comparison across countries and original languages (Portuguese, Hebrew, Japanese, and Spanish) for each model. The two studied settings were text-only (T. only) and text and image (T. & I.). Each cell represents the performance of each model in its native language dataset, highlighting how the presence or absence of images affects accuracy.", "description": "Table 4 shows the accuracy of different models on four languages with and without image input.", "section": "A.6 Detailed performance with and without images"}, {"figure_path": "2410.12722/tables/table_14_1.html", "caption": "Table 4: Accuracy comparison across countries and original languages (Portuguese, Hebrew, Japanese, and Spanish) for each model. The two studied settings were text-only (T. only) and text and image (T. & I.). Each cell represents the performance of each model in its native language dataset, highlighting how the presence or absence of images affects accuracy.", "description": "Table 4 shows the accuracy of various models in four languages with and without image input.", "section": "A.6 Detailed performance with and without images"}]