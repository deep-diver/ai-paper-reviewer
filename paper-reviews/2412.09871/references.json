{"references": [{"fullname_first_author": "Jordan Hoffmann", "paper_title": "Training compute-optimal large language models", "publication_date": "2022-01-01", "reason": "This paper introduced the concept of compute-optimal training, which is crucial for efficient training of large language models and serves as a primary baseline for comparison in this work."}, {"fullname_first_author": "Abhimanyu Dubey", "paper_title": "The llama 3 herd of models", "publication_date": "2024-01-01", "reason": "This work introduces Llama 3, which is the state-of-the-art tokenization based LLM used as a strong baseline in this work."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Llama 2: Open foundation and fine-tuned chat models", "publication_date": "2023-01-01", "reason": "Llama 2 is used as a baseline as well as to determine optimal architectural hyperparameters and training data quantities, and also provides the first of two datasets used to train and evaluate BLT."}, {"fullname_first_author": "Lili Yu", "paper_title": "Megabyte: Predicting million-byte sequences with multiscale transformers", "publication_date": "2023-01-01", "reason": "MegaByte is the closest prior work to BLT which motivates several architectural improvements and comparisons to advance the state-of-the-art of byte-level LLMs."}, {"fullname_first_author": "Linting Xue", "paper_title": "ByT5: Towards a token-free future with pre-trained byte-to-byte models", "publication_date": "2022-01-01", "reason": "ByT5 investigates a different approach to training byte-level LLMs using an encoder-decoder architecture without patching, which motivates architectural choices such as patching and the inclusion of local encoder and decoder modules in BLT."}]}