[{"figure_path": "https://arxiv.org/html/2412.09871/x3.png", "caption": "Figure 1: \nScaling trends for fixed inference flop models (fully) trained with varying training budgets.\nIn token-based models, a fixed inference budget determines the model size.\nIn contrast, the BLT architecture provides a new scaling axis allowing simultaneous increases in model and patch size while keeping the same training and inference budget.\nBLT patch-size (ps) 6 and 8 models quickly overtake scaling trends of bpe Llama\u00a02 and 3. Moving to the larger inference budget makes the larger patch size 8 model more desirable sooner. Both BPE compute-optimal point and crossover point are indicated with vertical lines.", "description": "This figure presents scaling trends of byte-level language models compared to token-based models. It demonstrates that for a fixed inference FLOP budget, BLT models with patch sizes 6 and 8 outperform BPE Llama 2 and 3 models when trained on varying training data sizes. The x-axis represents the training FLOPs, and the y-axis represents the bits-per-byte (BPB).  The lower the BPB, the better the performance of the model. As the training data size increases, BLT models achieve lower BPB than the other models. The vertical lines indicate the compute-optimal point for BPE and the crossover point where BLT starts outperforming BPE. The figure highlights that the BLT architecture enables scaling both model and patch size simultaneously for a fixed inference budget, unlike token-based models where the inference budget fixes the model size. The graph also indicates that larger patch sizes (e.g., 8) become more advantageous with larger inference budgets.", "section": "5 Scaling Trends"}, {"figure_path": "https://arxiv.org/html/2412.09871/extracted/6066458/assets/patching_types.png", "caption": "Figure 2: \nBLT comprises three modules, a lightweight Local Encoder that encodes input bytes into patch representations, a computationally expensive Latent Transformer over patch representations, and a lightweight Local Decoder to decode the next patch of bytes. BLT incorporates byte n\ud835\udc5bnitalic_n-gram embeddings and a cross-attention mechanism to maximize information flow between the Latent Transformer and the byte-level modules\u00a0(Figure\u00a05). Unlike fixed-vocabulary tokenization, BLT dynamically groups bytes into patches preserving access to the byte-level information.", "description": "The figure shows the architecture of the Byte Latent Transformer (BLT). It consists of three main modules: 1) a Local Encoder: This module takes raw bytes as input and encodes them into patch representations. It incorporates byte n-gram embeddings and cross-attention to enhance information flow. 2) a Latent Transformer: This is the core of the model and operates on the patch representations. It's computationally expensive and serves as a global context processor. 3) a Local Decoder: This lightweight module decodes the patch representations from the Latent Transformer and produces raw byte predictions. Unlike conventional tokenization-based models, BLT maintains direct access to byte-level details by dynamically grouping bytes into patches.", "section": "3 BLT Architecture"}, {"figure_path": "https://arxiv.org/html/2412.09871/x4.png", "caption": "Figure 3: \nPatching schemes group bytes in different ways, each leading to a different number of resulting patches.\nSince each patch is processed using a large transformer step, the number of patches directly determines the bulk of the compute expended in terms of flops.\nThese schemes group bytes into patches by (a) striding every four bytes\u00a0(\u00a72.1) as in MegaByte\u00a0(Yu et\u00a0al., 2023), (b) tokenizing with Byte-Pair Encoding (bpe), in this case the Llama-3\u00a0(Dubey et\u00a0al., 2024) tokenizer, (c & d) entropy-based patching as in this work\u00a0(\u00a72.3), (e) patching on space-bytes\u00a0(Slagle, 2024), (f) and patching on entropy using a small CNN byte-level model with 2-byte context.", "description": "Figure 3 illustrates various methods for grouping bytes into patches, impacting computational cost.  Each patch corresponds to a large transformer step, directly influencing FLOPS expenditure. Methods include: (a) 4-strided patching (MegaByte), (b) BPE tokenization (Llama-3 tokenizer), (c & d) entropy-based patching (this work), (e) space-byte patching, and (f) entropy patching with a CNN byte-level model.", "section": "Patching: From Individual Bytes to Groups of Bytes"}, {"figure_path": "https://arxiv.org/html/2412.09871/x5.png", "caption": "Figure 4: \nThis figure plots the entropy H\u2062(xi)\ud835\udc3bsubscript\ud835\udc65\ud835\udc56H(x_{i})italic_H ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) of each byte in \u201cDaenerys Targeryen is in Game of Thrones, a fantasy epic by George R.R. Martin.\u201d with spaces shown as underscores.\nPatches end when H\u2062(xi)\ud835\udc3bsubscript\ud835\udc65\ud835\udc56H(x_{i})italic_H ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) exceeds the global threshold \u03b8gsubscript\ud835\udf03\ud835\udc54\\theta_{g}italic_\u03b8 start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT, shown as a red horizontal line.\nThe start of new patches are shown with vertical gray lines.\nFor example, the entropies of \u201cG\u201d and \u201ce\u201d in \u201cGeorge R.R. Martin\u201d exceed \u03b8gsubscript\ud835\udf03\ud835\udc54\\theta_{g}italic_\u03b8 start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT, so \u201cG\u201d is the start of a single byte patch and \u201ce\u201d of a larger patch extending to the end of the named entity as the entropy H\u2062(xi)\ud835\udc3bsubscript\ud835\udc65\ud835\udc56H(x_{i})italic_H ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) stays low, resulting in no additional patches.", "description": "The figure shows the entropy of each byte in the given example text (\"Daenerys Targaryen is in Game of Thrones, a fantasy epic by George R.R. Martin.\"). The x-axis represents each byte and the y-axis entropy. A red horizontal line shows the global threshold. When the entropy of a byte exceeds this threshold, a new patch is started, marked by a vertical gray line. Longer patches occur when the entropy stays low after the first byte. For example, \"George R.R. Martin\" is split into multiple patches since both \"G\" and \"e\" have entropy above the threshold, but the next few bytes stay low and extend the \"e\" patch.", "section": "2 Patching: From Individual Bytes to Groups of Bytes"}, {"figure_path": "https://arxiv.org/html/2412.09871/x6.png", "caption": "Figure 5: The local encoder uses a cross-attention block with patch representations as queries, and byte representations as keys/values to encode byte representations into patch representations. The local decoder uses a similar block but with the roles reversed i.e. byte representations are now the queries and patch representations are the keys/values. Here we use Cross-Attn k=2\ud835\udc582k=2italic_k = 2.", "description": "The local encoder uses a cross-attention mechanism with patch representations as queries and byte representations as keys and values to transform byte representations into patch representations. Similarly, the local decoder uses a cross-attention block but reverses the roles: byte representations serve as queries while patch representations are used as keys and values.  The local encoder and decoder enable the model to transition between byte-level and patch-level representations, enhancing the model's ability to handle both granular byte-level information and higher-level patch-level abstractions. This dual-level processing empowers the model to capture intricate byte-level details while efficiently processing larger chunks of text via patches.  In this figure, a Cross-Attn parameter *k* = 2 is used.", "section": "3 BLT Architecture"}, {"figure_path": "https://arxiv.org/html/2412.09871/x7.png", "caption": "Figure 6: Scaling trends for BLT models with different architectural choices, as well as for baseline BPE token-based models. We train models at multiple scales from 1B up to 8B parameters for the optimal number of tokens as computed by\u00a0Dubey et\u00a0al. (2024) and report bits-per-byte on a sample from the training distribution. BLT models perform on par with state-of-the-art tokenizer-based models such as Llama 3, at scale. PS denotes patch size. We illustrate separate architecture improvements on space-patching (left) and combine them with dynamic patching (right).", "description": "This figure compares the scaling trends between BLT models with various architectural choices, baseline BPE token-based models (like LLama 2 and 3), and other byte-level models, by plotting training FLOPS against language modeling performance (bits-per-byte).  The key takeaway is that BLT models perform comparably to state-of-the-art tokenizer-based models at scale, demonstrating the viability of byte-level models. The left subplot focuses on space-patching and shows architectural improvements.  The right subplot demonstrates the improvements achieved by combining architectural changes with dynamic patching.", "section": "5 Scaling Trends"}, {"figure_path": "https://arxiv.org/html/2412.09871/x8.png", "caption": "Figure 7: Output responses from Llama 3 and BLT models for various tasks from CUTE benchmark. BLT model performs better on sequence manipulation tasks compared to the tokenizer-based Llama 3 model. Note that few-shot examples are not shown in the above prompts to maintain clarity.", "description": "Figure 7 showcases a comparison between the responses generated by Llama 3 and BLT models on various tasks from the CUTE benchmark. These tasks evaluate character-level understanding and manipulation abilities, including substitution, swapping, semantic similarity, orthographic similarity, and insertion of characters. The prompts used for each task are shown in the figure, but few-shot examples are omitted for clarity.  The results highlight BLT's superior performance on sequence manipulation tasks, indicating a better understanding and ability to manipulate text at the character level compared to the token-based Llama 3.", "section": "6. Byte Modeling Improves Robustness"}, {"figure_path": "https://arxiv.org/html/2412.09871/extracted/6066458/assets/patching.png", "caption": "Figure 8: Variation of language modeling performance in bits-per-byte (bpb) with training flops for 400m and 1b BLT models patched with entropy models of different sizes and context windows. Both dimensions improve scaling performance, with diminishing returns beyond 50m parameter entropy models with a context of 512 bytes.", "description": "This figure analyzes the impact of entropy model size and context window length on the performance of 400 million and 1 billion parameter BLT models. The x-axis represents training FLOPS, and the y-axis represents bits-per-byte (bpb), a measure of language modeling performance. Different lines correspond to varying entropy model sizes (1m, 10m, 50m, and 100m parameters) and context window lengths (64, 128, and 512 bytes).  The results indicate that increasing both entropy model size and context window length improves performance, but with diminishing returns beyond 50 million parameter entropy models with a 512-byte context window.", "section": "Ablations and Discussion"}]