[{"figure_path": "https://arxiv.org/html/2503.06955/x1.png", "caption": "Figure 1: Masking strategy comparison. This figure demonstrates the key differences between the previous random masking strategy [21] (top) and our attention-based masking (bottom). Our masking strategy focuses on the more significant and dynamic parts of the motion (colored) corresponding to the condition.", "description": "This figure illustrates the core difference between the traditional random masking approach used in previous autoregressive motion generation models and the novel attention-based masking method introduced in this paper. The top panel displays the random masking technique, where the model randomly masks various parts of the motion sequence, irrespective of the importance or relevance to the input condition. In contrast, the bottom panel showcases the attention-based masking strategy. The model assigns attention weights to different parts of the motion based on the input condition, and strategically masks out less relevant parts. The color-coded areas highlight the dynamic, crucial frames and body parts that are prioritized and preserved during the masking process, ensuring the quality and coherence of the generated motion based on the provided conditions.", "section": "3.2 Architecture"}, {"figure_path": "https://arxiv.org/html/2503.06955/x2.png", "caption": "Figure 2: Motion Anything architecture. The multimodal architecture consists of several key components: (a) temporal and (c) spatial attention-based masking, (b) motion generator, and (d) a single block of motion generator.\nThese components enable the model to learn key motions corresponding to the given conditions, and facilitate alignment between multi-modal conditions and motion features.", "description": "Figure 3 illustrates the architecture of Motion Anything, a multimodal motion generation framework.  It highlights four key components: (a) a temporal attention-based masking mechanism that selectively focuses on important time steps within a motion sequence based on the input conditions; (c) a spatial attention-based masking mechanism that similarly prioritizes key body parts or actions; (b) the overall motion generation model; and (d) a detailed view of a single block within the motion generator, showcasing the internal processing steps.  These components work together to ensure the generated motion accurately reflects the provided multimodal conditions (text, music, or both), enhancing control and coherence in the output.", "section": "3.2 Architecture"}, {"figure_path": "https://arxiv.org/html/2503.06955/x3.png", "caption": "Figure 3: Attention map. The attention map provides a direct visualization of our attention-based masking approach, which selectively masks regions in the motion sequence with high attention scores.", "description": "The figure visualizes the attention weights learned by the model's attention-based masking mechanism.  Different colored regions highlight areas of the motion sequence that receive high attention scores. The darker the color, the more attention the model paid to that specific region during the masking process. This attention is used to selectively mask parts of the motion sequence deemed less important based on the provided conditions (text, music, or both). The visualization helps demonstrate how the model focuses on dynamic and crucial parts of motion, enabling fine-grained control over the generated motion.", "section": "3.2 Architecture"}, {"figure_path": "https://arxiv.org/html/2503.06955/x4.png", "caption": "Figure 4: Qualitative evaluation on text-to-motion generation. We qualitatively compared the visualizations generated by our method with those produced by BAD [22], BAMM [44], and MoMask [21].", "description": "Figure 4 presents a qualitative comparison of text-to-motion generation results.  It showcases motion sequences generated by the proposed 'Motion Anything' method alongside those created by three other state-of-the-art methods: BAD, BAMM, and MoMask.  The figure allows for a visual assessment of the differences in motion quality, realism, and adherence to the text prompts across the various approaches.  By visually comparing the generated motions, the figure helps demonstrate the advantages of the 'Motion Anything' framework.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.06955/x5.png", "caption": "Figure 5: Qualitative evaluation on music-to-dance generation. We qualitatively compared the visualizations generated by our method with those produced by EDGE [54], Lodge [36], and Bailando [51].", "description": "Figure 5 showcases a qualitative comparison of music-to-dance generation results.  It presents visual examples of dance sequences generated by the proposed 'Motion Anything' method alongside those created by three other state-of-the-art techniques: EDGE, Lodge, and Bailando. This allows for a visual assessment of the relative quality, style, and fidelity of the generated dances, illustrating the improvements achieved by Motion Anything in terms of generating realistic and expressive dance motions synchronized with the input music.", "section": "4.5. Qualitative Evaluation"}, {"figure_path": "https://arxiv.org/html/2503.06955/extracted/6272770/figure/user.png", "caption": "Figure 1: User study form. The User Interface (UI) used in our user study.", "description": "The figure displays the user interface of the user study conducted in the paper. The interface presents a series of motion animation videos for evaluation.  Participants assess aspects such as motion accuracy, overall user experience, and visual quality. They rate each aspect from 1 (low) to 3 (high).  A comparison section allows participants to select the model with the best performance. The study involved three groups of motions: text-to-motion, music-to-dance, and text-and-music-to-dance.", "section": "2. User Study"}, {"figure_path": "https://arxiv.org/html/2503.06955/x6.png", "caption": "Figure 2: Comparisons on FID and AIT. All tests are conducted on the same NVIDIA GeForce RTX 2080 Ti. The closer the model is to the origin, the better.", "description": "Figure 2 presents a comparison of the Fr\u00e9chet Inception Distance (FID) and Average Inference Time (AIT) for various motion generation models.  All models were evaluated using the same NVIDIA GeForce RTX 2080 Ti GPU to ensure consistent testing conditions.  The chart plots FID and AIT scores for each method.  Lower FID scores indicate better-quality motion generation, while lower AIT scores represent faster inference times. The ideal model would be closest to the origin (0,0) as it produces high-quality motion quickly. The figure visually demonstrates the trade-off between generation quality and computational efficiency for each method.", "section": "3. Model Efficiency"}, {"figure_path": "https://arxiv.org/html/2503.06955/x7.png", "caption": "Figure 3: 4D Avatar Generation. This approach enables 4D avatar generation conditioned on multimodal inputs, achievable with just a single text prompt.", "description": "This figure illustrates the process of generating a 4D avatar using a multimodal approach.  It begins with a single text prompt as input, which is then processed by a motion generation model to create a motion sequence. Simultaneously, a 3D avatar generation model creates candidate 3D avatars. Then, a selective rigging mechanism determines which 3D avatar best fits the generated motion. Finally, the motion sequence is retargeted to the chosen avatar, resulting in a 4D avatar that combines 3D visual information with a realistic motion sequence.", "section": "4. Application: 4D Avatar Generation"}, {"figure_path": "https://arxiv.org/html/2503.06955/extracted/6272770/figure/3davatar.png", "caption": "Figure 4: 3D Avatars. This figure shows examples of 3D avatars generated by Tripo AI 2.0 [1]. These avatars will later serve as candidates for our Selective Rigging Mechanism.", "description": "This figure displays a set of 3D avatars generated using the Tripo AI 2.0 model.  These avatars represent diverse body shapes and poses.  They are not the final output of the paper's method but serve as the input candidates to a later stage, the Selective Rigging Mechanism, which selects the most suitable avatar for subsequent motion animation.", "section": "4. Application: 4D Avatar Generation"}, {"figure_path": "https://arxiv.org/html/2503.06955/x8.png", "caption": "Figure 5: Qualitative evaluation on text-&-music-to-dance generation. We qualitatively compared the visualizations generated by our method with those produced by TM2D [17] and MotionCraft [5].", "description": "Figure 5 presents a qualitative comparison of dance generation results.  It visually showcases the output from three different methods: Motion Anything (the proposed model), TM2D [17], and MotionCraft [5].  Each method was given the same text and music prompts to generate dance sequences. The figure allows for a direct visual comparison of the quality, style, coherence, and overall realism of the motion generated by each method, highlighting the strengths of Motion Anything in generating more natural and nuanced dance movements compared to the alternatives.", "section": "5. Qualitative Evaluation"}]