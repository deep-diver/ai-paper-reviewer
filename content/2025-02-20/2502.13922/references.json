{"references": [{"fullname_first_author": "Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-03-02", "reason": "This paper is a seminal work on training language models to follow instructions using human feedback, a crucial technique in aligning LLMs, and serves as the basis of RLHF."}, {"fullname_first_author": "Rafailov", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "publication_date": "2023-01-01", "reason": "This paper introduces Direct Preference Optimization (DPO), a more stable and efficient alternative to RLHF that simplifies the alignment process and is central to the LongPO method."}, {"fullname_first_author": "Stiennon", "paper_title": "Learning to summarize with human feedback", "publication_date": "2020-01-01", "reason": "This is a key paper in Reinforcement Learning from Human Feedback (RLHF), which this paper leverages to refine the alignment of language models, inspiring the constraint employed in LongPO."}, {"fullname_first_author": "Hendrycks", "paper_title": "Measuring massive multitask language understanding", "publication_date": "2021-01-01", "reason": "This paper introduces the MMLU benchmark, a standard for evaluating the short-context general language understanding and reasoning capabilities of LLMs, used in the paper to evaluate and benchmark LongPO against baselines."}, {"fullname_first_author": "Jiang", "paper_title": "Mistral 7B", "publication_date": "2023-10-06", "reason": "This paper introduces Mistral-7B, the base model used for training LongPO, showcasing its adaptation and improvement in long-context scenarios through the proposed methodology."}]}