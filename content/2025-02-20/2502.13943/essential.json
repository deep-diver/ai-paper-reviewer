{"importance": "This paper introduces AdaptiveStep, a novel and efficient method for training Process Reward Models (PRMs) by dynamically dividing reasoning steps based on model confidence. It outperforms existing PRMs, reduces construction costs, and shows strong generalization, opening new avenues for **improving LLM reasoning and code generation**.", "summary": "AdaptiveStep: Divides reasoning steps automatically through model confidence, enhancing PRM training & performance.", "takeaways": ["AdaptiveStep divides reasoning steps based on model confidence, providing more decision-making information.", "AdaptiveStep-trained PRMs achieve state-of-the-art performance in mathematical reasoning and code generation tasks.", "AdaptiveStep reduces construction costs by over 30% compared to existing open-source PRMs."], "tldr": "Current PRMs use rule-based methods to divide reasoning steps, which overlook crucial decision points. The methods results in coarse divisions lacking decision-making insights. Existing PRMs face challenges in balancing annotation costs and division granularity, limiting their broad use due to high building costs and reliance on fixed symbols. Thus, more informative step dividing methods are in urgent needs. \n\nThis paper presents **AdaptiveStep**, a novel method that divides reasoning steps based on the model's confidence in predicting the next word. This approach yields informative steps, enhancing reward model learning without manual annotation. The resulting AdaptiveStep Process Reward Model (ASPRM) outperforms existing PRMs in mathematical reasoning and code generation, reducing construction costs significantly. ASPRM also demonstrates strong transferability and generalization.", "affiliation": "Nanjing University", "categories": {"main_category": "Machine Learning", "sub_category": "Reinforcement Learning"}, "podcast_path": "2502.13943/podcast.wav"}