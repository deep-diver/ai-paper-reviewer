[{"figure_path": "https://arxiv.org/html/2502.13173/x7.png", "caption": "Figure 1: The illustration of our method ThinkPO and its performance on math reasoning tasks.\nTop: Our ThinkPO enhances fine-tuned LLMs (+SFT) by promoting detailed problem-solving\u2014using long chain-of-thought reasoning answers as positive (chosen) samples and short chain-of-thought reasoning answers as negative (rejected) samples.\nBottom Left: ThinkPO significantly boosts performance across mathematical benchmarks (e.g., 83.4% on MATH500 vs. 82.8% for +SFT and 74.0% for the Base model).\nBottom Right: ThinkPO generates more detailed solutions, with average completion lengths on AIME increasing from 0.94K to 21.57K to 23.9K tokens.\nThese results underscore Think Preference Optimization\u2019s effectiveness in fostering and enhancing advanced mathematical reasoning.", "description": "Figure 1 illustrates the Thinking Preference Optimization (ThinkPO) method and its impact on mathematical reasoning.  The top panel shows how ThinkPO works: it uses long chain-of-thought (CoT) reasoning responses as positive examples and short CoT responses as negative examples during training. The bottom-left panel displays the improved performance of ThinkPO across various math reasoning benchmarks (AIME, GPQA, Olympiad, MATH500).  ThinkPO significantly increased accuracy compared to both the baseline model and a model that only underwent supervised fine-tuning (SFT).  The bottom-right panel demonstrates that ThinkPO led to substantially longer model outputs (measured in tokens).  Specifically, for the AIME benchmark, the average response length increased from 0.94K to 23.9K tokens, showing ThinkPO's ability to foster more detailed and thorough solutions.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2502.13173/x15.png", "caption": "Figure 2: \nAnalysis of accuracy(Left), average response length(Middle) and reasoning-supportive words count(Right, like wait, hmm, etc) in SFT and ThinkPO process. We evaluate the model on MATH500 every 300 steps and record all the three metrics. In the early training stages, all of them improve significantly. However, in the later stages (e.g., after 1200 steps), the model\u2019s performance gradually plateau. When ThinkPO is applied, we see additional improvements in all of the three aspects, demonstrating the effectiveness of Thinking Preference Optimization.", "description": "Figure 2 presents a comparative analysis of three key metrics\u2014accuracy, average response length, and the frequency of reasoning-supportive words (like \"wait,\" \"hmm,\" etc.)\u2014across two training methods: Supervised Fine-Tuning (SFT) and Thinking Preference Optimization (ThinkPO).  The model's performance on the MATH500 dataset was evaluated every 300 training steps.  The results reveal significant initial improvements in all three metrics under both methods. However, SFT exhibits a performance plateau in later stages (around 1200 steps).  ThinkPO is shown to provide further improvement in accuracy, response length, and the use of reasoning-supportive words, highlighting its effectiveness in enhancing the model's reasoning capabilities beyond that of SFT alone.", "section": "2 Thinking Preference Optimization"}]