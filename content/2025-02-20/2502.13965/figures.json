[{"figure_path": "https://arxiv.org/html/2502.13965/x1.png", "caption": "(a) Chatbot", "description": "This figure shows a simple chatbot interaction workflow.  The workflow is represented as a sequence diagram, showing the interaction between a human user, the chatbot agent (which uses an LLM internally), and an optional tool.  The user inputs a message, the chatbot agent receives the message and uses its internal LLM to generate a response. The chatbot may then optionally interact with a tool to assist in generating the response (e.g., accessing a search engine or database). Finally, the chatbot agent returns its response to the human user. This is a straightforward, sequential process, representing one type of agentic program where each agent\u2019s activities are fairly simple and directly respond to the user.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2502.13965/x2.png", "caption": "(b) ReAct Agent", "description": "This figure illustrates the execution workflow of a ReAct (Reasoning and Acting) agent, a type of agentic program.  The ReAct agent iteratively cycles between generating text with an LLM and interacting with its environment (using tools or human input). Each cycle begins with an LLM call (represented by the rectangle with 'LLM Agent' inside), producing an action or response. The agent then executes an action in the environment (represented by a rectangle with 'Tool').  Subsequent LLM calls build upon the outcome of tool use or human interaction, iterating the process until the task is solved or a termination condition is met. The workflow is dynamic and non-deterministic, as the specific number and order of LLM calls and actions depend on the situation and the LLM's output.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2502.13965/x3.png", "caption": "(c) Map-Reduce", "description": "This figure shows a MapReduce workflow as an example of a multi-threaded agentic program.  MapReduce consists of multiple agents working in parallel on sub-tasks and combines the results at the end. The figure illustrates the dynamic nature of the workflow with agents performing LLM calls, accessing tools, and ultimately merging to produce a final result. Each agent's path is depicted as a directed acyclic graph (DAG), showing multiple stages involving calls to LLMs and interactions with external tools.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2502.13965/x4.png", "caption": "(d) Monte Carlo Tree Search", "description": "This figure illustrates a Monte Carlo Tree Search (MCTS) agentic program.  MCTS is a widely used search and planning algorithm where multiple threads are created and used to search the decision space. Each thread represents a sequence of LLM calls and external interrupts (tool calls or human inputs) that can vary over time. The figure shows that multi-threaded programs generally form directed acyclic graphs (DAGs) where the number of threads that fork and merge varies.  The complexity of the workflow depends on the number of threads and their respective sequences of LLM calls and interrupts.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2502.13965/x5.png", "caption": "Figure 1: Execution workflows for Agentic Programs. Agentic programs are highly dynamic execution workflows that follow a directed acyclic graph (DAG). It consists of LLM calls from one or more LLM agents and external interrupts (i.e. tool calls, humans).", "description": "This figure illustrates the dynamic nature of agentic programs, which are AI programs that use LLMs to reason and interact with tools and humans.  Unlike simple chatbots, these programs don't follow a predetermined flow; instead, their execution path resembles a directed acyclic graph (DAG).  The nodes in the DAG represent LLM calls made by one or more LLM agents, while the edges show the flow of execution.  The figure shows four examples: (a) a simple chatbot, where interactions are sequential; (b) a ReAct agent, combining reasoning and action; (c) a Map-Reduce agent, performing parallel processing; and (d) a Monte Carlo Tree Search agent, using a tree-like structure for planning and decision making.  Each example highlights the highly dynamic nature of these programs, where the sequence and number of LLM calls and interactions (tools or human input) are not fixed beforehand but depend on the specific situation and context.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2502.13965/x6.png", "caption": "(a) Programs, arriving at t=0", "description": "This figure shows the arrival of four different programs at time t=0. Each program consists of a different number of LLM calls, and the number of decode steps per LLM call also varies among programs.  This illustrates the diversity in workloads that an LLM serving engine needs to handle.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2502.13965/x7.png", "caption": "(b) First-Come First-Served (FCFS)", "description": "This Gantt chart visualizes the execution of four programs (A, B, C, D) with varying numbers of LLM calls and decoding steps per call, under a First-Come, First-Served (FCFS) scheduling policy.  The chart demonstrates how longer programs (A and B) block the execution of shorter programs (C and D) due to head-of-line (HOL) blocking, resulting in significant waiting times.  This illustrates a key performance challenge that the paper addresses: program-level blocking.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2502.13965/x8.png", "caption": "(c) Multilevel Feedback Queue (MLFQ)", "description": "This Gantt chart illustrates the execution of LLM calls from four programs (A, B, C, D) with varying numbers of calls and decode steps per call, under the Multilevel Feedback Queue (MLFQ) scheduling algorithm. MLFQ is a preemptive scheduling algorithm that prioritizes shorter tasks. In this figure, we observe that although MLFQ successfully preempts the long calls from programs A and B to allow the execution of short calls from programs C and D, it still incurs program-level blocking due to repeatedly prioritizing the new calls from programs A and B. This results in a total waiting time of 18 units, same as FCFS, showing that preemption alone without considering program-level context is not enough to solve the head-of-line blocking problem.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2502.13965/x9.png", "caption": "(d) Program-Level Attained Service (PLAS)", "description": "This Gantt chart illustrates the execution of LLM calls from four programs (A, B, C, D) with varying lengths and decode steps, using Program-Level Attained Service (PLAS).  Unlike First-Come, First-Served (FCFS) and Multilevel Feedback Queue (MLFQ) scheduling, PLAS prioritizes LLM calls based on the program's previously completed calls.  This addresses head-of-line blocking at both individual LLM request and program levels, enabling short programs to complete faster. In this example, PLAS prioritizes programs C and D's calls over A and B's resulting in a total wait time of 12 units compared to 18 units for FCFS and MLFQ.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2502.13965/x10.png", "caption": "Figure 2: Gantt chart of LLM call execution on an LLM serving engine with a max batch size (BS) of 2 (Y-axis) over decoding steps (X-axis).\n(a) Four programs vary in the number of LLM calls and decode steps per call. Long programs (A, B) and short programs (C, D) are shown.\n(b) First-Come First-Served (FCFS) incurs head-of-line blocking as long LLM calls delay short LLM calls, resulting in a waiting time of 18 units.\n(c) Multilevel Feedback Queue (MLFQ) reduces blocking with preemption but still incurs program-level blocking. Programs A and B\u2019s new LLM calls are placed in the highest priority queue, delaying Program D, incurring 18 units of waiting time.\n(d) Program-Level Attained Service (PLAS) leverages program-level statistics, delaying subsequent calls in A and B to prioritize programs C and D, reducing waiting time to 12 units.", "description": "This figure demonstrates how different scheduling algorithms handle LLM call execution within an LLM serving engine. Four programs with varying numbers of LLM calls and decode steps are introduced. (a) shows the program structure. (b) shows the First-Come First-Served (FCFS) algorithm, where long calls block short calls, resulting in 18 units of waiting time. (c) depicts Multilevel Feedback Queue (MLFQ), which reduces blocking through preemption but still suffers from program-level blocking, with a waiting time of 18 units. (d) illustrates Program-Level Attained Service (PLAS), which uses program-level statistics to prioritize calls and reduce waiting time to 12 units.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2502.13965/x11.png", "caption": "Figure 3: AI Agent Infrastructure. Top: Developers and users build and execute agentic programs that orchestrate execution and persist global, cumulative history across agents, tools, and humans. Bottom: LLM serving systems process agents\u2019 LLM calls and route calls across one or more LLM engines.", "description": "This figure illustrates the architecture of an AI agent infrastructure, showing how developers and users interact with the system.  The top half depicts the high-level interaction, where users create agentic programs (dynamic workflows involving multiple agents, tools and human interaction). These programs orchestrate the execution of tasks, maintaining a global history of actions and outputs. The bottom half shows the LLM serving layer, responsible for processing the LLM calls generated by the agentic programs.  This layer manages the calls, routing them across multiple LLM engines if needed for efficiency and scalability. This distribution of workload across engines handles the computational demands of the agentic programs, allowing for more concurrent task execution.", "section": "Background & Related Work"}, {"figure_path": "https://arxiv.org/html/2502.13965/x12.png", "caption": "Figure 4: Number of LLM calls in serving engine during steady state over 1 hour. Optimizing programs\u2019 wait times increases the volume of LLM calls at steady state.", "description": "This figure shows the number of LLM calls processed by the LLM serving engine over an hour, comparing the performance of Autellix with that of the baseline system vLLM.  The graph clearly indicates that by optimizing for program wait times, Autellix is able to process a significantly higher number of LLM calls within the same timeframe.  This demonstrates that reducing wait times not only improves individual program performance but also boosts the overall throughput of the entire LLM serving system.", "section": "6 Evaluation"}, {"figure_path": "https://arxiv.org/html/2502.13965/x13.png", "caption": "Figure 5: Program execution and wait times, over different programs and system loads. With moderate loads, programs spend the most time waiting. The duration of waiting depends on the workload.", "description": "This figure displays a breakdown of the time spent by different agentic programs (Chatbot, ReAct, MCTS) on program execution versus waiting time, under various system loads.  The key takeaway is that under moderate to high system loads, a significant portion of a program's runtime is spent waiting instead of actively executing LLM calls or interacting with other program components.  The specific duration of waiting time varies depending on the characteristics of the program itself.", "section": "3 Motivation"}, {"figure_path": "https://arxiv.org/html/2502.13965/x14.png", "caption": "(a) Chatbot, LLM Calls", "description": "This figure shows the ratio of waiting time to execution time for LLM calls in a chatbot program as a function of the number of decoding steps. It compares three scheduling policies: First-Come First-Served (FCFS), Multi-Level Feedback Queue (MLFQ), and Autellix. The figure demonstrates that FCFS suffers from significant call-level head-of-line blocking, resulting in long wait times for short LLM calls. MLFQ mitigates this issue through preemption but still experiences significant wait times. Autellix achieves the lowest wait times by effectively reducing both call-level and program-level head-of-line blocking.", "section": "3.1 Program-level Wait Times"}, {"figure_path": "https://arxiv.org/html/2502.13965/x15.png", "caption": "(b) Chatbot, Programs", "description": "This figure shows the ratio of waiting time to execution time for chatbot programs, plotted against the number of LLM calls. It demonstrates the impact of different scheduling algorithms (FCFS, MLFQ, Autellix) on program wait times.  Autellix effectively reduces wait times, improving program latency and throughput.", "section": "3.1 Program-level Wait Times"}, {"figure_path": "https://arxiv.org/html/2502.13965/x16.png", "caption": "(c) MCTS, LLM Calls", "description": "This figure shows a Gantt chart illustrating the execution of LLM calls within a Monte Carlo Tree Search (MCTS) program.  The chart specifically focuses on LLM calls, illustrating how call-level head-of-line (HoL) blocking and program-level HoL blocking impact the execution time and waiting time of different calls. The x-axis represents decoding steps, and the y-axis represents different LLM calls, grouped by program.  The chart compares three scheduling algorithms: First-Come First-Served (FCFS), Multilevel Feedback Queue (MLFQ), and Autellix's Program-Level Attained Service (PLAS), visualizing the effects of each scheduler on waiting times and execution order.", "section": "3 Motivation"}, {"figure_path": "https://arxiv.org/html/2502.13965/x17.png", "caption": "(d) MCTS, Programs", "description": "This figure shows a Gantt chart illustrating the execution of LLM calls in Monte Carlo Tree Search (MCTS) programs under different scheduling policies.  It highlights how program-level head-of-line (HoL) blocking affects program execution time and latency.  The chart compares the performance of the First-Come-First-Served (FCFS) policy, the Multilevel Feedback Queue (MLFQ) algorithm, and the proposed Program-Level Attained Service (PLAS) algorithm.  The chart visualizes the waiting and execution times of various LLM calls within multiple MCTS programs, demonstrating how PLAS effectively prioritizes LLM calls to reduce wait times and enhance throughput.", "section": "3 Motivation"}, {"figure_path": "https://arxiv.org/html/2502.13965/x18.png", "caption": "Figure 6: Ratio of Waiting to Execution Time for LLM Calls and Programs. Head-of-line blocking occurs when short LLM calls and programs wait significantly longer than their execution times.", "description": "This figure displays the ratio of waiting time to execution time for both individual LLM calls and entire programs.  It demonstrates the impact of head-of-line (HOL) blocking. HOL blocking occurs when longer calls or programs delay the execution of shorter ones, leading to disproportionately long wait times for the shorter tasks. The figure uses two workloads (Chatbot and MCTS) to illustrate how the ratio of waiting time to execution time changes with the number of LLM calls and the number of decoding steps. It compares the performance of three scheduling policies: First-Come, First-Served (FCFS), Multi-Level Feedback Queue (MLFQ), and Autellix (the proposed system).  The results show that Autellix is significantly more effective at reducing HOL blocking and therefore improving wait times compared to the other two scheduling algorithms. ", "section": "3 Motivation"}, {"figure_path": "https://arxiv.org/html/2502.13965/x19.png", "caption": "(a) Single thread: Chatbot", "description": "This figure shows the cache hit rates for LLM calls within and across programs in a single-threaded Chatbot program.  It illustrates how LLM calls within the same program tend to share common prefixes and therefore benefit from cache hits (high cache hit rate), while LLM calls across different programs show significantly lower cache hit rates due to a lack of shared context.", "section": "3.2 Program-level Execution Times"}, {"figure_path": "https://arxiv.org/html/2502.13965/x20.png", "caption": "(b) Multiple threads: MCTS", "description": "This figure shows the Gantt chart of a Monte Carlo Tree Search (MCTS) program with multiple threads.  It illustrates how multiple threads concurrently execute LLM calls and interrupts. Each thread represents a separate sequence of LLM calls and tools calls, and the overall program progresses as these threads interact and merge.  The figure demonstrates the dynamic, non-deterministic, and concurrent nature of multi-threaded agentic programs.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2502.13965/x21.png", "caption": "Figure 7: Prefix cache hit rates for LLM calls within and across programs. LLM calls within the same program often share KV cache, whereas LLM calls across programs typically do not.", "description": "This figure illustrates the cache hit rates for prefix caching in Large Language Models (LLMs). Prefix caching is a technique to improve efficiency by storing and reusing previously computed results. The figure shows that LLM calls within the same program tend to share a lot of common prefixes, resulting in high cache hit rates. In contrast, LLM calls from different programs typically do not share many common prefixes, leading to lower cache hit rates. This demonstrates the importance of considering the program-level context for optimizing LLM serving systems.", "section": "3.2 Program-level Execution Times"}, {"figure_path": "https://arxiv.org/html/2502.13965/x22.png", "caption": "Figure 8: Autellix\u2019s system architecture. Users run their programs locally, which initiates a stateful session and submits LLM calls to Autellix\u2019s backend. Autellix leverages a global process table to track sessions and better inform its custom load-balancer and scheduler.", "description": "Autellix's system architecture consists of a frontend where users run their agentic programs locally. Each program initiates a stateful session with Autellix's backend, which is responsible for managing the lifecycle of LLM calls.  The backend uses a global process table to maintain a record of the status of all running programs. This table provides contextual information to Autellix's custom load balancer, which distributes the LLM calls across available LLM engines based on various factors like data locality, and to the scheduler, which prioritizes LLM calls to reduce waiting times and improve throughput.", "section": "4 Autellix Design"}, {"figure_path": "https://arxiv.org/html/2502.13965/x23.png", "caption": "Figure 9: Critical path for multi-threaded programs. (Left) Example of a critical path through a DAG. (Right) Best-case scenario makespan, 14 units, versus worst-case makespan. 11 units.", "description": "This figure illustrates the concept of a critical path in multi-threaded programs, which are represented as directed acyclic graphs (DAGs).  The left panel shows an example DAG and highlights the critical path, which is the longest sequence of dependent tasks that determines the overall program execution time.  The right panel compares the best-case and worst-case makespans (total execution times) for this particular example DAG. The best-case scenario shows a makespan of 14 units, while the worst-case scenario shows a makespan of 11 units. This difference highlights the impact of task scheduling and resource allocation on the overall efficiency of multi-threaded programs.", "section": "4.2.1 Program-level Prioritization"}, {"figure_path": "https://arxiv.org/html/2502.13965/x24.png", "caption": "Figure 10: LLM call lifecycle based on discretized prioritization.", "description": "This figure illustrates the lifecycle of an LLM call within Autellix's program-aware scheduler.  The scheduler uses discretized priorities, assigning LLM calls to different queues (Q1, Q2,...QK) based on their program's priority, rather than using continuous priority values.  Calls within a queue are processed in FCFS (First-Come, First-Served) order.  A call that exhausts its time quantum in a queue is demoted to a lower-priority queue, while calls waiting too long are promoted to the highest priority queue via anti-starvation mechanisms. This approach ensures efficient processing of calls while preventing starvation of low-priority programs.", "section": "4.2.2 Preemptive Scheduling"}, {"figure_path": "https://arxiv.org/html/2502.13965/x25.png", "caption": "(a) ShareGPT", "description": "Figure 11(a) shows the distribution of the number of prefill and decode tokens used in the ShareGPT dataset.  The x-axis represents the number of tokens, and the y-axis represents the density of the token counts. The figure helps to illustrate the characteristics of the ShareGPT dataset, showing a heavy-tailed distribution with a larger number of shorter conversations and a smaller number of very long conversations. This distribution is important because it impacts the performance of LLM serving systems, which must handle a wide range of request sizes efficiently. ", "section": "6.1 Workloads"}, {"figure_path": "https://arxiv.org/html/2502.13965/x26.png", "caption": "(b) BFCL", "description": "This figure (Figure 11(b) in the paper) shows the distribution of prefill and decode tokens for the BFCL (Berkeley Function Calling Leaderboard) workload.  The BFCL workload involves multi-turn, multi-step tool usage tasks.  The x-axis represents the number of tokens, while the y-axis represents the probability density. The figure demonstrates that BFCL tasks are characterized by long prefill sequences (mean of 735 tokens) compared to relatively short decode sequences (mean of 34 tokens). This is because the prefill phase involves detailed system prompts and tool signatures, while the decoding phase mainly focuses on the agent's responses.", "section": "6.1 Workloads"}, {"figure_path": "https://arxiv.org/html/2502.13965/x27.png", "caption": "(c) LATS", "description": "This figure (c) shows the distribution of the number of LLM calls and the distribution of prefill and decode tokens for the LATS (Large Language Model Agents for Tree Search) workload.  LATS involves multi-threaded programs with many parallel LLM calls, resulting in a significant number of LLM calls per program and a higher average number of tokens compared to other workloads (ShareGPT and BFCL).  These distributions highlight the computationally intensive and iterative nature of MCTS (Monte Carlo Tree Search) which this workload is based on, presenting significant challenges for LLM serving systems.", "section": "6.1 Workloads"}, {"figure_path": "https://arxiv.org/html/2502.13965/x28.png", "caption": "(d) Number of LLM calls", "description": "This figure is a histogram showing the distribution of the number of LLM calls made by different agentic programs across four different workloads: ShareGPT, BFCL, LATS, and a mixed workload combining all three.  The x-axis represents the number of LLM calls, and the y-axis represents the density or frequency of programs with that number of calls.  It illustrates the variability in the number of LLM calls required by different agentic programs and the prevalence of long-tailed distributions, where some programs make many more calls than others.", "section": "6.1 Workloads"}, {"figure_path": "https://arxiv.org/html/2502.13965/x29.png", "caption": "Figure 11: Workload analysis. LLM call statistics of programs from each workload. Input and output length distributions for (a) ShareGPT, (b) BFCL, and (c) LATS. Subfigure (d) plots the distribution of number of LLM calls in each workload.", "description": "Figure 11 presents a detailed analysis of the characteristics of four different agentic program workloads: ShareGPT, BFCL, LATS, and a mixed workload combining all three.  The figure is divided into four subfigures. (a), (b), and (c) show the distributions of input and output token lengths for ShareGPT, BFCL, and LATS, respectively. These distributions reveal the varying lengths of prompts and responses in each dataset.  (d) displays the distribution of the total number of LLM calls used within each type of program, highlighting the differences in the complexity and scale of the tasks undertaken.", "section": "6 Evaluation"}, {"figure_path": "https://arxiv.org/html/2502.13965/x30.png", "caption": "Figure 12: Single Engine, Main Results. Average latency for different LLM serving systems across four real-world workloads.", "description": "This figure displays the average latency (in seconds per token) of four different LLM serving systems (vLLM, vLLM-opt, MLFQ, and Autellix) across four distinct real-world workloads (ShareGPT, BFCL, LATS, and Mixed).  The x-axis represents the arrival rate of programs (programs per second), and the y-axis represents the average latency.  The figure helps to illustrate the performance gains achieved by Autellix compared to the baseline systems under various workload conditions and arrival rates. It shows that Autellix consistently provides lower latency at higher arrival rates.", "section": "6 Evaluation"}, {"figure_path": "https://arxiv.org/html/2502.13965/x31.png", "caption": "Figure 13: Single Engine, Tail Latencies. 95th (P95) and 99th (P99) percentile latencies of different serving systems.", "description": "This figure compares the tail latencies (95th and 99th percentiles) of four different LLM serving systems (vLLM, vLLM-opt, MLFQ, and Autellix) across four real-world workloads (ShareGPT, BFCL, LATS, and Mixed).  It shows how the different systems perform under varying arrival rates of programs and highlights the impact of different scheduling strategies on the latency distribution, particularly focusing on how well each system handles extreme latency outliers.", "section": "6.3 End-to-End Single-Engine Performance"}, {"figure_path": "https://arxiv.org/html/2502.13965/x32.png", "caption": "Figure 14: Multi-engine, Main Results. Latencies (Avg., P95/99) w.r.t. different load balancing policies.", "description": "This figure compares the performance of different load balancing strategies (Round Robin, Least Used, and Autellix) across multiple LLM engines.  It shows the average latency, 95th percentile latency (P95), and 99th percentile latency (P99) for two different workloads (ShareGPT and LATS) and two different model sizes (LLaMA 3.1-8B and LLaMA 3.1-70B). The results demonstrate the effectiveness of Autellix's data locality-aware load balancing in maintaining low average and tail latencies across multiple engines, compared to round-robin and least-used strategies.", "section": "6 End-to-End Multi-Engine Performance"}, {"figure_path": "https://arxiv.org/html/2502.13965/x33.png", "caption": "Figure 15: Scalability Experiments. Given same SLO (defined as s/tok), Autellix\u2019s max arrival rate (program/s) scales linearly w.r.t number of replicas, or LLM engines.", "description": "This figure demonstrates the scalability of Autellix by showing that as the number of LLM engines (replicas) increases, the maximum program arrival rate that Autellix can handle while maintaining the same service level objective (SLO, measured as seconds per token) also increases linearly.  This highlights Autellix's ability to efficiently distribute the workload across multiple engines, ensuring consistent performance even under high loads. The experiment uses the ShareGPT workload to evaluate the performance.", "section": "6.5 Ablations"}, {"figure_path": "https://arxiv.org/html/2502.13965/x34.png", "caption": "Figure 16: Offline batch inference. Autellix decreases the time, or makespan, required to process a batch of programs.", "description": "This figure shows the results of an experiment comparing the time taken to process a batch of programs offline (without interactive user input).  The experiment compares Autellix to other systems (VLLM, vLLM-opt, MLFQ).  The y-axis represents the total time (makespan) for the entire batch to complete, and the x-axis represents the number of programs in the batch. The graph demonstrates that Autellix completes the same batch of programs significantly faster than the other systems.  This improvement is attributed to Autellix's program-level scheduling and load balancing capabilities which prioritize program completion over processing individual LLM calls.", "section": "6 Evaluation"}]