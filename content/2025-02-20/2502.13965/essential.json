{"importance": "This paper introduces **Autellix, a novel LLM serving system, addressing key challenges in deploying LLM agents**. Its program-aware scheduling and load balancing improve efficiency and reduce latency. As LLM agents become more prevalent, **Autellix provides valuable insights and methods for researchers to optimize their performance and scalability**, paving the way for more efficient and responsive AI systems.", "summary": "Autellix: Efficient LLM Serving for Agents", "takeaways": ["LLM serving systems must evolve to address dependencies and optimize programs.", "Program-aware scheduling significantly reduces latency and improves throughput.", "Data locality-aware load balancing enhances performance in multi-engine deployments."], "tldr": "Current Large Language Model (LLM) serving systems are not optimized for the evolving landscape of dynamic, general-purpose agentic programs. Existing systems often ignore dependencies between programs and calls, leading to missed optimization opportunities and significant waiting times. This results from head-of-line blocking at both the individual LLM request and the program levels. Addressing these issues is critical for enhancing the efficiency and responsiveness of AI agents.\n\nTo tackle these challenges, this paper introduces Autellix, an LLM serving system designed to treat programs as first-class citizens. Autellix intercepts LLM calls and enriches schedulers with program-level context, utilizing two scheduling algorithms: Program-Level Attained Service (PLAS) for single-threaded programs and Adaptive Thread-Level Attained Service (ATLAS) for distributed programs. Evaluation demonstrates that Autellix improves throughput by 4-15x compared to state-of-the-art systems, while maintaining comparable latency. ", "affiliation": "UC Berkeley", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}, "podcast_path": "2502.13965/podcast.wav"}