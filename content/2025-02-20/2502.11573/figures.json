[{"figure_path": "https://arxiv.org/html/2502.11573/x1.png", "caption": "Figure 1: The pipeline of pretrain data drocesses: heuristic filtering, reasoning-oriented text recall, deduplication, quality assessment and decontamination. Comparative experiments on LLaMA3.2-1B with differently cleaned datasets validate the significance of data quality.", "description": "This figure illustrates the data preprocessing pipeline used to create high-quality training data for small language models.  The pipeline consists of five stages: 1) Heuristic Filtering removes low-quality data using various methods. 2) Reasoning-Oriented Text Recall focuses on selecting data relevant to reasoning tasks.  3) Deduplication removes redundant data. 4) Quality Assessment employs various techniques to ensure the quality of the selected data. 5) Decontamination removes potentially biased or contaminated information. The caption notes that comparative experiments using LLaMA3.2-1B with datasets processed using different cleaning methods validate the importance of high data quality.", "section": "2 Small Language Model Pre-training"}, {"figure_path": "https://arxiv.org/html/2502.11573/x2.png", "caption": "Figure 2: Supervised fine-tuning data synthesis pipeline. The pipeline initiates with a set of high-quality seed data, which is augmented through instruction evolution. Response candidates are generated using the Qwen-2.5-32B-Instruct model, followed by rejection sampling with a reward model and sandbox environment. Finally, we score the curated data for quality and difficulty, and assign domain labels.", "description": "This figure illustrates the process of creating a high-quality dataset for supervised fine-tuning.  It starts with a set of high-quality seed data which is then expanded by generating variations of the instructions (instruction evolution).  A large language model (Qwen-2.5-32B-Instruct) generates multiple response candidates for each instruction.  To ensure high quality, a rejection sampling method is used, leveraging a reward model and a sandbox environment to filter out low-quality responses. The final curated data is scored for quality and difficulty, and then labeled with domain-specific tags before being used in the fine-tuning phase.", "section": "2.2 Annealing Data"}, {"figure_path": "https://arxiv.org/html/2502.11573/extracted/6208609/figures/mslm.png", "caption": "Figure 3: Illustration of the MSLM training pipeline and the MSLM training details, showcasing the progression from captioning and QA tasks to text rendering, followed by instruction-tuning, culminating in enhanced mathematical and operating system reasoning abilities.", "description": "Figure 3 illustrates the training process for Multimodal Small Language Models (MSLMs).  It begins with a pre-training stage focusing on fundamental visual-language alignment through tasks like captioning and question answering.  This is followed by a text rendering stage which generates data by converting various text formats (documents, code, etc.) into visual representations to enhance the model's understanding of the relationship between visual and textual information.  Next, instruction tuning further refines the model's capabilities. This stage includes instruction-tuning using multiple datasets which cover diverse domains (mathematics, operator system tasks, etc.) across various modalities (single image, multiple images, text). This progressive training approach culminates in an MSLM with enhanced reasoning abilities in both mathematical and operating system contexts.", "section": "4 Small Multimodal Language Model Training"}, {"figure_path": "https://arxiv.org/html/2502.11573/x3.png", "caption": "(a)", "description": "This figure shows the distribution of programming languages used in the dataset.  Python is the most prevalent language, followed by Java, JavaScript, and C++, indicating a focus on popular and widely used languages in software development. The other languages are used to a lesser extent but still contribute to the diversity of the dataset. This diversity ensures the model is exposed to a broad range of coding styles and paradigms.", "section": "2.1.1 Data Collection"}, {"figure_path": "https://arxiv.org/html/2502.11573/extracted/6208609/figures/bge_bar.jpg", "caption": "(b)", "description": "The figure shows a histogram of the similarity scores for 2500 image-text pairs sampled from the COCO-caption dataset. The x-axis represents the similarity score, and the y-axis represents the frequency.  The distribution is heavily concentrated around a similarity score of 0.7, indicating that most image-text pairs have relatively high similarity. There is a tail towards lower similarity scores, which may represent image-text pairs with lower semantic alignment.  This distribution is important because it informs a decision about a similarity threshold used for filtering low-quality pairs from the dataset during the data cleaning phase, impacting the overall quality of the multimodal model training.", "section": "4.1.2 Data Cleaning"}]