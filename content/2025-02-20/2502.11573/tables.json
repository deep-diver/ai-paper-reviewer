[{"content": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"A6.T7.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"A6.T7.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"A6.T7.1.1.1.1\">Type Source</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"A6.T7.1.1.1.2\">Datasets</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A6.T7.1.2.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A6.T7.1.2.1.1\">Web Pages</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A6.T7.1.2.1.2\">Common crawl, FineWeb-Edu, DCLM</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A6.T7.1.3.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"A6.T7.1.3.2.1\">Math</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A6.T7.1.3.2.2\">InfiMM-WebMath-40B, OpenWebMath, MathCode-Pile, Proof-pile-2, finemath</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A6.T7.1.4.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"A6.T7.1.4.3.1\">Code</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A6.T7.1.4.3.2\">the-Stack-v2, Starcoder</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A6.T7.1.5.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"A6.T7.1.5.4.1\">General Knowledge</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A6.T7.1.5.4.2\">arXiv, StackExchange, peS2o</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A6.T7.1.6.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"A6.T7.1.6.5.1\">Encyclopedia</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A6.T7.1.6.5.2\">Wikipedia</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A6.T7.1.7.6\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A6.T7.1.7.6.1\">Open-Source Instruction</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A6.T7.1.7.6.2\">FLAN, Infinity Instruct</td>\n</tr>\n</tbody>\n</table>", "caption": "Table 1: Performance of base models on various benchmarks using few-shot evaluation. The values in parentheses indicate the claimed results from the respective papers.", "description": "This table compares the performance of several different base language models on a variety of established benchmarks.  The models are evaluated using a few-shot evaluation approach.  The table shows each model's scores on various benchmarks, such as MMLU for general reasoning, GSM8K for math word problems, MATH for advanced math problems, HumanEval for code generation, and MBPP for Python programming tasks.  Scores reported in parentheses represent the performance values claimed by the original authors of the respective papers. This allows for a comparison between the results reported here and the performance reported by the original research teams.", "section": "5.1 Small Language Models"}, {"content": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"A6.T8.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"A6.T8.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"A6.T8.1.1.1.1\">Type Source</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"A6.T8.1.1.1.2\">Datasets</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A6.T8.1.2.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A6.T8.1.2.1.1\">Math</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A6.T8.1.2.1.2\">InfiMM-WebMath-40B, OpenWebMath, MathCode-Pile, Proof-pile-2, finemath</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A6.T8.1.3.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"A6.T8.1.3.2.1\">Code</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A6.T8.1.3.2.2\">the-Stack-v2, Starcoder, opc-annealing-corpus</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A6.T8.1.4.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"A6.T8.1.4.3.1\">General Knowledge</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A6.T8.1.4.3.2\">StackExchange, peS2o</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A6.T8.1.5.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"A6.T8.1.5.4.1\">Encyclopedia</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A6.T8.1.5.4.2\">Wikipedia</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A6.T8.1.6.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"A6.T8.1.6.5.1\">Open-Source corpus</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A6.T8.1.6.5.2\">FLAN, Infinity Instruct, Dolmino</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A6.T8.1.7.6\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A6.T8.1.7.6.1\">Synthetic data</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A6.T8.1.7.6.2\">Ours</td>\n</tr>\n</tbody>\n</table>", "caption": "Table 2: Performance of InfiR-1B-Instruct models on various benchmarks using zero-shot evaluation. The values in parentheses indicate the claimed results from the respective papers. InfiR-1B-Instruct outperforms Llama-3.2-1B-Instruct in these reasoning benchmarks, and is proximate to Qwen-2.5-1.5B-Instruct in mathematical reasoning and code tasks.", "description": "This table presents a comparison of the InfiR-1B-Instruct model's performance against Llama-3.2-1B-Instruct and Qwen-2.5-1.5B-Instruct across multiple reasoning benchmarks using zero-shot evaluation.  Zero-shot evaluation means the models were not given any examples before being tested on the benchmarks. The results are shown as scores for each benchmark. Scores in parentheses represent values reported in other papers for the same models. The comparison highlights that InfiR-1B-Instruct significantly outperforms Llama-3.2-1B-Instruct in several reasoning tasks and demonstrates performance comparable to Qwen-2.5-1.5B-Instruct, especially in mathematical reasoning and code generation tasks.", "section": "5.1 Small Language Models"}]