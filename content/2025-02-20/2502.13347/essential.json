{"importance": "This research offers a significant advancement in web crawling for LLM pretraining, addressing the critical issue of data quality and efficiency. By prioritizing pretraining influence, it reduces data processing waste and alleviates website burdens. It paves the way for more sustainable data acquisition, offering a new direction for future research to explore advanced, targeted crawling strategies.", "summary": "CRAW4LLM: Efficiently crawls web pages for LLM pretraining by prioritizing influence scores, boosting data quality & cutting crawling waste.", "takeaways": ["CRAW4LLM enhances web crawling for LLM pretraining by prioritizing pages based on their pretraining influence, outperforming traditional graph-connectivity methods.", "The method significantly reduces crawling waste, achieving comparable downstream performance with LLMs using only 21% of the data required by previous crawls.", "Analysis reveals CRAW4LLM quickly identifies documents aligned with oracle selection, demonstrating its efficiency in discovering high-quality pretraining data."], "tldr": "Web crawl is key for pretraining LLMs, but much crawled data gets discarded due to low quality. Traditional web crawlers prioritize graph connectivity, favoring high inlink counts which does not align well with LLM pretraining needs and causing computational waste and website burden. To address this inefficiency, this paper introduces **CRAW4LLM**, a novel web crawling method designed to enhance the efficiency of collecting high-quality pretraining data for LLMs. \n\nInstead of relying on traditional graph-based metrics, **CRAW4LLM** prioritizes webpages based on their **influence on LLM pretraining**.  It leverages a pretraining influence scorer derived from data-filtering pipelines to assess and rank newly discovered documents during each crawling iteration. Experiment results show that **CRAW4LLM** significantly improves crawling efficiency and reduces data waste, outperforming traditional methods by achieving similar performance with substantially less data. ", "affiliation": "Tsinghua University", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}, "podcast_path": "2502.13347/podcast.wav"}