{"importance": "This paper is important for researchers because it highlights the limitations of directly applying large model distillation to smaller models and introduces a novel approach (**Mix Distillation**) to overcome these limitations. This work paves the way for more effective knowledge transfer strategies and opens new avenues for research on adapting reasoning complexity for smaller models.", "summary": "Small language models struggle to learn complex reasoning from large models, but a novel \"Mix Distillation\" method balances complexity for effective capability transfer.", "takeaways": ["Small models do not consistently benefit from distillation from larger models.", "Small models perform better when trained on simpler reasoning chains.", "Mix Distillation, which balances reasoning complexity, significantly improves small model reasoning performance."], "tldr": "Large language models (LLMs) have excelled in reasoning, prompting efforts to distill their capabilities into smaller models. However, this paper reveals the **Small Model Learnability Gap**: small models struggle with complex reasoning traces or distillation from larger models, performing better with shorter, simpler reasoning. This is because they find it hard to internalize the logic due to their limited ability.\n\nTo address this, the paper introduces **Mix Distillation**, a strategy that balances reasoning complexity by combining long and short reasoning or blending responses from larger and smaller models. Experiments demonstrate that Mix Distillation significantly enhances small model reasoning compared to training on either data type alone, adapting reasoning for effective capability transfer.", "affiliation": "University of Washington", "categories": {"main_category": "Machine Learning", "sub_category": "Deep Learning"}, "podcast_path": "2502.12143/podcast.wav"}