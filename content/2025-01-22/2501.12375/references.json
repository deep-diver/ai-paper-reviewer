{"references": [{"fullname_first_author": "Reiner Birkl", "paper_title": "MiDaS v3: a model zoo for robust monocular relative depth estimation", "publication_date": "2023-07-14", "reason": "This paper provides the foundation model (MiDaS v3) used in the current work, and is a state-of-the-art model for monocular depth estimation."}, {"fullname_first_author": "Lihe Yang", "paper_title": "Depth anything: Unleashing the power of large-scale unlabeled data", "publication_date": "2024-00-00", "reason": "This paper introduces Depth Anything V2, which serves as the basis for Video Depth Anything, providing a strong backbone for the model."}, {"fullname_first_author": "Wenbo Hu", "paper_title": "DepthCrafter: Generating consistent long depth sequences for open-world videos", "publication_date": "2024-09-02", "reason": "This is a key comparative method, representing the current state-of-the-art in consistent video depth estimation,  and is extensively compared against in this paper."}, {"fullname_first_author": "Jiahao Shao", "paper_title": "Learning temporally consistent video depth from video diffusion priors", "publication_date": "2024-06-01", "reason": "This is another important comparative method leveraging video diffusion models, which is a key approach in the field, and is thoroughly analyzed against the new method presented."}, {"fullname_first_author": "Yiran Wang", "paper_title": "Neural video depth stabilizer", "publication_date": "2023-00-00", "reason": "This is an important comparative method, offering a different approach to video depth stabilization, using optical flow, and is rigorously benchmarked against the proposed method."}]}