[{"figure_path": "https://arxiv.org/html/2501.12375/x2.png", "caption": "Figure 1: Left: Our model can generate consistent depth predictions for long videos with rich actions. The demo video shows a 196-second (4690 frames) long take of pair skating, as sourced from\u00a0[14]. Right: Comparison to baselines in terms of accuracy (\u03b41subscript\ud835\udeff1\\delta_{1}italic_\u03b4 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT), consistency, and latency on the Nvidia A100 GPU (denoted with circle size). Consistency is defined as the maximum Temporal Alignment Error (TAE) among all models minus the TAE of each individual model. Our model achieves the best performance in all aspects.", "description": "Figure 1 demonstrates the model's capabilities in two aspects. The left panel showcases the model's ability to generate consistent depth maps for a long video (196 seconds, 4690 frames) depicting pair figure skating.  This highlights the model's performance on complex, real-world actions within extended video sequences. The right panel presents a quantitative comparison against several baseline methods, using three key metrics: accuracy (\u03b41), consistency (measured as the difference between the maximum Temporal Alignment Error (TAE) across all models and the individual model's TAE), and inference speed (latency) on an Nvidia A100 GPU.  Circle size in the chart represents latency.  The results show that the proposed model outperforms the baselines across all three metrics, indicating superior performance in both accuracy and temporal consistency for long-form video depth estimation.", "section": "Abstract"}, {"figure_path": "https://arxiv.org/html/2501.12375/x3.png", "caption": "Figure 2: Overall pipeline and the spatio-temporal head. Left: Our model is composed of a backbone encoder from Depth Anything V2 and a newly proposed spatio-temporal head. We jointly train our model on video data using ground-truth depth labels for supervision and on unlabeled images with pseudo labels generated by a teacher model. During training, only the head is learned. Right: Our spatiotemporal head inserts several temporal layers into the DPT head, while preserving the original structure of DPT head\u00a0[28].", "description": "Figure 2 illustrates the architecture of the Video Depth Anything model. The left panel shows the overall pipeline, highlighting the joint training process on video data with ground truth depth and unlabeled images with pseudo labels generated by a teacher model.  Only the spatio-temporal head is trained, keeping the Depth Anything V2 encoder frozen. The right panel focuses on the details of the spatio-temporal head, showing how it's built upon the DPT head [28] by incorporating multiple temporal attention layers. This design aims to effectively integrate temporal information for consistent depth estimation without significantly altering the existing DPT architecture.", "section": "3. Video Depth Anything"}, {"figure_path": "https://arxiv.org/html/2501.12375/extracted/6138791/figures/imgs/performance_comparison.png", "caption": "Figure 3: Inference strategy for long videos. N\ud835\udc41Nitalic_N is the video clip lenght consumed by our model. Each inference video clip is built by N\u2212To\u2212Tk\ud835\udc41subscript\ud835\udc47\ud835\udc5csubscript\ud835\udc47\ud835\udc58N-T_{o}-T_{k}italic_N - italic_T start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT - italic_T start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT future frames, Tosubscript\ud835\udc47\ud835\udc5cT_{o}italic_T start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT overlapping/adjacent frames, and Tksubscript\ud835\udc47\ud835\udc58T_{k}italic_T start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT key frames. The key frames are selected by taking every \u0394ksubscript\u0394\ud835\udc58\\Delta_{k}roman_\u0394 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT-th frame going backward. Then, the new depth predictions will be scale-shift-aligned to the previous frames based on the Tksubscript\ud835\udc47\ud835\udc58T_{k}italic_T start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT overlapping frames. We use N=32,To=8,Tk=2,\u0394k=12formulae-sequence\ud835\udc4132formulae-sequencesubscript\ud835\udc47\ud835\udc5c8formulae-sequencesubscript\ud835\udc47\ud835\udc582subscript\u0394\ud835\udc5812N=32,T_{o}=8,T_{k}=2,\\Delta_{k}=12italic_N = 32 , italic_T start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT = 8 , italic_T start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = 2 , roman_\u0394 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = 12.", "description": "This figure illustrates the inference strategy used for processing long videos.  The model processes the video in segments. Each segment includes future frames, overlapping frames from the previous segment, and keyframes selected from even further back. This approach ensures temporal consistency by using overlapping frames for alignment and keyframes to maintain consistent scale and shift across segments. The specific parameters used are N (total frames in segment) = 32, To (overlapping frames) = 8, Tk (key frames) = 2, and \u0394k (interval between keyframes) = 12.", "section": "3. Video Depth Anything"}, {"figure_path": "https://arxiv.org/html/2501.12375/x4.png", "caption": "Figure 4: Video depth estimation accuracy for different frame length. We compare our model (VDA-L) with DepthCrafter\u00a0[13] and DepthAnyVideo\u00a0[40] from 110 to 500 frames on Bonn\u00a0[24], Scannet\u00a0[7], and NYUv2\u00a0[22].", "description": "This figure displays a comparison of video depth estimation accuracy across varying video lengths.  The accuracy (\u03b4\u2081) of three different models\u2014the authors' Video Depth Anything (VDA-L), DepthCrafter [13], and DepthAnyVideo [40]\u2014is assessed using the metrics AbsRel and \u03b4\u2081 for video lengths ranging from 110 to 500 frames. The comparison is made across three distinct datasets: Bonn [24], Scannet [7], and NYUv2 [22], to demonstrate the performance of the proposed model (VDA-L) in handling long videos.", "section": "4.1. Evaluation"}, {"figure_path": "https://arxiv.org/html/2501.12375/x5.png", "caption": "Figure 5: Qualitative comparison for real-world long video depth estimation. We compare our model with DAv2-L\u00a0[42] and DepthCrafter\u00a0[13] on 500-frame videos from Scannet\u00a0[7] and Bonn\u00a0[24].", "description": "Figure 5 presents a qualitative comparison of real-world long-video depth estimation results.  Three models are compared: the authors' proposed Video Depth Anything model, DepthCrafter [13], and Depth Anything v2 [42]. The comparison uses 500-frame video sequences from the Scannet [7] and Bonn [24] datasets.  The figure visually demonstrates the performance differences between the models in terms of depth accuracy and temporal consistency.  It highlights instances where the authors' model produces superior depth estimates, particularly in scenarios with complex lighting or object movement, indicating better handling of challenging real-world conditions.", "section": "4.2. Zero-shot Depth Estimation"}, {"figure_path": "https://arxiv.org/html/2501.12375/x6.png", "caption": "Figure 6: Qualitative comparison for in-the-wild short video depth estimation. We compare with Depth-Anything-V2\u00a0[42], DepthCrafter\u00a0[13] and DepthAnyVideo\u00a0[40] on videos with less than 100 frames from DAVIS\u00a0[26]. Red boxes show incorrect depth estimation while blue boxes show inconsistent depth estimation.", "description": "This figure shows a qualitative comparison of depth estimation results for short, in-the-wild videos.  Four different methods are compared: Depth-Anything-V2, DepthCrafter, DepthAnyVideo, and the proposed method. The methods are evaluated on videos from the DAVIS dataset, all under 100 frames. Red boxes highlight examples where the depth estimations are incorrect, while blue boxes point to inconsistencies in the depth maps over time.  This visualization demonstrates the relative strengths and weaknesses of each method in terms of accuracy and temporal consistency for short, real-world video sequences.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2501.12375/x7.png", "caption": "Figure 7: Qualitative comparisons of different inference strategies. We compare overlap alignment (OA) with our proposed overlap interpolation and key-frame referencing (OI + KR) on a self-captured video with 7320 frames.", "description": "This figure compares two different inference strategies for processing super-long videos (videos with over 7000 frames): overlap alignment (OA) and overlap interpolation with key-frame referencing (OI+KR).  OA simply concatenates results from sequentially processed video segments. OI+KR, the authors' proposed method, uses overlapping segments and keyframes to maintain temporal consistency and avoid accumulating errors over very long videos.  The figure visually demonstrates how OI+KR produces significantly smoother and more accurate depth estimations compared to OA, especially over extended durations.", "section": "3.3 Inference strategy for super-long sequence"}, {"figure_path": "https://arxiv.org/html/2501.12375/x8.png", "caption": "Figure 8: Qualitative comparison for static image depth estimation. We compare our model with Depth-Anything-V2\u00a0[42], DepthCrafter\u00a0[13], and Depth Any Video\u00a0[40] on static image depth estimation. Our model demonstrates visualization results comparable to those of Depth-Anything-V2\u00a0[42].", "description": "Figure 8 presents a qualitative comparison of static image depth estimation results from four different models: the proposed Video Depth Anything model, Depth-Anything-V2, DepthCrafter, and Depth Any Video.  The figure visually demonstrates the depth maps generated by each model for several example images. This allows for a direct comparison of the accuracy and detail present in each model's depth prediction. The results showcase that the proposed model achieves comparable performance to Depth-Anything-V2 in terms of visualization quality, suggesting a successful transfer of the strong image depth estimation capabilities of Depth-Anything-V2 to the video domain.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2501.12375/x9.png", "caption": "Figure 9: Qualitative comparison for real-world long video depth estimation. We compare with Depth-Anything-V2\u00a0[42] and DepthCrafter\u00a0[13] on 500-frames videos from Scannet\u00a0[7] and Bonn\u00a0[24] . We show changes in color and depth over time at the vertical red line in videos. White boxes show inconsistent estimation. Blue boxes show our algorithm has higher accuracy.", "description": "Figure 9 presents a qualitative comparison of real-world long-video depth estimation results.  It compares the model's performance against Depth-Anything-V2 and DepthCrafter on videos containing 500 frames from the Scannet and Bonn datasets.  The figure visually demonstrates the temporal consistency (or inconsistency) of the depth estimation across the video sequence by highlighting changes in color and depth over time along vertical red lines.  White boxes highlight areas where depth estimation is inconsistent, whereas blue boxes highlight areas where the proposed method shows higher accuracy than the baselines. This allows for a visual assessment of temporal consistency and accuracy comparison.", "section": "4.2. Zero-shot Depth Estimation"}, {"figure_path": "https://arxiv.org/html/2501.12375/x10.png", "caption": "Figure 10: Temporal layer. The feature shape is adjusted for temporal attention.", "description": "This figure illustrates a temporal layer within the spatiotemporal head of the Video Depth Anything model.  The input features undergo a transformation to prepare them for the temporal attention mechanism. The temporal attention operates along the temporal dimension (number of frames), allowing the model to effectively capture and utilize the temporal relationships between frames within the input video sequence. The output features then return to the original shape for further processing. This layer is crucial for maintaining temporal consistency in the final depth prediction.", "section": "3.1 Architecture"}, {"figure_path": "https://arxiv.org/html/2501.12375/x11.png", "caption": "Figure 11: 3D Video Conversion. A video from the DAVIS dataset\u00a0[26] is transformed into a 3D video using our model.", "description": "This figure demonstrates the application of the Video Depth Anything model to generate a 3D video from a standard 2D video.  The input video is sourced from the DAVIS dataset [26]. The model processes the 2D video frames, estimating depth information for each frame. This depth information is then used to reconstruct a 3D representation of the scene, effectively converting the original 2D video into a 3D video. This showcases the model's ability to not only estimate depth accurately but also to utilize that depth information for higher-level tasks such as 3D video generation.", "section": "6. Applications"}]