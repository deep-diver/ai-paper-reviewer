{"importance": "This paper is important because it addresses the scarcity of publicly available multi-modal reward models for Large Vision Language Models (LVLMs).  **It introduces InternLM-XComposer2.5-Reward (IXC-2.5-Reward), a simple yet effective model**,  demonstrates its effectiveness across various benchmarks and applications (RL training, test-time scaling, data cleaning), and open-sources its weights and training recipes, thus advancing research and development in LVLMs.  **Its findings have strong implications for improving the quality and reliability of LVLMs** in diverse applications.", "summary": "InternLM-XComposer2.5-Reward: A novel multi-modal reward model boosting Large Vision Language Model performance.", "takeaways": ["IXC-2.5-Reward, a novel multi-modal reward model, significantly improves Large Vision Language Model (LVLM) performance.", "The model demonstrates effectiveness in RL training, test-time scaling, and data cleaning, showcasing its versatility.", "Open-sourcing the model and training recipes facilitates further research and development in the field."], "tldr": "Current Large Vision Language Models (LVLMs) often produce inaccurate outputs. While reward models (RMs) offer improvement potential, publicly available multi-modal RMs for LVLMs are lacking, hindering progress. This research addresses this gap by introducing InternLM-XComposer2.5-Reward (IXC-2.5-Reward), a simple yet effective multi-modal RM.  **The model uses a high-quality multi-modal preference corpus for training**, encompassing text, image, and video data across various domains. \n\nThe study demonstrates IXC-2.5-Reward's effectiveness in three key applications. **First**, it provides a supervisory signal for reinforcement learning (RL), improving instruction following and multi-modal dialogue. **Second**, it enables test-time scaling by selecting the best response from candidates. **Third**, it filters noisy samples from training data.  **IXC-2.5-Reward outperforms existing models on benchmark evaluations** and shows competitive results even on text-only benchmarks.  The authors have open-sourced all model weights and training recipes to encourage further research.", "affiliation": "Shanghai Artificial Intelligence Laboratory", "categories": {"main_category": "Multimodal Learning", "sub_category": "Vision-Language Models"}, "podcast_path": "2501.12368/podcast.wav"}