[{"figure_path": "https://arxiv.org/html/2501.12368/x1.png", "caption": "Figure 1: (a) To train the IXC-2.5-Reward, we construct a multi-modal preference dataset spanning diverse domains (e.g., natural scenes, text-rich, reasoning) and modalities (image, text, video). (b) The framework of IXC-2.5-Reward. (c) The IXC-2.5-Reward guides policy training for IXC-2.5-Chat via reinforcement learning.", "description": "Figure 1 illustrates the training and application of the InternLM-XComposer2.5-Reward (IXC-2.5-Reward) model.  Panel (a) shows the diverse multi-modal dataset used for training, encompassing various domains (natural scenes, text-rich documents, reasoning tasks) and modalities (image, text, video).  Panel (b) details the IXC-2.5-Reward model architecture, highlighting its components such as vision encoders, tokenizers, large language models, and a score head for reward prediction.  Panel (c) demonstrates the role of IXC-2.5-Reward in reinforcement learning, specifically guiding the training of IXC-2.5-Chat by providing a reward signal to improve the policy.", "section": "3. IXC2.5-Reward"}, {"figure_path": "https://arxiv.org/html/2501.12368/x2.png", "caption": "Table 1: Overview of existing preference datasets used in IXC-2.5-Reward. IF denotes to Instruction Following.", "description": "Table 1 provides an overview of existing datasets used to train IXC-2.5-Reward, a multi-modal reward model.  It categorizes the datasets based on their focus (Instruction Following, Safety, Chat, General) and lists specific dataset names for each category.  This table highlights the diversity of data sources utilized in training the reward model, which contributes to its robustness and performance.", "section": "3. IXC2.5-Reward"}, {"figure_path": "https://arxiv.org/html/2501.12368/x3.png", "caption": "Table 2: Overview of the source of newly collected data used in IXC-2.5-Reward.", "description": "Table 2 shows the sources of the newly collected dataset used to train the InternLM-XComposer2.5-Reward model.  The dataset is categorized by data modality (image or text), and then further sub-categorized by the task or domain the data originates from.  Each sub-category lists specific datasets used in the creation of the multi-modal preference data.  This illustrates the diversity of data sources used to ensure the model's robustness across various situations.", "section": "3. IXC2.5-Reward"}, {"figure_path": "https://arxiv.org/html/2501.12368/x4.png", "caption": "Figure 2: Using IXC-2.5-Reward for Data Cleaning. We visualize the outlier and noisy examples detected by IXC-2.5-Reward with low reward scores from existing image and video instruction-tuning datasets, such as ALLaVA [10] and LLaVA-Video-178K [115]. The \u201cExplain\u201d refers to explanations of error causes as identified by human experts, rather than outputs generated by the reward model.", "description": "Figure 2 showcases examples of outlier and noisy data points identified by the InternLM-XComposer2.5-Reward model.  These data points, sourced from the ALLaVA [10] and LLaVA-Video-178K [115] datasets, received low reward scores, indicating a mismatch between the model's response and human expectations.  The figure visually presents these examples, alongside human expert explanations of the errors, highlighting instances of hallucinations, missing answers, and misalignment between visual and textual content. These visualizations help illustrate how the IXC-2.5-Reward model facilitates data cleaning and improves the quality of training datasets for large vision-language models.", "section": "4.3. IXC-2.5-Reward for Data Cleaning"}]