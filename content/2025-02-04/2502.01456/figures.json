[{"figure_path": "https://arxiv.org/html/2502.01456/x1.png", "caption": "Figure 1: Overall math performance. Eurus-2-7B-PRIME excels at competition-level mathematics benchmarks, outperforming advanced math models and larger models. Notably, PRIME brings substantial performance gain (+16.7%) over Eurus-2-7B-SFT.", "description": "The bar chart displays the performance of various language models on several competitive mathematics benchmarks.  Eurus-2-7B-PRIME, a model enhanced with the PRIME method, demonstrates superior performance compared to other state-of-the-art models, including those with significantly larger parameter counts. The chart highlights the substantial improvement achieved by PRIME (+16.7%) over the Eurus-2-7B-SFT baseline, showcasing the effectiveness of the PRIME technique in enhancing mathematical reasoning capabilities.", "section": "1 INTRODUCTION"}, {"figure_path": "https://arxiv.org/html/2502.01456/x2.png", "caption": "Figure 2: Illustration of PRIME. PRIME follows that (1) initialize policy model and the Implicit PRM both with the reference model; (2) sample multiple responses for each prompt and filter with output accuracy; (3) obtain implicit process rewards by the Implicit PRM and update it using cross-entropy (CE) loss; (4) compute advantage and policy loss then update the policy model.", "description": "The figure illustrates the PRIME framework's workflow.  It begins by initializing both the policy model and the Implicit PRM using a reference model. Next, multiple responses are generated for each prompt, and a filter based on output accuracy selects the most promising ones. Then, the Implicit PRM calculates implicit process rewards for each token, which are used to update the PRM itself using cross-entropy loss. Finally, advantages are computed based on these rewards, and these are used in calculating the policy loss, enabling the update of the policy model.", "section": "3 PRIME"}, {"figure_path": "https://arxiv.org/html/2502.01456/x3.png", "caption": "Figure 3: Impact of online prompt filtering on training rewards.", "description": "This figure displays the effect of online prompt filtering on the training rewards during reinforcement learning.  The blue line shows the training rewards with online prompt filtering applied, while the orange line represents the training rewards without filtering. The plot demonstrates a significant reduction in variance of the training rewards when using online prompt filtering, suggesting improved stability in the learning process.", "section": "3.3 Other Techniques"}, {"figure_path": "https://arxiv.org/html/2502.01456/x4.png", "caption": "(a) Outcome training rewards (10-step moving).", "description": "This figure shows the outcome-based rewards obtained during the training process using a 10-step moving average. It illustrates the trend of training rewards over time, providing insights into the learning progress and the effectiveness of the reward system.", "section": "4 Experiments"}, {"figure_path": "https://arxiv.org/html/2502.01456/x5.png", "caption": "(b) Test accuracy across different gradient steps.", "description": "The figure shows the test accuracy of different models across various gradient update steps during training.  It illustrates how the accuracy changes over time as the model is updated and refined using different methods, providing insights into the learning progress and relative performance of the compared models.", "section": "4.4 DENSE REWARDS V.S. SPARSE REWARDS"}, {"figure_path": "https://arxiv.org/html/2502.01456/x6.png", "caption": "Figure 4: \nThe effect of dense reward. We compare PRIME and RLOO with outcome verifier (OV). Dense rewards in PRIME lead to 2.5\u00d72.5\\times2.5 \u00d7 sample efficiency and 6.9%percent6.96.9\\%6.9 % performance improvement. PRIME also substantially outperforms RLOO on downstream tasks.", "description": "Figure 4 presents a comparison of the performance of PRIME (Process Reinforcement through Implicit Rewards) and RLOO (Reward-Level Online Optimization) on various tasks. Both methods employ an outcome verifier (OV) to evaluate performance. The results show that PRIME, which utilizes dense rewards, achieves a 2.5x improvement in sample efficiency and a 6.9% increase in performance compared to RLOO, which uses only outcome-level rewards.  Moreover, PRIME demonstrates superior performance on downstream tasks, illustrating the advantages of dense rewards in reinforcement learning for LLMs.", "section": "4 Experiments"}, {"figure_path": "https://arxiv.org/html/2502.01456/x7.png", "caption": "(a) Outcome training rewards (10-step moving).", "description": "This figure shows the outcome-based rewards obtained during the training process. The rewards are calculated every 10 steps and smoothed using a 10-step moving average to reduce noise and highlight the overall trend. It visually represents the learning progress of the model, indicating how well the model is performing based on its final outputs. Higher reward values suggest improved performance in solving the tasks.", "section": "4 Experiments"}, {"figure_path": "https://arxiv.org/html/2502.01456/x8.png", "caption": "(b) Test accuracy across different gradient steps.", "description": "The graph shows how the test accuracy of the model changes as the number of gradient steps increases.  This illustrates the model's learning progress and performance improvement over time during the training process.  It provides insights into how effectively the model learns and generalizes to unseen data as training progresses.", "section": "4.4 DENSE REWARDS V.S. SPARSE REWARDS"}, {"figure_path": "https://arxiv.org/html/2502.01456/extracted/6173597/figures/images/policy_ref.png", "caption": "Figure 5: Comparison of different PRMs. Online PRM initialized from SFT model achieved the best results. Surprisingly, using PRMs trained on extra rollouts\nhurts the performance in both online and offline settings.", "description": "This figure compares the performance of different process reward models (PRMs) in a reinforcement learning setting.  The key finding is that online PRMs, particularly those initialized using the supervised fine-tuning (SFT) model, achieve superior results compared to offline PRMs or online PRMs trained on additional data.  The results demonstrate that training a PRM using only the on-policy rollouts from the SFT model is sufficient, and adding extra training data negatively impacts the performance in both online and offline scenarios.  This suggests that overfitting is a significant concern when training PRMs outside of the online reinforcement learning context.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2502.01456/extracted/6173597/figures/images/sfr_ref.png", "caption": "Figure 6: Impact of PRM online update. The offline PRM is gradully been overoptimized while online PRMs achieve higher accuracy throughout training.", "description": "This figure displays the impact of online vs. offline updates of the process reward model (PRM) on its accuracy. The x-axis represents the training steps, and the y-axis shows the PRM's accuracy in classifying rewards as correct or incorrect.  The online PRM, updated during training, maintains high accuracy, whereas the offline PRM, trained beforehand, gradually loses accuracy due to overoptimization. This highlights the importance of online PRM updates to prevent performance degradation.", "section": "3.3 OTHER TECHNIQUES"}, {"figure_path": "https://arxiv.org/html/2502.01456/x9.png", "caption": "(a) Policy ref: We use the policy logprob as \u03c0refsubscript\ud835\udf0bref\\pi_{\\text{ref}}italic_\u03c0 start_POSTSUBSCRIPT ref end_POSTSUBSCRIPT for PRM.", "description": "This figure compares two different methods for implementing the implicit process reward model (PRM) in reinforcement learning. The left panel (a) shows the results when using the policy's log probabilities as the reference distribution for the PRM. This method directly uses the model's own estimates of token probabilities to guide the PRM. The right panel (b) shows the alternative method, which employs the initial Supervised Fine-Tuning (SFT) model as the reference. In this case, the PRM is based on a pre-trained model, rather than using the model's dynamically changing probability estimates.", "section": "5.2 REFERENCE MODEL CHOICE IS FLEXIBLE"}, {"figure_path": "https://arxiv.org/html/2502.01456/x10.png", "caption": "(b) SFT ref: We retain the initial policy to provide \u03c0refsubscript\ud835\udf0bref\\pi_{\\text{ref}}italic_\u03c0 start_POSTSUBSCRIPT ref end_POSTSUBSCRIPT for PRM and KL.", "description": "This figure illustrates a design choice in the PRIME framework, specifically how the reference policy (\u03c0ref) is selected for calculating implicit process rewards.  The (b) part of the figure shows a setup where the initial policy model from the supervised fine-tuning (SFT) stage is used as the reference. This means that the probabilities from the initially trained SFT model are used when computing the implicit process reward, providing a consistent baseline against which to measure the current policy. This choice of reference model helps stabilize the training process and mitigates issues related to reward hacking and reward model overoptimization.", "section": "5.2 REFERENCE MODEL CHOICE IS FLEXIBLE"}, {"figure_path": "https://arxiv.org/html/2502.01456/x11.png", "caption": "Figure 7: Comparison of different reference policy implementations. One uses the running policy\u2019s old logprobs as reference (policy ref) while the other uses the initial SFT model as the reference model (SFT ref).", "description": "This figure compares two approaches for using a reference policy in the PRIME reinforcement learning framework.  The \"policy ref\" method uses the log probabilities from the current policy model as the reference.  The \"SFT ref\" method uses the initial Supervised Fine-Tuning (SFT) model's log probabilities as a reference. The comparison highlights how the choice of reference policy impacts the overall performance of the algorithm.  Both methods are visually shown in separate diagrams illustrating their process.", "section": "5.2 REFERENCE MODEL CHOICE IS FLEXIBLE"}, {"figure_path": "https://arxiv.org/html/2502.01456/x12.png", "caption": "Figure 8: Different reference model for PRM. We compare two reference model selection strategies for PRIME. Using the policy model as reference and using the initial SFT model as reference. Their rewards are similar.", "description": "This figure compares two different methods of selecting a reference model for the Implicit Process Reward Model (PRM) within the PRIME framework. The first method uses the current policy model's log probabilities as the reference. The second method uses the initial Supervised Fine-Tuning (SFT) model's log probabilities.  The results show that the training rewards obtained using both methods are quite similar, suggesting that the choice of reference model may not be a critical factor in the PRIME framework's performance.", "section": "5.2 REFERENCE MODEL CHOICE IS FLEXIBLE"}, {"figure_path": "https://arxiv.org/html/2502.01456/x13.png", "caption": "(a) PRM classification accuracy on training samples.", "description": "This figure shows the performance of the Implicit PRM (Process Reward Model) during training. The y-axis represents the classification accuracy of the PRM on the training samples. The x-axis represents the training steps. This plot helps visualize how well the PRM learns to predict process rewards over the course of training. The accuracy is measured on training samples and shows the improvement in the PRM's ability to correctly classify token-level rewards during training.", "section": "5.3 Single-Forward Vs. Double-Forward"}, {"figure_path": "https://arxiv.org/html/2502.01456/x14.png", "caption": "(b) Training outcome rewards.", "description": "The graph displays the training outcome rewards over time steps for two different methods: single forward and double forward.  It shows the cumulative outcome rewards achieved during the training process. The double-forward approach demonstrates slightly higher rewards, suggesting it might lead to better overall model performance.", "section": "5.3 Single-Forward Vs. Double-Forward"}, {"figure_path": "https://arxiv.org/html/2502.01456/x15.png", "caption": "Figure 9: Single and double forward. While double forward methods obtain higher accuracy after online update, the two variants achieve similar rewards during training.", "description": "This figure compares the performance of single-forward and double-forward methods in training an implicit process reward model (PRM). Single-forward updates the PRM once per training iteration, using the rewards from the previous PRM, while double-forward updates the PRM twice, first using the older rewards and then again using the newly calculated rewards. The results show that the double-forward method achieves higher PRM accuracy after the online update, suggesting it may be more effective at preventing overfitting. However, both methods yield similar training rewards, indicating comparable overall performance in terms of maximizing the cumulative reward.", "section": "3.2 Advantage Estimation and Policy Update"}, {"figure_path": "https://arxiv.org/html/2502.01456/x16.png", "caption": "Figure 10: PRIME also benefits REINFORCE, GRPO, and PPO, achieving similar improvement as RLOO.", "description": "Figure 10 presents a comparison of the performance of several reinforcement learning algorithms (REINFORCE, GRPO, PPO, and RLOO) when used with and without the PRIME method.  The figure demonstrates that incorporating PRIME consistently improves the performance of all four algorithms, achieving similar levels of improvement as seen with RLOO alone.  This highlights the general applicability and effectiveness of the PRIME method across different reinforcement learning algorithms.", "section": "5.4 PRIME WITH OTHER RL ALGORITHMS"}, {"figure_path": "https://arxiv.org/html/2502.01456/x17.png", "caption": "Figure 11: Comparison of value models and reward models. We show that value models, either the original PPO one or Implicit PRM, is substaintially worse than reward models.", "description": "This figure compares the performance of using value models versus reward models within a reinforcement learning framework for large language models.  Specifically, it contrasts the performance of three approaches: (1) a standard value model from Proximal Policy Optimization (PPO), (2) a value model based on the implicit process reward model (PRM) proposed in the paper, and (3) a reward model using the implicit PRM. The results demonstrate that using reward models, particularly the implicit PRM reward model, significantly outperforms value models in terms of training effectiveness.", "section": "5.5 Value or Reward, How to Use the Implicit PRM?"}]