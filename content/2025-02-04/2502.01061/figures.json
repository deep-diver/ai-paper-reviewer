[{"figure_path": "https://arxiv.org/html/2502.01061/extracted/6173608/imgs/framework.jpg", "caption": "Figure 1: The video frames generated by OmniHuman based on input audio and image. The generated results feature head and gesture movements, as well as facial expressions, that match the audio. OmniHuman generates highly realistic videos with any aspect ratio and body proportion, and significantly improves gesture generation and object interaction over existing methods, due to the data scaling up enabled by omni-conditions training.", "description": "This figure showcases example video frames generated by the OmniHuman model.  The input to the model is both audio and an image. The model outputs a video featuring realistic human movement that is synchronized to the input audio. This includes head and hand gestures, along with corresponding facial expressions. A key feature highlighted is the model's ability to generate these videos at various aspect ratios and with different body proportions. The authors also emphasize that OmniHuman significantly outperforms previous models in generating accurate gestures and realistic object interactions, largely due to the benefits of the novel 'omni-conditions training' approach.", "section": "Abstract"}, {"figure_path": "https://arxiv.org/html/2502.01061/x1.png", "caption": "Figure 2: The framework of OmniHuman. It consists of two parts: (1) the OmniHuman model, which is based on the DiT architecture and supports simultaneous conditioning with multiple modalities including text, image, audio, and pose; (2) the omni-conditions training strategy, which employs progressive, multi-stage training based on the motion-related extent of the conditions. The mixed condition training allows the OmniHuman model to benefit from the scaling up of mixed data.", "description": "Figure 2 illustrates the OmniHuman framework, detailing its two main components: the OmniHuman model and the omni-conditions training strategy.  The OmniHuman model, built upon the Diffusion Transformer (DiT) architecture, is capable of simultaneous conditioning using text, image, audio, and pose inputs to generate human videos.  The omni-conditions training strategy uses a progressive, multi-stage approach, starting with weaker conditions (like text) and gradually incorporating stronger ones (like pose), to maximize the utilization of diverse training data.  This mixed-condition training allows for scaling up by using data that would be typically excluded due to strict filtering requirements in single-condition training methods, resulting in a more robust and generalizable model.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2502.01061/extracted/6173608/imgs/pose_ratio0.jpeg", "caption": "Figure 3: Quantitative comparisons with audio-conditioned portrait animation baselines.", "description": "This figure presents a quantitative comparison of OmniHuman against several existing audio-conditioned portrait animation baselines.  The comparison uses various metrics to evaluate aspects like image quality (IQA, ASE), audio-visual synchronization (Sync-C), and the realism of the generated videos (FID, FVD).  The results demonstrate OmniHuman's performance relative to state-of-the-art methods on two popular benchmark datasets: CelebV-HQ and RAVDESS.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.01061/extracted/6173608/imgs/pose_ratio1.jpeg", "caption": "Figure 4: Quantitative comparisons with audio-conditioned body animation baselines.", "description": "This figure presents a quantitative comparison of OmniHuman's performance against other state-of-the-art baselines for audio-conditioned body animation.  It likely shows key metrics such as FID (Fr\u00e9chet Inception Distance) and FVD (Fr\u00e9chet Video Distance) scores, which evaluate the quality and realism of the generated videos.  The metrics might also include measures of lip synchronization accuracy and the quality of generated hand movements.  By comparing these metrics across different methods, the table helps demonstrate the superior performance of OmniHuman in producing realistic and high-quality body animations based on audio input.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.01061/x2.png", "caption": "Figure 5: Subjective comparison of different training ratios for audio conditions.", "description": "This figure displays a subjective comparison of using different ratios of audio data during training for an audio-driven human animation model. It visually demonstrates how varying the proportion of audio-specific training data affects the model's performance, particularly in terms of identity consistency, lip-sync accuracy, visual quality, action diversity, and overall quality.  Different percentages of audio-related data are used for training, and the resulting videos from each training condition are subjectively evaluated and compared.", "section": "4.3. Ablation Studies on Omni-Conditions Training"}, {"figure_path": "https://arxiv.org/html/2502.01061/extracted/6173608/imgs/omni2.jpg", "caption": "Figure 6: Ablation study on different audio condition ratios. The models are trained with different audio ratios (top: 10%, middle: 50%, bottom: 100%) and tested in an audio-driven setting with the same input image and audio.", "description": "This ablation study investigates the effect of varying the proportion of audio data used during training on the performance of an audio-driven video generation model. Three models were trained with different audio ratios: 10%, 50%, and 100%.  The results show that using only a small amount of audio data (10%) leads to poor generation quality, while using too much audio data (100%) can also negatively impact performance, possibly due to overfitting. A balanced approach (50%) seems to yield the best results.", "section": "4.3. Ablation Studies on Omni-Conditions Training"}, {"figure_path": "https://arxiv.org/html/2502.01061/extracted/6173608/imgs/omni3.jpg", "caption": "Figure 7: Ablation study on different pose condition ratios. The models are trained with different pose ratios (top: 20%, middle: 50%, bottom: 80%) and tested in an audio-driven setting with the same input image and audio.", "description": "This figure presents an ablation study on the impact of varying pose condition ratios during the training of a human animation model. Three models were trained with different pose condition ratios: 20%, 50%, and 80%.  Each model was then tested on the same audio and image input in an audio-driven setting. The visual results allow for a comparison of the generated human motion videos across different pose condition ratios. This helps to analyze how the proportion of pose conditioning data during training affects the quality and realism of the generated animations, specifically focusing on the accuracy of pose representation in the output videos.", "section": "3.3. Scaling up with Omni-Conditions Training"}, {"figure_path": "https://arxiv.org/html/2502.01061/extracted/6173608/imgs/omni4.jpg", "caption": "Figure 8: Ablation study on different pose condition ratios. The models are trained with different pose ratios (top: 20%, middle: 50%, bottom: 80%) and tested in an audio-driven setting with the same input image and audio.", "description": "This figure shows an ablation study on the impact of different training ratios for pose conditions on the performance of an audio-driven human animation model. Three models were trained with varying pose condition ratios: 20%, 50%, and 80%.  All three models were tested using the same input image and audio.  The results visually demonstrate how the training ratio affects the model's ability to generate realistic and coherent human movements in an audio-driven setting. The differences in motion generation quality, particularly in the synchronization of body movements with audio, can be observed by comparing the results of the three models. ", "section": "3.3. Scaling up with Omni-Conditions Training"}]