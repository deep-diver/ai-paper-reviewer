[{"figure_path": "https://arxiv.org/html/2502.01068/x1.png", "caption": "Figure 1: \nComparison of accuracy, TTFT, throughput across different KV cache compression methods on LLaMA-3.1-8B-Instruct", "description": "This figure compares the performance of various key-value (KV) cache compression methods on the LLaMA-3.1-8B-Instruct model.  It shows a comparison of the accuracy achieved by each method against the time-to-first-token (TTFT) and the overall throughput.  This allows for a visual assessment of the trade-off between speed and accuracy for different compression strategies.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2502.01068/x2.png", "caption": "Figure 2: \nComparison of the number of tokens processed in each layer/head of LLMs during prefill computation and KV caching across different KV cache compression techniques.\nAs each token produces its corresponding KV, the KV cache size is directly proportional to the number of tokens processed.\nThe blue background box indicates the set of sharing selected token indices.", "description": "This figure compares the number of tokens processed per layer and per attention head during the prefill computation stage of large language models (LLMs) across various KV cache compression techniques.  The prefill stage is where the LLM processes the input prompt to prepare for generating text. The amount of key-value (KV) cache memory required is directly proportional to the number of tokens processed. This figure visually demonstrates how different methods like SnapKV, AdaKV, HeadKV, GemFilter, and FastKV (the proposed method) handle token processing and KV cache allocation during the prefill stage. The blue boxes highlight tokens selected for sharing or compression, showing the different strategies used by each method to reduce the size of the KV cache.", "section": "3. Proposed FastKV"}, {"figure_path": "https://arxiv.org/html/2502.01068/x3.png", "caption": "Figure 3: \nRate of sum of attention scores for the top 2k important tokens selected to the total attention score at each layer.", "description": "This figure shows the proportion of total attention scores attributed to the top 2000 most important tokens in each layer of the language model.  It illustrates how the focus of attention shifts as processing progresses through the layers. The early layers have broader attention patterns, while the later layers concentrate attention on a smaller, more consistent set of important tokens. This visualization supports the Token-Selective Propagation (TSP) method used in FastKV, which leverages the increased focus of attention in deeper layers to improve efficiency.", "section": "3.2. Proposed TSP"}, {"figure_path": "https://arxiv.org/html/2502.01068/x4.png", "caption": "Figure 4: \nMinimum match rate between indices of 2k important tokens selected at each layer and its subsequent layers.", "description": "This figure shows the consistency of important tokens across different layers of LLMs.  For each layer, the top 2000 most important tokens (based on attention scores) were identified. Then, the minimum match rate was calculated between each layer's top 2000 tokens and the top 2000 tokens in subsequent layers.  The x-axis represents the normalized layer index, progressing from the initial layers to the final layers of the model. The y-axis shows the minimum match rate (percentage). The graph illustrates how the overlap of important tokens increases as the model progresses through its layers. This visualization demonstrates the rationale behind the Token-Selective Propagation (TSP) method used in the FastKV approach.", "section": "3.4. Dynamics of Token Importance"}, {"figure_path": "https://arxiv.org/html/2502.01068/x5.png", "caption": "Figure 5: \nVisualization of output logits by t-SNE for (left) LLaMA-3.1-8B-Instruct and (right) Mistral-Nemo-12B-Instruct.", "description": "This figure visualizes the output logits of two different large language models (LLMs) using t-distributed Stochastic Neighbor Embedding (t-SNE). t-SNE is a dimensionality reduction technique used to visualize high-dimensional data in a lower-dimensional space (in this case, 2D).  The left panel shows the visualization for LLaMA-3.1-8B-Instruct, while the right panel shows the visualization for Mistral-Nemo-12B-Instruct. By visualizing the logits in this way, the figure helps to understand how similar or different the predictions of the two models are across different tokens in the output sequence.  The clustering of points suggests the relationships between different output tokens in the prediction space of each model.", "section": "3.4. Dynamics of Token Importance"}, {"figure_path": "https://arxiv.org/html/2502.01068/x6.png", "caption": "Figure 6: \nLongBench results of LLaMA-3.1-8B-insturct with propagation of tokens selected at each layer.", "description": "This figure shows the results of the LongBench benchmark for the LLaMA-3.1-8B-Instruct model.  Different propagation strategies for tokens selected at various layers are compared.  It illustrates the impact of different token selection and propagation approaches on the model's performance across various tasks within the LongBench benchmark.", "section": "3.1 Overview of FastKV"}, {"figure_path": "https://arxiv.org/html/2502.01068/x7.png", "caption": "Figure 7: \nLongBench results (line) and TTFT (bar) of LLaMA-3.1-8B-insturct across various KV budgets and TSP lengths.", "description": "This figure shows the results of the LongBench benchmark and time-to-first-token (TTFT) for the LLaMA-3.1-8B-Instruct model under different KV cache budget and TSP length settings.  The line graph displays the average accuracy across various LongBench tasks as the KV budget and TSP length change. The bar graph displays the time it takes to generate the first token of a response, or TTFT, under the same varying settings.  This allows for a comparison of accuracy and speed trade-offs at different compression levels and configurations.", "section": "3.7 GQA Compatible KV Cache Compression"}, {"figure_path": "https://arxiv.org/html/2502.01068/x8.png", "caption": "Figure 8: \nNeedle-in-a-Haystack results of LLaMA-3.1-8B-Instruct with 512 KV budget.", "description": "This figure visualizes the performance of different key-value (KV) cache compression techniques on the Needle-in-a-Haystack benchmark using the LLaMA-3.1-8B-Instruct language model.  The results are shown with a KV budget of 512. Each heatmap represents a different method, showing the percentage of correct retrievals at varying depths within the context window.  The color intensity indicates the accuracy, with darker shades representing higher accuracy. This allows for a visual comparison of the effectiveness of various compression methods in maintaining accuracy across different depths in the context window.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.01068/x9.png", "caption": "Figure 9: \n(a) TTFT and (b) throughput results across different methods on LLaMA-3.1-8B-Instruct. (dashed line: Full KV)", "description": "Figure 9 presents a comparison of the time-to-first-token (TTFT) and throughput performance for various key-value (KV) cache compression methods on the LLaMA-3.1-8B-Instruct language model.  Subfigure (a) shows the TTFT, which measures the latency until the first token is generated, and subfigure (b) displays the throughput, representing the number of tokens generated per second.  The results are shown for different context lengths.  A dashed line is included for the Full KV method, which represents the baseline without any compression. The figure highlights the relative performance of various methods in terms of both speed and efficiency.", "section": "4.3. Latency and Throughput Evaluation"}, {"figure_path": "https://arxiv.org/html/2502.01068/extracted/6173599/images/appendix_NIAH_LLaMA-31.png", "caption": "Figure 10: \nNeedle-in-a-Haystack results of LLaMA-3.1-8B-Instruct.", "description": "This figure visualizes the performance of different KV cache compression methods on the Needle-in-a-Haystack benchmark using the LLaMA-3.1-8B-Instruct model.  It shows heatmaps illustrating the retrieval accuracy at varying depths and context lengths. Each heatmap represents a different method (Full KV, SnapKV, AdaKV, HeadKV, GemFilter, and FastKV), and the color intensity indicates the success rate of retrieving the target document.  The results show how each compression technique impacts retrieval accuracy across different context lengths.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.01068/extracted/6173599/images/appendix_NIAH_LLaMA-32.png", "caption": "Figure 11: \nNeedle-in-a-Haystack results of LLaMA-3.2-3B-Instruct.", "description": "This figure visualizes the performance of various KV cache compression techniques on the Needle-in-a-Haystack benchmark using the LLaMA-3.2-3B-Instruct model.  It displays heatmaps showing the percentage of successful retrievals at different depths and context lengths for each method.  The heatmaps allow for a direct comparison of the effectiveness of each method in retrieving information under varying resource constraints (KV budget).  Warmer colors indicate higher retrieval success rates.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.01068/extracted/6173599/images/appendix_NIAH_Mistral-Nemo.png", "caption": "Figure 12: \nNeedle-in-a-Haystack results of Mistral-Nemo-12B-Instruct.", "description": "This figure visualizes the results of the Needle-in-a-Haystack experiment using the Mistral-Nemo-12B-Instruct model.  The heatmaps illustrate the performance of different KV cache compression techniques (Full KV, SnapKV, AdaKV, HeadKV, GemFilter, and FastKV) across various KV budget sizes (512, 1024, and 2048) and context lengths. Each heatmap shows the percentage of correctly retrieved documents at different depths within the context.  The average score for each method is also provided, indicating the overall retrieval accuracy.", "section": "4. Experiments"}]