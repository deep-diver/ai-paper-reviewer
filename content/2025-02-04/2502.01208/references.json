{"references": [{"fullname_first_author": "Achiam, J.", "paper_title": "Constrained policy optimization", "publication_date": "2017-00-00", "reason": "This paper introduces constrained policy optimization, a key technique used in safe reinforcement learning, which is directly relevant to the problem of safe LLM alignment at inference time."}, {"fullname_first_author": "Ouyang, L.", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-00-00", "reason": "This paper introduces a crucial method for aligning LLMs with human preferences, Reinforcement Learning from Human Feedback (RLHF), which is a widely used approach to mitigate unsafe LLM outputs; this is a primary motivation and baseline for InferenceGuard."}, {"fullname_first_author": "Sootla, A.", "paper_title": "Saut\u00e9 RL: Almost surely safe reinforcement learning using state augmentation", "publication_date": "2022-00-00", "reason": "This paper provides the theoretical foundation for using state augmentation to guarantee almost sure safety in reinforcement learning, which is the core theoretical underpinning for InferenceGuard's safety guarantees."}, {"fullname_first_author": "Hern\u00e1ndez-Lerma, O.", "paper_title": "Discrete-time Markov control processes with discounted unbounded costs: optimality criteria", "publication_date": "1992-00-00", "reason": "This paper provides fundamental theoretical results on the convergence and optimality of value iteration in Markov Decision Processes (MDPs), which is used extensively in the theoretical analysis and justification of InferenceGuard's latent space optimization approach."}, {"fullname_first_author": "Mudgal, S.", "paper_title": "Controlled decoding from language models", "publication_date": "2023-00-00", "reason": "This paper introduces the concept of controlled decoding for LLMs, a crucial technique used in InferenceGuard for efficiently generating safe and aligned responses at inference time."}]}