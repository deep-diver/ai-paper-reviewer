[{"content": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S4.T1.1\">\n<tr class=\"ltx_tr\" id=\"S4.T1.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S4.T1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.1.1.1\">Benchmark</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.1.2.1\">GPT-4o (reported)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.1.3.1\">Qwen2.5-VL-7B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.1.4.1\">OpenVLThinker-7B</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T1.1.2.1\">MathVista</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.1.2.2\">63.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.1.2.3\">68.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.1.2.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.2.4.1\">70.2</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.1.3.1\">MathVerse</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.3.2\">50.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.3.3\">46.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.3.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.3.4.1\">47.9</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.1.4.1\">MathVision (testmini)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.4.2\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.4.3\">27.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.4.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.4.4.1\">29.6</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S4.T1.1.5.1\">MathVision (full)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.1.5.2\">30.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.1.5.3\">24.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.1.5.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.5.4.1\">25.3</span></td>\n</tr>\n</table>", "caption": "Table 1: Evaluation results across multi-modal reasoning benchmarks including MathVista, MathVerse and MathVision. We include the reported performance of GPT-4o as a reference. OpenVLThinker-7B consistently and effectively improves upon the performance of Qwen2.5-VL-7B, surpassing or matching the performance of GPT-4o.", "description": "Table 1 presents a comparison of the accuracy scores achieved by different large language models (LLMs) on three complex visual reasoning benchmarks: MathVista, MathVerse, and MathVision.  The models compared are OpenVLThinker-7B (the model introduced in this paper), Qwen2.5-VL-7B (a baseline model), and GPT-4 (a state-of-the-art model used as a reference point).  The table shows that OpenVLThinker-7B significantly outperforms the Qwen2.5-VL-7B baseline and achieves performance comparable to or exceeding that of GPT-4 on all three benchmarks, demonstrating the effectiveness of the iterative self-improvement approach used to train OpenVLThinker-7B.", "section": "4. OpenVLThinker: Iterative Self-improvement via SFT and RL"}, {"content": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S4.T2.1\">\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S4.T2.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.1\">Method</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" id=\"S4.T2.1.1.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T2.1.1.2.1\">\n<span class=\"ltx_p\" id=\"S4.T2.1.1.2.1.1\" style=\"width:284.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.2.1.1.1\">Data Sources</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T2.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.3.1\">Data Amount</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.1.2.1\">SFT-Iter1</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S4.T2.1.2.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T2.1.2.2.1\">\n<span class=\"ltx_p\" id=\"S4.T2.1.2.2.1.1\" style=\"width:284.5pt;\">FigureQA, GEOS, Geometry3K, TabMWP, VizWiz, and AI2D</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.2.3\">25k</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.3.1\">GRPO-Iter1</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S4.T2.1.3.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T2.1.3.2.1\">\n<span class=\"ltx_p\" id=\"S4.T2.1.3.2.1.1\" style=\"width:284.5pt;\">Geometry3K, GEOS, FigureQA, CLEVR, and SuperCLEVR</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.3.3\">5k</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.1.4.1\">SFT-Iter2</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S4.T2.1.4.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T2.1.4.2.1\">\n<span class=\"ltx_p\" id=\"S4.T2.1.4.2.1.1\" style=\"width:284.5pt;\">FigureQA, GEOS, Geometry3K, TabMWP, VizWiz, AI2D, CLEVR, SuperCLEVR, IconQA, MapQA, and ScienceQA</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.4.3\">5k</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.5.1\">GRPO-Iter2</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S4.T2.1.5.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T2.1.5.2.1\">\n<span class=\"ltx_p\" id=\"S4.T2.1.5.2.1.1\" style=\"width:284.5pt;\">Geometry3K, GEOS, FigureQA, CLEVR, and SuperCLEVR</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.5.3\">5k</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.6\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.1.6.1\">SFT-Iter3</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S4.T2.1.6.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T2.1.6.2.1\">\n<span class=\"ltx_p\" id=\"S4.T2.1.6.2.1.1\" style=\"width:284.5pt;\">FigureQA, GEOS, Geometry3K, TabMWP, VizWiz, AI2D, CLEVR, SuperCLEVR, IconQA, MapQA, and ScienceQA</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.6.3\">5k</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.7\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S4.T2.1.7.1\">GRPO-Iter3</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" id=\"S4.T2.1.7.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T2.1.7.2.1\">\n<span class=\"ltx_p\" id=\"S4.T2.1.7.2.1.1\" style=\"width:284.5pt;\">Geometry3K (larger proportion), GEOS, FigureQA, CLEVR, and SuperCLEVR (larger proportion)</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.1.7.3\">5k</td>\n</tr>\n</table>", "caption": "Table 2: Data sources and amounts for different iterations. We progressively evolve the data so that more challenging data are included in later iterations.", "description": "This table details the data used to train the OpenVLThinker model across three iterative rounds of supervised fine-tuning (SFT) and reinforcement learning (RL).  Each iteration builds upon the previous one, with the model's improved reasoning capabilities from the RL phase used to generate refined training data for the next SFT phase.  The table shows the datasets included in each iteration, illustrating a progressive increase in the difficulty of the reasoning tasks.  The amount of data used in each SFT and RL iteration is also specified.  This iterative process enables the model to learn more complex reasoning skills over time.", "section": "4. OpenVLThinker: Iterative Self-improvement via SFT and RL"}, {"content": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S4.T3.1\">\n<tr class=\"ltx_tr\" id=\"S4.T3.1.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S4.T3.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.2.1.1\">Model Variant</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T3.1.2.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.2.2.1\">Accuracy (%)</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T3.1.3.1\">Qwen2.5-VL-7B (baseline)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.3.2\">68.5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T3.1.4.1\">Vanilla SFT data (unfiltered)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.4.2\">48.4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T3.1.1.1\">Filtered by length (<math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.1.1.1.m1.1\"><semantics id=\"S4.T3.1.1.1.m1.1a\"><mo id=\"S4.T3.1.1.1.m1.1.1\" xref=\"S4.T3.1.1.1.m1.1.1.cmml\">&lt;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T3.1.1.1.m1.1b\"><lt id=\"S4.T3.1.1.1.m1.1.1.cmml\" xref=\"S4.T3.1.1.1.m1.1.1\"></lt></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T3.1.1.1.m1.1c\">&lt;</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.T3.1.1.1.m1.1d\">&lt;</annotation></semantics></math>500 words)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.1.2\">55.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S4.T3.1.5.1\">Removed reflections (<span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.5.1.1\">SFT-Iter1</span>)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.1.5.2\">62.5</td>\n</tr>\n</table>", "caption": "Table 3: Performance on the MathVista benchmark comparing different SFT data-filtering strategies.", "description": "This table presents the performance comparison of different data filtering strategies used during supervised fine-tuning (SFT) on the MathVista benchmark.  It shows how different preprocessing techniques applied to the training data impact the model's accuracy.  Specifically, it contrasts the results obtained using unfiltered data, data filtered by length (under 500 words), and data with reflections removed. This allows for an analysis of how data cleaning affects the model's ability to learn effective reasoning strategies.", "section": "4.2. SFT to Follow Reasoning Structure"}, {"content": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S4.T4.1\">\n<tr class=\"ltx_tr\" id=\"S4.T4.1.1\">\n<td class=\"ltx_td ltx_border_tt\" id=\"S4.T4.1.1.1\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T4.1.1.2\">Qwen2.5-VL-7B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T4.1.1.3\">R1-Onevision-7B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T4.1.1.4\">SFT-Iter1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"S4.T4.1.2.1\">MathVista</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T4.1.2.2\">68.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T4.1.2.3\">61.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T4.1.2.4\">62.5</td>\n</tr>\n</table>", "caption": "Table 4: Accuracy (%) on MathVista. While our evaluation recovered the reported performance of Qwen2.5-VL-7B, the performance of SFT-ed model R1-Onevision with 119k SFT data exhibited similar performance decline as our SFT-Iter1 under the same evaluation.", "description": "Table 4 presents a comparison of the accuracy achieved on the MathVista benchmark by different models.  The baseline is Qwen2.5-VL-7B, whose reported performance was replicated in this study.  The table also includes the performance of R1-Onevision, another model which was fine-tuned using supervised fine-tuning (SFT) with a substantially larger dataset (119k examples) compared to the current study.  The key finding is that both the current study's SFT-Iter1 model and R1-Onevision exhibit a performance decrease compared to the baseline, suggesting that simply applying SFT with a large dataset may not guarantee improved performance on this benchmark.", "section": "4. OpenVLThinker: Iterative Self-improvement via SFT and RL"}, {"content": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"A1.T5.1\">\n<tr class=\"ltx_tr\" id=\"A1.T5.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"A1.T5.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T5.1.1.1.1\">Caption Type</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A1.T5.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T5.1.1.2.1\">pass@1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A1.T5.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T5.1.1.3.1\">pass@2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A1.T5.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T5.1.1.4.1\">pass@4</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.1.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T5.1.2.1\">Original</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.1.2.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T5.1.2.2.1\">33</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.1.2.3\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T5.1.2.3.1\">37</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.1.2.4\">44</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.1.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A1.T5.1.3.1\">Refined</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A1.T5.1.3.2\">29</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A1.T5.1.3.3\">35</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A1.T5.1.3.4\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T5.1.3.4.1\">46</span></td>\n</tr>\n</table>", "caption": "Table 5: VQA accuracy after a single round of caption refinement. While pass@4 increases slightly, pass@1 and pass@2 remain largely unchanged.", "description": "This table presents the results of a single-round caption refinement experiment on Visual Question Answering (VQA) tasks.  It shows the impact of refining captions on the model's ability to correctly answer questions, measuring accuracy at different thresholds (pass@1, pass@2, pass@4). The results indicate only a minor improvement in accuracy at the highest threshold (pass@4), while accuracy remains largely unchanged at lower thresholds. This suggests that a single round of caption refinement is insufficient to produce significant gains in VQA performance.", "section": "A. Additional Empirical Study"}, {"content": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"A2.T6.1\">\n<tr class=\"ltx_tr\" id=\"A2.T6.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"A2.T6.1.1.1\">max_new_tokens</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T6.1.1.2\">2048</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T6.1.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A2.T6.1.2.1\">top_p</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T6.1.2.2\">0.001</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T6.1.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A2.T6.1.3.1\">top_k</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T6.1.3.2\">1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T6.1.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A2.T6.1.4.1\">temperature</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T6.1.4.2\">0.01</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T6.1.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"A2.T6.1.5.1\">repetition_penalty</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T6.1.5.2\">1.0</td>\n</tr>\n</table>", "caption": "Table 6: Inference hyperparameters.", "description": "This table lists the hyperparameters used during the inference phase of the OpenVLThinker model.  It includes values for parameters such as `max_new_tokens`, controlling the maximum number of tokens generated; `top_p` and `top_k`, which govern token selection probabilities;  `temperature`, influencing randomness in generation; and `repetition_penalty`, preventing repetitive outputs.", "section": "4. OpenVLThinker: Iterative Self-improvement via SFT and RL"}, {"content": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"A2.T7.1\">\n<tr class=\"ltx_tr\" id=\"A2.T7.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"A2.T7.1.1.1\">Data type</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T7.1.1.2\">bf16</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T7.1.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A2.T7.1.2.1\">Learning rate</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T7.1.2.2\">5e-7</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T7.1.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A2.T7.1.3.1\">Global batch size</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T7.1.3.2\">32</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T7.1.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A2.T7.1.4.1\">Scheduler</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T7.1.4.2\">Cosine</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T7.1.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A2.T7.1.5.1\">Warmup ratio</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T7.1.5.2\">0.1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T7.1.6\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A2.T7.1.6.1\">Num train epochs</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T7.1.6.2\">1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T7.1.7\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"A2.T7.1.7.1\">Image max pixels</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T7.1.7.2\">262144</td>\n</tr>\n</table>", "caption": "Table 7: Supervised fine-tuning hyperparameters.", "description": "Table 7 details the hyperparameters used during the supervised fine-tuning phase of the OpenVLThinker model training.  It lists the data type used (bf16), the learning rate, the global and rollout batch sizes, the scheduler type (Cosine), the warmup ratio, the number of training epochs, and the maximum number of image pixels.", "section": "4. OpenVLThinker: Iterative Self-improvement via SFT and RL"}, {"content": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"A2.T8.1\">\n<tr class=\"ltx_tr\" id=\"A2.T8.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"A2.T8.1.1.1\">Rollout batch size</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T8.1.1.2\">512</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T8.1.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A2.T8.1.2.1\">Global batch size</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T8.1.2.2\">128</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T8.1.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A2.T8.1.3.1\">Max grad norm</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T8.1.3.2\">1.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T8.1.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A2.T8.1.4.1\">Data type</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T8.1.4.2\">bf16</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T8.1.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A2.T8.1.5.1\">Learning rate</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T8.1.5.2\">1e-6</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T8.1.6\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A2.T8.1.6.1\">Weight decay</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T8.1.6.2\">1e-2</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T8.1.7\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A2.T8.1.7.1\">Warmup ratio</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T8.1.7.2\">0.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T8.1.8\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"A2.T8.1.8.1\">Rollout temperature</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T8.1.8.2\">1.0</td>\n</tr>\n</table>", "caption": "Table 8: GRPO hyperparameters.", "description": "Table 8 presents the hyperparameters used in the Group Relative Policy Optimization (GRPO) algorithm, a reinforcement learning method employed in the study.  These hyperparameters control various aspects of the GRPO training process, including the batch sizes for both rollout and gradient calculations, the maximum gradient norm to prevent exploding gradients, the learning rate and weight decay for optimization, and the rollout temperature that affects the randomness of action selection during the rollout phase.", "section": "4. OpenVLThinker: Iterative Self-improvement via SFT and RL"}]