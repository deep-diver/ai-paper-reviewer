[{"figure_path": "https://arxiv.org/html/2503.16921/x1.png", "caption": "Figure 1: Two examples from Pick-a-Pic\u00a0[14]. For both rows, the winning images are the right ones.", "description": "This figure shows two examples of image pairs from the Pick-a-Pic dataset [14]. Each row presents a pair of images generated in response to the same text prompt.  Human annotators have selected the image on the right as the preferred image in both examples. These examples highlight the subjective nature of human preferences in image generation and how those preferences can vary. The images illustrate different artistic styles and levels of image quality, highlighting the diversity of preferences present even within a seemingly simple task like image generation.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.16921/extracted/6298712/figures/new_metric.png", "caption": "Figure 2: Proportion of majority/minority at different flip ratios.", "description": "This figure shows the impact of randomly flipping preference labels (creating synthetic minority samples) on the proportion of majority and minority samples in a dataset.  The x-axis represents the percentage of labels flipped (the flip ratio), and the y-axis shows the resulting proportion of minority samples. As the flip ratio increases, the proportion of minority samples increases accordingly, demonstrating a clear relationship between the amount of label flipping and the imbalance introduced into the dataset.", "section": "3. Pilot Study: Influence of Minority Samples"}, {"figure_path": "https://arxiv.org/html/2503.16921/x2.png", "caption": "Table 1: DPO results with different noise level. Larger metric, better performance.", "description": "This table presents the results of Diffusion-DPO experiments conducted with varying levels of synthetic noise added to the training data.  The noise was introduced by randomly flipping the preference labels (winning/losing images) in a percentage of the dataset (0%, 10%, 20%, 30%).  The table shows how different metrics, including ImageReward (IR), PickScore (PS), Aesthetic Score (Aes), and HPS, are affected by the introduction of this synthetic noise. Higher scores indicate better model performance. The experiment was done on two different diffusion models: SD1.5 and SDXL.", "section": "3. Pilot Study: Influence of Minority Samples"}, {"figure_path": "https://arxiv.org/html/2503.16921/x3.png", "caption": "Figure 3: Here, we add 20% label flip to Pick-a-Pic v2\u00a0[14] and calculate the metric according to Eq.\u00a013. The x axis denotes the interval of the metric and the y axis denote the ratio of noisy samples. We can observes a significant increase of flipped sample ratio as the increase of the metric value.", "description": "This figure demonstrates the effectiveness of the proposed minority-instance-aware metric (Eq. 13) in identifying minority samples. By randomly flipping 20% of the labels in the Pick-a-Pic v2 dataset and calculating the metric for each sample, the figure shows a strong positive correlation between the metric value and the proportion of flipped (noisy) samples.  Higher metric values indicate a greater likelihood of a sample being a minority instance, confirming the metric's ability to distinguish between majority and minority preferences.", "section": "4.1. Measuring Minority Preferences"}, {"figure_path": "https://arxiv.org/html/2503.16921/x4.png", "caption": "Table 2: Diffusion-DPO results with different label-flip rate on SD1.5. For all metrics, the larger value indicates the model is better. We copy results of DPO from Tab.\u00a01 for better comparison and understanding.", "description": "This table presents a comparison of the performance of Diffusion-DPO and the proposed Adaptive-DPO method on the SD1.5 model under varying levels of artificially introduced noise (label flipping).  The noise is simulated by randomly changing a percentage of the original preference labels in the training data.  The table shows the ImageReward (IR), PickScore (PS), Aesthetic Score (Aes), and HPS metrics for both methods at label flip rates of 10%, 20%, and 30%. Higher scores indicate better performance.  The DPO results from Table 1 are included for easier comparison, providing a baseline against which the effectiveness of Adaptive-DPO in mitigating the negative effects of noisy labels can be assessed.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.16921/x5.png", "caption": "Table 3: Quantitative results on SD1.5 and SDXL. The larger metric indicates the model is better.", "description": "This table presents a quantitative comparison of different models (Diffusion-DPO, Robust-DPO, SFT (Supervised Fine-Tuning), and the proposed Adaptive-DPO) trained on two different diffusion model architectures, SD1.5 and SDXL.  The evaluation metrics used are ImageReward (IR), PickScore (PS), Aesthetic Score (Aes), and HPS (Human Preference Score). Higher scores indicate better model performance, reflecting the model's ability to align with human preferences in image generation.  The results show the performance of each model on three datasets: Pick-a-Pic validation set, Pick-a-Pic test set, and HPDv2.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.16921/x6.png", "caption": "Figure 4: Qualitative results with SD1.5 and SDXL as backbone. Please refer to supplementary for the corresponding prompts.", "description": "This figure showcases the qualitative results of image generation using two different diffusion models: Stable Diffusion 1.5 (SD1.5) and Stable Diffusion XL (SDXL).  The images were generated using three different methods:  the original Diffusion-DPO method, a Robust-DPO approach, and the novel Adaptive-DPO method proposed in the paper. The figure visually compares the image quality and detail produced by each method, demonstrating the effectiveness of the proposed Adaptive-DPO approach in generating higher quality images.  Specific prompts used to generate these images can be found in the supplementary materials of the paper.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.16921/x7.png", "caption": "Table 4: Comparison of Adaptive-DPO and voting strategy. For all metrics, the larger value indicates the model is better.", "description": "This table compares the performance of Adaptive-DPO and a majority voting strategy on three different metrics (IR, PS, Aes, and HPS) for evaluating the quality of images generated by diffusion models.  The majority voting approach involves re-annotating the dataset using multiple evaluation metrics, which are then used to determine a consensus preference. This table shows that Adaptive-DPO significantly outperforms the simple majority voting strategy across all metrics.  Larger values indicate better performance.", "section": "5. Experiments"}]