[{"figure_path": "https://arxiv.org/html/2503.17032/x2.png", "caption": "Figure 1: TaoAvatar generates photorealistic, topology-consistent 3D full-body avatars from multi-view sequences. It provides high-quality, real-time rendering with low storage requirements, compatible across various mobile and AR devices like the Apple Vision Pro.", "description": "This figure demonstrates the TaoAvatar system's pipeline.  (a) shows the input: multiple views of a person captured from different angles. (b) shows the output: a photorealistic, full-body, 3D avatar that can be manipulated in real-time, featuring realistic facial expressions and body movements.  This avatar is topologically consistent, meaning its 3D structure is accurate and reliable. Finally, (c) shows that this avatar is runnable on various AR devices such as Apple Vision Pro, making it highly portable and accessible. The system achieves high-quality rendering at real-time speeds with low storage needs.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.17032/x3.png", "caption": "Figure 2: Illustration of our Method.\nOur pipeline begins by reconstructing (a) a cloth-extended SMPLX mesh with aligned Gaussian textures. To address complex dynamic non-rigid deformations, (b) we employ a teacher StyleUnet to learn pose-dependent non-rigid maps, which are then baked into a lightweight student MLP to infer non-rigid deformations of our template mesh. For high-fidelity rendering, (c) we introduce learnable Gaussian blend shapes to enhance appearance details.", "description": "This figure illustrates the three main stages of the TaoAvatar method.  (a) shows the creation of a personalized clothed human model (SMPLX++) with Gaussian textures bound to its surface. (b) details the training of a StyleUnet (teacher network) to learn complex, pose-dependent non-rigid deformations. These deformations are then distilled into a simpler MLP (student network) for efficient real-time processing. (c) shows how learnable Gaussian blend shapes are added to further refine the avatar's appearance, resulting in high-fidelity rendering.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.17032/x4.png", "caption": "Figure 3: Qualitative comparisons on full-body talking tasks. Our method outperforms state-of-the-art methods by producing clearer clothing dynamics and enhanced facial details.", "description": "This figure presents a qualitative comparison of full-body talking avatar generation results across different methods, including the authors' proposed method (Ours) and several state-of-the-art baselines. Each row represents a different pose and expression from a single video sequence, showcasing the performance of each approach. The comparison highlights the superiority of the authors' method in capturing the fine details of the clothing, including natural folds and wrinkles, and in rendering more realistic and expressive facial features. This visual comparison underscores the effectiveness of the proposed approach in achieving high-fidelity, real-time avatar generation.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.17032/x5.png", "caption": "Figure 4: Results in challenging scenarios. Our method can obtain high-quality reconstruction even for challenging pose and expressions.", "description": "Figure 4 showcases the robustness of the proposed method in handling challenging scenarios.  It presents several examples of avatar reconstructions under extreme poses and exaggerated facial expressions, highlighting the method's ability to maintain high-quality rendering even in these difficult conditions. The images visually demonstrate the superior performance of the TaoAvatar model compared to existing techniques in accurately representing detailed facial features and clothing dynamics, irrespective of complex body movements.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.17032/x6.png", "caption": "Figure 5: Novel pose and expression animation. TaoAvatar can be driven by the same skeleton and expression parameters vividly.", "description": "This figure showcases TaoAvatar's capability to generate realistic animations using only skeleton and expression parameters.  Different individuals are shown performing the same actions, highlighting the model's ability to capture and reproduce nuanced movements and expressions consistently across various subjects.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.17032/x8.png", "caption": "Figure 6: Ablation Study. Red and Green boxes show artifacts and their improved counterparts.", "description": "This ablation study visualizes the impact of different components of the proposed method on the final avatar quality.  By comparing the results with and without certain modules (like mesh non-rigid deformation or Gaussian non-rigid deformation), the figure highlights their individual contributions to generating high-fidelity avatars. Red and green boxes specifically point out artifacts present in the results and how these artifacts are addressed in the improved versions, providing a clear visual demonstration of the effectiveness of each component.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.17032/x9.png", "caption": "Figure 7: The Pipeline of the Template SMPLX++ Reconstruction.", "description": "This figure illustrates the process of reconstructing a personalized clothed human model, called SMPLX++, which serves as the foundation for creating the 3D avatar.  The pipeline begins with multi-view images of a person. These images are used to reconstruct a 3D mesh using NeuS2, a neural radiance field method.  Next, the non-body components (like clothing and hair) are segmented and separated from the body mesh. The SMPLX model (a parametric human body model) parameters are estimated for a reference frame (usually a T-pose) and the skinning weights are propagated from the body mesh to the non-body components. Inverse skinning is then applied to transform the non-body components back to the reference pose. Finally, the body mesh and non-body components are combined to create the complete clothed parametric model SMPLX++.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.17032/x10.png", "caption": "Figure 8: Template Comparison.", "description": "Figure 8 displays a comparison of different template meshes used to represent human avatars.  It showcases the original SMPL-X model, the MeshAvatar template, the AnimatableGS template, and the novel SMPLX++ template developed by the authors. The visual differences highlight the improvements in detail and clothing representation achieved by the SMPLX++ template, which forms the foundation of the authors' proposed method for creating high-fidelity, real-time avatars.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.17032/x11.png", "caption": "Figure 9: Relighting Visualisation.", "description": "This figure shows the relighting visualization process.  The input is a raw rendered image, and the method uses the rendered normal map and an environment map to perform image-based relighting. The resulting image is more realistic and integrates better with the surrounding environment.", "section": "3.1 Hybrid Clothed Parametric Representations"}, {"figure_path": "https://arxiv.org/html/2503.17032/x12.png", "caption": "Figure 10: Network Architecture of Mesh Nonrigid Deformation Field.", "description": "This figure details the architecture of the mesh non-rigid deformation field, a key component in TaoAvatar's system for creating lifelike full-body avatars.  It showcases the network's input (pose and vertex coordinates), the processing steps involving two separate MLPs (Multilayer Perceptrons) to handle body and clothing deformations separately and the final output (the non-rigid deformation). The inclusion of a 'cloth mask' highlights the method's ability to specifically apply clothing deformation where needed.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.17032/x13.png", "caption": "Figure 11: The Impact of Normal Loss.", "description": "This figure shows an ablation study on the effect of adding a normal loss during the training process.  The normal loss helps to improve the accuracy of the rendered normal maps by promoting smoother and more realistic-looking normal vectors. The comparison shows a significant improvement in the quality of the normals when the normal loss is included, resulting in better rendering quality and realism.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.17032/x14.png", "caption": "Figure 12: Qualitative Visualization of Baking.", "description": "This figure shows a qualitative comparison of the results obtained from the teacher and student networks during the \"baking\" process.  The images compare the ground truth, the teacher network output, and the student network output.  It visually demonstrates how the student network, a lightweight model, effectively learns and reproduces the complex non-rigid deformations initially learned by the more computationally expensive teacher network. The comparison shows the accuracy of the student network in approximating the deformations from the teacher network across different poses and includes a detailed look at the generated semantic maps (Sem. Et, Sem. Es) and Gaussian non-rigid deformation maps (Gau. Non. \u2206Uf, Mesh Non. \u2206V).", "section": "3.2. Dynamic Mesh-based Gaussian Reconstruction"}, {"figure_path": "https://arxiv.org/html/2503.17032/x15.png", "caption": "Figure 13: 3D Digital Human Agent Pipeline.", "description": "This figure illustrates the complete pipeline for creating a 3D digital human agent that can interact in real time on Apple Vision Pro.  It shows how user audio input is processed through Automatic Speech Recognition (ASR), converted into text, and then interpreted by a large language model (LLM). The LLM's response generates text, which is then converted to speech via Text-to-Speech (TTS) and finally used to drive the facial expressions and body movements of the 3D avatar using a Body Motion Library and Audio2BS.", "section": "5. Application"}, {"figure_path": "https://arxiv.org/html/2503.17032/x16.png", "caption": "Figure 14: Ablation Study on Semantic Loss during Baking.", "description": "This ablation study visualizes the effect of including the semantic loss (Lsem) during the baking process of the neural network.  It shows a comparison of the rendered images with and without the semantic loss. The images demonstrate the improvements in detail and accuracy achieved by incorporating Lsem, particularly in areas where clothing and body intersect. The results highlight the importance of Lsem for resolving inconsistencies in the Gaussian splatting representation.", "section": "4 Experiments"}, {"figure_path": "https://arxiv.org/html/2503.17032/x17.png", "caption": "Figure 15: Qualitative Comparison of Details.", "description": "This figure provides a detailed qualitative comparison of the results produced by different methods for generating talking avatars. It showcases the level of detail achieved in face regions by comparing the results of the teacher model, the student model, and other state-of-the-art methods such as GaussianAvatar, MeshAvatar, 3DGS-Avatar, and AnimatableGS against the ground truth. This visual comparison highlights the superior detail achieved by the proposed model, especially in terms of facial expressions and textural details.", "section": "4. Experiments"}]