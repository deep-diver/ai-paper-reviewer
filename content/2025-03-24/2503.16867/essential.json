{"importance": "This paper introduces a novel evaluation metric and benchmark that addresses the limitations of existing methods for assessing text-to-video alignment. By providing a more fine-grained and human-aligned evaluation approach, this research can help researchers to **better understand and improve T2V generation models**. The proposed method opens avenues for developing models that **more accurately capture the nuances of text prompts** and generate videos that align with human expectations, ultimately advancing the field and **potentially leading to more sophisticated T2V applications**.", "summary": "ETVA evaluates text-to-video alignment via fine-grained question generation and answering.", "takeaways": ["ETVA, a new method, achieves higher correlation with human judgment than existing metrics.", "The benchmark features 2k diverse prompts and 12k atomic questions for T2V alignment evaluation.", "The research identifies capabilities and limitations of 15 existing T2V models, paving the way for better T2V generation."], "tldr": "Precisely evaluating semantic alignment between text prompts and generated videos remains a challenge in Text-to-Video (T2V) Generation. Existing text-to-video alignment metrics generate coarse-grained scores without fine-grained details, failing to align with human preference. To address this limitation, this paper introduces a novel Evaluation method of Text-to-Video Alignment via fine-grained question generation and answering. First, a multi-agent system parses prompts into semantic scene graphs to generate atomic questions. Then we design a knowledge-augmented multi-stage reasoning framework for question answering. \n\nThis paper proposes **ETVA**, a novel Evaluation method of Text-to-Video Alignment that contains a multi-agent framework for atomic question generation and a knowledge-augmented multi-stage reasoning framework to emulate human-like reasoning in question answering. The QG part of ETVA consists of three collaborative agents.  Based on ETVA, they further construct **ETVA-Bench**, a comprehensive benchmark for text-to-video alignment evaluation. Our findings reveal that these models still struggle in some areas such as camera movements or physics process.", "affiliation": "Renmin University of China", "categories": {"main_category": "Multimodal Learning", "sub_category": "Vision-Language Models"}, "podcast_path": "2503.16867/podcast.wav"}