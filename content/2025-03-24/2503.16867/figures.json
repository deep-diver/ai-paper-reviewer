[{"figure_path": "https://arxiv.org/html/2503.16867/x1.png", "caption": "Figure 1: Illustration of how ETVA works and comparison with existing metrics.", "description": "This figure demonstrates the workflow of ETVA (Evaluation of Text-to-Video Alignment) and compares its performance to existing text-to-video alignment metrics. The top half shows a text prompt describing a scene, along with two example videos generated by a text-to-video model. The bottom half displays how existing metrics (BLIP-BLEU, CLIPScore, VideoScore) evaluate these videos, showing that they don't align well with human preferences. In contrast, ETVA's approach is presented:  It generates fine-grained questions about specific aspects of the video (e.g., \"Is there a cup?\", \"Is the water pouring out?\") and uses a multi-stage reasoning mechanism, including common-sense knowledge, to determine whether these questions are correctly answered by the video, ultimately leading to an alignment score. ETVA shows significantly better alignment with human judgment compared to existing methods.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.16867/x2.png", "caption": "Figure 2: Overall pipeline of ETVA. ETVA contains a multi-agent framework for generating atomic questions and a knowledge-augmented multi-stage reasoning framework for question answering.", "description": "The figure illustrates the overall architecture of the ETVA (Evaluation of Text-to-Video Alignment) model.  ETVA uses a two-stage process. The first stage involves a multi-agent system for question generation which takes a text prompt as input. This system parses the prompt into a scene graph, representing entities, attributes, and relationships. Then, a graph traverser systematically traverses this scene graph to generate atomic (fine-grained) yes/no questions. The second stage consists of a knowledge-augmented multi-stage reasoning framework for question answering. Here, an auxiliary LLM provides common sense knowledge that is combined with the video content and the generated questions. The video LLM then answers the questions through a step-by-step reasoning process that involves video understanding, general reflection, and a final conclusion. The final ETVA score is computed based on the combined answers to all the questions.", "section": "3. Our Proposed ETVA Approach"}, {"figure_path": "https://arxiv.org/html/2503.16867/x3.png", "caption": "Figure 3: Evaluating results of 10 opensource T2V models in ETVABench-2k.", "description": "This radar chart visualizes the performance of ten open-source text-to-video (T2V) models across ten different categories within the ETVABench-2k benchmark. Each category represents a specific aspect of video generation, such as the presence of objects, actions, materials, spatial relationships, numbers, shapes, colors, camera angles, physics, and other miscellaneous elements. The chart allows for a direct comparison of the models' capabilities in each category, revealing their strengths and weaknesses in various video generation aspects.  A higher score indicates better performance for a dimension.", "section": "5.2. Evaluation of ETVA and Existing Metrics"}, {"figure_path": "https://arxiv.org/html/2503.16867/x4.png", "caption": "Figure 4: Four Cases of using ETVA to evaluate text-to-video alignment in T2V models, covering physics phenomenon, color transition, number accuracy and gestrual semnatic.", "description": "This figure showcases four examples that illustrate how ETVA evaluates text-to-video alignment. Each example highlights a different aspect of the alignment challenge: (1) Physics phenomenon: accurately modeling the behavior of liquids in microgravity; (2) Color transition: smoothly changing the color of an object across a video; (3) Number accuracy: correctly representing the count of objects in the video; and (4) Gestural semantic: correctly interpreting and rendering human actions and gestures.  The figure directly compares the results from different text-to-video (T2V) models using ETVA, showing how the models perform in each of these four alignment categories.", "section": "5.4 Case Study"}, {"figure_path": "https://arxiv.org/html/2503.16867/x5.png", "caption": "Figure 5: Prompt Category Distribution of ETVABench-2k and ETVABench-105", "description": "This figure shows a comparison of the prompt category distributions for two benchmark datasets: ETVABench-2k and ETVABench-105.  Both datasets are used to evaluate text-to-video alignment models. The figure visually represents the number of prompts belonging to each of the ten defined categories in both datasets. This allows for a comparison of the relative frequency of each prompt type across the two benchmark versions.", "section": "4. Our Proposed ETVABench"}, {"figure_path": "https://arxiv.org/html/2503.16867/x6.png", "caption": "Figure 6: Instruction for Human Annotation", "description": "This figure provides detailed instructions for human annotators evaluating the alignment between text prompts and generated videos.  It outlines the scoring process for assessing semantic consistency, emphasizing that the evaluation should focus solely on the semantic match between text and video, ignoring factors like video quality, resolution, or visual clarity.  It also explains the process of evaluating specific binary yes/no questions derived from the text descriptions to assess whether the generated videos correctly answer them.", "section": "5.2.2 Human Annotation"}, {"figure_path": "https://arxiv.org/html/2503.16867/x7.png", "caption": "Figure 7: Prompt for Element Extractor.", "description": "This figure shows the prompt used for the Element Extractor, one of the three agents in the multi-agent question generation framework of ETVA.  The prompt instructs the Element Extractor to meticulously analyze the input prompts and extract background information, camera information, entities, attributes, and relationships.  It provides a detailed explanation of how each of these data elements should be formatted before being processed by the following agents.", "section": "3.2 Multi-Agent Question Generation"}, {"figure_path": "https://arxiv.org/html/2503.16867/x8.png", "caption": "Figure 8: Prompt for Graph Builder.", "description": "This figure shows the prompt used to instruct the Graph Builder agent within the ETVA framework.  The Graph Builder agent takes the extracted entities, attributes, and relations from the text prompt as input and constructs a hierarchical scene graph. This graph represents the relationships between elements in the prompt, creating a structured knowledge representation for question generation. The prompt emphasizes the need for a coherent, hierarchical graph that accurately reflects the semantics of the input, ensuring that all mappings between elements are accurate and contextually relevant.", "section": "3.2 Multi-Agent Question Generation"}, {"figure_path": "https://arxiv.org/html/2503.16867/x9.png", "caption": "Figure 9: Prompt for Graph Traverser.", "description": "This figure details the prompt instructions given to the Graph Traverser agent within the ETVA framework.  The Graph Traverser is responsible for generating atomic yes/no questions from the structured scene graph created by previous agents in the system. The prompt provides examples of different question types (content, attribute, relation) and illustrates how the input, question type, and specific content should be formatted. This ensures consistency and facilitates the generation of comprehensive and relevant video evaluation questions.", "section": "3.2 Multi-Agent Question Generation"}, {"figure_path": "https://arxiv.org/html/2503.16867/x10.png", "caption": "Figure 10: Prompt for Knowledge Augmentation.", "description": "This figure displays the prompt used for the Knowledge Augmentation module within the ETVA framework.  The prompt instructs an LLM to identify implicit knowledge\u2014common sense or relevant physical principles\u2014not explicitly stated in a video's text prompt but necessary for generating a realistic video.  The prompt provides three examples to illustrate how to extract and express this implicit knowledge, focusing on providing detailed descriptions to enrich video understanding.", "section": "3. Our Proposed ETVA Approach"}, {"figure_path": "https://arxiv.org/html/2503.16867/x11.png", "caption": "Figure 11: Prompt for Multi-stage Reasoning.", "description": "This figure shows the prompt used for the Multi-stage Reasoning stage of the ETVA framework.  The prompt instructs a multimodal assistant to answer a video-related question by engaging in a three-stage process: Video Understanding, Critical Reflection, and Conclusion. The assistant has access to the textual prompt used to generate the video, additional common-sense knowledge, and the video itself. The prompt guides the assistant to analyze how well the video matches both the text and the common sense, identify discrepancies or gaps, and then provide a yes/no answer with a brief justification.", "section": "3. Our Proposed ETVA Approach"}, {"figure_path": "https://arxiv.org/html/2503.16867/x12.png", "caption": "Figure 12:  Illustration of a comparative analysis between ETVA and conventional evaluation metrics, based on the text prompt: \u201cWater is slowly pouring out of glass cup in the space station\u201d. We compare our ETVA score with conventional text-to-video alignment metrics.", "description": "This figure compares the performance of ETVA (a novel text-to-video alignment evaluation metric) against three existing metrics: BLIP-BLEU, CLIPScore, and VideoScore.  The comparison is based on a specific text prompt: \"Water is slowly pouring out of glass cup in the space station.\" For each metric, the scores for two generated videos are shown. The figure visually demonstrates how ETVA's fine-grained assessment differs from the coarser-grained scores of the conventional methods, ultimately better reflecting human preferences regarding video-text alignment.", "section": "5.2 Evaluation of ETVA and Existing Metrics"}, {"figure_path": "https://arxiv.org/html/2503.16867/x13.png", "caption": "Figure 13: Illustration of a comparative analysis between ETVA and conventional evaluation metrics, based on the text prompt: \u201dA leaf turns from green to red\u201d. We compare our ETVA score with conventional text-to-video alignment metrics.", "description": "Figure 13 presents a comparative analysis of ETVA and conventional evaluation metrics for a specific text prompt: \"A leaf turns from green to red.\"  It shows the generated videos from various models, alongside their scores from metrics like BLIP-BLEU, CLIPScore, VideoScore, and ETVA. This visual comparison allows for a direct assessment of how well each method aligns with human perception of text-to-video alignment quality. The figure highlights ETVA's ability to more accurately capture the nuanced aspects of semantic correspondence than traditional metrics, which often provide only coarse-grained evaluations.", "section": "5. Evaluation of ETVA and Existing Metrics"}]