[{"figure_path": "https://arxiv.org/html/2503.01935/x1.png", "caption": "Figure 1: Overview of MultiAgentBench evaluation process: Multi-Agent System Coordination in various interactive environments, with a focus on task performance, and coordination.", "description": "This figure illustrates the MultiAgentBench evaluation process, focusing on multi-agent system coordination within diverse interactive scenarios. It highlights three key aspects of the evaluation: the different environments used (with examples shown), the key performance indicators (KPIs) used to measure task completion, and the various coordination protocols being evaluated.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2503.01935/x2.png", "caption": "Figure 2: MARBLE : showcasing interactions between task information, persona data, domain databases, memory modules, and the environment through the coordinate engine and cognitive module.", "description": "The MARBLE framework is a multi-agent coordination system that uses interconnected modules to enable effective collaboration and efficient task execution.  This figure illustrates how task information, persona data (agent profiles), and domain-specific databases are integrated with memory modules (short-term and long-term) via the Coordinate Engine and the Cognitive Module. The Coordinate Engine manages the overall workflow and inter-agent relationships, while the Cognitive Module handles agent reasoning and strategy adaptation. The interactions between these modules and the environment are shown, highlighting the dynamic flow of information and actions within the MARBLE framework.", "section": "3.1 Framework Design"}, {"figure_path": "https://arxiv.org/html/2503.01935/x3.png", "caption": "Figure 3: Illustration of coordination protocols and planning prompt strategies.\n(a) shows centralized and decentralized planning structures (e.g., star, tree, graph, and chain).\n(b) describes strategies like group discussions and cognitive prompts, incorporating iterative feedback and task updates for effective planning.", "description": "This figure illustrates the different coordination protocols and planning strategies used in the MultiAgentBench framework. Panel (a) shows four different coordination protocols: star, tree, graph, and chain.  These represent different ways the agents can communicate and coordinate their actions to complete a task. The star and tree structures are centralized, meaning a single agent or a small group of agents leads the coordination effort. The graph and chain structures are decentralized; in the graph structure, multiple agents communicate directly with each other, while in the chain structure, communication flows sequentially from one agent to the next. Panel (b) depicts four different prompt strategies for planning: Vanilla Prompting, Chain-of-Thought (CoT), Group Discussion, and Cognitive Evolving Planning.  These methods showcase how different approaches can influence the planning process. The vanilla prompt provides a basic instruction, while CoT guides the agent through step-by-step reasoning. Group discussion involves multiple agents sharing ideas and refining the plan collaboratively, whereas cognitive self-evolving planning allows agents to continually update their plans based on their experiences and feedback.", "section": "3 Methodology"}, {"figure_path": "https://arxiv.org/html/2503.01935/x4.png", "caption": "Figure 4: Illustration of our benchmark curation and the dynamic milestones detecting for KPI metric.", "description": "This figure illustrates the process of creating the MultiAgentBench benchmark and the method for dynamically detecting milestones to compute Key Performance Indicators (KPIs).  The benchmark curation involves both human-generated datasets (for fixed tasks) and dynamic milestone detection (for scenarios with evolving tasks).  Human-generated datasets provide a baseline of well-defined tasks. Dynamic milestone detection uses LLMs to monitor task progress and identify completion of intermediate steps or \"milestones,\" contributing to a more nuanced evaluation of agent performance beyond just final task success.  The process is shown for two scenarios: Minecraft creation (a human-generated fixed dataset), and a bargaining scenario (dynamic milestone detection). The dynamic milestones are then used to calculate the KPI, providing a more comprehensive assessment of multi-agent system performance.", "section": "3.2 Benchmark Design"}, {"figure_path": "https://arxiv.org/html/2503.01935/x5.png", "caption": "Figure 5: Comparison of Different Coordination Protocols.\u2014Tree, Star, Graph, and Chain\u2014across multiple evaluation metrics. Specially, the token usages are scaled such that the lowest value is 00 and the hightest value is 100100100100. Details about metrics used for research task can be found at A.4", "description": "This figure compares four different multi-agent coordination protocols (Tree, Star, Graph, and Chain) across multiple evaluation metrics.  The metrics assess various aspects of the agents' performance, including task completion, communication effectiveness, planning quality, and resource usage (token usage).  The token usage metric is normalized, with 0 representing the lowest and 100 the highest usage.  Detailed information about the metrics used for the research task scenario is provided in Appendix A.4 of the paper. The figure visually represents the performance of each protocol, highlighting strengths and weaknesses of each approach for different aspects of coordination.", "section": "4.3 Main Experiment Two: Effects of Collaboration Protocols and Planning Strategies"}, {"figure_path": "https://arxiv.org/html/2503.01935/x6.png", "caption": "Figure 6: Average Metrics for Research Tasks for different planning prompt strategies. Cognitive Evolve Planning show best result in CS.", "description": "The figure presents a bar chart comparing the average metrics (Task Score and Coordination Score) across four different planning prompt strategies applied to the Research Task within the MultiAgentBench framework. The strategies compared are Vanilla Prompting, Chain-of-Thought prompting, Group Discussion, and Cognitive Evolving Planning.  The chart clearly demonstrates that the Cognitive Evolving Planning strategy yields the highest Coordination Score, indicating its effectiveness in facilitating collaboration among the LLM agents.", "section": "4.3 Main Experiment Two: Effects of Collaboration Protocols and Planning Strategies"}, {"figure_path": "https://arxiv.org/html/2503.01935/x7.png", "caption": "Figure 7: Scores for gpt-4o-mini across different iterations in Minecraft. The figure presents Task Score (TS), Communication Score (CS), Planning Score (PS), and Collaboration Score (CoS) over multiple iterations.", "description": "This figure displays the performance of the gpt-4o-mini model across multiple iterations of a Minecraft task.  It tracks four key metrics over time: Task Score (TS), which measures the overall success in completing the Minecraft task; Communication Score (CS), which assesses the quality of communication between agents; Planning Score (PS), which evaluates the effectiveness of the planning process; and Collaboration Score (CoS), which represents a combination of communication and planning, reflecting overall team effectiveness. By observing the trends in each metric across iterations, we can understand the model's learning trajectory, its ability to improve task performance over time, and its efficiency in collaborative problem-solving.", "section": "4.2 Main Experiment One: Model Performance Across Different Scenarios"}, {"figure_path": "https://arxiv.org/html/2503.01935/x8.png", "caption": "Figure 8: Scaling on Agents Number in Research Scenario. This figure shows the impact of agent number on KPI, Coordination Score, and Task Score.", "description": "This figure demonstrates the relationship between the number of agents involved in a research task and the overall performance metrics.  The x-axis represents the number of agents, while the y-axis displays three key performance indicators: KPI (Key Performance Indicator), Coordination Score, and Task Score.  The KPI represents the overall task completion success rate, the Coordination Score reflects the quality of collaboration and communication among agents, and the Task Score reflects the quality of the final research output. The graph shows how these three metrics change as the number of agents involved increases. This helps analyze the impact of team size on efficiency and collaboration in a multi-agent setting. ", "section": "5 Ablation Study"}, {"figure_path": "https://arxiv.org/html/2503.01935/x9.png", "caption": "Figure 9: Task Content Case.", "description": "This figure shows an example of the task content provided to the agents in the Research scenario.  It displays the introduction to a research paper on the Neural Language of Thought Model (NLoTM), which is designed to learn hierarchical, composable discrete representations from non-linguistic data. The agents are tasked with generating a novel research idea based on this introduction, following a specified 5-question (5q) format to ensure clarity, feasibility, and relevance.", "section": "3.2 Benchmark Design"}, {"figure_path": "https://arxiv.org/html/2503.01935/x10.png", "caption": "Figure 10: Agent Profile Case.", "description": "This figure shows an example of an agent profile used in the MultiAgentBench framework. The profile provides a detailed description of an agent's expertise, past work, and personality traits, which are relevant to the agent's participation in multi-agent tasks. This example illustrates the level of detail and specificity included in the agent profiles to ensure realistic interactions between agents in complex scenarios.", "section": "3.1 Framework Design"}, {"figure_path": "https://arxiv.org/html/2503.01935/x11.png", "caption": "Figure 11: 5Q cases.", "description": "This figure presents an example of a research idea formulated using the 5-question (5Q) format.  The 5Q format structures a research idea into five core questions addressing the problem's definition, significance, challenges, prior work, and proposed approach. The example showcases a research idea focused on enhancing malware detection and classification in network traffic using transformer-based models. Each of the five questions provides detailed information addressing various aspects of the proposed research, highlighting its novelty, relevance, challenges, and potential contributions.", "section": "3.2 Benchmark Design"}, {"figure_path": "https://arxiv.org/html/2503.01935/x12.png", "caption": "Figure 12: Net Score vs. Result Score (Scatter Plot)", "description": "This scatter plot visualizes the relationship between the \"Net Score\" and the \"Result Score\" in the Werewolf game simulations.  The Net Score represents the cumulative point difference between the Villagers and Werewolves throughout a game, reflecting the overall advantage of one team over the other. The Result Score indicates the final outcome of the game: the difference in the number of surviving villagers and werewolves. The plot shows that a higher Net Score strongly correlates with a positive Result Score (Villager victory), while a lower Net Score often leads to a negative Result Score (Werewolf victory). This suggests that the Net Score effectively predicts the game's outcome.", "section": "3.3 Evaluation Metrics"}]