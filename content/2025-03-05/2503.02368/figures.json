[{"figure_path": "https://arxiv.org/html/2503.02368/x1.png", "caption": "Figure 1: Visualization of different decoding strategies in the output space. Given a query, the base policy generates outputs with suboptimal rewards (lighter regions). Guided decoding with an estimated value function shifts the distribution towards higher-reward regions, while the optimal value function would guide the policy to achieve maximum rewards (darkest regions).", "description": "This figure illustrates how different decoding strategies impact the distribution of model outputs.  The base policy, without any guidance, produces outputs with a broad range of rewards, represented by lighter colors.  Using an estimated value function, guided decoding shifts the distribution towards higher-reward areas (darker colors).  Ideally, an optimal value function would generate outputs with maximum rewards (darkest region), demonstrating the effectiveness of value-guided decoding.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.02368/x2.png", "caption": "(a) Summarization", "description": "This figure shows the results of different methods on the summarization task, illustrating the trade-off between reward and KL divergence.  The x-axis represents the KL divergence between the guided policy and the base policy, while the y-axis represents the average reward.  Different colored lines represent different methods (IVO, FUDGE, VAS, ARGS, DPO, IPO). The figure demonstrates how each method performs under varying degrees of KL divergence, showing the balance between achieving high rewards and maintaining closeness to the original policy distribution.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.02368/x3.png", "caption": "(b) Multi-turn Dialogue", "description": "The figure shows the reward versus KL divergence for different methods on a multi-turn dialogue task.  It compares the performance of several guided decoding methods (IVO, FUDGE, VAS, ARGS, DPO, IPO) against the base policy.  The x-axis represents the KL divergence (a measure of difference between the guided policy and the base policy), and the y-axis represents the reward achieved by each method. The plot illustrates how each method balances the trade-off between generating high-reward responses and maintaining similarity to the base policy. IVO consistently achieves higher rewards than other methods across various KL divergence levels.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.02368/x4.png", "caption": "Figure 2: Reward vs. KL divergence for different methods on (a) summarization and (b) multi-turn dialogue.", "description": "This figure displays the performance of different language model decoding methods across two tasks: summarization and multi-turn dialogue.  The x-axis represents the KL divergence (a measure of difference) between the policy guided by the value function and the base policy. A lower KL divergence indicates that the guided policy is closer to the base policy. The y-axis represents the average reward, reflecting the quality of the generated text. Each line represents a different decoding method, showing how its reward changes with varying degrees of divergence from the base policy.  The plots reveal how different methods balance reward and similarity to the base policy.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.02368/x5.png", "caption": "(a) Summarization", "description": "The figure shows the comparison of different value functions using value-guided blockwise beam search on the summarization task.  The x-axis represents the different methods used, including ARGS, FUDGE, VAS, and IVO (the proposed method). The y-axis represents the average reward achieved by each method. The bars show that IVO achieves the highest average reward, demonstrating its superiority over the baseline methods in the summarization task.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.02368/x6.png", "caption": "(b) Multi-turn Dialogue", "description": "The figure shows the comparison of different methods on the multi-turn dialogue task in terms of reward versus KL divergence.  The x-axis represents the KL divergence from the base policy, measuring how much the guided policy deviates from the original model's behavior. The y-axis represents the average reward achieved by each method, reflecting the quality of the generated responses based on human preference judgments.  Different methods are represented by different colored lines. The figure demonstrates how the reward varies as the deviation from the base policy increases, and which method maintains the best reward at different KL divergence levels.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.02368/x7.png", "caption": "(c) Instruction Following", "description": "This figure shows the comparison of different value functions using the value-guided blockwise beam search method on the instruction-following task. It displays the average reward achieved by different models on this task: Base, ARGS, FUDGE, VAS, and IVO (with and without iterative on-policy optimization). The chart visually compares their performance, allowing for easy understanding of how IVO performs against other methods in aligning language models with instructions.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.02368/x8.png", "caption": "Figure 3: Comparison of different value functions using value-guided blockwise beam search on summarization (left), multi-turn dialogue (middle) and instruction following (right). IVO\u2217 denotes IVO without iterative on-policy optimization.", "description": "This figure compares the performance of different value function optimization methods on three tasks: text summarization, multi-turn dialogue, and instruction following.  Each bar represents the average reward achieved by a particular method, with taller bars signifying better performance.  The leftmost bar in each group represents the baseline using a base policy only.  The methods compared are ARGS, FUDGE, VAS, IVO (with iterative on-policy optimization), and IVO* (IVO without the iterative component).  The figure helps illustrate the effectiveness of the iterative approach used in IVO compared to other value function estimation techniques and how this affects downstream tasks.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.02368/x9.png", "caption": "(a) Number of ST", "description": "This figure shows the ablation study on the number of sampled trajectories in multi-turn dialogue using blockwise beam search.  The x-axis represents the number of sampled trajectories and the y-axis represents the average reward.  The different colored lines represent different methods being compared. The purpose is to demonstrate the impact of increasing the number of trajectories on the model's performance.", "section": "6.1. The Number of Sampled Trajectories"}, {"figure_path": "https://arxiv.org/html/2503.02368/x10.png", "caption": "(b) Number of TI", "description": "This figure shows the impact of the number of training iterations on the performance of the Iterative Value Function Optimization (IVO) method. The x-axis represents the number of training iterations, and the y-axis represents the average reward achieved by the model. The graph shows that as the number of training iterations increases, the average reward also increases, indicating that the iterative refinement of the value function improves the model's performance.", "section": "4. IVO: Iterative Value Function Optimization"}]