[{"figure_path": "https://arxiv.org/html/2504.17816/x2.png", "caption": "Figure 1: Result on video customization with our proposed decomposed subject-driven generation, demonstrating high-quality results across various scenarios.", "description": "This figure showcases the results of video customization using the proposed subject-driven video generation method.  It demonstrates the model's ability to generate high-quality videos across a variety of scenarios by seamlessly integrating user-specified subjects into different contexts and actions.  The examples presented showcase the method's capacity for preserving subject identity and generating realistic motion, highlighting the key advantages of the proposed disentangled approach.", "section": "Abstract"}, {"figure_path": "https://arxiv.org/html/2504.17816/x3.png", "caption": "Figure 2: Overview of our framework. We factorize the subject-driven video customization (S2V) into temporal-awareness preservation and ID injection (Left). To optimize the two objectives, we utilize stochastically-switched finetuning, randomly switching between two training objectives (Right).", "description": "This figure illustrates the proposed framework for subject-driven video generation. The framework decouples the task into two main components: identity injection and temporal awareness preservation.  The left side shows the factorization of the subject-driven video customization (S2V) into these two components. Identity injection leverages an image customization dataset to inject subject-specific features, while temporal awareness preservation uses a separate, small set of unpaired videos to maintain temporal coherence. The right side depicts the stochastically-switched fine-tuning process, which randomly switches between optimizing the identity injection and temporal awareness objectives during training. This approach mitigates catastrophic forgetting, enabling the model to effectively learn both subject identity and temporal dynamics simultaneously.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2504.17816/x4.png", "caption": "Figure 3: Qualitative comparison on Dreambooth dataset.", "description": "Figure 3 presents a qualitative comparison of several video generation methods on the DreamBooth dataset.  The figure showcases the results from four different approaches: Ours, OmniControl + I2V, Vidu 2.0, and VideoBooth, for two different prompts. Each row demonstrates how effectively the method preserves subject identity and generates temporally coherent video while accurately representing specified scene details.  Visual inspection allows for comparing the strengths and weaknesses of each approach in terms of subject identity consistency and motion quality.", "section": "4.3. Qualitative Result"}, {"figure_path": "https://arxiv.org/html/2504.17816/x5.png", "caption": "Figure 4: Several examples of qualitative comparison with Still-Moving\u00a0[8] (left) and ours (right).", "description": "This figure presents a qualitative comparison of video generation results between the proposed method and the Still-Moving method [8].  For each row, a reference image and text prompt are given.  The left column shows the results produced by the Still-Moving method, while the right column displays the results obtained using the proposed method.  The examples highlight differences in subject identity preservation, motion detail, and overall visual quality between the two methods.  The comparison focuses on the ability of each method to accurately reproduce the subject's characteristics and to generate temporally consistent and realistic motion.", "section": "4.3. Qualitative Result"}, {"figure_path": "https://arxiv.org/html/2504.17816/x6.png", "caption": "Figure 5: Qualitative result on ablation study of our component in temporal awareness preservance.", "description": "This figure shows the results of an ablation study evaluating the impact of different training strategies on the temporal awareness preservation component of the proposed model.  It compares three approaches: using only image data for training, using a two-stage training process (image data followed by video data), and the authors' proposed stochastically-switched approach that combines image and video data training. The results are presented in terms of the generated videos' characteristics, demonstrating the effectiveness of the stochastically-switched method in maintaining temporal coherence while accurately representing the subject identity.", "section": "4.4. Ablation Study"}, {"figure_path": "https://arxiv.org/html/2504.17816/x7.png", "caption": "Figure 6: Effect of random initial frame and image token dropping.", "description": "This figure shows the effects of two regularization techniques: random initial frame selection and image token dropping, used during image-to-video (I2V) fine-tuning in the proposed framework.  The leftmost column shows the results without using these techniques, leading to the model heavily relying on the first frame, resulting in repetitive and unnatural video sequences. The middle column shows the results when only random initial frame selection is applied. The rightmost column depicts the results when both random initial frame selection and image token dropping are employed. The image demonstrates that the proposed techniques effectively enhance video generation by mitigating the over-reliance on the first frame and producing more dynamic and diverse motion results.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2504.17816/x8.png", "caption": "Figure 7: Additional qualitative result. Comparison with other baselines.", "description": "Figure 7 presents a qualitative comparison of video generation results from the proposed method against three baselines: VideoBooth, Vidu 2.0, and OmniControl + I2V.  Two example scenarios are shown: a toy car on a wet street at night and a teddy bear in a bathtub.  The comparison highlights the ability of the proposed method to generate higher-quality videos with improved subject identity preservation and more natural motion compared to the baselines. ", "section": "4.3 Qualitative Result"}]