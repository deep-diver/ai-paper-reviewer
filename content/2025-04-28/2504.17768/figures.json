[{"figure_path": "https://arxiv.org/html/2504.17768/x1.png", "caption": "Figure 1: Overview of sparse attention methods for prefilling (left) and generation (right). These methods differ in the units of sparsification (blocks or pages vs.\u00a0verticals and slashes), importance estimation, and KV cache management strategies. Colours represent query\u2013key interactions preserved at different sparsity levels, while white areas indicate interactions that are not computed.", "description": "Figure 1 visualizes different sparse attention mechanisms used in Transformer LLMs during prefilling and generation phases.  The figure shows how different methods sparsify the attention matrix, which is a key component of self-attention calculations in Transformer models.  The left side illustrates the patterns for prefilling (processing the input prompt all at once), while the right side showcases those for generation (processing one token at a time). Each pattern is represented by a heatmap; the color intensity represents the preserved query-key interactions at various sparsity levels, with white indicating uncomputed interactions. The key differences highlighted are the unit of sparsification (whether attention is sparsified at block/page, vertical, or slash levels), the importance estimation method (whether sparsification is fixed or dynamic), and the KV cache management strategy (whether a full cache or eviction is used).  The figure helps to understand how different design choices affect the computational efficiency and accuracy of sparse attention methods.", "section": "Training-Free Sparse Attention"}, {"figure_path": "https://arxiv.org/html/2504.17768/x2.png", "caption": "(a) IsoFLOPS analysis for prefilling, using Vertical-Slash pattern.", "description": "This figure shows the results of an isoFLOPS analysis for the prefilling phase of text generation using the Vertical-Slash sparse attention pattern.  It compares the performance (measured as average accuracy across various tasks) against the FLOPS (floating point operations) for different model sizes and sparsity levels.  The purpose is to determine whether a smaller, dense model or a larger, more sparse model is more computationally efficient for achieving a particular level of accuracy at a given sequence length.", "section": "4.1 IsoFLOPS Analysis"}, {"figure_path": "https://arxiv.org/html/2504.17768/x3.png", "caption": "(b) IsoFLOPS analysis for decoding, using Quest pattern.", "description": "This figure shows the results of an isoFLOPS analysis for the decoding phase of text generation, specifically using the Quest sparse attention pattern.  The x-axis represents the total floating-point operations (FLOPS) used, and the y-axis represents the average performance across various tasks. Different colored lines represent different model sizes (7B, 14B, 32B, and 72B parameters), and different markers along the lines represent different levels of compression (sparsity). The left panel shows results for sequence lengths of 32k tokens, while the right panel shows results for 128k tokens. The figure helps to understand the trade-off between computational cost (FLOPS) and performance for different model sizes and levels of sparsity during decoding with the Quest pattern.", "section": "4.1 IsoFLOPS Analysis"}, {"figure_path": "https://arxiv.org/html/2504.17768/x4.png", "caption": "Figure 2: Performance comparison for batch size 1 across FLOPS, which are a function of sequence length, model size and sparsity level. We report 4 model sizes (markers) and compression ratios up to 20\u00d7\\times\u00d7 (heatmap). Performance scores are aggregated across all 9 tasks. In the plots, we display two sequence lengths\u201432k (left) and 128k (right)\u2014and two phases\u2014prefilling (top) and decoding (bottom). Crucially, there is a phase transition where after a critical sequence length (32\u201364k tokens for Qwen family models), highly sparse and large models surpass dense and small models in performance for the same FLOPS budget. See Appendix\u00a0D for details on how we estimate the FLOPS, including indexing costs for sparse attention methods.", "description": "This figure presents a comparison of the performance of different model sizes and sparsity levels across different sequence lengths.  It shows the trade-off between accuracy and FLOPs (floating point operations), a measure of computational cost.  The heatmap displays performance across different compression ratios (1x to 20x), indicating the level of sparsity. The plots separate the analysis by inference phase (prefilling and decoding) and sequence length (32k and 128k tokens). A key finding highlighted is a 'phase transition' where larger, highly sparse models become more efficient than smaller, dense models for long sequences.", "section": "4 Results"}, {"figure_path": "https://arxiv.org/html/2504.17768/x7.png", "caption": "Figure 3: Maximum compression ratio with statistically significant performance retention (y-axis) across different model sizes (colours) and sequence lengths (x-axis). Each point represents a task, with horizontal bars showing the average maximum compression across tasks and vertical bars indicating standard deviation. Left: Vertical-Slash pattern for prefilling. Right: Quest pattern for decoding. The key conclusion is that decoding tolerates higher compression than prefilling on average, with larger models maintaining performance even at very high compression ratios. However, almost every configuration has at least one task where maximum tolerable compression is below 5\u00d7\\times\u00d7 (72B Quest being the only exception).", "description": "Figure 3 illustrates the maximum compression ratio achievable for different sparse attention methods while maintaining statistically significant performance.  The y-axis shows the maximum compression ratio, while the x-axis represents the sequence length.  Each data point represents a specific task, and horizontal bars depict average maximum compression across all tasks for each model size and sequence length. Vertical bars illustrate the standard deviation of maximum compression across tasks.  The left panel shows results for the Vertical-Slash pattern (used during prefilling), and the right panel shows results for the Quest pattern (used during decoding).  The main finding is that the Quest method (decoding) allows for significantly higher compression ratios than Vertical-Slash (prefilling), and that larger models generally tolerate higher compression before a significant drop in performance.  However, even with the largest model (72B parameters), there is at least one task that exhibits significant performance degradation when using compression ratios exceeding 5x, with Quest being the only exception.", "section": "4 Results"}, {"figure_path": "https://arxiv.org/html/2504.17768/x8.png", "caption": "Figure 4: Performance comparison of different sparse attention methods across 9 tasks, aggregated over sequence lengths and models (shaded areas indicate the standard error). Top: prefilling methods (Vertical-Slash, FlexPrefill, Block-Sparse). Bottom: decoding methods (SnapKV, Ada-SnapKV, Quest). Each subplot shows the relationship between performance and compression for a specific task. The trade-off appears extremely task-dependent. Overall, Vertical-Slash performs best among prefilling methods, while Quest performs best among decoding methods.", "description": "Figure 4 presents a comprehensive performance comparison of six different sparse attention methods across nine distinct long-context tasks.  The results are aggregated across various sequence lengths and model sizes, with shaded areas representing standard error.  The figure is divided into two sections: the top section showcases three prefilling methods (Vertical-Slash, FlexPrefill, Block-Sparse), while the bottom section displays three decoding methods (SnapKV, Ada-SnapKV, Quest). Each subplot within the figure focuses on a single task, illustrating the trade-off between performance and the compression ratio achieved by each sparse attention method. A key takeaway is the significant task-dependency observed in the performance-compression trade-off, highlighting that no single method consistently outperforms others across all tasks.  However, the figure suggests that the Vertical-Slash method generally performs best among the prefilling methods, while the Quest method tends to be the top performer for decoding.", "section": "4.3 Performance on Individual Tasks and Methods"}, {"figure_path": "https://arxiv.org/html/2504.17768/x9.png", "caption": "Figure 5: Block-Sparse block size.", "description": "This figure shows the ablation study on the Block-Sparse method, investigating the impact of different block sizes on the performance. It reveals how the choice of block size influences the trade-off between computational efficiency and model accuracy, indicating the optimal block size for the Block-Sparse method.", "section": "A.1 Implementation Details"}, {"figure_path": "https://arxiv.org/html/2504.17768/x10.png", "caption": "Figure 6: Quest page size.", "description": "This figure shows the results of an ablation study on the Quest sparse attention method, investigating the impact of different page sizes on performance.  The x-axis represents the compression ratio, and the y-axis shows the performance score. Multiple lines are plotted, each representing a different page size (16, 32, and 64 tokens). The plot illustrates the trade-off between compression ratio and performance, showing that smaller page sizes may lead to better performance with higher compression. This suggests that for Quest, using smaller page sizes is better to maintain performance with high sparsity.", "section": "2 Training-Free Sparse Attention"}, {"figure_path": "https://arxiv.org/html/2504.17768/x11.png", "caption": "Figure 7: Ada-SnapKV min budget.", "description": "This figure shows the ablation study on the minimum budget hyperparameter for the Ada-SnapKV sparse attention method.  The x-axis represents the compression ratio, and the y-axis represents the performance score. Different lines represent different minimum budget settings, demonstrating how the minimum budget impacts performance under varying levels of sparsity.", "section": "3.2 Sparse Attention Methods"}, {"figure_path": "https://arxiv.org/html/2504.17768/x12.png", "caption": "Figure 8: FlexPrefill min budget.", "description": "This figure shows the impact of the `min_budget` hyperparameter on FlexPrefill's performance.  FlexPrefill is a sparse attention method that dynamically allocates a budget across layers and heads. The x-axis represents the compression ratio, indicating the level of sparsity. The y-axis shows the performance score. Different lines represent different values for `min_budget`, showing how the minimum budget allocated to each head affects performance at various levels of sparsity.  The graph illustrates the trade-off between sparsity and performance, highlighting the sensitivity of FlexPrefill to the choice of `min_budget` and how it influences the performance under high compression ratios.", "section": "A.1.3 FlexPrefill"}, {"figure_path": "https://arxiv.org/html/2504.17768/x13.png", "caption": "Figure 9: SnapKV/Ada-SnapKV approximation window.", "description": "This figure shows the ablation study on the approximation window size used by SnapKV and Ada-SnapKV methods. The x-axis represents the compression ratio, and the y-axis represents the aggregated performance across all nine tasks. Different colored lines represent different approximation window sizes.  The results indicate the optimal window size for achieving good performance at various compression levels.", "section": "3.2 Sparse Attention Methods"}, {"figure_path": "https://arxiv.org/html/2504.17768/x14.png", "caption": "Figure 10: SnapKV/Ada-SnapKV kernel size.", "description": "This figure shows the ablation study on the kernel size used for smoothing token importance scores in SnapKV and Ada-SnapKV methods.  The experiment evaluates various kernel sizes (5, 7, 13, and 21) across different compression ratios.  The results demonstrate that using a kernel size of 21 yields the best performance across various compression ratios, suggesting a balance between smoothing noisy importance scores and preserving fine-grained details.", "section": "3.2 Sparse Attention Methods"}]