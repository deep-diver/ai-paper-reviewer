{"references": [{"fullname_first_author": "Samira Abnar", "paper_title": "Parameters vs FLOPS: Scaling laws for optimal sparsity for mixture-of-experts language models", "publication_date": "2025-01-01", "reason": "This paper explores scaling laws for optimal sparsity in mixture-of-experts models, providing a related perspective on balancing parameters and computation."}, {"fullname_first_author": "Anastasios N. Angelopoulos", "paper_title": "Learn then test: Calibrating predictive algorithms to achieve risk control", "publication_date": "2021-10-01", "reason": "This paper introduces statistical testing, and it was inspired to develop their distribution-free risk control."}, {"fullname_first_author": "Dzmitry Bahdanau", "paper_title": "Neural machine translation by jointly learning to align and translate", "publication_date": "2015-01-01", "reason": "This work introduces the attention mechanism that is fundamental to the Transformer architecture and self-attention mechanism, the basic concept in the submitted paper."}, {"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-01-01", "reason": "This work is related to the ability to model long sequences in large language models and that language models are few-shot learners."}, {"fullname_first_author": "Jared Kaplan", "paper_title": "Scaling laws for neural language models", "publication_date": "2020-01-01", "reason": "This paper is related to establishing scaling laws for sparse attention, which can generalise beyond the range of configurations considered."}]}