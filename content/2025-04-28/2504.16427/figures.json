[{"figure_path": "https://arxiv.org/html/2504.16427/x1.png", "caption": "Figure 1: Overview of the MMLA benchmark. The left side shows examples from six evaluation dimensions and nine datasets. The right side displays three methods for evaluating both LLMs and MLLMs: (1) zero-shot inference (top right), which generates predictions from task-specific prompts; (2) supervised fine-tuning (middle right), which trains on each supervised task; and (3) instruction tuning (bottom right), which trains on multiple tasks simultaneously. Both (2) and (3) utilize LoRA to efficiently adapt foundation models.", "description": "Figure 1 provides a visual overview of the MMLA benchmark, a comprehensive evaluation benchmark for multimodal language analysis. The left-hand side showcases examples of the six core evaluation dimensions (intent, emotion, dialogue act, sentiment, speaking style, and communication behavior) and data samples from the nine datasets used in the benchmark.  The right-hand side illustrates the three evaluation methods employed: zero-shot inference (using task-specific prompts), supervised fine-tuning (training on individual supervised tasks), and instruction tuning (training on multiple tasks simultaneously).  Methods (2) and (3) both utilize LoRA (Low-Rank Adaptation) for efficient adaptation of pre-trained foundation models.", "section": "3 MMLA Benchmark"}, {"figure_path": "https://arxiv.org/html/2504.16427/x2.png", "caption": "Figure 2: Rank of foundation models after zero-shot inference.", "description": "This figure presents the performance ranking of various foundation models (both LLMs and MLLMs) after zero-shot inference on the MMLA benchmark.  The ranking is based on the average accuracy (ACC) across all testing datasets.  The models are listed in descending order of accuracy, showing which models performed best without any prior training or fine-tuning on the specific MMLA tasks.  This provides a baseline measurement of the models' generalization capabilities in understanding complex multimodal language.", "section": "5 Results and Discussion"}, {"figure_path": "https://arxiv.org/html/2504.16427/x3.png", "caption": "Figure 3: Rank of foundation models after SFT and IT.", "description": "This figure shows the ranking of various Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) after they have undergone supervised fine-tuning (SFT) and instruction tuning (IT).  The ranking is based on the average accuracy across all the datasets in the MMLA benchmark.  The figure highlights the performance differences between the models and the impact of the different training methods.", "section": "5 Results and Discussion"}, {"figure_path": "https://arxiv.org/html/2504.16427/x4.png", "caption": "Figure 4: Fine-grained zero-shot inference and SFT performance (ACC). Within each bar, the light-colored lower segment corresponds to zero-shot inference performance, while the darker upper segment represents the additional gains from SFT. The performance of SOTA MML methods (if available) and GPT-4o are indicated with purple and green dashed lines, respectively.", "description": "Figure 4 presents a detailed comparison of zero-shot inference and supervised fine-tuning (SFT) performance, measured by accuracy (ACC), across various multimodal language analysis tasks.  Each bar in the chart is divided into two segments: the lighter bottom segment shows the accuracy achieved using only zero-shot inference, and the darker top segment represents the improvement gained after performing SFT.  The chart includes results for several different large language models (LLMs) and multimodal LLMs (MLLMs), comparing their performance across different datasets.  For context, the performance of state-of-the-art (SOTA) multimodal machine learning (MML) methods, when available for a given task, and GPT-4 are also plotted as dashed lines. This visual representation allows for a direct comparison of how much SFT enhances the models' abilities on each task and dataset.", "section": "5 Results and Discussion"}, {"figure_path": "https://arxiv.org/html/2504.16427/x5.png", "caption": "Figure 5: Fine-grained performance (ACC) of instruction-tuned MLLMs and LLMs on each dataset across six dimensions. The performance of SOTA MML methods and humans are indicated with dashed lines, if available.", "description": "Figure 5 presents a detailed analysis of the accuracy achieved by various large language models (LLMs) and multimodal large language models (MLLMs) across six different semantic dimensions.  The models were fine-tuned using an instruction-tuning method.  The figure shows the performance of these models on nine different datasets.  For context, the performance of state-of-the-art (SOTA) multimodal machine learning (MML) methods and even human performance are included as dashed lines where available for comparison.  This granular breakdown allows for a precise evaluation of model capabilities across a spectrum of complexity in multimodal understanding.", "section": "5 Results and Discussion"}, {"figure_path": "https://arxiv.org/html/2504.16427/x6.png", "caption": "Figure 6: Scalability of Qwen2 and Qwen2-VL on the MMLA benchmark.", "description": "This figure demonstrates how the performance of Qwen2 and its multimodal variant, Qwen2-VL, change as the model size increases.  It shows the impact of scaling model parameters on various aspects of multimodal language analysis within the MMLA benchmark.  Both zero-shot inference and performance after supervised fine-tuning (SFT) are presented, giving a comprehensive understanding of the models' scalability and the trade-offs between model size and performance across different tasks.", "section": "5.3 Scalability of Foundation Models on MMLA"}]