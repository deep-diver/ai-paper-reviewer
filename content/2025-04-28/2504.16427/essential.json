{"importance": "This work introduces a new **MMLA benchmark** to push MLLM's limits in multimodal analysis. The code and data are publicly available, opening avenues for cognitive-level human-AI interaction research.", "summary": "MMLA: A multimodal benchmark for cognitive language analysis, revealing current MLLMs' limitations and providing a valuable resource for future research.", "takeaways": ["Current MLLMs have limitations in understanding high-level cognitive semantics in multimodal language analysis.", "Supervised fine-tuning and instruction tuning can significantly enhance MLLMs' capabilities, but challenges remain.", "Smaller-scale, well-trained MLLMs can achieve performance comparable to larger models, suggesting feasible, cost-effective solutions."], "tldr": "Multimodal language analysis enhances understanding of human conversations, but Multimodal Large Language Models' (MLLMs) capabilities are underexplored. Existing benchmarks focus on low-level semantics, missing cognitive-level nuances.This paper fills the gap by presenting a **comprehensive benchmark for Multimodal Language Analysis (MMLA)**, designed to evaluate MLLMs in understanding conversational semantics. \n\nMMLA includes over 61K multimodal utterances from staged and real-world scenarios, covering intent, emotion, dialogue act, sentiment, speaking style, and communication behavior. Experiments on LLMs and MLLMs using zero-shot inference, supervised fine-tuning, and instruction tuning reveal limitations in cognitive language understanding. **MMLA serves as a solid foundation** to explore LLMs in multimodal language analysis and provides resources to advance the field.", "affiliation": "Tsinghua University", "categories": {"main_category": "Multimodal Learning", "sub_category": "Multimodal Understanding"}, "podcast_path": "2504.16427/podcast.wav"}