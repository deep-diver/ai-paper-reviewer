[{"figure_path": "https://arxiv.org/html/2504.17025/x1.png", "caption": "Figure 1: Fertility for two different tokenizers, Mistral-7B-v0.1 (left) and Minerva (right), over Italian texts from CulturaX (blue) and Wikipedia (red).", "description": "This figure compares the token fertility of two different tokenizers when processing Italian text.  The tokenizers are Mistral-7B-v0.1 (an English-centric model) and Minerva (an Italian-centric model). The data used for comparison comes from two Italian text corpora: CulturaX and Wikipedia.  The bar chart visually represents the average number of tokens each tokenizer produces for a given word in each corpus, illustrating the difference in efficiency and potential impact on downstream tasks between the two approaches. Lower fertility numbers indicate more efficient tokenization and potentially faster processing.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2504.17025/extracted/6384534/mistral_it_0shot.png", "caption": "Figure 2: Average performance of Mistral-7B-v0.1 based models during training on Italian translated benchmarks. The average was calculated over six datasets.", "description": "This figure shows the average performance of six different Mistral-7B-v0.1 based models during continual training on Italian translated benchmarks.  The models are: the baseline Mistral-7B-v0.1 model (LAPT), and five variations produced using different vocabulary adaptation techniques: Random, FVT, CLP, SAVA and Minerva. The x-axis represents training steps, while the y-axis represents average accuracy across the six datasets. The graph illustrates how the performance of each model evolves over the course of the training, allowing for a comparison of the effectiveness of different vocabulary adaptation strategies.", "section": "4.2 Continual Training"}, {"figure_path": "https://arxiv.org/html/2504.17025/extracted/6384534/llama_it_0shot-new.png", "caption": "Figure 3: Average performance of Llama-3.1-8B based models during training on Italian translated benchmarks. The average was calculated over six datasets.", "description": "This figure displays the average performance of Llama-3.1-8B language models during continual training on Italian-translated benchmark datasets.  The continual training involved adapting the models to the Italian language, and the graph shows how their performance changes across different stages of training, measured by the average accuracy across six different benchmark datasets. This illustrates the model's learning progress over time, allowing for an assessment of its adaptation efficacy.", "section": "4.2 Continual Training"}, {"figure_path": "https://arxiv.org/html/2504.17025/extracted/6384534/mistral_en_0shot.png", "caption": "Figure 4: Average performance of Mistral-7B-v0.1 based models during training on English benchmarks. The average was calculated over six datasets.", "description": "This figure displays the average performance of Mistral-7B-v0.1 language models across six different English benchmark datasets throughout a continual training process.  The x-axis represents the number of training steps completed, and the y-axis shows the average accuracy achieved across the six benchmarks.  Different lines represent different vocabulary adaptation methods, showing how each method affects model performance over time.", "section": "5.1.2 English Results"}, {"figure_path": "https://arxiv.org/html/2504.17025/extracted/6384534/llama_en_0shot-new.png", "caption": "Figure 5: Average performance of Llama-3.1-8B based models during training on English benchmarks. The average was calculated over six datasets.", "description": "This figure displays the average performance of Llama-3.1-8B language models over six different English benchmark datasets throughout the continual training process.  The x-axis represents the number of training steps completed and the y-axis shows the average accuracy across the six datasets. This visual helps to assess how the model's performance on English language tasks evolves as it undergoes further training.", "section": "5.1.2 English Results"}, {"figure_path": "https://arxiv.org/html/2504.17025/extracted/6384534/mistral_training_loss-new.png", "caption": "Figure 6: Loss during continual training of Mistral-7B-v0.1 models.", "description": "This figure shows the training loss curves for Mistral-7B-v0.1 models during continual training.  The curves represent different vocabulary adaptation techniques applied to the model: Random, FVT, CLP, SAVA, and LAPT. The x-axis represents the training steps, and the y-axis represents the training loss. The plot visually compares the convergence speed and overall loss achieved by each method, offering insights into their efficiency and effectiveness in adapting the model to the target language.", "section": "4.2 Continual Training"}, {"figure_path": "https://arxiv.org/html/2504.17025/extracted/6384534/llama_training_loss-new.png", "caption": "Figure 7: Loss during continual training of Llama-3.1-8B models.", "description": "This figure shows the training loss curves for the Llama-3.1-8B language model during continual training.  Several different vocabulary adaptation techniques (FVT, SAVA, and LAPT) are compared to a random baseline. The x-axis represents the number of training steps, and the y-axis shows the training loss. The plot illustrates how the loss decreases over time for each method, indicating the model's improvement in learning the target language.  This visualization helps to assess the effectiveness of different vocabulary adaptation strategies and their impact on training efficiency.", "section": "4.2 Continual Training"}, {"figure_path": "https://arxiv.org/html/2504.17025/extracted/6384534/cross_similarity.jpg", "caption": "Figure 8: Similarity across models after continual training on 12B tokens.", "description": "This figure visualizes the cosine similarity between different Mistral-7B-v0.1 models after undergoing continual training with 12 billion tokens.  Each model was adapted using a different vocabulary adaptation technique (Random, FVT, CLP, SAVA). The heatmap shows the pairwise similarity scores, illustrating the relationships and differences in the embedding space structures resulting from each adaptation method.  Higher similarity scores indicate greater similarity in the embedding space representations learned by the models.", "section": "6 Differences in the Embedding Structure"}, {"figure_path": "https://arxiv.org/html/2504.17025/extracted/6384534/mistral_training_loss_sava_models-new.png", "caption": "Figure 9: Loss during continual training of Mistral models.", "description": "This figure displays the training loss curves for Mistral-7B-v0.1 models during continual training.  Different lines represent models adapted using various vocabulary adaptation techniques: SAVA with different helper model sizes (350M, 1B, and 3B parameters). The x-axis represents the number of training steps, and the y-axis shows the training loss.  The plot helps to visualize the convergence speed and overall performance of each adaptation method during the continual training process, showing how the size of the helper model affects training.", "section": "5.3 Training Loss"}, {"figure_path": "https://arxiv.org/html/2504.17025/extracted/6384534/mistral_training_loss_sava_anchors-new.png", "caption": "Figure 10: Loss during continual training of Mistral models.", "description": "This figure displays the training loss curves for Mistral-7B-v0.1 models during continual training using the SAVA method with varying numbers of tokens used to train the linear mapping function (\u03c6).  The curves show how the training loss changes over time (number of training steps) for different configurations of the SAVA method, demonstrating the impact of the number of tokens selected from the intersection of the source and target vocabularies on model convergence during the continual training process.", "section": "5.3 Training Loss"}]