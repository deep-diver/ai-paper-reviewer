[{"figure_path": "https://arxiv.org/html/2504.12080/x1.png", "caption": "Figure 1: Overview of the proposed DC-SAM method and IC-VOS benchmark. a) Comparison of the previous few-shot segmentation methods in 1), existing methods based on SAM/SAM2 in 2), and DC-SAM in 3). DC-SAM leverages multi-source features and generates positive and negative prompts by ensuring prompt consistency, integrating with SAM/SAM2 to achieve in-context segmentation for both images and videos; b) Visualization of image and video settings by DC-SAM; c) Quantitative comparison of DC-SAM with state-of-the-art approaches in terms of mIoU on COCO-20i and PASCAL-5i, \ud835\udca5&\u2131\ud835\udca5\u2131\\mathcal{J\\&F}caligraphic_J & caligraphic_F on the IC-VOS benchmark.", "description": "Figure 1 provides a comprehensive overview of the Dual Consistency Segment Anything Model (DC-SAM) and the In-Context Video Object Segmentation (IC-VOS) benchmark.  Part (a) compares different few-shot segmentation approaches, highlighting the unique features of DC-SAM. DC-SAM integrates multi-source features to generate positive and negative prompts, ensuring consistency in the prompt generation process. This approach allows DC-SAM to perform in-context segmentation for both images and videos by leveraging the strengths of SAM and SAM2.  Part (b) shows a visual representation of how DC-SAM processes both image and video inputs. Part (c) presents quantitative comparisons between DC-SAM and state-of-the-art methods on COCO-20, PASCAL-5, and the newly proposed IC-VOS benchmark, illustrating DC-SAM's performance improvements in terms of mean Intersection over Union (mIoU) and the J&F score.", "section": "INTRODUCTION"}, {"figure_path": "https://arxiv.org/html/2504.12080/x2.png", "caption": "Figure 2: Overview of our constructed IC-VOS benchmark. a) Distribution of video sources and their proportions. b) Word cloud of expressions. c) Categories in the dataset with the number of clips and frames for each category. d) Example cases illustrating the support image, support mask, and query video.", "description": "Figure 2 provides a comprehensive overview of the newly constructed IC-VOS benchmark dataset for in-context video object segmentation.  Panel (a) shows the distribution of videos across three existing video object segmentation datasets (DAVIS, MOSE, and LVOS v2), indicating their relative contributions to the IC-VOS dataset.  Panel (b) presents a word cloud visualizing the frequency of different semantic categories within the dataset. The size of each word is proportional to its occurrence frequency, offering insights into the dominant classes present in IC-VOS. Panel (c) displays a detailed breakdown of the dataset, presenting the number of video clips and frames per category. This provides precise statistical information about the dataset's composition. Finally, panel (d) showcases several exemplary cases illustrating how the support image, its corresponding mask, and the query video are structured within the IC-VOS benchmark.  These examples offer visual insights into the data format and the task's nature.", "section": "3 IN-CONTEXT VOS BENCHMARK"}, {"figure_path": "https://arxiv.org/html/2504.12080/x3.png", "caption": "Figure 3: Overview of the proposed DC-SAM framework. We use positive and negative branches to generate respective prompts, thereby refining the scope of the final generated mask. Additionally, we incorporate SAM features during the prompt generation process to better capture the characteristics of SAM, resulting in more accurate prompt boundaries. During the prompt generation process, we introduce cyclic consistent cross-attention to filter out non-cycle-consistent feature points, enhancing the precision of the prompts.", "description": "This figure illustrates the architecture of the Dual Consistency Segment Anything Model (DC-SAM).  It shows how DC-SAM enhances the Segment Anything Model (SAM) for in-context image segmentation by using positive and negative branches to generate more precise prompts.  These branches improve the quality of the mask boundaries. The incorporation of SAM features into the prompt generation process further refines the accuracy. A cyclic consistent cross-attention mechanism is also implemented to filter out irrelevant feature points, thereby further increasing prompt precision and the quality of segmentation masks produced.", "section": "4 Proposed Method"}, {"figure_path": "https://arxiv.org/html/2504.12080/x4.png", "caption": "Figure 4: Illustration of our proposed cyclic consistent cross-attention mechanism. This figure shows the version applied to query features with one head. The \u201cCyc\u201d operation represents the process described in Equation\u00a019, which ultimately generates a bias to filter out features that are not cycle-consistent.", "description": "Figure 4 illustrates the cyclic consistent cross-attention mechanism, a key component of the proposed DC-SAM model.  It focuses on the process applied to query features using a single attention head. The diagram visually represents how the 'Cyc' operation (detailed in Equation 19 of the paper) functions. This operation calculates a bias that effectively filters out features which do not exhibit cycle-consistency, strengthening the alignment and relevance of features used for prompt generation. Cycle consistency, in this context, ensures a strong correspondence between features from the support and query images, improving the accuracy of mask generation.", "section": "4.3 Consistent Prompt Generation"}, {"figure_path": "https://arxiv.org/html/2504.12080/x5.png", "caption": "Figure 5: Comparison of SAM segmentation results with and without negative prompts. (a) Segmentation of the cage using only positive prompts. (b) Segmentation of the cage using both positive and negative prompts. Although not achieving optimal segmentation results, adding negative prompts allowed for better differentiation between the background, the dinosaur, and the cage, resulting in a significantly improved result.", "description": "This figure demonstrates the impact of using negative prompts in addition to positive prompts within the SAM (Segment Anything Model) framework.  The image shows a segmentation task where the goal is to segment a cage containing a dinosaur.  Subfigure (a) displays the results when using only positive prompts; the segmentation is somewhat imprecise and includes some background elements. Subfigure (b) illustrates the result when both positive and negative prompts are used.  The inclusion of negative prompts results in a significantly better differentiation between the cage, the dinosaur, and the background, leading to a more accurate and refined segmentation of the target object (the cage).  Even though the result isn't perfect, it showcases the effectiveness of negative prompts in improving segmentation accuracy by explicitly defining regions that should *not* be included in the segmentation mask.", "section": "4.3 Consistent Prompt Generation"}, {"figure_path": "https://arxiv.org/html/2504.12080/x6.png", "caption": "Figure 6: Illustration of our proposed DC-SAM framework with SAM2. Unlike the image-level framework, we train the entire model for the video to acquire the image-to-video prompt ability. We apply different data augmentation techniques to the query image, and the augmented images compose a mask tube for training.", "description": "This figure illustrates the architecture of the Dual Consistency Segment Anything Model (DC-SAM) adapted for video segmentation using the Segment Anything Model 2 (SAM2).  Unlike the image-only version of DC-SAM, the video version trains the entire model end-to-end.  The key modification is the use of data augmentation techniques on the input query video frames. These augmented frames are then assembled into a \"mask tube\", a temporal sequence of masks, which becomes the training data.  This mask tube approach allows the model to learn the temporal consistency and context needed for accurate video segmentation.", "section": "4 Proposed Method"}, {"figure_path": "https://arxiv.org/html/2504.12080/x7.png", "caption": "Figure 7: Comparison of segmentation results from different methods on the PASCAL-5isuperscript5\ud835\udc565^{i}5 start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT. Each row displays an RGB image along with its corresponding ground truth segmentation and the results of the four methods. Notable errors are marked with a yellow \u201c\u00d7\\times\u00d7\u201d.", "description": "Figure 7 presents a comparison of segmentation results obtained from four different methods on the PASCAL-5i dataset. Each row showcases an RGB image alongside its corresponding ground truth segmentation mask and the segmentation outputs generated by PerSAM, Matcher, VRP-SAM, and the proposed DC-SAM method.  Yellow 'x' markings highlight notable errors in the segmentation results, allowing for a visual assessment of the methods' accuracy and identifying areas where improvements are needed. The figure helps to illustrate the performance differences between the various techniques.", "section": "5.1 Main Results"}, {"figure_path": "https://arxiv.org/html/2504.12080/x8.png", "caption": "Figure 8: Visual comparisons of our proposed model with PFENet and VRP-SAM on our proposed benchmark. The support mask in the video indicates the semantic category of motorcycle, and all three models share the same support image and mask.", "description": "Figure 8 showcases a comparison of the performance of three different models\u2014DC-SAM (the proposed model), PFENet, and VRP-SAM\u2014on a video object segmentation task.  A video clip depicting a motorcycle is used as the query.  The support image and mask (identifying the semantic category 'motorcycle') are identical for all three models. This allows for a direct comparison of how effectively each model segments the motorcycle throughout the video sequence using only this single reference image and mask. The results highlight the superior performance of DC-SAM in accurately segmenting the motorcycle across the video frames compared to the other models.", "section": "5.2 Visual Comparisons"}, {"figure_path": "https://arxiv.org/html/2504.12080/x9.png", "caption": "Figure 9: Ablation study on the number of queries. The x\ud835\udc65xitalic_x-axis represents the number of queries in one branch. Note that since our DC-SAM consists of both positive and negative branches, the total number of queries is twice the number shown on the x\ud835\udc65xitalic_x-axis. The y\ud835\udc66yitalic_y-axis represents the model\u2019s performance. These experiments are conducted on the PASCAL-5isuperscript5\ud835\udc565^{i}5 start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT dataset.", "description": "This ablation study analyzes the effect of varying the number of queries used in the DC-SAM model's positive and negative branches on the model's performance. The x-axis shows the number of queries in a single branch (positive or negative), while the y-axis represents the model's mean Intersection over Union (mIoU) performance.  Since DC-SAM uses both positive and negative branches, the total number of queries is double the value shown on the x-axis.  The experiment was conducted on the PASCAL-5i dataset.", "section": "5.3 Ablation Studies on DC-SAM"}, {"figure_path": "https://arxiv.org/html/2504.12080/x10.png", "caption": "Figure 10: Visualization of two failure cases of our proposed DC-SAM on IC-VOS. We still find missing matching objects due to the occlusion (in the top) and multiple instance inputs with the fast motion (in the bottom).", "description": "Figure 10 shows two examples where the proposed DC-SAM model for in-context video object segmentation fails. The top row illustrates a case of occlusion, where parts of the target object are hidden from view, leading to an incomplete segmentation mask. The bottom row displays a scene with multiple instances of the same object class and fast motion. In this situation, the model struggles to accurately track and segment all instances throughout the video sequence. These failure cases highlight the challenges of in-context video object segmentation, particularly when dealing with occlusions or rapidly changing scenes.", "section": "5.2 Visual Comparisons"}, {"figure_path": "https://arxiv.org/html/2504.12080/x11.png", "caption": "Figure 11: Comparison of one-shot segmentation results on the PASCAL-5isuperscript5\ud835\udc565^{i}5 start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT dataset.", "description": "This figure presents a comparison of one-shot segmentation results obtained using four different methods: VRP-SAM, Per-SAM, Matcher, and the proposed DC-SAM. Each row displays an RGB image along with its corresponding ground truth segmentation and the segmentation results produced by each of the four methods.  The examples illustrate the relative performance of each model on various objects and scenes within the PASCAL-5isuperscript2 dataset.  'x' symbols highlight notable errors or inconsistencies.", "section": "5.2 Visual Comparisons"}, {"figure_path": "https://arxiv.org/html/2504.12080/x12.png", "caption": "Figure 12: Comparison of semantic segmentation results for the \u201cSheep\u201d category on the IC-VOS dataset.", "description": "This figure presents a comparison of semantic segmentation results for the 'Sheep' category obtained using three different methods (VRP-SAM, PFENet, and DC-SAM) on the IC-VOS dataset.  Each method's performance is visually shown across multiple frames of a video sequence where sheep are present.  This allows for a direct visual comparison of the accuracy and effectiveness of each method in segmenting sheep from their backgrounds, especially in video contexts that present challenges in object segmentation, such as changes in viewpoint or occlusion.", "section": "5.2 Visual Comparisons"}, {"figure_path": "https://arxiv.org/html/2504.12080/x13.png", "caption": "Figure 13: Comparison of semantic segmentation results for the \u201cDog\u201d category on the IC-VOS dataset.", "description": "This figure presents a visual comparison of semantic segmentation results for the \"Dog\" category from the IC-VOS dataset. It shows the results generated by three different models: DC-SAM, PFENet, and VRP-SAM, alongside the ground truth segmentation. Each row in the figure represents a frame from a video sequence, where the dog is the subject of segmentation.  The comparison allows for a visual assessment of each model's ability to accurately segment the dog across various frames within the video.  It highlights the performance differences between the models and showcases the effectiveness of DC-SAM in accurately delineating the dog across different frames and potential challenges such as changes in pose or presence of occlusions.", "section": "5.2 Visual Comparisons"}, {"figure_path": "https://arxiv.org/html/2504.12080/x14.png", "caption": "Figure 14: Comparison of semantic segmentation results for the \u201cCup\u201d category on the IC-VOS dataset.", "description": "Figure 14 presents a visual comparison of semantic segmentation results for the \"Cup\" category from the IC-VOS dataset.  It showcases how three different models\u2014DC-SAM, PFENet, and VRP-SAM\u2014perform on this task. The figure displays a series of frames from a video, demonstrating the models' ability to segment cups consistently throughout the video sequence. By comparing the model outputs to the ground truth, one can assess the accuracy and robustness of each model in handling variations in cup appearance and context within the video.", "section": "5.2 Visual Comparisons"}]