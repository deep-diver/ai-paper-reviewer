[{"figure_path": "https://arxiv.org/html/2504.17821/x3.png", "caption": "Figure 1: An example of Chinese Culture in VideoVista-CulturalLingo. The correct answer is highlighted in yellow.", "description": "This figure shows a sample question from the VideoVista-CulturalLingo benchmark dataset.  The question is presented in both Chinese and English, along with multiple-choice answers.  The video clip relates to Chinese culture, as indicated by the caption. The correct answer is highlighted in yellow to illustrate the benchmark's annotation process.  This example demonstrates how the dataset assesses the ability of multimodal AI systems to comprehend videos that are culturally and linguistically diverse, including the ability to reason about cultural contexts presented within the video.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2504.17821/x5.png", "caption": "Figure 3: The three-stage annotation process of VideoVista-CulturalLingo.", "description": "This figure illustrates the three stages of the VideoVista-CulturalLingo annotation process.  Stage 1 (Video Collecting & Preprocessing) shows how videos are sourced from various platforms (YouTube, Xiaohongshu, Bilibili), undergo audio extraction and transcription, and are segmented into shorter clips based on audio quality.  Stage 2 (Automatic QA Annotation) details how a hybrid approach is used, combining LLMs (Qwen2-VL, Qwen2.5-7B, DeepSeek-V3, DeepSeek-R1) and human annotators to generate question-answer pairs for several task categories (Event, Culture, Object, and Science). Stage 3 (Human Check & Revision) demonstrates the human quality control step involving review and refinement of the automatically generated questions and answers. The entire process aims to create a high-quality, culturally and linguistically diverse video benchmark.", "section": "3 VideoVista-CulturalLingo"}, {"figure_path": "https://arxiv.org/html/2504.17821/extracted/6383417/figures/Evaluation_Culture.png", "caption": "(a) Culture-based Evaluation Results.", "description": "The bar chart visualizes the performance of different Large Multimodal Models (LMMs) across various cultural backgrounds, comparing their accuracy in answering questions related to Chinese, American, and European cultures.  It demonstrates the models' ability to generalize across different cultural contexts and highlights potential biases in their understanding.", "section": "4.3 Detailed Analysis"}, {"figure_path": "https://arxiv.org/html/2504.17821/extracted/6383417/figures/Evaluation_Language.png", "caption": "(b) Language-based Evaluation Results.", "description": "This figure shows the performance of different Large Multimodal Models (LMMs) in processing videos with different languages.  The x-axis represents the LMMs being evaluated, and the y-axis represents the accuracy scores.  Different colored bars show the accuracy of each LMM for videos in English and Chinese. This allows for a direct comparison of model performance across different languages, illustrating any biases or strengths the models exhibit in processing either English or Chinese language content within videos.", "section": "4.3 Detailed Analysis"}, {"figure_path": "https://arxiv.org/html/2504.17821/extracted/6383417/figures/Evaluation_Duration.png", "caption": "(c) Duration-based Evaluation Results.", "description": "This figure shows the performance of different Large Multimodal Models (LMMs) on video comprehension tasks, broken down by the duration of the videos.  It illustrates how model accuracy changes as video length increases from short videos (under 2 minutes) to medium-length (2-10 minutes) and long videos (over 10 minutes).  This helps to understand whether LMMs handle longer videos effectively.", "section": "4.3 Detailed Analysis"}, {"figure_path": "https://arxiv.org/html/2504.17821/extracted/6383417/figures/Evaluation_Domain_ytb_crop.png", "caption": "Figure 4: The LMMs performance divided by Culture, Language and Duration. The Duration in (c): <2 minutes (Short), 2-10 minutes (Medium), >10 minutes (Long).", "description": "Figure 4 presents a detailed analysis of the Large Multimodal Models (LMMs) performance across three key aspects: Culture (Chinese vs. English), Language (Chinese vs. English), and Video Duration (<2 minutes, 2-10 minutes, >10 minutes).  The bar charts visually compare the accuracy scores of various LMMs on different subtasks within the benchmark, highlighting strengths and weaknesses in each category. This allows for a deeper understanding of how well different models handle various cultural nuances, language differences, and varying video lengths.", "section": "4.3 Detailed Analysis"}, {"figure_path": "https://arxiv.org/html/2504.17821/extracted/6383417/figures/Evaluation_domain_xhs_crop.png", "caption": "(a) Domains in YouTube", "description": "This figure shows a visualization of the domains represented in the YouTube videos of the VideoVista-CulturalLingo dataset.  The visualization likely uses a radial chart or similar representation to show the relative proportions or frequencies of different video categories or topics found in the dataset.  It provides a visual summary of the dataset's diversity in terms of subject matter, offering insights into the breadth of topics covered by the videos sourced from YouTube.", "section": "4 Experiment"}, {"figure_path": "https://arxiv.org/html/2504.17821/extracted/6383417/figures/Evaluation_Domain_bilibili_crop.png", "caption": "(b) Domains in Xiaohongshu", "description": "This figure shows the distribution of video domains in the Xiaohongshu dataset used for VideoVista-CulturalLingo.  Each segment represents a different domain, and the size of the segment is proportional to the number of videos in that domain. The chart offers a visualization of the diverse range of topics covered within the Xiaohongshu portion of the VideoVista-CulturalLingo benchmark.  It helps illustrate the breadth of cultural and topical coverage within the dataset.", "section": "4. Experiment"}, {"figure_path": "https://arxiv.org/html/2504.17821/x6.png", "caption": "(c) Domains in BiliBili", "description": "This figure shows the distribution of video domains from the Bilibili video-sharing platform within the VideoVista-CulturalLingo dataset.  The visualization likely uses a radial chart or similar to represent the variety of video topics.  The size of each segment might correspond to the number of videos belonging to that specific domain, indicating the relative prevalence of each category within Bilibili's contribution to the dataset. The domains themselves would be labeled on or near the segments.", "section": "4. Experiment"}, {"figure_path": "https://arxiv.org/html/2504.17821/extracted/6383417/figures/language_category.png", "caption": "Figure 5: The LMMs performance divided by domains from 3 video sources: Gemini-2.0-Flash, GPT-4o, Qwen2.5-VL-72B, VideoLLaMA3, InternVideo2.5, MiniCPM-o 2.6. In Figures\u00a05(a) and Figures\u00a05(b), we present only the 18 domains with the highest number of videos. In Figure\u00a05(c), we exclude domains containing fewer than 10 videos. The domains in these figures are represented by abbreviations, as described in Appendix\u00a0A.2.", "description": "Figure 5 displays a comparison of Large Multimodal Model (LMM) performance across various video domains sourced from three different platforms: YouTube, Xiaohongshu, and Bilibili.  The figure uses radar charts to visualize the performance of six models (Gemini-2.0-Flash, GPT-4o, Qwen2.5-VL-72B, VideoLLaMA3, InternVideo2.5, MiniCPM-o 2.6) on numerous video domains. For better clarity, only the top 18 most frequent domains are shown in the charts for YouTube and Xiaohongshu, while Bilibili domains with fewer than 10 videos are excluded.  Appendix A.2 provides a complete mapping of the domain abbreviations used in the charts.", "section": "4 Experiment"}, {"figure_path": "https://arxiv.org/html/2504.17821/x7.png", "caption": "Figure 6: Two cases from VideoVista-CulturalLingo.", "description": "This figure showcases two example questions from the VideoVista-CulturalLingo benchmark dataset.  The first question focuses on a Chinese dish, testing the model's ability to identify representative dishes of specific Chinese cuisines.  The second question tests cultural knowledge related to a food item, demonstrating how VideoVista-CulturalLingo incorporates cultural understanding into video comprehension tasks.", "section": "4.5 Case Study"}, {"figure_path": "https://arxiv.org/html/2504.17821/extracted/6383417/figures_appendix/example_merge.jpg", "caption": "(a) The statistics of 14 subtasks divided by languages.", "description": "This figure shows a bar chart visualizing the number of questions per subtask, broken down by language (Chinese and English).  It provides a quantitative comparison of the dataset's size and distribution across different question categories for each language.", "section": "3.4 Statistic and Analysis"}, {"figure_path": "https://arxiv.org/html/2504.17821/extracted/6383417/figures_appendix/Evaluation_Science.png", "caption": "(b) The statistics of duration of videos in VideoVista-CulturalLingo.", "description": "This figure shows the distribution of video lengths in the VideoVista-CulturalLingo dataset. The x-axis represents the duration of videos in minutes, categorized into bins (e.g., <1 minute, 1-2 minutes, etc.). The y-axis indicates the number of videos falling into each duration bin.  This visualization helps to understand the temporal characteristics of the videos in the dataset, highlighting the prevalence of shorter videos compared to longer ones.", "section": "3.1 Video Collecting and Preprocessing"}, {"figure_path": "https://arxiv.org/html/2504.17821/extracted/6383417/figures_appendix/Evaluation_Math.png", "caption": "Figure 7: (a) shows the quantity statistics for the 14 task categories under both Chinese and English languages. (b) presents the duration statistics of all video clips in VideoVista-CulturalLingo, measured in minutes.", "description": "Figure 7 presents a dual-aspect statistical overview of the VideoVista-CulturalLingo dataset. Subfigure (a) provides a quantitative breakdown of the 14 distinct task categories within the dataset, illustrating the number of questions posed in both Chinese and English languages for each task.  This visualization allows for a direct comparison of the dataset's linguistic balance across diverse question types. Subfigure (b) presents a histogram displaying the distribution of video clip durations (measured in minutes) across the entire dataset.  This illustrates the temporal diversity of the videos and helps assess the representation of various video lengths within the benchmark.", "section": "3.4 Statistic and Analysis"}, {"figure_path": "https://arxiv.org/html/2504.17821/extracted/6383417/figures_appendix/Evaluation_Frames.png", "caption": "Figure 8: An example of eight images combined in a horizontal layout.", "description": "This figure shows a horizontal arrangement of eight individual images.  These images are likely frames extracted from a video, possibly to illustrate the input format or preprocessing steps involved in video comprehension task. The combination of multiple frames may suggest the model's ability to handle longer sequences or the usage of sampling techniques to process large volumes of video data efficiently.", "section": "3 VideoVista-CulturalLingo"}, {"figure_path": "https://arxiv.org/html/2504.17821/extracted/6383417/figures_appendix/Evaluation_Audio.png", "caption": "(a) Science-based Evaluation Results.", "description": "The bar chart displays a comparison of the performance of various Large Multimodal Models (LMMs) across four scientific disciplines: Mathematics, Physics, Chemistry, and Computer Science.  It shows the accuracy scores achieved by each model on questions related to these disciplines. The chart likely highlights the relative strengths and weaknesses of different models in tackling scientific reasoning tasks, with a focus on the differences between open-source and proprietary models.", "section": "4.3 Detailed Analysis"}, {"figure_path": "https://arxiv.org/html/2504.17821/extracted/6383417/figures_appendix/Evaluation_Temporal.png", "caption": "(b) Math-based Evaluation Results.", "description": "This figure presents a detailed breakdown of the performance of various Large Multimodal Models (LMMs) across different mathematical sub-disciplines.  It shows a comparison between open-source and proprietary models, highlighting the performance discrepancies in areas like Calculus (English and Chinese), Linear Algebra (English), and Statistics and Probability (English). The graph likely uses bar charts to visually represent the accuracy scores of each model for each sub-discipline, allowing for a clear comparison of their relative strengths and weaknesses in different mathematical domains.", "section": "4.3 Detailed Analysis"}, {"figure_path": "https://arxiv.org/html/2504.17821/x8.png", "caption": "Figure 9: The Evaluation results in 4 disciplines and 4 math sub-disciplines. The experimental results in the figure represent the average values of the four scientific sub-tasks. In (a),we have list the four disciplines covered by the scientific videos in VideoVista-CulturalLingo: Math, Physics, Chemistry, and Computer Science ; In (b), we have listed four math sub-disciplines with a larger number of questions: Calculus (English), Linear Algebra (English), Statistics and Probability (English), and Calculus (Chinese)/Advanced Mathematics.", "description": "Figure 9 presents a detailed analysis of model performance on scientific tasks within the VideoVista-CulturalLingo benchmark.  It compares the performance of various Large Multimodal Models (LMMs) across four main scientific disciplines (Math, Physics, Chemistry, Computer Science) and four specialized math sub-disciplines (Calculus (English), Linear Algebra (English), Statistics and Probability (English), and Calculus (Chinese)/Advanced Mathematics).  The results shown are average scores across the four sub-tasks within each discipline/sub-discipline, highlighting strengths and weaknesses of different models in handling various types of scientific questions.", "section": "4 Experiment"}, {"figure_path": "https://arxiv.org/html/2504.17821/x9.png", "caption": "Figure 10: The Evaluation results divided by frames upper bound of Qwen2.5-VL-7B. We conducted experiments with four sampling methods at frame upper bound of 64, 128, 256, and 300 frames.", "description": "This figure presents the performance of the Qwen2.5-VL-7B model on the VideoVista-CulturalLingo benchmark across four different frame sampling rates: 64, 128, 256, and 300 frames. The x-axis represents the upper bound of frame sampling, while the y-axis shows the overall accuracy across different tasks in the benchmark. The figure demonstrates how the model's performance changes as the number of sampled frames increases, providing insight into the model's sensitivity to video length and the impact of temporal resolution on its accuracy.", "section": "4 Experiment"}, {"figure_path": "https://arxiv.org/html/2504.17821/x10.png", "caption": "Figure 11: The Evaluation results divided by whether input audio transcript into Qwen2.5-VL-7B. The audio transcript is extracted using Whisper-Large-V3.", "description": "This figure presents the performance comparison of the Qwen2.5-VL-7B model on the VideoVista-CulturalLingo benchmark with and without using audio transcripts. The audio transcripts were generated using Whisper-Large-V3. The results are shown for various sub-tasks across different categories, allowing analysis of how much audio information contributes to the model's performance in video understanding.  The x-axis represents the different tasks (Event Description, Event Prediction, Event Sequence, Event Localization, Object Temporal Localization, Object Temporal Sequence, Object Spatial Localization, Chinese Culture, American Culture, European Culture, Summarization and Synthesis, Comparison and Contrast, Application and Procedure, and Scientific Principle) while the y-axis indicates the accuracy scores.", "section": "4.3 Detailed Analysis"}, {"figure_path": "https://arxiv.org/html/2504.17821/x11.png", "caption": "Figure 12: The Evaluation results divided by whether input audio transcript into Qwen2.5-VL-7B. The audio transcript is extracted using Whisper-Large-V3.", "description": "This figure displays the performance comparison of the Qwen2.5-VL-7B language model on the VideoVista-CulturalLingo benchmark when including or excluding audio transcripts during the evaluation. The audio transcripts were generated using the Whisper-Large-V3 model.  The results are broken down by task (Event, Object, Culture, Science) to show how the inclusion of audio impacts the model's performance on different aspects of video understanding.", "section": "4.3 Detailed Analysis"}, {"figure_path": "https://arxiv.org/html/2504.17821/x12.png", "caption": "Figure 13: Prompt for Video Processing.", "description": "This figure shows the prompt used in the automatic video annotation pipeline for determining whether an audio transcript is noisy or not. The prompt instructs the model to identify noisy audio based on three criteria:  (1) excessive meaningless or worthless information (over 50% of the total), (2) excessive repetition, and (3) brevity with no practical value.  The output is expected in JSON format: {\"result\": \"noisy\" or \"not noisy\"}", "section": "3.2 Automatic QA Annotation"}, {"figure_path": "https://arxiv.org/html/2504.17821/x13.png", "caption": "Figure 14: Prompt for Audio Refine.", "description": "This figure shows the prompt used in the automatic audio annotation pipeline for refining noisy audio transcripts.  The prompt instructs the model to improve the fluency and coherence of the transcript while preserving its original semantics.  Specific instructions are given for handling Chinese homophones and ensuring the final transcript is free of grammatical errors.", "section": "3.2 Automatic QA Annotation"}, {"figure_path": "https://arxiv.org/html/2504.17821/extracted/6383417/figures_appendix/Annotator.png", "caption": "Figure 15: Prompt for Event Annotation.", "description": "This figure shows a detailed prompt used for event annotation in the VideoVista-CulturalLingo dataset. The prompt instructs annotators to analyze video clips and audio transcripts to identify and summarize the main event. It emphasizes accuracy, detail, context integration from both audio and video, and objectivity in the summary.  Specific instructions handle cases with unclear events, conflicting video and audio information, and sequences of clips.  The prompt also provides guidance for different types of videos, such as narration-based versus visually-rich videos.", "section": "3.2 Automatic QA Annotation"}, {"figure_path": "https://arxiv.org/html/2504.17821/x14.png", "caption": "Figure 16: Prompt for Event Description Quetions, Options and Answer Generation.", "description": "This figure shows the detailed prompt used for generating event description questions, options, and answers in the VideoVista-CulturalLingo dataset.  The prompt instructs the language model to analyze video clips and their corresponding audio transcripts to generate questions that are specific and detailed, with four options (one correct and three plausible incorrect answers).  The prompt emphasizes the importance of accuracy, clarity, diversity among the incorrect options, and avoiding ambiguity. It also addresses how to handle multi-lingual scenarios and provides specific formatting instructions for the model's output.", "section": "3.2 Automatic QA Annotation"}, {"figure_path": "https://arxiv.org/html/2504.17821/x15.png", "caption": "Figure 17: Prompt for Chinese Culture Quetions, Options and Answer Generation.", "description": "This figure shows the prompt used for automatically generating questions related to Chinese culture within the VideoVista-CulturalLingo benchmark.  The prompt instructs a large language model to generate questions that require both video content analysis and knowledge of Chinese culture to answer. It emphasizes that questions should indirectly refer to cultural concepts without explicit naming and options should avoid ambiguity or overlap.", "section": "3.2 Automatic QA Annotation"}, {"figure_path": "https://arxiv.org/html/2504.17821/x16.png", "caption": "Figure 18: Prompt for Scientific Principle Quetions, Options and Answer Generation.", "description": "This figure shows a detailed prompt used for automatically generating questions and answers related to scientific principles.  The prompt is designed to create questions that assess the understanding of fundamental scientific concepts demonstrated in videos showing chemistry or physics experiments.  It explicitly instructs the model to avoid technical jargon, use general terminology, and include contextual cues. The response format requires four options for each question: a correct answer and three incorrect answers, representing a video comprehension error, a domain knowledge error, and a dual error, respectively.", "section": "D. Detailed Annotations Pipeline"}, {"figure_path": "https://arxiv.org/html/2504.17821/x17.png", "caption": "Figure 19: Gradio Interface for scoring.", "description": "This figure shows a screenshot of the Gradio interface used for human evaluation of the generated questions.  The interface displays the video clip, the question, its options, and a rating scale for correctness, type relevance, and video relevance.  Annotators enter their name, watch the video, select the correct answer, and assign scores using a slider for each of the three relevance dimensions.  The screenshot also shows the Wikipedia Entry for context, particularly useful for cultural questions.", "section": "3.3 Human Check and Revision"}, {"figure_path": "https://arxiv.org/html/2504.17821/x18.png", "caption": "Figure 20: An Example of Event Description from VideoVista-CulturalLingo.", "description": "This figure shows an example question from the VideoVista-CulturalLingo benchmark dataset. The question is in Chinese and asks how a woman folds a piece of paper in the early part of a video. Four multiple-choice options are provided, describing different folding sequences. This illustrates the Event Description task in the benchmark, which focuses on describing the events in the video clips.", "section": "3 VideoVista-CulturalLingo"}, {"figure_path": "https://arxiv.org/html/2504.17821/x19.png", "caption": "Figure 21: An Example of Event Prediction from VideoVista-CulturalLingo.", "description": "This figure shows an example question from the VideoVista-CulturalLingo benchmark dataset's Event Prediction task.  The question presented is in Chinese and asks what the woman in the video will do next after showing chicken feet.  Four multiple-choice options are provided, each describing a different potential action (trying Maotai jelly, braised pork, pork belly gummies, or ending the video). The image displays several video stills from the relevant section of the video.", "section": "3 VideoVista-CulturalLingo"}, {"figure_path": "https://arxiv.org/html/2504.17821/x20.png", "caption": "Figure 22: An Example of Event Sequence from VideoVista-CulturalLingo.", "description": "This figure shows a multiple choice question from the VideoVista-CulturalLingo benchmark dataset.  The question tests the model's ability to understand and order a sequence of events presented in a news report. The correct answer requires understanding the temporal relationships between different news segments (a fire in Paris, a school cafeteria renovation, a landslide in China, and a weather update) and arranging them in the correct chronological order as presented by a news anchor.", "section": "3 VideoVista-CulturalLingo"}, {"figure_path": "https://arxiv.org/html/2504.17821/x21.png", "caption": "Figure 23: An Example of Event Localization from VideoVista-CulturalLingo.", "description": "This figure shows an example question from the VideoVista-CulturalLingo benchmark dataset.  The question focuses on event localization, specifically asking at what time a particular event (tasting pizza) begins within the video.  Multiple-choice options with timestamps are provided for the user or model to select the correct answer. This highlights the dataset's capability of testing temporal understanding in videos, in particular the accuracy of models in identifying the precise start time of specific events.", "section": "3 VideoVista-CulturalLingo"}, {"figure_path": "https://arxiv.org/html/2504.17821/x22.png", "caption": "Figure 24: An Example of Object Temporal Localization from VideoVista-CulturalLingo.", "description": "This figure shows an example question from the VideoVista-CulturalLingo benchmark dataset, specifically focusing on the Object Temporal Localization task.  The question asks the user to identify the timestamp at which a particular object (in this case, a statue of Dr. Julius Kugy) first appears within a video. Four timestamps are provided as options, and a visual representation of video frames helps the user contextualize the event. This task evaluates the model's ability to pinpoint the precise moment an object appears in a video, testing its temporal understanding and localization capabilities.", "section": "3 VideoVista-CulturalLingo"}, {"figure_path": "https://arxiv.org/html/2504.17821/x23.png", "caption": "Figure 25: An Example of Object Temporal Sequence from VideoVista-CulturalLingo.", "description": "This figure shows an example question from the VideoVista-CulturalLingo benchmark dataset, specifically focusing on the 'Object Temporal Sequence' task.  The question asks the user to identify the correct order in which several ingredients (tomato, raw minced meat, white sugar, and soy sauce) appear in a video.  The visual element displays four image frames from the video to aid in answering the question, highlighting the task's reliance on understanding both the visual and temporal sequence of the presented ingredients.", "section": "3 VideoVista-CulturalLingo"}, {"figure_path": "https://arxiv.org/html/2504.17821/x24.png", "caption": "Figure 26: An Example of Object Spatial Localization from VideoVista-CulturalLingo.", "description": "This figure shows an example question from the VideoVista-CulturalLingo benchmark dataset that tests object spatial localization.  The question asks the model to identify the location of a specific object (a silver trophy with red, white, and blue ribbons) within a video frame at a particular time (4 seconds). The correct answer is presented as a bounding box in xyxy format, normalized to the range [0, 1], specifying the coordinates of the object's location within the image.", "section": "3.1 Video Collecting and Preprocessing"}, {"figure_path": "https://arxiv.org/html/2504.17821/x25.png", "caption": "Figure 27: An Example of Chinese Culture from VideoVista-CulturalLingo.", "description": "This figure shows an example question from the VideoVista-CulturalLingo benchmark dataset, specifically focusing on the Chinese Culture category.  The image displays a video still showing a dish and a multiple-choice question in Chinese asking about the main regions where that dish is popular. This demonstrates the dataset's ability to assess the understanding of cultural nuances in different regions of China. The correct answer is highlighted.", "section": "3 VideoVista-CulturalLingo"}, {"figure_path": "https://arxiv.org/html/2504.17821/x26.png", "caption": "Figure 28: An Example of American Culture from VideoVista-CulturalLingo.", "description": "Figure 28 shows a multiple-choice question from the VideoVista-CulturalLingo benchmark dataset.  The question tests the model's understanding of American culture by asking who directed a particular movie shown in a video clip. This exemplifies the dataset's ability to evaluate multimodal AI systems' cultural knowledge beyond simple visual recognition.", "section": "3 VideoVista-CulturalLingo"}, {"figure_path": "https://arxiv.org/html/2504.17821/x27.png", "caption": "Figure 29: An Example of European Culture from VideoVista-CulturalLingo.", "description": "This figure shows a multiple-choice question from the VideoVista-CulturalLingo benchmark dataset, focusing on European culture.  The question asks which country a specific beverage is primarily associated with in terms of anti-social behavior.  The video clip associated with the question is briefly shown. The correct answer is highlighted (Scotland, in this case), showcasing how the benchmark assesses cultural understanding within a specific regional context.", "section": "3 VideoVista-CulturalLingo"}]