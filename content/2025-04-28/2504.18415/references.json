{"references": [{"fullname_first_author": "Hongyu Wang", "paper_title": "Bitnet: Scaling 1-bit transformers for large language models", "publication_date": "2023-10-01", "reason": "This paper introduces BitNet, which is the foundation and main comparison point for the current work on BitNet v2."}, {"fullname_first_author": "Shuming Ma", "paper_title": "The era of 1-bit llms: All large language models are in 1.58 bits", "publication_date": "2024-02-01", "reason": "This paper builds upon the initial BitNet paper and serves as direct prior work, establishing the context for performance improvements and design choices made in BitNet v2."}, {"fullname_first_author": "Yoshua Bengio", "paper_title": "Estimating or propagating gradients through stochastic neurons for conditional computation", "publication_date": "2013-08-01", "reason": "This paper introduces the Straight-Through Estimator (STE), which is a crucial technique used for gradient approximation in BitNet v2's training process for quantizing activations."}, {"fullname_first_author": "Leo Gao", "paper_title": "A framework for few-shot language model evaluation", "publication_date": "2024-07-01", "reason": "This paper discusses evaluation, which shows that the toolkit is used to evaluate the performance of BitNet v2 and related models on various language tasks."}, {"fullname_first_author": "Colin Raffel", "paper_title": "Exploring the limits of transfer learning with a unified text-to-text transformer", "publication_date": "2019-10-01", "reason": "This work describes the dataset on which models were trained, providing details on the scale and preparation for training."}]}