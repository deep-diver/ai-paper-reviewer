[{"figure_path": "https://arxiv.org/html/2504.18415/x1.png", "caption": "(a) \ud835\udc16osubscript\ud835\udc16o\\mathbf{W}_{\\text{o}}bold_W start_POSTSUBSCRIPT o end_POSTSUBSCRIPT of BitNet b1.58", "description": "This figure shows the distribution of the activation weights of the output projection matrix (Wo) in the attention mechanism of the BitNet b1.58 model.  The plot visualizes the frequency or density of different weight values, offering insight into the distribution's characteristics. This is important because the distribution of activation weights significantly impacts the model's performance and efficiency, particularly when using low-bit quantization.", "section": "BitNet v2: Native 4-bit Activations"}, {"figure_path": "https://arxiv.org/html/2504.18415/x2.png", "caption": "(b) \ud835\udc16downsubscript\ud835\udc16down\\mathbf{W}_{\\text{down}}bold_W start_POSTSUBSCRIPT down end_POSTSUBSCRIPT of BitNet b1.58", "description": "The figure shows the distribution of the down projection weights (\ud835\udc16downsubscript\ud835\udc16down\\mathbf{W}_{\\text{down}}) in the feed-forward network (FFN) of the BitNet b1.58 model.  It illustrates the magnitude of these weights across different dimensions, providing insight into the model's internal representation and potentially highlighting areas of sparsity or concentration of weight values. This visualization is helpful for understanding the weight distribution and its influence on model performance, particularly in relation to the quantization techniques employed by the model.", "section": "2 BitNet v2: Native 4-bit Activations"}, {"figure_path": "https://arxiv.org/html/2504.18415/x3.png", "caption": "(c) \ud835\udc16osubscript\ud835\udc16o\\mathbf{W}_{\\text{o}}bold_W start_POSTSUBSCRIPT o end_POSTSUBSCRIPT of BitNet v2", "description": "This figure shows the distribution of the activation of the output projection matrix \ud835\udc16o (W_o) in the attention layer of BitNet v2.  It provides a visualization of the activation values, allowing for an understanding of the data distribution's characteristics.  Comparing this to other visualizations of activation distributions (like those from BitNet b1.58) allows for a visual demonstration of how the Hadamard transformation affects the distribution, resulting in a more Gaussian-like shape suitable for low-bit quantization.", "section": "2 BitNet v2: Native 4-bit Activations"}, {"figure_path": "https://arxiv.org/html/2504.18415/x4.png", "caption": "(d) \ud835\udc16downsubscript\ud835\udc16down\\mathbf{W}_{\\text{down}}bold_W start_POSTSUBSCRIPT down end_POSTSUBSCRIPT of BitNet v2", "description": "The figure shows the distribution of the down projection matrix (\ud835\udc16downsubscript\ud835\udc16down\\mathbf{W}_{\\text{down}}) in the feed-forward network (FFN) of BitNet v2.  This visualization helps illustrate the impact of the Hadamard transformation applied in BitNet v2's H-BitLinear module, which aims to smooth the activation distributions and reduce the number of outlier channels. By comparing this distribution to the distribution of the same weight matrix in previous models (such as BitNet b1.58, shown in other parts of the figure), one can understand the effectiveness of BitNet v2's outlier mitigation strategy.", "section": "2 BitNet v2: Native 4-bit Activations"}, {"figure_path": "https://arxiv.org/html/2504.18415/x5.png", "caption": "Figure 1: Top: Overview of BitNet v2 and \u210b\u210b\\mathcal{H}caligraphic_H-BitLinear. Bottom: The distribution of the activation of output projection \ud835\udc16osubscript\ud835\udc16o\\mathbf{W}_{\\text{o}}bold_W start_POSTSUBSCRIPT o end_POSTSUBSCRIPT in attention and down projection \ud835\udc16downsubscript\ud835\udc16down\\mathbf{W}_{\\text{down}}bold_W start_POSTSUBSCRIPT down end_POSTSUBSCRIPT in FFN. BitNet v2 utilizes \u210b\u210b\\mathcal{H}caligraphic_H-BitLinear to eliminate the large amount of outlier channels in the intermediate states. The Hadamard transformation reshapes the original sharp distribution into a more Gaussian-like form.", "description": "This figure illustrates the architecture of BitNet v2 and its core component, H-BitLinear. The top part shows a schematic overview of BitNet v2, highlighting the placement of H-BitLinear modules within the attention and feed-forward network (FFN) layers.  The bottom part presents a comparison of activation distributions before and after the Hadamard transformation in H-BitLinear.  Specifically, it shows the distributions of activations from the output projection (Wo) in the attention layer and the down projection (Wdown) in the FFN layer for both BitNet v1.58 (for comparison) and BitNet v2.  The plots visually demonstrate how H-BitLinear, using a Hadamard transform, effectively reduces outliers, transforming the sharp, non-Gaussian activation distributions into smoother, Gaussian-like distributions better suited for low-bit quantization, which is crucial for efficient deployment of 1-bit LLMs.", "section": "BitNet v2: Native 4-bit Activations"}, {"figure_path": "https://arxiv.org/html/2504.18415/", "caption": "(a) \ud835\udc16qkvsubscript\ud835\udc16qkv\\mathbf{W}_{\\text{qkv}}bold_W start_POSTSUBSCRIPT qkv end_POSTSUBSCRIPT of BitNet b1.58", "description": "This figure shows the distribution of the activation values for the query, key, and value weight matrix (Wqkv) in the attention layer of the BitNet b1.58 model.  It displays the distribution of the absolute values of the weights across the different tokens and dimensions. This visualization helps in understanding the distribution characteristics of the weight matrix in relation to the model's performance and its suitability for quantization.", "section": "2 BitNet v2: Native 4-bit Activations"}, {"figure_path": "https://arxiv.org/html/2504.18415/x7.png", "caption": "(b) \ud835\udc16osubscript\ud835\udc16o\\mathbf{W}_{\\text{o}}bold_W start_POSTSUBSCRIPT o end_POSTSUBSCRIPT of BitNet b1.58", "description": "This figure shows the distribution of the activation of the output projection (Wo) in the attention mechanism of the BitNet b1.58 model.  The distribution is visualized as a 3D histogram, showing the frequency of different activation values across various tokens and dimensions. This visualization is important for understanding the impact of outliers on the effectiveness of low-bit quantization in the model.", "section": "BitNet v2: Native 4-bit Activations"}, {"figure_path": "https://arxiv.org/html/2504.18415/x8.png", "caption": "(c) \ud835\udc16up,gatesubscript\ud835\udc16up,gate\\mathbf{W}_{\\text{up,gate}}bold_W start_POSTSUBSCRIPT up,gate end_POSTSUBSCRIPT of BitNet b1.58", "description": "Figure 2 shows the activation distribution of both BitNet b1.58 and BitNet v2, using 8-bit activations.  It displays the distribution of activations for four different weight matrices within the models: Wqkv (query, key, value), Wo (output projection in attention), Wup,gate (gate projection in feed-forward network), and Wdown (down projection in feed-forward network).  The plots illustrate how the Hadamard transformation in BitNet v2 alters the distribution, making it more Gaussian-like and less prone to outliers compared to BitNet b1.58. This is especially evident in the plots for Wo and Wdown, which show a significant reduction in outlier channels after the transformation.", "section": "2 BitNet v2: Native 4-bit Activations"}, {"figure_path": "https://arxiv.org/html/2504.18415/x9.png", "caption": "(d) \ud835\udc16downsubscript\ud835\udc16down\\mathbf{W}_{\\text{down}}bold_W start_POSTSUBSCRIPT down end_POSTSUBSCRIPT of BitNet b1.58", "description": "The figure shows the distribution of the weights in the down projection layer (\ud835\udc16downsubscript\ud835\udc16down\n\\mathbf{W}_{\\text{down}}) of the BitNet b1.58 model.  The distribution is visualized as a 3D histogram, illustrating the frequency of different weight values across various dimensions of the weight matrix. This visualization helps understand the characteristics of the weight distribution and how it might impact the model's performance and quantization.", "section": "2 BitNet v2: Native 4-bit Activations"}, {"figure_path": "https://arxiv.org/html/2504.18415/x10.png", "caption": "(e) \ud835\udc16qkvsubscript\ud835\udc16qkv\\mathbf{W}_{\\text{qkv}}bold_W start_POSTSUBSCRIPT qkv end_POSTSUBSCRIPT of BitNet v2", "description": "The figure shows the distribution of the activation of the output projection  for the query, key, and value matrices (Wqkv) in the attention mechanism of BitNet v2.  This visualization helps illustrate how the Hadamard transformation within the H-BitLinear module reshapes the distribution, reducing outliers and making it more suitable for 4-bit quantization. The x-axis represents the number of tokens, and the y-axis represents the dimension of the matrix. The color intensity shows the absolute value of the activation values.", "section": "2 BitNet v2: Native 4-bit Activations"}, {"figure_path": "https://arxiv.org/html/2504.18415/x11.png", "caption": "(f) \ud835\udc16osubscript\ud835\udc16o\\mathbf{W}_{\\text{o}}bold_W start_POSTSUBSCRIPT o end_POSTSUBSCRIPT of BitNet v2", "description": "The figure shows the distribution of the activation of the output projection matrix (Wo) in the attention layer of BitNet v2.  It visualizes the values of the matrix elements and their frequency to illustrate the impact of the Hadamard transformation on the activation distribution.  In particular, it demonstrates how H-BitLinear (the module using the Hadamard transformation) reshapes the activation distribution into a more Gaussian-like form, reducing the number of outliers compared to the original sharp distribution observed in previous models like BitNet b1.58.", "section": "2 BitNet v2: Native 4-bit Activations"}, {"figure_path": "https://arxiv.org/html/2504.18415/x12.png", "caption": "(g) \ud835\udc16up,gatesubscript\ud835\udc16up,gate\\mathbf{W}_{\\text{up,gate}}bold_W start_POSTSUBSCRIPT up,gate end_POSTSUBSCRIPT of BitNet v2", "description": "Figure 2(g) displays the distribution of the activation values for the \tWup,gate matrix in the BitNet v2 model.  \tWup,gate represents the weight matrix used in the gating mechanism of the feed-forward network within the BitNet v2 architecture. The figure shows a histogram or density plot illustrating the frequency or probability of different activation values within the \tWup,gate matrix. This visualization helps to understand the distribution of activations and how it may have been affected by the Hadamard transformation method implemented in the BitNet v2 model.", "section": "2 BitNet v2: Native 4-bit Activations"}, {"figure_path": "https://arxiv.org/html/2504.18415/x13.png", "caption": "(h) \ud835\udc16downsubscript\ud835\udc16down\\mathbf{W}_{\\text{down}}bold_W start_POSTSUBSCRIPT down end_POSTSUBSCRIPT of BitNet v2", "description": "This figure shows the distribution of the down projection weights (\ud835\udc16downsubscript\ud835\udc16down\\mathbf{W}_{\\text{down}}) in the feed-forward network of BitNet v2.  It illustrates the impact of the Hadamard transformation within the H-BitLinear module on the distribution of activation values. By comparing this to the distribution in other versions of BitNet (shown in other subfigures), we can see how H-BitLinear effectively reshapes the distribution, mitigating the effect of outliers and making it more suitable for low-bit quantization. This is a crucial aspect of BitNet v2's ability to achieve 4-bit activation quantization without significant performance loss.", "section": "2 BitNet v2: Native 4-bit Activations"}, {"figure_path": "https://arxiv.org/html/2504.18415/x14.png", "caption": "Figure 2: The activation distribution of BitNet b1.58 and BitNet v2 with 8-bit activations.", "description": "Figure 2 presents a comparison of activation distributions in BitNet b1.58 and BitNet v2, both using 8-bit activations.  It shows the distribution of activations for four key weight matrices: Wqkv and Wo in the attention mechanism, and Wup,gate and Wdown in the feed-forward network (FFN). The plots visually demonstrate how the Hadamard transformation in BitNet v2 effectively reshapes the original distributions, which contain many outliers, into smoother, more Gaussian-like distributions. This transformation is crucial for enabling efficient low-bit quantization of activations in BitNet v2.", "section": "2 BitNet v2: Native 4-bit Activations"}, {"figure_path": "https://arxiv.org/html/2504.18415/x15.png", "caption": "(a) \ud835\udc16downsubscript\ud835\udc16down\\mathbf{W}_{\\text{down}}bold_W start_POSTSUBSCRIPT down end_POSTSUBSCRIPT of BitNet b1.58", "description": "Figure 1(b) shows the distribution of the weights in the down projection layer (\ud835\udc16downsubscript\ud835\udc16down\\mathbf{W}_{\\text{down}}) of the BitNet b1.58 model.  The x-axis represents the number of tokens, the y-axis represents the number of dimensions, and the color intensity represents the absolute value of the weights. This visualization helps understand the distribution of weights in the network and its potential impact on model performance and efficiency.", "section": "BitNet v2: Native 4-bit Activations"}, {"figure_path": "https://arxiv.org/html/2504.18415/x16.png", "caption": "(b) \ud835\udc16downsubscript\ud835\udc16down\\mathbf{W}_{\\text{down}}bold_W start_POSTSUBSCRIPT down end_POSTSUBSCRIPT of BitNet v2", "description": "The figure shows the distribution of the down projection matrix (\ud835\udc16downsubscript\ud835\udc16down\n\\mathbf{W}_{\\text{down}}) in the feed-forward network of BitNet v2.  This visualization helps illustrate the impact of the Hadamard transformation within the H-BitLinear module on the activation distribution. By comparing this distribution to that of BitNet b1.58 (shown in another figure), one can observe how H-BitLinear effectively reduces outliers, making the distribution more Gaussian-like and better suited for low-bit quantization.", "section": "2 BitNet v2: Native 4-bit Activations"}, {"figure_path": "https://arxiv.org/html/2504.18415/x17.png", "caption": "(c) \ud835\udc16osubscript\ud835\udc16o\\mathbf{W}_{\\text{o}}bold_W start_POSTSUBSCRIPT o end_POSTSUBSCRIPT of BitNet b1.58", "description": "This figure shows the distribution of the activation of output projection Wo in the attention mechanism of the BitNet b1.58 model. It provides insights into the nature of activations in this specific layer of the model and how they might be affected by quantization.", "section": "2 BitNet v2: Native 4-bit Activations"}]