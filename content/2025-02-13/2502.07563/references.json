{"references": [{"fullname_first_author": "Vaswani et al.", "paper_title": "Attention is all you need", "publication_date": "2017-MM-DD", "reason": "This paper introduced the Transformer architecture, which is the foundation of many modern sequence modeling approaches, including linear attention."}, {"fullname_first_author": "Katharopoulos et al.", "paper_title": "Transformers are RNNs: Fast autoregressive transformers with linear attention", "publication_date": "2020-MM-DD", "reason": "This paper introduced linear attention, a key concept that forms the basis of many recent advancements in linear attention."}, {"fullname_first_author": "Dao et al.", "paper_title": "FlashAttention: Fast and memory-efficient exact attention with io-awareness", "publication_date": "2022-MM-DD", "reason": "This paper proposed FlashAttention, a significant advancement in scaling attention mechanisms to handle very long sequences."}, {"fullname_first_author": "Sun et al.", "paper_title": "Linear attention sequence parallelism", "publication_date": "2024-MM-DD", "reason": "This paper introduced LASP, the predecessor of LASP-2, which established a foundation for sequence parallelism in linear attention."}, {"fullname_first_author": "Dubey et al.", "paper_title": "The Llama 3 herd of models", "publication_date": "2024-MM-DD", "reason": "This paper introduced the Llama 3 model, which serves as the basis for the Linear-Llama3 model used in the experiments for evaluating LASP-2."}]}