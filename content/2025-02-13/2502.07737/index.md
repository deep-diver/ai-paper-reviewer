---
title: "Next Block Prediction: Video Generation via Semi-Autoregressive Modeling"
summary: "Next-Block Prediction (NBP) revolutionizes video generation by using a semi-autoregressive model that predicts blocks of video content simultaneously, resulting in significantly faster inference."
categories: ["AI Generated", "ü§ó Daily Papers"]
tags: ["Computer Vision", "Video Understanding", "üè¢ Peking University",]
showSummary: true
date: 2025-02-11
draft: false
---

<br>

{{< keywordList >}}
{{< keyword icon="fingerprint" >}} 2502.07737 {{< /keyword >}}
{{< keyword icon="writer" >}} Shuhuai Ren et el. {{< /keyword >}}
 
{{< keyword >}} ü§ó 2025-02-13 {{< /keyword >}}
 
{{< /keywordList >}}

{{< button href="https://arxiv.org/abs/2502.07737" target="_self" >}}
‚Üó arXiv
{{< /button >}}
{{< button href="https://huggingface.co/papers/2502.07737" target="_self" >}}
‚Üó Hugging Face
{{< /button >}}



<audio controls>
    <source src="https://ai-paper-reviewer.com/2502.07737/podcast.wav" type="audio/wav">
    Your browser does not support the audio element.
</audio>


### TL;DR


{{< lead >}}

Autoregressive models have become the standard for video generation, but they suffer from slow inference speeds due to their sequential, token-by-token generation process.  This approach often struggles to capture spatial dependencies effectively within video frames.  Prior work has attempted to resolve this by using methods like multi-token prediction, but these methods either introduce additional modules or impose significant constraints, thus hindering their effectiveness and scalability.



This paper introduces a novel semi-autoregressive framework called Next-Block Prediction (NBP) that addresses these limitations. Instead of predicting individual tokens, NBP predicts entire blocks of tokens simultaneously.  This parallelization significantly reduces the number of generation steps, leading to faster inference.  NBP also employs bidirectional attention within each block to better capture spatial dependencies. Experiments demonstrate that NBP significantly outperforms existing methods in terms of both speed and generation quality, achieving speed-ups exceeding 11 times on standard benchmarks.

{{< /lead >}}


#### Key Takeaways

{{< alert "star" >}}
{{< typeit speed=10 lifeLike=true >}} Next-Block Prediction (NBP) significantly speeds up video generation compared to traditional autoregressive methods. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=1000 lifeLike=true >}} NBP achieves high-quality video generation results, outperforming the traditional approach on standard benchmarks. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=2000 lifeLike=true >}} NBP demonstrates strong scalability, providing flexibility for different computational resources. {{< /typeit >}}
{{< /alert >}}

#### Why does it matter?
This paper is crucial because it presents a novel approach to video generation, significantly improving efficiency and scalability compared to existing methods.  Its semi-autoregressive framework and next-block prediction strategy offer a **new avenue for research** in video generation, potentially impacting applications like video editing, special effects, and AI-driven content creation. The results demonstrate **substantial speed improvements**, paving the way for more efficient and powerful video generation models. Moreover, the scalability of the method makes it adaptable to various model sizes, offering **flexibility for researchers** with different computational resources. This advancement is particularly important in the context of increasing demand for high-quality, computationally efficient video generation.

------
#### Visual Insights



![](https://arxiv.org/html/2502.07737/x1.png)

> üîº This figure illustrates the 3D discrete token map generated by the video tokenizer used in the paper.  The input video is processed as a sequence. It begins with a single initial frame, followed by a series of clips, each containing a fixed number of frames (FT). Each element in the 3D map, represented as xj(i), signifies a specific video token. The indices (i) and (j) denote the clip number and the token's position within that clip respectively.
> <details>
> <summary>read the caption</summary>
> Figure 1: 3D discrete token map produced by our video tokenizer. The input video consists of one initial frame, followed by nùëõnitalic_n clips, with each clip containing FTsubscriptùêπùëáF_{T}italic_F start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT frames. xj(i)subscriptsuperscriptùë•ùëñùëóx^{(i)}_{j}italic_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT indicates the jt‚Å¢hsuperscriptùëóùë°‚Ñéj^{th}italic_j start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT video token in the it‚Å¢hsuperscriptùëñùë°‚Ñéi^{th}italic_i start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT clip.
> </details>





{{< table-caption >}}
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T3.13">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T3.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.3.3.4">Block Size</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.2.2.2">Block Shape (T<math alttext="\times" class="ltx_Math" display="inline" id="S4.T3.1.1.1.m1.1"><semantics id="S4.T3.1.1.1.m1.1a"><mo id="S4.T3.1.1.1.m1.1.1" xref="S4.T3.1.1.1.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.m1.1b"><times id="S4.T3.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T3.1.1.1.m1.1d">√ó</annotation></semantics></math>H<math alttext="\times" class="ltx_Math" display="inline" id="S4.T3.2.2.2.m2.1"><semantics id="S4.T3.2.2.2.m2.1a"><mo id="S4.T3.2.2.2.m2.1.1" xref="S4.T3.2.2.2.m2.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S4.T3.2.2.2.m2.1b"><times id="S4.T3.2.2.2.m2.1.1.cmml" xref="S4.T3.2.2.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.2.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T3.2.2.2.m2.1d">√ó</annotation></semantics></math>W)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.3.3.3">FVD<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T3.3.3.3.m1.1"><semantics id="S4.T3.3.3.3.m1.1a"><mo id="S4.T3.3.3.3.m1.1.1" stretchy="false" xref="S4.T3.3.3.3.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S4.T3.3.3.3.m1.1b"><ci id="S4.T3.3.3.3.m1.1.1.cmml" xref="S4.T3.3.3.3.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.3.3.3.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.3.3.3.m1.1d">‚Üì</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.5.5">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.5.5.3">16</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.5.5.2">1<math alttext="\times" class="ltx_Math" display="inline" id="S4.T3.4.4.1.m1.1"><semantics id="S4.T3.4.4.1.m1.1a"><mo id="S4.T3.4.4.1.m1.1.1" xref="S4.T3.4.4.1.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S4.T3.4.4.1.m1.1b"><times id="S4.T3.4.4.1.m1.1.1.cmml" xref="S4.T3.4.4.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.4.4.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T3.4.4.1.m1.1d">√ó</annotation></semantics></math>4<math alttext="\times" class="ltx_Math" display="inline" id="S4.T3.5.5.2.m2.1"><semantics id="S4.T3.5.5.2.m2.1a"><mo id="S4.T3.5.5.2.m2.1.1" xref="S4.T3.5.5.2.m2.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S4.T3.5.5.2.m2.1b"><times id="S4.T3.5.5.2.m2.1.1.cmml" xref="S4.T3.5.5.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.5.5.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T3.5.5.2.m2.1d">√ó</annotation></semantics></math>4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.5.5.4">33.4</td>
</tr>
<tr class="ltx_tr" id="S4.T3.7.7">
<td class="ltx_td ltx_align_center" id="S4.T3.7.7.3">16</td>
<td class="ltx_td ltx_align_center" id="S4.T3.7.7.2">2<math alttext="\times" class="ltx_Math" display="inline" id="S4.T3.6.6.1.m1.1"><semantics id="S4.T3.6.6.1.m1.1a"><mo id="S4.T3.6.6.1.m1.1.1" xref="S4.T3.6.6.1.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S4.T3.6.6.1.m1.1b"><times id="S4.T3.6.6.1.m1.1.1.cmml" xref="S4.T3.6.6.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.6.6.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T3.6.6.1.m1.1d">√ó</annotation></semantics></math>1<math alttext="\times" class="ltx_Math" display="inline" id="S4.T3.7.7.2.m2.1"><semantics id="S4.T3.7.7.2.m2.1a"><mo id="S4.T3.7.7.2.m2.1.1" xref="S4.T3.7.7.2.m2.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S4.T3.7.7.2.m2.1b"><times id="S4.T3.7.7.2.m2.1.1.cmml" xref="S4.T3.7.7.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.7.7.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T3.7.7.2.m2.1d">√ó</annotation></semantics></math>8</td>
<td class="ltx_td ltx_align_center" id="S4.T3.7.7.4">29.2</td>
</tr>
<tr class="ltx_tr" id="S4.T3.9.9">
<td class="ltx_td ltx_align_center" id="S4.T3.9.9.3">16</td>
<td class="ltx_td ltx_align_center" id="S4.T3.9.9.2">1<math alttext="\times" class="ltx_Math" display="inline" id="S4.T3.8.8.1.m1.1"><semantics id="S4.T3.8.8.1.m1.1a"><mo id="S4.T3.8.8.1.m1.1.1" xref="S4.T3.8.8.1.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S4.T3.8.8.1.m1.1b"><times id="S4.T3.8.8.1.m1.1.1.cmml" xref="S4.T3.8.8.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.8.8.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T3.8.8.1.m1.1d">√ó</annotation></semantics></math>1<math alttext="\times" class="ltx_Math" display="inline" id="S4.T3.9.9.2.m2.1"><semantics id="S4.T3.9.9.2.m2.1a"><mo id="S4.T3.9.9.2.m2.1.1" xref="S4.T3.9.9.2.m2.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S4.T3.9.9.2.m2.1b"><times id="S4.T3.9.9.2.m2.1.1.cmml" xref="S4.T3.9.9.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.9.9.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T3.9.9.2.m2.1d">√ó</annotation></semantics></math>16</td>
<td class="ltx_td ltx_align_center" id="S4.T3.9.9.4"><span class="ltx_text ltx_font_bold" id="S4.T3.9.9.4.1">25.5</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.11.11">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.11.11.3">8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.11.11.2">2<math alttext="\times" class="ltx_Math" display="inline" id="S4.T3.10.10.1.m1.1"><semantics id="S4.T3.10.10.1.m1.1a"><mo id="S4.T3.10.10.1.m1.1.1" xref="S4.T3.10.10.1.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S4.T3.10.10.1.m1.1b"><times id="S4.T3.10.10.1.m1.1.1.cmml" xref="S4.T3.10.10.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.10.10.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T3.10.10.1.m1.1d">√ó</annotation></semantics></math>2<math alttext="\times" class="ltx_Math" display="inline" id="S4.T3.11.11.2.m2.1"><semantics id="S4.T3.11.11.2.m2.1a"><mo id="S4.T3.11.11.2.m2.1.1" xref="S4.T3.11.11.2.m2.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S4.T3.11.11.2.m2.1b"><times id="S4.T3.11.11.2.m2.1.1.cmml" xref="S4.T3.11.11.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.11.11.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T3.11.11.2.m2.1d">√ó</annotation></semantics></math>2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.11.11.4">32.7</td>
</tr>
<tr class="ltx_tr" id="S4.T3.13.13">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.13.13.3">8</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.13.13.2">1<math alttext="\times" class="ltx_Math" display="inline" id="S4.T3.12.12.1.m1.1"><semantics id="S4.T3.12.12.1.m1.1a"><mo id="S4.T3.12.12.1.m1.1.1" xref="S4.T3.12.12.1.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S4.T3.12.12.1.m1.1b"><times id="S4.T3.12.12.1.m1.1.1.cmml" xref="S4.T3.12.12.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.12.12.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T3.12.12.1.m1.1d">√ó</annotation></semantics></math>1<math alttext="\times" class="ltx_Math" display="inline" id="S4.T3.13.13.2.m2.1"><semantics id="S4.T3.13.13.2.m2.1a"><mo id="S4.T3.13.13.2.m2.1.1" xref="S4.T3.13.13.2.m2.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S4.T3.13.13.2.m2.1b"><times id="S4.T3.13.13.2.m2.1.1.cmml" xref="S4.T3.13.13.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.13.13.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T3.13.13.2.m2.1d">√ó</annotation></semantics></math>8</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.13.13.4"><span class="ltx_text ltx_font_bold" id="S4.T3.13.13.4.1">25.7</span></td>
</tr>
</tbody>
</table>{{< /table-caption >}}

> üîº This table compares the performance and speed of Next-Token Prediction (NTP) and Next-Block Prediction (NBP) models for video generation.  The comparison is based on the Kinetics-600 dataset, using a 5-frame condition and predicting 12 frames (768 tokens).  Key metrics include Fr√©chet Video Distance (FVD) to assess video quality, the number of forward steps required during inference, and frames per second (FPS) to measure inference speed.  All models were implemented and trained under identical conditions (20 epochs on a single NVIDIA A100 GPU). The inference speed includes the time taken for both tokenization and de-tokenization processes.  The KV-cache was used for both model types.
> <details>
> <summary>read the caption</summary>
> Table 1: Comparison of next-token prediction (NTP) and next-block prediction (NBP) models in terms of performance and speed, evaluated on the K600 dataset (5-frame condition, 12 frames (768 tokens) to predict). Inference time was measured on a single A100 Nvidia GPU. All models are implemented by us under the same setting and trained for 20 epochs. FPS denotes ‚Äúframe per second‚Äù. The measurement of inference speed includes tokenization and de-tokenization processes. KV-cache is used for both models.
> </details>





### In-depth insights


#### Semi-AR VideoGen
Semi-autoregressive (Semi-AR) video generation methods offer a compelling alternative to traditional autoregressive approaches.  By processing video data in blocks instead of individual tokens, **Semi-AR VideoGen significantly accelerates inference speed** while maintaining, and potentially improving, generation quality.  The approach leverages parallel processing capabilities, reducing the computational burden of generating sequences frame-by-frame.  This is crucial for high-resolution videos and complex scenes where the number of tokens to predict is substantial.  Moreover, the use of bidirectional attention within each block allows for a more robust capture of spatial dependencies compared to unidirectional models.  **This results in improved contextual understanding and coherence** in the generated video content.  However, challenges remain in optimizing block size and configuration, as well as exploring different block structures and shapes to best suit the temporal dynamics of diverse video sequences.  Further research should focus on optimizing these hyperparameters and evaluating performance across a wider range of video datasets and generation tasks.  **Balancing computational efficiency with the quality of the generated video remains a key challenge** for future developments in this area.

#### Block Prediction
The concept of 'Block Prediction' in video generation presents a compelling alternative to traditional next-token prediction methods.  By processing video data in blocks (e.g., sequences of frames or rows of pixels), instead of individual tokens, **it offers significant advantages in computational efficiency and inference speed**. This approach allows for parallel prediction of multiple tokens, leading to a substantial reduction in the number of prediction steps required for video generation.  Furthermore, **bidirectional attention mechanisms within each block enable more robust modeling of spatial dependencies**, which is a notable improvement over unidirectional approaches that may hinder contextual understanding. Although this method relies on a uniform block decomposition of video, which might not be suitable for all videos, **the trade-off between efficiency and potential loss of subtle details is worth investigating further** for different video types and applications.  The effectiveness of 'Block Prediction' hinges on the appropriate selection of block size and shape, influencing both speed and model performance. This suggests that careful parameter tuning is crucial to fully harness the potential of this method.

#### Efficiency Gains
The efficiency gains in semi-autoregressive video generation, as explored in the research paper, stem primarily from a paradigm shift: moving from predicting individual tokens (next-token prediction or NTP) to predicting blocks of tokens (next-block prediction or NBP) simultaneously.  **This parallelization drastically reduces the number of inference steps**, leading to a significant speedup.  The paper highlights an 11x speed increase with minimal performance degradation, achieving comparable FVD scores to NTP models with substantially fewer steps.  **Bidirectional attention within each block** further contributes to efficiency gains by allowing tokens to better capture spatial dependencies, improving model robustness and reducing computation.  The framework's scalability, demonstrated through experiments across varying model sizes (700M to 3B parameters), suggests that these efficiency benefits are not confined to smaller models, but are consistent across different model scales. **The uniform decomposition of video content into equal-sized blocks** allows for efficient processing and effective parallelization, offering significant advantages for high-resolution videos and computationally intensive tasks.

#### Scalability Tests
In assessing the scalability of a video generation model, a robust testing strategy is crucial.  **Comprehensive scalability tests** should involve systematically increasing model parameters (e.g., 700M, 1.2B, 3B parameters) while carefully monitoring performance metrics such as Fr√©chet Video Distance (FVD) and frames per second (FPS).  A successful scalability test demonstrates **consistent improvement in video generation quality (lower FVD)** with increasing model size, indicating the model's ability to effectively leverage additional parameters.  Simultaneously, **assessing the impact on inference speed (FPS)** reveals the trade-off between model size and computational efficiency. Ideally, scalability should manifest as both improved quality and acceptable speed, even at larger scales.  Analyzing the validation loss curves during training for different model sizes provides additional insights into model learning dynamics and potential bottlenecks. **Careful consideration of hardware limitations** is also essential when performing scalability tests, ensuring that experimental results are not skewed by resource constraints.  The results of such tests are crucial for determining the optimal model size and deployment strategies.

#### Future Scope
The future scope of semi-autoregressive video generation, as explored in the research paper, is vast and exciting.  **Improving efficiency** remains paramount, with opportunities to explore more sophisticated attention mechanisms and more efficient decoding strategies to speed up inference and reduce computational costs.  **Scaling to higher resolutions and longer video sequences** is another crucial direction; current models often struggle with high-resolution video generation, both in terms of speed and quality.  **Enhanced model architectures** could leverage advancements in large language models, incorporating innovative techniques for capturing long-range dependencies and intricate spatio-temporal relationships in video data more effectively.  Finally, **expanding capabilities** beyond simple video generation is also essential;  future research could focus on integrating other modalities (text, audio), enabling conditional video generation from multi-modal input, and even exploring interactive video generation where user input influences the output.  Addressing ethical considerations surrounding deepfake generation, through techniques like watermarking, is also critical for responsible development.


### More visual insights

<details>
<summary>More on figures
</summary>


![](https://arxiv.org/html/2502.07737/x2.png)

> üîº This figure illustrates different ways to group tokens in a 3D video representation.  Standard autoregressive (AR) models process tokens individually (token-wise), treating each pixel or voxel as a separate unit in the generation process.  This figure shows alternative block configurations. A row-wise block considers a horizontal line of tokens as a unit, a frame-wise block considers all tokens in one frame as a unit.  This allows for parallel processing across multiple tokens in each block instead of generating tokens one by one. When a block size of 1x1x1 is used, it defaults back to the individual token approach of vanilla AR, showing how the block-based approaches generalize the individual token approach.
> <details>
> <summary>read the caption</summary>
> Figure 2: Examples of block include token-wise, row-wise, and frame-wise representations. When the block size is set to 1√ó\times√ó1√ó\times√ó1, it degenerates into a token, as used in vanilla AR modeling. Note that the actual token corresponds to a 3D cube, we omit the time dimension here for clarity.
> </details>



![](https://arxiv.org/html/2502.07737/x3.png)

> üîº This figure compares the vanilla autoregressive (AR) model with next-token prediction and the proposed semi-AR model with next-block prediction.  The vanilla AR model (left) generates one token at a time, sequentially, using a decoder-only transformer. Each token depends only on the preceding tokens. In contrast, the semi-AR model (right) generates a block of tokens simultaneously. Each token in the current block predicts the corresponding token in the next block, taking advantage of bidirectional attention within the block to capture spatial dependencies.  During inference in the semi-AR model, the generated block of tokens is duplicated and concatenated with prefix tokens to form the input for the next prediction step.
> <details>
> <summary>read the caption</summary>
> Figure 3: Comparison between a vanilla autoregressive (AR) framework based on next-token prediction (left) and our semi-AR framework based on next-block prediction (right). xj(i)subscriptsuperscriptùë•ùëñùëóx^{(i)}_{j}italic_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT indicates the jt‚Å¢hsuperscriptùëóùë°‚Ñéj^{th}italic_j start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT video token in the it‚Å¢hsuperscriptùëñùë°‚Ñéi^{th}italic_i start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT block, with each block containing LùêøLitalic_L tokens. The dashed line in the right panel presents that the LùêøLitalic_L tokens generated in the current step are duplicated and concatenated with prefix tokens, forming the input for the next step‚Äôs prediction during inference.
> </details>



![](https://arxiv.org/html/2502.07737/x4.png)

> üîº This figure compares the attention mechanisms used in traditional Next-Token Prediction (NTP) and the proposed Next-Block Prediction (NBP) models.  In NTP, the attention is strictly causal, meaning each token only attends to previous tokens, represented by a lower triangular attention mask.  This limitation restricts the model's ability to capture long-range dependencies. In contrast, NBP employs a block-wise attention mechanism. Within each block, tokens can attend to all other tokens in the block bidirectionally, enabling richer contextual understanding.  The figure highlights this difference by showing the attention masks for both methods. The x-axis and y-axis represent the keys and queries of the attention mechanism, respectively.
> <details>
> <summary>read the caption</summary>
> Figure 4: Causal attention mask in NTP modeling v.s. block-wise attention mask in NBP modeling. The x-axis and y-axis represent keys and queries, respectively.
> </details>



![](https://arxiv.org/html/2502.07737/x5.png)

> üîº This figure shows the validation loss curves during the training process for semi-autoregressive (semi-AR) video generation models with different parameter scales: 700M, 1.2B, and 3B.  The x-axis represents the number of training steps, and the y-axis shows the validation loss.  The curves illustrate how the model's performance changes as it trains, with different sized models exhibiting various learning rates and convergence behaviors.  The plot allows for comparison of the training efficiency and stability across models of differing sizes.
> <details>
> <summary>read the caption</summary>
> Figure 5: Validation loss of various sizes of semi-AR models from 700M to 3B.
> </details>



![](https://arxiv.org/html/2502.07737/x6.png)

> üîº This figure shows the validation loss curves for different block sizes used in the Next-Block Prediction (NBP) model during training.  The x-axis represents the number of training steps, and the y-axis represents the validation loss. Multiple lines are plotted, each corresponding to a different block size, ranging from 1 to 256.  The plot illustrates how the model's performance changes with varying block sizes during training. The optimal block size that balances training efficiency and model performance can be determined by analyzing this graph.
> <details>
> <summary>read the caption</summary>
> Figure 6: Validation loss of various block sizes from 1 to 256.
> </details>



![](https://arxiv.org/html/2502.07737/x7.png)

> üîº This figure shows a comparison of the generation quality (measured by Fr√©chet Video Distance, FVD) and inference speed (frames per second, FPS) of the Next-Block Prediction (NBP) model using different block sizes for video generation.  The x-axis represents the block size, ranging from 1 to 256, where a block size of 1 corresponds to the traditional next-token prediction method. The y-axis shows both the FVD score and the FPS.  A lower FVD indicates higher generation quality (closer to the ground truth), while a higher FPS indicates faster inference. The graph visually demonstrates the trade-off between generation quality and inference speed as the block size changes, allowing readers to identify an optimal block size that balances both.
> <details>
> <summary>read the caption</summary>
> Figure 7: Generation quality (FVD, lower is better) and inference speed (FPS, higher is better) of various block sizes from 1 to 256.
> </details>



![](https://arxiv.org/html/2502.07737/x8.png)

> üîº This figure visualizes the results of class-conditional video generation on the UCF-101 dataset using the proposed Next-Block Prediction (NBP) method. Each row presents a different action class from UCF-101.  Multiple video clips generated for each class are displayed, demonstrating the model's ability to generate diverse instances of each action. The text below each set of video clips indicates the specific action class being depicted.
> <details>
> <summary>read the caption</summary>
> Figure 8: Visualization of class-conditional generation (UCF-101) results of our method. The text below each video clip is the class name.
> </details>



![](https://arxiv.org/html/2502.07737/x9.png)

> üîº This figure shows example results of the model's frame prediction capabilities on the Kinetics-600 dataset.  Each row represents a video sequence where the model has predicted subsequent frames based on a given initial set of frames. The results demonstrate the model's ability to generate temporally coherent and visually plausible video frames, accurately continuing the action or scene from the input.  This showcases the model's capacity to generate realistic and smooth video sequences.
> <details>
> <summary>read the caption</summary>
> Figure 9: Visualization of frame prediction (K600) results of our method.
> </details>



![](https://arxiv.org/html/2502.07737/x10.png)

> üîº This figure displays a comparison of frame prediction results between the OmniTokenizer method and the Next-Block Prediction (NBP) method proposed in the paper.  The left side shows the input frames (the 'condition'), a short video sequence used to initiate the prediction. The right side shows the generated frames (the 'predicted subsequent sequence') which are the frames predicted by each method. By comparing the generated frames with the ground truth, one can visually assess the performance of each method in terms of accuracy and quality in video prediction tasks.  The differences between the OmniTokenizer and NBP predictions highlight the improvements achieved by the NBP model.
> <details>
> <summary>read the caption</summary>
> Figure 10: Frame prediction results of OmniTokenizer and our method. The left part is the condition, and the right part is the predicted subsequent sequence.
> </details>



![](https://arxiv.org/html/2502.07737/x11.png)

> üîº This figure presents a comparison of video reconstruction results between the OmniTokenizer method and the Next Block Prediction (NBP) method proposed in the paper.  It shows sample video frames (17 frames total, 128x128 resolution) generated by each method to illustrate the visual quality differences. The original video is displayed at 25 frames per second (fps), but for easier viewing within the figure, the frames are displayed at 6.25 fps.  A visual inspection allows one to assess the level of detail, artifacts, and overall reconstruction accuracy achieved by each method.
> <details>
> <summary>read the caption</summary>
> Figure 11: Video reconstruction results (17 frames 128√ó\times√ó128 resolution at 25 fps and shown at 6.25 fps) of OmniTokenizer and our method.
> </details>



![](https://arxiv.org/html/2502.07737/x12.png)

> üîº This figure displays video generation results from three different sized models: 700 million parameters, 1.2 billion parameters, and 3 billion parameters.  Each row shows a different video generated by each model. The purpose is to demonstrate how increasing model size impacts the quality of generated videos.  Observe the visual details to compare and contrast the outputs of the three models.  The generated videos shown exemplify the ability of the model to generate different video clips.
> <details>
> <summary>read the caption</summary>
> Figure 12: Visualization of video generation results of various model sizes (700M, 1.2B, and 3B).
> </details>



![](https://arxiv.org/html/2502.07737/x13.png)

> üîº This figure visualizes the video generation results using different block sizes within the Next-Block Prediction (NBP) model.  It shows three sets of video generation outputs, each corresponding to a different block size: 1x1x1 (token-wise), 1x1x16 (row-wise), and 1x1x16 (a larger block spanning multiple tokens). The three rows represent three different sample videos. The aim is to illustrate how altering the size of the processing unit (block size) impacts the model's ability to generate coherent and visually accurate video sequences.
> <details>
> <summary>read the caption</summary>
> Figure 13: Visualization of video generation results of various block sizes (1√ó\times√ó1√ó\times√ó1, 1√ó\times√ó1√ó\times√ó16 and 1√ó\times√ó16√ó\times√ó16).
> </details>



![](https://arxiv.org/html/2502.07737/x14.png)

> üîº Figure 14 is a heatmap visualizing the attention weights within the model during the next-clip prediction task on the UCF-101 dataset. The x and y axes represent the keys and queries, respectively, with each axis divided into three segments by two red lines: the text (class name), the first video clip, and the second video clip.  Each pixel's brightness corresponds to the attention weight between a specific key and query. The attention weights for text tokens are downweighted by a factor of 5 to enhance visualization clarity, highlighting the attention relationships between the textual description, the previous video clip, and the model's prediction for the next clip.
> <details>
> <summary>read the caption</summary>
> Figure 14: Attention weights of next-clip prediction on UCF-101. The horizontal and vertical axis represent the keys and queries, respectively. Two red lines on each axis divide the axis into three segments, corresponding to the text (classname), the first clip, and the second clip. The brightness of each pixel reflects the attention score. We downweight the attention to text tokens by 5√ó5\times5 √ó to provide a more clear visualization.
> </details>



![](https://arxiv.org/html/2502.07737/x15.png)

> üîº Figure 15 visualizes the spatial attention distribution within the Next-Block Prediction (NBP) model for a single query token (marked in red).  It shows how the attention mechanism weights the relevance of different spatial locations within the video frame when predicting the corresponding token in the next block. This illustrates the model's ability to capture spatial dependencies and relationships across the frame, a key improvement over traditional autoregressive models that rely on unidirectional dependencies.
> <details>
> <summary>read the caption</summary>
> Figure 15: Spatial attention distribution for a specific query (represented by red √ó\times√ó) on UCF-101.
> </details>



</details>




<details>
<summary>More on tables
</summary>


{{< table-caption >}}
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A1.T4.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T4.4.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="A1.T4.4.1.1.1">Model</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="A1.T4.4.1.1.2">Parameters</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A1.T4.4.1.1.3">Layers</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A1.T4.4.1.1.4">Hidden Size</th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A1.T4.4.1.1.5">Heads</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T4.4.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A1.T4.4.2.1.1">NBP-XL</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A1.T4.4.2.1.2">700M</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.4.2.1.3">24</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.4.2.1.4">1536</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="A1.T4.4.2.1.5">16</td>
</tr>
<tr class="ltx_tr" id="A1.T4.4.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T4.4.3.2.1">NBP-XXL</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A1.T4.4.3.2.2">1.2B</th>
<td class="ltx_td ltx_align_center" id="A1.T4.4.3.2.3">24</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.3.2.4">2048</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A1.T4.4.3.2.5">32</td>
</tr>
<tr class="ltx_tr" id="A1.T4.4.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="A1.T4.4.4.3.1">NBP-3B</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="A1.T4.4.4.3.2">3B</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T4.4.4.3.3">32</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T4.4.4.3.4">3072</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="A1.T4.4.4.3.5">32</td>
</tr>
</tbody>
</table>{{< /table-caption >}}
> üîº Table 2 presents a comparison of video generation models' performance on two benchmark datasets: UCF-101 and Kinetics-600.  For UCF-101, the task is class-conditional video generation, where the model generates videos based on a given class label. For Kinetics-600, the task is frame prediction, where the model predicts future frames given a sequence of initial frames. The table includes various models from different categories, including GANs, diffusion models, mask token modeling (MTM) approaches, and autoregressive (AR) models.  The models are evaluated based on Fr√©chet Video Distance (FVD), indicating video quality, the number of tokens used, and the number of generation steps required.  The table also notes the number of parameters for each model.  A key aspect is that models on K600 are assessed fairly, with those trained significantly longer than 77 epochs (e.g., over 300 epochs) grayed out to ensure a fair comparison based on computational resources.
> <details>
> <summary>read the caption</summary>
> Table 2: Comparions of class-conditional generation results on UCF-101 and frame prediction results on K600. MTM indicates mask token modeling. Our model on K600 is trained for 77 epochs, we gray out models that use significantly more training computation (e.g., those trained for over 300 epochs) for a fair comparison.
> </details>

{{< table-caption >}}
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A1.T5.4">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T5.4.5.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="A1.T5.4.5.1.1">Hyper-parameters</th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T5.4.5.1.2">UCF101</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T5.4.5.1.3">K600</td>
</tr>
<tr class="ltx_tr" id="A1.T5.4.6.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T5.4.6.2.1">Video FPS</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.4.6.2.2">8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.4.6.2.3">8</td>
</tr>
<tr class="ltx_tr" id="A1.T5.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T5.4.4.5">Latent shape</th>
<td class="ltx_td ltx_align_center" id="A1.T5.2.2.2">5<math alttext="\times" class="ltx_Math" display="inline" id="A1.T5.1.1.1.m1.1"><semantics id="A1.T5.1.1.1.m1.1a"><mo id="A1.T5.1.1.1.m1.1.1" xref="A1.T5.1.1.1.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="A1.T5.1.1.1.m1.1b"><times id="A1.T5.1.1.1.m1.1.1.cmml" xref="A1.T5.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.1.1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="A1.T5.1.1.1.m1.1d">√ó</annotation></semantics></math>16<math alttext="\times" class="ltx_Math" display="inline" id="A1.T5.2.2.2.m2.1"><semantics id="A1.T5.2.2.2.m2.1a"><mo id="A1.T5.2.2.2.m2.1.1" xref="A1.T5.2.2.2.m2.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="A1.T5.2.2.2.m2.1b"><times id="A1.T5.2.2.2.m2.1.1.cmml" xref="A1.T5.2.2.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.2.2.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="A1.T5.2.2.2.m2.1d">√ó</annotation></semantics></math>16</td>
<td class="ltx_td ltx_align_center" id="A1.T5.4.4.4">5<math alttext="\times" class="ltx_Math" display="inline" id="A1.T5.3.3.3.m1.1"><semantics id="A1.T5.3.3.3.m1.1a"><mo id="A1.T5.3.3.3.m1.1.1" xref="A1.T5.3.3.3.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="A1.T5.3.3.3.m1.1b"><times id="A1.T5.3.3.3.m1.1.1.cmml" xref="A1.T5.3.3.3.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.3.3.3.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="A1.T5.3.3.3.m1.1d">√ó</annotation></semantics></math>16<math alttext="\times" class="ltx_Math" display="inline" id="A1.T5.4.4.4.m2.1"><semantics id="A1.T5.4.4.4.m2.1a"><mo id="A1.T5.4.4.4.m2.1.1" xref="A1.T5.4.4.4.m2.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="A1.T5.4.4.4.m2.1b"><times id="A1.T5.4.4.4.m2.1.1.cmml" xref="A1.T5.4.4.4.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.4.4.4.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="A1.T5.4.4.4.m2.1d">√ó</annotation></semantics></math>16</td>
</tr>
<tr class="ltx_tr" id="A1.T5.4.7.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T5.4.7.3.1">Vocabulary size</th>
<td class="ltx_td ltx_align_center" id="A1.T5.4.7.3.2">64K</td>
<td class="ltx_td ltx_align_center" id="A1.T5.4.7.3.3">64K</td>
</tr>
<tr class="ltx_tr" id="A1.T5.4.8.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T5.4.8.4.1">Embedding dimension</th>
<td class="ltx_td ltx_align_center" id="A1.T5.4.8.4.2">6</td>
<td class="ltx_td ltx_align_center" id="A1.T5.4.8.4.3">6</td>
</tr>
<tr class="ltx_tr" id="A1.T5.4.9.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T5.4.9.5.1">Initialization</th>
<td class="ltx_td ltx_align_center" id="A1.T5.4.9.5.2">Random</td>
<td class="ltx_td ltx_align_center" id="A1.T5.4.9.5.3">Random</td>
</tr>
<tr class="ltx_tr" id="A1.T5.4.10.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T5.4.10.6.1">Peak learning rate</th>
<td class="ltx_td ltx_align_center" id="A1.T5.4.10.6.2">5e-5</td>
<td class="ltx_td ltx_align_center" id="A1.T5.4.10.6.3">1e-4</td>
</tr>
<tr class="ltx_tr" id="A1.T5.4.11.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T5.4.11.7.1">Learning rate schedule</th>
<td class="ltx_td ltx_align_center" id="A1.T5.4.11.7.2">linear</td>
<td class="ltx_td ltx_align_center" id="A1.T5.4.11.7.3">linear</td>
</tr>
<tr class="ltx_tr" id="A1.T5.4.12.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T5.4.12.8.1">Warmup ratio</th>
<td class="ltx_td ltx_align_center" id="A1.T5.4.12.8.2">0.01</td>
<td class="ltx_td ltx_align_center" id="A1.T5.4.12.8.3">0.01</td>
</tr>
<tr class="ltx_tr" id="A1.T5.4.13.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T5.4.13.9.1">Perceptual loss weight</th>
<td class="ltx_td ltx_align_center" id="A1.T5.4.13.9.2">0.1</td>
<td class="ltx_td ltx_align_center" id="A1.T5.4.13.9.3">0.1</td>
</tr>
<tr class="ltx_tr" id="A1.T5.4.14.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T5.4.14.10.1">Generator adversarial loss weight</th>
<td class="ltx_td ltx_align_center" id="A1.T5.4.14.10.2">0.1</td>
<td class="ltx_td ltx_align_center" id="A1.T5.4.14.10.3">0.1</td>
</tr>
<tr class="ltx_tr" id="A1.T5.4.15.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T5.4.15.11.1">Optimizer</th>
<td class="ltx_td ltx_align_center" id="A1.T5.4.15.11.2">Adam</td>
<td class="ltx_td ltx_align_center" id="A1.T5.4.15.11.3">Adam</td>
</tr>
<tr class="ltx_tr" id="A1.T5.4.16.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T5.4.16.12.1">Batch size</th>
<td class="ltx_td ltx_align_center" id="A1.T5.4.16.12.2">256</td>
<td class="ltx_td ltx_align_center" id="A1.T5.4.16.12.3">256</td>
</tr>
<tr class="ltx_tr" id="A1.T5.4.17.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="A1.T5.4.17.13.1">Epoch</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T5.4.17.13.2">2000</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T5.4.17.13.3">100</td>
</tr>
</tbody>
</table>{{< /table-caption >}}
> üîº This table presents an ablation study on the impact of different block shapes on the video generation quality. The experiment uses a 700M parameter model and evaluates various block shapes (T√óH√óW) such as 1√ó4√ó4, 2√ó1√ó8, and 1√ó1√ó16.  The Fr√©chet Video Distance (FVD) metric is used to assess the quality of generated videos, with lower FVD scores indicating better generation quality. The results show the optimal block shape and its effect on the balance between generation quality and efficiency.
> <details>
> <summary>read the caption</summary>
> Table 3: Generation quality (FVD) of various block shape.
> </details>

{{< table-caption >}}
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A1.T6.4">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T6.4.5.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="A1.T6.4.5.1.1">Hyper-parameters</th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T6.4.5.1.2">UCF101</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T6.4.5.1.3">K600</td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.6.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T6.4.6.2.1">Video FPS</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.4.6.2.2">8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.4.6.2.3">16</td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T6.4.4.5">Latent shape</th>
<td class="ltx_td ltx_align_center" id="A1.T6.2.2.2">5<math alttext="\times" class="ltx_Math" display="inline" id="A1.T6.1.1.1.m1.1"><semantics id="A1.T6.1.1.1.m1.1a"><mo id="A1.T6.1.1.1.m1.1.1" xref="A1.T6.1.1.1.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="A1.T6.1.1.1.m1.1b"><times id="A1.T6.1.1.1.m1.1.1.cmml" xref="A1.T6.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A1.T6.1.1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="A1.T6.1.1.1.m1.1d">√ó</annotation></semantics></math>16<math alttext="\times" class="ltx_Math" display="inline" id="A1.T6.2.2.2.m2.1"><semantics id="A1.T6.2.2.2.m2.1a"><mo id="A1.T6.2.2.2.m2.1.1" xref="A1.T6.2.2.2.m2.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="A1.T6.2.2.2.m2.1b"><times id="A1.T6.2.2.2.m2.1.1.cmml" xref="A1.T6.2.2.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A1.T6.2.2.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="A1.T6.2.2.2.m2.1d">√ó</annotation></semantics></math>16</td>
<td class="ltx_td ltx_align_center" id="A1.T6.4.4.4">5<math alttext="\times" class="ltx_Math" display="inline" id="A1.T6.3.3.3.m1.1"><semantics id="A1.T6.3.3.3.m1.1a"><mo id="A1.T6.3.3.3.m1.1.1" xref="A1.T6.3.3.3.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="A1.T6.3.3.3.m1.1b"><times id="A1.T6.3.3.3.m1.1.1.cmml" xref="A1.T6.3.3.3.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A1.T6.3.3.3.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="A1.T6.3.3.3.m1.1d">√ó</annotation></semantics></math>16<math alttext="\times" class="ltx_Math" display="inline" id="A1.T6.4.4.4.m2.1"><semantics id="A1.T6.4.4.4.m2.1a"><mo id="A1.T6.4.4.4.m2.1.1" xref="A1.T6.4.4.4.m2.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="A1.T6.4.4.4.m2.1b"><times id="A1.T6.4.4.4.m2.1.1.cmml" xref="A1.T6.4.4.4.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A1.T6.4.4.4.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="A1.T6.4.4.4.m2.1d">√ó</annotation></semantics></math>16</td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.7.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T6.4.7.3.1">Vocabulary size</th>
<td class="ltx_td ltx_align_center" id="A1.T6.4.7.3.2">96K (including 32K text tokens)</td>
<td class="ltx_td ltx_align_center" id="A1.T6.4.7.3.3">64K</td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.8.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T6.4.8.4.1">Initialization</th>
<td class="ltx_td ltx_align_center" id="A1.T6.4.8.4.2">Random</td>
<td class="ltx_td ltx_align_center" id="A1.T6.4.8.4.3">Random</td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.9.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T6.4.9.5.1">Peak learning rate</th>
<td class="ltx_td ltx_align_center" id="A1.T6.4.9.5.2">6e-4</td>
<td class="ltx_td ltx_align_center" id="A1.T6.4.9.5.3">1e-3</td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.10.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T6.4.10.6.1">Learning rate schedule</th>
<td class="ltx_td ltx_align_center" id="A1.T6.4.10.6.2">linear</td>
<td class="ltx_td ltx_align_center" id="A1.T6.4.10.6.3">linear</td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.11.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T6.4.11.7.1">Warmup steps</th>
<td class="ltx_td ltx_align_center" id="A1.T6.4.11.7.2">5,000</td>
<td class="ltx_td ltx_align_center" id="A1.T6.4.11.7.3">10,000</td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.12.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T6.4.12.8.1">Weight decay</th>
<td class="ltx_td ltx_align_center" id="A1.T6.4.12.8.2">0.01</td>
<td class="ltx_td ltx_align_center" id="A1.T6.4.12.8.3">0.01</td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.13.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T6.4.13.9.1">Optimizer</th>
<td class="ltx_td ltx_align_center" id="A1.T6.4.13.9.2">Adam (0.9, 0.98)</td>
<td class="ltx_td ltx_align_center" id="A1.T6.4.13.9.3">Adam (0.9, 0.98)</td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.14.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T6.4.14.10.1">Dropout</th>
<td class="ltx_td ltx_align_center" id="A1.T6.4.14.10.2">0.1</td>
<td class="ltx_td ltx_align_center" id="A1.T6.4.14.10.3">0.1</td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.15.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T6.4.15.11.1">Batch size</th>
<td class="ltx_td ltx_align_center" id="A1.T6.4.15.11.2">256</td>
<td class="ltx_td ltx_align_center" id="A1.T6.4.15.11.3">64</td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.16.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="A1.T6.4.16.12.1">Epoch</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T6.4.16.12.2">2560</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T6.4.16.12.3">77</td>
</tr>
</tbody>
</table>{{< /table-caption >}}
> üîº This table details the different model sizes used in the experiments of the paper.  It shows the number of parameters, the number of layers, the hidden size, and the number of attention heads for three different models: NBP-XL, NBP-XXL, and NBP-3B.  The architecture of these models follows the LLaMA architecture (Touvron et al., 2023), a well-known large language model architecture.
> <details>
> <summary>read the caption</summary>
> Table 4: Model sizes and architecture configurations of our generation model. The configurations are following LLaMA¬†(Touvron et¬†al., 2023).
> </details>

{{< table-caption >}}
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A2.T7.8.8">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A2.T7.8.8.9.1">
<td class="ltx_td ltx_border_tt" colspan="5" id="A2.T7.8.8.9.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4" id="A2.T7.8.8.9.1.2" style="padding-left:2.0pt;padding-right:2.0pt;">UCF-101</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4" id="A2.T7.8.8.9.1.3" style="padding-left:2.0pt;padding-right:2.0pt;">K600</th>
</tr>
<tr class="ltx_tr" id="A2.T7.8.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="A2.T7.8.8.8.9" style="padding-left:2.0pt;padding-right:2.0pt;">Method</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A2.T7.8.8.8.10" style="padding-left:2.0pt;padding-right:2.0pt;">Backbone</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A2.T7.8.8.8.11" style="padding-left:2.0pt;padding-right:2.0pt;">Quantizer</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A2.T7.8.8.8.12" style="padding-left:2.0pt;padding-right:2.0pt;">Param.</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A2.T7.8.8.8.13" style="padding-left:2.0pt;padding-right:2.0pt;"># bits</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="A2.T7.1.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;">rFVD<math alttext="\downarrow" class="ltx_Math" display="inline" id="A2.T7.1.1.1.1.m1.1"><semantics id="A2.T7.1.1.1.1.m1.1a"><mo id="A2.T7.1.1.1.1.m1.1.1" stretchy="false" xref="A2.T7.1.1.1.1.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="A2.T7.1.1.1.1.m1.1b"><ci id="A2.T7.1.1.1.1.m1.1.1.cmml" xref="A2.T7.1.1.1.1.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T7.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="A2.T7.1.1.1.1.m1.1d">‚Üì</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="A2.T7.2.2.2.2" style="padding-left:2.0pt;padding-right:2.0pt;">PSNR<math alttext="\uparrow" class="ltx_Math" display="inline" id="A2.T7.2.2.2.2.m1.1"><semantics id="A2.T7.2.2.2.2.m1.1a"><mo id="A2.T7.2.2.2.2.m1.1.1" stretchy="false" xref="A2.T7.2.2.2.2.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="A2.T7.2.2.2.2.m1.1b"><ci id="A2.T7.2.2.2.2.m1.1.1.cmml" xref="A2.T7.2.2.2.2.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T7.2.2.2.2.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A2.T7.2.2.2.2.m1.1d">‚Üë</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="A2.T7.3.3.3.3" style="padding-left:2.0pt;padding-right:2.0pt;">SSIM<math alttext="\uparrow" class="ltx_Math" display="inline" id="A2.T7.3.3.3.3.m1.1"><semantics id="A2.T7.3.3.3.3.m1.1a"><mo id="A2.T7.3.3.3.3.m1.1.1" stretchy="false" xref="A2.T7.3.3.3.3.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="A2.T7.3.3.3.3.m1.1b"><ci id="A2.T7.3.3.3.3.m1.1.1.cmml" xref="A2.T7.3.3.3.3.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T7.3.3.3.3.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A2.T7.3.3.3.3.m1.1d">‚Üë</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="A2.T7.4.4.4.4" style="padding-left:2.0pt;padding-right:2.0pt;">LPIPS<math alttext="\downarrow" class="ltx_Math" display="inline" id="A2.T7.4.4.4.4.m1.1"><semantics id="A2.T7.4.4.4.4.m1.1a"><mo id="A2.T7.4.4.4.4.m1.1.1" stretchy="false" xref="A2.T7.4.4.4.4.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="A2.T7.4.4.4.4.m1.1b"><ci id="A2.T7.4.4.4.4.m1.1.1.cmml" xref="A2.T7.4.4.4.4.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T7.4.4.4.4.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="A2.T7.4.4.4.4.m1.1d">‚Üì</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="A2.T7.5.5.5.5" style="padding-left:2.0pt;padding-right:2.0pt;">rFVD<math alttext="\downarrow" class="ltx_Math" display="inline" id="A2.T7.5.5.5.5.m1.1"><semantics id="A2.T7.5.5.5.5.m1.1a"><mo id="A2.T7.5.5.5.5.m1.1.1" stretchy="false" xref="A2.T7.5.5.5.5.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="A2.T7.5.5.5.5.m1.1b"><ci id="A2.T7.5.5.5.5.m1.1.1.cmml" xref="A2.T7.5.5.5.5.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T7.5.5.5.5.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="A2.T7.5.5.5.5.m1.1d">‚Üì</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="A2.T7.6.6.6.6" style="padding-left:2.0pt;padding-right:2.0pt;">PSNR<math alttext="\uparrow" class="ltx_Math" display="inline" id="A2.T7.6.6.6.6.m1.1"><semantics id="A2.T7.6.6.6.6.m1.1a"><mo id="A2.T7.6.6.6.6.m1.1.1" stretchy="false" xref="A2.T7.6.6.6.6.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="A2.T7.6.6.6.6.m1.1b"><ci id="A2.T7.6.6.6.6.m1.1.1.cmml" xref="A2.T7.6.6.6.6.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T7.6.6.6.6.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A2.T7.6.6.6.6.m1.1d">‚Üë</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="A2.T7.7.7.7.7" style="padding-left:2.0pt;padding-right:2.0pt;">SSIM<math alttext="\uparrow" class="ltx_Math" display="inline" id="A2.T7.7.7.7.7.m1.1"><semantics id="A2.T7.7.7.7.7.m1.1a"><mo id="A2.T7.7.7.7.7.m1.1.1" stretchy="false" xref="A2.T7.7.7.7.7.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="A2.T7.7.7.7.7.m1.1b"><ci id="A2.T7.7.7.7.7.m1.1.1.cmml" xref="A2.T7.7.7.7.7.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T7.7.7.7.7.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A2.T7.7.7.7.7.m1.1d">‚Üë</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="A2.T7.8.8.8.8" style="padding-left:2.0pt;padding-right:2.0pt;">LPIPS<math alttext="\downarrow" class="ltx_Math" display="inline" id="A2.T7.8.8.8.8.m1.1"><semantics id="A2.T7.8.8.8.8.m1.1a"><mo id="A2.T7.8.8.8.8.m1.1.1" stretchy="false" xref="A2.T7.8.8.8.8.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="A2.T7.8.8.8.8.m1.1b"><ci id="A2.T7.8.8.8.8.m1.1.1.cmml" xref="A2.T7.8.8.8.8.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T7.8.8.8.8.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="A2.T7.8.8.8.8.m1.1d">‚Üì</annotation></semantics></math>
</th>
</tr>
<tr class="ltx_tr" id="A2.T7.8.8.10.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="A2.T7.8.8.10.2.1" style="padding-left:2.0pt;padding-right:2.0pt;">MaskGIT¬†<cite class="ltx_cite ltx_citemacro_citep">(Chang et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2502.07737v2#bib.bib8" title="">2022</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T7.8.8.10.2.2" style="padding-left:2.0pt;padding-right:2.0pt;">2D CNN</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T7.8.8.10.2.3" style="padding-left:2.0pt;padding-right:2.0pt;">VQ</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T7.8.8.10.2.4" style="padding-left:2.0pt;padding-right:2.0pt;">53M</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T7.8.8.10.2.5" style="padding-left:2.0pt;padding-right:2.0pt;">10</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T7.8.8.10.2.6" style="padding-left:2.0pt;padding-right:2.0pt;">216</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T7.8.8.10.2.7" style="padding-left:2.0pt;padding-right:2.0pt;">21.5</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T7.8.8.10.2.8" style="padding-left:2.0pt;padding-right:2.0pt;">.685</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T7.8.8.10.2.9" style="padding-left:2.0pt;padding-right:2.0pt;">.1140</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T7.8.8.10.2.10" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T7.8.8.10.2.11" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T7.8.8.10.2.12" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T7.8.8.10.2.13" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
</tr>
<tr class="ltx_tr" id="A2.T7.8.8.11.3">
<td class="ltx_td ltx_align_left" id="A2.T7.8.8.11.3.1" style="padding-left:2.0pt;padding-right:2.0pt;">TATS¬†<cite class="ltx_cite ltx_citemacro_citep">(Ge et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2502.07737v2#bib.bib16" title="">2022</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="A2.T7.8.8.11.3.2" style="padding-left:2.0pt;padding-right:2.0pt;">3D CNN</td>
<td class="ltx_td ltx_align_center" id="A2.T7.8.8.11.3.3" style="padding-left:2.0pt;padding-right:2.0pt;">VQ</td>
<td class="ltx_td ltx_align_center" id="A2.T7.8.8.11.3.4" style="padding-left:2.0pt;padding-right:2.0pt;">32M</td>
<td class="ltx_td ltx_align_center" id="A2.T7.8.8.11.3.5" style="padding-left:2.0pt;padding-right:2.0pt;">14</td>
<td class="ltx_td ltx_align_right" id="A2.T7.8.8.11.3.6" style="padding-left:2.0pt;padding-right:2.0pt;">162</td>
<td class="ltx_td ltx_align_right" id="A2.T7.8.8.11.3.7" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_right" id="A2.T7.8.8.11.3.8" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_right" id="A2.T7.8.8.11.3.9" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_right" id="A2.T7.8.8.11.3.10" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_right" id="A2.T7.8.8.11.3.11" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_right" id="A2.T7.8.8.11.3.12" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_right" id="A2.T7.8.8.11.3.13" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
</tr>
<tr class="ltx_tr" id="A2.T7.8.8.12.4">
<td class="ltx_td ltx_align_left" id="A2.T7.8.8.12.4.1" style="padding-left:2.0pt;padding-right:2.0pt;">OmniTokenizer¬†<cite class="ltx_cite ltx_citemacro_citep">(Wang et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2502.07737v2#bib.bib63" title="">2024a</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="A2.T7.8.8.12.4.2" style="padding-left:2.0pt;padding-right:2.0pt;">ViT</td>
<td class="ltx_td ltx_align_center" id="A2.T7.8.8.12.4.3" style="padding-left:2.0pt;padding-right:2.0pt;">VQ</td>
<td class="ltx_td ltx_align_center" id="A2.T7.8.8.12.4.4" style="padding-left:2.0pt;padding-right:2.0pt;">78M</td>
<td class="ltx_td ltx_align_center" id="A2.T7.8.8.12.4.5" style="padding-left:2.0pt;padding-right:2.0pt;">13</td>
<td class="ltx_td ltx_align_right" id="A2.T7.8.8.12.4.6" style="padding-left:2.0pt;padding-right:2.0pt;">42</td>
<td class="ltx_td ltx_align_right" id="A2.T7.8.8.12.4.7" style="padding-left:2.0pt;padding-right:2.0pt;">30.3</td>
<td class="ltx_td ltx_align_right" id="A2.T7.8.8.12.4.8" style="padding-left:2.0pt;padding-right:2.0pt;">.910</td>
<td class="ltx_td ltx_align_right" id="A2.T7.8.8.12.4.9" style="padding-left:2.0pt;padding-right:2.0pt;">.0733</td>
<td class="ltx_td ltx_align_right" id="A2.T7.8.8.12.4.10" style="padding-left:2.0pt;padding-right:2.0pt;">27</td>
<td class="ltx_td ltx_align_right" id="A2.T7.8.8.12.4.11" style="padding-left:2.0pt;padding-right:2.0pt;">28.5</td>
<td class="ltx_td ltx_align_right" id="A2.T7.8.8.12.4.12" style="padding-left:2.0pt;padding-right:2.0pt;">.883</td>
<td class="ltx_td ltx_align_right" id="A2.T7.8.8.12.4.13" style="padding-left:2.0pt;padding-right:2.0pt;">.0945</td>
</tr>
<tr class="ltx_tr" id="A2.T7.8.8.13.5">
<td class="ltx_td ltx_align_left" id="A2.T7.8.8.13.5.1" style="padding-left:2.0pt;padding-right:2.0pt;">MAGVIT-v1¬†<cite class="ltx_cite ltx_citemacro_citep">(Yu et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2502.07737v2#bib.bib75" title="">2023a</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="A2.T7.8.8.13.5.2" style="padding-left:2.0pt;padding-right:2.0pt;">3D CNN</td>
<td class="ltx_td ltx_align_center" id="A2.T7.8.8.13.5.3" style="padding-left:2.0pt;padding-right:2.0pt;">VQ</td>
<td class="ltx_td ltx_align_center" id="A2.T7.8.8.13.5.4" style="padding-left:2.0pt;padding-right:2.0pt;">158M</td>
<td class="ltx_td ltx_align_center" id="A2.T7.8.8.13.5.5" style="padding-left:2.0pt;padding-right:2.0pt;">10</td>
<td class="ltx_td ltx_align_right" id="A2.T7.8.8.13.5.6" style="padding-left:2.0pt;padding-right:2.0pt;">25</td>
<td class="ltx_td ltx_align_right" id="A2.T7.8.8.13.5.7" style="padding-left:2.0pt;padding-right:2.0pt;">22.0</td>
<td class="ltx_td ltx_align_right" id="A2.T7.8.8.13.5.8" style="padding-left:2.0pt;padding-right:2.0pt;">.701</td>
<td class="ltx_td ltx_align_right" id="A2.T7.8.8.13.5.9" style="padding-left:2.0pt;padding-right:2.0pt;">.0990</td>
<td class="ltx_td ltx_align_right" id="A2.T7.8.8.13.5.10" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_right" id="A2.T7.8.8.13.5.11" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_right" id="A2.T7.8.8.13.5.12" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_right" id="A2.T7.8.8.13.5.13" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
</tr>
<tr class="ltx_tr" id="A2.T7.8.8.14.6">
<td class="ltx_td ltx_align_left" id="A2.T7.8.8.14.6.1" style="padding-left:2.0pt;padding-right:2.0pt;">MAGVIT-v2¬†<cite class="ltx_cite ltx_citemacro_citep">(Yu et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2502.07737v2#bib.bib77" title="">2024</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="A2.T7.8.8.14.6.2" style="padding-left:2.0pt;padding-right:2.0pt;">C.-3D CNN</td>
<td class="ltx_td ltx_align_center" id="A2.T7.8.8.14.6.3" style="padding-left:2.0pt;padding-right:2.0pt;">LFQ</td>
<td class="ltx_td ltx_align_center" id="A2.T7.8.8.14.6.4" style="padding-left:2.0pt;padding-right:2.0pt;">158M</td>
<td class="ltx_td ltx_align_center" id="A2.T7.8.8.14.6.5" style="padding-left:2.0pt;padding-right:2.0pt;">18</td>
<td class="ltx_td ltx_align_right" id="A2.T7.8.8.14.6.6" style="padding-left:2.0pt;padding-right:2.0pt;">16.12</td>
<td class="ltx_td ltx_align_right" id="A2.T7.8.8.14.6.7" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_right" id="A2.T7.8.8.14.6.8" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_right" id="A2.T7.8.8.14.6.9" style="padding-left:2.0pt;padding-right:2.0pt;">.0694</td>
<td class="ltx_td ltx_align_right" id="A2.T7.8.8.14.6.10" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_right" id="A2.T7.8.8.14.6.11" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_right" id="A2.T7.8.8.14.6.12" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_right" id="A2.T7.8.8.14.6.13" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
</tr>
<tr class="ltx_tr" id="A2.T7.8.8.15.7">
<td class="ltx_td ltx_align_left" id="A2.T7.8.8.15.7.1" style="padding-left:2.0pt;padding-right:2.0pt;">MAGVIT-v2¬†<cite class="ltx_cite ltx_citemacro_citep">(Yu et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2502.07737v2#bib.bib77" title="">2024</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="A2.T7.8.8.15.7.2" style="padding-left:2.0pt;padding-right:2.0pt;">C.-3D CNN</td>
<td class="ltx_td ltx_align_center" id="A2.T7.8.8.15.7.3" style="padding-left:2.0pt;padding-right:2.0pt;">LFQ</td>
<td class="ltx_td ltx_align_center" id="A2.T7.8.8.15.7.4" style="padding-left:2.0pt;padding-right:2.0pt;">370M</td>
<td class="ltx_td ltx_align_center" id="A2.T7.8.8.15.7.5" style="padding-left:2.0pt;padding-right:2.0pt;">18</td>
<td class="ltx_td ltx_align_right" id="A2.T7.8.8.15.7.6" style="padding-left:2.0pt;padding-right:2.0pt;">8.62</td>
<td class="ltx_td ltx_align_right" id="A2.T7.8.8.15.7.7" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_right" id="A2.T7.8.8.15.7.8" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_right" id="A2.T7.8.8.15.7.9" style="padding-left:2.0pt;padding-right:2.0pt;">.0537</td>
<td class="ltx_td ltx_align_right" id="A2.T7.8.8.15.7.10" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_right" id="A2.T7.8.8.15.7.11" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_right" id="A2.T7.8.8.15.7.12" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_right" id="A2.T7.8.8.15.7.13" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
</tr>
<tr class="ltx_tr" id="A2.T7.8.8.16.8">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="A2.T7.8.8.16.8.1" style="padding-left:2.0pt;padding-right:2.0pt;">NBP-Tokenizer (Ours)</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A2.T7.8.8.16.8.2" style="padding-left:2.0pt;padding-right:2.0pt;">C.-3D CNN</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A2.T7.8.8.16.8.3" style="padding-left:2.0pt;padding-right:2.0pt;">FSQ</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A2.T7.8.8.16.8.4" style="padding-left:2.0pt;padding-right:2.0pt;">370M</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A2.T7.8.8.16.8.5" style="padding-left:2.0pt;padding-right:2.0pt;">16</td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="A2.T7.8.8.16.8.6" style="padding-left:2.0pt;padding-right:2.0pt;">15.50</td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="A2.T7.8.8.16.8.7" style="padding-left:2.0pt;padding-right:2.0pt;">29.3</td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="A2.T7.8.8.16.8.8" style="padding-left:2.0pt;padding-right:2.0pt;">.893</td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="A2.T7.8.8.16.8.9" style="padding-left:2.0pt;padding-right:2.0pt;">.0648</td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="A2.T7.8.8.16.8.10" style="padding-left:2.0pt;padding-right:2.0pt;">6.73</td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="A2.T7.8.8.16.8.11" style="padding-left:2.0pt;padding-right:2.0pt;">31.3</td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="A2.T7.8.8.16.8.12" style="padding-left:2.0pt;padding-right:2.0pt;">.944</td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="A2.T7.8.8.16.8.13" style="padding-left:2.0pt;padding-right:2.0pt;">.0828</td>
</tr>
</tbody>
</table>{{< /table-caption >}}
> üîº This table details the hyperparameters used during the training phase of the video tokenizer. It shows the settings specific to both the UCF101 and K600 datasets, including video frames per second (FPS), latent shape of the tokens, vocabulary size, embedding dimension, initialization method, learning rate schedule, peak learning rate, warmup ratio, and optimizer used.  It also indicates the perceptual loss weight, generator adversarial loss weight, batch size, and number of epochs used in the training process.
> <details>
> <summary>read the caption</summary>
> Table 5: Training configurations of video tokenizer.
> </details>

</details>




### Full paper

{{< gallery >}}
<img src="https://ai-paper-reviewer.com/2502.07737/1.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2502.07737/2.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2502.07737/3.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2502.07737/4.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2502.07737/5.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2502.07737/6.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2502.07737/7.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2502.07737/8.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2502.07737/9.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2502.07737/10.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2502.07737/11.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2502.07737/12.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2502.07737/13.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2502.07737/14.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2502.07737/15.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2502.07737/16.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2502.07737/17.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2502.07737/18.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2502.07737/19.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2502.07737/20.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
{{< /gallery >}}