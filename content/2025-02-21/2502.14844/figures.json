[{"figure_path": "https://arxiv.org/html/2502.14844/x1.png", "caption": "Figure 1. We personalize a video model to capture dynamic concepts \u2013 entities defined not only by their appearance but also by their unique motion patterns, such as the fluid motion of ocean waves or the flickering dynamics of a bonfire (left). This enables high-fidelity generation, editing, and the composition of these dynamic elements into a single video, where they interact naturally (right).", "description": "Figure 1 demonstrates the personalization of a video generation model to capture dynamic concepts. The left panel shows examples of dynamic concepts, such as ocean waves and a bonfire, which are defined by both their visual appearance and characteristic motion patterns.  The right panel illustrates the high-fidelity generation, editing, and composition capabilities enabled by this personalization. It shows how these dynamic elements can be seamlessly integrated into a single video, interacting naturally with each other.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2502.14844/extracted/6219306/images/editing.jpg", "caption": "Figure 2. Set-and-Sequence framework operates in two stages:\n(i) Identity Basis: We train LoRA Set Encoding on a unordered set of frames extracted from the video, focusing only on the appearance of the dynamic concept to achieve high fidelity without temporal distractions.\n(ii) Motion Residuals: The Basis of the Identity LoRAs is frozen and the coefficient part is augmented with coefficients of LoRA Sequence Encoding trained on the temporal sequence of full video clip, allowing the model to capture the motion dynamics of the concept.", "description": "The Set-and-Sequence framework is a two-stage process for personalizing video generation models with dynamic concepts.  Stage 1, Identity Basis, focuses on appearance. It uses an unordered set of frames from the input video to train LoRA (Low-Rank Adaptation) layers. This training learns an 'identity LoRA basis' representing the appearance of the concept without temporal information. In Stage 2, Motion Residuals, the identity LoRAs are frozen.  The coefficients are augmented using additional LoRAs trained on the full temporal sequence of the video. This captures the motion dynamics. The final result is a spatio-temporal weight space that effectively integrates both appearance and motion into the model, allowing for high-fidelity generation and editing.", "section": "3 Method"}, {"figure_path": "https://arxiv.org/html/2502.14844/extracted/6219306/images/pixar.jpg", "caption": "Figure 3. Local and Global Editing. Our Set-and-Sequence framework enables text-driven edits of dynamic concepts while preserving both their appearance and motion. Edits can be global (e.g., background and lighting) or local (e.g., clothing and object replacement), ensuring high fidelity to the original dynamic concepts.", "description": "This figure demonstrates the capabilities of the Set-and-Sequence framework for editing personalized videos.  The framework allows for both global edits (affecting the entire scene, such as changing the background or lighting) and local edits (focused changes, such as altering clothing or replacing an object).  Importantly, these edits are performed while maintaining the original appearance and motion characteristics of the dynamic concept, ensuring high-fidelity results. The images show several examples of these edits being applied, highlighting the flexibility and precision of the method.", "section": "2.3 Scene Composition in Video Models"}, {"figure_path": "https://arxiv.org/html/2502.14844/extracted/6219306/images/compose.jpg", "caption": "Figure 4. Stylization. Top: Stylization of dynamic concepts achieved by reweighting the identity basis. Bottom: Stylization and motion editing performed using prompt derived from the video in the top row.", "description": "This figure demonstrates the stylization capabilities of the Set-and-Sequence framework. The top row showcases how merely adjusting the identity basis weights allows for stylizing the appearance of the dynamic concept (a person) without altering its motion. The bottom row shows the result of applying both stylization and motion editing to the same concept; this time, edits are text-driven, demonstrating how the framework facilitates precise control over both the appearance and motion of dynamic elements.", "section": "2.2 Video Personalization and Motion Representation"}, {"figure_path": "https://arxiv.org/html/2502.14844/extracted/6219306/images/compare.jpg", "caption": "Figure 5. Dynamic Concepts Composition. Composition results achieved by our framework showcasing seamless integration of dynamic concepts. with each concept color-coded for clarity. For a more comprehensive demonstration, refer to the supplementary videos.", "description": "This figure demonstrates the ability of the Set-and-Sequence framework to seamlessly integrate multiple dynamic concepts into a single video.  Each concept is color-coded for easy identification. The results showcase high-fidelity generation and editing capabilities, where different dynamic elements interact naturally. For a more complete view of these capabilities, please refer to the supplementary videos.", "section": "Scene Composition in Video Models"}, {"figure_path": "https://arxiv.org/html/2502.14844/extracted/6219306/images/ablation.jpg", "caption": "Figure 6. Comparison with baselines. Comparison of our method with baseline approaches (NewMove\u00a0(Materzy\u0144ska et\u00a0al., 2024), DreamVideo\u00a0(Wei et\u00a0al., 2024), DB-LoRA\u00a0(Ryu, 2023; Ruiz et\u00a0al., 2023a), and DreamMix\u00a0(Molad et\u00a0al., 2023)) on two editing scenarios: changing the background and shirt, and adding a glass. Our method demonstrates superior adherence to the prompt while preserving the subject identity, outperforming the baselines.", "description": "This figure compares the performance of the proposed 'Set-and-Sequence' method against several baseline approaches on two video editing tasks.  The tasks involve modifying a video to (1) change the subject's background and shirt, and (2) add a glass to the scene. The baselines include NewMove, DreamVideo, DB-LoRA, and DreamMix. The comparison highlights the superior performance of the proposed method in adhering to the specified edits described in text prompts while maintaining the subject's identity and producing higher quality results than other methods.", "section": "Comparison with baselines"}, {"figure_path": "https://arxiv.org/html/2502.14844/extracted/6219306/images/stitch.jpg", "caption": "Figure 7. Ablation. Ablation of design choices on the editing task of adding a different shirt and background. Low-rank LoRA (LoRA-1) results in underfitting, failing to capture sufficient detail, while high-rank LoRA (LoRA-8) overfits, compromising adaptability. Our two-stage approach with added regularization achieves a balanced trade-off, preserving both fidelity and editability.", "description": "This ablation study analyzes the impact of different LoRA ranks and the two-stage training approach on video editing. Using the task of changing a shirt and background, it demonstrates that low-rank LoRAs (LoRA-1) cause underfitting, missing crucial details, while high-rank LoRAs (LoRA-8) lead to overfitting, reducing adaptability. The results highlight that the proposed two-stage approach with regularization provides the best balance, maintaining both high fidelity and the capacity for edits.", "section": "4.3 Evaluation Metrics"}]