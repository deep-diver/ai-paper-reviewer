{"references": [{"fullname_first_author": "Olah", "paper_title": "Zoom in: An introduction to circuits", "publication_date": "2020-01-01", "reason": "This work introduces circuit analysis, a fundamental methodology used in this study to represent the model's computation through structured subgraphs."}, {"fullname_first_author": "Meng", "paper_title": "Locating and editing factual associations in gpt", "publication_date": "2022-01-01", "reason": "This work provided a methodology used in this study to edit temporal knowledge by dynamically manipulating attention heads based on findings in their work."}, {"fullname_first_author": "Vrande\u010di\u0107", "paper_title": "Wikidata: a free collaborative knowledgebase", "publication_date": "2014-01-01", "reason": "The authors used data based on Wikidata, leveraging its temporal knowledge for the knowledge samples that embed a specific year."}, {"fullname_first_author": "Joshi", "paper_title": "TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension", "publication_date": "2017-01-01", "reason": "The authors utilized TriviaQA data in ChroKnowledge for unstructured, general QA to verify the ablation effect with basic LLM's tasks."}, {"fullname_first_author": "Touvron", "paper_title": "Llama 2: Open foundation and fine-tuned chat models", "publication_date": "2023-07-01", "reason": "This work uses the LLaMA-2-7b-chat-hf model, one of the primary LLMs investigated for temporal heads, contributing significantly to the study's findings and analysis."}]}