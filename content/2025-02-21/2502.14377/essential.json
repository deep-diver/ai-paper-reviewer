{"importance": "This research introduces RelaCtrl, a method that optimizes resource allocation in diffusion transformers. By evaluating layer relevance and streamlining control mechanisms, it offers significant efficiency gains, guiding future research in controllable generation and efficient model design.", "summary": "RelaCtrl: Relevance-guided control boosts diffusion transformer efficiency, cutting parameters by intelligently allocating resources.", "takeaways": ["Diffusion Transformer layers have varying relevance to control information.", "Relevance-guided allocation of control resources enhances efficiency without compromising performance.", "The Two-Dimensional Shuffle Mixer (TDSM) offers a lightweight alternative to self-attention in control blocks."], "tldr": "Diffusion Transformers(DiT) are powerful for text-to-image/video generation, existing methods suffer from high computational costs due to inefficient resource allocation, neglecting the varying importance of control info across layers. This leads to redundant parameters & computations, hindering efficiency in training/inference. \n\nTo solve this, the paper introduces RelaCtrl, a framework using \"ControlNet Relevance Score\" to evaluate each layer's importance. It tailors control layer placement, parameter scale, & capacity based on relevance, reducing unnecessary resources. They propose Two-Dimensional Shuffle Mixer (TDSM) in copy blocks for efficient token/channel mixing. Experiments show RelaCtrl achieves better performance with fewer resources.", "affiliation": "University of Science and Technology of China", "categories": {"main_category": "Computer Vision", "sub_category": "Image Generation"}, "podcast_path": "2502.14377/podcast.wav"}