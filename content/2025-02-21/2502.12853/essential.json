{"importance": "**S2R** offers an interpretable way for LLMs to self-verify and self-correct with minimal resources. It enhances reasoning and opens avenues for SFT/RL strategies, impacting long-CoT reasoning in research. The method is demonstrated by the experiments and analysis on the math datasets.", "summary": "S2R: Teaches LLMs to self-verify and self-correct, boosting reasoning with efficient reinforcement learning.", "takeaways": ["S2R enhances LLM reasoning by teaching models to self-verify and self-correct during inference.", "Reinforcement learning, particularly process-level supervision, improves the validity of self-verification and self-correction.", "The learned self-verifying and self-correcting abilities generalize to out-of-domain tasks."], "tldr": "LLM test-time scaling is effective. Existing methods need big data or training. Less powerful base models\u2019 thinking needs improvement. The paper introduces **S2R**, a framework enhancing LLM reasoning. It teaches models to self-verify/correct during inference. LLMs use iterative self-verification/correction via supervised fine-tuning.  The self-verification/correction skills are strengthened via reinforcement learning. \n\n**S2R** minimizes resources and lets the model refine its reasoning. With 3.1k samples, **Qwen2.5-math-7B** improves from 51.0% to 81.6%, outperforming models trained on equivalent long-CoT data. Results validate **S2R\u2019s** effectiveness on three base models across in/out-of-domain benchmarks. It teaches LLMs to think deeply. LLMs reassess solutions, find mistakes, and refine solutions.", "affiliation": "Tencent", "categories": {"main_category": "Machine Learning", "sub_category": "Reinforcement Learning"}, "podcast_path": "2502.12853/podcast.wav"}