[{"figure_path": "https://arxiv.org/html/2502.14409/x1.png", "caption": "Figure 1: We study query focused summarization with unstructured evidence citation from long contexts", "description": "This figure illustrates the core task of the paper: query-focused summarization with unstructured evidence citation from lengthy source texts.  It shows an example of a user query alongside a long excerpt from a document and highlights the challenge of generating a summary that accurately reflects the document's information while also citing the specific parts of the document that support the summary's claims. The unstructured nature emphasizes that the cited evidence doesn't need to adhere to any pre-defined structure (like sentences or paragraphs), making the task more complex and flexible.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2502.14409/x2.png", "caption": "Figure 2: Six stage inductive data generation pipeline. The full prompts for each stage are given in Appendix A Figure\u00a08 - Figure\u00a016.", "description": "This figure illustrates the six-stage pipeline used to generate the Summaries with Unstructured Evidence Text (SUnsET) dataset.  Each stage involves a different prompt to a large language model (LLM), generating various components of the dataset. The process begins with generating unique titles for fictional and non-fiction documents (P1). An outline of each document is then created (P2), followed by the generation of queries, summaries, and supporting evidence passages (P3).  The evidence passages are then incorporated into the actual document sections (P4).  Subsequently, the summaries and evidence are refined based on the completed document (P5). Finally, the generated data is validated to ensure that the summaries are accurate and the evidence is properly cited (P6).  The prompts used in each of these six stages are detailed in Appendix A (Figures 8-16).", "section": "3 Learning to Cite and Summarize"}, {"figure_path": "https://arxiv.org/html/2502.14409/x3.png", "caption": "Figure 3: Snippets from a SUnsET document.", "description": "This figure shows example snippets from a document in the SUnsET dataset.  It illustrates the structure of a SUnsET document, including the title, a section of the document text, an example query related to the document's content, a snippet of a generated summary that answers the query, and a snippet of the evidence used to support the summary. The snippets highlight the unstructured nature of the evidence citation within the SUnsET dataset.", "section": "3 Learning to Cite and Summarize"}, {"figure_path": "https://arxiv.org/html/2502.14409/x4.png", "caption": "(a) Llama 3.2 1B", "description": "This figure shows the distribution of extracted evidence locations within the source document for the Llama 3.2 1B model using three different methods: standard training, shuffled training, and the base model without fine-tuning. The x-axis represents the relative location of the extracted evidence within the document (0.0 being the beginning and 1.0 being the end), and the y-axis represents the count of extracted evidence at each location. The figure helps visualize the impact of training methods on the positional bias of the model, showing whether the model tends to extract evidence from specific parts of the document (e.g., the beginning or end) or distributes evidence extraction more evenly across the document.", "section": "4.2 RQ2: Is evidence lost-in-the-middle?"}, {"figure_path": "https://arxiv.org/html/2502.14409/x5.png", "caption": "(b) Llama 3.2 3B", "description": "This histogram displays the distribution of extracted evidence locations within the source document for the Llama 3.2 3B model.  The x-axis represents the relative location of the evidence within the document (0.0 being the beginning and 1.0 being the end), and the y-axis shows the frequency or count of extracted evidence at each location. Three sets of results are shown:  the base model (without fine-tuning on the SUnsET dataset), the model fine-tuned with standard context, and the model fine-tuned with shuffled context. The figure helps visualize the model's tendency to focus on specific locations within the document when extracting evidence (positional bias), and the effect of different fine-tuning methods on this bias.  Comparing this to the distribution of ground truth evidence in Figure 5 allows for a more direct assessment of the model's performance.", "section": "4.2 RQ2: Is evidence lost-in-the-middle?"}, {"figure_path": "https://arxiv.org/html/2502.14409/x6.png", "caption": "(c) Llama 3.1 8B", "description": "This figure is a histogram showing the distribution of extracted evidence locations within the provided source context for the Llama 3.1 8B model.  The x-axis represents the relative location of the extracted evidence (0.0 being the beginning and 1.0 being the end of the document), and the y-axis represents the count of evidence instances found at each location. The histogram compares three different methods: the base model, a model fine-tuned with standard context, and a model fine-tuned with shuffled context.  This visualization helps to understand the model's positional bias and the effectiveness of different fine-tuning methods in mitigating this bias.", "section": "4.2 RQ2: Is evidence lost-in-the-middle?"}, {"figure_path": "https://arxiv.org/html/2502.14409/x7.png", "caption": "(d) Mistral Nemo 2407", "description": "This figure is a histogram showing the distribution of extracted evidence locations within the provided source context for the Mistral Nemo 2407 model.  The x-axis represents the location of the extracted evidence (normalized to the range [0, 1]), with 0 representing the beginning of the document and 1 representing the end. The y-axis shows the count of evidence instances at each location.  The histogram allows for a visual comparison of the distribution of evidence across different extraction methods (standard, shuffled, and baseline).  This aids in assessing the impact of the training method on the positioning of extracted evidence within the document, particularly relating to the \"lost-in-the-middle\" phenomenon discussed in the paper.", "section": "4.2 RQ2: Is evidence lost-in-the-middle?"}, {"figure_path": "https://arxiv.org/html/2502.14409/x8.png", "caption": "(e) Mixtral 8x7B", "description": "This histogram displays the distribution of extracted evidence locations within the source document for the Mixtral 8x7B model.  The x-axis represents the relative location of the evidence within the document (0.0 being the beginning and 1.0 being the end), and the y-axis represents the count of evidence snippets found at each location.  The bars are grouped into three categories: \"Base\", representing the baseline model without fine-tuning; \"Standard\", representing the model fine-tuned with the standard SUnsET dataset; and \"Shuffled\", representing the model fine-tuned with the shuffled SUnsET dataset. This visualization helps to understand whether the model's evidence selection exhibits positional bias (favoring evidence from the beginning or end of the document) and how fine-tuning with SUnsET data, with and without shuffling, affects this bias.", "section": "4.2 RQ2: Is evidence lost-in-the-middle?"}, {"figure_path": "https://arxiv.org/html/2502.14409/x9.png", "caption": "(f) GPT 4o Mini", "description": "This figure shows the distribution of extracted evidence locations within the source document for the GPT-40 mini model.  The x-axis represents the location of evidence within the document, ranging from 0.0 (beginning) to 1.0 (end). The y-axis represents the count of extracted evidence spans found at each location. The bars show the distribution for three different methods: Base (the original, unadapted model), Standard (fine-tuned with SUnsET data), and Shuffled (fine-tuned with SUnsET data with shuffled document sections). This visualization helps illustrate the presence of positional bias (lost-in-the-middle) in language models and the effect of the proposed fine-tuning method on mitigating this bias.", "section": "4.2 RQ2: Is evidence lost-in-the-middle?"}, {"figure_path": "https://arxiv.org/html/2502.14409/x10.png", "caption": "Figure 4: Location of extracted evidence in the provided source context for different methods.", "description": "This figure displays histograms showing the distribution of extracted evidence locations within source documents for various methods.  Each histogram represents a different large language model (LLM) and shows the relative frequency of evidence extracted from different positions (beginning, middle, or end) within the document.  The comparison allows for analysis of how different LLMs and processing techniques (such as standard vs. shuffled fine-tuning) affect where evidence is selected from within a given document context.", "section": "4.2 RQ2: Is evidence lost-in-the-middle?"}, {"figure_path": "https://arxiv.org/html/2502.14409/x11.png", "caption": "(a) SQuALITY", "description": "This figure shows the distribution of ground truth evidence locations within the SQuALITY dataset.  The x-axis represents the relative location of the evidence within the document, ranging from 0.0 (beginning) to 1.0 (end). The y-axis shows the count of evidence instances at each location. The figure helps visualize the distribution of evidence across different sections of the documents in this dataset, indicating whether there's a bias toward the beginning, middle, or end of the documents.", "section": "4.2 RQ2: Is evidence lost-in-the-middle?"}, {"figure_path": "https://arxiv.org/html/2502.14409/x12.png", "caption": "(b) LexAbSumm", "description": "Figure 5(b) shows the distribution of ground truth evidence locations in the LexAbSumm dataset.  LexAbSumm consists of long legal documents, focusing on the location of evidence within those documents. The x-axis represents the relative location of evidence within the document (from 0.0 to 1.0), and the y-axis represents the count of evidence at each location. This visualization helps illustrate the distribution of evidence throughout the documents and can be used to understand any positional biases present in the data or in models trained on this data.  The graph shows a relatively uniform distribution of evidence across the documents, suggesting there is no strong positional bias towards either the beginning or end of the documents within this dataset.", "section": "4.2 RQ2: Is evidence lost-in-the-middle?"}, {"figure_path": "https://arxiv.org/html/2502.14409/x13.png", "caption": "(c) SummHay", "description": "The figure shows the distribution of ground truth evidence locations in the SummHay dataset.  The x-axis represents the relative location of evidence within the document (0.0 being the beginning and 1.0 being the end). The y-axis represents the count of evidence instances at each location. The distribution illustrates where the actual evidence is present in the documents, helping to understand any inherent biases in evidence placement within the dataset.", "section": "4.2 RQ2: Is evidence lost-in-the-middle?"}, {"figure_path": "https://arxiv.org/html/2502.14409/x14.png", "caption": "(d) ScholarQABench", "description": "Figure 5(d) presents the distribution of ground truth evidence locations within the ScholarQABench dataset.  ScholarQABench is a multi-document dataset consisting of computer science research papers. The x-axis represents the relative location of evidence within a document (0.0 being the beginning, 1.0 being the end), and the y-axis represents the count of evidence instances at each location. This visualization helps to understand the distribution of evidence within the documents of this specific dataset, allowing for a comparison with the distribution of evidence extracted by different LLMs (as shown in other subfigures of Figure 5 and Figure 4).", "section": "4.2 RQ2: Is evidence lost-in-the-middle?"}, {"figure_path": "https://arxiv.org/html/2502.14409/x15.png", "caption": "Figure 5: Location of ground truth evidence in each dataset.", "description": "This figure presents four histograms, one for each of the datasets used in the study (SQUALITY, LexAbSumm, SummHay, and ScholarQABench). Each histogram visualizes the distribution of ground truth evidence locations within the documents of the corresponding dataset.  The x-axis represents the relative location of the evidence within the document (0.0 being the beginning and 1.0 being the end), and the y-axis represents the count of evidence instances at each location.  This figure helps illustrate whether there's a bias in the location of evidence within documents, which is related to the 'lost-in-the-middle' phenomenon discussed in the paper.", "section": "4.2 RQ2: Is evidence lost-in-the-middle?"}, {"figure_path": "https://arxiv.org/html/2502.14409/x16.png", "caption": "(a) Llama 3.2 1B", "description": "This figure shows the location of extracted evidence within the source document for the Llama 3.2 1B model.  The x-axis represents the relative location of the evidence in the document (0.0 being the beginning and 1.0 being the end). The y-axis shows the count of evidence extracted at each location. Three bars are shown for each model: 'Base' (the unembellished model), 'Standard' (fine-tuned with SUnsET data in its original order), and 'Shuffled' (fine-tuned with SUnsET data with sections randomly shuffled).  This visualization helps illustrate the effect of fine-tuning and shuffled training on the tendency of the model to extract evidence primarily from the beginning or end of the document ('lost-in-the-middle' effect).", "section": "4.2 RQ2: Is evidence lost-in-the-middle?"}, {"figure_path": "https://arxiv.org/html/2502.14409/x17.png", "caption": "(b) Llama 3.1 8B", "description": "This figure is a histogram showing the location of extracted evidence within the source document for the Llama 3.1 8B language model.  The x-axis represents the relative location of evidence in the document (0.0 being the beginning, 1.0 being the end), and the y-axis represents the count of evidence instances found at each location. The histogram compares the distribution of evidence extracted by three different methods: the base model, the model fine-tuned with standard context, and the model fine-tuned with shuffled context.  The purpose is to visualize the impact of different fine-tuning methods on the positional bias of the model in evidence extraction. ", "section": "4.2 RQ2: Is evidence lost-in-the-middle?"}]