{"references": [{"fullname_first_author": "Akari Asai", "paper_title": "OpenScholar: Synthesizing Scientific Literature with Retrieval-Augmented Language Models", "publication_date": "2024-11-09", "reason": "This paper is an important reference for generating scientific literature using retrieval-augmented language models, a technique used as an inspiration in this paper."}, {"fullname_first_author": "Nelson F. Liu", "paper_title": "Lost in the Middle: How Language Models Use Long Contexts", "publication_date": "2024-01-01", "reason": "This paper is crucial because it identifies and analyzes the 'lost-in-the-middle' problem, which is also a key focus of the current research on evidence attribution in long contexts."}, {"fullname_first_author": "Huan Yee Koh", "paper_title": "An Empirical Survey on Long Document Summarization: Datasets, Models, and Metrics", "publication_date": "2023-01-01", "reason": "Koh et al.'s work is one of the key prior works in the field of long document summarization."}, {"fullname_first_author": "Philippe Laban", "paper_title": "Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems", "publication_date": "2024-01-01", "reason": "This paper is among the most important since it addresses the challenge of summarizing long contexts, which is a central focus of this paper, and is one of the datasets evaluated against."}, {"fullname_first_author": "Edward J. Hu", "paper_title": "LoRA: Low-Rank Adaptation of Large Language Models", "publication_date": "2022-01-01", "reason": "This paper is very important because it describes the Low-Rank Adaptation (LoRA) technique which is used for fine-tuning the LLMs in the described study."}]}