[{"figure_path": "https://arxiv.org/html/2502.14382/x1.png", "caption": "Figure 1: Performance improvement with\u00a0S\u2217superscript\ud835\udc46S^{*}italic_S start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT in LiveCodeBench (v2)\u00a0(Jain et\u00a0al., 2024). S\u2217superscript\ud835\udc46S^{*}italic_S start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT consistently improves models across different sizes, allowing non-reasoning models to surpass reasoning models and open models to be competitive with o1 (high reasoning effort). \"Qwen-Coder\" denotes \"Qwen2.5-Coder-Instruct,\"\u00a0(Hui et\u00a0al., 2024) and \"R1-Distill\" denotes \"DeepSeek-R1-Distill-Qwen.\" \u00a0(Guo et\u00a0al., 2025).", "description": "Figure 1 shows the performance improvement achieved by the proposed test-time scaling framework, S*, on various code generation models using LiveCodeBench (v2).  The bar chart compares the Pass@1 (the percentage of problems solved by the top-ranked solution) for each model with and without S*.  Results show that S* consistently boosts performance across different model sizes and types.  Importantly, it enables non-reasoning models to outperform reasoning models and open-source models to reach a performance level comparable to the state-of-the-art closed model, o1 (which represents high reasoning effort).  Model names are clarified with their full names in the caption (e.g., Qwen-Coder denotes Qwen2.5-Coder-Instruct).", "section": "4.2 S* Main Results"}, {"figure_path": "https://arxiv.org/html/2502.14382/x2.png", "caption": "Figure 2: \nOverview of S\u2217superscript\ud835\udc46S^{*}italic_S start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT. Stage 1: Generation\u2014S\u2217superscript\ud835\udc46S^{*}italic_S start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT enhances parallel samples through iterative debugging. Each sample is tested using public test cases executed via an interpreter, with outputs and/or error messages used to guide the next round of sample generation.\nStage 2: Selection\u2014S\u2217superscript\ud835\udc46S^{*}italic_S start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT selects the best sample by prompting an LLM to generate inputs that differentiate between paired samples, then leveraging actual execution results to inform the LLM to determine the optimal choice.", "description": "Figure 2 illustrates the two-stage S* framework for code generation. Stage 1 (Generation) enhances parallel code generation by iteratively debugging each generated sample using public test cases.  The outputs and errors from test execution guide subsequent rounds of code generation, refining the samples. Stage 2 (Selection) uses an LLM to generate distinguishing test inputs for pairs of generated code samples. The execution results of these test inputs on each sample are fed back to the LLM, allowing it to select the best performing code sample.", "section": "3 Method"}, {"figure_path": "https://arxiv.org/html/2502.14382/x3.png", "caption": "Figure 3: \nAblation of S\u2217superscript\ud835\udc46S^{*}italic_S start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT performance benefits: Qwen2.5-Coder-14B-Instruct (denoted as Qwen-Coder-14B)\u00a0(Hui et\u00a0al., 2024) with S\u2217superscript\ud835\udc46S^{*}italic_S start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT can surpass o1-preview without S\u2217superscript\ud835\udc46S^{*}italic_S start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT. DeepSeek-R1-Distill-Qwen-14B (denoted as R1-Distill-14B)\u00a0(Guo et\u00a0al., 2025) with S\u2217superscript\ud835\udc46S^{*}italic_S start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT outperforms o1-mini without S\u2217superscript\ud835\udc46S^{*}italic_S start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT.", "description": "This figure displays the performance improvements achieved by the proposed test-time scaling framework, S*. It shows that incorporating S* enhances the performance of both instruction-based models (Qwen2.5-Coder-14B) and reasoning-based models (R1-Distill-14B), enabling them to surpass the performance of baseline models (o1-preview and o1-mini) without the test-time scaling method. The ablation study demonstrates that S* significantly improves the coverage and selection accuracy of generated code.", "section": "4 Evaluation"}, {"figure_path": "https://arxiv.org/html/2502.14382/x4.png", "caption": "Figure 4: The effect of hyper-parameters. Left: The impact of temperature. A moderate temperature (0.7) balances diversity and quality, leading to higher Pass@N. In contrast, a higher temperature (0.95) does not further improve Pass@N, potentially degrading code quality. Right: The effect of increasing the number of samples. Performance improves log-linearly.", "description": "This figure shows the effects of two key hyperparameters on the performance of the S* model: temperature and the number of samples.  The left panel illustrates the relationship between temperature and Pass@N (the percentage of problems solved by the best sample).  It demonstrates that a moderate temperature (0.7) optimizes the balance between generating diverse code samples and ensuring high-quality code, resulting in the best performance. Higher temperatures (0.95) fail to significantly improve performance and may even reduce code quality.  The right panel shows how performance scales with the number of parallel samples. It exhibits a clear log-linear relationship, indicating that increasing the number of samples leads to steadily improved performance.", "section": "5 Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2502.14382/x5.png", "caption": "Figure 5: Performance with in-context examples across different numbers of parallel samples (N\ud835\udc41Nitalic_N), for GPT-4o mini, Qwen2.5-Coder-7B-Instruct, and Qwen2.5-Coder-32B-Instruct.", "description": "This figure displays the impact of in-context examples on the performance of three different language models (GPT-4o mini, Qwen-2.5-Coder-7B-Instruct, and Qwen-2.5-Coder-32B-Instruct) with varying numbers of parallel samples (N).  It shows how the addition of in-context examples affects the Pass@N (the percentage of problems solved correctly by at least one of N samples) across different numbers of parallel samples. This allows for an analysis of the interaction between parallel sampling and in-context learning, demonstrating how different models respond to varying numbers of samples and whether the inclusion of in-context examples enhances the effect of increasing the sample size.", "section": "5 Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2502.14382/x6.png", "caption": "Figure 6: Comparison of three iterative debugging approaches: Public Tests, + Generated Tests and Last Round Context. Results are obtained with N=8\ud835\udc418N=8italic_N = 8, temperature=0.7temperature0.7\\text{temperature}=0.7temperature = 0.7 and up to four rounds of debugging.", "description": "Figure 6 illustrates a comparison of three different iterative debugging strategies used in the S* framework. These strategies are: using only public test cases; using public test cases plus model-generated test cases; and using only the last round of code sample as context when debugging. The results show the performance for each of these methods across multiple rounds of debugging, using 8 parallel samples, a temperature of 0.7, and up to 4 debugging rounds. The figure visually displays how the performance changes as more iterations of debugging are performed for each method.", "section": "5.3 Impact of Iterative Debugging Variants"}, {"figure_path": "https://arxiv.org/html/2502.14382/x7.png", "caption": "Figure 7: The prompt for iterative debugging.", "description": "This prompt guides the LLM to iteratively refine its code generation by providing the previous round's reasoning, generated code, and test feedback. The LLM is instructed to reason about why the previous code failed and to correct it. The prompt ensures that the LLM only focuses on code generation, preventing non-code content in the code field.", "section": "3 Method"}, {"figure_path": "https://arxiv.org/html/2502.14382/x8.png", "caption": "Figure 8: The prompt for generating test cases.", "description": "This figure displays the prompt template used to instruct a large language model (LLM) to generate comprehensive test cases for a given coding problem. The prompt guides the LLM to create diverse test cases, including edge cases, complex scenarios, and cases designed to maximize the chance of detecting bugs.  The expected output is structured as a JSON array where each element contains an input and its corresponding expected output. This structured output aids in efficient evaluation and automated verification of code solutions.", "section": "3 Method"}, {"figure_path": "https://arxiv.org/html/2502.14382/x9.png", "caption": "Figure 9: The prompt for code generation.", "description": "This figure shows the prompt template used for the code generation stage in the S* framework.  The prompt instructs the LLM to generate only the body of a Python function based on a given problem description, without including any extra text or comments.  It's designed to elicit concise and functional code from the model. The template is structured to facilitate interaction with the model and manage responses in a clear, consistent format for processing and evaluation.", "section": "3 Method"}]