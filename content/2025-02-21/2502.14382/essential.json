{"importance": "This paper introduces a novel test-time scaling framework that **significantly improves code generation performance** in LLMs. The findings demonstrate that S* can consistently improve model performance and generalization across diverse models and code generation benchmarks. This opens new research avenues for test-time scaling techniques in code generation.", "summary": "S*: Hybrid test-time scaling for code generation, boosting both coverage and selection accuracy.", "takeaways": ["S* consistently improves performance across model families and sizes, surpassing GPT-40-mini with a 3B model.", "S* enables non-reasoning models to outperform reasoning models.", "S* enhances state-of-the-art reasoning models, approaching o1(high) performance."], "tldr": "Increasing test-time compute shows promise for LLMs but remains underexplored in code generation. Existing parallel scaling improves solution coverage, while sequential refinement enhances individual samples. Validating code needs executing tests to ensure accuracy. Challenges arise in designing reward models due to the complexity of execution. But, code offers advantage, programmatic interpreters for precise outputs.\n\nTo solve this, the paper introduces S*, a hybrid test-time scaling framework for code generation. **S* substantially improves the coverage and selection accuracy of generated code.** S* extends parallel scaling with sequential scaling and leverages selection mechanism to generate distinguishing inputs for comparison. Grounded execution info helps identify the correct solution.", "affiliation": "UC Berkeley", "categories": {"main_category": "Machine Learning", "sub_category": "Deep Learning"}, "podcast_path": "2502.14382/podcast.wav"}