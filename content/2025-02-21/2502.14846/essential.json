{"importance": "CoSyn offers a promising approach to **boost VLM performance** by generating synthetic data, which helps address the lack of diverse text-rich data. This research opens new avenues for **developing multimodal agents** and **improving VLM capabilities** in real-world applications.", "summary": "CoSyn: Code-guided synth data for scaling text-rich image understanding, achieving SOTA via targeted multimodal data generation!", "takeaways": ["Code-guided synthetic data generation enhances VLM performance on text-rich images.", "CoSyn improves generalization to new tasks with targeted synthetic data.", "Synthetic data helps mitigate biases in multimodal benchmarks."], "tldr": "Vision-Language Models (VLMs) struggle with text-rich images due to limited data. They tend to over-rely on language priors. The paper tackles this by introducing **CoSyn**, a framework that uses code-guided generation to create synthetic data for vision-language instruction tuning. It leverages the coding capabilities of text-only LLMs to automatically create synthetic text-rich multimodal data. Given input text describing a target domain, it prompts an LLM to generate code for rendering synthetic images. \n\nUsing CoSyn, the authors constructed a large-scale dataset of 400K images and 2.7M rows of instruction-tuning data. Experiments on seven benchmarks show that models trained on this data achieve state-of-the-art performance. CoSyn also enables sample-efficient learning and improves performance on tasks requiring multi-hop reasoning. It can synthesize pointing data, enabling VLMs to ground information within input images. This showcases the potential for developing multimodal agents capable of acting in real-world environments.", "affiliation": "University of Pennsylvania", "categories": {"main_category": "Multimodal Learning", "sub_category": "Vision-Language Models"}, "podcast_path": "2502.14846/podcast.wav"}