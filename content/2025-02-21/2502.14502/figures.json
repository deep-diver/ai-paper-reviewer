[{"figure_path": "https://arxiv.org/html/2502.14502/x1.png", "caption": "Figure 1: Decrease in quality with increase of new facts learned by the model: results of the fine-tuned Llama-3.1-8B-Instruct on TruthfulQA (solid line corresponds to the mean score, error margin \u2013 to the min/max scores of three runs with different random seeds).", "description": "This figure displays the performance of a fine-tuned Llama-3.1-8B-Instruct language model on the TruthfulQA benchmark as the amount of new knowledge incorporated during training increases.  The solid line represents the average accuracy across three independent trials (with different random seeds), and the shaded area shows the range (minimum to maximum) of those accuracy scores.  The x-axis indicates the number of 'unknown' facts (facts not present in the model's pre-training data) added to the training set.  The y-axis shows the accuracy on the TruthfulQA benchmark. The results illustrate a negative correlation: As more new facts are added, the model's accuracy on TruthfulQA tends to decrease.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2502.14502/x4.png", "caption": "Figure 2: Dynamics of the reliability score during training on 500 (left) and 3,000 (right) Unknown items along with paraphrases and HighlyKnown facts. Error bar is min-max for 3 seed run.", "description": "This figure displays the reliability scores across multiple training epochs for two different datasets (500 and 3000 Unknown items) while incorporating paraphrased and HighlyKnown facts.  The reliability metric reflects the model's ability to consistently generate correct answers, illustrating the impact of additional training data on model performance.  Separate lines represent different training approaches (using 0, 1, or 10 paraphrases or HighlyKnown facts per unknown item). The min-max error bars shown for each data point reflect variability across three independent training runs, suggesting the consistency of results.", "section": "5 Analysis"}, {"figure_path": "https://arxiv.org/html/2502.14502/x7.png", "caption": "Figure 3: MMLU: Accuracy dependent on the amount of Unknown learned. Pointed horizontal line indicates the baseline. Models trained with less additional data tend to disrupt reasoning less.", "description": "This figure displays the performance of the fine-tuned Llama-3.1-8B-Instruct model on the MMLU (Massive Multitask Language Understanding) benchmark as a function of the amount of new, previously unknown knowledge introduced during the fine-tuning process using LoRA (Low-Rank Adaptation).  The x-axis represents the number of 'Unknown' facts added to the training dataset. The y-axis shows the accuracy achieved on the MMLU benchmark.  A horizontal line represents the baseline accuracy of the original, untrained model. The graph illustrates that increasing the amount of new knowledge initially improves the model's performance on MMLU; however, beyond a certain point, adding more new facts leads to a decrease in accuracy, indicating that integrating too much novel information into the model via LoRA can negatively affect its overall reasoning abilities.  The results suggest a trade-off between incorporating new knowledge and maintaining the model\u2019s general capabilities. Less additional data during training leads to less disruption in reasoning.", "section": "5.3 Benchmarks"}]