[{"Alex": "Welcome back to the podcast, everyone! Today, we're diving into the future of healthcare \u2013 a world where your medical assistant fits right on your wrist, no internet required! We\u2019re unpacking a fascinating study on a multi-agent medical assistant for edge devices.", "Jamie": "That sounds incredibly cool! So, no more relying on patchy Wi-Fi when I need to book a doctor's appointment? I\u2019m Jamie, and I'm ready to learn all about it."}, {"Alex": "Exactly, Jamie! This paper explores a system designed to run directly on devices like smartwatches, addressing privacy and latency issues that plague current healthcare apps. It's about bringing intelligence closer to the user, literally.", "Jamie": "Okay, so it's like having a personal doctor that doesn't require internet. Neat. What are 'Large Action Models (LAMs)' and their role in all this?"}, {"Alex": "LAMs are essentially the brains behind intelligent automation, breaking down complex tasks into smaller, agent-specific components. However, the size and reliance on cloud infrastructure make them challenging for on-device implementation.", "Jamie": "Hmm, so, traditionally, they're too big and clunky to fit on something like a smartwatch. Makes sense. And that's where this research comes in, right? How did they tackle that problem?"}, {"Alex": "Precisely. The researchers proposed a multi-agent system using smaller, task-specific agents. By distributing the workload, they optimized resource allocation, enabling these models to operate effectively on edge devices.", "Jamie": "So instead of one giant brain, it's a team of specialized mini-brains working together. Genius! What kind of tasks can this system actually handle?"}, {"Alex": "Think of it as a one-stop healthcare solution. The system can handle appointment booking, medication reminders, health monitoring through wearable sensors, and even generating daily health reports. The ultimate goal is user-centric healthcare solutions. Very personalized too.", "Jamie": "Wow, that covers a lot of ground. It sounds incredibly convenient. How about emergencies? Can it handle those too?"}, {"Alex": "Yes, it includes an 'Emergency SOS' feature. It has the capabilities to trigger immediate assistance in critical situations. One of the key highlights include instantly notify emergency services or contacts.", "Jamie": "Okay, that\u2019s reassuring. So, if I fall and can't reach my phone, my watch can automatically call for help? Now that's a safety net I can get behind. Tell me how smart are these agents?"}, {"Alex": "The Planner and Caller Agents are powered by a model called the Qwen Code Instruct 2.5 7B. They finetuned it to perform planning and calling to incorporate planning and function calling abilities into their usecases.", "Jamie": "Okay, what exactly does planner and caller agents mean? Also, how do they improve from the baseline LLMs?"}, {"Alex": "The Planner Agent plans out a workflow according to user query with reasoning. Caller is responsible to call to external systems according to plan. The authors finetuned it for planning and calling to incorporate planning and function calling abilities into their usecases and improved the result, especially when there are many function calls.", "Jamie": "Function calling is definitely a plus when there are so many usecases of healthcare tasks to handle, umm... How did they train these agents to be so effective?"}, {"Alex": "That\u2019s a great question! They actually created a synthetic dataset specifically designed to train these agents. This involved generating and verifying trajectories, enhancing data quality, and then interleaving the data for the Planner and Caller agents.", "Jamie": "A synthetic dataset... so, not real patient data? That addresses some of the privacy concerns we talked about earlier. How about performance?"}, {"Alex": "Exactly. The generated dataset ensures a safe development for edge devices. On this generated dataset, the planner and caller agents have great planning with average RougeL scores of 85.5 and 96.5 respectively. It is also scalable and private.", "Jamie": "Okay, this sounds very promising. I wonder what are the current limitations and future direction of the research?"}, {"Alex": "The system does struggle with complex multimodal reasoning for generating health reports and analyzing medical images, as the models run on the edge devices. As for the future, the authors aim to transform the trained modules into formats compatible with on-device applications. It includes supporting other wearable devices, and multimodal understanding.", "Jamie": "Hmm, I see. So, the current focus is on improving existing functionalities and device compatibility. Makes sense. "}, {"Alex": "That's right. What's also exciting is that the team wants to improve a multimodal understanding engine by incorporating image-based diagnosis.", "Jamie": "Aha, using images for assistance definitely enhances the ability of healthcare assistant. But privacy is also a concern if we were to allow the model to analyze images. Very exciting and concerning though!"}, {"Alex": "The authors do believe that the architecture is private and it can run on edge devices, so the images does not need to leave the device for it to work. This is something the authors will need to consider.", "Jamie": "That's great, especially in the healthcare industry. What are the other challenges? Do you think that it is close to real world deployments?"}, {"Alex": "It is unclear on the paper on the challenges ahead. There is no mention about user feedbacks about the architecture. It would require real world usage by healthcare professionals and everyday consumers for it to be adopted.", "Jamie": "Aha, that's where we will learn if we are missing any blind spots!"}, {"Alex": "Indeed! One blind spot the authors mentioned is the lack of diverse dataset. As the model is private, it is hard to generate a diverse dataset that generalizes well.", "Jamie": "This also means that it is hard for other companies to use the architecture for development and research, right?"}, {"Alex": "There is a link to Github in the beginning of the paper, but it seems like it is not a fully deployable version of the architecture. It would require a lot of effort to transform it to a distributable format.", "Jamie": "That sounds like a great starting point, but with more work to get there. It seems like the authors wants to take that on to the next stage!"}, {"Alex": "Yes! I'm excited and concerned to see what we can do with it.", "Jamie": "I think this would potentially disrupt the way that users access healthcare for general population."}, {"Alex": "It definitely aligns with the vision of making healthcare personalized and accessible. And this will open up innovation and research to bring healthcare to edge devices.", "Jamie": "Umm, well, this has been an incredibly informative discussion, Alex! Thanks for shedding light on this exciting research."}, {"Alex": "My pleasure, Jamie! The idea that your smartwatch could one day be your personal healthcare assistant is a powerful one, and this research is a significant step in that direction.", "Jamie": "Where can the audiences find the Github link? And can you summarize this paper in a click bait sentence?"}, {"Alex": "You can find the Github link right on the abstract of the paper! So, to wrap it up, this paper introduces a private healthcare assistant that runs on your devices. Also, the team shows promising direction of making healthcare available to the edge! What do you think?", "Jamie": "That's so cool! Thanks Alex for everything!"}]