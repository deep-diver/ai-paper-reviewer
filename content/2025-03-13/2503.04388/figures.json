[{"figure_path": "https://arxiv.org/html/2503.04388/extracted/6257630/sections/main_fig.jpg", "caption": "Figure 1: More Documents, Same Length. We create various sets containing the same questions but differing in the number of distractor documents. Each set includes a multi-hop question, all of the supporting documents that contain the information to answer the question (pink), and varying distractor documents (blue). We begin with a 20-document version (left) and then reduce the number of documents while maintaining a fixed context size. When fewer documents are used, the remaining documents are extended (blue without text) so that concatenating them yields the same total length.", "description": "Figure 1 illustrates how the researchers created datasets for their experiment.  They started with multi-hop questions and 20 related documents for each question. Some documents contained information needed to answer the question (supporting documents, shown in pink), while others were distractors (shown in blue).  To isolate the effect of document number on model performance, they kept the total context length constant across different datasets. They varied the number of distractor documents,  extending the remaining documents when they reduced the distractor count to maintain a constant overall length. This ensured that the length of the context didn't affect the results, only the number of documents.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2503.04388/extracted/6257630/figures/plots_febuary/combining_all_models_reorder_original_docs.png", "caption": "Figure 2: Increasing the number of retrieved document can hurt performance.\nIn retrieval setups with fixed context windows, adding more documents could reduce performance by up to 10 percent. Two models (Llama-\n3.1 and Gemma-2) showed worse performance, while Qwen-2 remained unaffected. The smaller versions of the LLMs (7\u20139B) show a similar trend as their larger counterparts but the effect is weaker. The hues of the bars represent the amount of retrieved documents.", "description": "This figure displays the results of an experiment evaluating the effect of the number of retrieved documents on the performance of various Large Language Models (LLMs) in a retrieval-augmented generation (RAG) task.  The x-axis represents different numbers of retrieved documents, while the y-axis shows the F1 score, a measure of model performance.  Six LLMs of varying sizes (7B, 8B, 9B, 27B, 70B, and 72B parameters) are evaluated.  The results show that for most LLMs, increasing the number of documents generally leads to a decrease in performance (up to 10% reduction in F1 score) when the total context length is kept constant.  The exception is the Qwen-2 model, which shows relatively stable performance regardless of the document count.  Smaller versions of the models exhibit similar trends, but the effect is less pronounced.", "section": "3 Evaluation"}, {"figure_path": "https://arxiv.org/html/2503.04388/extracted/6257630/figures/plots_febuary/combining_all_models_reorder_replaced.png", "caption": "Figure 3: The effects of adding non-related documents. When adding irrelevant documents, LLMs\u2019 performance improves.", "description": "Figure 3 presents the results of an experiment assessing the impact of adding irrelevant documents to the input on the performance of Large Language Models (LLMs). The experiment shows that contrary to the negative effect of adding relevant but distracting documents, adding completely unrelated documents improved the performance of the LLMs.  This suggests that the presence of some unrelated documents may not significantly hinder the ability of LLMs to identify and utilize the relevant information to answer questions, or may even mitigate the confusion caused by many related but irrelevant pieces of information.", "section": "3.3 Analysis"}]