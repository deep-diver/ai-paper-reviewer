{"references": [{"fullname_first_author": "Wei-Lin Chiang", "paper_title": "Chatbot arena: An open platform for evaluating LLMs by human preference", "publication_date": "2024-01-01", "reason": "This reference is important as it describes Chatbot Arena, the source of the real-world instructions used in creating the WILDIFEVAL dataset."}, {"fullname_first_author": "Lianmin Zheng", "paper_title": "Lmsys-chat-1m: A large-scale real-world Ilm conversation dataset", "publication_date": "2023-09-01", "reason": "This reference is important because it describes LMSYS-Chat-1M, the large-scale dataset containing real-world instructions collected from the Chatbot Arena, from which the task instructions in WILDIFEVAL were extracted."}, {"fullname_first_author": "Thomas Palmeira Ferraz", "paper_title": "LLM self-correction with DeCRIM: Decompose, critique, and refine for enhanced following of instructions with multiple constraints", "publication_date": "2024-01-01", "reason": "This paper defines constrained generation tasks, provides the prompt which was the basis for constrained generation filtering, and their prompt to automatically extract the constraints for each of the tasks."}, {"fullname_first_author": "Jeffrey Zhou", "paper_title": "Instruction-following evaluation for large language models", "publication_date": "2023-11-01", "reason": "This paper describes IFEval, a prominent example of rule-based evaluation used for synthetic instructions, against which the current paper compares."}, {"fullname_first_author": "Dan Hendrycks", "paper_title": "Measuring massive multitask language understanding", "publication_date": "2020-09-01", "reason": "This paper presents MMLU, a significant benchmark and a good baseline for assessing WILDIFEVAL which shows high Kendall's Tau correlations to the WILDIFEVAL results."}]}