[{"figure_path": "https://arxiv.org/html/2502.02492/x2.png", "caption": "Figure 1: Text-to-video samples generated by VideoJAM. We present VideoJAM, a framework that explicitly instills a strong motion prior to any video generation model. Our framework significantly enhances motion coherence across a wide variety of motion types.", "description": "Figure 1 showcases example videos generated using the VideoJAM model, highlighting its ability to generate high-quality videos with coherent motion across diverse motion types.  The model takes text descriptions as input and produces videos that accurately reflect the described motion, improving on the limitations of previous video generation models that often struggle with realistic and fluid movement. The examples demonstrate various scenarios, ranging from a ballet dancer twirling to a skateboarder performing jumps, illustrating VideoJAM's versatility.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2502.02492/x3.png", "caption": "Figure 2: Motion incoherence in video generation. Examples of incoherent generations by DiT-30B\u00a0(Peebles & Xie, 2023). The model struggles with (a) basic motion, e.g., jogging (stepping on the same leg repeatedly); (b) complex motion e.g., gymnastics; (c) physics, e.g., object dynamics (the hoop passes through the woman); and (d) rotational motion, failing to replicate simple repetitive patterns.", "description": "Figure 2 showcases examples of motion incoherence produced by the DiT-30B video generation model.  Panel (a) demonstrates failure in generating basic, repetitive motions like jogging, showing the model's inability to accurately represent repeated actions such as foot placement. Panel (b) highlights problems with complex motions, such as gymnastics, illustrating issues like severe body deformation. Panel (c) illustrates difficulties with physical accuracy, with an example of a hula hoop passing unrealistically through a person, defying the laws of physics. Finally, panel (d) displays the model's struggle with rotational motions, showing its inability to produce simple, repetitive rotational movements.", "section": "3. Motivation"}, {"figure_path": "https://arxiv.org/html/2502.02492/x4.png", "caption": "Figure 3: Motivation Experiment. We compare the model\u2019s loss before and after randomly permuting the video frames, using a \u201cvanilla\u201d DiT (orange) and our fine-tuned model (blue). The original model is nearly invariant to temporal perturbations for t\u226460\ud835\udc6160t\\leq 60italic_t \u2264 60.", "description": "This figure displays the results of an experiment designed to assess the sensitivity of video generation models to temporal coherence.  Two versions of videos were used: original videos and videos with frames randomly permuted.  These videos were input to two models: a standard Diffusion Transformer (DiT) and the same model after fine-tuning with the VideoJAM method. The loss, a measure of the difference between model predictions and the original video, was calculated for each model at various denoising steps (t). The experiment reveals that the original DiT model shows almost no difference in loss between the original and the permuted videos until denoising step 60. This indicates that the model is largely unaffected by temporal inconsistencies until a late stage of the generation process. Conversely, the VideoJAM model exhibits a significant difference in loss, demonstrating its increased sensitivity to temporal coherence.", "section": "3. Motivation"}, {"figure_path": "https://arxiv.org/html/2502.02492/x5.png", "caption": "Figure 4: VideoJAM Framework. VideoJAM is constructed of two units; (a) Training. Given an input video x1subscript\ud835\udc651x_{1}italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and its motion representation d1subscript\ud835\udc511d_{1}italic_d start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, both signals are noised and embedded to a single, joint latent representation using a linear layer, Wi\u2062n+subscriptsuperscriptW\ud835\udc56\ud835\udc5b\\textbf{W}^{+}_{in}W start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i italic_n end_POSTSUBSCRIPT. The diffusion model processes the input, and two linear projection layers predict both appearance and motion from the joint representation. (b) Inference. We propose Inner-Guidance, where the model\u2019s own noisy motion prediction is used to guide the video prediction at each step.", "description": "VideoJAM is composed of two units: a training unit and an inference unit.  The training unit takes an input video (x<sub>1</sub>) and its motion representation (d<sub>1</sub>) as input.  Both are noised and combined into a single latent representation using a linear layer (W<sup>+</sup><sub>in</sub>). A diffusion model processes this combined representation. Two linear projection layers then predict both the appearance and the motion from this joint representation. The inference unit uses the model's own noisy motion prediction as a dynamic guidance signal to steer the video generation toward temporal coherence at each step. This is achieved through a mechanism called 'Inner-Guidance'.", "section": "4. VideoJAM"}, {"figure_path": "https://arxiv.org/html/2502.02492/x6.png", "caption": "Figure 5: Text-to-video results by VideoJAM-30B. VideoJAM enables the generation of a wide variety of motion types, from basic motion (e.g., running) to complex motion (e.g., acrobatics), and improved physics (e.g., jumping over a hurdle).", "description": "Figure 5 showcases various video clips generated using the VideoJAM-30B model.  The examples highlight the model's capacity to generate diverse and complex motions, ranging from simple actions like running to more intricate movements like acrobatics. The improved physics simulation capabilities are also demonstrated, with a clip illustrating a dog successfully jumping over a hurdle, a task that many previous video generation models have struggled with. The figure emphasizes the enhanced motion coherence and realistic physics achieved by VideoJAM.", "section": "4. VideoJAM"}, {"figure_path": "https://arxiv.org/html/2502.02492/x7.png", "caption": "Figure 6: Qualitative comparisons between VideoJAM-30B and the leading baselines- Sora, Kling, and DiT-30B on representative prompts from VideoJAM-bench. The baselines struggle with basic motion, displaying \u201cbackward motion\u201d (Sora, 2nd row) or unnatural motion (Kling, 2nd row). The generated content defies the basic laws of physics e.g., people passing through objects (DiT, 1st row), or objects that appear or evaporate (Sora, DiT, 4th row). For complex motion, the baselines display static motion or deformations (Sora, Kling, 1st, 3rd row). Conversely, in all cases, VideoJAM produces temporally coherent videos that better adhere to the laws of physics.", "description": "Figure 6 presents a qualitative comparison of video generation results between VideoJAM-30B and three leading baselines (Sora, Kling, and DiT-30B).  The comparison uses prompts selected from the VideoJAM-bench dataset, designed to challenge video generation models with various motion types. The results highlight the limitations of the baselines, which frequently exhibit motion incoherence and inconsistencies with the laws of physics.  Specific issues demonstrated by the baselines include backward motion (Sora), unnatural motion (Kling), objects passing through each other (DiT), and objects spontaneously appearing or disappearing (Sora, DiT).  These problems were observed in both basic motions (e.g., jogging) and complex motions (e.g., gymnastics). In contrast, VideoJAM-30B consistently generated videos with temporally coherent motion that adhered to the laws of physics.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.02492/x8.png", "caption": "Figure 7: Limitations. Our method is less effective for: (a) motion observed in \u201czoom-out\u201d (the moving object covers a small part of the frame). (b) Complex physics of object interactions.", "description": "Figure 7 demonstrates the limitations of the VideoJAM model.  Specifically, it shows that VideoJAM's performance is reduced when the motion is observed from a distance where the moving object occupies only a small portion of the frame (a \"zoom-out\" effect). This limitation is due to the lower resolution of motion representation at inference time.  The figure also highlights the model's difficulty with complex physical interactions between objects, illustrating that the model struggles to accurately represent realistic interactions, such as a soccer player kicking a ball.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.02492/x9.png", "caption": "Figure 8: Qualitative motivation. We noise input videos to different timesteps (20,60,8020608020,60,8020 , 60 , 80) and continue the generation. By step 60606060, the video\u2019s coarse motion and structure are mostly determined.", "description": "This figure shows the results of an experiment designed to understand how different timesteps in the video generation process affect the final output.  Three sets of videos were noised at timesteps 20, 60, and 80, then the generation process was continued. The figure shows the appearance and motion of the video at these different timesteps.  The results indicate that by timestep 60, the coarse motion and overall structure of the generated video have largely been determined, while finer details continue to be refined in subsequent steps.", "section": "3. Motivation"}]