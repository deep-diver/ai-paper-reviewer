{"importance": "This work demonstrates that **scaling laws are also applicable to formal mathematical reasoning**, offering insights and a practical approach for enhancing ATP models. This suggests potential for solving highly complex math problems and may be useful to various researchers to follow through.", "summary": "Leanabelle-Prover: Scaling Formal Reasoning via Posttraining", "takeaways": ["Posttraining scaling can significantly improve formal mathematical reasoning in ATP models.", "Integrating cognitive behaviors via synthetic data enhances self-reflection capabilities in models.", "The Leanabelle-Prover achieves state-of-the-art performance on the MiniF2F-test."], "tldr": "Recent advances in LLMs for automated theorem proving (ATP) show promise using Lean 4 codes. To catch up with recent posttraining scaling breakthroughs, this paper investigates ATP posttraining, aligning it with advancements in natural language reasoning models. The researchers continually trained ATP models using a hybrid dataset combining statement-proof pairs with data mimicking human-like reasoning. Reinforcement learning was incorporated using feedback from the Lean 4 compiler.\n\nThrough continual training and reinforcement learning, the researchers enhanced formal provers like DeepSeek-Prover and Goedel-Prover, achieving state-of-the-art performance in whole-proof generation. For example, Leanabelle-Prover attained a 59.8% pass rate on MiniF2F. The framework surpasses DeepSeek-Prover-v1.5-RL by 6.6% and Goedel-Prover-SFT by 2.2%. Additionally, the team is releasing data and training details to help follow through.", "affiliation": "Kuaishou Technology", "categories": {"main_category": "Machine Learning", "sub_category": "Reinforcement Learning"}, "podcast_path": "2504.06122/podcast.wav"}