[{"figure_path": "https://arxiv.org/html/2504.06122/x1.png", "caption": "Figure 1: \nBenchmark performance on MiniF2F-test\u00a0(Zheng et\u00a0al., 2021). Our method boosts both the two baseline models after employing RL training. Goedel-Prover-RL is our implementation. Our framework surpasses DeepSeek-Prover-v1.5-RL\u00a0and Goedel-Prover-SFT\u00a06.6% and 2.2%, respectively.", "description": "The bar chart displays the performance comparison of different methods on the MiniF2F-test benchmark.  The x-axis represents the different methods: DeepSeek-Prover-v1.5 (using Supervised Fine-Tuning, SFT), DeepSeek-Prover-v1.5 (with Reinforcement Learning, RL), Goedel-Prover (SFT), and the proposed method (Leanabell-Prover using RL). The y-axis represents the passing rate (%). The chart shows that the Leanabell-Prover method, using RL, significantly outperforms the baseline models (DeepSeek-Prover-v1.5-RL and Goedel-Prover-SFT), achieving a 6.6% and 2.2% higher passing rate, respectively.", "section": "3. Model Training"}, {"figure_path": "https://arxiv.org/html/2504.06122/x2.png", "caption": "Figure 2: Distributions of math domains in various Lean 4 dataset. Lean Workbook, Goedel-Prover, STP Lean and NuminaMath are training set. MiniF2F and ProofNet are test set.", "description": "Figure 2 is a bar chart that visualizes the distribution of mathematical domains across different Lean 4 datasets.  The x-axis represents various mathematical domains, such as calculus, linear algebra, logic, number theory, etc. The y-axis shows the ratio or proportion of each domain within a specific dataset. The figure compares the distribution of mathematical domains in four training datasets (Lean Workbook, Goedel-Prover, STP Lean, and NuminaMath) and two test datasets (MiniF2F and ProofNet). This allows for a comparison of the training data's representativeness compared to the test datasets, giving insight into potential biases or imbalances in the training data.  The chart provides valuable insights into whether the training data adequately covers the range of mathematical concepts present in the test sets, which is crucial for model generalizability.", "section": "3. Model Training"}, {"figure_path": "https://arxiv.org/html/2504.06122/x3.png", "caption": "Figure 3: Exploration ability: pass@16 measures how well base models explore.", "description": "Figure 3 illustrates the exploration capabilities of the base models (DeepSeek-Prover-v1.5-SFT and Goedel-Prover-SFT) before reinforcement learning.  The metric 'pass@16' represents the rate at which the model produces valid proofs (with at least 16 steps being successfully verified), while varying the sampling temperature. A higher pass@16 rate at lower temperatures indicates that the model is less prone to exploring unexpected or less optimal tactics during proof generation; conversely, a high pass@16 rate at higher temperatures suggests a greater capacity for exploration and discovery of alternative proof paths. This figure is crucial for evaluating the readiness of the base models for reinforcement learning, as it highlights their balance between exploration and exploitation during proof generation.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2504.06122/x8.png", "caption": "Figure 4: Left: Reward curve during training Leanabell-Prover-Prover-DS-RL. Right: Reward curve during training Leanabell-Prover-Prover-GD-RL.", "description": "This figure displays two reward curves, one for Leanabell-Prover-DS-RL and the other for Leanabell-Prover-GD-RL, illustrating the change in reward over the course of their respective reinforcement learning (RL) training processes.  The x-axis represents the training step, while the y-axis represents the reward received by the model. The curves visualize the learning progress of each model, indicating how effectively they learned to generate valid proofs during training.  The differences between the curves could reflect the effect of varying initial model parameters or training data.", "section": "3.3 Reinforcement Learning"}]