[{"figure_path": "https://arxiv.org/html/2504.05594/extracted/6339207/sec/figs/intro/balance5.png", "caption": "Figure 1: \nIllustration of balancing fidelity and editability.\nWe demonstrate examples of over-, balanced, and under-editing across six types of edits:\n(a) color change, (b) texture modification (c) object replacement (d) background editing, (e) global style transfer, and (f) human face attribute editing.\nOver-editing occurs when excessive changes distort the original image, while under-editing results in changes too subtle to meet the text prompt\u2019s requirements.\nIn contrast, our UnifyEdit balances fidelity and editability within a unified framework, ensuring edits align with the text prompt while preserving the essential integrity.", "description": "Figure 1 illustrates the balance between fidelity (preserving the original image) and editability (making the desired changes) in text-based image editing.  It showcases examples of over-editing (excessive changes), balanced editing (a good balance between fidelity and editability), and under-editing (insufficient changes). Six different edit types are demonstrated: (a) color change, (b) texture modification, (c) object replacement, (d) background editing, (e) global style transfer, and (f) human face attribute editing. The figure highlights how the proposed method, UnifyEdit, successfully achieves a balanced outcome, aligning edits with the text prompt while maintaining the original image's structural integrity.", "section": "1 INTRODUCTION"}, {"figure_path": "https://arxiv.org/html/2504.05594/x1.png", "caption": "Figure 2: \nUnifyEdit vs.\u00a0dual-branch editing paradigm.\n(a) The typical dual-branch editing paradigm consists of source and target branches, using attention injection to maintain fidelity while relying on the text prompt to achieve editability.\n(b) In contrast, our method explicitly models the fidelity and editability using two attention-based constraints and performs latent optimization within a unified framework, facilitating an adaptive balance across various editing types.", "description": "Figure 2 illustrates the difference between traditional dual-branch image editing methods and the proposed UnifyEdit approach.  Panel (a) shows a typical dual-branch method. It uses a source branch to reconstruct the original image and a target branch to generate the edited image. Fidelity (preserving the original image) relies on attention injection from the source branch, while editability (making the desired changes) depends on the inherent text alignment capabilities of the pre-trained model in the target branch.  However, this approach lacks a unified mechanism to balance fidelity and editability. Panel (b) presents the UnifyEdit method. Instead of separate branches and attention injections, UnifyEdit performs latent optimization within a unified framework.  It uses two attention-based constraints \u2013 one for self-attention preservation (fidelity) and one for cross-attention alignment (editability) \u2013 to achieve a balanced integration of both aspects.  An adaptive time-step scheduler dynamically adjusts the influence of these constraints, ensuring an optimal balance across various editing tasks.", "section": "Dual-Branch Tuning-Free Image Editing"}, {"figure_path": "https://arxiv.org/html/2504.05594/x2.png", "caption": "Figure 3: \nExperiments with self-attention and cross-attention.\n(a) Compared to SA injection, the SA constraint offers greater flexibility in editing.\n(b) When the CA map accurately focuses on the target region with a strong response, the resulting edits align effectively with the text prompt. However, attention leakage or low attention values can lead to misalignment or ineffective editing outcomes.", "description": "Figure 3 demonstrates the effects of self-attention (SA) and cross-attention (CA) constraints on image editing.  Part (a) shows that using an SA constraint, rather than directly injecting SA features, provides more flexibility in editing. Part (b) illustrates how edits are more effective when the cross-attention map strongly highlights the relevant region in the image, aligning with the text prompt.  Conversely, insufficient CA activation or attention leakage leads to poor results where edits are misaligned with the desired change.", "section": "4 Rethinking Self- and Cross-Attention for TIE"}, {"figure_path": "https://arxiv.org/html/2504.05594/x3.png", "caption": "Figure 4: Illustration of UnifyEdit.\nUnifyEdit is applied to the diffusion latent feature zt\u2217superscriptsubscript\ud835\udc67\ud835\udc61\u2217z_{t}^{\\ast}italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT in the target branch, involving two key steps: 1) calculating \u2112SAPsubscript\u2112SAP\\mathcal{L}_{\\rm{SAP}}caligraphic_L start_POSTSUBSCRIPT roman_SAP end_POSTSUBSCRIPT and \u2112CAAsubscript\u2112CAA\\mathcal{L}_{\\rm{CAA}}caligraphic_L start_POSTSUBSCRIPT roman_CAA end_POSTSUBSCRIPT for fidelity and editability, and 2) applying an adaptive time-step scheduler for latent optimization.", "description": "UnifyEdit, a tuning-free image editing method, is illustrated.  It operates on the target branch of a dual-branch diffusion model by optimizing the latent representation (z*t). This optimization is guided by two key components: 1) The calculation of two loss functions: \u2112SAP (Self-Attention Preservation Constraint) which ensures fidelity by measuring the difference in self-attention maps between source and target branches, and \u2112CAA (Cross-Attention Alignment Constraint) which enhances editability by promoting alignment between cross-attention maps and target text tokens.  2) An adaptive time-step scheduler dynamically adjusts the weighting of these loss functions across different denoising stages, achieving a balance between fidelity and editability. The figure visually depicts the flow of information and the interaction between these components within the UnifyEdit framework.", "section": "4 UNFIY-Edit via LATENT OPTIMIZATION"}, {"figure_path": "https://arxiv.org/html/2504.05594/x4.png", "caption": "Figure 5: Editing and visualization results of different gradients.\n(a) Using Eq.\u00a0(12) alone results in a significantly stronger influence of \u2112CAAsubscript\u2112CAA\\mathcal{L}_{\\rm{CAA}}caligraphic_L start_POSTSUBSCRIPT roman_CAA end_POSTSUBSCRIPT, disabling \u2112SAPsubscript\u2112SAP\\mathcal{L}_{\\rm{SAP}}caligraphic_L start_POSTSUBSCRIPT roman_SAP end_POSTSUBSCRIPT and causing an unbalanced guidance on ztsubscript\ud835\udc67\ud835\udc61z_{t}italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT.\n(b) Although calculating their norms as in Eq.\u00a0(13) brings the magnitudes of the constraints closer, the irregular dynamics lead to either under-editing or over-editing failures.\n(c) In contrast, applying the adaptive time-step scheduler in Eq.\u00a0(14) shapes the gradient trends in Eq.\u00a0(15) such that \u2207zt\u2217\u2112SAPsubscript\u2207superscriptsubscript\ud835\udc67\ud835\udc61subscript\u2112SAP\\nabla_{z_{t}^{*}}\\mathcal{L}_{\\rm{SAP}}\u2207 start_POSTSUBSCRIPT italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT roman_SAP end_POSTSUBSCRIPT starts small and gradually increases, whereas \u2207zt\u2217\u2112CAAsubscript\u2207superscriptsubscript\ud835\udc67\ud835\udc61subscript\u2112CAA\\nabla_{z_{t}^{*}}\\mathcal{L}_{\\rm{CAA}}\u2207 start_POSTSUBSCRIPT italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT roman_CAA end_POSTSUBSCRIPT exhibits the opposite trend, facilitating fidelity-editability balance.", "description": "Figure 5 presents a comparison of three different gradient calculation methods used in the UnifyEdit model for image editing.  Method (a) uses a simple weighted sum of the gradients (Equation 12), resulting in \u2112CAA dominating and causing unbalanced editing. Method (b) normalizes the gradients (Equation 13) which improves balance but still results in inconsistent editing outcomes. Method (c), using the adaptive time-step scheduler (Equations 14 and 15), dynamically adjusts the influence of each gradient. This results in a smooth, balanced gradient, improving fidelity and editability of the edits. The visualizations show how the gradients change over time steps in each method and the corresponding results. ", "section": "4 UNFIY-Edit via LATENT OPTIMIZATION"}, {"figure_path": "https://arxiv.org/html/2504.05594/x5.png", "caption": "Figure 6: \nQualitative comparisons across various editing types.\nWe use white dashed outlines to highlight the target object in foreground editing.\nOur proposed method achieves a superior balance compared to other baseline methods, demonstrating enhanced editing effects while more effectively maintaining structural consistency.", "description": "This figure displays a qualitative comparison of different image editing methods across various editing tasks (color change, texture modification, object replacement, background editing, global style transfer, and human face attribute editing).  Each row shows the same source image edited using different methods. White dashed outlines highlight the target area of modification in foreground edits. The results demonstrate that the proposed method (Ours) achieves a better balance between fidelity (preserving the original image structure) and editability (accurately reflecting the desired changes) than other baselines.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2504.05594/x6.png", "caption": "Figure 7: \nQuantitative comparisons of baselines and our UnifyEdit across various editing types.\nWe quantify editability and fidelity using CLIP sore (righter is better) and DINO similarity distance (lower is better), respectively.\nBalancing the aspects requires a high CLIP score and relatively low DINO similarity.\nTherefore, points closer to the pink region of the background represent better performance, while those closer to the blue region indicate poorer performance.", "description": "Figure 7 presents a quantitative comparison of different image editing methods, including the proposed UnifyEdit method, across various editing tasks.  The comparison is visualized using a 3D scatter plot where the x-axis represents the CLIP score (a measure of editability, higher is better), the y-axis represents the DINO similarity distance (a measure of fidelity, lower is better), and each point corresponds to a specific method and editing task.  The color of the point varies from blue (poor performance) to pink (excellent performance), reflecting the balance between editability and fidelity. Points closer to the pink region in the background indicate a better balance between high editability and high fidelity, while points closer to the blue region indicate a poorer balance (e.g., low fidelity and/or low editability).", "section": "5 EXPERIMENTS"}, {"figure_path": "https://arxiv.org/html/2504.05594/extracted/6339207/sec/figs/exp/user_study4.png", "caption": "Figure 8: \nAverage human preferences across various editing types.\nThe values indicate the proportion of users who preferred our proposed method over comparative approaches.", "description": "This figure presents the results of a user study comparing the proposed UnifyEdit method against other state-of-the-art text-based image editing methods.  The user study involved participants evaluating the results of different methods on a range of image editing tasks.  The bar chart shows the percentage of participants who preferred UnifyEdit over each of the comparison methods for each editing task. Higher percentages indicate stronger preference for UnifyEdit.", "section": "5 Experiments"}, {"figure_path": "https://arxiv.org/html/2504.05594/x7.png", "caption": "Figure 9: \nQualitative results of ablation study on attention-based constraints.\nWhite dashed outlines are used to highlight the target object in foreground editing.\nCombining both terms is crucial for achieving a good balance between fidelity and editability.", "description": "This ablation study investigates the effects of using self-attention preservation and cross-attention alignment constraints individually and together in a text-based image editing model. The results show that while each constraint improves either fidelity or editability, respectively, only combining both achieves a good balance between them. The images showcase examples of foreground editing where the target region is highlighted with white dashed outlines.", "section": "5.5 Ablation Study on Attention-Based Constraints"}, {"figure_path": "https://arxiv.org/html/2504.05594/x8.png", "caption": "Figure 10: \nQualitative results of ablation study on different gradients.\nThe target object is accentuated with white dashed outlines in the foreground editing.\n\ud835\udca2n\u2062a\u2062i\u2062v\u2062esubscript\ud835\udca2\ud835\udc5b\ud835\udc4e\ud835\udc56\ud835\udc63\ud835\udc52\\mathcal{G}_{naive}caligraphic_G start_POSTSUBSCRIPT italic_n italic_a italic_i italic_v italic_e end_POSTSUBSCRIPT\nin\u00a0Eq.\u00a0(12) can lead to over-editing and, in some cases, image collapse.\nWhile \ud835\udca2n\u2062o\u2062r\u2062msubscript\ud835\udca2\ud835\udc5b\ud835\udc5c\ud835\udc5f\ud835\udc5a\\mathcal{G}_{norm}caligraphic_G start_POSTSUBSCRIPT italic_n italic_o italic_r italic_m end_POSTSUBSCRIPT in Eq.\u00a0(13) mitigates these issues, it still encounters both under-editing and over-editing failures.\nIn contrast, our method, which employs \ud835\udca2b\u2062l\u2062csubscript\ud835\udca2\ud835\udc4f\ud835\udc59\ud835\udc50\\mathcal{G}_{blc}caligraphic_G start_POSTSUBSCRIPT italic_b italic_l italic_c end_POSTSUBSCRIPT in\nEq.\u00a0(15), successfully achieves a balanced result.", "description": "This ablation study compares three different gradient calculation methods used in the UnifyEdit image editing model:  the naive gradient (Gnaive), the normalized gradient (Gnorm), and the balanced gradient (Gblc). The figure shows example edits of images using each gradient method. The naive gradient often leads to excessive changes or complete image failure, while the normalized gradient, though improved, still exhibits inconsistencies in over- or under-editing. The balanced gradient approach employed in the UnifyEdit model demonstrably delivers more balanced and consistent results in image editing.", "section": "4 UNFIY-Edit via LATENT OPTIMIZATION"}, {"figure_path": "https://arxiv.org/html/2504.05594/x9.png", "caption": "Figure 11: \nAblation study on hyper-parameters in adaptive time-step scheduler.\nThe scaling factors \u03b21subscript\ud835\udefd1\\beta_{1}italic_\u03b2 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and \u03b22subscript\ud835\udefd2\\beta_{2}italic_\u03b2 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, along with the rate factors k1subscript\ud835\udc581k_{1}italic_k start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and k2subscript\ud835\udc582k_{2}italic_k start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, regulate the magnitude and changing rate, influencing the editing outcomes.", "description": "This figure presents an ablation study analyzing the impact of hyperparameters within the adaptive time-step scheduler on the editing outcomes of the UnifyEdit model.  The scheduler dynamically balances fidelity and editability constraints using scaling factors (\u03b21 and \u03b22) and rate factors (k1 and k2). The study systematically varies each hyperparameter individually, observing its effects on the gradients of the constraints (GSAP and GCAA) throughout the denoising process.  Visualizations show how altering these hyperparameters affects the balance between fidelity and editability, demonstrating their impact on the final editing results.", "section": "4 UNFIY-Edit via LATENT OPTIMIZATION"}, {"figure_path": "https://arxiv.org/html/2504.05594/x10.png", "caption": "Figure 12: \nEditing results using DDIM inversion.\nThe proposed method maintains effectiveness by employing SA constraints derived from the SA maps generated during the DDIM inversion.", "description": "This figure demonstrates the results of image editing using DDIM (Denoising Diffusion Implicit Models) inversion.  The key takeaway is that the proposed UnifyEdit method remains effective even when using SA (self-attention) constraints derived directly from the SA maps generated during the DDIM inversion process.  This highlights the robustness and adaptability of the UnifyEdit approach.", "section": "5.7 Discussions"}, {"figure_path": "https://arxiv.org/html/2504.05594/x11.png", "caption": "Figure 13: \nDiffusion latent optimization vs.\u00a0noise guidance.\nLatent optimization outperforms noise guidance in balancing fidelity and editability.", "description": "This figure compares the results of image editing using two different optimization techniques: diffusion latent optimization and noise guidance.  It shows that latent optimization leads to better results in balancing fidelity (preserving the original image's content) and editability (achieving the desired changes).  Specific examples are shown to illustrate how latent optimization maintains structural integrity and aligns edits with the text prompt better than noise guidance, which may result in either over-editing or under-editing.", "section": "5.7 Discussions"}, {"figure_path": "https://arxiv.org/html/2504.05594/x12.png", "caption": "Figure 14: \nMore editing results of UnifyEdit.\nWe highlight the target object with white dashed outlines in foreground editing.\nUnifyEdit can achieve balance across various editing types and can be applied to multiple target editing tokens.", "description": "Figure 14 presents supplementary results demonstrating UnifyEdit's effectiveness across diverse editing tasks.  The images showcase various editing types (color change, texture modification, object replacement, background editing, global style transfer, and human face attribute editing).  White dashed lines highlight the target object in foreground editing scenarios to emphasize the precise and accurate edits achieved by the model. The figure also demonstrates UnifyEdit's capability to handle multiple simultaneous edits (multiple target editing tokens), further highlighting its flexibility and robustness.", "section": "Experiments"}]