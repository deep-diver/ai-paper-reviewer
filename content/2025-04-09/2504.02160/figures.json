[{"figure_path": "https://arxiv.org/html/2504.02160/extracted/6331841/figs/intro_motivation.jpg", "caption": "Figure 1: Our UNO evolves as an universal customization from single to multi-subject.", "description": "This figure showcases the UNO model's ability to perform universal customization, progressing from single-subject to multi-subject image generation.  Each row demonstrates a different type of generation task: One2One (single image to single image), Two2One (two images to single image), Many2One (multiple images to single image), stylized generation, virtual try-on, product design, and identity preservation. The figure visually represents the model's capacity to incorporate multiple image inputs and textual descriptions, resulting in highly customized image outputs that preserve identity across subject variations.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2504.02160/x2.png", "caption": "Figure 2: The illustration of our motivation. We propose a novel model-data co-evolution paradigm, where less-controllable preceding models systematically synthesize better customization data for successive more-controllable variants, enabling persistent co-evolution between enhanced model and enriched data.", "description": "This figure illustrates the core concept of the paper: model-data co-evolution.  It starts with a less controllable model (e.g., a simple text-to-image model) that generates data. This data, though not perfect, is then used to train a more controllable model. This iterative process, where the output of one model improves the input for the next, leads to increasingly sophisticated and better-performing models. The cycle continues, with each iteration improving both the model and the quality of the data it generates. This approach addresses the challenges of data scarcity and subject limitations in customized image generation, enabling better controllability with improved data quality. The figure visually represents this iterative refinement, showing how less controllable models generate data which then informs the creation of more advanced and controllable models.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2504.02160/x3.png", "caption": "Figure 3: Illustration of our proposed synthetic data curation framework based on in-context data generation.", "description": "This figure illustrates the two-stage pipeline used for synthetic data generation. Stage 1 focuses on single-subject in-context generation, where a text prompt is used to generate a pair of images featuring the same subject in different scenes. This stage employs a Vision-Language Model (VLM) for filtering out low-quality image pairs. Stage 2 involves multi-subject in-context generation, building on the single-subject data from Stage 1.  This stage includes cropping subjects to isolate them and reusing them as multi-subject prompts. The resulting image pairs are again filtered using the VLM.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2504.02160/x4.png", "caption": "Figure 4: Illustration of the training framework of UNO. It introduces two pivotal enhancements to the model: progressive cross-modal alignment and universal rotary position embedding(UnoPE). The progressive cross-modal alignment is divided into two stages. In the Stage II\\mathrm{I}roman_I, we use single-subject in-context generated data to finetune the pretrained T2I model into an S2I model. In the Stage IIII\\mathrm{II}roman_II, we continue training on generated multiple-subject data pairs. The UnoPE can effectively equip UNO with the capability of mitigating the attribute confusion issue when scaling visual subject controls.", "description": "UNO's training framework is illustrated, highlighting two key improvements: progressive cross-modal alignment (a two-stage process starting with single-subject data and progressing to multiple-subject data) and universal rotary position embedding (UnoPE), which addresses attribute confusion when handling multiple visual subjects.", "section": "3.3 Customization Model Framework (UNO)"}, {"figure_path": "https://arxiv.org/html/2504.02160/x5.png", "caption": "Figure 5: Model performance on Dreambench\u00a0[33]. We conduct experiments under different quality score levels.", "description": "This figure shows the performance of the UNO model on the DreamBench benchmark, specifically focusing on how different levels of data quality impact the model's performance.  The graphs display the DINO and CLIP-I scores across multiple training steps, with each line representing a different quality score range of the generated data. This visualization demonstrates the correlation between data quality and model performance, highlighting the importance of high-quality data in achieving better results.  The x-axis represents training steps, and the y-axis represents the DINO and CLIP-I scores.", "section": "3.2 Synthetic Data Curation Framework"}, {"figure_path": "https://arxiv.org/html/2504.02160/x6.png", "caption": "Figure 6: Qualitative comparison with different methods on single-subject driven generation.", "description": "Figure 6 presents a qualitative comparison of different image generation methods in scenarios involving single-subject image generation.  Several methods are compared, showing their ability to generate images based on a text prompt and a single reference image. This comparison highlights the relative strengths and weaknesses of each model in terms of subject fidelity, image quality, and adherence to the given text prompts.  The visual comparison helps to assess how well each approach handles single-subject image customization.", "section": "4.2. Qualitative Analyses"}, {"figure_path": "https://arxiv.org/html/2504.02160/x7.png", "caption": "Figure 7: Qualitative comparison with different methods on multi-subject driven generation.", "description": "Figure 7 displays a qualitative comparison of various image generation methods on multi-subject scenarios.  Each row presents a prompt with two reference images specifying multiple subjects, and the resulting images generated by different models, including UNO (the authors' model), OmniGen, MS-Diffusion, MIP-Adapter, and SSR-Encoder.  This allows for a visual assessment of each model's ability to successfully incorporate multiple subjects specified in the input prompt, highlighting differences in subject fidelity, controllability, and overall image quality.", "section": "4.2 Qualitative Analyses"}, {"figure_path": "https://arxiv.org/html/2504.02160/x8.png", "caption": "Figure 8: Radar charts of user evaluation of methods for single-subject driven and multi-subject driven generation on different dimensions", "description": "Figure 8 presents a comparative analysis of user evaluations for various image generation methods. It uses radar charts to visually represent the performance across several key dimensions, including 'text fidelity' (for both subject and background), 'visual appeal', 'composition', and 'subject similarity'.  The charts separately evaluate the models' performance on single-subject and multi-subject driven image generation tasks, enabling a direct comparison of their strengths and weaknesses in handling different complexities of image synthesis.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2504.02160/x9.png", "caption": "Figure 9: Ablation study of UNO. Zoom in for details.", "description": "Figure 9 shows the results of an ablation study conducted on the UNO model.  Different components of the UNO model were removed or altered to evaluate their individual contributions to the model's performance. Specifically, the figure likely displays the impact of removing or modifying the following: the in-context data generation process, the cross-modal alignment process, and the UNO's Universal Rotary Position Embedding (UnoPE). For each ablation, the results (likely quantitative metrics like DINO and CLIP scores) are presented, demonstrating how each component affects the overall performance of the UNO model in subject similarity and text controllability. Zooming in on the image will provide more detailed information about the specific quantitative values shown for each ablation experiment.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2504.02160/x10.png", "caption": "Figure 10: Illustration of the taxonomy tree.", "description": "This figure illustrates the taxonomy tree used for generating diverse subjects and scenes in the paper.  The tree begins with broad categories (like Person, Object), which then branch out into more specific subclasses, such as different types of people (e.g., scientist, doctor), and types of objects (e.g., hat, umbrella).  For each subclass, example instances are provided to show the variety of subjects, and descriptive prompts are generated for their corresponding scene contexts.", "section": "F. In-Context Data Generation Pipeline"}, {"figure_path": "https://arxiv.org/html/2504.02160/x11.png", "caption": "Figure 11: Diptych text template for generating subject-consistent image-pair with FLUX.1[17].", "description": "This figure shows two text templates used to generate paired images of a subject. The first template generates two images of the same subject in different scenes. The second template generates a top and bottom image of the same subject, with the bottom image including another subject.", "section": "3.2 Synthetic Data Curation Framework"}, {"figure_path": "https://arxiv.org/html/2504.02160/x12.png", "caption": "(a) System prompt of LLM used to generate subject instances in creative type.", "description": "This figure shows the system prompt used to instruct a Large Language Model (LLM) to generate creative subject instances for image generation.  The prompt guides the LLM to produce a diverse set of brief subject descriptions, each limited to 12 words or less, following specific rules. These rules ensure that each generated prompt is unique, utilizes realistic descriptions or brand references, and avoids repetition. The goal is to create varied and imaginative training data for a subject-to-image model, providing a range of descriptive inputs for generating diverse and interesting images.", "section": "F. In-Context Data Generation Pipeline"}, {"figure_path": "https://arxiv.org/html/2504.02160/x13.png", "caption": "(b) System prompt of LLM used to generate subject instances in realistic type.", "description": "This figure shows the system prompt used to instruct a Large Language Model (LLM) to generate realistic subject instance descriptions for image generation.  The prompt provides instructions and examples to guide the LLM in creating descriptions that are grounded in reality, avoiding fantastical or impossible scenarios.  The goal is to create training data for a subject-to-image model that focuses on realistic depictions.", "section": "3.2 Synthetic Data Curation Framework"}, {"figure_path": "https://arxiv.org/html/2504.02160/x14.png", "caption": "(c) System prompt of LLM used to generate subject instances in text-decorated type.", "description": "This figure shows the system prompt used to instruct a large language model (LLM) to generate creative text-decorated subject instances.  The prompt provides guidelines, examples, and constraints to ensure the generated instances are suitable for use in generating customized images. Specifically, it instructs the LLM to create subject prompts for image generation, emphasizing that the descriptions should incorporate textual elements, be realistic and creative, not exceed 12 words, and avoid repetition. The asset category is provided as input to guide the LLM's generation. This process helps create varied and high-quality data for training image generation models.", "section": "3.2 Synthetic Data Curation Framework"}, {"figure_path": "https://arxiv.org/html/2504.02160/x15.png", "caption": "Figure 12: System prompt of LLM used to generate subject instances.", "description": "This figure shows the system prompts used to instruct the large language model (LLM) to generate diverse subject instances for image generation.  It showcases three distinct prompt styles: creative, realistic, and text-decorated, each designed to elicit different types of subject descriptions from the LLM. The prompts provide clear instructions regarding the desired format, length, and style of the generated descriptions.", "section": "3.2 Synthetic Data Curation Framework"}, {"figure_path": "https://arxiv.org/html/2504.02160/x16.png", "caption": "Figure 13: System prompt of LLM used to generate scene descriptions.", "description": "This figure shows the prompt used to instruct a large language model (LLM) to generate creative scene descriptions for given subject prompts.  The prompt instructs the LLM to generate eight detailed scene descriptions for each subject, varying the background, environment, camera view, and lighting to showcase creativity and diverse contexts. The LLM should maintain brevity (under 77 tokens per description).", "section": "F. In-Context Data Generation Pipeline"}, {"figure_path": "https://arxiv.org/html/2504.02160/x17.png", "caption": "(a) System prompt of the filter VLM.", "description": "The figure shows the system prompt used for the Vision-Language Model (VLM) which is part of a multi-stage filtering process for synthetic data curation. This prompt instructs the VLM to perform a detailed comparison of the subject in two images, breaking down the subject into its constituent parts and assigning a score (0-4) to each part based on the degree of similarity between the two images.  The prompt emphasizes objective evaluation, ignoring factors like background, position, and actions.", "section": "F.2.2 Subject-Consistent Image-Pair Filter"}, {"figure_path": "https://arxiv.org/html/2504.02160/x18.png", "caption": "(b) Prompt for the first round CoT of the filter VLM.", "description": "This figure shows the prompt used in the first round of the Chain-of-Thought (CoT) filtering process for evaluating the consistency of subjects in image pairs. The prompt instructs the Vision-Language Model (VLM) to analyze the differences between corresponding subject parts in two images. It directs the model to provide a quantitative score (0-4) for each part, considering the degree of resemblance. The prompt emphasizes the importance of focusing on the subject itself, ignoring background elements and variations in actions, poses, etc.  Each part's score reflects the level of similarity, with 0 indicating no resemblance and 4 representing near-identical parts. The prompt also guides the model on how to handle perspective changes and additional accessories, which may not appear in all images, and should be ignored when assessing similarity.", "section": "3.2 Synthetic Data Curation Framework"}, {"figure_path": "https://arxiv.org/html/2504.02160/x19.png", "caption": "(c) Prompt for the second round CoT of the filter VLM.", "description": "This figure shows the prompt for the second round of the Chain-of-Thought (CoT) reasoning process used to filter the synthesized image pairs.  The goal is to assign a numerical score (0-4) to each identified part of the subject in the image pairs, indicating the degree of consistency between the subject in the reference and target images. A higher score means greater similarity.  The prompt guides the evaluator to focus on specific details of the subject, ignoring background and contextual differences.  Scores are assigned based on detailed comparison of the subject's visual aspects, aiming for a more objective and thorough assessment of subject consistency.", "section": "F.2.2 Subject-Consistent Image-Pair Filter"}, {"figure_path": "https://arxiv.org/html/2504.02160/x20.png", "caption": "(d) Prompt for the third round CoT of the filter VLM.", "description": "This figure shows the prompt used in the third round of the Chain-of-Thought (CoT) filtering process for synthetic image data.  The CoT process aims to improve the quality of the generated image pairs by iteratively refining the evaluation criteria.  This specific prompt instructs the Vision-Language Model (VLM) to assign a numerical score (0-4) to different aspects of the subject in the images, based on their degree of similarity or difference. The higher the score, the more similar the subject is across the images.", "section": "F.2.2 Subject-Consistent Image-Pair Filter"}, {"figure_path": "https://arxiv.org/html/2504.02160/x21.png", "caption": "Figure 14: CoT prompt of the filter VLM.", "description": "This figure shows the three-step Chain-of-Thought (CoT) prompt engineering used to filter the image pairs generated by the in-context learning method.  Step 1 instructs the VLM to describe the two images and identify the subject. Step 2 directs the VLM to compare specific aspects of the subject across both images, noting differences in detail. Step 3 instructs the VLM to assign a score (0-4) to each aspect based on the similarity between the two images. The goal is to objectively assess the consistency of the subject across the images, filtering out those with low consistency scores.", "section": "F.2.2 Subject-Consistent Image-Pair Filter"}, {"figure_path": "https://arxiv.org/html/2504.02160/x22.png", "caption": "Figure 15: Sampled data from different VLM score intervals.", "description": "This figure displays samples of synthetic data generated using a Vision-Language Model (VLM) filter, categorized by VLM score intervals.  The purpose is to demonstrate the effect of the VLM filter on data quality and consistency. Higher VLM scores (e.g., 4) indicate better quality and more consistent image pairs, while lower scores (e.g., 0-1) indicate lower quality and less consistent image pairs. The images show that higher scores correspond to generated images with higher quality and better subject consistency, which proves the effectiveness of the data filtering method.", "section": "3.2 Synthetic Data Curation Framework"}, {"figure_path": "https://arxiv.org/html/2504.02160/x23.png", "caption": "Figure 16: Sampled data from our final multi-subject in-context data.", "description": "This figure displays examples from the final multi-subject dataset generated using the in-context learning method.  It showcases the diversity and quality of the synthetic images produced, demonstrating the model's ability to generate images with multiple, consistent subjects in various settings.", "section": "3.2 Synthetic Data Curation Framework"}, {"figure_path": "https://arxiv.org/html/2504.02160/x24.png", "caption": "Figure 17: Amount of data in each VLM score interval.", "description": "This figure is a pie chart showing the distribution of the synthetic dataset across different Visual Language Model (VLM) score intervals.  The VLM score is a measure of the quality and consistency of the generated image pairs, with higher scores indicating better quality and higher consistency.  The chart visualizes the percentage of generated image pairs that fall into each score range, highlighting the proportion of high-quality, consistent data versus lower-quality data after filtering.", "section": "F.2.2 Subject-Consistent Image-Pair Filter"}, {"figure_path": "https://arxiv.org/html/2504.02160/x25.png", "caption": "Figure 18: Analysis of model performance under different LoRA ranks.", "description": "This figure shows the impact of varying the LoRA (Low-Rank Adaptation) rank on the performance of the UNO model.  The LoRA rank controls the number of parameters updated during fine-tuning, affecting the model's ability to adapt to new data. The graph likely presents metrics such as FID (Fr\u00e9chet Inception Distance) or other relevant image quality scores across different LoRA ranks, showing how increasing the rank improves performance up to a certain point, beyond which improvements may diminish or even plateau due to overfitting or computational constraints.", "section": "G. Analysis on LoRA Rank"}, {"figure_path": "https://arxiv.org/html/2504.02160/x26.png", "caption": "Figure 19: More comparison with different methods on multi-subject driven generation. We italicize the subject-related editing part of the prompts.", "description": "Figure 19 presents a qualitative comparison of various image generation models' performance on multi-subject scenarios.  Several methods are tested, including the authors' UNO model, against others like OmniGen, MS-Diffusion, MIP-Adapter, and SSR-Encoder. The figure shows how each model responds to prompts involving multiple subjects and subject-specific edits. The prompts' subject-related parts are italicized to highlight what modifications are being requested.  The results illustrate how well each model can maintain consistency of the original subjects while incorporating the desired changes and additional subjects.", "section": "4.2 Qualitative Analyses"}, {"figure_path": "https://arxiv.org/html/2504.02160/x27.png", "caption": "Figure 20: More multi-subject generation results from our UNO model.", "description": "This figure showcases the UNO model's ability to generate images with multiple subjects and diverse contextual elements.  Each row presents an input (reference images and a text prompt) and the corresponding output generated by the UNO model. The examples demonstrate the model's capacity to combine multiple subjects seamlessly while adhering to the specified text prompt. For instance, one example shows a black cat wearing a hat riding a yellow duck in a forest scene.", "section": "H.2 Application Scenarios"}]