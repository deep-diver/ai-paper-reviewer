{"importance": "This research pioneers a novel model-data co-evolution, tackling data bottlenecks in customized image generation. **By balancing subject similarity and text controllability**, this work provides a new direction for future research in controllable AI content creation.", "summary": "UNO unlocks highly controllable in-context generation by a novel model-data co-evolution!", "takeaways": ["Introduces a data synthesis pipeline for high-consistency multi-subject paired data.", "Presents UNO, a multi-image conditioned subject-to-image model with progressive cross-modal alignment.", "Achieves state-of-the-art results in subject similarity and text controllability on DreamBench."], "tldr": "Subject-driven image generation is limited by data scalability & subject expandability. Scaling single-subject datasets to multi-subject ones is difficult.  Existing methods struggle w/ multi-subject scenarios. This paper proposes a data synthesis pipeline using diffusion transformers, generating consistent multi-subject paired data.\n\nTo address this, the paper introduces UNO, featuring progressive cross-modal alignment and universal rotary position embedding. Trained iteratively, UNO achieves high consistency while ensuring controllability in both single & multi-subject generation. Code & models are publicly available.", "affiliation": "ByteDance", "categories": {"main_category": "Computer Vision", "sub_category": "Image Generation"}, "podcast_path": "2504.02160/podcast.wav"}