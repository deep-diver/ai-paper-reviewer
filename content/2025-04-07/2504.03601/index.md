---
title: "APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated Agent-Human Interplay"
summary: "APIGen-MT: agentic pipeline for multi-turn data generation via simulated agent-human interplay."
categories: ["AI Generated", "ü§ó Daily Papers"]
tags: ["Natural Language Processing", "Dialogue Systems", "üè¢ Salesforce AI Research",]
showSummary: true
date: 2025-04-04
draft: false
---

<br>

{{< keywordList >}}
{{< keyword icon="fingerprint" >}} 2504.03601 {{< /keyword >}}
{{< keyword icon="writer" >}} Akshara Prabhakar et el. {{< /keyword >}}
 
{{< keyword >}} ü§ó 2025-04-07 {{< /keyword >}}
 
{{< /keywordList >}}

{{< button href="https://arxiv.org/abs/2504.03601" target="_self" >}}
‚Üó arXiv
{{< /button >}}
{{< button href="https://huggingface.co/papers/2504.03601" target="_self" >}}
‚Üó Hugging Face
{{< /button >}}



<audio controls>
    <source src="https://ai-paper-reviewer.com/2504.03601/podcast.wav" type="audio/wav">
    Your browser does not support the audio element.
</audio>


### TL;DR


{{< lead >}}

Generating high-quality, multi-turn agent interaction data is challenging due to its scarcity and cost. Current LLMs struggle with multi-turn interactions, tool usage, and complex function calls, highlighting limitations in capturing real-world agent dynamics. These limitations hinder the development of more reliable and efficient AI agents.



To tackle these issues, **APIGen-MT, a new pipeline, leverages environment execution feedback and a review committee to ensure the high-quality of multi-turn agent data**. It first creates detailed task blueprints with verifiable actions. Then, it transforms these blueprints into realistic agent trajectories via simulated agent-human interaction. The synthetic data and trained XLAM-2-fc-r models are open-sourced to advance AI agent research.

{{< /lead >}}


#### Key Takeaways

{{< alert "star" >}}
{{< typeit speed=10 lifeLike=true >}} Introduces APIGen-MT, an agentic data synthesis pipeline, for generating high-quality multi-turn agent data. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=1000 lifeLike=true >}} Two-phase framework creates detailed task blueprints with verifiable groundtruth actions, transforms these into realistic conversational agent trajectories via simulated human-agent interplay. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=2000 lifeLike=true >}} Trained models outperform frontier models, demonstrating superior performance on agentic benchmarks and potential for developing more reliable, efficient agents. {{< /typeit >}}
{{< /alert >}}

#### Why does it matter?
This study offers a reproducible method for creating high-quality, multi-turn interaction data, facilitating the development of more reliable and capable AI agents. The availability of synthetic datasets and trained models enables further exploration and innovation in agent design, addressing current limitations in agent capabilities.

------
#### Visual Insights



![](https://arxiv.org/html/2504.03601/x3.png)

> üîº This figure compares the performance of different large language models (LLMs) on two benchmark tasks: function calling and agentic reasoning.  The xLAM-2-fc-r models, trained on data generated by the APIGen-MT pipeline, are compared to state-of-the-art baselines (GPT-40 and Claude 3.5).  The comparison shows how well the models perform at executing functions (BFCL v3 benchmark) and engaging in multi-turn, interactive conversations requiring complex reasoning (T-bench benchmark). The results demonstrate the capabilities of the xLAM-2-fc-r models, especially their smaller variants, in executing function calls and handling complex multi-turn interactions.
> <details>
> <summary>read the caption</summary>
> Figure 1: Comparative performance of larger xLAM-2-fc-r¬† models (8B-70B, trained with APIGen-MT¬† data) against state-of-the-art baselines on function-calling (BFCL v3 [43]) and agentic (œÑùúè\tauitalic_œÑ-bench [47]) capabilities.
> </details>





{{< table-caption >}}
<table class="ltx_tabular ltx_centering ltx_align_middle" id="id3.3.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="id2.2.1.1">
<td class="ltx_td ltx_align_right" id="id2.2.1.1.1"><span class="ltx_text" id="id2.2.1.1.1.1" style="position:relative; bottom:-1.5pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="20" id="id2.2.1.1.1.1.g1" src="x1.png" width="22"/></span></td>
<td class="ltx_td ltx_align_left" id="id2.2.1.1.2"><span class="ltx_text ltx_font_bold" id="id2.2.1.1.2.1" style="font-size:90%;">Model</span></td>
<td class="ltx_td ltx_align_left" id="id2.2.1.1.3"><a class="ltx_ref ltx_href" href="https://huggingface.co/collections/Salesforce/xlam-2-67ef5be12949d8dcdae354c4" title="">https://huggingface.co/Salesforce/xLAM-2</a></td>
</tr>
<tr class="ltx_tr" id="id3.3.2.2">
<td class="ltx_td ltx_align_right" id="id3.3.2.2.1"><span class="ltx_text" id="id3.3.2.2.1.1" style="position:relative; bottom:-1.5pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="20" id="id3.3.2.2.1.1.g1" src="x2.png" width="20"/></span></td>
<td class="ltx_td ltx_align_left" id="id3.3.2.2.2"><span class="ltx_text ltx_font_bold" id="id3.3.2.2.2.1" style="font-size:90%;">Website &amp; Dataset</span></td>
<td class="ltx_td ltx_align_left" id="id3.3.2.2.3"><a class="ltx_ref ltx_href" href="https://apigen-mt.github.io/" title="">https://apigen-mt.github.io</a></td>
</tr>
</tbody>
</table>{{< /table-caption >}}

> üîº This table presents a performance comparison of various large language models (LLMs) on the BFCL (Benchmark for Function Calling) leaderboard as of April 3rd, 2025.  The ranking is determined by the overall accuracy score, which is a weighted average across multiple evaluation categories assessing different aspects of function-calling performance. The models are evaluated in two modes: function-calling (FC), where the model directly calls functions; and prompt-based, where a custom prompt is used to extract function calls.  The table includes the overall accuracy, single-turn accuracy, multi-turn accuracy, and hallucination metrics (relevance and irrelevance).  Details on the benchmark are available in reference [43] within the paper.
> <details>
> <summary>read the caption</summary>
> Table 1: Performance comparison of different models on BFCL leaderboard (as of date 04/03/2025). The rank is based on the overall accuracy, which is a weighted average of different evaluation categories. ‚ÄúFC' stands for function-calling mode in contrast to using a customized ‚Äúprompt' to extract the function calls. See the benchmark [43] for details.
> </details>





### In-depth insights


#### Agentic Data Gen
**Agentic data generation** is a crucial aspect of AI development, especially for multi-turn interactions. The paper addresses the scarcity of high-quality agent interaction data by introducing a novel framework called APIGen-MT. This framework leverages **LLMs** and iterative feedback loops to create detailed task blueprints and simulates human-agent interplay. The agentic pipeline produces diverse and verifiable data, enabling the training of more reliable and capable agents. The key is the **blueprint-to-details approach**, where a verified task blueprint guides the generation of realistic interactions. This ensures correctness and naturalness, leading to high-quality training data. The framework uses a committee of LLM reviewers and reflective mechanisms to improve data quality through feedback loops. Finally, it involves a simulated human and agent interaction to produce complete interaction trajectories for training the agent.

#### Verified Details
While the paper does not explicitly contain a section titled "Verified Details", the underlying principle permeates the entire APIGen-MT framework. The framework prioritizes **verifiability** through its two-phase approach. First, it generates task blueprints with ground truth actions, rigorously validated for format, executability, and semantic alignment by an LLM review committee. This ensures that the *core logic* of the interaction is correct and grounded. Second, the simulated agent-human interplay generates interaction trajectories, with **output validation** ensuring the final result aligns with pre-validated actions. This blueprint-to-details ensures generated data is both dynamically plausible and grounded in a correct solution. This is a paradigm shift, emphasizing reliable knowledge transfer and maintaining consistency across multiple trials, making the details more trustworthy.

#### Simulated Agents
**Simulated agents** play a crucial role in AI research, particularly in creating realistic and interactive environments. They allow for testing and training AI systems without real-world constraints. Simulated agents, driven by large language models, offer dynamic interactions and adaptive behaviors. In human-agent interplay, simulated agents provide a cost-effective approach to generating diverse datasets and offer a crucial avenue for robust and reliable AI agent development by mimicking realistic conversations.

#### Multi-Turn Focus
While 'Multi-Turn Focus' isn't a direct heading in this paper, the entire work revolves around precisely that. The APIGen-MT framework is designed to address the challenges of **multi-turn interactions** in AI agents. Existing benchmarks struggle with long-term dependencies and require agents to maintain context, request missing information, and perform complex function calls across multiple turns. APIGen-MT tackles this by generating high-quality data that captures **realistic human-agent dynamics** over extended conversations. The two-phase approach, creating detailed blueprints and then simulating interactions, ensures **verifiable and diverse** multi-turn data, which is crucial for training more robust and reliable AI agents, especially when compared to single-turn interaction models.

#### Scalable Models
While the paper doesn't explicitly use the heading 'Scalable Models,' the experiments section demonstrates a clear focus on model scalability. The XLAM-2-fc-r series, ranging from 1B to 70B parameters, addresses this directly. The observation that smaller models (e.g., xLAM-2-32b-fc-r) can outperform larger baselines highlights the efficiency of the APIGen-MT data generation approach. This suggests the framework enables effective knowledge transfer, allowing smaller models to achieve **competitive performance** with fewer parameters. The potential for scaling up the model size to the 70B parameter range also implies the capability to handle **increasingly complex tasks and larger datasets**, which is critical for real-world deployment. The focus on memory-efficient training via techniques like DeepSpeed further underscores the practical considerations for model scalability.


### More visual insights

<details>
<summary>More on figures
</summary>


![](https://arxiv.org/html/2504.03601/x4.png)

> üîº APIGen-MT is a two-phase framework. Phase 1 uses an agentic process with feedback loops to generate task configurations and ground truth actions.  These configurations define tasks with pre-determined steps and outcomes. Phase 2 simulates realistic human-agent conversations within an executable environment.  A simulated human interacts with a test agent, generating interaction trajectories that include dialogue and actions. This process simulates human-like conversational flows, while ensuring the correctness of the actions against the pre-defined task configuration. The figure illustrates the flow of the two phases, showing how they work together to create high-quality multi-turn data for training AI agents.
> <details>
> <summary>read the caption</summary>
> Figure 2: Overview of the APIGen-MT¬†framework. Phase 1 generates task configurations and groundtruth actions through an agentic process with feedback loops. Phase 2 collects human-agent-environment interaction trajectories by simulating realistic conversations between a human user and a test agent in an executable environment.
> </details>



![](https://arxiv.org/html/2504.03601/x5.png)

> üîº This figure illustrates the APIGen-MT framework's implementation for the T-bench benchmark.  It's a three-stage process: First, realistic task instances are created using a random walk on an API dependency graph and data sampling.  Second, these tasks undergo a multi-stage validation pipeline (format, execution, and policy checks, followed by LLM-based semantic review).  Tasks failing validation are fed back into the generator for improvement. Finally, successful tasks are used to generate multi-turn interaction trajectories through simulated human-agent interactions, where a simulated human interacts with a test agent turn by turn, providing query details.  Only trajectories that successfully complete the task, passing both state- and output-based checks, are retained.
> <details>
> <summary>read the caption</summary>
> Figure 3: Realization of APIGen-MT framework for œÑùúè\tauitalic_œÑ-bench. We first generate realistic task instances by random walk down the API graph and sampling. Next the tasks are validated following a multi-stage pipeline. Instances which fail are sent back to the Generator to be refined based on the validation feedback. Finally, trajectories are generated by a simulated human user that interacts with a test agent by supplying the query details in a turn-wise manner. Trajectories which pass state- and output- based evaluations are collected.
> </details>



![](https://arxiv.org/html/2504.03601/x6.png)

> üîº Figure 4 presents a summary of the dataset statistics generated by the APIGen-MT framework.  It shows the success rates for each stage of the process.  The success rate (S.R.) for task configuration is given with and without the agentic feedback loop employed in Phase 1.  Additionally, the success rate for the trajectory simulation in Phase 2 is reported.  These metrics illustrate the effectiveness of the framework at each stage in generating high-quality multi-turn data for training AI agents.
> <details>
> <summary>read the caption</summary>
> Figure 4: Statistics for the dataset generated using APIGen-MT. Success rates (S.R.) are reported for the task configuration (w. and w/o agentic feedback in Phase 1) and trajectory simulation (Phase 2) stages.
> </details>



![](https://arxiv.org/html/2504.03601/x7.png)

> üîº This figure shows the distribution of the number of turns taken by both the AI assistant and the user in the simulated conversations.  The x-axis represents the number of turns, and the y-axis shows the density (or frequency) of conversations with that many turns.  This visualization helps to understand the typical length of interactions generated by the APIGen-MT system and whether the interactions are balanced between AI assistant and user turns.
> <details>
> <summary>read the caption</summary>
> Figure 5: Density distribution of assistant and user turns in collected trajectories.
> </details>



![](https://arxiv.org/html/2504.03601/x8.png)

> üîº The figure displays two plots showing the probability that all five independent trials of a given task are successful.  The plots are generated for two different domains:  œÑ-retail (left plot) and œÑ-airline (right plot). The x-axis represents the number of trials (k), and the y-axis represents the probability (pass^k). Each curve represents a different model.  A higher probability (closer to 1.0) for a given k indicates that the model consistently produces successful outcomes across multiple attempts at the same task.  This metric is used to evaluate the consistency and reliability of the different models.
> <details>
> <summary>read the caption</summary>
> Figure 6: Pass^k curves measuring the probability that all 5 independent trials succeed for a given task, averaged across all tasks for œÑùúè\tauitalic_œÑ-retail (left) and œÑùúè\tauitalic_œÑ-airline (right) domains. Higher value indicates consistency of the models.
> </details>



</details>




<details>
<summary>More on tables
</summary>


{{< table-caption >}}
<table class="ltx_tabular ltx_align_middle" id="S4.F5.fig1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.F5.fig1.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.F5.fig1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.F5.fig1.1.1.1.1.1" style="font-size:90%;">Metric</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.F5.fig1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.F5.fig1.1.1.1.2.1" style="font-size:90%;">Value</span></td>
</tr>
<tr class="ltx_tr" id="S4.F5.fig1.1.2.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.F5.fig1.1.2.2.1"><span class="ltx_text" id="S4.F5.fig1.1.2.2.1.1" style="font-size:90%;">Task Config. S.R. (Phase 1)</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.F5.fig1.1.2.2.2"><span class="ltx_text" id="S4.F5.fig1.1.2.2.2.1" style="font-size:90%;">70%</span></td>
</tr>
<tr class="ltx_tr" id="S4.F5.fig1.1.3.3">
<td class="ltx_td ltx_align_left" id="S4.F5.fig1.1.3.3.1"><span class="ltx_text" id="S4.F5.fig1.1.3.3.1.1" style="font-size:90%;">Task Config. S.R. w/o Agentic Feedback</span></td>
<td class="ltx_td ltx_align_right" id="S4.F5.fig1.1.3.3.2"><span class="ltx_text" id="S4.F5.fig1.1.3.3.2.1" style="font-size:90%;">28%</span></td>
</tr>
<tr class="ltx_tr" id="S4.F5.fig1.1.4.4">
<td class="ltx_td ltx_align_left" id="S4.F5.fig1.1.4.4.1"><span class="ltx_text" id="S4.F5.fig1.1.4.4.1.1" style="font-size:90%;">Trajectory Sim. S.R. (Phase 2)</span></td>
<td class="ltx_td ltx_align_right" id="S4.F5.fig1.1.4.4.2"><span class="ltx_text" id="S4.F5.fig1.1.4.4.2.1" style="font-size:90%;">67%</span></td>
</tr>
<tr class="ltx_tr" id="S4.F5.fig1.1.5.5">
<td class="ltx_td ltx_align_left" id="S4.F5.fig1.1.5.5.1"><span class="ltx_text" id="S4.F5.fig1.1.5.5.1.1" style="font-size:90%;">Total Validated Trajectories</span></td>
<td class="ltx_td ltx_align_right" id="S4.F5.fig1.1.5.5.2"><span class="ltx_text" id="S4.F5.fig1.1.5.5.2.1" style="font-size:90%;">3,820</span></td>
</tr>
<tr class="ltx_tr" id="S4.F5.fig1.1.6.6">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.F5.fig1.1.6.6.1"><span class="ltx_text" id="S4.F5.fig1.1.6.6.1.1" style="font-size:90%;">Min. Turns per Trajectory</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.F5.fig1.1.6.6.2"><span class="ltx_text" id="S4.F5.fig1.1.6.6.2.1" style="font-size:90%;">1</span></td>
</tr>
<tr class="ltx_tr" id="S4.F5.fig1.1.7.7">
<td class="ltx_td ltx_align_left" id="S4.F5.fig1.1.7.7.1"><span class="ltx_text" id="S4.F5.fig1.1.7.7.1.1" style="font-size:90%;">Max. Turns per Trajectory</span></td>
<td class="ltx_td ltx_align_right" id="S4.F5.fig1.1.7.7.2"><span class="ltx_text" id="S4.F5.fig1.1.7.7.2.1" style="font-size:90%;">29</span></td>
</tr>
<tr class="ltx_tr" id="S4.F5.fig1.1.8.8">
<td class="ltx_td ltx_align_left" id="S4.F5.fig1.1.8.8.1"><span class="ltx_text" id="S4.F5.fig1.1.8.8.1.1" style="font-size:90%;">Avg. Tool Calls per Trajectory</span></td>
<td class="ltx_td ltx_align_right" id="S4.F5.fig1.1.8.8.2"><span class="ltx_text" id="S4.F5.fig1.1.8.8.2.1" style="font-size:90%;">7</span></td>
</tr>
<tr class="ltx_tr" id="S4.F5.fig1.1.9.9">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.F5.fig1.1.9.9.1"><span class="ltx_text" id="S4.F5.fig1.1.9.9.1.1" style="font-size:90%;">Avg. User Turns per Trajectory</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.F5.fig1.1.9.9.2"><span class="ltx_text" id="S4.F5.fig1.1.9.9.2.1" style="font-size:90%;">6</span></td>
</tr>
</tbody>
</table>{{< /table-caption >}}
> üîº This table compares the performance of various open-source and proprietary large language models (LLMs) on the Retail and Airline domains of the T-bench benchmark.  The success rate (pass@1 - the percentage of times the model correctly completes the task on the first try) is reported for each model, averaging across at least five trials for each.  The xLAM-2-fc-r models, trained using data generated by the APIGen-MT method described in the paper, are included for comparison.  The 'Overall' column represents the average success rate across both domains.  Notes indicate the sources for some models' results and clarify that only the benchmark's 'think' tool was used, without prompt optimizations.
> <details>
> <summary>read the caption</summary>
> Table 2: Success Rate (p‚Å¢a‚Å¢s‚Å¢s‚Å¢@‚Å¢1ùëùùëéùë†ùë†@1pass@1italic_p italic_a italic_s italic_s @ 1) of various open-source and proprietary models on the Retail and Airline settings of œÑùúè\tauitalic_œÑ-bench (averaged across at least 5 trials). The xLAM-2-fc-r¬† models are trained on the data generated using APIGen-MT. Overall indicates the average score across both domains. 1 indicates results from [13]; 2 indicates results from [2]; 3 indicate results from [3]; 4 indicates from [4]. Note. We evaluate only with the benchmark‚Äôs think tool and no prompt optimizations.
> </details>

{{< table-caption >}}
<table class="ltx_tabular ltx_align_middle" id="S5.T1.5.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T1.5.1.1.1">
<td class="ltx_td ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.5.1.1.1.1"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.5.1.1.1.2"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.5.1.1.1.3"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3" id="S5.T1.5.1.1.1.4"><span class="ltx_text" id="S5.T1.5.1.1.1.4.1">Single-Turn</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.5.1.1.1.5"><span class="ltx_text" id="S5.T1.5.1.1.1.5.1">Multi-Turn</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2" id="S5.T1.5.1.1.1.6"><span class="ltx_text" id="S5.T1.5.1.1.1.6.1">Hallucination</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.5.1.2.2">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S5.T1.5.1.2.2.1"><span class="ltx_text" id="S5.T1.5.1.2.2.1.1">Rank</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.2.2.2"><span class="ltx_text" id="S5.T1.5.1.2.2.2.1">Overall Acc</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.2.2.3"><span class="ltx_text" id="S5.T1.5.1.2.2.3.1">Model</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.5.1.2.2.4"><span class="ltx_text" id="S5.T1.5.1.2.2.4.1">Non-live (AST)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.5.1.2.2.5"><span class="ltx_text" id="S5.T1.5.1.2.2.5.1">Non-live (Exec)</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.5.1.2.2.6"><span class="ltx_text" id="S5.T1.5.1.2.2.6.1">Live (AST)</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.5.1.2.2.7"><span class="ltx_text" id="S5.T1.5.1.2.2.7.1">Overall Acc</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.5.1.2.2.8"><span class="ltx_text" id="S5.T1.5.1.2.2.8.1">Relevance</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.5.1.2.2.9"><span class="ltx_text" id="S5.T1.5.1.2.2.9.1">Irrelevance</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.5.1.3.3" style="background-color:#C6DDFE;">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.5.1.3.3.1"><span class="ltx_text" id="S5.T1.5.1.3.3.1.1" style="color:#000000;background-color:#C6DDFE;">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.5.1.3.3.2"><span class="ltx_text" id="S5.T1.5.1.3.3.2.1" style="color:#000000;background-color:#C6DDFE;"><span class="ltx_text ltx_font_bold" id="S5.T1.5.1.3.3.2.1.1">78.19</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.5.1.3.3.3"><span class="ltx_text" id="S5.T1.5.1.3.3.3.1" style="color:#000000;background-color:#C6DDFE;"><span class="ltx_text ltx_font_bold" id="S5.T1.5.1.3.3.3.1.1">xLAM-2-70b-fc-r (FC)</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.5.1.3.3.4"><span class="ltx_text" id="S5.T1.5.1.3.3.4.1" style="color:#000000;background-color:#C6DDFE;"><span class="ltx_text ltx_font_bold" id="S5.T1.5.1.3.3.4.1.1">88.48</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.5.1.3.3.5"><span class="ltx_text" id="S5.T1.5.1.3.3.5.1" style="color:#000000;background-color:#C6DDFE;"><span class="ltx_text ltx_font_bold" id="S5.T1.5.1.3.3.5.1.1">85.98</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.5.1.3.3.6" style="background-color:#C6DDFE;"><span class="ltx_text" id="S5.T1.5.1.3.3.6.1" style="color:#000000;background-color:#C6DDFE;"><span class="ltx_text ltx_font_bold" id="S5.T1.5.1.3.3.6.1.1">72.63</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.5.1.3.3.7" style="background-color:#C6DDFE;"><span class="ltx_text" id="S5.T1.5.1.3.3.7.1" style="color:#000000;background-color:#C6DDFE;"><span class="ltx_text ltx_font_bold" id="S5.T1.5.1.3.3.7.1.1">75.12</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.5.1.3.3.8"><span class="ltx_text" id="S5.T1.5.1.3.3.8.1" style="color:#000000;background-color:#C6DDFE;"><span class="ltx_text ltx_font_bold" id="S5.T1.5.1.3.3.8.1.1">66.67</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.5.1.3.3.9"><span class="ltx_text" id="S5.T1.5.1.3.3.9.1" style="color:#000000;background-color:#C6DDFE;"><span class="ltx_text ltx_font_bold" id="S5.T1.5.1.3.3.9.1.1">78.74</span></span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.5.1.4.4" style="background-color:#DAE8FC;">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S5.T1.5.1.4.4.1"><span class="ltx_text" id="S5.T1.5.1.4.4.1.1" style="color:#000000;background-color:#DAE8FC;"><span class="ltx_text ltx_font_bold" id="S5.T1.5.1.4.4.1.1.1">2</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.4.4.2"><span class="ltx_text" id="S5.T1.5.1.4.4.2.1" style="color:#000000;background-color:#DAE8FC;"><span class="ltx_text ltx_font_bold" id="S5.T1.5.1.4.4.2.1.1">75.83</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.4.4.3"><span class="ltx_text" id="S5.T1.5.1.4.4.3.1" style="color:#000000;background-color:#DAE8FC;"><span class="ltx_text ltx_font_bold" id="S5.T1.5.1.4.4.3.1.1">xLAM-2-32b-fc-r (FC)</span></span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.5.1.4.4.4"><span class="ltx_text" id="S5.T1.5.1.4.4.4.1" style="color:#000000;background-color:#DAE8FC;"><span class="ltx_text ltx_font_bold" id="S5.T1.5.1.4.4.4.1.1">89.50</span></span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.5.1.4.4.5"><span class="ltx_text" id="S5.T1.5.1.4.4.5.1" style="color:#000000;background-color:#DAE8FC;"><span class="ltx_text ltx_font_bold" id="S5.T1.5.1.4.4.5.1.1">86.48</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.4.4.6" style="background-color:#DAE8FC;"><span class="ltx_text" id="S5.T1.5.1.4.4.6.1" style="color:#000000;background-color:#DAE8FC;"><span class="ltx_text ltx_font_bold" id="S5.T1.5.1.4.4.6.1.1">73.79</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.4.4.7" style="background-color:#DAE8FC;"><span class="ltx_text" id="S5.T1.5.1.4.4.7.1" style="color:#000000;background-color:#DAE8FC;"><span class="ltx_text ltx_font_bold" id="S5.T1.5.1.4.4.7.1.1">66.38</span></span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.5.1.4.4.8"><span class="ltx_text" id="S5.T1.5.1.4.4.8.1" style="color:#000000;background-color:#DAE8FC;"><span class="ltx_text ltx_font_bold" id="S5.T1.5.1.4.4.8.1.1">83.33</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.4.4.9"><span class="ltx_text" id="S5.T1.5.1.4.4.9.1" style="color:#000000;background-color:#DAE8FC;"><span class="ltx_text ltx_font_bold" id="S5.T1.5.1.4.4.9.1.1">76.25</span></span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.5.1.5.5">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S5.T1.5.1.5.5.1"><span class="ltx_text" id="S5.T1.5.1.5.5.1.1">3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.5.5.2"><span class="ltx_text" id="S5.T1.5.1.5.5.2.1">74.31</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.5.5.3"><span class="ltx_text" id="S5.T1.5.1.5.5.3.1">watt-tool-70b (FC)</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.5.1.5.5.4"><span class="ltx_text" id="S5.T1.5.1.5.5.4.1">84.06</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.5.1.5.5.5"><span class="ltx_text" id="S5.T1.5.1.5.5.5.1">89.39</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.5.5.6"><span class="ltx_text" id="S5.T1.5.1.5.5.6.1">77.74</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.5.5.7"><span class="ltx_text" id="S5.T1.5.1.5.5.7.1">58.75</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.5.1.5.5.8"><span class="ltx_text" id="S5.T1.5.1.5.5.8.1">94.44</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.5.5.9"><span class="ltx_text" id="S5.T1.5.1.5.5.9.1">76.32</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.5.1.6.6" style="background-color:#E8F2FF;">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S5.T1.5.1.6.6.1"><span class="ltx_text" id="S5.T1.5.1.6.6.1.1" style="color:#000000;background-color:#E8F2FF;"><span class="ltx_text ltx_font_bold" id="S5.T1.5.1.6.6.1.1.1">4</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.6.6.2"><span class="ltx_text" id="S5.T1.5.1.6.6.2.1" style="color:#000000;background-color:#E8F2FF;"><span class="ltx_text ltx_font_bold" id="S5.T1.5.1.6.6.2.1.1">72.83</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.6.6.3"><span class="ltx_text" id="S5.T1.5.1.6.6.3.1" style="color:#000000;background-color:#E8F2FF;"><span class="ltx_text ltx_font_bold" id="S5.T1.5.1.6.6.3.1.1">xLAM-2-8b-fc-r (FC)</span></span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.5.1.6.6.4"><span class="ltx_text" id="S5.T1.5.1.6.6.4.1" style="color:#000000;background-color:#E8F2FF;"><span class="ltx_text ltx_font_bold" id="S5.T1.5.1.6.6.4.1.1">84.35</span></span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.5.1.6.6.5"><span class="ltx_text" id="S5.T1.5.1.6.6.5.1" style="color:#000000;background-color:#E8F2FF;"><span class="ltx_text ltx_font_bold" id="S5.T1.5.1.6.6.5.1.1">85.59</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.6.6.6" style="background-color:#E8F2FF;"><span class="ltx_text" id="S5.T1.5.1.6.6.6.1" style="color:#000000;background-color:#E8F2FF;"><span class="ltx_text ltx_font_bold" id="S5.T1.5.1.6.6.6.1.1">66.73</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.6.6.7" style="background-color:#E8F2FF;"><span class="ltx_text" id="S5.T1.5.1.6.6.7.1" style="color:#000000;background-color:#E8F2FF;"><span class="ltx_text ltx_font_bold" id="S5.T1.5.1.6.6.7.1.1">69.25</span></span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.5.1.6.6.8"><span class="ltx_text" id="S5.T1.5.1.6.6.8.1" style="color:#000000;background-color:#E8F2FF;"><span class="ltx_text ltx_font_bold" id="S5.T1.5.1.6.6.8.1.1">83.33</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.6.6.9"><span class="ltx_text" id="S5.T1.5.1.6.6.9.1" style="color:#000000;background-color:#E8F2FF;"><span class="ltx_text ltx_font_bold" id="S5.T1.5.1.6.6.9.1.1">64.11</span></span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.5.1.7.7">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S5.T1.5.1.7.7.1"><span class="ltx_text" id="S5.T1.5.1.7.7.1.1">5</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.7.7.2"><span class="ltx_text" id="S5.T1.5.1.7.7.2.1">72.08</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.7.7.3"><span class="ltx_text" id="S5.T1.5.1.7.7.3.1">GPT-4o-2024-11-20 (Prompt)</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.5.1.7.7.4"><span class="ltx_text" id="S5.T1.5.1.7.7.4.1">88.1</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.5.1.7.7.5"><span class="ltx_text" id="S5.T1.5.1.7.7.5.1">89.38</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.7.7.6"><span class="ltx_text" id="S5.T1.5.1.7.7.6.1">79.83</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.7.7.7"><span class="ltx_text" id="S5.T1.5.1.7.7.7.1">47.62</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.5.1.7.7.8"><span class="ltx_text" id="S5.T1.5.1.7.7.8.1">83.33</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.7.7.9"><span class="ltx_text" id="S5.T1.5.1.7.7.9.1">83.76</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.5.1.8.8">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S5.T1.5.1.8.8.1"><span class="ltx_text" id="S5.T1.5.1.8.8.1.1">6</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.8.8.2"><span class="ltx_text" id="S5.T1.5.1.8.8.2.1">69.94</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.8.8.3"><span class="ltx_text" id="S5.T1.5.1.8.8.3.1">GPT-4.5-Preview-02-27 (FC)</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.5.1.8.8.4"><span class="ltx_text" id="S5.T1.5.1.8.8.4.1">86.12</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.5.1.8.8.5"><span class="ltx_text" id="S5.T1.5.1.8.8.5.1">83.98</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.8.8.6"><span class="ltx_text" id="S5.T1.5.1.8.8.6.1">79.34</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.8.8.7"><span class="ltx_text" id="S5.T1.5.1.8.8.7.1">45.25</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.5.1.8.8.8"><span class="ltx_text" id="S5.T1.5.1.8.8.8.1">66.67</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.8.8.9"><span class="ltx_text" id="S5.T1.5.1.8.8.9.1">83.64</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.5.1.9.9">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S5.T1.5.1.9.9.1"><span class="ltx_text" id="S5.T1.5.1.9.9.1.1">7</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.9.9.2"><span class="ltx_text" id="S5.T1.5.1.9.9.2.1">69.58</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.9.9.3"><span class="ltx_text" id="S5.T1.5.1.9.9.3.1">GPT-4o-2024-11-20 (FC)</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.5.1.9.9.4"><span class="ltx_text" id="S5.T1.5.1.9.9.4.1">87.42</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.5.1.9.9.5"><span class="ltx_text" id="S5.T1.5.1.9.9.5.1">89.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.9.9.6"><span class="ltx_text" id="S5.T1.5.1.9.9.6.1">79.65</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.9.9.7"><span class="ltx_text" id="S5.T1.5.1.9.9.7.1">41</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.5.1.9.9.8"><span class="ltx_text" id="S5.T1.5.1.9.9.8.1">83.33</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.9.9.9"><span class="ltx_text" id="S5.T1.5.1.9.9.9.1">83.15</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.5.1.10.10">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S5.T1.5.1.10.10.1"><span class="ltx_text" id="S5.T1.5.1.10.10.1.1">8</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.10.10.2"><span class="ltx_text" id="S5.T1.5.1.10.10.2.1">68.39</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.10.10.3"><span class="ltx_text" id="S5.T1.5.1.10.10.3.1">ToolACE-2-8B (FC)</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.5.1.10.10.4"><span class="ltx_text" id="S5.T1.5.1.10.10.4.1">87.58</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.5.1.10.10.5"><span class="ltx_text" id="S5.T1.5.1.10.10.5.1">87.11</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.10.10.6"><span class="ltx_text" id="S5.T1.5.1.10.10.6.1">80.05</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.10.10.7"><span class="ltx_text" id="S5.T1.5.1.10.10.7.1">36.88</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.5.1.10.10.8"><span class="ltx_text" id="S5.T1.5.1.10.10.8.1">72.22</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.10.10.9"><span class="ltx_text" id="S5.T1.5.1.10.10.9.1">90.11</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.5.1.11.11">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S5.T1.5.1.11.11.1"><span class="ltx_text" id="S5.T1.5.1.11.11.1.1">9</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.11.11.2"><span class="ltx_text" id="S5.T1.5.1.11.11.2.1">67.98</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.11.11.3"><span class="ltx_text" id="S5.T1.5.1.11.11.3.1">watt-tool-8B (FC)</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.5.1.11.11.4"><span class="ltx_text" id="S5.T1.5.1.11.11.4.1">86.56</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.5.1.11.11.5"><span class="ltx_text" id="S5.T1.5.1.11.11.5.1">89.34</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.11.11.6"><span class="ltx_text" id="S5.T1.5.1.11.11.6.1">76.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.11.11.7"><span class="ltx_text" id="S5.T1.5.1.11.11.7.1">39.12</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.5.1.11.11.8"><span class="ltx_text" id="S5.T1.5.1.11.11.8.1">83.33</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.11.11.9"><span class="ltx_text" id="S5.T1.5.1.11.11.9.1">83.15</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.5.1.12.12">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S5.T1.5.1.12.12.1"><span class="ltx_text" id="S5.T1.5.1.12.12.1.1">10</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.12.12.2"><span class="ltx_text" id="S5.T1.5.1.12.12.2.1">67.88</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.12.12.3"><span class="ltx_text" id="S5.T1.5.1.12.12.3.1">GPT-4-2024-04-09 (FC)</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.5.1.12.12.4"><span class="ltx_text" id="S5.T1.5.1.12.12.4.1">84.73</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.5.1.12.12.5"><span class="ltx_text" id="S5.T1.5.1.12.12.5.1">85.21</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.12.12.6"><span class="ltx_text" id="S5.T1.5.1.12.12.6.1">80.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.12.12.7"><span class="ltx_text" id="S5.T1.5.1.12.12.7.1">38.12</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.5.1.12.12.8"><span class="ltx_text" id="S5.T1.5.1.12.12.8.1">72.22</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.12.12.9"><span class="ltx_text" id="S5.T1.5.1.12.12.9.1">83.81</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.5.1.13.13">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S5.T1.5.1.13.13.1"><span class="ltx_text" id="S5.T1.5.1.13.13.1.1">11</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.13.13.2"><span class="ltx_text" id="S5.T1.5.1.13.13.2.1">67.87</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.13.13.3"><span class="ltx_text" id="S5.T1.5.1.13.13.3.1">o1-2024-12-17 (Prompt)</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.5.1.13.13.4"><span class="ltx_text" id="S5.T1.5.1.13.13.4.1">85.67</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.5.1.13.13.5"><span class="ltx_text" id="S5.T1.5.1.13.13.5.1">87.45</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.13.13.6"><span class="ltx_text" id="S5.T1.5.1.13.13.6.1">80.63</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.13.13.7"><span class="ltx_text" id="S5.T1.5.1.13.13.7.1">36</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.5.1.13.13.8"><span class="ltx_text" id="S5.T1.5.1.13.13.8.1">72.22</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.13.13.9"><span class="ltx_text" id="S5.T1.5.1.13.13.9.1">87.78</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.5.1.14.14">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S5.T1.5.1.14.14.1"><span class="ltx_text" id="S5.T1.5.1.14.14.1.1">12</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.14.14.2"><span class="ltx_text" id="S5.T1.5.1.14.14.2.1">67.72</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.14.14.3"><span class="ltx_text" id="S5.T1.5.1.14.14.3.1">BitAgent-8B</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.5.1.14.14.4"><span class="ltx_text" id="S5.T1.5.1.14.14.4.1">86.92</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.5.1.14.14.5"><span class="ltx_text" id="S5.T1.5.1.14.14.5.1">89.52</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.14.14.6"><span class="ltx_text" id="S5.T1.5.1.14.14.6.1">76.14</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.14.14.7"><span class="ltx_text" id="S5.T1.5.1.14.14.7.1">38.5</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.5.1.14.14.8"><span class="ltx_text" id="S5.T1.5.1.14.14.8.1">83.33</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.14.14.9"><span class="ltx_text" id="S5.T1.5.1.14.14.9.1">82.38</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.5.1.15.15">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S5.T1.5.1.15.15.1"><span class="ltx_text" id="S5.T1.5.1.15.15.1.1">13</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.15.15.2"><span class="ltx_text" id="S5.T1.5.1.15.15.2.1">65.12</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.15.15.3"><span class="ltx_text" id="S5.T1.5.1.15.15.3.1">o3-mini-25-01-31 (Prompt)</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.5.1.15.15.4"><span class="ltx_text" id="S5.T1.5.1.15.15.4.1">86.15</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.5.1.15.15.5"><span class="ltx_text" id="S5.T1.5.1.15.15.5.1">89.46</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.15.15.6"><span class="ltx_text" id="S5.T1.5.1.15.15.6.1">79.08</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.15.15.7"><span class="ltx_text" id="S5.T1.5.1.15.15.7.1">28.75</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.5.1.15.15.8"><span class="ltx_text" id="S5.T1.5.1.15.15.8.1">72.22</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.15.15.9"><span class="ltx_text" id="S5.T1.5.1.15.15.9.1">82.96</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.5.1.16.16" style="background-color:#ECF4FF;">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S5.T1.5.1.16.16.1"><span class="ltx_text" id="S5.T1.5.1.16.16.1.1" style="color:#000000;background-color:#ECF4FF;"><span class="ltx_text ltx_font_bold" id="S5.T1.5.1.16.16.1.1.1">14</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.16.16.2"><span class="ltx_text" id="S5.T1.5.1.16.16.2.1" style="color:#000000;background-color:#ECF4FF;"><span class="ltx_text ltx_font_bold" id="S5.T1.5.1.16.16.2.1.1">65.11</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.16.16.3"><span class="ltx_text" id="S5.T1.5.1.16.16.3.1" style="color:#000000;background-color:#ECF4FF;"><span class="ltx_text ltx_font_bold" id="S5.T1.5.1.16.16.3.1.1">xLAM-2-3b-fc-r (FC)</span></span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.5.1.16.16.4"><span class="ltx_text" id="S5.T1.5.1.16.16.4.1" style="color:#000000;background-color:#ECF4FF;"><span class="ltx_text ltx_font_bold" id="S5.T1.5.1.16.16.4.1.1">82.94</span></span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.5.1.16.16.5"><span class="ltx_text" id="S5.T1.5.1.16.16.5.1" style="color:#000000;background-color:#ECF4FF;"><span class="ltx_text ltx_font_bold" id="S5.T1.5.1.16.16.5.1.1">81.88</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.16.16.6" style="background-color:#ECF4FF;"><span class="ltx_text" id="S5.T1.5.1.16.16.6.1" style="color:#000000;background-color:#ECF4FF;"><span class="ltx_text ltx_font_bold" id="S5.T1.5.1.16.16.6.1.1">58.69</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.16.16.7" style="background-color:#ECF4FF;"><span class="ltx_text" id="S5.T1.5.1.16.16.7.1" style="color:#000000;background-color:#ECF4FF;"><span class="ltx_text ltx_font_bold" id="S5.T1.5.1.16.16.7.1.1">56.00</span></span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.5.1.16.16.8"><span class="ltx_text" id="S5.T1.5.1.16.16.8.1" style="color:#000000;background-color:#ECF4FF;"><span class="ltx_text ltx_font_bold" id="S5.T1.5.1.16.16.8.1.1">94.44</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.16.16.9"><span class="ltx_text" id="S5.T1.5.1.16.16.9.1" style="color:#000000;background-color:#ECF4FF;"><span class="ltx_text ltx_font_bold" id="S5.T1.5.1.16.16.9.1.1">57.94</span></span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.5.1.17.17">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S5.T1.5.1.17.17.1"><span class="ltx_text" id="S5.T1.5.1.17.17.1.1">15</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.17.17.2"><span class="ltx_text" id="S5.T1.5.1.17.17.2.1">64.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.17.17.3"><span class="ltx_text" id="S5.T1.5.1.17.17.3.1">CoALM-405B</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.5.1.17.17.4"><span class="ltx_text" id="S5.T1.5.1.17.17.4.1">90.58</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.5.1.17.17.5"><span class="ltx_text" id="S5.T1.5.1.17.17.5.1">89.07</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.17.17.6"><span class="ltx_text" id="S5.T1.5.1.17.17.6.1">74.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.17.17.7"><span class="ltx_text" id="S5.T1.5.1.17.17.7.1">28.75</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.5.1.17.17.8"><span class="ltx_text" id="S5.T1.5.1.17.17.8.1">100</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.17.17.9"><span class="ltx_text" id="S5.T1.5.1.17.17.9.1">71.79</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.5.1.18.18">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S5.T1.5.1.18.18.1"><span class="ltx_text" id="S5.T1.5.1.18.18.1.1">16</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.18.18.2"><span class="ltx_text" id="S5.T1.5.1.18.18.2.1">64.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.18.18.3"><span class="ltx_text" id="S5.T1.5.1.18.18.3.1">GPT-4o-mini-24-07-18 (FC)</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.5.1.18.18.4"><span class="ltx_text" id="S5.T1.5.1.18.18.4.1">85.21</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.5.1.18.18.5"><span class="ltx_text" id="S5.T1.5.1.18.18.5.1">83.57</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.18.18.6"><span class="ltx_text" id="S5.T1.5.1.18.18.6.1">74.41</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.18.18.7"><span class="ltx_text" id="S5.T1.5.1.18.18.7.1">34.12</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.5.1.18.18.8"><span class="ltx_text" id="S5.T1.5.1.18.18.8.1">83.33</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.18.18.9"><span class="ltx_text" id="S5.T1.5.1.18.18.9.1">74.75</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.5.1.19.19">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.5.1.19.19.1"><span class="ltx_text" id="S5.T1.5.1.19.19.1.1">‚Ä¶</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.5.1.19.19.2"><span class="ltx_text" id="S5.T1.5.1.19.19.2.1">‚Ä¶</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.5.1.19.19.3"><span class="ltx_text" id="S5.T1.5.1.19.19.3.1">‚Ä¶</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="6" id="S5.T1.5.1.19.19.4"><span class="ltx_text" id="S5.T1.5.1.19.19.4.1">‚Ä¶</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.5.1.20.20">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.5.1.20.20.1"><span class="ltx_text" id="S5.T1.5.1.20.20.1.1">34</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.5.1.20.20.2"><span class="ltx_text" id="S5.T1.5.1.20.20.2.1">58.93</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.5.1.20.20.3"><span class="ltx_text" id="S5.T1.5.1.20.20.3.1">Gemini-2-Flash-Thinking</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.5.1.20.20.4"><span class="ltx_text" id="S5.T1.5.1.20.20.4.1">87.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.5.1.20.20.5"><span class="ltx_text" id="S5.T1.5.1.20.20.5.1">87.07</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.5.1.20.20.6"><span class="ltx_text" id="S5.T1.5.1.20.20.6.1">75.97</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.5.1.20.20.7"><span class="ltx_text" id="S5.T1.5.1.20.20.7.1">14.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.5.1.20.20.8"><span class="ltx_text" id="S5.T1.5.1.20.20.8.1">77.78</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.5.1.20.20.9"><span class="ltx_text" id="S5.T1.5.1.20.20.9.1">72.75</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.5.1.21.21">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S5.T1.5.1.21.21.1"><span class="ltx_text" id="S5.T1.5.1.21.21.1.1">35</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.21.21.2"><span class="ltx_text" id="S5.T1.5.1.21.21.2.1">58.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.21.21.3"><span class="ltx_text" id="S5.T1.5.1.21.21.3.1">Qwen2.5-14B-Instruct (FC)</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.5.1.21.21.4"><span class="ltx_text" id="S5.T1.5.1.21.21.4.1">85.42</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.5.1.21.21.5"><span class="ltx_text" id="S5.T1.5.1.21.21.5.1">84.86</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.21.21.6"><span class="ltx_text" id="S5.T1.5.1.21.21.6.1">76.68</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.21.21.7"><span class="ltx_text" id="S5.T1.5.1.21.21.7.1">15.88</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.5.1.21.21.8"><span class="ltx_text" id="S5.T1.5.1.21.21.8.1">55.56</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.21.21.9"><span class="ltx_text" id="S5.T1.5.1.21.21.9.1">77.69</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.5.1.22.22" style="background-color:#F0F6FF;">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S5.T1.5.1.22.22.1"><span class="ltx_text" id="S5.T1.5.1.22.22.1.1" style="color:#000000;background-color:#F0F6FF;"><span class="ltx_text ltx_font_bold" id="S5.T1.5.1.22.22.1.1.1">36</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.22.22.2"><span class="ltx_text" id="S5.T1.5.1.22.22.2.1" style="color:#000000;background-color:#F0F6FF;"><span class="ltx_text ltx_font_bold" id="S5.T1.5.1.22.22.2.1.1">58.90</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.22.22.3"><span class="ltx_text" id="S5.T1.5.1.22.22.3.1" style="color:#000000;background-color:#F0F6FF;"><span class="ltx_text ltx_font_bold" id="S5.T1.5.1.22.22.3.1.1">xLAM-2-1b-fc-r (FC)</span></span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.5.1.22.22.4"><span class="ltx_text" id="S5.T1.5.1.22.22.4.1" style="color:#000000;background-color:#F0F6FF;"><span class="ltx_text ltx_font_bold" id="S5.T1.5.1.22.22.4.1.1">76.23</span></span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.5.1.22.22.5"><span class="ltx_text" id="S5.T1.5.1.22.22.5.1" style="color:#000000;background-color:#F0F6FF;"><span class="ltx_text ltx_font_bold" id="S5.T1.5.1.22.22.5.1.1">74.86</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.22.22.6" style="background-color:#F0F6FF;"><span class="ltx_text" id="S5.T1.5.1.22.22.6.1" style="color:#000000;background-color:#F0F6FF;"><span class="ltx_text ltx_font_bold" id="S5.T1.5.1.22.22.6.1.1">59.88</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.22.22.7" style="background-color:#F0F6FF;"><span class="ltx_text" id="S5.T1.5.1.22.22.7.1" style="color:#000000;background-color:#F0F6FF;"><span class="ltx_text ltx_font_bold" id="S5.T1.5.1.22.22.7.1.1">43.12</span></span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.5.1.22.22.8"><span class="ltx_text" id="S5.T1.5.1.22.22.8.1" style="color:#000000;background-color:#F0F6FF;"><span class="ltx_text ltx_font_bold" id="S5.T1.5.1.22.22.8.1.1">88.89</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.22.22.9"><span class="ltx_text" id="S5.T1.5.1.22.22.9.1" style="color:#000000;background-color:#F0F6FF;"><span class="ltx_text ltx_font_bold" id="S5.T1.5.1.22.22.9.1.1">56.87</span></span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.5.1.23.23">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S5.T1.5.1.23.23.1"><span class="ltx_text" id="S5.T1.5.1.23.23.1.1">37</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.23.23.2"><span class="ltx_text" id="S5.T1.5.1.23.23.2.1">58.55</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.23.23.3"><span class="ltx_text" id="S5.T1.5.1.23.23.3.1">DeepSeek-V3 (FC)</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.5.1.23.23.4"><span class="ltx_text" id="S5.T1.5.1.23.23.4.1">89.17</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.5.1.23.23.5"><span class="ltx_text" id="S5.T1.5.1.23.23.5.1">92.32</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.23.23.6"><span class="ltx_text" id="S5.T1.5.1.23.23.6.1">68.41</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.23.23.7"><span class="ltx_text" id="S5.T1.5.1.23.23.7.1">18.62</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.5.1.23.23.8"><span class="ltx_text" id="S5.T1.5.1.23.23.8.1">88.89</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.23.23.9"><span class="ltx_text" id="S5.T1.5.1.23.23.9.1">59.36</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.5.1.24.24">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S5.T1.5.1.24.24.1"><span class="ltx_text" id="S5.T1.5.1.24.24.1.1">38</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.24.24.2"><span class="ltx_text" id="S5.T1.5.1.24.24.2.1">58.45</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.24.24.3"><span class="ltx_text" id="S5.T1.5.1.24.24.3.1">mistral-large-2407 (FC)</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.5.1.24.24.4"><span class="ltx_text" id="S5.T1.5.1.24.24.4.1">86.81</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.5.1.24.24.5"><span class="ltx_text" id="S5.T1.5.1.24.24.5.1">84.38</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.24.24.6"><span class="ltx_text" id="S5.T1.5.1.24.24.6.1">69.88</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.24.24.7"><span class="ltx_text" id="S5.T1.5.1.24.24.7.1">23.75</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.5.1.24.24.8"><span class="ltx_text" id="S5.T1.5.1.24.24.8.1">72.22</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.5.1.24.24.9"><span class="ltx_text" id="S5.T1.5.1.24.24.9.1">52.85</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.5.1.25.25">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r" id="S5.T1.5.1.25.25.1"><span class="ltx_text" id="S5.T1.5.1.25.25.1.1">39</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T1.5.1.25.25.2"><span class="ltx_text" id="S5.T1.5.1.25.25.2.1">58.42</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T1.5.1.25.25.3"><span class="ltx_text" id="S5.T1.5.1.25.25.3.1">ToolACE-8B (FC)</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T1.5.1.25.25.4"><span class="ltx_text" id="S5.T1.5.1.25.25.4.1">87.54</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T1.5.1.25.25.5"><span class="ltx_text" id="S5.T1.5.1.25.25.5.1">89.21</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T1.5.1.25.25.6"><span class="ltx_text" id="S5.T1.5.1.25.25.6.1">78.59</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T1.5.1.25.25.7"><span class="ltx_text" id="S5.T1.5.1.25.25.7.1">7.75</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T1.5.1.25.25.8"><span class="ltx_text" id="S5.T1.5.1.25.25.8.1">83.33</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T1.5.1.25.25.9"><span class="ltx_text" id="S5.T1.5.1.25.25.9.1">87.88</span></td>
</tr>
</tbody>
</table>{{< /table-caption >}}
> üîº This table presents the success rates of gpt-40 and xLAM-2-70b-fc-r models across five trials on the Retail domain of the T-bench benchmark.  Two user simulation methods are compared: a naive approach and a 'Best-of-N' (BoN) approach. The BoN method selects the best response from multiple generated responses, improving the consistency and stability of the evaluation. The table shows that the average success rate is higher, and the variance is lower with the BoN user simulation for both models, suggesting that this approach yields a more reliable evaluation.
> <details>
> <summary>read the caption</summary>
> Table 3: The Success Rate (SR) measured across 5 trials on the Retail domain of œÑùúè\tauitalic_œÑ-bench using gpt-4o and xLAM-2-70b-fc-r¬† as the test assistants. The average success rate is higher with lower variance using BoN based user simulation, indicative of a more stable evaluation.
> </details>

</details>




### Full paper

{{< gallery >}}
<img src="https://ai-paper-reviewer.com/2504.03601/1.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.03601/2.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.03601/3.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.03601/4.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.03601/5.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.03601/6.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.03601/7.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.03601/8.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.03601/9.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.03601/10.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.03601/11.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.03601/12.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.03601/13.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.03601/14.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.03601/15.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.03601/16.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.03601/17.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.03601/18.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
{{< /gallery >}}