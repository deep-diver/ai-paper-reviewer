[{"figure_path": "https://arxiv.org/html/2504.03601/x3.png", "caption": "Figure 1: Comparative performance of larger xLAM-2-fc-r\u00a0 models (8B-70B, trained with APIGen-MT\u00a0 data) against state-of-the-art baselines on function-calling (BFCL v3 [43]) and agentic (\u03c4\ud835\udf0f\\tauitalic_\u03c4-bench [47]) capabilities.", "description": "This figure compares the performance of different large language models (LLMs) on two benchmark tasks: function calling and agentic reasoning.  The xLAM-2-fc-r models, trained on data generated by the APIGen-MT pipeline, are compared to state-of-the-art baselines (GPT-40 and Claude 3.5).  The comparison shows how well the models perform at executing functions (BFCL v3 benchmark) and engaging in multi-turn, interactive conversations requiring complex reasoning (T-bench benchmark). The results demonstrate the capabilities of the xLAM-2-fc-r models, especially their smaller variants, in executing function calls and handling complex multi-turn interactions.", "section": "Experiment Results"}, {"figure_path": "https://arxiv.org/html/2504.03601/x4.png", "caption": "Figure 2: Overview of the APIGen-MT\u00a0framework. Phase 1 generates task configurations and groundtruth actions through an agentic process with feedback loops. Phase 2 collects human-agent-environment interaction trajectories by simulating realistic conversations between a human user and a test agent in an executable environment.", "description": "APIGen-MT is a two-phase framework. Phase 1 uses an agentic process with feedback loops to generate task configurations and ground truth actions.  These configurations define tasks with pre-determined steps and outcomes. Phase 2 simulates realistic human-agent conversations within an executable environment.  A simulated human interacts with a test agent, generating interaction trajectories that include dialogue and actions. This process simulates human-like conversational flows, while ensuring the correctness of the actions against the pre-defined task configuration. The figure illustrates the flow of the two phases, showing how they work together to create high-quality multi-turn data for training AI agents.", "section": "3.2 APIGen-MT Framework Overview"}, {"figure_path": "https://arxiv.org/html/2504.03601/x5.png", "caption": "Figure 3: Realization of APIGen-MT framework for \u03c4\ud835\udf0f\\tauitalic_\u03c4-bench. We first generate realistic task instances by random walk down the API graph and sampling. Next the tasks are validated following a multi-stage pipeline. Instances which fail are sent back to the Generator to be refined based on the validation feedback. Finally, trajectories are generated by a simulated human user that interacts with a test agent by supplying the query details in a turn-wise manner. Trajectories which pass state- and output- based evaluations are collected.", "description": "This figure illustrates the APIGen-MT framework's implementation for the T-bench benchmark.  It's a three-stage process: First, realistic task instances are created using a random walk on an API dependency graph and data sampling.  Second, these tasks undergo a multi-stage validation pipeline (format, execution, and policy checks, followed by LLM-based semantic review).  Tasks failing validation are fed back into the generator for improvement. Finally, successful tasks are used to generate multi-turn interaction trajectories through simulated human-agent interactions, where a simulated human interacts with a test agent turn by turn, providing query details.  Only trajectories that successfully complete the task, passing both state- and output-based checks, are retained.", "section": "3.2.1 Phase 1: Task Configuration and Groundtruth Generation"}, {"figure_path": "https://arxiv.org/html/2504.03601/x6.png", "caption": "Figure 4: Statistics for the dataset generated using APIGen-MT. Success rates (S.R.) are reported for the task configuration (w. and w/o agentic feedback in Phase 1) and trajectory simulation (Phase 2) stages.", "description": "Figure 4 presents a summary of the dataset statistics generated by the APIGen-MT framework.  It shows the success rates for each stage of the process.  The success rate (S.R.) for task configuration is given with and without the agentic feedback loop employed in Phase 1.  Additionally, the success rate for the trajectory simulation in Phase 2 is reported.  These metrics illustrate the effectiveness of the framework at each stage in generating high-quality multi-turn data for training AI agents.", "section": "3.2 APIGen-MT Framework Overview"}, {"figure_path": "https://arxiv.org/html/2504.03601/x7.png", "caption": "Figure 5: Density distribution of assistant and user turns in collected trajectories.", "description": "This figure shows the distribution of the number of turns taken by both the AI assistant and the user in the simulated conversations.  The x-axis represents the number of turns, and the y-axis shows the density (or frequency) of conversations with that many turns.  This visualization helps to understand the typical length of interactions generated by the APIGen-MT system and whether the interactions are balanced between AI assistant and user turns.", "section": "5.3 Consistency & Stability Experiments"}, {"figure_path": "https://arxiv.org/html/2504.03601/x8.png", "caption": "Figure 6: Pass^k curves measuring the probability that all 5 independent trials succeed for a given task, averaged across all tasks for \u03c4\ud835\udf0f\\tauitalic_\u03c4-retail (left) and \u03c4\ud835\udf0f\\tauitalic_\u03c4-airline (right) domains. Higher value indicates consistency of the models.", "description": "The figure displays two plots showing the probability that all five independent trials of a given task are successful.  The plots are generated for two different domains:  \u03c4-retail (left plot) and \u03c4-airline (right plot). The x-axis represents the number of trials (k), and the y-axis represents the probability (pass^k). Each curve represents a different model.  A higher probability (closer to 1.0) for a given k indicates that the model consistently produces successful outcomes across multiple attempts at the same task.  This metric is used to evaluate the consistency and reliability of the different models.", "section": "5.3 Consistency & Stability Experiments"}]