[{"figure_path": "https://arxiv.org/html/2504.02949/x1.png", "caption": "Figure 1: A comparative analysis of various MLLMs across multiple visual comprehension benchmarks is presented. The remaining metrics are derived from standard visual question-answering benchmarks and multi-modal comprehension benchmarks. Notably, our VARGPT-v1.1 model demonstrates significant superiority over the compared baselines across all comprehension benchmarks.", "description": "This figure presents a comparative analysis of various multimodal large language models (MLLMs) on several visual comprehension benchmarks.  The benchmarks include a mix of standard visual question answering (VQA) datasets and more recent multi-modal comprehension benchmarks. The results clearly show that the VARGPT-v1.1 model significantly outperforms other models across all the comprehension tasks evaluated.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2504.02949/x2.png", "caption": "Figure 2: Comparison of different model architectures, where, \u2018AR\u2019 denotes autoregressive, while \u2018VAR\u2019 signifies visual autoregressive. We present a comparative analysis of architectures designed for comprehension-only tasks, generation-only tasks, and unified comprehension and generation, alongside our proposed VARGPT-v1.1 an VARGPT\u00a0[14] model. Our VARGPT-v1.1 and VARGPT are conceptualized as purely autoregressive multimodel model, achieving visual comprehension through next-token prediction and visual generation through next-scale prediction paradigms.", "description": "Figure 3 illustrates various model architectures for multimodal tasks, categorized into comprehension-only, generation-only, and unified models.  The figure highlights the differences in how these models approach visual understanding and generation.  Specifically, it shows models that only perform understanding (e.g., using only an autoregressive language model), those only producing generation (e.g., using diffusion models), and those which attempt to unify both tasks into a single architecture. The figure also positions the authors' proposed models, VARGPT and VARGPT-v1.1, within this landscape, emphasizing their unique autoregressive approach that uses next-token prediction for understanding and next-scale prediction for generation.  This approach allows for a unified framework capable of both comprehension and visual generation, unlike many other methods which treat visual generation and understanding as separate processes.", "section": "2. Related Work"}, {"figure_path": "https://arxiv.org/html/2504.02949/x3.png", "caption": "Figure 3: The illustration of the proposed VARGPT-v1.1 framework similar to VARGPT\u00a0[107], which consists of (1) an LLM (Qwen2-7B-Instruct\u00a0[87, 2]), visual encoder and a understanding projector for visual understanding; (2) a visual decoder and dual generation projectors for visual generation. VARGPT-v1.1 employs causal attention in the LLM backbone while utilizing block causal attention in the visual decoder.", "description": "VARGPT-v1.1 architecture diagram showing its two main components: a visual understanding module and a visual generation module. The visual understanding module consists of a large language model (LLM, specifically Qwen2-7B-Instruct), a visual encoder that processes image inputs, and an understanding projector that maps the visual and textual features into a shared embedding space.  The visual generation module consists of a visual decoder that generates images from the LLM outputs and two generation projectors that map the LLM and visual features to the image generation space. The diagram highlights that VARGPT-v1.1 uses causal attention in the LLM and block causal attention in the visual decoder for improved performance.", "section": "3.1. Model Architecture"}, {"figure_path": "https://arxiv.org/html/2504.02949/x4.png", "caption": "Figure 4: The three training stages of the VARGPT, including stage-1 pretraining, stage-2 visual instruction tuning and stage-3 iterative training.", "description": "This figure illustrates the three-stage training process of the VARGPT model. Stage 1 is pretraining, where the model learns general visual and textual representations. Stage 2 is visual instruction tuning, where the model is fine-tuned using image-text pairs, focusing on improving visual understanding. Stage 3 is iterative training, which involves refining the model's visual generation abilities through multiple rounds of training, gradually increasing the image resolution and combining instruction tuning with reinforcement learning.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2504.02949/x5.png", "caption": "Figure 5: The proposed iterative training strategy for the third stage gradually increases the resolution of the image, while using instruction fine-tuning and reinforcement learning iterative training. Finally, we use the instruction-follow dataset for image editing to stimulate the model\u2019s visual editing ability.", "description": "This figure illustrates the iterative training process used in the third stage of VARGPT-v1.1's training. It shows how the model's image generation resolution is progressively increased from 256x256 to 512x512.  At each resolution, visual instruction fine-tuning (SFT) is performed, followed by reinforcement learning with Direct Preference Optimization (DPO).  This iterative process enhances the model's ability to generate images that accurately reflect user instructions.  The final step involves using an instruction-following dataset for image editing to further improve the model's visual editing capabilities.", "section": "3.2. Reinforcement Learning Human Feedback"}, {"figure_path": "https://arxiv.org/html/2504.02949/x6.png", "caption": "Figure 6: We present the data distribution we construct and collect, encompassing the proportional breakdown of data across the three training stages. Our composite dataset for stage-2 training is derived from LLaVA-1.5, LLaVA-OneVision.", "description": "Figure 6 shows the data distribution used in the three training stages of the VARGPT-v1.1 model.  Stage 1 uses a large dataset of 8.3 million image-text pairs sourced from LAION-COCO and Midjourney, a mix of real and synthetic data. Stage 2 uses a smaller composite dataset of 1.2 million image-text pairs derived from the LLaVA-1.5 and LLaVA-OneVision datasets. Stage 3 utilizes the same large dataset as Stage 1 (8.3 million samples) but also includes 11,000 samples from StyleBooth for image editing instruction fine-tuning. The figure visually represents the proportion of data from each source for each training stage, highlighting the increase in data scale for visual instruction tuning and the incorporation of image editing data in the later stages.", "section": "3.2 Training"}, {"figure_path": "https://arxiv.org/html/2504.02949/x7.png", "caption": "Figure 7: Some generated 512\u00d7\\times\u00d7512 samples by VARGPT-v1.1. Our VARGPT-v1.1 supports text-and-image instructions from user and outputs both text-and-image mixed modal data simultaneously.", "description": "This figure showcases various examples of 512x512 images generated by the VARGPT-v1.1 model.  Each image is accompanied by a short text prompt given to the model as input.  The figure highlights the model's capability to process both text and image-based instructions simultaneously, generating outputs that integrate text and images in a unified manner.  This demonstrates the model's advanced multimodal capabilities, bridging visual understanding and generation.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2504.02949/x8.png", "caption": "Figure 8: Some visual display of image editing results from VARGPT-v1.1, with all images having a resolution of 512\u00d7\\times\u00d7512.", "description": "This figure showcases the image editing capabilities of the VARGPT-v1.1 model.  It presents a series of example images, each originally generated by the model and then modified according to a specified instruction. All edited images share a resolution of 512x512 pixels. The figure demonstrates the model's ability to perform various edits, such as altering artistic style, adding or removing elements, and changing color palettes, all while maintaining image quality and coherence.", "section": "3.2.3 SFT for Visual Editing"}, {"figure_path": "https://arxiv.org/html/2504.02949/x9.png", "caption": "Figure 9: Some generated 512\u00d7\\times\u00d7512 samples by VARGPT-v1.1. Our VARGPT-v1.1 supports text-and-image instructions from user and outputs both text-and-image mixed modal data simultaneously.", "description": "Figure 9 showcases various image samples generated by the VARGPT-v1.1 model, each with a resolution of 512x512 pixels.  These examples demonstrate the model's ability to produce diverse and high-quality images from both text-based and image-based user prompts.  The model's unique capability is in its simultaneous generation of both textual and visual outputs, allowing for multimodal interaction and output.", "section": "4. Image Editing Data"}, {"figure_path": "https://arxiv.org/html/2504.02949/x10.png", "caption": "Figure 10: Some visual display of image editing results from VARGPT-v1.1, with all images having a resolution of 512\u00d7\\times\u00d7512.", "description": "Figure 10 shows examples of image editing capabilities of the VARGPT-v1.1 model.  Each image in the figure is 512x512 pixels.  The examples demonstrate the model's ability to modify images according to text instructions.", "section": "4.3 Image Editing Data"}]