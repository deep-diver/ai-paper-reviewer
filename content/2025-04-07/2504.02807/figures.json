[{"figure_path": "https://arxiv.org/html/2504.02807/x3.png", "caption": "Figure 1: \nThe overview of MegaMath dataset.", "description": "This figure presents a schematic overview of the MegaMath dataset's structure and creation process.  It illustrates the various data sources used, including Common Crawl web data, GitHub code data, and synthetically generated data (QA pairs, code, and interleaved text-code blocks). The figure also highlights the key steps involved in data curation, such as filtering, deduplication, and quality control, ultimately leading to the final MegaMath dataset.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2504.02807/x6.png", "caption": "Figure 2: Comparison with existing open math corpora and MegaMath-Web subsets.", "description": "This figure compares MegaMath-Web with other existing open-source math corpora.  It showcases the relative performance of MegaMath-Web subsets (Full, Top 50%, Top 75%, and Web-Pro) across different sizes in terms of training tokens (in billions) against other datasets such as FineMath-3+, FineMath-4+, InfiMM-WebMath, and OpenWebMath.  The chart illustrates MegaMath-Web-Pro's superior performance, especially considering its relatively smaller size, highlighting the efficacy of the MegaMath data curation and filtering techniques. The chart also visualizes the relationship between data quality and quantity, suggesting that MegaMath achieves a balance of both.", "section": "2 MegaMath Data Curation"}, {"figure_path": "https://arxiv.org/html/2504.02807/x7.png", "caption": "Figure 3: The pipeline for curating MegaMath-Web from Common Crawl data.", "description": "This figure illustrates the multi-stage pipeline used to curate the MegaMath-Web dataset from Common Crawl data.  It begins with URL filtering and language identification to select relevant web pages.  A two-stage text extraction process is then employed, using first a fast and then a more accurate, though slower, method to extract text from HTML, optimizing for mathematical content.  A fastText classifier is used to filter out non-math-related documents. Deduplication is performed to remove redundant entries, followed by further filtering and processing steps to ensure high-quality data.  The pipeline aims to balance the need for efficient processing with the creation of a high-quality dataset.", "section": "2.1 Curating MegaMath-Web"}, {"figure_path": "https://arxiv.org/html/2504.02807/x8.png", "caption": "Figure 4: The pipeline for curating MegaMath-Code.", "description": "The figure illustrates the process of creating the MegaMath-Code dataset.  It begins with the Stack-V2 code corpus. A language filter selects code written in specific programming languages relevant to mathematics and scientific computing. Subsequently, a small language model (SLM) further filters the selected code to isolate high-quality snippets relevant to mathematical reasoning and other math-centric applications. The final output is the MegaMath-Code dataset.", "section": "2.2 Curating MegaMath-Code"}, {"figure_path": "https://arxiv.org/html/2504.02807/x9.png", "caption": "Figure 5: The pipeline for curating synthetic data. Left: QA data generation; Middle: Python code augmentation; Right: text & ode block data curation.", "description": "Figure 5 illustrates the process of creating synthetic data for the MegaMath dataset.  The figure is divided into three main parts. The left-hand side shows how question-answer (QA) pairs are generated and refined from existing web data.  The middle section details how Python code is added to and improved through translation from other programming languages. Finally, the right-hand side depicts how text and code blocks are synthesized and integrated.", "section": "2 MegaMath Data Curation"}]