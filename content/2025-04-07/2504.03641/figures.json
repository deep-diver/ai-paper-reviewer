[{"figure_path": "https://arxiv.org/html/2504.03641/extracted/6335502/MME-U_Figures/Bin2.png", "caption": "(a) MME-U tasks.", "description": "This figure shows a comprehensive visualization of the various tasks included in the MME-Unify benchmark.  The figure is divided into two parts. The first part (a) illustrates the wide range of tasks encompassed by the benchmark, categorized into three main areas: Multimodal Understanding, Multimodal Generation, and the Unified tasks (mix-modality generation tasks), further divided into sub-tasks to comprehensively evaluate the different aspects of model capabilities. The second part (b) displays a leaderboard that presents the performance of 12 U-MLLMs across all of the tasks in MME-Unify, showing their relative strengths and weaknesses.", "section": "2. MME-Unify"}, {"figure_path": "https://arxiv.org/html/2504.03641/extracted/6335502/MME-U_Figures/leadboard.png", "caption": "(b) Leaderboard.", "description": "This leaderboard shows the performance rankings of twelve different unified multimodal large language models (U-MLLMs) on the MME-Unify benchmark.  The models are ranked by their overall MME-U score, a metric that combines performance across multiple tasks assessing multimodal understanding, generation, and unified capabilities. The table shows that Gemini2.0-Flash achieves the highest score.", "section": "2. MME-Unify"}, {"figure_path": "https://arxiv.org/html/2504.03641/extracted/6335502/MME-U_Figures/teaser.png", "caption": "Figure 1: A comprehensive visualization of the diverse tasks in MME-U and the leaderboard. The figure (a) illustrates the wide-ranging nature of the tasks covered in our benchmark, which spans from traditional understanding tasks to complex mixed-modality generation challenges. Additionally, the leaderboard (b) highlights the performance rankings of various U-MLLMs in our benchmark.", "description": "Figure 1 presents a comprehensive overview of the MME-Unify (MME-U) benchmark, showcasing its diverse tasks and the leaderboard of participating models.  Part (a) illustrates the breadth of tasks, ranging from basic single-modal understanding (e.g., image recognition, question answering) to complex mixed-modality generation tasks (e.g., generating auxiliary lines for geometrical reasoning or creating images based on complex instructions). This emphasizes the benchmark's capacity to assess the unified capabilities of multimodal large language models.  Part (b) displays the leaderboard, ranking 12 unified multimodal large language models (U-MLLMs) based on their performance across all MME-U tasks, providing a clear picture of model strengths and weaknesses.", "section": "2. MME-Unify"}, {"figure_path": "https://arxiv.org/html/2504.03641/extracted/6335502/MME-U_Figures/Main.png", "caption": "Figure 2: Complex instruction-based image generation comparison of results from open-source U-MLLMs (DeepSeek-Janus Flow, EMU3), closed-source U-MLLMs (GPT-4o, Gemini-2), and proprietary models (DALLE-3). The closed-source U-MLLMs have demonstrated abilities surpassing proprietary generation models, with a significantly larger gap compared to open-source models.", "description": "Figure 2 showcases a comparison of image generation results across various models, highlighting the capabilities of unified multimodal large language models (U-MLLMs).  It presents examples of complex image generation tasks, such as creating detailed scenes based on extensive instructions.  The models tested include both open-source U-MLLMs (DeepSeek-Janus Flow and EMU3), closed-source U-MLLMs (GPT-4o and Gemini-2), and proprietary models (DALL-E 3).  The results reveal that the closed-source U-MLLMs produce images that are superior to those produced by the proprietary models and show a considerably greater advantage over the images created by open-source models.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2504.03641/extracted/6335502/MME-U_Figures/VCoT_Accuracy/action_accuracy_chart.png", "caption": "Figure 3: Diagram of our MME-Unify. Our benchmark consists of 3 main domains, encompassing 15 subtasks to comprehensively evaluate U-MLLMs\u2019 understanding, generation, and unified capabilities. Specifically, each unify task includes at least one question, an input image, multiple text choices, and image choices. The image choices consist of a correct answer image and a set of manually crafted negative samples. During the evaluation process, we input the image, question, and text options, and the U-MLLMs are required to select the correct text answer and generate an image. The text answer is evaluated by matching it with the correct answer, while the generated image is compared with the constructed image choices. If the CLIP score between the generated image and the correct answer image is the highest, it is considered correct; otherwise, it is deemed incorrect.", "description": "This figure illustrates the MME-Unify benchmark's structure, which evaluates unified multimodal large language models (U-MLLMs).  It's divided into three main sections: Multimodal Understanding, Multimodal Generation, and Unify Capability.  Each section includes multiple subtasks designed to test different aspects of U-MLLM capabilities.  The Unify Capability section is further broken down into five specific tasks that assess how well U-MLLMs can integrate understanding and generation.  For each unify task, the model receives an image, question, and multiple-choice text options. It must select the correct text option and generate a corresponding image. The correctness of the text selection is straightforward.  The generated image is evaluated by comparing its CLIP score to the CLIP score of the correct answer image.  If the generated image has the highest CLIP score, it's deemed correct.", "section": "2. MME-Unify"}, {"figure_path": "https://arxiv.org/html/2504.03641/extracted/6335502/MME-U_Figures/VCoT_Accuracy/coordinate_accuracy.chart.png", "caption": "(a) Action Accuracy.", "description": "The figure shows the accuracy of different models on the action prediction subtask of the Visual CoT task across different steps.  The x-axis represents the step number in the maze navigation task, and the y-axis represents the accuracy.  The lines represent different models, allowing a comparison of their performance on action prediction at each step.", "section": "3.2 Analysis and Findings"}, {"figure_path": "https://arxiv.org/html/2504.03641/extracted/6335502/MME-U_Figures/VCoT_Accuracy/image_accuracy_chart.png", "caption": "(b) Coordinate Accuracy.", "description": "The figure shows the accuracy of coordinate prediction across different steps in the Visual CoT task. The x-axis represents the steps involved in the task, and the y-axis represents the accuracy of coordinate prediction. The accuracy is calculated as the proportion of correctly predicted coordinates for each step across all samples. The figure shows that coordinate accuracy decreases as the number of steps increases.", "section": "3.2 Analysis and Findings"}, {"figure_path": "https://arxiv.org/html/2504.03641/extracted/6335502/MME-U_Figures/t2i/t2i_gt.png", "caption": "(c) Image Accuracy.", "description": "The figure shows the accuracy of image generation across different steps in the Visual CoT task.  The x-axis represents the step number, and the y-axis represents the accuracy in percentage.  Multiple lines are displayed, each showing the accuracy of a different model.  This visualizes how the models' accuracy in generating images changes as the task progresses through multiple steps.", "section": "3.2. Analysis and Findings"}, {"figure_path": "https://arxiv.org/html/2504.03641/extracted/6335502/MME-U_Figures/t2i/t2i_dalle2.png", "caption": "Figure 4: Accuracy distribution across different dimensions on visual cot task. (a) action, (b) location, and (c) image.", "description": "This figure presents a detailed breakdown of accuracy across three key aspects\u2014action, location, and image\u2014within the Visual CoT task.  Each subplot (a, b, c) displays the accuracy for a specific aspect over increasing numbers of steps in the task. This illustrates how model accuracy changes as the complexity (number of steps) of the task increases, shedding light on the model's performance over time in each of the three considered aspects.", "section": "3.2. Analysis and Findings"}, {"figure_path": "https://arxiv.org/html/2504.03641/extracted/6335502/MME-U_Figures/t2i/t2i_dalle3.png", "caption": "(a) Ground Truth.", "description": "This figure shows the ground truth image used for comparison in a image generation task. The image depicts a man standing in a park, wearing a white t-shirt with the number 28 on it, grey shorts, grey socks and black shoes.  In the background there is a \"Run for Rights\" banner, other people in the park, some sitting on benches, and a bicycle leaning against a tree. This image serves as the reference for evaluating the quality and accuracy of image generation models.", "section": "3.1 Results"}, {"figure_path": "https://arxiv.org/html/2504.03641/extracted/6335502/MME-U_Figures/t2i/genemi.png", "caption": "(b) DALLE-2.", "description": "The image shows the results generated by the DALL-E 2 model in response to a complex instruction-based image generation task.  The task involved generating an image based on a detailed description, demonstrating the model's capabilities in image creation and comprehension of natural language instructions. The specific prompt and the model's output can be found in the paper.", "section": "2. MME-Unify"}, {"figure_path": "https://arxiv.org/html/2504.03641/extracted/6335502/MME-U_Figures/t2i/t2i_emu3.png", "caption": "(c) DALLE-3.", "description": "The image shows a result of instruction-based image generation by DALL-E 3, a proprietary model. The image depicts an astronaut standing on an alien planet, gazing at a breathtaking sky filled with twin moons and vibrant auroras.  A mission flag planted beside them reads 'WELCOME TO THE NEW ERA'. This is an example of a complex instruction followed successfully by a large language model (LLM), demonstrating its impressive generative capabilities.  This image is shown as part of a comparison of several different LLMs in terms of image generation, showcasing the varied abilities of different models to handle detailed and complex instructions.", "section": "Complex Instruction-based Image Generation"}, {"figure_path": "https://arxiv.org/html/2504.03641/extracted/6335502/MME-U_Figures/t2i/t2i_gill.png", "caption": "(d) Gemini2.0-flash-exp.", "description": "The image showcases the results produced by the Gemini2.0-flash-exp model for a complex instruction-based image generation task.  The model is tasked with generating an image from a detailed textual description, aiming to assess the model's ability to comprehend and visually represent complex scenes and objects. The image generated by Gemini2.0-flash-exp demonstrates the model's performance in this task, showing its ability to handle intricate details and complex instructions, highlighting its strong generative capabilities. This specific example is part of a larger comparison across multiple models in the paper, illustrating a high level of visual detail and accuracy in the generated output. ", "section": "3. Experiment"}, {"figure_path": "https://arxiv.org/html/2504.03641/extracted/6335502/MME-U_Figures/t2i/t2i_hflow.png", "caption": "(e) EMU3.", "description": "The image shows the results produced by the EMU3 model for a complex instruction-based image generation task.  The model was instructed to generate an image based on a detailed text prompt.  This image is part of a comparative study assessing the image generation capabilities of different multimodal large language models (MLLMs). The image's quality, detail, and faithfulness to the prompt description can be analyzed to evaluate the EMU3 model's performance.", "section": "Figure 2. Complex instruction-based image generation comparison"}, {"figure_path": "https://arxiv.org/html/2504.03641/extracted/6335502/MME-U_Figures/t2i/t2i_janus_pro.png", "caption": "(f) GILL.", "description": "The figure shows the results of GILL model in the fine-grained image reconstruction task. The input is a degraded image, and the model is tasked to reconstruct the image with high-fidelity, maintaining the details, textures, edges, and structural integrity.", "section": "3.1 Results"}, {"figure_path": "https://arxiv.org/html/2504.03641/extracted/6335502/MME-U_Figures/t2i/t2i_minigpt5.png", "caption": "(g) HermersFlow.", "description": "The figure shows the result of the Text-to-Image Generation task performed by HermersFlow model. The prompt was: \u201cA man is standing in a park with a 'Run for Rights' banner in the background. He is wearing a white t-shirt with the number 28 on it, grey shorts, and grey socks with black shoes. The park is filled with people, some sitting on benches, and there is a bicycle leaning against a tree.\u201d The image generated by HermersFlow shows a man in a park with a banner, but the details are not as accurate as those generated by other models. For instance, the number 28 on the t-shirt is not clearly visible, and the overall quality of the image is relatively low compared to the other models.", "section": "3.1 Results"}, {"figure_path": "https://arxiv.org/html/2504.03641/extracted/6335502/MME-U_Figures/t2i/t2i_mio.png", "caption": "(h) Janus-Pro.", "description": "The figure shows the results from Janus-Pro model in the text-to-image generation task. The model was given the following prompt: \u201cA man is standing in a park with a 'Run for Rights' banner in the background. He is wearing a white t-shirt with the number 28 on it, grey shorts, and grey socks with black shoes. The park is filled with people, some sitting on benches, and there is a bicycle leaning against a tree.\u201d The generated image attempts to capture these details, showing a man in a park, but some of the specifics, like the number 28 on the shirt and the bicycle, may not be as clear or accurate as in other models.", "section": "3. Results"}, {"figure_path": "https://arxiv.org/html/2504.03641/extracted/6335502/MME-U_Figures/t2i/t2i_showo.png", "caption": "(i) MiniGPT-5.", "description": "The figure shows the result of the text-to-image generation task performed by MiniGPT-5. The prompt was: \u201cA man is standing in a park with a 'Run for Rights' banner in the background. He is wearing a white t-shirt with the number 28 on it, grey shorts, and grey socks with black shoes. The park is filled with people, some sitting on benches, and there is a bicycle leaning against a tree.\u201d  The image generated by MiniGPT-5 shows a park scene with people, a bicycle, and a banner, but the details like the man's clothing and the number on his shirt are not accurately depicted, indicating room for improvement in the model\u2019s image generation accuracy.", "section": "3.1 Results"}, {"figure_path": "https://arxiv.org/html/2504.03641/extracted/6335502/MME-U_Figures/t2i/t2i_vilau.png", "caption": "(j) MIO-Instruct.", "description": "The image shows the result of a text-to-image generation task performed by the MIO-Instruct model. The prompt was: \u201cA man is standing in a park with a 'Run for Rights' banner in the background. He is wearing a white t-shirt with the number 28 on it, grey shorts, and grey socks with black shoes. The park is filled with people, some sitting on benches, and there is a bicycle leaning against a tree.\u201d The image generated by the MIO-Instruct model depicts a realistic scene consistent with the prompt.  The level of detail, the overall composition, and the degree of photorealism can be compared to the results produced by other models (shown in the same figure) to assess the relative strengths and weaknesses of MIO-Instruct in terms of image generation capabilities.", "section": "3.1 Results"}, {"figure_path": "https://arxiv.org/html/2504.03641/extracted/6335502/MME-U_Figures/Visualization.png", "caption": "(k) Show-O.", "description": "The figure shows the results from the Show-O model in the text-to-image generation task.  The model was given the prompt: \u201cA man is standing in a park with a 'Run for Rights' banner in the background. He is wearing a white t-shirt with the number 28 on it, grey shorts, and grey socks with black shoes. The park is filled with people, some sitting on benches, and there is a bicycle leaning against a tree.\u201d  The image generated by Show-O is compared to the ground truth and the images generated by other models to illustrate the model\u2019s ability to generate realistic and detailed images based on textual descriptions. The image shows a group of people in a park, although the visual quality and attention to detail may differ compared to the ground truth and other models' results.", "section": "3.1. Results"}, {"figure_path": "https://arxiv.org/html/2504.03641/extracted/6335502/MME-U_Figures/Generation_Visualization.png", "caption": "(l) Vila-u", "description": "The figure shows the results from the VILA-U model in the text-to-image generation task, where the model is asked to generate an image from a textual description. The description is: \u201cA man is standing in a park with a 'Run for Rights' banner in the background. He is wearing a white t-shirt with the number 28 on it, grey shorts, and grey socks with black shoes. The park is filled with people, some sitting on benches, and there is a bicycle leaning against a tree.\u201d The generated image by VILA-U appears to show a man in a park with a banner, though certain details are not entirely accurate compared to the description. It shows a lack of attention to detail compared to other models. This suggests that VILA-U might need improvements in generating images consistent with the input text descriptions.", "section": "3.1 Results"}, {"figure_path": "https://arxiv.org/html/2504.03641/extracted/6335502/MME-U_Figures/subtask_disturbution.png", "caption": "Figure 5: The generated results from various models in the text-to-image generation task, based on the following text prompt: A man is standing in a park with a \u2019Run for Rights\u2019 banner in the background. He is wearing a white t-shirt with the number 28 on it, grey shorts, and grey socks with black shoes. The park is filled with people, some sitting on benches, and there is a bicycle leaning against a tree.", "description": "Figure 5 presents a comparison of image generation results from different models given the same text prompt:  A man is in a park with a 'Run for Rights' banner. He's wearing a white shirt with '28' on it, grey shorts, grey socks, and black shoes.  Other park details include people sitting on benches, and a bicycle leaning against a tree. The figure showcases how various models interpret and render this detailed description, highlighting differences in image quality, detail accuracy, and overall fidelity to the prompt.", "section": "3. Experiment"}, {"figure_path": "https://arxiv.org/html/2504.03641/extracted/6335502/MME-U_Figures/step_distribution_chart.png", "caption": "Figure 6: Data samples from understanding task, which includes single-image perception and reasoning, multi-image and image-text interlaced perception and reasoning, video perception and reasoning, etc.", "description": "Figure 6 presents example questions from the Multimodal Understanding section of the MME-Unify benchmark.  It showcases the diversity of tasks included in the benchmark, spanning single-image perception and reasoning, multi-image and interleaved image-text understanding, and video perception and reasoning.  Each example visually illustrates the type of input provided to the model (image or video) and the types of questions asked of the model, demonstrating the complexity of understanding implicit and explicit relationships within different modalities.", "section": "2.1. Multi-Modal Understanding"}, {"figure_path": "https://arxiv.org/html/2504.03641/extracted/6335502/MME-U_Figures/Construct_Process.png", "caption": "Figure 7: Data samples from generation task. It includes subtasks such as Text-to-Image Generation, Text-to-Image Editing, Fine-Grained Image Reconstruction, Text-to-Video Generation, conditional Image-to-Video Generation, and Video Prediction.", "description": "Figure 7 presents example outputs from various multimodal generation tasks included in the MME-Unify benchmark.  It showcases the benchmark's capacity to evaluate different types of image and video generation tasks.  Specifically, examples are shown for: generating an image from text, editing an existing image based on text instructions, reconstructing a high-resolution image from a low-resolution version, generating a video from an image and text, generating a video from only text, and predicting future frames in a video sequence.", "section": "2. MME-Unify"}, {"figure_path": "https://arxiv.org/html/2504.03641/extracted/6335502/MME-U_Figures/System_Prompt_for_Unify/System_Prompt_1.png", "caption": "Figure 8: An overview of real-life scenarios included in the Understanding Task. The scores in the bars represent the proportion of the number of samples of the corresponding scenario to the total number of samples of the task.", "description": "Figure 8 presents a comprehensive breakdown of real-world scenarios included within the Understanding Task of the MME-Unify benchmark.  It visually represents the proportion of the total dataset dedicated to each scenario.  The chart showcases the diverse range of tasks, such as object recognition, scene understanding, and action recognition, that are essential for robust multimodal understanding.  The relative sizes of the sections in the chart directly reflect the weighting of these scenarios in the overall dataset used for the Understanding Task.", "section": "2.1. Multi-Modal Understanding"}, {"figure_path": "https://arxiv.org/html/2504.03641/extracted/6335502/MME-U_Figures/System_Prompt_for_Unify/System_Prompt_2.png", "caption": "Figure 9: Distribution of steps required for samples of mazes of different sizes in the Visual CoT task.", "description": "This figure shows the distribution of the number of steps needed to solve mazes of varying sizes (3x3, 4x4, and 5x5) in the Visual Chain of Thought (Visual CoT) task.  The x-axis represents the maze size, and the y-axis represents the number of steps. Each bar represents the number of mazes requiring a certain number of steps to be solved. The data shows that larger mazes generally require more steps to solve.", "section": "3. Experiment"}, {"figure_path": "https://arxiv.org/html/2504.03641/extracted/6335502/MME-U_Figures/System_Prompt_for_Unify/System_Prompt_3.png", "caption": "Figure 10: The overall construction process for five unified tasks, which consists of (a) Common Sense Question Answering, (b) Image Editing and Explaining, (c)SpotDiff, (d) Auxiliary Lines, and (e) Visual CoT.", "description": "Figure 10 illustrates the detailed construction process for the five unified tasks within the MME-Unify benchmark.  Each subtask's construction process is broken down step-by-step, showing how inputs (questions, images, etc.), processing steps, and outputs (answers, edited images, etc.) are handled.  Specifically, it displays examples of:\n(a) Common Sense Question Answering:  How questions are framed and images are generated for common sense reasoning.\n(b) Image Editing and Explaining:  The steps involved in understanding an edit instruction, modifying an image, and then explaining the changes.\n(c) SpotDiff:  The process of identifying and extracting differences between two similar images.\n(d) Auxiliary Lines:  Solving geometry problems using auxiliary lines, including the creation of the line and problem solving steps.\n(e) Visual CoT (Chain of Thought):  The step-by-step process involved in solving a maze, including decision-making at each step.", "section": "2. MME-Unify"}, {"figure_path": "https://arxiv.org/html/2504.03641/extracted/6335502/MME-U_Figures/System_Prompt_for_Unify/System_Prompt_4.png", "caption": "Figure 11: System prompt for Common Sense Question Answering task.", "description": "This figure shows the system prompt used for the Common Sense Question Answering task within the MME-Unify benchmark.  The prompt instructs the AI model to answer a common sense question by selecting the correct answer from multiple choices and then generating an image that visually represents that answer.  The prompt details the input data (question and choices), output requirements (answer selection and image generation), processing steps (understanding, evaluating, selecting, generating, verifying), and provides an example question, choices, and expected output.  The example clearly shows the desired format for both the text-based answer and the generated image.", "section": "2.3 Unify Capability"}, {"figure_path": "https://arxiv.org/html/2504.03641/extracted/6335502/MME-U_Figures/System_Prompt_for_Unify/System_Prompt_5.png", "caption": "Figure 12: Systemp prompt for Image Editing and Explaining task", "description": "This figure shows the system prompt used for the Image Editing and Explaining task within the MME-Unify benchmark.  The prompt instructs the AI model to act as an image editing assistant, modifying a provided image based on a given natural language instruction. The model must then generate a visual description explaining how the modification integrates into the scene and produce the edited image itself.  The prompt details specific requirements for the output, including clarity, realism, coherence, and preservation of the original image's visual qualities. An example question and desired output are also provided.", "section": "2. MME-Unify"}, {"figure_path": "https://arxiv.org/html/2504.03641/extracted/6335502/MME-U_Figures/System_Prompt_for_Unify/System_Prompt_6.png", "caption": "Figure 13: System prompt for SpotDiff task.", "description": "This figure presents the system prompt used for the SpotDiff task within the MME-Unify benchmark.  The prompt instructs the AI model to identify the differences between two similar images. It requires the model to: 1) Compare the two input images and determine the total number of differences. 2) Choose the correct answer based on the multiple-choice options provided. 3) Extract and display the differing image regions on a uniform white background, preserving the original details of these regions. The prompt also specifies the input and output formats, including an example of each, and outlines the necessary processing steps. The prompt highlights several key requirements, such as accurately selecting the correct answer, ensuring the extracted regions are clearly displayed on a clean white background, and preserving the original structural information within those extracted regions.", "section": "2. MME-Unify"}, {"figure_path": "https://arxiv.org/html/2504.03641/extracted/6335502/MME-U_Figures/vcot/vcot_anole7b.png", "caption": "Figure 14: System prompt for Auxiliary Lines task.", "description": "This figure shows the system prompt used for the Auxiliary Lines task within the MME-Unify benchmark.  The prompt instructs the AI model to solve geometry problems by analyzing a given question, image (geometric diagram), and multiple-choice answers. The model should draw auxiliary lines (like perpendiculars, bisectors, or diagonals) onto the diagram to aid in problem-solving, maintaining the diagram's original structure. The model then identifies the correct answer based on problem conditions and outputs both the selected answer and the modified diagram with the added auxiliary lines. An example question and its associated diagram are included to demonstrate the required input and output format. The prompt emphasizes using geometric reasoning and ensuring line clarity and logical placement within the constraints of the problem.", "section": "2. MME-Unify"}, {"figure_path": "https://arxiv.org/html/2504.03641/extracted/6335502/MME-U_Figures/vcot/vcot_gill.png", "caption": "Figure 15: Systemp prompt for Visual CoT task in the first step.", "description": "This figure shows the system prompt used for the first step of the Visual CoT task in the MME-Unify benchmark.  The Visual CoT task involves a grid-based puzzle game where the agent must navigate to a goal while avoiding holes, following specific movement rules, and providing the action, location, and updated grid map image at each step.  The prompt details the game rules, output format, and provides an initial game map.", "section": "2.3 Unify Capability"}, {"figure_path": "https://arxiv.org/html/2504.03641/extracted/6335502/MME-U_Figures/vcot/vcot_seed.png", "caption": "Figure 16: Systemp prompt for Visual CoT task after first step.", "description": "This figure shows the system prompt used for the Visual CoT task after the first step has been completed.  It provides a detailed description of the game's setup, rules, and the expected format of the model's response. The prompt includes information on the game's objective (reach the target while avoiding holes), the grid map (3x3), the movement rules, the expected output (action, location, and image), and includes the initial grid map shown to the model. Importantly, it also incorporates the history information from the preceding step in the sequence, prompting the model to consider this prior information when determining the next move. The inclusion of history ensures that the model is making decisions in a sequential, step-by-step manner.", "section": "2. MME-Unify"}, {"figure_path": "https://arxiv.org/html/2504.03641/extracted/6335502/MME-U_Figures/vcot/vcot_4.png", "caption": "(a) Anole.", "description": "This figure shows the intermediate process images generated by Anole, one of the multimodal large language models (MLLMs), during the Visual Composition Task (VCoT).  Specifically, it displays the model's intermediate outputs for the task, which requires the model to guide a character through a maze, one square at a time, generating images and actions at each step. The visualization demonstrates Anole's approach to problem-solving and step-by-step reasoning in the task.", "section": "3.2 Analysis and Findings"}, {"figure_path": "https://arxiv.org/html/2504.03641/extracted/6335502/MME-U_Figures/vcot/vcot_gemini.png", "caption": "(b) GILL.", "description": "This figure shows the intermediate results from the GILL model during the Visual CoT task.  It is one of several examples illustrating how different models approach this task's multi-step image generation and decision-making process.  The image displays the model's intermediate outputs for each step in navigating a maze, showing how GILL's visual generation and reasoning processes progress through the task.", "section": "3.2 Analysis and Findings"}, {"figure_path": "https://arxiv.org/html/2504.03641/extracted/6335502/MME-U_Figures/Fine_Grained_Examples/Src.png", "caption": "(c) SEED.", "description": "Figure 17 presents intermediate process images generated by different models during the Visual CoT task.  The subfigure (c) specifically showcases SEED's approach to visual composition.  It highlights the unique visual patterns and composition strategies employed by SEED as it works through the multi-step problem-solving process within the Visual CoT task. The image is a part of a sequence showing the process of generating the final answer. Comparing this image to the other models' progress would highlight differences in problem-solving approaches and visual output.", "section": "3.2 Analysis and Findings"}, {"figure_path": "https://arxiv.org/html/2504.03641/extracted/6335502/MME-U_Figures/Fine_Grained_Examples/OmniGen.png", "caption": "(d) MiniGPT-5.", "description": "Figure 18(d) presents the results produced by the MiniGPT-5 model for the fine-grained image reconstruction task.  The model attempts to reconstruct a high-fidelity image from a degraded input, aiming to preserve detailed features, textures, and structural integrity while maintaining perceptual realism.  The generated image showcases the model's ability (or lack thereof) to effectively reconstruct the original image from the degraded input, highlighting its strengths and weaknesses in capturing fine-grained details, preserving textural elements, and maintaining overall visual coherence.", "section": "3.1 Results"}, {"figure_path": "https://arxiv.org/html/2504.03641/extracted/6335502/MME-U_Figures/Fine_Grained_Examples/GILL.png", "caption": "(e) Gemini2.0-flash-exp.", "description": "The figure shows the intermediate process images generated by Gemini2.0-flash-exp model in the Visual CoT task.  The model demonstrates its approach to processing and generating visual content during the maze navigation task, illustrating a step in its decision-making process. This specific image displays one step within the multi-step maze solving process.", "section": "3.2 Analysis and Findings"}, {"figure_path": "https://arxiv.org/html/2504.03641/extracted/6335502/MME-U_Figures/Fine_Grained_Examples/MiniGPT5.png", "caption": "Figure 17: Intermediate process images generated by different models in VCoT. The figure illustrates the intermediate outputs of various models in the VCoT (Visual Composition Task), showing distinct approaches in processing and generating visual content. The models shown include (a) Anole, (b) GILL, (c) SEED, (d) MiniGPT-5, and (e) Gemini-2.0-flash-exp, each producing unique visual patterns and compositions.", "description": "Figure 17 showcases intermediate steps of visual reasoning in the Visual Composition Task (VCoT).  Five different unified multimodal large language models (U-MLLMs) are compared: Anole, GILL, SEED-LLaMA, MiniGPT-5, and Gemini 2.0-flash-exp.  Each model's unique approach to problem-solving is visually represented through a series of intermediate images generated during the task. The figure highlights the diversity in strategies used by these models to integrate visual information and generate the final output.", "section": "3.2 Analysis and Findings"}, {"figure_path": "https://arxiv.org/html/2504.03641/extracted/6335502/MME-U_Figures/Fine_Grained_Examples/Anole.png", "caption": "(a) Source Image.", "description": "This figure shows the original image used as input for a fine-grained image reconstruction task. The goal of this task is to evaluate the model's ability to reconstruct high-fidelity images from degraded inputs, preserving details such as textures and structural integrity.", "section": "2.2. Multi-Modal Generation"}, {"figure_path": "https://arxiv.org/html/2504.03641/extracted/6335502/MME-U_Figures/Fine_Grained_Examples/SEED-LLaMA.png", "caption": "(b) OmniGen.", "description": "The image is a result from OmniGen model in the text-to-image generation task.  The prompt is: A man is standing in a park with a 'Run for Rights' banner in the background. He is wearing a white t-shirt with the number 28 on it, grey shorts, and grey socks with black shoes. The park is filled with people, some sitting on benches, and there is a bicycle leaning against a tree.", "section": "3.1 Results"}, {"figure_path": "https://arxiv.org/html/2504.03641/extracted/6335502/MME-U_Figures/Fine_Grained_Examples/MIO-7B.png", "caption": "(c) GILL.", "description": "The figure shows the intermediate results produced by the GILL model during the Visual CoT task.  The Visual CoT task involves navigating a maze and generating corresponding images at each step. This image displays the intermediate state created by the GILL model during that process.", "section": "3.2 Analysis and Findings"}, {"figure_path": "https://arxiv.org/html/2504.03641/extracted/6335502/MME-U_Figures/Fine_Grained_Examples/Gemini.png", "caption": "(d) MiniGPT-5.", "description": "Figure 18(d) presents the results of MiniGPT-5 model's performance on the fine-grained image reconstruction task. The model is given a degraded input image and asked to reconstruct it with high fidelity, preserving fine-grained details, textures, and structural integrity while maintaining visual coherence and natural realism.  The reconstructed image generated by MiniGPT-5 shows a significant deviation from the original image, indicating the model struggles to effectively reconstruct fine details and maintain visual fidelity.", "section": "3.1 Results"}, {"figure_path": "https://arxiv.org/html/2504.03641/extracted/6335502/MME-U_Figures/Text-Guided_Image_Editing/Src.png", "caption": "(e) Anole.", "description": "This figure shows the intermediate process images generated by the Anole model in the Visual CoT task. The Visual CoT (Visual Composition Task) evaluates the model's ability to perform step-by-step decision-making in a maze navigation task. The figure illustrates the intermediate outputs of the Anole model in this process, showing a distinct approach in processing and generating visual content. Each image in the sequence represents a step in the maze-solving process.", "section": "3.2 Analysis and Findings"}, {"figure_path": "https://arxiv.org/html/2504.03641/extracted/6335502/MME-U_Figures/Text-Guided_Image_Editing/GT.png", "caption": "(f) SEED-LLaMA.", "description": "The figure shows the results from SEED-LLaMA model in the fine-grained image reconstruction task.  The model is given a degraded input image and asked to reconstruct it with high fidelity, preserving fine details, textures, and structural integrity. The reconstructed image demonstrates the model's capability to recover details and maintain overall visual coherence.", "section": "3.1 Results"}, {"figure_path": "https://arxiv.org/html/2504.03641/extracted/6335502/MME-U_Figures/Text-Guided_Image_Editing/GILL.png", "caption": "(g) MIO-Instruct.", "description": "The image showcases the result of the image generation task performed by the MIO-Instruct model.  The input prompt describes a man in a park, wearing specific clothing and near a banner, with other details of the park and surrounding people. The generated image aims to visualize this scene.", "section": "3.1 Results"}, {"figure_path": "https://arxiv.org/html/2504.03641/extracted/6335502/MME-U_Figures/Text-Guided_Image_Editing/MiniGPT5.png", "caption": "(h) Gemini2.0-flash-exp.", "description": "The image showcases the results produced by the Gemini2.0-flash-exp model for the text-to-image generation task. The model was provided with a detailed prompt describing a man standing in a park with a specific banner, clothing, and surroundings. The generated image displays the model's ability to interpret and create a visual representation based on the given textual input, showcasing its capabilities in multimodal generation.", "section": "3.1 Results"}, {"figure_path": "https://arxiv.org/html/2504.03641/extracted/6335502/MME-U_Figures/Text-Guided_Image_Editing/Anole.png", "caption": "Figure 18: The generated results from various models in the fine-grained image reconstruction task, based on the following text prompt: Reconstruct high-fidelity images from degraded inputs, preserving fine-grained details, textures, and structural integrity with perceptual realism.", "description": "Figure 18 showcases the outcomes of several models tasked with reconstructing images from degraded sources. The models were instructed to maintain fine-grained details, textures, and structural integrity while achieving perceptual realism.  The figure visually compares the original image with the reconstructions generated by each model, offering a side-by-side comparison to illustrate the effectiveness of different approaches in recovering image fidelity from damaged or low-quality input.", "section": "Multi-Modal Generation"}, {"figure_path": "https://arxiv.org/html/2504.03641/extracted/6335502/MME-U_Figures/Text-Guided_Image_Editing/SEED-LLaMA.png", "caption": "(a) Source Image.", "description": "The figure shows a source image used as input for a fine-grained image reconstruction task.  This task involves reconstructing a high-fidelity version of a degraded image, preserving fine details, textures, and structure.", "section": "Multi-Modal Generation"}, {"figure_path": "https://arxiv.org/html/2504.03641/extracted/6335502/MME-U_Figures/Text-Guided_Image_Editing/MIO.png", "caption": "(b) Ground Truth.", "description": "This image shows the ground truth image for a visual question answering task, specifically the \"Common Sense Question Answering\" task.  The ground truth depicts the Pyramids of Giza, serving as the correct visual representation of the answer to a question about ancient monumental tombs located in Egypt near Cairo.  The image is a high-quality, visually clear photograph of the Pyramids.", "section": "3.2 Analysis and Findings"}, {"figure_path": "https://arxiv.org/html/2504.03641/extracted/6335502/MME-U_Figures/Text-Guided_Image_Editing/Gemini.png", "caption": "(c) GILL.", "description": "The figure shows the intermediate image generated by the GILL model during the Visual CoT task.  The Visual CoT task involves guiding a character through a maze, requiring a model to generate images at each step to visualize its progress. This particular image shows the intermediate state of the maze-solving process generated by the GILL model, showcasing its approach to the task.", "section": "3.2 Analysis and Findings"}, {"figure_path": "https://arxiv.org/html/2504.03641/extracted/6335502/MME-U_Figures/Conditional_Image_to_Video_Generation/Src.png", "caption": "(d) MiniGPT-5.", "description": "The figure shows the intermediate process images generated by MiniGPT-5 in the Visual CoT task.  Visual CoT, or Visual Chain of Thought, is a task designed to evaluate a model's ability to perform step-by-step reasoning and generate corresponding multimodal outputs at each step.  This particular image displays the model's intermediate output during a maze navigation task.  The image demonstrates MiniGPT-5's attempt at visualizing the next state of the maze after considering a specific move, which is part of the multi-step reasoning process. The image helps illustrate the model's progress through the maze and how it visualizes its plan.", "section": "3.2 Analysis and Findings"}, {"figure_path": "https://arxiv.org/html/2504.03641/extracted/6335502/MME-U_Figures/Conditional_Image_to_Video_Generation/CogVideo_Frames.png", "caption": "(e) Anole.", "description": "This figure shows the intermediate process images generated by the Anole model during the Visual CoT (Visual Composition Task). It showcases the model's approach to visual reasoning and generation by displaying intermediate steps in the process.  The images illustrate the model's progress towards a final solution, highlighting the model's ability to synthesize visual information and develop a coherent overall composition.", "section": "3.2 Analysis and Findings"}, {"figure_path": "https://arxiv.org/html/2504.03641/extracted/6335502/MME-U_Figures/Conditional_Image_to_Video_Generation/MIO-Frames.png", "caption": "(f) SEED-LLaMA.", "description": "The figure shows the results from the SEED-LLaMA model in the fine-grained image reconstruction task.  The model is given a degraded image as input and tasked with reconstructing it to high fidelity, preserving fine-grained details and textures.  The reconstruction attempts to maintain visual coherence and natural realism.", "section": "3.1 Results"}, {"figure_path": "https://arxiv.org/html/2504.03641/extracted/6335502/MME-U_Figures/Text-to-Video_Generation/CogVideoX.png", "caption": "(g) MIO-Instruct.", "description": "The figure shows the results from the MIO-Instruct model in the text-to-image generation task. The model is given the following text prompt:\n\"A man is standing in a park with a 'Run for Rights' banner in the background. He is wearing a white t-shirt with the number 28 on it, grey shorts, and grey socks with black shoes. The park is filled with people, some sitting on benches, and there is a bicycle leaning against a tree.\"\nThe image generated by MIO-Instruct shows a realistic scene of people at a park, but the specifics of the prompt are not entirely met. For instance, the number 28 on the shirt and details of the clothes are not as clearly visible as one might expect from a model with accurate image generation capabilities.", "section": "3.1 Results"}, {"figure_path": "https://arxiv.org/html/2504.03641/extracted/6335502/MME-U_Figures/Text-to-Video_Generation/MIO-Instruct.png", "caption": "(h) Gemini2.0-flash-exp.", "description": "The image showcases the output generated by the Gemini2.0-flash-exp model for the text-to-image generation task.  The model was given the following prompt: \u201cA man is standing in a park with a 'Run for Rights' banner in the background. He is wearing a white t-shirt with the number 28 on it, grey shorts, and grey socks with black shoes. The park is filled with people, some sitting on benches, and there is a bicycle leaning against a tree.\u201d The image displays the model's interpretation and generation of this scene.", "section": "3.1 Results"}, {"figure_path": "https://arxiv.org/html/2504.03641/extracted/6335502/MME-U_Figures/Unify_Output/Unify_Output_2.png", "caption": "Figure 19: The generated results from various models in the text-guided image editing task, based on the following text prompt: Change this image into a watercolor art.", "description": "This figure showcases the outputs of several multi-modal large language models (U-MLLMs) when tasked with transforming an image into a watercolor painting style.  The original image depicts several boxes of a food product. The results highlight the varying capabilities of the different models in terms of style transfer, detail preservation, and overall image quality. Some models successfully capture the essence of a watercolor painting, while others produce results that are either stylistically inconsistent or lack the fine details of the source image.", "section": "3.1 Results"}, {"figure_path": "https://arxiv.org/html/2504.03641/extracted/6335502/MME-U_Figures/Unify_Output/Unify_Output_3.png", "caption": "(a) Source Image.", "description": "This figure shows the original image used as input for the fine-grained image reconstruction task.  The task challenges models to reconstruct a high-fidelity version of the image while maintaining fine-grained details such as textures, edges, and subtle features.", "section": "2.2. Multi-Modal Generation"}, {"figure_path": "https://arxiv.org/html/2504.03641/extracted/6335502/MME-U_Figures/Unify_Output/Unify_Output_1.png", "caption": "(b) CogVideoX.", "description": "The figure shows a video generated by the CogVideoX model for the conditional image-to-video generation task.  The input was an image of a man looking tired, and the text prompt was \u201cThe man is so tired. -camera zoom in\u201d. The generated video shows a close-up sequence focusing on the man's face, highlighting the tired expression.", "section": "3.1 Results"}, {"figure_path": "https://arxiv.org/html/2504.03641/extracted/6335502/MME-U_Figures/Unify_Output/Unify_Output_4.png", "caption": "(c) MIO-Instruct.", "description": "The image shows the results from MIO-Instruct model in the text-to-image generation task, where the model was given the following prompt: \u201cA man is standing in a park with a 'Run for Rights' banner in the background. He is wearing a white t-shirt with the number 28 on it, grey shorts, and grey socks with black shoes. The park is filled with people, some sitting on benches, and there is a bicycle leaning against a tree.\u201d The generated image from MIO-Instruct accurately captures the main elements of the scene described in the prompt including the man, the banner, and some people in the park. However, certain details might be missing or not as accurate as compared to other models, like the specific number 28 on the shirt or the bicycle against a tree.", "section": "3.1. Results"}]