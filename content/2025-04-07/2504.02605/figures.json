[{"figure_path": "https://arxiv.org/html/2504.02605/x1.png", "caption": "Figure 1: Resolved rate (%) on Multi-SWE-bench (Claude-3.5-Sonnet).", "description": "The figure displays the success rate of the Claude-3.5-Sonnet model in resolving issues across various programming languages included in the Multi-SWE-bench benchmark.  The results are presented in two formats: a radar chart showing the overall resolved rate for each language and a bar chart illustrating resolved rates across different difficulty levels (easy, medium, hard). The radar chart helps visualize the model's performance across languages, while the bar chart allows for a more detailed analysis of its capability relative to the complexity of the issues.", "section": "Experimental Results"}, {"figure_path": "https://arxiv.org/html/2504.02605/x2.png", "caption": "Figure 2: Construction of Multi-SWE-bench.", "description": "This figure details the five phases involved in constructing the Multi-SWE-bench dataset. Phase 1 focuses on selecting high-quality, well-maintained GitHub repositories across multiple programming languages.  Phase 2 crawls and filters pull requests (PRs) related to issues, ensuring they have proper tests and are merged. Phase 3 builds Dockerized development environments for each PR, accounting for its dependencies.  Phase 4 filters PRs based on test results, focusing on those clearly fixing bugs. Finally, Phase 5 involves manual verification of the remaining PRs by expert annotators, ensuring high-quality data for the benchmark.", "section": "3.1 Benchmark Construction"}, {"figure_path": "https://arxiv.org/html/2504.02605/x3.png", "caption": "Figure 3: Distribution of estimated time consumption of issues in Multi-SWE-bench.", "description": "This bar chart visualizes the distribution of estimated time needed to resolve issues within the Multi-SWE-bench dataset.  The x-axis categorizes issues by programming language (Java, TypeScript, JavaScript, Go, Rust, C, C++), while the y-axis represents the number of issues. Each language is further broken down into four time categories, representing the estimated time an expert would need to resolve each issue: \u226415 minutes, 15 minutes - 1 hour, 1 hour - 4 hours, and \u22654 hours. This allows for a comparison of issue resolution time across different programming languages within the benchmark.", "section": "3. Multi-SWE-bench"}, {"figure_path": "https://arxiv.org/html/2504.02605/x4.png", "caption": "Figure 4: Issue flow from locating to resolving.", "description": "This figure visualizes the process of resolving software issues using various LLMs and methods. It breaks down the issue resolution process into three stages: issue location, successful resolution, and unsuccessful attempts. For each method (MagentLess, MSWE-agent, and MopenHands) and each LLM, the figure shows the proportion of issues that were successfully located, successfully resolved, unresolved, or not even located in several programming languages such as Python, Java, TypeScript, JavaScript, Go, Rust, C, and C++. This provides a visual representation of the performance of different LLMs and methods on various programming languages.", "section": "Experimental Results"}, {"figure_path": "https://arxiv.org/html/2504.02605/x5.png", "caption": "Figure 5: Number of turns required across different programming languages.", "description": "This figure presents box plots illustrating the number of turns required by two different agent-based methods (MSWE-agent and MopenHands) to successfully resolve issues across various programming languages.  Each box plot represents a programming language, and the distribution of turns is shown for each method across different LLMs.  The plots allow for a visual comparison of the efficiency and consistency of the two methods across different languages and LLMs.  Outliers are also shown to highlight cases where the number of turns deviated significantly from the median.", "section": "6.1.2 Performance across Various Methods and LLMs"}, {"figure_path": "https://arxiv.org/html/2504.02605/x6.png", "caption": "Figure 6: Relationship between resolved rate and the number of stars and forks of a repository.", "description": "This figure displays the correlation between the success rate of resolving issues in a code repository and the repository's popularity metrics (number of stars and forks).  Each point represents a repository, with its x-coordinate indicating the number of stars and its y-coordinate showing the number of forks. The color intensity of each point represents the average resolution rate achieved for issues within that repository across various large language models and resolving methods. This visualization helps in understanding how repository popularity may influence the effectiveness of large language models in resolving issues.", "section": "6.1.3 Performance across Different Repositories"}, {"figure_path": "https://arxiv.org/html/2504.02605/x7.png", "caption": "Figure 7: Relationship between resolved rate and the number of issues and PRs of a repository.", "description": "This figure examines the correlation between the success rate of resolving issues in software repositories and the number of issues and pull requests (PRs) within those repositories.  It visualizes how the activity and community engagement, represented by the count of issues and PRs, potentially influences the effectiveness of Large Language Models (LLMs) in resolving issues. Higher numbers of issues and PRs might indicate greater community involvement and potentially more mature codebases, leading to higher success rates.", "section": "6.1.3. Performance across Different Repositories"}, {"figure_path": "https://arxiv.org/html/2504.02605/x8.png", "caption": "Figure 8: Relation between resolved rate and the repository complexity on Multi-SWE-bench.", "description": "This figure visualizes the correlation between the success rate of resolving issues and the complexity of the software repositories in the Multi-SWE-bench.  It shows how different aspects of repository complexity, such as the number of lines of code, the number of files, and language entropy, impact the ability of large language models (LLMs) to effectively resolve issues.  The charts illustrate the performance across different programming languages, highlighting the varying degrees of complexity and the corresponding impact on issue resolution.", "section": "6.1. Performance on Multi-SWE-bench"}, {"figure_path": "https://arxiv.org/html/2504.02605/x9.png", "caption": "Figure 9: Histogram of issue description length (#tokens).", "description": "This histogram shows the distribution of issue description lengths (measured in the number of tokens) within the Multi-SWE-bench dataset.  The x-axis represents the number of tokens, and the y-axis represents the frequency of issues with that token length. The distribution generally follows a power law, meaning that most issue descriptions have a relatively small number of tokens while fewer descriptions have a much larger number of tokens. This visualization helps understand the variability in issue description length and its potential impact on the effectiveness of large language models in resolving those issues.", "section": "6.2.2. Characteristics of Issue Description"}, {"figure_path": "https://arxiv.org/html/2504.02605/x10.png", "caption": "Figure 10: Influence of issue description length on resolved rate.", "description": "This figure (Figure 10) shows the effect of issue description length on the success rate of resolving issues using three different methods: MagentLess, MSWE-agent, and MopenHands across various programming languages.  Each bar represents a language, with different colors representing the methods. The x-axis shows different ranges of issue description lengths (in tokens), and the y-axis shows the resolved rate (%).  It illustrates how the resolution success changes depending on the length of issue descriptions and highlights the differences between the three methods.", "section": "6.2 Influencing Factors of Performance"}, {"figure_path": "https://arxiv.org/html/2504.02605/x11.png", "caption": "Figure 11: Histogram of fix patches length (#tokens).", "description": "This histogram visualizes the distribution of the lengths of fix patches (measured in the number of tokens) in the Multi-SWE-bench dataset.  The x-axis represents the length of fix patches, and the y-axis indicates the frequency or count of patches with that specific length. The distribution helps to understand the characteristics of fix patches and how their length might influence the difficulty of issue resolution tasks.", "section": "3.1.5. Phase 5: Manual Verification"}, {"figure_path": "https://arxiv.org/html/2504.02605/x12.png", "caption": "Figure 12: Histogram of the number of files modified by fix patches.", "description": "This histogram illustrates the distribution of the number of files altered within the fix patches across the Multi-SWE-bench dataset.  It provides a visual representation of how often a code fix involves modifying a single file, multiple files, or a large number of files. This is relevant to understanding the complexity of the code changes and the challenges posed to large language models (LLMs) in correctly implementing such changes.", "section": "6.2.3. Characteristics of Fix Patches"}, {"figure_path": "https://arxiv.org/html/2504.02605/x13.png", "caption": "Figure 13: Influence of fix patch length on resolved rate.", "description": "This figure displays the effect of the length of fix patches on the success rate of resolving issues, categorized into several length ranges.  It shows that shorter fix patches generally lead to higher success rates for various programming languages and methods used.  The success rate often significantly decreases as the fix patch length increases, indicating that longer, more complex patches are more challenging for the models to resolve effectively.  This suggests that the complexity of code changes required to resolve issues is an important factor influencing the effectiveness of AI models for software development tasks.", "section": "6.2.3. Characteristics of Fix Patches"}, {"figure_path": "https://arxiv.org/html/2504.02605/x14.png", "caption": "Figure 14: Influence of the number of files modified by fix patches on resolved rate.", "description": "This figure displays the relationship between the number of files modified in a fix patch and the success rate of resolving issues.  Separate bar charts are shown for each programming language (Python, Java, TypeScript, JavaScript, Go, Rust, C, and C++) for three different methods: MagentLess, MSWE-agent, and MopenHands.  The x-axis represents the number of files modified (categorized into 1, 1-5, 5-10, and >10), and the y-axis shows the resolved rate (%). The chart visualizes how the complexity of the patch (number of files changed) affects the likelihood of a successful resolution using different LLMs and methods.", "section": "6.2.3. Characteristics of Fix Patches"}]