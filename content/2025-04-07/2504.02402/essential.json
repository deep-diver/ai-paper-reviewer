{"importance": "This paper pioneers event-based cameras for sound recovery, offering a novel method to overcome limitations of traditional systems. By integrating spatial-temporal modeling and laser enhancement, it unlocks new possibilities for **high-frequency acoustic analysis** and non-contact sensing, potentially impacting fields from surveillance to material science.", "summary": "EvMic recovers sound from visual vibrations using spatial-temporal modeling on event-based camera data.", "takeaways": ["Event cameras, combined with laser matrix, are effective for capturing subtle, high-frequency vibrations.", "Spatial-temporal modeling enhances sound recovery from sparse event data.", "A new simulation pipeline and dataset (EvMic) are introduced for event-based sound recovery research."], "tldr": "Traditional sound recovery methods face trade-offs in sampling rate, bandwidth, and field of view. **Event cameras** offer high temporal resolution and are great for capturing high-frequency signals, but current event-based methods don't fully use the spatial-temporal information. So, there is a need to improve sound recovery utilizing the advantages of event cameras. \n\nTo improve event-based sound recovery, this paper introduces **EvMic**, a novel pipeline for non-contact sound recovery. They created a large training set with a new simulation. A new designed network captures spatial data and uses Mamba to model long-term temporal information.  A spatial aggregation block enhances signal quality. Experiments show this method is effective in synthetic and real-world scenarios.", "affiliation": "Shanghai AI Laboratory", "categories": {"main_category": "Speech and Audio", "sub_category": "Audio Enhancement"}, "podcast_path": "2504.02402/podcast.wav"}