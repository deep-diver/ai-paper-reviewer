[{"figure_path": "https://arxiv.org/html/2502.02671/x1.png", "caption": "Figure 1: Overview of our controlled experimental setup. Usually, the teacher model is trained on expert data before being distilled into the student LM. In the controlled setup of this paper, the teacher is itself distilled from an additional oracle model. This oracle model allows us to measure the quality of the distillation process into the student, and to reveal \u201cteacher hacking\u201d.", "description": "This figure illustrates the experimental setup used to investigate teacher hacking in language model distillation.  The standard knowledge distillation process involves training a teacher model on expert data, and then training a student model to mimic the teacher. This setup introduces an additional oracle model representing the true data distribution. The teacher model is first trained by distilling knowledge from the oracle. Then, a student model is trained by distilling knowledge from this imperfect teacher model. By comparing the student model to both the teacher and the oracle, the researchers can quantitatively assess the quality of the distillation process and identify instances of teacher hacking, where the student model over-optimizes the teacher's imperfections rather than learning the true data distribution.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2502.02671/x2.png", "caption": "Figure 2: Overview of the training pipeline. Two stages: (1) prompts x\ud835\udc65xitalic_x from a task-specific real dataset are used by the oracle model to generate the oracle pairs (x,y)\ud835\udc65\ud835\udc66(x,y)( italic_x , italic_y ), and afterwards, this dataset is used to get initial SFT checkpoints for both teacher and student model; (2) prompts from the same distribution are used to perform knowledge distillation, where the teacher model serves as a proxy to train the student model.", "description": "This figure illustrates the two-stage training pipeline used in the paper's experiments.  Stage 1 involves using an oracle model (representing the ideal ground truth) to generate a dataset of (prompt, response) pairs from a real-world task-specific dataset. This dataset is then used for supervised fine-tuning (SFT) to obtain initial checkpoints for both the teacher and student language models. Stage 2 focuses on knowledge distillation, where the student model learns to mimic the teacher model's behavior using prompts sampled from the same distribution as in Stage 1. The teacher model acts as an imperfect proxy for the ideal ground truth distribution. This setup allows for a controlled examination of the distillation process and the potential for teacher hacking.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2502.02671/x3.png", "caption": "Figure 3: Overview of the evaluation pipeline. We use the validation prompt dataset to measure the golden metric (the distance between the oracle and the student models) and the proxy metric (the distance between the teacher and the student models).", "description": "This figure illustrates the evaluation pipeline used to assess the performance of the student model.  It highlights the use of a validation prompt dataset and two key metrics: the 'golden metric' and the 'proxy metric'. The golden metric directly measures the performance of the student model by comparing its output to the ground-truth oracle model. In contrast, the proxy metric assesses how well the student model has learned from the teacher model, indicating the alignment between the student and teacher. This two-metric approach allows for a comprehensive evaluation of both the student's ability to approximate the ground truth and its success in imitating its teacher.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2502.02671/x4.png", "caption": "Figure 4: Proxy-Golden plot (offline data source).\nWe distill a T5-large teacher into a T5-base student on the XSUM dataset. The token-level training loss is the forward KL, the proxy metric is the distance to the teacher distribution and the golden metric is the distance to the ground-truth (oracle) distribution (available thanks to our semi-synthetic controlled experimental setup).\nIn this plot, the x\ud835\udc65xitalic_x-axis (proxy metric) indicates optimization progress, and the y\ud835\udc66yitalic_y-axis shows the ground-truth performance (golden metric): lower is better. Teacher hacking occurs in the case of offline data source: the orange curve has a U-type shape, indicating that during optimization, the orange metric starts increasing, whereas the proxy metric continues to decrease.", "description": "This figure shows a plot of the golden metric (distance to the oracle distribution, representing true performance) against the proxy metric (distance to the teacher distribution, representing optimization progress) during the distillation of a T5-large teacher model to a T5-base student model on the XSUM dataset. The x-axis represents the proxy metric, and the y-axis represents the golden metric; lower values indicate better performance. The orange curve shows the relationship between the two metrics for the offline data source.  The U-shape of this curve is indicative of teacher hacking: as optimization progresses (proxy metric decreases), the true performance (golden metric) initially improves but then begins to degrade.", "section": "Experimental Results"}, {"figure_path": "https://arxiv.org/html/2502.02671/x5.png", "caption": "Figure 5: Impact of using offline vs. online data sources.\nWhen using a fixed offline dataset, though the proxy metric continues to decrease, this is not visible in the golden metric, which continues to increase, a phenomenon we call teacher hacking. However, when using online response sampling, both from the teacher model or from the student model, this phenomenon does not occur.", "description": "This figure compares the training dynamics of language models when using offline versus online data sources for knowledge distillation.  The x-axis represents the number of training epochs, and the y-axis shows the values of three metrics: training loss, proxy metric (measuring the difference between student and teacher models), and golden metric (measuring the difference between student and oracle models).  The offline setting demonstrates a phenomenon called \"teacher hacking,\" where the proxy metric improves (decreases) but the golden metric worsens (increases), indicating that the model is overfitting to the teacher's imperfections rather than truly learning the underlying task. In contrast, the online data generation (from either the teacher or the student) successfully mitigates teacher hacking, leading to consistent improvement in both proxy and golden metrics.", "section": "4.2. When does teacher hacking appear?"}, {"figure_path": "https://arxiv.org/html/2502.02671/x6.png", "caption": "Figure 6: Impact of diversity of offline data sources.\nWe regulate the diversity of the dataset by decreasing the number of prompts in 2/5252/52 / 5 times and providing 2/5252/52 / 5-times more generations for each existing prompt, while preserving the size of the dataset.\nWhereas the dynamics of the train loss and proxy metric are almost the same, the effect of teacher hacking becomes more evident with a less diverse dataset.", "description": "This figure demonstrates the effect of data diversity on teacher hacking during knowledge distillation. By reducing the number of unique prompts and increasing the number of generations per prompt (while maintaining the overall dataset size), the experiment manipulates the diversity of the offline dataset.  The results show that although the training loss and proxy metrics (measuring the student's similarity to the teacher) remain largely unchanged, the degree of teacher hacking (the divergence between the student model and the ground truth, as measured by the golden metric) increases significantly as data diversity decreases.  This indicates that a more diverse dataset is crucial for mitigating teacher hacking during knowledge distillation.", "section": "4.3. How to mitigate teacher hacking?"}, {"figure_path": "https://arxiv.org/html/2502.02671/x7.png", "caption": "Figure 7: Impact of generation budget for offline data sources.\nAs the number of generations per prompt increases, both proxy and golden metrics improve, suggesting that the effect of teacher hacking is decreasing.", "description": "This figure demonstrates the effect of increasing the number of generated responses per prompt in an offline knowledge distillation setup.  The x-axis represents the number of training epochs, while the y-axis shows the values for three metrics: training loss, proxy metric (measuring the distance between student and teacher models), and golden metric (measuring the distance between student and oracle models).  The results show that as the number of generations per prompt increases (indicating a larger dataset and more data diversity), both the proxy and golden metrics improve, indicating a reduction in the teacher hacking phenomenon.  This suggests that the issue of teacher hacking can be mitigated by increasing the richness and diversity of the training data.", "section": "4.3. How to mitigate teacher hacking?"}, {"figure_path": "https://arxiv.org/html/2502.02671/x18.png", "caption": "Figure 8: Impact of the dataset choice: offline vs. online data sources.\nWe verify our claims on the presence of teacher hacking in the case of offline data sources for two different tasks: the translation task on WMT-14 en-de (top row) and the instruction following task on Natural Instruction (bottom row). In general, the behavior of the curves is the same across all the datasets: for online data sources, both proxy and golden metrics are decreasing. At the same time, for offline data sources, the proxy metric is decreasing or stagnating, whereas the golden metric is clearly increasing.", "description": "This figure displays the results of experiments comparing offline and online data sources for two distinct tasks: machine translation (WMT-14 en-de dataset) and instruction following (Natural Instructions dataset).  The top row shows the translation task results, while the bottom row displays the results for the instruction following task.  For both tasks, the plots demonstrate the behavior of the training loss, proxy metrics, and golden metrics across epochs. The key observation is that when using online data sources, both proxy and golden metrics consistently decrease, indicating successful model learning and generalization. Conversely, with offline data sources, while the proxy metric shows a decrease or remains relatively stable, the golden metric increases, clearly exhibiting teacher hacking. This highlights that teacher hacking occurs during training from fixed datasets.", "section": "4. Experimental results"}]