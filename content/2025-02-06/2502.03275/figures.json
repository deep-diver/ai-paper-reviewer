[{"figure_path": "https://arxiv.org/html/2502.03275/x1.png", "caption": "Figure 3.1: An example illustrating our replacement strategy. With chunk size L=16\ud835\udc3f16L=16italic_L = 16 and compression rate r=16\ud835\udc5f16r=16italic_r = 16, we encode 32 textual CoT tokens into 2 discrete latent tokens from left to right. The other CoT tokens will remain in their original forms.", "description": "This figure illustrates the method of compressing reasoning traces in the paper by replacing sequences of text tokens with latent tokens.  Specifically, it shows an example where 32 consecutive chain-of-thought (CoT) tokens are replaced by two latent tokens.  The replacement starts from the left and proceeds to the right up to a predefined point;  after that point, the remaining CoT tokens retain their original form. This technique is used to shorten the length of reasoning traces and reduce computational costs while preserving the crucial reasoning information. The chunk size (L) is set to 16, and the compression rate (r) is also 16, indicating that each chunk of 16 tokens is replaced by one latent token.", "section": "3 Methodology"}, {"figure_path": "https://arxiv.org/html/2502.03275/x2.png", "caption": "Figure 3.2: A graphical illustration of our VQ-VAE. fencsubscript\ud835\udc53enc{f_{\\text{enc}}}italic_f start_POSTSUBSCRIPT enc end_POSTSUBSCRIPT encodes the text tokens into latent embeddings, which are quantized by checking the nearest neighbors in the codebook. fdecsubscript\ud835\udc53dec{f_{\\text{dec}}}italic_f start_POSTSUBSCRIPT dec end_POSTSUBSCRIPT decodes those quantized embeddings back to text tokens. When applying the VQ-VAE to compress the text tokens, the discrete latent tokens Z\ud835\udc4dZitalic_Z are essentially the index of corresponding embeddings in the codebook.", "description": "The figure illustrates the Vector Quantized Variational Autoencoder (VQ-VAE) used to generate latent tokens for compressing reasoning traces. The VQ-VAE consists of an encoder (f_enc), a codebook, a quantizer (q), and a decoder (f_dec). The encoder maps sequences of text tokens representing reasoning steps into continuous embedding vectors. These vectors are then quantized to the nearest codebook entry using the quantizer, yielding a discrete latent token representing the original sequence. The decoder maps these latent tokens back to text tokens. This process converts the original sequence of text tokens into a compressed sequence of discrete latent tokens, which can be used to shorten reasoning traces and improve efficiency. The VQ-VAE is trained on the whole input sequence X to enhance the quality of latent abstractions, although it only operates on the reasoning steps during inference, effectively abstracting away the less informative initial steps of the reasoning process.", "section": "3 Methodology"}, {"figure_path": "https://arxiv.org/html/2502.03275/extracted/6181144/plots/entry_1.png", "caption": "(a) Prompt: What is the positive difference between\n$120%$ of 30 and $130%$ of 20?", "description": "This figure shows a comparison of the average attention weights across input prompt tokens between the proposed latent model and the baseline CoT model for the question: What is the positive difference between 120% of 30 and 130% of 20?.  The latent model demonstrates a stronger focus on numerical values and mathematical operation tokens, highlighting its improved ability to process mathematical problems.", "section": "Attention Weights Analysis"}, {"figure_path": "https://arxiv.org/html/2502.03275/extracted/6181144/plots/entry_7746.png", "caption": "(b) Prompt: Mark has $50 in his bank account. He earns $10 per day at his work. If he wants to buy a bike that costs $300, how many days does Mark have to save his money?", "description": "This figure shows the attention weights visualization for the second question in the prompt: Mark has $50 in his bank account; he earns $10 per day at his work. If he wants to buy a bike that costs $300, how many days does Mark have to save his money?. The visualization compares the attention weights of the model using the proposed latent approach against the baseline CoT model. It demonstrates that the latent approach focuses more attention on the numbers (50, 10, 300) and words representing mathematical operations, indicating a better understanding of the problem's numerical aspects.", "section": "Attention Weights Analysis"}, {"figure_path": "https://arxiv.org/html/2502.03275/extracted/6181144/plots/maze_env.png", "caption": "Figure 4.1: Comparing with the CoT model, our latent approach have high attention weights on numbers and text tokens representing mathematical operations.", "description": "Figure 4.1 presents a comparison of attention weights between the proposed latent approach and the baseline chain-of-thought (CoT) method.  The visualization shows the average attention weights assigned to input tokens during the generation of responses for two mathematical reasoning problems.  The latent model is shown to have significantly higher attention weights on numbers and operation-related tokens compared to the CoT model, indicating a stronger focus on core numerical computation in the latent approach.", "section": "4.2.1 SYNTHETIC BENCHMARKS"}, {"figure_path": "https://arxiv.org/html/2502.03275/extracted/6181144/plots/maze_traj1.png", "caption": "Figure A.1: An example of the keys-finding maze environment.", "description": "This figure shows a visual representation of the Keys-Finding Maze environment used in the experiments. It's a grid-based maze where an agent needs to navigate to a target location by collecting keys to open color-coded doors. The maze is composed of multiple interconnected rooms, making it more complex than a simple path-finding problem.", "section": "A.2 Keys-Finding Maze"}, {"figure_path": "https://arxiv.org/html/2502.03275/extracted/6181144/plots/maze_traj2.png", "caption": "(a) Phase 1", "description": "This figure shows the first phase of a multi-step process in a Keys-Finding Maze environment.  The agent (black circle) is in a maze and must navigate to a goal location (gold diamond).  To get there, they must collect keys of different colors (red, green, blue) that correspond to similarly colored doors, one key at a time. This first phase depicts the agent's initial state and the location of the goal, keys and doors.", "section": "A.2 Keys-Finding Maze"}, {"figure_path": "https://arxiv.org/html/2502.03275/extracted/6181144/plots/maze_traj3.png", "caption": "(b) Phase 2", "description": "This phase of the Keys-Finding Maze shows the agent, after obtaining the blue key in Phase 1, proceeding to open the blue door to access the red key.  The agent's next step will involve using the red key.", "section": "A.2 Keys-Finding Maze"}, {"figure_path": "https://arxiv.org/html/2502.03275/extracted/6181144/plots/maze_traj4.png", "caption": "(c) Phase 3", "description": "This figure shows the third phase of an agent's optimal trajectory in a keys-finding maze. The agent has already acquired the blue key, opened the blue door, and obtained the red key. In this phase, the agent is moving towards the red door to use the red key to open it.", "section": "A.2 Keys-Finding Maze"}]