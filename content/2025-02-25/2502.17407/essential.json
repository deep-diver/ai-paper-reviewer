{"importance": "This paper is important because it highlights that **test-time scaling might not generalize effectively to multilingual tasks**, offering insights into the complexities of achieving true multilinguality in mathematical reasoning and guiding future research in more robust, cross-lingual methods.", "summary": "Test-time scaling isn't a universal solve-all for multilingual math reasoning, unlike pre-training scaling, shows MCLM benchmark.", "takeaways": ["Test-time scaling methods don't generalize as effectively as pre-training scaling for multilingual math reasoning.", "MR1-1.5B, despite its small size, achieves comparable performance to GPT-4o-Mini in multilingual math.", "Increasing test-time compute can amplify performance variance and reduce cross-linguistic consistency."], "tldr": "Scaling pre-training compute improves multilinguality, but does test-time scaling work similarly? The paper introduces **MCLM, a multilingual math benchmark**, to test this. MCLM contains competition-level problems in 55 languages. Researchers tested outcome/process reward modeling (ORM/PRM) and budget forcing (BF) on Qwen2.5-1.5B Math and MR1-1.5B. Results reveal ", "affiliation": "Yonsei University", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}, "podcast_path": "2502.17407/podcast.wav"}