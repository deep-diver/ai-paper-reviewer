{"references": [{"fullname_first_author": "Stella Biderman", "paper_title": "Pythia: A suite for analyzing large language models across training and scaling", "publication_date": "2023-01-01", "reason": "This paper introduces the Pythia model suite, which is used as a baseline for comparison in the paper, making it crucial for evaluating Slam's performance."}, {"fullname_first_author": "Santiago Cuervo", "paper_title": "Scaling properties of speech language models", "publication_date": "2024-04-01", "reason": "This paper provides the foundational scaling laws for SLMs that Slam challenges and aims to surpass, which is a main contribution of the paper."}, {"fullname_first_author": "Tri Dao", "paper_title": "Flashattention-2: Faster attention with better parallelism and work partitioning", "publication_date": "2023-07-01", "reason": "This paper introduces FlashAttention2, which Slam leverages for efficient computation, essential for training within the compute constraints."}, {"fullname_first_author": "Wei-Ning Hsu", "paper_title": "HuBERT: Self-supervised speech representation learning by masked prediction of hidden units", "publication_date": "2021-01-01", "reason": "This paper introduces HuBERT, which is used to extract speech tokens, a key step in the Slam training pipeline for SLMs."}, {"fullname_first_author": "Rafael Rafailov", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "publication_date": "2024-01-01", "reason": "This paper introduces Direct Preference Optimization, used by the paper in their training recipe."}]}