{"importance": "This paper is **crucial for SLM research** by offering a practical recipe to train high-quality models on limited resources. It **challenges pessimistic views** on SLM feasibility, encouraging further exploration and refinement of scaling laws. The open-sourced code, models, and training recipes **democratize access**, opening new avenues for innovation in speech and audio processing.", "summary": "Slam: Train SLMs on one GPU in a day!", "takeaways": ["High-quality SLMs can be trained efficiently on limited compute resources.", "Careful optimization of model architecture, initialization, and training data is crucial for efficient SLM training.", "Preference optimization with synthetic data significantly enhances SLM performance."], "tldr": "Speech Language Models (SLMs) require extensive resources to train, hindering research and development for many. Training SLMs demands large-scale datasets, resulting in **high computational costs**, restricting research to well-funded labs. Existing scaling laws present a pessimistic view of the computational resources needed to train high-quality SLMs.\n\nThis paper introduces a **recipe for training high-quality SLMs on a single GPU** in 24 hours. The recipe, called **Slam**, uses empirical analysis of model initialization, synthetic training data, preference optimization to maximize performance. Empirical results show that the recipe scales well with more compute, performing on par with leading SLMs at a fraction of the cost. In SLM scaling laws, Slam gives an optimistic view of SLM feasibility.", "affiliation": "Hebrew University of Jerusalem", "categories": {"main_category": "Speech and Audio", "sub_category": "Speech Synthesis"}, "podcast_path": "2502.15814/podcast.wav"}