{"references": [{"fullname_first_author": "Shuai Bai", "paper_title": "Qwen2.5-vl technical report", "publication_date": "2025-02-13", "reason": "This paper reports the architecture, training details, and performance evaluation of the Qwen series of models, which are leveraged extensively in the current research for both caption generation and visual reasoning tasks."}, {"fullname_first_author": "Lin Chen", "paper_title": "Sharegpt4v: Improving large multi-modal models with better captions", "publication_date": "2024-01-01", "reason": "This paper is valuable to reference because it focuses on enhancing vision-language alignment through high-quality, attribute-specific captions, a key aspect of the current study."}, {"fullname_first_author": "Daya Guo", "paper_title": "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning", "publication_date": "2025-01-12", "reason": "This paper is essential due to the use of DeepSeek-R1 models in the current study for visual reasoning, where the performance improvements due to detailed captions are demonstrated."}, {"fullname_first_author": "Peng Wang", "paper_title": "Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution", "publication_date": "2024-09-12", "reason": "This paper is important because Qwen2-VL is utilized as a key component in the approach, either as a standalone model or as part of a hybrid model, particularly during the seed caption and caption extension phases."}, {"fullname_first_author": "Haotian Liu", "paper_title": "Visual instruction tuning", "publication_date": "2023-01-01", "reason": "This is an important reference because it relates to the technique used as the baseline for comparison in assessing downstream SFT performance."}]}