[{"figure_path": "https://arxiv.org/html/2504.07086/x1.png", "caption": "Figure 1: The Sombre State of LM Reasoning for Math. (left) when re-evaluating recent 1.5B reasoning-enhanced models on AIME-24 using a standardized framework (see Section\u00a04), we find substantial drops to reported results in the original papers, (right) the observed improvements from recent methods (gray highlighted area) fall entirely within the variance range (orange box plots) of DeepSeek-R1 1.5B model performance.\nThis suggests that these methods do not significantly outperform the base model\u2014underscoring the importance of rigorous, multi-seed evaluation protocols for obtaining reliable performance estimates.", "description": "This figure shows the results of re-evaluating recently published mathematical reasoning models.  The left panel displays a bar chart comparing the reported accuracy of several 1.5B parameter language models on the AIME-24 benchmark against their accuracy when re-evaluated using a standardized framework.  The re-evaluation shows substantially lower accuracy than initially reported, indicating issues with reproducibility in the original evaluations. The right panel shows box plots illustrating the variance in performance of the DeepSeek-R1 1.5B model across multiple runs, highlighting that the observed improvements of other models fall within this variance. This demonstrates that the reported improvements are not statistically significant and emphasizes the need for more rigorous evaluation methods, including multiple random seeds, to accurately assess language model performance on mathematical reasoning tasks.", "section": "3 Exploring the Design Space of Reasoning: What Matters Most?"}, {"figure_path": "https://arxiv.org/html/2504.07086/x2.png", "caption": "Figure 2: Accuracy varies significantly across random seeds. We find significantly high Pass@1 variation across 20 different random seeds for nine models on AIME\u201924, AMC\u201923, and MATH500. Variance is particularly high on AIME\u201924 (upto 15%) and AMC\u201923 (upto 13%) due to the small number of test samples, highlighting instability of single-seed evaluations.", "description": "Figure 2 illustrates the significant impact of random seeds on the accuracy of various large language models (LLMs) when performing mathematical reasoning tasks.  The results demonstrate a substantial variability in Pass@1 scores (the percentage of problems solved correctly in the first attempt) across multiple runs with different random seeds for nine different models. This variability is especially pronounced on smaller datasets like AIME'24 and AMC'23, exhibiting up to 15% and 13% variance, respectively, due to their limited number of test problems.  This underscores the unreliability of using single-seed evaluations to evaluate model performance, highlighting the importance of employing multiple seeds and averaging their results for robust performance estimates.", "section": "Seed Variance in Evaluation"}, {"figure_path": "https://arxiv.org/html/2504.07086/x3.png", "caption": "Figure 3: Bootstrapped seed averaging is reliable only beyond a threshold. We plot the variance of Mean Pass@1 scores on AIME\u201924 when averaging over K=1\ud835\udc3e1K=1italic_K = 1 to K=10\ud835\udc3e10K=10italic_K = 10 seed runs, finding that the variance is extremely high for small K\ud835\udc3eKitalic_K and significantly reduced by K=10\ud835\udc3e10K=10italic_K = 10. This suggests that using multi-seed evaluations (K\u226510\ud835\udc3e10K\\geq 10italic_K \u2265 10) would yield more stable estimates. For results on AMC23 and MATH500 see Figures 12 and 13 respectively.", "description": "This figure shows how the number of random seeds used in the evaluation process impacts the stability and reliability of the results.  The x-axis represents the number of random seeds (K) used, ranging from 1 to 10. The y-axis shows the variance of the mean Pass@1 score, a metric that assesses the model's performance. The graph demonstrates that when using only a small number of seeds (K<10), the variance is very high, indicating unstable and unreliable results.  However, as the number of seeds increases, the variance decreases significantly, converging to a more stable level around K=10.  This illustrates that using at least 10 random seeds for evaluation is crucial for obtaining reliable performance estimates, especially for small datasets like AIME'24. The results for AMC23 and MATH500 datasets are presented in Figures 12 and 13, respectively.", "section": "Seed Variance in Evaluation"}, {"figure_path": "https://arxiv.org/html/2504.07086/x4.png", "caption": "Figure 4: Higher temperatures yield better accuracies. We find across all three datasets, higher temperatures produce better peak accuracy but introduce instability, revealing a tradeoff between performance and reproducibility. Results obtained by varying temperature from 0 to 1 in increments of 0.1, while keeping top_p fixed at 0.9.", "description": "This figure illustrates the impact of temperature on the accuracy of various language models across three different datasets (AIME24, AMC23, MATH500).  It shows that while higher temperatures generally lead to improved peak accuracy, they also significantly increase the variance in model performance across different random seeds.  This highlights a trade-off between achieving high accuracy and ensuring reproducible results. The experiment was conducted by varying the temperature parameter from 0 to 1 in steps of 0.1, while keeping the top_p parameter constant at 0.9.", "section": "3.2 Seed Variance in Evaluation"}, {"figure_path": "https://arxiv.org/html/2504.07086/x5.png", "caption": "Figure 5: Higher top_p values improve performance at no cost to stability. Across all datasets, we find that higher top_p values generally improve performance while preserving similar amounts of variance as lower top_p values. Results were obtained by varying top_p from 0 to 1 in increments of 0.1, while holding the temperature constant at 0.8.", "description": "Figure 5 investigates the effect of the top_p hyperparameter on the performance of language models in mathematical reasoning tasks.  The experiment systematically varies top_p from 0 to 1 in 0.1 increments, while keeping the temperature constant at 0.8. The results demonstrate that increasing top_p generally leads to improved accuracy across all datasets (AIME24, AMC23, MATH500) without a significant increase in variance compared to lower top_p values.  This suggests that higher top_p values offer a beneficial trade-off: improved performance with similar stability levels.", "section": "3.2 Seed Variance in Evaluation"}, {"figure_path": "https://arxiv.org/html/2504.07086/x6.png", "caption": "Figure 6: Accuracies vary significantly across temperature values. Across nine different models and three datasets, we observe consistently large variations in performance (upto 15%) induced by changing the temperature. Results were obtained by varying the temperature from 0 to 1 in increments of 0.1, while holding top_p constant at 0.9.", "description": "This figure shows how sensitive the performance of language models is to the temperature parameter during the generation process.  Nine different language models were evaluated on three distinct mathematical reasoning datasets. The x-axis represents temperature values ranging from 0.0 to 1.0 in increments of 0.1. The y-axis shows the accuracy (Pass@1). The figure demonstrates significant performance variation across different temperature settings, with accuracy changes up to 15% observed.  Note that the top_p parameter was kept constant at 0.9 during the experiment to isolate the effect of temperature.", "section": "Seed Variance in Evaluation"}, {"figure_path": "https://arxiv.org/html/2504.07086/x7.png", "caption": "Figure 7: Accuracies vary significantly across top_p values. Across nine different models and three datasets, we observe consistently large variations in performance (upto 8%) induced by changing the top_p value. Results were obtained by varying top_p from 0 to 1 in increments of 0.1, while holding the temperature constant at 0.8.", "description": "Figure 7 shows how sensitive model performance is to the top_p hyperparameter, which controls the diversity of token selection during text generation.  Across nine different language models and three mathematical reasoning datasets, changing the top_p value from 0 to 1 (in 0.1 increments) while keeping the temperature fixed at 0.8 led to performance variations of up to 8%. This highlights the importance of consistent hyperparameter settings for reliable model comparisons and the impact of subtle hyperparameter changes on model performance. The high variance emphasizes the need for rigorous evaluation techniques, particularly using multiple random seeds, to obtain reliable results.", "section": "3.2 Seed Variance in Evaluation"}, {"figure_path": "https://arxiv.org/html/2504.07086/x8.png", "caption": "(a) AIME24. Significant differences are observed in model performance across compute clusters.", "description": "This figure shows the results of evaluating various language models on the AIME24 benchmark across five different compute clusters. Each cluster represents a unique hardware and software configuration. The variations in model performance across clusters highlight the significant impact of hardware and software differences on the reproducibility and reliability of language model evaluation results. The x-axis shows the different models while the y-axis shows the accuracy of the models. The bars represent the average accuracy of each model across the clusters.", "section": "3.3 Variance from Hardware and Software Factors"}, {"figure_path": "https://arxiv.org/html/2504.07086/x9.png", "caption": "(b) AMC23. Similar variability is seen across hardware in AMC23 results.", "description": "The bar chart visualizes the performance variation of different language models across various hardware configurations when evaluated on the AMC23 dataset.  Each bar represents a model's accuracy, with error bars indicating the variability due to hardware differences. The chart highlights the significant performance fluctuations caused by variations in hardware, underscoring the importance of consistent hardware setups for reliable model evaluation and reproducibility.", "section": "3.3 Variance from Hardware and Software Factors"}, {"figure_path": "https://arxiv.org/html/2504.07086/x10.png", "caption": "Figure 8: Performance variation across compute clusters. Accuracy differences emerge when the same models are evaluated across compute clusters for both AIME24 and AMC23 datasets\u2014these large differences in performance also persist when evaluating 7B models.", "description": "Figure 8 illustrates the impact of hardware variations on model performance consistency.  The figure presents accuracy results for various language models evaluated across multiple compute clusters (different hardware configurations).  The results for both AIME24 and AMC23 datasets are shown, demonstrating that substantial performance differences can emerge even when using the same models and evaluation parameters.  This variation is particularly concerning as it highlights a significant source of irreproducibility in experimental results,  and the problem persists even when evaluating larger, 7B parameter models.", "section": "3.3 Variance from Hardware and Software Factors"}, {"figure_path": "https://arxiv.org/html/2504.07086/x11.png", "caption": "Figure 9: Models are extremely sensitive to output token lengths. We sweep across different max_new_tokens (number of tokens that models are allowed to generate) for DeepScaleR-1.5B and DeepSeek-R1-Distill-1.5B/7B on three datasets and find that they are heavily sensitive to output length limits, with premature truncation degrading the performance.", "description": "This figure demonstrates the significant impact of the `max_new_tokens` parameter (the maximum number of tokens a model can generate in a single response) on the performance of large language models (LLMs) in mathematical reasoning tasks.  The experiment sweeps across a range of `max_new_tokens` values for three different sized models (DeepScaleR-1.5B, DeepSeek-R1-Distill-1.5B, and DeepSeek-R1-Distill-7B) across three different datasets (AIME24, AMC23, MATH500). The results show that the performance of all the models is highly sensitive to this parameter, indicating that insufficiently long generation limits lead to premature truncation of the model's responses and a significant decrease in accuracy.  Conversely, excessively long generation limits may lead to other issues such as repetitive or incoherent reasoning.", "section": "Exploring the Design Space of Reasoning: What Matters Most?"}, {"figure_path": "https://arxiv.org/html/2504.07086/x12.png", "caption": "Figure 10: Using no prompt templates yields worse performance. We compare Pass@1 scores across three prompt formats: (1) math-specific prompt with chat template, (2) default chat template only, and (3) no template. Instruction-tuned models perform best with structured prompts and templates; omitting templates leads to consistent performance drops.", "description": "This figure displays the results of an experiment comparing the performance of language models on mathematical reasoning tasks using different prompt formats.  Three prompt formats were tested: (1) a math-specific prompt with a chat template, (2) the default chat template only, and (3) no template at all. The results show that instruction-tuned models perform significantly better when provided with structured prompts and templates. Omitting the templates consistently leads to a decrease in performance, highlighting the importance of properly formatted prompts for optimal performance.", "section": "3 Exploring the Design Space of Reasoning: What Matters Most?"}, {"figure_path": "https://arxiv.org/html/2504.07086/x13.png", "caption": "Figure 11: Response Length vs. Accuracy. Histogram of correct vs. incorrect responses by response length, averaged over random seeds across AIME24, AIME25, AMC23, MATH500, Minerva and OlympiadBench benchmarks. Longer outputs tend to be more error-prone, even in complete responses not close to the maximum sequence length.", "description": "This figure displays the relationship between the length of model responses and their accuracy.  Histograms show the average number of correct and incorrect responses for different lengths, calculated across multiple random seeds and several mathematical reasoning benchmarks (AIME24, AIME25, AMC23, MATH500, Minerva, and OlympiadBench).  The key observation is that longer responses are significantly more likely to be incorrect, even when those longer responses are fully formed and do not reach the maximum sequence length allowed for the model.", "section": "Exploring the Design Space of Reasoning: What Matters Most?"}, {"figure_path": "https://arxiv.org/html/2504.07086/x14.png", "caption": "Figure 12: Variance of mean Pass@1 on AMC\u201923. Bootstrapped estimates show substantial variance even with K=5\ud835\udc3e5K=5italic_K = 5 evaluation runs, highlighting the instability of single-seed evaluations.", "description": "Figure 12 illustrates the significant variability in the Pass@1 metric (a measure of accuracy) on the AMC'23 benchmark even when averaging results over 5 different random seeds (K=5).  The box plots demonstrate that despite averaging across multiple seeds, the performance remains unstable. The substantial variance highlights a key limitation: single-seed evaluations (using only one random seed) are unreliable for assessing model performance on smaller datasets like AMC'23.  Averaging over multiple seeds is crucial for obtaining a more stable and reliable estimate of a model's actual performance.", "section": "3.2 Seed Variance in Evaluation"}, {"figure_path": "https://arxiv.org/html/2504.07086/x15.png", "caption": "Figure 13: Variance of mean Pass@1 on MATH500. Similar to AIME\u201924 and AMC\u201923, the estimates remain volatile across seeds. Even K=5\ud835\udc3e5K=5italic_K = 5 runs do not eliminate variance, underscoring the need for larger K\ud835\udc3eKitalic_K.", "description": "Figure 13 presents a detailed analysis of the variance in the mean Pass@1 scores across multiple random seeds (K) on the MATH500 dataset.  The results show that even with 5 seeds (K=5), the variance remains high, indicating that single-seed evaluations are insufficient to reliably assess model performance. The figure underscores that obtaining stable performance estimates necessitates averaging over a significantly larger number of seeds (K). This is consistent with the findings observed in Figures 12 and 13, which display similar volatility on the AIME'24 and AMC'23 datasets. The high variance highlights the inherent instability of small-scale benchmarks and emphasizes the critical need for robust, multi-seed evaluation protocols to ensure reliable model comparisons.", "section": "3.2 Seed Variance in Evaluation"}, {"figure_path": "https://arxiv.org/html/2504.07086/x16.png", "caption": "Figure 14: Performance variation across compute clusters on MATH500. Differences in GPU type and environment lead to non-trivial shifts in performance, reinforcing the importance of hardware standardization.", "description": "Figure 14 illustrates the impact of hardware and software variations on the performance of different language models when evaluated on the MATH500 benchmark.  The results highlight significant performance discrepancies between different compute clusters, even when using the same models, random seeds, and evaluation parameters.  The variations observed underscore the critical need for standardization in hardware and software configurations when conducting experiments and reporting results, to ensure reproducibility and reliability.", "section": "3.3 Variance from Hardware and Software Factors"}, {"figure_path": "https://arxiv.org/html/2504.07086/x17.png", "caption": "Figure 15: Impact of max_new_tokens on OpenRS models. Models with long context support (131,072 tokens) experience degraded performance when max_new_tokens is set too low.", "description": "This figure demonstrates the impact of the `max_new_tokens` parameter on the performance of OpenRS language models.  OpenRS models were initially evaluated with a long context window of 131,072 tokens. This experiment systematically varied the `max_new_tokens` setting to observe its effects on model accuracy across three benchmark datasets (AIME'24, AMC'23, MATH500).  The results reveal that reducing the maximum number of newly generated tokens significantly impairs performance, underscoring the importance of providing sufficient context length for optimal model accuracy in this specific set of models.", "section": "3 Exploring the Design Space of Reasoning: What Matters Most?"}, {"figure_path": "https://arxiv.org/html/2504.07086/x18.png", "caption": "Figure 16: Impact of max_new_tokens on OpenThinker and S1.1 models. Despite shorter context limits (32,768 tokens), performance still degrades noticeably when output length is constrained.", "description": "Figure 16 shows the effect of limiting the maximum number of newly generated tokens (max_new_tokens) on the performance of the OpenThinker and S1.1 language models.  Even though these models have a shorter maximum context length of 32,768 tokens compared to other models in the study, reducing the max_new_tokens parameter still leads to a significant decrease in performance.  This highlights the sensitivity of these models to output length constraints, even when operating within their specified context limits. The results are presented in the form of box plots showing the accuracy for each model on AIME24, AMC23, and MATH500 datasets, across different max_new_tokens settings.", "section": "3 Exploring the Design Space of Reasoning: What Matters Most?"}, {"figure_path": "https://arxiv.org/html/2504.07086/x19.png", "caption": "Figure 17: Response Length vs. Correctness \u2014 Models (1/2). Average number of correct and incorrect responses across response length bins for a subset of models. Longer responses consistently correlate with incorrect predictions.", "description": "This figure displays the relationship between response length and accuracy for several language models.  Histograms show the average number of correct and incorrect responses binned by response length. The data reveals a clear trend: longer responses are significantly more likely to be incorrect, suggesting a correlation between longer response generation and a higher chance of error. This holds true even when considering only complete responses (and not ones cut short due to token limits). This observation highlights a potential bias in using response length alone as a performance metric.", "section": "3.4 Do Discovered Phenomena Replicate? A Detailed Analysis"}]