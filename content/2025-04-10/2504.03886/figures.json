[{"figure_path": "https://arxiv.org/html/2504.03886/x1.png", "caption": "Figure 1: WildGS-SLAM. Given a monocular video sequence captured in the wild with dynamic distractors, our method accurately tracks the camera trajectory and reconstructs a 3D Gaussian map for static elements, effectively removing all dynamic components. This approach enables high-fidelity rendering even in complex, dynamic scenes. The illustration presents the final 3D Gaussian map, the camera tracking trajectory (in red), and view synthesis comparisons with baseline methods.", "description": "WildGS-SLAM accurately tracks camera movement and reconstructs a 3D Gaussian map of static elements while ignoring dynamic objects in a monocular video.  The figure shows the final 3D Gaussian map, the camera's path (red line), and comparisons of the view synthesis quality with other SLAM methods. This highlights WildGS-SLAM's ability to produce high-fidelity renderings even in complex, dynamic scenes.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2504.03886/extracted/6337032/figs/dataset/D455_cropped.png", "caption": "Figure 2: System Overview. WildGS-SLAM takes a sequence of RGB images as input and simultaneously estimates the camera poses while building a 3D Gaussian map \ud835\udca2\ud835\udca2\\mathcal{G}caligraphic_G of the static scene. Our method is more robust to the dynamic environment due to the uncertainty estimation module, where a pretrained DINOv2 model\u00a0[yue2025improving] is first used to extract the image features. An uncertainty MLP \ud835\udcab\ud835\udcab\\mathcal{P}caligraphic_P then utilizes the extracted features to predict per-pixel uncertainty. During the tracking, we leverage the predicted uncertainty as the weight in the dense bundle adjustment (DBA) layer to mitigate the impact of dynamic distractors. We further use monocular metric depth to facilitate the pose estimation. In the mapping module, the predicted uncertainty is incorporated into the rendering loss to update \ud835\udca2\ud835\udca2\\mathcal{G}caligraphic_G. Moreover, the uncertainty loss is computed in parallel to train \ud835\udcab\ud835\udcab\\mathcal{P}caligraphic_P. Note that \ud835\udcab\ud835\udcab\\mathcal{P}caligraphic_P and \ud835\udca2\ud835\udca2\\mathcal{G}caligraphic_G are optimized independently, as illustrated by the gradient flow in the gray dashed line. Faces are blurred to ensure anonymity.", "description": "WildGS-SLAM processes RGB image sequences to simultaneously estimate camera poses and reconstruct a 3D Gaussian map of the static scene.  The system's robustness to dynamic environments stems from an uncertainty estimation module. This module uses a pretrained DINOv2 model to extract image features, which are then fed into a Multilayer Perceptron (MLP) to predict per-pixel uncertainty.  This uncertainty map is then used to weigh the dense bundle adjustment (DBA) during tracking, reducing the effect of moving objects. Monocular metric depth further aids pose estimation. The predicted uncertainty also influences the rendering loss during map optimization, improving the 3D Gaussian map's accuracy. The MLP and the 3D Gaussian map are optimized independently. ", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2504.03886/extracted/6337032/figs/dataset/calibration_board.png", "caption": "Figure 3: Input View Synthesis Results on our Wild-SLAM MoCap Dataset. Regardless of the distractor type, our method is able to remove distractors and render realistic images. Faces are blurred to ensure anonymity.", "description": "This figure displays the results of input view synthesis using the WildGS-SLAM method on the Wild-SLAM MoCap dataset.  The images demonstrate the system's ability to accurately remove dynamic objects (distractors) from the scene reconstruction, regardless of the type of distractor present. The resulting 3D Gaussian maps generate realistic synthetic views that maintain high fidelity.  For privacy reasons, the faces of any people in the images have been blurred.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2504.03886/extracted/6337032/figs/dataset/scene1.png", "caption": "Figure 4: Novel View Synthesis Results on our Wild-SLAM MoCap Dataset. PSNR metrics (\u2191\u2191\\uparrow\u2191) are included in images.", "description": "This figure visualizes novel view synthesis results from the Wild-SLAM MoCap dataset.  It presents several sequences, each showing comparisons between input views and renderings produced by WildGS-SLAM and other state-of-the-art methods.  The quality of the novel view synthesis is evaluated using PSNR (Peak Signal-to-Noise Ratio), and the corresponding PSNR values are displayed directly on the images. The figure showcases the effectiveness of WildGS-SLAM in generating high-fidelity, artifact-free renderings even in complex dynamic scenes.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2504.03886/extracted/6337032/figs/dataset/scene2.png", "caption": "Figure 5: Input View Synthesis Results on our Wild-SLAM iPhone Dataset. We only show rendering results of monocular methods, as depth images are unavailable in this dataset.\nNote that our uncertainty map appears blurry, as DINOv2 outputs feature maps at 1/14 of the original resolution, and for mapping we also downsample to 1/3 of the original resolution, in order to maintain SLAM system efficiency. For a high-resolution, sharper uncertainty map, the resolution can be increased at the cost of some efficiency; further details and results are provided in the supplementary materials.\nFaces are blurred to ensure anonymity.", "description": "This figure showcases the results of input view synthesis using WildGS-SLAM on the Wild-SLAM iPhone Dataset.  Since depth data was unavailable for this dataset, only monocular methods are presented. The uncertainty map appears less sharp than expected because the DINOv2 feature extractor, used for uncertainty prediction, outputs feature maps at a significantly reduced resolution (1/14th of the original image). To maintain computational efficiency within the SLAM system, the resolution is further downsampled to 1/3rd of the original for the mapping stage. While this compromise reduces image sharpness, higher resolution maps are possible at the cost of decreased efficiency; supplementary materials contain further details and higher resolution results. Note that faces in the images have been blurred to maintain anonymity.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2504.03886/extracted/6337032/figs_rebuttal/online_uncertainty_input.png", "caption": "Figure 6: View Synthesis Results on Bonn RGB-D Dynamic Dataset\u00a0[palazzolo2019iros]. We show results on the Balloon (first row) and Crowd (second row) sequences.\nFor Balloon, ReFusion\u00a0[palazzolo2019iros] fails to remove the person from the TSDF, and DynaSLAM(N+G)[bescos2018dynaslam] struggles with limited static information from multiple views, resulting in partial black masks. In Crowd, DynaSLAM(N+G)[bescos2018dynaslam] cannot detect dynamic regions, defaulting the original image as the inpainted result. In contrast, ours achieves superior rendering even with motion blur in the input.", "description": "Figure 6 demonstrates the effectiveness of WildGS-SLAM on the Bonn RGB-D Dynamic Dataset, specifically showcasing its ability to handle dynamic objects and produce high-quality renderings.  The first row shows the 'Balloon' sequence; the comparison highlights WildGS-SLAM's superior performance compared to ReFusion and DynaSLAM (N+G), which struggle with removing the person entirely and producing artifacts respectively. The second row, featuring the 'Crowd' sequence, demonstrates how WildGS-SLAM outperforms DynaSLAM (N+G) by correctly identifying and removing dynamic objects, generating a clean and artifact-free rendering even in the presence of motion blur.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2504.03886/extracted/6337032/figs_rebuttal/online_uncertainty_80.pth.png", "caption": "(a)", "description": "This figure shows an Intel RealSense D455 camera used for capturing data in the Wild-SLAM MoCap dataset.  The camera is shown from different angles, illustrating its setup and orientation during data collection. The image highlights the device's physical characteristics and provides context for understanding the camera's role in the data acquisition process.", "section": "6. Wild-SLAM Dataset"}, {"figure_path": "https://arxiv.org/html/2504.03886/extracted/6337032/figs_rebuttal/online_uncertainty_215.pth.png", "caption": "(b)", "description": "The figure shows a calibration board used to align the camera reference frame with OptiTrack's rigid body frame.  This is a crucial step in the dataset creation process, ensuring accurate alignment between the camera's perspective and the ground truth data obtained from the motion capture system. The board provides a set of known 3D points, whose corresponding 2D projections in the camera image are used to calculate the transformation matrix between the two coordinate systems.", "section": "6. Wild-SLAM Dataset"}, {"figure_path": "https://arxiv.org/html/2504.03886/extracted/6337032/figs_rebuttal/online_uncertainty_451.pth.png", "caption": "(c)", "description": "This figure shows the second static scene used in the Wild-SLAM MoCap dataset.  It depicts a scene with multiple objects (a table, a gripper, and various items) that would constitute a relatively complex static scene for a SLAM system to reconstruct. This image is presented alongside other images illustrating the camera setup and a sample static scene in the dataset, highlighting the variety of static scenes in the dataset.", "section": "6. Wild-SLAM Dataset"}]