[{"content": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S3.T1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.1\">\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T1.1.1.1.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T1.1.1.1.1.1\">\n<span class=\"ltx_p\" id=\"S3.T1.1.1.1.1.1.1\" style=\"width:313.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.1.1.1.1.1\">Text</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T1.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.1.2.1\">Sense Year</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T1.1.2.1\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S3.T1.1.2.1.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T1.1.2.1.1.1\">\n<span class=\"ltx_p\" id=\"S3.T1.1.2.1.1.1.1\" style=\"width:313.0pt;\">\u201cThey had a bunch of crazy ideas that would never <span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.2.1.1.1.1.1\">work</span>\u201d</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.2.1.2\">1599</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.3.2\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S3.T1.1.3.2.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T1.1.3.2.1.1\">\n<span class=\"ltx_p\" id=\"S3.T1.1.3.2.1.1.1\" style=\"width:313.0pt;\">\u201cI tried to call the operator but the phone was <span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.3.2.1.1.1.1\">dead</span>\u201d</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.3.2.2\">1882</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.4.3\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S3.T1.1.4.3.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T1.1.4.3.1.1\">\n<span class=\"ltx_p\" id=\"S3.T1.1.4.3.1.1.1\" style=\"width:313.0pt;\">\u201cYou know how it is. I\u2019m not into ironing. It\u2019s not my <span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.4.3.1.1.1.1\">thing</span>\u201d</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.4.3.2\">1936</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.5.4\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" id=\"S3.T1.1.5.4.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T1.1.5.4.1.1\">\n<span class=\"ltx_p\" id=\"S3.T1.1.5.4.1.1.1\" style=\"width:313.0pt;\">\u201cLet\u2019s go where there\u2019s some life. Whatta ya say? Hey baby, I\u2019m <span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.5.4.1.1.1.1\">down</span>\u201d</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T1.1.5.4.2\">1952</td>\n</tr>\n</tbody>\n</table>", "caption": "Table 1: Cloze task examples and the year when the word sense first appeared", "description": "This table provides example sentences using words with multiple meanings, illustrating how the meaning of a word has changed over time.  Each row shows a sentence containing a word, the specific meaning of the word in that sentence, and the year that meaning first appeared. This data is used to evaluate how well language models understand the time-specific evolution of word meanings.", "section": "3.3 Evaluation"}, {"content": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S3.T2.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S3.T2.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T2.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.1.1.1.1\">Model</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T2.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.1.1.2.1\">Sentence</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T2.1.2.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.1.2.1.1\">1750 to 1820</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.1.2.1.2\">\n<span class=\"ltx_text\" id=\"S3.T2.1.2.1.2.1\" style=\"color:#750D0D;\">with</span> <span class=\"ltx_text\" id=\"S3.T2.1.2.1.2.2\" style=\"color:#590A0A;\">whom</span> <span class=\"ltx_text\" id=\"S3.T2.1.2.1.2.3\" style=\"color:#590A0A;\">he</span> <span class=\"ltx_text\" id=\"S3.T2.1.2.1.2.4\" style=\"color:#960F0F;\">talked</span> <span class=\"ltx_text\" id=\"S3.T2.1.2.1.2.5\" style=\"color:#6E0D0D;\">in</span> <span class=\"ltx_text\" id=\"S3.T2.1.2.1.2.6\" style=\"color:#540A0A;\">the</span> <span class=\"ltx_text\" id=\"S3.T2.1.2.1.2.7\" style=\"color:#FF1C1C;\">station</span> <span class=\"ltx_text\" id=\"S3.T2.1.2.1.2.8\" style=\"color:#730D0D;\">at</span> <span class=\"ltx_text\" id=\"S3.T2.1.2.1.2.9\" style=\"color:#A81212;\">fort</span> <span class=\"ltx_text\" id=\"S3.T2.1.2.1.2.10\" style=\"color:#BF1414;\">wayne</span> <span class=\"ltx_text\" id=\"S3.T2.1.2.1.2.11\" style=\"color:#ED1A1A;\">interested</span> <span class=\"ltx_text\" id=\"S3.T2.1.2.1.2.12\" style=\"color:#5E0A0A;\">him</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.3.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.3.2.1\">1820 to 1850</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.3.2.2\">\n<span class=\"ltx_text\" id=\"S3.T2.1.3.2.2.1\" style=\"color:#8C0F0F;\">with</span> <span class=\"ltx_text\" id=\"S3.T2.1.3.2.2.2\" style=\"color:#870F0F;\">whom</span> <span class=\"ltx_text\" id=\"S3.T2.1.3.2.2.3\" style=\"color:#6B0D0D;\">he</span> <span class=\"ltx_text\" id=\"S3.T2.1.3.2.2.4\" style=\"color:#CC1717;\">talked</span> <span class=\"ltx_text\" id=\"S3.T2.1.3.2.2.5\" style=\"color:#820F0F;\">in</span> <span class=\"ltx_text\" id=\"S3.T2.1.3.2.2.6\" style=\"color:#5E0A0A;\">the</span> <span class=\"ltx_text\" id=\"S3.T2.1.3.2.2.7\" style=\"color:#E31A1A;\">station</span> <span class=\"ltx_text\" id=\"S3.T2.1.3.2.2.8\" style=\"color:#870F0F;\">at</span> <span class=\"ltx_text\" id=\"S3.T2.1.3.2.2.9\" style=\"color:#BD1414;\">fort</span> <span class=\"ltx_text\" id=\"S3.T2.1.3.2.2.10\" style=\"color:#7A0D0D;\">wayne</span> <span class=\"ltx_text\" id=\"S3.T2.1.3.2.2.11\" style=\"color:#FF1C1C;\">interested</span> <span class=\"ltx_text\" id=\"S3.T2.1.3.2.2.12\" style=\"color:#540A0A;\">him</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.4.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.4.3.1\">1850 to 1880</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.4.3.2\">\n<span class=\"ltx_text\" id=\"S3.T2.1.4.3.2.1\" style=\"color:#730D0D;\">with</span> <span class=\"ltx_text\" id=\"S3.T2.1.4.3.2.2\" style=\"color:#7D0D0D;\">whom</span> <span class=\"ltx_text\" id=\"S3.T2.1.4.3.2.3\" style=\"color:#610A0A;\">he</span> <span class=\"ltx_text\" id=\"S3.T2.1.4.3.2.4\" style=\"color:#AD1212;\">talked</span> <span class=\"ltx_text\" id=\"S3.T2.1.4.3.2.5\" style=\"color:#7A0D0D;\">in</span> <span class=\"ltx_text\" id=\"S3.T2.1.4.3.2.6\" style=\"color:#540A0A;\">the</span> <span class=\"ltx_text\" id=\"S3.T2.1.4.3.2.7\" style=\"color:#B31414;\">station</span> <span class=\"ltx_text\" id=\"S3.T2.1.4.3.2.8\" style=\"color:#780D0D;\">at</span> <span class=\"ltx_text\" id=\"S3.T2.1.4.3.2.9\" style=\"color:#991212;\">fort</span> <span class=\"ltx_text\" id=\"S3.T2.1.4.3.2.10\" style=\"color:#C91717;\">wayne</span> <span class=\"ltx_text\" id=\"S3.T2.1.4.3.2.11\" style=\"color:#FF1C1C;\">interested</span> <span class=\"ltx_text\" id=\"S3.T2.1.4.3.2.12\" style=\"color:#590A0A;\">him</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.5.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.5.4.1\">1880 to 1910</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.5.4.2\">\n<span class=\"ltx_text\" id=\"S3.T2.1.5.4.2.1\" style=\"color:#850F0F;\">with</span> <span class=\"ltx_text\" id=\"S3.T2.1.5.4.2.2\" style=\"color:#820F0F;\">whom</span> <span class=\"ltx_text\" id=\"S3.T2.1.5.4.2.3\" style=\"color:#750D0D;\">he</span> <span class=\"ltx_text\" id=\"S3.T2.1.5.4.2.4\" style=\"color:#B01414;\">talked</span> <span class=\"ltx_text\" id=\"S3.T2.1.5.4.2.5\" style=\"color:#7A0D0D;\">in</span> <span class=\"ltx_text\" id=\"S3.T2.1.5.4.2.6\" style=\"color:#610A0A;\">the</span> <span class=\"ltx_text\" id=\"S3.T2.1.5.4.2.7\" style=\"color:#850F0F;\">station</span> <span class=\"ltx_text\" id=\"S3.T2.1.5.4.2.8\" style=\"color:#750D0D;\">at</span> <span class=\"ltx_text\" id=\"S3.T2.1.5.4.2.9\" style=\"color:#A61212;\">fort</span> <span class=\"ltx_text\" id=\"S3.T2.1.5.4.2.10\" style=\"color:#FF1C1C;\">wayne</span> <span class=\"ltx_text\" id=\"S3.T2.1.5.4.2.11\" style=\"color:#F71C1C;\">interested</span> <span class=\"ltx_text\" id=\"S3.T2.1.5.4.2.12\" style=\"color:#540A0A;\">him</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.6.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S3.T2.1.6.5.1\">1910 to 1940</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S3.T2.1.6.5.2\">\n<span class=\"ltx_text\" id=\"S3.T2.1.6.5.2.1\" style=\"color:#750D0D;\">with</span> <span class=\"ltx_text\" id=\"S3.T2.1.6.5.2.2\" style=\"color:#700D0D;\">whom</span> <span class=\"ltx_text\" id=\"S3.T2.1.6.5.2.3\" style=\"color:#610A0A;\">he</span> <span class=\"ltx_text\" id=\"S3.T2.1.6.5.2.4\" style=\"color:#8F0F0F;\">talked</span> <span class=\"ltx_text\" id=\"S3.T2.1.6.5.2.5\" style=\"color:#750D0D;\">in</span> <span class=\"ltx_text\" id=\"S3.T2.1.6.5.2.6\" style=\"color:#5C0A0A;\">the</span> <span class=\"ltx_text\" id=\"S3.T2.1.6.5.2.7\" style=\"color:#700D0D;\">station</span> <span class=\"ltx_text\" id=\"S3.T2.1.6.5.2.8\" style=\"color:#6B0D0D;\">at</span> <span class=\"ltx_text\" id=\"S3.T2.1.6.5.2.9\" style=\"color:#910F0F;\">fort</span> <span class=\"ltx_text\" id=\"S3.T2.1.6.5.2.10\" style=\"color:#FF1C1C;\">wayne</span> <span class=\"ltx_text\" id=\"S3.T2.1.6.5.2.11\" style=\"color:#B31414;\">interested</span> <span class=\"ltx_text\" id=\"S3.T2.1.6.5.2.12\" style=\"color:#540A0A;\">him</span>\n</td>\n</tr>\n</tbody>\n</table>", "caption": "Table 2: Normalized perplexities for different models, lighter red signifies higher surprisal.", "description": "This table displays the normalized perplexity scores for different language models across various time periods.  Lower scores indicate better model performance. The models include finetuned models with a Llama3 8B baseline and pretrained models with a BabyLlama2 baseline.  The data is presented visually using a color gradient, where lighter red shades represent higher perplexity (lower performance), and darker shades represent lower perplexity (higher performance). This visualization allows for easy comparison of model performance across different time periods and model types.", "section": "4 Results and discussion"}, {"content": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T3.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T3.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T3.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.1.1\">Model</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T3.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.2.1\">1750-1820</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T3.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.3.1\">1820-50</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T3.1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.4.1\">1850-80</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T3.1.1.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.5.1\">1880-1910</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T3.1.1.1.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.6.1\">1910-40</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T3.1.2.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T3.1.2.1.1\">pretrained</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T3.1.2.1.2\">0.67</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T3.1.2.1.3\">0.68</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T3.1.2.1.4\">0.69</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T3.1.2.1.5\">0.72</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T3.1.2.1.6\">0.72</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.3.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S4.T3.1.3.2.1\">finetuned</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S4.T3.1.3.2.2\">0.80</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S4.T3.1.3.2.3\">0.81</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S4.T3.1.3.2.4\">0.83</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S4.T3.1.3.2.5\">0.84</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S4.T3.1.3.2.6\">0.84</td>\n</tr>\n</tbody>\n</table>", "caption": "Table 3: Aggregate maximally filtered BLiMP accuracy across all timeslices.", "description": "This table presents the aggregate BLiMP accuracy scores for maximally filtered data across all five time slices (1750-1820, 1820-1850, 1850-1880, 1880-1910, 1910-1940).  It compares the performance of both the pretrained and finetuned models, showing the overall accuracy achieved by each model type across the different historical periods.  The maximally filtered data ensures that only words appearing in all models' vocabularies are included, providing a consistent comparison across time slices.", "section": "4 Results and discussion"}, {"content": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T4.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T4.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T4.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.1.1.1.1.1\">Model</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T4.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.1.1.1.2.1\">1750-1820</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T4.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.1.1.1.3.1\">1820-50</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T4.1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.1.1.1.4.1\">1850-80</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T4.1.1.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.1.1.1.5.1\">1880-1910</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T4.1.1.1.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.1.1.1.6.1\">1910-40</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T4.1.2.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T4.1.2.1.1\">pretrained</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T4.1.2.1.2\">0.00</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T4.1.2.1.3\">0.00</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T4.1.2.1.4\">0.33</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T4.1.2.1.5\">0.82</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T4.1.2.1.6\">0.91</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.3.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S4.T4.1.3.2.1\">finetuned</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S4.T4.1.3.2.2\">0.92</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S4.T4.1.3.2.3\">0.96</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S4.T4.1.3.2.4\">0.98</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S4.T4.1.3.2.5\">1.00</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S4.T4.1.3.2.6\">0.99</td>\n</tr>\n</tbody>\n</table>", "caption": "Table 4: Accuracy for maximally filtered BLiMP \u201donly NPI licenser present\u201d task across all timeslices. Our pretrained models begin to prefer \u201donly\u201d to \u201deven\u201d in later slices.", "description": "This table displays the accuracy scores achieved by both pretrained and finetuned models on the \"only NPI licenser present\" task within the BLiMP benchmark.  The task specifically assesses the models' ability to correctly identify and use \"only\" as a negative polarity item (NPI) licenser in different time slices representing historical language data.  The results highlight a key difference between pretrained and finetuned models: pretrained models gradually start showing a preference for using \"only\" over \"even\" as the NPI licenser in later historical time periods, while the finetuned models perform consistently well across all periods. This indicates the pretrained models better capture diachronic changes in grammatical usage over time.", "section": "Experiments"}, {"content": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T5.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T5.1.1.1\">\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T5.1.1.1.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T5.1.1.1.1.1\">\n<span class=\"ltx_p\" id=\"S4.T5.1.1.1.1.1.1\" style=\"width:113.8pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.1.1.1.1.1.1\">Sentence</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T5.1.1.1.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T5.1.1.1.2.1\">\n<span class=\"ltx_p\" id=\"S4.T5.1.1.1.2.1.1\" style=\"width:119.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.1.1.2.1.1.1\">Definition</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T5.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.1.1.3.1\">Year</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T5.1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.1.1.4.1\">Pretrain</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T5.1.1.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.1.1.5.1\">Finetune</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T5.1.2.1\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S4.T5.1.2.1.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T5.1.2.1.1.1\">\n<span class=\"ltx_p\" id=\"S4.T5.1.2.1.1.1.1\" style=\"width:113.8pt;\">I\u2019m going to sell my car\u2026 No more police towing [it] ..to a car <span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.2.1.1.1.1.1\">pound</span>.</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S4.T5.1.2.1.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T5.1.2.1.2.1\">\n<span class=\"ltx_p\" id=\"S4.T5.1.2.1.2.1.1\" style=\"width:119.5pt;\">A place in which vehicles impounded by the police or other authorities are kept\u2026</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T5.1.2.1.3\">1970</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T5.1.2.1.4\">101</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T5.1.2.1.5\">0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.1.3.2\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" id=\"S4.T5.1.3.2.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T5.1.3.2.1.1\">\n<span class=\"ltx_p\" id=\"S4.T5.1.3.2.1.1.1\" style=\"width:113.8pt;\">Hill \u2026 which won three gold and a <span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.3.2.1.1.1.1\">silver</span>.</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" id=\"S4.T5.1.3.2.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T5.1.3.2.2.1\">\n<span class=\"ltx_p\" id=\"S4.T5.1.3.2.2.1.1\" style=\"width:119.5pt;\">Elliptical for silver medal n.</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T5.1.3.2.3\">1960</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T5.1.3.2.4\">7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T5.1.3.2.5\">0</td>\n</tr>\n</tbody>\n</table>", "caption": "Table 5: Two examples for time slice 1750-1820 with their rank per model.", "description": "This table presents two examples of cloze tasks, where words are masked at the end of sentences.  The examples are drawn from the time slice 1750-1820. For each example, the table shows the sentence, its definition, the year the word sense first appeared, the rank of the correct completion for the pretrained model and the rank of the correct completion for the finetuned model. This allows for a comparison of the performance of the two model types on word prediction within a specific historical context.", "section": "4 Results and discussion"}, {"content": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T6.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T6.1.1.1\">\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T6.1.1.1.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T6.1.1.1.1.1\">\n<span class=\"ltx_p\" id=\"S4.T6.1.1.1.1.1.1\" style=\"width:136.6pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.1.1.1.1.1.1.1\">Sentence</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T6.1.1.1.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T6.1.1.1.2.1\">\n<span class=\"ltx_p\" id=\"S4.T6.1.1.1.2.1.1\" style=\"width:176.4pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.1.1.1.2.1.1.1\">Definition</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T6.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.1.1.1.3.1\">Sense Year</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T6.1.2.1\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t\" id=\"S4.T6.1.2.1.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T6.1.2.1.1.1\">\n<span class=\"ltx_p\" id=\"S4.T6.1.2.1.1.1.1\" style=\"width:136.6pt;\">They have nowhere to go. This is\u2014how do the Americans say it?\u2014the end of the <span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.1.2.1.1.1.1.1\">line</span>.</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t\" id=\"S4.T6.1.2.1.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T6.1.2.1.2.1\">\n<span class=\"ltx_p\" id=\"S4.T6.1.2.1.2.1.1\" style=\"width:176.4pt;\">V. A direction or course of movement. the end of the line ( transferred and figurative ). Cf. the end of the road at end n..</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T6.1.2.1.3\">1948</td>\n</tr>\n</tbody>\n</table>", "caption": "Table 6: The new sense of \u201dline\u201d is accepted by the finteuned (rank #1) and pretrained (#14).", "description": "Table 6 presents a comparison of how two different language models, a finetuned model and a pretrained model, ranked the word \"line\" in a cloze task. The new meaning of \"line\" being tested, as defined in the Oxford English Dictionary (OED), is \"the end of the line\" (figurative). This table showcases that while the finetuned model correctly identified the new meaning of \"line\", placing it at the highest rank, the pretrained model also recognized this sense and assigned it a relatively high rank (#14). This is notable as the pretrained model did not have access to as much data as the finetuned model.", "section": "4.2 BLIMP"}, {"content": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T7.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T7.1.1.1\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\" id=\"S4.T7.1.1.1.1\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S4.T7.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T7.1.1.1.2.1\">1750-1820</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T7.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T7.1.1.1.3.1\">1820-1850</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T7.1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T7.1.1.1.4.1\">1850-1880</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T7.1.1.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T7.1.1.1.5.1\">1880-1910</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T7.1.1.1.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T7.1.1.1.6.1\">1910-1940</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T7.1.2.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" id=\"S4.T7.1.2.1.1\">Pretrained</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" id=\"S4.T7.1.2.1.2\">41</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T7.1.2.1.3\">NaN</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T7.1.2.1.4\">NaN</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T7.1.2.1.5\">NaN</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T7.1.2.1.6\">NaN</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T7.1.3.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb\" id=\"S4.T7.1.3.2.1\">Finetuned</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb\" id=\"S4.T7.1.3.2.2\">18</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T7.1.3.2.3\">19</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T7.1.3.2.4\">11</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T7.1.3.2.5\">14</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T7.1.3.2.6\">11</td>\n</tr>\n</tbody>\n</table>", "caption": "Table 7: Rank of \u201dcholera\u201d completion. Llama3-8B ranks it 8, BabyLlama-2 ranks it 57. NaN indicates it is outside the top k.", "description": "This table presents the rank of the word \"cholera\" in the top k predictions generated by different language models when completing a sentence. Specifically, it shows the ranks for the pretrained models from five different time slices (1750-1820, 1820-1850, 1850-1880, 1880-1910, and 1910-1940).  For comparison, it also includes the ranks given by two larger baseline models: Llama3-8B and BabyLlama-2. The 'NaN' values indicate that the word \"cholera\" was not among the top k predictions for that specific model and time slice.", "section": "4.2 BLIMP"}, {"content": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"A1.T8.3\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A1.T8.3.4.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" id=\"A1.T8.3.4.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T8.3.4.1.1.1\">Hyperparameters (DoRA)</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A1.T8.3.4.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T8.3.4.1.2.1\">LLaMA3-8B</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T8.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"A1.T8.1.1.1\">Rank <math alttext=\"r\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T8.1.1.1.m1.1\"><semantics id=\"A1.T8.1.1.1.m1.1a\"><mi id=\"A1.T8.1.1.1.m1.1.1\" xref=\"A1.T8.1.1.1.m1.1.1.cmml\">r</mi><annotation-xml encoding=\"MathML-Content\" id=\"A1.T8.1.1.1.m1.1b\"><ci id=\"A1.T8.1.1.1.m1.1.1.cmml\" xref=\"A1.T8.1.1.1.m1.1.1\">\ud835\udc5f</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.T8.1.1.1.m1.1c\">r</annotation><annotation encoding=\"application/x-llamapun\" id=\"A1.T8.1.1.1.m1.1d\">italic_r</annotation></semantics></math>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T8.1.1.2\">16</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T8.2.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A1.T8.2.2.1\"><math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T8.2.2.1.m1.1\"><semantics id=\"A1.T8.2.2.1.m1.1a\"><mi id=\"A1.T8.2.2.1.m1.1.1\" xref=\"A1.T8.2.2.1.m1.1.1.cmml\">\u03b1</mi><annotation-xml encoding=\"MathML-Content\" id=\"A1.T8.2.2.1.m1.1b\"><ci id=\"A1.T8.2.2.1.m1.1.1.cmml\" xref=\"A1.T8.2.2.1.m1.1.1\">\ud835\udefc</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.T8.2.2.1.m1.1c\">\\alpha</annotation><annotation encoding=\"application/x-llamapun\" id=\"A1.T8.2.2.1.m1.1d\">italic_\u03b1</annotation></semantics></math></th>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T8.2.2.2\">32</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T8.3.5.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A1.T8.3.5.2.1\">Dropout</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T8.3.5.2.2\">0.05</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T8.3.6.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A1.T8.3.6.3.1\">Optimizer</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T8.3.6.3.2\">AdamW</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T8.3.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A1.T8.3.3.2\">LR</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T8.3.3.1\"><math alttext=\"1\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T8.3.3.1.m1.1\"><semantics id=\"A1.T8.3.3.1.m1.1a\"><mrow id=\"A1.T8.3.3.1.m1.1.1\" xref=\"A1.T8.3.3.1.m1.1.1.cmml\"><mn id=\"A1.T8.3.3.1.m1.1.1.2\" xref=\"A1.T8.3.3.1.m1.1.1.2.cmml\">1</mn><mo id=\"A1.T8.3.3.1.m1.1.1.1\" lspace=\"0.222em\" rspace=\"0.222em\" xref=\"A1.T8.3.3.1.m1.1.1.1.cmml\">\u00d7</mo><msup id=\"A1.T8.3.3.1.m1.1.1.3\" xref=\"A1.T8.3.3.1.m1.1.1.3.cmml\"><mn id=\"A1.T8.3.3.1.m1.1.1.3.2\" xref=\"A1.T8.3.3.1.m1.1.1.3.2.cmml\">10</mn><mrow id=\"A1.T8.3.3.1.m1.1.1.3.3\" xref=\"A1.T8.3.3.1.m1.1.1.3.3.cmml\"><mo id=\"A1.T8.3.3.1.m1.1.1.3.3a\" xref=\"A1.T8.3.3.1.m1.1.1.3.3.cmml\">\u2212</mo><mn id=\"A1.T8.3.3.1.m1.1.1.3.3.2\" xref=\"A1.T8.3.3.1.m1.1.1.3.3.2.cmml\">4</mn></mrow></msup></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A1.T8.3.3.1.m1.1b\"><apply id=\"A1.T8.3.3.1.m1.1.1.cmml\" xref=\"A1.T8.3.3.1.m1.1.1\"><times id=\"A1.T8.3.3.1.m1.1.1.1.cmml\" xref=\"A1.T8.3.3.1.m1.1.1.1\"></times><cn id=\"A1.T8.3.3.1.m1.1.1.2.cmml\" type=\"integer\" xref=\"A1.T8.3.3.1.m1.1.1.2\">1</cn><apply id=\"A1.T8.3.3.1.m1.1.1.3.cmml\" xref=\"A1.T8.3.3.1.m1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"A1.T8.3.3.1.m1.1.1.3.1.cmml\" xref=\"A1.T8.3.3.1.m1.1.1.3\">superscript</csymbol><cn id=\"A1.T8.3.3.1.m1.1.1.3.2.cmml\" type=\"integer\" xref=\"A1.T8.3.3.1.m1.1.1.3.2\">10</cn><apply id=\"A1.T8.3.3.1.m1.1.1.3.3.cmml\" xref=\"A1.T8.3.3.1.m1.1.1.3.3\"><minus id=\"A1.T8.3.3.1.m1.1.1.3.3.1.cmml\" xref=\"A1.T8.3.3.1.m1.1.1.3.3\"></minus><cn id=\"A1.T8.3.3.1.m1.1.1.3.3.2.cmml\" type=\"integer\" xref=\"A1.T8.3.3.1.m1.1.1.3.3.2\">4</cn></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.T8.3.3.1.m1.1c\">1\\times 10^{-4}</annotation><annotation encoding=\"application/x-llamapun\" id=\"A1.T8.3.3.1.m1.1d\">1 \u00d7 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T8.3.7.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A1.T8.3.7.4.1\">LR Scheduler</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T8.3.7.4.2\">Linear</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T8.3.8.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A1.T8.3.8.5.1\">Batch size</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T8.3.8.5.2\">16</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T8.3.9.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A1.T8.3.9.6.1\">Warmup Steps</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T8.3.9.6.2\">100</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T8.3.10.7\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A1.T8.3.10.7.1\">Epochs</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T8.3.10.7.2\">3</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T8.3.11.8\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"A1.T8.3.11.8.1\">Where</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A1.T8.3.11.8.2\">Q, K, V, Up, Down</td>\n</tr>\n</tbody>\n</table>", "caption": "Table 8: Hyperparameter configurations of DoRA for LLaMA3-8B.", "description": "This table details the hyperparameter settings used for the DoRA (Directly Optimized Rank Adaptation) finetuning technique applied to the LLaMA3-8B language model.  It lists specific values for parameters such as rank, alpha, dropout rate, optimizer, learning rate, learning rate scheduler, batch size, warmup steps, and the number of epochs.  Understanding these settings is crucial for replicating the experiment's results and analyzing the impact of DoRA on model performance.", "section": "3.2 Procedure"}, {"content": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"A1.T9.2\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A1.T9.2.3.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"A1.T9.2.3.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T9.2.3.1.1.1\">Hyperparameter</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A1.T9.2.3.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T9.2.3.1.2.1\">Value</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T9.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T9.1.1.2\">Learning rate</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T9.1.1.1\"><math alttext=\"7\\cdot 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T9.1.1.1.m1.1\"><semantics id=\"A1.T9.1.1.1.m1.1a\"><mrow id=\"A1.T9.1.1.1.m1.1.1\" xref=\"A1.T9.1.1.1.m1.1.1.cmml\"><mn id=\"A1.T9.1.1.1.m1.1.1.2\" xref=\"A1.T9.1.1.1.m1.1.1.2.cmml\">7</mn><mo id=\"A1.T9.1.1.1.m1.1.1.1\" lspace=\"0.222em\" rspace=\"0.222em\" xref=\"A1.T9.1.1.1.m1.1.1.1.cmml\">\u22c5</mo><msup id=\"A1.T9.1.1.1.m1.1.1.3\" xref=\"A1.T9.1.1.1.m1.1.1.3.cmml\"><mn id=\"A1.T9.1.1.1.m1.1.1.3.2\" xref=\"A1.T9.1.1.1.m1.1.1.3.2.cmml\">10</mn><mrow id=\"A1.T9.1.1.1.m1.1.1.3.3\" xref=\"A1.T9.1.1.1.m1.1.1.3.3.cmml\"><mo id=\"A1.T9.1.1.1.m1.1.1.3.3a\" xref=\"A1.T9.1.1.1.m1.1.1.3.3.cmml\">\u2212</mo><mn id=\"A1.T9.1.1.1.m1.1.1.3.3.2\" xref=\"A1.T9.1.1.1.m1.1.1.3.3.2.cmml\">4</mn></mrow></msup></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A1.T9.1.1.1.m1.1b\"><apply id=\"A1.T9.1.1.1.m1.1.1.cmml\" xref=\"A1.T9.1.1.1.m1.1.1\"><ci id=\"A1.T9.1.1.1.m1.1.1.1.cmml\" xref=\"A1.T9.1.1.1.m1.1.1.1\">\u22c5</ci><cn id=\"A1.T9.1.1.1.m1.1.1.2.cmml\" type=\"integer\" xref=\"A1.T9.1.1.1.m1.1.1.2\">7</cn><apply id=\"A1.T9.1.1.1.m1.1.1.3.cmml\" xref=\"A1.T9.1.1.1.m1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"A1.T9.1.1.1.m1.1.1.3.1.cmml\" xref=\"A1.T9.1.1.1.m1.1.1.3\">superscript</csymbol><cn id=\"A1.T9.1.1.1.m1.1.1.3.2.cmml\" type=\"integer\" xref=\"A1.T9.1.1.1.m1.1.1.3.2\">10</cn><apply id=\"A1.T9.1.1.1.m1.1.1.3.3.cmml\" xref=\"A1.T9.1.1.1.m1.1.1.3.3\"><minus id=\"A1.T9.1.1.1.m1.1.1.3.3.1.cmml\" xref=\"A1.T9.1.1.1.m1.1.1.3.3\"></minus><cn id=\"A1.T9.1.1.1.m1.1.1.3.3.2.cmml\" type=\"integer\" xref=\"A1.T9.1.1.1.m1.1.1.3.3.2\">4</cn></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.T9.1.1.1.m1.1c\">7\\cdot 10^{-4}</annotation><annotation encoding=\"application/x-llamapun\" id=\"A1.T9.1.1.1.m1.1d\">7 \u22c5 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T9.2.4.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T9.2.4.2.1\">Number of epochs</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T9.2.4.2.2\">8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T9.2.5.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T9.2.5.3.1\">Batch size</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T9.2.5.3.2\">128</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T9.2.6.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T9.2.6.4.1\">Weight decay</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T9.2.6.4.2\">5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T9.2.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"A1.T9.2.2.1\">Distillation <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T9.2.2.1.m1.1\"><semantics id=\"A1.T9.2.2.1.m1.1a\"><mi id=\"A1.T9.2.2.1.m1.1.1\" xref=\"A1.T9.2.2.1.m1.1.1.cmml\">\u03b1</mi><annotation-xml encoding=\"MathML-Content\" id=\"A1.T9.2.2.1.m1.1b\"><ci id=\"A1.T9.2.2.1.m1.1.1.cmml\" xref=\"A1.T9.2.2.1.m1.1.1\">\ud835\udefc</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.T9.2.2.1.m1.1c\">\\alpha</annotation><annotation encoding=\"application/x-llamapun\" id=\"A1.T9.2.2.1.m1.1d\">italic_\u03b1</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"A1.T9.2.2.2\">0.5</td>\n</tr>\n</tbody>\n</table>", "caption": "Table 9: Training and distillation hyperparameters of\nBabyLlama-2", "description": "This table lists the hyperparameters used during the training and distillation process of the BabyLlama-2 model.  It includes details such as the learning rate, number of epochs, batch size, weight decay, and the distillation alpha value. These parameters are crucial in controlling the training process and achieving optimal model performance. The table provides a concise summary of the key settings used to fine-tune the BabyLlama-2 language model, which is vital for reproducibility and understanding the model's training methodology.", "section": "3 Experiments"}, {"content": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"A1.T10.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A1.T10.1.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"A1.T10.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T10.1.1.1.1.1\">Hyperparameter</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A1.T10.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T10.1.1.1.2.1\">Value</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T10.1.2.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T10.1.2.2.1\">Vocabulary size</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T10.1.2.2.2\">16,000</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T10.1.3.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T10.1.3.3.1\">Number of layers</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T10.1.3.3.2\">32</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T10.1.4.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T10.1.4.4.1\">Number of heads</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T10.1.4.4.2\">15</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T10.1.5.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T10.1.5.5.1\">Number of KV heads</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T10.1.5.5.2\">5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T10.1.6.6\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T10.1.6.6.1\">Embedding dimension</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T10.1.6.6.2\">960</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T10.1.7.7\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T10.1.7.7.1\">Hidden dimension</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T10.1.7.7.2\">2560</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T10.1.8.8\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A1.T10.1.8.8.1\">Total parameters</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A1.T10.1.8.8.2\">345M</td>\n</tr>\n</tbody>\n</table>", "caption": "Table 10: BabyLlama-2 Model Architecture.", "description": "This table details the architecture of the BabyLlama-2 language model, including the vocabulary size, number of layers, number of heads, number of key/value heads, embedding dimension, hidden dimension, and total number of parameters.", "section": "3 Experiments"}, {"content": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"A2.T11.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A2.T11.1.1.1\">\n<td class=\"ltx_td ltx_border_tt\" id=\"A2.T11.1.1.1.1\"></td>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"A2.T11.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T11.1.1.1.2.1\">+/-1</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"A2.T11.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T11.1.1.1.3.1\">+/-10</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"A2.T11.1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T11.1.1.1.4.1\">DQ +/-1</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"A2.T11.1.1.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T11.1.1.1.5.1\">DQ +/-10</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T11.1.2.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A2.T11.1.2.2.1\">Llama3.3-70B</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A2.T11.1.2.2.2\">0.63</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A2.T11.1.2.2.3\">0.81</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A2.T11.1.2.2.4\">0.70</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A2.T11.1.2.2.5\">0.88</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T11.1.3.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"A2.T11.1.3.3.1\">GPT-4</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A2.T11.1.3.3.2\">0.74</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A2.T11.1.3.3.3\">0.89</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A2.T11.1.3.3.4\">0.87</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A2.T11.1.3.3.5\">0.99</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T11.1.4.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A2.T11.1.4.4.1\">GPT-4o</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A2.T11.1.4.4.2\">0.82</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A2.T11.1.4.4.3\">0.84</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A2.T11.1.4.4.4\">0.96</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A2.T11.1.4.4.5\">0.94</td>\n</tr>\n</tbody>\n</table>", "caption": "Table 11: Performance on work date attribution per LLM. +/- indicates year delta tolerance threshold, DQ indicates that extreme variations from the ground scores (+/-50) were not considered", "description": "This table presents the performance of three different Large Language Models (LLMs) on a work date attribution task.  The LLMs were evaluated on their ability to correctly predict the publication year of various works, using different tolerance levels.  The tolerance levels represent acceptable differences in years between the LLM's prediction and the actual publication year.  A tolerance of +/-1 year means the LLM's prediction must be within one year of the actual year, whereas a tolerance of +/-10 years allows a difference of up to 10 years. The 'DQ' column indicates whether or not predictions with very large errors (more than +/-50 years from the ground truth) were excluded in calculating the results. This helps evaluate how sensitive the models are to extreme errors.", "section": "3 Experiments"}, {"content": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"A3.T12.2\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"A3.T12.2.3.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"A3.T12.2.3.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"A3.T12.2.3.1.1.1\">Band</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"A3.T12.2.3.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A3.T12.2.3.1.2.1\">Freq./mil.</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"A3.T12.2.3.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"A3.T12.2.3.1.3.1\">% in OED</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A3.T12.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" id=\"A3.T12.1.1.2\">8</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A3.T12.1.1.1\">\n<math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T12.1.1.1.m1.1\"><semantics id=\"A3.T12.1.1.1.m1.1a\"><mo id=\"A3.T12.1.1.1.m1.1.1\" xref=\"A3.T12.1.1.1.m1.1.1.cmml\">&gt;</mo><annotation-xml encoding=\"MathML-Content\" id=\"A3.T12.1.1.1.m1.1b\"><gt id=\"A3.T12.1.1.1.m1.1.1.cmml\" xref=\"A3.T12.1.1.1.m1.1.1\"></gt></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A3.T12.1.1.1.m1.1c\">&gt;</annotation><annotation encoding=\"application/x-llamapun\" id=\"A3.T12.1.1.1.m1.1d\">&gt;</annotation></semantics></math>1,000</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A3.T12.1.1.3\">0.02%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T12.2.4.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"A3.T12.2.4.1.1\">7</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T12.2.4.1.2\">100 \u2013 1,000</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T12.2.4.1.3\">0.18%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T12.2.5.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"A3.T12.2.5.2.1\">6</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T12.2.5.2.2\">10 \u2013 100</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T12.2.5.2.3\">1%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T12.2.6.3\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"A3.T12.2.6.3.1\">5</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T12.2.6.3.2\">1 \u2013 10</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T12.2.6.3.3\">4%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T12.2.7.4\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"A3.T12.2.7.4.1\">4</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T12.2.7.4.2\">0.1 \u2013 1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T12.2.7.4.3\">11%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T12.2.8.5\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"A3.T12.2.8.5.1\">3</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T12.2.8.5.2\">0.01 \u2013 0.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T12.2.8.5.3\">20%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T12.2.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"A3.T12.2.2.2\">2</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T12.2.2.1\">\n<math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T12.2.2.1.m1.1\"><semantics id=\"A3.T12.2.2.1.m1.1a\"><mo id=\"A3.T12.2.2.1.m1.1.1\" xref=\"A3.T12.2.2.1.m1.1.1.cmml\">&lt;</mo><annotation-xml encoding=\"MathML-Content\" id=\"A3.T12.2.2.1.m1.1b\"><lt id=\"A3.T12.2.2.1.m1.1.1.cmml\" xref=\"A3.T12.2.2.1.m1.1.1\"></lt></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A3.T12.2.2.1.m1.1c\">&lt;</annotation><annotation encoding=\"application/x-llamapun\" id=\"A3.T12.2.2.1.m1.1d\">&lt;</annotation></semantics></math>0.01</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T12.2.2.3\">45%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T12.2.9.6\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb\" id=\"A3.T12.2.9.6.1\">1</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A3.T12.2.9.6.2\">\u2013</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A3.T12.2.9.6.3\">18%</td>\n</tr>\n</tbody>\n</table>", "caption": "Table 12: Word Frequency Bands and their respective counts per a million words and the percentage of non-obsolete OED entries", "description": "This table shows the frequency distribution of words in the corpus used for the study.  It categorizes words into eight bands based on their frequency per million words, ranging from very frequent words (appearing more than 1,000 times per million words) to very infrequent words (appearing less than once per million words).  For each band, it presents the number of words falling into that category and also shows the percentage of these words that are not marked as obsolete in the Oxford English Dictionary (OED). This provides context on the relative rarity and currency of words within the corpus.", "section": "3 Experiments"}]