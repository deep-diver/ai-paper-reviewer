[{"figure_path": "https://arxiv.org/html/2504.07083/x2.png", "caption": "Figure 1: Overview.\nTop: DataDoP data construction.\nGiven RGB video frames, we extract RGBD images and camera poses, then tag the pose sequence with different motion categories (in different colors). With LLM, we generate two types of captions from motion tags and RGBD inputs: Motion Caption describes the camera movements, while Directorial Caption describes the camera movements along with their interaction with the scene and directorial intent. Bottom: Our GenDoP method supports multi-modal inputs for trajectory creation. The generated camera sequence can be easily applied to various video generation tasks, including text-to-video (T2V)\u00a0[13] and image-to-video (I2V) generation\u00a0[15]. GenDoP paves the way for future advancements in camera-controlled video generation.", "description": "This figure provides a high-level overview of the GenDoP system. The top half illustrates the DataDoP dataset creation process: starting with RGB video frames, RGBD images and camera poses are extracted.  The pose sequence is then annotated with different motion categories.  These motion tags, along with the RGBD data, are fed into a Large Language Model (LLM) to generate two types of captions: 'Motion Captions', which purely describe the camera movement, and 'Directorial Captions', which describe the camera movement in the context of the scene and the director's artistic intent. The bottom half shows the GenDoP model, which takes multi-modal inputs (text and optionally RGBD data) to generate a camera trajectory. This trajectory can then be used in various video generation tasks, such as text-to-video and image-to-video.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2504.07083/x3.png", "caption": "(a) Distribution of Translation and Rotation Motion Tags.", "description": "This figure visualizes the distribution of motion tags in the DataDoP dataset, specifically breaking down the frequency of different translation and rotation movements captured within the video data.  The bar chart displays the occurrence counts for various combinations of translation (lateral, vertical, depth) and rotation (pitch, yaw, roll).  This provides insights into the types and prevalence of camera movements in the dataset, demonstrating its diversity and complexity.", "section": "3. DataDoP Dataset"}, {"figure_path": "https://arxiv.org/html/2504.07083/x4.png", "caption": "(b) Diverse Trajectories.", "description": "The figure shows multiple camera trajectories generated from the same caption. This demonstrates that the model produces diverse trajectories in terms of their length, direction, and speed, showcasing the model's ability to generate varied results while adhering to the input instructions.", "section": "3. DataDoP Dataset"}, {"figure_path": "https://arxiv.org/html/2504.07083/x5.png", "caption": "Figure 2: Dataset Statistics. (a) The figure illustrates the composition and distribution of 27 translation motions (left) and 7 rotation motions (right), emphasizing the complexity and diversity of trajectories in our DataDoP dataset.\n(b) Based on the same caption, our dataset includes diverse trajectories that still conform to the given caption. As shown in the figure, the trajectories exhibit variations in terms of length, direction, and speed, effectively showcasing the diversity within our dataset.", "description": "Figure 2 presents a statistical overview of the DataDoP dataset, highlighting the variety and complexity of camera movements.  Panel (a) shows histograms illustrating the distribution of 27 distinct translational camera motions (left) and 7 rotational motions (right). This visualization emphasizes the rich range of camera maneuvers captured in the dataset. Panel (b) provides an example demonstrating that multiple diverse trajectories can be generated from the same textual description. This illustrates variations in trajectory length, direction, and speed, further showcasing the dataset's capacity to represent nuanced and expressive camera movement.", "section": "3. DataDoP Dataset"}, {"figure_path": "https://arxiv.org/html/2504.07083/x6.png", "caption": "Figure 3: Our Auto-regressive Generation Model. Our model supports multi-modal inputs and generates trajectories based on these inputs. By treating the task as an auto-regressive next-token prediction problem, the model sequentially generates trajectories, with each new pose prediction influenced by previous camera states and input conditions.", "description": "GenDoP, an autoregressive model for camera trajectory generation, takes multi-modal inputs (text, optional RGBD image) to generate camera trajectories.  The model treats trajectory generation as a sequential, next-token prediction problem, where each new camera pose is predicted based on previous poses and the input. This approach ensures temporal and spatial coherence in the generated trajectories.", "section": "4. Method"}, {"figure_path": "https://arxiv.org/html/2504.07083/x7.png", "caption": "Figure 4: Qualitative Results of Text-conditioned Trajectory Generation. We offer a comparative analysis of text-conditioned trajectory generation in the figure. Our model\u2019s trajectories (color-coded to highlight text alignment) remain stable and closely follow the instructions, while other models exhibit significant jitter or fail to match the instructions well.", "description": "Figure 4 presents a qualitative comparison of text-to-trajectory generation results across different models.  The top row shows example input captions. The subsequent rows display the generated camera trajectories by each model in response to each caption.  The trajectories generated by GenDoP (the authors' model) are shown in color to highlight their close adherence to the input text instructions.  In contrast, other methods, including previously published models like CCD and E.T., and the DataDoP trained version of Director3D, exhibit significantly more unstable camera paths, frequently deviating from or failing to accurately reflect the specified instructions. This visual comparison showcases GenDoP's superior ability to produce smooth, accurate, and instructionally faithful camera movements.", "section": "5. Qualitative Results"}, {"figure_path": "https://arxiv.org/html/2504.07083/x8.png", "caption": "Figure 5: Qualitative Results of RGBD & Text-conditioned Generation. This figure compares the impact of incorporating RGBD input on trajectory generation under identical text conditions. While both models generate command-compliant trajectories, the RGBD & Text-conditioned model demonstrates superior scene adaptation by utilizing RGBD data to integrate geometric and contextual constraints.", "description": "This figure showcases a comparison of two trajectory generation methods: one using only text input and another using both text and RGBD (depth) information.  Both methods were given the same text prompts to generate camera trajectories. The results demonstrate that while both produce trajectories that follow the instructions, the model incorporating RGBD data shows significantly better understanding of the scene. The RGBD model produces trajectories that are more naturally integrated within the scene's geometry and context. This improved scene awareness is achieved through the incorporation of depth information, allowing the model to more accurately reflect the physical layout and relationships within the scene, leading to more realistic camera movements. The text-only model, while following commands, sometimes produces movements that lack a natural integration within the scene.", "section": "5. Qualitative Results"}, {"figure_path": "https://arxiv.org/html/2504.07083/x9.png", "caption": "(a) Shot Length.", "description": "This histogram shows the distribution of video shot lengths in the DataDoP dataset. The x-axis represents the duration of shots in seconds, and the y-axis represents the number of shots with that duration.  The distribution provides insights into the typical length of shots used in the dataset and aids in understanding the diversity of the data.", "section": "3. DataDoP Dataset"}, {"figure_path": "https://arxiv.org/html/2504.07083/x10.png", "caption": "(b) Trajectory Scale.", "description": "This histogram shows the distribution of trajectory scales in the DataDoP dataset.  The x-axis represents the scale of the trajectory, and the y-axis represents the number of trajectories with that scale.  The scale likely refers to a normalized measure of the spatial extent or the total distance covered by the camera during the shot. A wider distribution indicates greater diversity in the extent and size of camera movements in the dataset.", "section": "3. DataDoP Dataset"}, {"figure_path": "https://arxiv.org/html/2504.07083/x11.png", "caption": "Figure R1: Dataset Statistics in terms of video shot length and trajectory scale.", "description": "This figure presents two histograms visualizing the distribution of video shot lengths and trajectory scales within the DataDoP dataset.  The first histogram shows the frequency of shots across different durations, providing insights into the temporal characteristics of the dataset. The second histogram displays the distribution of trajectory scales, illustrating the range of camera movements captured, from small, localized movements to larger, more extensive camera paths.", "section": "3. DataDoP Dataset"}]