---
title: "OLMoTrace: Tracing Language Model Outputs Back to Trillions of Training Tokens"
summary: "OLMOTRACE: Tracing LM outputs back to training tokens."
categories: ["AI Generated", "ü§ó Daily Papers"]
tags: ["Natural Language Processing", "Large Language Models", "üè¢ Allen Institute for AI",]
showSummary: true
date: 2025-04-09
draft: false
---

<br>

{{< keywordList >}}
{{< keyword icon="fingerprint" >}} 2504.07096 {{< /keyword >}}
{{< keyword icon="writer" >}} Jiacheng Liu et el. {{< /keyword >}}
 
{{< keyword >}} ü§ó 2025-04-10 {{< /keyword >}}
 
{{< /keywordList >}}

{{< button href="https://arxiv.org/abs/2504.07096" target="_self" >}}
‚Üó arXiv
{{< /button >}}
{{< button href="https://huggingface.co/papers/2504.07096" target="_self" >}}
‚Üó Hugging Face
{{< /button >}}



<audio controls>
    <source src="https://ai-paper-reviewer.com/2504.07096/podcast.wav" type="audio/wav">
    Your browser does not support the audio element.
</audio>


### TL;DR


{{< lead >}}

Large language models are trained on massive datasets. It is hard to understand why they generate certain responses. Existing tracing methods cannot be scaled to multi-trillion token setting due to computation needs. This paper introduces a system for tracing language models back to their training data to address these issues. 



OLMOTRACE finds verbatim matches between language model output and documents. An extended version of infini-gram indexes training data. It also introduces a novel parallel algorithm speeds the matching process. The system allows users to explore fact-checking, creative expression, and math skills of language models.

{{< /lead >}}


#### Key Takeaways

{{< alert "star" >}}
{{< typeit speed=10 lifeLike=true >}} OLMOTRACE enables tracing language model outputs to their training data in real-time. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=1000 lifeLike=true >}} The system uses an extended version of infini-gram to achieve fast tracing results. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=2000 lifeLike=true >}} OLMOTRACE supports exploring fact-checking, hallucination, and creativity of language models. {{< /typeit >}}
{{< /alert >}}

#### Why does it matter?
This research introduces a tool to understand **how language models learn from vast datasets**. It allows researchers to investigate fact-checking, hallucination, and creativity, opening avenues for understanding model behavior and improving reliability. It impacts research in interpretability and responsible AI development.

------
#### Visual Insights



![](https://arxiv.org/html/2504.07096/extracted/6349197/images/pipeline.png)

> üîº The figure showcases OLMOTrace, a system that traces language model outputs back to their training data.  The left panel displays an example of OLMoTrace in action on the Ai2 Playground.  A response from the language model OLMo is shown with segments highlighted; brighter highlights represent more relevant matches to training data, and darker highlights represent less relevant matches.  Each highlighted segment links to its source document within the model's training data. The right panel shows that clicking on the 'View Document' button displays the source document with extended context, allowing for a deeper investigation of where the model learned specific phrases.
> <details>
> <summary>read the caption</summary>
> Figure 1:  OLMoTrace on Ai2 Playground. Left: On a response generated by OLMo, OLMoTrace highlights text spans found verbatim in the model‚Äôs training data and shows their source documents. Brighter highlights indicate spans from more relevant training documents, while darker highlights denote less relevant ones. Right: When user clicks the ‚ÄúView Document‚Äù button, the document is shown with extended context. Try OLMoTrace at https://playground.allenai.org.
> </details>





{{< table-caption >}}
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S2.T1.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S2.T1.1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S2.T1.1.1.1.1.1" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.1.1.1">Stage</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S2.T1.1.1.1.1.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.1.2.1">Dataset</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S2.T1.1.1.1.1.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.1.3.1"># Docs</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S2.T1.1.1.1.1.4" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.1.4.1"># Tokens</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T1.1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S2.T1.1.1.2.1.1" style="padding-left:4.0pt;padding-right:4.0pt;">pre-training</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S2.T1.1.1.2.1.2" style="padding-left:4.0pt;padding-right:4.0pt;"><a class="ltx_ref ltx_href" href="https://huggingface.co/datasets/allenai/olmo-mix-1124" title="">allenai/olmo-mix-1124</a></th>
<td class="ltx_td ltx_align_right ltx_border_t" id="S2.T1.1.1.2.1.3" style="padding-left:4.0pt;padding-right:4.0pt;">3081 M</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S2.T1.1.1.2.1.4" style="padding-left:4.0pt;padding-right:4.0pt;">4575 B</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.1.1.3.2.1" style="padding-left:4.0pt;padding-right:4.0pt;">mid-training</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.1.1.3.2.2" style="padding-left:4.0pt;padding-right:4.0pt;"><a class="ltx_ref ltx_href" href="https://huggingface.co/datasets/allenai/dolmino-mix-1124" title="">allenai/dolmino-mix-1124</a></th>
<td class="ltx_td ltx_align_right" id="S2.T1.1.1.3.2.3" style="padding-left:4.0pt;padding-right:4.0pt;">81 M</td>
<td class="ltx_td ltx_align_right" id="S2.T1.1.1.3.2.4" style="padding-left:4.0pt;padding-right:4.0pt;">34 B</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.1.1.4.3.1" style="padding-left:4.0pt;padding-right:4.0pt;">post-training</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.1.1.4.3.2" style="padding-left:4.0pt;padding-right:4.0pt;">
<a class="ltx_ref ltx_href" href="https://huggingface.co/datasets/allenai/tulu-3-sft-olmo-2-mixture-0225" title="">SFT</a> &amp; <a class="ltx_ref ltx_href" href="https://huggingface.co/datasets/allenai/olmo-2-0325-32b-preference-mix" title="">DPO</a> &amp; <a class="ltx_ref ltx_href" href="https://huggingface.co/datasets/allenai/RLVR-GSM-MATH-IF-Mixed-Constraints" title="">RLVR</a>
</th>
<td class="ltx_td ltx_align_right" id="S2.T1.1.1.4.3.3" style="padding-left:4.0pt;padding-right:4.0pt;">1.7 M</td>
<td class="ltx_td ltx_align_right" id="S2.T1.1.1.4.3.4" style="padding-left:4.0pt;padding-right:4.0pt;">1.6 B</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S2.T1.1.1.5.4.1" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.5.4.1.1">Total</span></th>
<th class="ltx_td ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S2.T1.1.1.5.4.2" style="padding-left:4.0pt;padding-right:4.0pt;"></th>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S2.T1.1.1.5.4.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.5.4.3.1">3164 M</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S2.T1.1.1.5.4.4" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.5.4.4.1">4611 B</span></td>
</tr>
</tbody>
</table>{{< /table-caption >}}

> üîº This table details the composition of the training data used for the OLMo-2-32B-Instruct language model, specifically showing the number of documents and tokens in the pre-training, mid-training, and post-training datasets. Note that for the mid-training data, any sources that were already present in the pre-training data were excluded from the statistics and the index used for matching by OLMOTrace.  The total number of documents and tokens across all stages is also provided, showing the massive scale of the model's training data.
> <details>
> <summary>read the caption</summary>
> Table 1:  The full training data of OLMo-2-32B-Instruct, which OLMoTrace matches against. For mid-training data, we excluded sources that already appeared in the pre-training data, from both the statistics and the index.
> </details>





### In-depth insights


#### Trace LM Output
While the paper doesn't explicitly have a section titled 'Trace LM Output,' OLMOTRACE is all about tracing language model outputs back to their training data, which is a novel approach. The system facilitates **understanding LM behavior** by revealing verbatim matches between model output and training documents. This has major implications for **fact-checking**, and understanding sources of creative output. The very core is about revealing the connections between what a model generates and what it has ingested during training, a vital step in model interpretability. OLMOTRACE opens up opportunities to evaluate and enhance the quality of training datasets and their impacts.

#### Maximal Spans
The paper addresses the challenge of finding "Maximal Spans" in the output of large language models, which are verbatim sequences from the training data. **Maximality ensures that identified spans are not simply substrings of longer matches**, providing more meaningful connections between model output and training data. The paper tackles the computational intensity of identifying these spans, which naively scales quadratically with the length of the LM output. They propose fast algorithm leverages suffix array indexing and parallel processing to reduce time complexity and latency, achieving results within seconds. This efficient computation enables real-time tracing and interaction with the model.

#### Relevance Rank
The 'Relevance Rank' aspect centers on enhancing user experience by prioritizing and presenting the most pertinent documents retrieved by OLMOTRACE. The process involves re-ranking documents to display those that are most closely related to both the user's query and the LM's response. This is achieved through a BM25 scoring mechanism, treating the retrieved documents as a corpus and the combined user prompt and LM response as the query. **This focuses on topical relevance** and enables efficient computation. The **documents are classified into high, medium, and low relevance levels**, indicated by a colored sidebar, aiding users in quickly identifying the most valuable information.

#### Training Data
The research paper emphasizes the significance of the training data utilized for language models (LMs), particularly in the context of OLMOTRACE.  It states that the three supported OLMo models are trained on the same pre-training and mid-training data, along with varying post-training data. OLMOTRACE matches against the entirety of an LM's training data. The training data for OLMo-2-32B-Instruct totals 3.2 billion documents and 4.6 trillion tokens.  This underscores the scale of data involved.  The authors provide statistics and note that other OLMo models have similar data sizes.  **Access to comprehensive training data is critical for tracing outputs** and understanding model behaviors.

#### Data vs. Model
The tension between data and models is central to modern AI. More data often improves model performance, but data quality, bias, and relevance are crucial. **Data-centric AI** focuses on improving data rather than model architecture. However, sophisticated models can extract more from the same data, revealing nuanced patterns. Choosing between improving data or the model is a complex decision, often depending on the specific task, dataset characteristics, and available resources. A balanced approach, considering both data quality and model complexity, typically yields the best results. **Regularization techniques** in models can reduce the impact of noisy data, but can also result in underfitting. Models that may be **robust on small data** may prove to be useless on the grand scale data when it comes to **edge cases**.


### More visual insights

<details>
<summary>More on figures
</summary>


![](https://arxiv.org/html/2504.07096/extracted/6349197/images/span.png)

> üîº This figure illustrates the five-step inference pipeline of OLMOTrace.  The process begins with identifying maximal matching spans within the language model (LM) output that are also present in the training data. These spans are then filtered to retain only long and unique spans, which are those that are less frequent in the training data.  Next, the system retrieves the documents from the training data that contain these spans.  Overlapping spans and documents are merged to reduce redundancy. Finally, the retrieved documents are ranked and colored based on their relevance to the LM output and user prompt, ensuring that the most relevant ones are highlighted. The caption mentions slight adjustments were made to the highlighted spans and document relevance for better visualization.
> <details>
> <summary>read the caption</summary>
> Figure 2:  The OLMoTrace inference pipeline, as described in ¬ß3. For better illustration, we slightly adjusted the highlighted spans and document relevance from the actual example.
> </details>



![](https://arxiv.org/html/2504.07096/extracted/6349197/images/usage_spaceneedle.png)

> üîº OLMOTrace efficiently finds maximal matching spans in a massive training dataset by leveraging the infini-gram index.  For each suffix in the language model's output, it identifies the longest matching prefix within the training data using a single, highly optimized 'Find' query. This process is parallelized across all suffixes, significantly speeding up the computation. A final step removes any spans that are not maximal (i.e., completely contained within another span).
> <details>
> <summary>read the caption</summary>
> Figure 3:  Computation of the maximal matching spans (¬ß3.1). For each suffix of the LM output, OLMoTrace computes its longest matching prefix (color-underlined) with a single Find query on the infini-gram index of the LM training data. All suffixes of the LM output are processed in parallel. Finally, non-maximal spans are suppressed.
> </details>



![](https://arxiv.org/html/2504.07096/extracted/6349197/images/usage_poem.png)

> üîº The figure shows an example of OLMOTRACE being used for fact-checking.  A language model (LM) output claims that the Space Needle was built for the 1962 World's Fair. OLMOTRACE highlights this claim and links it to a specific document within the LM's training data. The user can click the link to access the source document, which in this case provides evidence supporting the LM's statement. The figure helps to illustrate how OLMOTRACE allows for verification of facts generated by LMs by tracing them back to the original sources in their training data, thereby increasing the transparency of the LM's reasoning.
> <details>
> <summary>read the caption</summary>
> (a)  Fact checking: Inspecting the document (and its source URL) helps verify the factual claim made in the span.
> </details>



![](https://arxiv.org/html/2504.07096/extracted/6349197/images/usage_arithmetics.png)

> üîº This figure demonstrates how OLMOTRACE can identify the origin of seemingly novel phrases generated by large language models (LLMs).  By highlighting verbatim matches between the LLM's output and segments within its training data, it shows that even phrases perceived as creative or original often have direct counterparts in the training corpus.  This illustrates that LLM creativity is heavily influenced by its training data and that originality may be a matter of recombining existing phrases in novel ways rather than a demonstration of genuinely original thought.
> <details>
> <summary>read the caption</summary>
> (b)  Tracing ‚Äúcreative‚Äù expressions: Matching spans reveal potential source of LM-generated ‚Äúcreative‚Äù expressions.
> </details>



</details>




<details>
<summary>More on tables
</summary>


{{< table-caption >}}
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A3.T2.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A3.T2.1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="A3.T2.1.1.1.1.1" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="A3.T2.1.1.1.1.1.1">Score</span></th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="A3.T2.1.1.1.1.2" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.T2.1.1.1.1.2.1">
<span class="ltx_p" id="A3.T2.1.1.1.1.2.1.1" style="width:320.0pt;"><span class="ltx_text ltx_font_bold" id="A3.T2.1.1.1.1.2.1.1.1">Description</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A3.T2.1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A3.T2.1.1.2.1.1" style="padding-left:4.0pt;padding-right:4.0pt;">0</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A3.T2.1.1.2.1.2" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.T2.1.1.2.1.2.1">
<span class="ltx_p" id="A3.T2.1.1.2.1.2.1.1" style="width:320.0pt;">The snippet or context of the snippet is about a different topic than the query and model response (though possibly semantically similar): 
<br class="ltx_break"/>For example, for the query breast cancer symptoms, give a 0 to: 
<br class="ltx_break"/>‚ÄÉA snippet about heart attack symptoms ‚Äì wrong topic 
<br class="ltx_break"/>‚ÄÉA snippet about brain cancer symptoms ‚Äì may not necessarily apply to breast cancer symptoms</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A3.T2.1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A3.T2.1.1.3.2.1" style="padding-left:4.0pt;padding-right:4.0pt;">1</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A3.T2.1.1.3.2.2" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.T2.1.1.3.2.2.1">
<span class="ltx_p" id="A3.T2.1.1.3.2.2.1.1" style="width:320.0pt;">The snippet or context of the snippet is about a broader topic than the query and model response, or is potentially relevant but there‚Äôs not enough information: 
<br class="ltx_break"/>For example, for the query breast cancer symptoms, give a 1 to: 
<br class="ltx_break"/>‚ÄÉA snippet about cancer in general ‚Äì missing key specifics of symptoms</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A3.T2.1.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A3.T2.1.1.4.3.1" style="padding-left:4.0pt;padding-right:4.0pt;">2</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A3.T2.1.1.4.3.2" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.T2.1.1.4.3.2.1">
<span class="ltx_p" id="A3.T2.1.1.4.3.2.1.1" style="width:320.0pt;">The snippet or context of the snippet is on the right topic of the query and model response, but is in a slightly different context or is too specific to fit the exact query: 
<br class="ltx_break"/>For example, for the query breast cancer symptoms, give a 2 to: 
<br class="ltx_break"/>‚ÄÉA snippet referring a breast cancer treatment side effect</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A3.T2.1.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="A3.T2.1.1.5.4.1" style="padding-left:4.0pt;padding-right:4.0pt;">3</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" id="A3.T2.1.1.5.4.2" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.T2.1.1.5.4.2.1">
<span class="ltx_p" id="A3.T2.1.1.5.4.2.1.1" style="width:320.0pt;">The snippet or context of the snippet is about a subject that is a direct match, in topic and scope, of the most likely user intent for the query and model response: 
<br class="ltx_break"/>For example, for the query breast cancer symptoms, give a 3 to: 
<br class="ltx_break"/>‚ÄÉA snippet discussing a symptom specific to breast cancer</span>
</span>
</td>
</tr>
</tbody>
</table>{{< /table-caption >}}
> üîº This table presents two parts: the first part details the rubric used for human evaluation of document relevance, providing a Likert scale (0-3) with descriptions for each score level to ensure consistent judgment.  The second part shows the prompt used for automatically evaluating document relevance using an LLM (Large Language Model) as a judge, outlining the scoring criteria and instructions to be used by the LLM.  This combined approach allows for both human judgment and automated assessment of document relevance, enhancing the objectivity and efficiency of the evaluation process. 
> <details>
> <summary>read the caption</summary>
> Table 2:  Left: Rubrics for document relevance evaluation. Right: Prompt for automatically evaluating document relevance with LLM-as-a-Judge.
> </details>

{{< table-caption >}}
<table class="ltx_tabular ltx_align_middle" id="A3.T2.2.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A3.T2.2.1.1.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="A3.T2.2.1.1.1.1" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.T2.2.1.1.1.1.1">
<span class="ltx_p" id="A3.T2.2.1.1.1.1.1.1" style="width:230.0pt;"><span class="ltx_text ltx_font_bold" id="A3.T2.2.1.1.1.1.1.1.1">LLM-as-a-Judge Prompt</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A3.T2.2.1.2.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A3.T2.2.1.2.2.1" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A3.T2.2.1.2.2.1.1">
<span class="ltx_p" id="A3.T2.2.1.2.2.1.1.1" style="width:230.0pt;">You will be given a user prompt, a model‚Äôs response to the prompt, and a retrieved document. Please rate how relevant the document is to the prompt and model response. Rate on a scale of 0 (not relevant) to 3 (very relevant). Respond with a single number, and do not include any other text in your response.

<br class="ltx_break"/>
<br class="ltx_break"/>Rubric for rating:

<br class="ltx_break"/>0: The document is about a different topic than the prompt and model response.

<br class="ltx_break"/>1. The document is about a broader topic than the prompt and model response, or is potentially relevant but there‚Äôs not enough information.

<br class="ltx_break"/>2. The document is on the right topic of the prompt and model response, but is in a slightly different context or is too specific.

<br class="ltx_break"/>3. The document is about a subject that is a direct match, in topic and scope, of the most likely user intent for the prompt and model response.

<br class="ltx_break"/>
<br class="ltx_break"/>Prompt: {prompt}

<br class="ltx_break"/>Model response: {response}

<br class="ltx_break"/>Retrieved document: {document}</span>
</span>
</td>
</tr>
</tbody>
</table>{{< /table-caption >}}
> üîº This table presents the results of an evaluation measuring the relevance of the top documents retrieved by the OLMoTrace system.  Relevance is scored on a Likert scale (0-3), where 0 indicates completely unrelated and 3 indicates highly relevant.  The percentage of relevant documents (those with a score of 2 or 3) is also shown.  The evaluation uses an LLM-as-a-Judge (gpt-4o-2024-08-06) for most results; however, the last row shows scores from human evaluators, allowing for a comparison between automated and human assessment of relevance.
> <details>
> <summary>read the caption</summary>
> Table 3:  Evaluating the relevance level of top documents displayed by OLMoTrace. Avg score is on a likert scale of 0-3, where 0 is ‚Äúunrelated‚Äù and 3 is ‚Äúhighly relevant‚Äù. For % relevant, we consider a document as relevant if it gets a score of 2 or 3. We use LLM-as-a-Judge with gpt-4o-2024-08-06, except in the last row where we collect annotation from a human expert.
> </details>

</details>




### Full paper

{{< gallery >}}
<img src="https://ai-paper-reviewer.com/2504.07096/1.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.07096/2.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.07096/3.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.07096/4.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.07096/5.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.07096/6.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.07096/7.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.07096/8.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.07096/9.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
{{< /gallery >}}