[{"figure_path": "https://arxiv.org/html/2504.07046/x1.png", "caption": "Figure 1: An example of subject-driven image editing with human-annotated low scores.\nBoth traditional metrics and GPT-4o-based VIEScore assign high scores.\nBy integrating GPT-4o with tools, CIGEval, our agentic evaluation framework, highlights the glasses object in both images, and finds their different shapes and designs, thereby reaching the correct score. \u201cSource\u201d and \u201cSubject\u201d means \u201csource image\u201d and \u201csubject image\u201d.", "description": "This figure illustrates a subject-driven image editing task where the goal is to replace the glasses in a source image with those from a subject image.  Human evaluators gave this a low score, but traditional metrics and even the GPT-40-based VIEScore system gave it a high score, highlighting a limitation of those methods.  The CIGEVAL framework, however, uses additional tools (in this case, presumably object recognition and comparison) to compare the shapes and designs of the glasses in both images. By detecting these differences, CIGEVAL provides a more accurate and nuanced assessment, resulting in a lower score that aligns with human evaluation.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2504.07046/x6.png", "caption": "Figure 2: The evaluation process of CIGEval regarding the example in Figure\u00a01. CIGEval autonomously selects appropriate tools for each decomposed sub-task, and then conducts fine-grained analyses based on the observed tool outputs.", "description": "This figure illustrates the CIGEVAL evaluation framework. It shows how CIGEVAL, an AI agent, processes an image evaluation task.  First, the task is decomposed into smaller sub-tasks (e.g., evaluating whether the image follows the instructions, assessing the quality of edits).  Then, CIGEVAL autonomously selects the most appropriate tool from its toolbox for each sub-task.  These tools perform specific analyses on the image.  Finally, CIGEVAL integrates the results from all these analyses to generate a comprehensive and nuanced evaluation score, closely mimicking human judgment. The example shown is based on Figure 1.", "section": "3 CIGEVAL"}, {"figure_path": "https://arxiv.org/html/2504.07046/x7.png", "caption": "Figure 3: Case study of a text-guided image editing example with a low human annotation score.", "description": "This figure showcases a text-guided image editing example where the automated evaluation metric, VIEScore, gives a high score (8/10), while the human annotator gives a much lower score (2/10).  The discrepancy highlights the limitations of existing metrics in capturing subtle nuances of image edits.  The example involves adding water and flowers to a bathtub. VIEScore overlooks the unnatural placement of the added elements, while CIGEVAL identifies and highlights the over-editing issue through a fine-grained analysis using its agentic framework and toolset, leading to a more accurate assessment.", "section": "3.4 Agent Tuning"}, {"figure_path": "https://arxiv.org/html/2504.07046/x8.png", "caption": "Figure 4: Case study of GPT-4o\u2019s image generation. Examples are adapted from OpenAI\u2019s official website.", "description": "This figure showcases three examples of GPT-4's image generation capabilities, using cases taken directly from OpenAI's official website.  Each example highlights a different aspect of GPT-4's performance. The first shows successful text-guided image editing, accurately adding a detective hat and monocle to a cat. The second demonstrates text-to-image generation, with the model attempting to produce a periodic table; however, imperfections in the generated table highlight the limitations in accurately representing nuanced and detailed information. The third example shows that GPT-4 struggles with a text-guided image editing task requiring a reversal of steps in a multi-step process.  This illustrates both the successes and limitations of GPT-4's image generation abilities, emphasizing that performance is task-dependent.", "section": "4.6 Case Study"}, {"figure_path": "https://arxiv.org/html/2504.07046/x9.png", "caption": "Figure 5: Case study of a multi-concept image composition example. Here is the fine-grained score for concept consistency.", "description": "This figure showcases a case study of multi-concept image composition using the CIGEVAL framework.  It compares the evaluation results of VIESCORE and CIGEVAL for a task involving generating an image of a teddy bear holding a flower, given two source images (teddy bear and flower).  CIGEVAL leverages its toolbox, specifically the grounding and highlight tools, to perform a more detailed analysis than VIESCORE, focusing on the fine-grained aspects such as color and shape similarity between the generated and source flowers. The visual demonstrates that CIGEVAL provides a more nuanced and accurate evaluation, resulting in a lower score compared to VIESCORE due to its detection of discrepancies between the generated flower and the source image.", "section": "3. CIGEVAL"}, {"figure_path": "https://arxiv.org/html/2504.07046/x10.png", "caption": "Figure 6: Case study of GPT-4o\u2019s image generation. Examples are taken from ImagenHub\u2019s control-guided image generation task.", "description": "This figure showcases three examples of images generated by GPT-40 for the control-guided image generation task within the ImagenHub benchmark.  Each example displays the prompt, the generated image, the CIGEval score and rationale, and the human evaluation score.  The goal is to illustrate how CIGEVAL assesses images generated under specific control instructions, highlighting its ability to detect subtle differences in adherence to the guidance. The control guidance used varies across the examples, demonstrating the framework's versatility in evaluating different control mechanisms.", "section": "4.6 Case Study"}, {"figure_path": "https://arxiv.org/html/2504.07046/x11.png", "caption": "Figure 7: Case study of GPT-4o\u2019s image generation. Examples are taken from ImagenHub\u2019s multi-concept image composition task.", "description": "Figure 7 showcases three examples from the ImagenHub benchmark's multi-concept image composition task, highlighting GPT-4's performance in generating images based on multiple input concepts.  Each example shows the input concepts (e.g., a teddy bear, a flower, a cat), and the generated image. The goal of the task is to combine the concepts in a meaningful and visually coherent way, which assesses the model's ability to integrate diverse concepts into a singular output.", "section": "4.6 Case Study"}, {"figure_path": "https://arxiv.org/html/2504.07046/x12.png", "caption": "Figure 8: Case study of GPT-4o\u2019s image generation. Examples are taken from ImagenHub\u2019s subject-driven image editing task.", "description": "This figure showcases three examples from the ImagenHub benchmark's subject-driven image editing task, illustrating GPT-4's performance in this specific area. Each example presents the source image, the subject image to be incorporated, the generated image produced by GPT-4, and the evaluations from both CIGEVAL and human raters.  The scores for subject consistency and background preservation are provided for both automated and human assessments, allowing for a direct comparison of the results and highlighting any discrepancies. This visual representation serves to exemplify the detailed, fine-grained analysis capabilities of CIGEVAL, as opposed to other more generalized metrics.", "section": "4.6 Case Study"}, {"figure_path": "https://arxiv.org/html/2504.07046/x13.png", "caption": "Figure 9: Case study of GPT-4o\u2019s image generation. Examples are taken from ImagenHub\u2019s subject-driven image generation task.", "description": "Figure 9 presents three examples from the ImagenHub benchmark's subject-driven image generation task, showcasing GPT-4's performance. Each example shows the source image (subject), the generated image by GPT-4, the CIGEVAL scores, and the human evaluation scores.  The examples illustrate variations in prompt following and subject consistency, highlighting the nuanced evaluation capabilities of CIGEVAL.", "section": "4.6 Case Study"}, {"figure_path": "https://arxiv.org/html/2504.07046/x14.png", "caption": "Figure 10: Case study of GPT-4o\u2019s image generation. Examples are taken from ImagenHub\u2019s text-guided image generation task.", "description": "This figure showcases three examples of images generated by GPT-40 using text prompts in a text-guided image generation task from the ImagenHub benchmark.  Each example demonstrates GPT-40's ability to generate images corresponding to simple object descriptions (a black banana, a blue bird and brown bear, and a red car). The results highlight GPT-40's capacity to fulfill straightforward generation requests successfully.  This is particularly useful in illustrating how the model performs on basic tasks and highlights its accuracy in understanding and visualising simple textual inputs.", "section": "4.6 Case Study"}, {"figure_path": "https://arxiv.org/html/2504.07046/x15.png", "caption": "Figure 11: Case study of GPT-4o\u2019s image generation. Examples are taken from ImagenHub\u2019s text-guided image editing task.", "description": "This figure showcases three examples of text-guided image editing using GPT-40,  demonstrating the model's ability to follow instructions.  Each example shows the original image, the edited image generated by GPT-40, the CIGEval score, and the human evaluation score. The examples illustrate various levels of success in the editing task, highlighting the nuances of automatically evaluating AI-generated images.", "section": "4 Experiments"}]