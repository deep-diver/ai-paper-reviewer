{"importance": "This paper is vital for **LLM reasoning research**, highlighting overthinking flaws when premises are missing. It inspires **robust training strategies** and **critical thinking** in AI, pushing LLMs towards more reliable reasoning. The study prompts further exploration into balancing thoroughness with efficiency in AI reasoning models.", "summary": "Reasoning LLMs overthink with missing premises, hindering critical skill!", "takeaways": ["Reasoning LLMs generate longer responses for ill-posed questions, indicating a test-time scaling law contradiction.", "Non-reasoning models handle missing premise questions better, showcasing a robustness to critical info absence.", "Reasoning models often recognize missing premises but hesitate to stop, wasting computation on ineffective reasoning."], "tldr": "This paper identifies that LLMs, when faced with questions that lack necessary information (**missing premises or MiP**), tend to drastically increase their response length. This **MiP-Overthinking** issue leads to redundant, ineffective reasoning, contradicting the expected test-time scaling law. Surprisingly, LLMs not specifically trained for reasoning perform better in **MiP scenarios**, quickly identifying ill-posed queries. This highlights a flaw in current reasoning LLM training, which inadequately fosters efficient thinking. \n\nTo address this, the paper analyzes reasoning length, patterns, and critical thinking locations in various LLMs. The results improve the understanding of overthinking and offer new perspectives for problem mitigation. Key to this is the discovery that overthinking can spread through the **distillation of reasoning models\u2019 responses**. This highlights the need for efficient thinking encouragement in future LLM training recipes.", "affiliation": "University of Maryland", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}, "podcast_path": "2504.06514/podcast.wav"}