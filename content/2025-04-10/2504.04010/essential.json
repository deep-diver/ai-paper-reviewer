{"importance": "This work introduces a novel approach to listener head generation, enhancing realism and control through a video diffusion model. It contributes to AI-driven human interaction, offering improvements in visual fidelity, temporal coherence, and customizable behavior. This is important for virtual avatars, human-computer interaction, and social robotics.", "summary": "DiTaiListener: Controllable high-fidelity listener video generation achieved through diffusion models, improving realism and expressiveness.", "takeaways": ["DiTaiListener generates high-fidelity listener videos directly in pixel space, enhancing realism and detail.", "The CTM-Adapter aligns listener motions with speaker cues, improving temporal consistency.", "The model supports text-guided customization, allowing users to control listener behavior through prompts."], "tldr": "Generating natural listener motions is challenging due to limitations in visual fidelity and expressive richness. Existing methods often rely on low-dimensional motion codes, restricting the realism and nuanced expressions of listener behaviors. The challenge lies in creating listener responses that are well-timed, detailed, and controllable to produce natural and engaging interactions.\n\nTo address these challenges, **DiTaiListener**, introduces a video diffusion model with multimodal conditions for generating listener responses. It uses a Causal Temporal Multimodal Adapter to process auditory and visual cues, ensuring temporally coherent reactions. Additionally, a transition refinement model ensures seamless transitions between video segments. **DiTaiListener** achieves state-of-the-art performance in photorealism and motion representation with user preference for feedback, diversity, and smoothness.", "affiliation": "University of Southern California", "categories": {"main_category": "Multimodal Learning", "sub_category": "Multimodal Generation"}, "podcast_path": "2504.04010/podcast.wav"}