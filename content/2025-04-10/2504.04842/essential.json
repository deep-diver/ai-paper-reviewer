{"importance": "This paper presents a novel approach to generate realistic and coherent talking portraits. It contributes to the state-of-the-art diffusion model, identity preservation and the multi-modal audio-visual learning. These developments provide a more comprehensive and natural talking head generation.", "summary": "FantasyTalking: Generates realistic talking portraits with coherent motion via dual audio-visual alignment and facial-focused identity preservation.", "takeaways": ["Introduces a dual-stage audio-visual alignment strategy for coherent motion and precise lip sync.", "Employs facial-focused cross-attention for robust identity preservation.", "Integrates motion intensity modulation for controllable expression and body movement."], "tldr": "Creating realistic and controllable talking avatars from static portraits is challenging due to difficulties in capturing subtle expressions, body movements, and dynamic backgrounds. Existing methods often fall short in producing coherent and natural-looking animations. The realism of generated videos remains unsatisfactory due to neglecting of facial expressions and body movements while contextual objects remain static.\n\nThis paper introduces a new framework that uses a pre-trained video diffusion transformer model to generate high-fidelity talking portraits. At the core is a dual-stage audio-visual alignment strategy: first, establishing coherent global motion, then refining lip movements at the frame level. **It employs a facial-focused cross-attention module to maintain identity** and a motion intensity modulation module for controllable movements. The approach achieves higher quality, coherence, and identity preservation.", "affiliation": "AMAP, Alibaba Group", "categories": {"main_category": "Computer Vision", "sub_category": "Video Understanding"}, "podcast_path": "2504.04842/podcast.wav"}