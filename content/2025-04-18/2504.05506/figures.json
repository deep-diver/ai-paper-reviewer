[{"figure_path": "https://arxiv.org/html/2504.05506/extracted/6351499/emnlp2020-templates/imgs/chartqapro/performance_drop_comparison.png", "caption": "Figure 1: Performance gap between ChartQA Masry et\u00a0al. (2022) and\nChartQAPro for various LVLMs.", "description": "This figure compares the performance of various Large Vision-Language Models (LVLMs) on two chart question answering (CQA) benchmarks: ChartQA and the newly proposed ChartQAPro.  It highlights the significant performance drop observed when transitioning from ChartQA to the more challenging ChartQAPro benchmark for each of the listed LVLMs. This illustrates the increased complexity and diversity of ChartQAPro compared to ChartQA.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2504.05506/x1.png", "caption": "Figure 2: \nChartQAPro covers a more diverse range of questions compared to existing chart question answering datasets (Table\u00a01), providing an extensive evaluation of chart understanding abilities.", "description": "Figure 2 showcases the enhanced diversity of CHARTQAPRO compared to existing chart question answering datasets.  It visually demonstrates the wider variety of question types included in CHARTQAPRO, such as mathematical reasoning, visual reasoning, conversational questions, hypothetical questions, fact-checking, and unanswerable questions, highlighting its improved ability to comprehensively assess chart understanding capabilities. Each question type is exemplified by a sample chart and associated question, revealing CHARTQAPRO's broader scope and complexity.", "section": "3 THE CHARTQAPRO BENCHMARK"}, {"figure_path": "https://arxiv.org/html/2504.05506/x2.png", "caption": "Figure 3: \nChartQAPro Dataset Construction Process", "description": "The figure illustrates the three main stages of the CHARTQAPRO dataset creation process.  Stage 1 involves collecting chart images from diverse online sources such as Pew Research, Tableau, and others, as well as through web crawling. This collection prioritizes visual and topical diversity. Stage 2 focuses on generating question-answer pairs using a human-VLM collaboration approach. Seed questions are initially created by human annotators, and then expanded upon using several large language models (VLMs), followed by human refinement to ensure accuracy and clarity. Finally, Stage 3 involves a comprehensive review of the generated question-answer pairs to ensure high-quality annotations, employing several annotators for this purpose. This iterative process helps ensure the dataset covers a broad range of chart types, question styles, and complexities.", "section": "3.1 Dataset Construction"}, {"figure_path": "https://arxiv.org/html/2504.05506/x3.png", "caption": "Figure 4: Distribution of topics per source in\nChartQAPro. The inner ring represents online sources, while the outer ring shows topic distribution for each source.", "description": "Figure 4 is a donut chart visualizing the sources of chart images within the CHARTQAPRO dataset and the topic distribution for each source. The inner ring displays the proportion of charts from different sources (WebCharts, Tableau, Pew, OWID, PPIC), while the outer ring shows the percentage of charts pertaining to various topics (Economy, Health, Politics, Technology, Environment, etc.) within each source. This double-ring structure allows for a clear comparison of both the origins of the charts and the subject matter they represent, highlighting the diversity of the CHARTQAPRO dataset.", "section": "3.2 Dataset Analysis"}, {"figure_path": "https://arxiv.org/html/2504.05506/x4.png", "caption": "Figure 5: Sample errors across three categories: Visual Perception, Instruction Following, and Math Reasoning.", "description": "Figure 5 shows examples of common errors made by large language models (LLMs) when answering questions about charts.  These examples are categorized into three types of errors: visual perception errors, instruction-following errors, and math reasoning errors. Each example displays the original chart image, the question asked, the model's response, and the ground truth (correct) answer. Visual perception errors illustrate the difficulty LLMs have in accurately interpreting visual elements of the chart, such as data labels or values. Instruction-following errors highlight the models' struggle with complex reasoning processes, particularly when asked to perform multiple steps or apply chain-of-thought reasoning. Math reasoning errors demonstrate the difficulties models encounter when executing mathematical calculations based on chart data.", "section": "4.5 Qualitative Analysis"}, {"figure_path": "https://arxiv.org/html/2504.05506/x5.png", "caption": "Figure 6: Example of chart images collected from different sources and their corresponding QA pairs in \nChartQAPro.", "description": "This figure displays examples of chart images sourced from various online platforms and integrated into the ChartQAPro dataset. Each chart image is paired with corresponding question-answer pairs, showcasing the diversity of data visualizations and question types within the benchmark. The sources include Pew Research, Our World in Data (OWID), Public Policy Institute of California (PPIC), Tableau Public, and web-crawled charts, representing a broad spectrum of data visualizations and real-world scenarios.", "section": "3.1 Dataset Construction"}, {"figure_path": "https://arxiv.org/html/2504.05506/x6.png", "caption": "Figure 7: More examples of different question types in \nChartQAPro.", "description": "Figure 7 presents a diverse set of chart question-answering examples from the CHARTQAPRO benchmark.  Each example showcases a different question type, including mathematical reasoning, visual reasoning, fact-checking, hypothetical questions, conversational questions, and multi-chart questions. The visual diversity of the charts and the range of cognitive skills needed to answer the questions highlight the complexity of the CHARTQAPRO benchmark.", "section": "3 THE CHARTQAPRO BENCHMARK"}, {"figure_path": "https://arxiv.org/html/2504.05506/x7.png", "caption": "Figure 8: Examples of different charts related to major topics, i.e., \u2018Politics\u2019, \u2018Environment\u2019, \u2018Economy\u2019, \u2018Health\u2019, \u2018Technology\u2019, \u2018International Affairs\u2019 etc. in \nChartQAPro.", "description": "Figure 8 showcases examples of diverse chart types from ChartQAPro, categorized by major themes (Politics, Environment, Economy, Health, Technology, and International Affairs).  Each chart visual represents a real-world data visualization style and complexity, demonstrating the varied data representations and question types found within the CHARTQAPRO benchmark.  The figure highlights the breadth of visual and topical diversity incorporated into the dataset.", "section": "3.2 Dataset Analysis"}, {"figure_path": "https://arxiv.org/html/2504.05506/x8.png", "caption": "Figure 9: Examples of VLM-assisted question-and-answer pairs, where: (a) the VLM generates a question along with a correct answer, marked in Green text, (b) the VLM generates a question, but the answer is incorrect, marked in Red text.", "description": "Figure 9 showcases two instances of VLM-generated question-answer pairs. In (a), the VLM successfully generates a question and provides the correct answer, highlighted in green.  Conversely, in (b), the VLM generates a question with an incorrect answer, indicated in red. This figure illustrates the VLM's capability to produce questions but also highlights its susceptibility to errors in answer generation.", "section": "4.5 Qualitative Analysis"}, {"figure_path": "https://arxiv.org/html/2504.05506/extracted/6351499/emnlp2020-templates/imgs/chartqapro/boxplot_distance.png", "caption": "Figure 10: Box plot of pairwise cosine distances among chart images. ChartQAPro exhibits a higher median and consistently larger distances, indicating significantly greater visual diversity.", "description": "This box plot visualizes the pairwise cosine distances calculated between the chart images in three different datasets: ChartQA, CharXiv, and ChartQAPro.  The y-axis represents the cosine distance, a measure of similarity; a higher distance indicates greater dissimilarity or visual diversity between images. The plot shows that ChartQAPro has a significantly higher median cosine distance compared to the other two datasets, with a larger spread of distances.  This demonstrates that ChartQAPro's chart images are visually more diverse than those found in ChartQA and CharXiv.", "section": "3.2 Dataset Analysis"}, {"figure_path": "https://arxiv.org/html/2504.05506/extracted/6351499/emnlp2020-templates/imgs/chartqapro/linguistic_diversity_comparison_1.png", "caption": "Figure 11: Linguistic Diversity Comparison Across Datasets. The figure shows lexical diversity (TTR) and semantic diversity (cosine distance) for ChartQA, Chartxiv, and ChartQAPro. Higher TTR and semantic diversity indicate richer vocabulary and broader semantic coverage. ChartQAPro exhibits the highest diversity.", "description": "Figure 11 presents a comparison of lexical and semantic diversity across three chart question answering datasets: ChartQA, ChartXiv, and ChartQAPro. Lexical diversity, measured by the Type-Token Ratio (TTR), reflects the richness of vocabulary. Semantic diversity, calculated using average pairwise cosine distances between word embeddings, indicates the breadth of semantic coverage. The bar chart visually represents the TTR for each dataset, showing ChartQAPro having the highest value, signifying a richer vocabulary. The second bar chart displays the semantic diversity (cosine distance), again demonstrating that ChartQAPro has the highest value, indicating broader semantic coverage than the other two datasets.", "section": "3.2 Dataset Analysis"}, {"figure_path": "https://arxiv.org/html/2504.05506/x9.png", "caption": "Figure 12: Sample errors across three categories: Visual Perception, Instruction Following (CoT, PoT, Direct), and Mathematical Reasoning.", "description": "Figure 12 showcases examples of typical errors made by various large language models (LLMs) when processing chart data. The errors are categorized into three groups: visual perception issues (where the model fails to correctly interpret the visual elements of the chart), instruction-following errors (where the model fails to understand and execute the given instructions, regardless of whether the instructions are provided directly or as chain-of-thought or program-of-thought prompts), and mathematical reasoning errors (where the model fails to perform calculations and other mathematical operations correctly based on the chart data). Each example includes a brief description of the error, the model that made the error, the prompt style used, the ground truth (correct) answer, and the model's incorrect response. The figure is designed to illustrate common challenges and weaknesses of current LLMs in chart question answering tasks, highlighting areas where improvement is needed.", "section": "4.5 Qualitative Analysis"}, {"figure_path": "https://arxiv.org/html/2504.05506/x10.png", "caption": "Figure 13: Sample errors from open-source models across different categories in \nChartQAPro.", "description": "Figure 13 showcases instances where open-source large vision-language models (LVLMs) encountered difficulties across various question types within the CHARTQAPRO benchmark.  These examples highlight common errors, such as misinterpreting visual information (visual perception), failing to follow instructions correctly (instruction following), and struggling with mathematical calculations (math reasoning). The figure is divided into sub-sections by question category and error type.  Each sub-section displays a representative chart image, the associated question, the model's response, and the correct answer.  The visualizations demonstrate the challenges open-source LVLMs face when dealing with complex chart data and diverse question styles.", "section": "4.5 Qualitative Analysis"}]