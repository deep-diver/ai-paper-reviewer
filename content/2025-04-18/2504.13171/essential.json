{"importance": "This paper is important as it introduces **a novel approach to reduce the computational cost and latency** in LLMs, **making them more efficient for real-world applications**. By introducing 'sleep-time compute', the research **opens new avenues for optimizing LLM performance** and addresses critical limitations in test-time compute.", "summary": "Sleep-time compute: Reduce LLM cost and latency by pre-computing offline before queries!", "takeaways": ["Sleep-time compute significantly reduces test-time compute requirements, achieving up to 5x reduction in computation while maintaining accuracy.", "Scaling sleep-time compute can further improve accuracy, with gains of up to 18% on stateful reasoning tasks.", "Amortizing sleep-time compute across multiple related queries can substantially decrease the average cost per query, making LLMs more efficient for stateful applications."], "tldr": "Current LLMs rely on test-time compute, causing high latency and cost. Applying test-time compute assumes that problems are stateless, which incurs additional latency and cost. This is redundant when dealing with stateful applications that often reuse context. In reality, many LLM apps are stateful, working with persisted, reused context, such as question-answering, coding agents, and conversational assistants. \n\nThis paper introduces \"sleep-time compute,\" a method where models \"think\" offline about contexts before queries. By predicting user queries and pre-computing useful quantities, test-time compute needs are reduced. Experiments on Stateful GSM-Symbolic and AIME show sleep-time compute can reduce test-time compute by ~5x while boosting accuracy up to 18%. For multiple queries related to the same context, this method decreases the cost per query by 2.5x and is most effective when user queries are predictable.", "affiliation": "UC Berkeley", "categories": {"main_category": "Machine Learning", "sub_category": "Deep Learning"}, "podcast_path": "2504.13171/podcast.wav"}