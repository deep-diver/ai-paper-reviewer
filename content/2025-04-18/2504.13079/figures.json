[{"figure_path": "https://arxiv.org/html/2504.13079/x1.png", "caption": "Figure 1: An example from our RamDocs dataset (left) illustrating multiple sources of conflict in retrieved documents. Conflict may occur because of ambiguity in the query (in this case, different referents for Michael Jordan), but also because of misinformation from incorrect documents and noise from irrelevant ones. Madam-RAG (right) addresses this through multi-agent debate, where each agent summarizes and represents the information in one document. Agents discuss their responses across multiple rounds, with the final answers being combined via an aggregator module that summarizes the discussion.", "description": "The figure illustrates the challenges posed by the RAMDocs dataset and the MADAM-RAG solution.  The left side shows an example from the RAMDocs dataset where a simple query, \"In which year was Michael Jordan born?\", retrieves documents with conflicting information. This conflict arises from three sources: (1) ambiguity (different people named Michael Jordan), (2) misinformation (a document providing a factually incorrect birth year), and (3) noise (irrelevant documents). The right side depicts the MADAM-RAG approach, which uses multiple agents, each summarizing a single document's information, to engage in a multi-round debate and resolve this conflicting evidence. An aggregator then synthesizes a final coherent answer from the debate.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2504.13079/x2.png", "caption": "Figure 2: Madam-RAG operates by assigning each document to a separate agent.\nAgents are responsible for summarizing and representing their documents, and engage in a multi-agent, multi-round debate with each other to filter out misinformation and noise and address ambiguity.\nAn aggregator then summarizes the discussion into a single response.", "description": "The figure illustrates the MADAM-RAG architecture.  Each retrieved document is assigned to a separate LLM agent. These agents independently summarize and represent the information in their assigned document.  The agents then engage in a multi-round debate to identify and resolve inconsistencies, filter out misinformation and noise, and address ambiguity in the input. Finally, an aggregator module synthesizes the discussions from all agents to produce a single, coherent response.", "section": "4 MADAM-RAG: Multi-agent Debate for Ambiguity and Misinformation in RAG"}, {"figure_path": "https://arxiv.org/html/2504.13079/x3.png", "caption": "Figure 3: Ablation study on the importance of aggregator and multiple rounds of discussion.", "description": "This ablation study analyzes the effects of the aggregator module and the number of debate rounds in the MADAM-RAG model on the FaithEval and RAMDocs datasets.  It demonstrates how both the aggregator and multiple rounds of debate contribute to improved performance in handling conflicting information, misinformation, and ambiguity. The results show that the aggregator significantly enhances performance, especially in earlier rounds, by synthesizing evidence and suppressing unreliable information. Increasing the number of debate rounds further refines the model's responses and reduces errors, highlighting the iterative nature of the debate process.  The analysis shows the tradeoff between precision and recall, indicating the importance of precision in scenarios where incorrect answers are harmful.", "section": "6 Ablations and Analysis"}, {"figure_path": "https://arxiv.org/html/2504.13079/x4.png", "caption": "Figure 4: Performance of different methods with Llama3.3-70B-Instruct under setting (a) imbalance in supporting documents and (b) increasing the level of misinformation.", "description": "Figure 4 illustrates the performance comparison of different methods using the Llama 3.3-70B-Instruct model under two experimental conditions. Subfigure (a) examines the impact of an uneven distribution of supporting documents across multiple valid answers within a query. It shows how the number of supporting documents for one correct answer affects the performance of different approaches. Subfigure (b) assesses how increased levels of misinformation in the retrieved documents impact model performance, specifically examining the changes as the number of misinformation documents rises. This figure highlights how various methods handle conflict and imbalance in evidence within retrieval-augmented generation systems.", "section": "6 Ablations and Analysis"}, {"figure_path": "https://arxiv.org/html/2504.13079/x5.png", "caption": "Figure 5: Dataset statistics across eight dimensions. The first row shows document-level properties per example: total number of documents (avg: 5.53), number of documents supporting correct answers (avg: 3.84), incorrect answers (avg: 0.61), and noisy or irrelevant content (avg: 1.08). The second row presents answer-level properties: number of correct answers per example (avg: 2.20), wrong answers (avg: 0.86), and number of documents supporting each correct answer (avg: 1.77) and each wrong answer (avg: 0.73).", "description": "Figure 5 presents a detailed statistical analysis of the RAMDocs dataset, breaking down various aspects across both document and answer levels.  At the document level, it shows the average number of documents per example (5.53), the average number supporting correct answers (3.84), the average number supporting incorrect answers (0.61), and the average number containing irrelevant or noisy information (1.08). This gives insight into the prevalence of misinformation and noise within the dataset.  The second row focuses on the answer level, providing the average number of correct answers per example (2.20), the average number of incorrect answers (0.86), and the average number of documents supporting each correct answer (1.77) and each incorrect answer (0.73).  This highlights the level of ambiguity and the imbalance of supporting evidence for different answers within each example.", "section": "3 RAMDocs: Retrieval with Ambiguity & Misinformation in Documents"}]