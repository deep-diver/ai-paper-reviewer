{"references": [{"fullname_first_author": "Alec Radford", "paper_title": "Language models are unsupervised multitask learners", "publication_date": "2019-01-08", "reason": "This paper is a foundational work that introduced the concept of language models as multitask learners, influencing the development of CLIMB."}, {"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-01-01", "reason": "This work is considered among the most important because it significantly advanced the understanding of few-shot learning capabilities in large language models, a relevant factor for data mixture strategies."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Llama: Open and efficient foundation language models", "publication_date": "2023-02-01", "reason": "This is considered among the most important because the CLIMB 1B model is compared against Llama-3.2-1B, which means that this foundational language model directly informs the performance of CLIMB by comparison."}, {"fullname_first_author": "Leo Gao", "paper_title": "The pile: An 800gb dataset of diverse text for language modeling", "publication_date": "2020-01-01", "reason": "This is considered among the most important because it provides one of the datasets examined in CLIMB, and also is labor intensive, making CLIMB a valuable tool in contrast."}, {"fullname_first_author": "Sang Michael Xie", "paper_title": "Doremi: Optimizing data mixtures speeds up language model pretraining", "publication_date": "2024-01-01", "reason": "This is considered among the most important because this technique is compared and out-performed by CLIMB, making it a relevant evaluation baseline."}]}