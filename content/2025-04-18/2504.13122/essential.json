{"importance": "This paper is important for researchers because it **addresses critical limitations of existing LVMs by proposing a novel framework, VistaDPO, that enhances video-language preference alignment.** VistaDPO offers a fine-grained approach to preference optimization. This **opens new avenues for research to achieve more robust and human-aligned video understanding.**", "summary": "VistaDPO enhances LVMs via hierarchical spatiotemporal preference alignment, effectively mitigating video-language misalignment and hallucination.", "takeaways": ["VistaDPO, enhances text-video preference alignment across three hierarchical levels: Instance Level, Temporal Level, and Perceptive Level.", "VistaDPO-7k, a dataset of 7.2K QA pairs annotated with chosen and rejected responses, along with spatial-temporal grounding information is constructed.", "VistaDPO significantly improves the performance of existing LVMs and effectively mitigates video-language misalignment and hallucination."], "tldr": "Large Video Models (LVMs) have shown promise in video understanding, but often face misalignment and hallucination issues. To tackle these issues, the paper introduces a Video Hierarchical Spatial-Temporal Direct Preference Optimization framework. This novel approach improves text-video alignment across three hierarchical levels: aligning video content with responses, aligning video temporal semantics with event descriptions, and aligning spatial objects with language tokens.\n\nAddressing the lack of datasets for fine-grained video-language alignment, the paper constructs a dataset of 7.2K QA pairs, VistaDPO-7k, annotated with chosen and rejected responses and spatial-temporal grounding. The extensive experiments on benchmarks demonstrate that the proposed VistaDPO enhances the performance of existing LVMs, effectively mitigating video-language misalignment and hallucination issues. The code and dataset will be released to the community.", "affiliation": "University of Hong Kong", "categories": {"main_category": "Multimodal Learning", "sub_category": "Vision-Language Models"}, "podcast_path": "2504.13122/podcast.wav"}