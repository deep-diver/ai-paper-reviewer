{"importance": "This paper introduces a novel **collaborative data synthesis framework that leverages smaller LLMs to match the data quality of larger models**, potentially reducing computational costs and environmental impact. By challenging the necessity of monolithic LLMs for data synthesis, this work **opens new avenues for sustainable AI research** and resource-efficient model development.", "summary": "Smaller LLMs, strategically coordinated, rival large LLMs in data synthesis! GRA framework enables efficient, high-quality data generation with reduced resource demands.", "takeaways": ["Small LLMs can achieve data-level parity with large LLMs through strategic coordination.", "The GRA framework offers a resource-efficient approach to data synthesis by specializing roles across multiple small LLMs.", "Collaborative frameworks can potentially mitigate biases and enhance data diversity compared to single large LLMs."], "tldr": "Data synthesis and distillation often rely on Large Language Models (LLMs), which have high costs and potential biases. Smaller LLMs are accessible but lack generating quality data. Addressing this, this paper introduces a multiple small LLMs framework, GRA, which uses Generator, Reviewer, and Adjudicator roles to simulate peer-review. This framework aggregates specialized roles across small LLMs for refinement and quality control, similar to single large LLMs. \n\n The study uses collaborative small LLMs which achieve data-level parity with large LLM-based distillation. Experiments show that GRA-produced data matches or exceeds single large LLM outputs, such as Qwen-2.5-72B-Instruct. The work demonstrates strategically coordinated smaller agents and challenges the necessity of large models for data synthesis. Datasets, models, and code are publicly available.", "affiliation": "Shanghai AI Laboratory", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}, "podcast_path": "2504.12322/podcast.wav"}