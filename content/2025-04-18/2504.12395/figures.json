[{"figure_path": "https://arxiv.org/html/2504.12395/extracted/6367475/figs/1_intro.png", "caption": "Figure 1: Open-domain character personalization with InstantCharacter.", "description": "This figure showcases the capabilities of the InstantCharacter framework in personalizing various characters across diverse domains.  Multiple character images are presented alongside prompts guiding their customization.  The results demonstrate the model's ability to generate high-fidelity images with accurate text-based control,  effectively altering the characters' appearances, poses, and styles without losing their identity.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2504.12395/extracted/6367475/figs/1_pipeline.png", "caption": "Figure 2: Our framework seamlessly integrates a scalable adapter with a pretrained DiT model. The adapter consists of multiple stacked transformer encoders that incrementally refine character representations, enabling effective interaction with the latent space of the DiT. The training process employs a three-stage progressive strategy, beginning with unpaired low-resolution pretraining and culminating in paired high-resolution fine-tuning.", "description": "This figure illustrates the InstantCharacter framework's architecture.  It highlights the seamless integration of a novel scalable adapter with a pre-trained diffusion transformer (DiT). The adapter, composed of multiple stacked transformer encoders, progressively refines character representations for effective interaction within the DiT's latent space. The training process is depicted as a three-stage progressive approach: starting with unpaired, low-resolution images for pretraining, followed by paired low-resolution images for refinement, and concluding with paired, high-resolution images for fine-tuning, optimizing character consistency, textual controllability, and image fidelity. ", "section": "3 Methods"}, {"figure_path": "https://arxiv.org/html/2504.12395/extracted/6367475/figs/3_compare_full.jpg", "caption": "Figure 3: Qualitative comparison on character personalization. Our method generally demonstrates the best image fidelity and character consistency while maintaining the desirable textual controllability.", "description": "This figure presents a qualitative comparison of character personalization results across several methods, including InstantCharacter and state-of-the-art approaches such as OminiControl, EasyControl, ACE++, UNO, and GPT-40.  For each method, several example images are shown, all generated from the same text prompt ('a {character} is riding a bicycle on the street'). The comparison highlights InstantCharacter's superior ability to maintain high image fidelity and character consistency while achieving good textual controllability. Other methods demonstrate varying degrees of success in these areas, with some losing character identity details and others struggling with accurate depiction of actions described in the text prompt.  The results visually showcase InstantCharacter's advantages in generating high-quality, consistent, and text-controllable character images.", "section": "4 Experiments"}, {"figure_path": "https://arxiv.org/html/2504.12395/extracted/6367475/figs/3_compare_full_2.jpg", "caption": "Figure 4: Qualitative comparison on character personalization. Our method generally demonstrates the best image fidelity and character consistency while maintaining the desirable textual controllability.", "description": "Figure 4 presents a qualitative comparison of character personalization results across different methods.  It showcases the results of generating images based on text prompts and a reference character image. The figure highlights InstantCharacter's superior performance in maintaining both high image fidelity and consistent character features, while simultaneously exhibiting desirable textual controllability.  Comparisons are made with several state-of-the-art techniques, demonstrating InstantCharacter's ability to produce images with higher quality and more accurate reflection of the input text and character.", "section": "4 Experiments"}, {"figure_path": "https://arxiv.org/html/2504.12395/extracted/6367475/figs/3_compare_style.jpg", "caption": "Figure 5: Qualitative comparison on character personalization with different styles.", "description": "This figure showcases a qualitative comparison of character personalization results achieved using different styles.  It demonstrates how the InstantCharacter model can be used to generate images of a character in various styles (Ghibli and Makoto Shinkai styles are shown here as examples), maintaining consistency in character identity despite stylistic changes. The results are compared against other state-of-the-art methods to highlight the superior performance of InstantCharacter in generating high-fidelity, style-consistent character images.", "section": "4 Experiments"}, {"figure_path": "https://arxiv.org/html/2504.12395/extracted/6367475/figs/2_showcase_new.jpg", "caption": "Figure 6: More qualitative results of InstantCharacter.", "description": "Figure 6 showcases additional qualitative results generated by the InstantCharacter model.  It presents diverse examples of character image generation, demonstrating the model's ability to produce high-fidelity images across various poses, actions, backgrounds, and styles, while maintaining character consistency.  Each row features a reference character image and several generated images based on different text prompts, illustrating the model's responsiveness to textual instructions and its capacity for detailed and realistic image synthesis.", "section": "4 Experiments"}]