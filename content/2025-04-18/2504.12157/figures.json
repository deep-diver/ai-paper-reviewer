[{"figure_path": "https://arxiv.org/html/2504.12157/x1.png", "caption": "Figure 1. FocusedAD: We propose an automated character-centric AD generation model that emphasizes main character regions\u2019 appearances and actions while incorporating narrative context. Characters appearing in the movie clip are annotated with colored bounding boxes.", "description": "This figure illustrates the architecture of FocusedAD, a novel model for generating character-centric movie audio descriptions.  The model focuses on the appearances and actions of the main characters within specific regions of the movie clip.  Colored bounding boxes highlight the characters' locations in each frame. The overall process demonstrates how the system incorporates narrative context to generate concise and informative descriptions, addressing the challenges of creating effective audio descriptions for visually impaired audiences.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2504.12157/x2.png", "caption": "Figure 2. Overview of FocusedAD: FocusedAD takes movie clips as input and captures the character best query bank through clustering. The Character Perception Module identifies main characters in key frames and bi-directionally propagates character regions across the entire key frame sequence. Then, through the Dynamic Prior Module, it dynamically integrates visual and text priors using soft prompts. Finally, the Focused Caption Module takes scene-level tokens, character-level tokens, and soft prompts as input to generate character-centric audio descriptions.", "description": "FocusedAD processes movie clips to generate character-centric audio descriptions.  First, it creates a 'character best query bank' by clustering character images.  The Character Perception Module then identifies main characters in key frames and tracks their presence across the entire clip. The Dynamic Prior Module uses soft prompts to incorporate contextual information from previous audio descriptions and subtitles. Finally, the Focused Caption Module combines scene-level, character-level tokens, and soft prompts to produce the final narration.", "section": "3 Method"}, {"figure_path": "https://arxiv.org/html/2504.12157/x3.png", "caption": "Figure 3. Character Perception Module traverses the key frame sequence, detecting main characters in any frame and obtaining their segmented regions. Videos are processed in a streaming fashion, where each frame cross-attends to the main character memories from context frames. Finally, both the region prediction and key frame embeddings are stored into memory bank.", "description": "The Character Perception Module (CPM) processes video frames sequentially.  For each key frame, it identifies main characters using face detection and a character query bank. These detected regions are then propagated bidirectionally through the sequence, refining the character localization across time. Each frame's features are integrated with contextual information from previous frames (stored in a memory bank), improving the accuracy of character identification. Finally, the refined character region predictions and key frame embeddings are saved in the memory bank for use by subsequent modules.", "section": "3.1 Character Perception Module"}, {"figure_path": "https://arxiv.org/html/2504.12157/x4.png", "caption": "Figure 4. Instruction template with soft prompt. We use a well-designed instruction template with trainable soft prompts to inject the text prior and visual prior into Focused Caption Module.", "description": "Figure 4 illustrates the mechanism by which contextual information (text and visual priors) is incorporated into the Focused Caption Module.  The figure shows an instruction template which includes placeholders for the previous audio descriptions and subtitles (Text Prior) as well as the detected character bounding boxes (Visual Prior).  A trainable \"Soft Prompt\" is also included, acting as a flexible weighting mechanism to adjust the influence of the prior information.  This allows the model to dynamically adapt to varying scene complexities and character counts.", "section": "3.3 Focused Caption Module"}, {"figure_path": "https://arxiv.org/html/2504.12157/x5.png", "caption": "Figure 5. Samples of Storyboard-v2. Our dataset involves three main part, i.e., (i)movie clips, (ii) character regions, (iii) movie audio description ground-truth", "description": "Figure 5 shows examples from the Storyboard-v2 dataset.  This dataset is constructed for training a movie audio description (AD) model. Each sample in the dataset consists of three key parts: (i) a movie clip, which is a short segment of a movie; (ii) character regions, which are bounding boxes identifying the locations of the main characters within each frame of the movie clip; and (iii) movie audio description ground truth, which is a human-written, accurate textual description of the events occurring in the movie clip, focusing on the actions and interactions of the characters. These three components enable the training of a model to generate accurate and contextually relevant ADs.", "section": "4.1 Training Datasets"}, {"figure_path": "https://arxiv.org/html/2504.12157/x6.png", "caption": "Figure 6. The clustering results for obtaining the best query from the movie Harry Potter and the Deathly Hallows.", "description": "Figure 6 visualizes the results of a clustering algorithm used to determine the optimal 'best query' representations for each character in the movie *Harry Potter and the Deathly Hallows*.  The algorithm aims to generate a compact representation of each character's visual appearance across different scenes and lighting conditions, improving the accuracy of character recognition in the later stages of the video audio description (AD) generation pipeline. Each point in the graph represents a cropped image of a character's face.  Points of the same color are grouped together; the center of each group represents the 'best query'. This process minimizes the impact of variations in appearance on character identification.", "section": "Implementation Details"}, {"figure_path": "https://arxiv.org/html/2504.12157/x7.png", "caption": "Figure 7. Ablation study on the film Les Mis\u00e9rables to evaluate changes in FocusedAD indicators under varying thresholds. This film is selected for its representative nature, as its metrics closely align with the average of MAD-eval-Named.", "description": "This ablation study analyzes the impact of different thresholds used in the character recognition module of the FocusedAD model.  The experiment uses the movie *Les Mis\u00e9rables* as it provides a representative sample of data from the MAD-eval-Named dataset.  The graph plots the performance metrics (SPICE, METEOR, BertScore) against varying threshold values, demonstrating how different thresholds affect the model's ability to identify main characters accurately. The optimal threshold balances precision and recall in character identification and ultimately improves the quality of the generated audio descriptions.", "section": "5.2 Audio Description on GT Movie Clips"}, {"figure_path": "https://arxiv.org/html/2504.12157/x8.png", "caption": "Figure 8. Qualitative results of our method. The top two movie clips demos are from MAD-eval-Named and the bottom two movie clips demos are from Cinepile-AD. The Character Perception Module can recognize active main characters and feed their names into the AD generation pipeline. For visualization purposes, we display the portrait images of characters that have the closest distance to the best query, but the model actually utilizes the best query features as input.", "description": "This figure showcases qualitative examples of audio descriptions generated by the FocusedAD model.  The top two examples originate from the MAD-eval-Named dataset, while the bottom two are from Cinepile-AD.  Each example visually demonstrates how the Character Perception Module identifies and incorporates main characters' names into the generated audio description.  The displayed portrait images are those with the closest visual similarity to the 'best query' for each character, as determined by a clustering process.  However, the model itself uses the actual 'best query' features (rather than the portrait images) as input for its generation process. This highlights the model's ability to accurately identify and use main characters' features for improved narrative coherence.", "section": "5.3 Qualitative Results"}]