[{"figure_path": "https://arxiv.org/html/2504.06801/x1.png", "caption": "Figure 1: a) We compare augmentations from our learned placement with heuristic-based placements from Lift3D\u00a0[22]. In our augmentations, vehicles follow the lane orientations and are placed appropriately. b) These realistic augmentations significantly improve the 3D detection performance (KITTI\n\u00a0[6] val set, (easy)). Notably, we achieve detection performance comparable to that of the fully labeled dataset using only 50%percent5050\\%50 % of the dataset.", "description": "Figure 1 demonstrates the effectiveness of the proposed 3D-aware object placement method.  Part (a) compares augmentations generated by the proposed method with those from the Lift3D heuristic method [22]. The key difference is that the proposed method learns a distribution of plausible 3D bounding boxes, ensuring that generated vehicles align with lane orientations and road geometry. This is in contrast to Lift3D, which uses simpler heuristics that can lead to unrealistic placements. Part (b) shows that using the realistic augmentations from the proposed method significantly improves the accuracy of 3D object detection on the KITTI [6] validation set (easy difficulty).  Remarkably, using only 50% of the training data augmented with the new method achieves performance comparable to using the entire fully labeled dataset.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2504.06801/x2.png", "caption": "Figure 2: a) SA-PlaceNet Architecture: Given an input background image and corresponding depth to predict the means of a multi-dimensional Gaussian distribution over 3D bounding boxes. 3D bounding boxes are sampled from each of these Gaussian to compute the training loss. b) Geometry-aware augmentation in BEV (Birds Eye View). For a given source car location (bl\u2062o\u2062csubscript\ud835\udc4f\ud835\udc59\ud835\udc5c\ud835\udc50b_{loc}italic_b start_POSTSUBSCRIPT italic_l italic_o italic_c end_POSTSUBSCRIPT), we first find K\ud835\udc3eKitalic_K nearest neighbors with the same orientation and augment the location to b~l\u2062o\u2062csubscript~\ud835\udc4f\ud835\udc59\ud835\udc5c\ud835\udc50\\tilde{b}_{loc}over~ start_ARG italic_b end_ARG start_POSTSUBSCRIPT italic_l italic_o italic_c end_POSTSUBSCRIPT by interpolating with neighboring locations nl\u2062o\u2062csubscript\ud835\udc5b\ud835\udc59\ud835\udc5c\ud835\udc50n_{loc}italic_n start_POSTSUBSCRIPT italic_l italic_o italic_c end_POSTSUBSCRIPT (Alg.1)", "description": "Figure 2 illustrates the architecture and training process of the SA-PlaceNet, a key component of MonoPlace3D. (a) depicts the network architecture.  SA-PlaceNet takes as input a background image and its corresponding depth map. It processes this information to output the mean and covariance (implicitly represented) of a multi-dimensional Gaussian distribution. This distribution models the likely locations of 3D bounding boxes in the scene. During training, 3D bounding boxes are sampled from this Gaussian distribution to calculate the training loss.  (b) details the geometry-aware augmentation strategy. This augmentation technique operates in a Bird's Eye View (BEV) representation. For each source car's location, K nearest neighboring cars with similar orientations are identified.  The location of the source car is then adjusted by interpolation with the locations of these neighbors. Algorithm 1 in the paper provides further detail on this process.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2504.06801/x3.png", "caption": "Figure 3: Rendering pipeline: Given a 3D asset, we first render an image and shadow from a fixed light source according to the 3D box parameters. Next, we used edge-conditioned ControlNet\u00a0[53] to generate a realistic car version that follows the same orientation and scale as the rendered image. Finally, we use the obtained shadow, rendered car, and 3D location to place the car and render augmented images.", "description": "This figure illustrates the rendering pipeline used to create realistic augmented images of cars.  First, a 3D car model is rendered with a shadow, using parameters derived from the predicted 3D bounding box. Then, ControlNet [53] is employed to enhance the quality of the car rendering, resulting in a more realistic appearance that maintains the original orientation and scale. Finally, the rendered car and shadow are combined with the background image at the determined 3D location to produce a final, photorealistic augmented image.", "section": "3.2. What to place? Rendering cars"}, {"figure_path": "https://arxiv.org/html/2504.06801/x4.png", "caption": "Figure 4: Given an input source image, we plot the heatmaps of the mean objectness score at each pixel location. The generated heatmaps span a large region on the road with plausible locations of objects.\nNext, we show samples of bounding boxes and realistic renderings of cars in the scene.", "description": "Figure 4 visualizes the output of the Scene-Aware Plausible 3D Placement Network (SA-PlaceNet).  Given a road scene image as input, SA-PlaceNet predicts a probability distribution over possible locations for placing cars. This distribution is represented as a heatmap, where brighter regions indicate higher probabilities of car placement.  The heatmaps in Figure 4 demonstrate that SA-PlaceNet identifies a large area along the road as suitable for placing cars, reflecting a realistic understanding of how vehicles are typically positioned in road scenes. The figure also shows examples of the 3D bounding boxes and realistic renderings of cars generated based on the heatmap\u2019s predictions.  These examples further illustrate the plausibility and diversity of object placements learned by SA-PlaceNet.", "section": "3.1 Scene-aware Plausible 3D Placement"}, {"figure_path": "https://arxiv.org/html/2504.06801/x5.png", "caption": "Figure 5: a) Ablation for object placement - For a background road scene, we visualize the heatmaps of aggregated objectness scores at each pixel location. Geometric augmentation and variational inference help to generate diverse and plausible object placements. b) Histogram of the distribution of orientations of the ground truth bounding boxes and the generated bounding boxes.", "description": "Figure 5 presents an ablation study on the object placement module. (a) shows heatmaps representing the aggregated objectness scores for different placement methods: a naive baseline without geometric augmentation or variational inference, a model with only variational inference, a model with only geometric augmentation, and the full model. The heatmaps illustrate how the full model generates diverse and plausible placements across a wider area of the road, unlike the baseline and other variations. (b) displays histograms comparing the distributions of orientations from ground truth bounding boxes and the generated bounding boxes, demonstrating that the model's generated orientations closely match the real-world distributions.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2504.06801/x6.png", "caption": "Figure 6: Ablation over rendering methods: Given the source image and predicted 3D bounding boxes, we sample and render a synthetic ShapeNet\u00a0[5] car; Lift3D\u00a0[22] rendered method; and our realistic rendering. We show a smaller domain gap between the rendered cars and the original samples.", "description": "This figure demonstrates a comparison of different rendering methods used for generating synthetic car images for 3D object detection.  The input to all methods is a source image and predicted 3D bounding boxes. Three approaches are shown:  1) a simple rendering using a ShapeNet [5] 3D car model; 2) a rendering using the Lift3D [22] method which generates realistic car models; and 3) the method proposed by the authors, which aims to produce even more realistic renderings. The figure highlights that the authors' approach reduces the 'domain gap' \u2013 the difference in visual style \u2013 between the synthetic and real car images, which is a key factor in creating effective data augmentation for 3D object detection.", "section": "4.2. Evaluation of object renderings"}, {"figure_path": "https://arxiv.org/html/2504.06801/x7.png", "caption": "Figure 7: Qualitative comparison of the generated augmentations with all the baseline methods. Our augmentations are highly realistic, place cars following plausible placement properties, and have a minimal domain gap from the training dist.", "description": "Figure 7 presents a qualitative comparison of object placement augmentation results from MonoPlace3D against several baseline methods.  It visually demonstrates the key advantages of MonoPlace3D: highly realistic renderings of augmented vehicles that seamlessly integrate into the background scene, adherence to realistic placement constraints (such as vehicles being positioned within roadways and following lane orientations), and a minimal domain gap between the augmented data and real-world training data. The figure highlights how the baseline methods produce less realistic and less natural-looking augmentations.", "section": "4. Improving 3D Object Detection"}, {"figure_path": "https://arxiv.org/html/2504.06801/x8.png", "caption": "Figure 8: Placement on nuScenes\u00a0[3] dataset.", "description": "Figure 8 displays examples of 3D object placement generated by the MonoPlace3D model on the NuScenes dataset.  It showcases the model's ability to generate realistic and plausible 3D bounding boxes for objects within diverse real-world scenes. The figure visually demonstrates the successful application of the proposed method in a dataset known for its complexity and variety, highlighting the model's capacity to generate high-quality augmentations even in challenging scenarios.", "section": "A Additional placement results"}, {"figure_path": "https://arxiv.org/html/2504.06801/x9.png", "caption": "Figure 9: Augmented training dataset for 3D object detection: Given a sparse scene with few cars, we place cars at the predicted 3D bounding box locations using our rendering algorithm. We present two sets of results, one with low density (1\u22123131-31 - 3 cars added) and another with high density (4\u22125454-54 - 5 cars added) for each scene.", "description": "This figure demonstrates the augmentation process for 3D object detection in sparse scenes.  Starting with a real-world image containing few vehicles (low density), the algorithm predicts the 3D bounding box locations for new, synthetic vehicles. Using a rendering algorithm, these synthetic cars are added to the scene. Two versions are shown: one where 1\u20133 cars are added (low density), and another where 4\u20135 cars are added (high density). The goal is to show that the model is capable of generating realistic augmentations in various traffic density scenarios.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2504.06801/x10.png", "caption": "Figure 10: Placement results for pedestrian and cycle categories on KITTI dataset. Note that we applied copy-paste in the predicted 3D object box locations to generate the augmentations. Though copy-pasting causes image artifacts, these augmentations still improve 3D detection performance, as shown in the main paper.", "description": "Figure 10 presents qualitative results of applying the proposed augmentation method to pedestrian and cyclist categories within the KITTI dataset.  The augmentation process involved a simple copy-paste technique, directly placing the objects at the predicted 3D bounding box locations.  While this method introduces some visual artifacts due to the copy-paste approach, the resulting augmentations still demonstrably improve the overall 3D object detection performance, as detailed and quantitatively shown in the main body of the paper.", "section": "B Additional object detection results"}, {"figure_path": "https://arxiv.org/html/2504.06801/x11.png", "caption": "Figure 11: Detection improvement in corner cases.", "description": "This figure demonstrates the improved performance of the model on challenging \"corner cases.\"  The images show examples where standard object detection methods struggle.  Specifically, it showcases situations involving partial occlusions, unusual object orientations, or objects located near scene boundaries \u2013 difficult scenarios where accurate object placement and recognition are crucial for autonomous driving applications. The results highlight that the 3D-aware object placement approach enables the model to detect objects more reliably even under these complex conditions.", "section": "E. Implementations details"}, {"figure_path": "https://arxiv.org/html/2504.06801/x12.png", "caption": "Figure 12: Outputs generated from Stable Diffusion Inpainting pipeline \u00a0[38]. These inpainted images are used for training our placement model.", "description": "Figure 12 shows example images produced by the Stable Diffusion Inpainting pipeline [38], a method used in the paper to remove vehicles and objects from the KITTI dataset images.  The resulting images, which lack cars and other objects, serve as the input to the training process for the scene-aware placement network (SA-PlaceNet).  This inpainting step is crucial because the network is trained to predict where objects *could* plausibly be placed, which requires a clear view of the background scene without pre-existing objects that could bias the model's learning.", "section": "E Implementations details"}, {"figure_path": "https://arxiv.org/html/2504.06801/x13.png", "caption": "Figure 13: Sampled views rendered from Lift3D\u00a0[22].", "description": "Figure 13 displays various renderings of cars generated by the Lift3D model [22].  These renderings showcase the model's ability to create realistic car images from different perspectives and with various lighting conditions.  The images provide a visual representation of the diversity and quality of synthetic car data produced by Lift3D, which is relevant to understanding the comparison between the Lift3D method and the proposed approach in the paper.", "section": "F. Rendering details"}, {"figure_path": "https://arxiv.org/html/2504.06801/x14.png", "caption": "Figure 14: Sample cars from the Copy-Paste Database", "description": "This figure shows example images of cars from the Copy-Paste Database used in the paper.  The database contains cars and their corresponding 3D bounding boxes and binary segmentation masks. During inference, a car is selected from this database based on its similarity to the orientation of a predicted 3D bounding box, adding a degree of randomness to enhance diversity in the generated scenes. This technique helps ensure seamless integration of the synthetic cars into the background scene.", "section": "3.1. Scene-aware Plausible 3D Placement"}, {"figure_path": "https://arxiv.org/html/2504.06801/x15.png", "caption": "Figure 15: Sample of ShapeNet\u00a0[5] cars rendered at different views.", "description": "This figure showcases various 3D car models from the ShapeNet dataset, each rendered from multiple viewpoints.  The models are used to create realistic car augmentations in the paper's proposed 3D object detection method. Different views and poses demonstrate the diversity of the car models utilized in the rendering pipeline. These diverse models ensure realistic augmentation and prevent biases from limited car representations.", "section": "F.2. ShapeNet"}, {"figure_path": "https://arxiv.org/html/2504.06801/extracted/6350157/figs/fig13-orientation-correction.jpg", "caption": "Figure 16: Perspective and Absolute projection of cars with the same 3D orientation.", "description": "This figure illustrates the effect of perspective projection on the visual appearance of 3D objects.  Two cars are shown, both with the same 3D orientation parameters. However, due to their different positions relative to the camera in the scene, their projected orientations on the 2D image plane appear different. The image on the left shows the relative 3D orientation of the cars, which is the same for both.  The image on the right shows the absolute 3D orientation, as they would appear when projected in an image.", "section": "F. Rendering details"}, {"figure_path": "https://arxiv.org/html/2504.06801/x16.png", "caption": "Figure 17: a) Diverse renderings generated with edge-conditioned ControlNet. B) Shadows are generated by rendering 3D assets with a point light source in the blender\u00a0[8] environment", "description": "Figure 17 demonstrates the realistic car rendering process using ControlNet.  Panel (a) shows how ControlNet, conditioned on edge maps derived from ShapeNet car renderings and a text prompt ('A realistic car on the street'), produces diverse and realistic car images. The diversity is crucial for enhancing the realism of the augmentations. Panel (b) illustrates the shadow generation process. By placing a simple 2D plane under the 3D car model and adding a point light source (simulating the sun) in the Blender environment, realistic car shadows are generated, which are crucial for enhancing the visual realism of the augmented scenes.  These realistic cars and shadows are then integrated into the background image for the final augmented dataset. ", "section": "F. Rendering details"}]