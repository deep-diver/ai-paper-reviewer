[{"figure_path": "https://arxiv.org/html/2504.04974/extracted/6341787/figures/example3_cut.png", "caption": "Figure 1: An example from InfographicsVQA.\nQuestion: What is the resource and expertise level needed for Pinterest?\nAnswer: high.\nThe LLM is expected to generate the answer together with the corresponding grounded bounding boxes that can support its answer, which requires deeper spatial understanding and reasoning and sometimes instruction-following abilities.", "description": "Figure 1 presents an example from the InfographicsVQA dataset, illustrating the task of visual text grounding.  The image shows an infographic about social media platforms. A question is posed: \"What is the resource and expertise level needed for Pinterest?\" The correct answer is \"high.\"  The figure demonstrates the expected output from a multimodal large language model (MLLM): not only the answer itself, but also the identification of the relevant section within the infographic (via bounding boxes) that supports the answer. This highlights the challenge of requiring models to integrate visual and textual information for precise and accurate responses, demanding sophisticated spatial understanding and reasoning abilities. ", "section": "3 TRIG: Text-Rich Image Grounding"}, {"figure_path": "https://arxiv.org/html/2504.04974/extracted/6341787/figures/sample.png", "caption": "Figure 2: \nText-rich document image examples from different source datasets.", "description": "This figure shows examples of text-rich document images from four different datasets used in the paper: DocVQA, ChartQA, InfographicsVQA, and TRINS.  Each dataset presents unique challenges for visual text grounding due to variations in layout complexity, text density, and image content. The images highlight the diversity of document types and the need for robust models capable of handling the complexities inherent in visual text grounding.", "section": "3 TRIG: Text-Rich Image Grounding"}, {"figure_path": "https://arxiv.org/html/2504.04974/extracted/6341787/figures/main_figure.png", "caption": "Figure 3: \nMain Constriction Pipeline. The pipeline contains 4 steps: Preprocessing, Generation, Correction, and Human Evaluation. The benchmark data will go through all of these 4 steps, and the training data will go through the previous 3 steps.", "description": "This figure illustrates the four-step pipeline used to create the benchmark and training datasets.  First, the document images undergo preprocessing using PaddleOCR for OCR information extraction. Second, this OCR information is used with the question and answer to generate bounding boxes that support the answer.  Third, a correction step uses an LLM to refine the accuracy of the generated bounding boxes. Finally, human evaluation is performed to ensure high quality before inclusion in the benchmark dataset.  The training data only goes through the first three steps. ", "section": "3.1 Data Construction"}, {"figure_path": "https://arxiv.org/html/2504.04974/extracted/6341787/figures/embedding_illustrate.png", "caption": "Figure 4: \nThe illustrative pipeline of our Embedding-Based Method. The example is from ChartQA. Question: Is the percentage value of \u201cSTEM\u201d segment 52? Answer: Yes.\nThe visualization of ground-truth bounding boxes is presented on the left in green, and the visualization of the generated grounding area (patches) by our embedding-based method is presented on the right in red.\nThe input image will be processed into 32\u00d732323232\\times 3232 \u00d7 32 patches before sending it into the MLLM and obtaining the image patch embeddings. After obtaining both the image patch embeddings and text token embeddings, a similarity vector with the length of the number of image patches is generated. The higher scores represent the alignment between image and text, whose position will be selected as the grounding patches. For simplicity, the embedding merge and 2-level selection mechanisms are not presented in this figure.", "description": "Figure 4 illustrates the Embedding-Based Method proposed in the paper.  It uses an example from the ChartQA dataset, where the question is whether the percentage value of the \"STEM\" segment is 52%, and the answer is yes. The figure visually compares ground-truth bounding boxes (left, green) with the model's predicted grounding patches (right, red). The process begins by dividing the input image into 32x32 patches. These patches are then fed into a Multimodal Large Language Model (MLLM) to generate image patch embeddings.  These embeddings, along with text token embeddings, are used to compute a similarity vector, the length of which corresponds to the number of image patches.  Higher similarity scores indicate better alignment between image and text; the corresponding patches are selected as the grounding regions. Note:  The figure simplifies the process by omitting details like embedding merging and the two-level selection mechanism.", "section": "4 Proposed Methods"}, {"figure_path": "https://arxiv.org/html/2504.04974/extracted/6341787/figures/eval.png", "caption": "Figure 5: \nIllustrations on Evaluation Settings. In evaluation setting 1, no OCR model is used, representing the hardest scenario. While in settings 2 & 3, an additional OCR model is utilized to facilitate LLM on grounding information generation. The \u201cInstruction\u201d in the prompt describes the requirement of generating grounded bounding boxes and defines the desired format.", "description": "Figure 5 illustrates the three different evaluation settings used in the paper. Setting 1 is the most challenging, as it requires the model to generate bounding boxes without any OCR data.  The model must perform both visual and textual grounding from scratch. Settings 2 and 3 are simpler, providing OCR data to aid in bounding box generation or selection. The instructions given in the prompt specify the expected format for the bounding boxes, guiding the model to produce results in the desired format.", "section": "3 TRIG: Text-Rich Image Grounding"}, {"figure_path": "https://arxiv.org/html/2504.04974/extracted/6341787/figures/ablation_detailed.png", "caption": "Figure 6: \nThe visualization of detailed ablation studies of our embedding-based method on OCR-free grounding setting for the 2-level selection mechanism. The utilization of the 2-level selection mechanism avoids the performance decline when more patches are selected, making the performances more stable and getting better performances.", "description": "This figure visualizes the results of ablation studies conducted on the embedding-based method used for OCR-free grounding.  The experiments systematically varied the number of image patches used in the grounding process. The key finding is that employing a two-level selection mechanism prevents performance degradation when an increased number of patches are included. The two-level approach selects top-performing patches and then incrementally adds neighboring patches only if they enhance the results, making the overall performance more stable and, ultimately, better.", "section": "5.3 Ablation Study"}, {"figure_path": "https://arxiv.org/html/2504.04974/extracted/6341787/figures/evaluation_prompt.png", "caption": "Figure 7: \nThe prompt we used to request GPT4o to generate the grounded bounding boxes that support the answer to the question.", "description": "Figure 7 displays the prompt used to instruct GPT-4 to generate the bounding boxes supporting the answer to a question. The prompt provides the question and answer, along with information about the image, including sentence-level bounding boxes and their indices. It requests that GPT-4 identify the indices of the bounding boxes that support the answer and provide a rationale for its choices.", "section": "3.1 Data Construction"}, {"figure_path": "https://arxiv.org/html/2504.04974/extracted/6341787/figures/example_chart.png", "caption": "Figure 8: \nThe prompt we used to request GPT4o to evaluate the grounded bounding boxes generated in the previous step.", "description": "Figure 8 shows the prompt used to evaluate the bounding boxes generated by the model in the previous step.  Specifically, it provides instructions for GPT-4 to act as an assistant in analyzing the previously generated bounding boxes.  The prompt includes the question, answer, and the list of indexes with corresponding text for the bounding boxes.  GPT-4 is then asked to determine if the provided bounding boxes accurately and sufficiently support the answer to the question, outputting either 'YES' or 'NO' followed by a justification.", "section": "E. Evaluation Prompts"}, {"figure_path": "https://arxiv.org/html/2504.04974/extracted/6341787/figures/example_doc.png", "caption": "Figure 9: \nThe evaluation prompts for different evaluation settings.", "description": "Figure 9 shows the different prompts used for evaluating the three visual grounding settings in the paper.  Each setting has three variations of the prompt, differing in how the model is instructed to provide the bounding boxes (using CSS, a simple list format, or relative coordinates).  The prompts are designed to evaluate the model's ability to perform visual grounding at different levels of difficulty and with varied output formats.", "section": "3.2 Evaluation Settings"}, {"figure_path": "https://arxiv.org/html/2504.04974/extracted/6341787/figures/example_info.png", "caption": "Figure 10: \nBenchmark data examples from ChatQA. The grounded bounding boxes have already been visualized in the original image for better illustration.", "description": "Figure 10 presents four example questions and answers from the ChartQA dataset. Each example displays a chart followed by a question about the chart's data. The answers are accompanied by bounding boxes highlighting the relevant parts of the chart that support the answer. These examples showcase the complexity and variety of visual text grounding in document images.  The bounding boxes are already drawn on the images to show the exact parts that correspond to the answers.", "section": "3.1 Data Construction"}, {"figure_path": "https://arxiv.org/html/2504.04974/extracted/6341787/figures/example_trins.png", "caption": "Figure 11: \nBenchmark data examples from DocVQA. The grounded bounding boxes have already been visualized in the original image for better illustration.", "description": "Figure 11 presents example data from the DocVQA dataset, which is used to evaluate visual text grounding in text-rich documents.  The figure showcases several diverse document images, each with a question and answer pair, highlighting the challenge of accurately identifying and localizing the specific regions within the image that provide support for the answer.  Bounding boxes are overlaid onto the images to visually indicate the ground truth locations relevant to the corresponding answers. This is to improve understanding of the task and demonstrate the complexity of visual text grounding in such document images.", "section": "3.1 Data Construction"}]