[{"figure_path": "https://arxiv.org/html/2504.07957/x2.png", "caption": "Figure 1: \n(a) Limitations of existing Multimodal Instruction Following (IF) benchmarks.\n(b) Overview of the MM-IFEval benchmark, which significantly surpasses existing benchmarks in terms of constraint diversity, quantity, and instruction complexity. Our benchmark consists of Compose-Level (C-Level) problems that impose constraints on model outputs (e.g., format requirements, keyword limits) and Perception-Level (P-Level) problems that require reasoning about specific visual elements in images.\n(c) Our MM-IFEngine generates a large-scale, diverse training dataset suitable for both Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO).", "description": "Figure 1 illustrates the limitations of existing multimodal instruction following (IF) benchmarks (a), showcasing the lack of constraint diversity and complexity, and the insufficiency of current evaluation strategies.  In contrast, it introduces the MM-IFEval benchmark (b), a significant improvement over existing benchmarks. MM-IFEval features a broad range of constraints, including compose-level constraints (affecting model output formats and keywords) and perception-level constraints (requiring reasoning about image details).  Finally, it presents the MM-IFEngine (c), a system for creating a large, diverse training dataset suitable for various training methods (supervised fine-tuning and direct preference optimization) to improve multimodal instruction following capabilities.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2504.07957/x3.png", "caption": "Figure 2: Overall pipeline of MM-IFEngine. Part (a) demonstrates the three-stage workflow of our engine: (1) Image filter; (2) Task generation using GPT-4o for images without QA pairs and instruct refinement for existing annotations; and (3) Constraints integration incorporating 6 main categories and 32 subcategories, ensuring compatibility between constraints and tasks. MM-IFEngine is employed to generate SFT and DPO training datasets and MM-IFEval benchmark, as shown in part (b) and (c). MM-IFEval implements three evaluation metrics combining rule-based verification functions and a judge model to ensure accurate assessment.", "description": "The MM-IFEngine pipeline consists of three main stages: image filtering (selecting high-quality images), task generation (creating diverse instructions using GPT-40 for images without existing QA pairs, and refining existing annotations), and constraint integration (combining 6 main categories and 32 subcategories of constraints with tasks to ensure compatibility).  The pipeline generates the MM-IFInstruct-23k and MM-IFDPO-23k datasets for supervised fine-tuning (SFT) and direct preference optimization (DPO), respectively, and the MM-IFEval benchmark.  MM-IFEval uses three evaluation metrics: rule-based verification, direct judgment, and comparative judgment using a judge model, to provide comprehensive and accurate assessment of the model's performance.", "section": "3. MM-IFEngine"}, {"figure_path": "https://arxiv.org/html/2504.07957/extracted/6352110/figs/category_v3.png", "caption": "Figure 3: Constraint Quantity Distribution in MM-IFInstruct-23k. Our MM-IFInstruct-23k exhibits systematic variation in constraint complexity, with each sample containing 3-12 constraints per instruction.", "description": "The figure shows a histogram illustrating the distribution of the number of constraints applied per instruction in the MM-IFInstruct-23k dataset.  The x-axis represents the number of constraints, ranging from 3 to 12. The y-axis shows the frequency or proportion of instructions with that specific number of constraints. The dataset demonstrates a diverse range of instruction complexity, as indicated by the spread of the data across the different constraint numbers, rather than a concentration at a particular value. This variation in complexity is a key feature of the MM-IFInstruct-23k dataset, which is designed to enhance the training of multimodal large language models in various instruction following scenarios.", "section": "3. MM-IFEngine"}, {"figure_path": "https://arxiv.org/html/2504.07957/extracted/6352110/figs/taxonomy.png", "caption": "Figure 4: Constraint Category Distribution in Compose-Level Problems of MM-IFEval. This part comprises six primary constraint categories with 32 subcategories, forming a multi-level taxonomy for instruction-following evaluation.", "description": "The figure shows a breakdown of the constraint categories used in the MM-IFEval benchmark's compose-level problems.  It illustrates a hierarchical structure, starting with six main constraint categories, each further divided into multiple subcategories (32 in total). This multi-level taxonomy provides a comprehensive and granular way to evaluate the ability of multimodal instruction-following models.  The visualization helps understand the variety and complexity of constraints considered in the benchmark.", "section": "4. MM-IFEval"}, {"figure_path": "https://arxiv.org/html/2504.07957/extracted/6352110/figs/vision_v2.png", "caption": "Figure 5: Demonstration of constraints categories. We designed 6 main categories for all the constraints used, with a total of 32 subcategories", "description": "This figure provides a detailed breakdown of the 32 constraint subcategories used in the MM-IFEval benchmark, organized into six main categories.  Each category represents a different type of constraint applied to the generated instructions and model outputs, encompassing aspects such as text length, formatting, language, logical structure, keywords, and action requirements. The visualization helps clarify the diversity and complexity of constraints within the benchmark, highlighting its enhanced evaluation capabilities compared to previous work.", "section": "3. MM-IFEngine"}, {"figure_path": "https://arxiv.org/html/2504.07957/x4.png", "caption": "Figure 6: Image Source Distribution in perception-level problems.Perception-level problems in MM-IFEval presents a systematic categorization of 100 challenging vision-based instruction-following tasks, organized into 13 distinct classes according to image content characteristics and task complexity.", "description": "MM-IFEval's perception-level challenges encompass 100 diverse vision-based instruction-following tasks.  These tasks are systematically categorized into 13 distinct classes based on the image content's characteristics and the complexity of the instructions.  The figure visualizes the distribution of these 100 tasks across those 13 categories, offering insights into the benchmark's diversity and difficulty.", "section": "4. MM-IFEval"}, {"figure_path": "https://arxiv.org/html/2504.07957/x5.png", "caption": "Figure 7: A compose-level problem example from the MM-IFEval benchmark in the general image category.", "description": "This figure shows a sample question from the MM-IFEval benchmark dataset's compose-level category, which focuses on generating text responses that adhere to various constraints. The prompt image depicts a dog exhibiting aggressive behavior. The task is to describe the possible causes of the dog's behavior within several constraints.  These constraints cover audience (dog lovers), tense (present and past), tone (reassuring and empathetic), paragraph count (3), word usage ('sorry'), highlighting specific terms, summarization, and perspective (second-person). This comprehensive example illustrates the complexity and variety of constraints within the MM-IFEval benchmark, moving beyond simple instructions found in previous datasets.", "section": "4. MM-IFEval"}, {"figure_path": "https://arxiv.org/html/2504.07957/x6.png", "caption": "Figure 8: A compose-level problem example from the MM-IFEval benchmark in the chart image category.", "description": "This figure shows a compose-level problem from the MM-IFEval benchmark dataset, focusing on chart image analysis. The task requires the model to identify the region with the highest apple production from a given chart and provide reasons for this high yield, while applying several constraints. The constraints include specifying the unit of measurement for the answer, writing from the perspective of a Mexican agricultural expert, limiting the number of sentences per paragraph, and using a specific formatting style for the analysis. This exemplifies the benchmark's complexity and diversity in evaluating multimodal instruction following.", "section": "4. MM-IFEval"}, {"figure_path": "https://arxiv.org/html/2504.07957/extracted/6352110/figs/Appendix/t_web.png", "caption": "Figure 9: A compose-level problem example from the MM-IFEval benchmark in the geometry image category.", "description": "This figure shows a geometry problem from the MM-IFEval benchmark's compose-level questions.  The problem presents a diagram of a triangle with several midpoints marked, and asks the model to calculate the area of a specific region within the triangle using geometric principles, applying a series of constraints to guide the response generation. The constraints include audience (a liberal arts student), response length, structure (number of paragraphs and sentences), vocabulary restrictions (avoiding certain terms), tone, and overall clarity in the explanation.", "section": "4. MM-IFEval"}, {"figure_path": "https://arxiv.org/html/2504.07957/extracted/6352110/figs/Appendix/v_web.png", "caption": "Figure 10: A compose-level problem example from the MM-IFEval benchmark in the website image category.", "description": "This figure showcases a compose-level problem from the MM-IFEval benchmark, specifically focusing on website images.  The image itself is a screenshot of a webpage, likely featuring a blog post or online article. The task, presented as an instruction to the model, requires a detailed answer to a question about the webpage's content or functionality, subject to multiple constraints. These constraints might involve aspects such as word count, format (e.g., a numbered list), specific keywords to include or exclude, tone, and overall structure of the response. This example illustrates the complexity and diversity of the MM-IFEval benchmark, which assesses the model's ability to interpret not only text but also visual information to generate accurate and constrained responses.", "section": "4. MM-IFEval"}, {"figure_path": "https://arxiv.org/html/2504.07957/extracted/6352110/figs/Appendix/v_diagram.png", "caption": "Figure 11: A perception-level problem example from the MM-IFEval benchmark in the web category.", "description": "This perception-level problem from the MM-IFEval benchmark tests a model's ability to extract information from a dynamic web page.  The image shows a portion of a cryptocurrency exchange website displaying real-time pricing data for various cryptocurrencies (Bitcoin, Ethereum, Tether, etc.). The associated question requires the model to perform a calculation based on the provided exchange rates. Specifically, it asks the model to compute the total value of a hypothetical investment in two cryptocurrencies given their purchase prices and the current exchange rates displayed on the webpage, rounding to the nearest whole number.", "section": "4. MM-IFEval"}, {"figure_path": "https://arxiv.org/html/2504.07957/extracted/6352110/figs/Appendix/v_poster.png", "caption": "Figure 12: A perception-level problem example from the MM-IFEval benchmark in the diagram category.", "description": "This figure presents a perception-level problem from the MM-IFEval benchmark, focusing on diagram interpretation. The task requires identifying the flowchart node reached after the first encountered 'Yes' condition, starting from the 'Start' node. This tests the model's ability to understand and follow instructions within a visual context, going beyond simple image recognition to require logical reasoning.", "section": "4. MM-IFEval"}, {"figure_path": "https://arxiv.org/html/2504.07957/extracted/6352110/figs/Appendix/v_difference.png", "caption": "Figure 13: A perception-level problem example from the MM-IFEval benchmark in the poster category.", "description": "This figure presents a perception-level problem from the MM-IFEval benchmark, specifically focusing on the poster image category.  The task requires the model to interpret a visual representation of the alphabet (using white dots and lines) and identify a specific letter based on the number of dots. The image's visual complexity and the need for accurate letter recognition assess the model's ability to perform image-based reasoning and instruction following.", "section": "4. MM-IFEval"}, {"figure_path": "https://arxiv.org/html/2504.07957/extracted/6352110/figs/Appendix/p_instruction_generation_prompt.png", "caption": "Figure 14: A perception-level problem example from the MM-IFEval benchmark in the finding difference category.", "description": "This figure shows a perception-level problem from the MM-IFEval benchmark, specifically designed to test the model's ability to identify differences between images.  The image shows two scenes of a camping trip, with subtle variations between them.  The instruction prompts the model to identify which of two people (Sam or Tom) found more differences between the scenes. This assesses visual perception, comparison skills, and precise answer generation.", "section": "4. MM-IFEval"}, {"figure_path": "https://arxiv.org/html/2504.07957/extracted/6352110/figs/Appendix/p_constraint_integration_prompt.png", "caption": "Figure 15: Prompt template for image generation instructions using a large language model in MM-IFEngine.", "description": "This figure shows the prompt template used by the MM-IFEngine pipeline for generating image-based instructions. The prompt instructs a large language model (LLM) to create concise and appropriate instructions for a given image.  It guides the LLM to avoid overly detailed or specific instructions, offering examples and suggesting a structured output format.  The prompt ensures the generated instructions are relevant and suitable for the image's content while maintaining a balance between general applicability and tailored specificity.", "section": "3. MM-IFEngine"}, {"figure_path": "https://arxiv.org/html/2504.07957/extracted/6352110/figs/Appendix/cc3m.png", "caption": "Figure 16: prompt template for integrating constraints in MM-IFEngine.", "description": "This figure displays the prompt template utilized within the MM-IFEngine pipeline to incorporate constraints into image instructions.  The prompt instructs an expert-level large language model to expand an initial instruction by adding as many appropriate constraints as possible.  It emphasizes maintaining thematic consistency with the original instruction, ensuring relevance and conciseness, and avoiding conflicts between constraints or with the original instruction.  The prompt provides examples to illustrate acceptable constraint addition and offers a structured format for the model's output.", "section": "3. MM-IFEngine"}, {"figure_path": "https://arxiv.org/html/2504.07957/extracted/6352110/figs/Appendix/allava.png", "caption": "Figure 17: A sample constructed by MM-IFEngine pipeline from cc3m dataset", "description": "This figure shows a poem generated by the MM-IFEngine pipeline, trained on the CC3M dataset.  The poem, titled \"Answer\", depicts the hustle and bustle of city life, incorporating details from a provided image. The MM-IFEngine pipeline is designed to create high-quality image-instruction pairs for multimodal instruction following. This example highlights the pipeline's capacity to generate creative text formats (here, a poem) while adhering to specific constraints. The constraints applied are indicated in the caption and include structural elements (stanza length, number of lines, and use of parentheses), stylistic aspects (use of bold for nouns and present tense), and narrative structure (beginning and ending lines).", "section": "3. MM-IFEngine"}, {"figure_path": "https://arxiv.org/html/2504.07957/extracted/6352110/figs/Appendix/diversity.png", "caption": "Figure 18: A sample constructed by MM-IFEngine pipeline from Allava dataset", "description": "This figure shows an example of image-text pair generated by MM-IFEngine from the ALLAVA dataset. The image contains a picture of several cookies and muffins on a plate. The generated text is a response to the instruction \"What treat can be baked in an oven?\", which is tailored for a general adult audience with minimal baking experience, providing clear and simple explanations. The text also follows various constraints, such as writing style, sentence structure, and keywords.", "section": "3. MM-IFEngine"}, {"figure_path": "https://arxiv.org/html/2504.07957/extracted/6352110/figs/Appendix/DPO.png", "caption": "Figure 19: A sample constructed by MM-IFEngine pipeline from geo170k dataset", "description": "This figure shows an example of a geometry problem generated by the MM-IFEngine pipeline from the geo170k dataset.  It showcases a complex geometry problem involving angles within a circle and the application of geometric principles to find a specific angle. The problem includes a diagram and a detailed instruction, demonstrating the pipeline's ability to create challenging and diverse problems for multimodal instruction following.", "section": "4. MM-IFEval"}, {"figure_path": "https://arxiv.org/html/2504.07957/extracted/6352110/figs/Appendix/p_prompt_to_choose_f_and_p.png", "caption": "Figure 20: A DPO training set sample, where the rejected data is obtained by removing 33% of the constraints", "description": "Figure 20 showcases a pair of image-instruction examples used in Direct Preference Optimization (DPO).  The \"chosen\" example represents a high-quality image-instruction pair generated by the MM-IFEngine pipeline. The \"rejected\" example is derived from the same image and initial instruction but with approximately 33% of the original constraints removed. This demonstrates how the MM-IFEngine produces negative samples for DPO training by systematically altering the constraints applied to the image-instruction pairs. The comparison highlights the importance of carefully crafted constraints in producing effective training data.", "section": "3. MM-IFEngine"}, {"figure_path": "https://arxiv.org/html/2504.07957/extracted/6352110/figs/Appendix/p_compare_judge_prompt.png", "caption": "Figure 21: Prompt template for automated verification function selection and paramater extraction", "description": "This figure shows the prompt template used for automatically selecting the appropriate verification function and extracting the necessary parameters for rule-based evaluation in the MM-IFEval benchmark. The prompt guides a large language model (LLM) to select the best function from a list of candidates and extract the required parameters based on the constraint. This automation streamlines the evaluation process and improves efficiency.", "section": "5.3 Ablation Studies on Different DPO Settings"}, {"figure_path": "https://arxiv.org/html/2504.07957/extracted/6352110/figs/Appendix/p_direct_judge_prompt.png", "caption": "Figure 22: Prompt template for Compare Judge Method", "description": "This prompt template instructs the LLM to act as a judge, comparing two model outputs: one generated with a specific constraint and another without.  The judge must determine if the constrained response meaningfully differs from the unconstrained one, indicating whether the constraint was effectively applied. A simple 'True' or 'False' response is required.", "section": "5. Experiments"}]