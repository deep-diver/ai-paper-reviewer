[{"figure_path": "https://arxiv.org/html/2504.07491/x1.png", "caption": "Figure 1: Comparison between Kimi-VL-Thinking\u00a0and frontier open-source VLMs, including short-thinking VLMs (e.g.\u00a0Gemma-3 series, Qwen2.5-VL series) and long-thinking VLMs (QVQ-72B-Preview), on MathVision benchmark. Our model achieves strong multimodal reasoning with just 2.8B LLM activated parameters.", "description": "This figure compares the performance of Kimi-VL-Thinking against several state-of-the-art open-source Vision-Language Models (VLMs) on the MathVision benchmark.  It highlights that despite having only 2.8 billion activated parameters in its Language Model (LLM), Kimi-VL-Thinking achieves strong multimodal reasoning capabilities.  The comparison includes both short-thinking VLMs (like those in the Gemma-3 and Qwen-2.5-VL series) and long-thinking VLMs (like QVQ-72B-Preview) to demonstrate Kimi-VL-Thinking's competitive performance and efficiency.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2504.07491/x3.png", "caption": "Figure 2: Highlights of Kimi-VL\u00a0performance for a wide range of benchmarks like, general benchmarks (MMMU, MMBench), OCR (InfoVQA), multi-image (BLINK), long video (LongVideoBench, Video-MME), long document (MMLongBench-Doc), and agent (ScreenSpot-Pro and OSWorld). Detailed results are presented in Table\u00a03.", "description": "Figure 2 presents a comprehensive overview of Kimi-VL's performance across diverse benchmark categories.  It showcases the model's capabilities in general-purpose vision-language tasks (MMMU and MMBench), optical character recognition (OCR) (InfoVQA), multi-image understanding (BLINK), long video comprehension (LongVideoBench and Video-MME), long document processing (MMLongBench-Doc), and agent capabilities (ScreenSpot-Pro and OSWorld). The bar chart visually compares Kimi-VL's performance against several other state-of-the-art vision-language models, offering a concise summary of its strengths across various modalities. For detailed numerical results, refer to Table 3.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2504.07491/x4.png", "caption": "Figure 3: The model architecture of Kimi-VL\u00a0and Kimi-VL-Thinking, consisting of a MoonViT that allows native-resolution images, an MLP projector, and a Mixture-of-Experts (MoE) language decoder.", "description": "This figure illustrates the architecture of both Kimi-VL and its enhanced version, Kimi-VL-Thinking.  The core components are a native-resolution vision encoder called MoonViT, an MLP projector, and a Mixture-of-Experts (MoE) language decoder. MoonViT's ability to handle images at their native resolutions eliminates the need for resizing or downsampling, improving efficiency and potentially preserving fine-grained details. The MLP projector acts as a bridge, transforming the visual features from MoonViT into a format suitable for processing by the MoE decoder. The MoE decoder is the language model, employing a Mixture-of-Experts architecture for better efficiency and scalability.  The diagram highlights the data flow through these components and shows example input types such as small images, fine-grained images, long videos, and OCR screenshots, illustrating the model's diverse multimodal capabilities.", "section": "2 Approach"}, {"figure_path": "https://arxiv.org/html/2504.07491/x5.png", "caption": "Figure 4: The pre-training stages of Kimi-VL consume a total of 4.4T tokens after text-only pre-training of its language model. To preserve text abilities, all stages that update the language model are joint training stages.", "description": "This figure illustrates the pre-training pipeline for the Kimi-VL model.  The process begins with text-only pre-training of the language model using 5.2 trillion tokens of text data. This step establishes a strong foundation in language understanding before incorporating multimodal data. Subsequent stages involve joint pre-training of the language model and vision encoder, progressively increasing the proportion of multimodal data to gradually align the vision and language modalities.  A total of 4.4 trillion tokens are consumed across these stages. To maintain the high-quality text capabilities established during the initial text-only pre-training phase, all subsequent stages that modify the language model are conducted as joint training stages, ensuring that vision-language understanding and textual competence are developed in parallel.", "section": "2 Approach"}, {"figure_path": "https://arxiv.org/html/2504.07491/x6.png", "caption": "Figure 5: The post-training stages of Kimi-VL\u00a0and Kimi-VL-Thinking, including two stages of joint SFT in 32K and 128K context, and further long-CoT SFT and RL stages to activate and enhance long thinking abilities.", "description": "This figure illustrates the post-training process for both the Kimi-VL and Kimi-VL-Thinking models.  It details a multi-stage approach involving supervised fine-tuning (SFT) at two different context window sizes (32K and 128K tokens). This is followed by further refinement using long chain-of-thought (long-CoT) SFT, aiming to encourage more comprehensive and logical reasoning.  Finally, reinforcement learning (RL) is applied to optimize the model's reasoning abilities, resulting in the Kimi-VL-Thinking model which is designed for enhanced long-term thinking capabilities. The diagram visually shows the flow of these stages and highlights the key techniques used to improve the model's reasoning performance.", "section": "2.4 Post-Training Stages"}, {"figure_path": "https://arxiv.org/html/2504.07491/x7.png", "caption": "Figure 6: Manuscript reasoning visualization. Kimi-VL-Thinking\u00a0demonstrates the ability to perform historical and scientific inference by analyzing handwritten manuscripts step by step. In this example, our model identifies the author as Albert Einstein based on handwriting style, content analysis, and language cues. It reasons that the manuscripts relate to gravitational field equations, consistent with Einstein\u2019s contributions to general relativity.", "description": "This figure showcases Kimi-VL-Thinking's ability to perform historical and scientific inference.  The model analyzes a handwritten manuscript step-by-step, identifying Albert Einstein as the author based on handwriting style, the content of the equations (related to gravitational fields), and the presence of German terminology.  The model's reasoning process demonstrates its capacity for detailed analysis of visual and textual information to reach conclusions about the author and the subject matter of the document, highlighting its advanced multimodal reasoning capabilities.", "section": "3 Data Construction"}, {"figure_path": "https://arxiv.org/html/2504.07491/x8.png", "caption": "Figure 7: Kimi-VL\u00a0exhibits strong visual reasoning capabilities by grounding visual content in spatial, contextual, and cultural knowledge. It accurately identifies matching urban locations based on structural and layout features, interprets scenes from video games like Cyberpunk 2077 using stylistic cues, and recognizes real-world landmarks such as the Rogers Centre in Toronto.", "description": "Figure 7 showcases Kimi-VL's capacity for visual reasoning by demonstrating its ability to connect visual information with spatial, contextual, and cultural understanding.  The examples highlight three key capabilities:  First, the model correctly identifies similar urban areas based solely on their structural and layout similarities. Second, the model demonstrates an understanding of visual style by accurately interpreting a scene from the video game Cyberpunk 2077.  Third, the model recognizes real-world landmarks like the Rogers Centre in Toronto.  These examples illustrate the model's advanced visual reasoning skills and its ability to integrate diverse sources of information.", "section": "3.2 Instruction Data"}, {"figure_path": "https://arxiv.org/html/2504.07491/x9.png", "caption": "Figure 8: Kimi-VL\u00a0demonstrates its capability to perform symbolic reasoning and geometric inference by solving a circle geometry problem step by step. The model analyzes given conditions, applies geometric theorems such as the inscribed angle theorem and properties of triangle angles, and accurately derives the target angle.", "description": "Figure 8 showcases Kimi-VL's ability to solve a geometry problem step by step, demonstrating its symbolic reasoning and geometric inference capabilities.  The problem involves finding the measure of angle ACO in a circle where AB is the diameter, points C and D are on the circle, and angle D is given.  Kimi-VL systematically analyzes the given information (AB is the diameter, C and D are on the circle, and the measure of angle D), applies relevant geometric theorems (inscribed angle theorem and properties of triangle angles), and correctly calculates the measure of the target angle (angle ACO). This figure highlights the model's ability to perform multi-step reasoning and accurate mathematical derivations using visual and textual inputs.", "section": "Evaluation"}, {"figure_path": "https://arxiv.org/html/2504.07491/x10.png", "caption": "Figure 9: Diverse OCR visualization. Kimi-VL\u00a0demonstrates strong OCR capabilities across varied content types, including structured financial tables, complex mathematical formulas, and handwritten Chinese text. The model accurately parses tabular data into markdown, converts formulas to LaTeX, and transcribes handwritten paragraphs with contextual understanding, showcasing its versatility in multimodal text extraction and interpretation.", "description": "Figure 9 showcases Kimi-VL's robust Optical Character Recognition (OCR) capabilities across diverse data types.  It accurately converts structured financial tables into Markdown format, translates complex mathematical formulas into LaTeX code, and transcribes handwritten Chinese text, demonstrating contextual understanding in its transcriptions.  This highlights the model's versatility in extracting and interpreting multimodal text data, showcasing its ability to handle a wide range of input formats.", "section": "4.1.4 Document Understanding and OCR"}, {"figure_path": "https://arxiv.org/html/2504.07491/x11.png", "caption": "Figure 10: Kimi-VL\u00a0is capable of following multi-step reasoning processes to complete complex GUI tasks. In this example, it successfully enables the \u201cDo Not Track\u201d feature in the Chrome browser to enhance online privacy. The agent interprets each screen, identifies relevant UI elements, and performs the appropriate actions sequentially with clear thoughts, actions, and API calls.", "description": "This figure demonstrates Kimi-VL's ability to perform complex, multi-step interactions within a graphical user interface (GUI).  It showcases the model's step-by-step reasoning process, as it successfully enables the \"Do Not Track\" privacy setting in the Chrome web browser.  For each step, the figure shows the visual state of the GUI (a screenshot), the model's reasoned thought process, the action it takes, and the corresponding API call used to implement that action. This illustrates how Kimi-VL interprets the visual information on the screen, identifies relevant UI elements (buttons, menus, etc.), and executes the necessary actions sequentially to achieve a goal, all while maintaining a clear, logical thought process.", "section": "2.4 Post-Training Stages"}, {"figure_path": "https://arxiv.org/html/2504.07491/x12.png", "caption": "Figure 11: Video scene splitting.\nKimi-VL\u00a0processes a long-form video by segmenting it into coherent scenes and providing detailed start/end timestamps along with fine-grained natural language descriptions for each scene.\u2021\u2021footnotemark: \u2021", "description": "Figure 11 showcases Kimi-VL's capacity for detailed video understanding.  A long video is processed, automatically divided into individual scenes, each with a precise start and end time.  Furthermore, Kimi-VL generates a comprehensive natural language description for each scene, summarizing its key events and visual details. This demonstrates the model's ability to not only segment videos temporally but also to reason about visual content and provide human-like summaries.", "section": "4.1.4 Document Understanding and OCR"}, {"figure_path": "https://arxiv.org/html/2504.07491/x13.png", "caption": "Figure 12: Catching and understanding key details from an hour-long video course.\nKimi-VL\u00a0demonstrates its ability to comprehend and interpret instructional video content by analyzing frame sequences and extracting conceptual progression over time. In this case, the model identifies a deepening of the traditional saying \u201cTeach a man to fish, and you feed him for a lifetime\u201d into a more nuanced idea: \u201cTeach him the taste of fish and make him hungry.\u201d\u00b6\u00b6footnotemark: \u00b6", "description": "Figure 12 showcases Kimi-VL's ability to understand and summarize key concepts from a lengthy video.  The video in question is an hour-long instructional course.  The figure demonstrates that Kimi-VL can analyze the video's frame sequences to extract a conceptual progression over time. The example highlights Kimi-VL's ability to not only understand the familiar proverb \"Teach a man to fish, and you feed him for a lifetime,\" but also to grasp a more nuanced interpretation presented in the video: \"Teach him the taste of fish, and make him hungry.\" This nuanced interpretation emphasizes the importance of fostering a desire for continued learning and self-improvement, beyond simply acquiring a specific skill.", "section": "4.1.5 Agent Grounding and Multi-turn Agent Interaction"}]