[{"figure_path": "https://arxiv.org/html/2503.08588/x1.png", "caption": "Figure 1: Debiasing a language model with BiasEdit.", "description": "This figure illustrates the BiasEdit model's debiasing process.  A stereotypical sentence, such as \"Girls tend to be more [soft] than boys,\" is input into a pre-trained language model.  BiasEdit uses lightweight editor networks to modify the model's internal parameters, focusing on specific parts of the model rather than retraining the whole thing. This modification changes the model's output probability to assign equal likelihood to both the stereotypical phrase and a corresponding anti-stereotypical phrase, such as \"Girls tend to be more [determined] than boys.\"  Importantly, BiasEdit also includes a loss function to maintain the model's ability to generate grammatically correct and meaningful text, even as the bias is removed. This ensures that removing the bias does not negatively impact the language model's overall functionality.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2503.08588/x2.png", "caption": "Figure 2: Debiasing a language model with BiasEdit. Editor networks \u03d5italic-\u03d5\\phiitalic_\u03d5 are trained  to produce edit shifts on partial parameters \ud835\udcb2\ud835\udcb2\\mathcal{W}caligraphic_W of a language model while its parameters \u03b8\ud835\udf03\\thetaitalic_\u03b8 are frozen . After editing, an unbiased LM is obtained with the robustness of gender reversal and semantic generality. \u2112dsubscript\u2112\ud835\udc51\\mathcal{L}_{d}caligraphic_L start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT and \u2112rsubscript\u2112\ud835\udc5f\\mathcal{L}_{r}caligraphic_L start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT refer to Equation 1 and 2 respectively. s: stereotyped. a: anti-stereotyped. m: meaningless.", "description": "The figure illustrates the BiasEdit model, which uses lightweight editor networks to debias a language model.  The editor networks generate parameter updates that specifically target stereotypical biases while preserving the model's overall language modeling capabilities.  This is achieved using two loss functions: a debiasing loss (Ld) which guides the networks to remove the bias by equalizing probabilities of stereotypical and anti-stereotypical contexts, and a retention loss (Lr) to ensure that unrelated associations are maintained.  The figure shows that after editing, the model is debiased, and this debiasing effect is robust to gender reversal and semantic generality tests. The labels s, a, and m represent stereotyped, anti-stereotyped, and meaningless contexts respectively.", "section": "3 BIASEDIT"}, {"figure_path": "https://arxiv.org/html/2503.08588/x3.png", "caption": "Figure 3: SS (%) and \u0394\u0394\\Deltaroman_\u0394LMS (%) of debiased language models after editing the last layer in the MLP of different blocks. 1/2/3: the first/second/third block. 12: the first 2 blocks. 123: the first 3 blocks. -1/-2/-3, the last/penultimate/antepenultimate block, -321: the last 3 blocks. -21: the last 2 blocks.", "description": "This figure displays the Stereotype Score (SS) and the change in Language Modeling Score (\u0394LMS) after applying bias editing to different layers of four distinct language models.  The x-axis represents various combinations of blocks within the language model that were modified.  '1', '2', and '3' indicate edits to the first, second, and third blocks respectively.  '12' signifies edits to the first two blocks, and '123' shows edits to the first three blocks. Negative numbers (-1, -2, -3) denote edits to the last, second-to-last, and third-to-last blocks respectively. '\u221221' indicates edits to the last two blocks, and '\u2212321' represents edits to the last three blocks.  The y-axis shows the SS and \u0394LMS for each model, revealing the impact of the bias editing on both bias mitigation and overall language modeling capability.", "section": "4.4 Further Discussion on Editing Different Components for Debiasing"}, {"figure_path": "https://arxiv.org/html/2503.08588/x4.png", "caption": "Figure 4: Gender Reversal Robustness. Pre-debias refers to SS of pre-trained language models on the gender reversal test set before debiasing. Debiased refers to SS of debiased models by BiasEdit.", "description": "This figure demonstrates the robustness of the BiasEdit model to gender reversal.  It displays the Stereotype Score (SS) for four different language models before debiasing (Pre-debias) and after debiasing with BiasEdit (Debiased). The gender reversal test set was created by reversing the gender terms in the original StereotypeSet dataset.  A lower SS indicates less bias. The figure visually shows the impact of BiasEdit on reducing gender bias, even when presented with reversed gender terms, indicating that the model learns a more nuanced and equitable representation of gender.", "section": "4.5 Reversing Gender Attribute Words"}, {"figure_path": "https://arxiv.org/html/2503.08588/x5.png", "caption": "Figure 5: Gender bias tracing on GPT2-medium. (a) Comparing bias associations of bias attribute words on hidden states, attention layers, and MLP layers. (b) Comparing bias associations on single states of the bias attribute word, the token before the attribute term, and the attribute term. The bias impacts on output probability are mapped for the effect of (c-d) each hidden state on the context, (e-f) only MLP activations, and (g-h) only attention activations. * marks the corrupted bias attribute words and [] refers to the attribute terms in (c-h).", "description": "Figure 5 presents a detailed analysis of gender bias in the GPT2-medium language model using bias tracing.  Panel (a) compares the strength of bias associations across different layers of the model (hidden states, attention, and MLP layers). Panel (b) shows how bias associations vary depending on which specific words within the sentence are examined: the bias attribute word itself, the word preceding it, and the attribute term.  Panels (c-h) further investigate the impact of these bias associations on the model's output probabilities.  This is done by selectively corrupting (adding noise to) the embeddings of bias-related words and then restoring different parts of the model's internal activations (hidden states, attention, or MLP layers only). By comparing the changes in output probabilities before and after these manipulations, the figure illustrates how different parts of the model contribute to the overall gender bias.", "section": "A Bias Tracing"}, {"figure_path": "https://arxiv.org/html/2503.08588/x6.png", "caption": "Figure 6: Race bias tracing on GPT2-medium.", "description": "This figure presents the results of a bias tracing analysis performed on the GPT-2-medium language model, focusing on racial bias.  It visualizes the association between bias and different components of the model's architecture (hidden states, attention, and MLP layers) across various layers.  The plots likely show how the model's representation of race changes throughout the processing stages and how manipulating specific states within the model impacts the overall bias in its output. The subplots might show the effect of different interventions, such as removing or restoring specific activations.", "section": "A Bias Tracing"}]