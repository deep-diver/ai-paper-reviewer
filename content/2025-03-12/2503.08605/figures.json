[{"figure_path": "https://arxiv.org/html/2503.08605/x3.png", "caption": "Figure 1: Multi-event long video generation results using our tuning-free inference framework, SynCoS.\nEach example is around 21 seconds of video at 24 fps (4\u00d7\\times\u00d7 longer than the base model ).\nFrame indices are displayed in each frame.\nSynCoS generates high-quality, long videos with multi-event dynamics while achieving both seamless transitions between frames and long-term semantic consistency throughout.", "description": "This figure showcases examples of long-form video generation (approximately 21 seconds at 24 frames per second) produced using the SynCoS model.  Each video demonstrates multiple events unfolding within a single, coherent narrative.  SynCoS is highlighted for its ability to generate high-quality, temporally consistent videos with seamless transitions between events, maintaining consistent visual style and semantic meaning throughout the entire sequence. The frame index is shown on each frame.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.08605/x4.png", "caption": "Figure 2: Qualitative comparisons on CogVideoX-2B\u00a0[55].\nAll examples are 5 times longer in duration compared to the underlying base model, generating a 30-second video.\nUnlike Gen-L-Video\u00a0[50] and FIFO-Diffusion\u00a0[21], which often struggle with overlapping artifacts and style drift, our method, SynCoS, ensures consistency in both content and style throughout the entire video.\nAdditionally, SynCoS generates long videos where each frame faithfully follows its designated prompt while ensuring seamless transitions between frames.", "description": "Figure 2 presents a qualitative comparison of SynCoS with other methods on the CogVideoX-2B model.  The generated videos are five times longer (30 seconds) than those produced by the base model.  The comparison highlights SynCoS's ability to maintain consistent content and style throughout the video, unlike other methods that experience overlapping artifacts or style drift.  SynCoS also faithfully follows each frame's designated prompt and creates seamless transitions between frames.", "section": "3. Key observations"}, {"figure_path": "https://arxiv.org/html/2503.08605/x5.png", "caption": "Figure 3: t-SNE visualization of CLIP\u00a0[37] features for the predicted video frames \ud835\udc31^0|tsubscript^\ud835\udc31conditional0\ud835\udc61\\hat{\\mathbf{x}}_{0|t}over^ start_ARG bold_x end_ARG start_POSTSUBSCRIPT 0 | italic_t end_POSTSUBSCRIPT, at each timestep using different samplings.\nFaded colors indicate earlier timesteps (t\u22481000\ud835\udc611000t\\approx 1000italic_t \u2248 1000), while vivid colors indicate later, small timesteps (t\u22480\ud835\udc610t\\approx 0italic_t \u2248 0), illustrating feature trajectory evolution over time (top to bottom).", "description": "This figure visualizes the feature trajectories of video frames generated by different sampling methods (Gen-L-Video, CSD, and SynCoS) using t-SNE.  Each point represents a frame's CLIP features at a specific timestep during the denoising process.  Color intensity indicates the timestep, with faded colors representing earlier timesteps (near the beginning of the denoising process, t\u22481000) and vivid colors representing later timesteps (near the end of the process, t\u22480).  The visualization helps to show how the feature trajectories evolve over time and how the different sampling methods affect the consistency and coherence of the generated video.  Specifically, it demonstrates the divergence of Gen-L-Video's trajectories, the collapse of CSD's trajectories, and the consistent and coherent trajectories of SynCoS.", "section": "Key observations"}, {"figure_path": "https://arxiv.org/html/2503.08605/x7.png", "caption": "Figure 4: Qualitative comparison of sampling methods motivating SynCoS. Gen-L-Video\u00a0[50] fails to maintain global coherence, resulting in abrupt appearance changes (e.g., a man morphing into a woman).\nCSD\u00a0[22] retains a similar appearance of a man but shows poor adherence to local prompts, suffering from low frame quality with severe noise-like artifacts.\nIn contrast, our method achieves a balance, ensuring high-quality generation, strong prompt fidelity, and temporal coherence.", "description": "Figure 4 presents a qualitative comparison of three different long video generation sampling methods: Gen-L-Video, CSD, and the proposed SynCoS method.  Gen-L-Video struggles to maintain global consistency in the video, resulting in jarring transitions and sudden shifts in appearance, such as a man unexpectedly transforming into a woman.  CSD, while preserving the general appearance of the subject, exhibits a failure to properly adhere to the detailed instructions (local prompts) provided, leading to low quality video frames plagued with significant noise and artifacts. In contrast, SynCoS demonstrates a superior balance between maintaining overall consistency and accuracy of the local prompts.  It produces high-quality, coherent video with excellent fidelity to the intended content and smooth transitions.", "section": "Key observations"}, {"figure_path": "https://arxiv.org/html/2503.08605/x8.png", "caption": "Figure 5: \nOverall illustration of our proposed method, Synchronized Coupled Sampling (SynCoS), a tuning-free inference framework for multi-event long video generation.\nSynCoS performs one-step denoising in three iterative stages, repeated from t=1000\ud835\udc611000t=1000italic_t = 1000 to t=0\ud835\udc610t=0italic_t = 0.\nIn the first stage, SynCoS performs temporal co-denoising with DDIM, dividing the long video into overlapping short chunks, denoising each chunk, and applying fusion for local smoothness.\nIn the second stage, SynCoS refines the locally fused output, enforcing global coherence by synchronizing information across both short- and long-distance chunks.\nFinally, in the third stage, it reverts the locally and globally refined output to the previous timestep.\nThrough these three synchronized stages of local and global denoising, SynCoS ensures smooth transitions, global semantic coherence, and high prompt fidelity, enabling multi-event long video generation.", "description": "This figure illustrates the SynCoS method, a three-stage iterative process for generating long, multi-event videos.  Stage 1 uses DDIM for local temporal coherence by dividing the video into chunks, denoising each, and fusing them. Stage 2 refines the result globally, ensuring consistency across all video chunks (both nearby and distant ones).  Stage 3 reverts the refined result to the previous timestep, improving accuracy.  The combination of these three stages produces smooth transitions, maintains long-term coherence, and achieves high fidelity to the prompts, enabling high-quality, multi-event long videos.", "section": "4. Method"}, {"figure_path": "https://arxiv.org/html/2503.08605/x11.png", "caption": "Figure 6: Qualitative comparisons on Open-Sora Plan\u00a0[23].\nAll examples are 4 times longer in duration compared to the underlying base model, generating a 20-second video. Gen-L-Video\u00a0[50] suffers from abrupt appearance changes, while FIFO-Diffusion\u00a0[21] introduces noticeable noise-like artifacts.\nIn contrast, our proposed method, SynCoS, generates high-quality, temporally coherent videos that faithfully follow the prompt throughout the sequence.", "description": "Figure 6 presents a qualitative comparison of long video generation results using three different methods: Gen-L-Video, FIFO-Diffusion, and the authors' proposed SynCoS approach.  All videos are four times longer than those generated by the base Open-Sora Plan model, resulting in 20-second videos. The figure visually demonstrates that Gen-L-Video suffers from abrupt and jarring appearance changes within the video, lacking temporal coherence. FIFO-Diffusion, while maintaining some consistency, introduces noticeable noise-like artifacts, degrading the visual quality.  In contrast, SynCoS produces high-quality videos with smooth transitions and consistent visual style that accurately reflect the input prompt from beginning to end.", "section": "5.2. Main experiments"}, {"figure_path": "https://arxiv.org/html/2503.08605/x12.png", "caption": "Figure 7: Qualitative ablation study. Without a grounded timestep, structural inconsistencies arise (e.g., the volcano alternates between one and two peaks).\nWithout a fixed baseline noise, it fails to follow local prompts faithfully (e.g., missing eruptions or absent smoke).", "description": "Figure 7 presents an ablation study on the SynCoS model, demonstrating the importance of two key components: grounded timesteps and fixed baseline noise.  The top row shows results where the grounded timestep is absent. This leads to inconsistencies in the generated video's structure, such as a volcano that inconsistently changes between having one and two peaks across frames. This demonstrates that the grounded timestep is crucial for maintaining a consistent temporal reference throughout the video generation process.  The bottom row shows results with the fixed baseline noise removed. This results in a failure to adhere to local prompts. For instance, the video may not show the expected volcanic eruption or any smoke, showing that the fixed baseline noise is essential for ensuring the model faithfully follows the prompt's specifications across all stages of the video generation process.", "section": "5.3. Ablation study"}, {"figure_path": "https://arxiv.org/html/2503.08605/x19.png", "caption": "Figure 8: Instruction for generating structured prompt. This instruction follows the guidelines to create individual local prompts and a shared global prompt based on a scenario and the number of prompts the user gives.", "description": "Figure 8 illustrates the process of generating structured prompts for long video generation.  A scenario (a long description of an event) is provided by the user, along with the desired number of short prompts. The figure details the steps involved in converting this long scenario into multiple short, action-focused prompts and a single global prompt. The global prompt encompasses the overall scene description and shared properties, while the local prompts each focus on a specific action, creating a structured input for the video generation model to improve coherence and consistency.", "section": "4. Method"}, {"figure_path": "https://arxiv.org/html/2503.08605/x22.png", "caption": "Figure 9: \nQualitative visualization of the ablation study on the three coupled stages of SynCoS.\nAll examples in the second box are 3 times longer in duration compared to the underlying base model.", "description": "Figure 9 shows the results of an ablation study on the three stages of the SynCoS model.  The ablation study systematically removes one stage at a time to understand its contribution to the overall performance. The images in the figure visually demonstrate the impact of removing each stage on the generated video quality. Specifically, removing the first stage (local temporal co-denoising with DDIM) leads to inconsistent transitions; removing the second stage (global temporal co-denoising with CSD) results in a loss of global coherence, causing a blurry and unstable video; and removing the third stage (DDIM-based temporal co-denoising) produces unnatural transitions.  The examples in the second box are all three times longer than videos generated by the base model, which further highlights the crucial role of all three stages in successful long video generation.", "section": "5.3. Ablation study"}]