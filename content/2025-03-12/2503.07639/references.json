{"references": [{"fullname_first_author": "Radford", "paper_title": "Language models are unsupervised multitask learners", "publication_date": "2019-01-01", "reason": "This paper introduces the Transformer-based language model GPT, which serves as a foundational baseline for many subsequent language model architectures."}, {"fullname_first_author": "Waswani", "paper_title": "Attention is all you need", "publication_date": "2017-01-01", "reason": "This paper introduces the Transformer architecture, which revolutionized the field of natural language processing and serves as the base of most LLMs."}, {"fullname_first_author": "Shazeer", "paper_title": "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer", "publication_date": "2017-01-01", "reason": "This paper introduces the Mixture-of-Experts (MoE) layer and it provides a scalable alternative to dense networks by activating only a subset of experts for any given input."}, {"fullname_first_author": "Elhage", "paper_title": "A mathematical framework for transformer circuits", "publication_date": "2021-01-01", "reason": "This paper introduces the field of mechanistic interpretability, which aims to understand deep neural networks by analyzing individual units (e.g., neurons) reverse-engineering their computations."}, {"fullname_first_author": "Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-01-01", "reason": "This paper demonstrates that sufficiently large language models can perform few-shot learning, enabling them to generalize to new tasks with only a few examples."}]}