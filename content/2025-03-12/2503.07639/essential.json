{"importance": "This research presents **MoE-X**, to build intrinsically interpretable models, potentially leading to more **trustworthy and understandable AI systems**. The method is relevant to current trends in mechanistic interpretability and opens avenues for **investigating sparse architectures and routing mechanisms**.", "summary": "MoE-X: An intrinsically interpretable Mixture-of-Experts language model that uses sparse, wide networks to enhance transparency.", "takeaways": ["Wider, sparser MLP layers encourage more disentangled internal representations.", "ReLU activation within experts and sparsity-aware routing are key designs for intrinsic interpretability.", "MoE-X achieves strong performance and superior interpretability compared to dense models and SAE-based approaches."], "tldr": "Large language models excel, but their inner workings remain a mystery, leading to unpredictable behavior. **Polysemanticity**, where neurons encode multiple concepts, hinders interpretability. Post-hoc methods like Sparse Auto-Encoders (SAEs) are used, but these are costly and often incomplete. Instead, architectural changes can directly design interpretability into the model, but this is often done on toy tasks or comes at a compromise.\n\nTo address this, the paper introduces **MoE-X**, a Mixture-of-Experts model designed for intrinsic interpretability. It leverages wider, sparser networks, made possible by MoE architectures, to capture interpretable factors. It is done by Rewriting the MoE layer as a sparse, large MLP while enforcing sparse activation within each expert, and routes based on activation sparsity. Evaluations shows MoE-X achieves both competitive performance and enhanced interpretability.", "affiliation": "University of Oxford", "categories": {"main_category": "AI Theory", "sub_category": "Interpretability"}, "podcast_path": "2503.07639/podcast.wav"}