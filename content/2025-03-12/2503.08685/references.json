{"references": [{"fullname_first_author": "Alexey Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2021-01-01", "reason": "This paper is important because it introduces the Vision Transformer (ViT) architecture, which is used as the encoder in SEMANTICIST."}, {"fullname_first_author": "William Peebles", "paper_title": "Scalable diffusion models with transformers", "publication_date": "2023-01-01", "reason": "This is important as it introduces the Diffusion Transformer (DiT) architecture which is used as the decoder in SEMANTICIST."}, {"fullname_first_author": "Patrick Esser", "paper_title": "Taming Transformers for high-resolution image synthesis", "publication_date": "2021-01-01", "reason": "This paper presents the VQGAN, a key component integrated into the Latent Diffusion Model (LDM) approach that SEMANTICIST leverages."}, {"fullname_first_author": "Robin Rombach", "paper_title": "High-resolution image synthesis with latent diffusion models", "publication_date": "2022-01-01", "reason": "This paper introduces Latent Diffusion Models (LDM), which SEMANTICIST utilizes for efficient training of the diffusion-based decoder in the latent space."}, {"fullname_first_author": "Peize Sun", "paper_title": "Autoregressive model beats diffusion: Llama for scalable image generation", "publication_date": "2024-01-01", "reason": "This paper is important as it presents the LlamaGen architecture, which is used for autoregressive image generation in SEMANTICIST."}]}