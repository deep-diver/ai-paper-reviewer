[{"figure_path": "https://arxiv.org/html/2503.05978/extracted/6258787/Figure/teaser_0308.png", "caption": "Figure 1: \nGiven a portrait image, our model can generate compelling, realistic, and vivid animation videos with control over text and voice, ensuring temporal coherence and perceptual quality even under significant head pose variations and diverse portrait styles.", "description": "Figure 1 showcases the capabilities of the MagicInfinite model.  It demonstrates the generation of high-quality, realistic animation videos from a single portrait image. The animation is controlled by both text and audio inputs. The figure highlights the model's ability to maintain temporal coherence and visual fidelity across diverse character styles, head poses, and even varied backgrounds. The examples include realistic humans, stylized anime characters, and full-body figures, demonstrating the versatility of the MagicInfinite approach.", "section": "1 INTRODUCTION"}, {"figure_path": "https://arxiv.org/html/2503.05978/x1.png", "caption": "Figure 2: \nOverview of MagicInfinite. MagicInfinite employs a hybrid dual-to-single-stream denoising network with Audio Cross-Attention in final blocks. MLLM encodes static portrait and text into tokens, concatenated for T2V, refined, and denoised. Wav2Vec encodes audio, resampled by an Audio Encoder, and guided by a Face Region Mask for precise lip sync and adaptive loss.", "description": "Figure 2 illustrates the architecture of the MagicInfinite model, a hybrid dual-to-single-stream diffusion model.  The process begins with a static portrait image and textual prompt. A Multimodal Large Language Model (MLLM) encodes this information into tokens. Simultaneously, the audio input is processed by Wav2Vec, and an audio encoder adjusts the sampling rate. A face region mask guides the audio processing to focus on lip synchronization.  These tokens (text and audio) are combined and fed into a text-to-video (T2V) model. This model is refined through denoising steps with audio cross-attention incorporated in the final blocks to ensure precise lip synchronization.  The output is a high-fidelity, talking-head video.", "section": "3 METHOD"}, {"figure_path": "https://arxiv.org/html/2503.05978/x2.png", "caption": "Figure 3: The overview of our modified DMD2. We employed a curriculum learning strategy to gradually reduce the weight of the base loss while progressively increasing the weight of the SDS loss, effectively avoiding abrupt shifts in learning objectives. Furthermore, we adopted a two-fold to three-fold CFG attenuation strategy in the calculation of the real data distribution, which significantly enhances the motion dynamics of the generated video.", "description": "This figure illustrates the architecture of the modified DMD2 model used for efficient inference.  The key improvements are a curriculum learning strategy that smoothly transitions between different loss functions (base loss and SDS loss), preventing instability during training.  Additionally, a two-fold to three-fold CFG (classifier-free guidance) attenuation method is used during the calculation of the real data distribution. This refinement significantly improves the motion dynamics and quality of the generated video, enabling more fluid and natural movement.", "section": "3.4 Model Acceleration"}, {"figure_path": "https://arxiv.org/html/2503.05978/x3.png", "caption": "Figure 4: Qualitative experimental results of MagicInfinite", "description": "Figure 4 showcases various video generation results from the MagicInfinite model, highlighting its ability to create high-quality and realistic talking avatar videos with diverse character types and styles, including realistic humans and stylized anime characters.  The videos demonstrate accurate lip synchronization with the input audio, natural head and facial movements, and coherent background integration, even under significant head pose variations and different lighting conditions.  The figure visually emphasizes the model's capability for precise control over video generation parameters, showcasing its ability to generate compelling and diverse animation across various scenarios.", "section": "4 Experiment"}]