[{"figure_path": "https://arxiv.org/html/2503.12533/x1.png", "caption": "Figure 1: Overview of the Being-0 framework.\nThe humanoid agent framework, Being-0, comprises three key components: (1) the Foundation Model (FM) for high-level task planning and reasoning, (2) the Connector, a vision-language model (VLM) that bridges the FM and low-level skills, and (3) the Modular Skill Library for robust locomotion and dexterous manipulation. Together, these components enable Being-0 to effectively control a full-sized humanoid robot equipped with multi-fingered hands and active vision, solving complex, long-horizon embodied tasks in real-world environments.", "description": "Being-0 is a hierarchical framework for humanoid robots.  It consists of three main components: a Foundation Model (FM) for high-level planning and reasoning; a Connector, a vision-language model (VLM), which translates high-level plans into low-level actions; and a Modular Skill Library for locomotion and manipulation.  These components work together to enable the robot to perform complex, long-horizon tasks in real-world environments.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.12533/x2.png", "caption": "Figure 2: Workflow of Being-0 for the task \u201cmake a cup of coffee\u201d. The figure illustrates the step-by-step execution of the task, with images arranged in two rows. The execution order proceeds left to right in the first row, then continues left to right in the second row. Images with yellow borders indicate decision-making points for the Foundation Model (FM). The yellow dialog boxes display the FM\u2019s plans, the green boxes show decisions made by the Connector, and the blue boxes represent the skills called from the modular skill library.", "description": "Figure 2 illustrates the step-by-step workflow of the Being-0 system in accomplishing the task of 'making a cup of coffee'.  The process is divided into two rows of images, progressing from left to right.  Yellow borders highlight the decision points where the Foundation Model (FM) plans the next step, presented in yellow dialog boxes.  The Connector module's decisions are shown in green boxes, while the blue boxes represent the low-level skills from the modular skill library that are executed. This visualization helps to understand the hierarchical interaction between the high-level planning (FM), mid-level coordination (Connector), and low-level skill execution. ", "section": "3. The Hierarchical Agent Framework"}, {"figure_path": "https://arxiv.org/html/2503.12533/x3.png", "caption": "Figure 3: A comparison of Being-0 w/o Connector and Being-0 in the long-horizon task \u201cPrepare-coffee.\u201d The first row shows recordings of Being-0 without the Connector, while the second row shows recordings of Being-0 with the Connector. Being-0 w/o Connector frequently queries the FM, which often fails to provide correct plans due to its limited embodied scene understanding. In contrast, Being-0 with the Connector completes the task, requiring only a few queries to the FM.", "description": "This figure compares the performance of two versions of the Being-0 robotic agent on the \"Prepare-coffee\" task. The top row shows the agent without the Connector module, highlighting frequent queries to the Foundation Model (FM) due to poor embodied scene understanding, ultimately failing to complete the task. The bottom row demonstrates the improved performance of the agent with the Connector module, which successfully bridges the high-level planning of the FM and low-level robot control, resulting in task completion with far fewer queries to the FM.", "section": "3. The Hierarchical Agent Framework"}, {"figure_path": "https://arxiv.org/html/2503.12533/x4.png", "caption": "Figure 4: Recordings from the ablation study on the active camera. Each row represents a different camera configuration, with the left three images depicting the navigation task and the right three images depicting the manipulation task. Only Being-0 with an active camera achieves robust performance in both navigation and manipulation.", "description": "This figure presents an ablation study comparing the performance of Being-0 with different camera configurations on navigation and manipulation tasks.  The left three images in each row show the navigation phase, where the robot moves toward a target object (coffee machine). The right three images display the subsequent manipulation phase (grasping coffee).  The rows represent different setups: fixed cameras with varying pitch angles (0.3, 0.6, and 0.9 radians) and Being-0 with its active camera.  The results clearly show that only Being-0, using its active camera, successfully completes both tasks reliably. Fixed camera setups, regardless of angle, fail to achieve consistent success in either navigation or manipulation.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.12533/x5.png", "caption": "Figure 5: A comparison of Being-0 with and without the adjustment method in two-stage tasks involving navigation and manipulation. Each row corresponds to a specific task, with the left three images showing results for Being-0 w/o Adjustment and the right three images showing results for Being-0. Without adjustment, the agent may terminate navigation in improper poses, leading to failed manipulations.", "description": "This figure compares the performance of the Being-0 agent with and without the proposed adjustment method for navigation.  Two-stage tasks were used, each consisting of navigation followed by manipulation.  The left three images in each row show the results of Being-0 without the adjustment, demonstrating that improper navigation poses often led to failed manipulation attempts. The right three images show Being-0 with the adjustment, highlighting its success in producing appropriate poses for successful manipulation.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.12533/x6.png", "caption": "Figure 6: First-person view recordings of the learned manipulation skills. Each row corresponds to a specific skill, with images from left to right depicting the progression of the manipulation process.", "description": "This figure presents a series of first-person viewpoints demonstrating the humanoid robot's learned manipulation skills. Each row showcases a different skill, with images progressing from left to right to illustrate the sequential steps involved in successfully completing each task.  The skills shown highlight the robot's dexterity and precision in manipulating various objects.", "section": "3.1 Modular Skill Library"}, {"figure_path": "https://arxiv.org/html/2503.12533/x7.png", "caption": "Figure 7: Planning traces of the Foundation Model in Being-0 for the task \u201cPrepare-coffee.\u201d", "description": "This figure shows the detailed reasoning process of the Foundation Model (FM) in Being-0 while executing the \"Prepare-coffee\" task. It breaks down the task into subtasks and illustrates how the FM uses image observation and reasoning to decide on the next action. The process is depicted in three stages: 1. Information gathering, including image description, target identification, and self-reflection. 2. Task Inference, involving subtask identification and reasoning. 3. Action planning, outlining the next action to be taken and the reasoning behind it. Each step demonstrates how the FM reasons about the task, identifies the next step, and plans accordingly, showcasing its hierarchical decision-making process in tackling complex tasks.", "section": "3. The Hierarchical Agent Framework"}, {"figure_path": "https://arxiv.org/html/2503.12533/x8.png", "caption": "Figure 8: (Continued) Planning traces of the Foundation Model in Being-0 for the task \u201cPrepare-coffee.\u201d", "description": "This figure shows a continuation of the planning traces from Figure 7, illustrating how the Foundation Model (FM) in the Being-0 framework plans and reasons to complete the \"Prepare-coffee\" task.  It details the FM's image observations, reasoning, task inference, action planning, and self-reflection steps as it guides the robot through various subtasks, like grabbing the cup and interacting with the coffee machine. The traces provide a detailed, step-by-step view of the FM's decision-making process and highlight its ability to break down a complex task into smaller, manageable actions.", "section": "3. The Hierarchical Agent Framework"}, {"figure_path": "https://arxiv.org/html/2503.12533/x9.png", "caption": "Figure 9: (Continued) Planning traces of the Foundation Model in Being-0 for the task \u201cPrepare-coffee.\u201d", "description": "Figure 9 shows a continuation of the detailed planning process of the Foundation Model in the Being-0 framework for the \"Prepare-coffee\" task.  It illustrates the step-by-step reasoning, decision-making, and action planning of the Foundation Model as it processes visual information and updates its understanding of the task's progress. The figure details the model's internal state, including its understanding of the current situation, the selection of subsequent actions, and its self-reflection on past actions.  This detailed trace provides insights into the internal workings of the Foundation Model and its ability to decompose complex tasks into smaller, manageable subtasks.", "section": "A.2. Foundation Model Planning Traces"}, {"figure_path": "https://arxiv.org/html/2503.12533/x10.png", "caption": "Figure 10: (Continued) Planning traces of the Foundation Model in Being-0 for the task \u201cPrepare-coffee.\u201d", "description": "This figure shows a continuation of the detailed planning steps taken by the Foundation Model (FM) in the Being-0 system while attempting to complete the \"Prepare-coffee\" task.  It provides a detailed, step-by-step view of how the FM processes visual information from the robot's cameras, reasons about the task's progress, and plans subsequent actions.  The detailed text shows the FM's internal reasoning steps, including image descriptions, subtask inferences, self-reflection analysis, and the selection of actions. This illustrates the complex decision-making process involved in executing even a seemingly simple task like making coffee with a humanoid robot.", "section": "A.2. Foundation Model Planning Traces"}, {"figure_path": "https://arxiv.org/html/2503.12533/x11.png", "caption": "Figure 11: Planning traces of the Foundation Model in Being-0 w/o Connector for the task \u201cPrepare-coffee.\u201d", "description": "This figure displays the planning traces of the Foundation Model used in the Being-0 system (without the Connector module) while executing the \"Prepare-coffee\" task.  It shows a step-by-step breakdown of how the model reasons, makes decisions, and selects actions based solely on its high-level understanding and without the benefit of real-time visual feedback from the Connector. Each step includes the model's interpretation of the image data, its reasoning process, the selected action, and a self-reflection evaluating the success of the previous action. The detailed traces highlight the challenges faced by a purely high-level approach in executing complex, real-world tasks, particularly involving navigation and manipulation in dynamic environments.", "section": "3. The Hierarchical Agent Framework"}, {"figure_path": "https://arxiv.org/html/2503.12533/x12.png", "caption": "Figure 12: (Continued) Planning traces of the Foundation Model in Being-0 w/o Connector for the task \u201cPrepare-coffee.\u201d", "description": "This figure shows the detailed planning traces of the Foundation Model in the Being-0 framework, specifically when the Embodied Connector module is NOT used, for the task of preparing coffee.  It provides a step-by-step breakdown of the Foundation Model's reasoning, including image descriptions, self-reflection on previous actions, inference of subtasks, and ultimately, the planned actions for the robot. This detailed view highlights the challenges faced by the Foundation Model when operating without the intermediary assistance of the Connector, revealing its difficulties in consistently achieving successful task completion due to limitations in real-time visual understanding and precise skill execution.", "section": "A. Additional Results"}, {"figure_path": "https://arxiv.org/html/2503.12533/x13.png", "caption": "Figure 13: (Continued) Planning traces of the Foundation Model in Being-0 w/o Connector for the task \u201cPrepare-coffee.\u201d", "description": "This figure shows the detailed planning traces of the Foundation Model in the Being-0 system without the Connector module for the \"Prepare coffee\" task.  It provides a step-by-step breakdown of the model's reasoning, including image descriptions, self-reflection, task inference, and action planning.  It illustrates the challenges of using the Foundation Model alone, such as failure to detect the cup on the table and inefficient search strategies, due to a lack of direct perception and embodiment.", "section": "A. Additional Results"}]