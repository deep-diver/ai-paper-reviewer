[{"figure_path": "https://arxiv.org/html/2503.12605/extracted/6284801/fig/cover-teasure-map.png", "caption": "Figure 1: Developing timeline of Multimodal Chain-of-Thought (MCoT) reasoning. Models with names in gray are text-only LLMs. For clarity, the models in the figure are assumed to include the image modality by default, unless specified with special modalities indicated by colored circles.", "description": "This figure is a timeline showcasing the development of Multimodal Chain-of-Thought (MCoT) reasoning models.  It visually represents the chronological order of significant model releases, highlighting the integration of various modalities beyond text. Models shown in gray represent text-only Large Language Models (LLMs), while colored circles added to model names indicate the inclusion of specific additional modalities (e.g., audio, video, 3D). This visualization helps to understand the evolution of MCoT reasoning across various modalities and the increasing complexity of the models over time.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2503.12605/x1.png", "caption": "Figure 2: Taxonomy of MCoT reasoning.", "description": "This figure presents a comprehensive taxonomy of Multimodal Chain-of-Thought (MCoT) reasoning methods.  It organizes various MCoT techniques across several dimensions, including the modality of data involved (image, video, audio, 3D, table/chart, cross-modal), the methodology employed (rationale construction, structural reasoning, information enhancing, objective granularity, multimodal rationale, test-time scaling), and the applications in which these methods are used (embodied AI, agentic systems, autonomous driving, healthcare, social and human interaction, and multimodal generation).  Each category contains specific examples of relevant models and papers.", "section": "4 Methodologies in MCoT Reasoning"}, {"figure_path": "https://arxiv.org/html/2503.12605/x2.png", "caption": "Figure 3: Different thought paradigms of CoT and MCoT.", "description": "This figure illustrates the evolution of reasoning structures or topologies used in Chain-of-Thought (CoT) and Multimodal Chain-of-Thought (MCoT) reasoning. It starts by showing basic paradigms for CoT and MCoT, highlighting the differences between direct input-output methods and methods that employ majority voting or self-consistency checks. Then, it moves to structured CoT and MCoT methods, which employ more sophisticated reasoning structures.  Specifically, the figure displays three main types of topologies: chain, tree, and graph. Chain topologies represent linear, sequential reasoning; tree topologies allow for exploration and backtracking; and graph and hypergraph topologies facilitate aggregation among multiple nodes, enabling reasoning over more complex multimodal inputs. The figure showcases how the evolution of these topologies directly reflects a progression from linear reasoning to parallel and branching exploration with higher-order associations.", "section": "2.2 Thought Paradigm"}, {"figure_path": "https://arxiv.org/html/2503.12605/x3.png", "caption": "Figure 4: Common architectures for comprehension-only and comprehension-generation MLLMs.", "description": "Figure 4 illustrates the typical architectures of multimodal large language models (MLLMs).  It contrasts comprehension-only MLLMs with those capable of both comprehension and generation.  Comprehension-only models process multimodal embeddings or tokens into a decoder structure that produces an output.  Comprehension and generation models utilize a similar process but also include a generation component allowing them to produce both multimodal outputs (text, image, video, audio, 3D data) and textual outputs.", "section": "2.3 Multimodal LLMs"}, {"figure_path": "https://arxiv.org/html/2503.12605/x4.png", "caption": "Figure 5: Examples of MCoT applications in various modalities and tasks.", "description": "Figure 5 presents several example applications of Multimodal Chain-of-Thought (MCoT) reasoning across various modalities.  Each example showcases a task, the input modalities (e.g., audio, image, video), the reasoning process as a chain of thoughts, and the final answer. Modalities involved include audio, images, video, 3D data, and tables; task types include question answering, grounding, and generation.  This highlights MCoT's adaptability and wide applicability.", "section": "3 MCOT Reasoning Under Various Modalities"}, {"figure_path": "https://arxiv.org/html/2503.12605/x5.png", "caption": "Figure 6: MCoT reasoning methods under different rationale construction perspectives.", "description": "This figure illustrates three main methodologies used in Multimodal Chain-of-Thought (MCoT) reasoning, categorized by how the rationale (the step-by-step reasoning process) is constructed.  Prompt-based methods use carefully crafted prompts to guide the model's reasoning.  Plan-based methods allow models to dynamically explore and refine thoughts using tree, graph, or hypergraph structures. Learning-based methods incorporate rationale construction directly into the training or fine-tuning process, teaching the model to generate rationales alongside the final answers. The figure also highlights how each method integrates multimodal inputs from various modalities (Image, Video, Audio, etc.) into the LLM.", "section": "Methodologies in MCoT Reasoning"}, {"figure_path": "https://arxiv.org/html/2503.12605/x6.png", "caption": "Figure 7: MCoT methods under different structural reasoning perspectives.", "description": "This figure illustrates different methodologies employed in MCoT reasoning, categorized by their structural reasoning approach. It shows how various methods handle the organization and flow of information during the reasoning process, contrasting asynchronous modality processing, defined procedures, and autonomous procedure learning. Asynchronous approaches process modalities independently, defined procedures follow predefined steps, and autonomous methods let the model decide the sequence of reasoning steps.", "section": "4 Methodologies in MCoT Reasoning"}, {"figure_path": "https://arxiv.org/html/2503.12605/x7.png", "caption": "Figure 8: MCoT reasoning under perspectives with information enhancing.", "description": "Figure 8 illustrates different methodologies employed in MCOT reasoning, focusing on how enhancing the input information improves the reasoning process.  It shows three main approaches: using expert tools to integrate external knowledge or perform specialized operations (e.g., geometric manipulation), retrieving world knowledge from external sources (e.g., knowledge graphs, databases) to enhance the reasoning process, and leveraging in-context knowledge retrieval from the existing information in the input or already generated rationales.  Each approach is represented visually, highlighting how these techniques improve multimodal reasoning in MCOT.", "section": "4 Methodologies in MCoT Reasoning"}, {"figure_path": "https://arxiv.org/html/2503.12605/x8.png", "caption": "Figure 9: MCoT reasoning under the perspectives of various objective granularities.", "description": "Figure 9 illustrates different methodologies within Multimodal Chain-of-Thought (MCoT) reasoning, categorized by the granularity of their objectives.  It shows how approaches vary depending on whether the goal is a high-level overview (coarse understanding), precise identification of specific elements (semantic grounding), or detailed analysis of individual components (fine-grained understanding).  The figure visually represents the different levels of detail and information processing involved in each approach.", "section": "4 Methodologies in MCoT Reasoning"}, {"figure_path": "https://arxiv.org/html/2503.12605/x9.png", "caption": "Figure 10: MCoT reasoning with multimodal rationale.", "description": "This figure illustrates the concept of Multimodal Chain-of-Thought (MCoT) reasoning using multimodal rationales.  It shows that, unlike traditional text-only CoT reasoning, MCoT reasoning integrates multiple modalities (represented by 'M') into the rationale generation process. This means that the reasoning steps are not limited to text but can also incorporate information from images, audio, video, or other modalities, leading to a more comprehensive and nuanced understanding of the problem and ultimately, a more accurate answer.", "section": "4.5 From Multimodal Rationale Perspective"}, {"figure_path": "https://arxiv.org/html/2503.12605/x10.png", "caption": "Figure 11: MCoT reasoning with test-time scaling strategies.\nRL can help improve reasoning quality, or active long-CoT reasoning ability without annotated long-CoT training data. SFT is optional.", "description": "This figure illustrates different test-time scaling strategies used in Multimodal Chain-of-Thought (MCoT) reasoning.  It shows how reinforcement learning (RL) can enhance the quality of reasoning and enable active long-CoT reasoning capabilities even without extensive annotated training data for long chains of thought.  The figure depicts various approaches, emphasizing that supervised fine-tuning (SFT) is an optional component in this process.  The options shown highlight different ways to scale the reasoning process at test time, thus allowing for more efficient and effective reasoning in resource-constrained settings.", "section": "Methodologies in MCoT Reasoning"}]