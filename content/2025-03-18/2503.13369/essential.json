{"importance": "SIGHTATION bridges the gap in BLV-aligned diagram descriptions, enabling more inclusive VLM applications and setting a precedent for future accessibility-focused research.", "summary": "SIGHTATION: A BLV-aligned dataset utilizing sighted user feedback to enhance diagram descriptions generated by VLMs, improving accessibility for visually impaired learners.", "takeaways": ["Sighted user feedback can effectively guide VLMs to generate diagram descriptions better aligned with BLV needs.", "SIGHTATION, a new diagram description dataset, facilitates training and evaluating VLMs for BLV accessibility.", "Fine-tuning VLMs on SIGHTATION improves performance across various downstream tasks, enhancing accessibility for BLV users."], "tldr": "Existing methods for generating diagram descriptions for BLV users are often **costly, biased, and misaligned** with their specific needs. Current evaluation metrics also struggle to accurately assess the quality of generated content from a BLV perspective. This leads to **ineffective and inaccessible learning materials** for visually impaired individuals. This paper introduces an approach to address these shortcomings.\n\nThis paper introduces SIGHTATION, a new dataset leveraging sighted user feedback to improve diagram descriptions generated by vision-language models (VLMs) for BLV users. By **guiding VLMs with latent supervision** and incorporating feedback from sighted individuals, the dataset reduces bias and improves alignment with BLV preferences. SIGHTATION encompasses **5k diagrams and 137k samples**, designed for completion, preference, retrieval, question answering, and reasoning tasks, which **demonstrates fine-tuning potential**.", "affiliation": "KAIST AI", "categories": {"main_category": "Multimodal Learning", "sub_category": "Vision-Language Models"}, "podcast_path": "2503.13369/podcast.wav"}