[{"figure_path": "https://arxiv.org/html/2503.13327/x2.png", "caption": "Figure 1: Edit Transfer aims to learn a transformation from a given source\u2013target editing example, and apply the edit to a query image.\nOur framework can effectively transfer both (b) single and (c) compositional non-rigid edits via proposed visual relation in-context learning.", "description": "Figure 1 illustrates the core concept of Edit Transfer.  Given a single example of an image and its edited version (source-target pair), the model learns the transformation applied.  This learned transformation is then applied to a new, unseen query image to produce a corresponding edit.  The figure showcases two main applications: (b) transferring single edits (e.g., changing a person's pose) and (c) transferring compositional edits (e.g., combining multiple pose changes and other modifications simultaneously). This capability is achieved through the paper's proposed visual relation in-context learning approach.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2503.13327/x3.png", "caption": "Figure 2: Comparisons with existing editing paradigms.\n(a) Existing TIE methods\u00a0[20, 21, 22, 23, 24, 25] rely solely on text prompts to edit images, making them ineffective for complex non-rigid transformations that are difficult to describe accurately.\n(b) Existing RIE methods\u00a0[6, 7, 8, 9, 10, 11, 12, 13, 14, 15] incorporate visual guidance via a reference image but primarily focus on appearance transfer, failing in non-rigid pose modifications.\n(c) In contrast, our proposed Edit Transfer learns and applies the transformation observed in editing examples to a query image, effectively handling intricate non-rigid edits.", "description": "This figure compares three image editing approaches: text-based image editing (TIE), reference-based image editing (RIE), and the proposed Edit Transfer method. (a) illustrates the limitations of TIE, showing that complex, non-rigid transformations are hard to describe accurately using only text prompts. (b) demonstrates how RIE methods, while incorporating visual guidance, often struggle with non-rigid pose changes because they mainly focus on transferring appearance. (c) highlights that the novel Edit Transfer method overcomes these limitations by learning the transformation from a source-target pair and effectively applying it to new images, thus handling complex non-rigid edits.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2503.13327/x4.png", "caption": "Figure 3: Visual relation in-context learning for Edit Transfer.\n(a) We arrange in-context examples in a four-panel layout:\nthe top row (an editing pair (\u2110s,\u2110t)subscript\u2110\ud835\udc60subscript\u2110\ud835\udc61(\\mathcal{I}_{s},\\mathcal{I}_{t})( caligraphic_I start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT , caligraphic_I start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )) and the bottom row (the query pair (\u2110^s,\u2110^t)subscript^\u2110\ud835\udc60subscript^\u2110\ud835\udc61(\\mathcal{\\hat{I}}_{s},\\mathcal{\\hat{I}}_{t})( over^ start_ARG caligraphic_I end_ARG start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT , over^ start_ARG caligraphic_I end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )).\nOur goal is to\nto learn the transformation from\n\u2110s\u2192\u2110t\u2192subscript\u2110\ud835\udc60subscript\u2110\ud835\udc61\\mathcal{I}_{s}\\to\\mathcal{I}_{t}caligraphic_I start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT \u2192 caligraphic_I start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, and apply it to the bottom-left image \u2110^ssubscript^\u2110\ud835\udc60\\hat{\\mathcal{I}}_{s}over^ start_ARG caligraphic_I end_ARG start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT, producing the target\n\u2110^tsubscript^\u2110\ud835\udc61\\hat{\\mathcal{I}}_{t}over^ start_ARG caligraphic_I end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, in the bottom-right. (b) We fine-tune a lightweight LoRA in the MMA to better capture visual relations.\nNoise addition and removal are applied only to ztsubscript\ud835\udc67\ud835\udc61z_{t}italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, while the conditional tokens cTsubscript\ud835\udc50\ud835\udc47c_{T}italic_c start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ( derived from (\u2110s,\u2110t,\u2110^s)subscript\u2110\ud835\udc60subscript\u2110\ud835\udc61subscript^\u2110\ud835\udc60(\\mathcal{I}_{s},\\mathcal{I}_{t},\\hat{\\mathcal{I}}_{s})( caligraphic_I start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT , caligraphic_I start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , over^ start_ARG caligraphic_I end_ARG start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT )) remain noise-free.\n(c) Finally, we cast Edit Transfer as an image generation task by initializing the bottom-right latent token zTsubscript\ud835\udc67\ud835\udc47z_{T}italic_z start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT with random noise and concatenating it with the clean tokens\ncIsubscript\ud835\udc50\ud835\udc3cc_{I}italic_c start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT.\nLeveraging the enhanced in-context capability of the fine-tuned DiT blocks, the model generates\n\u2110tsubscript\u2110\ud835\udc61\\mathcal{I}_{t}caligraphic_I start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, effectively transferring the same edits from the top row to the bottom-left image.", "description": "Figure 3 illustrates the visual relation in-context learning process used in Edit Transfer.  Panel (a) shows the four-panel input arrangement: the top row contains an example source image and its corresponding edited version, while the bottom row shows a new query image and its to-be-generated edited version. The model learns the transformation from the top row and applies it to the query image.  Panel (b) details the fine-tuning process where a lightweight LoRA (Low-Rank Adaptation) is applied to the Multi-Modal Attention (MMA) module to improve the model's ability to capture the visual relationship between the images.  Noise is added and removed only from the latent tokens representing the edited image, while the conditional tokens remain noise-free. Panel (c) depicts the final image generation step, where the latent token is initialized with random noise, combined with clean tokens, and then passed through the refined DiT (Diffusion Transformer) blocks to produce the final edited query image.", "section": "Methodology"}, {"figure_path": "https://arxiv.org/html/2503.13327/x5.png", "caption": "Figure 4: Edit Transfer exhibits impressive versatility to transfer visual exemplar pairs\u2019edit into the requested source image, delivering high-quality (a) single-edit transformations as well as (b) effective compositional edits that seamlessly combine multiple modifications.", "description": "Figure 4 demonstrates the capabilities of Edit Transfer in handling both single and multiple edits.  The top row (a) showcases examples where a single editing transformation (e.g., changing a person's pose) from a source-target image pair is successfully applied to a new query image.  The bottom row (b) shows examples of 'compositional edits,' where multiple edits from a single source-target example are successfully combined and applied to a new image. This highlights the model's ability to learn complex transformations and apply them effectively to unseen images.", "section": "3.2. Visual Relation In-Context Learning"}, {"figure_path": "https://arxiv.org/html/2503.13327/x6.png", "caption": "Figure 5: Qualitative comparisons.\nCompared with TIE and RIE methods, our method consistently outperforms in various non-rigid editing tasks.\nWe provide the detailed text prompt of TIE methods in \u00a0Section\u00a0B.1.", "description": "Figure 5 presents a qualitative comparison of different image editing methods.  It showcases the results of several techniques, including text-based image editing (TIE), reference-based image editing (RIE), and the proposed Edit Transfer method.  The goal is to illustrate the Edit Transfer method's superior performance in handling complex, non-rigid transformations, such as changes in pose and viewpoint.  The figure displays a series of image edits performed by each technique, allowing for a visual comparison of the results.  Detailed text prompts used for TIE methods are provided separately in Section B.1 of the paper.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.13327/x7.png", "caption": "Figure 6: Results of user study and VLM evaluation.\nWe compare our Edit Transfer (IV) with P2P\u00a0[20] (I), RF-Solver-Edit\u00a0[23] (II) and MimicBrush\u00a0[13] (III).\n(a) The values show the proportion of users who prefer our method over the others.\n(b) The values represent the average scores given to each method by GPT-4o\u00a0[31].", "description": "Figure 6 presents a comparison of Edit Transfer with three other image editing methods: P2P, RF-Solver-Edit, and MimicBrush.  The comparison is done through both a user study and a VLM (Vision-Language Model) evaluation.  Part (a) shows the percentage of users who preferred Edit Transfer over the other methods. Part (b) displays the average scores assigned to each method by the GPT-40 model, indicating the overall performance perceived by the AI.", "section": "4. Experimental Results"}, {"figure_path": "https://arxiv.org/html/2503.13327/x8.png", "caption": "Figure 7: Influence of dataset scale.\n(a)\nSetting the number of training samples per editing type to Nc=2subscript\ud835\udc41\ud835\udc502N_{c}=2italic_N start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT = 2 is sufficient for learning effective non-rigid edits, even when the total number of editing types NT=10subscript\ud835\udc41\ud835\udc4710N_{T}=10italic_N start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT = 10.\n(b) Increasing NT=21subscript\ud835\udc41\ud835\udc4721N_{T}=21italic_N start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT = 21 further improves the model\u2019s ability to capture subtle local edits and enhances its generalization to cases where the editing example and query image are spatially misaligned.", "description": "This figure demonstrates the impact of dataset size on the model's performance in Edit Transfer.  Panel (a) shows that using only two training samples per editing type is enough to achieve effective non-rigid edits, even with a limited number (10) of editing types. Panel (b) illustrates that expanding the dataset to 21 editing types significantly improves the model's ability to handle subtle local edits and enhances its generalization capabilities, even when there is a spatial mismatch between the editing example and the query image.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.13327/x9.png", "caption": "Figure 8: Ours vs. w/o fine-tuning.\nWithout fine-tuning, Flux can only capture some of the pose information identified in Itsubscript\ud835\udc3c\ud835\udc61I_{t}italic_I start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT regardless of the relation between Is\u2192It\u2192subscript\ud835\udc3c\ud835\udc60subscript\ud835\udc3c\ud835\udc61I_{s}\\to I_{t}italic_I start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT \u2192 italic_I start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT and I^ssubscript^\ud835\udc3c\ud835\udc60\\hat{I}_{s}over^ start_ARG italic_I end_ARG start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT.\nIn contrast, with our few-shot fine-tuning, the model effectively learns the visual relation from example pairs and applied to I^ssubscript^\ud835\udc3c\ud835\udc60\\hat{I}_{s}over^ start_ARG italic_I end_ARG start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT.", "description": "This figure compares the performance of the proposed Edit Transfer method with and without fine-tuning.  The left column shows the results of the proposed method with fine-tuning, demonstrating its ability to accurately transfer complex pose information from a source image-edited image pair to a new query image. In contrast, the right column shows the results without fine-tuning, which only partially captures the pose information from the source image-edited image pair, indicating that fine-tuning is essential for effective visual relation learning. This highlights the few-shot learning capability of the proposed approach, where minimal training data enables effective adaptation to new images.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.13327/x10.png", "caption": "Figure 9: Investigating the alignment between text and visual example pairs.\nWhen the text prompt and visual demonstrations convey different semantics, the generated images \u2110^tsubscript^\u2110\ud835\udc61\\hat{\\mathcal{I}}_{t}over^ start_ARG caligraphic_I end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT tend to (a)(b) exhibit mixed semantics from both sources, and either (c) follow the text or (d) the visual demonstrations.\nNote that the red label indicates misalignment, while green label indicates alignment.", "description": "This figure explores the impact of inconsistencies between textual prompts and visual examples in image editing.  The experiment shows that when the text prompt and visual example convey different meanings, the generated images often reflect a blend of both (a, b).  In other cases, the generated image may solely reflect the text (c) or the visual example (d).  Red labels highlight mismatches between the prompt and generated image, while green indicates alignment.", "section": "4.4 Ablation Study"}, {"figure_path": "https://arxiv.org/html/2503.13327/x11.png", "caption": "Figure 10: Generalization performance of Edit Transfer.\nOur model demonstrates remarkable generalization by: (b) Generating novel pose variations within a given editing type, even if such variations were unseen during training; (c) Flexibly combining different editing types; (d) Transferring its capabilities across other species.", "description": "Figure 10 showcases the generalization capabilities of the Edit Transfer model.  The figure demonstrates that the model can (b) generate novel pose variations within a single editing type, even without seeing those variations during training; (c) flexibly combine multiple editing types to create more complex edits; and (d) successfully transfer the learned editing capabilities to images of different species (e.g., applying human pose edits to an animal). This highlights the model's ability to generalize beyond the specific training examples.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.13327/x12.png", "caption": "Figure 11: Limitations.\nOur method struggles with low-level properties transfer e.g. color.", "description": "Figure 11 demonstrates the limitations of the proposed Edit Transfer method.  Specifically, it shows that the model struggles with transferring low-level image properties, such as color changes. While the model excels at transferring complex spatial transformations and pose changes, as shown in previous figures, it cannot reliably change color attributes of an image, even when given a direct example of the desired color change. This highlights a key limitation in the approach and suggests areas for future research to improve the model's ability to handle low-level property transfers.", "section": "4.5 Discussion"}, {"figure_path": "https://arxiv.org/html/2503.13327/x13.png", "caption": "Figure 12: Image samples of each editing type in the dataset.", "description": "This figure shows examples of the 21 different editing types used in the Edit Transfer dataset. Each editing type is represented by two example image pairs: a source image and its corresponding edited version. The editing types cover a variety of transformations, including changes in pose, expression, clothing, and accessories. The figure provides a visual representation of the diversity and complexity of the edits that the model is trained to perform.", "section": "A. Implementation Details"}, {"figure_path": "https://arxiv.org/html/2503.13327/x14.png", "caption": "Figure 15: Prompt template of VLM score.", "description": "This figure details the prompt template used in the Vision-Language Model (VLM) evaluation section of the paper.  The prompt guides the evaluator through a role-play scenario, presenting them with source images and editing prompts.  Evaluators assess the quality of edited images generated by three different methods based on two metrics: Editing Accuracy (how well the edit matches the prompt) and Overall Performance (the overall quality and coherence of the edited image). The evaluators provide their scores in a structured JSON format for each image.", "section": "4.3 Comparisons with Baselines"}, {"figure_path": "https://arxiv.org/html/2503.13327/x15.png", "caption": "Figure 16: Additional experimental results of single edit transfer.", "description": "This figure displays further examples of single edit transfer, showcasing the model's ability to successfully apply learned transformations from a single source-target example to new query images.  The edits demonstrate the model's capability of handling diverse non-rigid transformations, including changes in pose, facial expressions, clothing, and background. The results highlight the model's flexibility and generalizability in transferring edits.", "section": "4. Experiments"}]