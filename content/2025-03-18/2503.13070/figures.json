[{"figure_path": "https://arxiv.org/html/2503.13070/x1.png", "caption": "Figure 1: Samples are taken from the corresponding papers of Diff-Instruct++ and RG-LCM. It can be observed that there exist certain artifacts in samples, e.g., repeated text/objects in the background. We hypothesize this comes from reward hacking.", "description": "Figure 1 shows examples of image generation from Diff-Instruct++ and RG-LCM, highlighting common artifacts like repeated text or objects in the background. These artifacts are hypothesized to result from reward hacking, where the model exploits reward mechanisms to produce images that are not truly aligned with the intended prompt but maximize the reward score.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.13070/x2.png", "caption": "Figure 2: 4-step samples at 1024 resolution generated by R0. The R0 here is trained from SD3-medium purely by reward maximization.", "description": "This figure displays images generated using the R0 model.  The model was trained solely on reward maximization without using any diffusion loss, highlighting the effectiveness of rewards in image generation. Four steps of the generation process are shown for each of the images, and all images are at a 1024x1024 resolution.", "section": "3. Rewards are enough"}, {"figure_path": "https://arxiv.org/html/2503.13070/x3.png", "caption": "Figure 3: Log Gradient norm curve of DI++ through training process. We use the best configuration reported in DI++\u00a0(Luo, 2024).", "description": "This figure shows the log gradient norms of both the distillation loss and the reward loss during the training process of the DI++ model.  The x-axis represents the training iteration, and the y-axis shows the log of the gradient norm. This visualization helps to illustrate the relative dominance of the reward loss compared to the distillation loss as training progresses, suggesting that reward signals become the dominant force in guiding the model's learning process, particularly in scenarios with complex generation conditions. The use of the best configuration from the DI++ model (as reported in Luo, 2024) ensures a fair comparison.", "section": "3. Rewards are enough"}, {"figure_path": "https://arxiv.org/html/2503.13070/x4.png", "caption": "Figure 4: Comparison with Referee can play\u00a0(Liu et\u00a0al., 2024). The baseline samples are taken from their paper. It can be seen that our method has significantly better visual quality.", "description": "This figure compares the image generation quality of the proposed method (R0) against the baseline method, Referee can play (Liu et al., 2024).  The images shown are examples generated by each method for the same prompts. The comparison highlights the superior visual quality achieved by R0, showing more detail, realism, and overall better adherence to the prompt compared to the baseline method.", "section": "3. Rewards are enough"}, {"figure_path": "https://arxiv.org/html/2503.13070/x5.png", "caption": "Figure 5: Samples diffusion models with 100 NFE and 7.5 CFG by varying the \u03b7\ud835\udf02\\etaitalic_\u03b7 in sampling. The samples are generated from the same initial noise. The prompt is \u201cA pikachu, best quality\u201d.", "description": "This figure displays the results of an experiment using diffusion models to generate images of a Pikachu.  The models were run with 100 noise-removing steps (NFE) and a classifier-free guidance (CFG) scale of 7.5. The key variable manipulated was \u03b7 (eta), a parameter controlling the sampling process during image generation.  The images shown are all generated from the same starting point (the same initial noise), allowing a clear visualization of how different eta values impact the final output.  The prompt used for all generations was: \u201cA Pikachu, best quality\u201d. This illustrates the effect of this hyperparameter on the generated image.", "section": "3.2 Parameterization of Generator"}, {"figure_path": "https://arxiv.org/html/2503.13070/x6.png", "caption": "Figure 6: Cosine Similarity between DI gradient and Reward gradient.", "description": "This figure displays the cosine similarity between the gradients of the distillation loss and the reward loss during the training process of the Diff-Instruct++ model.  The x-axis represents the training iteration, and the y-axis shows the cosine similarity. A high cosine similarity indicates that the updates from the distillation loss and reward loss are aligned, while a low similarity implies that the updates are pulling the model in different directions. This graph helps to visually illustrate the dominance of reward loss gradient over distillation loss gradient during the training process, as discussed in the paper.", "section": "3. Rewards are enough"}, {"figure_path": "https://arxiv.org/html/2503.13070/x7.png", "caption": "Figure 7: The prior distillation-based reward maximization methods collapse when the reward is chosen to be HPS v2.1. In contrast, our R0 still works well, benefiting from the proposed effective regularization technique.", "description": "This figure compares the performance of prior distillation-based reward maximization methods (like RG-LCM and DI++) and the proposed R0 method when using HPS v2.1 as the reward.  The prior methods fail to generate reasonable images, indicated by a 'collapse' in their performance. In contrast, R0, due to its effective regularization techniques, is robust and produces satisfactory results even with this challenging reward function. This illustrates the robustness and effectiveness of R0's regularization strategies.", "section": "3.3. Regularization"}, {"figure_path": "https://arxiv.org/html/2503.13070/x8.png", "caption": "Figure 8: Training progress of various metrics over iterations. It can be seen that the normalized gradient shows better performance. This is evaluated on 1k prompts from HPS benchmark.", "description": "This figure displays the training progress curves for various metrics, specifically focusing on the HPS (Human Preference Score) benchmark.  The curves illustrate how different metrics change over a series of training iterations (50 iterations are shown on the x-axis).  It highlights that using normalized gradients leads to better performance during the training process compared to unnormalized gradients.  The evaluation is conducted on 1000 prompts from the HPS benchmark, providing a robust assessment of the model's performance.", "section": "3.3 Regularization"}, {"figure_path": "https://arxiv.org/html/2503.13070/x9.png", "caption": "Figure 9: Four-step samples generated by R0. We observed that the quality of the image monotonically increases with the gradual increment of the reward count.", "description": "This figure showcases a series of images generated by the R0 model after 4 steps. Each image corresponds to a different number of reward signals used during the generation process.  The purpose is to illustrate how the visual quality of the generated image improves as more reward signals are integrated into the R0 model, indicating a monotonic relationship between the number of rewards and image quality. The improvement in image quality suggests that the use of multiple reward functions enhances the overall result.", "section": "3.4 Power of multiple rewards"}, {"figure_path": "https://arxiv.org/html/2503.13070/x10.png", "caption": "Figure 10: The complementary effect of different reward. We do not use random \u03b7\ud835\udf02\\etaitalic_\u03b7 sampling and set small weight regularization in training here to highlight the complementary effect between rewards.", "description": "This figure demonstrates that combining multiple reward signals during training leads to better image generation compared to using a single reward. It showcases the complementary effect of using HPS, Image Reward, and Clip Score rewards, resulting in improved image quality and a closer alignment between the generated image and the text prompt. The experiment was conducted without the use of random \u03b7 sampling and with minimal weight regularization to highlight the benefits of combining rewards.", "section": "4. Photo-Realistic Generation From Rewards"}, {"figure_path": "https://arxiv.org/html/2503.13070/x11.png", "caption": "Figure 11: The effect of regularization loss. Images are from the same initial noise.", "description": "This figure demonstrates the impact of the regularization loss on image generation using the R0 model.  It showcases multiple images generated from the same initial noise, with and without the regularization loss applied. The differences in image quality and features illustrate how the regularization term influences the output and helps prevent issues like reward hacking, ensuring the generator stays within a desirable distribution and produces images of better quality.", "section": "4. Photo-Realistic Generation From Rewards"}, {"figure_path": "https://arxiv.org/html/2503.13070/x12.png", "caption": "Figure 12: Comparison on the convergence speed between R0 and R0+. This is evaluated on 1k prompts from HPS benchmark.", "description": "Figure 12 presents a comparison of the convergence speed achieved by two different models, R0 and R0+, during training.  The training involved 1000 prompts sourced from the HPS (Human Preference Score) benchmark dataset. The figure likely shows plots of various metrics (e.g., FID, HPS scores, CLIP scores) against training iterations. This visual comparison allows for a quantitative assessment of the efficiency improvements gained by incorporating intermediate supervision into the training process of the R0+ model.  The figure is useful for determining the impact of the training modifications introduced in the R0+ model on training efficiency.", "section": "4. Photo-Realistic Generation From Rewards"}, {"figure_path": "https://arxiv.org/html/2503.13070/x13.png", "caption": "Figure 13: Path comparison between R0 and R0+. The prompt is \u201cTwo dogs, best quality\u201d.", "description": "This figure compares the generation process of the R0 and R0+ models, which are both few-step text-to-image generation models.  The prompt used for both models is \"Two dogs, best quality\".  The images shown illustrate the intermediate steps in the generation process, from initial noise to final image. By comparing the intermediate steps for both models, the figure highlights how the introduction of intermediate supervision in R0+ leads to a smoother and more stable generation process compared to R0, which shows more artifacts during intermediate steps before converging to the final image.", "section": "4. Photo-Realistic Generation From Rewards"}, {"figure_path": "https://arxiv.org/html/2503.13070/x14.png", "caption": "Figure 14: The effect of high-resolution guidance loss. Images are from the same initial noise.", "description": "This figure demonstrates the impact of incorporating a high-resolution guidance loss during the training of the R0+ model.  It visually compares image generation results from the same initial noise input, with and without the high-resolution guidance loss applied. The comparison highlights the improved image quality, particularly in terms of fine details and clarity, that the high-resolution guidance loss contributes to.", "section": "4. Photo-Realistic Generation From Rewards"}, {"figure_path": "https://arxiv.org/html/2503.13070/x15.png", "caption": "Figure 15: Qualitative comparisons of R0 against distillation-based methods and diffusion base models on SD-v1.5 backbone. All images are generated by the same initial noise. We surprisingly observe that our proposed R0 has better image quality and text-image alignment compared to prior distillation-based reward maximization methods in 4-step text-to-image generation.", "description": "This figure presents a qualitative comparison of image generation results using different methods.  It compares R0 (the proposed method) to several other approaches including various distillation-based methods, all using the same SD-v1.5 model backbone and starting from identical initial noise conditions.  The goal is to demonstrate that R0 achieves superior image quality and better alignment between the generated image and the corresponding text prompt, even with fewer steps (4 steps in this case), when compared to distillation-based reward maximization techniques.", "section": "4. Photo-Realistic Generation From Rewards"}, {"figure_path": "https://arxiv.org/html/2503.13070/x16.png", "caption": "Figure 16: Qualitative comparison against competing methods and applications in downstream tasks.", "description": "Figure 16 presents a qualitative comparison of the proposed R0 model against other competing methods on various downstream tasks, including image editing and integration with ControlNet.  The image shows that R0, even without relying on distillation losses, achieves comparable or superior performance in downstream applications. It demonstrates versatility in handling image editing (changing a squirrel to a cat), and seamless integration with ControlNet for image generation guided by Canny edge detection, while retaining reasonable image quality. This showcases R0's effectiveness and adaptability beyond simple image generation.", "section": "Additional Application"}]