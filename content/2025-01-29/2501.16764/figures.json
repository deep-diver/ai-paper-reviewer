[{"figure_path": "https://arxiv.org/html/2501.16764/x1.png", "caption": "Figure 1: Comparison with Previous 3D Diffusion Generative Models. (1) Native 3D methods and (2) rendering-based methods encounter challenges in training 3D diffusion models from scratch with limited 3D data. (3) Reconstruction-based methods struggle with inconsistencies in generated multi-view images. In contrast, (4) DiffSplat leverages pretrained image diffusion models for the direct 3DGS generation, effectively utilizing 2D diffusion priors and maintaining 3D consistency. \u201cGT\u201d refers to ground-truth samples in a 3D representation used for diffusion loss computation.", "description": "This figure compares four different approaches to 3D content generation using diffusion models.  Method (1) trains a 3D diffusion model directly on 3D data, which is limited by the scarcity of high-quality 3D datasets. Method (2) uses 2D supervision via differentiable rendering techniques but suffers from the same dataset limitation. Method (3) leverages pre-trained 2D diffusion models to generate multi-view images that are then used to reconstruct 3D content, but suffers from consistency issues in the generated multi-view images.  In contrast, the proposed method (4), DiffSplat, directly generates 3D Gaussian splats using pre-trained image diffusion models and leverages 2D priors while maintaining 3D consistency. This approach bypasses the need for large-scale 3D datasets, directly utilizing the power of web-scale 2D image data.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2501.16764/x2.png", "caption": "Figure 2: Method Overview.\n(1) A lightweight reconstruction model provides high-quality structured representation for \u201cpseudo\u201d dataset curation. (2) Image VAE is fine-tuned to encode Gaussian splat properties into a shared latent space. (3) DiffSplat is natively capable of generating 3D contents by image and text conditions utilizing 2D priors from text-to-image diffusion models.", "description": "This figure illustrates the three main stages of the DIFFSPLAT method.  First, a lightweight reconstruction model takes multi-view images as input and generates a structured representation of 3D Gaussian splats. This serves to create a 'pseudo' dataset for training. Second, an image Variational Autoencoder (VAE) is fine-tuned; its purpose is to encode the properties of these Gaussian splats into a lower-dimensional latent space shared with the pre-trained image diffusion model.  Finally, the core DIFFSPLAT model uses this latent space to generate new 3D content conditioned on either text or image inputs. This leverages the power of pre-trained 2D image diffusion models to enable native 3D Gaussian splat generation.", "section": "3 METHOD"}, {"figure_path": "https://arxiv.org/html/2501.16764/x3.png", "caption": "Figure 3: Qualitative Results and Comparisons on Text-conditioned 3D Generation. More visualizations of DiffSplat results are provided in Appendix Figure\u00a09, 10 and 11.", "description": "This figure presents a qualitative comparison of text-to-3D generation results between DiffSplat and several other state-of-the-art methods.  Each model is given the same text prompt, and the generated 3D objects are shown.  DiffSplat's outputs are compared visually with results from GVGEN, LN3Diff, 3DTopia, LGM, and GRM. The figure highlights DiffSplat's ability to generate high-quality, detailed 3D models that closely match the input text descriptions, in comparison to other models. Additional visualizations are available in the appendix for a more comprehensive evaluation.", "section": "4.2 Text-Conditioned Generation"}, {"figure_path": "https://arxiv.org/html/2501.16764/x4.png", "caption": "Figure 4: Qualitative Results and Comparisons on Image-conditioned 3D Generation. More visualizations of DiffSplat results are provided in Appendix Figure\u00a012, 13 and 14.", "description": "Figure 4 presents a qualitative comparison of image-conditioned 3D generation results using different methods.  It showcases the 3D models generated from single input images by DiffSplat and other state-of-the-art methods.  The figure highlights DiffSplat's ability to produce high-quality 3D models that accurately reflect the input images. For a more comprehensive view of the results generated by DiffSplat, refer to Appendix Figures 12, 13, and 14.", "section": "4.3 Image-Conditioned Generation"}, {"figure_path": "https://arxiv.org/html/2501.16764/x5.png", "caption": "Figure 5: Controllable Generation. ControlNet can seamlessly adapt to DiffSplat for controllable text-to-3D generation in various formats, such as normal and depth maps, and Canny edges.", "description": "Figure 5 showcases the adaptability of ControlNet with DiffSplat.  ControlNet's ability to incorporate various image formats, including normal maps, depth maps, and Canny edge maps, as guidance for text-to-3D generation is highlighted. The figure demonstrates how these different input formats enhance the control and precision of the 3D model generation process within the DiffSplat framework.  This allows for more fine-grained control over the final output, resulting in more realistic and nuanced 3D objects.", "section": "3 METHOD"}, {"figure_path": "https://arxiv.org/html/2501.16764/x6.png", "caption": "Table 3: Ablation study of inputs for structured splat reconstruction.", "description": "This table presents an ablation study on the impact of different inputs used for reconstructing structured splat representations.  The goal is to determine which inputs are most effective for generating high-quality 3D Gaussian splat grids. The experiment compares the performance of using RGB images only versus incorporating additional geometric guidance such as normal and coordinate maps.  The results are quantified using PSNR, SSIM, and LPIPS metrics to evaluate the quality of the reconstructed splats.", "section": "3.1 Data Curation: Structured Splat Reconstruction"}, {"figure_path": "https://arxiv.org/html/2501.16764/x7.png", "caption": "Table 4: Ablation study for Gaussian splat property auto-encoding strategies.", "description": "This table presents an ablation study on different strategies for auto-encoding Gaussian splat properties.  It compares the performance of various methods for encoding these properties into a latent space suitable for use with a diffusion model.  The methods compared involve using a pretrained VAE (Variational Autoencoder) either frozen or fine-tuned, and with or without incorporating a rendering loss into the training process. The table shows the impact of these choices on the quality of the reconstructed Gaussian splat grids, measured by metrics such as PSNR, SSIM, and LPIPS.", "section": "3.2 Splat Latents"}, {"figure_path": "https://arxiv.org/html/2501.16764/x8.png", "caption": "Figure 6: Ablation of \u2112rendersubscript\u2112render\\mathcal{L}_{\\text{render}}caligraphic_L start_POSTSUBSCRIPT render end_POSTSUBSCRIPT. Both text- (1st row) and image-conditioned (2nd row) DiffSplat with \u2112rendersubscript\u2112render\\mathcal{L}_{\\text{render}}caligraphic_L start_POSTSUBSCRIPT render end_POSTSUBSCRIPT produces more aesthetic and textured 3D content with fewer translucent floaters.", "description": "Figure 6 demonstrates the effect of the rendering loss (\u2112render) on the quality of 3D content generated by DiffSplat.  The top row shows results from text-conditioned generation, and the bottom row shows image-conditioned generation.  In both cases, the inclusion of the rendering loss leads to more visually appealing results with richer textures and a reduction in translucent artifacts (areas that appear semi-transparent). The comparison highlights the importance of the rendering loss in improving the overall quality and realism of the 3D models produced by the DiffSplat model.", "section": "4.5.2 DIFFSPLAT 3D GENERATION"}, {"figure_path": "https://arxiv.org/html/2501.16764/x9.png", "caption": "Figure 7: Splat Latents Visualization. 3DGS properties are structured in grids. \u201cDecoded GS\u201d shows the splat latents decoded by an image diffusion VAE.", "description": "Figure 7 visualizes the latent representations (splat latents) used in the DIFFSPLAT model.  It shows how the properties of 3D Gaussian splats\u2014color, depth, opacity, scale, and rotation\u2014are organized into structured 2D grids.  The figure highlights how these splat grids are encoded by a Variational Autoencoder (VAE) and then decoded using a pretrained image diffusion model's decoder.  The 'Decoded GS' section demonstrates the result of decoding the splat latents back into an image-like representation, revealing that they maintain meaningful information related to the 3D object's appearance and structure. This visualization helps illustrate the core concept of DIFFSPLAT's approach:  using image diffusion models to generate and manipulate 3D Gaussian splat representations.", "section": "3.2 Splat Latents"}, {"figure_path": "https://arxiv.org/html/2501.16764/x10.png", "caption": "Figure 8: Controllable Generation with Multi-modal Conditions. DiffSplat can effectively utilize both text and image conditions for single-view reconstruction with text understanding.", "description": "Figure 8 showcases the capability of DiffSplat to generate 3D models using both text and image cues.  Instead of requiring multiple views, it successfully reconstructs a 3D object from a single image, demonstrating its ability to integrate textual context with visual information for improved accuracy and detail in the reconstruction process. The examples visually demonstrate how different image and text prompts lead to variations in the generated 3D output. This highlights the model's capacity for text understanding in conjunction with visual input during reconstruction.", "section": "4.5 ABLATION AND ANALYSIS"}, {"figure_path": "https://arxiv.org/html/2501.16764/x11.png", "caption": "Figure 9: More results of text-conditioned DiffSplat.", "description": "This figure showcases various 3D models generated by the DiffSplat model using different text prompts as input.  Each row displays a different text prompt and the corresponding generated 3D object from multiple viewpoints. The models are detailed and realistic, demonstrating the model's capability for generating high-quality 3D content based on text descriptions. The variety of objects presented highlights the versatility of DiffSplat in generating diverse 3D scenes.", "section": "4.2 Text-Conditioned Generation"}, {"figure_path": "https://arxiv.org/html/2501.16764/x12.png", "caption": "Figure 10: More results of text-conditioned DiffSplat.", "description": "This figure displays additional examples of 3D models generated by the DiffSplat model using text prompts as input.  It showcases the model's ability to generate diverse and high-quality 3D objects from a range of descriptive text prompts, highlighting its capacity for detailed and nuanced object creation based on textual descriptions. The figure visually demonstrates the variety of objects, textures, and contexts that DiffSplat can generate from text alone.", "section": "4.2 Text-Conditioned Generation"}, {"figure_path": "https://arxiv.org/html/2501.16764/x13.png", "caption": "Figure 11: More results of text-conditioned DiffSplat.", "description": "This figure displays more examples of 3D models generated by the DiffSplat model using text prompts as input.  Each row shows a different text prompt (e.g., \"A golden retriever with a blue bowtie\") and the corresponding generated 3D Gaussian splat representations from multiple viewpoints.  The figure demonstrates the model's ability to generate diverse and detailed 3D objects based on textual descriptions.", "section": "4.2 Text-Conditioned Generation"}, {"figure_path": "https://arxiv.org/html/2501.16764/x14.png", "caption": "Figure 12: More results of image-conditioned DiffSplat.", "description": "This figure showcases additional examples of 3D models generated using the image-conditioned DiffSplat method.  Each row presents a single input image followed by multiple views (front, back, and side) of the corresponding 3D object generated by the model.  The variety of objects demonstrates the model's ability to handle diverse shapes and textures.", "section": "4.3 Image-Conditioned Generation"}]