[{"figure_path": "https://arxiv.org/html/2501.17161/x1.png", "caption": "Figure 1: A comparative study of RL and SFT on the visual navigation environment V-IRL\u00a0(Yang et\u00a0al., 2024a) for OOD generalization. OOD curves represent performance on the same task, using a different textual action space. See detailed descriptions of the task in Section\u00a05.1.", "description": "Figure 1 illustrates a comparative analysis of Reinforcement Learning (RL) and Supervised Fine-Tuning (SFT) methods applied to the V-IRL visual navigation task.  The key focus is on out-of-distribution (OOD) generalization.  The figure shows how the success rate of each method changes as computational resources used for training increase.  Importantly, separate curves show the in-distribution performance (using the same textual action space during training and testing) and out-of-distribution performance (using a different action space during testing than during training). This comparison highlights the relative ability of RL and SFT to generalize learned knowledge to new, unseen scenarios.", "section": "5. Results"}, {"figure_path": "https://arxiv.org/html/2501.17161/x4.png", "caption": "Figure 2: An example of the sequential revision formulation with a verifier. The model generate the next answer \ud835\udc2ft+1outsubscriptsuperscript\ud835\udc2fout\ud835\udc611\\mathbf{v}^{\\text{out}}_{t+1}bold_v start_POSTSUPERSCRIPT out end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT conditioned on all previous answers and information (\ud835\udc2fiout,\ud835\udc2ftver,0\u2264i\u2264t)subscriptsuperscript\ud835\udc2fout\ud835\udc56subscriptsuperscript\ud835\udc2fver\ud835\udc610\ud835\udc56\ud835\udc61(\\mathbf{v}^{\\text{out}}_{i},\\mathbf{v}^{\\text{ver}}_{t},0\\leq i\\leq t)( bold_v start_POSTSUPERSCRIPT out end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , bold_v start_POSTSUPERSCRIPT ver end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , 0 \u2264 italic_i \u2264 italic_t ) from the verifier.", "description": "Figure 2 illustrates the multi-turn reinforcement learning process with a verifier.  The model doesn't simply generate a single answer; instead, it iteratively refines its responses based on feedback from a verifier.  At each step, the model receives the initial prompt (system prompt), all previous model outputs (answers), and the verifier's feedback on those previous answers.  This combined information forms the input for generating the next answer. The verifier evaluates the model's output and provides a reward and additional textual information (e.g., indicating correctness or errors), which is then incorporated into the next round of the process. This iterative refinement allows the model to learn from its mistakes and improve its answer accuracy.", "section": "4. Evaluation Tasks"}, {"figure_path": "https://arxiv.org/html/2501.17161/x5.png", "caption": "Figure 3: An template of our prompt update for constructing \ud835\udc2ft+1insubscriptsuperscript\ud835\udc2fin\ud835\udc611\\mathbf{v}^{\\text{in}}_{t+1}bold_v start_POSTSUPERSCRIPT in end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT. The  brown parts marks the task and related information, and the  purple parts denote the state (st)subscript\ud835\udc60\ud835\udc61(s_{t})( italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) specific info. The  blue and  red describe the output from the  model and  verifier, respectively.", "description": "Figure 3 illustrates how the prompt is updated iteratively in a multi-turn reinforcement learning setting.  The input prompt for each turn (v<sup>in</sup><sub>t+1</sub>) is constructed by concatenating the previous turn's prompt, the model's output from the previous turn (v<sup>out</sup><sub>t</sub>), and the verifier's output from the previous turn (ver<sub>t</sub>). The figure highlights the different components of the prompt: the brown sections represent the task description and other task-related information that remains constant across turns, while the purple sections show the state-specific information (s<sub>t</sub>) which changes at each turn. The blue section is the model's output (v<sup>out</sup><sub>t</sub>), and the red section is the verifier's feedback (ver<sub>t</sub>). This iterative prompt construction allows the model to refine its response based on previous interactions and feedback.", "section": "4. Evaluation Tasks"}, {"figure_path": "https://arxiv.org/html/2501.17161/x6.png", "caption": "Figure 4: Demonstration of one navigation task in V-IRL. Agent navigates from place to place following the given linguistic navigation instructions in V-IRL. The navigation procedure is shown at the top, with the navigation instructions displayed below. Visual observation-related information is highlighted in  green, while action-related information is marked in  orange.", "description": "This figure illustrates a single navigation task within the V-IRL (Visual-IRL) environment.  The top part visually depicts the agent's journey through a series of locations.  The agent moves from place to place based on textual instructions.  The sequence of locations and actions, along with corresponding visual observations, shows a step-by-step progression of the navigation process. Key elements in the visual observations are color-coded:  Green highlights visual information that guides the agent, while orange indicates the actions the agent takes.", "section": "4. Evaluation Tasks"}, {"figure_path": "https://arxiv.org/html/2501.17161/x7.png", "caption": "Figure 5: Success rate (%) - GFLOPs trendlines for RL and SFT on GeneralPoints and V-IRL. The top row shows in-distribution performance, while the bottom row shows out-of-distribution performance. Results are presented for both pure language (-L) and vision-language (-VL) variants of each task. For GeneralPoints, we report the episode success rate, while for V-IRL, we report per-step accuracy with overall success rate in\u00a0Figures\u00a01 and\u00a019. Detailed evaluation setups (and curve smoothing) are provided in\u00a0Section\u00a0C.3.", "description": "This figure displays a comparison of the performance of Reinforcement Learning (RL) and Supervised Fine-Tuning (SFT) on two tasks: GeneralPoints and V-IRL.  The tasks are presented in two variants each: one using only language ('-L') and the other using both language and vision ('-VL'). The top half of the figure shows the success rate (in-distribution) of each method on each task variant, plotted against the amount of computational resources (GFLOPs) used for training. The bottom half shows the same, but for out-of-distribution performance, testing the models' ability to generalize to unseen data or rules. For GeneralPoints, the metric used is episode success rate, while for V-IRL, per-step accuracy is used with overall success rate referenced to other figures in the paper.  The figure demonstrates the comparative generalization capabilities of RL versus SFT for both textual and visual tasks.", "section": "5. Results"}, {"figure_path": "https://arxiv.org/html/2501.17161/x8.png", "caption": "Figure 6: Comparison of out-of-distribution performance under rule variants. We report the success rate for GeneralPoints and per-step-accuracy for V-IRL. For each subplot, RL and SFT are trained with equal computation, and their shared initial checkpoint (marked as Init) is set as baseline. Detailed setups are provided in\u00a0Section\u00a0C.3.", "description": "This figure compares the out-of-distribution (OOD) generalization performance of Reinforcement Learning (RL) and Supervised Fine-Tuning (SFT) on two tasks: GeneralPoints (a rule-based arithmetic reasoning task) and V-IRL (a visual navigation task).  For each task, both RL and SFT are trained with the same computational budget. The figure shows the success rates (GeneralPoints) and per-step accuracy (V-IRL) for both in-distribution (ID) and OOD scenarios. The 'Init' point represents the performance of the model before any post-training.  The results demonstrate the generalization capabilities of RL compared to SFT, which tends to memorize the training data.", "section": "5. Results"}, {"figure_path": "https://arxiv.org/html/2501.17161/x9.png", "caption": "Figure 7: Comparison of out-of-distribution performance under visual variants. Similar to\u00a0Figures\u00a05 and\u00a06, we present both the performance dynamics (shown as lines) and final performance (shown as bars) for visual out-of-distribution evaluations. The previous state-of-the-art on V-IRL VLN mini benchmark\u00a0(Yang et\u00a0al., 2024a) is marked in  orange. Detailed evaluation setups (and curve smoothing) are provided in\u00a0Section\u00a0C.3.", "description": "Figure 7 displays a comparison of the out-of-distribution performance of RL and SFT models when faced with visual variations in the V-IRL VLN task. The figure shows both the performance trends over training computation (as lines) and the final performance levels (as bars) for both in-distribution and out-of-distribution scenarios.  This allows for a direct comparison of how well each method generalizes to unseen visual data. Notably, the previous state-of-the-art result from Yang et al. (2024a) is included as a reference point (in orange), highlighting RL's superior performance. More details on the experimental setup and data smoothing techniques can be found in Section C.3 of the paper. ", "section": "5.2. Generalization in Visual Out-of-Distribution Tasks"}, {"figure_path": "https://arxiv.org/html/2501.17161/extracted/6162311/figures/general-point.jpeg", "caption": "Figure 8: Recognition vs. success rate for RL and SFT under different variants in GP-VL. We report both in-distribution ( red) and OOD ( blue) performance of recognition (y-axis) and episode success rate (x-axis). We denote the training compute of each data point via transparency (color bar) while connected (\u22c6\u22c6\\star\u22c6-\u2218\\circ\u2218) pairs are evaluated using same checkpoints. As scaling up post-training compute, RL improves both recognition and overall accuracy, while SFT shows opposite effect.", "description": "Figure 8 presents a comparative analysis of Reinforcement Learning (RL) and Supervised Fine-Tuning (SFT) methods on the GeneralPoints-VL task (a visual variant of an arithmetic reasoning card game). The figure uses two line graphs to showcase the relationship between visual recognition accuracy and episode success rate for both RL and SFT.  The x-axis represents the episode success rate, and the y-axis represents the visual recognition accuracy. Different colors and transparency levels of the lines represent varying computational budgets used during training. Connected star and circle pairs indicate that these data points share the same checkpoint.  The results demonstrate that as computational resources increase, RL improves both visual recognition accuracy and the overall task success rate, unlike SFT which shows the opposite trend.", "section": "5.3. RL Improves Visual Capabilities"}, {"figure_path": "https://arxiv.org/html/2501.17161/extracted/6162311/figures/virl_2x2grid/2x2grid_example.jpeg", "caption": "Figure 9: RL experiments on GP-L without SFT initialization. All trials fail due to poor instruction following capability of the base model.", "description": "This figure displays the results of reinforcement learning (RL) experiments conducted on the General Points-Language (GP-L) task without any prior supervised fine-tuning (SFT).  The experiment aimed to assess whether RL alone could effectively train the language model to solve arithmetic reasoning problems presented in a textual format.  The results show that all RL training attempts failed. This failure is attributed to the base language model's inherent deficiency in accurately following instructions, highlighting the critical role SFT plays in stabilizing the model's behavior and enabling successful RL training.", "section": "5. Results"}, {"figure_path": "https://arxiv.org/html/2501.17161/x10.png", "caption": "Figure 10: In-distribution vs. OOD performance growth on GP-L. We record RL experiments with different number of verification iterations (VIter) as scaling up training compute (color transparency).", "description": "This figure shows how the in-distribution and out-of-distribution performance of the General Points (GP-L) task changes with increasing computational resources used for training.  The x-axis represents the training computation in GigaFLOPS (GFLOPS). The y-axis shows the percentage growth of both in-distribution (ID) and out-of-distribution (OOD) performance.  Different lines represent different numbers of verification iterations used during training (VIter). The color transparency of the lines represents the amount of computational resources used. The results indicate that increasing computation generally improves both ID and OOD performance, especially when more verification iterations are used. This suggests that using more verification iterations during RL training leads to more generalizable models.", "section": "5. Results"}, {"figure_path": "https://arxiv.org/html/2501.17161/x11.png", "caption": "Figure 11: An example of our prompt update for constructing \ud835\udc2ft+1insubscriptsuperscript\ud835\udc2fin\ud835\udc611\\mathbf{v}^{\\text{in}}_{t+1}bold_v start_POSTSUPERSCRIPT in end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT using \ud835\udc2ftin,\ud835\udc2ftoutsubscriptsuperscript\ud835\udc2fin\ud835\udc61subscriptsuperscript\ud835\udc2fout\ud835\udc61\\mathbf{v}^{\\text{in}}_{t},\\mathbf{v}^{\\text{out}}_{t}bold_v start_POSTSUPERSCRIPT in end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_v start_POSTSUPERSCRIPT out end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT and \ud835\udc2ftversubscriptsuperscript\ud835\udc2fver\ud835\udc61\\mathbf{v}^{\\text{ver}}_{t}bold_v start_POSTSUPERSCRIPT ver end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT. This example provides an optional vision input for VLMs, adding a visual recognition challenge. The  brown parts marks the task and related information, and the  purple parts denote the state (st)subscript\ud835\udc60\ud835\udc61(s_{t})( italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) specific info. The  blue and  red describe the output from the  model and  verifier, respectively.", "description": "Figure 11 illustrates how the model's input prompt is updated iteratively during the multi-turn reinforcement learning process.  The initial prompt (v<sup>in</sup><sub>t</sub>) contains the task description. After each turn, the model's output (v<sup>out</sup><sub>t</sub>) and the verifier's feedback (v<sup>ver</sup><sub>t</sub>) are appended to the prompt, creating the new input for the next turn (v<sup>in</sup><sub>t+1</sub>). The figure highlights the different components of the prompt: the task instructions (brown), the current state information (purple), the model's response (blue), and the verifier's response (red).  The inclusion of an image in the prompt demonstrates how the model handles both textual and visual information, adding a visual recognition challenge for visual language models.", "section": "A. Details on the General Points Environment"}, {"figure_path": "https://arxiv.org/html/2501.17161/x12.png", "caption": "Figure 12: An example of our prompt update for constructing \ud835\udc2ft+1insubscriptsuperscript\ud835\udc2fin\ud835\udc611\\mathbf{v}^{\\text{in}}_{t+1}bold_v start_POSTSUPERSCRIPT in end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT using \ud835\udc2ftin,\ud835\udc2ftoutsubscriptsuperscript\ud835\udc2fin\ud835\udc61subscriptsuperscript\ud835\udc2fout\ud835\udc61\\mathbf{v}^{\\text{in}}_{t},\\mathbf{v}^{\\text{out}}_{t}bold_v start_POSTSUPERSCRIPT in end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_v start_POSTSUPERSCRIPT out end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT and \ud835\udc2ftversubscriptsuperscript\ud835\udc2fver\ud835\udc61\\mathbf{v}^{\\text{ver}}_{t}bold_v start_POSTSUPERSCRIPT ver end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT. This example provides an optional vision input for VLMs, adding a visual recognition challenge. The  brown parts marks the task and related information, and the  purple parts denote the state (st)subscript\ud835\udc60\ud835\udc61(s_{t})( italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) specific info. The  blue and  red describe the output from the  model and  verifier, respectively.", "description": "Figure 12 illustrates how the prompt is updated iteratively in a multi-turn reinforcement learning setting.  The initial prompt (\ud835\udc63\ud835\udc56\ud835\udc5b) includes the task description and any relevant information.  After each turn, the model's output (\ud835\udc63\ud835\udc5c\ud835\udc62\ud835\udc61) and the verifier's feedback (\ud835\udc63\ud835\udc63\ud835\udc52\ud835\udc5f) are appended to the prompt to form the next input (\ud835\udc63\ud835\udc56\ud835\udc5b\ud835\udc61+1). This example showcases a visual input variant where an image of cards is included, adding a layer of visual processing. Different colors highlight different parts of the prompt: brown for the task and related information, purple for the current state\u2019s information, blue for the model\u2019s output and red for the verifier's output.", "section": "A. Details on the General Points Environment"}, {"figure_path": "https://arxiv.org/html/2501.17161/x13.png", "caption": "Figure 13: An example of our prompt update for constructing \ud835\udc2ft+1insubscriptsuperscript\ud835\udc2fin\ud835\udc611\\mathbf{v}^{\\text{in}}_{t+1}bold_v start_POSTSUPERSCRIPT in end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT using \ud835\udc2ftin,\ud835\udc2ftoutsubscriptsuperscript\ud835\udc2fin\ud835\udc61subscriptsuperscript\ud835\udc2fout\ud835\udc61\\mathbf{v}^{\\text{in}}_{t},\\mathbf{v}^{\\text{out}}_{t}bold_v start_POSTSUPERSCRIPT in end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_v start_POSTSUPERSCRIPT out end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT and \ud835\udc2ftversubscriptsuperscript\ud835\udc2fver\ud835\udc61\\mathbf{v}^{\\text{ver}}_{t}bold_v start_POSTSUPERSCRIPT ver end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT. The  brown parts marks the task and related information, and the  purple parts denote the state (st)subscript\ud835\udc60\ud835\udc61(s_{t})( italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) specific info. The  brown parts marks the task and related information, and the  purple parts denote the state (st)subscript\ud835\udc60\ud835\udc61(s_{t})( italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) specific info. The  blue and  red describe the output from the  model and  verifier, respectively.", "description": "Figure 13 illustrates the process of updating the prompt used by the model at each step of the multi-turn reinforcement learning process.  The input to the model at time step t+1 (v<sup>in</sup><sub>t+1</sub>) is constructed by concatenating the previous input (v<sup>in</sup><sub>t</sub>), the model's output at time step t (v<sup>out</sup><sub>t</sub>), and the verifier's output at time step t (v<sup>ver</sup><sub>t</sub>). The figure highlights the different parts of the prompt: the brown sections represent task instructions and related information, while the purple sections show state-specific details relevant to the current step in the process. Finally, the blue and red text indicate outputs from the model and verifier, respectively.", "section": "4. Evaluation Tasks"}, {"figure_path": "https://arxiv.org/html/2501.17161/x14.png", "caption": "Figure 14: An example of our prompt update for constructing \ud835\udc2ft+1insubscriptsuperscript\ud835\udc2fin\ud835\udc611\\mathbf{v}^{\\text{in}}_{t+1}bold_v start_POSTSUPERSCRIPT in end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT using \ud835\udc2ftin,\ud835\udc2ftoutsubscriptsuperscript\ud835\udc2fin\ud835\udc61subscriptsuperscript\ud835\udc2fout\ud835\udc61\\mathbf{v}^{\\text{in}}_{t},\\mathbf{v}^{\\text{out}}_{t}bold_v start_POSTSUPERSCRIPT in end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_v start_POSTSUPERSCRIPT out end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT and \ud835\udc2ftversubscriptsuperscript\ud835\udc2fver\ud835\udc61\\mathbf{v}^{\\text{ver}}_{t}bold_v start_POSTSUPERSCRIPT ver end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT. This example provides an optional vision input for VLMs, adding a visual recognition challenge. The  brown parts marks the task and related information, and the  purple parts denote the state (st)subscript\ud835\udc60\ud835\udc61(s_{t})( italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) specific info. The  blue and  red describe the output from the  model and  verifier, respectively.", "description": "Figure 14 illustrates how the prompt is updated iteratively in a multi-turn reinforcement learning (RL) setting for a vision-language model (VLM).  The initial prompt (v<sub>in</sub>) contains the task instructions. After each turn, the model's output (v<sub>out</sub>) and the verifier's feedback (v<sub>ver</sub>) are appended to the prompt to form the input for the next turn (v<sub>in<sub>t+1</sub></sub>). This iterative process allows the model to refine its response based on the verifier's assessment. The figure highlights the visual input component for VLMs, the task-related information, and state-specific information which is essential for the model to perform the task. Different color codings are used to differentiate the different parts of the prompt.", "section": "4. Evaluation Tasks"}, {"figure_path": "https://arxiv.org/html/2501.17161/extracted/6162311/figures/virl_2x2grid/2x2grid_fail.jpeg", "caption": "Figure 15: SFT experiments on GP-L with suboptimal trajectories. Similar to results in\u00a0Figure\u00a05, SFT overfits the training data even we increase the trajectory diversity.", "description": "This figure displays the results of experiments using Supervised Fine-Tuning (SFT) on the General Points (GP-L) task, where the model is trained on suboptimal trajectories instead of optimal ones.  Suboptimal trajectories contain errors and verification messages, making the training data more diverse and less clean compared to what was used in Figure 5. The graph shows the in-distribution and out-of-distribution success rates of the model as a function of computational resources (GFLOPS). Despite the increased diversity in training data, the results are consistent with Figure 5: SFT still significantly overfits the training data, failing to generalize well to unseen instances.", "section": "C. Experimental Setup"}]