{"references": [{"fullname_first_author": "Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-XX-XX", "reason": "This paper is foundational for understanding the capabilities of large language models and their use in few-shot learning, a core concept in this research."}, {"fullname_first_author": "Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-XX-XX", "reason": "This paper is highly influential due to its introduction of Reinforcement Learning from Human Feedback (RLHF), a crucial technique for aligning LLMs with human preferences, directly relevant to this study's use of RL."}, {"fullname_first_author": "Zhai", "paper_title": "Fine-tuning large vision-language models as decision-making agents via reinforcement learning", "publication_date": "2024-XX-XX", "reason": "This paper is significant due to its introduction of RL4VLM, a framework for training vision-language models using RL, which serves as the foundation for this study's V-IRL experiments."}, {"fullname_first_author": "Allen-Zhu", "paper_title": "Physics of language models: Part 3.1, knowledge storage and extraction", "publication_date": "2023-XX-XX", "reason": "This paper contributes to the theoretical understanding of LLMs' memorization capabilities and generalization limits, which is directly relevant to this work's exploration of SFT vs. RL in terms of memorization vs. generalization."}, {"fullname_first_author": "Wei", "paper_title": "Finetuned language models are zero-shot learners", "publication_date": "2022-XX-XX", "reason": "This paper is important because it demonstrates the effectiveness of fine-tuning large language models for zero-shot learning, a technique that is compared against reinforcement learning in this research."}]}