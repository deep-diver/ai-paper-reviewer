{"references": [{"fullname_first_author": "Bommasani, R.", "paper_title": "On the Opportunities and Risks of Foundation Models", "publication_date": "2021-00-00", "reason": "This paper provides a comprehensive overview of foundation models, which are the large language models (LLMs) that the main paper focuses on, establishing the context for understanding the challenges of LLM compression."}, {"fullname_first_author": "Hu, E. J.", "paper_title": "LORA: Low-Rank Adaptation of Large Language Models", "publication_date": "2022-00-00", "reason": "This paper introduces LoRA, a crucial low-rank adaptation technique used in the main paper as a foundation for creating more efficient LLMs."}, {"fullname_first_author": "Mu\u00f1oz, J. P.", "paper_title": "LoNAS: Elastic Low-Rank Adapters for Efficient Large Language Models", "publication_date": "2024-00-00", "reason": "This paper introduces LoNAS, a key method integrating low-rank adapters with neural architecture search, directly forming the basis of the main paper's research."}, {"fullname_first_author": "Mu\u00f1oz, J. P.", "paper_title": "Shears: Unstructured Sparsity with Neural Low-rank Adapter Search", "publication_date": "2024-00-00", "reason": "This paper presents Shears, a significant extension of the LoNAS method that further improves efficiency and addresses the challenges of model sparsity, directly related to the main paper's exploration."}, {"fullname_first_author": "Mu\u00f1oz, J. P.", "paper_title": "SQFT: Low-cost Model Adaptation in Low-precision Sparse Foundation Models", "publication_date": "2024-00-00", "reason": "This paper introduces SQFT, which builds on the previous work by incorporating low-precision techniques into the model compression strategies, directly relevant to the main paper's focus on efficient model adaptations."}]}