[{"figure_path": "https://arxiv.org/html/2501.16975/x1.png", "caption": "Figure 1: Scaling trend for Over-Encoded models and baselines on OLMo2. We plot the loss with 400B tokens\u2019 training. For over-encoding, input vocabulary size is extended from 0.1 to 1.2 and 12.8 million (12\u00d712\\times12 \u00d7 and 128\u00d7128\\times128 \u00d7 larger than baseline), referred to as OE-1.2M and OE-12.8M. We observe OE-12.8M with 400M parameters matches the baseline with 1B parameters.", "description": "This figure displays the scaling trend of Over-Encoded (OE) models compared to baseline models on the OLMo2 dataset.  The x-axis represents the number of dense parameters (log scale), while the y-axis shows the training loss.  Four lines are plotted: a baseline model and three OE models.  The baseline model's loss is shown for models with 151 million, 400 million, and 1 billion parameters. The three OE models represent different increases in input vocabulary size compared to the baseline: OE-1.2M (12x larger), and OE-12.8M (128x larger).  The training data used to generate these results included 400 billion tokens. Notably, the OE-12.8M model with 400 million parameters achieves a loss comparable to that of the baseline model with 1 billion parameters, demonstrating the effect of increased input vocabulary size on model performance.", "section": "3. Experiments"}, {"figure_path": "https://arxiv.org/html/2501.16975/x2.png", "caption": "Figure 2: Performance comparison for models trained on CFG data. The left panel compares 1-gram and 3-gram tokenizers, showing that 3-gram improves larger (85M parameters) models but harms smaller (2.4M parameters) ones. The right panel examines 3-gram usage in encoders and decoders, revealing consistent gains with 3-gram encoders regardless of model size, while 3-gram decoders degrade performance in smaller models.", "description": "This figure displays the results of experiments comparing the performance of language models trained on context-free grammar (CFG) data using different tokenization strategies.  The left panel shows that using 3-gram tokenizers (which consider groups of three consecutive characters) improves the performance of larger language models (85 million parameters) compared to 1-gram tokenizers (considering single characters), but it harms smaller models (2.4 million parameters). This suggests that larger models benefit from the richer information captured by 3-gram tokens, while smaller models might be over-burdened by the increased complexity. The right panel investigates the impact of 3-gram tokenization on encoders (input processing) and decoders (output generation) separately. The results reveal consistent performance improvements when using 3-gram encoders, regardless of the model's size. However, 3-gram decoders hurt the performance of smaller models, highlighting the importance of considering the model size when designing tokenization strategies.", "section": "3.1. Insights from Synthetic Experiments"}, {"figure_path": "https://arxiv.org/html/2501.16975/x3.png", "caption": "Figure 3: Illustration of 2-gram encoding/decoding GPT. Note that 2-gram decoding only preserves the predicted next 1 token though next 2 is predicted, which keeps inference cost identical to the vanilla model.", "description": "This figure illustrates the difference between a standard GPT (Vanilla GPT) and a GPT model using 2-gram encoding and decoding.  In Vanilla GPT, both input and output tokens are single units (1-gram).  In the 2-gram encoding GPT, the input is encoded as 2-grams (sequences of two consecutive tokens), allowing the model to learn relationships between adjacent tokens. However, the output (prediction) remains 1-gram to maintain a similar inference cost as Vanilla GPT. The 2-gram decoding GPT works inversely. The input remains a 1-gram but the output is a 2-gram prediction. It shows that the 2-gram encoding captures local context, and the 2-gram decoding provides finer-grained supervision signals.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2501.16975/x4.png", "caption": "Figure 4: Training curves for OE-12.8M and baseline model on OLMo2-1B. The metrics are smoothed via exponential moving average with weight 0.99 for loss and 0.9 for downstream tasks. We observe significant convergence acceleration for the OE model: 5.7\u00d75.7\\times5.7 \u00d7 on loss, 3.2\u00d73.2\\times3.2 \u00d7 on MMLU-Var, 3.0\u00d73.0\\times3.0 \u00d7 on Hellaswag, 2.6\u00d72.6\\times2.6 \u00d7 on ARC-Challenge, 3.1\u00d73.1\\times3.1 \u00d7 on ARC-Easy and 3.9\u00d73.9\\times3.9 \u00d7 on PIQA.", "description": "Figure 4 presents a detailed comparison of training curves between the baseline OLMo2-1B model and the over-encoded model (OE-12.8M) which uses a significantly larger input vocabulary.  The comparison includes training loss and zero-shot performance on several downstream tasks: MMLU-Var, Hellaswag, ARC-Challenge, ARC-Easy, and PIQA.  Exponential moving averages (EMA) were applied for smoothing the curves (0.99 for loss and 0.9 for downstream metrics).  The results show a dramatic speedup in convergence for the OE-12.8M model, achieving 5.7x faster convergence in loss compared to the baseline.  Substantial improvements are also observed in the downstream task scores (3.2x on MMLU-Var, 3x on Hellaswag, 2.6x on ARC-Challenge, 3.1x on ARC-Easy, and 3.9x on PIQA).  This demonstrates the substantial performance gains enabled by increasing the input vocabulary size.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2501.16975/x5.png", "caption": "Figure 5: Log-linear relationship is observed between vocabulary size m\ud835\udc5amitalic_m and training loss \u2112\u2112\\mathcal{L}caligraphic_L, i.e. \u2112=2.6754\u22120.0256\u00d7log10\u2061m\u21122.67540.0256subscript10\ud835\udc5a\\mathcal{L}=2.6754-0.0256\\times\\log_{10}{m}caligraphic_L = 2.6754 - 0.0256 \u00d7 roman_log start_POSTSUBSCRIPT 10 end_POSTSUBSCRIPT italic_m. The values are collected with 500B tokens\u2019 training on OLMoE-1.3B models.", "description": "This figure displays the log-linear relationship between the input vocabulary size (m) and the training loss (L) observed during experiments.  Specifically, it shows that as the logarithm of the vocabulary size increases linearly, the training loss decreases linearly. This relationship was determined using 500 billion tokens of training data on the OLMoE-1.3B model. The equation representing this relationship is provided:  L = 2.6754 - 0.0256 * log\u2081\u2080(m).", "section": "4.2. Ablation Study"}, {"figure_path": "https://arxiv.org/html/2501.16975/extracted/6161742/figures/cfg_example.png", "caption": "Figure 6: Left Panel: CFG rules used in our experiments; Right Panel: an example of the generated sequences using the rules. This figure is taken from (Allen-Zhu & Li, 2024).", "description": "This figure is from Allen-Zhu & Li (2024).  The left panel shows the context-free grammar (CFG) rules used to generate synthetic data for the experiments in the paper.  The rules define the relationships between different symbols in the language. The right panel displays an example of a sequence generated using these rules.  This sequence is a string of characters created according to the grammatical rules defined on the left.", "section": "3.1. Insights from Synthetic Experiments"}, {"figure_path": "https://arxiv.org/html/2501.16975/x6.png", "caption": "Figure 9: All metrics for OLMo2-1B, comparing OE-12.8M and baseline.", "description": "This figure displays a comprehensive comparison of various metrics between the baseline OLMo2-1B model and the OLMo2-1B model enhanced with Over-Encoding (OE-12.8M).  Metrics include training loss and perplexity, as well as several downstream task evaluation metrics, such as performance on the MMLU, HellaSwag, ARC (Challenge and Easy), PIQA, BoolQ, COPA, CommonsenseQA, and Social-IQA benchmarks.  The visualization allows for a direct assessment of how Over-Encoding impacts both training dynamics and overall model performance across a range of tasks.  Each metric's trend is shown over the course of training, providing insights into convergence speed and final performance.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2501.16975/x7.png", "caption": "Figure 11: All metrics for OLMoE-1.3B, comparing OE-12.8M and baseline.", "description": "Figure 11 presents a comprehensive comparison of various performance metrics between the OLMoE-1.3B model with over-encoding (OE-12.8M) and its baseline counterpart.  The metrics encompass training loss, validation loss, perplexity scores on several datasets (C4-en, Dolma Books, etc.), and zero-shot performance across numerous downstream tasks (e.g., MMLU-Var, HellaSwag, ARC-Challenge, ARC-Easy, PIQA, etc.). The figure visually depicts the training dynamics and final evaluation scores, offering a detailed assessment of how OE-12.8M impacts both model training efficiency and overall performance across various benchmarks.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2501.16975/x8.png", "caption": "Figure 12: All metrics for OLMoE-7B, comparing OE-12.8M and baseline.", "description": "This figure presents a comprehensive comparison of various metrics for the OLMoE-7B model with and without over-encoding (OE-12.8M).  Metrics include training loss and perplexity, as well as downstream task performance across several benchmarks like MMLU (various sub-categories), HellaSwag, SciTail, ARC (Challenge and Easy), PIQA, Winogrande, BoolQ, COPA, CommonsenseQA, and SocialIQA.  Each metric is shown over the course of the training process, illustrating the impact of over-encoding on both training dynamics and final performance across a range of tasks.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2501.16975/x9.png", "caption": "Figure 13: All metrics for OLMoE-1.3B, comparing OT-12.8 and OE-12.8M.", "description": "Figure 13 presents a comprehensive comparison of various evaluation metrics for the OLMoE-1.3B model, contrasting the performance achieved using Over-Tokenized Transformers (OT-12.8M) against that of Over-Encoded Transformers (OE-12.8M).  The metrics cover a wide range, including training loss, perplexity, and various downstream benchmark scores across diverse tasks like MMLU (covering STEM, humanities, and social sciences), HellaSwag, ARC (easy and challenge), PIQA, Winogrande, BoolQ, COPA, CommonsenseQA, and SocialIQA.  This detailed visualization allows readers to directly assess the impact of integrating over-decoding with over-encoding, facilitating a comprehensive understanding of the relative strengths and weaknesses of each approach in a large language model setting.", "section": "4. Experiments"}]