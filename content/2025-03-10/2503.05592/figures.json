[{"figure_path": "https://arxiv.org/html/2503.05592/x1.png", "caption": "Figure 1: Performance comparisons between R1-Searcher and other methods on four multi-hop QA benchmarks. R1-Searcher achieves state-of-the-art performance on each dataset.", "description": "Figure 1 presents a bar chart comparing the performance of R1-Searcher against several other methods across four distinct multi-hop question answering (QA) benchmark datasets: HotpotQA, 2WikiMultiHopQA, Bamboogle, and Musique.  The chart displays the accuracy or percentile achieved by each method on each dataset.  R1-Searcher consistently outperforms the other models, demonstrating its state-of-the-art performance on all four benchmarks.", "section": "3 Experiment"}, {"figure_path": "https://arxiv.org/html/2503.05592/extracted/6261303/figures/1.png", "caption": "Figure 2: The log of reward, response length, and retrieval numbers for Llama-3.1-8B-Instruct comparison between using GRPO and Reinforce++.", "description": "This figure compares the performance of two reinforcement learning algorithms, GRPO and Reinforce++, when training the Llama-3.1-8B-Instruct large language model.  It shows the log of the reward, the length of the generated response, and the number of retrievals performed during training for each algorithm.  This allows for a visual comparison of the training process and the impact of each algorithm on different aspects of the model's behavior.", "section": "4.1 Basic Training Methods"}, {"figure_path": "https://arxiv.org/html/2503.05592/extracted/6261303/figures/4.png", "caption": "Figure 3: The log of reward, response length, and retrieval numbers for the Qwen-2.5-7B-Base model utilizing different metrics for outcome-supervised reward calculation.", "description": "This figure displays three graphs showing the training progress of the Qwen-2.5-7B-Base language model using different reward metrics for outcome-supervised reinforcement learning.  Each graph plots a different aspect of the training process against the training step: the first shows the log of the reward received, the second shows the length of the model's generated responses, and the third shows the number of retrievals performed. Different line colors represent different reward metrics: EM (Exact Match), CEM (Cover Exact Match), and F1 (F1-score). Analyzing these graphs allows one to see how the choice of reward metric impacts training efficiency and the model's behavior.", "section": "4.2 Reward Design"}, {"figure_path": "https://arxiv.org/html/2503.05592/extracted/6261303/figures/5.png", "caption": "Figure 4: The log of reward, response length, and retrieval numbers for the Qwen-2.5-7B-Base model, trained on datasets of varying difficulty levels.", "description": "This figure displays three graphs showing the training process of the Qwen-2.5-7B-Base language model using reinforcement learning. The graphs track the log of reward, response length, and retrieval numbers over training steps. Two different training datasets are used: one with varying difficulty levels (w. Difficult), and another without the most difficult questions (w/o Difficult).  The figure aims to illustrate how the difficulty of the training data influences the model's learning behavior and its reliance on external search (retrieval).", "section": "4.3 Training Data"}, {"figure_path": "https://arxiv.org/html/2503.05592/extracted/6261303/figures/6.png", "caption": "Figure 5: The log of reward, response length, and retrieval numbers for the Qwen-2.5-7B-Base model trained on different datasets.", "description": "This figure presents a comparison of training results for the Qwen-2.5-7B-Base language model using three different training datasets: HotpotQA, 2Wiki, and a mixture of both.  It displays graphs showing the change in training reward, response length (number of tokens in the model's output), and retrieval count (number of times external knowledge was accessed) over the course of training. This visualization allows for the analysis of how different data sources impact the training dynamics and the model's performance. Differences in reward curves, response length, and retrieval counts across the three datasets indicate varying learning behavior and model efficiency under different conditions.", "section": "4.3 Training Data"}, {"figure_path": "https://arxiv.org/html/2503.05592/x2.png", "caption": "Figure 6: Preference comparison of our models that utilize local search and online search and the baselines on the Bamboogle dataset. Search-o1 utilizes online search, and all other baselines employ local search.", "description": "Figure 6 presents a performance comparison on the Bamboogle dataset between models using local search (R1-Searcher with different LLMs as backbones and other baselines) and online search (R1-Searcher and Search-01).  The graph displays the accuracy or percentile achieved by each method on the Bamboogle dataset. This comparison highlights the generalization capability of R1-Searcher when adapting to different search environments (local vs. online). Search-01, a strong baseline, is included in the comparison to showcase R1-Searcher's performance relative to a model specifically designed for online search.", "section": "3.4 Main Results"}]