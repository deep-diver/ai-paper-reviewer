[{"figure_path": "https://arxiv.org/html/2503.05179/x1.png", "caption": "Figure 1: A comparison of accuracy and token usage in the Chain-of-Thought (CoT)\u00a0Wei et\u00a0al. (2023) and the proposed Sketch-of-Thought (SoT). Average of tests across 15 datasets using the family of Qwen-2.5 models. Shaded region represents more efficient reasoning.", "description": "This figure compares the performance of two large language model (LLM) prompting techniques, Chain-of-Thought (CoT) and the proposed Sketch-of-Thought (SoT), in terms of accuracy and the number of tokens used.  The results are averaged across 15 different reasoning datasets and utilize the Qwen-2.5 family of LLMs.  The shaded region highlights the area where the model demonstrates more efficient reasoning (higher accuracy with fewer tokens). SoT is shown to be superior to CoT in this regard, offering comparable accuracy with a significantly reduced number of tokens.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2503.05179/x2.png", "caption": "Figure 2: Comparison of Chain-of-Thought (CoT) and Sketch-of-Thought (SoT) workflows. While CoT generates verbose reasoning steps directly from prompts, SoT employs a router model to select the optimal reasoning paradigm, producing significantly more compact intermediate steps while maintaining accuracy.", "description": "The figure illustrates the difference between the Chain-of-Thought (CoT) and Sketch-of-Thought (SoT) prompting methods for large language models (LLMs).  CoT directly generates verbose reasoning steps from the prompt, resulting in longer, more computationally expensive outputs. In contrast, SoT uses a router model to dynamically select the most appropriate reasoning paradigm (from three options: Conceptual Chaining, Chunked Symbolism, or Expert Lexicons) for a given query. This approach produces significantly more compact intermediate reasoning steps while maintaining similar accuracy to CoT.", "section": "2 Method"}]