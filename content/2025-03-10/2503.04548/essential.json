{"importance": "This research provides valuable insights into training effective reasoning models, paving the way for future advancements in complex AI systems. The exploration of tool manipulation and RL techniques offers new avenues for enhancing model capabilities and solving intricate problems.", "summary": "This paper explores and improves R1-like reasoning models through RL and tool manipulation, achieving significant accuracy gains.", "takeaways": ["RL training significantly enhances the reasoning abilities of base models, leading to complex reasoning patterns.", "Tool manipulation dramatically boosts reasoning performance, especially when combined with large reasoning models.", "Careful hyperparameter tuning and on-policy learning are crucial for successful RL training of reasoning models."], "tldr": "Large reasoning models (LRMs) enhance complex tasks by generating extended thought processes. Unlike one-time train-time scaling, test-time scaling learns to trade more token outputs for improved performance. This requires models to generate and employ critical reasoning steps, enabling solution searches within the natural language space. Current methods use rule-based rewards with verifiable problems, but general applicability remains challenging in complex scenarios.\n\nThis research explores scaling reinforcement learning (RL) training for LRMs as part of the STILL project. The study experiments with diverse factors influencing RL training, focusing on base and fine-tuned models. The RL approach improves base models (QWEN2.5-32B) consistently, enhancing response length and test accuracy. Refining models like DEEPSEEK-R1-DISTILL-QWEN-1.5B through RL reaches AIME 2024 accuracy of 39.33%. Tool manipulation further enhances large reasoning models, achieving 86.67% accuracy on AIME 2024 with greedy search.", "affiliation": "Renmin University of China", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}, "podcast_path": "2503.04548/podcast.wav"}