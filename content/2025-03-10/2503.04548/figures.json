[{"figure_path": "https://arxiv.org/html/2503.04548/x1.png", "caption": "Figure 1: The performance comparison of Qwen2.5-7B between different RL training settings.", "description": "This figure compares the performance of the QWEN2.5-7B model under various RL training settings.  It shows the training set response length and test set accuracy (on AIME) across different hyperparameters, including training batch size, learning strategies (on-policy vs. off-policy), rollout parameters (number of times and temperature), and the coefficient of KL penalty. Each subplot illustrates the impact of varying one hyperparameter while holding others constant, revealing the effects of these parameters on training efficiency and model performance.", "section": "3.1 Exploring the Settings of RL Training"}, {"figure_path": "https://arxiv.org/html/2503.04548/x2.png", "caption": "Figure 2: The accuracy on training set and test set, and the average length of responses of Qwen2.5-7B and Qwen2.5-1.5B trained on different prompts.", "description": "This figure compares the performance of two Qwen2.5 language models (7B and 1.5B parameters) during reinforcement learning (RL) training.  The models were trained using two different prompt styles: a short prompt with basic instructions and a long prompt with more detailed instructions on how to perform the reasoning process.  The plots show the accuracy on a training set and a MATH-OAI test set, along with the average length of the model's responses over the course of the RL training. This comparison allows for analysis of how prompt engineering and model size affect the effectiveness of RL training for enhancing reasoning ability.", "section": "3.1 Exploring the Settings of RL Training"}, {"figure_path": "https://arxiv.org/html/2503.04548/x3.png", "caption": "Figure 3: The accuracy on training set and test set, and the average length of responses of STILL-3-Zero-32B during RL process.", "description": "This figure displays the performance of the STILL-3-Zero-32B model during reinforcement learning (RL) training.  It shows three key metrics plotted against training steps: 1) Training set accuracy, representing the model's performance on the training data; 2) Test set accuracy (AIME), measuring performance on the AIME 2024 benchmark; and 3) Average response length, indicating the average number of tokens in the model's responses. The graph allows for visualization of how these metrics evolve as the model undergoes RL training. This helps to assess the effectiveness of the RL training in improving both accuracy and reasoning capabilities (as measured by response length).", "section": "3 RL Experiments on the Base Model"}, {"figure_path": "https://arxiv.org/html/2503.04548/extracted/6258170/figures/aha_moment.png", "caption": "Figure 4: The ratio of keyword categories in responses generated by STILL-3-Zero-32B on AIME 2024. The self-verification category includes words like \u201cverify\u201d, \u201cdouble check\u201d, and \u201cconfirm\u201d. The self-reflection category contains terms such as \u201chowever\u201d, \u201creflect\u201d, and \u201cwait\u201d. The self-correction category encompasses words including \u201ccorrect\u201d, \u201crevise\u201d, and \u201cadjust\u201d.", "description": "Figure 4 shows the evolution of reasoning patterns in the STILL-3-Zero-32B model during RL training on the AIME 2024 dataset.  It tracks the proportion of keywords associated with three key reasoning processes over training steps.  The \"self-verification\" category (e.g., \"verify\", \"double check\", \"confirm\") reflects the model's tendency to check its work. The \"self-reflection\" category (e.g., \"however\", \"reflect\", \"wait\") indicates the model's ability to reconsider its approach. The \"self-correction\" category (e.g., \"correct\", \"revise\", \"adjust\") demonstrates its capacity to identify and fix errors. The increasing proportions of these keywords suggest a refinement of the model's reasoning process towards more human-like, deliberate thinking during training.", "section": "3.2 STILL-3-ZERO-32B: RL Improves Reasoning Ability on Base Model"}, {"figure_path": "https://arxiv.org/html/2503.04548/x4.png", "caption": "Figure 5: The accuracy on training set, the average length of responses, and the ratio of completed responses of Qwen2.5-7B fine-tuned on different reasoning data during the RL process.", "description": "Figure 5 presents a comparative analysis of three key metrics\u2014training set accuracy, average response length, and the ratio of completed responses\u2014for the QWEN2.5-7B language model fine-tuned using two distinct methods: data synthesis and data distillation.  The figure illustrates how these metrics evolve across the RL training process for both fine-tuning strategies.  It highlights the differences in performance and training dynamics resulting from these two distinct approaches to data preparation for reinforcement learning.", "section": "4.1 Fine-tuning Base Models with Long CoT Data"}, {"figure_path": "https://arxiv.org/html/2503.04548/x5.png", "caption": "Figure 6: The accuracy on training set, the average length of responses, and the ratio of completed responses of the model trained on different auxiliary approaches during the training process.", "description": "Figure 6 presents a comparative analysis of three different auxiliary training methods used to enhance the length of model responses during reinforcement learning. The x-axis represents the training step, while the y-axis displays three key metrics: training set accuracy, the average length of generated responses, and the ratio of completed responses.  Each line on the graph represents the performance of the model under a specific auxiliary method (QS, RS, RRL, RRA, ORM). By examining these metrics, the figure allows for a comprehensive understanding of how each strategy influences the model's ability to generate longer, more accurate, and complete responses over the course of training.", "section": "4.2 Discussion about Length Hacking in RL"}, {"figure_path": "https://arxiv.org/html/2503.04548/x6.png", "caption": "Figure 7: The accuracy on training set and test set, and the average length of responses of STILL-3-1.5B during the training process.", "description": "This figure displays the performance of the STILL-3-1.5B model throughout its reinforcement learning (RL) training process.  The x-axis represents the training step, indicating the progression of the training. The three lines show the model's accuracy on the training dataset, its accuracy on the test dataset (a separate dataset used to evaluate generalization performance), and the average length of the responses generated by the model at each training step.  The plot illustrates how the model's accuracy and response length evolve as it learns through RL, providing insights into the training dynamics and the relationship between model performance and response length.", "section": "4.3 STILL-3-1.5B: Enhancing Slow Thinking Abilities of Small Models via RL"}, {"figure_path": "https://arxiv.org/html/2503.04548/x7.png", "caption": "Figure 8: A case study of STILL-3-Tool-32B.", "description": "This figure showcases a specific example of how STILL-3-Tool-32B, a large language model enhanced with tool manipulation capabilities, approaches a mathematical problem.  It contrasts the model's reasoning process before and after reinforcement learning (RL) training. The \"Before RL Training\" section illustrates an initial attempt at solving the problem, revealing errors and flawed reasoning. The \"After RL Training\" section demonstrates the model's improved reasoning abilities after RL training; it identifies and corrects its previous mistakes, even using a Python code snippet for verification and successfully arrives at the correct answer. This figure highlights the significant improvement in both the accuracy and the sophistication of the model's reasoning process after RL training.", "section": "4.4 STILL-3-TOOL-32B: Empowering Reasoning Models with Tool Manipulation"}, {"figure_path": "https://arxiv.org/html/2503.04548/x8.png", "caption": "Figure 9: The impact of different coefficients of entropy loss.", "description": "This figure displays the effects of varying the coefficient of entropy loss during reinforcement learning (RL) training.  It shows three subplots tracking the training set accuracy, training set response length, and entropy loss over the course of training. Each subplot presents two lines, representing the results using two different coefficients for entropy loss (0.001 and 0.005). This visualization helps to understand how different coefficients influence the exploration-exploitation trade-off during RL. The trends in accuracy, response length, and entropy loss are presented to assess the impact of each coefficient on the overall training process.", "section": "4.2 Discussion about Length Hacking in RL"}]