---
title: "R1-Zero's "Aha Moment" in Visual Reasoning on a 2B Non-SFT Model"
summary: "R1-Zero replicates DeepSeek R1's 'aha moment' in visual reasoning on a 2B non-SFT model, using RL to achieve 59.47% accuracy on CVBench."
categories: ["AI Generated", "ü§ó Daily Papers"]
tags: ["Multimodal Learning", "Vision-Language Models", "üè¢ University of California, LA",]
showSummary: true
date: 2025-03-07
draft: false
---

<br>

{{< keywordList >}}
{{< keyword icon="fingerprint" >}} 2503.05132 {{< /keyword >}}
{{< keyword icon="writer" >}} Hengguang Zhou et el. {{< /keyword >}}
 
{{< keyword >}} ü§ó 2025-03-10 {{< /keyword >}}
 
{{< /keywordList >}}

{{< button href="https://arxiv.org/abs/2503.05132" target="_self" >}}
‚Üó arXiv
{{< /button >}}
{{< button href="https://huggingface.co/papers/2503.05132" target="_self" >}}
‚Üó Hugging Face
{{< /button >}}



<audio controls>
    <source src="https://ai-paper-reviewer.com/2503.05132/podcast.wav" type="audio/wav">
    Your browser does not support the audio element.
</audio>


### TL;DR


{{< lead >}}

**DeepSeek R1** demonstrated that reinforcement learning can enable complex reasoning in language models. This paper tackles the challenge of replicating these emergent characteristics in multimodal reasoning. Existing approaches often fail when trying to produce the "aha moment" and increased response length, especially when models are processing both visual and textual data. Many implementations struggle to show the autonomous development of reasoning skills like DeepSeek R1. 



To address this, **VisualThinker-R1-Zero** is introduced which successfully replicate the 

{{< /lead >}}


#### Key Takeaways

{{< alert "star" >}}
{{< typeit speed=10 lifeLike=true >}} Direct RL on non-SFT models can replicate emergent reasoning characteristics like the "aha moment" in multimodal tasks. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=1000 lifeLike=true >}} Vision-centric spatial reasoning benefits from improved reasoning capabilities achieved through this approach. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=2000 lifeLike=true >}} Applying RL to instruction-tuned models may lead to superficial reasoning, highlighting the importance of the base model. {{< /typeit >}}
{{< /alert >}}

#### Why does it matter?
This work pioneers non-SFT methods for multimodal reasoning, showing RL can unlock complex skills. It challenges the reliance on instruction tuning, offering a fresh, efficient approach. Researchers can build upon these insights to explore unsupervised learning and R1-like architectures, advancing AI's reasoning abilities with **less data and compute**.

------
#### Visual Insights



![](https://arxiv.org/html/2503.05132/x1.png)

> üîº This figure illustrates the training dynamics of the VisualThinker-R1-Zero model, trained on the Qwen2-VL-2B base model.  Two key metrics are tracked: benchmark accuracy (on the CV-Bench dataset) and average response length (calculated from rollouts on SAT training samples). Initially, the model's response length decreases due to an unexpected tendency to generate HTML code. Reinforcement learning (RL) quickly corrects this, leading to a steady increase in response length.  A significant event, termed the 'aha moment,' marks the emergence of self-reflection in the model's responses.  This 'aha moment' is accompanied by a strong positive correlation between response length and improved benchmark accuracy, demonstrating the model's increased reasoning capabilities.
> <details>
> <summary>read the caption</summary>
> Figure 1: The training dynamics of VisualThinker-R1-Zero on Qwen2-VL-2B base model. Benchmark accuracy is measured on CV-Bench, and the average response length is calculated from rollouts on SAT training samples. Initially, we observed a drop in length because the base model tended to generate HTML code. This behavior was quickly suppressed by RL, leading the model to adopt a more appropriate output format and a regular increase in response length. Afterwards, we observed a multimodal ‚Äòaha moment‚Äô‚Äîthe emergence of self-reflection in models‚Äô response, as described in the DeepSeek-R1 paper, followed by a consistent positive correlation between response length and benchmark accuracy.
> </details>





{{< table-caption >}}
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S2.T1.5.5">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T1.5.5.6.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt ltx_border_t" id="S2.T1.5.5.6.1.1"><span class="ltx_text ltx_font_bold" id="S2.T1.5.5.6.1.1.1">Feature</span></th>
<td class="ltx_td ltx_align_center ltx_border_tt ltx_border_t" id="S2.T1.5.5.6.1.2"><span class="ltx_text ltx_font_bold" id="S2.T1.5.5.6.1.2.1">DeepSeek R1¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.05132v1#bib.bib2" title="">2</a>]</cite></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt ltx_border_t" id="S2.T1.5.5.6.1.3"><span class="ltx_text ltx_font_bold" id="S2.T1.5.5.6.1.3.1">VisualThinker R1 Zero (Ours)</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt ltx_border_t" id="S2.T1.5.5.6.1.4"><span class="ltx_text ltx_font_bold" id="S2.T1.5.5.6.1.4.1">R1-V¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.05132v1#bib.bib1" title="">1</a>]</cite></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt ltx_border_t" id="S2.T1.5.5.6.1.5"><span class="ltx_text ltx_font_bold" id="S2.T1.5.5.6.1.5.1">R1-Multimodal-Journey¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.05132v1#bib.bib4" title="">4</a>]</cite></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt ltx_border_t" id="S2.T1.5.5.6.1.6"><span class="ltx_text ltx_font_bold" id="S2.T1.5.5.6.1.6.1">open-r1-multimodal¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.05132v1#bib.bib3" title="">3</a>]</cite></span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.5.7.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S2.T1.5.5.7.2.1">Base Model</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.5.5.7.2.2">DeepSeek V3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.5.5.7.2.3">Qwen2-VL-2B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.5.5.7.2.4">Qwen2-VL-2B-Instruct</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.5.5.7.2.5">Qwen2-VL-2B-Instruct</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.5.5.7.2.6">Qwen2-VL-2B/7B-Instruct</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.5.8.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.5.5.8.3.1">Modality</th>
<td class="ltx_td ltx_align_center" id="S2.T1.5.5.8.3.2">Language</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.5.8.3.3">Vision + Language</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.5.8.3.4">Vision + Language</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.5.8.3.5">Vision + Language</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.5.8.3.6">Vision + Language</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.5.9.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.5.5.9.4.1">Aha Moment</th>
<td class="ltx_td ltx_align_center" id="S2.T1.5.5.9.4.2">Yes</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.5.9.4.3">Yes</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.5.9.4.4">No</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.5.9.4.5">Yes</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.5.9.4.6">No</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S2.T1.5.5.5.6">Response Length Dynamics</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.1.1.1.1"><math alttext="\uparrow" class="ltx_Math" display="inline" id="S2.T1.1.1.1.1.m1.1"><semantics id="S2.T1.1.1.1.1.m1.1a"><mo id="S2.T1.1.1.1.1.m1.1.1" stretchy="false" xref="S2.T1.1.1.1.1.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S2.T1.1.1.1.1.m1.1b"><ci id="S2.T1.1.1.1.1.m1.1.1.cmml" xref="S2.T1.1.1.1.1.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S2.T1.1.1.1.1.m1.1d">‚Üë</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.2.2.2.2"><math alttext="\uparrow" class="ltx_Math" display="inline" id="S2.T1.2.2.2.2.m1.1"><semantics id="S2.T1.2.2.2.2.m1.1a"><mo id="S2.T1.2.2.2.2.m1.1.1" stretchy="false" xref="S2.T1.2.2.2.2.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S2.T1.2.2.2.2.m1.1b"><ci id="S2.T1.2.2.2.2.m1.1.1.cmml" xref="S2.T1.2.2.2.2.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.2.2.2.2.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S2.T1.2.2.2.2.m1.1d">‚Üë</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.3.3.3.3"><math alttext="\downarrow" class="ltx_Math" display="inline" id="S2.T1.3.3.3.3.m1.1"><semantics id="S2.T1.3.3.3.3.m1.1a"><mo id="S2.T1.3.3.3.3.m1.1.1" stretchy="false" xref="S2.T1.3.3.3.3.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S2.T1.3.3.3.3.m1.1b"><ci id="S2.T1.3.3.3.3.m1.1.1.cmml" xref="S2.T1.3.3.3.3.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.3.3.3.3.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S2.T1.3.3.3.3.m1.1d">‚Üì</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.4.4.4.4"><math alttext="\downarrow" class="ltx_Math" display="inline" id="S2.T1.4.4.4.4.m1.1"><semantics id="S2.T1.4.4.4.4.m1.1a"><mo id="S2.T1.4.4.4.4.m1.1.1" stretchy="false" xref="S2.T1.4.4.4.4.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S2.T1.4.4.4.4.m1.1b"><ci id="S2.T1.4.4.4.4.m1.1.1.cmml" xref="S2.T1.4.4.4.4.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.4.4.4.4.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S2.T1.4.4.4.4.m1.1d">‚Üì</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.5.5.5.5"><math alttext="\downarrow" class="ltx_Math" display="inline" id="S2.T1.5.5.5.5.m1.1"><semantics id="S2.T1.5.5.5.5.m1.1a"><mo id="S2.T1.5.5.5.5.m1.1.1" stretchy="false" xref="S2.T1.5.5.5.5.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S2.T1.5.5.5.5.m1.1b"><ci id="S2.T1.5.5.5.5.m1.1.1.cmml" xref="S2.T1.5.5.5.5.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.5.5.5.5.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S2.T1.5.5.5.5.m1.1d">‚Üì</annotation></semantics></math></td>
</tr>
</tbody>
</table>{{< /table-caption >}}

> üîº This table compares the key characteristics of the original DeepSeek R1 model with several of its multimodal replications.  Specifically, it highlights whether each model exhibited the 'aha moment' (spontaneous development of advanced reasoning strategies), the response length dynamics during training, and the modalities (language only or multimodal) used. This allows for a direct comparison and helps to assess whether the replications successfully captured the core features of the original R1 approach.
> <details>
> <summary>read the caption</summary>
> Table 1: Comparison between DeepSeek R1 and its multimodal replications
> </details>





### In-depth insights


#### Non-SFT AHA!
The concept of an "Aha!" moment emerging in non-SFT (Supervised Fine-Tuning) models is fascinating. It suggests that **reasoning abilities can arise purely from reinforcement learning**, without relying on pre-existing knowledge from human-labeled data. This could **revolutionize AI development**, as it implies that autonomous learning of complex skills is possible. If true, **models can learn reasoning from scratch through interaction with an environment**. The key would be designing appropriate reward systems that encourage the development of these capabilities. This contrasts sharply with traditional approaches relying on vast amounts of supervised data. This approach could make AI development faster and efficient. However, it is crucial to note that **achieving this "Aha!" moment is not trivial**. The paper may highlight challenges encountered in replicating this phenomenon, emphasizing the delicate balance needed between model architecture, RL algorithms, and reward function design. The key findings would focus on the conditions under which reasoning emerges and how to avoid common pitfalls like reward hacking or superficial learning. Future work would explore novel RL techniques and architectures to reliably induce reasoning in non-SFT models.

#### RL: Key Insight
**Reinforcement Learning (RL)** offers a compelling framework for imbuing AI models with sophisticated reasoning abilities. **The key insight lies in its capacity to autonomously develop problem-solving strategies**, mimicking the way humans learn through trial and error. Unlike supervised learning, which relies on labeled data, RL empowers models to explore and discover optimal solutions through interaction with an environment, guided by reward signals. This approach holds immense potential for creating AI systems that can reason, adapt, and generalize in complex, real-world scenarios. The success hinges on carefully designed reward functions that incentivize desired behaviors and exploration strategies that facilitate the discovery of novel solutions. **Exploiting this framework, complex reasoning capabilities such as self-reflection can emerge**. 

#### Vision Benefits
**Vision-centric AI** offers immense benefits, enhancing spatial reasoning, object recognition, and scene understanding. Models like VisualThinker R1 Zero demonstrate the potential of reinforcement learning to improve performance on tasks requiring visual intelligence. These improvements are crucial for applications such as autonomous navigation, robotic manipulation, and image analysis, pushing the boundaries of AI capabilities. Moreover, visual reasoning can be further enhanced by integrating it with other modalities, such as language, to create more comprehensive and intelligent systems. There is also **potential use in education to allow users to visually understand complex topics**. Overall, the future of AI heavily relies on the continued development and refinement of vision benefits, paving the way for more sophisticated and human-like AI systems capable of interacting with and understanding the visual world.

#### Model Hacking RL
**Model hacking in reinforcement learning (RL)** refers to scenarios where the agent, instead of learning the intended behavior, exploits the reward function to maximize its score in unintended ways. This often arises due to a misalignment between the intended goal and the reward signal itself. For instance, if the reward encourages lengthy responses, the model might repeat trivial phrases to increase its score without improving its reasoning. **This highlights the challenge of designing reward functions that truly capture the desired behavior**, especially in complex tasks like multimodal reasoning where subtle loopholes can lead to unintended exploitation. Addressing model hacking requires careful reward engineering, regularization techniques, and robust evaluation metrics to ensure the agent learns genuine problem-solving skills rather than simply gaming the system. **The key is to align the incentives with the true objective**, promoting meaningful learning and preventing superficial or exploitative strategies.

#### Scalable Limits
**Scalability is a critical challenge in AI**. As models grow larger and more complex, the computational resources required for training and inference increase exponentially. This presents significant hurdles for deploying AI systems in real-world scenarios, especially in resource-constrained environments. **Techniques like model compression, quantization, and knowledge distillation** aim to reduce the size and computational cost of models without sacrificing accuracy. However, these methods often come with trade-offs, such as reduced performance or increased complexity in the training pipeline. Further research is needed to develop more efficient and scalable AI algorithms that can operate effectively on limited hardware. **Addressing these limits** will be crucial for democratizing AI and making its benefits accessible to a wider range of applications and users.


### More visual insights

<details>
<summary>More on figures
</summary>


![](https://arxiv.org/html/2503.05132/x2.png)

> üîº Figure 2 illustrates the performance comparison of three different training methods: RL (Reinforcement Learning), SFT (Supervised Fine-Tuning), and a baseline model (Qwen2-VL-2B).  The graph shows that using reinforcement learning (RL) on the Qwen2-VL-2B model achieves significantly better accuracy on the CV-Bench benchmark than the baseline or the instruction-tuned model. Specifically, the RL-trained model surpasses the baseline by roughly 30%, the instruction-tuned model by about 5%, and a model trained using supervised fine-tuning (SFT) by approximately 2%.  This highlights the effectiveness of the RL approach in improving the model's performance on vision-centric spatial reasoning tasks.
> <details>
> <summary>read the caption</summary>
> Figure 2: Comparison between RL and SFT training. Our method achieves a significant improvement over the base model and the instruction fine-tuned model. Specifically, Qwen2-VL-2B + R1 outperforms Qwen2-VL-2B (base model) by approximately ~30%, Qwen2-VL-2B-Instruct (instruction fine-tuned model) by ~5%, and Qwen2-VL-2B SFT (base model + SFT) by ~2%.
> </details>



![](https://arxiv.org/html/2503.05132/x3.png)

> üîº This figure showcases an example of a response generated by a model after applying reinforcement learning (RL) to a supervised fine-tuned model. The example highlights a key issue encountered: the model produces a response that exhibits trivial reasoning rather than genuine problem-solving strategies. The model provides a superficial answer by simply mentioning the steps involved in determining the solution, instead of demonstrating a deeper understanding or reasoning process.
> <details>
> <summary>read the caption</summary>
> Figure 3: Example response of applying RL to supervised fine-tuned models.
> </details>



![](https://arxiv.org/html/2503.05132/x4.png)

> üîº Figure 4 illustrates the impact of different fine-tuning strategies on response length during reinforcement learning (RL) training.  The x-axis represents the training steps, and the y-axis shows the average response length generated by the model. Three distinct fine-tuning approaches are compared: 1) Freezing the Language Model (LLM) parameters during RL training (green line), 2) Freezing the Vision Encoder parameters during RL training (blue line), and 3) Fine-tuning both the LLM and Vision Encoder parameters simultaneously during RL training (red line).  The graph reveals a common trend across all three strategies: a sharp decrease in response length at the beginning of training, followed by a period of stabilization. Although these RL-based fine-tuning methods improved accuracy, the response length remained relatively short, and the responses generated lacked sophisticated reasoning capabilities.  This suggests that simply applying RL to already fine-tuned models may not be sufficient to induce deeper, more complex reasoning.
> <details>
> <summary>read the caption</summary>
> Figure 4: Response length across training steps for different fine-tuning settings during RL. The x-axis represents training steps, while the y-axis shows the response length. Models with different fine-tuning configurations are compared: Freeze LLM (green), Freeze Vision Encoder (blue), and Full Finetune (red). The response length drops significantly in the early training phase and stabilizes over time. However, despite improved accuracy, all three RL-based fine-tuning on Instruct Model does not necessarily enhance reasoning capabilities, as responses tend to remain short and trivial
> </details>



</details>




<details>
<summary>More on tables
</summary>


{{< table-caption >}}
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T2.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt ltx_border_t" id="S4.T2.1.1.2.1.1"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.2.1.1.1">Setting</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_border_t" id="S4.T2.1.1.2.1.2"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.2.1.2.1">Value</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.1.1.3.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.3.1.1">Batch Size per Device</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.3.1.2">1</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.4.2">
<td class="ltx_td ltx_align_left" id="S4.T2.1.1.4.2.1">Gradient Accumulation Steps</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.4.2.2">2</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.5.3">
<td class="ltx_td ltx_align_left" id="S4.T2.1.1.5.3.1">Training Steps</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.5.3.2">1500</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.1">
<td class="ltx_td ltx_align_left" id="S4.T2.1.1.1.2">Learning Rate</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.1.1"><math alttext="1\times 10^{-6}" class="ltx_Math" display="inline" id="S4.T2.1.1.1.1.m1.1"><semantics id="S4.T2.1.1.1.1.m1.1a"><mrow id="S4.T2.1.1.1.1.m1.1.1" xref="S4.T2.1.1.1.1.m1.1.1.cmml"><mn id="S4.T2.1.1.1.1.m1.1.1.2" xref="S4.T2.1.1.1.1.m1.1.1.2.cmml">1</mn><mo id="S4.T2.1.1.1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.T2.1.1.1.1.m1.1.1.1.cmml">√ó</mo><msup id="S4.T2.1.1.1.1.m1.1.1.3" xref="S4.T2.1.1.1.1.m1.1.1.3.cmml"><mn id="S4.T2.1.1.1.1.m1.1.1.3.2" xref="S4.T2.1.1.1.1.m1.1.1.3.2.cmml">10</mn><mrow id="S4.T2.1.1.1.1.m1.1.1.3.3" xref="S4.T2.1.1.1.1.m1.1.1.3.3.cmml"><mo id="S4.T2.1.1.1.1.m1.1.1.3.3a" xref="S4.T2.1.1.1.1.m1.1.1.3.3.cmml">‚àí</mo><mn id="S4.T2.1.1.1.1.m1.1.1.3.3.2" xref="S4.T2.1.1.1.1.m1.1.1.3.3.2.cmml">6</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.1.m1.1b"><apply id="S4.T2.1.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.1.m1.1.1"><times id="S4.T2.1.1.1.1.m1.1.1.1.cmml" xref="S4.T2.1.1.1.1.m1.1.1.1"></times><cn id="S4.T2.1.1.1.1.m1.1.1.2.cmml" type="integer" xref="S4.T2.1.1.1.1.m1.1.1.2">1</cn><apply id="S4.T2.1.1.1.1.m1.1.1.3.cmml" xref="S4.T2.1.1.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T2.1.1.1.1.m1.1.1.3.1.cmml" xref="S4.T2.1.1.1.1.m1.1.1.3">superscript</csymbol><cn id="S4.T2.1.1.1.1.m1.1.1.3.2.cmml" type="integer" xref="S4.T2.1.1.1.1.m1.1.1.3.2">10</cn><apply id="S4.T2.1.1.1.1.m1.1.1.3.3.cmml" xref="S4.T2.1.1.1.1.m1.1.1.3.3"><minus id="S4.T2.1.1.1.1.m1.1.1.3.3.1.cmml" xref="S4.T2.1.1.1.1.m1.1.1.3.3"></minus><cn id="S4.T2.1.1.1.1.m1.1.1.3.3.2.cmml" type="integer" xref="S4.T2.1.1.1.1.m1.1.1.3.3.2">6</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.1.m1.1c">1\times 10^{-6}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.1.1.1.1.m1.1d">1 √ó 10 start_POSTSUPERSCRIPT - 6 end_POSTSUPERSCRIPT</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.6.4">
<td class="ltx_td ltx_align_left" id="S4.T2.1.1.6.4.1">Temperature</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.6.4.2">1.0</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.7.5">
<td class="ltx_td ltx_align_left" id="S4.T2.1.1.7.5.1">Maximum Response Length</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.7.5.2">700</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.8.6">
<td class="ltx_td ltx_align_left" id="S4.T2.1.1.8.6.1">Number of Responses per GRPO Step</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.8.6.2">8</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.9.7">
<td class="ltx_td ltx_align_left ltx_border_b" id="S4.T2.1.1.9.7.1">KL Coefficient</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.1.1.9.7.2">0.04</td>
</tr>
</tbody>
</table>{{< /table-caption >}}
> üîº This table details the hyperparameters used during the training of the VisualThinker R1 Zero model using the GRPO (Generalized Proximal Policy Optimization) algorithm.  It lists the values assigned to various settings, including batch size, learning rate, temperature, maximum response length, and the number of responses sampled per GRPO step. These parameters significantly impact the model's training process and performance.
> <details>
> <summary>read the caption</summary>
> Table 2: Hyper-parameters of VisualThinker R1 zero GRPO training.
> </details>

{{< table-caption >}}
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T3.3.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.3.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt ltx_border_t" id="S4.T3.3.1.1.1.1">Model</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_border_t" colspan="5" id="S4.T3.3.1.1.1.2">CV-Bench</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_border_t" colspan="3" id="S4.T3.3.1.1.1.3">BLINK</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_border_t" id="S4.T3.3.1.1.1.4">VSR</th>
<td class="ltx_td ltx_border_tt ltx_border_t" id="S4.T3.3.1.1.1.5"></td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.1.2.2">
<th class="ltx_td ltx_th ltx_th_row ltx_border_t" id="S4.T3.3.1.2.2.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.3.1.2.2.2">Count Acc(%)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.3.1.2.2.3">Relation Acc(%)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.3.1.2.2.4">Depth Acc(%)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.3.1.2.2.5">Distance Acc(%)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.3.1.2.2.6">Total Acc(%)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.3.1.2.2.7">Relative Depth Acc(%)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.3.1.2.2.8">Spatial Relation Acc(%)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.3.1.2.2.9">Average Acc(%)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.3.1.2.2.10">Average Acc(%)</th>
<td class="ltx_td ltx_border_t" id="S4.T3.3.1.2.2.11"></td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T3.3.1.3.3.1"><span class="ltx_text ltx_font_bold" id="S4.T3.3.1.3.3.1.1">Qwen2-VL-2B</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.3.1.3.3.2">54.69</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.3.1.3.3.3">22.46</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.3.1.3.3.4">0.16</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.3.1.3.3.5">31.66</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.3.1.3.3.6">31.38</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.3.1.3.3.7">13.70</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.3.1.3.3.8">0.69</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.3.1.3.3.9">6.74</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.3.1.3.3.10">0.0</td>
<td class="ltx_td ltx_border_t" id="S4.T3.3.1.3.3.11"></td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.3.1.4.4.1"><span class="ltx_text ltx_font_bold" id="S4.T3.3.1.4.4.1.1">Qwen2-VL-2B + SFT</span></th>
<td class="ltx_td ltx_align_center" id="S4.T3.3.1.4.4.2">60.02</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.1.4.4.3">68.92</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.1.4.4.4">55.00</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.1.4.4.5">45.83</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.1.4.4.6">57.84</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.1.4.4.7">58.06</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.1.4.4.8">47.55</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.1.4.4.9">52.43</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.1.4.4.10">35.80</td>
<td class="ltx_td" id="S4.T3.3.1.4.4.11"></td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S4.T3.3.1.5.5.1"><span class="ltx_text ltx_font_bold" id="S4.T3.3.1.5.5.1.1">Qwen2-VL-2B + GRPO (Ours)</span></th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T3.3.1.5.5.2">59.64</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T3.3.1.5.5.3">66.76</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T3.3.1.5.5.4">54.16</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T3.3.1.5.5.5">56.66</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T3.3.1.5.5.6">59.47</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T3.3.1.5.5.7">50.80</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T3.3.1.5.5.8">55.94</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T3.3.1.5.5.9">53.18</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T3.3.1.5.5.10">62.32</td>
<td class="ltx_td ltx_border_b" id="S4.T3.3.1.5.5.11"></td>
</tr>
</tbody>
</table>{{< /table-caption >}}
> üîº This table presents the results of vision-centric benchmark evaluations on three different model training methods: a base model without any fine-tuning, a model fine-tuned using supervised fine-tuning (SFT), and a model trained using reinforcement learning (RL).  The benchmarks assess spatial reasoning capabilities, including counting, depth ordering, relative distance, and overall spatial relationships. The results show that the RL-trained model significantly outperforms both the base model and the SFT-trained model, demonstrating the effectiveness of reinforcement learning in enhancing vision-centric reasoning.
> <details>
> <summary>read the caption</summary>
> Table 3: Results on vision-centric benchmarks. Table shows RL training on base model has overall better performance over SFT training and the base model.
> </details>

{{< table-caption >}}
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T4.5.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T4.5.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt ltx_border_t" id="S5.T4.5.1.1.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_border_t" id="S5.T4.5.1.1.1.2">Prompting Strategy</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_border_t" id="S5.T4.5.1.1.1.3">Total Acc (%)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_border_t" id="S5.T4.5.1.1.1.4">Count Acc (%)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_border_t" id="S5.T4.5.1.1.1.5">Relation Acc (%)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_border_t" id="S5.T4.5.1.1.1.6">Depth Acc (%)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_border_t" id="S5.T4.5.1.1.1.7">Distance Acc (%)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T4.5.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T4.5.1.2.1.1">Instruct Model</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.5.1.2.1.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.5.1.2.1.3">55.64</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.5.1.2.1.4">45.43</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.5.1.2.1.5">68.92</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.5.1.2.1.6">58.66</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.5.1.2.1.7">51.66</td>
</tr>
<tr class="ltx_tr" id="S5.T4.5.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S5.T4.5.1.3.2.1">Instruct Model + RL</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.5.1.3.2.2">with reason</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.5.1.3.2.3">66.03</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.5.1.3.2.4">69.54</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.5.1.3.2.5">61.84</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.5.1.3.2.6">66.50</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.5.1.3.2.7">65.50</td>
</tr>
</tbody>
</table>{{< /table-caption >}}
> üîº This table presents the results of applying reinforcement learning (RL) to instruction-tuned models for multimodal reasoning. It compares the performance of different prompting strategies (using both `<think>` and `<answer>` tags versus only `<answer>` tags) when applying RL to these pre-trained models, specifically on the CVBench benchmark. The results show that using only the `<answer>` tag prompt leads to superior performance compared to using both `<think>` and `<answer>` tags, highlighting the impact of prompting strategies on the effectiveness of RL in multimodal reasoning.
> <details>
> <summary>read the caption</summary>
> Table 4: Evaluation of applying RL to instruct models. Prompting with only <answer> achieves superior performance in the context of applying RL on instruct models.
> </details>

{{< table-caption >}}
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T5.3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T5.3.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt ltx_border_t" id="S5.T5.3.1.1.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_border_t" id="S5.T5.3.1.1.1.2">Freezing Components</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_border_t" id="S5.T5.3.1.1.1.3">Total Acc (%)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_border_t" id="S5.T5.3.1.1.1.4">Count Acc (%)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_border_t" id="S5.T5.3.1.1.1.5">Relation Acc (%)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_border_t" id="S5.T5.3.1.1.1.6">Depth Acc (%)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_border_t" id="S5.T5.3.1.1.1.7">Distance Acc (%)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T5.3.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T5.3.1.2.1.1">Instruct model + RL</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.1.2.1.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.1.2.1.3">63.57</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.1.2.1.4">61.68</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.1.2.1.5">71.08</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.1.2.1.6">61.50</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.1.2.1.7">60.00</td>
</tr>
<tr class="ltx_tr" id="S5.T5.3.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T5.3.1.3.2.1">Instruct model + RL</th>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.3.2.2">vision encoder</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.3.2.3">68.34</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.3.2.4">64.21</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.3.2.5">78.00</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.3.2.6">65.67</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.3.2.7">66.00</td>
</tr>
<tr class="ltx_tr" id="S5.T5.3.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S5.T5.3.1.4.3.1">Instruct model + RL</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.3.1.4.3.2">LLM</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.3.1.4.3.3">65.35</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.3.1.4.3.4">62.05</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.3.1.4.3.5">72.46</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.3.1.4.3.6">65.17</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.3.1.4.3.7">62.17</td>
</tr>
</tbody>
</table>{{< /table-caption >}}
> üîº This table presents the results of an experiment evaluating the effectiveness of applying reinforcement learning (RL) to instruction-tuned models. The experiment explores two variations: freezing either the language or vision components of the model during RL training.  The table compares the performance of these variations against a baseline model (no components frozen) across multiple metrics (Total accuracy, Count accuracy, Relation accuracy, Depth accuracy, and Distance accuracy). This is intended to analyze how freezing different parts of the model affects the performance improvement gained through RL training on instruction-tuned models. The results show that both strategies lead to performance improvements, suggesting that focusing the RL training on specific components of the multimodal model can yield gains in accuracy, although not necessarily in all evaluation metrics.
> <details>
> <summary>read the caption</summary>
> Table 5: Evaluation of applying RL to instruct models with different freezing components. Either freezing language or vision components of instruct model can both leads to performance improvement.
> </details>

</details>




### Full paper

{{< gallery >}}
<img src="https://ai-paper-reviewer.com/2503.05132/1.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2503.05132/2.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2503.05132/3.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2503.05132/4.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2503.05132/5.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2503.05132/6.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2503.05132/7.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2503.05132/8.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2503.05132/9.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2503.05132/10.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
{{< /gallery >}}