[{"figure_path": "https://arxiv.org/html/2503.18931/x1.png", "caption": "Figure 1: Overview of CoMP. Our method accepts an image at native resolution and its corresponding text. Then, in addition to training through text decoding in next-token prediction paradigm, we also explicitly project the visual features into the language space of LLM using Alignment Loss.", "description": "The figure illustrates the CoMP (Continual Multimodal Pre-training) framework.  CoMP takes an image at its original resolution and its corresponding text as input. It then performs training in two ways: 1) standard next-token prediction using text decoding, and 2)  explicitly projects the visual features into the language space of a large language model (LLM) via an Alignment Loss. This dual-training approach aims to better align visual and textual representations, improving the model's performance on various tasks.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.18931/x2.png", "caption": "Figure 2: \nArchitecture of our CoMP. (a) Overview of our pre-training framework;\n(b) Detail of C-RoPE. For ease of visualization, the projection layers \ud835\udcab\u2062r\u2062o\u2062jq,k,v,o\ud835\udcab\ud835\udc5f\ud835\udc5csubscript\ud835\udc57\ud835\udc5e\ud835\udc58\ud835\udc63\ud835\udc5c\\mathcal{P}roj_{q,k,v,o}caligraphic_P italic_r italic_o italic_j start_POSTSUBSCRIPT italic_q , italic_k , italic_v , italic_o end_POSTSUBSCRIPT and scale operators are omitted.", "description": "Figure 2 illustrates the architecture of the COMP (Continual Multimodal Pre-training) framework. Panel (a) provides a high-level overview of the entire pre-training pipeline, showcasing the integration of a Vision Foundation Model (VFM), a Large Language Model (LLM), and a newly proposed continual multimodal pre-training process.  It highlights the two key loss functions: the decoding loss for text generation and the alignment loss for aligning visual and textual representations.  Panel (b) focuses specifically on the Continual Rotary Position Embedding (C-ROPE) module, detailing how it handles variable-resolution visual inputs. For clarity, minor components like projection and scaling operations are omitted in the diagram.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.18931/x3.png", "caption": "Figure 3: Alignment Loss. We illustrate it in the case of one single pair of global vision and text features \ud835\udc05vsubscript\ud835\udc05\ud835\udc63\\mathbf{F}_{v}bold_F start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT and \ud835\udc05tsubscript\ud835\udc05\ud835\udc61\\mathbf{F}_{t}bold_F start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT for simplicity. \ud835\udc05vsubscript\ud835\udc05\ud835\udc63\\mathbf{F}_{v}bold_F start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT and \ud835\udc05tsubscript\ud835\udc05\ud835\udc61\\mathbf{F}_{t}bold_F start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT are mapped by frozen learned prototype \ud835\udc16\ud835\udc16\\mathbf{W}bold_W, i.e., the word embedding of LLMs. Then, they are converted into normalized probabilities using the Softmax function and iterative Sinkhorn-Knopp algorithm\u00a0[12], respectively. Then, cross-entropy is applied as the loss. To prevent information leakage, the text features are extracted without image prefixes.", "description": "The Alignment Loss in the figure is calculated using global visual features (Fv) and text features (Ft). These features are projected into the language space using a learned prototype (W), which is the word embedding of Large Language Models (LLMs).  The Sinkhorn-Knopp algorithm normalizes the projected features into probability distributions, and then cross-entropy is used to compute the loss, encouraging alignment between visual and textual representations.  To avoid information leakage from the image, text features are extracted without the image prefix.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.18931/x4.png", "caption": "Figure 4: Varying the image resolution during inference. We investigate the impact of image resolution on DocVQA\u00a0[47] and ChartQA\u00a0[46] by our CoMP-MM-1B.", "description": "This figure analyzes how varying image resolution at inference time affects the performance of the CoMP-MM-1B model on two specific downstream tasks: DocVQA [47] and ChartQA [46].  DocVQA focuses on question answering about document images, while ChartQA involves understanding chart-related queries. Both tasks are sensitive to image resolution.  The graph shows that increased resolution leads to improved accuracy. This demonstrates CoMP-MM-1B's ability to handle high-resolution inputs effectively.", "section": "4.3. Image Recognition"}]