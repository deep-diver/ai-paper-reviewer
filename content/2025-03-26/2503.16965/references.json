{"references": [{"fullname_first_author": "Hu", "paper_title": "VIVA: A benchmark for vision-grounded decision-making with human values", "publication_date": "2024-05-01", "reason": "This paper introduces the VIVA benchmark, which is the primary dataset used in the study for evaluating human-centered decision-making abilities of VLMs."}, {"fullname_first_author": "Wolf", "paper_title": "Transformers: State-of-the-art natural language processing", "publication_date": "2020-01-01", "reason": "This paper introduces the Transformers library, which is essential for implementing the experiments involving LLMs and VLMs."}, {"fullname_first_author": "Liu", "paper_title": "Improved baselines with visual instruction tuning", "publication_date": "2023-01-01", "reason": "This paper likely discusses visual instruction tuning, a relevant technique for improving VLMs which the current paper contrasts with its text-only approach."}, {"fullname_first_author": "Rajbhandari", "paper_title": "ZeRO: Memory optimizations toward training trillion parameter models", "publication_date": "2020-01-01", "reason": "This paper introduces DeepSpeed ZeRO, a memory optimization technique used for training large models in this study."}, {"fullname_first_author": "Hu", "paper_title": "LORA: Low-Rank Adaptation of Large Language Models", "publication_date": "2021-01-01", "reason": "This paper introduces LoRA, a parameter-efficient fine-tuning technique used to train VLMs in this study."}]}