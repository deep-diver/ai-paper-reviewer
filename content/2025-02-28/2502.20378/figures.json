[{"figure_path": "https://arxiv.org/html/2502.20378/x1.png", "caption": "Figure 1: (a) Given a set of monocular multi-view images and camera poses, our method achieves real-time rendering for dynamic scenes while maintaining high rendering quality. (b) Our method achieves promising rendering quality with faster rendering speed and fewer Gaussians. The radius of the circle is the number of time-variant Gaussians whose attributes need to be queried by MLPs. (c) The bottleneck of the rendering speed for dynamic scenes is the number of Gaussians. The fewer the number of Gaussians, the faster the rendering speed.", "description": "Figure 1 demonstrates the performance of the proposed method, Efficient Dynamic Gaussian Splatting (EDGS), for rendering dynamic scenes. (a) showcases real-time rendering results with high quality from monocular multi-view images. (b) compares EDGS against other state-of-the-art methods in terms of rendering speed, quality (PSNR), and the number of Gaussians used, highlighting EDGS's efficiency.  The circle's radius represents the count of time-variant Gaussians whose attributes require querying from MLPs, indicating the computational cost. (c) illustrates the strong correlation between rendering speed and the number of Gaussians, emphasizing that EDGS's reduced Gaussian count leads to faster rendering.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2502.20378/x2.png", "caption": "Figure 2: The pipeline of our EDGS. 1) We first initialize voxelized sparse anchor points from Structure from Motion (SfM) points derived from COLMAP. 2) A time-mask MLP is applied to classify if the anchor belongs to the static area or the deformable area. 3) k\ud835\udc58kitalic_k Gaussian offsets are initialized for each anchor \ud835\udc82\ud835\udc82\\bm{a}bold_italic_a belonging to static area. The time-invariant attributes of each Gaussian, i.e., opacity, quaternion, scale, and color are calculated from its feature by corresponding tiny MLPs. 4) Time-variant attributes for anchors from dynamic areas are decoded by a deformable attribute MLP. RBF kernel function is employed to compute the location of each Gaussian at timestep t\ud835\udc61titalic_t by calculating the similarity between each Gaussian and its belonging anchor point. This pipeline is compact and efficient, featuring only a few tiny MLPs for the attributes of the Gaussians and a single network for deformations. Notably, the position of each anchor remains static and is not subject to updates.", "description": "This figure illustrates the pipeline of Efficient Dynamic Gaussian Splatting (EDGS).  It begins by initializing sparse anchor points from a Structure from Motion (SfM) point cloud using COLMAP. A time-mask Multi-Layer Perceptron (MLP) then classifies each anchor point as either static or dynamic.  Static anchors receive k Gaussian offsets, and their time-invariant attributes (opacity, quaternion, scale, color) are determined by small MLPs.  Dynamic anchors have their time-variant attributes processed by a deformable attribute MLP, with Gaussian locations at each time step calculated using a Radial Basis Function (RBF) kernel to determine the similarity between each Gaussian and its anchor.  The entire process is efficient, using small MLPs for attribute calculation and a single network for deformation. Importantly, anchor positions remain fixed.", "section": "4 Method"}, {"figure_path": "https://arxiv.org/html/2502.20378/x3.png", "caption": "Figure 3: Qualitative comparison on the NeRF-DS dataset\u00a0(Yan, Li, and Lee 2023). Compared with other SOTA methods, our method reconstructs finer details and produces a structured rendering of the moving objects, e.g., the cup on human\u2019s hand.", "description": "Figure 3 presents a qualitative comparison of novel view synthesis results on the NeRF-DS dataset.  The figure shows how different methods, including the authors' proposed approach and several state-of-the-art (SOTA) techniques, render a scene containing multiple moving objects. The key takeaway is that the authors' method reconstructs finer details and achieves more structured rendering of moving elements.  This is demonstrated by a comparison of how each method renders a cup held in a hand. The improved detail and structure in the authors' result indicate a more accurate and visually pleasing representation of the dynamic scene.", "section": "5.2 Quantitative Comparisons"}, {"figure_path": "https://arxiv.org/html/2502.20378/x4.png", "caption": "Figure 4: Qualitative comparison on the HyperNeRF dataset\u00a0(Park et\u00a0al. 2021b). Our EDGS reconstructs detailed texture and reliable structure compared with other SOTA methods.", "description": "Figure 4 presents a qualitative comparison of dynamic scene reconstruction results on the HyperNeRF dataset. It visually demonstrates the superior performance of the proposed Efficient Dynamic Gaussian Splatting (EDGS) method against other state-of-the-art (SOTA) techniques.  EDGS is shown to reconstruct significantly more detailed textures and a more reliable overall scene structure compared to the other methods.  This showcases its ability to capture fine details and accurately represent the scene geometry, even in complex dynamic scenarios.", "section": "5.2 Quantitative Comparisons"}, {"figure_path": "https://arxiv.org/html/2502.20378/x5.png", "caption": "Figure 5: Visuazization of the difference map (diff.) and the optical flow with fixed camera views. We synthesis fixed novel view across time for (Yang et\u00a0al. 2023; Wu et\u00a0al. 2023) and ours. The 1s\u2062tsuperscript1\ud835\udc60\ud835\udc611^{st}1 start_POSTSUPERSCRIPT italic_s italic_t end_POSTSUPERSCRIPT row is the rendered frames at various time steps. The 2n\u2062dsuperscript2\ud835\udc5b\ud835\udc512^{nd}2 start_POSTSUPERSCRIPT italic_n italic_d end_POSTSUPERSCRIPT and 3r\u2062dsuperscript3\ud835\udc5f\ud835\udc513^{rd}3 start_POSTSUPERSCRIPT italic_r italic_d end_POSTSUPERSCRIPT rows are the difference map between tt\u2062hsuperscript\ud835\udc61\ud835\udc61\u210et^{th}italic_t start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT frame and the 1s\u2062tsuperscript1\ud835\udc60\ud835\udc611^{st}1 start_POSTSUPERSCRIPT italic_s italic_t end_POSTSUPERSCRIPT frame and the optical flow, respectively. The response in the highlighted red area indicates that the static area rendered by deformable GS and 4DGS is jittering. Our method achieves better quality for static and dynamic objects.", "description": "Figure 5 compares the rendering results of three different methods: Deformable 3DGS, 4DGS, and the proposed method (Ours).  It shows three rows of visualizations for each method, all using a fixed camera perspective across several time steps. The top row displays the rendered frames. The second row presents the difference between each frame and the first frame, highlighting changes over time.  The third row illustrates the optical flow, showing movement vectors. The red box emphasizes a region where the static elements appear to exhibit jittering (unwanted movement) in Deformable 3DGS and 4DGS, indicating a deficiency in representing these parts of the scene. The authors' method avoids this issue, resulting in improved quality for both static and dynamic scene elements.", "section": "5.3 Qualitative Comparisons"}, {"figure_path": "https://arxiv.org/html/2502.20378/x6.png", "caption": "Figure 6: Visualization of anchor features and time mask. We visualize the rendered images, anchor features, and time masks for two deformation scenes. Anchor features are visualized using the UMAP function. In the time-mask visualization, the time masks are predicted by time-mask MLP \u03a6m\u2062a\u2062s\u2062ksubscript\u03a6\ud835\udc5a\ud835\udc4e\ud835\udc60\ud835\udc58\\varPhi_{mask}roman_\u03a6 start_POSTSUBSCRIPT italic_m italic_a italic_s italic_k end_POSTSUBSCRIPT. Green anchors represent static scenes, while red anchors indicate deformation scenes.", "description": "Figure 6 visualizes the learned anchor features and the predicted time masks.  The visualization employs the UMAP technique to reduce the dimensionality of anchor features for easier interpretation. Two scenes are shown, each with a side and front view.  The time mask, generated by the time-mask MLP (\u03a6mask), distinguishes between static (green) and dynamic (red) scene regions by classifying anchor points as static or dynamic based on their time-varying attributes. This highlights the network's ability to differentiate between these two regions without explicit supervision.", "section": "5.4 Ablation Studies"}]