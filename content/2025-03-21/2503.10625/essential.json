{"importance": "This paper introduces a novel method of **creating animatable 3D human avatars from a single image in seconds**. It surpasses existing methods and provides new avenues for real-time avatar creation and animation. Its **generalization ability and potential impact on AR/VR applications** make it significant for future research.", "summary": "LHM: Animatable 3D avatars from a single image in seconds.", "takeaways": ["LHM achieves state-of-the-art performance in single-image 3D human reconstruction and animation.", "The multimodal transformer effectively fuses 3D and 2D features, enhancing geometric and visual detail.", "A large-scale video dataset and self-supervised training enable generalizable human priors without requiring 3D scans."], "tldr": "Creating 3D avatars from single images is hard because of geometry, appearance, and movement. Existing methods mainly focus on static models, rely on synthetic data which limits real-world use, or need intensive refinement processes. To solve these issues, researchers need methods that are good at handling clothing, facial details, and animation consistency without needing lots of computing power or special setups. A model that can quickly and accurately create 3D avatars from single images would greatly benefit AR/VR. \n\nThis paper introduces a new approach called LHM, which is short for Large Animatable Human Reconstruction Model. It is a **feed-forward transformer model** that creates animatable 3D human avatars in seconds from a single image. The model uses a special transformer design to **fuse 3D body features with 2D image features**, and a new head feature method to improve face details. LHM is trained using readily available video data which avoids needing expensive 3D scans and achieves better results.", "affiliation": "Alibaba Group", "categories": {"main_category": "Computer Vision", "sub_category": "3D Vision"}, "podcast_path": "2503.10625/podcast.wav"}