[{"figure_path": "https://arxiv.org/html/2503.16365/x1.png", "caption": "Figure 1: We present JARVIS-VLA, a novel Vision-Language-Action (VLA) model trained with ActVLP paradigm, post-trained on vision language tasks (non-decision-making tasks) before training on trajectory datasets to have better decision-making capabilities.", "description": "Figure 1 illustrates the JARVIS-VLA model architecture and training process.  JARVIS-VLA is a novel Vision-Language-Action (VLA) model that leverages a multi-stage training approach called ActVLP. This approach first involves post-training the model on non-decision making visual-language tasks to enhance its world knowledge, visual recognition, and spatial understanding.  Only after this initial post-training phase, does the model undergo training on trajectory datasets (data showing sequences of actions) to improve its decision-making capabilities.  This two-stage process is shown schematically in the figure, highlighting the key components of the model (Visual Transformer, Causal Transformer, Action Decoder) and their interaction with the environment.", "section": "2. Learning to Act from Vision Language Post-Training"}, {"figure_path": "https://arxiv.org/html/2503.16365/x2.png", "caption": "Figure 2: Previous VLA methods usually directly use imitation learning to finetune original vision-language models on large-scale multi-domain decision-making datasets to predict the actions\u00a0[25, 7]. Our ActVLP training pipeline includes three stages: 1) post-training language models on text-only world knowledge with next-token prediction supervised fine-tuning, 2) post-training both vision encoder and language models on multimodal vision-language alignment and spatial grounding datasets with next-token prediction supervised fine-tuning, and 3) post-training only language models on multi-modal instruction following datasets with imitation learning.", "description": "This figure illustrates the difference between traditional VLA training methods and the proposed ActVLP approach. Traditional methods directly fine-tune a vision-language model on large-scale datasets using imitation learning to predict actions.  In contrast, ActVLP is a three-stage training pipeline. Stage 1 post-trains the language model on text-based world knowledge using supervised fine-tuning with next-token prediction. Stage 2 further post-trains both the vision encoder and language model on multimodal datasets focusing on vision-language alignment and spatial grounding, again using supervised fine-tuning with next-token prediction. Finally, Stage 3 post-trains only the language model on multimodal instruction-following datasets via imitation learning. This multi-stage approach aims to enhance the model's world knowledge, visual understanding, and spatial reasoning capabilities before relying solely on imitation learning for action prediction.", "section": "2. Learning to Act from Vision Language Post-Training"}, {"figure_path": "https://arxiv.org/html/2503.16365/x3.png", "caption": "Figure 3: Illustration of various post-training datasets.\nModels can post-train on various vision-language datasets using a unified tokenizer and support diverse vision-language applications, such as question answering, image captioning, image/video question answering, visual grounding (including points and bounding box), and decision-making.\nMore examples can be found in Appendix\u00a0D.", "description": "Figure 3 illustrates the diverse visual-language datasets used for post-training the model.  A unified tokenizer allows the model to leverage these datasets, improving its performance on various vision-language tasks including question answering, image captioning, image/video question answering, and visual grounding (which incorporates both point and bounding box annotations).  This multifaceted approach enhances the model's decision-making capabilities.", "section": "2. Learning to Act from Vision Language Post-Training"}, {"figure_path": "https://arxiv.org/html/2503.16365/x6.png", "caption": "Figure 4: Ablation results on different post-training datasets.\nWe select knowledge datasets, visual question-answering datasets, and spatial grounding datasets to conduct ablation experiments. Our goal is to evaluate which capabilities and post-training datasets most significantly influence downstream decision-making tasks.", "description": "This figure displays the results of ablation studies conducted to determine the impact of different post-training datasets on downstream decision-making tasks. Three types of datasets were used: knowledge datasets, visual question-answering datasets, and spatial grounding datasets.  The results show the success rates for three downstream tasks (crafting a sword, mining obsidian, and cooking beef) when different combinations of post-training datasets were used. This illustrates which visual-language capabilities are most crucial for effective task performance, highlighting the importance of a well-rounded, multi-faceted post-training process.", "section": "3.3. Ablation on Non-Trajectory Datasets"}, {"figure_path": "https://arxiv.org/html/2503.16365/x7.png", "caption": "Figure 5: The relation between downstream task success rate, training loss, and training steps. The curve shows that scaling downstream finetuning trajectories can scale up the success rate when the loss is lower than 0.22.", "description": "Figure 5 illustrates the relationship between the success rate of downstream tasks, the training loss, and the number of training steps in a model. The graph shows that increasing the number of downstream fine-tuning trajectories improves the model's success rate, but only when the training loss is below 0.22.  This indicates a threshold where additional data significantly impacts performance.", "section": "3.4. Scaling Experiments"}, {"figure_path": "https://arxiv.org/html/2503.16365/x8.png", "caption": "Figure 6: \nThe relationship between post-training loss and downstream task success rates. Our findings indicate that increasing the size of post-training non-trajectory datasets can significantly enhance downstream task success rates, even with a fixed number of fine-tuning trajectories.", "description": "This figure illustrates the correlation between the loss during post-training on non-trajectory datasets and the success rate on downstream tasks.  It demonstrates that increasing the size of the non-trajectory training data improves the performance on downstream tasks, even when the amount of fine-tuning trajectory data remains constant.  This highlights the importance of comprehensive pre-training in enhancing the model's ability to generalize to new tasks.", "section": "3.3. Ablation on Non-Trajectory Datasets"}]