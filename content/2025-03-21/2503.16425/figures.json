[{"figure_path": "https://arxiv.org/html/2503.16425/x1.png", "caption": "Figure 1: Pipeline of our method.", "description": "This figure illustrates the pipeline of the proposed Tokenize Image as a Set method.  It begins with an image encoder that transforms image patches into an unordered set of tokens (TokenSet). A dual transformation mechanism converts this set into a fixed-length integer sequence with a summation constraint. This sequence is then processed by a novel Fixed-Sum Discrete Diffusion model, which ensures the model respects the summation constraint throughout the diffusion process. Finally, an image decoder reconstructs the image from the resulting sequence.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.16425/x2.png", "caption": "Figure 2: Pipeline of our set tokenizer.", "description": "The figure illustrates the process of converting an image into an unordered set of tokens.  The input image is processed by an encoder, which combines image patches and learnable latent tokens. The encoder outputs continuous representations which are then discretized using a VQ-VAE codebook. This results in a 1D sequence of tokens which then has the positional information removed by randomly shuffling the order of the tokens before feeding to the decoder.  The decoder reconstructs the image from the unordered set of tokens, demonstrating the method's permutation invariance.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.16425/x3.png", "caption": "Figure 3: Fixed-Sum Diffusion Process. Sample Xtsubscript\ud835\udc4b\ud835\udc61X_{t}italic_X start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT at noise level t=0.6\ud835\udc610.6t=0.6italic_t = 0.6 is first sampled from the mixed gaussian distribution of X0subscript\ud835\udc4b0X_{0}italic_X start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT and X1subscript\ud835\udc4b1X_{1}italic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, and then adjusted through greedy adjustment. Samples dropped during greedy adjustment are marked with dashed line.", "description": "This figure illustrates the Fixed-Sum Discrete Diffusion process, a key component of the proposed image generation method. It shows how a sample from a noisy distribution (at noise level t=0.6) is generated by combining the initial clean image distribution and a fully noisy distribution.  The process also incorporates a greedy adjustment step to enforce the fixed-sum constraint, ensuring the summation of all elements in the sample remains constant. This constraint is crucial for maintaining the bijection between sets and sequences. Samples that were removed during the adjustment phase to satisfy this constraint are marked with dashed lines. The process visually demonstrates how the algorithm iteratively approaches a clean image representation whilst adhering to the defined constraint.", "section": "3.3 Fixed-Sum Discrete Diffusion"}, {"figure_path": "https://arxiv.org/html/2503.16425/extracted/6297212/figs/setimages/ori3.png", "caption": "(a) Original image", "description": "This figure demonstrates the permutation invariance of the proposed set-based tokenizer.  It shows the reconstruction of an image from its encoded tokens arranged in different orders: (a) the original image; (b) tokens in the original order; (c) tokens in reversed order; (d) randomly shuffled tokens; (e) tokens sorted in ascending order; (f) tokens sorted in descending order. The visual similarity between all six reconstructed images confirms the permutation invariance\u2014the reconstruction is insensitive to token ordering.", "section": "3.1 Image Set Tokenizer"}, {"figure_path": "https://arxiv.org/html/2503.16425/extracted/6297212/figs/setimages/1order2.png", "caption": "(b) Original order", "description": "This image shows the reconstruction of an image from its encoded tokens in their original order.  This is part of an experiment to demonstrate the permutation invariance of the proposed set-based tokenization method. The image quality should be identical to reconstructions from other token orderings (reversed, shuffled, sorted ascending, sorted descending), highlighting that the method is not sensitive to the sequence of tokens.", "section": "4.2.1. Permutation-invariance"}, {"figure_path": "https://arxiv.org/html/2503.16425/extracted/6297212/figs/setimages/2order2.png", "caption": "(c) Reversed order", "description": "This image shows a reconstruction of an image from its encoded tokens, where the order of the tokens has been reversed.  This demonstrates the permutation invariance of the image set tokenizer. The visual appearance is identical to the original image, highlighting that the reconstruction is not sensitive to the ordering of the tokens.", "section": "4.2.1. Permutation-invariance"}, {"figure_path": "https://arxiv.org/html/2503.16425/extracted/6297212/figs/setimages/3order2.png", "caption": "(d) Random shuffle", "description": "This image shows the reconstruction of an image from its encoded tokens, but with the order of those tokens randomly shuffled.  Despite the random order, the reconstructed image remains identical to the reconstructions from other token orders (original, reversed, ascending, descending), visually demonstrating the permutation invariance achieved by the set-based tokenization method. This property means the model's performance is unaffected by the order of the input tokens, a key advantage of the set-based approach.", "section": "3.1. Image Set Tokenizer"}, {"figure_path": "https://arxiv.org/html/2503.16425/extracted/6297212/figs/setimages/4order2.png", "caption": "(e) Sorted ascending", "description": "This image shows the reconstruction of an image from its encoded tokens, which are sorted in ascending order.  This demonstrates the permutation invariance of the set-based tokenizer. Even though the order of tokens has been changed, the reconstructed image remains the same as the original. This highlights the model's ability to reconstruct images regardless of the order of encoded tokens, proving the effectiveness of representing images as an unordered set of tokens rather than a sequence.", "section": "3.1. Image Set Tokenizer"}, {"figure_path": "https://arxiv.org/html/2503.16425/extracted/6297212/figs/setimages/5order2.png", "caption": "(f) Sorted descending", "description": "This figure shows the reconstruction results of an image using tokens in a sorted descending order.  This demonstrates that the set-based tokenizer in the proposed method is permutation-invariant.  The identical results in this image compared to the other images (a-e) in the figure highlight that changing the token order during decoding does not impact the final image reconstruction.  The images are identical because the model learns to treat tokens as a set, ignoring their position.", "section": "3.1. Image Set Tokenizer"}, {"figure_path": "https://arxiv.org/html/2503.16425/extracted/6297212/figs/globalv4.png", "caption": "Figure 4: Visual comparison of the reconstructed images from various order permutations of the encoded tokens. All reconstructed images are identical, demonstrating the set-based tokenizer is permutation-invariance.", "description": "This figure shows six reconstructed images from the same encoded tokens, but with different ordering. The first image uses the original order of tokens, followed by the reversed order, a random shuffle, and then ascending and descending sorted orders. Despite the different arrangements of input tokens, all six resulting images are identical. This demonstrates the permutation invariance of the set-based image tokenizer, proving that the order of tokens does not affect the final reconstructed image.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.16425/x4.png", "caption": "Figure 5: The receptive fields of individual tokens. Each column represents the receptive field corresponding to the same token. Previous methods such as VQGAN\u00a0[10] encoded tokens strictly correspond to specific positions. In contrast, our approach demonstrates a unique property that many tokens possess global receptive fields.", "description": "Figure 5 visualizes the receptive fields of individual tokens generated by the proposed method and compares them to those of previous methods like VQGAN. Each column displays the receptive field for the same token.  Previous methods, such as VQGAN, show tokens that strictly correspond to specific spatial image locations. In contrast, the proposed method shows that many tokens have receptive fields that span across a significant portion of the image, demonstrating a global contextual awareness.", "section": "4.2 Set tokenizer"}]