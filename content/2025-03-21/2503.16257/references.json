{"references": [{"fullname_first_author": "Coleman Richard Charles Hooper", "paper_title": "KVQuant: Towards 10 million context length Ilm inference with kv cache quantization.", "publication_date": "2024-01-01", "reason": "This paper is important because it introduces KVQuant, a technique relevant to extending context length in large language models using KV cache quantization, a focus of the analyzed paper."}, {"fullname_first_author": "Zirui Liu", "paper_title": "KIVI: A tuning-free asymmetric 2bit quantization for kv cache.", "publication_date": "2024-01-01", "reason": "This paper is important because KIVI is a prior work focusing on KV cache quantization with a tuning-free asymmetric 2-bit approach, and VidKV directly compares against this method."}, {"fullname_first_author": "Muhammad Maaz", "paper_title": "Video-chatgpt: Towards detailed video understanding via large vision and language models.", "publication_date": "2023-01-01", "reason": "This paper is important as VidKV is also focused on improvements to VideoLLMs, so this foundational paper directly relates to the context."}, {"fullname_first_author": "Bo Li", "paper_title": "Llava-onevision: Easy visual task transfer.", "publication_date": "2024-01-01", "reason": "This paper is important because VidKV evaluates their method on this model, thus demonstrating its effectiveness."}, {"fullname_first_author": "Shuming Ma", "paper_title": "The era of 1-bit llms: All large language models are in 1.58 bits.", "publication_date": "2024-01-01", "reason": "This paper is important because it focuses on 1.58-bit quantization which VidKV uses in its approach."}]}