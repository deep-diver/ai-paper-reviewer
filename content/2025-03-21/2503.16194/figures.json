[{"figure_path": "https://arxiv.org/html/2503.16194/x1.png", "caption": "Figure 1: \n(a) The codeword clustering process, where token indices are grouped based on the similarity of their corresponding feature vectors in the codebook.\n(b) Visual demonstration of token redundancy: replacing each token with another randomly sampled from the same cluster produces images with only minor variations in detail, preserving the overall structure and content.\n(c) Illustration of our two-stage generation process: in the first stage, the model autoregressively predicts coarse labels (cluster indices) for each token in the sequence; then the second stage model predicts fine labels (indices in the codebook) for all tokens in a single step.", "description": "Figure 1 illustrates the core concepts of the proposed Coarse-to-Fine (CTF) image generation method. (a) shows the process of clustering similar codewords (vectors representing image features) in the codebook of the VQ-VAE into groups, forming coarse labels.  (b) demonstrates the redundancy within large codebooks by showing that images reconstructed after replacing tokens with similar ones from the same cluster retain overall structure and content with minimal detail changes. This highlights that fine-grained token-level prediction might be unnecessary. (c) details the two-stage generation process. The first stage predicts a sequence of coarse labels autoregressively, representing clusters of codewords rather than individual tokens. The second stage then uses the coarse label sequence as context to simultaneously predict all the fine-grained labels (original tokens) for the whole image in a single step. This approach reduces the autoregressive model's computational complexity.", "section": "Method"}, {"figure_path": "https://arxiv.org/html/2503.16194/x2.png", "caption": "Figure 2: Model performance comparison on different epochs. When our method is applied, models achieve significantly better performance.", "description": "This figure displays a comparison of the performance of the baseline LlamaGen models and the models enhanced with the proposed coarse-to-fine (CTF) method across different training epochs.  The plots show the Inception Score (IS) and Fr\u00e9chet Inception Distance (FID) metrics over the training epochs.  The CTF-enhanced models exhibit significantly better performance and faster convergence, achieving superior IS and lower FID scores compared to the baseline LlamaGen models.  The graph also visually shows a speedup in convergence for the CTF models.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.16194/x3.png", "caption": "Figure 3: Generation results of our method (based on LlamaGen-XL) on ImageNet 256\u00d7256 benchmark.", "description": "This figure showcases several images generated using the proposed Coarse-to-Fine (CTF) method, specifically using the LlamaGen-XL model, as a benchmark on the ImageNet dataset with 256x256 resolution.  The images demonstrate the model's ability to generate high-quality, detailed images with rich textures and structures by leveraging a two-stage prediction approach. Each image shows the resulting image from the model, highlighting the effectiveness of the CTF method in producing visually appealing and coherent outputs.", "section": "5.6. Visualization"}]