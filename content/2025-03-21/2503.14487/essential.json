{"importance": "This work on dynamic token selection addresses **scalability and efficiency** in diffusion models, offering state-of-the-art image generation. It opens avenues for new architectures in AI, potentially impacting **various applications beyond image synthesis.**", "summary": "DiffMoE: Dynamically selects tokens for scalable diffusion transformers, unlocking new efficiency levels in image generation.", "takeaways": ["Global token accessibility is crucial for MoE success in diffusion models.", "DiffMoE architecture achieves state-of-the-art performance with dynamic computation.", "Capacity predictor dynamically adjusts computational resource allocation."], "tldr": "Diffusion Transformers excel in visual generation but treat all inputs uniformly, missing heterogeneity benefits. Mixture-of-Experts (MoE) aims to fix this, but struggles with limited token access and fixed patterns. **The current MoE limits token selection within individual samples and noise levels. Dense and TC-MoE isolate tokens while EC-DiT restricts intra-sample interaction.** Thus, it hinders model capture of heterogeneity in the diffusion process.\n\nTo solve these issues, DiffMoE was introduced. **DiffMoE uses a batch-level global token pool for enhanced cross-sample interaction. A capacity predictor dynamically allocates resources. This leads to state-of-the-art performance, outperforming dense architectures with 3\u00d7 activated parameters while maintaining 1\u00d7.** DiffMoE's method extends to text-to-image tasks and is broadly applicable across diffusion models.", "affiliation": "Tsinghua University", "categories": {"main_category": "Computer Vision", "sub_category": "Image Generation"}, "podcast_path": "2503.14487/podcast.wav"}