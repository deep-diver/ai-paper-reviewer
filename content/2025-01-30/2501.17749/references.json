{"references": [{"fullname_first_author": "T. Xie", "paper_title": "Sorry-bench: Systematically evaluating large language model safety refusal behaviors", "publication_date": "2024-06-14", "reason": "This paper proposes Sorry-bench, a benchmark for evaluating the safety refusal behaviors of LLMs, which is directly relevant to the methodology and evaluation of the current study."}, {"fullname_first_author": "X. Yuan", "paper_title": "S-eval: Automatic and adaptive test generation for benchmarking safety evaluation of large language models", "publication_date": "2024-05-14", "reason": "S-eval provides an automated and adaptive test generation approach for benchmarking LLM safety, offering a comparative method to the current study's approach."}, {"fullname_first_author": "Z. Zhang", "paper_title": "Safetybench: Evaluating the safety of large language models with multiple choice questions", "publication_date": "2023-09-07", "reason": "Safetybench offers a comprehensive safety evaluation framework for LLMs, providing a context for comparing the safety evaluation techniques used in this study."}, {"fullname_first_author": "W. Zhang", "paper_title": "Chisafetybench: A chinese hierarchical safety benchmark for large language models", "publication_date": "2024-06-10", "reason": "This paper focuses on safety benchmarks in Chinese, demonstrating a multilingual approach that the current study could adapt or extend."}, {"fullname_first_author": "Z. Ying", "paper_title": "Safebench: A safety evaluation framework for multimodal large language models", "publication_date": "2024-10-18", "reason": "Safebench expands safety evaluation to multimodal LLMs, offering a potential future direction for the current study's work."}]}