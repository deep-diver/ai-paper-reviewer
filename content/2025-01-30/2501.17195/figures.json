[{"figure_path": "https://arxiv.org/html/2501.17195/extracted/6157738/figures/Fig1.png", "caption": "Figure 1: Atla Selene Mini outperforms current state-of-the-art SLMJs: a) Overall task-average performance, comparing Atla Selene Mini (black) with the best and most widely used SLMJs. b) Breakdown of performance by task type and benchmark \u2013 see Table\u00a01 for full comparison.", "description": "Figure 1 presents a comparative analysis of Atla Selene Mini's performance against other state-of-the-art small language models as judges (SLMJs).  Part (a) shows the overall average performance across multiple evaluation tasks, highlighting Atla Selene Mini's superior performance. Part (b) provides a detailed breakdown of the performance across specific benchmark tasks and task types (absolute scoring, classification, and pairwise preference), offering a granular view of Atla Selene Mini's strengths and weaknesses in comparison to other SLMJs. Table 1 in the paper provides a complete listing of the benchmarks and their results.", "section": "3 Results"}, {"figure_path": "https://arxiv.org/html/2501.17195/extracted/6157738/figures/Fig2.png", "caption": "Figure 2: Data curation strategy: The process of transforming a candidate dataset (left) into the final training mix (right). Yellow boxes indicate filtering steps, purple represents synthetic generation of chosen and rejected pairs (blue and red) for preference optimization, and red circles highlight ablation-informed decisions, such as reward thresholds and dataset inclusion.", "description": "This figure illustrates the process of creating a high-quality training dataset for Atla Selene Mini. It starts with a candidate dataset, applies filtering to remove low-quality data points, and uses synthetic data generation to augment the dataset with 'chosen' and 'rejected' pairs. These pairs are generated with chain-of-thought critiques to aid in preference optimization. Ablation studies are performed to determine which datasets to include or exclude from the final training mix. Yellow boxes indicate filtering, purple indicates synthetic pair generation, and red circles represent ablation-informed decisions.", "section": "2 Methods"}, {"figure_path": "https://arxiv.org/html/2501.17195/extracted/6157738/figures/Fig3.png", "caption": "Figure 3: Real-world evaluation: a) Performance on domain-specific industry benchmarks of Atla Selene Mini (black) compared to base model (orange) measured in accuracy. Trained model shows higher expert agreement on FinanceBench, a financial benchmark, and CRAFT-MD, a medical dataset. b) Performance on RewardBench of Atla Selene Mini compared to base model, when prompt format is changed. Trained model shows consistent improvement across formats. c) Performance measured by ELO scores, based on head-to-head comparisons in Judge Arena. An early snapshot of Atla Selene Mini (bold) beats all other evaluators as of Jan 22, 2025. Error bars indicate 95% CI.", "description": "Figure 3 presents a real-world evaluation of Atla Selene Mini's performance.  Panel (a) shows accuracy comparisons on domain-specific industry benchmarks (FinanceBench and CRAFT-MD), demonstrating improved expert agreement for the trained model versus a base model. Panel (b) illustrates the model's robustness to prompt format variations on RewardBench, showing consistent performance improvements. Finally, panel (c) displays the model's ELO score in a community-driven Judge Arena, showcasing its superior performance against other evaluators as of January 22, 2025. Error bars represent 95% confidence intervals.", "section": "3 Real-world evaluation"}, {"figure_path": "https://arxiv.org/html/2501.17195/extracted/6157738/figures/nomic.png", "caption": "Figure 4: Training dataset map: Topic-stratified, two-dimensional embedding representation of Atla Selene Mini\u2019s training dataset generated using Nomic Atlas [33].", "description": "This figure visualizes the training data used for Atla Selene Mini.  Using Nomic Atlas, a tool for visualizing datasets, the data is displayed as a two-dimensional embedding.  The data points are clustered by topic, providing a visual representation of the diverse topics covered in the training data and how they relate to one another. This helps illustrate the breadth and depth of the training data's subject matter.", "section": "2 Methods"}, {"figure_path": "https://arxiv.org/html/2501.17195/extracted/6157738/figures/rm-ablation-report.png", "caption": "Figure 5: Example data point: Training example from FeedbackCollection [34], including the reference response, which is an optional field for Atla Selene Mini. This instance uses a similar prompt template to [10].", "description": "Figure 5 displays a training data example from the FeedbackCollection dataset [34]. It showcases how a prompt, an LLM response, scoring rubrics, and a reference response are used to train Atla Selene Mini.  The example includes the evaluation criteria, the LLM's response, and two different evaluations (one 'chosen' and one 'rejected')  with reasoning for each score. The evaluation is structured to focus on the humor and wit in the author's note and provides a numeric score from 1 to 5, referencing the provided rubric. The reference answer serves as a guide for assessing the quality of the LLM's response. This specific prompt template is similar to the one used in [10].", "section": "2 Methods"}]