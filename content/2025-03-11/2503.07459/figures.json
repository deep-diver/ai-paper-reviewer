[{"figure_path": "https://arxiv.org/html/2503.07459/x1.png", "caption": "Figure 1: Performance analysis of large language models on medical tasks. Overall Pass@1 accuracy comparison across models in zero-shot setting.\nThe score is an average of seven test sets\u2019 results (MedQA, PubMedQA, MedMCQA, MedBullets, MMLU, MMLU-Pro, MedExQA, and MedXpertQA).", "description": "This figure compares the zero-shot performance of various large language models (LLMs) on seven medical question answering datasets: MedQA, PubMedQA, MedMCQA, MedBullets, MMLU, MMLU-Pro, MedExQA, and MedXpertQA.  The y-axis represents the overall Pass@1 accuracy, which is the percentage of questions correctly answered by the model with the highest probability. The x-axis shows the different LLMs tested.  Each bar represents the average Pass@1 accuracy across all seven datasets. This visualization helps to benchmark the performance of different LLMs on complex medical reasoning tasks.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2503.07459/x2.png", "caption": "Figure 2: Performance analysis of agents and models on MedAgentsBench. Cost-performance trade-off analysis showing Pass@1 accuracy versus cost per sample (in log scale), with marker sizes indicating inference time. Different markers represent various prompting methods\n, while colors distinguish different models. The Pareto frontier (red dashed line) indicates optimal cost-performance trade-offs.", "description": "This figure presents a cost-performance trade-off analysis for various large language models (LLMs) and reasoning methods evaluated on the MedAgentsBench benchmark.  The x-axis represents the cost per sample (log scale), while the y-axis shows the Pass@1 accuracy (the percentage of times the model correctly identifies the top answer). The size of each marker indicates the inference time for that model and method. Different markers represent different prompting methods (e.g., zero-shot, few-shot, chain-of-thought, etc.), and different colors distinguish between different LLMs. The red dashed line highlights the Pareto frontier, which represents the optimal trade-off between cost and performance\u2014points on or above this line indicate models that provide the best performance for a given cost or the lowest cost for a given level of performance.", "section": "3 MEDAGENTSBENCH"}, {"figure_path": "https://arxiv.org/html/2503.07459/x3.png", "caption": "Figure 3: Distribution of model performance across eight medical datasets (MedQA, MedMCQA, PubMedQA, MedBullets, MMLU-Pro, MMLU, MedExQA, and MedXpertQA. Each subplot shows the number of questions answered correctly by different proportions of models (x-axis: k/N, where k is the number of correct models and N is the total number of models). Questions are categorized as either hard (left of the dashed line, <<< 50% of models correct) or easy (right of the dashed line, \u2265\\geq\u2265 50% of models correct), with selected questions highlighted in darker shades. The total question count for each dataset is indicated in the subplot titles.", "description": "Figure 3 analyzes the performance of various models on eight medical datasets (MedQA, MedMCQA, PubMedQA, MedBullets, MMLU-Pro, MMLU, MedExQA, and MedXpertQA). Each subplot displays the distribution of correct answers for each dataset, showing the number of questions answered correctly by different percentages of models.  The x-axis represents the proportion of models that answered correctly (k/N, where k is the number of correctly answering models and N is the total number of models). A dashed line separates questions categorized as 'hard' (less than 50% of models answered correctly) from 'easy' (50% or more answered correctly).  Questions selected for a subset are highlighted in darker shades.  The total number of questions in each dataset is shown in the subplot title.", "section": "3 MEDAGENTSBENCH"}, {"figure_path": "https://arxiv.org/html/2503.07459/x4.png", "caption": "Figure 4: Data contamination analysis across medical question-answering datasets using MELD. The boxplots display similarity percentages between model-generated text and original question text, with higher values potentially indicating memorization of training data.\nLower similarity scores suggest minimal data contamination, while higher values may indicate potential contamination in model training data.", "description": "Figure 4 presents a comprehensive analysis of data contamination across various medical question-answering datasets.  The analysis leverages the MELD (Memorization Effects Levenshtein Detector) technique to quantify the extent to which models memorize training data rather than genuinely reasoning. The figure employs box plots to visually represent the similarity scores (Levenshtein distance ratios) between model-generated text continuations and the original, unseen portions of the questions.  Higher similarity scores indicate a greater likelihood of memorization, suggesting potential contamination of the training data. Lower scores, conversely, imply less memorization and stronger evidence of true reasoning ability.  Each box plot corresponds to a specific dataset, allowing for a direct comparison of memorization across different benchmarks.", "section": "5.1 Data Contamination"}, {"figure_path": "https://arxiv.org/html/2503.07459/x5.png", "caption": "Figure 5: Cost-performance analysis across seven medical datasets, comparing open and closed-source language models. Each subplot shows Pass@1 accuracy (%) versus cost per sample (USD, log scale). Marker shapes distinguish thinking models from non-thinking models, while colors indicate open-source (blue) versus closed-source (red) models. Marker sizes represent inference time, and the red dashed line shows the Pareto frontier of optimal cost-performance trade-offs.", "description": "Figure 5 presents a comprehensive cost-performance analysis of various language models on seven medical datasets.  Each dataset is represented by a separate subplot, showing the trade-off between Pass@1 accuracy (the percentage of correctly answered questions) and the cost per sample (in USD, displayed on a logarithmic scale).  The plot visually distinguishes between different types of language models: 'thinking models' (those exhibiting complex reasoning capabilities) are depicted with distinct marker shapes compared to 'non-thinking models'.  Model source (open-source vs. closed-source) is represented by different colors (blue for open-source, red for closed-source). Marker size is proportional to the model's inference time.  Finally, the Pareto frontier is overlaid as a red dashed line, representing the optimal combinations of accuracy and cost.", "section": "4.3 Main Results"}]