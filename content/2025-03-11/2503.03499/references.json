{"references": [{"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-01-01", "reason": "This paper introduced the Transformer architecture, which serves as a baseline model that SSMs are compared to."}, {"fullname_first_author": "Edward J Hu", "paper_title": "LoRA: Low-Rank Adaptation of Large Language Models", "publication_date": "2021-01-01", "reason": "This paper presents LoRA, a parameter-efficient fine-tuning method that is used as a baseline for comparison in this study."}, {"fullname_first_author": "Albert Gu", "paper_title": "Hippo: Recurrent memory with optimal polynomial projections", "publication_date": "2020-01-01", "reason": "This paper introduces HiPPO, a key initialization method for the state matrix in SSMs, and it is essential for enabling long-range dependency modeling."}, {"fullname_first_author": "Xiang Lisa Li", "paper_title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation", "publication_date": "2021-01-01", "reason": "This paper introduces Prefix-Tuning, a prompt-based PEFT method that is compared against state-based methods in this study, especially in the context of SSMs."}, {"fullname_first_author": "Brian Lester", "paper_title": "The power of scale for parameter-efficient prompt tuning", "publication_date": "2021-01-01", "reason": "This paper introduces Prompt Tuning, another prompt-based PEFT method that is widely used in Transformers and serves as a baseline for comparison with the proposed state-based methods for SSMs."}]}