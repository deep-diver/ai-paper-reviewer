{"importance": "This paper introduces **state-based PEFT methods**, offering new research directions in parameter-efficient fine-tuning for SSMs. It addresses the limitations of existing prompt-based methods and opens avenues for **developing more efficient and effective adaptation strategies** for state space models in NLP and other domains.", "summary": "State-offset Tuning: A novel state-based PEFT method for SSMs, outperforming prompt-based approaches with fewer trainable parameters, enabling effective adaptation.", "takeaways": ["State-based PEFT methods are a superior alternative to prompt-based methods for SSMs.", "State-offset Tuning is a novel state-based PEFT method that directly adjusts state-related features for effective adaptation.", "State-offset Tuning achieves comparable performance to full fine-tuning with significantly fewer trainable parameters."], "tldr": "State Space Models (SSMs) offer an efficient alternative to Transformers, but existing Parameter-Efficient Fine-Tuning (PEFT) methods, particularly prompt-based approaches, are ineffective for SSMs. This is because prompt-based methods indirectly influence the hidden state using external virtual tokens, which limits their expressiveness. To address this issue, this paper proposes **state-based methods** which directly modify intrinsic state-related features within the SSM module, offering a more direct and expressive adaptation strategy. \n\nThe authors introduce **State-offset Tuning**, a novel state-based PEFT method that adds a constant, learnable state-offset to the hidden state at each timestep, ensuring a uniform impact. This method eliminates the time-varying coefficient seen in other approaches, leading to more consistent effects. Experiments across various datasets demonstrate that State-offset Tuning outperforms existing fine-tuning techniques and achieves comparable performance to full fine-tuning with significantly fewer trainable parameters. The results show that all state-based methods surpass prompt-based methods, supporting the idea that state-based methods are superior to prompt-based methods on SSMs.", "affiliation": "Seoul National University", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}, "podcast_path": "2503.03499/podcast.wav"}