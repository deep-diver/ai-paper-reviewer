[{"heading_title": "Text's Blind Faith", "details": {"summary": "The phenomenon of \"blind faith in text\" within Vision-Language Models (VLMs) is a critical area of investigation. **VLMs tend to prioritize textual information over visual cues**, even when text is misleading or incorrect. This is problematic, as it undermines the VLM's ability to ground responses in reality. **This reliance on text can lead to significant performance degradation, especially when textual data is corrupted.** Several factors influence this bias, including language model size, relevance of text, token order, and the interplay between visual and textual certainty. Addressing this bias is essential for building robust and reliable VLMs, particularly in real-world applications where data inconsistencies are common."}}, {"heading_title": "Modality Matters", "details": {"summary": "The exploration of modality preference in Vision-Language Models (VLMs) is crucial because VLMs integrate visual and textual information. When inconsistencies arise between these modalities, it is important to know which one the model will trust more, **the performance will be affected depending on the decision**. This preference influences how robust the VLMs are. If a VLM disproportionately trusts textual data, even when it's corrupted, the entire system's safety and reliability could be compromised. **Understanding and mitigating this text bias** requires a deep dive into factors influencing modality preference, such as instruction prompts, relevance, token order, uni-modal certainty and language model size. Addressing this bias is important for reliability and safety in real-world applications."}}, {"heading_title": "Text Bias Factors", "details": {"summary": "**VLMs exhibit a 'blind faith in text', often favoring textual data over visual cues, even when inconsistent.** This text bias is influenced by factors like **instruction prompts**, which have limited effectiveness in adjusting modality preference. **Language model size** plays a role; scaling up mitigates the bias, but effects saturate in larger models. **Text relevance** intensifies the preference for textual data. **Token order matters**, placing text before images exacerbates bias due to positional biases. Furthermore, the **interplay between visual and textual certainty** shapes modality preference. Mitigating this bias requires careful consideration of these factors in VLM design and training."}}, {"heading_title": "SFT mitigates Bias", "details": {"summary": "**Supervised Fine-Tuning (SFT)** is presented as a method to reduce the bias of VLMs. SFT adjusts model parameters using a dataset of corrected examples, guiding the model away from reliance on text and towards a more balanced integration of visual and textual information. The success of SFT hinges on the composition of the training data, which must include examples that challenge the model's pre-existing biases. Also SFT's effectiveness needs rigorous testing across diverse datasets and real-world scenarios. SFT can improve a VLM, as data and setup are crucial to improving the model with it."}}, {"heading_title": "VLM Data Imbalance", "details": {"summary": "The concept of VLM data imbalance highlights a critical challenge in training Vision-Language Models. **If a VLM is predominantly trained on textual data, it may develop a stronger reliance on text**, even when visual cues are available and more reliable. This imbalance can manifest as a 'blind faith in text,' where the model prioritizes textual information, even if it contradicts visual evidence. **This can lead to performance degradation in tasks requiring accurate multimodal integration**. Addressing this imbalance requires careful consideration of data composition during training, ensuring a more equitable representation of visual and multimodal data to foster robust cross-modal reasoning."}}]