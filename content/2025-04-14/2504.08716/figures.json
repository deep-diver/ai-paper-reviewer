[{"figure_path": "https://arxiv.org/html/2504.08716/x1.png", "caption": "Figure 1: Downstream Performance on QA throughout the pre-training stage. wsd are the models tested before the cooldown period.", "description": "This figure displays the F1 scores achieved on Question Answering (QA) downstream tasks for various models throughout their pre-training phase.  The x-axis represents the number of tokens processed during pre-training, while the y-axis shows the corresponding F1 score.  Multiple models are compared, including ModernBERT-CV2, ModernBERT-HQ, CamemBERTav2, and CamemBERTv2.  The lines labeled with '-wsd' represent the models' performance *before* the additional cooldown training period. This visualization helps to compare the training dynamics and sample efficiency of different transformer models.", "section": "4.3 Pre-training Dynamics and Sample Efficiency"}, {"figure_path": "https://arxiv.org/html/2504.08716/x2.png", "caption": "Figure 2: Downstream Performance on NER throughout the pre-training stage. wsd are the models tested before the cooldown period.", "description": "This figure illustrates the performance of different models on the NER (Named Entity Recognition) task throughout the pre-training phase.  It specifically tracks the F1 score, a common metric for evaluating NER performance, as the models are trained on an increasing amount of data.  The lines represent different models, including those tested both before and after a 'cooldown' period (indicated by 'wsd'). This allows for a comparison of the models' performance at different stages of training and helps to understand their relative efficiency and convergence speed. The x-axis represents the amount of data (in tokens) used for pre-training, while the y-axis shows the F1 score achieved.", "section": "4.3 Pre-training Dynamics and Sample Efficiency"}, {"figure_path": "https://arxiv.org/html/2504.08716/x3.png", "caption": "Figure 3: Instances of divergence during QA fine-tuning. Colored lines illustrate the maximum score at a given step.", "description": "Figure 3 illustrates the training stability of different transformer models during fine-tuning on the FQuAD question answering task.  The colored lines track the maximum F1 score achieved at each training step for each model. The figure highlights instances where specific models (particularly ModernBERT variants) failed to converge, indicating potential instability issues during the fine-tuning process.", "section": "4.5 Downstream Training Stability"}]