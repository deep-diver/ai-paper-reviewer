{"importance": "This paper introduces **GigaTok, a groundbreaking approach to scaling visual tokenizers** that overcomes a critical limitation in autoregressive image generation. It will help researchers **develop more effective image generation models** by addressing the trade-off between reconstruction and generation quality, enabling better understanding and future work.", "summary": "GigaTok: Scale visual tokenizers for better image generation!", "takeaways": ["Semantic regularization mitigates the reconstruction vs. generation dilemma.", "Asymmetric encoder-decoder scaling and entropy loss are key to scaling tokenizers to billions of parameters.", "GigaTok achieves state-of-the-art results in image reconstruction, generation, and representation learning."], "tldr": "In autoregressive (AR) image generation, visual tokenizers compress images into compact discrete latent tokens, enabling efficient training. Scaling visual tokenizers improves image reconstruction quality, but often degrades downstream generation quality. This challenge is not adequately addressed. Thus, there exists a reconstruction vs. generation dilemma, where scaling tokenizer improves reconstruction fidelity but degrades downstream generation quality.\n\nTo address this, GigaTok introduces **semantic regularization**, which aligns tokenizer features with pre-trained visual encoder features. This constraint prevents excessive latent space complexity during scaling, improving both reconstruction and generation. Building on it, GigaTok explores key practices for scaling tokenizers: 1D tokenizers, decoder scaling, and entropy loss. Scaling to **3 billion parameters**, GigaTok achieves state-of-the-art performance.", "affiliation": "Hong Kong University of Science and Technology", "categories": {"main_category": "Computer Vision", "sub_category": "Image Generation"}, "podcast_path": "2504.08736/podcast.wav"}