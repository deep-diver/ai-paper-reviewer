[{"figure_path": "https://arxiv.org/html/2504.08736/x1.png", "caption": "Figure 1: Reconstruction vs. generation dilemma: Naively scaling visual tokenizers achieves better reconstruction but degrades downstream autoregressive (AR) generation. In contrast, GigaTok achieves better performance for both reconstruction and generation as tokenizers scale up.", "description": "This figure illustrates the trade-off between reconstruction quality and downstream autoregressive (AR) generation performance when scaling visual tokenizers.  The left panel shows that simply increasing the size of the visual tokenizer improves reconstruction fidelity (lower rFID score), as measured by the quality of reconstructed images. However, the right panel demonstrates that this naive scaling negatively impacts the quality of images generated by the downstream AR model (higher gFID score).  This is known as the \"reconstruction vs. generation dilemma.\" In contrast, GigaTok, the proposed method in the paper, simultaneously improves both reconstruction and generation quality as the tokenizer size increases, showcasing the effectiveness of its approach.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2504.08736/x2.png", "caption": "Figure 2: \nThe 2.9B GigaTok achieves SOTA autoregressive image generation with a 1.4B AR model on ImageNet 256\u00d7\\times\u00d7256 resolution.", "description": "This figure showcases the state-of-the-art (SOTA) results achieved by GigaTok, a 2.9 billion parameter visual tokenizer, when used in conjunction with a 1.4 billion parameter autoregressive (AR) model.  The impressive image generation quality is demonstrated on the ImageNet dataset at 256x256 resolution. The image samples shown highlight the diversity and realism of the generated images. This figure visually emphasizes the significant improvement in autoregressive image generation resulting from GigaTok's substantial scale.", "section": "4. GigaTok"}, {"figure_path": "https://arxiv.org/html/2504.08736/x3.png", "caption": "Figure 3: Scaling trend for vanilla 1D tokenizers. As the model size increases, the reconstruction quality of vanilla tokenizers improves but the downstream AR Probing gFID consistently degrades. The increasing AR Probing validation loss indicates that scaling vanilla tokenizers results in a more complex latent space, making it difficult for AR models to learn effectively.", "description": "This figure demonstrates the challenges of simply increasing the size of visual tokenizers in autoregressive image generation.  While larger tokenizers improve the reconstruction quality (as measured by rFID), the downstream performance of the autoregressive generation model suffers. This is shown by the increasing gFID (lower is better), suggesting a more complex latent space that the generation model struggles to learn.  The increased validation loss from AR Probing further supports this finding, showing that the model has difficulty learning effective token distributions from the larger, more complex tokenizer.", "section": "3. Pilot Study"}, {"figure_path": "https://arxiv.org/html/2504.08736/x4.png", "caption": "Figure 4: GigaTok architecture and semantic regularization. Top: We use a hybrid CNN-Transformer design for our visual tokenizer. The transformer layers are implemented with ViT for 2D tokenizer and Q-Former for 1D tokenizer. Bottom: We use a frozen DINOv2\u00a0[43] image encoder\nfor semantic regularization.", "description": "This figure illustrates the architecture of GigaTok, a visual tokenizer, and its incorporation of semantic regularization.  The top half shows the hybrid CNN-Transformer structure of GigaTok.  The CNN layers initially process the image, followed by Transformer layers which are implemented using the Vision Transformer (ViT) architecture for 2D tokenizers and the Q-Former architecture for 1D tokenizers. The Transformer layers handle the encoding of the image into discrete latent tokens. The bottom half of the figure demonstrates how semantic regularization is applied.  A pre-trained DINOv2 image encoder (frozen weights) provides semantic features. These features are compared to the features learned by the GigaTok tokenizer's decoder, guiding the training to ensure that the latent representation generated by the tokenizer is semantically consistent and meaningful.", "section": "4. GigaTok"}, {"figure_path": "https://arxiv.org/html/2504.08736/x5.png", "caption": "Figure 5: Training curves for 2.9B XL-XXL tokenizers with and without entropy loss. A 2.9B tokenizer does not converge without entropy loss. The entropy loss encourages high codebook usage and stabilizes training loss.", "description": "This figure displays training curves for two large-scale visual tokenizers (XL and XXL) with 2.9 billion parameters each.  The training curves show the behavior of the perceptual loss and codebook usage. These curves are presented separately for experiments run with and without entropy loss. The results demonstrate that the 2.9B parameter tokenizer fails to converge without entropy loss.  However, the introduction of entropy loss effectively addresses this issue, leading to the convergence of both perceptual loss and codebook usage and stabilizes the training process.", "section": "4. GigaTok"}, {"figure_path": "https://arxiv.org/html/2504.08736/x6.png", "caption": "Figure 6: Correlation between AR Probing Performance and Larger AR models. For 3 tokenizers: S-S, S-L, and B-L, we present that as the tokenizer improves, the performance improvements of AR Probing correlate to the performance improvements of larger AR models. Therefore, the AR Probing can effectively indicate how the tokenizer affects downstream larger AR models with limited computational costs.", "description": "This figure demonstrates the strong correlation between the performance of a lightweight AR probing model and larger, more computationally expensive AR models when evaluating the impact of different visual tokenizers. Three tokenizer sizes (S-S, S-L, and B-L) were used to train both the probing model and the larger AR models.  The results show that as the quality of the tokenizer increases (e.g., improved reconstruction fidelity), the probing model shows corresponding improvements (lower gFID and higher linear probing accuracy). Crucially, these improvements in the probing model mirror the trends observed in the larger AR models, confirming that the probing model accurately reflects the impact of the tokenizer on downstream generation. This establishes AR probing as an efficient method for evaluating tokenizers, as it avoids the high computational cost of training and evaluating large-scale AR models for each tokenizer variant.", "section": "3. Pilot Study"}, {"figure_path": "https://arxiv.org/html/2504.08736/x7.png", "caption": "Figure 7: Scaling trends of tokenizers for reconstruction, downstream generation and representation quality with and without semantic regularization. By semantic regularization, GigaTok resolves the reconstruction vs. generation dilemma for tokenizer scaling in contrast to the vanilla version without semantic regularization. Moreover, GigaTok consistently improves the representation quality of downstream AR models by scaling up visual tokenizers. Note that in the last two figures, the red and blue curves correspond to different scales on the y-axis.", "description": "This figure displays a comparison of scaling trends for visual tokenizers with and without semantic regularization.  The graphs show how various metrics (rFID for reconstruction quality, gFID for downstream AR generation quality, and linear probing accuracy for representation quality) change as the number of parameters in the tokenizer increases.  The results demonstrate that naive scaling without semantic regularization leads to a trade-off between better reconstruction and worse generation (the \"reconstruction vs. generation dilemma\").  In contrast, GigaTok, which incorporates semantic regularization, consistently improves across all three metrics as the tokenizer scales up, showing a significant enhancement in both reconstruction and generation performance.", "section": "4. GigaTok"}, {"figure_path": "https://arxiv.org/html/2504.08736/x8.png", "caption": "Figure 8: Visualization\nof tokenizer features with and without semantic regularization. We compute PCA among the tokenizer features of a group of images of the same \u201cgolden retriever\u201d class\nand visualize the first 3 PCA components.\nWe observe that the latent space of vanilla tokenizers shows inconsistent features both within a single image or across multiple semantically similar images. In contrast, GigaTok encodes images with semantic consistency and thus reduces the latent space complexity for AR models.", "description": "This figure visualizes the effects of semantic regularization on the latent space learned by visual tokenizers.  It uses Principal Component Analysis (PCA) to reduce the dimensionality of features extracted from a set of images depicting 'golden retrievers'. The first three principal components are then plotted, revealing the structure of the latent space. The visualization demonstrates that vanilla tokenizers (without semantic regularization) produce a less structured latent space with inconsistent features both within individual images and across similar images, hindering downstream autoregressive generation. In contrast, GigaTok, with its semantic regularization, shows a much more consistent and structured latent space, resulting in improved downstream generation due to reduced latent space complexity.", "section": "Pilot Study"}, {"figure_path": "https://arxiv.org/html/2504.08736/x9.png", "caption": "Figure 9: \nScalability comparison for 1D and 2D tokenizers. Using the same training setting, 1D tokenizers shows better reconstruction\u00a0(rFID) and downstream representation quality\u00a0(AR Probing: Lin Acc.). For downstream generation\u00a0(gFID), 1D tokenizers present a steeper improving trend than 2D tokenizers.", "description": "This figure compares the scalability of 1D and 2D tokenizers in autoregressive image generation.  Using identical training parameters, the results show that 1D tokenizers consistently achieve superior reconstruction fidelity (measured by rFID), as well as better quality of learned representations in downstream AR models (measured by linear probing accuracy).  Furthermore, while both types of tokenizers improve downstream generation quality (gFID) as model size increases, the improvement is significantly steeper for the 1D tokenizers, indicating better scalability.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2504.08736/x10.png", "caption": "Figure 10: The architecture of GigaTok with Q-Former.", "description": "This figure details the architecture of GigaTok, specifically highlighting the use of the Q-Former module. GigaTok uses a hybrid CNN-Transformer architecture. The encoder consists of CNN blocks that progressively downsample the input image, followed by Transformer layers and a vector quantizer to produce discrete latent codes.  The decoder mirrors this process, starting with Transformer layers and then using CNN blocks to upsample the features for image reconstruction.  The Q-Former is a key component, enabling the use of 1D tokens, improving scalability.  The diagram visually represents the flow of information through the encoder and decoder stages, emphasizing the role of the Q-Former and the generation of discrete tokens.", "section": "4. GigaTok"}, {"figure_path": "https://arxiv.org/html/2504.08736/x11.png", "caption": "Figure 11: Initialization of 1D queries in Q-Former modules.", "description": "This figure illustrates how 1D queries are initialized in the Q-Former modules of GigaTok.  It depicts a multi-level average pooling strategy applied to 2D input features from the CNN encoder. At each level, the input features are divided into regions, and average pooling is performed on each region. The resulting pooled features are then flattened and concatenated from level 0 to the final level, creating the 1D query sequence for the Q-Former encoder.  During decoding, 2D queries are initialized from the 1D latent features. This process ensures a smooth transition between the 2D input features and the 1D latent representation used by the Q-Former.", "section": "4.1 Architecture"}, {"figure_path": "https://arxiv.org/html/2504.08736/x12.png", "caption": "Figure 12: \nTraining duration scaling trends of tokenizers for reconstruction, downstream generation and representation quality with and without semantic regularization. Note that in the last two figures, the red and blue curves correspond to different scales on the y-axis.", "description": "Figure 12 shows the effects of increasing the training duration of visual tokenizers on reconstruction quality (rFID and LPIPS), downstream generation quality (gFID), and representation quality (linear probing accuracy).  The experiment compares results with and without semantic regularization.  The plots reveal a trade-off:  increasing training duration initially improves downstream generation but eventually degrades it, particularly when semantic regularization isn't used.  The y-axes scales differ between the last two plots (gFID and linear probing accuracy) to better visualize the trends.  The figure highlights the importance of semantic regularization for stable and effective tokenizer scaling.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2504.08736/x13.png", "caption": "Figure 13: \nThe linear probing accuracy of tokenizer encoders does not necessarily reflect downstream model performance. As the training proceeds, the XL-XXL tokenizer encoder presents an overfitting trend measured by linear probing accuracy, but downstream model performances consistently improve.", "description": "This figure demonstrates that the linear probing accuracy of tokenizer encoders, while useful for evaluating the encoders themselves, is not a reliable predictor of downstream model performance.  The XL-XXL tokenizer encoder shows overfitting in terms of its linear probing accuracy during training. However, despite this overfitting, the downstream model performance (as measured by other metrics) consistently improves. This indicates that there is a decoupling between the encoder's internal representation learned through linear probing, and the features it produces that actually benefit the downstream tasks.", "section": "5.3 Asymmetric 1D Tokenizer is More Scalable"}]