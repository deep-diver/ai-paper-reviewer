{"references": [{"fullname_first_author": "Liu", "paper_title": "Rethinking machine unlearning for large language models.", "publication_date": "2025-02-01", "reason": "This paper is cited as a recent work in machine unlearning and is important because it frames machine unlearning in the context of LLMs and is referenced multiple times."}, {"fullname_first_author": "Shi", "paper_title": "MUSE: Machine unlearning six-way evaluation for language models.", "publication_date": "2024-07-01", "reason": "This paper introduces the MUSE benchmark, which is a key benchmark for evaluating machine unlearning methods and cited multiple times in the paper."}, {"fullname_first_author": "Li", "paper_title": "The wmdp benchmark: Measuring and reducing malicious use with unlearning.", "publication_date": "2024-01-01", "reason": "This paper introduces the WMDP dataset, which is used to evaluate the proposed method, and is a key benchmark for evaluating hazardous knowledge unlearning."}, {"fullname_first_author": "Karvonen", "paper_title": "Saebench: A comprehensive benchmark for sparse autoencoders in language model interpretability.", "publication_date": "2025-03-01", "reason": "This paper introduces SAEBench, a benchmark for sparse autoencoders, and is used for evaluation in the paper."}, {"fullname_first_author": "Farrell", "paper_title": "Applying sparse autoencoders to unlearn knowledge in language models.", "publication_date": "2024-01-01", "reason": "This paper describes an early SAE-based unlearning method, is a direct baseline for comparison, and provides a foundation for the proposed method."}]}