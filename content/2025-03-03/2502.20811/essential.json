{"importance": "This work significantly enhances human action understanding by introducing high-quality, detailed video caption data and evaluation resources, offering immediate improvements and opening new avenues for research in video understanding and generation. It provides the resources of HAICTrain and HAICBench.", "summary": "HAIC improves MLLMs' action understanding with high-quality video captions & new benchmark, boosting performance and generation.", "takeaways": ["A novel two-stage data annotation pipeline can generate high-quality video caption data for improving human action understanding.", "The curated datasets, HAICTrain and HAICBench, significantly enhance MLLMs' performance in human action understanding and text-to-video generation.", "Training with HAICTrain improves human understanding across various benchmarks and text-to-video generation results."], "tldr": "Multi-modal Large Language Models (MLLMs) show promise in video understanding, their grasp of human actions lags due to data scarcity. Existing datasets often provide coarse captions, failing to capture nuanced behaviors crucial for emotional analysis and relationship modeling, especially in multi-person scenarios. Challenges include accumulating videos with clear multi-person actions and defining caption formats that distinguish individuals and detail their interactions.\n\nTo solve the issue, the paper introduces a novel two-stage data generation pipeline. The method accumulates videos featuring clear human actions and annotates them using a standardized caption format that distinguishes individuals by attributes and details their actions/interactions chronologically. This approach curates HAICTrain (126K video-caption pairs) for training and HAICBench (500 annotated pairs, 1,400 QA pairs) for evaluation. **Experiments show HAICTrain enhances human understanding abilities and text-to-video generation.**", "affiliation": "Kuaishou Technology", "categories": {"main_category": "Multimodal Learning", "sub_category": "Vision-Language Models"}, "podcast_path": "2502.20811/podcast.wav"}