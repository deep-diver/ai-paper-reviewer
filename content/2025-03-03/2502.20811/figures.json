[{"figure_path": "https://arxiv.org/html/2502.20811/x1.png", "caption": "Figure 1: Our standardized caption format presents each individual\u2019s detailed attributes, body actions, and interactions in chronological order, making it easier to distinguish individuals and comprehend their behaviors.", "description": "This figure showcases the standardized caption format used to annotate videos in the HAIC dataset.  Each caption chronologically details the attributes (e.g., age, clothing, etc.) of each individual in the video clip, describes their body actions, and outlines their interactions with others.  This structured format significantly improves the clarity and facilitates accurate understanding of complex multi-person interactions compared to simpler captioning methods.", "section": "3 HAIC Data Pipeline"}, {"figure_path": "https://arxiv.org/html/2502.20811/x2.png", "caption": "Figure 2: Our data generation pipeline. (a) The video accumulation stage collects videos featuring clear human actions from the Internet. Based on this, (b) HAICTrain is curated through Gemini-1.5-Pro re-captioning, and\n(c) HAICBench is created by LLM-assisted human annotation.", "description": "This figure illustrates the two-stage data generation pipeline used to create the HAICTrain and HAICBench datasets.  Stage (a) shows the process of accumulating videos from the internet that contain clear human actions.  This involves filtering videos based on metadata (resolution, presence of action verbs), ensuring sufficient human presence and action. Stage (b) details the creation of HAICTrain, where the collected videos are re-captioned using the Gemini-1.5-Pro LLM to create a standardized caption format. Finally, stage (c) describes the construction of HAICBench.  This dataset is built using a combination of LLM-generated captions and QA pairs, which are subsequently reviewed and refined by human annotators. This results in a high-quality, manually-verified dataset ideal for evaluating multi-modal large language models.", "section": "3 HAIC Data Pipeline"}, {"figure_path": "https://arxiv.org/html/2502.20811/x3.png", "caption": "Figure 3: Statistics of HAICBench. Although videos are relatively short, the video captions are of high details including various action details and sequential actions.", "description": "Figure 3 presents a statistical overview of the HAICBench dataset, showcasing several key characteristics.  The first subplot (a) displays a histogram of the word counts in the captions, revealing the length and detail level of the descriptive text. The second subplot (b) is a histogram of the video durations within the dataset, demonstrating the generally short length of the videos.  Subplot (c) provides a word cloud visualization of the most frequently used words in the captions, highlighting the common themes and actions captured within the dataset.  Finally, subplot (d) illustrates the distribution of questions across five different categories (interaction, action details, action sequence, count, and attribute), offering insights into the diversity and scope of the information captured in the HAICBench annotations.", "section": "3 HAIC Data Pipeline"}, {"figure_path": "https://arxiv.org/html/2502.20811/x4.png", "caption": "Figure 4: A video caption example in HAICBench.", "description": "The figure shows a comparison of video captions generated by different models for the same video clip.  The first caption is a concise description of the video, suitable for general video understanding tasks. However, it lacks the level of detail necessary for fine-grained action understanding. The second caption, generated using the HAIC annotation pipeline, shows a significantly more detailed and structured description. This caption includes specific attributes of the individuals involved (e.g., age, clothing, accessories), precise descriptions of their actions, and an accurate chronological ordering of events. This demonstrates the HAIC caption format's ability to provide richer, more detailed annotations than existing methods, thereby improving the performance of downstream multimodal tasks.", "section": "3 HAIC Data Pipeline"}, {"figure_path": "https://arxiv.org/html/2502.20811/x6.png", "caption": "Figure 5: Prompt for Gemini-Pro Re-captioning.", "description": "This figure shows the detailed prompt used to instruct the Gemini-Pro language model to generate captions for videos in the HAICTrain dataset.  The prompt emphasizes the importance of accurately describing the number of people in the video, their attributes (gender, age, clothing, etc.), and the sequence of their actions. It stresses that the description should be based solely on what is visually apparent in the video and not on any external knowledge or assumptions.  The prompt provides two examples of correctly formatted captions to guide the model.", "section": "3 HAIC Data Pipeline"}, {"figure_path": "https://arxiv.org/html/2502.20811/x7.png", "caption": "Figure 6: Prompt for action interaction QA generation.", "description": "This figure shows the prompt used for generating question-answer pairs focusing on the interactions between individuals in a video.  The prompt instructs an AI assistant to analyze a video caption describing the number of people, their attributes, and their actions. The assistant should then generate a multiple-choice question and answer about the interaction between subjects. The JSON format for the output is specified, requiring a question, candidate answers, and the correct answer.", "section": "3 HAIC Data Pipeline"}, {"figure_path": "https://arxiv.org/html/2502.20811/x8.png", "caption": "Figure 7: Prompt for action details QA generation.", "description": "This figure details the prompt used for generating question-answer pairs focusing on the details of actions within videos.  The prompt instructs the LLM to create multiple-choice questions concerning precise actions, body movements, expressions, or postures within a video, emphasizing the use of specific body parts (e.g., left hand, right leg) and directional details (e.g., upward, downward, diagonally).  The prompt further specifies the response format as JSON, with an example provided for clarity.  It also requests the use of distinctive attributes to identify the subjects instead of generic labels like 'Subject 1'.", "section": "3.3 HAICBench"}, {"figure_path": "https://arxiv.org/html/2502.20811/x9.png", "caption": "Figure 8: Prompt for action sequence QA generation.", "description": "This figure shows the prompt used for generating question-answer pairs focusing on action sequences within videos.  The prompt instructs a large language model (LLM) to generate QA pairs from a video caption, where the question should focus on the order of actions. The question must include a phrase like \"What does...do immediately after (or just before)...\", and it should specify attributes of the person to ensure clear identification. The answer options are limited to 15 words or less, with a JSON format required for the output.  The prompt also includes examples for better understanding and proper formatting.", "section": "3.3 HAICBench"}, {"figure_path": "https://arxiv.org/html/2502.20811/x10.png", "caption": "Figure 9: Prompt for action count QA generation.", "description": "This figure displays the prompt used to instruct a large language model (LLM) for generating question-answer pairs related to the count of actions within a video.  The prompt instructs the LLM to format the output as JSON, including a question about the number of times an action occurs, multiple choice answers, and the correct answer. The prompt also specifies that the question should be phrased as \"How many...\", ensuring consistency in question formatting.", "section": "3.3 HAICBench"}, {"figure_path": "https://arxiv.org/html/2502.20811/x11.png", "caption": "Figure 10: Prompt for human attribute QA generation.", "description": "This figure shows the prompt used for generating question-answer pairs related to human attributes in the HAICBench dataset.  The prompt instructs an AI assistant to generate a multiple-choice question-answer pair focusing on the attributes of the human subjects in a video. The AI should use detailed attributes to describe the people in the video instead of using generic labels like \"Subject 1\". The format of the output should be JSON.", "section": "3.3 HAICBench"}, {"figure_path": "https://arxiv.org/html/2502.20811/x12.png", "caption": "Figure 11: Prompt caption evaluation setting.", "description": "This figure details the prompt used for evaluating the captioning capabilities of various Multi-modal Large Language Models (MLLMs). The evaluation involves using a given video caption to answer a question about that video.  The prompt provides a caption and asks for the color of the hat worn by a specific person shown in the video, providing four color options as choices.", "section": "4.2 Results on HAICBench"}, {"figure_path": "https://arxiv.org/html/2502.20811/x13.png", "caption": "Figure 12: Videos generated by captions from LLaVA-Video and LLaVA-Video-ActionPro of the first sample in MovieGenBench. The main subject in this case is one woman walking along the street. LLaVA-Video-ActionPro provides a more detailed appearance of the woman than LLaVA-Video.", "description": "This figure displays a comparison of video generation results using two different models: LLaVA-Video and LLaVA-Video-ActionPro.  Both models were given the same text caption as input, describing a woman walking down a street. The generated videos are shown, highlighting that LLaVA-Video-ActionPro produces a video with significantly more detail and visual fidelity regarding the woman's appearance and attire compared to LLaVA-Video, which produces a less detailed and less realistic result.", "section": "4.5 Effectiveness for Text-video Generation"}, {"figure_path": "https://arxiv.org/html/2502.20811/x14.png", "caption": "Figure 13: Videos generated by captions from LLaVA-Video and LLaVA-Video-ActionPro of the 17th sample in MovieGenBench. The main subject in this case is one blue animated character. LLaVA-Video incorrectly identifies the main subject.", "description": "This figure compares video generation results from two different models, LLaVA-Video and LLaVA-Video-ActionPro, using the same caption.  The caption describes a video clip from the MovieGenBench dataset featuring a single, blue animated character. The generated videos illustrate how LLaVA-Video-ActionPro more accurately captures the main subject of the video, while LLaVA-Video fails to correctly identify it, highlighting the improvement in subject identification achieved through the model's improved captioning.", "section": "4.5 Effectiveness for Text-video Generation"}]