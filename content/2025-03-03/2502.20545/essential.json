{"importance": "This paper pioneers the use of LLMs in solving complex mathematical problems like determining polynomial non-negativity. It highlights the potential of **AI to advance mathematical research** and tackle NP-hard problems, paving the way for future advancements. The meticulously curated SoS-1K dataset will serve as a valuable resource for future research.", "summary": "SoS1: O1 and R1-Like Reasoning LLMs are Sum-of-Square Solvers.", "takeaways": ["High-quality reasoning instructions significantly improve LLM accuracy in solving SoS problems, boosting performance up to 81%.", "Fine-tuning a 7B model (SoS-7B) on the SoS-1K dataset outperforms larger models like DeepSeek-V3 and GPT-40-mini with less computation time.", "Reasoning-focused LLMs generally outperform general-purpose LLMs in solving SoS problems."], "tldr": "**Large Language Models (LLMs) have shown impressive abilities, but struggle with rigorous mathematical reasoning**. Determining if a polynomial is non-negative, crucial for optimization, remains a challenge. This paper introduces SoS-1K, a dataset of 1,000 polynomials, with expert-designed instructions based on five increasingly difficult criteria. Evaluating state-of-the-art LLMs reveals poor performance without guidance, hovering near random guessing. LLMs need explicit help to perform this math task. \n\nThe study demonstrated that **providing high-quality reasoning instructions significantly boosts LLM accuracy**. A fine-tuned 7B model, SoS-7B, outperforms much larger models. Findings highlight LLMs' potential to push mathematical reasoning boundaries and tackle computationally hard (NP-hard) problems. Code is available at the project's github.", "affiliation": "Nanjing University of Aeronautics and Astronautics", "categories": {"main_category": "Machine Learning", "sub_category": "Deep Learning"}, "podcast_path": "2502.20545/podcast.wav"}