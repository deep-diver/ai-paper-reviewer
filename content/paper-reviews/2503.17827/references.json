{"references": [{"fullname_first_author": "Josh Achiam", "paper_title": "GPT-4 technical report", "publication_date": "2023-03-01", "reason": "This report details the capabilities of GPT-4, a state-of-the-art multimodal model, making it crucial for comparison and evaluation in the benchmark."}, {"fullname_first_author": "Peng Wang", "paper_title": "Qwen2-VL: Enhancing vision-language model's perception of the world at any resolution", "publication_date": "2024-09-12", "reason": "This paper introduces Qwen2-VL, a vision-language model frequently used to generate captions or complete QA tasks in this paper's approach."}, {"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-01-01", "reason": "This paper describes CLIP, a vision-language model that serves as the basis of data curation, and as a quality classifier in this benchmark."}, {"fullname_first_author": "Matt Deitke", "paper_title": "Objaverse-XL: A universe of 10M+ 3D objects", "publication_date": "2024-01-01", "reason": "This paper details Objaverse-XL, the dataset used to source the 4D objects used as data in 4D-Bench dataset."}, {"fullname_first_author": "Haotian Liu", "paper_title": "Visual instruction tuning", "publication_date": "2024-01-01", "reason": "This paper describes a method for visual instruction tuning, a crucial step in creating multimodal models leveraged as a comparison benchmark in the paper."}]}