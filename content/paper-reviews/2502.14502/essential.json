{"importance": "This paper is important for researchers as it highlights the **trade-offs involved in updating LLMs** with new knowledge using LoRA. Understanding these dynamics is crucial for developing **effective and safe fine-tuning strategies**. The research also uncovers the risk of reasoning disruption and suggests directions for mitigating these issues, paving the way for further research.", "summary": "Packing new knowledge into LoRA adapters can harm LLMs! A delicate balance is needed to prevent performance decline.", "takeaways": ["Mixing known and new facts during LoRA fine-tuning yields better results but can degrade performance on external QA benchmarks.", "Biasing training data towards certain entities leads to answer regression.", "Fine-tuning can make models overconfident and less likely to admit uncertainty."], "tldr": "Large Language Models (LLMs) rely on pre-training knowledge, but Low-Rank Adaptation (LoRA) is used for updates. Integrating new facts can compromise previously learned knowledge. Existing techniques like Retrieval-Augmented Generation (RAG) and few-shot learning have limitations, motivating a revisit to fine-tuning for knowledge integration. Fine-tuning LLMs is computationally expensive, leading to Parameter-Efficient Fine-Tuning (PEFT) techniques like LoRA. However, modified LLMs can suffer from catastrophic forgetting or loss of associations. Increased new data can degrade pre-existing knowledge, prompting investigation into how much knowledge can be added without harm.\n\nThis study investigates additional knowledge integration into LLMs via LoRA while preserving capabilities. Experiments fine-tuned Llama-3.1-8B-instruct with varying new knowledge amounts, tracking degradation using metrics and benchmarks(MMLU, TruthfulQA). The best results came from mixing known and new facts, but performance on external question-answering declined. Training data bias led to over-represented answers, while models became more confident and refused answers less often. The findings underscore LoRA-based LLM updates' pitfalls and the importance of balanced training data.", "affiliation": "AIRI", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}, "podcast_path": "2502.14502/podcast.wav"}