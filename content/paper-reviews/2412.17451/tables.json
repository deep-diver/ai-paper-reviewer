[{"content": "| Method | \\mathcal{M} | \\mathcal{O} | Iteration Interval (#) | MathV360K | MathVista |\n|---|---|---|---|---|---| \n| MiniCPM-V-2.5 | - | - | - | 13.6 | 52.4 |\n| +warmup | - | - | - | 38.8 | 52.6 |\n| SFT | - | - | - | 44.3 | 54.8 |\n| Iterative RFT | \\pi_{\\theta}^{t} | \\times | 100%(180K) | 42.3 | 55.7 |\n| Rest<sup>EM</sup> | \\pi_{\\theta}^{0} | \\times | 100%(180K) | 42.3 | 55.1 |\n| Continous Self-Evolving | \\pi_{\\theta}^{t} | \\checkmark | 100%(180K) | 42.2 | 56.7 |\n|  |  |  | 50%(90K) | **43.1** | 56.2 |\n|  |  |  | 25%(45K) | **43.1** | **57.2** |\n|  |  |  | 12.5%(22K) | 42.3 | 56.1 |\n|  |  |  | 6.25%(11K) | 41.0 | 56.8 |", "caption": "Table 1: \nAccuracy results (%) of self-evolving training using various training methods and iteration intervals.\nIteration Interval (#) stands for the ratio of data we traverse in one iteration, and we also record the number of corresponding queries.\n\u2133\u2133\\mathcal{M}caligraphic_M represents the policy model from which training is initialized in each iteration. \ud835\udcaa\ud835\udcaa\\mathcal{O}caligraphic_O denotes whether the optimization process is continuous, i.e., the optimizer states and lr scheduler are inherited from the last checkpoint. Please refer to Table 4 to check the full results on all sub-tasks of MathVista.", "description": "Table 1 presents a detailed comparison of different self-evolving training methods and their impact on model accuracy. It explores the effects of varying the training method (continuous vs. iterative), the initialization of the policy model at each iteration, and the size of the data processed in each iteration (iteration interval).  The table shows accuracy results on the MathV360K and MathVista datasets.  For a complete breakdown of performance across all MathVista subtasks, please refer to Table 4 in the paper.", "section": "3 DIVING INTO SELF-EVOLVING DESIGN COMPONENTS"}, {"content": "| Method | \nmathcal{H} | PRM | MathV360K | MathVista |\n|---|---|---|---|---|\n| Continuous Self-Evolving | - | \u00d7 | 43.1 | 57.2 |\n| + Random Selection | Random-2 | \u00d7 | 41.0 | 55.5 |\n| +PRM-based Selection |  | \u2713 |  |  |\n| &gt;\u03b1 | 43.8 | 57.5 |\n| Top-1 | 43.0 | 59.0 |\n| Top-2 | **45.3** | **59.2** |\n| Top-4 | 44.0 | 58.4 |", "caption": "Table 2: The results of self-evolving training with PRM and different strategies to leverage reward scores. \u210b\u210b\\mathcal{H}caligraphic_H stands for the method to further pick out high-quality responses from the correct rollouts: (1) Top\u2212kTopk\\operatorname{Top-k}roman_Top - roman_k is we select K correct responses with highest rewards, and (2) >\u03b1absent\ud835\udefc>\\alpha> italic_\u03b1 is we pick out the correct responses with reward scores larger than \u03b1\ud835\udefc\\alphaitalic_\u03b1.\nPlease refer to Table 4 to check the full results on all sub-tasks of MathVista.", "description": "Table 2 presents the results of experiments using different methods to leverage reward scores within a process reward model (PRM) for self-evolving training.  The table compares the performance of self-evolving training with PRM, using two different strategies to select high-quality responses from the correct rollouts. The first strategy selects the top k responses with the highest reward scores (Top-k), while the second strategy selects all responses with reward scores above a certain threshold (>\n\u03b1).  The table shows the performance metrics (MathV360K and MathVista) for each method and helps to determine the best strategy for utilizing the PRM in self-evolving training.  More detailed results broken down by subtask are available in Table 4.", "section": "3.3 Reward Models"}, {"content": "| Oracle | PRM |  T<sub>mixin</sub> | MathV360K | MathVista |\n|---|---|---|---|---| \n| - | \u00d7 | - | 43.1 | 57.2 |\n| - | \u2713 | - | 45.3 | 59.2 |\n| \u2713 | \u00d7 | 0% | 42.5 | 58.2 |\n| \u2713 | \u2713 | 0% | 42.9 | 59.1 |\n| \u00d7 | \u2713 | 0% | 43.3 | 58.2 |\n| \u00d7 | \u2713 | 25% | 42.4 | 57.6 |\n| \u00d7 | \u2713 | 50% | 42.9 | 58.2 |\n| \u00d7 | \u2713 | 75% | 45.0 | 58.8 |", "caption": "Table 3: Results of involving unlabeled data. Tmixinsubscript\ud835\udc47mixinT_{\\texttt{mixin}}italic_T start_POSTSUBSCRIPT mixin end_POSTSUBSCRIPT denotes when to mixin the unlabeled data. The use of PRM follows \u00a73.3, except we first get a pesudo \u201cground truth\u201d through weighted voting on unlabeled prompts.", "description": "This table presents the results of experiments investigating the impact of incorporating unlabeled data into the self-evolving training process.  It explores different strategies for mixing labeled and unlabeled data at various stages of training. The 'Tmixin' column indicates the percentage of training completed before unlabeled data is introduced.  The use of a Process Reward Model (PRM) is consistent with the method described in section 3.3 of the paper, but a 'pseudo ground truth' is obtained using a weighted voting scheme for the unlabeled prompts.  The table shows the performance in terms of accuracy on MathV360K and MathVista datasets for several configurations, enabling analysis of the effects of different timing for unlabeled data introduction, with and without the PRM.", "section": "3.4 Prompt Variation"}, {"content": "|       | MathVista | M3CoT | MMStar-R | MMBench-R | AI2D | Average |\n| :---- | :-------: | :-------: | :-------: | :-------: | :-------: | :-------: |\n| MiniCPM-V-2.5 | 52.4 | 41.2 | 44.6 | 72.6 | 64.4 | 55.0 |\n| + warmup | 52.6 | 47.8 | 45.1 | 76.9 | 65.9 | 57.7 |\n| M-STaR | **59.5**<sub>\u2191 6.9</sub> | **48.7**<sub>\u2191 0.9</sub> | **50.7**<sub>\u2191 5.6</sub> | **79.9**<sub>\u2191 3</sub> | **69.1**<sub>\u2191 3.2</sub> | **61.6**<sub>\u2191 3.9</sub> |\n| Phi-3.5-vision | 46.5 | 39.4 | 42.5 | 56.8 | 47.5 | 46.5 |\n| + warmup | 49.3 | 46.5 | 44.2 | 70.9 | 65.5 | 55.3 |\n| M-STaR | **54.5**<sub>\u2191 5.2</sub> | **51.3**<sub>\u2191 4.8</sub> | **48.8**<sub>\u2191 4.6</sub> | **73.6**<sub>\u2191 2.7</sub> | **67.9**<sub>\u2191 2.4</sub> | **59.2**<sub>\u2191 3.9</sub> |\n| InternVL2-2B | 46.4 | 16.7 | 20.0 | 14.2 | 33.5 | 26.2 |\n| + warmup | 47.6 | 45.6 | 41.8 | **68.8** | **60.0** | 52.8 |\n| M-STaR | **50.3**<sub>\u2191 2.7</sub> | **47.1**<sub>\u2191 1.5</sub> | **42.0**<sub>\u2191 0.2</sub> | 67.3<sub>\u2193 1.5</sub> | 59.7<sub>\u2193 0.3</sub> | **53.3**<sub>\u2191 0.5</sub> |", "caption": "Table 4: Full analysis of MathVista. Task types: FQA: figure question answering, GPS: geometry problem solving, MWP: math word problem, TQA: textbook question answering, VQA: visual question answering. We highlight the relative improvement of M-STaR\u00a0over the pre-evolved model, i.e., the \u201c+warmup\u201d row.", "description": "Table 4 presents a comprehensive breakdown of the MathVista benchmark results, categorized by task type: Figure Question Answering (FQA), Geometry Problem Solving (GPS), Math Word Problem (MWP), Textbook Question Answering (TQA), and Visual Question Answering (VQA).  The table compares the performance of several models, including a baseline model (+warmup) which has undergone a warm-up phase, various self-evolving training methods, and finally the proposed M-STaR model.  The key focus is on illustrating the significant performance gains achieved by M-STaR relative to the pre-evolved model (+warmup) across all task types within MathVista, demonstrating its robustness and effectiveness.", "section": "4.2 M-STAR- FINAL RECIPE WITH OPTIMAL DESIGN CHOICES & ADAPTIVE EXPLORATIONS"}]