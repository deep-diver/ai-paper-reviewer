[{"figure_path": "https://arxiv.org/html/2412.17451/x1.png", "caption": "Figure 1: Overview of our self-evolving training framework for multimodal reasoning. We investigate the three essential design components of it, namely Training method (\ud835\udcaf\ud835\udcaf\\mathcal{T}caligraphic_T), Reward model (\u211b\u211b\\mathcal{R}caligraphic_R), and Prompt variation (\ud835\udcab\ud835\udcab\\mathcal{P}caligraphic_P). Orthogonal to the static factors, the Dynamics of self-evoloution is also monitered, and provides control signals to the training process.", "description": "This figure illustrates the self-evolving training framework for multimodal reasoning.  It highlights three key factors: the training method (how the model is updated), the reward model (how the model's performance is evaluated), and prompt variation (how the input prompts are modified).  The framework also monitors the dynamics of self-evolution, using the observed dynamics to provide feedback and adjust the training process.  This dynamic adjustment helps optimize the model's reasoning abilities.", "section": "1 INTRODUCTION"}, {"figure_path": "https://arxiv.org/html/2412.17451/x2.png", "caption": "(a)", "description": "This figure shows the trend of Pass@K accuracy across different temperatures during the self-evolving training process.  Pass@K represents the percentage of samples where at least one correct response is generated when sampling K candidates.  The figure illustrates how the exploration ability of the model (Pass@K) changes at different temperatures (0.5, 0.7, 1.0, 1.2, 1.5, 1.7, 2.0) throughout the training process.  The x-axis represents the number of training steps, and the y-axis represents the Pass@K accuracy. Different colored lines correspond to the different temperatures, showing how exploration varies at each temperature over the course of training.", "section": "4.1 Monitoring the Training Dynamics"}, {"figure_path": "https://arxiv.org/html/2412.17451/x3.png", "caption": "(b)", "description": "This histogram shows the distribution of the average number of reasoning steps in correct responses re-ranked by the reward model (PRM).  The x-axis represents the number of steps, and the y-axis represents the frequency or count of responses with that many steps. The distribution is compared for the top 2 responses selected by the PRM and for the rest of the correct responses, highlighting the difference in the complexity of reasoning between the PRM's top choices and other correct responses.  The PRM tends to select responses with a fewer number of steps, implying that the model is prioritizing more concise reasoning paths.", "section": "3.3 Reward Models"}, {"figure_path": "https://arxiv.org/html/2412.17451/x4.png", "caption": "(c)", "description": "The figure shows the distribution of the relativity scores for the top 2 responses (selected by the PRM) and the rest of the correct responses. The relativity score, annotated by GPT-4, measures how directly relevant a response is to the given question.  The x-axis represents the relativity score (ranging from 0 to 10), and the y-axis shows the frequency/proportion of responses with that score.  The figure illustrates that responses selected by the PRM tend to have higher relativity scores, meaning they are more directly related to the question and contain fewer irrelevant steps, demonstrating the effectiveness of the PRM in selecting high-quality responses.", "section": "3.3 Reward Models"}, {"figure_path": "https://arxiv.org/html/2412.17451/x5.png", "caption": "Figure 2: (a): Accuracy on the val. set of greedy decoding and three selection strategy across different numbers of rollouts; (b)/(c): Distribution of average # of steps and average relativity score annotated by GPT4-o of Top 2 and the rest responses re-ranked by rewards, we only calculate on correct ones.", "description": "This figure analyzes the impact of using a process reward model (PRM) on selecting high-quality responses in a self-evolving training framework.  Panel (a) shows how the validation accuracy of greedy decoding changes with different numbers of response rollouts (K) and three different selection strategies (Top-K, Random Selection, and PRM-based selection).  Panels (b) and (c) provide further insight into the characteristics of the top-2 responses selected by the PRM compared to the rest of the correct responses. Panel (b) displays the distribution of the average number of reasoning steps, while panel (c) shows the distribution of the average relativity score (as annotated by GPT-4) for these responses.  The relativity score indicates how directly related each response is to the original question. Only correct responses are included in the analysis of panels (b) and (c).", "section": "3.3 REWARD MODELS"}, {"figure_path": "https://arxiv.org/html/2412.17451/x6.png", "caption": "Figure 3: The opposite trend of Greedy Decoding Accuracy and Pass@K.", "description": "This figure shows two line graphs plotting the validation accuracy over training steps. The blue line represents the greedy decoding accuracy, showing a consistent increase in performance as training progresses. The orange line shows the Pass@K accuracy, which displays an opposite trend.  Pass@K measures the model's ability to find at least one correct answer among K samples. The decreasing trend indicates a loss of exploration capability during training, which could limit the model's potential for improvement and lead to performance saturation. This contrast highlights the trade-off between exploitation (greedy decoding) and exploration (Pass@K) during self-evolving training, where initially high exploration is gradually lost in favor of increased exploitation as training progresses.", "section": "4.1 Monitoring the Training Dynamics"}, {"figure_path": "https://arxiv.org/html/2412.17451/x7.png", "caption": "(a)", "description": "This figure shows the trend of Pass@K metric during the self-evolving training process with different temperatures. Pass@K represents the percentage of samples for which the model generates at least one correct response when sampling K candidates.  The x-axis represents the number of training steps, and the y-axis shows the Pass@K accuracy. Different colored lines represent different temperatures used during the generation process. The figure reveals the impact of temperature on the model's exploration ability and how it changes over the training process.  Higher temperatures generally maintain better exploration abilities during later training stages, as indicated by slower decreases in Pass@K compared to lower temperatures.", "section": "4.1 Monitoring the Training Dynamics"}, {"figure_path": "https://arxiv.org/html/2412.17451/x8.png", "caption": "(b)", "description": "This histogram shows the distribution of the number of reasoning steps in correct responses, categorized into two groups: those re-ranked to the top 2 by the process reward model (PRM) and the rest. The PRM's ability to select responses with fewer steps, indicating a more focused and efficient reasoning process, is highlighted.", "section": "3.3 REWARD MODELS"}, {"figure_path": "https://arxiv.org/html/2412.17451/x9.png", "caption": "(c)", "description": "The figure shows the distribution of the relativity score of the top 2 responses re-ranked by rewards and other correct responses annotated by GPT-4.  The relativity score measures how directly related a response is to the question. This analysis helps to understand why the process reward model (PRM) improves the quality of the responses, even though the PRM itself isn't a strong verifier of response correctness.", "section": "3.3 REWARD MODELS"}, {"figure_path": "https://arxiv.org/html/2412.17451/x10.png", "caption": "Figure 4: (a): Pass@K decreases for all different temperatures; (b): The gap between Pass@K and Greedy Decoding; (c): The Reward-Pass@2 saturates quickly. All metrics, including the greedy decoding accuracy, are calculated on validation set.", "description": "Figure 4 presents a detailed analysis of the training dynamics of a multimodal self-evolving model.  It displays three key metrics over the course of training: (a) Pass@K, which measures the model's ability to generate at least one correct answer among K samples at various temperatures, demonstrating a decline in exploration capability as training progresses; (b) the difference between Pass@K and Greedy decoding accuracy, highlighting the diminishing exploration-exploitation trade-off; and (c) Reward-Pass@2, showcasing the rapid saturation of the reward model's ability to identify high-quality responses.  All metrics are calculated on a held-out validation set to ensure unbiased evaluation of the model's performance.", "section": "4.1 Monitoring the Training Dynamics"}, {"figure_path": "https://arxiv.org/html/2412.17451/x11.png", "caption": "Figure 5: Comparing the smoothed Pass@K and Reward-Pass@2 curves with the optimal static training progress, which fixs T=1.0\ud835\udc471.0T=1.0italic_T = 1.0.", "description": "Figure 5 presents a comparison of the performance of two training strategies: one using a dynamic temperature adjustment, and another using a fixed temperature (T=1.0). The figure displays the smoothed trends of two key metrics: Pass@K (measuring the model's exploration ability) and Reward-Pass@2 (measuring the exploitation efficacy of the reward model). The dynamic strategy aims to balance exploration and exploitation by adapting the temperature based on performance, while the static approach maintains a constant temperature throughout the training process. By comparing the curves of these metrics under both strategies, the figure illustrates the effects of adaptive temperature tuning on exploration and exploitation in self-evolving training.", "section": "4.2 M-STAR: FINAL RECIPE WITH OPTIMAL DESIGN CHOICES & ADAPTIVE EXPLORATIONS"}]