[{"figure_path": "https://arxiv.org/html/2502.10852/x1.png", "caption": "Figure 1: The relationship between population size and dataset size in OSCAR (y-axis, in MB) for various high-, middle-, and low-resource languages.", "description": "This figure visually represents the disparity in the amount of available data for various languages within the OSCAR corpus.  The x-axis displays the population size of speakers for each language (in millions), while the y-axis shows the corresponding dataset size in megabytes (MB).  Languages are categorized into high-resource, mid-resource, and low-resource groups, illustrating how the amount of available data often does not reflect the number of speakers.  For instance, several languages with millions of speakers have limited or no data available in OSCAR, highlighting the data scarcity issue for low-resource languages.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2502.10852/x2.png", "caption": "Figure 2: An overview of the shared weight framework for efficiently adapting multilingual encoders to text generation in low-resource languages.", "description": "This figure illustrates the architecture of the proposed Shared Weights Framework for adapting multilingual encoders to text generation tasks, particularly focusing on low-resource languages.  It shows how a pre-trained multilingual encoder (like XLM-R) is combined with a decoder.  A key feature is the weight sharing between encoder and decoder layers, allowing the decoder to leverage the knowledge learned by the encoder during its initial training. This is designed to improve efficiency and effectiveness in low-resource language settings. The framework is shown to process a large multilingual corpus and a smaller Chinese minority language corpus before producing results on downstream tasks. ", "section": "3 Method"}, {"figure_path": "https://arxiv.org/html/2502.10852/x3.png", "caption": "Figure 3: The weight initialization schemes for the CustomDecoderLayer. The colored arrows indicate the initialization of weights between the different components.", "description": "This figure illustrates how weights are initialized in the CustomDecoderLayer of the XLM-SWCM model.  The CustomDecoderLayer is a modified transformer decoder layer that incorporates pre-trained weights from the encoder to improve efficiency. The diagram shows the flow of weight initialization from the encoder's self-attention and feed-forward network blocks to the corresponding components in the decoder.  Colored arrows visually represent this weight transfer.  The weights are strategically transferred to leverage the encoder's learned knowledge effectively, while the decoder retains its capacity to learn generation-specific features via its own randomly initialized layers.", "section": "3.1.2 Model Architecture"}, {"figure_path": "https://arxiv.org/html/2502.10852/x4.png", "caption": "Figure 4: ROUGE-L scores on Tibetan summarization for different X-values (insertion frequency of normal layers). The three lines correspond to different dataset sizes.", "description": "This figure shows the impact of varying the insertion frequency (X-value) of normal decoder layers on the ROUGE-L scores for Tibetan text summarization.  The results are shown for three different training dataset sizes (10K, 20K, and 50K samples), illustrating how the optimal X-value changes with the amount of available training data.  In essence, the figure explores the trade-off between model capacity (larger X values mean smaller decoders) and the risk of overfitting, demonstrating that the best X-value depends on data availability.", "section": "5 Ablation Studies"}]