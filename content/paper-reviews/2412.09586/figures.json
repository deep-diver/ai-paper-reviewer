[{"figure_path": "https://arxiv.org/html/2412.09586/x1.png", "caption": "Figure 1: Prior approaches for gaze target estimation carefully fuse features from a separate head encoder, scene encoder, and auxiliary models for multimodal cues like depth and pose. We propose Gaze-LLE, a novel, streamlined approach that uses a single feature representation from a frozen image encoder and injects a person-specific positional prompt to decode gaze targets.", "description": "Traditional gaze estimation methods use multiple encoders (head, scene, and auxiliary encoders for depth and pose) to combine features and predict gaze targets.  This figure contrasts these complex, multi-branch approaches with the proposed Gaze-LLE method.  Gaze-LLE simplifies the process by using a single, frozen (pre-trained and not further updated during training) image encoder. It extracts a scene-wide feature representation and adds a person-specific positional prompt to directly decode the gaze target, eliminating the need for feature fusion from multiple encoders.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2412.09586/x2.png", "caption": "Figure 2: We introduce Gaze-LLE, a new framework for gaze estimation that learns a small gaze decoder on top of a frozen DINOv2 backbone. Using this backbone, we first extract scene tokens from an RGB image and project them to dmodelsubscript\ud835\udc51modeld_{\\text{model}}italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT with a linear layer. We then perform head prompting by adding a learned head position embedding pheadsubscript\ud835\udc5dheadp_{\\text{head}}italic_p start_POSTSUBSCRIPT head end_POSTSUBSCRIPT to the scene tokens at a given person\u2019s head location. Next, we update the scene tokens and an optional learnable auxiliary in/out prediction task token tin/outsubscript\ud835\udc61in/outt_{\\text{in/out}}italic_t start_POSTSUBSCRIPT in/out end_POSTSUBSCRIPT with 3 transformer layers. Finally, we upsample and decode the scene tokens into a heatmap and use the in/out task token to predict if the gaze target is in or out of frame.", "description": "Gaze-LLE is a novel gaze estimation framework. It uses a pre-trained DINOv2 model (frozen weights) as its backbone to extract visual features from an input RGB image.  These scene features are linearly projected to a lower dimension.  A learned head position embedding is then added to the scene features, specifically at the location of the person's head (obtained from a bounding box). This combined representation is then processed by three transformer layers.  An optional auxiliary task token is included in the input if the task requires in/out-of-frame gaze target prediction. Finally, a lightweight decoder module upsamples the processed features and predicts a heatmap representing the gaze target location, along with the in/out classification if applicable.", "section": "3. Gaze-LLE"}, {"figure_path": "https://arxiv.org/html/2412.09586/x3.png", "caption": "Figure 3: Qualitative results of our GazeFollow-trained ViT-B model on GazeFollow and applied without finetuning to VideoAttentionTarget, ChildPlay, and GOO-Real. We show ground truth on the left and the predicted heatmap & maximal point on the right.", "description": "This figure shows a qualitative comparison of the Gaze-LLE model's performance on four different datasets.  The GazeFollow-trained ViT-B model is applied to GazeFollow, VideoAttentionTarget, ChildPlay, and GOO-Real datasets without any fine-tuning. For each dataset, several examples are provided. Each example shows the ground truth gaze location on the left and the model's predicted gaze heatmap (with the maximum probability point highlighted) on the right. This visualization helps to assess the model's ability to generalize to unseen datasets and its qualitative performance in different scenarios.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.09586/x4.png", "caption": "Figure 4: Training convergence: our method achieves strong results in fewer GPU hours than prior approaches.", "description": "This figure shows the training convergence curves for Gaze-LLE and several state-of-the-art methods on the GazeFollow dataset.  The x-axis represents training time in GPU hours, and the y-axis represents the Area Under the Curve (AUC) metric, a common evaluation metric for gaze estimation.  The plot demonstrates that Gaze-LLE achieves comparable or better performance than other methods in significantly less training time, highlighting its efficiency.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.09586/extracted/6063027/figs/convergence.png", "caption": "Figure 5: Without head prompting, our model succeeds on single-person cases, but cannot effectively condition gaze target estimation on the correct person in multi-person scenarios.", "description": "This figure demonstrates the importance of head prompting in Gaze-LLE, especially when dealing with multiple people in a scene.  The leftmost column shows ground truth gaze targets.  The center column shows the model's gaze predictions without head prompting.  In single-person scenes, the model generally predicts the gaze target correctly. However, when multiple people are present (as in the other two columns), the model struggles to accurately determine which person's gaze it should estimate, resulting in inaccurate predictions. This highlights the crucial role of head prompting in disambiguating gaze targets in multi-person contexts.", "section": "3.2 Key Design Decisions for Foundation Models"}, {"figure_path": "https://arxiv.org/html/2412.09586/extracted/6063027/figs/prompt_types.png", "caption": "Figure 6: We show the output gaze instances (predicted head bounding box & gaze heatmap) from Tonini et al.\u2019s model [63] for 3 examples. We identify the instances selected by Tonini et al.\u2019s matching cost (which uses the ground truth gaze) and our altered matching cost (which excludes ground truth gaze and instead performs matching based on bounding box overlap). Tonini et al.\u2019s matching algorithm selects the instance with the closest gaze prediction to the ground truth, but the bounding box prediction does not always correspond to the correct person (Rows 1-2). Additionally, we observe overdetection, where the algorithm predicts multiple instances for the same person with different gaze heatmaps (Row 3). Without the use of ground truth gaze information, the model cannot determine which of these instances is best.", "description": "This figure compares the results of gaze estimation using two different matching algorithms: Tonini et al.'s original algorithm, which incorporates ground truth gaze information, and a modified algorithm that excludes ground truth gaze information and relies only on bounding box overlap for matching.  The figure uses three examples to illustrate the differences. The first two rows highlight cases where Tonini et al.'s algorithm incorrectly associates a gaze prediction with the wrong person due to the inclusion of ground truth gaze information in the matching process. The third row shows an example of overdetection where multiple gaze predictions are made for the same person, and the algorithm selects one based on heatmap similarity. The comparison demonstrates that the absence of ground truth gaze in the matching process makes it more difficult to select the best gaze prediction when multiple predictions are present, highlighting a limitation of the detection-based approach.", "section": "8. Comparison to Detection Methods"}, {"figure_path": "https://arxiv.org/html/2412.09586/x5.png", "caption": "(a) Runtime vs. Performance", "description": "The figure shows a comparison of the inference speed and accuracy of Gaze-LLE against other state-of-the-art gaze estimation methods. It highlights that Gaze-LLE achieves significantly faster inference speed while maintaining high accuracy compared to methods using multiple branches and auxiliary models. The plot demonstrates a clear tradeoff between runtime and performance. The y-axis represents the Area Under the Curve (AUC) metric, a measure of accuracy, while the x-axis shows the inference time in milliseconds (ms) on a logarithmic scale.", "section": "9. Runtime Analysis"}, {"figure_path": "https://arxiv.org/html/2412.09586/x6.png", "caption": "(b) Runtime scaling for multi-person inference", "description": "This figure shows how the model's runtime scales when processing multiple people in a single image.  It compares the inference time (in milliseconds) for a single person versus multiple people (1 to 10 people).  Two different model configurations are tested: the default model and a modified version that employs cross-attention and dot-product decoding. This allows for an assessment of the computational efficiency of each method with increasing scene complexity.", "section": "9. Runtime Analysis"}, {"figure_path": "https://arxiv.org/html/2412.09586/extracted/6063027/figs_supp/runtime.png", "caption": "Figure 7: Runtime analysis of our approach: we show the tradeoff of inference time vs. performance (7(a)), and analyze how different variants of our approach paired with a head detector scale for multi-person prediction, compared to detection methods (7(b)). All experiments are performed on a single NVIDIA RTX 4090 GPU.", "description": "Figure 7 presents a runtime analysis comparing Gaze-LLE with other methods.  Part (a) illustrates the trade-off between inference speed and performance.  It shows that while Gaze-LLE is faster than some state-of-the-art methods, others achieve higher performance at the cost of speed. Part (b) explores how Gaze-LLE scales with the number of people detected in the image, which shows the model's runtime increasing only slightly when using a single RTX 4090 GPU, even when estimating gaze for multiple individuals. This contrasts with methods using a detection-based approach.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.09586/extracted/6063027/figs_supp/scale.png", "caption": "Figure 8: Architecture details for our reimplementation of Horanyi et al.\u2019s model [25]. The model consists of a FoV Map Generator (shown on right), which uses an auxiliary 3D gaze angle estimator and an auxiliary depth model to produce an FoV map for a given person. The FoV map, estimated depth, and image are passed to a ResNet50-based encoder and convolutional decoder to produce a gaze prediction. In our experiments, we consider both freezing vs. training the 3D gaze angle estimator as part of the model.", "description": "This figure details the architecture of the re-implementation of Horanyi et al.'s model for gaze estimation.  It illustrates a multi-stage process. First, a FoV (Field of View) map is generated using auxiliary modules: a 3D gaze angle estimator and a depth estimation model.  These provide context for gaze prediction.  Then, the FoV map, the estimated depth map, and the input image are combined and fed into a ResNet50-based encoder.  Features extracted by the encoder are processed by a convolutional decoder which ultimately outputs a gaze prediction. The researchers explored two experimental variations: one where the auxiliary modules are frozen (weights are not updated during training) and another where they are trained alongside the main model.", "section": "3. Gaze-LLE"}, {"figure_path": "https://arxiv.org/html/2412.09586/x7.png", "caption": "(a) GazeFollow", "description": "This figure shows qualitative results from the GazeFollow dataset.  It displays several example images where the model's predicted gaze heatmap is overlaid on the original image. The heatmap indicates the model's prediction of where the person is looking, with warmer colors representing a higher probability of gaze focus.  The ground truth gaze annotations are also shown for comparison.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.09586/x8.png", "caption": "(b) VideoAttentionTarget", "description": "This figure shows qualitative results on the VideoAttentionTarget dataset.  For each example, the model's predicted heatmap (with its maximum point) is displayed on top, and the ground truth gaze annotations are shown at the bottom. This allows for a visual comparison of the model's gaze prediction accuracy against the human-annotated gaze targets.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.09586/x9.png", "caption": "(c) ChildPlay", "description": "This figure shows qualitative results from the ChildPlay dataset.  Each example shows the model's predicted heatmap (with its maximum point overlaid) and the ground truth gaze annotations.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.09586/x10.png", "caption": "(d) GOO-Real", "description": "Figure 9(d) presents qualitative results from the GOO-Real dataset, illustrating the model's gaze prediction performance in a retail setting.  For each example, the model's heatmap is displayed with its maximal point (predicted gaze target location), alongside the ground truth gaze annotation.  GOO-Real presents a unique challenge as it involves a retail environment where the person is often not facing directly toward the camera. This figure demonstrates the model's ability to generalize to diverse scenarios and various gaze behaviors.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.09586/x11.png", "caption": "Figure 9: Additional qualitative results on the 4 evaluation datasets: For each example, we show our model\u2019s predicted heatmap with its maximum point on the top, and the ground truth gaze annotations on the bottom.", "description": "Figure 9 presents a qualitative analysis of the Gaze-LLE model's performance across four datasets: GazeFollow, VideoAttentionTarget, ChildPlay, and GOO-Real.  For each dataset, several example images are shown, each displaying a comparison between the model's predicted gaze heatmap (with the maximum activation point highlighted) and the corresponding ground truth gaze annotation. This visual comparison helps to assess the model's accuracy and identify potential failure cases.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.09586/x12.png", "caption": "Figure 10: Lower performing cases: we observe errors in some cases where the head is facing away from the camera (examples 1-2), the head is occluded (examples 3), or the face is blurred (examples 4-5).", "description": "This figure showcases instances where the Gaze-LLE model struggles to accurately predict gaze targets.  These failure cases highlight the model's limitations when dealing with specific visual challenges. Examples 1 and 2 illustrate scenarios where the person's head is turned away from the camera, making it difficult to determine their gaze direction. Example 3 presents a situation where the head is occluded, hindering the model's ability to process relevant visual cues. Finally, examples 4 and 5 depict situations with blurred faces, leading to inaccurate gaze estimations. These examples demonstrate the model's sensitivity to factors such as head orientation, occlusion, and image clarity, which can affect the reliability of gaze prediction.", "section": "14. Additional Visualizations & Failure Modes"}]