[{"content": "| Method | Encoder | AUC \u2191 | Avg L2 \u2193 | Min L2 \u2193 |\n|---|---|---|---|---|\n| Chong et al. [9] | **Original (Res50)** | **0.921** | **0.137** | **0.077** |\n|  | Trained DINOv2 ViT-B | 0.908 | 0.167 | 0.101 |\n|  | Frozen DINOv2 ViT-B | 0.875 | 0.191 | 0.125 |\n| Miao et al. [42] | **Original (Res50)** | **0.934** | **0.123** | **0.065** |\n|  | Trained DINOv2 ViT-B | 0.910 | 0.152 | 0.093 |\n|  | Frozen DINOv2 ViT-B | 0.892 | 0.173 | 0.109 |\n| Gupta et al. [22] IMAGE | **Original** | **0.933** | **0.134** | **0.071** |\n|  | Trained DINOv2 ViT-B | 0.912 | 0.155 | 0.090 |\n|  | Frozen DINOv2 ViT-B | 0.894 | 0.184 | 0.116 |", "caption": "Table 1: Existing gaze architectures\ndo not leverage features from large transformer models effectively.\nWe replace the scene encoder in 3 existing open source methods with the DINOv2 ViT-B backbone and evaluate on GazeFollow (see Appendix for details).\nUsing DINOv2 does not improve performance\u2014whether or not its parameters are frozen.", "description": "This table compares the performance of three existing gaze estimation models when using different scene encoders.  The original scene encoders in each of the three models are replaced with a DINOv2 ViT-B backbone. The table shows that using the DINOv2 encoder, whether its parameters are frozen or fine-tuned, does not lead to improved performance compared to the original encoders in these models. The experiments are conducted on the GazeFollow dataset.", "section": "2. Related work"}, {"content": "|   | (1) Head Integration | (2) Decoder | (3) Branches | GazeFollow AUC \u2191 | GazeFollow Avg L2 \u2193 | GazeFollow Min L2 \u2193 |\n|---|---|---|---|---|---|---|\n| a. | early | conv | H+S | 0.854 | 0.254 | 0.168 |\n| b. | early | tran | H+S | 0.904 | 0.178 | 0.113 |\n| c. | late | conv | H+S | 0.932 | 0.155 | 0.089 |\n| d. | late | tran | H+S | **0.954** | **0.113** | **0.053** |\n| e. | late | conv | S | 0.916 | 0.184 | 0.115 |\n| f. | late | tran | S | **0.953** | **0.114** | **0.054** |", "caption": "Table 2: We investigate design choices across 3 axes: (1) early vs. late head integration, (2) convolutional vs. transformer decoder, and (3) using a head & scene branch (H+S) vs. a scene branch alone (S). Row a is the setting most similar to prior work. Conversely, we develop our final Gaze-LLE design from row f.", "description": "This table presents an ablation study analyzing the impact of different architectural design choices on gaze estimation performance.  Three key design aspects are examined: the timing of head integration (early vs. late), the type of decoder used (convolutional vs. transformer), and whether the model uses a separate head branch or just a scene branch. Row 'a' represents an architecture similar to existing methods, while row 'f' describes the final Gaze-LLE model architecture. The results highlight the effectiveness of the choices made in the final model.", "section": "3.2 Key Design Decisions for Foundation Models"}, {"content": "| Method | Learnable Params | Input | GazeFollow AUC \u2191 | GazeFollow Avg L2 \u2193 | GazeFollow Min L2 \u2193 | VideoAttentionTarget AUC \u2191 | VideoAttentionTarget L2 \u2193 | VideoAttentionTarget AP \u2191 | \n|---|---|---|---|---|---|---|---|---|\n| _One Human_ |  |  | _0.924_ | _0.096_ | _0.040_ | _0.921_ | _0.051_ | _0.925_ | \n| Recasens et al. [50] | 50M* | I | 0.878 | 0.19 | 0.113 | - | - | - | \n| Chong et al. [8] | 51M* | I | 0.896 | 0.187 | 0.112 | 0.833 | 0.171 | 0.712 | \n| Lian et al. [36] | 55M | I | 0.906 | 0.145 | 0.081 | - | - | - | \n| Chong et al. [9] | 61M | I | 0.921 | 0.137 | 0.077 | 0.860 | 0.134 | 0.853 | \n| Chen et al. [4] | 50M* | I | 0.908 | 0.136 | 0.074 | - | - | - | \n| Fang et al. [16] | 68M | I+D+E | 0.922 | 0.124 | 0.067 | 0.905 | 0.108 | 0.896 | \n| Bao et al. [2] | 29M* | I+D+P | 0.928 | 0.122 | - | 0.885 | 0.120 | 0.869 | \n| Jin et al. [30] | >52M* | I+D+P | 0.920 | 0.118 | 0.063 | 0.900 | 0.104 | 0.895 | \n| Tonini et al. [62] | 92M | I+D | 0.927 | 0.141 | - | 0.862\u2021 | 0.125 | 0.742 | \n| Hu et al. [27] | >61M* | I+D+O | 0.923 | 0.128 | 0.069 | 0.880 | 0.118 | 0.881 | \n| Gupta et al. [22] | 35M | I+D+P | 0.943 | 0.114 | 0.056 | 0.914 | 0.110 | 0.879 | \n| Horanyi et al. [25]\u2020 | 46M\u2020 | I+D | 0.896\u2020 | 0.196\u2020 | 0.127\u2020 | 0.832\u2020 | 0.199\u2020 | 0.800\u2020 | \n| Miao et al. [42] | 61M | I+D | 0.934 | 0.123 | 0.065 | 0.917 | 0.109 | 0.908 | \n| Tafasca et al. [58] | >25M* | I+D | 0.939 | 0.122 | 0.062 | 0.914 | 0.109 | 0.834 | \n| Tafasca et al. [59] | >135M* | I | 0.944 | 0.113 | 0.057 | - | 0.107 | 0.891 | \n| **Gaze-LLE (ViT-B)** | **2.8M** | **I** | **0.956** | **0.104** | **0.045** | **0.933** | **0.107** | **0.897** | \n| **Gaze-LLE (ViT-L)** | **2.9M** | **I** | **0.958** | **0.099** | **0.041** | **0.937** | **0.103** | **0.903** | ", "caption": "Table 3: Gaze target estimation results on GazeFollow and VideoAttentionTarget. We report the number of learnable parameters for each model, and if auxiliary models are used for inputs: I is image, D is depth, and P is pose, O is objects, and E is eyes. (\u2217Parameter estimate. \u2020Our reimplementation, see Appendix. \u2021Metric re-evaluated to match benchmark\u2019s calculation protocol [9].)", "description": "This table presents a comparison of different methods for gaze target estimation on two benchmark datasets: GazeFollow and VideoAttentionTarget.  For each method, it shows the number of trainable parameters, the input data used (image (I), depth (D), pose (P), objects (O), and eyes (E)), and the performance metrics achieved: Area Under the Curve (AUC), Average L2 error, and Minimum L2 error.  AUC measures the accuracy of the heatmap produced by the model, while L2 error measures the distance between the predicted gaze target and the ground truth. The table highlights the relatively small number of parameters in the proposed Gaze-LLE model compared to existing methods.  Notes clarify that some parameter counts are estimates, one entry is based on a reimplementation of a prior model, and one metric was recalculated to match the methodology of a specific prior publication.", "section": "4.1 Main Results"}, {"content": "| Method | AUC \u2191 | L2 \u2193 | AP \u2191 | P.Head \u2191 |\n|---|---|---|---|---|\n| Gupta et al. [22] | 0.919 | 0.113 | 0.983 | 0.694 |\n| Tafasca et al. [58] | 0.935 | 0.107 | 0.986 | 0.663 |\n| Tafasca et al. [59] | - | 0.106 | 0.990 | 0.600 |\n| **Gaze-LLE (ViT-B)** | **0.949** | **0.106** | **0.994** | **0.715** |\n| **Gaze-LLE (ViT-L)** | **0.951** | 0.101 | **0.994** | 0.662 |", "caption": "Table 4: Gaze target estimation results on ChildPlay.", "description": "This table presents a comparison of different methods for gaze target estimation on the ChildPlay dataset.  It shows the Area Under the Curve (AUC) of the receiver operating characteristic (ROC) curve, the average L2 distance (Avg L2), the minimum L2 distance (Min L2), and the precision of head prediction (P.Head) for each method.  The ChildPlay dataset is a benchmark dataset specifically designed for evaluating gaze estimation models' performance on the gaze behavior of children.  The metrics provide a comprehensive evaluation of the accuracy and precision of gaze prediction methods on this challenging dataset.", "section": "4. Main Results"}, {"content": "| Method | VAT AUC \u2191 | VAT L2 \u2193 | GOO-Real AUC \u2191 | GOO-Real L2 \u2193 | ChildPlay AUC \u2191 | ChildPlay L2 \u2193 |\n|---|---|---|---|---|---|---|\n| Chong et al. [9]<sup>\u2217</sup> | 0.906 | 0.119 | 0.670 | 0.334 | 0.912 | 0.121 |\n| Jin et al. [30] | 0.900 | 0.104 | - | - | - | - |\n| Tonini et al. [62] w/ UDA | - | - | 0.840 | 0.238 | - | - |\n| Miao et al. [42]<sup>\u2217</sup> | 0.923 | 0.109 | 0.869 | 0.202 | 0.933 | 0.113 |\n| Gupta et al. [22] | 0.907 | 0.137 | - | - | 0.923 | 0.142 |\n| Tafasca et al. [58] | 0.911 | 0.123 | - | - | 0.932 | 0.115 |\n| **Gaze-LLE (B)** | **0.932** | **0.105** | **0.901** | **0.174** | **0.946** | **0.114** |\n| **Gaze-LLE (L)** | **0.937** | **0.100** | **0.898** | **0.175** | **0.951** | **0.101** |", "caption": "Table 5: Cross-dataset results on VideoAttentionTarget (VAT), GOO-Real, and ChildPlay. (\u2217Results we evaluated ourselves from the official code releases.)", "description": "This table presents the cross-dataset generalization performance of the Gaze-LLE model.  It shows the Area Under the ROC Curve (AUC) and average L2 distance metrics for the model trained only on the GazeFollow dataset, but tested on three other datasets: VideoAttentionTarget, GOO-Real, and ChildPlay. The results demonstrate the model's ability to generalize to new, unseen datasets without requiring any fine-tuning.", "section": "4.1 Main Results"}, {"content": "| Backbone | AUC \u2191 | Avg L2 \u2193 | Min L2 \u2193 |\n|---|---|---|---| \n| Supervised [55] | 0.928 | 0.151 | 0.086 |\n| MAE [23] | 0.947 | 0.126 | 0.061 |\n| CLIP [48] | 0.953 | 0.107 | 0.049 |\n| DINOv2 [45] | **0.958** | **0.099** | **0.041** |", "caption": "Table 6: Ablation of different pretrained Vit-L backbones with Gaze-LLE on GazeFollow.", "description": "This table presents an ablation study evaluating the impact of different pre-trained ViT-L backbones on the performance of the Gaze-LLE model.  It shows how using different pre-trained models (Supervised, MAE, CLIP, and DINOv2) as the backbone for Gaze-LLE affects performance on the GazeFollow dataset, as measured by AUC, Avg L2, and Min L2.", "section": "4.2. Analysis"}, {"content": "| Head Prompt | (1) Attention | (2) Decoder | AUC \u2191 | Avg L2 \u2193 | Min L2 \u2193 |\n|---|---|---|---|---|---| \n| token | (b) cross | (c) mlp | 0.937 | 0.117 | 0.059 |\n|  | (b) cross | (b) dot | 0.945 | 0.114 | 0.055 |\n|  | (a) self | (c) mlp | 0.939 | 0.115 | 0.058 |\n|  | (a) self | (b) dot | 0.952 | 0.113 | 0.052 |\n|  | (a) self | (a) conv | **0.956** | **0.106** | **0.047** |\n| embedding | (a) self | (a) conv | **0.956** | **0.104** | **0.045** |", "caption": "Table 7: As an alternative to adding the head position embedding pheadsubscript\ud835\udc5dheadp_{\\text{head}}italic_p start_POSTSUBSCRIPT head end_POSTSUBSCRIPT to the scene tokens, we explore representing the head\u2019s center position as an additional token, tpossubscript\ud835\udc61post_{\\text{pos}}italic_t start_POSTSUBSCRIPT pos end_POSTSUBSCRIPT. We consider self attention vs. cross attention across the token list, and different ways to decode the heatmap from the scene tokens and tpossubscript\ud835\udc61post_{\\text{pos}}italic_t start_POSTSUBSCRIPT pos end_POSTSUBSCRIPT.", "description": "This table explores alternative methods for incorporating head position information into the Gaze-LLE model. Instead of adding a learned head position embedding to the scene features, the head's center position is represented as an additional token.  The experiment compares different attention mechanisms (self-attention vs. cross-attention) applied to this modified token list, and also assesses the impact of various decoding methods on the final gaze heatmap prediction.  The goal is to determine the effectiveness of representing head position information as a separate token compared to the original method which directly adds head position embedding to the scene features.", "section": "3.2 Key Design Decisions for Foundation Models"}, {"content": "| Prompt type | AUC \u2191 | Avg L2 \u2193 | Min L2 \u2193 |\n|---|---|---|---|\n| No prompting | 0.926 | 0.169 | 0.105 |\n| With prompting | 0.956 | 0.104 | 0.045 |", "caption": "(a) Head prompt ablation", "description": "This table presents an ablation study on the head prompt mechanism within the Gaze-LLE model. It shows the impact of removing the head prompt entirely and explores the effect of injecting the head prompt into different layers of the gaze decoder.  The results quantify the effectiveness of head prompting for gaze estimation performance.", "section": "3.2 Key Design Decisions for Foundation Models"}, {"content": "| Prompt location | AUC \u2191 | Avg L2 \u2193 | Min L2 \u2193 |\n|---|---|---|---|\n| Layer 3 | 0.955 | 0.108 | 0.048 |\n| Layer 2 | 0.955 | 0.106 | 0.047 |\n| Layer 1 (default) | **0.956** | **0.104** | **0.045** |", "caption": "(b) Head prompt location", "description": "This table presents ablation study results on the impact of head prompt placement within the Gaze-LLE decoder. It shows the performance (AUC, Avg L2, Min L2) when the head prompt is injected at different transformer layers (Layer 1, Layer 2, Layer 3).  The results help determine the optimal layer for head prompt integration to achieve the best balance between model performance and computational efficiency.", "section": "3.2 Key Design Decisions for Foundation Models"}, {"content": "| DINOv2 Training | Learning rate | AUC \u2191 | Avg L2 \u2193 | Min L2 \u2193 |\n|---|---|---|---|---|\n| Original Method | 2.5e-4 | **0.921** | **0.137** | **0.077** |\n| Frozen | 2.5e-4 | 0.858 | 0.196 | 0.133 |\n|  | 1.0e-4 | 0.857 | 0.201 | 0.145 |\n|  | 1.0e-5 | 0.808 | 0.230 | 0.166 |\n|  | 1.0e-6 | 0.726 | 0.287 | 0.218 |\n| Frozen + proj | 2.5e-4 | 0.875 | 0.191 | 0.125 |\n|  | 1.0e-4 | 0.872 | 0.198 | 0.129 |\n|  | 1.0e-5 | 0.850 | 0.212 | 0.143 |\n|  | 1.0e-6 | 0.766 | 0.282 | 0.208 |\n| Trained + proj | 2.5e-4 | 0.876 | 0.185 | 0.120 |\n|  | 1.0e-4 | **0.908** | **0.167** | **0.101** |\n|  | 1.0e-5 | 0.870 | 0.199 | 0.132 |\n|  | 1.0e-6 | 0.805 | 0.260 | 0.187 |", "caption": "Table 8: We demonstrate the effectiveness of our head prompting mechanism (17), and find that injecting the head prompt before the first transformer layer in our gaze decoder module slightly outperforms later layers (8(b))", "description": "This table presents an ablation study on the head prompt's effectiveness and placement within the Gaze-LLE model.  It shows that adding a head prompt improves gaze estimation accuracy.  Furthermore, it compares placing the head prompt before different transformer layers in the decoder, demonstrating that injecting it before the first layer yields slightly better results than injecting it later.  The metrics used are Area Under the Curve (AUC), Average L2 distance, and Minimum L2 distance, all commonly used to evaluate gaze estimation performance.", "section": "3.2 Key Design Decisions for Foundation Models"}, {"content": "| DINOv2 Training | Learning rate | AUC \u2191 | Avg L2 \u2193 | Min L2 \u2193 |\n|---|---|---|---|---|\n| Original Method | 2.5e-4 | **0.934** | **0.123** | **0.065** |\n| Frozen | 2.5e-4 | 0.858 | 0.207 | 0.141 |\n|  | 1.0e-4 | 0.859 | 0.203 | 0.138 |\n|  | 1.0e-5 | 0.807 | 0.236 | 0.169 |\n|  | 1.0e-6 | 0.702 | 0.297 | 0.228 |\n| Frozen + proj | 2.5e-4 | 0.892 | 0.173 | 0.109 |\n|  | 1.0e-4 | 0.887 | 0.176 | 0.113 |\n|  | 1.0e-5 | 0.859 | 0.203 | 0.137 |\n|  | 1.0e-6 | 0.761 | 0.286 | 0.213 |\n| Trained + proj | 2.5e-4 | 0.899 | 0.165 | 0.103 |\n|  | 1.0e-4 | **0.910** | **0.152** | **0.093** |\n|  | 1.0e-5 | 0.900 | 0.161 | 0.098 |\n|  | 1.0e-6 | 0.847 | 0.220 | 0.149 |", "caption": "Table 9: Comparison of integrating DINOv2 into Chong et al. [9] with different training configurations (DINOv2 encoder learning strategy & learning rate) on GazeFollow.", "description": "This table presents a comparison of different training strategies for integrating a pre-trained DINOv2 model into Chong et al.'s gaze estimation method.  It explores the effects of freezing the DINOv2 encoder's weights (Frozen), only training a projection layer to handle a four-channel input including head pose (Frozen + proj), and training the entire DINOv2 encoder (Trained + proj).  The results, evaluated on the GazeFollow dataset, show the performance (AUC, Avg L2, Min L2) achieved with each configuration and different learning rates for the DINOv2 encoder.", "section": "6. Integration of DINOv2 into Existing Methods"}, {"content": "| DINOv2 Training | Learning rate | AUC \u2191 | Avg L2 \u2193 | Min L2 \u2193 |\n|---|---|---|---|---|\n| Original Method | 2.5e-4 | **0.933** | **0.134** | **0.071** |\n| Frozen + proj | 2.5e-4 | 0.893 | 0.180 | 0.113 |\n|  | 1.0e-3 | 0.894 | 0.184 | 0.116 |\n|  | 1.0e-4 | 0.897 | 0.175 | 0.108 |\n|  | 1.0e-5 | 0.874 | 0.199 | 0.129 |\n|  | 1.0e-6 | 0.818 | 0.228 | 0.161 |\n| Trained + proj | 2.5e-4 | 0.908 | 0.165 | 0.099 |\n|  | 1.0e-3 | **0.912** | **0.155** | **0.091** |\n|  | 1.0e-4 | 0.911 | 0.159 | 0.095 |\n|  | 1.0e-5 | 0.899 | 0.167 | 0.101 |\n|  | 1.0e-6 | 0.842 | 0.219 | 0.149 |", "caption": "Table 10: Comparison of integrating DINOv2 into Miao et al. [42] with different training configurations (DINOv2 encoder learning strategy & learning rate) on GazeFollow.", "description": "This table presents the results of experiments integrating the DINOv2 model into Miao et al.'s [42] gaze estimation method.  It investigates the impact of different training strategies for DINOv2, varying the encoder's learning approach (frozen, frozen with a projection layer added, and fully trained) and the learning rate. The performance is evaluated on the GazeFollow dataset using AUC, average L2 distance, and minimum L2 distance metrics. The table helps to analyze the effectiveness of different DINOv2 training configurations within the broader context of Miao et al.'s method.", "section": "4. Experiments"}, {"content": "| Method | Input size | AUC | Avg L2 | Min L2 |\n|---|---|---|---|---|\n| Chong et al. - Original | 224 | 0.921 | 0.137 | 0.077 |\n| Chong et al. - Original | 448 | 0.923 | 0.138 | 0.076 |\n| Chong et al. - Trained DINOv2 | 224 | 0.908 | 0.170 | 0.101 |\n| Chong et al. - Trained DINOv2 | 448 | 0.897 | 0.169 | 0.105 |\n| Miao et al. - Original | 224 | 0.934 | 0.123 | 0.065 |\n| Miao et al. - Original | 448 | 0.923 | 0.151 | 0.086 |\n| Miao et al. - Trained DINOv2 | 224 | 0.910 | 0.152 | 0.093 |\n| Miao et al. - Trained DINOv2 | 448 | 0.908 | 0.154 | 0.094 |\n| Gupta et al. - Original | 224 | 0.943 | 0.114 | 0.056 |\n| Gupta et al. - Original | 448 | 0.939 | 0.108 | 0.052 |\n| Gupta et al. - Trained DINOv2 | 224 | 0.912 | 0.155 | 0.091 |\n| Gupta et al. - Trained DINOv2 | 448 | 0.908 | 0.170 | 0.103 |", "caption": "Table 11: Comparison of integrating DINOv2 into Gupta et al. [22] (Image-only variant) with different training configurations (DINOv2 encoder learning strategy & learning rate) on GazeFollow.", "description": "This table presents the results of experiments integrating a pre-trained DINOv2 model into the image-only variant of Gupta et al.'s [22] gaze estimation method.  Different training configurations were tested: the DINOv2 encoder was either frozen, or partially trained (only the projection layer), or fully trained.  Each configuration was evaluated at different learning rates, showing the impact of these choices on the performance of the gaze estimation model, measured by AUC, Avg L2, and Min L2 metrics on the GazeFollow dataset.", "section": "3.2 Key Design Decisions for Foundation Models"}, {"content": "| Transformer Decoder |\n|---|---| \n| Linear ($d \\to 256$) |\n| Trans. Layer (dim=256, heads=8, mlp_dim=1024) |\n| ConvT ($256 \\to 256$, k=2, s=2) |\n| Conv ($256 \\to 1$, k=1, s=1) |\n| Sigmoid |", "caption": "Table 12: Effect of increasing the input scene image size for Chong et al., Miao et al., and Gupta et al.\u2019s original methods and best variants with DINOv2. We do not observe clear gains from using a larger input size.", "description": "This table presents the results of an experiment investigating the impact of input image size on gaze estimation performance. Three existing methods (Chong et al., Miao et al., and Gupta et al.) were tested, both in their original forms and with their best-performing variations using a DINOv2 encoder. The input image size was increased from 224x224 to 448x448. The table displays the Area Under the Curve (AUC), average L2 error, and minimum L2 error for each method and image size.  The results demonstrate that increasing the input image size does not consistently lead to significant improvements in gaze estimation accuracy, suggesting that a larger input size is not necessary for high performance when high resolution head crops are available.", "section": "4. Experiments"}, {"content": "| Conv Decoder |\n|---|---| \n| Conv(d\u2192768, k=1, s=1) |\n| Conv(768\u2192384, k=1, s=1) |\n| Conv(384\u2192192, k=2, s=2) |\n| ConvT(192\u219296, k=2, s=2) |\n| ConvT(96\u21921, k=2, s=2) |\n| Conv(1\u21921, k=1, s=1) |\n| Sigmoid |", "caption": "Table 13: Architecture details for Transformer Decoder and Convolutional Decoder for experiments in Section 3.1", "description": "This table details the architectures of two different decoder modules used in the Gaze-LLE model for experiments within Section 3.1.  One decoder uses a transformer-based approach, while the other uses a convolutional approach.  The table specifies the layer types, input and output dimensions, kernel sizes, strides, and activation functions for each layer in both decoder architectures.", "section": "3.1 Model Architecture"}, {"content": "| Method | AUC \u2191 | Avg L2 \u2193 | Min L2 \u2193 |\n|---|---|---|---|\n| *with ground truth gaze matching* |  |  |  |\n| Tu et al. [64] | 0.917 | 0.133 | 0.069 |\n| Tu et al. [65] | 0.928 | 0.114 | 0.057 |\n| Tonini et al. [63] | 0.922 | 0.069 | 0.029 |\n| Tonini et al.* [63] | 0.924 | 0.068 | 0.030 |\n| *no ground truth gaze matching* |  |  |  |\n| Tonini et al.* [63] | 0.767 | 0.211 | 0.148 |\n| Ours | 0.956 | 0.104 | 0.045 |", "caption": "Table 14: Quantitative comparison with detection-based methods on GazeFollow. The results with ground truth gaze matching use the ground truth gaze labels to perform bipartite matching at test time, and thus are not a direct comparison to our method and prior work. The no ground truth gaze matching results report our method compared to Tonini et al.\u2019s model evaluated with the altered matching cost function in Equation 7, which excludes ground truth gaze information. (\u2217Results we obtained ourselves by running Tonini et al.\u2019s published code.)", "description": "This table compares the performance of Gaze-LLE with detection-based methods (Tu et al. [64, 65], Tonini et al. [63]) on the GazeFollow dataset.  The comparison is complicated because the detection-based methods use a different evaluation protocol.  Specifically, they use bipartite matching with ground truth gaze at test time to assess performance.  This differs from Gaze-LLE and traditional approaches that do not utilize ground truth labels during testing.  To facilitate a fair comparison, the table shows results for Tonini et al.'s model using a modified matching cost (Equation 7) that excludes ground truth gaze, making the evaluation more comparable to Gaze-LLE's. Results using the standard (ground truth gaze-based) evaluation method are also included for context.", "section": "8. Comparison to Detection Methods"}, {"content": "| Method | AUC \u2191 | Avg L2 \u2193 | Min L2 \u2193 |\n|---|---|---|---|\n| ViT-B + GT | 0.956 | 0.104 | 0.045 |\n| ViT-B + YOLO | 0.955 | 0.106 | 0.047 |\n| ViT-L + GT | 0.958 | 0.099 | 0.041 |\n| ViT-L + YOLO | 0.958 | 0.101 | 0.043 |", "caption": "Table 15: Gaze-LLE achieves consistent results when using head detections from an out-of-the-box YOLOv5 detector instead of head ground truth bounding boxes.", "description": "This table presents a comparison of Gaze-LLE's performance using ground truth head bounding boxes versus head detections from a pre-trained YOLOv5 object detector.  It demonstrates the robustness of Gaze-LLE by showing consistent performance regardless of whether ground truth or detected head locations are used as input. The metrics compared are AUC, average L2 distance, and minimum L2 distance, assessing the accuracy of gaze prediction.  The results highlight the model's ability to generalize well to real-world scenarios where perfect head bounding box annotations might not be available.", "section": "10. Performance with Estimated Head Bounding Boxes"}, {"content": "| Experiment | GazeFollow |  |  | VideoAttentionTarget |  |  |\n|---|---|---|---|---|---|---|\n|  | AUC \u2191 | Avg L2 \u2193 | Min L2 \u2193 | AUC \u2191 | L2 \u2193 | APin/out \u2191 |\n| Frozen Aux. Angle | 0.869 | 0.217 | 0.146 | 0.802 | 0.234 | 0.720 |\n| Trained Aux. Angle | **0.896** | **0.196** | **0.127** | **0.832** | **0.199** | **0.800** |", "caption": "Table 16: Experimental results for our implementation of Horanyi et al.[25] on GazeFollow and VideoAttentionTarget. We consider the setting where we freeze the auxiliary 3D gaze angle model vs. where we train it along with the rest of the network.", "description": "This table presents a comparison of the performance of a reimplementation of the Horanyi et al. gaze estimation model on two datasets, GazeFollow and VideoAttentionTarget.  The model includes auxiliary components for estimating 3D gaze angles and depth. The table shows results under two conditions: one where the auxiliary model's parameters are frozen during training, and one where it is trained alongside the main model.  The comparison highlights how training the auxiliary model affects the overall accuracy (AUC), average L2 error, and minimum L2 error in predicting gaze targets, and also the Average Precision for 'in/out' classification (applicable only to VideoAttentionTarget).", "section": "11. Reimplementation of Horanyi et al."}, {"content": "| d_model | Params | AUC \u2191 | Avg L2 \u2193 | Min L2 \u2193 |\n|---|---|---|---|---|\n| 128 | 1.2M | **0.956** | 0.106 | 0.046 |\n| 256 (default) | 2.8M | **0.956** | **0.104** | **0.045** |\n| 384 | 5.0M | **0.956** | 0.105 | 0.046 |\n| 512 | 7.7M | 0.953 | 0.108 | 0.049 |\n| 768 | 14.8M | 0.953 | 0.108 | 0.049 |", "caption": "(a) Dimension of gaze estimation module.", "description": "This table presents ablation studies on the dimension of the gaze estimation module within the Gaze-LLE architecture.  It shows how varying the dimension (dmodel) of the module, from 128 to 768, affects the model's performance.  The evaluation metrics used are AUC, Avg L2, and Min L2, allowing for a comprehensive assessment of the impact of dmodel on accuracy and localization precision of gaze estimation.  The number of parameters in the module are also provided for each dimension.", "section": "Additional Ablation Studies"}, {"content": "| Layers | Params | AUC \u2191 | Avg L2 \u2193 | Min L2 \u2193 |\n|---|---|---|---|---|\n| 1 layer | 1.2M | 0.953 | 0.115 | 0.054 |\n| 2 layers | 2.0M | 0.955 | 0.108 | 0.049 |\n| 3 layers | 2.8M | **0.956** | 0.104 | **0.045** |\n| 4 layers | 3.6M | **0.956** | **0.103** | **0.045** |\n| 5 layers | 4.4M | **0.956** | 0.104 | **0.045** |", "caption": "(b) Number of transformer layers.", "description": "This table presents the ablation study on the number of transformer layers used in the gaze decoder. It shows how the performance (AUC, Avg L2, Min L2) changes as the number of layers increases from 1 to 5, while keeping other hyperparameters constant.  The results demonstrate the impact of the number of transformer layers on the model's accuracy and the trade-off between model complexity and performance.", "section": "12. Additional Ablation Studies"}, {"content": "| Backbone | Params | AUC \u2191 | Avg L2 \u2193 | Min L2 \u2193 |\n|---|---|---|---|---|\n| *One Human* |  | *0.924* | *0.096* | *0.040* |\n| ViT-B | 2.8M | 0.956 | 0.104 | 0.045 |\n| ViT-B + LoRA | 3.1M | 0.957 | 0.103 | 0.045 |\n| ViT-L | 2.9M | 0.958 | 0.099 | 0.041 |\n| ViT-L + LoRA | 3.7M | 0.960 | 0.097 | 0.040 |", "caption": "Table 17: We investigate the effect of different internal model dimensions and number of transformer layers for our gaze estimation module with a ViT-Base DINOv2 backbone. We observe diminishing returns as we increase the dimension and number of layers. We select dmodel=256subscript\ud835\udc51model256d_{\\text{model}}=256italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT = 256 with 3 transformer layers as our default configuration.", "description": "This table presents an ablation study on the architecture of the Gaze-LLE model's gaze estimation module.  Specifically, it examines the impact of varying the internal dimension (dmodel) and the number of transformer layers on the model's performance.  The study uses a ViT-Base DINOv2 backbone and evaluates the results on the GazeFollow dataset.  The results show that increasing either the dimension or the number of layers yields diminishing returns in performance after a certain point.  Based on this analysis, the researchers selected a dmodel of 256 and 3 transformer layers as the optimal configuration for their default model.", "section": "12. Additional Ablation Studies"}]