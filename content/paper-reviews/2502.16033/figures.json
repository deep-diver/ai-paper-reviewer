[{"figure_path": "https://arxiv.org/html/2502.16033/x1.png", "caption": "Figure 1: An illustration of multimodal inconsistency reasoning on a webpage. An agent examines a webpage where the brand \u201cIKEA AB\u201d is mentioned, but other elements clearly refer to \u201cLorell.\u201d Detecting this brand identity misattribution requires the ability to compare text fields across different sections of the page and reconcile them with accompanying images or context\u2014an inherently multimodal reasoning task.", "description": "The figure shows a webpage with conflicting information.  The text mentions the brand \"IKEA AB\", but other visual elements (images and text) clearly indicate the brand is \"Lorell\".  The task of identifying this inconsistency requires a multimodal reasoning model to integrate information from both text and images, comparing textual information across different parts of the page and relating it to the corresponding visuals to determine the correct brand.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2502.16033/x2.png", "caption": "Figure 2: There are five inconsistency categories in the MMIR benchmark, posing diverse challenges.", "description": "Figure 2 presents five example inconsistencies from the MMIR benchmark dataset, each representing a different category of multimodal inconsistency.  These examples illustrate the diverse challenges that the benchmark presents for multimodal reasoning models.  The categories are: Factual Contradiction (a conflict between text and image), Identity Misattribution (mismatched brand information across different elements), Contextual Mismatch (thematic mismatch between elements), Quantitative Discrepancy (numerical errors in a chart compared to text), and Temporal/Spatial Incoherence (time or location conflicts within the presented elements).  Each example demonstrates the complexity of detecting inconsistencies across modalities and different layouts.", "section": "3 MMIR"}, {"figure_path": "https://arxiv.org/html/2502.16033/extracted/6221352/figures/data_filter.png", "caption": "Figure 3: MMIR Data filtering process.", "description": "The figure illustrates the four-stage process of MMIR data curation. It begins with the collection and parsing of 521 artifacts, followed by the synthetic injection of inconsistencies using the 01-1217 language model, resulting in 2,534 proposals. After automated editing and human verification, the final dataset of 534 validated quintuples is obtained. These quintuples consist of the modified artifact, its elements, the type of inconsistency introduced, and the rationale behind it. This filtered dataset forms the MMIR benchmark.", "section": "3 MMIR"}, {"figure_path": "https://arxiv.org/html/2502.16033/x3.png", "caption": "Figure 4: Fine-grained analysis of model performance.", "description": "This figure presents a detailed breakdown of model performance on the MMIR benchmark.  Panel (a) shows accuracy broken down by the five types of inconsistencies (factual contradiction, identity misattribution, contextual mismatch, quantitative discrepancy, and temporal/spatial incoherence). Panel (b) displays accuracy broken down by the type of modality involved in the inconsistency (text-only, image-only, text and image). The figure shows that models perform better on inconsistencies involving a single modality, particularly text, and struggle more with cross-modal inconsistencies. The superior performance of proprietary models (o1 and GPT-40) is also evident across all inconsistency categories and modalities.", "section": "4.2 Error Analysis"}, {"figure_path": "https://arxiv.org/html/2502.16033/x4.png", "caption": "Figure 5: Model performance on layout complexity.", "description": "This figure shows the relationship between the number of elements in an artifact and model performance on the MMIR benchmark.  It reveals that as the complexity of the layout increases (i.e., more elements in the artifact), the accuracy of all models decreases.  Proprietary models maintain higher accuracy in simpler layouts but show a similar decline in performance as complexity increases compared to open-source models. Open-source models struggle even with low-complexity artifacts.", "section": "4.2 Error Analysis"}, {"figure_path": "https://arxiv.org/html/2502.16033/x5.png", "caption": "Figure 6: Example of original artifact in MMIR (left) and artifact annotated with Set-of-Mark in the probing analysis (right).", "description": "This figure shows a side-by-side comparison: on the left, the original webpage from the Multimodal Inconsistency Reasoning (MMIR) benchmark; on the right, the same webpage after it has been annotated using the Set-of-Marks (SoM) method for probing experiments. SoM highlights relevant elements (bounding boxes) in the image to help evaluate the effectiveness of prompting methods on visual multimodal reasoning. This helps assess how well models can leverage visual cues when integrated with textual prompting for improved reasoning performance.", "section": "4.3 Probing on Prompting Methods"}, {"figure_path": "https://arxiv.org/html/2502.16033/x6.png", "caption": "Figure 7: A test sample with model responses under the two main settings in MMIR: open-ended and multiple-choice.", "description": "This figure displays a sample from the MMIR benchmark, illustrating how different models perform under open-ended and multiple-choice evaluation settings.  The left side shows the original image, which contains a factual contradiction: a map labels a geographic region as \"Rocky Mountain Range\" when it is in fact a different arctic region. The right side presents model responses for both question types; open-ended responses show the model's ability to identify the error without predefined options, while multiple-choice responses test accuracy when provided with choices of potential inconsistent elements.", "section": "4 Experiments and Analysis"}]