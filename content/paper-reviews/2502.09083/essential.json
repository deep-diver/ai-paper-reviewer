{"importance": "This paper is crucial for **AI researchers and HCI professionals** working on explainable AI and fact-checking. It bridges the gap between theoretical explainability and the practical needs of fact-checkers, offering valuable insights and recommendations for developing more effective automated tools.  The findings highlight the **importance of integrating user needs** into the design process and **identifying the specific types of explanations** required for different stages of the fact-checking workflow. This research directly addresses current limitations in automated fact-checking and opens new avenues for improving the design and usability of explainable AI systems.", "summary": "Fact-checkers need explainable AI: This study reveals how AI tools can better support fact-checkers by providing explanations tailored to their workflows, addressing unmet needs, and improving the effectiveness of automated fact-checking systems.", "takeaways": ["Fact-checkers require explanations that trace reasoning, reference evidence, and highlight uncertainty.", "Automated tools need improvements in evidence retrieval and explanation generation to meet fact-checkers' needs.", "Explainable AI systems should align with fact-checkers' decision-making processes to be effective."], "tldr": "Automated fact-checking struggles to meet fact-checkers' needs due to a lack of understanding of their workflows and explanation requirements.  Existing AI tools often fail to provide the specific, context-rich explanations that are crucial for fact-checkers to effectively assess evidence, scrutinize outputs, and integrate automated tools into their workflows. This leads to a disconnect between current AI technology and the practical demands of professional fact-checking. \n\nThis research addresses this gap by conducting semi-structured interviews with fact-checking professionals to understand their workflow and explanation needs. The findings reveal significant unmet explanation needs and highlight crucial criteria for effective explanations that include tracing the model's reasoning, referencing specific evidence, and highlighting uncertainty. The study provides actionable recommendations for developing more user-friendly automated fact-checking tools that meet the practical demands of fact-checkers, bridging the gap between AI capabilities and real-world fact-checking practices.  The study emphasizes the importance of human-centered design in the development of automated fact-checking systems.", "affiliation": "University of Copenhagen", "categories": {"main_category": "Natural Language Processing", "sub_category": "Question Answering"}, "podcast_path": "2502.09083/podcast.wav"}