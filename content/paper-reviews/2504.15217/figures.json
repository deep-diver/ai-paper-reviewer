[{"figure_path": "https://arxiv.org/html/2504.15217/x3.png", "caption": "(a) Overall diagram of DRAGON, a versatile on-policy learning framework for media generation models that can optimize various types of reward functions.", "description": "DRAGON is a versatile on-policy reinforcement learning framework designed for fine-tuning media generation models.  It's depicted as a flowchart showing the process of generating on-policy samples, calculating rewards using various methods (instance-wise, instance-to-distribution, distribution-to-distribution), creating positive and negative demonstration sets, and using these sets to refine the model via a contrastive learning approach. The framework is shown to be adaptable to diverse reward functions, making it a powerful tool for optimizing generative models towards desired outcomes.", "section": "3 Distributional Reward Optimization For Diffusion Models"}, {"figure_path": "https://arxiv.org/html/2504.15217/x4.png", "caption": "(b) DRAGON significantly improves a full suite of rewards. Each vertex of the plot considers a reward metric and reports the win rate of the DRAGON model optimized for the metric.", "description": "This figure visualizes the performance improvement achieved by the DRAGON model across various reward metrics. Each point in the radar chart represents a specific reward function (e.g., aesthetics score, CLAP score, etc.), and its distance from the center indicates the win rate of the DRAGON model (optimized for that specific reward) compared to a baseline model.  A higher win rate corresponds to a larger distance from the center, indicating a greater improvement by DRAGON. The chart effectively summarizes the model's overall performance enhancement across a range of reward functions, highlighting its versatility.", "section": "2 Background"}, {"figure_path": "https://arxiv.org/html/2504.15217/x5.png", "caption": "Figure 2: DPO versus KTO loss function; paired versus unpaired demonstrations.", "description": "This figure compares the performance of two different loss functions, Direct Preference Optimization (DPO) and Kahneman-Tversky Optimization (KTO), in fine-tuning diffusion models. It also explores the impact of using paired versus unpaired demonstrations. Paired demonstrations refer to having pre-defined pairs of preferred and less-preferred samples for comparison and training, while unpaired demonstrations utilize individual samples without explicit pairwise preferences. The figure showcases the win rates achieved by each method, offering insights into which combination of loss function and demonstration type performs best in the context of optimizing diffusion models.", "section": "5.2 Optimizing Instance-Level Rewards"}, {"figure_path": "https://arxiv.org/html/2504.15217/x6.png", "caption": "Figure 3: DRAGON with different demonstration diffusion steps and inference steps.", "description": "This figure analyzes the effect of varying the number of diffusion steps used during training and inference on the performance of the DRAGON model.  It shows that reducing the number of diffusion steps during training (while keeping the inference steps constant at 40) has a minimal impact on the final generation quality.  Furthermore, it demonstrates that using fewer diffusion steps during both training and inference can even lead to slightly better results, making the training process more efficient.", "section": "5.2 Optimizing Instance-Level Rewards"}, {"figure_path": "https://arxiv.org/html/2504.15217/x7.png", "caption": "Figure 4: Vendi score of models optimized for each reward type.\nPoint height represents Vendi score and point size represents aesthetics win rate.\nEach per-song/dataset FAD point train with a different reference statistic.\nBar height averages point height.", "description": "This figure displays the Vendi score, a measure of diversity in generated music, for models trained with different reward functions.  The height of each point represents the Vendi score achieved by a model optimized for a specific reward function. The size of the point corresponds to the model's aesthetics win rate, indicating its success in generating aesthetically pleasing music. Different reward functions were used for the experiment: instance-wise reward (Aesthetics score), instance-to-instance reward (CLAP score), instance-to-distribution reward (per-song FAD), and distribution-to-distribution reward (dataset FAD).  For per-song and dataset FAD, models were trained using different reference statistics, creating multiple data points for each.  The bar heights show the average Vendi score across all models using the same reward type.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2504.15217/x9.png", "caption": "Figure 5: Ablation study on aesthetics model settings.\nHigher correlation with human ratings means better aesthetics model performance.", "description": "This ablation study investigates the impact of various settings on the aesthetics prediction model's performance.  The goal is to determine the optimal configuration that yields the highest correlation between the model's predicted aesthetic scores and actual human ratings. Different settings are explored, including changes to the way audio embeddings are calculated (averaging methods and normalization strategies), and the number of input audio chunks. The results are shown in terms of both overall Pearson Linear Correlation Coefficient (PLCC) and per-prompt Spearman Rank Correlation Coefficient (SRCC), providing a comprehensive assessment of the model's performance across different approaches.", "section": "4.1 Instance-Wise Reward - Human Preference Dataset and Aesthetics Score Predictor"}, {"figure_path": "https://arxiv.org/html/2504.15217/x10.png", "caption": "Figure 6: Histograms of human-rated and predicted aesthetics score over the DMA dataset after global label normalization.", "description": "This figure displays the distribution of human-rated and model-predicted aesthetics scores from the Dynamo Music Aesthetics (DMA) dataset.  Global label normalization was applied to the scores before generating the histograms.  The histograms allow for a visual comparison of how well the model's predictions align with human perception of musical aesthetics.  The x-axis represents the aesthetics score (ranging from approximately -2 to +2), and the y-axis represents the count or frequency of scores within each score bin.", "section": "4.1 Instance-Wise Reward - Human Preference Dataset and Aesthetics Score Predictor"}, {"figure_path": "https://arxiv.org/html/2504.15217/x11.png", "caption": "(a) Aesthetics score before vs after DRAGON.", "description": "This figure is a scatter plot showing the relationship between the aesthetics scores of generated music before and after applying the DRAGON model. Each point represents a single generated music piece. The x-axis displays the aesthetics score before applying DRAGON, and the y-axis shows the aesthetics score after applying DRAGON.  The plot visually demonstrates how DRAGON improves the quality of the generated music, as indicated by the shift in points towards higher aesthetics scores. The improvement is more pronounced for pieces initially receiving lower scores.", "section": "5.2 Optimizing Instance-Level Rewards \u2013 Predicted Aesthetics Score and CLAP Score"}, {"figure_path": "https://arxiv.org/html/2504.15217/x12.png", "caption": "(b) Aesthetics improvement vs baseline score.", "description": "This figure shows the improvement in aesthetics scores achieved by the DRAGON model compared to a baseline model.  The x-axis represents the aesthetics score of the baseline model, while the y-axis represents the difference in aesthetics scores between the DRAGON model and the baseline model. Each point represents a single generated example.  The plot visualizes how much DRAGON improves upon the baseline across a range of baseline scores. A positive y-value means DRAGON yielded better aesthetics than the baseline, while a negative y-value indicates that the baseline was better.", "section": "5.2 Optimizing Instance-Level Rewards \u2013 Predicted Aesthetics Score and CLAP Score"}, {"figure_path": "https://arxiv.org/html/2504.15217/x13.png", "caption": "(c) Aesthetics improvement vs DRAGON score.", "description": "This figure shows the improvement in aesthetics scores achieved by DRAGON compared to the baseline model's scores.  The x-axis represents the aesthetics scores generated by DRAGON, while the y-axis represents the difference in aesthetics scores between DRAGON and the baseline model. Each point on the graph corresponds to a single generated music sample. The plot visually demonstrates how DRAGON's improvement in aesthetics score relates to its own generation quality.", "section": "5.2 Optimizing Instance-Level Rewards \u2013 Predicted Aesthetics Score and CLAP Score"}, {"figure_path": "https://arxiv.org/html/2504.15217/x14.png", "caption": "(d) Aesthetics score histogram.", "description": "This histogram visualizes the distribution of the aesthetics scores assigned by the human raters in the DMA dataset. It shows the frequency of each score (1-5) in the dataset. This allows for analyzing the overall distribution and the concentration of scores in certain ranges which informs the nature and difficulty of the dataset used in evaluating human-perceived music quality.", "section": "5.2 Optimizing Instance-Level Rewards - Predicted Aesthetics Score and CLAP Score"}, {"figure_path": "https://arxiv.org/html/2504.15217/x15.png", "caption": "Figure 7: When optimizing aesthetics score, DRAGON improves low to medium-quality examples the most.", "description": "This figure visualizes the results of optimizing the aesthetics score using the DRAGON model.  It shows that DRAGON primarily improves the quality of audio generations that were initially rated as low or medium quality by the aesthetics model, while the improvement for already high-quality generations is less pronounced.  This suggests that the DRAGON model is most effective in refining those generations that need the most improvement, rather than simply making already good quality generations slightly better.", "section": "5.2 Optimizing Instance-Level Rewards - Predicted Aesthetics Score and CLAP Score"}]