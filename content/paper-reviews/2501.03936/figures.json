[{"figure_path": "https://arxiv.org/html/2501.03936/x1.png", "caption": "Figure 1: Comparison between our PPTPPTAgent approach (left) and the conventional abstractive summarization method (right). Our method, which begins by editing a reference slide, aligns more closely with the human presentation creation process.", "description": "Figure 1 illustrates the difference between the proposed PPTAgent method and traditional abstractive summarization techniques in presentation generation.  The left side shows PPTAgent's approach, which starts by using an existing presentation slide as a template and making edits. This approach is closer to how humans create presentations, involving iterative refinement. The right side depicts the conventional method, which generates a presentation directly from text, often leading to less engaging visual results and a poor structural coherence. The figure highlights that PPTAgent's edit-based approach better accounts for visual design and structural coherence.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2501.03936/x2.png", "caption": "Figure 2: Overview of the PPTPPTAgent workflow. Stage I: Presentation Analysis involves analyzing the input presentation to cluster slides into groups and extract their content schemas. Stage II: Presentation Generation generates new presentations guided by the outline, incorporating feedback mechanisms to ensure robustness.", "description": "PPTAgent is a two-stage process.  Stage I, Presentation Analysis, takes an input presentation and analyzes its slides.  It groups similar slides together and extracts the content schema from each group.  Stage II, Presentation Generation, uses this information to generate a new presentation.  It does this by first creating an outline, then generating slides based on this outline and the content schemas.  A feedback mechanism is incorporated to enhance robustness.", "section": "2 PPTAgent"}, {"figure_path": "https://arxiv.org/html/2501.03936/x3.png", "caption": "Figure 3: This figure illustrates the evaluation process in PPTPPTEval, which assesses three key dimensions: content, design, and coherence. Content evaluates the quality of text and images within the slides. Design examines the visual consistency and appeal. Coherence focuses on the logical flow of the presentation. Each dimension is rated on a scale from 1 to 5, with detailed feedback provided for improvement.", "description": "The figure illustrates the three-dimensional evaluation framework, PPT PPTEval, used to assess the quality of automatically generated presentations.  It evaluates presentations across Content (quality of text and images), Design (visual consistency and appeal), and Coherence (logical flow). Each dimension is scored on a scale of 1 to 5, with detailed feedback provided to support improvement.  The diagram shows the evaluation process, including an MLLM judge providing scores and feedback based on the three dimensions.", "section": "3 PPTEval"}, {"figure_path": "https://arxiv.org/html/2501.03936/x4.png", "caption": "Figure 4: The number of iterative self-corrections required to generate a single slide under different models.", "description": "This figure displays the number of iterative self-correction steps needed to generate a single slide using three different large language models (LLMs): GPT-40, Qwen2.5, and Qwen2-VL.  The x-axis represents the iteration number (0, 1, or 2), indicating the number of times the model's output was refined through a self-correction mechanism. The y-axis shows the number of slides requiring a given number of iterations.  Each bar represents a different model, showing how often each model successfully generated a slide within a specified number of iterations and how many times it failed (requiring more than two iterations). This visual comparison helps to assess the robustness and efficiency of each model's self-correction process during slide generation.", "section": "4.6 Error Analysis"}, {"figure_path": "https://arxiv.org/html/2501.03936/x5.png", "caption": "Figure 5: Correlation heatmap between existing automated evaluation metrics and PPTPPTEval.", "description": "This heatmap visualizes the correlation coefficients between the scores of existing automated metrics (perplexity, Fr\u00e9chet Inception Distance (FID), and human-like evaluation scores) and the three dimensions of the newly proposed PPT Eval metric (Content, Design, and Coherence) for evaluating presentation quality.  Each cell's color intensity represents the strength and direction of the correlation; darker shades of red indicate strong positive correlations, while darker shades of blue indicate strong negative correlations. The figure helps to assess how well established automated metrics align with the more nuanced human judgment captured by the PPT Eval framework.", "section": "4.5 Ablation Study"}, {"figure_path": "https://arxiv.org/html/2501.03936/x6.png", "caption": "Figure 6: Scoring Examples of PPTPPTEval.", "description": "Figure 6 shows example scores and justifications from human evaluators using the PPTEval framework.  Each example demonstrates how different aspects of a presentation (content, design) are assessed and scored on a 1-5 scale.  The examples highlight the criteria used in judging content quality (impact, clarity, image support), visual design (color harmony, visual elements), and overall coherence (logical flow, contextual information). The variety of scores and justifications illustrate the nuanced evaluation capabilities of PPTEval.", "section": "3 PPTEval"}, {"figure_path": "https://arxiv.org/html/2501.03936/x7.png", "caption": "Figure 7: Example of slide clusters.", "description": "This figure shows examples of different slide clusters generated by the PPTAgent's slide clustering algorithm.  Each cluster represents a group of slides with similar functionalities or visual characteristics. The clustering helps the PPTAgent to understand the structure and content patterns of the reference presentation, facilitating the generation of a new presentation that is more coherent and visually consistent.", "section": "2.2 Stage I: Presentation Analysis"}, {"figure_path": "https://arxiv.org/html/2501.03936/extracted/6108346/figures/ray-so-export-6.png", "caption": "Figure 8: Example of rendering a slide into HTML format.", "description": "This figure shows an example of how a PowerPoint slide is converted into an HTML format.  This conversion is a crucial step in the PPTAgent workflow, as it allows Large Language Models (LLMs) to more easily process and interpret the slide's structure and content for editing and generation purposes. The HTML representation provides a structured, text-based view of the slide's elements, making it easier for the LLMs to understand and manipulate the content using their coding abilities.  This is in contrast to directly working with the complex XML format of PowerPoint files.", "section": "2 PPTAgent"}, {"figure_path": "https://arxiv.org/html/2501.03936/x8.png", "caption": "Figure 9: Illustration of the prompt used for clustering structural slides.", "description": "This figure shows the prompt used in the PPTAgent framework to instruct a Large Language Model (LLM) to identify and categorize structural slides within a presentation.  Structural slides are those which define the overall presentation structure, such as opening slides, table of contents, section headers, and closing slides. The prompt guides the LLM to differentiate these structural slides from content slides based on their function and to provide a JSON output categorizing each slide appropriately. This step is crucial in the first stage of PPTAgent, where the LLM analyzes the reference presentation to understand its structure before generating a new presentation.", "section": "2.2 Stage I: Presentation Analysis"}, {"figure_path": "https://arxiv.org/html/2501.03936/x9.png", "caption": "Figure 10: Illustration of the prompt used to infer layout patterns.", "description": "This figure shows the prompt used in the PPTAgent framework for layout analysis.  The prompt instructs a language model to analyze the content layout and media types within provided slide images. The goal is to generate a concise title that describes the presentation pattern and structural arrangement of content elements without referencing specific topics or content. The prompt emphasizes focusing on *how* content is structured and presented, not *what* the content is, and provides examples of appropriate output.", "section": "2 PPTAgent"}, {"figure_path": "https://arxiv.org/html/2501.03936/x10.png", "caption": "Figure 11: Illustration of the prompt used to extract the slide schema.", "description": "This figure shows the prompt used in the PPTAgent framework to extract the slide schema.  The prompt instructs the model to analyze the slide elements and generate a structured JSON schema that identifies key content elements (text and images), their purpose, and the actual text or image data.  This schema is crucial for the next stage of presentation generation where the model will use this information to create new slides.", "section": "2 PPTAgent"}, {"figure_path": "https://arxiv.org/html/2501.03936/x11.png", "caption": "Figure 12: Illustration of the prompt used for generating the outline.", "description": "This figure shows the prompt used in the PPTAgent framework to generate the presentation outline.  The prompt instructs the LLM to create a structured outline containing multiple entries, each specifying a reference slide, relevant document sections, a slide title, and a description. The LLM uses planning and summarizing capabilities, along with input from both the document and semantic information extracted from the reference presentation, to ensure a coherent and engaging outline for the new presentation. This outline then guides the subsequent slide generation process.", "section": "2 PPTAgent"}, {"figure_path": "https://arxiv.org/html/2501.03936/x12.png", "caption": "Figure 13: Illustration of the prompt used for generating slide content.", "description": "This figure shows the prompt used to instruct a large language model (LLM) to generate slide content. The prompt details the requirements and rules the LLM must follow.  These include using concise and impactful presentation styles, deriving content only from provided reference text and image information, following default quantities for elements, and ensuring that generated text meets character limits. The LLM is also instructed on extracting key content from reference text, including supporting images.  Additionally, the prompt specifies how to handle generating supporting elements (e.g., presenter and logo information) and outputting the final content in a structured JSON format.", "section": "2.3 Stage II: Presentation Generation"}, {"figure_path": "https://arxiv.org/html/2501.03936/x13.png", "caption": "Figure 14: Illustration of the prompt used for generating editing actions.", "description": "This figure shows the prompt used to instruct a large language model (LLM) on how to generate a sequence of API calls to edit slides.  The prompt details the rules for quantity adjustment (adding or removing content using cloning or deletion), content replacement (modifying text or images), and output formatting (including comments explaining actions and handling of specific elements like paragraphs and images). The LLM uses these rules to transform a set of commands into executable code for slide editing.", "section": "2.3 Stage II: Presentation Generation"}, {"figure_path": "https://arxiv.org/html/2501.03936/x14.png", "caption": "Figure 15: Illustration of the prompt used to describe content in PPTEval.", "description": "This figure shows the prompt used in the PPTEval framework to evaluate the content quality of a presentation slide.  The prompt instructs the evaluator to assess the slide based on three criteria: (1) amount of information (whether the slide provides too little or too much information); (2) content clarity and language quality (checking for grammatical errors, unclear expressions, and overall organization); and (3) image relevance (evaluating the presence and appropriateness of images in relation to the slide's content).  The evaluator is asked to provide an objective, concise description and a score (on a five-point scale) for each criterion.", "section": "3 PPTEval"}, {"figure_path": "https://arxiv.org/html/2501.03936/x15.png", "caption": "Figure 16: Illustration of the prompt used to describe style in PPTEval.", "description": "This figure shows the prompt used in the PPTEval framework to evaluate the style of a presentation slide.  The prompt instructs the evaluator to assess three aspects of the slide's style: visual consistency (checking for readability issues like overlapping elements or low contrast), color scheme (determining if it's monochromatic or colorful), and the use of visual elements (such as icons, images, or geometric shapes). A five-point scoring rubric is provided to guide the evaluation, ensuring objective and consistent assessment of the slide's visual appeal.", "section": "3 PPTEval"}, {"figure_path": "https://arxiv.org/html/2501.03936/x16.png", "caption": "Figure 17: Illustration of the prompt used to extract content in PPTEval.", "description": "This figure shows the detailed instructions given to the large language model (LLM) for extracting content information from a presentation during the evaluation phase of PPT-Eval.  The prompt instructs the LLM to summarize the content of each slide and extract presentation metadata (such as the speaker, date, etc.) from the opening and closing slides. The expected output is a JSON object containing the summarized content for each slide and a \"background\" field for the extracted metadata.", "section": "3 PPTEval"}, {"figure_path": "https://arxiv.org/html/2501.03936/x17.png", "caption": "Figure 18: Illustration of the prompt used to evaluate content in PPTEval.", "description": "This figure shows the prompt used in the PPTEval framework to evaluate the content quality of a presentation slide. The prompt provides detailed scoring criteria on a five-point scale, with specific descriptions for each level (1-point to 5-points).  It guides the evaluator to assess aspects like grammatical errors, clarity of text, structure of content, and the relevance of any images to the overall message. The goal is to obtain an objective and concise evaluation focusing exclusively on the specified dimensions.", "section": "3 PPTEval"}, {"figure_path": "https://arxiv.org/html/2501.03936/x18.png", "caption": "Figure 19: Illustration of the prompt used to evaluate style in PPTEval.", "description": "This figure shows the prompt used in the PPTEval framework to assess the style of a presentation slide.  The prompt instructs the evaluator to judge the slide's visual appeal based on three aspects: visual consistency (absence of readability issues like border overflow or low contrast), color scheme (monochromatic or colorful), and use of visual elements (icons, backgrounds, images, or geometric shapes).  A five-point scoring system is provided, with detailed descriptions of what constitutes each score level. The goal is to provide a structured and consistent evaluation of the presentation's visual design.", "section": "3 PPTEval"}, {"figure_path": "https://arxiv.org/html/2501.03936/x19.png", "caption": "Figure 20: Illustration of the prompt used to evaluate coherence in PPTEval.", "description": "This figure displays the prompt used within the PPTEval framework to assess the coherence of a presentation.  The prompt instructs the evaluator to consider the logical flow and contextual information of the presentation, assigning a score based on five levels of quality: Poor, Fair, Average, Good, and Excellent.  Each score level has specific criteria that must be met for that rating to be assigned.  The prompt aims to ensure a consistent and fair evaluation of the presentation's narrative structure and organization.", "section": "3 PPTEval"}]