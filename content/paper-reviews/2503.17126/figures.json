[{"figure_path": "https://arxiv.org/html/2503.17126/x1.png", "caption": "Figure 1: Our post-training approach to diversify creative writing generation while maintaining quality.", "description": "This figure illustrates a post-training approach for enhancing the diversity of creative writing generated by large language models (LLMs) while preserving output quality.  The process starts with a prompt, which receives multiple responses.  A deviation score is calculated for each response, measuring its difference from other responses to the same prompt.  The responses are then fed into either Direct Preference Optimization (DPO) or Odds Ratio Preference Optimization (ORPO), with the loss for each response pair weighted by the winning response's deviation score.  This weighting emphasizes learning from rare, high-quality responses, thus promoting both diversity and quality in the final LLM output.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2503.17126/x2.png", "caption": "Figure 2: Results on writing quality (reddit-reward, x\ud835\udc65xitalic_x axes) and diversity (semantic or style diversity, y\ud835\udc66yitalic_y axes). Error bars in this paper indicate 95% confidence intervals.", "description": "Figure 2 presents a comparative analysis of various large language model (LLM) training methods in terms of writing quality and diversity. The x-axis represents the writing quality measured using a reddit-reward model, indicating higher scores for better quality writing. The y-axis represents the diversity of the generated writing samples, categorized into semantic and style diversity.  Each point represents the performance of a specific model (e.g. DPO, DDPO, and ORPO) trained using different approaches. The error bars indicate the 95% confidence intervals, showing the uncertainty in the measurements. This figure helps understand the trade-offs between writing quality and diversity achieved by different model training methods. The results demonstrate the effectiveness of the diversified optimization methods (DDPO and DORPO) in improving diversity without significantly reducing quality.", "section": "5.2 Results"}, {"figure_path": "https://arxiv.org/html/2503.17126/x3.png", "caption": "Figure 3: Ablation results by varying the maximum number of responses per prompt. When the maximum number of responses is four, we also experimented with 1) setting a minimum \u03b4\ud835\udeff\\deltaitalic_\u03b4 and 2) using high-quality responses.", "description": "This figure displays the results of an ablation study that investigates the impact of varying the maximum number of responses per prompt on the performance of the proposed diversified DPO and ORPO models.  The x-axis represents the maximum number of responses considered per prompt (4, 6, 8, 12, or all).  Three separate y-axes show the model's performance in terms of: (1) Reddit reward (quality), (2) Semantic diversity, and (3) Style diversity.  The figure reveals the robustness of the proposed approach across different numbers of responses per prompt. Importantly, it also shows additional experiments performed when the maximum number of responses per prompt is four. These supplemental experiments involve (1) enforcing a minimum deviation threshold and (2) using only high-quality training responses to analyze their effect on the quality and diversity of the outputs.", "section": "7 Ablation and Comparison to DivPO"}, {"figure_path": "https://arxiv.org/html/2503.17126/x4.png", "caption": "Figure 4: Llama-3.1-8B results on compression ratio, homogenization score, and n-gram diversity score.", "description": "This figure displays the results obtained using Llama-3.1-8B model, focusing on three distinct metrics: compression ratio, homogenization score, and n-gram diversity score.  Each metric provides a unique perspective on the diversity and quality of text generated by the model.  The compression ratio indicates how well the generated text can be compressed, with lower scores suggesting higher diversity. The homogenization score measures the similarity between generated text samples, and lower scores imply higher diversity. Finally, the n-gram diversity score assesses the diversity at the n-gram level.  The plot visually compares the performance of several model variants and baselines across these three metrics.", "section": "5.2 Results"}, {"figure_path": "https://arxiv.org/html/2503.17126/x5.png", "caption": "Figure 5: Mistral-7B-v0.3 results on compression ratio, homogenization score, and n-gram diversity score.", "description": "This figure presents the results of evaluating Mistral-7B-v0.3, a large language model, across three different diversity metrics: compression ratio, homogenization score, and n-gram diversity score.  The compression ratio measures the effectiveness of compression algorithms on the generated text, indicating the level of redundancy or repetitiveness.  A lower score suggests higher diversity. The homogenization score quantifies the similarity between text sequences within the generated outputs; a lower score implies more diverse outputs. Finally, n-gram diversity score assesses the variety of n-grams (sequences of n words) in the generated text. Higher score indicates higher diversity.  The figure likely displays these scores for various model settings or conditions, allowing for a comparison of their impact on text diversity.", "section": "5.2 Results"}, {"figure_path": "https://arxiv.org/html/2503.17126/extracted/6292296/figures/divtuning_humaneval_interface.png", "caption": "Figure 6: Human evaluation interface.", "description": "The figure shows the interface used for human evaluation in the study.  It presents two sets of story summaries, labeled Set A and Set B, each containing four summaries.  Evaluators were asked to choose which set contained the most interesting/high-quality story, which set had more diverse stories, and could optionally provide comments. The interface was designed to simplify the evaluation process by showing concise information.", "section": "Human Evaluation"}]