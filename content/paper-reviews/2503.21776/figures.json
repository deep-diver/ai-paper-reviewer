[{"figure_path": "https://arxiv.org/html/2503.21776/x1.png", "caption": "Figure 1: \nReasoning paths of Video-R1 trained by GRPO and our proposed T-GRPO on test samples. Without explicit temporal modeling, models may learn sub-optimal video reasoning patterns by taking shortcuts, therefore failing to generalize well.", "description": "This figure illustrates the reasoning processes of the Video-R1 model trained with two different algorithms: GRPO and the proposed T-GRPO.  The example demonstrates how a model without explicit temporal modeling (GRPO) may take shortcuts and arrive at an incorrect answer by focusing on a single frame instead of considering the temporal evolution of events in the video. In contrast, the model trained with T-GRPO shows a more thorough and accurate reasoning process by considering the temporal relationships between frames. This highlights the importance of incorporating temporal information for robust video reasoning.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2503.21776/x2.png", "caption": "Figure 2: \nThe data distribution of our Video-R1-260k dataset.", "description": "Figure 2 shows the composition of the Video-R1-260k dataset used for training the Video-R1 model.  The dataset is a mixture of image and video data, categorized into six types: General Video (everyday scenarios), General Image (general question answering), Chart (reasoning over charts and graphs), OCR (text recognition from images), Math (math problem solving from images), and Spatial (spatial reasoning tasks).  The percentage of each data type within the Video-R1-260k dataset is visually represented in a pie chart.  This diverse dataset is designed to provide the model with a wide range of reasoning tasks and challenges, spanning various complexities and modalities.", "section": "3.1 Dataset Construction"}, {"figure_path": "https://arxiv.org/html/2503.21776/x3.png", "caption": "Figure 3: \nAn example of Video-R1-7B\u2019s reasoning output on MMVU Benchmark.", "description": "This figure showcases an example of Video-R1-7B's reasoning process on the MMVU benchmark.  The model is presented with a video and a question: \"Which move motion in the video loses the system energy?\" The model's response includes step-by-step reasoning. It analyzes each of three depicted movements (pool, bowling, car crash), deducing that only the car crash involves a loss of system energy due to dissipation of kinetic energy. This demonstrates the model's capability to perform temporal reasoning and integrate visual and physics-based knowledge for logical deduction.  The model revisits and confirms its answer through multiple iterations of thought, highlighting a \"self-reflection reasoning\" behavior. This multi-stage reasoning process and its explanation are captured and presented in the figure.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2503.21776/x4.png", "caption": "Figure 4: \nAn example of Video-R1-7B\u2019s reasoning output on VSI-Bench.", "description": "This figure shows an example of Video-R1-7B's reasoning process on the VSI-Bench benchmark.  The task involves a robot navigation problem where the robot needs to reach a bathroom sink from a starting point. The model generates a chain of thought to determine the correct sequence of actions (turn right, go forward, turn left, go forward)  to reach the destination, demonstrating its ability to perform spatial and logical reasoning in a video context. The model demonstrates a self-reflection reasoning behavior in the process, showing its ability to revise its initial plan, identify inconsistencies, and ultimately reach a more accurate conclusion.", "section": "Experiments"}]