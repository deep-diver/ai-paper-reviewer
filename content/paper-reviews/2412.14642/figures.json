[{"figure_path": "https://arxiv.org/html/2412.14642/x1.png", "caption": "Figure 1: Comparison of Text-Based Targeted Molecule Generation (a) v.s. Text-Based Open Molecule Generation (b).", "description": "Figure 1 contrasts two approaches to molecule generation using text. Panel (a) shows targeted generation, where the goal is to generate one specific molecule from a textual description. There's only one correct answer. Panel (b) illustrates open generation, where a textual description can lead to many correct answers, testing the model's flexibility and understanding of the chemical space.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2412.14642/x2.png", "caption": "Figure 2: Data construction workflow and evaluation process of TOMG-Bench.", "description": "This figure illustrates the workflow and evaluation process of the TOMG-Bench benchmark.  The workflow begins with the selection of molecules from the Zinc-250k database, categorized by their functional groups and fundamental properties. These molecules then serve as inputs for three main tasks: molecule editing (MolEdit), molecule optimization (MolOpt), and customized molecule generation (MolCustom). Each task involves several subtasks, as detailed in the paper. The LLM processes these molecular inputs following prompts specified for each task.  RDKit, a molecular toolbox, is used to evaluate the generated molecules, assessing metrics like accuracy, novelty, similarity, and validity. The final evaluation metrics provide a comprehensive evaluation of the LLM's performance on open molecule generation tasks. ", "section": "3 TOMG-Bench"}, {"figure_path": "https://arxiv.org/html/2412.14642/extracted/6081791/figures/performance.png", "caption": "Figure 3: The performance of models benchmarked in TOMG-Bench. In TOMG-Bench, LLMs are divided into 4 categories: Proprietary Models, Open-source General LLMs, Open-source ChEBI-20 Fine-tuned LLMs, and OpenMolIns Fine-tuned LLMs. Models whose parameters are known are plotted as dots, while models of unknown parameters are denoted as horizontal lines.", "description": "Figure 3 presents a comparison of the performance of various large language models (LLMs) on the TOMG-Bench benchmark.  The x-axis shows the number of parameters (in billions) for each model, while the y-axis represents the weighted average accuracy achieved on the benchmark. The LLMs are categorized into four groups: proprietary models, open-source general LLMs, open-source LLMs fine-tuned on the ChEBI-20 dataset, and open-source LLMs fine-tuned on the OpenMolIns dataset. Models with known parameter counts are represented as individual data points, while those with unknown parameter counts are shown as horizontal lines. This visualization allows for a direct comparison of model performance across different architectures and training regimes, highlighting the relative strengths and weaknesses of each category of LLMs on the TOMG-Bench.", "section": "4 Results"}]