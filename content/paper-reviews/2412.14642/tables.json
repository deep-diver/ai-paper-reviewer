[{"content": "| Item | Data Size |\n|---|---| \n| TOMG-Bench |  |\n| subtask | 5,000 |\n| OpenMolIns |  |\n| light | 4,500 |\n| small | 18,000 |\n| medium | 45,000 |\n| large | 90,000 |\n| xlarge | 1,200,000 |", "caption": "Table 1: Statisics of TOMG-Bench and OpenMolIns.", "description": "This table presents the dataset statistics for both the TOMG-Bench benchmark and the OpenMolIns instruction tuning dataset.  TOMG-Bench is composed of three main tasks (molecule editing, molecule optimization, and custom molecule generation), each with three subtasks, each containing 5000 test samples. OpenMolIns is an instruction-tuning dataset designed to complement TOMG-Bench, providing data at five different scales (light, small, medium, large, and extra-large) to assess the effect of data size on model performance. The table shows the number of data samples associated with each subtask in TOMG-Bench and each data scale in OpenMolIns.", "section": "3 TOMG-Bench"}, {"content": "| Prompt Templates for MolEdit |\n|---|---|---|---|\n| *AddComponent* | Please add a {} to the molecule {}. | Modify the molecule {} by adding a {}. | Add a {} to the molecule {}. |\n| *DelComponent* | Please remove a {} from the molecule {}. | Modify the molecule {} by removing a {}. | Remove a {} from the molecule {}. |\n| *SubComponent* | Please substitute a {} in the molecule {} by {}. | Modify the molecule {} by replacing a {} by {}. | Replace a {} in the molecule {} by {}. | Please replace a {} in the molecule {} with {}. | Modify the molecule {} by substituting a {} with {}. | Substitute a {} in the molecule {} with {}. |", "caption": "Table 2: Prompt Templates for MolEdit", "description": "This table presents the prompt templates used for the molecule editing task in the TOMG-Bench benchmark.  The molecule editing task is divided into three subtasks: AddComponent, DelComponent, and SubComponent. Each subtask has multiple variations of prompt templates designed to instruct the large language model (LLM) to perform specific molecule modifications.  These variations help to ensure robust evaluation and reduce bias stemming from the wording of instructions.", "section": "3.1 Dataset Categorization"}, {"content": "| Functional Group | benzene ring | hydroxyl | aldehyde | carboxyl | amide |\n|---|---|---|---|---|---| \n| Weights | 15 | 15 | 5 | 5 | 10 |\n| Functional Group | amine | nitro | halo | nitrile | thiol |\n| Weights | 5 | 5 | 5 | 1 | 1 |", "caption": "Table 3: Functional Groups that are considered in AddComponent and DelComponent, as well as their weights to be selected in AddComponent.", "description": "This table lists the functional groups used in the AddComponent and DelComponent subtasks of the MolEdit task within the TOMG-Bench benchmark.  For AddComponent, it shows the probability (weight) of each functional group being selected when a molecule is modified by adding a functional group.  The weights reflect the relative frequency of these functional groups in real-world molecules, aiming for a more realistic simulation of molecule editing scenarios.", "section": "3.1 Dataset Categorization"}, {"content": "| Prompt Templates for MolOpt |\n|---|---|---|---|\n| *LogP* | Please optimize the molecule {} to have a lower/higher LogP value. | Modify the molecule {} to decrease/increase its LogP value. | Optimize the molecule {} to have a lower/higher LogP value. |\n|  | Please modify the molecule {} to decrease/increase its LogP value. | Modify the molecule {} to have a lower/higher LogP value. |  |\n| *MR* | Please optimize the molecule {} to have a lower/higher MR value. | Modify the molecule {} to decrease/increase its MR value. | Optimize the molecule {} to have a lower/higher MR value. |\n|  | Please modify the molecule {} to decrease/increase its MR value. | Modify the molecule {} to have a lower/higher MR value. |  |\n| *QED* | Please optimize the molecule {} to have a lower/higher QED value. | Modify the molecule {} to decrease/increase its QED value. | Optimize the molecule {} to have a lower/higher QED value. |\n|  | Please modify the molecule {} to decrease/increase its QED value. | Modify the molecule {} to have a lower/higher QED value. |  |\n", "caption": "Table 4: Prompt Templates for MolOpt", "description": "This table presents the prompt templates used in the MolOpt task of the TOMG-Bench benchmark. MolOpt focuses on optimizing molecular properties (LogP, MR, and QED) by modifying molecular structures. The table provides multiple variations of prompts for each property (LogP, MR, and QED), instructing LLMs to either increase or decrease the specified property value. These variations aim to comprehensively evaluate the LLMs' ability to optimize molecules based on different instructions.", "section": "3.1 Dataset Categorization"}, {"content": "| Prompt Templates for MolCustom |\n|---|---|---|---|---|\n| _AtomNum_ | Please generate a molecule with {} atom(s). | Please generate a molecule composed of {} atom(s). | Please generate a molecule consisting {} atom(s). | The molecule has {} atom(s). |\n|  | The molecule is composed of {} atom(s). | The molecule consists of {} atom(s). | There is a molecule with {} atom(s). | There is a molecule composed of {} atom(s). |\n|  | There is a molecule consisting of {} atom(s). | The molecule contains {} atom(s). |  _BondNum_ | Please generate a molecule with {} bond(s). |\n|  | Please generate a molecule composed of {} bond(s). | Please generate a molecule consisting {} bond(s). | The molecule has {} bond(s). | The molecule is composed of {} bond(s). |\n|  | The molecule consists of {} bond(s). | There is a molecule with {} bond(s). | There is a molecule composed of {} bond(s). | There is a molecule consisting of {} bond(s). |\n|  | The molecule contains {} bond(s). | _FunctionalGroup_ | Please generate a molecule with {} group(s). | Please generate a molecule composed of {} group(s). |\n|  | Please generate a molecule consisting {} group(s). | The molecule has {} group(s). | The molecule is composed of {} group(s). | The molecule consists of {} group(s). |\n|  | There is a molecule with {} group(s). | There is a molecule composed of {} group(s). | There is a molecule consisting of {} group(s). | The molecule contains {} group(s). |", "caption": "Table 5: Prompt Templates for MolCustom", "description": "This table presents the various prompt templates used in the MolCustom task of the TOMG-Bench benchmark.  The MolCustom task focuses on generating molecules with specific constraints.  The prompts are designed to test the model's ability to generate molecules based on different criteria such as the number of atoms, bonds, or functional groups.  Each prompt template provides slightly different wording to assess how variations in instruction affect the LLM's performance.", "section": "3.1 Dataset Categorization"}, {"content": "| Atom | carbon | oxygen | nitrogen | sulfur | fluorine | chlorine | bromine | iodine | phosphorus |\n|---|---|---|---|---|---|---|---|---|---| \n| Weights | [Mandatory] | 5 | 3 | 3 | 2 | 2 | 2 | 2 | 1 |\n| Atom | boron | silicon | selenium | tellurium | arsenic | antimony | bismuth | polonium |  |\n| Weights | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 |  |", "caption": "Table 6: Atoms that are considered in AtomNum, as well as their weights to be selected.", "description": "This table lists the atoms considered when generating molecules for the AtomNum subtask within the MolCustom task of the TOMG-Bench benchmark.  Each atom is assigned a weight reflecting its relative frequency in real-world molecules. Carbon is mandatory, with other atoms having varying weights.", "section": "3.1 Dataset Categorization"}, {"content": "| Bond | single | double | triple | rotatable | aromatic |\n|---|---|---|---|---|---| \n| Weights | 5 | 4 | 3 | 1 | 1 |", "caption": "Table 7: Chemical bonds that are considered in BondNum, as well as their weights to be selected.", "description": "This table lists the types of chemical bonds considered when generating molecules for the BondNum subtask in the MolCustom task of the TOMG-Bench benchmark.  For each bond type (single, double, triple, rotatable, aromatic), a weight is assigned, indicating the relative likelihood of that bond type being selected during molecule generation.  These weights influence the diversity and types of molecules produced by the model.", "section": "3.2 Dataset Construction"}, {"content": "| Functional Group | benzene ring | hydroxyl | anhydride | aldehyde | ketone | carboxyl | ester | amide | amine | nitro |\n|---|---|---|---|---|---|---|---|---|---|---|\n| Weights | 15 | 15 | 2 | 5 | 5 | 10 | 5 | 5 | 5 | 2 |\n| Functional Group | halo | thioether | nitrile | thiol | sulfide | disulfide | sulfoxide | sulfone | borane |  |\n|---|---|---|---|---|---|---|---|---|---|---|\n| Weights | 2 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 |  |", "caption": "Table 8: Functional Groups that are considered in FunctionalGroup, as well as their weights to be selected.", "description": "This table lists the functional groups considered for the 'FunctionalGroup' subtask within the 'MolCustom' task of the TOMG-Bench benchmark.  It shows each functional group and the corresponding weight assigned for its selection during data generation. Higher weights indicate a greater probability of a functional group being selected.  The weights reflect an attempt to mimic the distribution of these functional groups in real-world datasets.", "section": "A.3 MolCustom"}, {"content": "| Item | Value |\n|---|---| \n| Generation |  |\n| temperature | 0.75 |\n| top_p | 0.85 |\n| num_beams | 1 |\n| max_new_tokens | 512 |\n| Instruction Tuning |  |\n| batchsize | 32 |\n| lr | 3e-4 |\n| cutoff_len | 1024 |\n| Lora Settings |  |\n| r | 64 |\n| \u03b1 | 128 |\n| dropout | 0.1 |", "caption": "Table 9: Hyper-parameters", "description": "This table lists the hyperparameters used in the experiments for training and evaluating the language models.  It includes values for generation parameters (temperature, top_p, num_beams, max_new_tokens) and instruction tuning parameters (batchsize, learning rate, cutoff_len) as well as LoRA (Low-Rank Adaptation) settings (r, alpha, dropout). These parameters control various aspects of the model's behavior during training and inference, such as randomness, speed, and the quality of generated molecules.", "section": "4.1 Setup"}, {"content": "| Model | #Parameters (B) | \\bar{Acc} (%) | \\bar{wAcc}(\n%) | \n|---|---|---|---| \n| Claude-3.5 Anthropic (2024b) | - | 51.10 | 35.92 | \n| Gemini-1.5-pro Deepmind (2024) | - | 52.25 | 34.80 | \n| GPT-4-turbo Achiam et al. (2023) | - | 50.74 | 34.23 | \n| GPT-4o Achiam et al. (2023) | - | 49.08 | 32.29 | \n| Claude-3 Anthropic (2024a) | - | 46.14 | 30.47 | \n| OpenMolIns-large (Llama-3.1-8B) | 8 | 43.1 | 27.22 | \n| OpenMolIns-xlarge (Galactica-125M) | 0.125 | 44.48 | 25.73 | \n| Llama3-70B-Instruct (Int4) Dubey et al. (2024) | 70 | 38.54 | 23.93 | \n| OpenMolIns-large (Galactica-125M) | 0.125 | 39.28 | 23.42 | \n| OpenMolIns-medium (Galactica-125M) | 0.125 | 34.54 | 19.89 | \n| GPT-3.5-turbo Achiam et al. (2023) | - | 28.93 | 18.58 | \n| OpenMolIns-small (Galactica-125M) | 0.125 | 24.17 | 15.18 | \n| Llama3.1-8B-Instruct Dubey et al. (2024) | 8 | 26.26 | 14.09 | \n| Llama3-8B-Instruct Dubey et al. (2024) | 8 | 26.40 | 13.75 | \n| chatglm-9B GLM et al. (2024) | 9 | 18.50 | 13.13(7) | \n| OpenMolIns-light (Galactica-125M) | 0.125 | 20.95 | 13.13(6) | \n| OpenMolIns-large (Llama3.2-1B) | 1 | 14.11 | 8.10 | \n| yi-1.5-9B Young et al. (2024) | 9 | 14.10 | 7.32 | \n| Mistral-7B-Instruct-v0.2 Jiang et al. (2023) | 7 | 11.17 | 4.81 | \n| BioT5-base Pei et al. (2023) | 0.25 | 24.19 | 4.21 | \n| MolT5-large Edwards et al. (2022) | 0.78 | 23.11 | 2.89 | \n| Llama-3.1-1B-Instruct Dubey et al. (2024) | 1 | 3.95 | 1.99 | \n| MolT5-base Edwards et al. (2022) | 0.25 | 11.11 | 1.30(0) | \n| MolT5-small Edwards et al. (2022) | 0.08 | 11.55 | 1.29(9) | \n| Qwen2-7B-Instruct Yang et al. (2024) | 7 | 0.18 | 0.15 |", "caption": "Table 10: Leaderboard of TOMG-Benchmark.", "description": "This table presents the leaderboard of the Text-based Open Molecule Generation Benchmark (TOMG-Bench).  It ranks various large language models (LLMs) based on their weighted average accuracy across nine subtasks within the benchmark. The models are categorized into four groups: proprietary models (accessible via commercial APIs), open-source general LLMs, open-source LLMs fine-tuned on the ChEBI-20 dataset, and open-source LLMs fine-tuned on the OpenMolIns dataset.  The table shows the number of parameters (in billions) for each model and its corresponding weighted average accuracy, providing a comprehensive comparison of different LLMs' capabilities in open-domain molecule generation tasks.", "section": "4 Results"}, {"content": "| Models | AddComponent |  |  | DelComponent |  |  | SubComponent |  |  |\n|---|---|---|---|---|---|---|---|---|---|---|\n|  | Accuracy | Similarity | Validity | Accuracy | Similarity | Validity | Accuracy | Similarity | Validity |\n|---|---|---|---|---|---|---|---|---|---|---|\n| GPT-4o <cite class=\"ltx_cite ltx_citemacro_cite\">Achiam et al. (2023)</cite> | 0.6188 | 0.6782 | 0.7412 | 0.7012 | 0.6038 | 0.8474 | 0.7992 | 0.7225 | 0.9368 |\n| GPT-4-turbo <cite class=\"ltx_cite ltx_citemacro_cite\">Achiam et al. (2023)</cite> | 0.699 | 0.6936 | 0.7934 | 0.7244 | 0.5735 | 0.906 | 0.7778 | 0.7323 | 0.916 |\n| GPT-3.5-turbo <cite class=\"ltx_cite ltx_citemacro_cite\">Achiam et al. (2023)</cite> | 0.5832 | 0.6545 | 0.798 | 0.3082 | 0.7797 | 0.8468 | 0.2918 | 0.6333 | 0.6822 |\n| Claude-3.5 <cite class=\"ltx_cite ltx_citemacro_cite\">Anthropic (2024b)</cite> | 0.6832 | 0.7017 | 0.4414 | 0.5414 | 0.6678 | 0.796 | 0.8104 | 0.731 | 0.9588 |\n| Claude-3 <cite class=\"ltx_cite ltx_citemacro_cite\">Anthropic (2024a)</cite> | 0.6766 | 0.684 | 0.818 | 0.5556 | 0.6408 | 0.8984 | 0.655 | 0.7159 | 0.9184 |\n| Gemini-1.5-pro <cite class=\"ltx_cite ltx_citemacro_cite\">Deepmind (2024)</cite> | 0.7058 | 0.6792 | 0.8254 | 0.759 | 0.5949 | 0.9158 | 0.7148 | 0.7139 | 0.8684 |\n| Llama3-70B-Instruct (Int4) <cite class=\"ltx_cite ltx_citemacro_cite\">Dubey et al. (2024)</cite> | 0.5198 | 0.6801 | 0.5922 | 0.6122 | 0.5637 | 0.7182 | 0.5094 | 0.717 | 0.6822 |\n| Llama3-8B-Instruct <cite class=\"ltx_cite ltx_citemacro_cite\">Dubey et al. (2024)</cite> | 0.3914 | 0.6649 | 0.5374 | 0.4348 | 0.5058 | 0.57 | 0.2602 | 0.6841 | 0.4838 |\n| Llama3.1-8B-Instruct <cite class=\"ltx_cite ltx_citemacro_cite\">Dubey et al. (2024)</cite> | 0.2992 | 0.6088 | 0.4954 | 0.4336 | 0.5257 | 0.591 | 0.3401 | 0.6424 | 0.5076 |\n| Mistral-7B-Instruct-v0.2 <cite class=\"ltx_cite ltx_citemacro_cite\">Jiang et al. (2023)</cite> | 0.1868 | 0.6251 | 0.376 | 0.2018 | 0.3774 | 0.359 | 0.0602 | 0.6227 | 0.355 |\n| Qwen2-7B-Instruct <cite class=\"ltx_cite ltx_citemacro_cite\">Yang et al. (2024)</cite> | 0.001 | 0.2527 | 0.0036 | 0.0006 | 0.4024 | 0.0012 | 0.0004 | 0.2895 | 0.0068 |\n| Yi-1.5-9B <cite class=\"ltx_cite ltx_citemacro_cite\">Young et al. (2024)</cite> | 0.1742 | 0.417 | 0.4216 | 0.2858 | 0.5936 | 0.4909 | 0.137 | 0.4619 | 0.4368 |\n| Chatglm-9B <cite class=\"ltx_cite ltx_citemacro_cite\">GLM et al. (2024)</cite> | 0.2932 | 0.7622 | 0.5686 | 0.2956 | 0.7494 | 0.6914 | 0.1498 | 0.715 | 0.5084 |\n| Llama-3.2-1B-Instruct <cite class=\"ltx_cite ltx_citemacro_cite\">Dubey et al. (2024)</cite> | 0.0374 | 0.5343 | 0.1982 | 0.0768 | 0.575 | 0.3028 | 0.0102 | 0.3671 | 0.1468 |\n| MolT5-small <cite class=\"ltx_cite ltx_citemacro_cite\">Edwards et al. (2022)</cite> | 0.122 | 0.1027 | 0.449 | 0.1598 | 0.1125 | 0.4504 | 0.0708 | 0.1029 | 0.4876 |\n| MolT5-base <cite class=\"ltx_cite ltx_citemacro_cite\">Edwards et al. (2022)</cite> | 0.1354 | 0.1066 | 0.4686 | 0.1562 | 0.1144 | 0.4472 | 0.0584 | 0.1028 | 0.4426 |\n| MolT5-large <cite class=\"ltx_cite ltx_citemacro_cite\">Edwards et al. (2022)</cite> | 0.2834 | 0.1084 | 0.9282 | 0.2228 | 0.1201 | 0.9198 | 0.1692 | 0.0932 | 0.941 |\n| BioT5-base <cite class=\"ltx_cite ltx_citemacro_cite\">Pei et al. (2023)</cite> | 0.3462 | 0.1567 | 1 | 0.1668 | 0.1597 | 1 | 0.0684 | 0.1576 | 0.9998 |\n| OpenMolIns-large (Llama-3.2-1B) | 0.1756 | 0.5676 | 0.3216 | 0.1816 | 0.4963 | 0.2466 | 0.0844 | 0.5415 | 0.2958 |\n| OpenMolIns-large (Llama-3.1-8B) | 0.5822 | 0.6541 | 0.673 | 0.5104 | 0.5074 | 0.6896 | 0.544 | 0.6258 | 0.84 |\n| OpenMolIns-light (Galactica-125M) | 0.3786 | 0.5958 | 0.3786 | 0.2062 | 0.6521 | 0.7048 | 0.3102 | 0.5879 | 0.6674 |\n| OpenMolIns-small (Galactica-125M) | 0.3472 | 0.6172 | 0.5356 | 0.3258 | 0.6025 | 0.5758 | 0.2692 | 0.6181 | 0.5692 |\n| OpenMolIns-medium (Galactica-125M) | 0.4736 | 0.5682 | 0.7442 | 0.4886 | 0.5184 | 0.7488 | 0.3282 | 0.5975 | 0.6958 |\n| OpenMolIns-large (Galactica-125M) | 0.5866 | 0.5876 | 0.8228 | 0.6078 | 0.5577 | 0.7934 | 0.3438 | 0.6491 | 0.8438 |\n| OpenMolIns-xlarge (Galactica-125M) | 0.5842 | 0.5859 | 0.8438 | 0.6526 | 0.5084 | 0.8286 | 0.1872 | 0.6024 | 0.8538 |", "caption": "Table 11: Results on MolEdit. For each task, we highlight the best accuracy and underline the second best accuracy.", "description": "This table presents the results of the MolEdit task in the TOMG-Bench benchmark.  The MolEdit task evaluates the ability of Large Language Models (LLMs) to edit existing molecular structures.  The table shows the performance of various LLMs across three subtasks of MolEdit: AddComponent, DelComponent, and SubComponent.  For each subtask and each LLM, the table provides three metrics: Accuracy (the percentage of successfully edited molecules), Similarity (a measure of how similar the edited molecule is to the original), and Validity (the percentage of generated molecules that are chemically valid). The best accuracy for each task is highlighted, and the second-best accuracy is underlined.", "section": "3 TOMG-Bench"}, {"content": "| Models | LogP Accuracy | LogP Similarity | LogP Validity | MR Accuracy | MR Similarity | MR Validity | QED Accuracy | QED Similarity | QED Validity |\n|---|---|---|---|---|---|---|---|---|---| \n| GPT-4o [Achiam et al. (2023)] | 0.719 | 0.6586 | 0.8796 | 0.6864 | 0.642 | 0.8352 | 0.3952 | 0.618 | 0.857 |\n| GPT-4-turbo [Achiam et al. (2023)] | 0.7662 | 0.6984 | 0.9048 | 0.7388 | 0.6821 | 0.8848 | 0.3946 | 0.6587 | 0.905 |\n| GPT-3.5-turbo [Achiam et al. (2023)] | 0.4048 | 0.6327 | 0.854 | 0.412 | 0.6263 | 0.8486 | 0.3316 | 0.5635 | 0.8354 |\n| Claude-3.5 [Anthropic (2024b)] | 0.797 | 0.7124 | 0.9422 | 0.6962 | 0.7112 | 0.911 | 0.5361 | 0.7042 | 0.8604 |\n| Claude-3 [Anthropic (2024a)] | 0.7984 | 0.6067 | 0.9096 | 0.6094 | 0.6398 | 0.9062 | 0.4678 | 0.5855 | 0.9044 |\n| Gemini-1.5-pro [Deepmind (2024)] | 0.7712 | 0.7022 | 0.9274 | 0.7876 | 0.6744 | 0.8926 | 0.4704 | 0.6077 | 0.9484 |\n| Llama3-70B-Instruct (Int4) [Dubey et al. (2024)] | 0.5984 | 0.6028 | 0.6482 | 0.5684 | 0.6032 | 0.6272 | 0.2774 | 0.4828 | 0.634 |\n| Llama3-8B-Instruct [Dubey et al. (2024)] | 0.4642 | 0.3658 | 0.6086 | 0.4332 | 0.4793 | 0.5704 | 0.2568 | 0.4547 | 0.6112 |\n| Llama3.1-8B-Instruct [Dubey et al. (2024)] | 0.399 | 0.4235 | 0.5122 | 0.4164 | 0.483 | 0.5238 | 0.2655 | 0.4499 | 0.6158 |\n| Mistral-7B-Instruct-v0.2 [Jiang et al. (2023)] | 0.222 | 0.4501 | 0.2802 | 0.1908 | 0.2578 | 0.3795 | 0.121 | 0.3244 | 0.2532 |\n| Qwen2-7B-Instruct [Yang et al. (2024)] | 0 | 0.2923 | 0.0004 | 0.0002 | 0.4123 | 0.0004 | 0 | 0 | 0 |\n| Yi-1.5-9B | 0.2884 | 0.5461 | 0.4927 | 0.205 | 0.3724 | 0.4126 | 0.1064 | 0.6596 | 0.4526 |\n| Chatglm-9B | 0.3666 | 0.6902 | 0.4736 | 0.3514 | 0.682 | 0.5 | 0.1832 | 0.6506 | 0.4342 |\n| Llama-3.2-1B-Instruct [Dubey et al. (2024)] | 0.0644 | 0.5055 | 0.1664 | 0.0822 | 0.441 | 0.1604 | 0.0714 | 0.4757 | 0.1796 |\n| MolT5-small | 0.2158 | 0.1052 | 0.4302 | 0.2316 | 0.1011 | 0.442 | 0.2214 | 0.1031 | 0.4326 |\n| MolT5-base | 0.2074 | 0.1051 | 0.4168 | 0.1856 | 0.1073 | 0.3796 | 0.2358 | 0.1054 | 0.4536 |\n| MolT5-large | 0.4244 | 0.1015 | 0.8156 | 0.4496 | 0.1072 | 0.8678 | 0.4654 | 0.119 | 0.9214 |\n| BioT5-base | 0.5158 | 0.1526 | 1 | 0.506 | 0.1597 | 1 | 0.5068 | 0.158 | 1 |\n| OpenMolIns-large (Llama-3.2-1B) | 0.2898 | 0.5951 | 0.385 | 0.2644 | 0.5956 | 0.3678 | 0.1996 | 0.5849 | 0.349 |\n| OpenMolIns-large (Llama-3.1-8B) | 0.8054 | 0.6678 | 0.872 | 0.7122 | 0.6548 | 0.8514 | 0.5224 | 0.6398 | 0.8802 |\n| OpenMolIns-light (Galactica-125M) | 0.3202 | 0.6547 | 0.6416 | 0.3508 | 0.6435 | 0.6358 | 0.269 | 0.6521 | 0.638 |\n| OpenMolIns-small (Galactica-125M) | 0.4172 | 0.642 | 0.5568 | 0.3958 | 0.6452 | 0.5338 | 0.2956 | 0.6385 | 0.5376 |\n| OpenMolIns-medium (Galactica-125M) | 0.5904 | 0.5812 | 0.789 | 0.5874 | 0.5873 | 0.7384 | 0.4608 | 0.5859 | 0.7768 |\n| OpenMolIns-large (Galactica-125M) | 0.6454 | 0.5927 | 0.8198 | 0.6388 | 0.5973 | 0.8028 | 0.495 | 0.5962 | 0.81 |\n| OpenMolIns-xlarge (Galactica-125M) | 0.7362 | 0.5744 | 0.8902 | 0.7124 | 0.5697 | 0.8612 | 0.5786 | 0.5677 | 0.8626 |", "caption": "Table 12: Results on MolOpt. For each task, we highlight the best accuracy and underline the second best accuracy.", "description": "This table presents the results of the molecule optimization (MolOpt) task from the TOMG-Bench benchmark.  The MolOpt task evaluates the ability of large language models (LLMs) to optimize molecular properties (LogP, MR, and QED) by modifying their structures.  For each of the three properties (LogP, MR, QED), and for each LLM tested, the table shows the accuracy, similarity, and validity of the generated molecules.  Accuracy reflects how many molecules successfully met the optimization criteria. Similarity measures the structural similarity between the original molecule and the optimized molecule. Validity checks if the generated molecules are chemically valid. The best accuracy for each property is highlighted, and the second-best accuracy is underlined.", "section": "3 TOMG-Bench"}, {"content": "| Models | AtomNum Accuracy | AtomNum Novelty | AtomNum Validity | BondNum Accuracy | BondNum Novelty | BondNum Validity | FunctionalGroup Accuracy | FunctionalGroup Novelty | FunctionalGroup Validity |\n|---|---|---|---|---|---|---|---|---|---| \n| GPT-4o <cite class=\"ltx_cite ltx_citemacro_cite\">Achiam et al. (2023)</cite> | 0.1998 | 0.6703 | 0.5852 | 0.065 | 0.6336 | 0.8564 | 0.233 | 0.6513 | 0.859 |\n| GPT-4-turbo <cite class=\"ltx_cite ltx_citemacro_cite\">Achiam et al. (2023)</cite> | 0.1702 | 0.6991 | 0.4904 | 0.0774 | 0.6301 | 0.9068 | 0.218 | 0.6605 | 0.8778 |\n| GPT-3.5-turbo <cite class=\"ltx_cite ltx_citemacro_cite\">Achiam et al. (2023)</cite> | 0.107 | 0.5054 | 0.6947 | 0.0518 | 0.6871 | 0.5522 | 0.1136 | 0.6585 | 0.8686 |\n| Claude-3.5 <cite class=\"ltx_cite ltx_citemacro_cite\">Anthropic (2024b)</cite> | 0.1928 | 0.6926 | 0.6548 | 0.1058 | 0.6584 | 0.886 | 0.2364 | 0.6582 | 0.8892 |\n| Claude-3 <cite class=\"ltx_cite ltx_citemacro_cite\">Anthropic (2024a)</cite> | 0.1044 | 0.6833 | 0.591 | 0.1042 | 0.6598 | 0.8696 | 0.1816 | 0.9158 | 0.6644 |\n| Gemini-1.5-pro <cite class=\"ltx_cite ltx_citemacro_cite\">Deepmind (2024)</cite> | 0.1742 | 0.6902 | 0.6774 | 0.0708 | 0.6522 | 0.8688 | 0.2486 | 0.6673 | 0.924 |\n| Llama3-70B-Instruct (Int4) <cite class=\"ltx_cite ltx_citemacro_cite\">Dubey et al. (2024)</cite> | 0.1404 | 0.6675 | 0.5474 | 0.067 | 0.6478 | 0.7378 | 0.1752 | 0.6576 | 0.765 |\n| Llama3-8B-Instruct <cite class=\"ltx_cite ltx_citemacro_cite\">Dubey et al. (2024)</cite> | 0.0242 | 0.6649 | 0.3812 | 0.026 | 0.6303 | 0.57 | 0.0848 | 0.6167 | 0.7216 |\n| Llama3.1-8B-Instruct <cite class=\"ltx_cite ltx_citemacro_cite\">Dubey et al. (2024)</cite> | 0.0228 | 0.702 | 0.3862 | 0.0395 | 0.6541 | 0.6387 | 0.13 | 0.6274 | 0.6905 |\n| Mistral-7B-Instruct-v0.2 <cite class=\"ltx_cite ltx_citemacro_cite\">Jiang et al. (2023)</cite> | 0.0078 | 0.6732 | 0.2986 | 0.0102 | 0.6309 | 0.4524 | 0.0048 | 0.6012 | 0.402 |\n| Qwen2-7B-Instruct <cite class=\"ltx_cite ltx_citemacro_cite\">Yang et al. (2024)</cite> | 0.011 | 0.9061 | 0.2622 | 0.001 | 0.8645 | 0.0796 | 0.0022 | 0.8601 | 0.0622 |\n| Yi-1.5-9B <cite class=\"ltx_cite ltx_citemacro_cite\">Young et al. (2024)</cite> | 0.0392 | 0.6848 | 0.617 | 0.0208 | 0.6407 | 0.7072 | 0.0126 | 0.6945 | 0.6521 |\n| Chatglm-9B <cite class=\"ltx_cite ltx_citemacro_cite\">GLM et al. (2024)</cite> | 0.0002 | 0.7483 | 0.2131 | 0.0254 | 0.7189 | 0.4682 | 0 | 0.6908 | 0.5926 |\n| Llama-3.2-1B-Instruct <cite class=\"ltx_cite ltx_citemacro_cite\">Dubey et al. (2024)</cite> | 0.004 | 0.6807 | 0.185 | 0.008 | 0.7465 | 0.2226 | 0.0008 | 0.7461 | 0.2818 |\n| MolT5-small <cite class=\"ltx_cite ltx_citemacro_cite\">Edwards et al. (2022)</cite> | 0.0006 | 0.6586 | 0.661 | 0.0064 | 0.598 | 0.6202 | 0.0114 | 0.5287 | 0.8354 |\n| MolT5-base <cite class=\"ltx_cite ltx_citemacro_cite\">Edwards et al. (2022)</cite> | 0.0008 | 0.6868 | 0.756 | 0.007 | 0.6509 | 0.8422 | 0.013 | 0.5464 | 0.8382 |\n| MolT5-large <cite class=\"ltx_cite ltx_citemacro_cite\">Edwards et al. (2022)</cite> | 0.015 | 0.7103 | 0.8412 | 0.0118 | 0.5611 | 0.8916 | 0.0382 | 0.6088 | 0.9406 |\n| BioT5-base <cite class=\"ltx_cite ltx_citemacro_cite\">Pei et al. (2023)</cite> | 0.0118 | 0.8353 | 0.995 | 0.0078 | 0.6667 | 0.9992 | 0.0476 | 0.6792 | 0.9998 |\n| OpenMolIns-large (LLama-3.2-1B) | 0.0144 | 0.649 | 0.5616 | 0.035 | 0.615 | 0.6186 | 0.0252 | 0.6373 | 0.4412 |\n| OpenMolIns-large (LLama-3.1-8B) | 0.0136 | 0.6634 | 0.7582 | 0.0544 | 0.6614 | 0.7456 | 0.1344 | 0.6396 | 0.6435 |\n| OpenMolIns-light (Galactica-125M) | 0.0044 | 0.6054 | 0.793 | 0.0216 | 0.5724 | 0.7596 | 0.0244 | 0.5756 | 0.8442 |\n| OpenMolIns-small (Galactica-125M) | 0.0146 | 0.6568 | 0.8424 | 0.053 | 0.6365 | 0.7926 | 0.057 | 0.5954 | 0.8874 |\n| OpenMolIns-medium (Galactica-125M) | 0.0294 | 0.6553 | 0.8698 | 0.0622 | 0.6473 | 0.7474 | 0.0882 | 0.6091 | 0.8932 |\n| OpenMolIns-large (Galactica-125M) | 0.0464 | 0.6729 | 0.9116 | 0.0716 | 0.6695 | 0.7374 | 0.0996 | 0.6276 | 0.8966 |\n| OpenMolIns-xlarge (Galactica-125M) | 0.1862 | 0.6899 | 0.9308 | 0.1656 | 0.6887 | 0.7952 | 0.2006 | 0.6445 | 0.9162 |", "caption": "Table 13: Results on MolCustom. For each task, we highlight the best accuracy and underline the second best accuracy.", "description": "Table 13 presents the results of the MolCustom task in the TOMG-Bench benchmark.  MolCustom focuses on the generation of molecules with specified properties, testing the ability of LLMs to create novel molecules based on various constraints. The table shows the performance of several LLMs across three subtasks within MolCustom: AtomNum (generating molecules with a specific number of atoms), BondNum (generating molecules with a specific number of bonds), and FunctionalGroup (generating molecules containing specific functional groups).  For each subtask and each LLM, the table displays the accuracy, novelty, and validity of the generated molecules.  Accuracy measures the percentage of successfully generated molecules that satisfy the given constraints; novelty assesses the uniqueness of the generated molecules compared to existing molecules; and validity checks if the generated molecules are chemically valid. The best accuracy for each subtask is highlighted, and the second-best accuracy is underlined.", "section": "3 TOMG-Bench"}]