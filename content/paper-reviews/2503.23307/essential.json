{"importance": "This paper pioneers movie-grade character synthesis, offering researchers a novel approach to AI-driven content creation. By addressing speech-driven animation & enabling cinematic storytelling, it paves the way for more realistic, expressive, & controllable virtual characters in film & interactive media.", "summary": "MoCha: Movie-Grade Talking Character Synthesis!", "takeaways": ["MoCha, a novel diffusion transformer, generates realistic talking characters from speech and text.", "A new speech-video window attention mechanism improves lip-sync accuracy.", "Joint speech-text training enhances generalization across diverse character actions."], "tldr": "Recent advancements in video generation often neglect character-driven storytelling. To address this, the paper introduces the task of **Talking Characters**: generating animations from speech & text, focusing on realistic full-body actions. Current models lack control over spoken content, have limited body movement, and cannot handle multi-character interactions.\n\nTo solve these issues, the paper presents **MoCha**, a diffusion transformer model for high-quality talking character generation. Key innovations include a speech-video window attention mechanism for accurate lip-sync, and a joint training strategy using speech & text-labeled data for better generalization. MoCha enables multi-character conversations & sets a new standard for AI-driven cinematic storytelling with improved realism.", "affiliation": "University of Waterloo", "categories": {"main_category": "Multimodal Learning", "sub_category": "Multimodal Generation"}, "podcast_path": "2503.23307/podcast.wav"}