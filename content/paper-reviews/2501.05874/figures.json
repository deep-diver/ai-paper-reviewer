[{"figure_path": "https://arxiv.org/html/2501.05874/x1.png", "caption": "Figure 1: A conceptual illustration of existing and the proposed RAG scenarios. (A) Textual RAG retrieves documents (relevant to queries) from a text corpus and incorporates them when generating answers. (B) Conventional multimodal RAG extends retrieval to include static images. (C) VideoRAG (ours) further extends the external knowledge source to videos.", "description": "This figure illustrates the conceptual differences between three types of Retrieval Augmented Generation (RAG) systems. (A) shows a traditional Textual RAG, which retrieves text documents relevant to a given query and incorporates them into the answer generation process. (B) demonstrates a Conventional Multimodal RAG system, which extends retrieval to include static images, providing more context than text alone.  (C) presents VideoRAG, the proposed method, which further integrates video data as a knowledge source. Videos offer rich temporal and spatial information, allowing for a more comprehensive understanding and answer generation compared to text or images only.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2501.05874/x2.png", "caption": "Figure 2: Impact of varying the interpolation ratio between textual and visual features on retrieval performance.", "description": "This figure shows how retrieval performance, measured by R@1, R@5, and R@10, changes depending on the ratio of textual and visual features combined.  The x-axis represents the interpolation ratio (\u03b1) ranging from 0 to 1.  A ratio of 0 means only visual features are used, while a ratio of 1 means only textual features are used.  Values between 0 and 1 indicate a combination of both. The figure helps determine the optimal balance of visual and textual information for best retrieval accuracy.", "section": "4.2 Video Retrieval"}, {"figure_path": "https://arxiv.org/html/2501.05874/x3.png", "caption": "Figure 3: Visualization of latent space of features across modalities with Principal Component Analysis (PCA).", "description": "This PCA plot visualizes the relationships between visual and textual features extracted from videos, alongside query embeddings.  The proximity of points indicates semantic similarity.  Clusters or groupings suggest how well different modalities capture similar semantic concepts within the video data.  The plot helps to understand whether the visual and textual features align well with the queries, providing insights into the effectiveness of the multimodal representation used in VideoRAG for retrieval.", "section": "4.2 Video Retrieval"}, {"figure_path": "https://arxiv.org/html/2501.05874/x4.png", "caption": "Figure 4: Breakdown performance of different models across 10 different categories.", "description": "This bar chart visualizes the performance of different models (Naive, TextRAG (BM25), TextRAG (DPR), TextVideoRAG, VideoRAG-T, VideoRAG-V, VideoRAG-VT) across ten distinct categories.  Each category represents a different topic or theme within the datasets used in the study (e.g., Food, Hobbies, Home & Garden, etc.).  The height of each bar indicates the performance of a particular model on that specific category, allowing for a direct comparison of model effectiveness across diverse query types.  The chart provides insights into how well each model handles questions relating to each topic area. This is likely measured by one or more metrics described in the paper.", "section": "4.1 Main Results"}]