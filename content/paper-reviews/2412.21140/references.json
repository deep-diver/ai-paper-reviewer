{"references": [{"fullname_first_author": "Rohan Taori", "paper_title": "Stanford alpaca: An instruction-following llama model", "publication_date": "2023-00-00", "reason": "This paper introduced Alpaca, an early and influential method for knowledge distillation of state-of-the-art LLMs, inspiring further advancements in open-source multilingual chatbots."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Llama: Open and efficient foundation language models", "publication_date": "2023-02-13", "reason": "This paper introduced LLaMA, a foundational open-source large language model that has been extensively used and adapted by the research community, accelerating LLM adoption."}, {"fullname_first_author": "Albert Q Jiang", "paper_title": "Mistral 7B", "publication_date": "2023-10-06", "reason": "This paper introduced Mistral-7B, a large language model used in the experiments of the current paper, demonstrating its competitive performance in multilingual settings."}, {"fullname_first_author": "Yiming Cui", "paper_title": "Efficient and effective text encoding for chinese llama and alpaca", "publication_date": "2023-04-08", "reason": "This paper presented a language-specific continued pre-training pipeline for LLM language adaptation that inspired the current paper's approach for embedding propagation and vocabulary optimization."}, {"fullname_first_author": "Ilya Gusev", "paper_title": "Dataset for automatic summarization of russian news", "publication_date": "2020-10-07", "reason": "This paper introduced the Gazeta dataset, a benchmark for Russian automatic summarization used in the current paper's Darumeru benchmark for LLM text generation quality evaluation."}]}