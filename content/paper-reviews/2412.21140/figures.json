[{"figure_path": "https://arxiv.org/html/2412.21140/extracted/6098179/image31.png", "caption": "Figure 1: Performance comparison of proposed adaptation method on Darumeru benchmark", "description": "This figure displays the performance comparison results of the proposed Learned Embedding Propagation (LEP) method against several other state-of-the-art LLMs on the Darumeru benchmark.  The benchmark focuses on evaluating text generation robustness, particularly for Russian language adaptation. The graph showcases the Micro-Avg scores (a combined metric) for different models, including the original models (LLaMa-3-8B and Mistral-7B), the models adapted using the LEP method with various configurations and calibration methods, as well as several other existing open-source and closed-source LLMs for reference.  This allows for a direct comparison to see how the LEP method performs relative to other techniques.", "section": "2 Method"}, {"figure_path": "https://arxiv.org/html/2412.21140/extracted/6098179/image44.png", "caption": "Figure 2: Micro average benchmark score dynamic throughout training", "description": "Figure 2 presents a line graph illustrating the change in the micro-average benchmark score across various training steps.  The benchmark score, a composite metric representing overall performance, is plotted against the number of training steps. Multiple lines are shown, each representing a different combination of LLM model (LLaMa-3-8B or Mistral-7B) and vocabulary optimization technique (BPE, Unigram, Extended, or Optimized). This visualization allows for comparison of the training progress and performance achieved by various model and vocabulary combinations during the continued pre-training phase.", "section": "2.1 Model Language Adaptation"}, {"figure_path": "https://arxiv.org/html/2412.21140/extracted/6098179/image45.png", "caption": "Figure 3: An example of generation using the OpenChat-3.5 model and its adapted versions.", "description": "This figure shows an example demonstrating the performance improvements of the proposed Learned Embedding Propagation (LEP) method.  Three model outputs are shown, comparing the original OpenChat-3.5 model's response to the same prompt against outputs from the LEP-adapted model and the LEP-adapted model after further self-calibration. The example highlights how the LEP method improves the model's understanding and accuracy in handling phraseological units and nuanced language, showcasing the effectiveness of the approach.", "section": "3.6 Examples"}]