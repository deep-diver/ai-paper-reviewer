{"references": [{"fullname_first_author": "Guanzheng Chen", "paper_title": "CLEX: Continuous length extrapolation for large language models", "publication_date": "2024-XX-XX", "reason": "This paper proposes a method for continuous length extrapolation, which is a crucial problem addressed by the current paper"}, {"fullname_first_author": "Peter Shaw", "paper_title": "Self-attention with relative position representations", "publication_date": "2018-XX-XX", "reason": "This paper introduces relative position embeddings, which are a key component of the RoPE method analyzed and improved in the current paper"}, {"fullname_first_author": "Ofir Press", "paper_title": "Train short, test long: Attention with linear biases enables input length extrapolation", "publication_date": "2021-XX-XX", "reason": "This paper introduces ALiBi, a method for improving attention mechanisms' performance with long sequences, which is compared with the current paper\u2019s approach"}, {"fullname_first_author": "Jianlin Su", "paper_title": "Roformer: Enhanced transformer with rotary position embedding", "publication_date": "2024-XX-XX", "reason": "This paper introduces RoPE, which is the core focus of the analysis and improvements in the current paper"}, {"fullname_first_author": "Bowen Peng", "paper_title": "YARN: Efficient context window extension of large language models", "publication_date": "2023-XX-XX", "reason": "This paper proposes YARN, a method used for extrapolation in the experiments, showing its importance as a comparative method"}]}