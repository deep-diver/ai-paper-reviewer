[{"figure_path": "https://arxiv.org/html/2412.15035/x1.png", "caption": "Figure 1: Safety comparison of English (ALERT) vs. Multilingual (M-ALERT) on different prompts. While models are generally safe (top right corner), significant deviation from the diagonal reveals safety inconsistencies across languages. (cf.\u00a0Table\u00a03 & 4)", "description": "Figure 1 is a scatter plot comparing the safety scores of LLMs evaluated on the English-only ALERT benchmark and the multilingual M-ALERT benchmark. Each point represents a single prompt. The x-axis shows the safety score on ALERT (English), and the y-axis shows the safety score on M-ALERT (multilingual).  A point in the top right corner indicates high safety scores in both English and multiple languages, while deviation from the diagonal line indicates inconsistencies in safety across languages. This illustrates how an LLM may be safe in English but unsafe when evaluating the same prompt translated into another language. Tables 3 and 4 provide more details on the safety scores.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2412.15035/extracted/6083134/img/taxonomy_5.png", "caption": "Figure 2: M-ALERT follows the ALERT Tedeschi et\u00a0al. (2024) taxonomy with 6 macro and 32 micro categories.", "description": "Figure 2 illustrates the hierarchical structure of the M-ALERT safety taxonomy.  It's based on the ALERT taxonomy (Tedeschi et al., 2024), which categorizes potential safety risks in Large Language Models (LLMs). The figure visually depicts the 6 macro-categories and their corresponding 32 micro-categories.  This structure allows for a granular and detailed analysis of LLM safety performance across various types of potentially harmful outputs.", "section": "3 M-ALERT"}, {"figure_path": "https://arxiv.org/html/2412.15035/extracted/6083134/img/M-ALERT_framework.png", "caption": "Figure 3: M-ALERT framework. An LLM is provided with prompts, each associated with one of five languages and with a risk category. Its responses are classified for safety by a multilingual judge. This way, M-ALERT furnishes a general safety score along with category- and language-specific safety scores, offering detailed insights.", "description": "The M-ALERT framework evaluates the safety of LLMs across multiple languages.  It begins by providing an LLM with a prompt in one of five languages (English, French, German, Italian, or Spanish), each prompt categorized by a specific risk. The LLM's response is then assessed for safety by a multilingual judge.  The system generates an overall safety score and breaks down the safety assessment into category-specific and language-specific scores. This detailed analysis offers granular insights into the model's safety performance in different linguistic contexts and risk categories.", "section": "3 M-ALERT"}, {"figure_path": "https://arxiv.org/html/2412.15035/x2.png", "caption": "Figure 4: Overall safety scores for 5 languages. All models exacerbate unsafe behavior at least for one language\u2014three models even highly unsafe. (y-axis scaled)", "description": "Figure 4 presents a bar chart visualizing the overall safety scores of ten different large language models (LLMs) across five languages (English, French, German, Italian, and Spanish). Each bar represents a single LLM's performance, and the height of the bar indicates its overall safety score.  The chart shows that none of the LLMs achieve a perfect safety score across all five languages.  Importantly, three LLMs demonstrate highly unsafe behavior in at least one language, indicating significant cross-linguistic safety inconsistencies. The y-axis is scaled to better highlight the differences in safety scores among the models.", "section": "5 Evaluating LLMs' Safety with M-ALERT"}, {"figure_path": "https://arxiv.org/html/2412.15035/x3.png", "caption": "Figure 5: Comparing model size with safety scores. One cannot see a clear trend between model size and safety. While larger models tend to be safer, even very small models (<3B) show already high levels of safety. For base models, the trend is more clear than for Instruct models. (y-axis scaled)", "description": "This figure displays the relationship between the size of large language models (LLMs), measured in billions of parameters, and their safety scores as assessed by the M-ALERT benchmark.  Contrary to expectations, there's no strong correlation between model size and safety. While larger models generally exhibit higher safety scores, the data shows that even relatively small models (with fewer than 3 billion parameters) can achieve high safety levels. Notably, the trend of increasing safety with increasing model size is clearer for base models than for instruction-tuned (Instruct) models, suggesting that instruction tuning might introduce complexities or inconsistencies in this relationship. The y-axis of the graph is scaled to emphasize smaller differences in safety scores.", "section": "5 Evaluating LLMs' Safety with M-ALERT"}, {"figure_path": "https://arxiv.org/html/2412.15035/x4.png", "caption": "Figure 6: Visualizing safety scores as a function of release date", "description": "This figure shows a line graph plotting the safety scores of various large language models (LLMs) against their release dates.  The graph visually represents the trend of LLM safety performance over time.  Different LLMs are represented by distinct data points.  The x-axis denotes the release date, while the y-axis represents the safety scores (higher scores indicate higher safety). The graph helps illustrate whether newer LLMs tend to have better safety performance compared to their older counterparts, demonstrating improvements or lack thereof in safety over time. The figure uses color-coding to indicate the safety level, allowing for a quick visual assessment of each model's safety.", "section": "5 Evaluating LLMs' Safety with M-ALERT"}]