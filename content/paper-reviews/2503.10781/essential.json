{"importance": "This paper is important for researchers as it **addresses the gap in large-scale, densely annotated video datasets**, enabling more effective training of video understanding models. The introduced GROVE model and iGround dataset **set a new benchmark for grounded video captioning**, offering a valuable resource and a strong foundation for future work in embodied perception and human-robot interaction.", "summary": "GROVE: Pre-training on large-scale data for grounded video caption generation.", "takeaways": ["Introduces a method for large-scale automatic annotation of grounded video captions, creating the HowToGround1M dataset.", "Proposes GROVE, a Grounded Video Caption Generation model, pre-trained on HowToGround1M and fine-tuned on iGround.", "Achieves state-of-the-art results on iGround, VidSTG, and ActivityNet-Entities datasets, demonstrating the effectiveness of the approach."], "tldr": "Grounded video caption generation, which involves generating natural language descriptions for videos while simultaneously localizing key objects within the video using bounding boxes, is crucial for advancing areas like human-robot interaction. However, the **lack of large-scale video datasets with dense spatio-temporal grounding** has been a major bottleneck. Existing datasets either focus on localizing single objects or provide sparse annotations, limiting the potential for training robust models. \n\nThis work introduces a novel approach to tackle this challenge by **presenting a large-scale automatic annotation method** that aggregates captions and bounding boxes across individual frames into temporally dense and consistent annotations. They create **HowToGround1M dataset** and propose a **Grounded Video Caption Generation model, dubbed GROVE**, which is pre-trained on HowToGround1M and fine-tuned on the new **iGround dataset**. Experimental results demonstrate state-of-the-art performance on multiple datasets.", "affiliation": "Czech Institute of Informatics, Robotics and Cybernetics", "categories": {"main_category": "Multimodal Learning", "sub_category": "Vision-Language Models"}, "podcast_path": "2503.10781/podcast.wav"}