[{"figure_path": "https://arxiv.org/html/2501.05510/x1.png", "caption": "Figure 1: \nA demonstrative comparison between offline and online video understanding\u00a0[5].\nOffline video understanding focuses on answering questions based on the entirety of a video. In contrast, online video understanding involves posing queries about the context of a video at intermediate points, demanding the ability to trace back past information, perceive ongoing events, and adapt to continuous input.", "description": "Figure 1 illustrates the core difference between offline and online video understanding.  Offline methods answer questions using the entire video. In contrast, online video understanding requires answering questions at various points during the video, necessitating the ability to access past information, comprehend current events, and adjust to new input as the video progresses.  This highlights the need for temporal awareness in video understanding.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2501.05510/x2.png", "caption": "Figure 2: Examples of each task in OVO-Bench. The 14 tasks are categorized into three different kinds of perceiving modes in online video understanding: Backward Tracing, Real-Time Visual Perception, and Forward Active Responding.", "description": "Figure 2 showcases examples of the 14 tasks included in the OVO-Bench benchmark.  These tasks are designed to evaluate different aspects of online video understanding capabilities. The tasks are grouped into three main categories reflecting different temporal reasoning aspects: 1) Backward Tracing (requiring the model to reason about past events to answer the question), 2) Real-Time Visual Perception (evaluating the model's understanding of the current events), and 3) Forward Active Responding (testing the model's ability to delay its response until sufficient future information is available).  Each example demonstrates the type of question and the expected response within the context of a short video clip.", "section": "3. OVO-Bench"}, {"figure_path": "https://arxiv.org/html/2501.05510/extracted/6119562/sec/image/model0.png", "caption": "Figure 3: \nGeneration pipeline of OVO-Bench.\nWithin public annotations,\ndata is carefully filtered and relevant multiple-choice QAs are auto-generated.\nThe effective system prompt and efficient answer prompt are employed to guide MLLMs toward precise outputs. The Video-LLMs we use to annotate videos are GPT-4o and Gemini-1.5 Pro.", "description": "This figure illustrates the pipeline for creating the OVO-Bench benchmark dataset.  It starts with existing public video annotation datasets and web-crawled videos.  These sources undergo filtering to select relevant videos, and then multiple choice questions and answers (QAs) are automatically generated.  The process uses effective system and answer prompts to guide large language models (LLMs) in generating high-quality annotations. Specifically, GPT-4 and Gemini-1.5 Pro LLMs were used to annotate the videos.", "section": "3. OVO-Bench"}, {"figure_path": "https://arxiv.org/html/2501.05510/x3.png", "caption": "Figure 4: \nLeft Queries Temporal Distribution in OVO-Bench. Center Linguistic Characteristics of Text Queries. Right Video category distribution of OVO-Bench.", "description": "This figure provides a comprehensive overview of the OVO-Bench dataset. The left panel displays a histogram showing the distribution of query timestamps throughout the videos. The center panel presents a word cloud summarizing the frequent words used in the queries, offering insight into the linguistic characteristics of the questions. The right panel displays a pie chart illustrating the distribution of videos across different categories within the dataset.", "section": "3.3 Datasets Statistics"}, {"figure_path": "https://arxiv.org/html/2501.05510/x4.png", "caption": "Figure 5: \nPerformance comparison between online Video-LLMs and offline Video-LLMs. The figure illustrates the average scores of different models on the OVO-Bench in real-time visual perception tasks.", "description": "Figure 5 presents a radar chart comparing the performance of various Video-LLMs on real-time visual perception tasks within the OVO-Bench benchmark.  Each axis represents a specific subtask within real-time visual perception (e.g., object recognition, action recognition, spatial understanding). The length of each spoke indicates the average score achieved by each model on that particular subtask. The chart visually demonstrates the relative strengths and weaknesses of online versus offline Video-LLMs in these real-time visual reasoning capabilities.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2501.05510/x5.png", "caption": "Figure 6: \nMultiple triggering evaluation pipeline of prompt offline models for online video understanding. Offline Video-LLMs are densely queried along the temporal axes to make independent decisions of whether existing visual content provide enough clues for answering.", "description": "This figure illustrates how offline Video-LLMs are evaluated for online video understanding. Unlike online models that process video streams continuously, offline models receive the entire video at once.  To simulate online understanding, the offline models are repeatedly queried at different timestamps throughout the video. Each query prompts the model to determine whether sufficient visual information is available at that point to answer the question. This process allows for evaluating the model's capacity to reason with evolving visual context, similar to how a human would process a video.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2501.05510/extracted/6119562/sec/image/supplimentary/222.png", "caption": "Figure 7: \nPrompts used for Online(up) and Offline(down) Models on Forward Active Responding and Response Examples. Despite our vision for online models, existing online models, like videollm-online, are still far from satisfactory, showing limited adaptation ability, and would easily encounter collapse when processing complicated or out-of-training-domain video and queries. Offline models are inclined to perform random guessing when the queries contain words like \u201dis/currently/ongoing\u201d.", "description": "Figure 7 demonstrates the prompts and example responses for both online and offline models when performing the Forward Active Responding task in the OVO-Bench benchmark.  The upper half shows prompts and responses from online models, revealing that existing online models struggle with complex or out-of-distribution videos, often failing completely. The lower half presents results from offline models. These models, while generally strong offline, show a tendency towards random guessing when the prompts include terms like \"is,\" \"currently,\" or \"ongoing,\" highlighting their limitations when dealing with real-time, dynamic aspects of online video understanding.", "section": "3.2 Benchmark Construction"}, {"figure_path": "https://arxiv.org/html/2501.05510/extracted/6119562/sec/image/supplimentary/111.png", "caption": "Figure 8: \nPrompts used for Online(up) and Offline(down) Models on Real-Time Visual Perception and Response Examples.\nThree tasks including [ACR], [OCR], and [ASI] are included as demonstrations. Our benchmarks involve a large ratio of questions, whose answers shift over time, which means that models can hardly figure out the answer by randomly selecting frames from original videos.", "description": "Figure 8 demonstrates the prompts used to evaluate both online and offline video language models on real-time visual perception tasks.  It showcases examples for three tasks: Action Recognition ([ACR]), Optical Character Recognition ([OCR]), and Action Sequential Identification ([ASI]). The figure highlights the challenge posed by the benchmark's design. Many questions have answers that change over time, making it difficult for models to find the correct answer by simply selecting random frames from the video.", "section": "3. OVO-Bench"}]