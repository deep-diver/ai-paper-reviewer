{"references": [{"fullname_first_author": "Leonard B\u00e4rmann", "paper_title": "Where did I leave my keys?-episodic-memory-based question answering on egocentric videos", "publication_date": "2022-00-00", "reason": "This paper introduces a novel approach to episodic memory in egocentric videos, a key aspect of online video understanding."}, {"fullname_first_author": "Fabian Caba Heilbron", "paper_title": "Activitynet: A large-scale video benchmark for human activity understanding", "publication_date": "2015-00-00", "reason": "This paper presents ActivityNet, a foundational large-scale video benchmark that has significantly influenced subsequent video understanding research."}, {"fullname_first_author": "Mu Cai", "paper_title": "TemporalBench: Benchmarking fine-grained temporal understanding for multimodal video models", "publication_date": "2024-00-00", "reason": "This paper introduces TemporalBench, a benchmark specifically designed to evaluate temporal understanding in multimodal video models, directly addressing a limitation of offline video understanding benchmarks."}, {"fullname_first_author": "Joya Chen", "paper_title": "VideoLLM-online: Online video large language model for streaming video", "publication_date": "2024-00-00", "reason": "This paper introduces VideoLLM-online, a pioneering work in online video understanding which directly addresses the need for models capable of handling streaming video data and dynamic queries."}, {"fullname_first_author": "Kristen Grauman", "paper_title": "Ego4D: Around the world in 3,000 hours of egocentric video", "publication_date": "2022-00-00", "reason": "This paper introduces Ego4D, a large-scale egocentric video dataset that has substantially advanced research in first-person video understanding, which is closely related to online video understanding."}]}