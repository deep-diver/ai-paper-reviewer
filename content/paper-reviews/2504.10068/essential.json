{"importance": "This paper matters because it tackles the crucial issue of **efficiently processing long videos in MLLMs** by balancing computational cost and information retention. By introducing a multi-granularity video representation, it opens new avenues for research in video understanding, especially for tasks requiring fine-grained spatio-temporal reasoning. This can **significantly impact applications** like video summarization, surveillance, and content analysis.", "summary": "Mavors: Multi-granularity video representation for multimodal large language model", "takeaways": ["Mavors introduces a multi-granularity video representation that balances computational efficiency and fine-grained spatio-temporal pattern retention.", "The framework uses an Intra-chunk Vision Encoder (IVE) and an Inter-chunk Feature Aggregator (IFA) to encode and aggregate video features.", "Experiments show Mavors maintains spatial fidelity and temporal continuity, outperforming existing methods in spatio-temporal reasoning tasks."], "tldr": "Current MLLMs struggle with long videos, facing challenges in balancing computational efficiency and retaining fine-grained details. Existing methods like sparse sampling, low-resolution input, and token compression often lose important information, especially in videos with complex motion or varying resolutions. This leads to degraded performance in tasks requiring detailed temporal and spatial understanding. Therefore, there is a need to efficiently encode long videos while preserving critical details.\n\nThis paper introduces a framework that addresses these issues by encoding raw video content into latent representations using an Intra-chunk Vision Encoder (IVE) and an Inter-chunk Feature Aggregator (IFA). The IVE preserves spatial features, while the IFA establishes temporal coherence across video segments. It also unifies image and video understanding by treating images as single-frame videos. Experiments demonstrate that it maintains spatial fidelity and temporal continuity.", "affiliation": "Peking University", "categories": {"main_category": "Multimodal Learning", "sub_category": "Vision-Language Models"}, "podcast_path": "2504.10068/podcast.wav"}