[{"figure_path": "https://arxiv.org/html/2412.11231/x1.png", "caption": "Figure 1: Comparison of performance on Llama-3-8B during three iterations of instruction evolution, using Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct as supervised models for each round under Evol-Instruct scenario.", "description": "This figure compares the performance of a smaller language model (Llama-3.1-8B-Instruct) and a larger language model (Llama-3.1-70B-Instruct) in evolving instructions for a smaller backbone model (Llama-3-8B) across three iterations.  The performance is evaluated on four downstream tasks: instruction following (IFEval with prompt-based and instance-based scores), math reasoning (GSM8K and MATH), and code generation (HumanEval and MBPP). The x-axis represents the iteration number (0-3, where 0 represents the seed data). The y-axis represents the performance metric for each task.  The lines indicate the performance trend across iterations.  This visualization aims to demonstrate whether smaller or larger language models are more effective at evolving instructions for improved downstream task performance.", "section": "RQ1: Do SLMs Perform Better than LLMs in Evolving Instructions?"}, {"figure_path": "https://arxiv.org/html/2412.11231/x2.png", "caption": "Figure 2: Distribution of difficulty levels for instructions evolved during three iterations, using Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct as supervised models for each round under Evol-Instruct scenario.", "description": "This figure presents the distribution of difficulty levels for instructions generated by a smaller language model (Llama-3.1-8B-Instruct) and a larger language model (Llama-3.1-70B-Instruct) across three iterations of instruction evolution.  The difficulty levels are categorized as Very Easy, Easy, Medium, Hard, and Very Hard. The x-axis represents the difficulty levels, and the y-axis represents the percentage of instructions falling into each category. Each bar group represents one round of evolution (Iter1, Iter2, and Iter3), and within each group, the blue and orange bars represent the distributions from the smaller and larger models, respectively. The figure aims to demonstrate whether smaller or larger language models generate more complex instructions during the evolution process across different datasets: Alpaca (instruction following), GSM8K (mathematical reasoning), and HumanEval (code generation).", "section": "RQ1: Do SLMs Perform Better than LLMs in Evolving Instructions?"}, {"figure_path": "https://arxiv.org/html/2412.11231/x3.png", "caption": "Figure 3: Comparison of performance among Qwen-2.5 series models. Detailed results can be found in Table\u00a011.", "description": "This figure compares the performance of different sizes of models (0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B) within the Qwen-2.5 series on four downstream tasks after instruction tuning with instruction data generated by both small language models (SLMs) and large language models (LLMs).  The tasks include instruction following (IFEval, Pr.(S) and In.(S)), mathematical reasoning (GSM8K and MATH), and code generation (HumanEval and MBPP). For larger models (14B, 32B, and 72B), LORA was used for fine-tuning due to limited computational resources. The metrics used for evaluation are: Pr.(S) and In.(S) (strict and inclusive accuracy on IFEval), GSM8K and MATH (accuracy scores), HumanEval and MBPP (pass@1).  The graph visually represents how SLM-generated and LLM-generated instruction data impact the performance of the models of different sizes for each of the four tasks. The complete numerical results of this comparison can be found in Table 11 of the paper.", "section": "RQ1: Do SLMs Perform Better than LLMs in Evolving Instructions?"}, {"figure_path": "https://arxiv.org/html/2412.11231/x4.png", "caption": "Figure 4: Distribution of Minimum Neighbor Distance for instructions generated by Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct in the AutoIF scenario.", "description": "This histogram displays the distribution of minimum neighbor distances (MND) for instructions generated by two models, Llama-3.1-8B-Instruct (SLM) and Llama-3.1-70B-Instruct (LLM), within the AutoIF instruction generation scenario.  The x-axis represents the MND, a measure of similarity between instructions, calculated in the embedding space using all-mpnet-base-v2. The y-axis represents the frequency density of instructions at each MND. A higher MND suggests greater dissimilarity between instructions, implying better diversity. The figure aims to visually compare the diversity of instructions generated by the smaller and larger language models.", "section": "3.2 AutoIF Scenario"}, {"figure_path": "https://arxiv.org/html/2412.11231/x5.png", "caption": "Figure 5: Comparison of output token probability distributions in the Evol-Instruct scenario.", "description": "This figure presents a comparison of probability distributions for the top-1 output tokens generated by smaller language models (SLMs) and larger language models (LLMs) during instruction evolution in the Evol-Instruct scenario. The x-axis represents the probability, and the y-axis is the density. SLMs show a broader distribution of probabilities, with a lower peak and a longer tail, suggesting that their output space is more diverse than LLMs, which have a sharper peak around higher probabilities.", "section": "RQ1: Do SLMs Perform Better than LLMs in Evolving Instructions?"}, {"figure_path": "https://arxiv.org/html/2412.11231/x6.png", "caption": "Figure 6: Performance comparison of three data selection ratios on Alpaca dataset between IC-IFD and IFD.", "description": "This figure compares the performance of instruction-tuned language models using different data filtering strategies. IC-IFD and IFD, two metrics for evaluating the quality of instructions, are used to filter the Alpaca dataset, a collection of instruction-following data. Three different filtering ratios (5%, 10%, and 15%) are applied.  The filtered data is then used to fine-tune two language models: Llama-3-8B and Llama-3.2-3B. Performance is evaluated using AlpacaFarm, a benchmark for evaluating instruction following models. The comparison is shown in terms of win-tie-lose ratios, derived from assessments by GPT-4 on whether a model's response to an instruction is better, worse, or equal to another model's response. The results show that IC-IFD consistently outperforms IFD across all filtering ratios for both models, suggesting that IC-IFD is a more effective metric for filtering and selecting high-quality instruction data.", "section": "RQ3: How Do We Determine Whether An Instruction is Effective without Instruction Tuning?"}, {"figure_path": "https://arxiv.org/html/2412.11231/x7.png", "caption": "Figure 7: Performance comparison of three data selection ratios on Alpaca dataset between IC-IFD and full dataset.", "description": "This figure compares the performance of a Llama-3.2-3B model fine-tuned on different subsets of the Alpaca dataset, created using the Instruction Complex-Aware Instruction Following Difficulty (IC-IFD) metric.  Three different selection ratios (5%, 10%, and 15%) are used to filter the full Alpaca dataset and create smaller training sets. The performance of the models trained on these filtered datasets is then compared to a model fine-tuned on the complete Alpaca dataset. The results are visualized to demonstrate the effectiveness of IC-IFD for instruction data selection.", "section": "RQ3: How Do We Determine Whether An Instruction is Effective without Instruction Tuning?"}, {"figure_path": "https://arxiv.org/html/2412.11231/x8.png", "caption": "Figure 8: Comparison of cases between LLMs and SLMs under adding constraints strategy.", "description": "This figure presents two examples of how large language models (LLMs) and smaller language models (SLMs) evolve instructions when given the constraint of a busy schedule and restrictive diet. The original instruction is to give three tips for staying healthy.  The LLM adds the constraint of a \"moderate lifestyle\" and requests an explanation of how to incorporate the tips into a daily routine. The SLM adds the constraints of \"limited time for exercise\" and \"restrictive diet,\" and requests \"evidence-based\" tips.  This demonstrates how SLMs are capable of generating more complex and challenging instructions compared to LLMs, by incorporating more constraints into the evolved prompt.", "section": "3 RQ1: Do SLMs Perform Better than LLMs in Evolving Instructions?"}, {"figure_path": "https://arxiv.org/html/2412.11231/x9.png", "caption": "Figure 9: Comparison of cases between LLMs and SLMs under deepening strategy.", "description": "This figure presents two examples of evolved instructions under the \"deepening\" strategy in the Evol-Instruct scenario, comparing the outputs of a Smaller Language Model (SLM) and a Large Language Model (LLM). The original instruction is a simple math word problem. The LLM adds a single additional condition about prorating the hourly wage, while the SLM adds several significantly more complex conditions regarding bonuses, weekday/weekend rates, and timeliness.  This illustrates how SLMs can evolve more complex instructions than LLMs, leading to potentially more effective instruction tuning data.", "section": "RQ1: Do SLMs Perform Better than LLMs in Evolving Instructions?"}, {"figure_path": "https://arxiv.org/html/2412.11231/x10.png", "caption": "Figure 10: In-depth evolution prompt template utilized in Evol-Instruct scenario.", "description": "This figure presents the prompt template used for in-depth evolution of instructions within the Evol-Instruct scenario.  The template instructs an LLM to act as a prompt rewriter, with the objective of increasing the complexity of a given prompt to challenge other large language models (e.g. ChatGPT, GPT-4).  The rewritten prompt is expected to remain comprehensible and answerable by humans while only adding 10-20 words.  Placeholders `{METHOD}` and `{INSTRUCTION}` within the template are to be replaced with the chosen evolution method (e.g., 'Adding Constraints', 'Deepening', etc.) and the given instruction to be evolved, respectively.", "section": "3.1 Evol-Instruct Scenario"}, {"figure_path": "https://arxiv.org/html/2412.11231/x11.png", "caption": "Figure 11: Four in-depth methods utilized in Evol-Instruct scenario.", "description": "This figure lists the four in-depth methods utilized in the Evol-Instruct scenario:\n1. **Adding Constraints:** This method involves adding one or more constraints or requirements to the original prompt.\n2. **Deepening:** If the given prompt contains inquiries about certain issues, this method increases the depth and breadth of the inquiry to make it more complex.\n3. **Concretizing:** This method involves replacing general concepts in the prompt with more specific concepts.\n4. **Adding Reasoning Steps:** If the original prompt can be solved with a few simple thinking processes, this method rewrites the prompt to explicitly request multiple-step reasoning, making it more challenging for the language model.", "section": "3.1 Evol-Instruct Scenario"}, {"figure_path": "https://arxiv.org/html/2412.11231/x12.png", "caption": "Figure 12: In-breadth evolution prompt template utilized in Evol-Instruct scenario.", "description": "This figure presents the prompt template employed for in-breadth evolution within the Evol-Instruct scenario.  In-breadth evolution aims to generate entirely new prompts inspired by a given prompt, but within the same domain, while exhibiting increased rarity. The generated prompt should maintain a similar length and complexity as the original.  Crucially, the output should solely consist of the new prompt without any additional explanations or symbols. The template includes placeholders for the original prompt and the newly created prompt.", "section": "3.1 Evol-Instruct Scenario"}, {"figure_path": "https://arxiv.org/html/2412.11231/x13.png", "caption": "Figure 13: Prompt template of Self-Instruct Seed Instructions in AutoIF scenario.", "description": "This figure presents the prompt template employed for generating seed instructions within the AutoIF scenario.  It directs the language model to generate 50 distinct instructions, emphasizing that these instructions should focus on the format rather than the style of the response. It also highlights the importance of verifiability, stating that adherence to the instructions should be easily assessable by a Python function. Example instructions are provided for both desired and undesired instruction types, and format specifications for the generated output are clearly articulated. The prompt leverages a few seed examples within AutoIF to create verifiable instructions.", "section": "3.2 AutoIF Scenario"}, {"figure_path": "https://arxiv.org/html/2412.11231/x14.png", "caption": "Figure 14: Prompt template of Verification Funcs and Cases Generation in AutoIF scenario.", "description": "This figure presents the prompt template utilized for the \"Verification Funcs and Cases Generation\" phase within the AutoIF process.  This stage focuses on generating Python code to evaluate whether responses adhere to given instructions. The prompt instructs the language model to create both an evaluation function and three test cases (input and expected output).  The provided JSON example illustrates the format.\n\nIn essence, this prompt guides the model to create an automated verification process for newly generated instructions in the AutoIF pipeline by providing sample JSON of the output function and the cases.", "section": "2 Preliminaries"}, {"figure_path": "https://arxiv.org/html/2412.11231/x15.png", "caption": "Figure 15: Prompt template of Auto Evol-Instruct scenario.", "description": "This figure presents the prompt template employed in the Auto Evol-Instruct scenario. The user acts as an instruction rewriter, tasked with transforming a given instruction into a more complex form. The process involves four steps: 1) Listing potential methods to increase instruction complexity, 2) Formulating a plan to implement these methods, 3) Rewriting the instruction based on the plan, and 4) Reviewing and refining the rewritten instruction for clarity and conciseness.  The objective is to make instructions challenging for large language models, while still maintaining their understandability for humans. The template specifies the format strictly.", "section": "3.3 Auto Evol-Instruct Scenario"}, {"figure_path": "https://arxiv.org/html/2412.11231/x16.png", "caption": "Figure 16: Prompt template of response generation.", "description": "This figure presents the prompt template used for response generation in the experiments.  It shows two variations of the prompt, one for when an input is provided along with the instruction, and another for when only an instruction is given. In both cases, the model is instructed to provide a comprehensive and accurate response. This ensures consistent prompting across different experimental setups.", "section": "RQ1: Do SLMs Perform Better than LLMs in Evolving Instructions?"}, {"figure_path": "https://arxiv.org/html/2412.11231/x17.png", "caption": "Figure 17: Prompt template of evaluating the difficulty levels.", "description": "This figure presents the prompt template used for evaluating the difficulty levels of instructions.  The prompt asks an LLM to assess an instruction's difficulty (\"very easy\", \"easy\", \"medium\", \"hard\", or \"very hard\") based on the user's intent and knowledge needed to address it.  The output should be just the difficulty level, without any additional text or symbols.", "section": "RQ3: How Do We Determine Whether An Instruction is Effective without Instruction Tuning?"}, {"figure_path": "https://arxiv.org/html/2412.11231/x18.png", "caption": "Figure 18: Prompt template of extracting the keywords from evolution trajectories.", "description": "This figure presents the prompt template employed to extract keywords from the trajectories generated during the instruction evolution process.  This process helps analyze the strategies employed by different language models, providing insights into how they modify instructions during the evolution.", "section": "RQ3: How Do We Determine Whether An Instruction is Effective without Instruction Tuning?"}, {"figure_path": "https://arxiv.org/html/2412.11231/x19.png", "caption": "Figure 19: Prompt template of evaluating the difficulty scores.", "description": "This figure presents the prompt template utilized for evaluating the difficulty scores of instructions.  The prompt instructs an LLM to assess an instruction's difficulty based on user intent and required knowledge.  It requests the LLM to output a numerical score from 0 to 100, reflecting the estimated difficulty, without any additional text or symbols.  This prompt is used to analyze the complexity of instructions in datasets.", "section": "5. RQ3: How Do We Determine Whether An Instruction is Effective without Instruction Tuning?"}, {"figure_path": "https://arxiv.org/html/2412.11231/x20.png", "caption": "Figure 20: Prompt template of evaluating the win-tie-lose rates.", "description": "This figure presents the prompt template used for evaluating win/tie/lose rates between two AI assistants, which is used in the AlpacaFarm evaluation. The prompt consists of a user query and responses from two AI assistants.  The prompt instructs an evaluator (likely a stronger LLM like GPT-4) to compare the quality of the two AI assistants\u2019 responses based on criteria including alignment with user needs, conciseness, comprehensiveness, logical flow, use of technical terms, and factual accuracy.  The evaluator is then asked to output a single label indicating whether \u2018Assistant 1 is better than Assistant 2\u2019, \u2018Assistant 1 is worse than Assistant 2\u2019, or \u2018Assistant 1 is equal to Assistant 2\u2019.", "section": "5 RQ3: How Do We Determine Whether An Instruction is Effective without Instruction Tuning?"}]