{"importance": "This paper is important for TA-LLM researchers as it introduces a novel DPO method (DiaTool-DPO) that **enhances dialogue capabilities without relying on extensive human labeling**. The finding that slot-filling benefits from diverse difficulty levels and indirect learning between tool call actions opens new avenues for research in TA-LLM training.", "summary": "DiaTool-DPO: Enhances tool-augmented LLMs with direct preference optimization for multi-turn dialogue control!", "takeaways": ["DiaTool-DPO, a new DPO method, improves TA-LLM dialogue control.", "TA-LLM interactions are modeled as a Markov Decision Process with distinct states.", "Performance benefits from exposure to varying difficulty levels in training data."], "tldr": "Tool-Augmented Large Language Models (TA-LLMs) struggle with incomplete queries and out-of-scope requests, relying on Supervised Fine-Tuning (SFT) with expert trajectories. Early benchmarks focused on successful tool calls, recent ones evaluate user conversations. Current research lacks TA-LLMs, yet reinforcement learning captures preferences in other LLM areas such as WebShop. \n\nThis paper introduces DiaTool-DPO, which enhances TA-LLMs' dialogue via Direct Preference Optimization, modeling interactions as a Markov Decision Process. It categorizes user queries into three types based on dialogue states, automatically constructing paired trajectory datasets and introduces a specialized loss for dialogue control. This method matches GPT-4 performance, improving information gathering and tool call rejection with no human labeling.", "affiliation": "Kakao Corp.", "categories": {"main_category": "Natural Language Processing", "sub_category": "Dialogue Systems"}, "podcast_path": "2504.02882/podcast.wav"}