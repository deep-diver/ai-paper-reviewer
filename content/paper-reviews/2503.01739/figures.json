[{"figure_path": "https://arxiv.org/html/2503.01739/x1.png", "caption": "Figure 1: The glowing firefly: (a) generated by Sora [2] and (b) captured in a real video. The generated firefly is noticeably different from its real-life counterpart and thus unsatisfying. We attribute this primarily to a lack of exposure to such topics.", "description": "The figure shows a comparison between a firefly image generated by the Sora text-to-video model and a real-world firefly image.  The generated firefly appears noticeably different from the real one, highlighting the limitations of current text-to-video models when dealing with topics (like glowing fireflies) not adequately represented in their training data.  The model struggles to accurately represent the visual characteristics of the firefly, suggesting a need for datasets with broader topic coverage.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.01739/x2.png", "caption": "Figure 2: VideoUFO is the first dataset curated in alignment with real-world users\u2019 focused topics for text-to-video generation. Specifically, the dataset comprises over 1.091.091.091.09 million video clips spanning 1,29112911,2911 , 291 topics. Here, we select the top 20202020 most popular topics for illustration. Researchers can use our VideoUFO to train or fine-tune their text-to-video generative models to better meet users\u2019 needs.", "description": "VideoUFO is the first dataset created specifically to align with real-world user interests for text-to-video generation.  It contains over 1.09 million video clips covering 1,291 distinct topics identified from user prompts.  The figure displays example images from the 20 most popular topics within the dataset to illustrate the diverse range of content included. Researchers can use VideoUFO to improve the performance of text-to-video models by training or fine-tuning them on this dataset.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.01739/x3.png", "caption": "Figure 3: The semantic distribution of users\u2019 focused topics. It is visualized by WizMap [39]. Please \\faSearch\u00a0zoom in to see the details.", "description": "Figure 3 visualizes the distribution of user-focused topics extracted from the VidProM dataset.  It uses WizMap to represent these topics semantically, showing relationships and groupings between them. The figure is a visual representation of the clusters of user-provided prompts, illustrating the diversity and interconnectivity of the topics relevant to text-to-video generation. Due to its detailed nature, zooming in is recommended for a comprehensive view.", "section": "3. Curating VideoUFO"}, {"figure_path": "https://arxiv.org/html/2503.01739/x4.png", "caption": "Figure 4: Each data point in our VideoUFO includes a video clip, an ID, a topic, start and end times, a brief caption, and a detailed caption. Beyond that, we evaluate each clip with six different video quality scores from VBench [7].", "description": "Figure 4 shows a sample data point from the VideoUFO dataset.  Each data point contains a video clip, a unique ID, the topic of the video clip, the start and end timestamps of the clip within its original video, a brief caption summarizing the video, and a more detailed caption providing a more in-depth description. In addition to this textual information, VideoUFO provides six video quality scores for each clip using VBench, a video quality assessment tool. These scores evaluate aspects like subject consistency, background consistency, motion smoothness, and more.", "section": "3. Curating VideoUFO"}, {"figure_path": "https://arxiv.org/html/2503.01739/x5.png", "caption": "Figure 5: The statistical information of captions and video clips in the proposed VideoUFO dataset. The average word count for brief and detailed captions is 13.813.8\\mathbf{13.8}bold_13.8 and 155.5155.5\\mathbf{155.5}bold_155.5, respectively, while the average clip duration is 12.612.6\\mathbf{12.6}bold_12.6 seconds.", "description": "Figure 5 presents the statistical distributions of the lengths of brief captions, detailed captions, and video clip durations within the VideoUFO dataset.  The histogram for brief captions shows a mean length of approximately 13.8 words.  The histogram for detailed captions shows a mean length of roughly 155.5 words. Finally, a histogram of video clip durations shows an average length of around 12.6 seconds.  These data provide insights into the characteristics of the text and video content within the VideoUFO dataset.", "section": "3. Curating VideoUFO"}, {"figure_path": "https://arxiv.org/html/2503.01739/x6.png", "caption": "Table 1: The comparison between the proposed VideoUFO with other recent video datasets based on their fundamental attributes. Unlike previous collections, our VideoUFO is derived directly from real user-focused topics and also offers a more flexible license, while remaining comparable to these datasets in other key aspects. \u201c##\\##\u201d and \u201c\u00af\u00afabsent\\ \\bar{}over\u00af start_ARG end_ARG \u201d are abbreviations for \u201cnumbers\u201d and \u201caverage\u201d, respectively.", "description": "This table compares VideoUFO with other recent large-scale video datasets, highlighting key differences in their attributes. VideoUFO is distinguished by its focus on real user-centric topics derived from text-to-video prompts, unlike datasets which collect videos from open domains.  Additionally, VideoUFO offers a more flexible Creative Commons license, unlike others with restrictive licenses. Despite its unique approach to data collection, VideoUFO maintains comparable video characteristics (number of videos, video length, caption length, resolution) to other prominent datasets.  The abbreviations used are: '#' for 'numbers' and '\u2013' for 'average'.", "section": "4. Comparison with Other Video Datasets"}, {"figure_path": "https://arxiv.org/html/2503.01739/x7.png", "caption": "Figure 6: The number of user-focused topics covered by recent video datasets. None successfully includes all user-focused topics.", "description": "Figure 6 is a bar chart comparing the number of user-focused topics covered by various recently published video datasets.  The x-axis lists the dataset names, while the y-axis represents the count of topics covered.  Each bar's height visually indicates how many user-focused topics from the VideoUFO dataset are also present in that specific dataset. The chart highlights that no single existing dataset encompasses all the user-focused topics identified in the VideoUFO dataset.  This emphasizes the novelty and unique contribution of VideoUFO, which addresses a gap in existing resources.", "section": "4. Comparison with Other Video Datasets"}, {"figure_path": "https://arxiv.org/html/2503.01739/x8.png", "caption": "Figure 7: The calculation process of BenchUFO. It is designed to evaluate whether a text-to-video model can effectively generate videos that contain user-focused topics. It comprises 791 concrete noun topics, each paired with 10101010 real-world user-provided prompts.", "description": "The figure illustrates the BenchUFO process, a novel benchmark designed to assess the ability of text-to-video models to generate videos accurately reflecting user-focused topics.  The process involves selecting 791 concrete noun topics, each represented by 10 real-world user-provided prompts. For each prompt, a text-to-video model generates a video, which is then described using a video understanding model. Finally, the similarity between the generated description and the original prompt is calculated to evaluate the model's performance on that specific topic.", "section": "5. VideoUFO Benefits Video Generation"}, {"figure_path": "https://arxiv.org/html/2503.01739/x10.png", "caption": "Table 2: The performance of both publicly available text-to-video models and our trained models on the proposed BenchUFO. The publicly available models are trained on various datasets, including both public and private ones. MVDiT [20] is trained on VidGen [29], OpenVid [20], and VideoUFO, respectively. \u201cLow/Top N\ud835\udc41Nitalic_N\u201d denotes the average score of the worst/best-performing N\ud835\udc41Nitalic_N topics.", "description": "This table presents a benchmark comparing the performance of several text-to-video generation models on user-focused topics.  It shows the average similarity scores between generated video descriptions and original prompts for the worst (Low N) and best (Top N) performing topics.  The models compared include publicly available models trained on various datasets (both public and private) and a model (MVDiT) specifically trained on three different datasets: VidGen, OpenVid, and the authors' VideoUFO dataset.  This allows for an assessment of how well different models perform on a variety of topics, with a focus on identifying those where models struggle.", "section": "5. VideoUFO Benefits Video Generation"}]