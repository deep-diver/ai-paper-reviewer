[{"content": "| System: | A chat between a curious user and an artificial intelligence assistant. |\n|---|---| \n|  | The assistant gives helpful, detailed, and polite answers to the user\u2019s questions. The conversation centers around an indoor scene: `<OBJ001>` $F_{1}^{2d},F_{1}^{v},F_{12}^{e},F_{2}^{v}F_{1}^{v},F_{14}^{e},F_{4}^{v}...`<OBJN>` $F_{N}^{2d},F_{N}^{v},F_{Nk_{1}}^{e},F_{k_{1}}^{v}F_{N}^{v},F_{Nk_{2}}^{e},F_{k_{2}}^{v}$ | \n| User: | According to the given description, *there are brown wooden cabinets*, | \n|  | *placed on the side of the kitchen*, please provide the ID of the object that closely matches this description. | \n| Assistant: | `<OBJ001>`. |", "caption": "Table 1: \nExample of prompt for the language model containing scene graph.", "description": "This table provides an example of how a prompt is structured for the language model when it contains a scene graph. The prompt includes system instructions, user query, and assistant response sections. The scene is described with a sequence of object identifiers and embeddings, along with relationships between them. This structured format allows the language model to process the information from the scene graph effectively to answer user questions.", "section": "3 Method"}, {"content": "## Table 1: Performance comparison on ScanRefer, Multi3DRefer, ScanQA, Sqa3D and Scan2Cap.\n\n| Methods | ScanRefer A@0.25\u2191 | ScanRefer A@0.5\u2191 | ScanRefer F1@0.25\u2191 | ScanRefer F1@0.5\u2191 | Multi3DRefer C\u2191 | Multi3DRefer B-4\u2191 | Multi3DRefer EM\u2191 | ScanQA C@0.5\u2191 | Sqa3D B-4@0.5\u2191 | Scan2Cap B-4@0.5\u2191 |\n|---|---|---|---|---|---|---|---|---|---|---|\n| **Expert models** |  |  |  |  |  |  |  |  |  |  |\n| ScanRefer [Chen et al., 2020] | 37.3 | 24.3 | - | - | - | - | - | - | - | - |\n| MVT [Huang et al., 2022] | 40.8 | 33.3 | - | - | - | - | - | - | - | - |\n| 3DVG-Trans [Zhao et al., 2021] | 45.9 | 34.5 | - | - | - | - | - | - | - | - |\n| ViL3DRel [Chen et al., 2022] | 47.9 | 37.7 | - | - | - | - | - | - | - | - |\n| M3DRef-CLIP [Zhang et al., 2023] | 51.9 | 44.7 | 42.8 | 38.4 | - | - | - | - | - | - |\n| Scan2Cap [Chen et al., 2021] | - | - | - | - | - | - | - | 35.2 | 22.4 | - |\n| ScanQA [Azuma et al., 2022] | - | - | - | - | 64.9 | 10.1 | - | - | - | - |\n| Sqa3D [Ma et al., 2022] | - | - | - | - | - | - | 47.2 | - | - | - |\n| 3D-VisTA [Zhu et al., 2023] | 50.6 | 45.8 | - | - | 72.9 | 13.1 | 48.5 | 66.9 | 34.0 | - |\n| BUTD-DETR [Jain et al., 2022] | 52.2 | 39.8 | - | - | - | - | - | - | - | - |\n| PQ3D [Zhu et al., 2025] | - | 51.2 | - | - | 50.1 | 87.8 | - | 47.1 | 80.3 | 36.0 |\n| **LLM-based models** |  |  |  |  |  |  |  |  |  |  |\n| ZSVG3D [Yuan et al., 2024] | 36.4 | 32.7 | - | - | - | - | - | - | - | - |\n| 3D-LLM(Flamingo) [Hong et al., 2023b] | 21.2 | - | - | - | 59.2 | 7.2 | - | - | - | - |\n| 3D-LLM(BLIP2-flant5) [Hong et al., 2023b] | 30.3 | - | - | - | 69.4 | 12.0 | - | - | - | - |\n| Chat-3D v2 [Huang et al., 2023] | 35.9 | 30.4 | - | - | 77.1 | 7.3 | - | - | - | - |\n| Scene-LLM [Fu et al., 2024] | - | - | - | - | 80.0 | 12.0 | 54.2 | - | - | - |\n| LL3DA [Chen et al., 2023] | - | - | - | - | 76.8 | 13.5 | - | 65.2 | 36.8 | - |\n| Grounded 3D-LLM [Chen et al., 2024] | 47.9 | 44.1 | 45.2 | 40.6 | 72.7 | 13.4 | - | 70.6 | 35.5 | - |\n| Chat-Scene [Huang et al., 2024] | 55.5 | 50.2 | 57.1 | 52.4 | 87.7 | 14.3 | 54.6 | 77.1 | 36.3 | - |\n| **3DGraphLLM Vicuna-1.5 (ours)** | 57.0 | 51.3 | 60.1 | 55.4 | 87.6 | 12.1 | 53.1 | 81.2 | 36.3 | - |\n| **3DGraphLLM LLAMA3-8B (ours)** | 60.2 | 54.6 | 63.0 | 58.2 | 83.1 | 12.5 | 55.2 | 82.9 | 37.8 | - |", "caption": "Table 2: \nPerformance comparison of 3DGraphLLM with state-of-the-art approaches for 3D vision-language tasks. \"Expert models\" use specialized heads to deal with different 3D vision-language tasks. Our approach falls into the category of \"LLM-based models\" that consider different tasks as different user queries to a generative model. C denotes the CIDEr metric.", "description": "Table 2 presents a comparative analysis of 3DGraphLLM's performance against state-of-the-art methods in 3D vision-language tasks.  It highlights the differences between approaches using specialized task heads ('expert models') versus those using a large language model (LLM) that handles various tasks through different queries ('LLM-based models'). 3DGraphLLM is categorized as an LLM-based model.  The table showcases results across several benchmark datasets, encompassing metrics such as accuracy at different Intersection over Union (IoU) thresholds (Acc@0.25, Acc@0.5), F1 scores (F1@0.25, F1@0.5), CIDEr scores (C), and BLEU scores (B-4), providing a comprehensive evaluation of each model's capabilities.", "section": "4 Experiments"}, {"content": "| Methods | Pre-train | of edges | ScanRefer Acc@0.5\u2191 | ScanRefer F1@0.5\u2191 | Multi3DRefer C\u2191 | Multi3DRefer B-4\u2191 | ScanQA EM\u2191 | Sqa3D C@0.5\u2191 | Scan2Cap B-4@0.5\u2191 | Scan2Cap C@0.5\u2191 |\n|---|---|---|---|---|---|---|---|---|---|---|\n| 3DGraphLLM-0 Vicuna1.5 | \u2717 | 0 | 50.2 | 52.4 | 87.7 | 14.3 | 54.6 | 77.1 | 36.3 |  |\n| 3DGraphLLM-2 Vicuna1.5 | \u2717 | 2 | 50.1 | 52.7 | 92.2 | 15.5 | 54.7 | 80.4 | 36.9 |  |\n| **3DGraphLLM-2 Vicuna1.5** | \u2713 | **2** | **51.3** | **55.4** | 87.6 | 12.1 | 53.1 | **81.2** | **36.3** |  |\n| 3DGraphLLM-0 LLAMA3-8B | \u2717 | 0 | 52.0 | 55.1 | 84.0 | 15.8 | 53.8 | 80.0 | 37.5 |  |\n| 3DGraphLLM-2 LLLAMA3-8B | \u2717 | 2 | 54.3 | 57.3 | **87.4** | 14.9 | 54.5 | **85.6** | **39.6** |  |\n| **3DGraphLLM-2 LLLAMA3-8B** | \u2713 | **2** | **54.6** | **58.2** | 83.1 | 12.5 | **55.2** | 82.9 | 37.8 | ", "caption": "Table 3: Ablation study on semantic edges role and training pipeline. C denotes the CIDEr metric.", "description": "This table presents the results of an ablation study investigating the impact of incorporating semantic relationships (edges) in the 3D scene graph representation used by the 3DGraphLLM model.  It compares the performance of different model variants with varying numbers of edges and training strategies (pre-training included or not) across five different 3D vision-language tasks: ScanRefer, Multi3DRefer, ScanQA, Sqa3D, and Scan2Cap.  The metrics used to assess performance include accuracy (Acc), F1 score (F1), CIDEr (C), BLEU-4 (B-4), and exact match (EM), providing a comprehensive evaluation of the model's effectiveness across various tasks and different numbers of edges in the graph.", "section": "4.2 Ablation Studies. Role of Semantic Relations and Training Pipeline"}, {"content": "| Methods | Instance segmentation | Number of edges | Minimal distance, cm | Acc@0.25\u2191 | Acc@0.5\u2191 |\n|---|---|---|---|---|---| \n| 3DGraphLLM-0 | GT | 0 | - | 48.9 | 48.9 |\n| **3DGraphLLM-2** | **GT** | **2** | 0 | **54.4(+5.6%)** | **54.4(+5.6%)** |\n| 3DGraphLLM-0 | Mask3D | 0 | - | 46.0 | 34.2 |\n| 3DGraphLLM-2 | Mask3D | 2 | 0 | 47.3(+1.3%) | 35.6(+1.4%) |\n| 3DGraphLLM-2 | Mask3D | 2 | 1 | 48.0(+2.0%) | 36.2(+2.0%) |\n| **3DGraphLLM-2** | **Mask3D (+ NMS)** | **2** | **1** | **48.1(+2.1%)** | **36.5(+2.3%)** |\n| 3DGraphLLM-0 | OneFormer3D | 0 | - | 45.4 | 34.5 |\n| 3DGraphLLM-2 | OneFormer3D | 2 | 0 | 47.1(+1.7%) | 35.7(+1.2%) |\n| **3DGraphLLM-2** | **OneFormer3D (+NMS)** | **2** | **1** | **47.5(+2.1%)** | **36.1(+1.6%)** |", "caption": "Table 4: Ablation study on semantic edges role depending on quality of instance segmentation.", "description": "This table presents the results of an ablation study investigating the impact of using semantic relationships (edges) in a scene graph on the performance of the 3DGraphLLM model.  The study varies the quality of instance segmentation used to generate the scene graph, comparing ground truth segmentation with results from two state-of-the-art instance segmentation models (Mask3D and OneFormer3D).  For each segmentation method, experiments are conducted with and without using k-nearest neighbor subgraphs and a minimum distance filter to mitigate the impact of noisy segmentation. The table shows how the accuracy of the model (measured by Acc@0.25 and Acc@0.5) on the ScanRefer benchmark is affected by these changes in scene graph representation and segmentation quality.", "section": "4.2 Ablation Studies. Role of Semantic Relations and Training Pipeline"}, {"content": "| Methods | Edge Number | Spatial relation | Acc@0.5\u2191 |\n|---|---|---|---|\n| 3DGraphLLM | 0 | \u2713 | 42.6 |\n| 3DGraphLLM | 2 | \u2713 | 48.9(+6.3%) |\n| 3DGraphLLM | 2 | \u2717 | **50.1(+7.5%)** |", "caption": "Table 5: Ablation study on spatial relation module on RioRefer dataset (GT Instance segmentation).", "description": "This table presents the results of an ablation study focusing on the impact of incorporating spatial relationship information into the 3DGraphLLM model.  Specifically, it investigates the effect of adding a spatial relation module on the accuracy of 3D object grounding. The study uses the RioRefer dataset and relies on ground truth (GT) instance segmentation for object identification.  The table shows how different configurations of the spatial relation module, indicated by the number of edges used, affect the overall performance as measured by Accuracy@0.5.", "section": "4.3 Ablation Studies. 3D Scene Graph Representation"}]