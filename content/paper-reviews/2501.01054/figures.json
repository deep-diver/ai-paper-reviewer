[{"figure_path": "https://arxiv.org/html/2501.01054/x1.png", "caption": "Figure 1: Scaling the quantities of unit tests for majority voting leads to improvements in performance across different policy models and reward models. Policy refers to the model that produces code solutions, while reward denotes the model that generates unit tests.", "description": "This figure displays the results of an experiment investigating the impact of scaling the number of unit tests on the performance of a majority voting system for selecting optimal code solutions.  Multiple lines are shown, each representing a different combination of a model generating code solutions (the 'policy' model) and a model generating unit tests (the 'reward' model). The x-axis represents the number of unit tests used, and the y-axis shows the percentage of problems solved correctly. The results demonstrate that increasing the number of unit tests generally improves the accuracy of identifying correct solutions, regardless of the specific policy and reward models used.  This improvement is more pronounced for larger numbers of code solution candidates. ", "section": "Pioneer Experiment"}, {"figure_path": "https://arxiv.org/html/2501.01054/x2.png", "caption": "Figure 2: \nThe correlation between the quantities of unit tests and the performance on different unit test generators (reward model) with 200200200200 candidate code solutions.", "description": "This figure shows the relationship between the number of unit tests used and the success rate of identifying the best code solution among 200 candidates.  Different large language models (LLMs) were used as \"reward models\" to generate the unit tests. The results illustrate how the quality of the reward signal, and thus the accuracy of selecting the optimal solution, improves with an increasing number of unit tests.", "section": "Pioneer Experiment"}, {"figure_path": "https://arxiv.org/html/2501.01054/x3.png", "caption": "Figure 3: \nThe improvements of best-of-N performance on problems of different difficulties.\nQuintile 1 (easiest) has the highest pass rate, while Quintile 2 (hardest) has the lowest pass rate.\nScaling the quantity of unit tests significantly improves the accuracy on more complex problems.", "description": "Figure 3 illustrates how increasing the number of unit tests affects the success rate of solving coding problems with varying difficulty levels.  The problems are categorized into five quintiles based on their difficulty, with Quintile 1 representing the easiest problems (highest pass rate) and Quintile 5 representing the hardest problems (lowest pass rate).  The graph shows that as the number of unit tests increases, the problem-solving accuracy improves significantly for all difficulty levels. However, the improvement is more substantial for the harder problems (Quintiles 4 and 5), demonstrating the effectiveness of scaling unit tests, particularly in tackling more challenging tasks.", "section": "Pioneer Experiment"}, {"figure_path": "https://arxiv.org/html/2501.01054/x4.png", "caption": "Figure 4: Overview for efficient and high-quality unit test scaling. First, we train a lightweight unit test generator based on high-quality synthetic data. Subsequently, we employ dynamic unit test scaling to further improve efficiency.", "description": "This figure illustrates the process of CodeRM-8B, a system for efficient and high-quality unit test scaling.  It begins with dataset preprocessing and unit test generation to create high-quality synthetic training data.  This data is used to train a lightweight unit test generator. Subsequently, a dynamic unit test scaling mechanism is employed to adjust the number of unit tests based on problem difficulty, thus enhancing efficiency. The overall system aims to improve the accuracy and efficiency of identifying correct code solutions by generating a sufficient and adaptive number of unit tests.", "section": "3 Towards Efficient and High-Quality Unit Test Scaling"}, {"figure_path": "https://arxiv.org/html/2501.01054/x5.png", "caption": "Figure 5: The performance of three different unit test generators (reward model) on different quantities of unit tests, while employing Llama3-8B as the policy model.", "description": "This figure illustrates the impact of scaling the number of unit tests on the performance of different unit test generators.  The experiment uses Llama3-8B as the code generation model (policy model), while three different LLMs serve as unit test generators (reward models). The x-axis represents the number of unit tests used, and the y-axis shows the percentage of problems solved. The results demonstrate how the performance of each reward model changes as the number of unit tests increases. This analysis helps to understand the effect of scaling unit tests on the quality of reward signals used for selecting accurate code solutions.", "section": "4.2 Main Results"}, {"figure_path": "https://arxiv.org/html/2501.01054/x6.png", "caption": "Figure 6: Best-of-N performance comparison under unit test scaling with three computation budget allocation strategies: dynamic allocation with gold pass rate, dynamic allocation with predicted pass rate, and equal allocation.", "description": "Figure 6 illustrates the performance comparison of three different computation budget allocation strategies in a best-of-N scenario, where N is the number of code solutions generated by a language model. The three strategies are: 1) Dynamic allocation with gold pass rate, which uses the actual pass rates of solutions to guide resource allocation; 2) Dynamic allocation with predicted pass rate, which utilizes a predicted pass rate of solutions to determine resource allocation; and 3) Equal allocation, which distributes the computation budget evenly among all problems.  The figure demonstrates the impact of these strategies on the success rate (y-axis) as the computation budget (x-axis) increases, showing how the different methods affect performance and resource utilization on two different benchmarks, HumanEval Plus and MBPP Plus.", "section": "4.2 Main Results"}, {"figure_path": "https://arxiv.org/html/2501.01054/x7.png", "caption": "Figure 7: The effects of data size.", "description": "This figure demonstrates the impact of the size of the training dataset on the performance of the CodeRM-8B model.  It shows that increasing the size of the training data leads to significant improvements in the model's performance on both the HumanEval Plus and MBPP Plus benchmarks.  This highlights the importance of high-quality and sufficiently large datasets in achieving optimal performance for unit test generation models.", "section": "3.1 Unit Test Generator"}, {"figure_path": "https://arxiv.org/html/2501.01054/x8.png", "caption": "Figure 8: The performance gain of scaling the number of unit tests on problems of different difficulties across various policy model and reward model.\nOverall, increasing the number of unit tests yields greater performance improvements on more challenging problems, particularly when employing Llama3.1-70B as the policy model.", "description": "Figure 8 presents a detailed analysis of how scaling the number of unit tests impacts performance on problems with varying difficulty levels.  The results are shown across four different combinations of policy models (which generate code solutions) and reward models (which generate unit tests). The x-axis represents the problem's difficulty, ranging from easiest to hardest. The y-axis shows the performance gain achieved by increasing the number of unit tests, with each line representing a different number of tests.  The heatmap visualization clearly illustrates that the performance gains from scaling unit tests are significantly higher for more challenging problems. This effect is particularly pronounced when using the Llama3.1-70B model as the policy model.", "section": "Quality of Generated Unit Tests"}, {"figure_path": "https://arxiv.org/html/2501.01054/x9.png", "caption": "Figure 9: An example of the training data for the unit test generator.", "description": "This figure shows an example from the training data used to train the unit test generator.  The example includes the instructions given to the large language model (LLM), the code that the LLM was asked to test, and the resulting unit test code that the LLM generated. The instructions specify the task: to generate unit tests for a given function (here, `expected_strangle_return`).  The code shows the function definition, which calculates the expected return of a financial strategy called a 'strangle.' The unit test code then demonstrates testing various scenarios (positive, negative, and zero return) to ensure the function performs correctly under different conditions.  The example highlights the expected input parameters and the expected return values for each test case.", "section": "3.1 Unit Test Generator"}]