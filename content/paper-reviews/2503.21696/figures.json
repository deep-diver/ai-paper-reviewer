[{"figure_path": "https://arxiv.org/html/2503.21696/x1.png", "caption": "Figure 1: We design an embodied interactive task: searching for objects in an unknown room. Then we propose Embodied-Reasoner, which presents spontaneous reasoning and interaction ability. Before each action, it generates diverse thoughts, e.g., self-reflection or spatial reasoning, forming an image-text interleaved trajectory. It shows consistent reasoning and efficient search behaviors, whereas OpenAI\no3-mini often exhibits repetitive searches and logical inconsistencies with higher failure rates.", "description": "The figure illustrates an embodied interactive task where an agent searches for objects within an unfamiliar room.  The Embodied-Reasoner model is introduced, showcasing its ability to engage in spontaneous reasoning and interact with the environment.  Before each action, the model generates diverse thoughts (e.g., self-reflection, spatial reasoning), creating an image-text interleaved trajectory.  This approach results in consistent reasoning and efficient search strategies.  In contrast, the figure highlights how OpenAI's o3-mini model frequently performs repetitive searches and demonstrates logical inconsistencies, leading to a higher failure rate in completing the task.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.21696/x2.png", "caption": "Figure 2: Embodied-Reasoner exhibits spontaneous thinking behaviors, e.g., analyzing environmental states (#1,3), reflecting on missed details (#4), reasoning based on the latest observations (#5), and recalling cues for efficient planning (#9). These thoughts remain coherent and logically consistent despite spanning multiple rounds. In contrast, general VLMs lacking thinking abilities struggle with long-horizon interactive tasks and produce unreasonable actions, e.g., forget tasks or repetitive searching.", "description": "The figure showcases the Embodied-Reasoner's superior performance in handling complex, interactive tasks compared to traditional Vision-Language Models (VLMs). It highlights the model's ability to generate spontaneous and coherent thoughts (analysis, reflection, planning) across multiple steps.  Specifically, it demonstrates how Embodied-Reasoner effectively analyzes the environment (#1, #3), considers previously missed information (#4), reasons using the latest observations (#5), and recalls previous cues to create efficient plans (#9).  This contrasts with traditional VLMs, which often struggle with long-horizon tasks, leading to inconsistent or illogical actions, such as forgetting tasks or repetitive searching.", "section": "2. Observation-Thought-Action Corpora"}, {"figure_path": "https://arxiv.org/html/2503.21696/x3.png", "caption": "Figure 3: Left: Data Engine for <<<Instruction, Interactive Trajectory>>> synthesis. First, we synthesize instructions from task templates, and build an affiliation graph from scene\u2019s meta-data. It enables us to derive key actions needed for task. We add exploratory actions and insert thinking thoughts between observation and actions. Right: Three-stage training recipe. \u2460We fine-tune on synthesized trajectory to develop interaction skills. \u2461We sample multiple trajectories on novel tasks and evaluate their correctness. The successful ones are used for developing its exploring abilities. \u2462We continue to sample trajectories using updated model, injecting anomalous states and reflective thoughts in successful cases and correcting errors in failed ones. This self-correction training yields Embodied-Reasoner.", "description": "Figure 3 illustrates the process of creating a dataset for training an embodied reasoning model and the three-stage training process used. The left side shows how instructions are generated from task templates and an affiliation graph, which represents relationships between objects, is built.  Exploratory actions and interleaved thoughts are then added to create interactive trajectories.  The right side depicts the three-stage training recipe: 1) imitation learning using the synthesized trajectories, 2) self-exploration through rejection sampling to enhance exploration abilities, and 3) self-correction by adding anomalous states and reflective thoughts to refine the model's behavior.  The final outcome is the Embodied-Reasoner model.", "section": "2. Embodied Interactive Task"}, {"figure_path": "https://arxiv.org/html/2503.21696/x4.png", "caption": "Figure 4: We analyze the frequency of five types of thoughts and their flexible transition relationships in all trajectories.", "description": "This figure visualizes the frequency of five different types of thought processes (Situation Analysis, Task Planning, Spatial Reasoning, Self-Reflection, and Double Verification) within the generated embodied reasoning trajectories.  It also shows the dynamic transitions between these thought types, highlighting their flexible and interconnected nature within the problem-solving process. This demonstrates the model's ability to adapt its reasoning approach depending on the task's demands and the current situation.", "section": "2. Observation-Thought-Action Corpora"}, {"figure_path": "https://arxiv.org/html/2503.21696/x5.png", "caption": "Figure 5: Relations between task length and success rate, and output token number. As task complexity increases, our model generates more reasoning tokens to maintain high success rates.", "description": "Figure 5 illustrates the relationship between task complexity, model performance, and the number of reasoning tokens generated.  The x-axis represents task length (number of key actions required), indicating increasing complexity. The y-axis shows two key metrics: success rate and the number of reasoning tokens produced by the model.  The figure demonstrates that as task complexity increases (longer task lengths), the success rate of baseline models drops significantly. However, the Embodied-Reasoner model maintains high success rates even for complex tasks, achieving this by generating a substantially larger number of reasoning tokens. This suggests that the model leverages more extensive reasoning to tackle more challenging problems, showcasing the effectiveness of its deep-thinking mechanism.", "section": "5. Main Results"}, {"figure_path": "https://arxiv.org/html/2503.21696/x6.png", "caption": "Figure 6: Repetitive Exploration Rate measures repetitive search issues, which are often observed in baselines. Our models reduce repetitive searches by recalling and reflecting on past trajectories.", "description": "This figure illustrates the results of evaluating the models' tendency to repeatedly explore the same areas during a search task.  The x-axis represents different task types (search, manipulate, transport, composite, overall), while the y-axis shows the percentage of repetitive explorations.  The bars indicate the repetitive exploration rate (RER) for various models including the authors' proposed Embodied-Reasoner and Embodied-Explorer, as well as several baseline models (GPT-40, Claude 3.5-Sonnet, Gemini-2.0 Flash Thinking, Qwen-VL-Max, GPT-03-mini, and two versions of Qwen2.5-VL-72B). The lower the RER, the more efficient the search strategy.  This figure highlights that the proposed models significantly reduce repetitive searches compared to baseline models, demonstrating the efficiency of their planning and self-reflection capabilities in avoiding unnecessary exploration.", "section": "5.1 Main Results"}, {"figure_path": "https://arxiv.org/html/2503.21696/x7.png", "caption": "Figure 7: Real-world experiments. Our model achieves a higher success rate (56.7%) than OpenAI o3-mini (43.4%) and o1 (50%).", "description": "The figure showcases a comparison of the success rates achieved in real-world experiments across three different models.  Embodied-Reasoner demonstrated a significantly higher success rate (56.7%) compared to OpenAI's O3-mini (43.4%) and O1 (50%).  This highlights the model's improved performance in real-world settings for object searching tasks.", "section": "5.3. Real-world Experiments"}, {"figure_path": "https://arxiv.org/html/2503.21696/x8.png", "caption": "Figure C1: The distribution of the training dataset with 9,390 samples, including 4 task types and 10 sub-task types.", "description": "This figure shows a breakdown of the 9,390 samples in the training dataset.  It visualizes the proportions of four main task types (Search, Manipulate, Transport, Composite) and their further subdivisions into ten sub-task types.  The sizes of the sections in the circular diagram represent the relative number of samples belonging to each category. This provides a clear overview of the dataset's composition and the distribution of various task complexities.", "section": "C. Dataset Details"}, {"figure_path": "https://arxiv.org/html/2503.21696/x9.png", "caption": "Figure C2: The distribution of the test set with 809 tasks, including 4 task types and 11 sub-task types.", "description": "This figure shows a breakdown of the 809 tasks used in the test set of the Embodied-Reasoner model.  It details the distribution of tasks across four main task types (Search, Manipulate, Transport, Composite) and further specifies the distribution within each main type according to 11 sub-task categories. This provides a visual representation of the complexity and diversity of tasks the model was evaluated on.", "section": "4. Dataset Statistics"}, {"figure_path": "https://arxiv.org/html/2503.21696/x10.png", "caption": "Figure C3: The distribution of the training set interactions, including 8 interaction types in trajectories: navigate to, pickup, open, close, put in, observe, move forward, and toggle.", "description": "Figure C3 shows a breakdown of the actions taken within the training dataset's trajectories.  It displays the frequency of eight distinct interaction types: 'navigate to', 'pickup', 'open', 'close', 'put in', 'observe', 'move forward', and 'toggle'.  The bar chart visually represents the number of times each action was performed across all trajectories in the training dataset, providing insights into the distribution of actions within the embodied interactive tasks.", "section": "4. Dataset Statistics"}, {"figure_path": "https://arxiv.org/html/2503.21696/x11.png", "caption": "Figure C4: The distribution of the test set interactions, including 6 interaction types in key actions: navigate to, pickup, open, close, put in, and toggle.", "description": "Figure C4 shows the frequency of six different interaction types within the key actions of the test dataset.  The six interaction types are: 'navigate to', 'pickup', 'open', 'close', 'put in', and 'toggle'. The chart visually represents the number of times each action occurred in the test set's key action sequences, providing insights into the relative frequency of different action types during task execution.", "section": "4. Dataset Statistics"}, {"figure_path": "https://arxiv.org/html/2503.21696/x12.png", "caption": "Figure C5: The quantity distribution of trajectory lengths in the training set and the corresponding task type composition, where Search Task is mainly within 1-9, Manipulate Task within 2-11, Transport Task within 3-14, and Composite Task above 8, extending beyond 23.", "description": "This figure shows the distribution of trajectory lengths for different task types in the training dataset.  The x-axis represents the length of the trajectory (number of actions taken), and the y-axis shows the number of tasks with that trajectory length.  Each colored bar represents a different task type: Search, Manipulate, Transport, and Composite. The figure highlights that Search tasks tend to have shorter trajectories (mostly between 1 and 9 actions), Manipulate tasks have slightly longer trajectories (between 2 and 11 actions), Transport tasks are longer still (between 3 and 14 actions), and Composite tasks have the longest trajectories, often exceeding 23 actions.", "section": "Dataset Statistics"}, {"figure_path": "https://arxiv.org/html/2503.21696/x13.png", "caption": "Figure C6: The quantity distribution of key action lengths in the test set and the corresponding task type composition, where Search Task is mainly within 1-2, Manipulate Task within 2, 4-5, Transport Task within 4-7, and Composite Task above 8, extending beyond 19.", "description": "Figure C6 shows the distribution of the number of key actions needed to complete tasks in the test dataset.  The x-axis represents the length of the action sequence, and the y-axis shows the count of tasks.  Each bar is further divided into four colors representing the four task types: Search, Manipulate, Transport, and Composite.  The figure reveals that Search tasks generally require 1\u20132 actions, Manipulate tasks 2\u20135, Transport tasks 4\u20137, and Composite tasks 8 or more, with some extending beyond 19 actions.", "section": "Dataset Statistics"}, {"figure_path": "https://arxiv.org/html/2503.21696/x14.png", "caption": "Figure C7: The quantity distribution of the top 32 object types in the training dataset\u2019s trajectories, with Others representing the remaining 62 categories, such as Bread, Book, DeskLamp, etc.", "description": "This figure shows the frequency distribution of the top 32 most common object types across all trajectories in the training dataset.  The 'Others' category encompasses the remaining 62 less frequent object types, examples of which include Bread, Book, and DeskLamp.  The visualization helps to understand the prevalence of different object types within the simulated environments used for training the embodied reasoning model.", "section": "C. Dataset Details"}, {"figure_path": "https://arxiv.org/html/2503.21696/x15.png", "caption": "Figure C8: The quantity distribution of the top 32 object types in the test set\u2019s key actions, with Others representing the remaining 44 categories, such as Watch, Pencil, Cup, etc.", "description": "Figure C8 shows the frequency distribution of the top 32 most frequently appearing object types within the key actions of the test set.  The chart visually represents the number of times each object type is involved in the key action sequences during testing.  The category \"Others\" encompasses the remaining 44 object types that did not rank within the top 32, including items like watches, pencils, cups, etc. This visualization helps to understand the prevalence of various object types in the tasks and the overall composition of the test dataset.", "section": "C. Dataset Details"}, {"figure_path": "https://arxiv.org/html/2503.21696/x26.png", "caption": "Figure F9: Trajectory Case for Embodied Reasoner", "description": "This figure visualizes a step-by-step example of Embodied Reasoner completing a complex task. It showcases the model's ability to generate coherent reasoning tokens (thoughts), plan actions, and execute them successfully in a simulated environment. Each step includes an image from the agent's perspective, followed by the model's reasoning process and the selected action. The process demonstrates capabilities like spatial understanding, planning, and self-reflection.", "section": "F. Case Study"}, {"figure_path": "https://arxiv.org/html/2503.21696/x27.png", "caption": "Figure F10: Trajectory Case for GPT-o1", "description": "This figure shows a step-by-step breakdown of GPT-01's performance on the task of placing a laptop on a sofa and then a cellphone in a drawer.  It highlights GPT-01's struggles with task completion due to issues like forgetting the task objective, getting stuck in action loops, and failing to appropriately respond to feedback regarding illegal actions or unavailable objects.  The figure contrasts with Figure F9, which showcases Embodied-Reasoner's superior performance on the same task.", "section": "F. Case Study"}, {"figure_path": "https://arxiv.org/html/2503.21696/x28.png", "caption": "Figure F11: Trajectory Case for Embodied Reasoner in Real World", "description": "This figure showcases a real-world application of the Embodied Reasoner model.  The task is to place a carton of milk on a coffee table. The figure depicts a sequence of images showing the robot's actions and the model's thought process at each step.  The model begins by locating the milk in the refrigerator, retrieves it, and then, after a moment of re-evaluation of the environment (to make sure the coffee table's location is clearly identified), places it on the coffee table and concludes the task.", "section": "5.3. Real-world Experiments"}]