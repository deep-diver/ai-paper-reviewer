{"references": [{"fullname_first_author": "Jinze Bai", "paper_title": "Qwen technical report", "publication_date": "2023-09-16", "reason": "This paper introduces Qwen, a large language model (LLM) which serves as the base model upon which SynerGen-VL is built."}, {"fullname_first_author": "Zheng Cai", "paper_title": "InternLM2 technical report", "publication_date": "2024-03-17", "reason": "SynerGen-VL uses the same text tokenizer and conversation format as InternLM2-1.8B"}, {"fullname_first_author": "Patrick Esser", "paper_title": "Taming transformers for high-resolution image synthesis.", "publication_date": "2021-01-01", "reason": "The image tokenizer employed by SynerGen-VL is based on VQ tokenizers, for which this paper is a foundational work."}, {"fullname_first_author": "Xinlong Wang", "paper_title": "Emu3: Next-token prediction is all you need.", "publication_date": "2024-09-18", "reason": "Emu3 is a state-of-the-art encoder-free unified MLLM that SynerGen-VL draws inspiration from and compares its performance against, especially regarding the use of its discrete image tokenizer."}, {"fullname_first_author": "Zhe Chen", "paper_title": "How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites.", "publication_date": "2024-04-30", "reason": "This work provides InternVL-1.5, an important dataset used for alignment pre-training data and joint instruction tuning in SynerGen-VL and serves as a key comparison model."}]}