[{"figure_path": "https://arxiv.org/html/2412.09604/x1.png", "caption": "Figure 1: Comparison among exemplary unified MLLMs for synergizing image understanding and generation tasks. Compared with methods (a)\u223csimilar-to\\sim\u223c(d) that incorporate complicated designs of model architectures, training methods, and the use of external pretrained diffusion models, (e) encoder-free unified MLLMs adopt a simple design that uses the simple next token prediction framework for both images understanding and generation tasks, allowing for broader data distribution and better scalability.", "description": "This figure provides a comparison of different unified Multimodal Large Language Models (MLLMs) architectures designed for both image understanding and generation tasks.  The architectures are categorized into two groups: (a) through (d), which employ complex designs involving external diffusion models, distinct training objectives, or specialized encoders, and (e), which represents encoder-free unified MLLMs. The encoder-free approach (e) is highlighted for its simplicity, utilizing a shared next-token prediction framework for both understanding and generation, and offering advantages in scalability and data distribution.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2412.09604/x2.png", "caption": "Figure 2: Comparision between SynerGen-VL and previous encoder-free unified MLLMs. SynerGen-VL adopts a token folding and unfolding mechanism and vision experts to build a strong and simple unified MLLM. With the same image context length, SynerGen-VL can support images of much higher resolutions, ensuring the performance of both high-resolution image understanding and generation.", "description": "SynerGen-VL uses a token folding and unfolding mechanism and vision experts, which allows it to support higher resolution images compared to other encoder-free unified MLLMs with the same image context length. Other methods such as Chameleon and Emu3 lack these features.  The diagram compares SynerGen-VL with other encoder-free unified MLLMs by illustrating the token processing stages in each architecture. It shows that SynerGen-VL can handle images with 4 times the resolution (4H x 4W, resulting in 4096 tokens after tokenization) while maintaining the same input context length for the MLLM (256 tokens) as other models that work with lower resolution images (H x W, 256 tokens after tokenization). SynerGen-VL incorporates a token folding mechanism to compress the visual token sequence (from 4096 to 256 tokens), effectively reducing its length, and it employs a shallow autoregressive transformer head to reconstruct detailed image sequences during generation using a token unfolding mechanism (from 256 to 4096 tokens). Additionally, SynerGen-VL introduces Vision Expert MLLMs to enhance visual capabilities within the framework.", "section": "3. SynerGen-VL"}, {"figure_path": "https://arxiv.org/html/2412.09604/x3.png", "caption": "Figure 3: Overview of the proposed SynerGen-VL. The image and text are represented as discrete tokens, and modeled with a single LLM and unified next-token prediction paradigm. Text and vision expert FFNs are introduced to incorporate visual capabilities into the pretrained LLM. To support processing high-resolution images, the input image token sequence is folded to reduce its length, and unfolded by a shallow autoregressive Transformer head to generate images.", "description": "SynerGen-VL is a unified MLLM capable of image understanding and generation through next-token prediction. It represents images and text as discrete tokens, processed by an LLM enhanced with text and vision expert FFNs. To handle high-resolution images, it uses a token folding mechanism, reducing the input sequence length, and a shallow autoregressive Transformer head unfolds the tokens for image generation.", "section": "3.1. Architecture"}, {"figure_path": "https://arxiv.org/html/2412.09604/x4.png", "caption": "Figure 4: Cosine similarity of visual features between generation and understanding tasks across different layers. The representations of the image understanding and generation tasks are similar in shallow layers but disentagle in deeper layers.", "description": "This figure, located in Section 5.3 (Analysis of Relationship Between Image Generation and Understanding), visualizes the cosine similarity between visual features of image generation and understanding tasks across different layers of SynerGen-VL. The x-axis represents the layer depth, while the y-axis represents the cosine similarity. The plot shows high similarity in shallow layers, indicating shared representations. However, the similarity decreases significantly in deeper layers, suggesting a disentanglement of representations as the model processes information specific to each task.", "section": "5.3 Analysis of Relationship Between Image Generation and Understanding"}, {"figure_path": "https://arxiv.org/html/2412.09604/x5.png", "caption": "Figure 5: Attention map visualization of understanding and generation tasks. In the second and fourth rows, we visualize a query token (red) and its attended tokens (blue) in the input image. Each token corresponds to a horizontal rectangular area in the original image due to the 2\u00d74242\\times 42 \u00d7 4 token folding. Darker blue indicates larger attention weights.", "description": "This figure visualizes the attention maps of an encoder-free multimodal large language model (MLLM) for both image understanding and generation tasks. The model uses a token folding mechanism, where each token represents a 2x4 rectangular area in the original image.  The first two rows show attention maps for the understanding task, while the last two rows show attention maps for the generation task. Each visualization includes attention maps for four different layers of the MLLM (layers 4, 12, 20, and 24). Within each layer visualization, the red dot indicates the query token, and the blue dots indicate the tokens attended to by the query token. The darker the blue color, the higher the attention weight.  Locality is observed at early layers, where tokens attend mostly to nearby tokens. Global interactions emerge at later layers. The attention visualization suggests that locality is more prominent in the generation task, while global context is more crucial for the understanding task.", "section": "5.3. Analysis of Relationship Between Image Generation and Understanding"}, {"figure_path": "https://arxiv.org/html/2412.09604/x6.png", "caption": "Figure 6: Qualitative results of image generation. The images are of size 512\u00d7512512512512\\times 512512 \u00d7 512.", "description": "Qualitative examples of 512x512 images generated by SynerGen-VL, showcasing its ability to generate images from various text prompts such as landscapes, character portraits, and object close-ups.  The examples include detailed descriptions of the generated images within the figure.", "section": "4. Experiments"}]