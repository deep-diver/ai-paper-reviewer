{"importance": "This research introduces UnifyEdit, a novel method to balance fidelity and editability without task-specific training, which is crucial for practical applications. It offers an alternative to existing attention injection methods and opens new avenues for exploring attention control in diffusion models.", "summary": "UnifyEdit: Tuning-free image editing balancing fidelity and editability via unified latent diffusion, achieving robust structure preservation and text alignment.", "takeaways": ["UnifyEdit achieves a balance between fidelity and editability.", "Attention-based constraints are effective for balancing fidelity and editability.", "Adaptive time-step scheduler resolves gradient conflicts, guiding diffusion latent to an optimal balance."], "tldr": "Text-based image editing aims to modify images based on text prompts while preserving the original content. Existing methods often struggle to balance **fidelity (preserving original content) and editability (making desired changes)**, leading to over- or under-editing. These methods rely on attention injections for structure preservation but lack explicit mechanisms to balance these objectives, often requiring manual hyperparameter tuning.\n\nThis paper introduces a tuning-free method that performs diffusion latent optimization using two attention-based constraints: a **self-attention (SA) preservation constraint for structural fidelity and a cross-attention (CA) alignment constraint for editability**. To prevent gradient conflicts, an **adaptive time-step scheduler dynamically adjusts the influence of these constraints**. The method is validated through experiments, demonstrating superior balance between structure preservation and text alignment across various editing tasks.", "affiliation": "Communication University of China", "categories": {"main_category": "Computer Vision", "sub_category": "Image Generation"}, "podcast_path": "2504.05594/podcast.wav"}