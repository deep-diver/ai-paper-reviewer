[{"content": "| Loss | # modes \u2191 | D<sub>KL</sub> \u2193 |\n|---|---|---|\n| RpGAN +R<sub>1</sub>+R<sub>2</sub> | **1000** | **0.0781** |\n| GAN +R<sub>1</sub>+R<sub>2</sub> | 693 | 0.9270 |\n| RpGAN +R<sub>1</sub> | Fail | Fail |\n| GAN +R<sub>1</sub> | Fail | Fail |", "caption": "Table 1: Effect of our simplification and modernization efforts evaluted on FFHQ-256.", "description": "This table presents the results of a series of experiments conducted on the FFHQ-256 dataset to evaluate the impact of simplifying and modernizing the StyleGAN2 architecture. It compares several configurations, starting with the original StyleGAN2 and progressively removing features (tricks) and updating the backbone architecture with modern components. The configurations are denoted as Config A through Config E, with Config A representing the original StyleGAN2, and Config E representing the final, simplified and modernized model. The table shows the Fr\u00e9chet Inception Distance (FID), which measures the quality of generated images, and the number of trainable parameters for both the generator (G) and discriminator (D) in each configuration. The results illustrate how the removal of unnecessary features and the incorporation of modern architectural designs can improve the performance of GANs while reducing the complexity of the model.", "section": "3 A Roadmap to a New Baseline R3GAN"}, {"content": "| Configuration | FID\u2193 | G #params | D #params |\n|---|---|---|---| \n| StyleGAN2 | 7.516 | 24.767M | 24.001M |\n| Stripped StyleGAN2 |  |  |  |  |\n|  - z normalization<br> - Minibatch stddev<br> - Equalized learning rate<br> - Mapping network<br> - Style injection<br> - Weight mod / demod<br> - Noise injection<br> - Mixing regularization<br> - Path length regularization<br> - Lazy regularization | 12.46 | 18.890M | 23.996M |\n| Well-behaved Loss | 12.46 | 18.890M | 23.996M |\n| + RpGAN loss | 11.77 | 18.890M | 23.996M |\n| + R<sub>2</sub> gradient penalty | 11.65 | 18.890M | 23.996M |\n| ConvNeXt-ify pt. 1 |  |  |  |  |\n| + ResNet-ify G & D | 10.17 | 23.400M | 23.282M |\n| - Output skips | 9.950 | 23.378M | 23.282M |\n| ConvNeXt-ify pt. 2 |  |  |  |  |\n| + ResNeXt-ify G & D | 7.507 | 23.188M | 23.091M |\n| + Inverted bottleneck | 7.045 | 23.058M | 23.010M |", "caption": "Table 2: StackedMNIST 1000-mode coverage.", "description": "This table presents the results of an experiment conducted on the StackedMNIST dataset, which consists of 1000 uniformly distributed modes. The experiment evaluated different loss functions, specifically different combinations of RpGAN (relativistic pairing GAN), R1, and R2 regularization, to assess their impact on the training dynamics of a GAN and its ability to capture all modes of the data.  The table shows the number of modes covered by the generator (out of a possible 1000) and the reverse Kullback-Leibler (KL) divergence between the generated and true distributions, indicating how well the generator captures the true data distribution.  The results illustrate the importance of regularization for GAN stability and mode coverage, highlighting the effectiveness of combining RpGAN with both R1 and R2 for achieving better results.", "section": "2 Serving Two Masters: Stability and Diversity with RpGAN +R1 + R2"}, {"content": "| -z normalization\n| - Minibatch stddev\n| - Equalized learning rate\n| - Mapping network\n| - Style injection\n| - Weight mod / demod\n| - Noise injection\n| - Mixing regularization\n| - Path length regularization\n| - Lazy regularization", "caption": "Table 3: CIFAR-10 performance.", "description": "This table presents a comparison of the Fr\u00e9chet Inception Distance (FID) scores achieved by various generative models on the CIFAR-10 dataset.  It shows how the FID, a metric reflecting image quality and diversity, varies across different GAN (Generative Adversarial Network) and diffusion models, including both models using and not using ImageNet pre-training (indicated by *). The table helps illustrate the performance of the proposed model (Ours-Config E) compared to existing state-of-the-art models.", "section": "4.5 FID - CIFAR-10"}, {"content": "|---|---|---|---|---|---|---|---| \n| ![Refer to caption](https://arxiv.org/html/2501.05441/image-64.jpg) | ![Refer to caption](https://arxiv.org/html/2501.05441/image-65.jpg) | ![Refer to caption](https://arxiv.org/html/2501.05441/image-66.jpg) | ![Refer to caption](https://arxiv.org/html/2501.05441/image-67.jpg) | ![Refer to caption](https://arxiv.org/html/2501.05441/image-128.jpg) | ![Refer to caption](https://arxiv.org/html/2501.05441/image-69.jpg) | ![Refer to caption](https://arxiv.org/html/2501.05441/image-70.jpg) | ![Refer to caption](https://arxiv.org/html/2501.05441/image-71.jpg) |\n| ![Refer to caption](https://arxiv.org/html/2501.05441/image-72.jpg) | ![Refer to caption](https://arxiv.org/html/2501.05441/image-73.jpg) | ![Refer to caption](https://arxiv.org/html/2501.05441/image-74.jpg) | ![Refer to caption](https://arxiv.org/html/2501.05441/image-75.jpg) | ![Refer to caption](https://arxiv.org/html/2501.05441/image-76.jpg) | ![Refer to caption](https://arxiv.org/html/2501.05441/image-77.jpg) | ![Refer to caption](https://arxiv.org/html/2501.05441/image-78.jpg) | ![Refer to caption](https://arxiv.org/html/2501.05441/image-79.jpg) |\n| ![Refer to caption](https://arxiv.org/html/2501.05441/image-80.jpg) | ![Refer to caption](https://arxiv.org/html/2501.05441/image-81.jpg) | ![Refer to caption](https://arxiv.org/html/2501.05441/image-82.jpg) | ![Refer to caption](https://arxiv.org/html/2501.05441/image-83.jpg) | ![Refer to caption](https://arxiv.org/html/2501.05441/image-84.jpg) | ![Refer to caption](https://arxiv.org/html/2501.05441/image-85.jpg) | ![Refer to caption](https://arxiv.org/html/2501.05441/image-86.jpg) | ![Refer to caption](https://arxiv.org/html/2501.05441/image-87.jpg) |\n| ![Refer to caption](https://arxiv.org/html/2501.05441/image-88.jpg) | ![Refer to caption](https://arxiv.org/html/2501.05441/image-89.jpg) | ![Refer to caption](https://arxiv.org/html/2501.05441/image-90.jpg) | ![Refer to caption](https://arxiv.org/html/2501.05441/image-91.jpg) | ![Refer to caption](https://arxiv.org/html/2501.05441/image-92.jpg) | ![Refer to caption](https://arxiv.org/html/2501.05441/image-93.jpg) | ![Refer to caption](https://arxiv.org/html/2501.05441/image-94.jpg) | ![Refer to caption](https://arxiv.org/html/2501.05441/image-95.jpg) |\n| ![Refer to caption](https://arxiv.org/html/2501.05441/image-96.jpg) | ![Refer to caption](https://arxiv.org/html/2501.05441/image-97.jpg) | ![Refer to caption](https://arxiv.org/html/2501.05441/image-98.jpg) | ![Refer to caption](https://arxiv.org/html/2501.05441/image-99.jpg) | ![Refer to caption](https://arxiv.org/html/2501.05441/image-100.jpg) | ![Refer to caption](https://arxiv.org/html/2501.05441/image-101.jpg) | ![Refer to caption](https://arxiv.org/html/2501.05441/image-102.jpg) | ![Refer to caption](https://arxiv.org/html/2501.05441/image-103.jpg) |\n| ![Refer to caption](https://arxiv.org/html/2501.05441/image-104.jpg) | ![Refer to caption](https://arxiv.org/html/2501.05441/image-105.jpg) | ![Refer to caption](https://arxiv.org/html/2501.05441/image-106.jpg) | ![Refer to caption](https://arxiv.org/html/2501.05441/image-107.jpg) | ![Refer to caption](https://arxiv.org/html/2501.05441/image-108.jpg) | ![Refer to caption](https://arxiv.org/html/2501.05441/image-109.jpg) | ![Refer to caption](https://arxiv.org/html/2501.05441/image-110.jpg) | ![Refer to caption](https://arxiv.org/html/2501.05441/image-111.jpg) |\n| ![Refer to caption](https://arxiv.org/html/2501.05441/image-112.jpg) | ![Refer to caption](https://arxiv.org/html/2501.05441/image-113.jpg) | ![Refer to caption](https://arxiv.org/html/2501.05441/image-114.jpg) | ![Refer to caption](https://arxiv.org/html/2501.05441/image-115.jpg) | ![Refer to caption](https://arxiv.org/html/2501.05441/image-116.jpg) | ![Refer to caption](https://arxiv.org/html/2501.05441/image-117.jpg) | ![Refer to caption](https://arxiv.org/html/2501.05441/image-118.jpg) | ![Refer to caption](https://arxiv.org/html/2501.05441/image-119.jpg) |\n| ![Refer to caption](https://arxiv.org/html/2501.05441/image-120.jpg) | ![Refer to caption](https://arxiv.org/html/2501.05441/image-121.jpg) | ![Refer to caption](https://arxiv.org/html/2501.05441/image-122.jpg) | ![Refer to caption](https://arxiv.org/html/2501.05441/image-123.jpg) | ![Refer to caption](https://arxiv.org/html/2501.05441/image-124.jpg) | ![Refer to caption](https://arxiv.org/html/2501.05441/image-125.jpg) | ![Refer to caption](https://arxiv.org/html/2501.05441/image-126.jpg) | ![Refer to caption](https://arxiv.org/html/2501.05441/image-127.jpg) |", "caption": "Table 4: Hyperparameters for each experiment. The decay factor \u03b2\ud835\udefd\\betaitalic_\u03b2 of EMA can be obtained using the formula \u03b2=0.5Minibatch sizeEMA half-life\ud835\udefdsuperscript0.5Minibatch sizeEMA half-life\\beta=0.5^{\\frac{\\text{Minibatch size}}{\\text{EMA half-life}}}italic_\u03b2 = 0.5 start_POSTSUPERSCRIPT divide start_ARG Minibatch size end_ARG start_ARG EMA half-life end_ARG end_POSTSUPERSCRIPT,\u00a0e.g. for CIFAR-10, EMA \u03b2=0.55125\u00d7106\u22480.9999\ud835\udefdsuperscript0.55125superscript1060.9999\\beta=0.5^{\\frac{512}{5\\times 10^{6}}}\\approx 0.9999italic_\u03b2 = 0.5 start_POSTSUPERSCRIPT divide start_ARG 512 end_ARG start_ARG 5 \u00d7 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT end_ARG end_POSTSUPERSCRIPT \u2248 0.9999.", "description": "This table details the hyperparameters used in the experiments across different datasets (Stacked MNIST, CIFAR-10, FFHQ, and ImageNet).  It includes settings such as resolution, whether class conditioning was used, the number of GPUs employed, the total training time (measured in millions of images), the burn-in period, minibatch size, learning rate schedule,  R1 and R2 regularization parameters (and their decay), Adam's beta2 parameter, EMA half-life (in millions of images), the number of residual blocks and groups per resolution, model parameter counts for both the generator (G) and discriminator (D), data augmentation (horizontal flips), and the schedule for augmentation probability.  The formula for calculating the EMA decay factor (beta) is provided and an example calculation is given for the CIFAR-10 experiment. ", "section": "Experiments Details"}]