[{"figure_path": "https://arxiv.org/html/2412.18597/x2.png", "caption": "Figure 1: \nThe proposed DiTCtrl takes multiple text prompts as input and demonstrates superior capability in generating longer videos with complex motion and smooth transitions as output. In this figure, we showcase a challenging example where an athlete glides through three distinct scenes. Despite the complex subject motion and dramatic camera movement, our method maintains remarkable stability throughout the sequence and seamless semantic transitions that faithfully follow the prompt descriptions.", "description": "DiTCtrl, a novel multi-prompt video generation method, is showcased.  The figure presents a challenging example: an athlete gracefully gliding through three diverse scenes (ocean, snowy slopes, desert dunes).  The method successfully generates a long video with complex motion and dramatic camera angles while maintaining smooth transitions between scenes and accurate semantic consistency with the input text prompts. This demonstrates DiTCtrl's superior capability in handling multiple, sequential prompts and generating coherent, visually appealing longer videos.", "section": "Abstract"}, {"figure_path": "https://arxiv.org/html/2412.18597/x3.png", "caption": "Figure 2: MM-DiT Attention Analysis. We find the attention matrix in MM-DiT attention can be divided into four different regions. As for the prompt of \u201c a cat watch a black mouse\u201d, each text token shows a high-light response using the average of the text-to-video and video-to-text attention.", "description": "This figure illustrates the four distinct regions of the attention matrix within the Multi-Modal Diffusion Transformer (MM-DiT) architecture.  These regions correspond to text-to-text, text-to-video, video-to-text, and video-to-video attention mechanisms. The analysis focuses on the prompt \u201ca cat watches a black mouse,\u201d demonstrating how each text token highlights a specific response based on the average attention weights from the text-to-video and video-to-text interactions. This visualization helps to understand how the MM-DiT processes both textual and visual information to generate videos.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2412.18597/x4.png", "caption": "Figure 3: MM-DiT Text-to-Text and Video-to-Video Attention Visualization. We find that the current MM-DiT has a stronger potential to construct the individual attention in the previous UNet-like structure\u00a0[10, 11, 41].", "description": "This figure visualizes the attention mechanisms within the Multi-Modal Diffusion Transformer (MM-DiT) architecture, specifically focusing on the Text-to-Text and Video-to-Video attention components.  Unlike traditional UNet architectures which employ separate attention mechanisms, the MM-DiT uses a single 3D full attention process. The visualization reveals distinct diagonal patterns in both the text-to-text and video-to-video attention maps. In the text-to-text attention, the diagonal pattern indicates that each word primarily attends to its neighboring words, reflecting the sequential nature of language.  In the video-to-video attention, the diagonal patterns suggest strong correlations between consecutive frames in both spatial and temporal dimensions. These findings indicate that the 3D full attention in MM-DiT behaves similarly to cross/self-attention mechanisms in UNet-like architectures, demonstrating its potential for precise semantic control and manipulation, which is essential for tasks like video editing and multi-prompt video generation.", "section": "3.1 MM-DIT Attention Mechanism Analysis"}, {"figure_path": "https://arxiv.org/html/2412.18597/x5.png", "caption": "Figure 4:  Pipeline of the proposed DiTCtrl. Our method tries to synthesize content-consistent and motion-consistent videos based on multi-prompts. The first video is synthesized with source text prompt Pi\u22121subscript\ud835\udc43\ud835\udc561P_{i-1}italic_P start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT. During the denoising process for video synthesis, we convert the full-attention into masked-guided KV-sharing strategy to query video contents from source video \ud835\udcb1i\u22121subscript\ud835\udcb1\ud835\udc561\\mathcal{V}_{i-1}caligraphic_V start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT, so that we can synthesize content-consistent video under the modified target prompt Pisubscript\ud835\udc43\ud835\udc56P_{i}italic_P start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. Note that initial latents are assumed to be 5 frames. The first three frames are used to generate the contents of Pi\u22121subscript\ud835\udc43\ud835\udc561P_{i-1}italic_P start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT, and the last three frames are used to generate contents of Pisubscript\ud835\udc43\ud835\udc56P_{i}italic_P start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. The pink latent represents the overlapping frame, while the blue and green latents are used to distinguish different prompt segments.", "description": "DiTCtrl synthesizes content- and motion-consistent videos from multiple prompts.  It uses a pre-trained single-prompt model and applies a masked-guided KV-sharing attention mechanism to maintain consistency across prompts.  The process involves generating initial latent representations (5 frames), using the first three to create the first video segment from prompt P<sub>i-1</sub>, and the last three to create the second video segment from prompt P<sub>i</sub>.  The overlapping frame is highlighted in pink to show the transition, with blue and green latents distinguishing the segments.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2412.18597/x6.png", "caption": "Figure 5: Latent blending strategy for video transition between video clips.", "description": "This figure illustrates the latent blending strategy used in DiTCtrl for smooth transitions between consecutive video clips generated from different prompts.  It shows how the latent representations (features) of overlapping frames from adjacent video segments are blended using a position-dependent weighting function. This function assigns higher weights to frames closer to their respective segments and lower weights to frames at the boundaries, creating a smooth transition between the distinct semantic content of the clips. The strategy ensures a seamless and temporally coherent video, even across different semantic contexts.", "section": "3.3 Latent Blending Strategy for Transition"}, {"figure_path": "https://arxiv.org/html/2412.18597/x7.png", "caption": "Figure 6: Generation results on given prompts by our method and baseline models. Kling is the commercial model, and Freenoise+DiT is our implementation of Freenoise on CogVideoX.", "description": "Figure 6 presents a qualitative comparison of video generation results from different methods, showcasing the superior performance of the proposed DiTCtrl model.  The comparison includes three state-of-the-art methods (Gen-L-Video, Video-Infinity, FreeNoise) and a commercial model (Kling). FreeNoise+DiT represents a modified version of FreeNoise integrated with the CogVideoX architecture, providing a strong baseline for comparison. The figure highlights DiTCtrl's excellence in text-to-video alignment, temporal coherence, and motion quality, particularly in handling dynamic scenarios with sequential semantic transitions. Each method's video generation is presented side-by-side for direct visual comparison, enabling an assessment of text-to-video alignment quality, temporal consistency across prompts, and motion accuracy.", "section": "4.1. Qualitative Results"}, {"figure_path": "https://arxiv.org/html/2412.18597/x8.png", "caption": "Figure 7: T-SNE visualization of CLIP embeddings. Each point represents the CLIP embedding of a single video frame after dimensionality reduction. The visualization demonstrates that conventional multi-prompt videos form distinct clusters, while our method produces a more continuous distribution, indicating smoother semantic transitions.", "description": "This figure visualizes the results of applying t-distributed stochastic neighbor embedding (t-SNE) dimensionality reduction to CLIP embeddings of video frames. Each point represents a frame from a generated video.  The plot shows that videos generated by conventional multi-prompt methods tend to cluster in distinct groups.  In contrast, videos generated by the proposed DiTCtrl method form a more continuous distribution. This indicates that DiTCtrl produces videos with smoother transitions between the semantic concepts represented in different prompts, resulting in a more coherent and less jarring overall video experience.", "section": "4. Quantitative Results"}, {"figure_path": "https://arxiv.org/html/2412.18597/x9.png", "caption": "Figure 8: Ablation Component in DiTCtrl. The first and second rows have 98 frames, while the remaining methods generate 105 frames.", "description": "This ablation study visualizes the impact of different components within the DiTCtrl model on multi-prompt video generation.  The figure shows that the full DiTCtrl model (with mask-guided KV-sharing and latent blending) produces videos with 98 frames that maintain consistent visual coherence across multiple prompts, unlike other methods that generate 105 frames but show inconsistencies or unnatural transitions.  The results highlight the importance of each component: mask-guided KV-sharing ensures accurate object tracking, latent blending enables seamless transitions, and both contribute to the overall visual quality and temporal consistency.", "section": "4.3. Ablation Study"}, {"figure_path": "https://arxiv.org/html/2412.18597/x10.png", "caption": "Figure 9: single prompt longer video generation example.", "description": "Figure 9 showcases the capabilities of the proposed DiTCtrl method in generating a longer video from a single prompt.  It visually demonstrates that the model can produce a coherent and temporally consistent video sequence extending well beyond the typical length limitations of previous video generation models.  The example visualizes the extended length and consistent quality achievable with the method.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.18597/x11.png", "caption": "Figure 10: Mask-guided KV-sharing details.", "description": "This figure illustrates the mask-guided KV-sharing mechanism used in DiTCtrl for multi-prompt video generation.  It shows how attention maps from two consecutive prompts (Pi-1 and Pi) are used to generate a mask that guides the attention mechanism in the MM-DiT block. By focusing on specific tokens (like 'a running horse'), a foreground mask (Mi-1) is created from the Pi-1 prompt and used to guide the attention of the current prompt (Pi), ensuring that the object's appearance remains consistent across prompts. The resulting foreground-focused attention (Ffore) and background attention (Fback) are combined with the mask to generate the final fused attention (Ffusion) for the current video frame. This ensures a smooth and semantically consistent transition between video segments generated from different prompts.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2412.18597/x12.png", "caption": "Figure 11: More multi-prompt results", "description": "Figure 11 showcases example video generation results from the DiTCtrl model using multiple prompts.  The figure visually demonstrates the model's ability to generate coherent and smooth transitions between distinct scenes described by sequential prompts.  Each row shows a different video generation scenario, illustrating various transitions, such as changes in camera angle, location, or subject actions. The consistent visual quality and smooth transitions highlight DiTCtrl's effectiveness in handling multi-prompt video generation tasks.", "section": "4.1 Qualitative Results"}, {"figure_path": "https://arxiv.org/html/2412.18597/x13.png", "caption": "Figure 12: More multi-prompt results", "description": "Figure 12 presents three examples of videos generated using DiTCtrl with multiple prompts. Each example showcases a different scene and transitions between them. The first example shows a white SUV transitioning from a dirt road to a snowy path and finally to a starry night scene. The second example depicts a dark knight on a horse transitioning from a grassland, to a snowy field, and finally to a desert. The third example showcases a flower bud transitioning from a closed bud to an open flower in full bloom. These examples highlight DiTCtrl's ability to generate coherent and visually appealing videos by smoothly transitioning between different scenes and semantic concepts.", "section": "4.1 Qualitative Results"}, {"figure_path": "https://arxiv.org/html/2412.18597/x14.png", "caption": "Figure 13: Motion and background transition.", "description": "This figure displays a comparison of video generation results from several methods for a multi-prompt video generation task focusing on motion and background transitions.  The top row shows the results for a prompt sequence depicting a dark knight on horseback in a grassland, then galloping across a snowy field. The bottom row shows a transition from a snowy city street to a sunny city street. The goal is to illustrate how well each method maintains coherence in both the motion of the subject (dark knight/vehicles) and the overall background between scene changes.  Each column represents a different method: Kling (commercial), Gen-L-Video, Video-Infinity, FreeNoise, FreeNoise+DiT, and the authors' proposed DiTCtrl method.", "section": "4.1 Qualitative Results"}, {"figure_path": "https://arxiv.org/html/2412.18597/x15.png", "caption": "Figure 14: Background transition.", "description": "This figure showcases the effectiveness of the proposed DiTCtrl model in generating videos with smooth background transitions.  It compares the output of DiTCtrl against several baselines (Kling, Gen-L-Video, Video-Infinity, FreeNoise, and FreeNoise+DiT) on a specific prompt requesting a background transition from a snowy city street to a sunny city street.  The comparison highlights DiTCtrl's superior ability to produce natural and coherent transitions between scenes, unlike the baselines which often exhibit abrupt changes, artifacts, or unnatural motion.", "section": "4. Qualitative Results"}, {"figure_path": "https://arxiv.org/html/2412.18597/x16.png", "caption": "Figure 15: Visualization of single prompt longer video generation.", "description": "This figure visualizes the results of single-prompt longer video generation.  It showcases several example videos, each generated from a single text prompt, demonstrating the model's capability to produce videos of considerable length while maintaining coherence and visual quality. Each sequence is presented with frames from different stages of the generated video, illustrating the temporal consistency. This demonstrates the effectiveness of the model in generating longer videos exceeding what was previously possible.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.18597/x17.png", "caption": "Figure 16: Reweighting example of Video Editing.", "description": "This figure demonstrates the effect of reweighting attention scores in the MM-DiT architecture for video editing. Subfigure (a) shows how reducing attention scores for certain tokens (e.g., \"pink\") leads to a decrease in the prominence of that color in the generated video, while subfigure (b) shows that increasing attention scores for specific tokens enhances the presence of those features (e.g., \"snowy\"). This illustrates the precise control over semantic aspects that the DiTCtrl method offers for video editing.", "section": "C. Video Editing"}, {"figure_path": "https://arxiv.org/html/2412.18597/x18.png", "caption": "Figure 17: Word Swap example of Video Editing.", "description": "This figure demonstrates the results of applying the word swap video editing technique.  The top row showcases editing where \"a large bear\" is replaced with \"a large lion.\" The bottom row shows how changing \"a white vintage SUV\" to \"a red vintage SUV\" alters the generated video. Both edits are made using the DiTCtrl method, illustrating the model's ability to make precise semantic changes in generated videos by swapping text tokens in the prompts, whilst maintaining video consistency and temporal coherence.", "section": "C. Applications"}, {"figure_path": "https://arxiv.org/html/2412.18597/x19.png", "caption": "Figure 18: Ablation study of mask-guided KV-sharing results. First row shows our model without mask-guided KV-sharing, while the second row demonstrates our full model with mask-guided KV-sharing. The prompt for (a) transitions from \u201cA powerful horse gallops across a field\u2026\u201d to \u201cA striking zebra leads its herd across the field\u2026\u201d. The prompt for (b) evolves from \u201cA white SUV drives a dirt road\u2026\u201d to \u201cA white SUV powers through snow\u2026\u201d", "description": "This figure demonstrates the effectiveness of the proposed mask-guided KV-sharing mechanism.  The top row showcases results obtained without this mechanism, revealing inconsistencies in object identity (horse to zebra) and scene transitions (dirt road to snowy road). The bottom row displays results from the full model, incorporating mask-guided KV-sharing. It highlights improved consistency in object identity and scene transitions, demonstrating the method's ability to maintain visual coherence across multiple prompts.", "section": "E. Ablation Study"}]