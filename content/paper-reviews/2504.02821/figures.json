[{"figure_path": "https://arxiv.org/html/2504.02821/x1.png", "caption": "Figure 1: Training Sparse Autoencoders (SAEs) over VLMs (e.g. CLIP): The highly activating images of a neuron in a given layer of a pretrained VLM are polysemantic (top), and a neuron in an SAE trained to reconstruct the same layer is more monosemantic (bottom, with higher Monosemanticity Score (MS))", "description": "This figure illustrates the effect of training sparse autoencoders (SAEs) on a vision-language model (VLM), such as CLIP. The top panel shows that a neuron in a pretrained VLM's layer responds to a variety of semantically dissimilar images (polysemantic). In contrast, the bottom panel demonstrates that after training an SAE to reconstruct the same layer of the VLM, the corresponding neuron in the SAE now primarily activates for images that are semantically similar (monosemantic), as indicated by a higher monosemanticity score (MS). This signifies that SAEs can enhance the interpretability of VLMs by making the representation of individual neurons clearer and more focused.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2504.02821/x2.png", "caption": "Figure 2: Illustration of the computation of our Monosemanticity Score (MS). (A) We extract embeddings and activations from a given set of images, (B) then compute the pairwise embedding similarities and pairwise neuron activations. (C) MS is equal to the average of embedding similarities weighted by the neuron activations.", "description": "Figure 2 illustrates the calculation of the Monosemanticity Score (MS), a metric designed to quantify the monosemanticity of individual neurons in a Sparse Autoencoder (SAE).  The process is broken down into three steps: (A) Image embeddings and neuron activations are extracted from a set of input images using a pre-trained image encoder. (B) A pairwise similarity matrix is computed, reflecting the semantic similarity between pairs of images, and a matrix representing pairwise neuron activations is created. (C) The MS is then computed by averaging these pairwise image similarities, with each similarity weighted by the corresponding neuron activations. This weighted average provides a measure of how similar the images are that strongly activate a particular neuron.", "section": "3. Sparse Autoencoders for VLMs"}, {"figure_path": "https://arxiv.org/html/2504.02821/x3.png", "caption": "Figure 3: Qualitative examples of highest activating images for different neurons from high (left) to low (right) Monosemanticity Scores (MS). As the score gets higher, images become more similar, illustrating the correlation with monosemanticity.", "description": "Figure 3 visually demonstrates the relationship between a neuron's Monosemanticity Score (MS) and the similarity of its highest-activating images.  The figure presents a series of images, each set corresponding to a different neuron within a Sparse Autoencoder (SAE).  Moving from left to right, the MS decreases. Concurrently, the visual similarity among images within each set diminishes.  For example, neurons with high MS values (left side) activate primarily on visually similar images, indicating a strong focus on a single semantic concept.  In contrast, neurons with lower MS values (right side) exhibit a greater variety in the images they activate, reflecting a more polysemantic response\u2014the neuron responds to a wider range of concepts. This visual progression underscores the correlation between a neuron's MS and its monosemanticity, or degree of semantic specificity.", "section": "Evaluating Interpretability of VLM Neurons"}, {"figure_path": "https://arxiv.org/html/2504.02821/x4.png", "caption": "Figure 4: Monosemanticity Scores in decreasing order across neurons, normalized by width. Results are shown for the last layer of the model, without SAE (\u201cNo SAE\u201d, in black dashed line), and with SAE using different expansion factors (in straight lines,  for \u03b5=1\ud835\udf001\\varepsilon=1italic_\u03b5 = 1,  for \u03b5=4\ud835\udf004\\varepsilon=4italic_\u03b5 = 4 and  for \u03b5=16\ud835\udf0016\\varepsilon=16italic_\u03b5 = 16).", "description": "Figure 4 presents a comparison of monosemanticity scores across neurons, normalized by the width of the layer.  The results are displayed for the last layer of a model with and without the application of Sparse Autoencoders (SAEs). The \"No SAE\" line (black dashed line) represents the monosemanticity scores for the original model's neurons. The colored lines represent the results obtained using SAEs with different expansion factors (\u03b5).  Specifically, the figure shows the results for expansion factors of 1, 4, and 16, illustrating how increasing the expansion factor affects the monosemanticity of the learned features. The x-axis represents the normalized neuron index, and the y-axis represents the monosemanticity score.", "section": "4.2. Evaluating Interpretability of VLM Neurons"}, {"figure_path": "https://arxiv.org/html/2504.02821/x5.png", "caption": "Figure 5: Impact of sparsity factor K\ud835\udc3eKitalic_K on Monosemanticity Scores across neurons extracted from SAEs trained on the last layer of the model with expansion factor\n\u03b5=1\ud835\udf001\\varepsilon=1italic_\u03b5 = 1. Results are shown for different sparsity levels, with straight lines  for\nK=1\ud835\udc3e1K=1italic_K = 1,  for\nK=10\ud835\udc3e10K=10italic_K = 10,  for\nK=20\ud835\udc3e20K=20italic_K = 20, and  for\nK=50\ud835\udc3e50K=50italic_K = 50. Scores of the original neurons (\u201cNo SAE\u201d, in black dashed lines) are added for comparison.", "description": "This figure shows how the sparsity factor (K) in the sparse autoencoder (SAE) affects the monosemanticity scores of the resulting neurons.  Monosemanticity, in this context, measures how focused a neuron is on a single concept. The SAE was trained on the last layer of a vision-language model with an expansion factor (\u03b5) of 1. Four different sparsity levels are tested (K=1, 10, 20, 50), and their monosemanticity scores are plotted against a normalized neuron index.  The scores from the original neurons (before applying the SAE) are also shown for comparison, demonstrating the improved monosemanticity achieved by the SAE.", "section": "4.2. Evaluating Interpretability of VLM Neurons"}, {"figure_path": "https://arxiv.org/html/2504.02821/x6.png", "caption": "Figure 6: We steer the outputs of LLaVA by clamping the activation values of a chosen neuron, i.e. Neuron ##\\##39 = pencil neuron, in the CLIP SAE. We observe that while initially the poem follows the instruction (the prompt + white image) strongly, the outputs become more and more influenced by the concept that this neuron represents with increasing intervention weight \u03b1\ud835\udefc\\alphaitalic_\u03b1, talking about attributes of pencil first, and then just the concept pencil. This shows that our interventions enable new capabilities for the unsupervised steering of these models.", "description": "Figure 6 demonstrates the ability to control the output of a large language model (LLM) called LLaVA by manipulating the activation of a specific neuron within a sparse autoencoder (SAE). The SAE is trained on a vision-language model (VLM), specifically CLIP.  The experiment focuses on a neuron identified as representing the concept of a \"pencil\".  The image input is a plain white image.  When the activation of the \"pencil\" neuron in the SAE is increased (\"intervention weight \u03b1\"), the LLaVA's response changes from a poem unrelated to pencils to a poem that increasingly focuses on pencils, starting with descriptions of pencil attributes and ultimately mentioning only \"pencil\". This showcases how carefully controlling SAE neuron activations can effectively steer the LLM output in an unsupervised manner, showcasing a novel method for controlling the output of such models.", "section": "3.3. Steering MLLMs with Vision SAEs"}, {"figure_path": "https://arxiv.org/html/2504.02821/x7.png", "caption": "Figure 7: Effects of neuron interventions on MLLM-generated scientific article titles. Steering magnitudes are categorized as \u201c0\u201d, \u201cmedium\u201d, and \u201chigh\u201d based on the intervention strength. The neurons are visualized with the highest activating images from which we deduce their associated concepts: \u201cpolka dots\u201d, \u201cshipwreck\u201d, and \u201crainbow\u201d.", "description": "This figure demonstrates the impact of manipulating specific neurons within a Sparse Autoencoder (SAE) on the output of a Multimodal Large Language Model (MLLM).  Three different intervention strengths (0, medium, high) are applied to three separate SAE neurons. Each neuron is associated with a specific concept, represented by the images shown next to it: polka dots, shipwreck, and rainbow. The prompt given to the MLLM in each case is: \"Generate a scientific article title.\"  By changing the strength of the intervention, the generated titles shift to reflect the specific concept associated with the manipulated neuron more strongly. This highlights the ability to steer the MLLM's output towards desired concepts by subtly altering neuron activations in the SAE without modifying the underlying language model.", "section": "4.4. Steering Multimodal LLMs"}, {"figure_path": "https://arxiv.org/html/2504.02821/x8.png", "caption": "Figure A1: LLaVA-like models can be steered towards seeing a concept (e.g. panda) not present in the input image \ud835\udc31\ud835\udc31\\mathbf{x}bold_x. By attaching SAE after vision encoder and intervening on its neuron representing that concept, we effectively manipulate the LLM\u2019s response. Such flexible and precise steering is possible thanks to the extensive concept dictionary identified through the SAE.", "description": "This figure demonstrates how a sparse autoencoder (SAE) can be used to steer the output of a large language model (LLM) by manipulating its visual input.  A LLaVA-like model is shown, which combines image and text inputs to generate a text output.  An SAE is attached to the model's vision encoder. By selectively activating a specific neuron in the SAE that corresponds to a desired concept (in this case, a panda), the model's response can be subtly altered. Even if the concept is not present in the original image, the SAE's intervention guides the LLM's interpretation to incorporate it. This illustrates the ability to control and enhance the LLM's outputs in a precise way, which is facilitated by the rich conceptual representation discovered by the SAE.", "section": "A. More details on steering"}, {"figure_path": "https://arxiv.org/html/2504.02821/x9.png", "caption": "Figure A2: MS in decreasing order across neurons. Results are shown for a layer without SAE (\u201cNo SAE\u201d), and with SAE using different expansion factors (\u00d71,\u00d74\\times 1,\\times 4\u00d7 1 , \u00d7 4 and \u00d716absent16\\times 16\u00d7 16).", "description": "Figure A2 presents a comparative analysis of monosemanticity scores across neurons.  The x-axis represents the index of neurons ordered by decreasing monosemanticity score, while the y-axis displays the monosemanticity score itself.  Multiple lines are plotted, each representing a different condition: one line shows the scores for the layer without a Sparse Autoencoder (SAE), and other lines illustrate scores for the layer with an SAE, trained with different expansion factors (1x, 4x, and 16x). This visualization helps in understanding the impact of SAEs and varying expansion factors on the monosemanticity of the learned features.  Higher scores indicate greater monosemanticity, meaning that the neuron responds more consistently to semantically similar inputs.", "section": "A. More details on steering"}, {"figure_path": "https://arxiv.org/html/2504.02821/x10.png", "caption": "(a) Neurons of CLIP\u00a0ViT-L evaluated with CLIP\u00a0ViT-B as the image encoder E\ud835\udc38Eitalic_E", "description": "This figure shows the monosemanticity scores of neurons in a CLIP ViT-L model.  Monosemanticity measures how well a neuron represents a single, clear concept.  The x-axis represents the neurons ordered by their monosemanticity scores (from high to low), while the y-axis shows the monosemanticity score itself. The graph compares neurons from the original CLIP ViT-L model (No SAE) to neurons obtained after applying Sparse Autoencoders (SAEs).  Specifically, it illustrates the improved monosemanticity (more focused concepts) achieved by the SAEs, with different expansion factors representing variations in the size of the SAE's latent space (i.e. the number of neurons). CLIP ViT-B acted as the image encoder used to compute the monosemanticity scores. The results demonstrate the effectiveness of SAEs in improving the clarity and interpretability of the vision model.", "section": "4.2. Evaluating Interpretability of VLM Neurons"}, {"figure_path": "https://arxiv.org/html/2504.02821/x11.png", "caption": "(b) Neurons of CLIP\u00a0ViT-L evaluated with DINOv2\u00a0ViT-B as the image encoder E\ud835\udc38Eitalic_E", "description": "This figure displays the results of evaluating the monosemanticity of CLIP ViT-L model neurons using DINOv2 ViT-B as the image encoder. Monosemanticity, a measure of how focused a neuron's response is on a single concept, is assessed.  The figure likely shows a graph or chart comparing the monosemanticity scores of neurons in the CLIP ViT-L model (with the original image features) to those obtained when using the features extracted by DINOv2 ViT-B. This comparison helps understand the impact of different image encoders on the interpretability and concept clarity of neurons in vision-language models.", "section": "Additional results on monosemanticity"}, {"figure_path": "https://arxiv.org/html/2504.02821/x12.png", "caption": "(c) Neurons of SigLIP\u00a0SoViT-400m evaluated with CLIP\u00a0ViT-B as the image encoder E\ud835\udc38Eitalic_E", "description": "This figure shows the monosemanticity scores of neurons in the SigLIP SoViT-400m model.  The monosemanticity was calculated using the CLIP ViT-B model as the image encoder. The figure likely displays a graph or chart visualizing how monosemantic the neurons are across different layers of the model, potentially comparing the SigLIP SoViT-400m model's neurons to those of other models or comparing different configurations within the SigLIP SoViT-400m model itself. This information is helpful to understand the quality of learned representations and how well-separated different concepts are in the model's neurons.", "section": "Additional results on monosemanticity"}, {"figure_path": "https://arxiv.org/html/2504.02821/extracted/6333825/pencil_neuron.png", "caption": "Figure A3: MS in decreasing order across neurons. Results are shown for the last layers of two different models, without SAE (black dashed line), and with SAE being trained with expansion factor 1111 ( green solid line). MS is computed with distinct image encoders E\ud835\udc38Eitalic_E.", "description": "Figure A3 presents a comparative analysis of monosemanticity scores (MS) across neurons.  It displays the MS values for neurons in the final layer of two distinct vision-language models: one without a Sparse Autoencoder (SAE) and another with an SAE trained using an expansion factor of 1. The key comparison is the distribution of MS values between these two models, illustrating how the SAE influences the monosemanticity of the resulting neuron representations. Furthermore, the figure highlights the impact of using different image encoders (E) for calculating MS, demonstrating the robustness or sensitivity of the monosemanticity measure to the choice of encoder.", "section": "A. More details on steering"}, {"figure_path": "https://arxiv.org/html/2504.02821/x13.png", "caption": "Figure A4: Images highly activating the neuron we intervene on in Figure 6, which we manually labeled as \u201cPencil Neuron\u201d.", "description": "Figure A4 shows a set of images that strongly activate a specific neuron within a Sparse Autoencoder (SAE). This neuron, manually labeled as the \"Pencil Neuron\", is part of a larger SAE trained on vision data to enhance interpretability in vision-language models. The images displayed in Figure A4 are those most strongly associated with this particular neuron, illustrating its monosemantic nature. Because the images shown are all related to pencils, the figure helps to demonstrate how the SAE is able to learn concepts that improve both interpretability and control of vision-language models.", "section": "A. More details on steering"}]