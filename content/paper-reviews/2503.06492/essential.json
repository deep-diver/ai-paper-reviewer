{"importance": "This paper introduces **a new benchmark that enables a more detailed evaluation** of LVLMs, which can help researchers to identify and address the specific weaknesses of these models. It **offers insights into both visual and linguistic capabilities** for further research.", "summary": "VisualSimpleQA: A new benchmark for fine-grained evaluation of visual and linguistic modules in fact-seeking LVLMs.", "takeaways": ["VisualSimpleQA enables streamlined and decoupled evaluation of LVLMs in visual and linguistic modalities.", "VisualSimpleQA incorporates well-defined difficulty criteria to guide human annotation and facilitates the extraction of a challenging subset.", "Experiments highlight opportunities for improvement in both visual and linguistic modules of LVLMs."], "tldr": "Large Vision-Language Models(LVLMs) are not perfect, often giving incorrect answers in question answering. Existing tests mainly check the final answer, not how each part (vision, language) performs. This limits improvement in specific areas. So, there is a need for better tests that can pinpoint where models struggle, leading to more effective advancements in these AI systems.\n\nTo solve this, VisualSimpleQA is introduced. It has two main features: First, it lets you easily test the vision and language parts of LVLMs separately. Second, it uses clear difficulty levels to help create tricky questions. Tests on 15 LVLMs showed even the best models like GPT-40 only got around 60% correct, and just 30% on the hardest questions. This new test shows big improvements are possible in both seeing and understanding.", "affiliation": "Zhongguancun Laboratory", "categories": {"main_category": "Multimodal Learning", "sub_category": "Vision-Language Models"}, "podcast_path": "2503.06492/podcast.wav"}