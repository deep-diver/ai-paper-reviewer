[{"figure_path": "https://arxiv.org/html/2503.06492/x1.png", "caption": "Figure 1: Illustration of an example in VisualSimpleQA. The red box highlights the region of interest (ROI).\nEach sample has several attributes and tags, which allow us to measure its overall difficulty score based on our proposed difficulty criteria.", "description": "VisualSimpleQA is a multimodal fact-seeking QA benchmark.  This figure shows an example from the benchmark. A multimodal question is posed, along with an image containing a region of interest (ROI, highlighted by a red box).  The ROI is crucial for answering the question. The figure also displays attributes and tags associated with the example. These attributes and tags (such as image resolution, rationale granularity, etc.) are used to compute a difficulty score for each sample in the benchmark, helping researchers understand model performance on more challenging questions.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2503.06492/x2.png", "caption": "Figure 2: Decoupled evaluation process.", "description": "This figure illustrates the decoupled evaluation process used in the VisualSimpleQA benchmark.  It shows how the benchmark separates the evaluation of the linguistic and visual modules of large vision-language models (LVLMs).  The linguistic module is assessed using text-only questions, while the visual module is evaluated by comparing the performance on multimodal questions (which include visual input) against the performance on the corresponding text-only questions. A smaller performance difference between text-only and multimodal question answering indicates a stronger visual module.  This decoupled approach allows for a more fine-grained analysis of LVLMs' strengths and weaknesses.", "section": "3.1 Decoupled Evaluation"}, {"figure_path": "https://arxiv.org/html/2503.06492/x3.png", "caption": "Figure 3: Flowchart of the annotation process. Evidence is used to guarantee the correctness of the answer, while ROI is annotated to calculate the difficulty of each sample.", "description": "This flowchart illustrates the two main annotation processes used in creating the VisualSimpleQA dataset.  The first process starts with existing image datasets (like OK-VQA or V*), where annotators create multimodal questions based on the image, provide the ground truth answer with evidence, specify the region of interest (ROI) and its rationale (the key information needed to answer the question), and formulate a corresponding text-only question. The second process begins with creating a text-only question, finding a suitable image online, and then following the same steps as the first process to complete the annotation for the multimodal question.  Both processes ensure that the answers are accurate and verifiable, and that difficulty is carefully considered through the selection of images and the definition of the ROI and rationale.", "section": "4.1 Annotation Process"}, {"figure_path": "https://arxiv.org/html/2503.06492/x4.png", "caption": "Figure 4: Distribution of topics in VisualSimpleQA.", "description": "This figure shows a pie chart that visualizes the distribution of topics in the VisualSimpleQA dataset.  Each slice represents a different category of question topics, with its size proportional to the percentage of questions belonging to that category.  The categories are Research & Education, Company & Brand, Film & Television Entertainment, Politics, History, Geography, Art, Sports, Games, and Other.  This provides a visual representation of the diversity of topics covered in the benchmark dataset.", "section": "5 Statistics of VisualSimpleQA"}, {"figure_path": "https://arxiv.org/html/2503.06492/x15.png", "caption": "Figure 5: Distributions of factors that influence the difficulty of visual recognition. TI denotes Text in Image.", "description": "Figure 5 presents four subfigures, each showing the distribution of a factor influencing visual recognition difficulty in the VisualSimpleQA dataset.  The top row displays histograms for Normalized Resolution, Proportion of ROI, and the presence or absence of text within the image (TI).  The bottom row presents a histogram showing the distribution of Rationale Granularity.  These visualizations illustrate the frequency of different values for each feature, providing insight into how these elements contribute to a sample's visual difficulty level.", "section": "5 Statistics of VisualSimpleQA"}, {"figure_path": "https://arxiv.org/html/2503.06492/x16.png", "caption": "Figure 6: Distributions of knowledge popularity and overall difficulty.", "description": "This figure presents two histograms visualizing the distributions of knowledge popularity and overall difficulty scores within the VisualSimpleQA dataset.  The knowledge popularity score, determined by GPT-40, reflects the prevalence of the required knowledge in large language model training corpora (with higher scores indicating higher popularity).  The overall difficulty score combines visual and linguistic factors, quantifying the challenge of each question for large vision-language models.  The histograms show the frequency of different score ranges for both metrics, allowing for analysis of the dataset's balance between easier and harder questions.", "section": "Statistics of VisualSimpleQA"}, {"figure_path": "https://arxiv.org/html/2503.06492/x17.png", "caption": "Figure 7: Ratio of failures (incorrect responses and refusals) by GPT-4o across varying difficulty levels. The left sub-figure is based on samples where GPT-4o correctly answers the text-only questions but fails to answer the multimodal questions. The right is based on samples where GPT-4o fails to answer the text-only questions.", "description": "This figure displays the ratio of failed responses (incorrect answers and refusals) given by the GPT-40 model across different difficulty levels.  The left graph focuses on instances where GPT-40 successfully answered the text-only question but failed on the multimodal question. This helps isolate the impact of visual information processing on the model's accuracy. The right graph examines cases where GPT-40 failed at the text-only question, indicating limitations in the model's linguistic understanding, regardless of the visual component.", "section": "6.2 Evaluation Results"}]