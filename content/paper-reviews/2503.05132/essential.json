{"importance": "This work pioneers non-SFT methods for multimodal reasoning, showing RL can unlock complex skills. It challenges the reliance on instruction tuning, offering a fresh, efficient approach. Researchers can build upon these insights to explore unsupervised learning and R1-like architectures, advancing AI's reasoning abilities with **less data and compute**.", "summary": "R1-Zero replicates DeepSeek R1's \"aha moment\" in visual reasoning on a 2B non-SFT model, using RL to achieve 59.47% accuracy on CVBench.", "takeaways": ["Direct RL on non-SFT models can replicate emergent reasoning characteristics like the \"aha moment\" in multimodal tasks.", "Vision-centric spatial reasoning benefits from improved reasoning capabilities achieved through this approach.", "Applying RL to instruction-tuned models may lead to superficial reasoning, highlighting the importance of the base model."], "tldr": "**DeepSeek R1** demonstrated that reinforcement learning can enable complex reasoning in language models. This paper tackles the challenge of replicating these emergent characteristics in multimodal reasoning. Existing approaches often fail when trying to produce the \"aha moment\" and increased response length, especially when models are processing both visual and textual data. Many implementations struggle to show the autonomous development of reasoning skills like DeepSeek R1. \n\nTo address this, **VisualThinker-R1-Zero** is introduced which successfully replicate the ", "affiliation": "University of California, LA", "categories": {"main_category": "Multimodal Learning", "sub_category": "Vision-Language Models"}, "podcast_path": "2503.05132/podcast.wav"}