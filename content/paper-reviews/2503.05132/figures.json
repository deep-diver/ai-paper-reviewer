[{"figure_path": "https://arxiv.org/html/2503.05132/x1.png", "caption": "Figure 1: The training dynamics of VisualThinker-R1-Zero on Qwen2-VL-2B base model. Benchmark accuracy is measured on CV-Bench, and the average response length is calculated from rollouts on SAT training samples. Initially, we observed a drop in length because the base model tended to generate HTML code. This behavior was quickly suppressed by RL, leading the model to adopt a more appropriate output format and a regular increase in response length. Afterwards, we observed a multimodal \u2018aha moment\u2019\u2014the emergence of self-reflection in models\u2019 response, as described in the DeepSeek-R1 paper, followed by a consistent positive correlation between response length and benchmark accuracy.", "description": "This figure illustrates the training dynamics of the VisualThinker-R1-Zero model, trained on the Qwen2-VL-2B base model.  Two key metrics are tracked: benchmark accuracy (on the CV-Bench dataset) and average response length (calculated from rollouts on SAT training samples). Initially, the model's response length decreases due to an unexpected tendency to generate HTML code. Reinforcement learning (RL) quickly corrects this, leading to a steady increase in response length.  A significant event, termed the \"aha moment,\" marks the emergence of self-reflection in the model's responses.  This \"aha moment\" is accompanied by a strong positive correlation between response length and improved benchmark accuracy, demonstrating the model's increased reasoning capabilities.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2503.05132/x2.png", "caption": "Figure 2: Comparison between RL and SFT training. Our method achieves a significant improvement over the base model and the instruction fine-tuned model. Specifically, Qwen2-VL-2B + R1 outperforms Qwen2-VL-2B (base model) by approximately ~30%, Qwen2-VL-2B-Instruct (instruction fine-tuned model) by ~5%, and Qwen2-VL-2B SFT (base model + SFT) by ~2%.", "description": "Figure 2 illustrates the performance comparison of three different training methods: RL (Reinforcement Learning), SFT (Supervised Fine-Tuning), and a baseline model (Qwen2-VL-2B).  The graph shows that using reinforcement learning (RL) on the Qwen2-VL-2B model achieves significantly better accuracy on the CV-Bench benchmark than the baseline or the instruction-tuned model. Specifically, the RL-trained model surpasses the baseline by roughly 30%, the instruction-tuned model by about 5%, and a model trained using supervised fine-tuning (SFT) by approximately 2%.  This highlights the effectiveness of the RL approach in improving the model's performance on vision-centric spatial reasoning tasks.", "section": "4.3 Main Results"}, {"figure_path": "https://arxiv.org/html/2503.05132/x3.png", "caption": "Figure 3: Example response of applying RL to supervised fine-tuned models.", "description": "This figure showcases an example of a response generated by a model after applying reinforcement learning (RL) to a supervised fine-tuned model. The example highlights a key issue encountered: the model produces a response that exhibits trivial reasoning rather than genuine problem-solving strategies. The model provides a superficial answer by simply mentioning the steps involved in determining the solution, instead of demonstrating a deeper understanding or reasoning process.", "section": "5 Challenges of Applying RL to Supervised Fine-Tuned Models"}, {"figure_path": "https://arxiv.org/html/2503.05132/x4.png", "caption": "Figure 4: Response length across training steps for different fine-tuning settings during RL. The x-axis represents training steps, while the y-axis shows the response length. Models with different fine-tuning configurations are compared: Freeze LLM (green), Freeze Vision Encoder (blue), and Full Finetune (red). The response length drops significantly in the early training phase and stabilizes over time. However, despite improved accuracy, all three RL-based fine-tuning on Instruct Model does not necessarily enhance reasoning capabilities, as responses tend to remain short and trivial", "description": "Figure 4 illustrates the impact of different fine-tuning strategies on response length during reinforcement learning (RL) training.  The x-axis represents the training steps, and the y-axis shows the average response length generated by the model. Three distinct fine-tuning approaches are compared: 1) Freezing the Language Model (LLM) parameters during RL training (green line), 2) Freezing the Vision Encoder parameters during RL training (blue line), and 3) Fine-tuning both the LLM and Vision Encoder parameters simultaneously during RL training (red line).  The graph reveals a common trend across all three strategies: a sharp decrease in response length at the beginning of training, followed by a period of stabilization. Although these RL-based fine-tuning methods improved accuracy, the response length remained relatively short, and the responses generated lacked sophisticated reasoning capabilities.  This suggests that simply applying RL to already fine-tuned models may not be sufficient to induce deeper, more complex reasoning.", "section": "5 Challenges of Applying RL to Supervised Fine-Tuned Models"}]