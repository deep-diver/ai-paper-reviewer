{"references": [{"fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-03-02", "reason": "This paper introduces a key safety technique, preference tuning, which is directly relevant to the core methodology and problem addressed in the target paper."}, {"fullname_first_author": "Yangyi Chen", "paper_title": "Why should adversarial perturbations be imperceptible? rethink the research paradigm in adversarial NLP", "publication_date": "2022-12-01", "reason": "This paper highlights the challenges of adversarial attacks on LLMs, motivating the need for safety-aligned models and methods to mitigate such attacks."}, {"fullname_first_author": "Yang", "paper_title": "Shadow alignment: The ease of subverting safely-aligned language models", "publication_date": "2023-10-02", "reason": "This paper directly addresses the core challenge explored in the target paper - the degradation of safety in fine-tuned LLMs, thus forming a strong basis for the proposed solution."}, {"fullname_first_author": "Mitchell Wortsman", "paper_title": "Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time", "publication_date": "2022-07-01", "reason": "This paper introduces the concept of model merging, a crucial technique upon which the target paper's method is based, providing foundational support for the proposed approach."}, {"fullname_first_author": "Hasan Abed Al Kader Hammoud", "paper_title": "Model merging and safety alignment: One bad model spoils the bunch", "publication_date": "2024-12-01", "reason": "This paper directly investigates the intersection of model merging and safety alignment, which is the core focus of the target paper, providing further context and rationale for the proposed method."}]}