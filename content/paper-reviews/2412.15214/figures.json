[{"figure_path": "https://arxiv.org/html/2412.15214/x2.png", "caption": "Figure 1: LeviTor is capable of generating videos with controlled occlusion, better depth changes, and complex 3D orbiting movement based on user inputs. Given an initial frame, users can easily draw 3D trajectory using our inference pipeline to represent their desired movements for designated area. We highly recommend viewing the supplementary materials for detailed video demonstrations.", "description": "This figure showcases LeviTor's ability to generate videos with fine-grained control over 3D object movement.  Starting with a single image, users can interactively define a 3D trajectory by drawing on the 2D image and specifying depth. The generated videos demonstrate LeviTor's capacity to handle complex 3D orbiting motion, accurate depth changes, and controlled occlusion between objects along that trajectory. The results highlight the system's ability to translate simple user inputs into realistic and dynamic video sequences.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2412.15214/x3.png", "caption": "Figure 2: An example of object movement and occlusion represented by K-means clustered points.", "description": "This figure shows how object movement and occlusion are represented using K-means clustering. The K-means algorithm groups similar pixels in the object masks into clusters, represented by points.  The spatial distribution and density of these points convey information about the object's movement and occlusion. For instance, as an object moves closer to the camera, the points spread out due to perspective scaling, indicating depth changes. Similarly, when objects occlude each other, the distribution of points shifts, reflecting the occlusion dynamics. This representation simplifies user interaction while capturing essential 3D attributes of object trajectories without explicitly estimating 3D trajectories.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2412.15214/x4.png", "caption": "Figure 3: Control signal generation process of LeviTor.", "description": "This figure illustrates the process of generating the control signals used by the LeviTor model for image-to-video synthesis.  It starts with input video data, including object masks. These masks undergo K-means clustering to identify key representative points. A depth estimation network (DepthAnythingV2) is then used to determine the relative depth of these points. Finally, these points with their depth information and instance information are combined to create a comprehensive control signal that guides the video generation process.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2412.15214/x5.png", "caption": "Figure 4: Inference pipeline of LeviTor, which consists of user retrieval panel, interactive panel, 3D rendered object masks generation and video synthesis. Users can easily draw 3D trajectories through our retrieval panel and interactive panel, and our system later use these inputs to generate user desired videos.", "description": "The figure illustrates the LeviTor inference pipeline, detailing the user interaction and processing steps for 3D trajectory-based video generation.  Users start by selecting objects in a retrieval panel, then use an interactive panel to draw 3D trajectories by clicking points and specifying depth.  The system processes this input by generating 3D rendered object masks, which are then used as control signals for a video diffusion model to synthesize the final video, aligning with the user-defined 3D trajectories.", "section": "3.3. Inference Pipeline"}, {"figure_path": "https://arxiv.org/html/2412.15214/x6.png", "caption": "Figure 5: 3D rendered object masks generation pipeline.", "description": "This figure illustrates the process of generating 3D rendered object masks, a crucial step in LeviTor's inference pipeline.  It starts with user-provided sparse 3D trajectories (points in 2D space with associated depth values). These points are projected into a 3D camera coordinate system.  Then, a 3D rendering process creates 3D object masks. Finally, K-means clustering on these rendered masks refines the control signals by generating a set of representative clustered points that serve as input for the video generation model. This process efficiently translates user-provided sparse 3D trajectory information into a representation suitable for controlling the video generation process while addressing depth and occlusion challenges inherent in 3D scene synthesis.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2412.15214/x7.png", "caption": "Figure 6: Qualitative comparison with DragAnything\u00a0[47] and DragNUWA\u00a0[53]. LeviTor and DragAnything both support moving user-selected mask areas, whereas DragNUWA directly encodes trajectories as control signals and does not support user selection of operation areas. The top two rows show evaluation on control of mutual occlusion between objects. The left bottom images show comparison of forward and backward object movements control. The right bottom images show a case of complex motion implementation.", "description": "Figure 6 presents a qualitative comparison of LeviTor against two state-of-the-art methods, DragAnything and DragNUWA, in controlling object movements within video generation.  All three methods are evaluated on three key aspects: handling mutual occlusion between objects (top two rows), controlling forward and backward object movements (bottom-left), and implementing complex motions (bottom-right).  The comparison highlights LeviTor's ability to provide more nuanced control compared to methods that use only 2D trajectory inputs or pre-encoded trajectories. DragAnything allows user-selected mask areas to be moved, similar to LeviTor. DragNUWA, however, operates by directly encoding trajectories without user selection of areas of operation.", "section": "4.2. Comparison with Other Approaches"}, {"figure_path": "https://arxiv.org/html/2412.15214/x8.png", "caption": "Figure 7: Ablation on Instance and Depth information. Enlarged details are shown in red boxes. Zoom in for better viewing.", "description": "This figure shows an ablation study on the impact of instance and depth information on the image-to-video synthesis results.  The results are presented as videos. The top row shows the results when both instance and depth information are used. Subsequent rows show results when either instance or depth information are omitted.  Red boxes highlight key areas to emphasize the differences in video synthesis quality. Zooming in enhances the visibility of details.", "section": "4.3. Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2412.15214/x9.png", "caption": "Figure 8: Ablation on number of inference control points.", "description": "This ablation study investigates the effect of varying the number of control points used during inference in the LeviTor model.  The figure shows several examples of video generation results using different scaling factors applied to the initial number of control points.  It demonstrates the trade-off between motion amplitude and generation quality. Fewer control points may lead to large movements but blurry results, while excessively many control points may constrain the motion and introduce artifacts.  The optimal number of control points balances these factors to achieve both realistic movement and visual fidelity.", "section": "4.3. Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2412.15214/x10.png", "caption": "Figure S1: Ablation results on the Number of Control Points for Inference. We highly recommend viewing the visualization results for detailed video demonstrations.", "description": "This figure shows an ablation study on the impact of varying the number of control points used during inference in the LeviTor model.  The study compares results using the model's default number of control points and a significantly higher density of control points. The results demonstrate that while using the default number of points yields realistic results for fluid motion and human running, excessively increasing the number of points can lead to unnatural artifacts and limitations in capturing non-rigid movements. This highlights the importance of using a carefully balanced number of control points in LeviTor to effectively generate video with both rigid and non-rigid motions.", "section": "Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2412.15214/x11.png", "caption": "Figure S2: Comparison with Single-point Control model. We highly recommend viewing the visualization results for detailed video demonstrations.", "description": "Figure S2 shows a comparison between LeviTor and a single-point control model for video generation.  The single-point control method uses only the center point of each object's mask and its depth changes as the control signal, lacking the richer information provided by multiple clustered points used in LeviTor. The figure visually demonstrates the limitations of the single-point method in representing 3D motion accurately, highlighting LeviTor's advantages in handling complex scenarios such as occlusions and size changes due to movement. The differences in visual fidelity and accuracy of motion representation are apparent.  To fully appreciate the differences, it is recommended to view the accompanying videos.", "section": "C. Comparison with Single-point Control"}]