[{"figure_path": "https://arxiv.org/html/2412.14015/x2.png", "caption": "Figure 1: \nIllustration and capabilities of Prompt Depth Anything.\n(a) Prompt Depth Anything is a new paradigm for metric depth estimation, which is formulated as prompting a depth foundation model with a metric prompt, specifically utilizing a low-cost LiDAR as the prompt.\n(b) Our method enables consistent depth estimation, addressing the limitations of Metric3D v2\u00a0[24] that suffer from inaccurate scale and inconsistency.\n(c) It achieves accurate 4K accurate depth estimation, significantly surpassing ARKit LiDAR Depth (240 \u00d7\\times\u00d7 320).", "description": "This figure illustrates the Prompt Depth Anything paradigm and its capabilities. (a) shows the overall framework: a low-cost LiDAR is used as a metric prompt to guide a depth foundation model for accurate metric depth estimation. (b) highlights the consistency of multi-view depth estimation in a static scene, comparing the results of Prompt Depth Anything with Metric3D v2 and ground truth measurements.  It demonstrates that Prompt Depth Anything maintains accuracy and consistency across different viewpoints, while Metric3D v2 suffers from scale and consistency issues. (c) demonstrates high-resolution accurate depth estimation in a dynamic scene, comparing Prompt Depth Anything with ARKit LiDAR Depth. Prompt Depth Anything produces high-resolution (4K) and more accurate depth for the moving subject, significantly outperforming the low-resolution (240x320) and noisy output of ARKit LiDAR Depth.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2412.14015/x3.png", "caption": "Figure 2: Overview of Prompt Depth Anything.\n(a) Prompt Depth Anything builds on a depth foundation model\u00a0[70] with a ViT encoder and a DPT decoder, and adds a multi-scale prompt fusion design, using a prompt fusion block to fuse the metric information at each scale.\n(b)\nSince training requires both low-cost LiDAR and precise GT depth, we propose a scalable data pipeline that simulates LiDAR depth for synthetic data with precise GT depth, and generates pseudo GT depth for real data with LiDAR.\nAn edge-aware depth loss is proposed to merge accurate edges from pseudo GT depth with accurate depth in textureless areas from FARO annotated GT depth on real data.", "description": "Prompt Depth Anything uses a low-cost LiDAR as a prompt to guide a depth foundation model for accurate metric depth output, achieving a resolution up to 4K. This is achieved using a multi-scale prompt fusion design within the model's architecture, specifically a DPT decoder. The training process involves a scalable data pipeline that addresses the challenge of limited datasets containing both LiDAR and precise ground truth depth. This pipeline uses synthetic data LiDAR simulation, real data pseudo ground truth depth generation, and an edge-aware depth loss function that leverages pseudo and annotated ground truth data.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2412.14015/x4.png", "caption": "Figure 3: Effects on the synthetic data lidar simulation and real data pseudo GT generation with the edge-aware depth loss. The middle and right columns are the depth prediction results of our different models. The two rows highlight the significance of sparse anchor interpolation for lidar simulation and pseudo GT generation with edge-aware depth loss, respectively.", "description": "This figure shows the effect of using sparse anchor interpolation for LiDAR simulation and pseudo ground truth depth generation with edge-aware depth loss. It includes two examples, one for synthetic data and one for real-world data. In each example, the leftmost image is the input RGB image along with the simulated LiDAR depth data represented by colored points. For synthetic data, the middle image shows the depth prediction without sparse anchor interpolation, effectively showcasing depth super-resolution. The right image shows the depth prediction using sparse anchor interpolation, showing accurate depth prediction with simulated LiDAR noise.  For real-world data, the middle image shows the depth prediction directly supervised by noisy and hole-filled ground truth depth, resulting in poor depth quality, especially at edges. The right image shows the depth prediction using edge-aware depth loss with pseudo ground truth and noisy ground truth, which yields high-quality edge depth and accurate depth prediction. The two rows emphasize the improvements of using sparse anchor interpolation and edge-aware loss respectively by highlighting their depth prediction improvements at the edges.", "section": "Method"}, {"figure_path": "https://arxiv.org/html/2412.14015/x5.png", "caption": "Figure 4: Qualitative comparisons with the state-of-the-art.\n\u201cMetric3D v2\u201d and \u201cDepth Any. v2\u201d are scale-shift corrected with ARKit depth.\nThe pink boxes denote the GT depth and depth percentage error map, where red represents high error, and blue indicates low error.", "description": "This figure presents qualitative results of the proposed method comparing with other state-of-the-art methods on two datasets: ARKitScenes and ScanNet++. It contains predicted depth maps from various methods and also ground truth and error maps. We can observe that the proposed method outperforms previous methods significantly, producing more accurate depth maps that faithfully align with the ground truth, and resulting in lower error rates, particularly in challenging areas like thin structures.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.14015/x6.png", "caption": "Figure 5: Qualitative comparisons of TSDF reconstruction. *_align denotes the scale-shift corrected depth with ARKit depth.", "description": "This figure shows qualitative results of 3D scene reconstruction using the TSDF fusion method with different depth estimation methods.  The figure compares the proposed method's reconstruction quality against Depth Anything v2 and Metric3D v2, highlighting improvements in reconstructing details and overall scene completeness.  \"*_align\" in the caption indicates that the depth maps from monocular depth estimation methods (Marigold, Depth Any v2, Metric 3D v2) are aligned to the scale of ARKit depth using RANSAC to allow for a fair comparison of the metric depth results.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.14015/x7.png", "caption": "Figure 6: Outdoor reconstruction by taking the vehicle LiDAR as metric prompt. Please refer to the supp. for more video results.", "description": "This figure showcases the 3D reconstruction of an outdoor street scene using vehicle LiDAR as a metric prompt.  It demonstrates the application of Prompt Depth Anything to large-scale outdoor environments by replacing the low-cost LiDAR prompt with vehicle LiDAR data and training on the Shift autonomous driving dataset. The top left section shows the input RGB image of the street scene, while the remaining sections display different views of the reconstructed 3D point cloud, illustrating depth and structural details.", "section": "4.5. Application: 3D Reconstruction"}, {"figure_path": "https://arxiv.org/html/2412.14015/x8.png", "caption": "Figure 7: Zero-shot testing on diverse scenes.", "description": "This figure showcases the zero-shot performance of Prompt Depth Anything on a diverse range of scenes, highlighting the model's robustness and generalization ability.  The scenes depicted include indoor environments with challenging lighting and thin structures (rooms, gyms, and museums), and extend to outdoor environments and human subjects.", "section": "4.4. Zero-shot Testing on Diverse Scenes"}, {"figure_path": "https://arxiv.org/html/2412.14015/x9.png", "caption": "Figure 8: Robotic grasping setup and input signal types. Our goal is to grasp objects of various types using image/LiDAR/depth inputs. Red rectangles indicate potential object positions.", "description": "This figure illustrates the robotic grasping setup used in the experiments.  The setup involves a robotic arm tasked with grasping objects of varying properties (transparent, specular, diffusive) placed at different distances (near, mid, far). The input signals used for controlling the robotic arm are: RGB images from a camera, LiDAR depth data, and the monocular depth estimations generated by the proposed model. The red rectangles highlight potential positions of the objects.", "section": "4.6. Application: Generalized Robotic Grasping"}, {"figure_path": "https://arxiv.org/html/2412.14015/x10.png", "caption": "Figure 9: \nOur accurate and high-resolution depth enables dynamic 3D reconstruction from a single moving camera.\nHere we illustrate the reconstruction results of a human walking in the library. The foreground is segmented with a SAM2\u00a0[49] model.", "description": "This figure demonstrates the capability of the proposed method to perform dynamic 3D reconstruction of a human subject walking in a library setting using a single moving camera. The high accuracy and resolution of the depth map facilitate precise 3D reconstruction, and SAM2 is employed to segment the foreground, effectively separating the human subject from the background.", "section": "4.5. Application: 3D Reconstruction"}, {"figure_path": "https://arxiv.org/html/2412.14015/x11.png", "caption": "Figure 10: Generalizability to different resolutions.\nOur model can infer depth for images of different resolutions from 512p to 2160p.", "description": "This figure shows the results of applying Prompt Depth Anything to images of different resolutions from 512p to 2160p, including RGB image, the predicted depth by the model, and the LiDAR depth provided by the iPhone. Even with higher resolution input, Prompt Depth Anything predicts plausible depth estimations, showing its generalizability to different resolutions.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.14015/x12.png", "caption": "Figure 11: Effects of using real data.", "description": "This figure illustrates the improvement of adding real data to train the model by showing two qualitative results on plants. The top row shows the input RGB image and the predicted depth using ARKit. The bottom row shows the model output trained with only synthetic data and trained with both synthetic and real data, respectively. It demonstrates that real data improves the quality of the edges of the plant, while training with synthetic data only has good performance on flat regions.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.14015/x13.png", "caption": "Figure 12: Visualization results of simulated LiDAR.\n\u201cInterp. Simu.\u201d is the proposed interpolation method, which is interpolated from sparse anchors depth. This method effectively simulates the noise of real LiDAR data. We also provide the naive downsampled simulated LiDAR for comparison.", "description": "This figure visualizes different methods for simulating LiDAR data, comparing a naive downsampling approach with the proposed sparse anchor interpolation method. The naive approach involves directly downsampling the high-resolution depth map to a lower resolution, which fails to capture the noise characteristics of real LiDAR data. In contrast, the sparse anchor interpolation method first downsamples the depth map and then samples points on this map using a distorted grid. The remaining depth values are interpolated from these sampled points using RGB similarity with KNN. This approach effectively simulates the noise present in real LiDAR data, resulting in more realistic simulated LiDAR depth maps for training depth estimation models. The figure includes visualizations of RGB image patch, full RGB image, depth output from the proposed model, and depth from ARKit.", "section": "4. Experiments/4.3. Ablations and Analysis"}, {"figure_path": "https://arxiv.org/html/2412.14015/x14.png", "caption": "Figure 13: ZipNeRF depth of different training frames.\nTraining with resampled frames removing blurred frames leads to a better ZipNeRF reconstruction.", "description": "This figure shows the effect of training ZipNeRF with different frames. The left image shows the result when trained with original frames which include blurred frames. The right image shows the result when trained with resampled frames which remove the blurred ones. Training with resampled frames yield better reconstruction quality.", "section": "Appendix B. Additional Results"}, {"figure_path": "https://arxiv.org/html/2412.14015/x15.png", "caption": "Figure 14: Illustration of different depth annotation types.\nPlease refer to Appendix\u00a0B for more descriptions.", "description": "This figure shows three different types of depth annotations used for training: (1) Ground Truth (GT) from FARO scanned mesh, which suffers from incompleteness and poor edge quality due to occlusions during scanning. (2) Pseudo GT generated using NeRF reconstruction, which shows better edges compared to GT, but struggles with planar regions. (3) ZipNeRF reconstruction which was trained with resampled frames. It has been observed that resampling frames removes blurring and improves reconstruction.", "section": "Appendix B. Additional Results"}, {"figure_path": "https://arxiv.org/html/2412.14015/x16.png", "caption": "Figure 15: Illustrations of our method and optional designs.\nPlease refer to Sec.\u00a0C.2 for more details.", "description": "This figure illustrates the architecture of Prompt Depth Anything and optional designs for incorporating a low-resolution depth map as a prompt into a depth foundation model. (a) The Prompt Fusion Block integrates the low-resolution depth information at multiple scales within the DPT Decoder.  The block design takes a low-resolution depth map, resizes it, passes it through a shallow convolutional network, projects it to the same dimension as the image features, and adds the resulting depth features to the DPT intermediate features. Optional designs for prompting architecture are: (b) Adapting the Layer Normalization parameters of the encoder based on the conditioning input. (c) Injecting a cross-attention block after each self-attention block and integrating the conditioning input via cross-attention. (d) Copying the encoder blocks and inputting control signals to the copied blocks to control the output depth.", "section": "C. More Details"}]