[{"figure_path": "https://arxiv.org/html/2504.05303/x2.png", "caption": "Figure 1: \nWe present InteractVLM, a novel method for estimating contact points on both human bodies and objects from a single in-the-wild image,\nshown here\nas red patches.\nOur method goes beyond traditional binary contact estimation methods by estimating contact points on a human in relation to a specified object.\nWe do so by leveraging the broad visual knowledge of a large Visual Language Model.", "description": "This figure showcases InteractVLM's ability to predict 3D contact points between humans and objects using only a single, unconstrained image. Red patches highlight the estimated contact points on the human and object.  Unlike traditional methods that only determine whether contact exists (binary contact), InteractVLM identifies specific contact locations and relates them to particular objects, making the interaction more semantically rich. This advanced capability stems from the model's utilization of a large Visual Language Model (VLM), which provides a broad understanding of visual information and the relationships between objects and human actions.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2504.05303/x3.png", "caption": "Figure 2: \nOverview of InteractVLM.\nGiven a color image, our VLM performs the core reasoning, and guides a novel MV-Loc model to localize contacts on both bodies and objects in 3D.\nHere we show only the body;\nfor details, and object contact, see Fig.\u00a03.", "description": "InteractVLM processes a color image using a Vision-Language Model (VLM) to understand the scene and reason about potential human-object interactions.  The VLM's output guides a novel Multi-view Localization (MV-Loc) model.  The MV-Loc model then uses multi-view rendering of 3D human and object models to identify and localize 2D contact points.  These 2D contact points are lifted to 3D space to estimate contact points on both the human body and the object.  This figure focuses on the process for the human body; object contact details are shown in Figure 3.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2504.05303/x4.png", "caption": "Figure 3: \nMethod overview.\nGiven a single in-the-wild color image, our novel InteractVLM method estimates 3D contact points on both humans and objects (a). Then, we reconstruct a 3D human and object in interaction by exploiting these contacts (b).\nMore specifically:\n(a)\u00a0Contact estimation.\nGiven an image, I\ud835\udc3cIitalic_I, and\nprompt text, Ti\u2062n\u2062psubscript\ud835\udc47\ud835\udc56\ud835\udc5b\ud835\udc5dT_{inp}italic_T start_POSTSUBSCRIPT italic_i italic_n italic_p end_POSTSUBSCRIPT,\nour VLM, \u03a8\u03a8\\Psiroman_\u03a8, produces contact tokens for humans and objects, <HCON> and <OCON>, which are projected (\u0393\u0393\\Gammaroman_\u0393) into feature embeddings, EHsuperscript\ud835\udc38\ud835\udc3bE^{H}italic_E start_POSTSUPERSCRIPT italic_H end_POSTSUPERSCRIPT and EOsuperscript\ud835\udc38\ud835\udc42E^{O}italic_E start_POSTSUPERSCRIPT italic_O end_POSTSUPERSCRIPT.\nThese guide a \u201cMulti-View [contact]\nLocalization\u201d model.\nThis renders the 3D human and object geometry via cameras, K\ud835\udc3e{K}italic_K, into multi-view 2D renders and passes these to encoder, \u0398\u0398\\Thetaroman_\u0398, while decoders, \u03a9Hsuperscript\u03a9\ud835\udc3b\\Omega^{H}roman_\u03a9 start_POSTSUPERSCRIPT italic_H end_POSTSUPERSCRIPT, \u03a9Osuperscript\u03a9\ud835\udc42\\Omega^{O}roman_\u03a9 start_POSTSUPERSCRIPT italic_O end_POSTSUPERSCRIPT, estimate and highlight 2D contacts in these renders. Then,\nthe FeatLift module, \u03a6\u03a6\\Phiroman_\u03a6,\ntransforms\nthe VLM\u2019s features (EHsuperscript\ud835\udc38\ud835\udc3bE^{H}italic_E start_POSTSUPERSCRIPT italic_H end_POSTSUPERSCRIPT, EOsuperscript\ud835\udc38\ud835\udc42E^{O}italic_E start_POSTSUPERSCRIPT italic_O end_POSTSUPERSCRIPT)\nto become 3D-aware\n(E3\u2062DHsubscriptsuperscript\ud835\udc38\ud835\udc3b3\ud835\udc37E^{H}_{3D}italic_E start_POSTSUPERSCRIPT italic_H end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 3 italic_D end_POSTSUBSCRIPT, E3\u2062DOsubscriptsuperscript\ud835\udc38\ud835\udc423\ud835\udc37E^{O}_{3D}italic_E start_POSTSUPERSCRIPT italic_O end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 3 italic_D end_POSTSUBSCRIPT)\nby exploiting the camera parameters, K\ud835\udc3e{K}italic_K.\nA final module lifts the detected 2D contacts to 3D.\n(b)\u00a03D HOI reconstruction.\nFor joint human-object reconstruction, we use InteractVLM\u2019s inferred contacts in an optimization framework.", "description": "Figure 3 illustrates the InteractVLM method.  Part (a) details the contact estimation process:  A vision-language model (VLM) processes an image and text prompt to generate contact tokens for humans and objects.  These tokens are converted into feature embeddings which guide a multi-view localization model.  This model renders the 3D human and object geometry from multiple viewpoints, creating 2D renderings that are processed to identify 2D contact points. The 2D points are transformed into 3D-aware representations using camera parameters. Finally, a module lifts these 2D contacts to 3D. Part (b) shows that the 3D human-object interaction (HOI) reconstruction leverages the estimated 3D contact points within an optimization framework.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2504.05303/x5.png", "caption": "Figure 4: \n\u201cSemantic Human Contact\u201d estimation\u00a0(Sec.\u00a04.2).\nGiven an image and an object label, InteractVLM infers body contacts for this object.\nInteractVLM outperforms a Semantic-DECO\u00a0[60] baseline.\nObjects are shown in green circles, and contacts as red patches.", "description": "This figure visually compares the performance of InteractVLM and the Semantic-DECO baseline on the task of \"Semantic Human Contact\" estimation.  Given an input image and the label of an object, both methods attempt to predict the contact points on the human body that are interacting with that specific object.  InteractVLM's predictions (shown in red patches) are highlighted on the body meshes (blue). The interacting objects are indicated by green circles. The results visually demonstrate that InteractVLM outperforms the Semantic-DECO baseline by producing more accurate and precise contact point predictions.", "section": "4.2. \u201cSemantic Human Contact\u201d Estimation"}, {"figure_path": "https://arxiv.org/html/2504.05303/x6.png", "caption": "Figure 5: \nInteractVLM\u2019s reliance on 3D annotations.\nWe evaluate performance for \u201cbinary human contact\u201d (F1 score, Y-axis) for models trained on a varying percentage of DAMON\u00a0[60] training data (X-axis).\nThe DECO baseline trains on 100% of DAMON.\nInstead, InteractVLM trains on a varying (smaller) portion of this dataset. Yet, it achieves a significantly higher performance,\nby leveraging the broad visual knowledge of foundation models.", "description": "This figure demonstrates InteractVLM's efficiency in using 3D annotations for training.  It shows that even with a small percentage of the DAMON dataset for training, InteractVLM significantly outperforms DECO, a baseline model that requires the full dataset. This superior performance highlights InteractVLM's ability to leverage the broader visual knowledge inherent in foundation models.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2504.05303/x7.png", "caption": "Figure 6: \n3D HOI reconstruction (Sec.\u00a05).\nWe build an optimization method that fits a SMPL-X body and OpenShape-retrieved object to an in-the-wild image.\nWe evaluate against the SotA method PHOSA\u00a0[69].\nReconstruction is guided by InteractVLM-inferred contacts.", "description": "Figure 6 showcases the 3D human-object interaction (HOI) reconstruction results obtained using the proposed InteractVLM method.  The method refines a SMPL-X human body model and an OpenShape-retrieved object model by fitting them to a single in-the-wild image. This fitting process is guided by the 3D contact points predicted by InteractVLM.  The results are compared against those of the state-of-the-art (SotA) PHOSA method for quantitative evaluation.", "section": "5. Joint Human-Object Reconstruction"}, {"figure_path": "https://arxiv.org/html/2504.05303/x8.png", "caption": "Figure S.1: \nContact Estimation Failure Cases.\nOur method struggles with unusual human poses (left). For objects (right), training on affordances rather than actual contacts can sometimes lead to ambiguous contact predictions, especially for large objects like chairs.\nHowever, no dataset exists for 3D object contacts for in-the-wild images.", "description": "Figure S.1 illustrates instances where the InteractVLM model's contact prediction accuracy falters.  The left side showcases examples of unusual or non-standard human poses that challenge the model's ability to accurately estimate contact points.  These poses deviate significantly from the typical interaction patterns observed during training.  The right side displays examples where contact predictions for objects are ambiguous. This ambiguity is particularly noticeable for larger objects, such as chairs. The reason for this is the use of object affordances during training in place of direct 3D contact labels, due to the non-existence of datasets containing 3D object contacts in real-world scenes.", "section": "S.5. Failure Cases"}, {"figure_path": "https://arxiv.org/html/2504.05303/x9.png", "caption": "Figure S.2: \nObject Retrieval Failure Cases.\nThe retrieved object meshes (right) differ notably from the actual objects in the input images (left), especially in cases of significant occlusion, atypical object instances, or limited database coverage. Despite these inaccuracies, the retrieval consistently selects objects within the correct semantic category.", "description": "Figure S.2 shows examples where the OpenShape object retrieval system, used in the InteractVLM framework, fails to perfectly match the 3D model to the object in the input image.  This can occur due to factors such as significant occlusion in the image, an atypical or unusual object instance not well-represented in the database of 3D models, or limitations in the database's coverage of object variations.  However, despite these inaccuracies in the retrieved object's precise shape and appearance, the system consistently selects objects from the correct semantic category.", "section": "S.5. Failure Cases"}, {"figure_path": "https://arxiv.org/html/2504.05303/x10.png", "caption": "Figure S.3: \nObject Affordance Prediction.\nHere we compare our InteractVLM method trained for object affordance prediction on PIAD\u00a0[67] dataset with the state-of-the-art IAGNet method.\nWe train for affordance detection because there exists no dataset of in-the-wild images paired with ground-truth 3D contacts for objects.\nNote that given an image of a person performing an action like \u201csit\u201d or \u201cgrasp\u201d, the affordance prediction task estimates \u201ccontact possibilities\u201d\non the object.", "description": "Figure S.3 showcases a comparison of object affordance prediction results between the InteractVLM model and the state-of-the-art IAGNet model.  The comparison uses the PIAD dataset.  The focus is on affordance prediction because there isn't a publicly available dataset of real-world images with corresponding 3D contact point annotations for objects.  The figure illustrates that for actions like 'sitting' or 'grasping', affordance prediction identifies areas on the object where contact is likely.", "section": "S.6. Qualitative Results"}, {"figure_path": "https://arxiv.org/html/2504.05303/x11.png", "caption": "Figure S.4: \nSemantic Human Contact estimation.\nHere we show results for \u201csemantic human contact\u201d estimation from in-the-wild images.\nEach row shows a person in contact with multiple objects.\nNote how InteractVLM estimates contact on bodies that is specific to the object.", "description": "Figure S.4 visualizes the results of semantic human contact estimation using the InteractVLM model. Each row presents a different scenario where a person interacts with multiple objects simultaneously. The figure highlights the model's capability to precisely predict contact points on the human body that are specifically related to each object interacted with, demonstrating the model's ability to distinguish between multiple contacts in complex interactions.", "section": "S.4. Semantic Human Contact Estimation"}]