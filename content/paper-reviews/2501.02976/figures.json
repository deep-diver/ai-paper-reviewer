[{"figure_path": "https://arxiv.org/html/2501.02976/x2.png", "caption": "Figure 1: Visualization comparisons on both real-world and synthetic low-resolution videos.\nCompared to the state-of-the-art VSR models\u00a0[73, 75], our results demonstrate more natural facial details and better structure of the text.\n(Zoom-in for best view)", "description": "Figure 1 presents a comparison of video super-resolution (VSR) results on both real-world and synthetic low-resolution video clips.  The figure directly compares the output of the proposed STAR model against two state-of-the-art VSR methods ([73, 75]). The comparison highlights the superior performance of STAR in preserving fine details, particularly showcasing the more natural rendering of facial features and text compared to the other methods. The enhanced clarity and detail achieved by STAR are visually evident in the figure.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2501.02976/x3.png", "caption": "Figure 2: Overview of the proposed\u00a0STAR.", "description": "This figure provides a detailed overview of the STAR model architecture.  It illustrates the flow of information through the various modules:  the input LR video and text are processed by separate encoders.  The LR video's latent representation is combined with the text embeddings.  A ControlNet module guides the T2V (Text-to-Video) model, which includes a Local Information Enhancement Module (LIEM) and a Global Spatial/Temporal Self-Attention module. The T2V model then generates the predicted velocity and the HR video.  Losses (velocity-prediction loss and dynamic frequency loss) are calculated and used for optimization during training.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2501.02976/x4.png", "caption": "Figure 3: Motivation of LIEM. Left: schematic diagram illustrating the impact of using only global structure versus a combination of local and global structures. Right: visual comparison on real-world and synthetic videos. (Zoom-in for best view)", "description": "Figure 3 demonstrates the effectiveness of the Local Information Enhancement Module (LIEM). The left panel shows a schematic comparing using only global information against a combination of local and global information.  The right panel presents visual comparisons on both real-world and synthetic videos to showcase the improved performance when employing both local and global information.  The results highlight how LIEM enhances local details and mitigates artifacts by incorporating local information before the global self-attention mechanism.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2501.02976/x5.png", "caption": "Figure 4: Motivation of DF Loss. Left: PSNR curves of low- and high-frequency components relative to ground truth across diffusion steps. The low-frequency PSNR increases during the early diffusion steps, while the high-frequency PSNR rises in the later diffusion steps.\nRight: visual results of low- and high-frequency components at different diffusion stage. (Zoom-in for best view)", "description": "Figure 4 illustrates the concept of Dynamic Frequency Loss by showing the change in PSNR values for low- and high-frequency components throughout the diffusion process.  The left panel graphs the PSNR, demonstrating that low-frequency details improve earlier in the process, while high-frequency details improve later. The right panel shows example visual results, supporting the observation that low-frequency components (larger structures) are recovered first, followed by high-frequency components (fine details like edges) as the diffusion process progresses.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2501.02976/x6.png", "caption": "Figure 5: Dynamic Frequency Loss. Left: curves of weighting function c\u2062(t)\ud835\udc50\ud835\udc61c(t)italic_c ( italic_t ) for different \u03b1\ud835\udefc\\alphaitalic_\u03b1. Right: details of DF loss.", "description": "Figure 5 illustrates the Dynamic Frequency Loss (DFL) used in the STAR model.  The left panel shows curves of the weighting function *c(t)* for different values of *\u03b1*. This weighting function controls how much emphasis the loss places on low-frequency versus high-frequency components at different stages of the diffusion process.  The right panel provides a detailed breakdown of the DFL components, showing how low-frequency and high-frequency components are combined based on the *c(t)* weighting to achieve balanced fidelity.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2501.02976/x7.png", "caption": "Figure 6: Qualitative comparisons on synthetic LR videos from OpenVid30 and REDS30[35]. (Zoom-in for best view)", "description": "This figure displays a qualitative comparison of video super-resolution (VSR) results on synthetic low-resolution (LR) videos.  It compares the output of several different VSR models, including StableSR, RealBasicVSR, Upscale-A-Video, RealViformer, and the authors' proposed STAR model, against the ground truth (GT) high-resolution videos.  The comparison demonstrates the visual quality differences between various methods in terms of detail, sharpness, and artifact reduction on both the OpenVid30 and REDS30 datasets. Zoom in for a detailed view of the results.", "section": "4.2. Comparisons"}, {"figure_path": "https://arxiv.org/html/2501.02976/x8.png", "caption": "Figure 7: Qualitative comparisons on real-world test videos in VideoLQ [11] dataset. (Zoom-in for best view)", "description": "Figure 7 presents a qualitative comparison of real-world video super-resolution (VSR) results on the VideoLQ dataset.  It visually demonstrates the performance of various state-of-the-art VSR models, including Real-ESRGAN, DBVSR, RealBasicVSR, Upscale-A-Video, RealViformer, StableSR, and the authors' proposed STAR model. The figure showcases the differences in the ability of each model to restore high-resolution videos from low-resolution inputs, highlighting improvements in sharpness, detail, and artifact reduction. Zooming in allows for better appreciation of the finer details of each restored video.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2501.02976/x9.png", "caption": "Figure 8: Qualitative comparisons on temporal consistency in REDS30 [35] and OpenVid dataset. (Zoom-in for best view)", "description": "This figure demonstrates the temporal consistency of video super-resolution (VSR) results generated by different methods, including STAR (the proposed approach), on the REDS30 and OpenVid datasets.  Temporal consistency refers to how well the model maintains smooth and realistic motion across consecutive frames in a video.  The figure directly compares the results of STAR to those of several other state-of-the-art VSR methods. By visually inspecting the results, the user can easily assess which method yields a more temporally consistent video and the degree of artifacts or flickering that is present.", "section": "4.2. Comparisons"}, {"figure_path": "https://arxiv.org/html/2501.02976/x10.png", "caption": "Figure 9: Ablation study about LIEM. Left: illustration of different insertion positions of LIEM and the structure of LIEM. Right: visual comparison on real-world and synthetic videos with different LIEM positions.", "description": "This figure presents an ablation study on the Local Information Enhancement Module (LIEM). The left side illustrates the different positions where LIEM can be inserted within the model architecture and details its internal structure. The right side shows a visual comparison of real-world and synthetic video results obtained using LIEM in various positions, demonstrating the impact of LIEM's placement on the overall performance of the model.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2501.02976/extracted/6113603/figure_of_supp/bt_ablation.png", "caption": "Figure 10: Illustration on scaling up with larger t2v models on a real-world low-quality video. (Zoom-in for best view)", "description": "This figure demonstrates the impact of using larger text-to-video (T2V) models within the STAR framework.  It showcases a real-world low-quality video enhanced by several different models, including the STAR model using different T2V backbones.  By comparing the results, the figure highlights the improvements in visual quality obtained by leveraging more powerful T2V models for video super-resolution.  Specifically, it illustrates the increases in detail and realism achieved when scaling up to larger T2V models.  Zooming in on the image will reveal finer details.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2501.02976/x11.png", "caption": "Figure 11: Ablation on b\u2062(t)\ud835\udc4f\ud835\udc61b(t)italic_b ( italic_t ). Higher hyper-parameter \u03b2\ud835\udefd\\betaitalic_\u03b2 produces results with greater fidelity, while lower \u03b2\ud835\udefd\\betaitalic_\u03b2 emphasizes more perceptual quality.", "description": "This figure shows the effect of changing the hyperparameter \\(\\beta\\) in the weighting function \\(b(t) = \\beta (1 - \\frac{t}{t_{max}})\\) of the Dynamic Frequency Loss.  The x-axis represents the diffusion timestep \\(t\\), and the y-axis represents the value of \\(b(t)\\). Different colored lines represent different values of \\(\\beta\\).  Higher values of \\(\\beta\\) place more emphasis on the fidelity of the reconstruction, which leads to results with more accurate details but potentially less natural appearance. Lower values of \\(\\beta\\) put more emphasis on perceptual quality, leading to results that may look more natural but may sacrifice some detail accuracy.  The figure demonstrates the trade-off between fidelity and perceptual quality controlled by \\(\\beta\\).", "section": "3.3 Dynamic Frequency Loss"}, {"figure_path": "https://arxiv.org/html/2501.02976/x12.png", "caption": "Figure 12: User study results. Our STAR\u00a0is preferred by human evaluators for both visual quality and temporal consistency.", "description": "A user study was conducted to compare the visual quality and temporal consistency of video super-resolution results generated by STAR against other state-of-the-art methods.  Human evaluators were asked to choose the best results based on these two criteria from a set of options including STAR and competing methods. The pie charts show that a significant majority of evaluators preferred STAR's output for both visual quality and temporal consistency across both synthetic and real-world datasets.", "section": "4.2. Comparisons"}, {"figure_path": "https://arxiv.org/html/2501.02976/x13.png", "caption": "Figure 13: Qualitative comparisons on synthetic datasets. Our\u00a0STAR\u00a0generates more detailed and realistic results. (Zoom-in for best view)", "description": "Figure 13 presents a qualitative comparison of video super-resolution (VSR) results on synthetic datasets.  It visually showcases the outputs of several state-of-the-art VSR methods alongside the results generated by the STAR model. The comparison highlights that STAR produces significantly more detailed and realistic results compared to the other methods, demonstrating its superior performance in restoring fine details and textures within the video frames.  The \"Zoom-in for best view\" notation indicates that the details of the results are best appreciated at higher magnification.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2501.02976/x14.png", "caption": "Figure 14: Qualitative comparisons on real-world datasets. Our\u00a0STAR\u00a0produces the clearest facial details and the most accurate text structure. (Zoom-in for best view)", "description": "This figure presents a qualitative comparison of different video super-resolution (VSR) methods on real-world datasets. It showcases the results of several state-of-the-art VSR models, including Real-ESRGAN, RealBasicVSR, ResShift, StableSR, MGLDVSR, Upscale-A-Video, RealViformer, and the proposed STAR method.  Each model's output is displayed alongside the input low-resolution video and the ground truth high-resolution video.  The comparison highlights STAR's superior performance in restoring fine details, particularly in facial features and text, demonstrating its enhanced ability to reconstruct clear, sharp images from real-world low-resolution inputs.", "section": "4.2. Comparisons"}]