[{"figure_path": "https://arxiv.org/html/2502.15681/extracted/6217836/img/main_3.png", "caption": "Figure 1: The gradient update for the one-step student in f\ud835\udc53fitalic_f-distill. The gradient is a product of the difference between the teacher score and fake score, and a weighting function determined by the chosen f\ud835\udc53fitalic_f-divergence and density ratio. The density ratio is readily available from the discriminator in the auxiliary GAN objective.", "description": "Figure 1 illustrates the training process for a one-step diffusion model using the proposed f-distill method.  The core idea is to match the distribution of samples generated by a simplified, one-step student model to that of a more complex, multi-step teacher model. This is achieved by minimizing an f-divergence between the teacher and student sample distributions. The gradient update equation for the student model is shown.  It reveals three key components: (1) the difference between the teacher's score (gradient of the log-probability density) and the student's (fake) score, (2) a weighting function determined by the f-divergence chosen and the density ratio between the teacher and student distributions, and (3) the gradient of the student's generative function. The weighting function dynamically adjusts the contribution of samples from different density regions, ensuring stability and improved accuracy. Critically, the density ratio, which acts as a weight factor emphasizing high-density samples in the teacher distribution, is easily obtained from an auxiliary GAN (Generative Adversarial Network) objective. This GAN discriminator allows for direct calculation of the density ratio, streamlining the calculation and improving computational efficiency.", "section": "3. Method: general f-divergence minimization"}, {"figure_path": "https://arxiv.org/html/2502.15681/x1.png", "caption": "Figure 2: Score difference and the weighting function on a 2D example. h\u210ehitalic_h is the weighting function in forward-KL. Observe that the teacher and fake scores often diverge in lower-density regions (darker colors in the bottom left figure indicate larger score differences), where larger estimation errors occur. The weighting function downweights these regions\u00a0(lighter colors in the bottom right figure) during gradient updates for f\ud835\udc53fitalic_f-distill.", "description": "Figure 2 visualizes the concept of f-divergence distribution matching in a 2D setting. It displays the difference between the teacher and student scores (left) and the weighting function h (right) in forward-KL divergence. The visualization shows that significant score discrepancies occur in low-density regions of the teacher distribution (darker areas in the left panel).  Crucially, the weighting function h down-weights these low-density areas (lighter areas in the right panel) during the gradient updates. This behavior, characteristic of f-distill, reduces the influence of potentially inaccurate score estimations in low-density regions, leading to more stable training and improved performance.", "section": "3. Method: general f-divergence minimization"}, {"figure_path": "https://arxiv.org/html/2502.15681/x2.png", "caption": "(a)", "description": "The figure shows the absolute value of the derivative of the f-divergence function (f'(r)) and the weighting function (h(r)) for different f-divergences. The x-axis represents the likelihood ratio (r = p(x)/q(x)), and the y-axis represents the value of f'(r) or h(r). Different lines correspond to different f-divergences, including reverse-KL, softened RKL, Jensen-Shannon, squared Hellinger, forward-KL, and Jeffreys. The figure demonstrates the different behaviors of these f-divergences in terms of their gradient and how their weighting function affects the gradient update in the f-distill framework.", "section": "4. Comparing properties of f-divergence"}, {"figure_path": "https://arxiv.org/html/2502.15681/x3.png", "caption": "(b)", "description": "The figure shows the weighting function h(r) for different f-divergences as a function of the likelihood ratio r = p(x)/q(x). The weighting function h(r) is a key component in the proposed f-distill framework, which generalizes the distribution matching distillation approach using the f-divergence. The weighting function determines how the score differences between the teacher and student distributions are emphasized during gradient updates. The plot illustrates the trade-offs between different f-divergences in terms of mode-seeking behavior and variance.  Less mode-seeking divergences have weighting functions that emphasize high-density regions in the teacher distribution more significantly, leading to better mode coverage but potentially higher variance.", "section": "4. Comparing properties of f-divergence"}, {"figure_path": "https://arxiv.org/html/2502.15681/x4.png", "caption": "Figure 3: The absolute value of f\u2032superscript\ud835\udc53\u2032f^{\\prime}italic_f start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT\u00a0(a) and weighting function h\u2062(r)\u210e\ud835\udc5fh(r)italic_h ( italic_r )\u00a0(b) in different f\ud835\udc53fitalic_f-divergences.", "description": "Figure 3 visualizes the properties of different f-divergences used in the f-distill framework. Panel (a) displays the absolute values of the first derivative of each f-divergence's function (f\u2032), showing how sensitive the f-divergence is to changes in the likelihood ratio. Panel (b) plots the weighting function, h(r), which is derived from the second derivative of the f-divergence and determines how much weight is given to score differences based on the density ratio (r=p(x)/q(x)) during training.  By observing the differences in f\u2032 and h(r) across these divergences, one can understand their different tradeoffs between mode-seeking tendency and training stability, which have important implications for the overall model performance.", "section": "4. Comparing properties of f-divergence"}, {"figure_path": "https://arxiv.org/html/2502.15681/x5.png", "caption": "(a)", "description": "The figure shows the absolute values of the derivative of the f-divergence function (f'(r)) and the weighting function (h(r)) for different f-divergences.  The x-axis represents the likelihood ratio (r), and the y-axis represents the values of f'(r) and h(r). The plots illustrate how different f-divergences, such as reverse-KL, Jensen-Shannon, and forward-KL, have varying gradients and weighting functions, impacting the mode-seeking properties and the training stability in the distribution matching distillation approach.", "section": "4. Comparing properties of f-divergence"}, {"figure_path": "https://arxiv.org/html/2502.15681/x6.png", "caption": "(b)", "description": "The figure shows the absolute value of the derivative of the f-divergence (f'(r)) and the weighting function (h(r)) for different f-divergences as a function of the likelihood ratio (r).  The derivative f'(r) shows how sensitive the f-divergence is to changes in the likelihood ratio, influencing the magnitude of gradient updates during training. The weighting function h(r), derived from the second derivative of the f-divergence, highlights how each divergence prioritizes samples from different regions of the teacher distribution.  Specifically, it demonstrates how mode-seeking divergences, such as reverse KL, assign nearly uniform weights, while less mode-seeking divergences, like forward KL, emphasize high-density regions of the teacher distribution, making them less prone to ignoring under-represented data modes during training.", "section": "4. Comparing properties of f-divergence"}, {"figure_path": "https://arxiv.org/html/2502.15681/x7.png", "caption": "Figure 4: (a) Normalized variance versus the mean difference between two Gaussians. (b) Training losses of forward-KL w/ and w/o normalizations.", "description": "Figure 4(a) shows the relationship between the normalized variance of the weighting function and the mean difference between two 1D unit-variance Gaussian distributions. The normalized variance is calculated using the formula: Varq(f\u2033(p/q)(p/q)\u00b2)/Eq[f\u2033(p/q)(p/q)\u00b2]. This figure illustrates how the variance of the weighting function changes as the distance between the two Gaussian distributions increases. Figure 4(b) shows the training loss curves for the forward-KL divergence with and without the normalization techniques proposed in the paper. The figure demonstrates the effectiveness of the normalization techniques in reducing the variance of the gradients during training, which improves the stability of the training process.", "section": "4. Comparing properties of f-divergence"}, {"figure_path": "https://arxiv.org/html/2502.15681/x8.png", "caption": "(a)", "description": "The figure shows the absolute value of the derivative of the f-divergence function (f'(r)) and the weighting function (h(r)) for different f-divergences.  The x-axis represents the likelihood ratio (r = p(x)/q(x)), where p(x) is the teacher distribution and q(x) is the student distribution. The y-axis represents the values of f'(r) and h(r). Different lines represent different f-divergences (reverse-KL, softened RKL, Jensen-Shannon, squared Hellinger, forward-KL, Jefferys). The plot visually demonstrates how the weighting function h(r), derived from the second derivative of the f-divergence, varies across different divergences, which is crucial in determining the behavior of the gradient update in f-distill (f-divergence distribution matching).  This variation highlights the different trade-offs in mode-seeking behavior and variance between various divergences used in the algorithm.", "section": "4. Comparing properties of f-divergence"}, {"figure_path": "https://arxiv.org/html/2502.15681/extracted/6217836/img/vis_2.png", "caption": "(b)", "description": "The weighting function h(r) for different f-divergences plotted against the likelihood ratio r.  The graph shows how the weight assigned to the score difference between the teacher and student models varies depending on the density ratio.  The weighting function plays a key role in how f-distill balances mode coverage and training stability, with different f-divergences exhibiting different behaviors. The plot helps illustrate the trade-offs between mode-seeking and variance in different f-divergences.", "section": "4. Comparing properties of f-divergence"}, {"figure_path": "https://arxiv.org/html/2502.15681/x9.png", "caption": "(c)", "description": "The figure shows a comparison of the weighting function, h(r), across different f-divergences. The weighting function is a crucial component in the f-distill framework, as it determines how much weight is assigned to different regions in the distribution based on the density ratio between the teacher and student.  The figure visually demonstrates the different properties of each f-divergence, highlighting the impact on training stability and mode coverage.  By understanding the behavior of the weighting function, one can better select an appropriate f-divergence for distribution matching in diffusion models.", "section": "Comparing properties of f-divergence"}, {"figure_path": "https://arxiv.org/html/2502.15681/x10.png", "caption": "Figure 5: Normalized weighting function h\u210ehitalic_h and score difference versus the index of 6.4k generated samples, sorted by the score difference, on (a) CIFAR-10 with forward-KL; (b) ImageNet-64 with JS; (c) SDv1.5 with JS.", "description": "This figure visualizes the relationship between the normalized weighting function (h) and the score difference for 6,400 generated samples. The samples are sorted by their score difference.  The score difference is calculated between the teacher's score and the student's (fake) score. Three subplots are shown, each representing a different dataset and f-divergence: (a) CIFAR-10 with forward-KL divergence; (b) ImageNet-64 with Jensen-Shannon (JS) divergence; and (c) Stable Diffusion v1.5 with JS divergence.  The weighting function (h) is derived from the chosen f-divergence and is designed to emphasize samples with higher density in the teacher distribution when a less mode-seeking divergence is used, thus influencing the impact of score differences during training.", "section": "4. Comparing properties of f-divergence"}, {"figure_path": "https://arxiv.org/html/2502.15681/extracted/6217836/img/teaser_4.png", "caption": "Figure 6: (a) Uncurated generated samples by the multi-step teacher diffusion models (top), and one-step student in f\ud835\udc53fitalic_f-distill (bottom), using the same random seed. The teacher diffusion models use 35 and 50 steps on ImageNet-64 and Stable Diffusion v1.5, respectively. (b) Generated samples by reverse-KL and JS, using a prompt in COYO: \u201ca blue and white passenger train coming to a stop\u201d.", "description": "Figure 6 demonstrates the image generation capabilities of the proposed f-distill method compared to a multi-step teacher model.  Part (a) shows uncurated samples generated by the teacher model (using 35 steps for ImageNet64 and 50 steps for Stable Diffusion v1.5) and the one-step f-distill student model, both using the same random seed. This showcases the ability of f-distill to significantly reduce the number of sampling steps while maintaining comparable image quality. Part (b) presents a comparison of image generation using two different f-divergences within the f-distill framework (reverse-KL and Jensen-Shannon) for the same text prompt: \u201ca blue and white passenger train coming to a stop\u201d. This illustrates the impact of f-divergence choice on the resulting image generation.", "section": "5.1 Image generation"}, {"figure_path": "https://arxiv.org/html/2502.15681/x11.png", "caption": "(a)", "description": "The figure shows the absolute value of the derivative of the f-divergence function (f'(r)) and the weighting function (h(r)) for different f-divergences. The x-axis represents the likelihood ratio (r = p(x)/q(x)), where p(x) is the probability density of the teacher model and q(x) is the probability density of the student model. The y-axis represents the value of f'(r) or h(r). The plots illustrate the different properties of these functions and how they vary across different choices of f-divergence.  Understanding these properties is important for selecting the most appropriate f-divergence for distribution matching in diffusion models. The weighting function emphasizes samples with higher densities in the teacher distribution when using a less mode-seeking divergence.", "section": "4. Comparing properties of f-divergence"}, {"figure_path": "https://arxiv.org/html/2502.15681/extracted/6217836/img/cp1.png", "caption": "(b)", "description": "The figure shows the weighting function h(r) for different f-divergences as a function of the likelihood ratio r. The weighting function is a key component of the f-distill framework, which uses f-divergences to match the distributions of the teacher and student models. The weighting function h(r) emphasizes samples with higher density in the teacher distribution, leading to better mode coverage. The plot shows that the weighting function h(r) varies significantly across different f-divergences, with some divergences showing a higher weight in high-density regions and others in low-density regions. This variation in the weighting function h(r) is crucial for achieving the state-of-the-art one-step generation performance in the f-distill framework.", "section": "Comparing properties of f-divergence"}, {"figure_path": "https://arxiv.org/html/2502.15681/extracted/6217836/img/cp2.png", "caption": "Figure 7: FID score versus training iteration on CIFAR-10. Fake score feature: fake score as the extractor, updating both the fake score and classification head in the GAN discriminator loss. Teacher score feature: teacher score as the extractor, updating classification head in the GAN discriminator loss. Teacher score feature, updating cls head: fake score as the extractor, updating classification head in the GAN discriminator loss. (a) is the zoomed-in visualization of (b).", "description": "This figure compares the training performance of different variations of the f-distill method on the CIFAR-10 dataset.  Three experimental setups are compared: using the fake score network's output to update both the fake score and the classifier in the GAN discriminator (Fake score feature), using the teacher score network's output to update only the classifier (Teacher score feature), and using the fake score network's output but only updating the classifier (Teacher score feature, updating cls head).  The plot shows FID score (lower is better) over training iterations.  The second plot (b) shows the entire training run, while the first plot (a) is a zoomed-in view of the more interesting part of the training.", "section": "4. Comparing properties of f-divergence"}, {"figure_path": "https://arxiv.org/html/2502.15681/extracted/6217836/img/cp3.png", "caption": "Figure 8: Illustration of how the weighting function h\u210ehitalic_h\u00a0(red dotted line) in less mode-seeking divergence\u00a0(forward-KL, h=p/q\u210e\ud835\udc5d\ud835\udc5eh=p/qitalic_h = italic_p / italic_q) helps to learn the true data distribution p\ud835\udc5dpitalic_p, compared to more mode-seeking divergence\u00a0(reverse-KL, h\u22611\u210e1h\\equiv 1italic_h \u2261 1). We illustrate how using a less mode-seeking divergence can better capture different modes, from a skewed initial generative distribution q\ud835\udc5eqitalic_q, with the help of the weighting function.", "description": "Figure 8 illustrates the effect of the weighting function  \u210eh in f-divergence on the learning process of generative models.  The figure compares two scenarios: one using a less mode-seeking divergence (forward KL divergence) where the weighting function \u210eh is proportional to the ratio of the teacher and student probability densities (\u210e=\ud835\udc5d/\ud835\udc5eh=p/q), and another using a more mode-seeking divergence (reverse KL divergence) where \u210e=1h=1. The plots show that with a less mode-seeking divergence and its corresponding weighting function, the model can better learn the true data distribution by capturing various modes, even when starting from a skewed initial generative distribution. This is achieved because the weighting function down-weights samples from low-density regions and thus prevents the model from overemphasizing high density regions, unlike the mode-seeking divergence.", "section": "4. Comparing properties of f-divergence"}, {"figure_path": "https://arxiv.org/html/2502.15681/extracted/6217836/img/cp4.png", "caption": "Figure 9: Training dynamics of JS and forward-KL on COYO-700M.", "description": "This figure compares the training loss curves for the Jensen-Shannon (JS) divergence and the forward Kullback-Leibler (forward-KL) divergence during the training process of the f-distill model on the COYO-700M dataset. The x-axis represents training iterations, and the y-axis represents the value of the loss function. The plot shows the training loss for JS divergence and the training loss for forward-KL divergence. By comparing the loss curves of these two methods, we can understand the effect of different choices of f-divergences on training stability, and whether one method provides better stability than the other during model training.", "section": "4. Comparing properties of f-divergence"}, {"figure_path": "https://arxiv.org/html/2502.15681/extracted/6217836/img/cp5.png", "caption": "Figure 10: Generated samples from multi-step teachers and single-step students, using the same prompts and random seeds. The real data used for GAN objective are from COYO-700M.", "description": "This figure compares image generation results from a multi-step teacher diffusion model and a single-step student model trained using the proposed f-distill method.  The same prompts and random seeds were used for both models, ensuring a fair comparison.  The images showcase the ability of the single-step student model to generate images comparable to the multi-step teacher model, highlighting the efficiency gains of f-distill.  Real data from the COYO-700M dataset was used to train the GAN component in f-distill.", "section": "5.1 Image generation"}, {"figure_path": "https://arxiv.org/html/2502.15681/extracted/6217836/img/cp6.png", "caption": "Figure 11: Generated samples from multi-step teachers and single-step students, using the same prompts and random seeds. The real data used for GAN objective are from COYO-700M.", "description": "This figure compares image generation results from a multi-step diffusion model (teacher) and a single-step diffusion model (student) trained using the f-distill method.  Both models were given the same prompts and random seeds, ensuring a fair comparison. The COYO-700M dataset was used as real data for the GAN (Generative Adversarial Network) objective in the f-distill training process. The images showcase the visual quality and diversity achievable by each model, illustrating the effectiveness of the f-distill method in distilling the knowledge of the complex multi-step diffusion model into a simplified one-step model.", "section": "5. Experiment"}, {"figure_path": "https://arxiv.org/html/2502.15681/extracted/6217836/img/cifar10_edm.png", "caption": "Figure 12: Generated samples from multi-step teachers and single-step students, using the same prompts and random seeds. The real data used for GAN objective are from COYO-700M.", "description": "This figure visualizes the results of image generation using both multi-step teacher diffusion models and single-step student models trained with the proposed f-distill method.  The same prompts and random seeds are used for both types of models to ensure a fair comparison. The images show the generation quality from both models side-by-side for two different scenarios: a border collie surfing and a macro shot of a ladybug. The COYO-700M dataset is used as real data for training the GAN objective in the f-distill model.", "section": "5. Experiment"}, {"figure_path": "https://arxiv.org/html/2502.15681/extracted/6217836/img/cifar10_kl.png", "caption": "Figure 13: 35-step generated CIFAR-10 samples, by EDM\u00a0[17]\u00a0(teacher). FID score: 1.79", "description": "This figure displays 35 images generated from a 35-step diffusion model (EDM [17]) trained on the CIFAR-10 dataset.  These images represent a sample of the model's output, showcasing its ability to generate diverse and realistic images of objects from the CIFAR-10 classes. The Fr\u00e9chet Inception Distance (FID) score, a metric used to evaluate the quality of generated images by comparing them to real images, is reported as 1.79, indicating high image quality. A lower FID score indicates better performance.", "section": "5.1. Image generation"}, {"figure_path": "https://arxiv.org/html/2502.15681/extracted/6217836/img/imagenet_64_edm.png", "caption": "Figure 14: One-step generated CIFAR-10 samples, by KL in f\ud835\udc53fitalic_f-distill. FID score: 1.92", "description": "This figure displays images generated by a one-step diffusion model, specifically using the KL divergence method within the f-distill framework. The model generated these CIFAR-10 images in a single step, demonstrating the efficiency of this approach.  The FID (Fr\u00e9chet Inception Distance) score of 1.92 indicates the quality of the generated images relative to real CIFAR-10 images; a lower FID score suggests higher image quality and similarity to real data.", "section": "5.1 Image generation"}, {"figure_path": "https://arxiv.org/html/2502.15681/extracted/6217836/img/imagenet_64_js.png", "caption": "Figure 15: 79-step generated ImageNet-64 samples, by EDM\u00a0[17]\u00a0(teacher). FID score: 2.35", "description": "This figure displays 79 samples generated from the ImageNet-64 dataset using the EDM (Energy-based Diffusion Model) method, which serves as the teacher model in this study.  Each image is 64x64 pixels. The FID (Fr\u00e9chet Inception Distance) score, a metric for evaluating the quality of generated images by comparing them to real images, is 2.35. A lower FID score indicates that the generated images are more realistic and similar to real images from the dataset.", "section": "5.1 Image generation"}, {"figure_path": "https://arxiv.org/html/2502.15681/extracted/6217836/img/sd_teacher.png", "caption": "Figure 16: One-step generated ImageNet-64 samples, by JS in f\ud835\udc53fitalic_f-distill. FID score: 1.16", "description": "This figure displays images generated by a one-step diffusion model using the Jensen-Shannon (JS) divergence in the f-distill framework.  The images are of size 64x64 and belong to the ImageNet-64 dataset.  The FID score, a metric assessing the quality of generated images by comparing them to real images, is 1.16, indicating high-quality generation.", "section": "5.1. Image generation"}, {"figure_path": "https://arxiv.org/html/2502.15681/extracted/6217836/img/sd_js.png", "caption": "Figure 17: 50-step generated SD v1.5 samples, using randomly sampled COYO-700M prompts, by SD v1.5 model\u00a0[43]\u00a0(teacher). CFG=3. FID score: 8.59", "description": "This figure displays 16 images generated by a 50-step Stable Diffusion v1.5 model using prompts from the COYO-700M dataset.  The model was run with a classifier-free guidance (CFG) scale of 3.  The Fr\u00e9chet Inception Distance (FID) score, a metric evaluating the quality of generated images, was 8.59. The images showcase the model's ability to generate diverse and realistic images based on text prompts.", "section": "5.1 Image generation"}, {"figure_path": "https://arxiv.org/html/2502.15681/extracted/6217836/img/step_36000.png", "caption": "Figure 18: One-step generated SD v1.5 samples, using randomly sampled COYO-700M prompts, by JS in f\ud835\udc53fitalic_f-distill, with CFG=1.75. FID score: 7.42", "description": "This figure displays images generated using a single-step method (f-distill with JS divergence) on the Stable Diffusion v1.5 model.  The prompts used were randomly selected from the COYO-700M dataset.  The FID (Fr\u00e9chet Inception Distance) score, a common metric for evaluating the quality of generated images, is 7.42, indicating the quality of these generated images.", "section": "5.1. Image generation"}]