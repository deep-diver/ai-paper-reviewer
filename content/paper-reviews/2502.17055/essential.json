{"importance": "This paper introduces Stable-SPAM, stabilizing 4-bit LLM training. **It outperforms Adam and SPAM, reducing training steps and improving performance.** It is crucial for researchers aiming for efficient, low-precision LLM training, opening avenues for exploration of stabilized optimization.", "summary": "Stable-SPAM stabilizes 4-bit LLM training, outperforming Adam.", "takeaways": ["Low-bit precision amplifies sensitivity to learning rates.", "Stable-SPAM effectively stabilizes gradient norms in 4-bit LLM training.", "Stable-SPAM outperforms Adam and SPAM in 4-bit LLM training, achieving better performance with fewer training steps."], "tldr": "**Low-bit training is a key area for efficient Large Language Models (LLMs), yet it suffers from unstable gradient norms and high sensitivity to learning rates, causing divergence.** Existing optimizers like SPAM, while improving performance, still struggle with gradient stabilization, requiring careful tuning, especially in 4-bit training, limiting practical applications and widespread adoption.\n\nThis paper introduces Stable-SPAM, an optimizer that enhances gradient normalization and clipping. Stable-SPAM adaptively updates clipping thresholds, normalizes gradients based on historical statistics, and inherits momentum reset from SPAM. **The experiments show the effectiveness of Stable-SPAM in stabilizing gradient norms, delivering superior performance, and reducing training steps in 4-bit LLM training.**", "affiliation": "University of Exeter", "categories": {"main_category": "Machine Learning", "sub_category": "Deep Learning"}, "podcast_path": "2502.17055/podcast.wav"}