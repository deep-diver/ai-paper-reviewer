[{"figure_path": "https://arxiv.org/html/2503.09641/x1.png", "caption": "Figure 1: (a) Our SANA-Sprint accelerate the inference speed for generating 1024 \u00d7\\times\u00d7 1024 images, achieving a remarkable speedup from FULX-Schnell\u2019s 1.94 seconds to only 0.03 seconds. This represents a 64\u00d7\\times\u00d7 improvement over the current state-of-the-art step-distilled model, FLUX-Schnell, as measured with a batch size of 1 on an NVIDIA A100 GPU. The ratio is calculated based on Transformer latency.\n(b) Additionally, our model demonstrates efficient GPU memory usage during training, outperforming other distillation methods in terms of memory cost. The GPU memory is measured using official code, 1024 \u00d7\\times\u00d7 1024 images and on a single A100 GPU.", "description": "Figure 1 presents a comparison of SANA-Sprint's performance against existing state-of-the-art models in terms of inference speed and GPU memory usage.  The left panel (a) shows a dramatic speed improvement by SANA-Sprint, reducing the time to generate 1024x1024 images from 1.94 seconds (Flux-Schnell) to a mere 0.03 seconds \u2013 a 64x speedup. This improvement is calculated based on the time taken by the transformer components of the models, using a batch size of 1 on an NVIDIA A100 GPU. The right panel (b) demonstrates SANA-Sprint's efficiency in terms of GPU memory usage during training, outperforming other distillation methods. The memory usage was measured using the official code and a single A100 GPU with 1024x1024 images.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.09641/x2.png", "caption": "Figure 2: \nTraining paradigm of SANA-Sprint. In SANA-Sprint, we use the student model for synthetic data generation\u00a0(x0^^subscript\ud835\udc650\\hat{x_{0}}over^ start_ARG italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_ARG) and JVP calculation, and we use the teacher model for velocity\u00a0(d\u2062x/d\u2062td\ud835\udc65d\ud835\udc61\\mathrm{d}x/\\mathrm{d}troman_d italic_x / roman_d italic_t) compute and its feature for the GAN loss, which allows us train sCM and GAN together and have only one training model purely in the latent space. Details of training objective and TrigFlow Transformation are in Eq.\u00a09, Eq.\u00a011 and Sec.\u00a03.1.", "description": "The figure illustrates the training process of the SANA-Sprint model.  It uses a hybrid approach, combining continuous-time consistency model (sCM) training with Generative Adversarial Network (GAN) training. The student model is responsible for generating synthetic data and computing Jacobian-vector products (JVPs), while the teacher model provides velocity information (dx/dt) and features for the GAN loss function. This setup allows for simultaneous training of both sCM and GAN, simplifying the process and reducing computational overhead. Importantly, all training occurs solely in the latent space. Details regarding the training objective, the TrigFlow transformation (used to adapt a pre-trained model for sCM distillation), and specific equations are available in the paper\u2019s Section 3.1 and Equations 9 and 11.", "section": "Method"}, {"figure_path": "https://arxiv.org/html/2503.09641/x3.png", "caption": "Figure 3: \nEfficient Distillation via QK Normalization, Dense Timestep Embedding, and Training-free Schedule Transformation.\n(a) We compare gradient norms and visualizations with/without QK Normalization, showing its stabilizing effect.\n(b) Gradient norm curves for timestep scales (0\u223csimilar-to\\sim\u223c1 vs. 0\u223csimilar-to\\sim\u223c1000) highlight impacts on stability and stability and quality.\n(c) PCA-based similarity analysis of timestep embeddings.\n(d) Image results after 5,000 iterations of fine-tuning with (left) and without (right) the proposed schedule transfer (Sec.\u00a03.1).", "description": "This figure demonstrates the effectiveness of three key improvements to the SANA-Sprint model's training process: QK normalization, dense timestep embeddings, and training-free schedule transformation.  Panel (a) shows a comparison of training gradient norms with and without QK normalization, illustrating the stabilizing impact. Panel (b) presents gradient norm curves comparing the use of two different timestep ranges (0-1 and 0-1000), highlighting how these impact stability and image quality.  Panel (c) displays a PCA-based similarity analysis of the timestep embeddings, providing visual evidence of their relationships. Finally, Panel (d) displays image results after 5000 training iterations to showcase the effects of the proposed training-free schedule transformation, with images generated with (left) and without (right) this transformation.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.09641/x4.png", "caption": "Figure 4: Visual comparison among SANA-Sprint and selected competing methods in different inference steps. \u2020\u00a0indicates that distinct models are required for different inference steps, and time below the method name is the latency of 4 steps tested on A100 GPU. SANA-Sprint produces images with superior realism and text alignment in all inference steps with the fastest speed.", "description": "Figure 4 presents a comparison of image generation results from SANA-Sprint and other state-of-the-art diffusion models at varying numbers of inference steps (1, 2, and 4).  The models are tested on the task of generating images from text prompts. The figure visually demonstrates the image quality generated by each method.  It highlights that SANA-Sprint achieves superior realism and better text alignment compared to other methods across all inference step counts.  Importantly, SANA-Sprint achieves this superior performance while also being significantly faster.  The latency values shown beneath the method names represent the total inference time needed for 4 steps on an NVIDIA A100 GPU. The symbol \u2020 indicates that some models require separate training for each inference step count, unlike SANA-Sprint, which uses a single unified model across all step counts.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.09641/x5.png", "caption": "Table 3: Comparison of loss combination.", "description": "This table presents the results of an ablation study comparing different loss function combinations used in training the SANA-Sprint model.  It shows how combining the continuous-time consistency model (sCM) loss with the latent adversarial distillation (LADD) loss affects the model's performance, as measured by FID and CLIP scores.  The table allows researchers to understand the individual contributions of each loss component and their optimal balance for effective training.", "section": "3.3 Improving Continuous-Time CMs with GAN"}, {"figure_path": "https://arxiv.org/html/2503.09641/x6.png", "caption": "Table 4: Comparison of CFG training strategies.", "description": "This table presents a comparison of different Classifier-Free Guidance (CFG) training strategies used in the SANA-Sprint model.  It shows how the choice of CFG embedding strategy (with or without embedding) affects the final model's performance, as measured by FID (Fr\u00e9chet Inception Distance) and CLIP score (a measure of text-image alignment). This helps to understand the impact of different training approaches on the model's ability to generate images that accurately reflect the user's textual prompts.", "section": "3.2 Stabilizing Continuous-Time Distillation"}, {"figure_path": "https://arxiv.org/html/2503.09641/x7.png", "caption": "Table 5: sCM and LADD loss weighting.", "description": "This table presents an ablation study comparing different weightings of the continuous-time consistency model (sCM) loss and the latent adversarial distillation (LADD) loss. It shows how varying the balance between these two loss functions affects the final model performance, measured by FID and CLIP scores.", "section": "3.3 Improving Continuous-Time CMs with GAN"}, {"figure_path": "https://arxiv.org/html/2503.09641/x8.png", "caption": "Table 6: Comparison of max-time weighting strategy.", "description": "This table presents an ablation study on the effect of different max-time weighting strategies on the performance of the SANA-Sprint model.  The max-time weighting strategy modifies the training process by randomly selecting a timestep at the maximum value (t = \u03c0/2) with a certain probability, while the rest of the timesteps are sampled according to the model's original distribution. The table compares the FID and CLIP scores achieved with different max-time probabilities (0%, 50%, 70%), showing how this approach impacts the overall quality of the generated images.", "section": "3.3 Improving Continuous-Time CMs with GAN"}, {"figure_path": "https://arxiv.org/html/2503.09641/x9.png", "caption": "Figure 5: Visual comparison among SANA-Sprint with different inference steps and the teacher model SANA. SANA-Sprint can generate high-quality images with one or two steps and the images can be better when increasing steps.", "description": "This figure compares image generation results of SANA-Sprint with varying numbers of inference steps (1, 2, and 4) against its 20-step teacher model, SANA.  It visually demonstrates that SANA-Sprint can produce high-quality images even with a significantly reduced number of steps, showcasing the effectiveness of the proposed one-step diffusion method.  The improvement in image quality is noticeable as the number of steps increases from one to four, while still maintaining superior speed compared to the teacher model.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.09641/x10.png", "caption": "Figure 6: \nInference timesteps search. This figure illustrates the performance of timesteps search for achieving optimal results during inference with 0.6B and 1.6B models. The subplots compare FID\u00a0(top row) and CLIP-Score\u00a0(bottom row) across different timesteps for 1-step, 2-step, and 4-step inference settings. The x-axis represents the timestep being searched at the current step; for multi-step settings (e.g., 4 steps), the timesteps for earlier steps are fixed to their previously optimized values.", "description": "This figure visualizes the process of finding the optimal timestep settings for inference with SANA-Sprint models of sizes 0.6B and 1.6B parameters.  It explores the impact of timestep choices on the FID (Frechet Inception Distance) and CLIP (Contrastive Language-Image Pre-training) scores for different inference scenarios (1-step, 2-step, and 4-step).  Each subplot displays the FID and CLIP scores obtained for various timesteps.  For multi-step inference, previously optimized timesteps from earlier steps are retained, making the search more efficient and focused.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.09641/x11.png", "caption": "Figure 7: Controlling the sCM noise distribution. This figure compares FID and CLIP-Score across different noise distribution settings over 40k training steps in sCM-only experiments. The green curve (Pmean,Pstd)=(0.0,1.6)subscript\ud835\udc43meansubscript\ud835\udc43std0.01.6(P_{\\text{mean}},P_{\\text{std}})=(0.0,1.6)( italic_P start_POSTSUBSCRIPT mean end_POSTSUBSCRIPT , italic_P start_POSTSUBSCRIPT std end_POSTSUBSCRIPT ) = ( 0.0 , 1.6 ) demonstrates optimal performance, achieving stable training dynamics and superior generation quality.", "description": "The figure displays the impact of various noise distribution configurations on the performance of the continuous-time consistency model (sCM) during training.  The x-axis represents the training steps (up to 40,000), and the y-axis shows the FID (Fr\u00e9chet Inception Distance) and CLIP (Contrastive Language-Image Pre-training) scores, which are used to evaluate image quality and similarity to text prompts respectively. Multiple lines depict the results for different configurations of the mean (Pmean) and standard deviation (Pstd) of the noise distribution. The plot reveals that the configuration with Pmean=0.0 and Pstd=1.6 (green line) yields the best results, achieving both low FID (indicating high-quality images) and high CLIP scores (showing good alignment with text prompts) while maintaining stable training dynamics. Other configurations exhibit either instability during training or suboptimal performance.", "section": "3.2. Stabilizing Continuous-Time Distillation"}]