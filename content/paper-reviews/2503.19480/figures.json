[{"figure_path": "https://arxiv.org/html/2503.19480/x2.png", "caption": "Figure 1: Perfect generation (reconstruction) does not always yield desirable visual representations. (a) Pipeline of fine-grained visual enhancements, where generative models take visual tokens as conditions and perform reconstruction. (b) Experiments across four dimensions, i.e., training iterations, denoiser size, ratio of local tokens as conditions, and whether to use pre-trained denoisers. We measure generation (CLIP score \u2191\u2191\\uparrow\u2191) and visual representations (MMVP-VLM \u2191\u2191\\uparrow\u2191) performance. As the results demonstrate, although increasing the number of training iterations, adding more denoiser blocks, using a larger ratio of local tokens as conditions, and employing pre-trained denoisers lead to better generation results, the performance of visual representations does not always improve. Best viewed zoomed in.", "description": "Figure 1 explores the relationship between perfect image generation and the quality of resulting visual representations.  Panel (a) illustrates the method: a generative model reconstructs an image conditioned on visual tokens.  Panel (b) shows experiments varying four factors: training iterations, denoiser size, the proportion of local tokens used for conditioning, and whether a pre-trained denoiser was used.  The evaluation metrics were CLIP score (higher is better, indicating better generation) and MMVP-VLM score (higher is better, indicating better visual representation). The results demonstrate that while increasing training iterations, adding denoiser blocks, using more local tokens, and using pre-trained denoisers improve generation quality (CLIP score), this does not guarantee improved visual representation quality (MMVP-VLM score).  In other words, perfect image reconstruction does not always lead to better visual features.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.19480/x3.png", "caption": "Figure 2: Comparison with prior method\u00a0[46]. (a) We only need a lightweight denoiser, but (b) achieve stronger performance than DIVA\u00a0[46], which relies on pre-trained heavy generative models.", "description": "Figure 2 demonstrates the efficiency and effectiveness of the proposed GenHancer method compared to the existing DIVA [46] method.  Subfigure (a) highlights that GenHancer only requires a lightweight denoiser, unlike DIVA which relies on heavy, pre-trained generative models.  Subfigure (b) shows that despite its simplicity, GenHancer achieves superior performance in terms of visual representation quality, as measured by the MMVP-VLM benchmark.", "section": "2. Related Works"}, {"figure_path": "https://arxiv.org/html/2503.19480/x4.png", "caption": "Figure 3: The two-stage post-training framework for visual enhancements. (a) Overall training pipeline. (b) Continuous generative model as the denoiser. We employ a lightweight FLUX-like DiT\u00a0[22] (but with fewer blocks) and employ a regression loss of flow matching. (c) Discrete generative model as the denoiser. We choose a lightweight Perceiver\u00a0[17] and employ cross-entropy loss to predict masked tokens.", "description": "Figure 3 illustrates the two-stage training process used to enhance visual representations.  Panel (a) shows the overall pipeline, starting with noisy input and resulting in enhanced representations. Panel (b) details the continuous generative model (denoiser) which uses a lightweight FLUX-like diffusion transformer (DiT) with fewer blocks than the original, minimizing computational cost.  A regression loss based on flow matching is used for training this denoiser.  Panel (c) presents the discrete generative model (denoiser) which uses a lightweight Perceiver architecture. This model employs a cross-entropy loss function to predict masked tokens during training.", "section": "4. Method"}, {"figure_path": "https://arxiv.org/html/2503.19480/x5.png", "caption": "Figure 4: Qualitative results. Although DIVA achieves better reconstructions of input images, it fails to perceive fine-grained visual details between \u2018tongue out\u2019 and \u2018without tongue out\u2019.", "description": "Figure 4 presents a qualitative comparison of image reconstruction and captioning results between the proposed GenHancer method and the DIVA method.  Both methods aim to improve CLIP's fine-grained visual understanding. While DIVA achieves visually superior image reconstructions, it fails to accurately differentiate between subtle visual details, such as a minion's tongue being out or not. GenHancer, conversely, shows better performance in distinguishing these fine-grained differences, highlighting its ability to enhance CLIP's capacity for precise visual comprehension.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.19480/x6.png", "caption": "Figure 5: Performance of CLIP across various conditional visual tokens on MMVP-VLM, i.e., [CLS] + n%percent\ud835\udc5bn\\%italic_n % [LOCAL].", "description": "This figure displays the performance of CLIP (Contrastive Language\u2013Image Pre-training) models on the MMVP-VLM (Multimodal Visual Perception Visual-Linguistic Benchmark) when different combinations of visual tokens are used as input. Specifically, it shows how the model's performance changes when using the [CLS] token (representing the class or global image information) in conjunction with varying percentages (0%, 10%, 20%, 50%, 80%, 100%) of [LOCAL] tokens (representing local image features). The x-axis indicates the percentage of local tokens included, and the y-axis represents the MMVP-VLM score, which measures the model's performance in terms of fine-grained visual understanding.  Separate bars indicate results for both continuous and discrete generative models.", "section": "5.3 Key Explorations and Ablations"}, {"figure_path": "https://arxiv.org/html/2503.19480/x7.png", "caption": "Figure 6: Comparison of CLIP with end-to-end and the proposed two-stage training on MMVP-VLM. Here, Cont. and Disc. denote continuous and discrete denoisers. O: OpenAICLIP. S: SigLIP.", "description": "This figure compares the performance of CLIP (Contrastive Language-Image Pre-training) models trained using two different methods: end-to-end training and a proposed two-stage training approach.  The comparison is done on the MMVP-VLM benchmark, which evaluates fine-grained visual perception abilities. The results are shown separately for models using continuous and discrete generative denoisers.  The abbreviations O and S represent OpenAICLIP and SigLIP, respectively, indicating different CLIP model architectures.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.19480/x8.png", "caption": "Figure 7: Probability density function of different distributions.", "description": "This figure displays the probability density functions for several different timestamp sampling distributions used in the continuous generative model.  These distributions control how frequently different timestamps are sampled during the training process.  The figure shows the distributions for uniform sampling and several variations of scaled Logit-Normal sampling, with different scale parameters (s = 0.1, 0.5, 1, 5, 10).  The Logit-Normal distributions allow for more focused sampling around the midpoint (t=0.5) of the interval, as opposed to uniform sampling which gives equal weight to all timestamps.", "section": "4.3 Denoising Configurations"}, {"figure_path": "https://arxiv.org/html/2503.19480/x9.png", "caption": "Figure 8: The effect of LoRA on several CLIP backbones.", "description": "This figure compares the performance of CLIP models enhanced using LoRA (Low-Rank Adaptation) with those trained without LoRA.  It showcases the results across various CLIP backbones (OpenAI, MetaCLIP). The graph likely illustrates that employing LoRA improves the fine-grained visual representation learning of the CLIP model by preventing overfitting during the training process, thus achieving better performance compared to training without LoRA.", "section": "5.3. Key Explorations and Ablations"}, {"figure_path": "https://arxiv.org/html/2503.19480/x10.png", "caption": "Figure 9: The performance of whether to update the denoiser and the projector in Stage-2.", "description": "This figure compares the performance of updating only the visual encoder (CLIP ViT) versus updating both the visual encoder and the generative model components (denoiser and projector) during the second stage of training.  The results show that only updating the visual encoder yields better performance on the MMVP-VLM benchmark, suggesting that additional updates to the generative model components are not beneficial in this stage.", "section": "5.3 Key Explorations and Ablations"}, {"figure_path": "https://arxiv.org/html/2503.19480/x11.png", "caption": "Figure 10: Qualitative results of CLIP on MMVP-VLM benchmark. The enhanced CLIP overcomes original visual shortcomings in fine-grained details.", "description": "Figure 10 presents a qualitative comparison of the original CLIP model and the enhanced CLIP model's performance on the MMVP-VLM benchmark.  The images showcase several examples where the original CLIP model struggles with fine-grained visual details, such as color, quantity, and orientation. The enhanced CLIP model, however, demonstrates a significantly improved ability to perceive and correctly identify these fine-grained details, overcoming the visual shortcomings of the original model. This improvement highlights the effectiveness of the proposed method in enhancing the visual understanding capabilities of CLIP.", "section": "5. Experiments"}]