[{"figure_path": "https://arxiv.org/html/2503.20757/x3.png", "caption": "Figure 1: \nAn illustration of MCTS-RAG workflow for answering the question sampled from ComplexWebQA.", "description": "This figure illustrates the step-by-step process of the MCTS-RAG framework in answering a complex question from the ComplexWebQA dataset. It showcases how MCTS-RAG dynamically integrates retrieval and reasoning through an iterative decision-making process. The figure visually depicts the different actions within the MCTS-RAG workflow, including direct answering, quick reasoning, question decomposition, retrieval reasoning, and answer summarization.  Each step involves refining both retrieval and reasoning, ultimately converging to a final answer.  The example question is displayed, along with the decision tree and intermediate steps taken.", "section": "3 MCTS-RAG"}, {"figure_path": "https://arxiv.org/html/2503.20757/x4.png", "caption": "Figure 2: \nAn illustration of MCTS-RAG retrieval process (i.e., R1-R4) within one step of the retrieval decomposition action highlighted in Figure\u00a02.", "description": "This figure illustrates the four steps involved in MCTS-RAG's adaptive retrieval process: (R1) Query Generation, where the model identifies a knowledge gap and formulates a query; (R2) Query Execution, where external knowledge sources are consulted; (R3) Knowledge Reflection, where the retrieved information's relevance and consistency are evaluated; and (R4) Summary Reasoning, where the retrieved information is integrated into the model's reasoning process to answer a sub-question. This adaptive process is shown within a single step of the \"retrieval decomposition\" action (detailed in Figure 1), emphasizing the dynamic interaction between reasoning and information retrieval in MCTS-RAG.", "section": "3 MCTS-RAG"}, {"figure_path": "https://arxiv.org/html/2503.20757/x5.png", "caption": "Figure 3: \nAn illustration of MCTS Amplification Error. Early MCTS retrieval errors amplify mistakes, leading to a final answer favoring incorrect paths.", "description": "The figure illustrates how errors in the early stages of the Monte Carlo Tree Search (MCTS) process can be amplified as the search progresses.  Minor inaccuracies in initial retrievals compound through subsequent iterations, leading to a final answer that strongly favors incorrect reasoning paths, even if more accurate options exist.", "section": "4.4 Human Analysis and Case Study"}, {"figure_path": "https://arxiv.org/html/2503.20757/x6.png", "caption": "Figure 4: \nAn illustration of Factual Confusion. Wrong understanding of the relationship between project launch and moon landing, leading to wrong answers.", "description": "This figure illustrates a case where the model incorrectly identifies John F. Kennedy as the U.S. president during the moon landing due to a misunderstanding of the timeline.  The model confuses the setting of the goal to land on the moon (Kennedy's presidency) with the actual event of the landing (Nixon's presidency). This showcases how a flawed understanding of the relationship between the project's initiation and its completion leads to factually inaccurate results.", "section": "4.4 Human Analysis and Case Study"}, {"figure_path": "https://arxiv.org/html/2503.20757/x7.png", "caption": "Figure 5: \nAn illustration of Information Overload. Too much coastline information, resulting in the model answering the coastline length instead of the capital city.", "description": "Figure 5 demonstrates a failure case in the MCTS-RAG model due to information overload.  The question is simple: what is the capital of the country with the longest coastline? The model correctly retrieves information identifying Canada as having the longest coastline, but focuses excessively on the length of the coastline (202,080 km) to the point of answering that length instead of the actual capital city, Ottawa.  This highlights the challenge of managing information retrieval in complex reasoning tasks where an abundance of relevant but not directly answer-related information can lead to incorrect responses.  The model's reasoning process is disrupted by the excessive detail surrounding the coastline length, overshadowing the core question of the capital city.", "section": "4.4 Human Analysis and Case Study"}, {"figure_path": "https://arxiv.org/html/2503.20757/x8.png", "caption": "Figure 6: \nIllustration of how MCTS-RAG achieves a rich reasoning space and tightly integrates reasoning with retrieval.", "description": "This figure illustrates the MCTS-RAG workflow for a sample Complex WebQA question.  It visually demonstrates the iterative decision-making process where the system explores multiple reasoning paths.  Each path involves actions such as providing a direct answer, performing quick reasoning, decomposing the question into sub-questions, and performing retrieval reasoning. The system dynamically integrates retrieval actions at key decision points, acquiring relevant external knowledge to evaluate intermediate states and guide the search process toward beneficial retrieval and reasoning pathways. The figure shows how the various action types, A1-A6, are employed and integrated within the MCTS search tree.  The final answer is selected based on the consensus of these paths.", "section": "3 MCTS-RAG"}, {"figure_path": "https://arxiv.org/html/2503.20757/x9.png", "caption": "Figure 7: \nIllustration of the effectiveness of MCTS-RAG. How further reasoning reduces retrieval-introduced hallucinations and improves accuracy.", "description": "Figure 7 demonstrates the effectiveness of the MCTS-RAG framework in mitigating retrieval-induced hallucinations and improving accuracy.  The example shows how MCTS-RAG's multi-step reasoning process allows it to refine its understanding and ultimately reach the correct answer, even after initially retrieving potentially misleading information.  The iterative refinement, facilitated by both reasoning and further retrieval, allows the model to avoid errors introduced by inaccurate or incomplete knowledge from the initial retrieval step.", "section": "4.4 Human Analysis and Case Study"}, {"figure_path": "https://arxiv.org/html/2503.20757/x10.png", "caption": "Figure 8: An illustration of the effectiveness of MCTS-RAG. Based on a clear chain of reasoning, it can generate higher quality retrieval queries and final answers, reduce hallucinations and improve accuracy.", "description": "Figure 8 demonstrates MCTS-RAG's superior performance compared to standard RAG approaches.  The example shows how MCTS-RAG's structured reasoning process leads to more precise retrieval queries. This results in more accurate and relevant information being integrated into the model's answer generation, ultimately reducing errors and improving overall accuracy. Unlike standard RAG, which may make assumptions or hallucinate, MCTS-RAG's iterative refinement ensures a more reliable and evidence-based response.", "section": "4.4 Human Analysis and Case Study"}, {"figure_path": "https://arxiv.org/html/2503.20757/x11.png", "caption": "Figure 9: An illustration of standard RAG. Because the reasoning process is not clear enough, the final answer to the question is an illusion and the answer is wrong.", "description": "Figure 9 shows an example where a standard Retrieval-Augmented Generation (RAG) model fails to accurately answer a question about the number of products resulting from a chemical reaction. The model's reasoning process is unclear and lacks the detailed steps necessary to arrive at the correct answer.  The model's response indicates it only considered the possibilities for the reaction superficially, leading to an incorrect answer of 3, while the actual correct answer is 6.  This highlights a key limitation of standard RAG: its inability to perform thorough, multi-step reasoning without additional mechanisms to enhance its reasoning capabilities.", "section": "4.4 Human Analysis and Case Study"}]