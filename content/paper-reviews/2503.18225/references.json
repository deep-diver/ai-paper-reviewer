{"references": [{"fullname_first_author": "Edward J Hu", "paper_title": "Lora: Low-rank adaptation of large language models", "publication_date": "2022-01-01", "reason": "This paper introduces LoRA, the foundational parameter-efficient finetuning technique that DeLoRA aims to improve upon."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Llama 2: Open foundation and fine-tuned chat models", "publication_date": "2023-01-01", "reason": "This paper presents LLaMA-2, the large language model used to evaluate DeLoRA's performance on instruction tuning tasks."}, {"fullname_first_author": "Robin Rombach", "paper_title": "High-resolution image synthesis with latent diffusion models", "publication_date": "2022-01-01", "reason": "This paper introduces Stable Diffusion, the latent diffusion model used to evaluate DeLoRA's performance on image generation tasks."}, {"fullname_first_author": "Massimo Bini", "paper_title": "ETHER: Efficient finetuning of large-scale models with hyperplane reflections", "publication_date": "2024-01-01", "reason": "This paper introduces ETHER, a robust finetuning approach that DeLoRA builds upon and aims to improve in terms of expressivity."}, {"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-01-01", "reason": "This paper introduces CLIP, a model used to measure the subject-fidelity, of generated images."}]}