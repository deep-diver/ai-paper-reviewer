{"importance": "This paper introduces a method that balances data quality and diversity, improving the efficiency of instruction tuning. It opens new avenues for research in automatic data selection. The method enables smaller, high-quality datasets to achieve comparable or superior performance to larger datasets.", "summary": "MIG optimizes data selection for instruction tuning by maximizing information gain in semantic space, achieving state-of-the-art performance with smaller, diverse datasets.", "takeaways": ["MIG effectively balances data quality and diversity for instruction tuning by maximizing information gain in semantic space.", "MIG consistently outperforms existing data selection methods across various datasets and base models.", "MIG significantly enhances sampling efficiency, reducing computational overhead compared to embedding-based methods."], "tldr": "Data quality and diversity are crucial for instruction tuning. Existing methods often prioritize instance quality but lack a comprehensive view of the entire collection, leading to suboptimal results. They use heuristic rules for diversity, focusing on distance or clustering, which doesn't accurately capture complex instructions in the semantic space. The process of manual dataset creation is time-consuming and labor-intensive. \n\nThis paper proposes a unified method for quantifying the information content of datasets, modeling the semantic space by constructing a label graph and quantifying diversity based on information distribution within the graph. It introduces an efficient sampling method that iteratively selects data samples to **Maximize the Information Gain (MIG)** in semantic space. Experiments demonstrate that MIG consistently outperforms state-of-the-art methods, achieving comparable performance with significantly smaller datasets.", "affiliation": "Shanghai AI Laboratory", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}, "podcast_path": "2504.13835/podcast.wav"}