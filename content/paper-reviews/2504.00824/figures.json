[{"figure_path": "https://arxiv.org/html/2504.00824/x1.png", "caption": "Figure 1: Comparison of traditional Retrieval-Augmented Generation (RAG) systems and our proposed ScholarCopilot. Traditional RAG systems (left) separately perform retrieval and generation, leading to representation misalignment. In contrast, ScholarCopilot (right) dynamically generates retrieval tokens ([RET]) during text generation for integrated and context-aware reference retrieval.", "description": "This figure compares traditional Retrieval-Augmented Generation (RAG) systems with the ScholarCopilot system. Traditional RAG systems perform retrieval and generation as separate steps, which can lead to inconsistencies. In contrast, ScholarCopilot integrates these processes by dynamically generating retrieval tokens within the generation process, leading to more contextually relevant and accurate citations.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2504.00824/x2.png", "caption": "Figure 2: Comparison between traditional Retrieval-Augmented Generation (RAG) methods (left) and ScholarCopilot (right). Traditional RAG follows a static retrieval-then-generation pipeline, retrieving references independently before generation. ScholarCopilot dynamically interleaves retrieval and generation by producing retrieval tokens ([RET]) based on current context, enabling context-aware citation retrieval and optional user refinement.", "description": "This figure illustrates the key difference between traditional Retrieval Augmented Generation (RAG) and the ScholarCopilot method. Traditional RAG systems perform retrieval and generation as two separate steps.  First, a query is submitted to a retriever to obtain relevant documents. Then, these documents are fed into a language model generator to create the output text. This separation limits the dynamic adaptation to the evolving context during text generation. In contrast, ScholarCopilot integrates both retrieval and generation iteratively. It dynamically generates special tokens ([RET]) during the text generation process. These tokens act as triggers for the retrieval process, which then integrates the retrieved information into the ongoing text generation in a context-aware manner. This iterative process allows for a more flexible and refined approach to both content generation and citation insertion, enhancing the accuracy and relevance of citations.", "section": "2 Related Work"}, {"figure_path": "https://arxiv.org/html/2504.00824/x3.png", "caption": "Figure 3: The pipeline for creating the ScholarCopilot dataset. Our final dataset includes 10M citations matched from arXiv and 6.8M citations matched from Semantic Scholar (one paper may be cited by multiple articles). However, at inference time, to ensure reference quality, we only use the 670K articles from arXiv as the corpus.", "description": "The figure illustrates the process of building the ScholarCopilot dataset.  It starts with collecting 670,000 computer science papers from arXiv. These papers are then parsed to extract structured information and bibliographic entries, resulting in 501,000 papers. The citations from these papers are matched against citations in both the arXiv and Semantic Scholar databases. The resulting dataset contains 10 million citations from arXiv and 6.8 million from Semantic Scholar.  Importantly, although the training data incorporates citations from both sources, at inference time only the 670,000 arXiv papers are used to ensure reference quality.", "section": "3 ScholarCopilot"}, {"figure_path": "https://arxiv.org/html/2504.00824/x4.png", "caption": "Figure 4: Unified training framework of ScholarCopilot. The architecture jointly optimizes the next token prediction loss for text generation and the contrastive loss for citation retrieval.\nRetrieval tokens ([RET]) dynamically trigger retrieval.\n<q,d+><q,d^{+}>< italic_q , italic_d start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT > indicates the positive pair of query and document during contrastive learning, and <q,d\u2212><q,d^{-}>< italic_q , italic_d start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT > indicates the negative pair.\nThe generation model and retrieval model share parameters.\nIn this figure, Paper 1 and Paper 2 can be considered as hard negatives for each other.", "description": "This figure illustrates the unified training framework of ScholarCopilot, which simultaneously optimizes text generation and citation retrieval.  The model uses retrieval tokens ([RET]) to dynamically trigger the retrieval of relevant citations from a database. These tokens act as signals for when to pause the text generation process and consult the database.  The system uses contrastive learning; the model learns to distinguish between positive (relevant) citation-query pairs (<q,d+>) and negative (irrelevant) citation-query pairs (<q,d->). Notably, the generation and retrieval models share parameters, making the system more efficient.  The example in the figure shows how two papers (Paper 1 and Paper 2) can be considered hard negatives during training, meaning they are highly similar yet represent different relevant citations.", "section": "3 ScholarCopilot"}, {"figure_path": "https://arxiv.org/html/2504.00824/x5.png", "caption": "Figure 5: Comparison of citation retrieval performance (Recall@k) between ScholarCopilot and baseline retrieval methods (BM25 and E5-Mistral-7B-Instruct).", "description": "Figure 5 presents a detailed comparison of the citation retrieval performance achieved by ScholarCopilot against two established baseline methods: BM25 and E5-Mistral-7B-Instruct.  The graph displays Recall@k values for k = 1 to 10. Recall@k indicates the percentage of times the correct citation was found among the top k retrieved results. This visualization effectively showcases ScholarCopilot's superior performance across all recall levels, highlighting a substantial improvement over both BM25 and E5-Mistral-7B-Instruct.", "section": "4.4.1 Retrieval Performance"}, {"figure_path": "https://arxiv.org/html/2504.00824/x6.png", "caption": "(a) Average ratings for ScholarCopilot across evaluation dimensions: citation quality (yellow), user experience (green), and content quality (purple), from user study (N=10).", "description": "This figure presents a bar chart summarizing the results of a user study evaluating ScholarCopilot.  Three main categories are evaluated: citation quality, user experience, and content quality.  Each category is further broken down into several sub-metrics which are shown as individual bars within each category.  The height of each bar represents the average rating (on a scale of 1-5, presumably) given by the 10 participants (N=10) in the study. The color coding helps distinguish between the three main categories of evaluation.", "section": "5 User Study"}, {"figure_path": "https://arxiv.org/html/2504.00824/x7.png", "caption": "(b) Comparative analysis of ScholarCopilot vs. ChatGPT across five dimensions: Citation Quality, Writing Quality, Ease of Use, Time Efficiency, and Overall Usefulness. Darker blue indicates higher percentages of ratings.", "description": "This figure displays a comparative analysis of ScholarCopilot and ChatGPT across five key aspects: Citation Quality, Writing Quality, Ease of Use, Time Efficiency, and Overall Usefulness.  Each aspect is represented by a bar graph, with the height of the bar representing the percentage of participants who rated ScholarCopilot more favorably than ChatGPT. The use of darker shades of blue indicates a higher percentage of positive ratings for ScholarCopilot in each category, allowing for a visual comparison of user preferences between the two tools.", "section": "5 User Study"}, {"figure_path": "https://arxiv.org/html/2504.00824/x8.png", "caption": "Figure 6: Human evaluation of ScholarCopilot and comparative analysis with ChatGPT", "description": "This figure presents the results of a human evaluation study comparing ScholarCopilot and ChatGPT.  The left subplot (6a) shows the average ratings across several dimensions, including citation quality, user experience, and content quality. Each dimension was rated on a 5-point Likert scale, and the bars represent the mean ratings for ScholarCopilot across multiple user tasks. The right subplot (6b) presents a comparative analysis of ScholarCopilot and ChatGPT across various aspects (citation quality, writing quality, ease of use, time efficiency, and overall usefulness). Each bar graph within this subplot indicates the percentage of participants who rated ScholarCopilot as better, similar, or worse than ChatGPT on each specific aspect. This allows for a direct comparison of user perception regarding the performance and utility of both systems. ", "section": "5 User Study"}, {"figure_path": "https://arxiv.org/html/2504.00824/x9.png", "caption": "Figure 7: Human Study Questionnaire Page 1", "description": "This page shows the first page of the questionnaire used in a user study evaluating ScholarCopilot.  It gathers demographic information about participants (name, education level, experience with academic writing and ChatGPT). It also collects information about the academic topics participants wrote about using ScholarCopilot, and asks for ratings on a 1-5 Likert scale for various metrics related to citation quality, user experience, and content quality.", "section": "5 User Study"}, {"figure_path": "https://arxiv.org/html/2504.00824/x10.png", "caption": "Figure 8: Human Study Questionnaire Page 2", "description": "This questionnaire page gathers user feedback on ScholarCopilot, focusing on citation quality, user experience, and content quality.  Users rate various aspects on a 5-point Likert scale, providing both quantitative and qualitative feedback.  They also compare ScholarCopilot to ChatGPT across multiple criteria and provide open-ended responses about their experience and suggestions for improvement.", "section": "5 User Study"}, {"figure_path": "https://arxiv.org/html/2504.00824/x11.png", "caption": "Figure 9: Human Study Questionnaire Page 3", "description": "This page of the human study questionnaire collects final comments from participants and records of the content generated by ScholarCopilot for three different topics.  For each topic, participants provide the topic title, the section of the paper generated (e.g., introduction, related work), the generated content itself, and a list of citations suggested by the tool.", "section": "5 User Study"}]