{"references": [{"fullname_first_author": "Tsung-Yi Lin", "paper_title": "Microsoft coco: Common objects in context", "publication_date": "2014-01-01", "reason": "This paper introduces a widely-used dataset for object detection, segmentation, and captioning, making it a fundamental resource for computer vision tasks."}, {"fullname_first_author": "OpenAI", "paper_title": "GPT-4 Technical Report", "publication_date": "2023-01-01", "reason": "This report details the capabilities of GPT-4, a pivotal large language model used in the semi-automated pipeline for generating the datasets used to test the reasoning of other video-LLMs."}, {"fullname_first_author": "Kunchang Li", "paper_title": "MVBench: A Comprehensive Multi-Modal Video Understanding Benchmark", "publication_date": "2024-01-01", "reason": "This work presents a comprehensive benchmark for video understanding, which is used as the foundation to determine how to score the accuracy of open-ended what questions."}, {"fullname_first_author": "Jie Lei", "paper_title": "TVQA: Localized, Compositional Video Question Answering", "publication_date": "2018-01-01", "reason": "This benchmark is an important work that proposes Grounded Video Question Answering requiring temporal grounded evidence in TV series videos, providing a benchmark to expand upon."}, {"fullname_first_author": "Peng Wang", "paper_title": "Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution", "publication_date": "2024-01-01", "reason": "This paper details the Qwen2-VL, a vision-language model used to score answers and which forms the basis for evaluating a model's performance."}]}