[{"figure_path": "https://arxiv.org/html/2504.10326/x1.png", "caption": "Figure 1. The concepts and illustrations of LLM inference", "description": "Figure 1 illustrates the process of large language model (LLM) inference.  Panel (a) shows a high-level view of an LLM service, taking a prompt as input and generating a response. Panel (b) details the stages of LLM inference: tokenization and embedding of the input text, processing through multiple transformer layers (each containing self-attention and feed-forward neural networks), and finally, generating the response text. Panel (c) zooms in on the multi-head self-attention mechanism, showing how the input is split into multiple heads, and attention weights are computed for each head. Panel (d) further clarifies the self-attention calculation, highlighting the use of query (Q), key (K), and value (V) matrices along with the KV cache.", "section": "2 LLM Inference"}, {"figure_path": "https://arxiv.org/html/2504.10326/", "caption": "Figure 2. Summary of LLM inference solutions", "description": "This figure compares four different approaches to LLM inference: coupled architecture, KV cache disaggregation, retrieval-based sparse attention, and AlayaDB.  Each approach is illustrated with a diagram showing how the LLM inference engine, attention mechanism, context, and KV cache interact.  A table summarizes the performance characteristics of each approach across four key metrics: GPU memory consumption, inference latency, generation quality, and solution usability.  AlayaDB is shown to offer advantages in each of these areas compared to existing solutions.", "section": "Analysis of Existing Solutions"}, {"figure_path": "https://arxiv.org/html/2504.10326/x3.png", "caption": "Figure 3. System overview of AlayaDB", "description": "This figure presents a high-level architecture of AlayaDB, a vector database system designed for efficient and effective long-context LLM inference.  It shows the three main components: the User Interface, the Query Processing Engine, and the Vector Storage Engine. The User Interface provides a simplified abstraction for LLM developers. The Query Processing Engine is responsible for efficient query planning and processing, including the use of a query optimizer to select optimal query types and indexes for attention computations.  Finally, the Vector Storage Engine manages the underlying vector data efficiently using a buffer manager and a vector file system. The diagram illustrates how these components interact with each other and with LLM inference engines to achieve efficient long-context LLM inference.", "section": "4 System Overview of AlayaDB"}, {"figure_path": "https://arxiv.org/html/2504.10326/x6.png", "caption": "Figure 4. Using AlayaDB APIs for LLM inference", "description": "This figure illustrates how to integrate AlayaDB into an existing LLM inference system using the provided APIs. It showcases code snippets demonstrating the ease of replacing standard flash-attention and transformers functionalities with AlayaDB's equivalents. The code examples highlight the simplicity of using AlayaDB's session management and attention computation APIs.  It compares original code using flash-attention and transformers with modified code that leverages AlayaDB's APIs. The comparison underscores the minimal code changes needed for integration, emphasizing AlayaDB's ease of use and compatibility with existing LLM frameworks.", "section": "5 User Interface"}, {"figure_path": "https://arxiv.org/html/2504.10326/x7.png", "caption": "Figure 5. The number of selected tokens in different heads", "description": "This figure visualizes the varying number of critical tokens required across different attention heads within various layers of a transformer-based large language model (LLM).  The x-axis represents the layer and head ID, while the y-axis denotes the count of critical tokens necessary to achieve a 90% recovery ratio (meaning 90% of the attention scores are captured). The plot showcases the dynamic nature of sparse attention, highlighting how the number of critical tokens fluctuates significantly depending on the specific layer and head. This underscores the limitations of traditional methods that assume a fixed number of critical tokens for all layers and heads.", "section": "6 Query Processing in AlayaDB"}, {"figure_path": "https://arxiv.org/html/2504.10326/x8.png", "caption": "(a) Passage R.", "description": "This figure shows the number of critical tokens retrieved by different query methods for the Passage Retrieval task in the LongBench benchmark.  It compares the performance of the proposed Dynamic Inner Product Range (DIPR) query against traditional top-k queries, demonstrating DIPR's ability to dynamically adjust the number of tokens retrieved based on the specific task and head, leading to higher efficiency and accuracy.", "section": "6 Query Processing in AlayaDB"}, {"figure_path": "https://arxiv.org/html/2504.10326/x10.png", "caption": "(b) LCC", "description": "This figure shows the number of critical tokens identified by the DIPR query and the top-k query for the LCC (Long Context Completion) task.  It illustrates how DIPR adaptively determines the number of critical tokens needed, unlike the fixed k in top-k, resulting in higher efficiency and accuracy. The x-axis represents the number of critical tokens, and the y-axis shows the performance metric.  The comparison highlights DIPR's superior performance in identifying critical tokens needed for accurate LLM inference with long contexts.", "section": "6 Query Processing in AlayaDB"}, {"figure_path": "https://arxiv.org/html/2504.10326/x15.png", "caption": "Figure 6. The number of critical tokens in different tasks", "description": "Figure 6 presents a comparison of the number of critical tokens retrieved by two different query methods (DIPR and Top-k) across two distinct tasks (Passage R. and LCC) from the LongBench benchmark.  The x-axis represents the number of critical tokens identified, while the y-axis shows the performance of each query method. This figure highlights the dynamic nature of sparse attention and the advantage of the DIPR query over the traditional Top-k query in accurately capturing this dynamic nature.", "section": "6 Query Processing in AlayaDB"}]