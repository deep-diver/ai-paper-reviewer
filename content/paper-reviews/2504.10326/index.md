---
title: "AlayaDB: The Data Foundation for Efficient and Effective Long-context LLM Inference"
summary: "AlayaDB: A novel vector DB for efficient long-context LLM inference, decoupling KV cache and attention for superior performance and reduced resource consumption."
categories: ["AI Generated", "ğŸ¤— Daily Papers"]
tags: ["Natural Language Processing", "Large Language Models", "ğŸ¢ AlayaDB.AI",]
showSummary: true
date: 2025-04-14
draft: false
---

<br>

{{< keywordList >}}
{{< keyword icon="fingerprint" >}} 2504.10326 {{< /keyword >}}
{{< keyword icon="writer" >}} Yangshen Deng et el. {{< /keyword >}}
 
{{< keyword >}} ğŸ¤— 2025-04-17 {{< /keyword >}}
 
{{< /keywordList >}}

{{< button href="https://arxiv.org/abs/2504.10326" target="_self" >}}
â†— arXiv
{{< /button >}}
{{< button href="https://huggingface.co/papers/2504.10326" target="_self" >}}
â†— Hugging Face
{{< /button >}}



<audio controls>
    <source src="https://ai-paper-reviewer.com/2504.10326/podcast.wav" type="audio/wav">
    Your browser does not support the audio element.
</audio>


### TL;DR


{{< lead >}}

Long-context LLM inference struggles with high resource consumption and latency due to the self-attention mechanism's quadratic complexity. Existing solutions like KV cache disaggregation and sparse attention have limitations in simultaneously optimizing generation quality, latency, and resource usage. This paper addresses the challenges in long-context LLM inference, where existing systems struggle to balance generation quality, latency, and resource consumption.



This paper introduces **AlayaDB, a novel vector database system designed to decouple KV cache and attention computation** from the LLM inference engine. AlayaDB introduces a novel query type called dynamic inner product range query (DIPR) to capture the dynamic nature of sparse attention, and the co-optimization enables reduced resource consumption, lower latency, and higher generation quality.

{{< /lead >}}


#### Key Takeaways

{{< alert "star" >}}
{{< typeit speed=10 lifeLike=true >}} AlayaDB decouples KV cache and attention computation from LLM inference, improving performance and reducing resource usage. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=1000 lifeLike=true >}} The dynamic inner product range query (DIPR) adaptively determines the number of critical tokens for efficient sparse attention. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=2000 lifeLike=true >}} AlayaDB's architecture and optimizations enable high generation quality and low latency for long-context LLM inference. {{< /typeit >}}
{{< /alert >}}

#### Why does it matter?
This paper introduces a novel approach to long-context LLM inference, offering a promising solution for efficient resource utilization and high generation quality. It advances research in vector databases and LLM optimization, enabling further exploration of attention mechanisms and distributed inference.

------
#### Visual Insights



![](https://arxiv.org/html/2504.10326/x1.png)

> ğŸ”¼ Figure 1 illustrates the process of large language model (LLM) inference.  Panel (a) shows a high-level view of an LLM service, taking a prompt as input and generating a response. Panel (b) details the stages of LLM inference: tokenization and embedding of the input text, processing through multiple transformer layers (each containing self-attention and feed-forward neural networks), and finally, generating the response text. Panel (c) zooms in on the multi-head self-attention mechanism, showing how the input is split into multiple heads, and attention weights are computed for each head. Panel (d) further clarifies the self-attention calculation, highlighting the use of query (Q), key (K), and value (V) matrices along with the KV cache.
> <details>
> <summary>read the caption</summary>
> Figure 1. The concepts and illustrations of LLM inference
> </details>





{{< table-caption >}}
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T1.4">
<tr class="ltx_tr" id="S3.T1.4.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.4.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.4.1.1.1" style="font-size:90%;">Existing</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.4.1.2"><span class="ltx_text ltx_font_bold" id="S3.T1.4.1.2.1" style="font-size:90%;">GPU memory</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.4.1.3"><span class="ltx_text ltx_font_bold" id="S3.T1.4.1.3.1" style="font-size:90%;">Inference</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.4.1.4"><span class="ltx_text ltx_font_bold" id="S3.T1.4.1.4.1" style="font-size:90%;">Generation</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.4.1.5"><span class="ltx_text ltx_font_bold" id="S3.T1.4.1.5.1" style="font-size:90%;">Solution</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.2">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S3.T1.4.2.1"><span class="ltx_text ltx_font_bold" id="S3.T1.4.2.1.1" style="font-size:90%;">solution</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.4.2.2"><span class="ltx_text ltx_font_bold" id="S3.T1.4.2.2.1" style="font-size:90%;">consumption</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.4.2.3"><span class="ltx_text ltx_font_bold" id="S3.T1.4.2.3.1" style="font-size:90%;">latency</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.4.2.4"><span class="ltx_text ltx_font_bold" id="S3.T1.4.2.4.1" style="font-size:90%;">quality</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.4.2.5"><span class="ltx_text ltx_font_bold" id="S3.T1.4.2.5.1" style="font-size:90%;">usability</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.3">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.4.3.1"><span class="ltx_text" id="S3.T1.4.3.1.1" style="font-size:90%;">â‘ </span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.4.3.2"><span class="ltx_text" id="S3.T1.4.3.2.1" style="font-size:90%;">Large</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.4.3.3"><span class="ltx_text" id="S3.T1.4.3.3.1" style="font-size:90%;">High</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.4.3.4"><span class="ltx_text" id="S3.T1.4.3.4.1" style="font-size:90%;">Good</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.4.3.5"><span class="ltx_text" id="S3.T1.4.3.5.1" style="font-size:90%;">Good</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.4">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.4.4.1"><span class="ltx_text" id="S3.T1.4.4.1.1" style="font-size:90%;">â‘¡</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.4.4.2"><span class="ltx_text" id="S3.T1.4.4.2.1" style="font-size:90%;">Large</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.4.4.3"><span class="ltx_text" id="S3.T1.4.4.3.1" style="font-size:90%;">Medium</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.4.4.4"><span class="ltx_text" id="S3.T1.4.4.4.1" style="font-size:90%;">Good</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.4.4.5"><span class="ltx_text" id="S3.T1.4.4.5.1" style="font-size:90%;">Medium</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.5">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.4.5.1"><span class="ltx_text" id="S3.T1.4.5.1.1" style="font-size:90%;">â‘¢</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.4.5.2"><span class="ltx_text" id="S3.T1.4.5.2.1" style="font-size:90%;">Small</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.4.5.3"><span class="ltx_text" id="S3.T1.4.5.3.1" style="font-size:90%;">â€”</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.4.5.4"><span class="ltx_text" id="S3.T1.4.5.4.1" style="font-size:90%;">Medium</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.4.5.5"><span class="ltx_text" id="S3.T1.4.5.5.1" style="font-size:90%;">Bad</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.6">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.4.6.1"><span class="ltx_text ltx_font_bold" id="S3.T1.4.6.1.1" style="font-size:90%;">AlayaDB</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T1.4.6.2"><span class="ltx_text ltx_font_bold" id="S3.T1.4.6.2.1" style="font-size:90%;">Small</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T1.4.6.3"><span class="ltx_text ltx_font_bold" id="S3.T1.4.6.3.1" style="font-size:90%;">Low</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T1.4.6.4"><span class="ltx_text ltx_font_bold" id="S3.T1.4.6.4.1" style="font-size:90%;">Good</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T1.4.6.5"><span class="ltx_text ltx_font_bold" id="S3.T1.4.6.5.1" style="font-size:90%;">Good</span></td>
</tr>
</table>{{< /table-caption >}}

> ğŸ”¼ This table compares different Large Language Model (LLM) inference solutions across four key aspects: GPU memory consumption, inference latency, generation quality, and solution usability.  It helps to understand the trade-offs between different approaches to optimizing LLM inference, highlighting the advantages and disadvantages of coupled architectures, KV cache disaggregation, retrieval-based sparse attention, and the authors' proposed AlayaDB solution.
> <details>
> <summary>read the caption</summary>
> Table 1. LLM inference solutions analysis
> </details>





### In-depth insights


#### LLM Native DB
An LLM-native DB represents a paradigm shift, **tightly integrating database functionalities with large language models**. This entails optimizing data storage and retrieval for LLM workloads, potentially using vector embeddings for semantic search. Key features could include efficient management of long contexts, optimized KV cache handling, and support for sparse attention mechanisms. Such a database would abstract away complexities, offering easy-to-use APIs for LLM developers, **similar to traditional databases in web applications**. This approach promises to significantly reduce resource consumption, lower latency, and improve generation quality, marking a crucial advancement in LLM infrastructure.

#### Sparse Attention
**Sparse attention** emerges as a crucial technique for enhancing the efficiency of LLM inference, particularly in long-context scenarios. By selectively attending to a subset of the most relevant tokens, it significantly reduces the computational burden and memory footprint associated with the attention mechanism. The core idea behind sparse attention is that not all tokens in the context are equally important for generating the next token. By identifying and focusing on the most critical tokens, the model can achieve comparable generation quality with a fraction of the computational cost. This approach becomes increasingly vital as context lengths grow, making full attention impractical due to its quadratic complexity. There are various strategies for determining which tokens to attend to, ranging from fixed patterns to data-driven approaches. The effectiveness of sparse attention hinges on the ability to accurately identify the most informative tokens while avoiding the risk of discarding crucial information. This trade-off between efficiency and accuracy is a key consideration in the design and implementation of sparse attention mechanisms.

#### Dynamic DIPR
**Dynamic Inner Product Range (DIPR) query** emerges as a novel solution to overcome limitations of static top-k queries in sparse attention. Unlike top-k, which uses a fixed 'k,' DIPR adaptively determines the number of critical tokens based on the query vector and a parameter Î², which facilitates capturing the dynamic nature of sparse attention across various LLM layers/tasks. The core idea revolves around finding tokens whose inner product with the query vector exceeds a certain threshold relative to the maximum inner product. This adaptive selection mechanism aims to enhance generation quality by retrieving more relevant tokens, reducing computational costs by avoiding unnecessary tokens, and achieves superior performance compared to static top-k queries.

#### AlayaDB: Design
While the heading 'AlayaDB: Design' isn't explicitly present, we can infer its design principles. It aims for **ease-of-use** via simple abstractions, **high generation quality** by identifying critical tokens, and **good efficiency** through performance optimizations and resource management. The architecture likely involves a modular approach, decoupling KV cache and attention computation into a vector database. This design likely emphasizes **co-optimization** of attention and cache management, leveraging techniques like data-centric attention. Key components probably include a user interface for LLM integration, a query processing engine for efficient attention computation, and a vector storage engine with intelligent data layout and caching. Focus on a query optimizer is crucial to intelligently select efficient query plan by considering hardware and application-specific workloads, and ultimately achieve the desired trade-off between latency, quality and resource consumption. The modularity also means more opportunities for different kinds of hardware acceleration.

#### End-to-End Perf.
Based on the provided context, the 'End-to-End Perf.' section likely discusses the overall performance evaluation of AlayaDB. This would involve analyzing metrics like **TPOT (Time-Per-Output-Token)**, **quality of generated text**, and **GPU memory consumption**. The evaluation would compare AlayaDB's performance against other LLM inference solutions, including **full attention methods**, **KV cache disaggregation approaches**, and **sparse attention techniques**. Key aspects include how AlayaDB balances these three factors to achieve **low latency, high quality, and low resource consumption** for long-context LLM serving. It could also contains information about whether AlayaDB can meet the **SLO (Service Level Objectives)**. In addition, the section might delve into the **methodologies** employed for testing, the **specific datasets** used, and the **implementation details** of AlayaDB that contribute to its end-to-end performance.


### More visual insights

<details>
<summary>More on figures
</summary>


![](https://arxiv.org/html/2504.10326/)

> ğŸ”¼ This figure compares four different approaches to LLM inference: coupled architecture, KV cache disaggregation, retrieval-based sparse attention, and AlayaDB.  Each approach is illustrated with a diagram showing how the LLM inference engine, attention mechanism, context, and KV cache interact.  A table summarizes the performance characteristics of each approach across four key metrics: GPU memory consumption, inference latency, generation quality, and solution usability.  AlayaDB is shown to offer advantages in each of these areas compared to existing solutions.
> <details>
> <summary>read the caption</summary>
> Figure 2. Summary of LLM inference solutions
> </details>



![](https://arxiv.org/html/2504.10326/x3.png)

> ğŸ”¼ This figure presents a high-level architecture of AlayaDB, a vector database system designed for efficient and effective long-context LLM inference.  It shows the three main components: the User Interface, the Query Processing Engine, and the Vector Storage Engine. The User Interface provides a simplified abstraction for LLM developers. The Query Processing Engine is responsible for efficient query planning and processing, including the use of a query optimizer to select optimal query types and indexes for attention computations.  Finally, the Vector Storage Engine manages the underlying vector data efficiently using a buffer manager and a vector file system. The diagram illustrates how these components interact with each other and with LLM inference engines to achieve efficient long-context LLM inference.
> <details>
> <summary>read the caption</summary>
> Figure 3. System overview of AlayaDB
> </details>



![](https://arxiv.org/html/2504.10326/x6.png)

> ğŸ”¼ This figure illustrates how to integrate AlayaDB into an existing LLM inference system using the provided APIs. It showcases code snippets demonstrating the ease of replacing standard flash-attention and transformers functionalities with AlayaDB's equivalents. The code examples highlight the simplicity of using AlayaDB's session management and attention computation APIs.  It compares original code using flash-attention and transformers with modified code that leverages AlayaDB's APIs. The comparison underscores the minimal code changes needed for integration, emphasizing AlayaDB's ease of use and compatibility with existing LLM frameworks.
> <details>
> <summary>read the caption</summary>
> Figure 4. Using AlayaDB APIs for LLM inference
> </details>



![](https://arxiv.org/html/2504.10326/x7.png)

> ğŸ”¼ This figure visualizes the varying number of critical tokens required across different attention heads within various layers of a transformer-based large language model (LLM).  The x-axis represents the layer and head ID, while the y-axis denotes the count of critical tokens necessary to achieve a 90% recovery ratio (meaning 90% of the attention scores are captured). The plot showcases the dynamic nature of sparse attention, highlighting how the number of critical tokens fluctuates significantly depending on the specific layer and head. This underscores the limitations of traditional methods that assume a fixed number of critical tokens for all layers and heads.
> <details>
> <summary>read the caption</summary>
> Figure 5. The number of selected tokens in different heads
> </details>



![](https://arxiv.org/html/2504.10326/x8.png)

> ğŸ”¼ This figure shows the number of critical tokens retrieved by different query methods for the Passage Retrieval task in the LongBench benchmark.  It compares the performance of the proposed Dynamic Inner Product Range (DIPR) query against traditional top-k queries, demonstrating DIPR's ability to dynamically adjust the number of tokens retrieved based on the specific task and head, leading to higher efficiency and accuracy.
> <details>
> <summary>read the caption</summary>
> (a) Passage R.
> </details>



![](https://arxiv.org/html/2504.10326/x10.png)

> ğŸ”¼ This figure shows the number of critical tokens identified by the DIPR query and the top-k query for the LCC (Long Context Completion) task.  It illustrates how DIPR adaptively determines the number of critical tokens needed, unlike the fixed k in top-k, resulting in higher efficiency and accuracy. The x-axis represents the number of critical tokens, and the y-axis shows the performance metric.  The comparison highlights DIPR's superior performance in identifying critical tokens needed for accurate LLM inference with long contexts.
> <details>
> <summary>read the caption</summary>
> (b) LCC
> </details>



![](https://arxiv.org/html/2504.10326/x15.png)

> ğŸ”¼ Figure 6 presents a comparison of the number of critical tokens retrieved by two different query methods (DIPR and Top-k) across two distinct tasks (Passage R. and LCC) from the LongBench benchmark.  The x-axis represents the number of critical tokens identified, while the y-axis shows the performance of each query method. This figure highlights the dynamic nature of sparse attention and the advantage of the DIPR query over the traditional Top-k query in accurately capturing this dynamic nature.
> <details>
> <summary>read the caption</summary>
> Figure 6. The number of critical tokens in different tasks
> </details>



</details>




<details>
<summary>More on tables
</summary>


{{< table-caption >}}
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S5.T2.4">
<tr class="ltx_tr" id="S5.T2.4.1">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.4.1.1"><span class="ltx_text ltx_font_bold" id="S5.T2.4.1.1.1">DB abstraction and provided APIs</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.2">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.4.2.1"><code class="ltx_verbatim ltx_font_typewriter" id="S5.T2.4.2.1.1">DB.create_session(prompts) -&gt; Session, prompts</code></td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.3">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.4.3.1"><code class="ltx_verbatim ltx_font_typewriter" id="S5.T2.4.3.1.1">DB.import(prompts, kv_cache)</code></td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.4">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.4.4.1"><code class="ltx_verbatim ltx_font_typewriter" id="S5.T2.4.4.1.1">DB.store(session)</code></td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.5">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.4.5.1"><span class="ltx_text ltx_font_bold" id="S5.T2.4.5.1.1">Session abstraction and provided APIs</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.6">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.4.6.1"><code class="ltx_verbatim ltx_font_typewriter" id="S5.T2.4.6.1.1">Session.attention(q, layer) -&gt; o</code></td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.7">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.4.7.1"><code class="ltx_verbatim ltx_font_typewriter" id="S5.T2.4.7.1.1">Session.update(q, k, v, layer) -&gt; k, v</code></td>
</tr>
</table>{{< /table-caption >}}
> ğŸ”¼ This table lists the Application Programming Interfaces (APIs) provided by AlayaDB for interacting with its database and managing LLM sessions.  It details the functions for creating sessions, importing and storing data, and performing attention calculations within the context of a session.
> <details>
> <summary>read the caption</summary>
> Table 2. AlayaDB APIs
> </details>

{{< table-caption >}}
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S5.F4.2">
<tr class="ltx_tr" id="S5.F4.1.1">
<td class="ltx_td ltx_align_center" id="S5.F4.1.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="348" id="S5.F4.1.1.1.g1" src="x4.png" width="789"/></td>
</tr>
<tr class="ltx_tr" id="S5.F4.2.3">
<td class="ltx_td ltx_align_center" id="S5.F4.2.3.1"><span class="ltx_text" id="S5.F4.2.3.1.1" style="font-size:90%;">(a) Original code using flash-attention and transformers</span></td>
</tr>
<tr class="ltx_tr" id="S5.F4.2.2">
<td class="ltx_td ltx_align_center" id="S5.F4.2.2.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="348" id="S5.F4.2.2.1.g1" src="x5.png" width="789"/></td>
</tr>
<tr class="ltx_tr" id="S5.F4.2.4">
<td class="ltx_td ltx_align_center" id="S5.F4.2.4.1"><span class="ltx_text" id="S5.F4.2.4.1.1" style="font-size:90%;">(b) Modified code using AlayaDB with transformers</span></td>
</tr>
</table>{{< /table-caption >}}
> ğŸ”¼ This table presents the number of critical tokens (k) needed for different tasks in long-context large language model (LLM) inference to achieve the same accuracy as using all tokens.  The tasks represent various applications of LLMs, including question answering (Qasper, HotpotQA, TriviaQA), summarization (QMSum), code completion (LCC), and passage retrieval.  The table shows that the required number of tokens varies significantly across tasks, ranging from a small number (20 for TriviaQA) to a much larger number (350 for Qasper), reflecting the complexity of each task.
> <details>
> <summary>read the caption</summary>
> Table 3. The number kğ‘˜kitalic_k of required tokens in different tasks
> </details>

{{< table-caption >}}
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S6.T3.7">
<tr class="ltx_tr" id="S6.T3.7.1">
<td class="ltx_td ltx_align_center ltx_border_ll ltx_border_r ltx_border_t" id="S6.T3.7.1.1"><span class="ltx_text" id="S6.T3.7.1.1.1" style="font-size:90%;">Task</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T3.7.1.2"><span class="ltx_text" id="S6.T3.7.1.2.1" style="font-size:90%;">k</span></td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" id="S6.T3.7.1.3"><span class="ltx_text" id="S6.T3.7.1.3.1" style="font-size:90%;">proportion</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T3.7.1.4"><span class="ltx_text" id="S6.T3.7.1.4.1" style="font-size:90%;">Task</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T3.7.1.5"><span class="ltx_text" id="S6.T3.7.1.5.1" style="font-size:90%;">k</span></td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" id="S6.T3.7.1.6"><span class="ltx_text" id="S6.T3.7.1.6.1" style="font-size:90%;">proportion</span></td>
</tr>
<tr class="ltx_tr" id="S6.T3.7.2">
<td class="ltx_td ltx_align_center ltx_border_ll ltx_border_r ltx_border_tt" id="S6.T3.7.2.1"><span class="ltx_text ltx_font_sansserif" id="S6.T3.7.2.1.1" style="font-size:90%;">Qasper</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S6.T3.7.2.2"><span class="ltx_text" id="S6.T3.7.2.2.1" style="font-size:90%;">350</span></td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_tt" id="S6.T3.7.2.3"><span class="ltx_text" id="S6.T3.7.2.3.1" style="font-size:90%;">9.67%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S6.T3.7.2.4"><span class="ltx_text ltx_font_sansserif" id="S6.T3.7.2.4.1" style="font-size:90%;">LCC</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S6.T3.7.2.5"><span class="ltx_text" id="S6.T3.7.2.5.1" style="font-size:90%;">65</span></td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_tt" id="S6.T3.7.2.6"><span class="ltx_text" id="S6.T3.7.2.6.1" style="font-size:90%;">5.26%</span></td>
</tr>
<tr class="ltx_tr" id="S6.T3.7.3">
<td class="ltx_td ltx_align_center ltx_border_ll ltx_border_r ltx_border_t" id="S6.T3.7.3.1"><span class="ltx_text ltx_font_sansserif" id="S6.T3.7.3.1.1" style="font-size:90%;">Passage R.</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T3.7.3.2"><span class="ltx_text" id="S6.T3.7.3.2.1" style="font-size:90%;">250</span></td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" id="S6.T3.7.3.3"><span class="ltx_text" id="S6.T3.7.3.3.1" style="font-size:90%;">2.69%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T3.7.3.4"><span class="ltx_text ltx_font_sansserif" id="S6.T3.7.3.4.1" style="font-size:90%;">HotpotQA</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T3.7.3.5"><span class="ltx_text" id="S6.T3.7.3.5.1" style="font-size:90%;">200</span></td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" id="S6.T3.7.3.6"><span class="ltx_text" id="S6.T3.7.3.6.1" style="font-size:90%;">2.19%</span></td>
</tr>
<tr class="ltx_tr" id="S6.T3.7.4">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_ll ltx_border_r ltx_border_t" id="S6.T3.7.4.1"><span class="ltx_text ltx_font_sansserif" id="S6.T3.7.4.1.1" style="font-size:90%;">QMSum</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S6.T3.7.4.2"><span class="ltx_text" id="S6.T3.7.4.2.1" style="font-size:90%;">150</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_rr ltx_border_t" id="S6.T3.7.4.3"><span class="ltx_text" id="S6.T3.7.4.3.1" style="font-size:90%;">1.41%</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S6.T3.7.4.4"><span class="ltx_text ltx_font_sansserif" id="S6.T3.7.4.4.1" style="font-size:90%;">TriviaQA</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S6.T3.7.4.5"><span class="ltx_text" id="S6.T3.7.4.5.1" style="font-size:90%;">20</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_rr ltx_border_t" id="S6.T3.7.4.6"><span class="ltx_text" id="S6.T3.7.4.6.1" style="font-size:90%;">0.24%</span></td>
</tr>
</table>{{< /table-caption >}}
> ğŸ”¼ This table compares different index types used in AlayaDB for efficient sparse attention computation in LLMs.  It shows how each index type (Coarse, Fine, and Flat) impacts GPU memory consumption, latency for both small and large numbers of required tokens (k), and the types of queries each supports (Top-k, Filter, DIPR). This helps to understand the trade-offs between memory usage, speed, and query capabilities when selecting an index for different LLM workloads and performance needs.
> <details>
> <summary>read the caption</summary>
> Table 4. Characteristics of index types
> </details>

{{< table-caption >}}
<table class="ltx_tabular ltx_centering ltx_figure_panel ltx_align_middle" id="S6.F7.2">
<tr class="ltx_tr" id="S6.F7.2.2">
<td class="ltx_td ltx_align_center" id="S6.F7.1.1.1">Â Â Â  (a) <math alttext="|C|\leq l_{0}" class="ltx_Math" display="inline" id="S6.F7.1.1.1.m1.1"><semantics id="S6.F7.1.1.1.m1.1a"><mrow id="S6.F7.1.1.1.m1.1.2" xref="S6.F7.1.1.1.m1.1.2.cmml"><mrow id="S6.F7.1.1.1.m1.1.2.2.2" xref="S6.F7.1.1.1.m1.1.2.2.1.cmml"><mo id="S6.F7.1.1.1.m1.1.2.2.2.1" stretchy="false" xref="S6.F7.1.1.1.m1.1.2.2.1.1.cmml">|</mo><mi id="S6.F7.1.1.1.m1.1.1" xref="S6.F7.1.1.1.m1.1.1.cmml">C</mi><mo id="S6.F7.1.1.1.m1.1.2.2.2.2" stretchy="false" xref="S6.F7.1.1.1.m1.1.2.2.1.1.cmml">|</mo></mrow><mo id="S6.F7.1.1.1.m1.1.2.1" xref="S6.F7.1.1.1.m1.1.2.1.cmml">â‰¤</mo><msub id="S6.F7.1.1.1.m1.1.2.3" xref="S6.F7.1.1.1.m1.1.2.3.cmml"><mi id="S6.F7.1.1.1.m1.1.2.3.2" xref="S6.F7.1.1.1.m1.1.2.3.2.cmml">l</mi><mn id="S6.F7.1.1.1.m1.1.2.3.3" xref="S6.F7.1.1.1.m1.1.2.3.3.cmml">0</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S6.F7.1.1.1.m1.1b"><apply id="S6.F7.1.1.1.m1.1.2.cmml" xref="S6.F7.1.1.1.m1.1.2"><leq id="S6.F7.1.1.1.m1.1.2.1.cmml" xref="S6.F7.1.1.1.m1.1.2.1"></leq><apply id="S6.F7.1.1.1.m1.1.2.2.1.cmml" xref="S6.F7.1.1.1.m1.1.2.2.2"><abs id="S6.F7.1.1.1.m1.1.2.2.1.1.cmml" xref="S6.F7.1.1.1.m1.1.2.2.2.1"></abs><ci id="S6.F7.1.1.1.m1.1.1.cmml" xref="S6.F7.1.1.1.m1.1.1">ğ¶</ci></apply><apply id="S6.F7.1.1.1.m1.1.2.3.cmml" xref="S6.F7.1.1.1.m1.1.2.3"><csymbol cd="ambiguous" id="S6.F7.1.1.1.m1.1.2.3.1.cmml" xref="S6.F7.1.1.1.m1.1.2.3">subscript</csymbol><ci id="S6.F7.1.1.1.m1.1.2.3.2.cmml" xref="S6.F7.1.1.1.m1.1.2.3.2">ğ‘™</ci><cn id="S6.F7.1.1.1.m1.1.2.3.3.cmml" type="integer" xref="S6.F7.1.1.1.m1.1.2.3.3">0</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.F7.1.1.1.m1.1c">|C|\leq l_{0}</annotation><annotation encoding="application/x-llamapun" id="S6.F7.1.1.1.m1.1d">| italic_C | â‰¤ italic_l start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center" id="S6.F7.2.2.3">Â Â Â Â Â Â Â Â Â Â Â Â Â  (b) Point pruning</td>
<td class="ltx_td ltx_align_center" id="S6.F7.2.2.2">Â Â  (c) <math alttext="\bm{q}\cdot\bm{k}^{T}\geq\bm{q}\cdot\hat{\bm{c}}^{T}-\beta" class="ltx_Math" display="inline" id="S6.F7.2.2.2.m1.1"><semantics id="S6.F7.2.2.2.m1.1a"><mrow id="S6.F7.2.2.2.m1.1.1" xref="S6.F7.2.2.2.m1.1.1.cmml"><mrow id="S6.F7.2.2.2.m1.1.1.2" xref="S6.F7.2.2.2.m1.1.1.2.cmml"><mi id="S6.F7.2.2.2.m1.1.1.2.2" xref="S6.F7.2.2.2.m1.1.1.2.2.cmml">ğ’’</mi><mo id="S6.F7.2.2.2.m1.1.1.2.1" lspace="0.222em" rspace="0.222em" xref="S6.F7.2.2.2.m1.1.1.2.1.cmml">â‹…</mo><msup id="S6.F7.2.2.2.m1.1.1.2.3" xref="S6.F7.2.2.2.m1.1.1.2.3.cmml"><mi id="S6.F7.2.2.2.m1.1.1.2.3.2" xref="S6.F7.2.2.2.m1.1.1.2.3.2.cmml">ğ’Œ</mi><mi id="S6.F7.2.2.2.m1.1.1.2.3.3" xref="S6.F7.2.2.2.m1.1.1.2.3.3.cmml">T</mi></msup></mrow><mo id="S6.F7.2.2.2.m1.1.1.1" xref="S6.F7.2.2.2.m1.1.1.1.cmml">â‰¥</mo><mrow id="S6.F7.2.2.2.m1.1.1.3" xref="S6.F7.2.2.2.m1.1.1.3.cmml"><mrow id="S6.F7.2.2.2.m1.1.1.3.2" xref="S6.F7.2.2.2.m1.1.1.3.2.cmml"><mi id="S6.F7.2.2.2.m1.1.1.3.2.2" xref="S6.F7.2.2.2.m1.1.1.3.2.2.cmml">ğ’’</mi><mo id="S6.F7.2.2.2.m1.1.1.3.2.1" lspace="0.222em" rspace="0.222em" xref="S6.F7.2.2.2.m1.1.1.3.2.1.cmml">â‹…</mo><msup id="S6.F7.2.2.2.m1.1.1.3.2.3" xref="S6.F7.2.2.2.m1.1.1.3.2.3.cmml"><mover accent="true" id="S6.F7.2.2.2.m1.1.1.3.2.3.2" xref="S6.F7.2.2.2.m1.1.1.3.2.3.2.cmml"><mi id="S6.F7.2.2.2.m1.1.1.3.2.3.2.2" xref="S6.F7.2.2.2.m1.1.1.3.2.3.2.2.cmml">ğ’„</mi><mo id="S6.F7.2.2.2.m1.1.1.3.2.3.2.1" xref="S6.F7.2.2.2.m1.1.1.3.2.3.2.1.cmml">^</mo></mover><mi id="S6.F7.2.2.2.m1.1.1.3.2.3.3" xref="S6.F7.2.2.2.m1.1.1.3.2.3.3.cmml">T</mi></msup></mrow><mo id="S6.F7.2.2.2.m1.1.1.3.1" xref="S6.F7.2.2.2.m1.1.1.3.1.cmml">âˆ’</mo><mi id="S6.F7.2.2.2.m1.1.1.3.3" xref="S6.F7.2.2.2.m1.1.1.3.3.cmml">Î²</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.F7.2.2.2.m1.1b"><apply id="S6.F7.2.2.2.m1.1.1.cmml" xref="S6.F7.2.2.2.m1.1.1"><geq id="S6.F7.2.2.2.m1.1.1.1.cmml" xref="S6.F7.2.2.2.m1.1.1.1"></geq><apply id="S6.F7.2.2.2.m1.1.1.2.cmml" xref="S6.F7.2.2.2.m1.1.1.2"><ci id="S6.F7.2.2.2.m1.1.1.2.1.cmml" xref="S6.F7.2.2.2.m1.1.1.2.1">â‹…</ci><ci id="S6.F7.2.2.2.m1.1.1.2.2.cmml" xref="S6.F7.2.2.2.m1.1.1.2.2">ğ’’</ci><apply id="S6.F7.2.2.2.m1.1.1.2.3.cmml" xref="S6.F7.2.2.2.m1.1.1.2.3"><csymbol cd="ambiguous" id="S6.F7.2.2.2.m1.1.1.2.3.1.cmml" xref="S6.F7.2.2.2.m1.1.1.2.3">superscript</csymbol><ci id="S6.F7.2.2.2.m1.1.1.2.3.2.cmml" xref="S6.F7.2.2.2.m1.1.1.2.3.2">ğ’Œ</ci><ci id="S6.F7.2.2.2.m1.1.1.2.3.3.cmml" xref="S6.F7.2.2.2.m1.1.1.2.3.3">ğ‘‡</ci></apply></apply><apply id="S6.F7.2.2.2.m1.1.1.3.cmml" xref="S6.F7.2.2.2.m1.1.1.3"><minus id="S6.F7.2.2.2.m1.1.1.3.1.cmml" xref="S6.F7.2.2.2.m1.1.1.3.1"></minus><apply id="S6.F7.2.2.2.m1.1.1.3.2.cmml" xref="S6.F7.2.2.2.m1.1.1.3.2"><ci id="S6.F7.2.2.2.m1.1.1.3.2.1.cmml" xref="S6.F7.2.2.2.m1.1.1.3.2.1">â‹…</ci><ci id="S6.F7.2.2.2.m1.1.1.3.2.2.cmml" xref="S6.F7.2.2.2.m1.1.1.3.2.2">ğ’’</ci><apply id="S6.F7.2.2.2.m1.1.1.3.2.3.cmml" xref="S6.F7.2.2.2.m1.1.1.3.2.3"><csymbol cd="ambiguous" id="S6.F7.2.2.2.m1.1.1.3.2.3.1.cmml" xref="S6.F7.2.2.2.m1.1.1.3.2.3">superscript</csymbol><apply id="S6.F7.2.2.2.m1.1.1.3.2.3.2.cmml" xref="S6.F7.2.2.2.m1.1.1.3.2.3.2"><ci id="S6.F7.2.2.2.m1.1.1.3.2.3.2.1.cmml" xref="S6.F7.2.2.2.m1.1.1.3.2.3.2.1">^</ci><ci id="S6.F7.2.2.2.m1.1.1.3.2.3.2.2.cmml" xref="S6.F7.2.2.2.m1.1.1.3.2.3.2.2">ğ’„</ci></apply><ci id="S6.F7.2.2.2.m1.1.1.3.2.3.3.cmml" xref="S6.F7.2.2.2.m1.1.1.3.2.3.3">ğ‘‡</ci></apply></apply><ci id="S6.F7.2.2.2.m1.1.1.3.3.cmml" xref="S6.F7.2.2.2.m1.1.1.3.3">ğ›½</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.F7.2.2.2.m1.1c">\bm{q}\cdot\bm{k}^{T}\geq\bm{q}\cdot\hat{\bm{c}}^{T}-\beta</annotation><annotation encoding="application/x-llamapun" id="S6.F7.2.2.2.m1.1d">bold_italic_q â‹… bold_italic_k start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT â‰¥ bold_italic_q â‹… over^ start_ARG bold_italic_c end_ARG start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT - italic_Î²</annotation></semantics></math>
</td>
</tr>
</table>{{< /table-caption >}}
> ğŸ”¼ Table 5 presents a comparison of the generation quality achieved by different sparse attention algorithms within the âˆ-Bench benchmark.  The algorithms evaluated include Full Attention (the baseline), InfLLM, StreamingLLM, Top100, Top2000, and DIPRS (the authors' proposed method).  For each algorithm, the generation quality is assessed across eight tasks, encompassing various aspects of language understanding and generation.  The evaluation metric used for each task is not explicitly specified in the caption but appears to be a normalized score from 0-100 across various test cases. The number of tokens used for attention computation varies across methods, reflecting different approaches to managing the context window. This number is reported as the sum of initial tokens, last tokens, and the retrieved tokens, showing how much context was effectively used for each algorithm during attention calculation.
> <details>
> <summary>read the caption</summary>
> Table 5. Generation quality of different sparse attention algorithms in âˆ\inftyâˆ-Bench. Each method used the number of [initial+last]+retrieved tokens for attention computation.
> </details>

</details>




### Full paper

{{< gallery >}}
<img src="https://ai-paper-reviewer.com/2504.10326/1.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.10326/2.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.10326/3.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.10326/4.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.10326/5.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.10326/6.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.10326/7.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.10326/8.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.10326/9.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.10326/10.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.10326/11.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.10326/12.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.10326/13.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.10326/14.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
{{< /gallery >}}