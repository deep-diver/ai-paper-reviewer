{"importance": "This work introduces a **novel test-time adaptation method, R2-T2, for multimodal MoE models, improving performance without retraining**. This approach tackles the critical issue of routing optimization, offering a new direction for enhancing LMMs' adaptability and robustness across diverse tasks, potentially unlocking greater generalization capabilities.", "summary": "R2-T2: Boost multimodal MoE performance by re-routing experts in test-time, no retraining needed!", "takeaways": ["R2-T2 enhances LMM performance by optimizing routing weights in test-time.", "The study introduces three effective R2-T2 strategies for expert re-routing.", "R2-T2 significantly improves performance on challenging multimodal benchmarks."], "tldr": "**Large Multimodal Models (LMMs) often underperform due to limitations in perceiving non-language modalities.** Recent efforts mitigate this by using Mixture-of-Experts (MoE) to provide diverse representations. However, the router, which mixes experts, can be suboptimal for test samples, hindering performance. This paper addresses this issue by improving routing weights during test time. \n\nThe authors propose \"Re-Routing in Test-Time (R2-T2)\", a method that optimizes routing weights locally. **R2-T2 enhances performance by locally optimizing routing weights towards those of correctly predicted neighboring samples**. They introduce three R2-T2 strategies with different optimization objectives, consistently improving LMM performance without training base-model parameters.", "affiliation": "Johns Hopkins University", "categories": {"main_category": "Multimodal Learning", "sub_category": "Multimodal Reasoning"}, "podcast_path": "2502.20395/podcast.wav"}