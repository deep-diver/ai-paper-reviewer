[{"figure_path": "https://arxiv.org/html/2504.02436/extracted/6332719/precase.png", "caption": "Figure 1: Examples of elements-to-video results from our proposed SkyReels-A2 model. Given reference with multiple images and textual prompt, our method can generate realistic and naturally composed videos while preserving specific identity consistent.", "description": "This figure showcases example outputs from the SkyReels-A2 model.  The model takes as input multiple reference images (e.g., of a character, an object, and a scene) and a textual description. The output is a video that combines these elements in a realistic and natural way, while maintaining the visual identity of each reference image.  For example, a character's face will appear consistent throughout the video, and the object will maintain its shape and texture.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2504.02436/extracted/6332719/framework.png", "caption": "Figure 2: Overview of Skyreels-A2 framework. Our approach initiates by encoding all reference images using two distinct branches. The first, termed the spatial feature branch (represented in red bottom arrow), leverages a fine-grained 3D VAE encoder to process per-composition images. The second, identified as the semantic feature branch (represented in red top arrow), utilizes a CLIP vision encoder followed by an MLP projection to encode semantic references. Subsequently, the spatial features are concatenated with the noised video tokens along the channel dimension before being passed through the diffusion transformer blocks. Meanwhile, the semantic features extracted from the reference images are incorporated into the diffusion transformers via supplementary cross-attention layers, ensuring that the semantic context is effectively integrated during diffusion.", "description": "SkyReels-A2 uses two encoding branches for reference images: a spatial branch (3D VAE encoder) for per-composition image processing, and a semantic branch (CLIP vision encoder + MLP projection) for semantic information extraction.  Spatial features are combined with noisy video tokens, and semantic features are integrated via cross-attention layers within diffusion transformers, ensuring effective semantic context integration during video generation.", "section": "2.2 Architecture"}, {"figure_path": "https://arxiv.org/html/2504.02436/extracted/6332719/datapipeline.png", "caption": "Figure 3: Data processing pipeline in SkyReels-A2.\nThe pipeline begins with preprocessing, where raw videos are filtered by resolution, labels, types, and sources, followed by temporal segmentation based on key-frames.\nNext, a proprietary multi-expert video captioning model generates both holistic descriptions for video clips and structured concept annotations.\nSubsequently, detection and segmentation models extract visual elements (e.g., humans, objects, environments).\nTo mitigate duplication, reference images are retrieved from other clips based on clip/facial similarity score.\nFurther refinement includes face detection and human parsing to obtain facial/attire elements.\nFinally, the extracted elements are matched with structured descriptions to form training triplets (visual elements, video clips, and textual captions).", "description": "This figure details the data processing pipeline used in the SkyReels-A2 model.  It starts with preprocessing raw videos, filtering them based on factors like resolution, labels, type, and source.  Then, keyframes are used for temporal segmentation. A proprietary multi-expert video captioning model creates both overall descriptions of video clips and structured annotations detailing specific concepts.  Object detection and segmentation models then extract visual elements (humans, objects, backgrounds). To prevent redundant data, reference images are selected from other clips using clip and facial similarity scores. Additional refinement steps involve facial detection and human parsing to extract facial and clothing elements.  Finally, the extracted elements are paired with their corresponding descriptions to create training triplets consisting of visual elements, video clips, and textual captions.", "section": "2 Methodology"}, {"figure_path": "https://arxiv.org/html/2504.02436/extracted/6332719/bench.png", "caption": "Figure 4: The dimensions covered in A2-Bench.  Our evaluation consider both automatic metrics and user study, meantime, it covers multiple perspectives that precisely reflects the quality of E2V task.", "description": "A2-Bench is a comprehensive evaluation benchmark for elements-to-video (E2V) tasks.  It assesses both the automatic metrics and user study results, considering multiple perspectives to accurately reflect the quality of generated videos.  The figure illustrates the dimensions evaluated by A2-Bench, including composition consistency (Character ID consistency, Object consistency, Background consistency), visual quality (Comprehensive image quality, Aesthetic quality, Motion smoothness, Dynamic degree), and prompt following (Image-Text similarity). Each dimension uses both automatic metrics and (optionally) user study scores to provide a holistic evaluation.", "section": "2.5 A2-Bench Evaluation"}, {"figure_path": "https://arxiv.org/html/2504.02436/extracted/6332719/user_study.png", "caption": "Figure 5: Consistency and video quality of user study results for elements-to-video generation. We can see that our SkyReels-A2 achieve comparable generative performance compared with top-tier closed source commercial video models.", "description": "Figure 5 presents a comprehensive comparison of SkyReels-A2's performance against top-tier commercial video generation models, as assessed through user studies.  The radar charts visually represent the consistency and video quality metrics from these studies.  Each axis represents a key aspect of video generation, such as character consistency, background consistency, or motion smoothness. The position of each model's point on each axis reflects its performance on that particular aspect, providing a multi-faceted view of the overall generation capabilities. The results demonstrate that SkyReels-A2 achieves comparable performance to the closed-source commercial models, indicating its strong potential as a competitive open-source alternative.", "section": "3.2 Quantitative Results"}, {"figure_path": "https://arxiv.org/html/2504.02436/extracted/6332719/results2.png", "caption": "Figure 6: Comparative results of elements-to-video generation with closed-source models. We can see that our SkyReels-A2 achieve achieves similar performance in composition and excels in the texture of light and shadow.", "description": "Figure 6 presents a comparison of video generation results between SkyReels-A2 and three leading commercial closed-source models (Pika, Vidu, and Keling).  Each row showcases a different elements-to-video scenario.  The results highlight SkyReels-A2's comparable performance in overall composition, while demonstrating its superiority in rendering realistic lighting and shadow effects. The figure demonstrates that SkyReels-A2 is competitive with leading commercial models, and its unique strength lies in the detail of light and shadow.", "section": "3 Experiments"}, {"figure_path": "https://arxiv.org/html/2504.02436/extracted/6332719/results1.png", "caption": "Figure 7: More generated results of SkyReels-A2. Our model has strong generalization ability and supports reference combinations of any subjects.", "description": "Figure 7 showcases various video generation results produced by the SkyReels-A2 model.  Each row presents a unique example consisting of input reference images (character, object, scene) and the resulting generated video frames. The figure highlights SkyReels-A2's capacity to synthesize videos from diverse combinations of input elements, demonstrating its robust generalization capabilities and ability to handle a wide range of composition scenarios.", "section": "3 Experiments"}]