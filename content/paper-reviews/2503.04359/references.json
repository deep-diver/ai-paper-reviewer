{"references": [{"fullname_first_author": "Yushi Bai", "paper_title": "LongBench: A bilingual, multitask benchmark for long context understanding", "publication_date": "2023-08-14", "reason": "This is one of the benchmark papers which is used for evaluating long-context understanding."}, {"fullname_first_author": "Jiawei Liu", "paper_title": "RepoQA: Evaluating long context code understanding", "publication_date": "2024-06-09", "reason": "This paper introduces a needle function retrieval task, which serves as a crucial component for evaluating the complex long code understanding ability of LCLMs."}, {"fullname_first_author": "Chenxin An", "paper_title": "L-Eval: Instituting standardized evaluation for long context language models", "publication_date": "2023-07-20", "reason": "This paper presents a benchmark for long context language models, which is used as a comparison to the current work, pointing out its limitations such as the use of synthetic long code."}, {"fullname_first_author": "Carlos E Jimenez", "paper_title": "SWE-bench: Can language models resolve real-world github issues?", "publication_date": "2023-10-04", "reason": "This paper is one of the benchmark papers used for evaluating long-context understanding based on downstream tasks."}, {"fullname_first_author": "Baptiste Roziere", "paper_title": "Code llama: Open foundation models for code", "publication_date": "2023-08-24", "reason": "This paper presents the CodeLlama model, which is among the LCLMs that the current paper evaluates."}]}