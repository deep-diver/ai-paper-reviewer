[{"figure_path": "https://arxiv.org/html/2503.04359/x1.png", "caption": "Figure 1: Examples of a synthetic long code with independent functions and a real-world long code with non-standalone functions. Dependencies are highlighted.", "description": "This figure showcases two examples of long code to illustrate the difference between synthetic and real-world codebases. The first example (a) is a synthetic long code constructed from independent functions, highlighting the simplicity of such an approach. The second example (b) is a real-world long code snippet, emphasizing the presence of non-standalone functions and the complex dependencies between them. The dependencies are visually highlighted within both code examples to emphasize the intricate relationships present in real-world code.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2503.04359/x2.png", "caption": "Figure 2: Four understanding aspects in LongCodeU.", "description": "This figure illustrates the four key aspects of long code understanding that are evaluated in the LONGCODEU benchmark. These aspects are: 1) Code Unit Perception (identifying individual code units such as functions); 2) Intra-Code Unit Understanding (analyzing the internal logic and semantics of a single code unit); 3) Inter-Code Unit Relation Understanding (analyzing relationships between different code units); and 4) Long Documentation Understanding (understanding and extracting relevant information from code documentation).  Each aspect is represented visually, showing how LONGCODEU aims to comprehensively assess a model's ability to understand long code.", "section": "3 LONGCODEU Benchmark"}, {"figure_path": "https://arxiv.org/html/2503.04359/x3.png", "caption": "Figure 3: Performance comparison across tasks and long code lengths on LongCodeU (grey blocks indicate unavailable configurations). The rate of performance degradation exhibits task-specific and model-specific patterns.", "description": "Figure 3 presents a detailed analysis of the performance of various Large Context Language Models (LCLMs) across different code understanding tasks within the LongCodeU benchmark.  The x-axis represents five different length ranges of code (0-8K, 8-16K, 16-32K, 32-64K, and 64-128K tokens), showcasing how model performance changes as code length increases. The y-axis lists nine different LCLMs, categorized into general models and code-specific models. Each cell in the heatmap displays the performance of a specific model on a particular task and code length range, represented by color intensity (higher intensity indicating better performance).  Missing data points are represented as grey blocks.  The figure highlights that the performance degradation as code length increases is inconsistent and varies across different models and tasks (task-specific and model-specific patterns).", "section": "5 Results and Analysis"}, {"figure_path": "https://arxiv.org/html/2503.04359/x4.png", "caption": "Figure 4: Assessing long code understanding vs. memorization on CU_SA (left) and DRA_T2 (right) tasks.", "description": "This figure compares the performance of large language models (LLMs) on two tasks: Code Unit Semantic Analysis (CU_SA) and Dependency Relation Analysis (DRA_T2), both focusing on long code understanding.  The left panel (CU_SA) shows the performance with and without the long code context, highlighting the model's ability to understand code semantics. The right panel (DRA_T2) similarly compares performance with and without context, assessing the model's ability to identify relationships between different code units within a long code sequence. The comparison reveals the extent to which LLMs rely on memorization versus genuine code understanding.", "section": "5 Results and Analysis"}, {"figure_path": "https://arxiv.org/html/2503.04359/x5.png", "caption": "Figure 5: The value of Kendall-Tau \u03c4\ud835\udf0f\\tauitalic_\u03c4 between our automatic metrics and human evaluation.", "description": "This figure displays the correlation between the automatically computed evaluation metrics and the human-evaluated scores for the LONGCODEU benchmark.  The Kendall-Tau correlation coefficient (\u03c4) is used to quantify the strength of the monotonic relationship between automatic and human judgments.  Higher Kendall-Tau values indicate a stronger correlation and higher reliability of the automated metrics. The figure shows a bar chart with Kendall-Tau values for each of the eight tasks, demonstrating the consistency and reliability of the automatic evaluation metrics.", "section": "3.3 Automatic Evaluation"}, {"figure_path": "https://arxiv.org/html/2503.04359/x6.png", "caption": "Figure 6: Performance comparison across long code lengths on tasks which can be measured by precision-based metrics. (grey blocks indicate unavailable configurations). The rate of performance degradation exhibits task-specific and model-specific patterns.", "description": "Figure 6 presents a detailed analysis of Large Language Model (LLM) performance across varying lengths of code, focusing on tasks where precision-based metrics are applicable.  The heatmaps visualize the performance (precision) of different LLMs on various tasks, categorized by code length ranges (0-8K, 8-16K, 16-32K, 32-64K, 64-128K tokens).  Grey blocks represent unavailable data points due to limitations in LLM context window sizes. The key finding highlighted is that the performance degradation patterns are not uniform across LLMs and tasks, showcasing task-specific and model-specific variations in how effectively LLMs handle increasingly long code inputs.", "section": "5 Results and Analysis"}, {"figure_path": "https://arxiv.org/html/2503.04359/x7.png", "caption": "Figure 7: For the dependency relation analysis task, the output of GPT-4o extracts a error code unit \u201cstream_async\" that is confusing to correct invoked function \u201cstream\".", "description": "Figure 7 demonstrates a failure case in the dependency relation analysis task within the LONGCODEU benchmark.  Specifically, when using GPT-4, the model incorrectly identifies the `stream_async` function as related to the `stream` function.  This highlights a limitation of current LCLMs: they may extract code units based on superficial similarities (like similar names) rather than a true understanding of the code's functional relationships and dependencies.  This type of error emphasizes the difficulty in accurately identifying relationships between code units within a larger codebase.", "section": "3.1 Tasks"}, {"figure_path": "https://arxiv.org/html/2503.04359/x8.png", "caption": "Figure 8: For the semantic relation extraction task, the output contains an error \u201cdelete\" function which has opposite functionalities to the anchor input, i.e., the given natural language description.", "description": "The figure displays an example from the Semantic Relation Extraction task within the LONGCODEU benchmark.  The task requires the model to identify code units semantically similar to a given input. The model incorrectly identifies the `delete` function as semantically similar to the anchor input, which is described in natural language.  The error highlights the challenge of accurately capturing semantic similarity in code, even for advanced models. The opposite functionalities of the `delete` function and the anchor input underscore the model's failure to correctly understand the semantic relationship between code units.", "section": "3.1 Tasks"}]