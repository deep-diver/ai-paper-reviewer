{"importance": "This paper is a **crucial step towards developing better software engineering tools**. It provides a comprehensive benchmark that can be used to evaluate and improve the long code understanding capabilities of LCLMs, fostering progress in areas like code generation and issue resolution, **driving future research & innovation**.", "summary": "LONGCODEU: A new benchmark to challenge & enhance long code understanding in language models for software engineering!", "takeaways": ["Current LCLMs struggle with long code understanding, especially when code length exceeds 32K tokens.", "Inter-code unit relation understanding is the most challenging aspect for LCLMs.", "The LONGCODEU benchmark provides valuable insights for optimizing LCLMs and driving advancements in software engineering."], "tldr": "Current long-context language models (LCLMs) hold promise for real-world software engineering applications but lack rigorous evaluation frameworks for long code understanding. Current benchmarks are limited by task diversity, use of synthetic code, and entangled tasks. To address this, this paper introduces LONGCODEU benchmark, designed to comprehensively evaluate LCLMs' capacity to understand real-world, dependency-rich, long code contexts. This benchmark contains comprehensive and practical tasks, extra-long code context, real-world repositories, and reduced data contamination.\n\nThe paper evaluates nine popular LCLMs using LONGCODEU. The experiments reveal key limitations in LCLMs' capabilities for long code understanding. LCLMs' performance drops dramatically beyond 32K tokens. Inter-code unit relation understanding is the most challenging aspect. The evaluation results provide insights for optimizing LCLMs. This can help for real-world applications of those technologies in software engineering.", "affiliation": "Peking University", "categories": {"main_category": "AI Applications", "sub_category": "Software Engineering"}, "podcast_path": "2503.04359/podcast.wav"}