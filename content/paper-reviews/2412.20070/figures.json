[{"figure_path": "https://arxiv.org/html/2412.20070/extracted/6099059/images/Train_and_test_CG2.jpg", "caption": "Figure 1: Examples of Compositional Generalization: The model is required to understand unseen images by recombining the fundamental elements it has learned.", "description": "This figure illustrates the concept of compositional generalization (CG) using examples of medical images.  The 'Train' column shows examples of images the model has been trained on, categorized by Modality (MRI or CT), Anatomical Area (brain or lung), and Task (cancer detection or not). The 'Test' column shows unseen combinations of these elements, such as a CT scan of a cat's brain (combining CT Modality, brain anatomical area, and an unseen task).  The idea is that if the model truly understands the elements, it should be able to generalize to unseen combinations of those elements.  The success of the model in classifying the test images demonstrates the power of compositional generalization.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2412.20070/extracted/6099059/images/mat6.jpg", "caption": "Figure 2: The process of integrating a vast amount of labeled medical image data to create Med-MAT.", "description": "This figure illustrates the creation of the Med-MAT dataset.  It starts with 106 individual medical image datasets. These datasets are categorized and grouped based on their MAT-Triplet (Modality, Anatomical Area, and Task), resulting in 53 subsets. Each subset consists of various modalities, anatomical areas, and medical tasks. The QA pairs construction is shown, where image-label pairs are converted into question-answer pairs suitable for MLLM training. This process ensures each sample in Med-MAT is clearly defined by its MAT-Triplet, enabling research on compositional generalization.", "section": "2 Med-MAT"}, {"figure_path": "https://arxiv.org/html/2412.20070/extracted/6099059/images/qaformat.jpg", "caption": "Figure 3: The QA formatting process of Med-MAT.", "description": "This figure illustrates the process of converting image-label pairs in Med-MAT into a visual question-answering (VQA) format suitable for training and evaluating multimodal large language models (MLLMs).  The process involves assigning multiple instructions to each subset of images, converting each image-label pair into a single-choice question with four options, and randomly selecting distractor options from other labels within the subset.  The integration of the ImageWikiQA dataset helps mitigate potential evaluation biases from varying option counts. The figure provides a detailed breakdown of the transformation from raw medical images and captions to structured questions and answers, which are essential for the MLLM training and testing.", "section": "2.2 A Pilot Study of Data Composition"}, {"figure_path": "https://arxiv.org/html/2412.20070/x5.png", "caption": "Figure 4: Accuracy results on the Target dataset for various models. \u2019All Related/Unrelated\u2019 models are trained on all the related or unrelated datasets of the Target Data. \u2019w/o Modality/Area/Task\u2019 are trained on All Related datasets but omit those sharing the same element as the Target Data, to intentionally disrupt CG. \u2019All Data\u2019 uses all available training sets. (Note: The Target Data is excluded from training to observe generalization.)", "description": "Figure 4 presents a comparative analysis of the generalization performance of several multimodal large language models (MLLMs) on unseen medical image data.  The models were trained using different strategies, and their accuracy on a target dataset is shown.  Specifically, the figure displays results for models trained on all related datasets, all unrelated datasets, all related datasets excluding those that share a Modality, Anatomical Area, or Task with the target dataset (to disrupt compositional generalization, or CG), and models trained on all available datasets.  The target data itself was excluded from training to assess genuine generalization ability. This visual comparison helps illustrate the impact of compositional generalization on the model's capacity to generalize.", "section": "3.2 Analysis of Scaling Experiment"}]