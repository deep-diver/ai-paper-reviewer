[{"figure_path": "https://arxiv.org/html/2501.10132/x1.png", "caption": "Figure 1: (a) Simple Function Calling. (b) Complex Function Calling with multi-step, constraints, parameter value reasoning, long parameter values and long context. Different colors correspond to the corresponding features marked in the figure.", "description": "Figure 1 presents a comparison between simple and complex function calling scenarios. (a) depicts a simple function call, where the user's request is directly translated into a single API call. (b) shows a complex function call, which involves multiple steps, constraints specified by the user, inferring parameter values based on implicit user information, handling long parameter values, and processing long contexts. Different colors highlight these features in the illustration.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2501.10132/x2.png", "caption": "Figure 2: Overview of the data collection process. (a) is the high-level process of data collection. (b) is the example of human correction process.(c) is the example of disambiguation process. The grey part is removed during annotation. A detailed annotation example is shown in Appendix A.1.", "description": "Figure 2 illustrates the multi-stage data collection process for ComplexFuncBench.  Panel (a) provides a high-level overview of the three stages: coarse generation, fine-grained annotation, and generalization. Panel (b) shows an example of the human correction process, focusing on refining queries, adjusting the order of function calls, and correcting errors in parameter values. Panel (c) details the disambiguation process used to eliminate ambiguity from API responses.  The greyed-out sections in (b) and (c) represent the parts removed during annotation to ensure a single valid function call path for each sample.  A more detailed example of the annotation process is provided in Appendix A.1.", "section": "2 ComplexFuncBench"}, {"figure_path": "https://arxiv.org/html/2501.10132/x3.png", "caption": "Figure 3: Overview of ComplexEval. Different colors represent different API response types. Color blue represents format error with specific error message. Color green represents correct function call with corresponding golden API response. Color red represents invalid function call with general error message.", "description": "ComplexEval is an automatic evaluation framework for assessing the quality of complex function calls generated by LLMs.  The figure provides a visual representation of the process.  It begins with a user query and the available functions, then shows how the model generates function calls. These calls are checked for correct format and compared to the 'golden' (correct) function call path using a three-part matching system: rule-based (exact match), response-based (comparing API responses), and LLM-based (using an LLM to judge equivalence). Different colors indicate different outcomes. Blue indicates format errors, green indicates a successful match with the golden standard, and red shows an invalid function call.", "section": "3 ComplexEval: Automatic Evaluation"}, {"figure_path": "https://arxiv.org/html/2501.10132/extracted/6138262/images/error_analysis.png", "caption": "Figure 4: Error type analysis for different models.", "description": "This bar chart displays the percentage breakdown of different error types for four selected Large Language Models (LLMs) during complex function calling.  The error types include:  `value_error` (incorrect parameter values), `stop_early` (premature termination of function calls), `hallucination` (generating non-existent parameters), `func_error` (calling the wrong function), and `param_missing` (missing parameters).  The chart allows for a comparison of the relative frequency of each error type across the four models, highlighting their respective strengths and weaknesses in handling different aspects of the complex function calling task.  The models are GPT-40, Claude-3.5-Sonnet, GLM-4-Long, and Qwen2.5-72B.", "section": "4.3.1 Error Analysis"}, {"figure_path": "https://arxiv.org/html/2501.10132/extracted/6138262/images/value_error_distribution.png", "caption": "Figure 5: Error rates for each parameter type of different models", "description": "This bar chart visualizes the error rates of different Large Language Models (LLMs) when dealing with various parameter types during complex function calls.  Each parameter type (e.g., filter, legs, token, slug, date, location, etc.) is represented on the x-axis. The y-axis shows the error rate (percentage) for each parameter type.  Multiple bars for each parameter type correspond to the performance of different LLMs (Claude-3.5-Sonnet, GPT-40, GLM-4-Long, and Qwen2.5-72B). The chart allows for a comparison of the models' accuracy across different parameter types, revealing which parameters pose the most significant challenge for each model.", "section": "4.3 Results Analysis"}, {"figure_path": "https://arxiv.org/html/2501.10132/x4.png", "caption": "Figure 6: Function calling steps distribution.", "description": "This figure displays the distribution of function call steps in a complex function calling task.  It shows the number of steps predicted by different models compared to the shortest (optimal) number of steps needed to accomplish the task. The distributions show that the models often require more steps than the shortest path, indicating areas where model efficiency can be improved. The chart facilitates a comparison of model performance in terms of planning and step efficiency during complex multi-step function calls.", "section": "4.3 Results Analysis"}, {"figure_path": "https://arxiv.org/html/2501.10132/x5.png", "caption": "Figure 7: An example for golden function call updating. Path on the left is the annotated shortest function call path with three steps.", "description": "This figure illustrates the process of updating the golden function call list during the evaluation of complex function calling.  The left side shows the annotated shortest function call path, serving as the ground truth, consisting of three steps.  The right side dynamically updates this list based on model predictions.  As the model generates function calls, the list is refined step-by-step, adding successful calls and discarding unsuccessful ones, reflecting a progressive refinement of the solution toward the correct path.", "section": "ComplexEval: Automatic Evaluation"}, {"figure_path": "https://arxiv.org/html/2501.10132/x6.png", "caption": "Figure 8: Prompt for Query Generation.", "description": "This figure shows the prompt used to instruct GPT-4 to generate queries for the ComplexFuncBench dataset.  The prompt emphasizes the creation of diverse and realistic queries involving multiple API calls and complex constraints. It provides a template and guidelines for constructing queries, specifying the required format and the need to avoid ambiguous or unrealistic parameters. The instructions highlight the importance of clear, detailed queries that can be solved using a series of API calls.", "section": "2.1.1 Stage 1: Coarse Generation"}, {"figure_path": "https://arxiv.org/html/2501.10132/x7.png", "caption": "Figure 9: Prompt for Query Generalization.", "description": "This prompt instructs a large language model (LLM) to generate variations of a given query by replacing specific pieces of information while maintaining the original sentence structure.  The goal is to create a diverse set of queries for a dataset used to evaluate complex function-calling capabilities in LLMs.  The prompt specifies which types of information can be modified (e.g., quantities, dates, locations, times, etc.) and provides concrete examples to guide the LLM.  It emphasizes the need to keep the essential structure of the query intact while introducing varied data.", "section": "2.1.3 Stage 3: Generalization"}, {"figure_path": "https://arxiv.org/html/2501.10132/x8.png", "caption": "Figure 10: Prompt for LLM-based Match.", "description": "This figure details the prompt used for evaluating the LLM's ability to perform LLM-based matching.  LLM-based matching assesses whether two function calls are equivalent, considering factors such as variations in language, parameter formats, and default values. The prompt provides instructions, examples, and a structured output format for the LLM to follow, ensuring consistency and accuracy in the evaluation.", "section": "3 ComplexEval: Automatic Evaluation"}, {"figure_path": "https://arxiv.org/html/2501.10132/x9.png", "caption": "Figure 11: Prompt for Completeness Evaluation.", "description": "This figure presents the prompt used to evaluate the completeness of model responses.  The prompt instructs evaluators to assess whether a model's response fully addresses all aspects of a user's query. It provides three scoring levels (0, 1, 2) based on the extent of the information provided, with examples illustrating each level and guidelines for providing justification for the given score.", "section": "4 Experiments"}, {"figure_path": "https://arxiv.org/html/2501.10132/x10.png", "caption": "Figure 12: Prompt for Correctness Evaluation.", "description": "This figure shows the prompt used to evaluate the correctness of the model's response based on the API response.  The prompt instructs the evaluator to score the response (0-2) based on whether it's entirely correct, partially correct, or completely incorrect when compared to the API's output. It also requires a justification of the score given.", "section": "3 ComplexEval: Automatic Evaluation"}]