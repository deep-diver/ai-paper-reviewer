[{"content": "| Position | 8 Latents | 16 Latents | 32 Latents | 64 Latents |\n|---|---|---|---|---|\n| 1 | -1.53% | -2.48% | -3.28% | -3.94% |\n| 2 | -1.67% | -2.41% | -3.15% | -3.70% |\n| 4 | -1.39% | -1.98% | -2.66% | -3.17% |\n| 8 | -1.22% | -1.56% | -2.11% | -2.61% |\n| 16 | -0.85% | -1.08% | -1.50% | -1.88% |\n| 32 | -0.55% | -0.64% | -0.88% | -1.20% |", "caption": "Table 1: Relative perplexity reduction (in %) achieved by augmented Gemma-2 2B models compared to the baseline, for various numbers of latents and prediction positions following latent augmentation. \"Position\" indicates the token position relative to the augmentation point (e.g., Position 1 is the immediately following token).", "description": "This table presents the perplexity reduction percentages achieved by Gemma-2 2B models augmented with different numbers of latent embeddings compared to the baseline model (no augmentation). It shows how much the perplexity decreases for predicting tokens at various positions following the latent embedding insertion.  Position 1 refers to the immediately following token, while higher position numbers represent tokens further ahead. The table helps to illustrate the impact of increasing latent embedding counts on the model's ability to predict tokens both in the short and long term.  The consistent reduction in perplexity across different positions and latent counts highlights the effectiveness of the proposed cache augmentation approach.", "section": "3.1. Perplexity Evaluation"}, {"content": "| Benchmark | Metric | Baseline | 4 Latents | 8 Latents | 16 Latents | 32 Latents | 64 Latents |\n|---|---|---|---|---|---|---|---| \n| MMLU | 5-shot | 52.00 | 52.45 (+0.45) | 52.24 (+0.24) | 52.34 (+0.34) | 54.61 (+2.61) | **56.70 (+4.70)** |\n| GSM8K | 8-shot | 21.38 | 22.67 (+1.29) | 23.12 (+1.74) | 24.72 (+3.34) | 26.76 (+5.38) | **31.43 (+10.05)** |\n| DROP | 3-shot, F1 | 53.69 | 54.64 (+0.95) | 54.91 (+1.23) | 56.23 (+2.55) | 57.37 (+3.68) | **57.77 (+4.08)** |\n| ARC-e | 0-shot | 80.56 | 81.52 (+0.97) | 81.57 (+1.01) | 83.12 (+2.57) | 83.04 (+2.48) | **83.67 (+3.11)** |\n| ARC-c | 0-shot | 50.26 | 51.28 (+1.02) | 52.39 (+2.13) | 53.24 (+2.99) | **54.44 (+4.18)** | **54.44 (+4.18)** |\n| MATH | 4-shot | 16.50 | 16.38 (-0.12) | 16.78 (+0.28) | 17.00 (+0.50) | 17.18 (+0.68) | **18.56 (+2.06)** |\n| Winogrande | 0-shot | 64.01 | 65.35 (+1.34) | 65.35 (+1.34) | 66.30 (+2.29) | 66.30 (+2.29) | **66.61 (+2.60)** |\n| PIQA | 0-shot | 78.18 | 78.62 (+0.44) | 78.67 (+0.49) | 78.94 (+0.76) | 78.94 (+0.76) | **79.00 (+0.82)** |\n| SIQA | 0-shot | 51.79 | 51.59 (-0.20) | 51.64 (-0.15) | 51.74 (-0.05) | **52.30 (+0.51)** | 52.00 (+0.20) |\n| HellaSwag | 0-shot | 73.77 | 74.41 (+0.64) | 74.41 (+0.64) | 74.82 (+1.05) | 75.04 (+1.27) | **75.31 (+1.54)** |\n| Boolq | 0-shot | 75.41 | 75.29 (-0.12) | 77.22 (+1.80) | **78.17 (+2.75)** | 77.03 (+1.62) | 76.91 (+1.50) |\n| MBPP | 3-shot | 30.40 | 29.00 (-1.40) | 31.60 (+1.20) | 31.20 (+0.80) | 31.40 (+1.00) | **31.80 (+1.40)** |\n| AGIEval | 3-5-shot | 31.71 | 32.18 (+0.47) | 30.04 (-1.67) | 31.32 (-0.38) | 32.78 (+1.07) | **33.85 (+2.14)** |\n| TriviaQA | 5-shot | 60.29 | 60.30 (+0.01) | 60.83 (+0.54) | 61.43 (+1.14) | 62.05 (+1.76) | **62.23 (+1.94)** |\n| NQ | 5-shot | 17.14 | 17.35 (+0.21) | 17.89 (+0.75) | 18.16 (+1.02) | 18.91 (+1.77) | **19.20 (+2.06)** |\n| HumanEval | pass@1 | 19.51 | 18.29 (-1.22) | 19.51 (+0.00) | 20.73 (+1.22) | 20.73 (+1.22) | **22.56 (+3.05)** |\n| BBH | 3-shot | 42.22 | 42.36 (+0.14) | 42.37 (+0.15) | 42.53 (+0.31) | 42.48 (+0.26) | **42.64 (+0.41)** |", "caption": "Table 2: Performance of baseline and augmented models across various benchmarks. Results are shown for the baseline (frozen Gemma-2 2B pretrained model) and the model augmented with a learned coprocessor using 4, 8, 16, 32, and 64 latent embeddings, respectively. Results are reported for zero/few-shot settings as indicated in the \u201cMetric\u201d column. Results are accuracy (in %) if not specified in the Metric column. Improvements over the baseline are shown in parentheses. In this setting, the coprocessor is called once, at the end of the prompt.", "description": "This table presents the performance comparison of different models on various benchmark tasks. The baseline model is a frozen Gemma-2 2B pretrained language model. Other models augment this baseline model with a learned coprocessor that uses 4, 8, 16, 32, or 64 latent embeddings. The table shows the performance of each model in zero-shot or few-shot settings for each benchmark.  Performance is measured by accuracy (in percentage) unless otherwise specified in the 'Metric' column. The improvement in performance compared to the baseline model is indicated in parentheses. For all models, the coprocessor is used only once at the end of the prompt.", "section": "3.2 Public Benchmark Evaluation"}, {"content": "| Method | Validation set perplexity (\u2193) | GSM8K 8-shot accuracy (\u2191) |\n|---|---|---|\n| Baseline Gemma-2 2B | 10.96 | 21.38 |\n| Pause Token | 11.63 | 22.37 |\n| Latent embeddings (Ours) | **10.60** | **26.76** |", "caption": "Table 3: Comparison between the baseline Gemma-2 2B model, the Pause Token method\u00a0(Goyal et\u00a0al., 2023) (using 32 embeddings), and our approach (also using 32 embeddings). Lower perplexity indicates better next token prediction. Higher accuracy indicates better performance on GSM8K.", "description": "This table compares the performance of three different methods on next-token prediction and the GSM8K benchmark.  The methods are: (1) a baseline Gemma-2 2B model; (2) the Pause Token method (Goyal et al., 2023), which uses trainable embeddings inserted between the input and output text; and (3) the proposed method in this paper, which uses a coprocessor to generate dynamic latent embeddings conditioned on the input text.  All three methods use 32 embeddings. Lower perplexity scores indicate better next-token prediction, while higher accuracy signifies better performance on the GSM8K benchmark.  The table demonstrates the effectiveness of the proposed approach.", "section": "3.3 Comparison with other baselines and variations"}, {"content": "| Baseline | 0-shot CoT | 16 Latents | 32 Latents |\n|---|---|---|---| \n| 21.38 | 23.20 | 24.72 | **26.76** |", "caption": "Table 4: Accuracy on GSM8K 8-shot for the baseline Gemma-2 2B model, zero-shot Chain-of-Thought (CoT) prompting, and our approach with 16 and 32 latent embeddings.", "description": "This table presents the accuracy results on the GSM8K benchmark's 8-shot setting.  It compares the performance of three different methods: the baseline frozen Gemma-2 2B model, zero-shot Chain-of-Thought (CoT) prompting, and the proposed cache augmentation approach with 16 and 32 latent embeddings. The table showcases how the proposed approach improves the reasoning capability of the model by augmenting its key-value cache with latent embeddings, leading to better performance than both the baseline and zero-shot CoT prompting.", "section": "3. Experiments"}, {"content": "| Method | GSM8K Accuracy |\n|---|---| \n| Baseline | 21.38 |\n| LoRA (Rank 64) | 23.35 |\n| LoRA (Rank 128) | 24.03 |\n| From Scratch Training | 25.78 |\n| Full Finetuning | **26.76** |", "caption": "Table 5: GSM8K accuracy comparison of different finetuning methods for the coprocessor, all using 32 latent embeddings. LoRA offers a memory-efficient alternative to full finetuning, achieving reasonable performance gains.", "description": "This table compares the performance of different fine-tuning methods for the coprocessor model on the GSM8K benchmark, focusing on memory efficiency. All methods use 32 latent embeddings.  The table showcases the accuracy achieved by the baseline (no fine-tuning), full fine-tuning of the pretrained Language Model (LLM), LoRA (Low-Rank Adaptation) finetuning with ranks 64 and 128, and training the coprocessor from scratch.  It demonstrates that LoRA provides a memory-efficient alternative to full finetuning, offering a reasonable performance improvement over the baseline, although full finetuning achieves the best performance. The table highlights the trade-off between memory efficiency and accuracy improvements.", "section": "3.3 Comparison with other baselines and variations"}, {"content": "| Baseline | 4 Ahead | 8 Ahead | 16 Ahead | 32 Ahead |\n|---|---|---|---|---|\n| 21.38 | 24.03 (+2.65) | 24.11 (+2.73) | **24.72** (+3.34) | 23.73 (+2.35) |", "caption": "Table 6: GSM8K accuracy for varying numbers of ahead tokens during coprocessor training. 16 ahead tokens achieves the highest accuracy (24.72%, +3.34% over the baseline of 21.38%). 16 latent embeddings are used for all these experiments.", "description": "This table presents the results of experiments evaluating the impact of varying the number of ahead tokens used during the training of the coprocessor on the GSM8K benchmark.  The coprocessor is a component of a larger system designed to augment a frozen language model's performance by generating latent embeddings that enhance the model's understanding of the input context. The number of ahead tokens refers to how many future tokens the model is trained to predict during coprocessor training.  The table shows that while using more ahead tokens may show some improvement, the highest accuracy (24.72%, a 3.34% improvement over the baseline of 21.38%) was achieved with 16 ahead tokens.  All experiments within this table used 16 latent embeddings.", "section": "3. Experiments"}, {"content": "| Benchmark | Metric | Baseline | 4 Latents | 8 Latents | 16 Latents | 32 Latents | 64 Latents |\n|---|---|---|---|---|---|---|---| \n| MMLU | 5-shot | 52.00 | 52.03 (+0.03) | 52.21 (+0.21) | 52.75 (+0.75) | 53.55 (+1.55) | 56.63 (+4.63) |\n| GSM8K | 8-shot | 21.38 | 22.52 (+1.14) | 22.59 (+1.21) | 24.41 (+3.03) | 25.78 (+4.40) | 29.80 (+8.42) |\n| ARC-e | 0-shot | 80.56 | 81.69 (+1.13) | 81.86 (+1.30) | 82.79 (+2.23) | 83.12 (+2.56) | 83.21 (+2.65) |\n| ARC-c | 0-shot | 50.26 | 51.71 (+1.45) | 52.22 (+1.96) | 52.47 (+2.21) | 54.27 (+4.01) | 53.24 (+2.98) |\n| MATH | 4-shot | 16.50 | 16.22 (-0.28) | 16.46 (-0.04) | 16.92 (+0.42) | 17.18 (+0.68) | 18.34 (+1.84) |\n| Winogrande | 0-shot | 64.01 | 65.19 (+1.18) | 65.98 (+1.97) | 66.54 (+2.53) | 66.69 (+2.68) | 67.25 (+3.24) |\n| PIQA | 0-shot | 78.18 | 78.13 (-0.05) | 79.00 (+0.82) | 79.16 (+0.98) | 79.27 (+1.09) | 79.22 (+1.04) |\n| SIQA | 0-shot | 51.79 | 51.94 (+0.15) | 51.64 (-0.15) | 51.84 (+0.05) | 51.94 (+0.15) | 51.89 (+0.10) |\n| HellaSwag | 0-shot | 73.77 | 74.37 (+0.60) | 74.68 (+0.91) | 74.82 (+1.05) | 74.89 (+1.12) | 75.18 (+1.41) |\n| Boolq | 0-shot | 75.41 | 75.66 (+0.25) | 76.94 (+1.53) | 76.97 (+1.56) | 77.80 (+2.39) | 77.46 (+2.05) |\n| MBPP | 3-shot | 30.40 | 30.40 (0.00) | 30.60 (+0.20) | 30.80 (+0.40) | 32.00 (+1.60) | 32.60 (+2.20) |\n| AGIEval | 3-5-shot | 31.71 | 32.52 (+0.81) | 32.22 (+0.51) | 31.92 (+0.21) | 32.78 (+1.07) | 32.35 (+0.64) |\n| TriviaQA | 5-shot | 60.29 | 60.53 (+0.24) | 60.95 (+0.66) | 61.45 (+1.16) | 61.93 (+1.64) | 62.62 (+2.33) |\n| NQ | 5-shot | 17.14 | 17.26 (+0.12) | 17.89 (+0.75) | 18.47 (+1.33) | 18.68 (+1.54) | 19.00 (+1.86) |\n| HumanEval | pass@1 | 19.51 | 18.29 (-1.22) | 18.90 (-0.61) | 20.73 (+1.22) | 19.51 (0.00) | 19.51 (0.00) |\n| BBH | 3-shot | 42.22 | 42.16 (-0.06) | 42.24 (+0.02) | 42.42 (+0.20) | 43.19 (+0.97) | 42.93 (+0.71) |", "caption": "Table 7: Performance of baseline and augmented models across various benchmarks with coprocessor training from scratch. Check Table\u00a02 for more detailed description.", "description": "This table presents the performance of the baseline frozen Gemma-2 2B model and augmented models across various reasoning benchmarks.  The augmented models use a coprocessor trained from scratch (i.e., with randomly initialized weights) rather than fine-tuned from the pretrained model, as in other experiments.  The table shows the performance improvement or reduction compared to the baseline model for each benchmark and varying numbers of latent embeddings used during the augmentation process.  The metrics vary depending on the benchmark, but typically include accuracy (%), perplexity, or F1 scores.  Refer to Table 2 for a more complete explanation of the metrics and benchmarks. The results demonstrate the coprocessor's performance when trained without using the pretrained LLM's weights as initialization.", "section": "3. Experiments"}]