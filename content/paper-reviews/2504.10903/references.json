{"references": [{"fullname_first_author": "Wei", "paper_title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models", "publication_date": "2022-01-01", "reason": "This paper introduces the chain-of-thought (CoT) prompting technique, which has become a fundamental approach in reasoning with large language models by generating a sequence of intermediate steps before reaching the final answer."}, {"fullname_first_author": "Guo", "paper_title": "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning", "publication_date": "2025-01-01", "reason": "This paper is the largest large language model and the baseline of the models of interest."}, {"fullname_first_author": "Han", "paper_title": "Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding", "publication_date": "2016-01-01", "reason": "Han et al.'s paper is very important because it introduces model compression, a technique that efficiently reduces model size redundancy in standard LLMs."}, {"fullname_first_author": "Hinton", "paper_title": "Distilling the Knowledge in a Neural Network", "publication_date": "2015-01-01", "reason": "This is a seminal work on knowledge distillation, a technique used to compress large models into smaller ones while retaining their reasoning abilities."}, {"fullname_first_author": "Sun", "paper_title": "Fast Best-of-N Decoding via Speculative Rejection", "publication_date": "2024-01-01", "reason": "This paper presents a method of speculative rejection during sampling and proactively discarding low-quality reasoning paths to provide better efficiency for test-time scaling strategies."}]}