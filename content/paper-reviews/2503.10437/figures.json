[{"figure_path": "https://arxiv.org/html/2503.10437/x2.png", "caption": "Figure 1: Visualization of the learned language features of our 4D LangSplat.\nWe observe that 4D LangSplat effectively learns dynamic semantic features that change over time, such as the gradual diffusion of coffee shown in the first two rows, and the \u201cchicken\u201d toggling between open and closed states in the latter two rows. Additionally, our semantic field captures consistent features for semantics that remain unchanged over time, with the clear object boundaries in the visualization demonstrating the precision of our semantic field.", "description": "This figure visualizes the language features learned by the 4D LangSplat model.  The top two rows showcase the model's ability to capture dynamic changes over time, such as the gradual diffusion of coffee. The bottom two rows demonstrate the model's capacity to track changes in object state, specifically the opening and closing of a chicken container. Importantly, the figure highlights that 4D LangSplat maintains consistent semantic features for aspects of the scene that do not change over time, illustrating the precision of its semantic field through clear object boundaries.", "section": "Abstract"}, {"figure_path": "https://arxiv.org/html/2503.10437/x3.png", "caption": "Figure 2: The framework of constructing a time-varying semantic field in 4D LangSplat. We first use multimodal object-wise prompting to convert a video into pixel-aligned object-level caption features. Then, we learn a 4D language field with a status deformable network.", "description": "This figure illustrates the architecture of 4D LangSplat, a method for creating a time-varying semantic field.  The process begins with a video input that is segmented into individual objects using the Segment Anything Model (SAM). Multimodal object-wise prompting is then applied, combining visual and textual prompts to guide a Multimodal Large Language Model (MLLM) in generating detailed, temporally consistent captions for each object.  These captions are encoded using a Large Language Model (LLM) into sentence embeddings, which act as pixel-aligned, object-level feature supervision.  A status deformable network models the smooth transitions between object states over time.  Finally, these features are integrated into a 4D language field using 4D Gaussian splatting.", "section": "3.2 4D LangSplat Framework"}, {"figure_path": "https://arxiv.org/html/2503.10437/x4.png", "caption": "Table 3: Comparisons of Visual prompts.", "description": "This table presents a quantitative comparison of the effectiveness of different visual prompting techniques used in the 4D LangSplat model.  The visual prompts are designed to guide a Multimodal Large Language Model (MLLM) in generating accurate and detailed captions for objects in video frames.  The table shows the average cosine similarity scores (Asim) achieved when using different visual prompts (Blur, Gray, Contour) in combination with image and video prompts. The Asim score reflects how well the generated captions align with the actual object features and their temporal dynamics.", "section": "3.3 Multimodal Object-Wise Video Prompting"}, {"figure_path": "https://arxiv.org/html/2503.10437/x5.png", "caption": "Table 4: Comparisons of Text prompts.", "description": "This table presents ablation study results, comparing the performance of different text prompt strategies used in the multimodal object-wise video prompting method. It shows how various combinations of visual and textual prompts affect the quality of captions generated by the MLLM. The metrics used to evaluate the quality are the cosine similarity scores between generated captions and query features (both positive and negative samples). The results demonstrate how different prompting techniques influence the ability of the MLLM to generate precise, temporally consistent, and contextually relevant captions for each object in a video frame.", "section": "3.3. Multimodal Object-Wise Video Prompting"}]