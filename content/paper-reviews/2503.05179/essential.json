{"importance": "This paper is important because it tackles the **growing verbosity of LLM reasoning** directly. SoT's cognitive-inspired approach offers a promising direction for creating more efficient AI systems, which is particularly crucial for **resource-constrained applications**. It also opens new research avenues such as how to combine SoT with other reasoning approaches.", "summary": "Sketch-of-Thought(SoT) reduces LLM token usage by up to 76% while maintaining (or improving) accuracy via cognitive-inspired sketching.", "takeaways": ["Sketch-of-Thought (SoT) offers a new prompting method that leverages cognitive science principles to reduce token usage in LLM reasoning.", "SoT combines Conceptual Chaining, Chunked Symbolism, and Expert Lexicons, dynamically selected by a lightweight router model.", "Evaluations show SoT significantly reduces token usage (up to 76%) with minimal accuracy impact and even improves certain tasks."], "tldr": "Large language models excel at reasoning, but often use too many tokens, increasing computational cost. This paper introduces **Sketch-of-Thought (SoT)**, a new prompting framework to make LLM reasoning more efficient. It combines cognitive-inspired methods with linguistic constraints to minimize token usage while maintaining accuracy. \n\nSoT has three reasoning paradigms: **Conceptual Chaining, Chunked Symbolism, and Expert Lexicons**. A lightweight router model selects the best approach for each task. Evaluations across 15 datasets (including multilingual and multimodal) show SoT reduces token usage by up to 76% with minimal accuracy impact and sometimes even improves accuracy.", "affiliation": "KAIST", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}, "podcast_path": "2503.05179/podcast.wav"}