[{"figure_path": "https://arxiv.org/html/2504.13157/x1.png", "caption": "Figure 1: First row: Examples of our generated cross-view (aerial-ground) geometry data, including co-registered pseudo-synthetic (i.e., mesh-rendered) aerial and real ground-level images, with corresponding depth maps, point clouds, and camera intrinsics/extrinsics in a unified coordinate system, for a variety of scenes. Second row: Leveraging such data curated over 137 landmarks and 132K geo-registered images, we show significant improvements in learning-based methods on real unseen ground-aerial scenarios across two representative tasks: 1) multi-view geometry prediction using DUSt3R\u00a0[65] finetuned on our data, and 2) novel view synthesis from a single image conditioned on a target pose by fine-tuning ZeroNVS\u00a0[46] that was originally trained on MegaScenes\u00a0[61].", "description": "Figure 1 showcases examples of generated aerial-ground geometry data.  The first row displays co-registered pseudo-synthetic (mesh-rendered) aerial images and real ground-level images from various scenes. Each example includes depth maps, point clouds, and camera parameters (intrinsics and extrinsics) in a unified coordinate system. The second row demonstrates the effectiveness of the generated dataset.  Using this dataset (137 landmarks, 132K images), two learning-based methods are significantly improved on real, unseen aerial-ground scenarios.  Specifically, multi-view geometry prediction is enhanced using a fine-tuned DUSt3R model, and novel view synthesis is improved by fine-tuning a ZeroNVS model originally trained on MegaScenes. ", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2504.13157/x2.png", "caption": "Figure 2: Overview of the data generation framework. To address the challenges of ground-aerial camera registration and novel-view synthesis, we propose a flexible framework combining pseudo-synthetic renderings from 3D city-wide meshes (e.g. Google Earth) with real, ground-level images (e.g. MegaDepth\u00a0[29]). The pseudo-synthetic data is captured at varying altitudes, while the real, crowd-sourced images help improve visual fidelity especially for ground-level images where mesh-based renderings lack detail. The pipeline generates pseudo-synthetic images from different altitudes, co-registers them with real images, and aligns ground-level images with aerial data for 3D reconstruction. This hybrid dataset of real and pseudo-synthetic images provides geometric supervision that helps improve performance on downstream tasks such as ground-aerial camera registration and novel view synthesis, particularly in ground-aerial settings.", "description": "This figure illustrates the process of creating a hybrid dataset for aerial-ground 3D reconstruction and view synthesis.  It starts by using 3D city models (like those from Google Earth) to generate synthetic aerial images at various altitudes. These synthetic images are then combined with real, ground-level images from existing datasets (such as MegaDepth) to create a more comprehensive dataset that bridges the gap between real and synthetic data. The process involves co-registering the real and synthetic images and aligning ground-level images with the aerial data to enable 3D reconstruction. This hybrid dataset improves the accuracy of downstream tasks like camera registration and view synthesis, especially in scenarios with large viewpoint differences (such as those between ground and aerial perspectives).", "section": "3. Generating Aerial-Ground 3D Data"}, {"figure_path": "https://arxiv.org/html/2504.13157/extracted/6370271/images/training_lv.jpg", "caption": "Figure 3: Feature matching between real and pseudo-synthetic images. The pseudo-synthetic rendering has a noticeable domain gap compared to the real MegaDepth image (e.g., no transients, simplistic lighting) but still enables reliable feature matching\u00a0[47] to register real images into the pseudo-synthetic reconstruction.", "description": "This figure showcases the comparison between real-world images from the MegaDepth dataset and pseudo-synthetic images generated from a 3D model.  Despite a clear difference in visual quality (the pseudo-synthetic images lack details like transients and realistic lighting), reliable feature matching techniques successfully align these disparate image types. This alignment is crucial for integrating real-world data into the pseudo-synthetic 3D reconstruction, ultimately enhancing the accuracy and realism of the final model. The successful feature matching demonstrates that the domain gap between real and synthetic data, while visually apparent, does not prevent accurate geometric registration.", "section": "3. Generating Aerial-Ground 3D Data"}, {"figure_path": "https://arxiv.org/html/2504.13157/extracted/6370271/images/result_pairwise.jpg", "caption": "Figure 4: AerialMegaDepth data (top: MegaDepth, bottom: Google Earth) features diverse viewpoints & lighting conditions.", "description": "Figure 4 shows a comparison of image data from two sources: MegaDepth (top) and Google Earth (bottom).  The images illustrate the diversity in viewpoints and lighting conditions found within the AerialMegaDepth dataset. MegaDepth provides real-world street-level images, while Google Earth offers pseudo-synthetic aerial images rendered from 3D models. This combined dataset allows for training models that can handle a broader range of perspectives and lighting scenarios than datasets using only one data source. The visual differences emphasize the hybrid nature of the AerialMegaDepth dataset, which incorporates both real and synthetic data to overcome limitations encountered in using each source independently.", "section": "3. Generating Aerial-Ground 3D Data"}, {"figure_path": "https://arxiv.org/html/2504.13157/extracted/6370271/images/matches_mast3r.jpg", "caption": "Figure 5: Zero-shot ground-aerial camera and geometry prediction results. Given two input images, one aerial and one ground, we compare the performance of the baseline DUSt3R\u00a0[65] with the model fine-tuned on our varying-altitude data. The results demonstrate significant improvements over the baseline in unseen, challenging ground-aerial scenarios, showing the effectiveness of fine-tuning DUSt3R\u00a0[65] with our data. Additionally, the last column presents qualitative results on a challenging ground-aerial pair from the WxBS\u00a0[39] dataset, which involves significant viewpoint change.", "description": "This figure showcases the effectiveness of fine-tuning the DUSt3R model with the AerialMegaDepth dataset.  It compares the performance of the baseline DUSt3R model with a model fine-tuned using the dataset on a zero-shot ground-aerial camera and geometry prediction task.  Given pairs of one aerial image and one ground image as input, the figure demonstrates significant improvements achieved by the fine-tuned model in challenging, unseen ground-aerial scenarios. The last column displays qualitative results from the WxBS dataset, a particularly difficult case involving a substantial viewpoint change.", "section": "5. Experiments: 5.1. Multiview Pose Estimation and Reconstruction"}, {"figure_path": "https://arxiv.org/html/2504.13157/extracted/6370271/images/result_1aNg.jpg", "caption": "Figure 6: Challenging ground-aerial feature matching. Fine-tuned MASt3R\u00a0[26] achieves accurate and robust feature matching across ground-aerial pairs with extreme viewpoint changes (correspondences extracted via reciprocal nearest neighbor from MASt3R\u2019s local feature maps). This highlights the effectiveness of our AerialMegaDepth data in improving matching performance.", "description": "This figure demonstrates the improved feature matching capabilities of the MASt3R algorithm after fine-tuning with the AerialMegaDepth dataset.  Specifically, it showcases accurate and robust matching results even between ground-level and aerial images exhibiting extreme viewpoint differences.  The correspondences are determined using the reciprocal nearest neighbor approach based on MASt3R's locally computed feature maps. The success of the matching highlights the dataset's effectiveness in enhancing the algorithm's ability to handle significant viewpoint variations.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2504.13157/extracted/6370271/images/nvs_result_real.png", "caption": "Figure 7: 3D reconstruction from one aerial and four ground images with virtually no overlap. We use the global alignment process of DUSt3R\u00a0[65] to merge pointmaps predictions. Despite the lack of overlap among the ground images, we find that incorporating a reference aerial image can effectively serve as a \u201cmap\u201d, significantly improving pose estimation accuracy when fine-tuned on our cross-view data.", "description": "This figure demonstrates 3D reconstruction using a novel method that incorporates both aerial and ground images, even when the ground images have minimal overlap. The global alignment process of DUSt3R is used to combine point cloud predictions. The results show that including an aerial reference image significantly improves the accuracy of pose estimation, especially when the model is fine-tuned on data containing both aerial and ground views.  This is because the aerial image acts as a 'map', providing a broader context for aligning the ground-level perspectives.", "section": "5.1. Multiview Pose Estimation and Reconstruction"}, {"figure_path": "https://arxiv.org/html/2504.13157/x3.png", "caption": "Figure 8: Results of extreme viewpoint change in novel-view synthesis with ZeroNVS\u00a0[46] finetuned on MegaScenes\u00a0[61] (ZeroNVS MS) & additionally finetuned on our data. Though by no means perfect, note the big improvement in visual quality and viewpoint accuracy.", "description": "Figure 8 displays the results of novel-view synthesis using the ZeroNVS model.  Two versions of the model are compared: one fine-tuned only on MegaScenes [61] (a large-scale dataset for novel view synthesis), and another fine-tuned on both MegaScenes and the authors' new AerialMegaDepth dataset.  The goal is to synthesize a realistic ground-level view from an aerial input image with extreme viewpoint differences. While the results aren't perfect, the figure demonstrates the significant improvement in both visual quality (more realistic ground views) and viewpoint accuracy (synthesized views more closely match the desired perspective) achieved by including the AerialMegaDepth dataset in the model training.", "section": "5.2 Novel View Synthesis"}]