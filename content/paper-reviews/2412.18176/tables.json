[{"content": "| Dataset | Amazon | PixelRec | MovieLens |\n|---|---|---|---|\n| # User | 993,087 | 50,000 | 6,040 |\n| # Item | 301,312 | 82,864 | 3,706 |\n| # Interaction | 8,813,442 | 989,476 | 1,000,209 |", "caption": "Table 1: Statistics of Datasets.", "description": "This table presents a summary of the statistics for three datasets used in the paper's experiments: Amazon, PixelRec, and MovieLens.  For each dataset, it shows the number of users, the number of items, and the total number of user-item interactions.  This information is crucial for understanding the scale and characteristics of the data used to evaluate the proposed recommendation model.", "section": "4.1 Experimental Settings"}, {"content": "| Methods | Amazon* N@10 | Amazon* N@20 | Amazon* R@10 | Amazon* R@20 | PixelRec* N@10 | PixelRec* N@20 | PixelRec* R@10 | PixelRec* R@20 | Movielens* N@10 | Movielens* N@20 | Movielens* R@10 | Movielens* R@20 |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| **Traditional** |  |  |  |  |  |  |  |  |  |  |  |  |\n| FPMC | 0.1037 | 0.1059 | 0.1152 | 0.1232 | 0.0107 | 0.0129 | 0.0191 | 0.0290 | 0.0907 | 0.1129 | 0.1708 | 0.2756 |\n| GRU4Rec | 0.1029 | 0.1054 | 0.1107 | 0.1190 | 0.0109 | 0.0127 | 0.0189 | 0.0284 | 0.0828 | 0.1081 | 0.1657 | 0.2664 |\n| SASRec | 0.1080 | 0.1105 | 0.1188 | 0.1281 | 0.0131 | 0.0149 | 0.0207 | 0.0311 | 0.1116 | 0.1395 | 0.2137 | 0.3245 |\n| DuoRec | 0.1281 | 0.1342 | 0.1406 | 0.1616 | 0.0147 | 0.0181 | 0.0241 | 0.0362 | 0.1530 | 0.1790 | 0.2704 | 0.3738 |\n| **Content-based** |  |  |  |  |  |  |  |  |  |  |  |  |\n| SASRec<sub>Bert</sub> | 0.1116 | 0.1130 | 0.1275 | 0.1365 | 0.0131 | 0.0161 | 0.0238 | 0.0357 | 0.1172 | 0.1465 | 0.2244 | 0.3407 |\n| SASRec<sub>Vit</sub> | 0.1142 | 0.1187 | 0.1237 | 0.1373 | 0.0126 | 0.0155 | 0.0211 | 0.0317 | 0.1204 | 0.1499 | 0.2295 | 0.3481 |\n| SASRec<sub>Bert+Vit</sub> | 0.1164 | 0.1179 | 0.1308 | 0.1437 | 0.0136 | 0.0167 | 0.0210 | 0.0315 | 0.1258 | 0.1567 | 0.2382 | 0.3599 |\n| **LLM-based** |  |  |  |  |  |  |  |  |  |  |  |  |\n| CoLLM | 0.1298 | 0.1344 | 0.1388 | 0.1602 | 0.0173 | 0.0213 | 0.0296 | 0.0444 | 0.1658 | 0.1880 | 0.2895 | 0.4058 |\n| HLLM | 0.1285 | 0.1351 | 0.1457 | 0.1668 | 0.0189 | 0.0232 | 0.0352 | 0.0528 | 0.1652 | 0.1933 | 0.2920 | 0.4037 |\n| **Ours** |  |  |  |  |  |  |  |  |  |  |  |  |\n| Molar | **0.1407** | **01478** | **0.1580** | **0.1773** | **0.0197** | **0.0242** | **0.0359** | **0.0539** | **0.1768** | **0.2068** | **0.3124** | **0.4320** |", "caption": "Table 2: Performance comparison of Molar with baseline models. The underlined values indicate the best and second-best results across all models. The abbreviations N and R represent Normalized Discounted Cumulative Gain (NDCG) and Recall, respectively. Statistically significant improvements are marked with * (p\ud835\udc5dpitalic_p-value <<0.05much-less-thanabsent0.05<<0.05< < 0.05). Overall, Molar consistently achieves superior performance across all datasets, demonstrating its effectiveness in leveraging multimodal and collaborative filtering features.", "description": "Table 2 presents a performance comparison of the proposed Molar model against various baseline models for sequential recommendation.  The models are evaluated on three datasets using two metrics: Normalized Discounted Cumulative Gain (NDCG@K) and Recall@K, with K=10 and K=20. Underlined values highlight the top two performing models for each metric and dataset.  Statistically significant improvements of Molar over the baselines (p<0.05) are marked with an asterisk. The results consistently demonstrate Molar's superior performance across all datasets, showcasing the benefits of its multimodal and collaborative filtering approach.", "section": "4 Experiments"}, {"content": "| Method | N@10 | N@20 | N@50 | R@10 | R@20 | R@50 |\n|---|---|---|---|---|---|---|\n| Image Only | 0.0182 | 0.0217 | 0.0292 | 0.0329 | 0.0512 | 0.0858 |\n| Text Only | 0.0181 | 0.0228 | 0.0296 | 0.0335 | 0.0514 | 0.0860 |\n| Image + Text | **0.0197** | **0.0242** | **0.0313** | **0.0359** | **0.0539** | **0.0895** |", "caption": "Table 3: Performance comparison with different modality inputs. The table highlights the impact of using Image Only, Text Only, and Image + Text inputs for sequential recommendation tasks. The combined modality (Image + Text) consistently achieves the best performance across all evaluation metrics, demonstrating the advantage of multimodal integration.", "description": "This table presents a comparison of the performance of a sequential recommendation model using different input modalities: Image Only, Text Only, and a combination of Image + Text. The results show that incorporating both image and text data consistently yields the best performance across various evaluation metrics.  This highlights the significant advantage of integrating multimodal information (images and text) in improving the accuracy and effectiveness of sequential recommendation.", "section": "4.4 Impact of Input Data Modality (RQ3)"}, {"content": "| Post-Alignment Model | N@10 | N@20 | R@10 | R@20 |\n|---|---|---|---|---|\n| FPMC | 0.0194 | 0.0237 | 0.0347 | 0.0527 |\n| GRU4Rec | 0.0195 | 0.0240 | 0.0360 | 0.0531 |\n| SASRec | 0.0197 | 0.0242 | 0.0359 | 0.0539 |\n| DuoRec | 0.0200 | 0.0253 | 0.0371 | 0.0569 |", "caption": "Table 4: Performance comparison of different post-alignment models for contrastive learning. Results show that stronger sequential models yield better performance, demonstrating the benefits of post-alignment.", "description": "This table presents a comparison of different post-alignment models used in contrastive learning within the Molar framework.  It shows how the choice of underlying sequential recommendation model (e.g., FPMC, GRU4Rec, SASRec, DuoRec) affects the performance of the post-alignment process. The results demonstrate that stronger sequential recommendation models lead to better performance, highlighting the effectiveness of this post-alignment contrastive learning technique in improving recommendation accuracy.", "section": "4 Experiments"}, {"content": "|                   | N@10   | N@20   | N@50   | R@10   | R@20   | R@50   |\n|-------------------|--------|--------|--------|--------|--------|--------|\n| *Full Model*      |        |        |        |        |        |        |\n| **Molar**         | **0.0197** | **0.0242** | **0.0313** | **0.0359** | **0.0539** | **0.0895** |\n| *Fine-Tuning Data* |        |        |        |        |        |        |\n| *w/o IT*          | 0.0186 | 0.0227 | 0.0298 | 0.0339 | 0.0512 | 0.0841 |\n| *w/o SA*          | 0.0189 | 0.0237 | 0.0302 | 0.0349 | 0.0528 | 0.0859 |\n| *w/o UB*          | 0.0183 | 0.0220 | 0.0287 | 0.0324 | 0.0495 | 0.0828 |\n| *w/o ALL*         | 0.0180 | 0.0219 | 0.0285 | 0.0313 | 0.0479 | 0.0808 |\n| *Post-Alignment*  |        |        |        |        |        |        |\n| *w/o CL*          | 0.0182 | 0.0225 | 0.0294 | 0.0325 | 0.0496 | 0.0819 |", "caption": "Table 5: Ablation study on the PixelRec dataset. The table evaluates the impact of different fine-tuning data components (Image-Text, Structured Attributes, User Behavior) and the post-alignment module. Results demonstrate that using all fine-tuning components achieves optimal performance, while removing any single component degrades performance. The post-alignment contrastive learning module is shown to be critical for maintaining high recommendation accuracy.", "description": "This ablation study investigates the individual contributions of different components within the Molar model using the PixelRec dataset. It assesses the impact of removing each of the three fine-tuning data components (Image-Text, Structured Attributes, User Behavior) on the model's performance, individually and in combination.  Additionally, it evaluates the criticality of the post-alignment contrastive learning module. The results demonstrate the importance of all components for achieving optimal recommendation accuracy; removing any single component leads to a performance decrease.  The post-alignment module is also shown to be essential for maintaining high recommendation accuracy.", "section": "4 Experiments"}, {"content": "| MLLM Backbone | Training Type | N@10 | N@20 | R@10 | R@20 |\n|---|---|---|---|---|---| \n| Qwen2-VL-2B | Full-tuning | 0.0197 | 0.0242 | 0.0359 | 0.0539 |\n| InternVL2.5-2B<sup id=\"fnref:6\">[6]</sup> | Full-tuning | 0.0191 | 0.0237 | 0.0349 | 0.0521 |\n| deepseek-vl-1.3b<sup id=\"fnref:7\">[7]</sup> | Full-tuning | 0.0183 | 0.0225 | 0.0334 | 0.0499 |\n| Qwen2-VL-7B | LoRA | 0.0200 | 0.0251 | 0.0369 | 0.0555 |\n| Llama-3.2-11B-Vision<sup id=\"fnref:8\">[8]</sup> | LoRA | 0.0194 | 0.0249 | 0.0357 | 0.0542 |\n<br> \n<div id=\"footnotes\">\n<hr />\n<ol>\n<li id=\"fn:6\"><p>https://huggingface.co/OpenGVLab/InternVL2_5-2B <a href=\"#fnref:6\" rev=\"footnote\">&#8617;</a></p></li>\n<li id=\"fn:7\"><p>https://huggingface.co/deepseek-ai/deepseek-vl-1.3b-chat <a href=\"#fnref:7\" rev=\"footnote\">&#8617;</a></p></li>\n<li id=\"fn:8\"><p>https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct <a href=\"#fnref:8\" rev=\"footnote\">&#8617;</a></p></li>\n</ol>\n</div>", "caption": "Table 6: Comparison of Different MLLM Backbone.", "description": "This table presents a comparison of the performance achieved by Molar using different Multimodal Large Language Model (MLLM) backbones. It shows the results obtained using various MLLMs with different parameter sizes and training methods (full-tuning and LoRA), evaluating the performance using metrics such as NDCG@10, NDCG@20, Recall@10 and Recall@20. The goal is to analyze how the choice of MLLM backbone and training strategy affects the overall performance of the Molar framework.", "section": "A Impact of Different MLLM Backbone"}]