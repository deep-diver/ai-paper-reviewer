[{"figure_path": "https://arxiv.org/html/2501.08326/x2.png", "caption": "Figure 1: Representative demo examples of Omni-RGPT. We introduce a unified multimodal large language model that integrates region-level understanding for both images and videos. Given user-defined localized region inputs (boxes or masks) accompanied by a corresponding text prompt, Omni-RGPT generates responses tailored to the visual context of each region for both images and videos.222Our model refers the girl in the video as \u201cthey\u201d as a singular term to alleviate ethical issues instead of \u201cshe\u201d, similar to ChatGPT\u00a0[47].", "description": "This figure showcases the capabilities of Omni-RGPT, a unified multimodal large language model, in understanding images and videos at the region level.  It demonstrates how Omni-RGPT processes user-defined region inputs (bounding boxes or masks) along with corresponding text prompts to generate region-specific responses.  The examples illustrate both image and video scenarios, highlighting Omni-RGPT's ability to provide detailed captions and answer questions about specific regions within the visual content. The use of 'they' instead of 'she' to refer to the girl in the video example is noted as a deliberate choice to mitigate potential ethical concerns.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2501.08326/x3.png", "caption": "Figure 2: \nMethod comparison. (a) RoI-based methods generate visual region prompts using RoI-aligned visual features, potentially leading to temporal drift in the visual features of the target object in the video domain. (b) In contrast, our Token Mark is assigned to the corresponding region, preserving a consistent spatio-temporal target reference.", "description": "This figure compares two different methods for generating visual region prompts in multimodal large language models (MLLMs), focusing on the issue of temporal drift in video.  (a) shows the traditional RoI (Region of Interest)-based approach, which extracts visual features from a region of interest within each frame of a video. Because the RoI features are extracted independently for each frame, the representation of the target object might change inconsistently over time (temporal drift), leading to inaccurate or inconsistent understanding. (b) illustrates the proposed Omni-RGPT method using 'Token Marks'.  Here, a unique token is assigned to the target region across all video frames.  This ensures a consistent representation of the target object throughout the video, preventing temporal drift and enhancing the model's understanding.", "section": "3. Omni-RGPT"}, {"figure_path": "https://arxiv.org/html/2501.08326/x4.png", "caption": "Figure 3: \n(a) Overview: Omni-RGPT enables region-level understanding across image and video inputs. Given region prompts (e.g. boxes or masks) in a single image or the initial frame of a video, we assign Token Mark \u2014 a set of vectors serving as spatio-temporal region indicators \u2014 to the region.\nThese vectors are embedded into the spatial region localized by the region prompt and directly injected into both visual and text prompts to indicate the target. (b) Auxiliary Head: We further introduce Temporal Region Guide Head to achieve robust region understanding in videos without relying on tracklet prompts. Building on Token Mark\u2019s consistent representation of target objects across frames, this auxiliary task classifies the target Token Mark for visual tokens in subsequent frames.", "description": "Figure 3 illustrates the Omni-RGPT architecture for region-level understanding in images and videos.  Panel (a) shows the core process: region prompts (bounding boxes or masks) are used to identify target regions.  A 'Token Mark', a unique vector representation, is then assigned to each target region and embedded into both the visual features (extracted by a visual encoder) and the text prompt. This direct embedding allows the large language model (LLM) to directly connect visual and textual information about the target regions. Panel (b) details the 'Temporal Region Guide Head', an auxiliary head used only during training for video inputs. This head leverages the consistent Token Marks across frames to guide the LLM in understanding target regions even without explicit object tracking, addressing temporal drift issues.", "section": "3. Omni-RGPT"}, {"figure_path": "https://arxiv.org/html/2501.08326/x5.png", "caption": "Figure 4: \nOverview of our instruction sample generation pipeline. From a video with region masklets and nouns, the region-level captions, which contain contextual and temporal information about regions, are generated from GPT4o (left). Then, the hallucinations in the captions are mitigated (middle). Lastly, the instruction samples that cover diverse aspects of the regions are generated (right).", "description": "This figure illustrates the three-stage pipeline for generating region-level video instructions.  Starting with a video containing region masklets (segmented regions) and their corresponding noun labels, the process begins with generating detailed region-level captions using GPT-4. These captions include rich contextual and temporal information about each region.  Next, a hallucination mitigation step refines these captions, removing inaccuracies or false information. Finally, diverse instruction samples are generated from these refined captions, encompassing various aspects of each region such as detailed descriptions, summaries, or conversational question-answer pairs.", "section": "4. Region-level Video Instruction Dataset"}]