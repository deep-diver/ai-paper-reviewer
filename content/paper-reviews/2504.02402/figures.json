[{"figure_path": "https://arxiv.org/html/2504.02402/x2.png", "caption": "Figure 1: Illustration of our event-based non-contact sound recovery. We try to recover sound from the visual vibration of the object caused by the sound wave. Compared with the traditional high-speed camera solution (top), we proposed to use an event camera to capture a temporally dense signal (bottom). We first utilize a laser matrix (left) to amplify the gradient and an event camera to capture the vibrations. Then, our learning-based approach to spatial-temporal modeling enables us to recover better signals.", "description": "This figure illustrates the process of event-based non-contact sound recovery. The top half shows the traditional method using a high-speed camera to capture vibrations caused by sound waves, which are then processed to recover the sound. The bottom half presents the proposed method, which utilizes a laser matrix to amplify the vibration gradient before capturing it with an event camera. The event camera captures temporally dense data, providing more information about the vibration. Finally, a learning-based spatial-temporal model is used to reconstruct the sound signal from this richer data, leading to better recovery.  The figure highlights the benefits of using event cameras and a laser matrix for improved sound recovery compared to the traditional high-speed camera approach.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2504.02402/x3.png", "caption": "Figure 2: (a) Our data simulation starts with controlling the objects\u2019 vibration. We utilize audio to manipulate the coordinates of objects resulting in their vibrations across random directions. Then we use an event simulator to generate the corresponding events. The generated events are used for training. (b) The synthetic vibrating speckles are used for fine-tuning and testing.", "description": "Figure 2 illustrates the data simulation process for the proposed event-based sound recovery system.  Panel (a) details the simulation of object vibrations driven by audio signals.  Audio input controls object coordinates, producing vibrations in random directions. An event simulator then generates corresponding events from these vibrations, creating a training dataset for the model. Panel (b) shows a secondary simulation used to create synthetic vibrating speckles. This data is used for fine-tuning and testing the model, helping improve its performance and generalization capabilities.", "section": "4. Method"}, {"figure_path": "https://arxiv.org/html/2504.02402/x4.png", "caption": "Figure 3: (a) Overview of our proposed network architecture. The event stream is processed into voxel grids, from which patches centered around the speckles are selected. First, the patches are input into a sparse convolution-based lightweight backbone to extract visual features. Next, a spatial attention block aggregates the information in the different patches. Finally, Mamba is employed to model long-term temporal information and reconstruct the audio that caused the object\u2019s vibration. (b) and (c) illustrate the detailed structure of SAB and SSM. (c) At time t gtsubscript\ud835\udc54\ud835\udc61g_{t}italic_g start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT is the input feature, otsubscript\ud835\udc5c\ud835\udc61o_{t}italic_o start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT is the output and htsubscript\u210e\ud835\udc61h_{t}italic_h start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT denotes the hidden state. A, B, and C are the gating weights optimized by Mamba. \u0394\u0394\\Deltaroman_\u0394 is used to discretize the continuous parameters A\ud835\udc34Aitalic_A and B\ud835\udc35Bitalic_B.", "description": "Figure 3 illustrates the proposed network architecture for event-based non-contact sound recovery.  The process begins by converting the event stream into spatio-temporal voxel grids.  Patches, centered on detected speckles, are extracted from these grids and fed into a sparse convolution-based feature extraction module. A spatial aggregation block then combines information from multiple patches, considering the varying vibration directions.  Finally, a Mamba module models long-term temporal dependencies within the feature sequence, leading to the reconstruction of the audio signal. Sub-figures (b) and (c) provide detailed views of the Spatial Aggregation Block (SAB) and the Structured State Space Model (SSM), respectively.  Sub-figure (c) defines the input feature (g<sub>t</sub>), output (o<sub>t</sub>), hidden state (h<sub>t</sub>), and parameters (A, B, C) of the SSM, showing how the Mamba module uses these components and the \u0394 operator to discretize continuous parameters and model long-term temporal information.", "section": "4. Method"}, {"figure_path": "https://arxiv.org/html/2504.02402/x5.png", "caption": "Figure 4: Qualitative comparison results on the real-world data of a chipbag. Audio is provided in the supplementary.", "description": "Figure 4 presents a qualitative comparison of sound recovery results from a real-world experiment involving a chip bag.  The experiment involved generating sound using a speaker, and the resulting chip bag vibrations were captured using several methods: a traditional RGB-based camera system (RGBPhase), an event-based system from prior work (EvPhase), the proposed EvMic system (Ours), and a reference microphone recording. The figure shows the experimental setup, the visualized events from the event camera, and spectrograms of the recovered audio from each method alongside the spectrogram from the microphone. This allows for a visual comparison of the accuracy and detail captured by each sound recovery method.", "section": "5.2.1. Results on the real-world data"}, {"figure_path": "https://arxiv.org/html/2504.02402/x6.png", "caption": "Figure 5: Qualitative comparison results on the real-world data of a speaker. Audio is provided in the supplementary.", "description": "Figure 5 presents a qualitative comparison of sound recovery results from a real-world experiment involving a speaker. The experiment setup includes an event camera, a speaker, and a laser matrix to enhance the visual representation of sound vibrations. The figure displays visualizations of the event stream, the results obtained using the EvPhase method (a baseline method), the results from the proposed EvMic method, and a reference spectrogram from a microphone recording. This allows for a visual comparison of the methods' ability to capture and reconstruct the audio signal, demonstrating the superior performance of the proposed method in terms of accuracy and high-frequency detail.", "section": "5.2.2. Results on the real-world data"}, {"figure_path": "https://arxiv.org/html/2504.02402/x7.png", "caption": "Figure 6: Capture objects from a distance to obtain a large field of view. Top: Capture glitter papers while playing chirp audio. Bottom: Capture multiple speakers to recover stereo audio. The left and right speakers play left and right channels respectively, while the medium speaker plays a mixed mono channel. Audio is provided in the supplementary.", "description": "Figure 6 demonstrates the system's wide field of view by capturing audio sources from a distance.  The top half shows glitter papers reacting to a chirp audio signal. The bottom half shows three speakers playing different audio channels (left, right, and a mixed mono channel from the center speaker). This setup showcases the system's ability to recover stereo audio from spatially separated sources.", "section": "5.3 Discussion"}, {"figure_path": "https://arxiv.org/html/2504.02402/x8.png", "caption": "Figure 7: Ablation analysis for different vibration direction. The object is placed in different orientations to produce various vibration directions. Audio is provided in the supplementary.", "description": "This figure demonstrates an ablation study on the impact of varying vibration directions on sound recovery accuracy.  The object's orientation is modified to induce vibrations along different axes.  The resulting spectrograms for each orientation are displayed and compared to ground truth audio (microphone recording). This helps to illustrate the robustness and sensitivity of the proposed method to different vibration patterns, a key factor in real-world non-contact sound recovery scenarios.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2504.02402/x9.png", "caption": "Figure A1: Qualitative comparison for models trained w or w/o speckle data.", "description": "This figure displays a qualitative comparison of audio generated by models trained with and without synthetic speckle data.  The spectrograms visually represent the audio output, allowing for a direct comparison of the quality and detail in the audio reconstruction achieved by the two models. The use of speckle data during training demonstrably improves the model's ability to generate cleaner, higher-fidelity audio, as indicated by the differences between the spectrograms.", "section": "Supplementary Material"}, {"figure_path": "https://arxiv.org/html/2504.02402/x10.png", "caption": "Figure A2: Qualitative results for ablation analysis.", "description": "Figure A2 presents a qualitative comparison of audio signals reconstructed using different model variations.  Each subfigure shows a spectrogram of reconstructed audio.  The models compared include one using only sparse convolutions, one with a temporal modeling module (LSTM), one with the spatial aggregation block (SAB), and the complete model with SAB and Mamba.  The ground truth spectrogram is also included. The figure visually demonstrates the impact of each component of the proposed network on the final audio reconstruction quality.", "section": "Supplementary Material"}, {"figure_path": "https://arxiv.org/html/2504.02402/x11.png", "caption": "Figure A3: Qualitative comparison results of our model with other methods on the synthetic data.", "description": "This figure presents a qualitative comparison of the proposed model's performance against other methods using synthetic data.  It visually compares the spectrograms generated by the different methods. The spectrograms show frequency and amplitude information over time for each method, including the ground truth. This allows for a direct visual comparison of the accuracy and detail preserved by each method in reconstructing audio signals from visual vibration data.  Different color schemes and visual patterns in the spectrograms represent how the methods capture different aspects of the audio signal.", "section": "Qualitative results on synthetic data"}]