{"importance": "This multilingual hallucination study is vital! It tackles the critical issue of **LLM accuracy across languages**, moving beyond English-centric approaches. The findings on model size and language support impacting hallucination rates open new research avenues for **improving LLM reliability globally**.", "summary": "Multilingual LLMs Hallucinate! This study measures hallucination across 30 languages.", "takeaways": ["LLM hallucination rates vary across languages.", "Smaller LLMs tend to hallucinate more than larger ones.", "Hallucination rate does not correlate with the language representation."], "tldr": "Large Language Models (LLMs) tendency to generate non-factual/unfaithful responses poses a risk to their global utility. The majority of research on detecting LLM hallucination are English-centric. They focus on machine translation/summarization, tasks that are less common compared to open information seeking. The study aims to quantify LLM hallucination across languages in knowledge-intensive question answering.\n\nThe study trains a multilingual hallucination detection model and conduct a large-scale study across 30 languages/6 LLM families. It uses MT to generate training data in other languages. Silver/gold test sets estimate hallucination rates, using a QA dataset with LLM-generated prompts and Wikipedia articles. It finds that, while LLMs generate longer responses, there is no correlation between length-normalized hallucination rates and digital representation. Smaller LLMs have larger hallucination rates.", "affiliation": "W\u00fcNLP, CAIDAS, University of W\u00fcrzburg", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}, "podcast_path": "2502.12769/podcast.wav"}