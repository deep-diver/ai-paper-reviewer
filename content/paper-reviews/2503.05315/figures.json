[{"figure_path": "https://arxiv.org/html/2503.05315/x1.png", "caption": "Figure 1: LoRACode Architecture: The input consists of code-code or text-code pairs. The base models are enhanced with LoRA layers, that output pooled embeddings, optimized using the Contrastive Trainer to improve retrieval accuracy. Finally, Mean Reciprocal Rank (MRR) measures the quality of ranked retrieval results.", "description": "The LoRACode architecture uses either code-code or text-code pairs as input.  These inputs are fed into a base model (CodeBERT, GraphCodeBERT, or UniXcoder).  LoRA (Low-Rank Adaptation) layers are added to the base model to enhance it. These layers produce pooled embeddings which are then optimized by a Contrastive Trainer to boost retrieval accuracy. The final evaluation metric used is Mean Reciprocal Rank (MRR), which assesses the quality of the ranked retrieval results.", "section": "3 DESIGN AND IMPLEMENTATION"}, {"figure_path": "https://arxiv.org/html/2503.05315/x2.png", "caption": "Table 1: Training hyperparameters", "description": "This table lists the hyperparameters used during the training process of the LoRACode model.  It includes the batch size (number of samples processed in one iteration), the number of training epochs (complete passes through the training dataset), the learning rate scheduler (algorithm used to adjust the learning rate during training), logging frequency (how often training metrics are recorded), the frequency of saving model checkpoints, and the evaluation strategy.", "section": "3 Design and Implementation"}]