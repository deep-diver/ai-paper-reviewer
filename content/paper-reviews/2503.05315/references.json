{"references": [{"fullname_first_author": "Edward J. Hu", "paper_title": "Lora: Low-rank adaptation of large language models", "publication_date": "2021-06-01", "reason": "This paper introduces the LoRA technique, which is central to the paper's approach of parameter-efficient fine-tuning for code embeddings."}, {"fullname_first_author": "Zhangyin Feng", "paper_title": "Codebert: A pre-trained model for programming and natural languages", "publication_date": "2020-02-01", "reason": "CodeBERT is a foundational model in code embedding and is used as a base model in the current paper, making its citation highly important."}, {"fullname_first_author": "Hamel Husain", "paper_title": "Codesearchnet challenge: Evaluating the state of semantic code search", "publication_date": "2020-09-01", "reason": "CodeSearchNet provides the dataset used for training and evaluating the models, crucial for benchmarking code retrieval tasks."}, {"fullname_first_author": "Daya Guo", "paper_title": "Unixcoder: Unified cross-modal pre-training for code representation", "publication_date": "2022-03-01", "reason": "UniXCoder is another key base model used in the experiments, serving as a strong baseline and benefiting from LoRA adaptation."}, {"fullname_first_author": "Prannay Khosla", "paper_title": "Supervised contrastive learning", "publication_date": "2021-04-01", "reason": "The paper introduces the contrastive learning framework, which is used for fine-tuning the models to improve retrieval accuracy."}]}