[{"content": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S3.T2.fig1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S3.T2.fig1.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T2.fig1.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.fig1.1.1.1.1.1\">Hyperparameter</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T2.fig1.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.fig1.1.1.1.2.1\">Value</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T2.fig1.1.2.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.fig1.1.2.1.1\">Batch Size</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.fig1.1.2.1.2\">16 per device</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.fig1.1.3.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.fig1.1.3.2.1\">Epochs</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.fig1.1.3.2.2\">1 (rapid evaluation)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.fig1.1.4.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.fig1.1.4.3.1\">Learning Rate Scheduler</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.fig1.1.4.3.2\">1000 warmup steps</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.fig1.1.5.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.fig1.1.5.4.1\">Logging</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.fig1.1.5.4.2\">Every 200 steps</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.fig1.1.6.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.fig1.1.6.5.1\">Save Strategy</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.fig1.1.6.5.2\">End of each epoch</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.fig1.1.7.6\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S3.T2.fig1.1.7.6.1\">Evaluation Strategy</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S3.T2.fig1.1.7.6.2\">No intermediate eval</td>\n</tr>\n</tbody>\n</table>", "caption": "Table 3: \nMRR results for Text2Code search of LoRA models UniXCoder, GraphCodeBERT, and CodeBERT fine-tuned with ranks 32 and 64, compared with base models, evaluated over XLCost dataset per language. LoRACode Combined denotes a single adapter of given rank fine-tuned over 3 base models: UniXcoder, GraphCodeBERT, and CodeBERT, but evaluated over UniXcoder.", "description": "This table presents the Mean Reciprocal Rank (MRR) scores for Text-to-Code search using various models.  It compares the performance of three base models (UniXcoder, GraphCodeBERT, and CodeBERT) with their LoRA-adapted versions (using ranks 32 and 64).  The results are broken down by programming language (Ruby, Go, PHP, Python, Java, Javascript) and show the improvement achieved by using LoRA adapters.  A 'LoRACode Combined' model, which combines the adapters from all three base models and is evaluated on UniXcoder, is also included for comparison.  This helps assess the effectiveness of the LoRA fine-tuning technique and its impact on cross-lingual code retrieval.", "section": "4.2 Text2Code"}, {"content": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S3.T2.fig2.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S3.T2.fig2.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T2.fig2.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.fig2.1.1.1.1.1\">Hyperparameter</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T2.fig2.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.fig2.1.1.1.2.1\">Value</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T2.fig2.1.2.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.fig2.1.2.1.1\">Ranks</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.fig2.1.2.1.2\">16, 32, 64</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.fig2.1.3.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.fig2.1.3.2.1\">LoRA Alpha</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.fig2.1.3.2.2\">32, 64, 128</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.fig2.1.4.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.fig2.1.4.3.1\">Target Modules</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.fig2.1.4.3.2\">Query and Value</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.fig2.1.5.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S3.T2.fig2.1.5.4.1\">Dropout</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S3.T2.fig2.1.5.4.2\">10%</td>\n</tr>\n</tbody>\n</table>", "caption": "Table 4: Increase in Mean Reciprocal Rank and Normalized Discounted Cumulative Gain @ k=10 for LoRACode combined adapter (rank 64) trained on CosQA dataset (Huang et\u00a0al., 2021), compared to the UniXCoder base model. LoRACode Combined denotes a single adapter fine-tuned over the 3 base models (UniXcoder, GraphCodeBERT, and CodeBERT), but evaluated on UniXcoder.", "description": "This table presents a comparison of the performance of the UniXCoder base model and the LoRACode model (with a combined adapter of rank 64) on the CosQA dataset.  The LoRACode model uses a parameter-efficient fine-tuning technique called Low-Rank Adaptation (LoRA) and incorporates adapters trained on the UniXcoder, GraphCodeBERT, and CodeBERT base models.  The table shows the improvement in Mean Reciprocal Rank (MRR) and Normalized Discounted Cumulative Gain (NDCG@10) achieved by LoRACode compared to the UniXCoder base model, indicating the effectiveness of the LoRA approach. The key metric is the percentage increase for both MRR and NDCG@10.", "section": "4 Evaluation"}, {"content": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T3.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T3.1.1.1\">\n<td class=\"ltx_td ltx_border_tt\" id=\"S4.T3.1.1.1.1\"></td>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T3.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.2.1\">Ruby</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T3.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.3.1\">Go</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T3.1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.4.1\">PHP</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T3.1.1.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.5.1\">Python</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T3.1.1.1.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.6.1\">Java</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T3.1.1.1.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.7.1\">Javascript</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.2.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T3.1.2.2.1\">LoRACode - Combined (rank 64)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T3.1.2.2.2\">42.83</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T3.1.2.2.3\">48.34</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T3.1.2.2.4\">20.88</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T3.1.2.2.5\">28.60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T3.1.2.2.6\">33.08</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T3.1.2.2.7\">30.55</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.3.3\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.3.3.1\">LoRACode - Combined (rank 32)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.3.3.2\">43.96</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.3.3.3\">48.98</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.3.3.4\">21.86</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.3.3.5\">29.85</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.3.3.6\">34.15</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.3.3.7\">31.38</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.4.4\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.4.4.1\">UnixCoder</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.4.4.2\">44.06</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.4.4.3\">49.59</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.4.4.4\">22.31</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.4.4.5\">29.76</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.4.4.6\">34.47</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.4.4.7\">32.05</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.5.5\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.5.5.1\">GraphCodeBERT</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.5.5.2\">20.80</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.5.5.3\">12.48</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.5.5.4\">8.08</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.5.5.5\">10.38</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.5.5.6\">8.60</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.5.5.7\">7.30</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.6.6\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.6.6.1\">CodeBERT</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.6.6.2\">0.37</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.6.6.3\">0.15</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.6.6.4\">0.03</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.6.6.5\">0.06</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.6.6.6\">0.04</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.6.6.7\">0.06</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.7.7\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.1.7.7.1\">Starencoder</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.1.7.7.2\">4.41</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.1.7.7.3\">1.85</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.1.7.7.4\">0.57</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.1.7.7.5\">2.14</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.1.7.7.6\">1.89</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.1.7.7.7\">1.55</td>\n</tr>\n</tbody>\n</table>", "caption": "Table 5: Language-Specific Adapter Performance: MRR and NDCG Improvements Across Programming Languages. Across 6 programming languages, the Mean Reciprocal Rank and Normalized Discounted Cumulative Gain @ k=10 show significant improvements for the UniXCoder model adapted with language-wise adapters, compared to the base model. For comparison, the table details the number of code samples in the training dataset for each from CodeSearchNet, as well as the training time over 2xH100 GPUs and the % increase in MRR. Training time is mentioned in minutes.", "description": "This table presents the performance of the LoRACode model with language-specific adapters on six programming languages (Ruby, Go, PHP, Python, Java, Javascript).  It shows the improvements in Mean Reciprocal Rank (MRR) and Normalized Discounted Cumulative Gain (NDCG@10) achieved by using language-specific adapters compared to the baseline UniXCoder model.  The table also provides details on the number of code samples used for training in each language, the training time on 2xH100 GPUs, and the percentage increase in MRR.", "section": "4 Evaluation"}, {"content": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T4.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T4.1.1\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\" id=\"S4.T4.1.1.2\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T4.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.1.1.3.1\">UniXCoder</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T4.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.1.1.1.1\">LoRACode - Combined (<math alttext=\"r\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.1.1.1.1.m1.1\"><semantics id=\"S4.T4.1.1.1.1.m1.1a\"><mi id=\"S4.T4.1.1.1.1.m1.1.1\" xref=\"S4.T4.1.1.1.1.m1.1.1.cmml\">r</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.T4.1.1.1.1.m1.1b\"><ci id=\"S4.T4.1.1.1.1.m1.1.1.cmml\" xref=\"S4.T4.1.1.1.1.m1.1.1\">\ud835\udc5f</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T4.1.1.1.1.m1.1c\">r</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.T4.1.1.1.1.m1.1d\">italic_r</annotation></semantics></math>=64)</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T4.1.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" id=\"S4.T4.1.2.1.1\">MRR</th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T4.1.2.1.2\">31.36</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T4.1.2.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.1.2.1.3.1\">36.02</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S4.T4.1.3.2.1\">NDCG@10</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T4.1.3.2.2\">35.64</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T4.1.3.2.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.1.3.2.3.1\">40.44</span></td>\n</tr>\n</tbody>\n</table>", "caption": "Table 6: \nMRR results for Code2Code search of LoRA models UniXCoder, GraphCodeBERT, and CodeBERT fine-tuned with ranks 32 and 64, compared with base models, evaluated over the XLCost dataset per language. LoRACode Combined denotes a single adapter of given rank fine-tuned over the 3 base models (UniXcoder, GraphCodeBERT and CodeBERT), but evaluated on UniXcoder.", "description": "This table presents the Mean Reciprocal Rank (MRR) scores for Code-to-Code (Code2Code) search using different models.  It compares the performance of three base models (UniXcoder, GraphCodeBERT, and CodeBERT) with LoRA-adapted versions of these models using two different rank settings (32 and 64).  The 'LoRACode Combined' results represent a single adapter trained across all three base models and evaluated using UniXcoder as a baseline. The evaluation is performed per programming language using the XLCost dataset.", "section": "4.4 CODE2CODE"}, {"content": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T5.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T5.1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T5.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.1.1.1.1\">Languages</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" id=\"S4.T5.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.1.1.2.1\">MRR</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" id=\"S4.T5.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.1.1.3.1\">NDCG</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T5.1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.1.1.4.1\"># Samples</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T5.1.1.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.1.1.5.1\">Train Time</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T5.1.1.1.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.1.1.6.1\">% Increase</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T5.1.2.1\">\n<td class=\"ltx_td\" id=\"S4.T5.1.2.1.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.2.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.2.1.2.1\">Base</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.2.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.2.1.3.1\">LoRA</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.2.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.2.1.4.1\">Base</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.2.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.2.1.5.1\">LoRA</span></td>\n<td class=\"ltx_td\" id=\"S4.T5.1.2.1.6\"></td>\n<td class=\"ltx_td\" id=\"S4.T5.1.2.1.7\"></td>\n<td class=\"ltx_td\" id=\"S4.T5.1.2.1.8\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.1.3.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T5.1.3.2.1\">Ruby</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T5.1.3.2.2\">44.06</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T5.1.3.2.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.3.2.3.1\">45.78</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T5.1.3.2.4\">47.95</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T5.1.3.2.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.3.2.5.1\">49.77</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T5.1.3.2.6\">1558</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T5.1.3.2.7\">7:01</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T5.1.3.2.8\">3.90</th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.1.4.3\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.4.3.1\">Go</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.4.3.2\">49.59</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.4.3.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.4.3.3.1\">82.88</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.4.3.4\">53.66</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.4.3.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.4.3.5.1\">85.35</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.4.3.6\">10456</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.4.3.7\">47:36</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.4.3.8\">67.13</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.1.5.4\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S4.T5.1.5.4.1\">PHP</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S4.T5.1.5.4.2\">35.22</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S4.T5.1.5.4.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.5.4.3.1\">52.46</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S4.T5.1.5.4.4\">24.73</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S4.T5.1.5.4.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.5.4.5.1\">56.54</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S4.T5.1.5.4.6\">15078</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S4.T5.1.5.4.7\">1:08:39</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S4.T5.1.5.4.8\">48.94</th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.1.6.5\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.6.5.1\">Python</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.6.5.2\">29.76</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.6.5.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.6.5.3.1\">55.56</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.6.5.4\">32.83</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.6.5.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.6.5.5.1\">59.49</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.6.5.6\">15739</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.6.5.7\">2:10:39</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.6.5.8\">86.69</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.1.7.6\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S4.T5.1.7.6.1\">Java</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S4.T5.1.7.6.2\">34.47</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S4.T5.1.7.6.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.7.6.3.1\">53.47</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S4.T5.1.7.6.4\">37.94</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S4.T5.1.7.6.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.7.6.5.1\">57.45</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S4.T5.1.7.6.6\">10308</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S4.T5.1.7.6.7\">1:25:19</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S4.T5.1.7.6.8\">31.91</th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.1.8.7\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T5.1.8.7.1\">Javascript</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T5.1.8.7.2\">32.05</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T5.1.8.7.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.8.7.3.1\">38.75</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T5.1.8.7.4\">35.04</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T5.1.8.7.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.8.7.5.1\">42.35</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T5.1.8.7.6\">3627</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T5.1.8.7.7\">16:41</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T5.1.8.7.8\">20.9</td>\n</tr>\n</tbody>\n</table>", "caption": "Table 7: Time taken to generate embeddings (in MM:SS) for Code2Code search of the base embedding models vs embedding models altered with LoRA adapters. The latency is measured over each programming language\u2019s dataset, for the combined adapter of LoRA rank 64, over UniXCoder as base model.", "description": "This table presents a comparison of the time taken to generate code embeddings using the base embedding models (UniXcoder, CodeBERT, GraphCodeBERT) versus those enhanced with LoRA adapters.  The measurements are reported in minutes and seconds (MM:SS), and are broken down by programming language (Ruby, Go, PHP, Java, C++, Javascript, Python). For the LoRA-adapted models, the results reflect the use of a combined adapter with a rank of 64, using UniXcoder as the base model. This comparison highlights the impact of the LoRA adapters on the efficiency of the embedding generation process for different languages.", "section": "4.4 CODE2CODE"}]