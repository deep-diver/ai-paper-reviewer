{"importance": "This paper introduces LoRACode, a parameter-efficient method using **LoRA adapters** for code embeddings. It achieves state-of-the-art performance with significantly reduced computational costs. This opens new avenues for multilingual code search and adaptation and sets a baseline for future research in PEFT for code.", "summary": "LoRACode enhances code embeddings using LoRA, achieving SOTA in code retrieval with minimal computational cost.", "takeaways": ["LoRACode, utilizing LoRA, significantly improves code retrieval accuracy with minimal computational overhead.", "Language-specific adapters outperform task-specific adapters in capturing the nuances of code for Text2Code retrieval.", "The size and quality of training datasets significantly impact the effectiveness of LoRA adapters in code retrieval tasks."], "tldr": "Code embeddings are essential for semantic code search, but current approaches face scalability and efficiency challenges. Open-source models have limitations, while high-performing proprietary systems are computationally expensive. To address this, this paper introduces a parameter-efficient fine-tuning method based on Low-Rank Adaptation (LoRA) that constructs task-specific adapters. \n\nDubbed **LoRACode**, the approach reduces trainable parameters to less than 2% of the base model, enabling rapid fine-tuning on large code datasets. Experiments demonstrate significant improvements such as up to 9.1% in Mean Reciprocal Rank (MRR) for Code2Code search, and up to 86.69% for Text2Code search tasks. It also explores language-wise adaptation and highlights the sensitivity of code retrieval to syntactic and linguistic variations.", "affiliation": "Max Planck Institute for Software Systems", "categories": {"main_category": "Machine Learning", "sub_category": "Deep Learning"}, "podcast_path": "2503.05315/podcast.wav"}