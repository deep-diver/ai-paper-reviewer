{"importance": "This work addresses the challenge of creating multi-view videos from a single input, which is valuable for VR/AR content generation. By simplifying the process and improving results, it offers new possibilities for immersive experiences and further research in 4D video creation.", "summary": "Reangle-A-Video generates synchronized multi-view videos from a single video via video-to-video translation, surpassing existing methods without specialized 4D training.", "takeaways": ["Reangle-A-Video reframes multi-view video generation as video-to-video translation, using available diffusion priors.", "The method uses multi-view motion learning to distill view-invariant motion from warped videos.", "The approach achieves multi-view consistent image translation through DUSt3R guidance."], "tldr": "**Generating dynamic 4D environments is challenging.** Existing methods rely on training multi-view video diffusion models on large 4D datasets, but these are often limited to specific domains, inaccessible, or require multiple inputs. In essence, a new method is needed for generating multi-view videos in a way that is more accessible and adaptable to different scenes. Addressing this is crucial for creating immersive VR/AR experiences.\n\nTo tackle this issue, **Reangle-A-Video** reframes the task as video-to-video translation. It uses multi-view motion learning via self-supervised fine-tuning of an image-to-video diffusion transformer on warped videos. For multi-view consistency, it uses DUSt3R to guide image translation. This approach surpasses existing methods for static view transport and dynamic camera control. This work sets a new standard for multi-view video generation.", "affiliation": "KAIST AI", "categories": {"main_category": "Computer Vision", "sub_category": "Video Understanding"}, "podcast_path": "2503.09151/podcast.wav"}