[{"figure_path": "https://arxiv.org/html/2503.09151/extracted/6271571/figures/training-pipeline.png", "caption": "Figure 1: \nFrom a single monocular video of any scene, Reangle-A-Video generates synchronized videos from diverse camera viewpoints or movements without relying on any multi-view generative prior\u2014using only single fine-tuning of a video generator.\nThe first row shows the input video, while the rows below present videos generated by Reangle-A-Video.\n(Left): Static view transport results.\n(Right): Dynamic camera control results.\nFull video examples are available on our project page: \nhyeonho99.github.io/reangle-a-video", "description": "This figure demonstrates the capabilities of Reangle-A-Video, a novel method for generating synchronized multi-view videos from a single monocular input video.  Unlike traditional methods that require extensive multi-view training data, Reangle-A-Video leverages a single fine-tuning step of a pre-trained video generator. The figure showcases two scenarios: static view transport (left) and dynamic camera control (right).  In static view transport, the model generates videos from various static viewpoints, while dynamic camera control allows generating videos that emulate different camera movements and trajectories.  The first row displays the original input video, followed by rows showing Reangle-A-Video's output videos for each scenario.  Links to full video examples are provided.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.09151/extracted/6271571/figures/inpainting-method.jpeg", "caption": "Figure 2: \nQualitative results on static view transport (left) & dynamic camera control (right). Click with Acrobat Reader to play videos.", "description": "This figure displays qualitative results of the Reangle-A-Video model.  The left side shows examples of static view transport, where the model generates videos from different viewpoints of the same scene. The right side shows examples of dynamic camera control, where the model generates videos simulating various camera movements, such as orbiting and zooming.  The videos can be played using Adobe Acrobat Reader.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.09151/extracted/6271571/figures/inpainting-compare.jpeg", "caption": "Figure 3: \nMulti-view motion learning pipelines for (a) Static view transport and (b) Dynamic camera control.\nFor both tasks, we distill view-robust motion of the underlying scene to a pre-trained MM-DiT video model [85], using all visible pixels within the sampled videos.\nThis few-shot, self-supervised training optimizes only the LoRA layers [35, 62], enabling lightweight training.", "description": "This figure illustrates the training process for Reangle-A-Video's two main functionalities: static view transport and dynamic camera control.  It shows how a pre-trained Multi-modal Diffusion Transformer (MM-DiT) video model is fine-tuned using a self-supervised approach.  The method leverages visible pixels from warped videos to learn view-invariant motion,  optimizing only the Low-Rank Adaptation (LoRA) layers for efficient training. The few-shot nature of this training allows for lightweight model adaptation.", "section": "3. Reangle-A-Video"}, {"figure_path": "https://arxiv.org/html/2503.09151/extracted/6271571/figures/comparisons.jpg", "caption": "Figure 4: \nMulti-view consistent image inpainting using stochastic control guidance.\nIn experiments, we set S=25\ud835\udc4625S=25italic_S = 25.", "description": "This figure illustrates the process of multi-view consistent image inpainting, a crucial step in generating synchronized multi-view videos.  The method uses stochastic control guidance to ensure consistency across different viewpoints. Starting with warped first frames (representing the scene from various perspectives), the algorithm iteratively inpaints these images, utilizing a diffusion model. At each step, multiple (in this case, 25) sample paths are generated, and a multi-view consistency evaluation selects the best option, promoting coherence between the inpainted images from various viewpoints. The final output is a set of consistent starting images for the next video generation stage.", "section": "3. Stage III: Multi-View Image Inpainting"}, {"figure_path": "https://arxiv.org/html/2503.09151/extracted/6271571/figures/main-user-study.png", "caption": "Figure 5: \nQualitative inpainting comparisons.\nWe compare naive inpainting to inpainting with stochastic control guidance.", "description": "This figure compares the results of two different image inpainting methods: naive inpainting (which processes each warped image independently) and inpainting with stochastic control guidance (which uses a multi-view stereo reconstruction network to enforce consistency across multiple views).  The comparison highlights the improved cross-view consistency achieved by the stochastic control guidance method, leading to more realistic and coherent inpainted images.", "section": "3.4 Stage III: Multi-View Image Inpainting"}, {"figure_path": "https://arxiv.org/html/2503.09151/extracted/6271571/figures/without-warped.jpeg", "caption": "Figure 6: \nQualitative comparisons.\nTop half shows (a) Static view transport and bottom half presents (b) Dynamic camera control results.\nThe first row in each half displays the input videos, and for each input video, two generated videos corresponding to target cameras (1 and 2) are shown for each method.\nAcross baseline, same camera parameters were used for each 1,2.\nVisit our page for full-video results.", "description": "Figure 6 presents a qualitative comparison of video generation results for static view transport and dynamic camera control.  The top half shows results for static view transport, where the goal is to generate views from different static viewpoints. The bottom half shows dynamic camera control results, generating videos with dynamic camera movements. Each section is organized with the input video in the first row, followed by generated videos from two target viewpoints (camera 1 and camera 2) for each method (Reangle-A-Video and baselines). Consistent camera parameters were used across all baseline methods for a fair comparison.  For higher-quality video results, please visit the project page linked in the paper.", "section": "4.2. Qualitative Results"}, {"figure_path": "https://arxiv.org/html/2503.09151/extracted/6271571/figures/robustness.jpeg", "caption": "Figure 7: \nUser study results.\nTop: Static view transport results.\nBottom: Dynamic camera control results.", "description": "This figure presents the results of a user study comparing the performance of Reangle-A-Video against baseline methods in two scenarios: static view transport and dynamic camera control. The top half shows the results for static view transport, where users rated the accuracy of the transported viewpoint and how well the generated video preserved the input video's motion. The bottom half shows the results for dynamic camera control, where users rated the accuracy of the target camera movement and the preservation of the input video's motion in the generated videos.  The bar charts illustrate the relative performance of different methods.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.09151/extracted/6271571/figures/camera-vis.png", "caption": "Figure 8: \nNovel view video generation with and without using warped video for training (target viewpoint: dolly zoom in).", "description": "This figure demonstrates the impact of using warped videos during training on novel view video generation.  The \"dolly zoom in\" perspective is used as the target viewpoint.  The left-hand side shows results when warped videos were *not* included in training; the right-hand side shows the results when warped videos *were* included.  The comparison highlights the improved accuracy and quality achieved by incorporating warped videos in the training process.  This is particularly evident in the clarity and consistency of the generated videos.", "section": "4.3 Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2503.09151/extracted/6271571/figures/mask-downsampling.png", "caption": "Figure 9: \nTop: Unseen view video generation.\nBottom: Novel view video generation using an appearance-edited first frame.", "description": "This figure demonstrates the robustness of Reangle-A-Video's few-shot training strategy. The top row shows unseen view video generation, where the model is trained without specific warped views (vertical up/down orbits) and then generates a video from that omitted view using an inpainted first frame as input. The bottom row illustrates novel view video generation using an appearance-edited first frame. The input video's first frame is modified using FlowEdit, and a novel-view video is generated using the fine-tuned model. Both scenarios showcase the model's ability to generate videos from views and appearances not seen during training, highlighting its generalization capability and robustness.", "section": "4.3 Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2503.09151/extracted/6271571/figures/masked-diffusion-loss.png", "caption": "Figure 10: \nVisualizations of the used camera types.", "description": "This figure visualizes the six different camera movements used in the paper's experiments.  These movements provide six degrees of freedom, enabling diverse viewpoints and camera trajectories. The visualization shows examples of static view transport (orbit left, orbit right, orbit up, orbit down, dolly zoom in, dolly zoom out) and dynamic camera control using similar camera movements.", "section": "A. Additional Experimental Details"}, {"figure_path": "https://arxiv.org/html/2503.09151/extracted/6271571/figures/appendix_limit_1.jpeg", "caption": "Figure 11: \nTemporal downsampling of visibility masks.\nExcept for the first mask frame, pixel-wise (element-wise) logical AND operation is done for every four masks.", "description": "This figure illustrates how the visibility masks are downsampled temporally for processing efficiency.  The original visibility masks (which indicate visible and invisible regions in the video frames) have a high temporal resolution. To reduce computational complexity, the algorithm downsamples these masks.  It retains the first mask frame unchanged. For every subsequent set of four mask frames, it performs an element-wise logical AND operation. This means that a pixel will only be marked as 'visible' in the downsampled mask if it is visible in all four of the corresponding original frames.  This process effectively compresses the temporal dimension of the visibility masks, simplifying processing while preserving essential information about visibility over time.", "section": "A. Additional Experimental Details"}, {"figure_path": "https://arxiv.org/html/2503.09151/extracted/6271571/figures/appendix_limit_2.jpeg", "caption": "Figure 12: \nImpact of masked diffusion loss on video quality.\nMasking the diffusion loss effectively prevents artifacts.", "description": "This figure demonstrates a comparison of video generation results using a standard diffusion loss versus a masked diffusion loss. The left side shows videos generated with the standard diffusion loss, exhibiting noticeable artifacts. In contrast, the right side showcases videos produced with the masked diffusion loss, which effectively removes the artifacts, resulting in cleaner and higher-quality videos. The results highlight the effectiveness of using a masked diffusion loss in mitigating artifacts during the video generation process.", "section": "B.2. Masking Diffusion Loss"}, {"figure_path": "https://arxiv.org/html/2503.09151/extracted/6271571/figures/failure-case.png", "caption": "Figure 13: \nGeometric misalignment in the warped frame.\nIn this example, the target camera view is shifted 10 degrees to the (horizontal orbit) right of the input frame.", "description": "This figure demonstrates a failure case of the Reangle-A-Video model, specifically highlighting geometric misalignment during the warping process.  The input video frame is warped to simulate a camera view shifted 10 degrees to the right. However, due to inaccuracies in depth estimation and/or camera parameters used in the warping procedure, the resulting warped frame exhibits noticeable geometric distortions and misalignments compared to the expected output.", "section": "3. Reangle-A-Video"}]