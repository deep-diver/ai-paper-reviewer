{"importance": "This paper introduces **V-MAGE, a new game-based benchmark that addresses the limitations of existing methods**. By focusing on visual-centric capabilities and interactive environments, it provides **a more realistic and challenging evaluation** for MLLMs, driving advancements in multimodal intelligence and agent design.", "summary": "V-MAGE: A new benchmark for evaluating visual-reasoning skills of multimodal AI in game environments, revealing critical limitations in current models.", "takeaways": ["Current MLLMs struggle with visual perception and reasoning in dynamic game environments, despite excelling in static benchmarks.", "Existing game-based benchmarks often rely on text or grid-based settings, limiting the assessment of spatial, temporal, and dynamic complexities.", "V-MAGE, a new framework features five diverse games with 30+ handcrafted levels, testing models on core visual skills such as positioning, trajectory tracking, timing, and visual memory."], "tldr": "**Multimodal Large Language Models (MLLMs) have shown advancements in various multimodal tasks, yet current game-based benchmarks fall short**. They lack visual-centric tasks and fail to evaluate the diverse reasoning skills needed for real-world decision-making. Existing setups rely on text-based or grid-based games, limiting their ability to assess spatial, temporal, and dynamic complexities crucial for problem-solving. This results in a lack of insights into MLLMs' visual reasoning limitations. The need for game-based evaluation environments that better align with how MLLMs process visual information is essential to improving gameplay tasks and strengthening visual interactive capabilities. \n\nTo tackle the challenges, this paper introduces **Visual-centric Multiple Abilities Game Evaluation (V-MAGE)**, a game-based evaluation framework designed to assess MLLMs' visual reasoning capabilities. V-MAGE includes five diverse games with over 30 handcrafted levels, testing visual skills like positioning, trajectory tracking, timing, and memory, alongside higher-level reasoning. Evaluating leading MLLMs with V-MAGE reveals significant challenges in visual perception and reasoning. Findings highlight critical limitations and suggest potential avenues for improvement from an agent-centric perspective.", "affiliation": "Nanjing University", "categories": {"main_category": "Multimodal Learning", "sub_category": "Vision-Language Models"}, "podcast_path": "2504.06148/podcast.wav"}