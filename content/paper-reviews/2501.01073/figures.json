[{"figure_path": "https://arxiv.org/html/2501.01073/x4.png", "caption": "Figure 1: Illustration of our proposed graph sequence representation. This representation can be viewed as a sequence of actions: first generating all nodes (node type, node index), then explicitly adding edges (source node index, destination node index, edge type) step by step until completion. A unified vocabulary is used to map different types of actions into a shared token space.", "description": "This figure illustrates how a graph is represented as a sequence of actions.  First, all nodes are generated, each represented by its type and a unique index. Then, edges are added one by one, each defined by the source node index, the destination node index, and the edge type. The entire graph generation process is thus encoded as a sequence of tokens, which allows the use of a transformer-decoder architecture to model this sequence and predict the next token in the sequence during training.  A unified vocabulary is used to represent all node types, edge types, and actions (node creation and edge creation), making this representation suitable for the transformer-decoder.", "section": "3. Graph Generative Pre-trained Transformer"}, {"figure_path": "https://arxiv.org/html/2501.01073/x11.png", "caption": "Figure 2: Goal-oriented molecule generation using QED, SA and GSK3\u03b2\ud835\udefd\\betaitalic_\u03b2 scores. Top row (a) shows the results using RFT, and bottom row (b) shows the results using RL.", "description": "This figure presents the results of goal-oriented molecule generation experiments, focusing on three key properties: quantitative estimation of drug-likeness (QED), synthetic accessibility (SA), and activity against Glycogen Synthase Kinase 3 beta (GSK3\u03b2).  The top row displays results obtained using Rejection Sampling Fine-Tuning (RFT), which involves retraining the model with a filtered dataset of molecules possessing the desired properties. The bottom row illustrates results achieved via Reinforcement Learning (RL), employing a Proximal Policy Optimization (PPO) approach. Each graph shows the distribution of the target properties in generated molecules after fine-tuning, compared to the distribution of the same properties in molecules generated by the original pre-trained model. The graphs visually represent how fine-tuning using both RFT and RL methods modifies the distribution of molecules toward the desired properties.", "section": "4.1 Goal-oriented Generation"}]