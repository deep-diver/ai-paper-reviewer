{"references": [{"fullname_first_author": "Brown, T.", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper is foundational for the field of large language models and their few-shot learning capabilities, directly influencing the work on long-context processing."}, {"fullname_first_author": "Vaswani, A.", "paper_title": "Attention is all you need", "publication_date": "2017-12-01", "reason": "This paper introduced the transformer architecture, a fundamental building block of modern LLMs, and its attention mechanism which is crucial for long-context processing."}, {"fullname_first_author": "Ainslie, J.", "paper_title": "GQA: Training generalized multi-query transformer models from multi-head checkpoints", "publication_date": "2023-05-13", "reason": "This paper introduced Grouped-Query Attention (GQA), a key technique used in the FastKV method for improving efficiency and memory usage in LLMs."}, {"fullname_first_author": "Bai, Y.", "paper_title": "LongBench: A bilingual, multitask benchmark for long context understanding", "publication_date": "2023-08-14", "reason": "This paper provides a benchmark dataset, LongBench, used for evaluating the performance of long-context LLMs, which is crucial for assessing the effectiveness of the proposed FastKV method."}, {"fullname_first_author": "Shi, Z.", "paper_title": "Discovering the gems in early layers: Accelerating long-context LLMs with 1000x input token reduction", "publication_date": "2024-09-17", "reason": "This paper presents GemFilter, a method for enhancing computational efficiency of prefill stage in LLMs, which is directly compared with the proposed FastKV method in this paper."}]}