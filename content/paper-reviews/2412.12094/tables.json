[{"content": "|         | GSM8K-CoT |       |   | MMLU |        |       |        |       |\n| :------ | :-------- | :---- | :- | :---- | :---- | :---- | :---- | :---- |\n|         | flexible  | strict | r.KV(%) | humanities | stem | social | other | Overall |\n| Vanilla | 77.79     | 77.26  | 100.00 | 60.49 | 56.61 | 76.50 | 72.19 | 65.72 |\n| StrmLLM (*n*=380) | 70.89 | 71.42 | 47.54 | 57.73 | 54.46 | 74.39 | 70.13 | 63.39 |\n| StrmLLM (*n*=256) | 69.67 | 68.61 | 26.00 | 62.10 | 54.49 | 73.06 | 69.78 | 62.10 |\n| SepLLM (*n*=256) | 77.18 | 77.18 | 47.36 | 57.66 | 56.49 | 76.21 | 72.19 | 64.68 |", "caption": "Table 1: Evaluation results and average running time KV cache usage for training-free experiments on GSM8K-CoT 8-shots and MMLU 5-shots. For\u00a0SepLLM\u00a0and StreamingLLM, three initial tokens\u2019 KV is kept for this experiment. r.KV(%) here represents the ratio of KV usage at runtime for the respective method compared to Vanilla.", "description": "This table presents the evaluation results of different large language models (LLMs) on two benchmark datasets, GSM8K-CoT (8-shot) and MMLU (5-shot), in a training-free setting.  The models compared are a vanilla Transformer, StreamingLLM, and SepLLM.  The table shows the model performance on each dataset, along with the percentage of key-value (KV) cache usage relative to the vanilla Transformer (r.KV%).  Lower KV cache usage indicates better efficiency.  For SepLLM and StreamingLLM, the KV cache of the first three tokens are always kept.  This table demonstrates the effectiveness of SepLLM in reducing KV cache usage while maintaining performance comparable to the vanilla Transformer.", "section": "4.2. Training-free"}, {"content": "| Method | ARC-c | ARC-e | LBD-ppl | LBD-acc | LogiQA | PIQA | SciQ | Attn(%) | r.KV(%) |\n|---|---|---|---|---|---|---|---|---|---|\n| Vanilla | 20.14 | 46.80 | 34.83 | 33.28 | 23.81 | 62.84 | 81.50 | 100.00 | 100.00 |\n| StrmLLM(*n*=64) | 20.65 | 47.39 | 44.03 | 26.74 | 21.97 | 63.82 | 75.80 | 16.58 | 15.28 |\n| SepLLM(*n*=64) | 19.62 | 46.46 | 40.08 | 28.97 | 26.42 | 63.82 | 80.10 | 25.83 | 25.40 |\n| SepLLM(*n*=128) | 19.97 | 47.35 | 30.16 | 33.18 | 22.73 | 64.64 | 82.60 | 35.64 | 32.27 |\n| SepLLM(*n*=64,H) | 20.73 | 48.44 | 36.54 | 30.45 | 25.35 | 64.36 | 80.60 | 32.01 | 31.58 |\n| SepLLM(*n*=64,H/T) | 21.42 | 47.26 | 33.41 | 32.80 | 22.73 | 63.98 | 81.20 | 38.18 | 37.75 |", "caption": "Table 2: The performance of downstream tasks and the usage of running-time KV cache in the training from scratch setting.", "description": "This table presents the performance of different models on various downstream tasks, including ARC-Challenge, ARC-Easy, LAMBADA (perplexity and accuracy), LogiQA, PIQA, and SciQ. It also shows the percentage of attention used (Attn) and the ratio of key-value cache usage at runtime compared to the vanilla transformer model (r.KV) in a training-from-scratch scenario.", "section": "4. Experiments and Results"}, {"content": "| Arch. | StrmLLM | SepLLM |     |   |   | Vanilla |\n|---|---|---|---|---|---|---| \n| Setting | n=64 | n=64 | n=128 | n=64,H | n=64,H/T | full |\n| FLOPs(%) | 70.11 | 71.77 | 72.58 | 72.83 | 73.90 | 100.0 |\n| Attn.(%) | 6.43 | 17.21 | 22.48 | 24.11 | 31.01 | 100.0 |", "caption": "Table 3: The comparison of FLOPs and Attention Map Ratio.", "description": "This table shows the comparison of the Floating Point Operations (FLOPs) and the Attention Map Ratio for different model configurations during training from scratch. The Attention Map Ratio represents the proportion of '1's in the lower triangular part of the attention mask matrix, which is an indicator of the sparsity of the attention mechanism.  Lower FLOPs and Attention Map Ratio indicate higher computational efficiency.", "section": "4.3. Training from Scratch"}, {"content": "| PG19 | 1M | 1.5M | 2M | 2.5M | 3M | 3.5M | 4M |\n|---|---|---|---|---|---|---|---| \n| **StrmLLM** | 39.5 | 38.2 | 38.3 | 37.6 | 36.4 | 35.8 | 36.1 |\n| SepLLM (*s*=32) | 37.7 | 36.6 | 36.6 | 36.0 | 34.9 | 34.2 | 34.5 |\n| SepLLM (*s*=64) | 37.1 | 36.0 | 36.1 | 35.4 | 34.3 | 33.7 | 33.9 |", "caption": "Table 4: The perplexity comparison on the PG19 test set\u00a0(Rae et\u00a0al., 2020). For fair evaluation, we keep the KV cache capacity c as 324 and keep Sink Cache a as 4 for both StreamingLLM and\u00a0SepLLM.", "description": "This table compares the perplexity of StreamingLLM and SepLLM on the PG19 test set across different context lengths (1 million to 4 million tokens). The KV cache capacity (c) is kept constant at 324, and the Sink Cache size (a) is kept constant at 4 for fair comparison between the two models. The results show that SepLLM consistently outperforms StreamingLLM in terms of perplexity across all tested context lengths while keeping the sink cache size (a) and max KV capacity (c) the same. ", "section": "4.5. Streaming Applications"}, {"content": "| Length | Methods | _c_ | r.KV | ppl | time (s) |\n|---|---|---|---|---|---| \n| 20K | Vanilla | 20K | 10K | 302.6 | 523.8 |\n| | StrmLLM | 800 | 800 | 31.5 | 341.2 |\n| | SepLLM | 800 | 562 | 28.3 | 325.8 |\n| 64K | Vanilla | 64K | 32K | 1090.8 | 3380.6 |\n| | StrmLLM | 800 | 800 | 37.9 | 1096.0 |\n| | SepLLM | 800 | 562 | 33.4 | 1049.7 |", "caption": "Table 5: The perplexity and runing time comparison on the PG19 test set\u00a0(Rae et\u00a0al., 2020). r.KV means the average runtime KV cache usage in the generation process.", "description": "This table presents a comparison of perplexity and runtime performance between vanilla, StreamingLLM, and SepLLM on the PG19 test set. The \"r.KV\" column represents the average runtime key-value cache usage during sequence generation. Lower perplexity values indicate better language modeling performance, while shorter runtimes indicate faster inference speed. Different sequence lengths and KV cache capacities are examined. ", "section": "4.5. Streaming Applications"}, {"content": "| *s* | 5K | 10K | 15K | 20K | r.KV |\n|---|---|---|---|---|---| \n| 32 | 13.11 | 11.31 | 8.74 | 8.79 | 292 |\n| 48 | 13.03 | 11.26 | 8.70 | 8.76 | 300 |\n| 64 | 13.01 | 11.17 | 8.67 | 8.72 | 308 |", "caption": "Table 6: The perplexity and average runtime KV cache usage of SepLLM with respect to different Separator Cache sizes (s) on WikiText\u00a0(Merity et\u00a0al., 2017), in which a=4, w=224, c=324.", "description": "This table presents the perplexity and average runtime key-value (KV) cache usage of the SepLLM model on the WikiText language modeling benchmark. The table explores the impact of varying the Separator Cache size (s), which stores the compressed representations of segments in the input text. Other hyperparameters are fixed: a=4 (Initial Cache size), w=224 (Local Window Cache size), and c=324 (total KV cache capacity).  The goal is to understand how the Separator Cache size affects performance and resource usage during inference.", "section": "4.6. Ablation Study"}, {"content": "| Method | *w* | *c* | r.KV | 5K | 10K | 15K | 20K |\n|---|---|---|---|---|---|---|---|\n| | 320 | 324 | 324 | 13.18 | 11.51 | 8.85 | 8.91 |\n| StrmLLM | 512 | 516 | 516 | 12.87 | 11.37 | 8.74 | 8.78 |\n| | 796 | 800 | 800 | 11.96 | 11.01 | 8.67 | 8.72 |\n| | 224 | 324 | 308 | 13.01 | 11.17 | 8.67 | 8.72 |\n| SepLLM | 320 | 516 | 452 | 12.91 | 11.26 | 8.67 | 8.72 |\n| | 512 | 800 | 690 | 12.09 | 11.03 | 8.56 | 8.62 |", "caption": "Table 7: Average downstream performance (ppl) over different input lengths and average runtime KV usage with different c,w on WikiText, in which a=4 for both methods and s=64 for SepLLM.", "description": "This table presents the average perplexity scores (ppl) and runtime key-value (KV) cache usage of different models on the WikiText language modeling benchmark. Specifically, it compares a standard Transformer model (Vanilla) with StreamingLLM and the proposed SepLLM under various hyperparameter settings for context window size (c), local window size (w), and initial cache size (a). The table explores the impact of these parameters on perplexity across different input lengths (5K, 10K, 15K, and 20K tokens).  The values of 'a' and 's' are fixed to 4 and 64 respectively for a controlled comparison across different values of 'c' and 'w'.", "section": "4. Experiments and Results"}, {"content": "| Method | initial | shift | 5K | 10K | 15K | 20K | r.KV |\n|---|---|---|---|---|---|---|---| \n| StrmLLM | \u2713 | \u2713 | 13.2 | 11.5 | 8.9 | 8.9 | 324 |\n| StrmLLM | \u2717 | \u2713 | 14.6 | 13.2 | 10.8 | 10.9 | 324 |\n| StrmLLM | \u2713 | \u2717 | 425.5 | 513.1 | 509.5 | 506.8 | 324 |\n| StrmLLM | \u2717 | \u2717 | 409.4 | 540.5 | 527.5 | 558.2 | 324 |\n| SepLLM | \u2713 | \u2713 | 13.1 | 11.3 | 8.7 | 8.8 | 292 |\n| SepLLM | \u2717 | \u2713 | 14.9 | 14.3 | 12.4 | 12.5 | 290 |\n| SepLLM | \u2713 | \u2717 | 192.7 | 214.6 | 175.0 | 174.4 | 292 |\n| SepLLM | \u2717 | \u2717 | 226.4 | 264.7 | 227.5 | 228.8 | 290 |", "caption": "Table 8:  The perplexity and average runtime KV cache usage of SepLLM and StreamingLLM tested on WikiText\u00a0(Merity et\u00a0al., 2017). c=324, a=0/4 for both methods. s=32,w=224 for SepLLM", "description": "This table compares the perplexity and average runtime KV cache usage of two models, SepLLM and StreamingLLM, on the WikiText language modeling benchmark. Various configurations of initial cache size (a) and positional encoding shifting are tested, with c (maximum KV cache capacity) fixed at 324, s (Separator Cache capacity) at 32, and w (Local Window Cache capacity) at 224 for SepLLM.", "section": "4.5. Streaming Applications"}, {"content": "|                      | Vanilla (Full Attention) | SepLLM (n=64) | SepLLM (n=128) |\n|----------------------|--------------------------|-----------------|------------------|\n| time per iteration (ms) | 2524.45                 | 1648.11        | 1653.11         |\n| samples / second      | 405.82                  | 622.31         | 620.3          |", "caption": "Table 9: The details about training acceleration.", "description": "This table compares the training speed of a standard Transformer model (Vanilla, using full attention) with the proposed SepLLM model using two different settings (n=64 and n=128).  It measures and reports the wall-clock time per training iteration and the throughput (samples processed per second) to demonstrate the acceleration achieved by SepLLM during the training process.  The table shows that SepLLM significantly reduces the training time per iteration, leading to higher throughput.", "section": "4. Experiments and Results -> 4.3 Training from Scratch"}, {"content": "| Backbone | Arch. | _c_ | r.KV | ppl | time(s) |\n|---|---|---|---|---|---| \n| | Vanilla | 64K | 32K | 1037.6 | 4160.7 |\n| Pythia-6.9B | StrmLLM | 800 | 800 | 15.9 | 1522.6 |\n| | SepLLM | 800 | 562 | 15.8 | 1456.0 |\n| | Vanilla | 64K | 32K | 1090.8 | 3380.6 |\n| Llama-3-8B | StrmLLM | 800 | 800 | 37.9 | 1096.0 |\n| | SepLLM | 800 | 562 | 33.4 | 1049.7 |", "caption": "Table 10: The comparison of SepLLM adapted to different architectures.", "description": "This table compares the performance of SepLLM and StreamingLLM with the vanilla LLaMA 3 and Pythia 6.9B models on the PG19 dataset, generating 64K tokens. The table shows the perplexity and inference time for each model and architecture, with different cache sizes (c) and runtime key-value usage (r.KV).", "section": "4.2. Training-free"}, {"content": "| Backbone | *a* | *s* | *w* | *c* | r.KV | ppl | time(s) |\n|---|---|---|---|---|---|---|---|\n| | 4 | 64 | 256 | 800 | 562 | 13.0 | 445.0 |\n| Pythia-6.9B | 4 | 64 | 800 | 1024 | 946 | 12.7 | 450.4 |\n| | 4 | 64 | 928 | 1280 | 1138 | 12.7 | 454.4 |\n| Pythia-12B | 4 | 64 | 256 | 800 | 562 | 12.1 | 577.0 |", "caption": "Table 11: The comparison of SepLLM adapted to Pythia\u00a0(Biderman et\u00a0al., 2023) with different scales.", "description": "This table compares the performance of SepLLM when adapted to different scales of the Pythia model family (6.9B and 12B parameters) on the PG19 dataset, generating 20K tokens. It shows the impact of model scale and KV cache capacity on perplexity and inference time.", "section": "4. Experiments and Results"}, {"content": "| Length | Methods | _c_ | r.KV | ppl | time (s) |\n|---|---|---|---|---|---| \n| 20K | StrmLLM | 1024 | 1024 | 8.98 | 1512.88 |\n| 20K | StrmLLM | 800 | 800 | 9.02 | 1430.59 |\n| 20K | SepLLM | 1024 | 906 | 8.92 | 1440.89 |\n| 20K | SepLLM | 800 | 690 | 9.00 | 1368.07 |\n| 64K | StrmLLM | 1024 | 1024 | 11.01 | 4844.79 |\n| 64K | StrmLLM | 800 | 800 | 11.09 | 4623.90 |\n| 64K | SepLLM | 1024 | 906 | 10.96 | 4619.63 |\n| 64K | SepLLM | 800 | 690 | 11.07 | 4414.72 |", "caption": "Table 12: The comparison of SepLLM adapted to Falcon-40B\u00a0(Almazrouei et\u00a0al., 2023).", "description": "This table compares the performance of StreamingLLM and SepLLM, adapted to the Falcon-40B architecture, on generating different lengths of text (20K and 64K tokens). The metrics used for comparison include average runtime KV cache usage (r.KV), perplexity (ppl), and inference time (time(s)). The comparison aims to show the efficiency and performance benefits of SepLLM in handling long text generation with a constrained KV cache size.", "section": "4.5. Streaming Applications"}, {"content": "| Backbone | Algorithm | GSM8K-CoT | r.KV (%) |\n|---|---|---|---| \n| Base | Vanilla | 54.44 | 100 |\n|  | SepLLM ft. | 55.95 | 47.36 |\n| Instruct | Vanilla | 77.26 | 100 |\n|  | SepLLM ft. | 77.63 | 47.36 |", "caption": "Table 13: The comparison of SepLLM adapted to Llama-3-8B\u00a0(Dubey et\u00a0al., 2024) with base or instruct versions.", "description": "This table compares the performance of the SepLLM architecture adapted to two versions of the Llama-3-8B model: the base model and the instruction-tuned (instruct) version. It evaluates their performance on the GSM8K-CoT benchmark for reasoning ability, both with and without fine-tuning using SepLLM, and also shows the ratio of KV usage at runtime compared to the full attention baseline.", "section": "Appendix D: The Performance of Different Models"}]