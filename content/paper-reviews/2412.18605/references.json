{"references": [{"fullname_first_author": "Kaiming He", "paper_title": "Masked autoencoders are scalable vision learners", "publication_date": "2022-00-00", "reason": "This paper introduces the Masked Autoencoders (MAE) framework, a self-supervised learning method used for pre-training the visual encoder in Orient Anything, significantly improving its synthetic-to-real transfer ability."}, {"fullname_first_author": "Andreas Geiger", "paper_title": "Are we ready for autonomous driving? The KITTI vision benchmark suite", "publication_date": "2012-00-00", "reason": "The KITTI dataset is one of the five real-world benchmark datasets used to evaluate the zero-shot performance of Orient Anything on real-world images."}, {"fullname_first_author": "Garrick Brazil", "paper_title": "Omni3D: A large benchmark and model for 3d object detection in the wild", "publication_date": "2023-00-00", "reason": "Omni3D is a relevant reference as it is another work focusing on 3D orientation estimation, allowing for a comparison of approaches and performance with the proposed method."}, {"fullname_first_author": "Maxime Oquab", "paper_title": "DINOv2: Learning robust visual features without supervision", "publication_date": "2023-00-00", "reason": "DINOv2 is a self-supervised learning method providing pre-trained visual encoders used in Orient Anything, contributing to the model's strong generalization capabilities and improved synthetic-to-real transfer."}, {"fullname_first_author": "Tsung-Yi Lin", "paper_title": "Microsoft COCO: Common objects in context", "publication_date": "2014-00-00", "reason": "The COCO dataset is one of the datasets used for creating the Ori-Bench benchmark, which evaluates the ability of 2D Vision Language Models (VLMs) to understand object orientation, highlighting the limitations of existing approaches."}]}