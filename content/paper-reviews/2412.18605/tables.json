[{"content": "| Object | Direction | Spatial Part | Spatial Relation | Overall |\n|---|---|---|---|---|\n| Random | 12.93 | 22.12 | 17.54 | 16.75 |\n| GPT-4o | 49.32 | 15.38 | 27.27 | 32.50 |\n| Gemini-1.5-pro | 58.90 | 15.38 | 18.18 | 33.00 |\n| Orient Anything+LLM | **67.12** | **46.15** | **40.91** | **51.50** |", "caption": "Table 1: Quantitative results on the proposed Ori-Bench.", "description": "This table presents a quantitative analysis of the Ori-Bench, a newly proposed benchmark designed to evaluate the ability of visual language models (VLMs) to understand and reason about object orientation in images.  It compares the performance of several models, including GPT-40 and Gemini-1.5-pro, on three distinct tasks within Ori-Bench: Object Direction Recognition, Spatial Part Reasoning, and Spatial Relation Reasoning.  The results show the accuracy of each model on each task, highlighting the challenges inherent in orientation understanding for current VLMs and showcasing the relative strengths and weaknesses of each model in this domain.", "section": "3. Orientation Understanding in 2D VLMs"}, {"content": "| Object | Direction |\n|---|---|", "caption": "Table 2: Orientation estimation on both in-domain rendered images and out-of-domain real images. The best results are bold.", "description": "This table presents a quantitative comparison of object orientation estimation performance on both synthetic (rendered) and real-world images.  It evaluates different models, including the proposed \"Orient Anything\" at various scales (ViT-S, ViT-B, ViT-L), against baselines such as Cube RCNN and large language models (LLMs) like GPT-40 and Gemini-1.5-pro. The metrics used include accuracy of orientation judgment (Does the object have a meaningful front face?), and the absolute error and accuracy at different tolerance thresholds (\u00b15\u00b0, \u00b122.5\u00b0) for the estimation of azimuth, polar, and rotation angles.  Bold values highlight the best performing model for each metric.", "section": "6.2. Rendered-Images Orientation Estimation"}, {"content": "| Spatial | Part |\n|---|---|", "caption": "Table 3: Zero-shot orientation estimation on five unseen real image benchmarks. Reported in absolute error.", "description": "This table presents the results of zero-shot object orientation estimation on five real-world image datasets: SUN RGB-D, KITTI, nuScenes, Objectron, and ARKitScenes.  The model was trained only on synthetically generated data and not on any of these real-world datasets. The performance is evaluated using absolute error in degrees for the azimuth, polar, and rotation angles of the 3D object orientation.  Lower absolute errors indicate better performance, showing the model's ability to generalize to unseen data.", "section": "6.3. Zero-shot Real-Image Orientation Recognition"}, {"content": "| Spatial | Relation |\n|---|---|", "caption": "Table 4: Ablation study for Orientation Annotation.", "description": "This ablation study investigates the impact of different annotation methods on object orientation estimation accuracy.  It compares the performance of using a single view, multiple canonical views, and multiple views incorporating symmetry analysis to identify the front face of the 3D objects before rendering images for training. The results demonstrate how combining multiple views, particularly with symmetry analysis, significantly improves the accuracy of orientation prediction.", "section": "3. Orientation Understanding in 2D VLMS"}, {"content": "| Models | Rendered Image |  |  |  |  |  | Real Image |  | \n|---|---|---|---|---|---|---|---|---|---|\n|  | Judgment | Azimuth Estimation |  | Polar Estimation |  | Rotation Estimation | Judgment | Recognition | \n|---|---|---|---|---|---|---|---|---|---|\n|  | Acc\u2191 | Abs\u2193 | Acc@22.5\u00b0\u2191 | Abs\u2193 | Acc@5\u00b0\u2191 | Abs\u2193 | Acc@5\u00b0\u2191 | Acc\u2191 | Acc\u2191 |\n|---|---|---|---|---|---|---|---|---|---|\n| Random | 50.00 | - | 12.50 | - | 5.55 | - | 16.67 | 50.00 | 12.50 |\n| Cube RCNN | - | 89.00 | 12.44 | 27.99 | 10.37 | 132.74 | 2.50 | - | 20.25 |\n| Gemini-1.5-pro | 57.29 | 79.51 | 19.06 | 20.10 | 16.31 | 2.61 | 85.12 | 66.96 | 31.95 |\n| GPT-4o | 61.85 | 81.07 | 19.94 | 16.02 | 17.56 | 4.65 | 81.00 | 69.29 | 45.78 |\n| Ours (ViT-S) | 73.88 | 45.27 | 63.18 | 5.12 | 71.62 | 0.82 | 97.06 | 78.54 | 63.44 |\n| Ours (ViT-B) | 74.88 | 39.03 | 71.94 | 3.81 | 81.37 | **0.26** | **99.56** | **81.25** | 70.19 |\n| Ours (ViT-L) | **76.00** | **38.60** | **73.94** | **2.94** | **86.75** | 0.70 | 98.31 | 80.30 | **72.44** |", "caption": "Table 5: Ablation study for Learning Objective, Number of Views, Training Initialization and Data Augmentation.", "description": "This table presents the results of ablation experiments conducted to analyze the impact of various design choices on the model's performance.  Specifically, it investigates the effects of different learning objectives (regression, classification, and probability distribution fitting), varying the number of rendered images used per object, different model initializations (using pre-trained weights from MAE, CLIP, and DINOv2), and the use of data augmentation techniques (random cropping and mask-based object isolation). The results help determine which combination of these factors yielded the best performance in terms of azimuth and recognition accuracy.", "section": "6. Experiments"}, {"content": "|           | SUN RGB-D                      |           |           | KITTI                         |           |           | nuScenes                       |           |           | Objectron                      |           |           | ARKitScenes                    |           |           |\n| :-------- | :------------------------------- | :-------- | :-------- | :------------------------------- | :-------- | :-------- | :------------------------------- | :-------- | :-------- | :------------------------------- | :-------- | :-------- | :------------------------------- | :-------- | :-------- |\n|           | *Azimuth* | *Polar* | *Rotation* | *Azimuth* | *Polar* | *Rotation* | *Azimuth* | *Polar* | *Rotation* | *Azimuth* | *Polar* | *Rotation* | *Azimuth* | *Polar* | *Rotation* |\n| Cube RCNN | 93.58                           | 39.73     | 140.10     | 98.61                           | 39.73     | 121.21     | 89.63                           | 15.64     | 132.57     | 122.99                           | 60.01     | 113.31     | 91.16                           | 37.39     | 132.86     |\n| Ours (ViT-S) | 58.20                           | 11.63     | **3.59**   | 65.85                           | 5.00      | 1.08      | 72.68                           | 5.58      | 2.16      | 39.45                           | 23.47     | 18.26     | 69.37                           | 14.25     | 2.63      |\n| Ours (ViT-B) | 56.34                           | 9.15      | 3.75      | 54.02                           | 5.86      | **0.21**   | 66.56                           | 5.72      | **1.28**   | 36.49                           | **22.13** | **18.34** | 75.45                           | 12.48     | **2.60**   |\n| Ours (ViT-L) | **42.98**                       | **8.38**   | 3.66      | **44.22**                       | **3.57**   | 0.89      | **55.17**                       | **4.08**   | 1.78      | **30.09**                       | 22.19     | 18.54     | **67.56**                       | **11.47** | 2.82      |", "caption": "Table 6: Detailed horizontal direction recognition accuracy for each object category in COCO that is annotated with front face and orientation. The differences between Orient Anything and the best results achieved by other alternative methods are also provided.", "description": "Table 6 presents a detailed breakdown of the horizontal direction recognition accuracy achieved by Orient Anything and other comparative methods across various object categories within the COCO dataset.  Each object category includes data only if it was annotated with a discernible front face and clear orientation. The table highlights Orient Anything's performance relative to the best-performing alternative method for each category, showcasing its strengths and weaknesses in handling different types of objects and orientations.", "section": "6.3 Zero-shot Real-Image Orientation Recognition"}]