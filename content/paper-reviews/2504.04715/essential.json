{"importance": "This paper is important for researchers because it **highlights the critical need for verifiable AI systems** and offers a practical investigation for auditing model substitution in LLM APIs. It unveils vulnerabilities in current verification techniques and **proposes a promising hardware-based solution**. The research also **opens avenues for standardized protocols** and more robust auditing techniques, ultimately fostering trust and transparency in AI service.", "summary": "LLM providers may covertly substitute models; this paper audits detection methods!", "takeaways": ["Current text-output-based verification techniques are vulnerable to adversarial attacks.", "Log probability analysis offers stronger guarantees but has limited accessibility.", "Trusted Execution Environments (TEEs) hold promise for provable model integrity."], "tldr": "The proliferation of LLMs via APIs introduces trust issues, as providers might substitute advertised models with cheaper alternatives, compromising fairness and reliability. Detecting such substitutions is difficult due to the black-box nature of APIs. Current verification techniques, including statistical tests and benchmark evaluations, often fall short, especially against adaptive attacks like model quantization and benchmark evasion. The paper formalizes the problem and evaluates existing techniques.\n\nThis paper systematically evaluates various detection methodologies under realistic adversarial scenarios. Findings reveal the limitations of text output methods and the stronger guarantees of log probability analysis, though its accessibility is limited. The study analyzes attacks like randomized substitution and benchmark evasion. Ultimately, the paper discusses the potential of hardware-based solutions like Trusted Execution Environments (TEEs) as a pathway toward provable model integrity, noting trade-offs.", "affiliation": "UC Berkeley", "categories": {"main_category": "AI Theory", "sub_category": "Robustness"}, "podcast_path": "2504.04715/podcast.wav"}