{"references": [{"fullname_first_author": "Ouyang, L.", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-12-01", "reason": "This paper is foundational for the alignment techniques used in the current work, showing the effectiveness of using human feedback to align language models."}, {"fullname_first_author": "Rafailov, R.", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "publication_date": "2024-12-01", "reason": "This paper introduces Direct Preference Optimization (DPO), a key alignment algorithm adapted and extended in this work for video generation."}, {"fullname_first_author": "He, X.", "paper_title": "Videoscore: Building automatic metrics to simulate fine-grained human feedback for video generation", "publication_date": "2024-06-01", "reason": "This paper provides a crucial dataset of human-rated video preferences, which is leveraged for training and evaluating reward models in this work."}, {"fullname_first_author": "Liu, X.", "paper_title": "Flow straight and fast: Learning to generate and transfer data with rectified flow", "publication_date": "2022-09-01", "reason": "This paper introduces rectified flow, the foundation for the video generation models analyzed and improved in the current work."}, {"fullname_first_author": "Wang, P.", "paper_title": "Qwen2-VL: Enhancing vision-language model's perception of the world at any resolution", "publication_date": "2024-09-01", "reason": "This paper provides the vision-language model (Qwen2-VL-2B) used as the base for the reward model, demonstrating the effectiveness of VLMs in visual understanding and reward modeling."}]}