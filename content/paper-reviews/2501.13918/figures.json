[{"figure_path": "https://arxiv.org/html/2501.13918/x1.png", "caption": "Figure 1: Overview of Our Video Alignment Paradigm.\n(a) Human Preference Annotation\u00a0(Sec.\u00a03.1). We construct a dataset of 182k (prompt, video A, video B) triplets, collecting preference annotations on Visual Quality\u00a0(VQ), Motion Quality\u00a0(MQ), and Text Alignment\u00a0(TA) from human evaluators.\n(b) Reward Mode Training\u00a0(Sec.\u00a03.2). We train a VLM-based reward model using the Bradley-Terry-Model-with-Ties formulation.\n(c) Video Alignment\u00a0(Sec.\u00a04). We adapt alignment techniques \u2014 DPO, RWR, and reward guidance \u2014 to flow-based video generation models and provide a comprehensive comparison of their effectiveness.", "description": "This figure illustrates the pipeline for aligning video generation models with human preferences.  Panel (a) shows the creation of a human preference dataset: 182,000 triplets of video pairs (A and B) generated from prompts, rated by human evaluators across three dimensions: Visual Quality (VQ), Motion Quality (MQ), and Text Alignment (TA). Panel (b) details the training of a Vision-Language Model (VLM) based reward model using the Bradley-Terry model with ties to predict preference scores based on the dataset. Panel (c) outlines the three alignment algorithms applied to flow-based video generation models: Direct Preference Optimization (DPO), Reward Weighted Regression (RWR), and reward guidance, comparing their effectiveness.", "section": "4. Video Alignment"}, {"figure_path": "https://arxiv.org/html/2501.13918/x2.png", "caption": "Figure 2: Statistics of our training data.", "description": "Figure 2 presents a comprehensive statistical overview of the training dataset used in the study. It includes visualizations of the distribution of prompts across different categories, a word cloud illustrating the most frequent words used in prompts, a bar chart illustrating prompt lengths, and a distribution of video durations and resolutions.", "section": "3. VideoReward"}, {"figure_path": "https://arxiv.org/html/2501.13918/x3.png", "caption": "Figure 3: Accuracy comparison between the BT and regression reward models across varying training data fractions (log scale).", "description": "This figure compares the performance of two reward model types: Bradley-Terry (BT) and regression models, across different training dataset sizes.  The x-axis represents the fraction of the training data used (on a logarithmic scale), and the y-axis shows the resulting accuracy of each model type.  The plot demonstrates how the accuracy of both models improves as more training data is available. Notably, the Bradley-Terry model consistently outperforms the regression model, particularly with smaller datasets. This suggests that for limited data, pairwise comparisons (as used in BT) are more effective at capturing relative quality scores than direct regression.", "section": "3.2. Reward Model Learning"}, {"figure_path": "https://arxiv.org/html/2501.13918/x4.png", "caption": "Figure 4: Visualization of the \u0394\u2062r\u0394\ud835\udc5f\\Delta rroman_\u0394 italic_r distribution for the BT reward model\u00a0(Left) and the BTT reward model\u00a0(Right). The BTT model effectively distinguishes tie pairs from chosen/rejected pairs.", "description": "This figure compares the distributions of the reward difference (\u0394r) between two videos for the Bradley-Terry (BT) model and the Bradley-Terry-with-Ties (BTT) model.  The BT model struggles to differentiate between videos rated as ties and those clearly preferred or rejected, exhibiting overlap in their \u0394r values. In contrast, the BTT model shows a clear separation between tied pairs and chosen/rejected pairs.  This demonstrates the BTT model's improved ability to handle tied preferences in reward modeling.", "section": "3.2. Reward Model Learning"}, {"figure_path": "https://arxiv.org/html/2501.13918/x5.png", "caption": "Figure 5: Visual comparison of videos generated by the original pretrained model and the Flow-DPO aligned model.", "description": "This figure presents a visual comparison of videos generated using two different methods: (1) a pretrained video generation model and (2) the same model after being aligned using the Flow-DPO algorithm.  The figure shows six video examples, each illustrating a scene described by a text prompt.  Each example includes a side-by-side comparison of the video generated by the pretrained model and the video generated by the Flow-DPO-aligned model, allowing viewers to visually assess the differences in visual quality, motion, and overall coherence between the two methods.", "section": "4. Video Alignment"}, {"figure_path": "https://arxiv.org/html/2501.13918/x6.png", "caption": "Figure 6: Human evaluation of Flow-DPO aligned model vs. pretrained model on VideoGen-Eval, which contains 400 prompts.", "description": "This figure presents the results of a human evaluation comparing videos generated by a pretrained video generation model and a Flow-DPO aligned model. The evaluation was conducted on the VideoGen-Eval dataset, which includes 400 prompts. The evaluation metrics used were Visual Quality, Motion Quality, and Text Alignment. The results are presented in a bar chart showing the percentage of wins for each model in each category. The chart displays the number of times videos from either model were preferred in terms of Visual Quality, Motion Quality, and Text Alignment, as well as the frequency of ties.", "section": "5. Video Alignment"}, {"figure_path": "https://arxiv.org/html/2501.13918/x7.png", "caption": "Figure 7: Accuracy of time-dependent \u03b2tsubscript\ud835\udefd\ud835\udc61\\beta_{t}italic_\u03b2 start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT vs. constant \u03b2\ud835\udefd\\betaitalic_\u03b2 for TA: Flow-DPO with a constant \u03b2\ud835\udefd\\betaitalic_\u03b2 consistently outperforms the timestep-dependent \u03b2\ud835\udefd\\betaitalic_\u03b2 across various settings.", "description": "This figure compares the performance of Flow-DPO, a video generation alignment algorithm, using two different strategies for the KL divergence regularization term (\u03b2).  The first strategy uses a time-dependent \u03b2 (\u03b2t = \u03b2(1-t)\u00b2), where \u03b2 changes throughout the video generation process. The second uses a constant \u03b2. The results are shown for Text Alignment (TA) task. The figure shows that Flow-DPO with a constant \u03b2 consistently outperforms the time-dependent \u03b2 across various settings. This suggests that a fixed KL divergence regularization strength is more effective for aligning the model than a varying strength.", "section": "5.2 Video Alignment"}, {"figure_path": "https://arxiv.org/html/2501.13918/x8.png", "caption": "Figure 8: Video Duration and Resolution in GenAI-Bench and VideoGen-Reward Bench", "description": "This figure is a comparison of video durations and resolutions present in the GenAI-Bench and VideoGen-RewardBench datasets.  The GenAI-Bench dataset contains videos generated by models from before the release of the Sora model, while the VideoGen-RewardBench dataset includes videos from more modern, state-of-the-art models. The chart visually shows the distribution of video durations (in seconds) across different resolutions for both datasets, allowing for a direct comparison of the characteristics of videos included in each benchmark.", "section": "3.1 Human Preference Data Collection"}, {"figure_path": "https://arxiv.org/html/2501.13918/x9.png", "caption": "Figure 9: The model coverage across the training sets of different baselines and the two evaluation benchmarks. VideoScore, VisionReward, and GenAI-Bench primarily focus on pre-SoRA-era models, while our training set and VideoGen-RewardBench concentrate on state-of-the-art T2V models.", "description": "Figure 9 is a timeline visualization showing the model coverage of various datasets used in the paper.  It illustrates the temporal distribution of different text-to-video (T2V) models used in the training sets of different baselines (VideoScore, VisionReward, GenAI-Bench) and the two evaluation benchmarks (GenAI-Bench and VideoGen-RewardBench). The figure highlights that earlier baselines primarily utilized pre-SoRA era models, while the paper's training set and newer VideoGen-RewardBench focus on state-of-the-art models. This shows the progression of models over time and the focus of the datasets utilized in the paper.", "section": "3.1 Human Preference Data Collection"}, {"figure_path": "https://arxiv.org/html/2501.13918/x10.png", "caption": "Figure 10: Human evaluation of Flow-DPO on TA-Hard prompt.", "description": "This figure presents a human evaluation comparing the performance of the Flow-DPO model (with a constant beta) against the original pretrained model on a set of challenging prompts focusing on Text Alignment (TA-Hard). The results are broken down across three key dimensions: Visual Quality (VQ), Motion Quality (MQ), and Text Alignment (TA). The bar chart shows the percentage of wins for each model in each dimension, as well as the percentage of ties observed.  This allows for a detailed assessment of how effectively Flow-DPO improves text alignment in video generation while also considering the potential trade-offs in visual and motion quality.", "section": "5. Video Alignment"}]