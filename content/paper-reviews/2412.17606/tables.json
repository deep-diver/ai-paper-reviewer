[{"content": "| Dataset | human | aug. | avg |\n|---|---|---|---| \n| Scratch | 31.28 | 77.76 | 54.42 |\n| FigureQA [Kahou et al. (2018)](https://arxiv.org/html/2412.17606/bib.bib8) | 13.44 | 9.36 | 11.40 |\n| DVQA [Kafle et al. (2018)](https://arxiv.org/html/2412.17606/bib.bib7) | 26.88 | 72.16 | 49.52 |\n| PlotQA [Methani et al. (2020)](https://arxiv.org/html/2412.17606/bib.bib18) | 30.56 | 74.00 | 52.28 |\n| SBS Figures (Ours) | **39.44** | **82.24** | **60.84** |", "caption": "Table 1: \nComparison of the pre-training effect of SBS Figures\u00a0with other synthetic datasets. All datasets were trained using the Donut model.", "description": "This table compares the performance of the Donut model when pre-trained on various synthetic datasets, including SBS Figures and other existing datasets like FigureQA, DVQA, and PlotQA.  The comparison focuses on the model's ability to answer questions about figures after pre-training.  The results are presented as the average F1 score across human-annotated and augmented datasets to evaluate the pre-training's effectiveness.", "section": "4.2 Main Results"}, {"content": "| Model | human | aug. | avg |\n|---|---|---|---| \n| VisionTaPas (Masry et al., 2022) | 29.60 | 61.44 | 45.52 |\n| T5 (Raffel et al., 2020) | 25.12 | 56.96 | 41.04 |\n| VL-T5 (Cho et al., 2021) | 26.24 | 56.88 | 41.56 |\n| Donut (Kim et al., 2022) | 31.28 | 77.76 | 54.42 |\n| Donut+SBS Figures (Ours) | **39.20** | **81.20** | **60.84** |\n| Pix2Struct (Lee et al., 2023) | 35.92 | 85.92 | 60.92 |\n| Pix2Struct+SBS Figures (Ours) | **41.84** | **87.20** | **64.52** |", "caption": "Table 2: \nComparison of the model pre-trained on our SBS Figures\u00a0to other models.", "description": "This table compares the performance of different models on figure question answering tasks.  Specifically, it shows how the performance of two models (Donut and Pix2Struct) changes when they are pre-trained on the SBS Figures dataset compared to their performance when not pre-trained or pre-trained on other datasets.  The results are presented for two different splits of the ChartQA dataset ('human' and 'augmented'), representing different difficulty levels of questions. The table highlights the impact of pre-training on SBS Figures on the overall accuracy of the models.", "section": "4.2 Main Results"}, {"content": "| Table 3: (F1) Appearance | Randomize |  | \u2713 | \n|---|---|---| \n| human | 33.44 | **35.92** | \n| aug. | 80.16 | **80.48** | \n\n| Table 4: (F2) Pre-training task |  | JSON | QA | \n|---|---|---| \n| human | 31.44 | **35.92** | \n| aug. | 79.12 | **80.48** | \n\n| Table 5: (F3) QA quality |  | Template | Gemma | GPT-3.5 | GPT-4o | \n|---|---|---|---|---|---| \n| human | 30.00 | 31.52 | **35.92** | 34.56 | \n| aug. | 77.92 | 79.04 | 80.48 | **81.84** |", "caption": "Table 8: \nEvaluation of the pre-training effect of SBS Figures\u00a0on the PlotQA and FigureQA tasks. All pre-training and fine-tuning were conducted using the Donut model.", "description": "This table presents the results of an experiment evaluating the impact of pre-training with the SBS Figures dataset on the performance of the Donut model on two figure question answering (QA) datasets: PlotQA and FigureQA.  The experiment involved pre-training the Donut model on SBS Figures and then fine-tuning it on PlotQA and FigureQA.  The table shows the performance (likely accuracy scores) achieved by the model after pre-training with SBS Figures, compared to the performance of a model trained from scratch (without pre-training).  This allows for assessing the effectiveness of the SBS Figures dataset as a pre-training resource for improving the model's ability to answer questions about figures.", "section": "4 Experiments"}, {"content": "| Randomize |  | \u2713 |\n|---|---|---|\n| human | 33.44 | **35.92** |\n| aug. | 80.16 | **80.48** |", "caption": "Table 9: \nEvaluation of the pretraining effect of our SBS Figures\u00a0for the UniChart reasoning training based on steps. We evaluate on ChartQA dataset (human\u2223\u2223\\mid\u2223aug.).", "description": "This table presents the results of an experiment evaluating the impact of pre-training with SBS Figures on the UniChart model's performance in ChartQA reasoning tasks.  The UniChart model was trained in two ways: (1) from scratch (without pre-training), and (2) with pre-training using SBS Figures. The experiment was conducted with varying amounts of data from the SBS Figures dataset (10k, 30k, and 50k QA pairs) to assess the influence of the pre-training data volume on the model's performance.  The model's performance was measured on both the human-annotated and augmented splits of the ChartQA dataset, providing a comprehensive evaluation of the pre-training effect across different data sizes and annotation types.", "section": "4.2 Main Results"}, {"content": "|   | JSON | QA |\n|---|---|---|\n| human | 31.44 | **35.92** |\n| aug. | 79.12 | **80.48** |", "caption": "Table 10: \nComparison of the pre-training effect of SBS Figures\u00a0with other synthetic datasets. All datasets were trained using the Donut model.", "description": "This table compares the effectiveness of pre-training on the SBS Figures dataset against other synthetic datasets for figure question answering (QA).  It shows the performance results achieved by fine-tuning a Donut model after pre-training on each dataset, using the average F1 score across human and augmented QA splits from the ChartQA dataset. The table highlights the relative improvement in performance provided by pre-training with SBS Figures compared to using other synthetic datasets.", "section": "4.2 Main Results"}]