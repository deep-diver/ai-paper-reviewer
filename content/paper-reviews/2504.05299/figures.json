[{"figure_path": "https://arxiv.org/html/2504.05299/extracted/6335965/figures/combined_plots.png", "caption": "Figure 1: Smol yet Mighty: comparison of SmolVLM with other state-of-the-art small VLM models. Image results are sourced from the OpenCompass OpenVLM leaderboard\u00a0(Duan et\u00a0al., 2024).", "description": "This figure compares the performance and resource usage of SmolVLM with other leading small-scale vision-language models (VLMs).  It showcases SmolVLM's efficiency by demonstrating that it outperforms significantly larger models while requiring far less GPU memory. The graph displays the models' performance on image-based tasks (Video-MME scores) plotted against their RAM usage.  SmolVLM models are shown to achieve similar or better performance than larger models with drastically reduced memory footprints, highlighting the model's effectiveness and efficiency.", "section": "Abstract"}, {"figure_path": "https://arxiv.org/html/2504.05299/x20.png", "caption": "Figure 2: SmolVLM Architecture. Images are split into subimages, frames are sampled from videos, and then encoded into visual features. These features are first rearranged via a pixel-shuffle operation, then mapped into the LLM input space as visual tokens using an MLP projection. Visual tokens are then concatenated/interleaved with text embeddings (orange/red). This combined sequence is passed to the LLM for text output.", "description": "The figure illustrates the architecture of SmolVLM, a compact vision-language model.  It begins by splitting input images into smaller subimages and sampling frames from video inputs.  These image/video segments are encoded into visual features using a vision encoder.  A pixel-shuffle operation rearranges these features to improve efficiency. An MLP projection then transforms the features into visual tokens, which are suitable for input into a language model (LLM). These visual tokens are concatenated or interleaved with textual embeddings (represented in orange and red in the figure), forming a combined sequence that is then fed into the LLM for final processing and text output. This design emphasizes compact size and efficiency.", "section": "2 Smoller Model Architecture"}, {"figure_path": "https://arxiv.org/html/2504.05299/x21.png", "caption": "Figure 3: Performance analysis of SmolVLM configurations.\n(Left) Impact of vision encoder and language model sizes. Smaller language models (135135135135M) benefit less from larger vision encoders (SigLIP-SO-400400400400M, 428428428428M) compared to SigLIP-B/16161616 (93939393M), while larger language models gain more from powerful encoders.\n(Middle-left) Performance significantly improves with increased context lengths (2222k to 16161616k tokens).\n(Middle-right) Optimal pixel shuffle factor (PS=2 vs.\u00a0PS=4) varies by model size.\n(Right) Frame averaging reduces video performance, with a rapid decline as more frames are averaged. Metrics average CIDEr (captioning) and accuracy (visual question answering).", "description": "Figure 3 presents a performance analysis of various SmolVLM configurations, broken down into four sub-figures.  The leftmost subfigure shows how vision and language model sizes affect performance. It reveals that smaller language models don't benefit as much from larger vision encoders, while larger language models see significant improvement with larger encoders.  The middle-left subfigure demonstrates a significant performance increase when the context length (number of tokens) is increased. The middle-right subfigure shows that the optimal pixel shuffle factor (a parameter that trades spatial resolution for increased channel depth, reducing visual tokens) depends on the model size. The rightmost subfigure reveals a performance drop in video tasks when frame averaging is increased, demonstrating that the optimal strategy for video processing is not simply averaging frames.", "section": "2 Smoller Model Architecture"}, {"figure_path": "https://arxiv.org/html/2504.05299/extracted/6335965/figures/pixel_shuffle.png", "caption": "Figure 4: Pixel shuffle. Rearranges encoded images, trading spatial resolution for increased channel depth. This reduces visual token count while preserving information density.", "description": "Figure 4 illustrates the concept of pixel shuffling, a technique used to reduce the number of visual tokens in compact vision-language models.  By rearranging the spatial dimensions of encoded images, pixel shuffling trades spatial resolution for increased channel depth. This means that the image is divided into smaller blocks, and these blocks are stacked to create a deeper image representation. This transformation reduces the total number of tokens required to represent the image while preserving important visual information, which ultimately improves efficiency and reduces computational costs.", "section": "2 Smoller Model Architecture"}, {"figure_path": "https://arxiv.org/html/2504.05299/extracted/6335965/figures/all_three_plots_horizontal.png", "caption": "Figure 5: Tokenization Strategy Comparisons.\n(Left)\u00a0Training loss curves illustrating the \u201cOCR loss plague\u201d when using string-based tokens in smaller models.\n(Center)\u00a0Aggregated evaluation metrics showing consistently higher scores with learned tokens (orange).\n(Right)\u00a0Scatter plot of OpenCompass-Image vs.\u00a0OpenCompass-Video: learned tokens dominate the higher-scoring region, especially in image-intensive tasks.", "description": "Figure 5 is a composite figure showing the results of comparing two different tokenization strategies (string-based vs. learned tokens) for training vision-language models. The left panel displays training loss curves, highlighting a phenomenon called the \"OCR loss plague\" where string-based tokens hinder the training process of smaller models. The center panel presents aggregated evaluation metrics across various tasks, demonstrating the superior performance of models trained with learned tokens (represented in orange). The right panel is a scatter plot illustrating the relationship between OpenCompass-Image and OpenCompass-Video scores, revealing that models using learned tokens achieve significantly better results, particularly in tasks involving a high proportion of image data. Overall, this figure highlights the importance of using learned tokens for effective training, especially when working with compact vision-language models.", "section": "3 Smol Instruction Tuning"}, {"figure_path": "https://arxiv.org/html/2504.05299/x22.png", "caption": "Figure 6: \nCumulative Effect of Training Strategies on SmolVLM Performance.\nThe visualization shows the progression of performance improvements as different tokenization and prompt engineering strategies are applied sequentially to the SmolVLM base model.\n(Left)\u00a0Image benchmark results show consistent improvements with each added strategy.\n(Right)\u00a0Video benchmark results reveal similar patterns with more pronounced gains.", "description": "Figure 6 demonstrates the incremental performance improvements achieved by applying various training strategies sequentially to the SmolVLM base model.  The left panel shows results for image benchmarks, illustrating consistent performance gains with each successive strategy (system prompts, media intro/outro tokens, masked user prompts). The right panel presents corresponding results for video benchmarks, exhibiting a similar trend but with more significant gains, particularly after the addition of media intro/outro tokens.", "section": "Smol Instruction Tuning"}, {"figure_path": "https://arxiv.org/html/2504.05299/x23.png", "caption": "Figure 7: Impact of Training Strategies on Smol-Scale Multimodal Models.\n(Left)\u00a0Reusing text data from LLM-SFT (SmolTalk) reduces both image and video scores in smaller models.\n(Middle)\u00a0A minimal fraction (0.020.020.020.02%\u20130.050.050.050.05%) of Chain-of-Thought (CoT) data yields optimal results, while heavier CoT usage degrades performance.\n(Right)\u00a0Increasing average video duration beyond 3.53.53.53.5 min leads to diminished returns for both image and video tasks.", "description": "This figure examines the effects of various training strategies on the performance of SmolVLM, a family of compact multimodal models. The left panel demonstrates that reusing text data from a large language model's supervised fine-tuning (LLM-SFT) negatively impacts smaller SmolVLM models' performance on both image and video tasks. The middle panel shows that incorporating a small amount (0.02-0.05%) of Chain-of-Thought (CoT) data improves performance, but using more CoT data reduces performance.  The right panel reveals that increasing the average length of video sequences used during training beyond 3.5 minutes yields diminishing returns on performance for both image and video tasks.", "section": "Smol Instruction Tuning"}, {"figure_path": "https://arxiv.org/html/2504.05299/x26.png", "caption": "Figure 8: Data Details. Training dataset details for Vision (Left) and video (Right), broken down by modality and sub-categories.", "description": "Figure 8 presents a detailed breakdown of the training datasets used in the SmolVLM model.  The left panel shows the composition of the vision training data, categorized by sub-categories such as image captioning, document understanding, visual question answering, and reasoning tasks. The percentages indicate the proportion of each subcategory within the overall vision dataset.  The right panel provides a similar breakdown for the video training data, with sub-categories including video description, visual question answering, temporal understanding, and narrative comprehension. The percentages show the proportion of each subcategory in the video training data. This figure clarifies the diverse and multi-faceted nature of data used in training the SmolVLM model, which contributes to its robust performance across various tasks.", "section": "4 Experimental Results"}]