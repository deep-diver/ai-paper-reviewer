[{"figure_path": "https://arxiv.org/html/2412.09428/extracted/6065175/figs/framework.png", "caption": "Figure 1: Overview of the VMB framework. We employ text and music as two explicit bridges for multimodal music generation. Text-form music description is obtained with the Multimodal Music Description model. Reference music is retrieved with the Dual-track Music Retrieval module. The two bridges are fed into the Explicitly Conditioned Music Generation module to generate output music.", "description": "The VMB framework uses text and music as explicit bridges to enhance multimodal music generation.  The process starts with the Multimodal Music Description model, which takes multimodal inputs (like images or videos) and converts them into detailed text descriptions of music.  Simultaneously, the Dual-Track Music Retrieval module retrieves relevant music pieces from a database, serving as a musical reference. These text and music components are then fed into the Explicitly Conditioned Music Generation module, which uses this information to synthesize the final musical output.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2412.09428/extracted/6065175/figs/3.1.png", "caption": "Figure 2: Pipeline of the Multimodal Music Description Model (MMDM). This process starts with the collection of music videos, followed by automated tagging to refine audio annotations using CLAP embedding similarities. Metadata and thematic descriptions are synthesized by the Llama 3.1 model to create training targets. The training utilizes LoRA fine-tuning in the MMDM to transform multimodal inputs into targeted music descriptions that align with the visual content\u2019s themes.", "description": "The Multimodal Music Description Model (MMDM) pipeline begins by collecting music videos and their associated metadata.  Automated tagging refines the audio annotations by leveraging CLAP embedding similarities. The Llama 3.1 model then synthesizes metadata and generates thematic descriptions. These descriptions serve as training targets.  Finally, the MMDM is trained using LoRA fine-tuning.  This allows it to effectively transform multimodal inputs (video and metadata) into detailed textual music descriptions that capture the essence of the visual content.", "section": "3.1. Multimodal Music Description Model"}, {"figure_path": "https://arxiv.org/html/2412.09428/extracted/6065175/figs/3.2.png", "caption": "Figure 3: Framework of Dual-track Music Retrieval and Explicitly Conditioned Music Generation. The left part illustrates the Dual-track Music Retrieval process, which leverages our multimodal dataset to perform both broad and targeted retrieval. The right part shows the Explicitly Conditioned Music Generation pathway, where music is generated through a ControlFormer block integrating embeddings from selected music bridge, text bridge, and noisy inputs.", "description": "The figure presents the architecture of the Dual-track Music Retrieval and the Explicitly Conditioned Music Generation. The Dual-track Music Retrieval uses a multimodal dataset to retrieve music based on two strategies: broad retrieval, which focuses on overall thematic alignment, and targeted retrieval, which allows for user control over specific musical attributes like tempo, instrumentation, and genre.  The Explicitly Conditioned Music Generation module uses a ControlFormer block to generate music by integrating embeddings from the retrieved music (music bridge), the textual description (text bridge), and noise inputs.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2412.09428/extracted/6065175/figs/Distribution_of_PAM_Scores.png", "caption": "Table 4: Video-to-Description Generation Performance.", "description": "This table presents a comparison of different models on the video-to-description generation task, evaluating their ability to generate textual descriptions of music that align with the content of a given video. The table includes CLAPScore results for GPT-4V, InternVL, and the proposed MMDM. A higher CLAPScore indicates better alignment between the generated description and the music.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.09428/extracted/6065175/figs/dataset_distributions.png", "caption": "Table 5: Attribute control effectiveness measured by average change (\u0394\u0394\\Deltaroman_\u0394) in CLAPScore.", "description": "This table presents the results of the controllable generation experiment, demonstrating VMB's capability to generate music with distinct attributes.  The table focuses on the average change (\u0394) in CLAPScore across different control dimensions: Instrument, Genre, and Mood.  CLAPScore measures how well the generated music aligns with textual descriptions, thus a larger \u0394 indicates more effective control. The results show that VMB can effectively control for specific instruments, moderately control genre, and achieve some level of mood manipulation in generated music.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.09428/extracted/6065175/figs/dataset_distributions2.png", "caption": "Figure 4: Distribution of PAM Scores across the raw training dataset.", "description": "This histogram displays the distribution of Perceptual Audio Quality Measure (PAM) scores calculated for the initial dataset of 512k music tracks before filtering. The PAM score, ranging from 0 to 1, quantifies the perceptual quality of audio, with higher scores indicating better quality.  The x-axis represents the PAM score, while the y-axis represents the number of tracks that fall within each score range. This distribution helps understand the initial quality of the music tracks and aids in defining a filtering threshold, as detailed in Appendix A.1.", "section": "A. Dataset Analysis"}, {"figure_path": "https://arxiv.org/html/2412.09428/extracted/6065175/figs/mood_distribution.png", "caption": "Figure 5: Histogram of music duration in the training dataset.", "description": "This histogram displays the distribution of music duration in the training dataset, measured in seconds. The x-axis represents the duration of the music tracks, while the y-axis represents the number of tracks that fall within each duration range. The majority of music tracks are concentrated between approximately 100 and 200 seconds.", "section": "A. Dataset Analysis"}, {"figure_path": "https://arxiv.org/html/2412.09428/extracted/6065175/figs/genre_distribution.png", "caption": "Figure 6: Histogram of text word counts in the training dataset.", "description": "This histogram displays the distribution of word counts for the textual descriptions accompanying the music tracks in the training dataset. The x-axis represents the number of words, while the y-axis represents the frequency of descriptions with that specific word count. Most descriptions fall within a moderate word count range, indicating a balance between conciseness and descriptive detail. The distribution also suggests a variety in the length and complexity of the descriptions, reflecting the diversity of the music they describe.", "section": "A. Dataset Analysis"}, {"figure_path": "https://arxiv.org/html/2412.09428/extracted/6065175/figs/instrument_distribution.png", "caption": "Figure 7: Distribution of mood tags across the retrieval dataset. This histogram shows the frequency of various mood categories, illustrating the emotional diversity captured in our data.", "description": "This histogram presents the distribution of mood tags within the retrieval dataset. The x-axis represents various mood categories, such as \"fun,\" \"energetic,\" \"romantic,\" \"emotional,\" \"holiday,\" \"dreamy,\" \"calm,\" and \"dark.\" The y-axis represents the number of music tracks tagged with each mood. The height of each bar corresponds to the frequency of each mood tag in the dataset. The figure demonstrates a balanced distribution across many mood categories, indicating the diverse emotional representation within the curated dataset for music retrieval. This diversity enables VMB to cater to a wide range of user preferences and input modalities, generating music that accurately aligns with the desired emotional tone.", "section": "A.2. MMDM Training and Retrieval Dataset"}, {"figure_path": "https://arxiv.org/html/2412.09428/extracted/6065175/figs/bpm_distribution.png", "caption": "Figure 8: Genre distribution within the retrieval dataset. This bar graph reflects the variety of music genres represented, indicating the dataset\u2019s broad applicability for genre-specific retrieval tasks.", "description": "This bar chart, situated within the Methodology section under the subsection \"Dual-track Music Retrieval\", displays the distribution of various music genres present in the dataset curated for retrieval tasks. The bars represent the quantity of music tracks associated with each genre, like Pop, Hip Hop & Rap, Folk & Country, Latin Music and Rock, indicating the diversity of musical styles available. This broad genre representation suggests the dataset's suitability for retrieval tasks aimed at generating music aligned with specific genre preferences.", "section": "Methodology"}, {"figure_path": "https://arxiv.org/html/2412.09428/extracted/6065175/figs/hist_audio_duration.png", "caption": "Figure 9: Histogram of instrument tags in our retrieval dataset. This figure shows the range of musical instruments represented, underscoring the dataset\u2019s comprehensive coverage of instrumental music.", "description": "This histogram illustrates the distribution of instrument tags within the dataset curated for music retrieval.  It highlights the variety of instruments represented, emphasizing the dataset's comprehensive coverage of diverse instrumental music, which is crucial for effective retrieval-augmented music generation.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.09428/extracted/6065175/figs/hist_text_word_count.png", "caption": "Figure 10: Density curve of BPM across the retrieval dataset. This plot illustrates the distribution of Beats Per Minute, showcasing the tempo range covered in our collection.", "description": "This density plot visualizes the distribution of beats per minute (BPM) within the music retrieval dataset.  The x-axis represents BPM, and the y-axis represents the density, showing how frequently different tempos occur.  The plot demonstrates the range of tempos available, indicating the variety of musical pacing in the dataset.  This information is used to inform and guide retrieval strategies based on user-specified tempo preferences. ", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2412.09428/extracted/6065175/figs/hist_lexical_diversity.png", "caption": "Figure 11: Histogram of audio durations in retrieval dataset. This shows the distribution of song lengths in the dataset.", "description": "This histogram displays the distribution of audio durations within the MMDM and DMR retrieval dataset, indicating the typical length of music tracks included. Most of the audio durations fall within a consistent range, which suggests that the lengths of songs in this dataset are relatively uniform.", "section": "A.2. MMDM Training and Retrieval Dataset"}, {"figure_path": "https://arxiv.org/html/2412.09428/extracted/6065175/figs/survey.jpg", "caption": "Figure 12: Histogram of text word counts in retrieval dataset. This represents the distribution of word counts in the associated text data.", "description": "This histogram displays the distribution of word counts within the text descriptions associated with each music piece in the retrieval dataset. The x-axis represents the number of words, while the y-axis represents the frequency of occurrence. Most descriptions fall within a mid-range word count, indicating a moderate level of detail in the textual data.", "section": "A. Dataset Analysis"}]