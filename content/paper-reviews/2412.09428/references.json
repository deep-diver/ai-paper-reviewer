{"references": [{"fullname_first_author": "Andrea Agostinelli", "paper_title": "MusicLM: Generating music from text", "publication_date": "2023-01-26", "reason": "This work introduces MusicLM, a state-of-the-art model for generating high-fidelity music from text descriptions, which has significantly influenced the field of text-to-music generation."}, {"fullname_first_author": "Zhe Chen", "paper_title": "InternVL: Scaling up vision foundation models and aligning for generic visual-linguistic tasks", "publication_date": "2023-12-25", "reason": "InternVL introduces a powerful vision foundation model that aligns visual and textual representations, and serves as the foundation for the MMDM in the present work."}, {"fullname_first_author": "Jade Copet", "paper_title": "Simple and controllable music generation", "publication_date": "2024-01-01", "reason": "MusicGen demonstrates simple and controllable music generation, inspiring aspects of the controllable generation in the current work."}, {"fullname_first_author": "Sanjoy Chowdhury", "paper_title": "MeLFusion: Synthesizing music from image and language cues using diffusion models", "publication_date": "2024-01-01", "reason": "This paper proposed a strong baseline for image-text to music generation, by using both image and text as condition for music generation."}, {"fullname_first_author": "Zach Evans", "paper_title": "Stable audio open", "publication_date": "2024-07-26", "reason": "This work open-sources the Stable Audio Open framework, including the DiT model and audio autoencoder which serves as the foundation for the ECMG module in the present work."}]}