[{"figure_path": "https://arxiv.org/html/2504.12322/x1.png", "caption": "Figure 1: Average performance across GRA, vanilla seed dataset and lagrge LLMs distilled data with Qwen-2.5-7B base model.", "description": "This figure displays a bar chart comparing the average performance of different models on a specific benchmark, using the Qwen-2.5-7B model as a base.  The models compared include GRA (the proposed multi-agent small LLM framework), the vanilla seed dataset used for training, and data distilled from large language models (LLMs).  The chart allows for a visual assessment of how GRA performs against traditional methods of data synthesis.  Each bar represents the average performance across various tasks, demonstrating the relative effectiveness of each approach.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2504.12322/x2.png", "caption": "Figure 2: Overview of GRA\u2019s architecture, highlighting its four key modules: (a) The Generator creates domain-specific samples, (b) followed by collaborative evaluation by Reviewers, (c) The Adjudicator resolves conflicts, and (d) Post-Processing refines the results by removing redundancies.", "description": "The figure illustrates the GRA framework's architecture, detailing its four main modules.  The Generator produces initial data samples for a specific domain. These samples are then collaboratively evaluated by multiple Reviewers, who assess their quality and consistency.  Any conflicts in the reviewers' evaluations are resolved by the Adjudicator. Finally, a post-processing module refines the dataset by removing redundant or low-quality samples.", "section": "3.1 Pipeline Overview"}, {"figure_path": "https://arxiv.org/html/2504.12322/x3.png", "caption": "Figure 3: Performance along data iterations with Qwen-2.5-7B-Base model.", "description": "This figure illustrates the performance of the Qwen-2.5-7B-Base language model as it is trained on data generated iteratively by the GRA framework.  The x-axis represents the cumulative amount of synthetic data used for training, measured in thousands of samples (10K, 20K, 30K, 40K, 50K). The y-axis displays the average performance of the model across various benchmark datasets. The figure shows how the model's performance improves with each iteration of data synthesis, demonstrating the effectiveness of GRA's iterative data refinement process.", "section": "4 Experiment"}, {"figure_path": "https://arxiv.org/html/2504.12322/x4.png", "caption": "Figure 4: comparison across different setting of reviewer and adjudicator, with alpaca as seed dataset and Llama-3.1-8B as base model.", "description": "This figure displays a bar chart comparing the average performance across different settings of the GRA model. Specifically, it contrasts the performance when there is no reviewer, only a single Llama-3.1-8B model as a reviewer, a GRA setting with multiple reviewers, and a GRA setting without an adjudicator. The Alpaca dataset served as the seed dataset for all settings, and Llama-3.1-8B was the base model used for training.", "section": "4.2 Main Results"}, {"figure_path": "https://arxiv.org/html/2504.12322/x5.png", "caption": "Figure 5: Data coverage comparison between vanilla seed dataset and GRA synthetic data.", "description": "This figure uses t-SNE dimensionality reduction to visualize the data distribution of both the vanilla seed dataset and the data generated by GRA.  By comparing the spread and coverage of data points across the 2D embedding space, the figure helps demonstrate GRA's ability to enhance data diversity and explore regions under-represented in the original seed dataset.  The visual comparison highlights how GRA generates synthetic data that expands the scope and distribution of the original dataset.", "section": "5.3 Data Diversity"}, {"figure_path": "https://arxiv.org/html/2504.12322/x6.png", "caption": "Figure 6: Data coverage comparison between large LLMs distilled data and GRA synthetic data.", "description": "This figure displays a t-SNE visualization comparing the diversity of data generated by GRA and data created using distillation from large language models.  The plot shows the distribution of data points in a two-dimensional embedding space, with each point representing an instruction.  A wider spread of points indicates greater data diversity.  The comparison helps to demonstrate whether GRA, using multiple smaller LLMs, can match or exceed the data diversity produced by the traditional method of distilling knowledge from a single, large LLM.", "section": "5.3 Data Diversity"}, {"figure_path": "https://arxiv.org/html/2504.12322/x7.png", "caption": "Figure 7: Data quality score comparison between GRA and single Large LLMs\u2014Qwen-2.5-72B-Instruct.", "description": "This figure presents a comparative analysis of data quality scores generated by the GRA framework and a single large language model (LLM), specifically Qwen-2.5-72B-Instruct.  It displays the frequency distribution of quality scores, offering insights into the distribution and range of scores obtained by both methods. By visually comparing the distributions, one can assess whether GRA produces data with comparable or superior quality to the single large LLM.  The x-axis represents the quality score range, and the y-axis represents the frequency of scores within that range.", "section": "5.4 Data Quality"}, {"figure_path": "https://arxiv.org/html/2504.12322/x8.png", "caption": "Figure 8: The IFD score comparison between the data generated by GRA, the vanilla seed dataset WizardLM, and single large LLM (Qwen-2.5-72B-Instruct) distilled data.", "description": "Figure 8 presents a comparison of Instruction Following Difficulty (IFD) scores across three datasets: data generated using the GRA framework, the WizardLM vanilla seed dataset, and data distilled from the Qwen-2.5-72B-Instruct large language model.  The IFD score measures the complexity and knowledge density of the data.  Higher IFD scores indicate that the data requires a more advanced understanding and reasoning capabilities from the model.", "section": "5.5 Data Difficulty"}]