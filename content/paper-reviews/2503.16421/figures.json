[{"figure_path": "https://arxiv.org/html/2503.16421/x2.png", "caption": "Figure 1: Example videos generated by MagicMotion. MagicMotion consists of three stages, each supporting a different level of control from dense to sparse: mask, box, and sparse box. Given an input image and any form of trajectory, MagicMotion can generate high-quality videos, animating objects in the image to move along the user-specified path.", "description": "This figure showcases example videos generated using MagicMotion.  The process involves three stages, each demonstrating a different level of control, progressing from dense to sparse guidance.  Stage 1 uses dense masks to define object movement. Stage 2 employs bounding boxes for less precise control.  Finally, Stage 3 utilizes sparse bounding boxes, offering the most flexible, though less precise, control. The input to MagicMotion is a single image and a trajectory (path) defined by the user. The system then generates high-quality video where the designated objects within the image smoothly move along the specified trajectory path.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2503.16421/x3.png", "caption": "Figure 2: Overview of MagicMotion Architecture (text prompt and encoder are omitted for simplicity).\nMagicMotion employs a pretrained 3D VAE to encode the input trajectory, first-frame image, and training video into latent space. It has two separate branches: the video branch processes video and image tokens, and the trajectory branch uses Trajectory ControlNet to fuse trajectory and image tokens, which is later integrated to the video branch through a zero-initialized convolution layer.\nBesides, diffusion features from DiT blocks are concatenated and processed by a trainable segment head to predict latent segmentation masks, which contribute to our latent segment loss.", "description": "MagicMotion uses a pretrained 3D Variational Autoencoder (VAE) to transform the input trajectory, the first frame of the video, and the training video into a latent space.  The architecture has two main branches. The video branch processes video and image tokens. The trajectory branch uses a Trajectory ControlNet to combine trajectory and image tokens. These combined tokens are then integrated into the video branch via a zero-initialized convolution layer.  Diffusion features from DiT (Diffusion Transformer) blocks are then combined and processed by a trainable segment head to generate latent segmentation masks, improving the model's understanding of object shapes and contributing to the latent segmentation loss. ", "section": "3. Model Architecture"}, {"figure_path": "https://arxiv.org/html/2503.16421/x4.png", "caption": "Figure 3: Overview of the Dataset Pipeline. The Curation Pipeline is used to construct trajectory annotations, while the Filtering Pipeline filters out unsuitable videos for training.", "description": "Figure 3 illustrates the two-stage pipeline used to create the MagicData dataset. The first stage, the Curation Pipeline, extracts trajectory information from the video-text dataset, starting by identifying main moving objects from the textual descriptions and then using SAM to generate segmentation masks and bounding boxes. The second stage, the Filtering Pipeline, removes unsuitable videos using various criteria, such as optical flow analysis and checks on mask and bounding box sizes, to ensure only high-quality and relevant videos are included for training.  This process results in a refined dataset suitable for training a trajectory controllable video generation model.", "section": "3.5. Data Pipeline"}, {"figure_path": "https://arxiv.org/html/2503.16421/x5.png", "caption": "Figure 4: Comparison results of different object number on MagicBench. To present the results more clearly, we have negated the FVD and FID scores.", "description": "Figure 4 presents a radar chart comparison of various video generation methods across different numbers of moving objects, evaluated using the MagicBench benchmark.  The chart visualizes the performance of each method on metrics such as Fr\u00e9chet Video Distance (FVD) and Fr\u00e9chet Inception Distance (FID), both critical for assessing video quality.  Importantly, FVD and FID scores have been negated for clearer visual representation, with lower scores indicating better performance. This allows for easy comparison of methods' effectiveness in handling varying numbers of objects (1, 2, 3, 4, 5 and more than 5 moving objects). The figure illustrates how effectively each method controls objects in videos with differing levels of complexity, providing insights into their ability to maintain accuracy and quality with increasing object counts.", "section": "4. Experiment"}, {"figure_path": "https://arxiv.org/html/2503.16421/x6.png", "caption": "Figure 5: MagicMotion successfully controls the main objects moving along the provided trajectory, while all other methods exhibit significant defects marked with the orange box.", "description": "Figure 5 presents a qualitative comparison of video generation results from MagicMotion and seven other state-of-the-art methods. Each row shows the results of a different method in response to the same prompt and trajectory input.  The top row shows results from MagicMotion, demonstrating successful control over the objects' movement along the specified path. The subsequent rows display results from the competing methods, which all show significant flaws in object tracking and/or visual consistency.  These errors are highlighted with orange boxes in the figure.", "section": "4.2. Comparison with Other Approaches"}, {"figure_path": "https://arxiv.org/html/2503.16421/x7.png", "caption": "Figure 6: Ablation Study on MagicData. Not using MagicData causes the model to generate an unexpected child.", "description": "This ablation study compares the performance of the MagicMotion model trained on the MagicData dataset against a model trained on a different dataset (an ablation dataset created from MeViS and MOSE datasets). The image shows the results of a video generation task where the goal is to move a boy from one position to another.  The model trained without MagicData fails to maintain consistent object identity, generating an unexpected child alongside the intended boy. In contrast, the model trained with MagicData correctly performs the video generation task.", "section": "4.3 Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2503.16421/x8.png", "caption": "Figure 7: Ablation study on the progressive training procedure. Without it, the generated head shapes become noticeably distorted.", "description": "This ablation study investigates the impact of the progressive training procedure on MagicMotion's performance.  The figure compares video frames generated with and without the progressive training. The results show that omitting this training step leads to significant distortions, particularly noticeable in the generated shapes of objects' heads, demonstrating the importance of this training strategy for accurate and consistent video generation.", "section": "4.3 Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2503.16421/x9.png", "caption": "Figure 8: Ablation Study on latent segment loss. Without it, the generated arms appear partially missing.", "description": "This ablation study investigates the impact of the latent segment loss on the model's ability to generate accurate object shapes, particularly when dealing with sparse trajectory information.  The figure shows a comparison of video frames generated with and without the latent segment loss. The results demonstrate that omitting the latent segment loss leads to incomplete or inaccurate object shapes, such as missing parts of a person's arms in the example shown. This highlights the importance of the latent segment loss in improving the model's understanding of object shapes and ensuring the generation of visually consistent and accurate video sequences.", "section": "4.3 Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2503.16421/x10.png", "caption": "Figure 9: Camera motion controlled results. By setting specific trajectory conditions, MagicMotion can control camera movements.", "description": "This figure showcases MagicMotion's capability to control camera movements by manipulating trajectory conditions.  The top row demonstrates rotation, the second and third rows show zooming in and out, respectively, achieved by altering the size of bounding boxes. The bottom two rows illustrate panning left and down,  demonstrating versatile camera control during video generation.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.16421/x11.png", "caption": "Figure 10: Video Editing Results. We use FLUX\u00a0[29] to edit the first-frame image and MagicMotion Stage1 to move the foreground objects following the trajectory of the origin video.", "description": "Figure 10 demonstrates the application of MagicMotion for video editing.  The process begins by using FLUX [29] to modify the first frame of a video. This modified frame then serves as input to MagicMotion's Stage 1.  The trajectory information from the original video is used as guidance for MagicMotion. Consequently, the foreground objects in the video are smoothly animated along the paths from the original video, resulting in high-quality edited videos.  The figure showcases several examples of this video editing approach with varying types of video edits, including transforming a swan, a camel, and a hiker.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.16421/x12.png", "caption": "Figure 11: MagicMotion can generate videos using the same input image and different trajectories (marked by red arrows).", "description": "This figure demonstrates the controllability of MagicMotion. Using a single input image, MagicMotion generates multiple videos by applying different trajectories to the same objects within the image. The red arrows highlight the varied trajectories used for each video.  This showcases MagicMotion's ability to generate diverse video outputs from a single starting point, controlled solely by altering the input trajectory.", "section": "4.2. Comparison with Other Approaches"}, {"figure_path": "https://arxiv.org/html/2503.16421/x13.png", "caption": "Figure 12: Latent Segment Masks visualization. MagicMotion can predict out latent segment masks of each frame even when only provided with sparse bounding boxes guidance.", "description": "Figure 12 shows the visualization of latent segment masks predicted by MagicMotion.  Even when only sparse bounding box trajectory information is available as input, MagicMotion can still accurately generate latent segment masks for every frame of the generated video. This demonstrates MagicMotion's ability to infer detailed object shapes and boundaries from limited guidance, which is crucial for high-quality and precise video generation.", "section": "3.4 Latent Segmentation Loss"}, {"figure_path": "https://arxiv.org/html/2503.16421/x14.png", "caption": "Figure 13: Qualitative Comparisons Results. MagicMotion successfully control the cat jumping over the bowl, while all other methods exhibit significant defects.", "description": "Figure 13 presents a qualitative comparison of different video generation models' performance on a task involving a cat jumping over a bowl.  Each model is given the same input: a still image depicting the cat and bowl along with trajectory information guiding the cat's movement.  The generated videos are shown, revealing that only MagicMotion successfully animates the cat accurately along the specified trajectory, while all other models produce videos that exhibit flaws such as artifacts, shape distortions, or inaccurate motion.", "section": "4.2. Comparison with Other Approaches"}, {"figure_path": "https://arxiv.org/html/2503.16421/x15.png", "caption": "Figure 14: Qualitative Comparisons Results. MagicMotion successfully control the witch flying over the input trajectory, while all other methods exhibit significant defects.", "description": "This figure presents a qualitative comparison of different video generation models' performance in controlling the trajectory of a witch flying over houses.  The input consists of a starting image and a trajectory path.  Each row shows the output of a different model, illustrating how effectively they can generate a video where the witch follows the specified path while maintaining visual quality and consistency.  The comparison highlights MagicMotion's success in accurately controlling the object's movement along the designated trajectory, while other models show significant flaws, such as inconsistencies in object appearance or motion.", "section": "4.2. Comparison with Other Approaches"}, {"figure_path": "https://arxiv.org/html/2503.16421/x16.png", "caption": "Figure 15: Qualitative Comparisons Results. MagicMotion successfully control the elephant walking along the input trajectory, while all other methods exhibit significant defects.", "description": "Figure 15 presents a qualitative comparison of various video generation models' ability to animate an elephant along a predefined trajectory.  The input to each model includes a still image of an elephant and the desired trajectory. MagicMotion accurately animates the elephant following the path, maintaining its shape and visual quality. Conversely, the other methods demonstrate significant inconsistencies, including distortions in the elephant's form, inaccurate trajectory following, or artifacts in the generated video frames.", "section": "4.2. Comparison with Other Approaches"}, {"figure_path": "https://arxiv.org/html/2503.16421/x17.png", "caption": "Figure 16: Qualitative Comparisons Results. MagicMotion successfully control the robot moving along the input trajectory, while all other methods exhibit significant defects.", "description": "Figure 16 presents a qualitative comparison of various video generation models' ability to accurately render a robot's movement along a predetermined path.  The input is a trajectory indicating the desired robot motion, and each model attempts to generate a video sequence showing the robot following that trajectory.  MagicMotion, the method proposed in this paper, successfully animates the robot along the intended path. In contrast, other state-of-the-art methods shown (DragNUWA, DragAnything, LeViTor, SG-I2V, MotionI2V, ImageConductor, and Tora) exhibit significant discrepancies between their generated robot motion and the target trajectory. These discrepancies manifest as inaccuracies in the robot's position, speed, and overall movement fidelity.", "section": "4.2. Comparison with Other Approaches"}, {"figure_path": "https://arxiv.org/html/2503.16421/x18.png", "caption": "Figure 17: Qualitative Comparisons Results. MagicMotion successfully control the tiger\u2019s head moving along the input trajectory, while all other methods exhibit significant defects.", "description": "Figure 17 presents a qualitative comparison of various video generation models' ability to accurately control the movement of a tiger's head along a predefined trajectory.  The comparison highlights MagicMotion's superior performance in precisely animating the tiger's head compared to other methods, which show significant discrepancies and inaccuracies in following the specified trajectory. The other models fail to generate consistent or realistic head movements, underscoring the advanced trajectory control capabilities of MagicMotion.", "section": "4.2. Comparison with Other Approaches"}, {"figure_path": "https://arxiv.org/html/2503.16421/x19.png", "caption": "Figure 18: Additional Ablation results.", "description": "This figure shows a qualitative comparison of video generation results using different settings and datasets. It includes three sets of comparisons: (1) comparing the model trained on MagicData versus the model trained on an ablation dataset; (2) comparing the model trained using the progressive training procedure versus a model trained without it; and (3) comparing the model trained using Latent Segment Loss versus one trained without it.  Each comparison shows a sequence of frames generated by each approach, along with a red box highlighting any noticeable defects or inconsistencies. This allows for a visual evaluation of the impact of different training techniques and data on the quality and accuracy of video generation.", "section": "4.3 Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2503.16421/x20.png", "caption": "Figure 19: Detail information on MagicData.", "description": "Figure 19 presents a detailed statistical overview of the MagicData dataset.  It provides histograms illustrating the distribution of video lengths (number of frames), video heights, and video widths.  This allows for a better understanding of the dataset's characteristics and the variability in video parameters.", "section": "3.5. Data Pipeline"}]