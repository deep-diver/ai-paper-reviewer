[{"figure_path": "https://arxiv.org/html/2502.14499/x1.png", "caption": "Figure 1: Diagram of \\mlgym, a unified framework designed to integrate diverse and open-ended AI research tasks into a single platform for developing and evaluating LLM agents on these tasks.", "description": "The figure shows a schematic diagram of the MLGYM framework.  The framework integrates various components for developing and evaluating large language model (LLM) agents on diverse AI research tasks.  It highlights the interaction between an agent (the LLM), an environment (which includes tools and data access), and a computer system.  The agent receives a task description and interacts with the environment to perform actions, such as generating hypotheses, running experiments, and interpreting results. Feedback from these actions is used to guide the agent's further iterations and improvements. The unified nature of MLGYM allows for streamlined evaluation of multiple LLM agents across different AI research domains.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2502.14499/x2.png", "caption": "Figure 2: Performance profiles comparing Best Attempt@4 and Best Submission@4 across all models and tasks. The x-axis shows the performance ratio threshold \u03c4\ud835\udf0f\\tauitalic_\u03c4 and the y-axis shows the fraction of tasks where a model achieves performance within \u03c4\ud835\udf0f\\tauitalic_\u03c4 of the best model.", "description": "Figure 2 presents performance profiles that compare the models' performance across all tasks within the MLGYM-Bench benchmark. Two performance profiles are shown: one for \"Best Attempt\" (the best performance achieved at any point during the four runs), and one for \"Best Submission\" (the best final performance achieved at the end of the four runs).  The x-axis represents the performance ratio threshold (\u03c4). This threshold indicates how much worse a model's performance can be compared to the best-performing model on a task and still be counted. The y-axis shows the fraction of tasks for which a given model's performance is within \u03c4 of the best model's performance.  In simpler terms, the plots illustrate the probability that a model's performance is within a certain range of the optimal performance across the entire set of tasks.", "section": "7 Results"}, {"figure_path": "https://arxiv.org/html/2502.14499/x3.png", "caption": "Figure 3: Best Attempt AUP@4 vs cost for all models. The x-axis shows the API cost in USD and the y-axis shows the AUP@4 score.", "description": "This figure compares the cost-effectiveness of different large language models (LLMs) in terms of their performance on a benchmark of AI research tasks.  The x-axis represents the total cost of using each LLM's API, while the y-axis shows the Area Under the Performance Profile (AUP) score, a measure that summarizes the model's performance across all tasks.  A higher AUP score indicates better performance.  The plot allows for a visual assessment of which models provide the best balance between performance and cost.  Models appearing higher on the y-axis and lower on the x-axis would be considered more cost-effective.", "section": "7 Results"}, {"figure_path": "https://arxiv.org/html/2502.14499/x4.png", "caption": "Figure 4: Termination Error Distribution by model. The size of the bars corresponds to the number of times each model triggered an exit status.", "description": "This bar chart visualizes the frequency of different error types encountered by various large language models (LLMs) during the execution of machine learning research tasks. Each bar represents an LLM, and the height of the bar corresponds to the total number of times the model terminated due to errors.  The chart provides a breakdown of error types, such as context length exceeded, evaluation errors, file permission errors, cost limit exceeded, and others. This allows for a comparison of the robustness and reliability of different LLMs in handling various issues that might arise during complex AI research tasks.", "section": "Agent Behavior Analysis"}, {"figure_path": "https://arxiv.org/html/2502.14499/x5.png", "caption": "Figure 5: Number of Failed and Incomplete runs per model. The criteria for marking a run as incomplete or failed is described in Section\u00a07.4.1", "description": "This figure presents a bar chart visualizing the number of failed and incomplete runs achieved by different language models during the execution of AI research tasks. A run is classified as failed if it terminates with an error and doesn't produce any valid results. It's marked as incomplete if it finishes with an error but yields at least one valid intermediate result. The chart provides a comparison of model reliability and robustness across various tasks, highlighting the frequency of failures and incomplete runs for each model.", "section": "Agent Behavior Analysis"}, {"figure_path": "https://arxiv.org/html/2502.14499/x6.png", "caption": "Figure 6: Action distribution across all runs. We group the actions into categories following the grouping defined in\u00a0Section\u00a03.5 and Section\u00a07.4.2.", "description": "This figure visualizes the frequency of different actions performed by the AI agents across all their attempts in solving various tasks. The actions are categorized according to their functionalities (e.g., editing, viewing, validating code; running Python scripts; executing bash commands) as described in Sections 3.5 and 7.4.2 of the paper.  It offers insights into the AI agents' workflow strategies when tackling complex tasks.  For example, the proportion of 'Edit' and 'View' actions relative to 'Python' actions may reflect the balance between code refinement and experiment execution during the problem-solving process. The relative scarcity of 'Search' actions could highlight areas for improvement in the agents' information gathering strategies.", "section": "7.4.2 Action Analysis"}, {"figure_path": "https://arxiv.org/html/2502.14499/x7.png", "caption": "Figure 7: Action distribution for each model. We group the actions into categories following the grouping defined in\u00a0Section\u00a03.5 and\u00a0Section\u00a07.4.2.", "description": "Figure 7 is a bar chart that visualizes the frequency of different types of actions taken by five large language models (LLMs) while performing AI research tasks within the MLGYM framework.  The actions are categorized into groups defined in Sections 3.5 and 7.4.2 of the paper. These categories include actions related to editing, viewing, validating, submitting, searching, using Python scripts, and executing bash commands. The chart allows for a comparison of the action patterns of the five LLMs, providing insights into their approaches to solving AI research tasks.", "section": "Agent Behavior Analysis"}, {"figure_path": "https://arxiv.org/html/2502.14499/x8.png", "caption": "Figure 8: Action distribution for each step. We group the actions into categories following the grouping defined in Section\u00a03.5 and Section\u00a07.4.2.", "description": "This figure visualizes the frequency of different action types taken by AI agents at each step during the task-solving process.  Actions are categorized according to their function (e.g., editing files, running code, searching, validating, or submitting results). The chart shows how the relative importance of each action category changes over the course of the process, reflecting the agents' strategy and problem-solving behavior.", "section": "Action Analysis"}, {"figure_path": "https://arxiv.org/html/2502.14499/x9.png", "caption": "Figure 9: Number of Failed and Incomplete runs per task. The criteria for marking a run as incomplete or failed is described in Section\u00a07.4.1", "description": "This figure visualizes the number of failed and incomplete runs for each task in the MLGYM-Bench benchmark.  A 'failed' run is defined as a run that terminates with an error and does not produce a valid intermediate submission (at least one score on the test set is not obtained).  An 'incomplete' run is defined as a run that terminates with an error but does produce at least one valid intermediate submission.  The chart allows for a visual comparison of the relative reliability of different tasks within the benchmark, highlighting those that present the most challenges to the AI agent, as well as those more easily solved. This helps to understand the relative difficulty of the tasks within the MLGYM-Bench suite.", "section": "Agent Behavior Analysis"}, {"figure_path": "https://arxiv.org/html/2502.14499/x10.png", "caption": "Figure 10: Action Distribution for each task. We group the actions into categories following the grouping defined in Section\u00a03.5 and Section\u00a07.4.2.", "description": "Figure 10 displays a breakdown of the actions performed by AI agents across various tasks in the MLGYM benchmark.  The actions are categorized into groups (Edit, View, Validate, Submit, Search, Python, Bash), defined earlier in the paper. The chart visually represents the frequency of each action type for each task, offering insights into the AI agent's workflow and strategies.", "section": "7 Results"}]