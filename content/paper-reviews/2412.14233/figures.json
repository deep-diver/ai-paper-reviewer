[{"figure_path": "https://arxiv.org/html/2412.14233/x1.png", "caption": "Figure 1: (a) We present a comparison of captions from DCE, human, and generalist LMM models annotations, including InternVL2-26B, LLaVA-NeXT, and GPT-4V. (b) visualizes the extent to which the captions in (a) describe multiple objects and various attributes, including Objects 1-8, Object Attributes, OCR, HOI, 2D spatial relations and 3D spatial relations.", "description": "Figure 1(a) compares image captions generated by three different methods: human annotators, the proposed Descriptive Caption Enhancement (DCE) method, and three state-of-the-art Large Multimodal Models (LMMs): InternVL2-26B, LLaVA-NeXT, and GPT-4V.  This visual comparison demonstrates the relative completeness and detail of captions produced by each method. Figure 1(b) provides a quantitative analysis showing how well each method describes the image's content, including specific objects (1-8), various attributes (such as object size, color, and emotion), Optical Character Recognition (OCR) information, Human-Object Interactions (HOI), and 2D/3D spatial relationships between objects.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2412.14233/x2.png", "caption": "Figure 2: Comparisons of caption quality. (a) and (b) show the downstream task performance of LLaVA-v1.5 and LLaVA-NeXT after pretraining with different image captions.", "description": "Figure 2 presents a comparison of the performance of two large multimodal models, LLaVA-v1.5 and LLaVA-NeXT, on downstream tasks.  The models were pre-trained using image captions generated by different methods: human annotation, captions from the InternVL2 model, and captions generated by the proposed DCE method. The figure shows that using captions generated by DCE leads to significantly better performance compared to the other methods on various benchmark tasks, highlighting the quality improvements offered by the detailed and comprehensive captions generated by DCE.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2412.14233/x3.png", "caption": "Figure 3: The DCE pipeline first utilizes various visual specialists to extract both Object and Relation attributes. Then, it uses an LLM to integrate the object attributes into detailed region captions, followed by combining the region captions with relational attributes to generate a comprehensive image caption.", "description": "The figure illustrates the architecture of the Descriptive Caption Enhancement (DCE) pipeline.  The pipeline begins by employing several visual specialists to extract both object-level and relational attributes from an input image. Object-level attributes might include things like size, depth, emotion, OCR text, fine-grained categories (like specific types of animals or plants), and logos. Relational attributes describe the relationships between objects, such as their relative positions (2D and 3D) and human-object interactions (HOI). These extracted attributes are then fed into a large language model (LLM). The LLM first generates detailed captions for individual image regions, integrating the object-level attributes. Subsequently, these regional captions are combined by the LLM along with the relational attributes to generate a final, comprehensive image caption that is richer and more detailed than those produced by other methods.", "section": "3. Approach"}, {"figure_path": "https://arxiv.org/html/2412.14233/x4.png", "caption": "Figure 4: The prompt for using LLM\nto generate an region caption\nby considering object attributes and\nreference captions.", "description": "Figure 4 illustrates the prompt engineering process used to generate detailed region captions with a large language model (LLM).  The prompt incorporates various visual attributes extracted by different specialist models (object detection, emotion recognition, OCR, fine-grained classification) and combines them with a reference caption to provide the LLM with rich contextual information for generating a comprehensive description of a specific image region. This multi-step process aims to generate a caption that integrates various levels of visual detail and avoids inconsistencies.", "section": "3. Approach"}, {"figure_path": "https://arxiv.org/html/2412.14233/x5.png", "caption": "Figure 5: The prompt for LLM\nto generate an image caption\nby considering relation attributes, region location information and captions.", "description": "Figure 5 shows the prompt engineering process used to generate image captions with a large language model (LLM). The prompt combines information from multiple sources: the overall image description, detailed descriptions of specific image regions (with their bounding box coordinates), object relationships (e.g., Human-Object Interaction, relative positions), and object counts.  The goal is to generate a complete and coherent caption that integrates these diverse details, while preserving crucial information such as OCR text, relative object positions, and spatial relationships.", "section": "3. Approach"}, {"figure_path": "https://arxiv.org/html/2412.14233/x6.png", "caption": "Figure 6: Visualization of DCE\u2019s Attribute Fusion: DCE combines object and relational attributes to generate detailed and comprehensive captions.", "description": "Figure 6 showcases two example image captions generated by the Descriptive Caption Enhancement (DCE) system and a baseline Large Multimodal Model (LMM).  The examples highlight how DCE, by incorporating object-level attributes (such as OCR text, fine-grained object categories, and object properties) and relational attributes (spatial relationships between objects and human-object interactions), produces richer and more detailed captions than the LMM. The DCE captions provide more comprehensive context and describe a greater variety of details within the images.", "section": "3. Analysis"}]