[{"content": "| Methods | Inspiration Retrieval Strategy | NF | VF | CF | EA | LMI | R | AQC |\n|---|---|---|---|---|---|---|---|---|\n| SciMON [Wang et al., 2024a] | Semantic & Concept & Citation Neighbors | \u2713 | - | - | - | - | - | - |\n| MOOSE [Yang et al., 2024a] | LLM Selection | \u2713 | \u2713 | \u2713 | - | - | - | \u2713 |\n| MCR [Sprueill et al., 2023] | - | - | \u2713 | - | - | - | \u2713 | - |\n| Qi [Qi et al., 2023] | - | \u2713 | \u2713 | - | - | - | - | - |\n| FunSearch [Romera-Paredes et al., 2024] | - | - | \u2713 | - | \u2713 | - | \u2713 | - |\n| ChemReasoner [Sprueill et al., 2024] | - | - | \u2713 | - | - | - | \u2713 | - |\n| HypoGeniC [Zhou et al., 2024b] | - | - | \u2713 | - | - | - | \u2713 | - |\n| ResearchAgent [Baek et al., 2024] | Concept Co-occurrence Neighbors | \u2713 | \u2713 | \u2713 | - | - | - | - |\n| LLM-SR [Shojaee et al., 2024] | - | - | \u2713 | - | \u2713 | - | \u2713 | - |\n| SGA [Ma et al., 2024] | - | - | \u2713 | - | \u2713 | - | - | - |\n| AIScientist [Lu et al., 2024] | - | \u2713 | \u2713 | - | \u2713 | - | \u2713 | \u2713 |\n| MLR-Copilot [Li et al., 2024f] | - | - | - | - | - | - | - | \u2713 |\n| IGA [Si et al., 2024] | - | - | - | - | - | - | \u2713 | - |\n| SciAgents [Ghafarollahi and Buehler, 2024] | Random Selection | \u2713 | \u2713 | - | - | - | - | - |\n| Scideator [Radensky et al., 2024a] | Semantic & Concept Matching | \u2713 | - | - | - | - | - | - |\n| MOOSE-Chem [Yang et al., 2024b] | LLM selection | \u2713 | \u2713 | \u2713 | \u2713 | \u2713 | \u2713 | - |\n| VirSci [Su et al., 2024] | - | \u2713 | \u2713 | \u2713 | - | - | - | \u2713 |\n| CoI [Li et al., 2024g] | - | \u2713 | - | - | - | - | - | \u2713 |\n| Nova [Hu et al., 2024a] | LLM selection | - | - | - | - | \u2713 | - | - |\n| CycleResearcher [Weng et al., 2024] | - | - | - | - | - | - | \u2713 | - |\n| SciPIP [Wang et al., 2024b] | Semantic & Concept & Citation Neighbors | - | - | - | - | - | - | - |", "caption": "Table 1. Discovery Methods. Here \u201cNF\u201d = Novelty Feedback, \u201cVF\u201d = Validity Feedback, and \u201cCF\u201d = Clarity Feedback, \u201cEA\u201d = Evolutionary Algorithm, \u201cLMI\u201d = Leveraging Multiple Inspirations, \u201cR\u201d = Ranking, \u201cAQC\u201d = Automatic Research Question Construction. The order of methods reflect their first appearance time.", "description": "This table summarizes the key characteristics and components of various methods used for scientific hypothesis discovery.  It contrasts different approaches to literature-based discovery (LBD) and inductive reasoning, showcasing their use of inspiration retrieval strategies, feedback mechanisms (novelty, validity, and clarity), evolutionary algorithms, and techniques for leveraging multiple inspirations and ranking hypotheses. It also indicates whether each method incorporates automated research question construction. The methods are ordered chronologically by their first appearance in the scientific literature.", "section": "2 LLMs for Scientific Hypothesis Discovery"}, {"content": "| Name | Annotator | RQ | BS | I | H | Size | Discipline | Date |\n|---|---|---|---|---|---|---|---|---|\n| SciMON (Wang et al., 2024a) | IE models | \u2713 | - | - | \u2713 | 67,408 | NLP & Biomedical | from 1952 to June 2022 (NLP) |\n| Tomato (Yang et al., 2024a) | PhD students | \u2713 | - | \u2713 | \u2713 | 50 | Social Science | from January 2023 |\n| Qi et al. (2023) | ChatGPT | - | \u2713 | - | \u2713 | 2900 | Biomedical | from August 2023 (test set) |\n| Kumar et al. (2024) | PhD students | - | \u2713 | - | \u2713 | 100 | Five disciplines | from January 2022 |\n| Tomato-Chem (Yang et al., 2024b) | PhD students | \u2713 | \u2713 | \u2713 | \u2713 | 51 | Chemistry & Material Science | from January 2024* |", "caption": "Table 2. Discovery benchmarks aiming for novel scientific findings. The Biomedical data SciMON\u00a0(Wang et\u00a0al., 2024a) collected is up to January 2024.\nRQ = Research Question; BS = Background Survey; I = Inspiration; H = Hypothesis.\nQi et\u00a0al. (2023)\u2019s dataset contains a train set where the publication date of the papers is before January 2023.\n* in the date column represents the authors have checked the papers should not only be published after the date, but are also not available online before the date\u00a0(e.g., through arXiv).\nThe five disciplines Kumar et\u00a0al. (2024) cover are Chemistry, Computer Science, Economics, Medical, and Physics.", "description": "Table 2 presents benchmarks for evaluating methods that aim to discover novel scientific findings using large language models.  The table compares different methods across several key features.  These features include the source of annotations used to create the benchmark (human annotators vs. AI models), the presence or absence of a research question, background survey, inspiration, and hypothesis within the dataset, the size of the dataset (number of papers), the disciplines covered (Biomedical, Social Science, Chemistry, Computer Science, Economics, Medical, Physics), and finally, the date range covered by the dataset. Noteworthy points highlighted are the upper limit of the Biomedical data in SciMON to January 2024, the inclusion of a training dataset with papers published prior to January 2023 in Qi et al. (2023), and the criteria for the * symbol in the date column indicating that the papers had not been available online prior to the date.", "section": "2.4 Benchmarks"}, {"content": "| Benchmark Name | ED | DP | EW | DA | Discipline | Additional Task Details |\n|---|---|---|---|---|---|---|\n| TaskBench (Shen et al., 2023b) | \u2713 | - | - | - | General | Task decomposition, tool use |\n| DiscoveryWorld (Jansen et al., 2024) | \u2713 | - | \u2713 | \u2713 | General | Hypothesis generation, design & testing |\n| MLAgentBench (Huang et al., 2024c) | \u2713 | \u2713 | \u2713 | - | Machine Learning | Task decomposition, plan selection, optimization |\n| AgentBench (Liu et al., 2024b) | \u2713 | - | \u2713 | \u2713 | General | Workflow automation, adaptive execution |\n| Spider2-V (Cao et al., 2024) | - | - | \u2713 | - | Data Science & Engineering | Multi-step processes, code & GUI interaction |\n| DSBench (Jing et al., 2024) | - | \u2713 | - | \u2713 | Data Science | Data manipulation, data modeling |\n| DS-1000 (Lai et al., 2023) | - | \u2713 | - | \u2713 | Data Science | Code generation for data cleaning & analysis |\n| CORE-Bench (Siegel et al., 2024) | - | - | - | \u2713 | Computer Science, Social Science & Medicine | Reproducibility testing, setup verification |\n| SUPER (Bogin et al., 2024) | - | \u2713 | \u2713 | - | General | Experiment setup, dependency management |\n| MLE-Bench (Chan et al., 2024b) | - | \u2713 | \u2713 | \u2713 | Machine Learning | End-to-end ML pipeline, training & tuning |\n| LAB-Bench (Laurent et al., 2024) | - | - | \u2713 | \u2713 | Biology | Manipulation of DNA and protein sequences |\n| ScienceAgentBench (Chen et al., 2024a) | - | \u2713 | \u2713 | \u2713 | Data Science | Data visualization, model development |", "caption": "Table 3. Benchmark for LLM-Assisted Experiment Planning and Implementation. ED = Optimizing Experimental Design, DP = Data Preparation, EW = Experiment Execution & Workflow Automation, DA = Data Analysis & Interpretation. \u201cGeneral\u201d in discipline means a benchmark is not designed for a particular discipline.", "description": "This table presents benchmarks evaluating Large Language Model (LLM) assistance in the experiment planning and implementation phase of scientific research.  It details the benchmarks' focus on various aspects such as optimizing experimental design (ED), data preparation (DP), automating experiment execution and workflows (EW), and data analysis and interpretation (DA). The 'discipline' column indicates whether the benchmark is general-purpose or targeted toward a specific scientific field.  For those familiar with this research, it provides a quick overview of the relevant benchmarks and their characteristics. For those less familiar with this research, it serves as a comprehensive summary of the capabilities of LLMs in aiding experimental procedures and data handling. ", "section": "3 LLMs for Experiment Planning and Implementation"}, {"content": "| Task | Benchmark | Dataset | Metric |\n|---|---|---|---| \n| Citation Text Generation | ALEC (Gao et al., 2023) | ASQA (Stelmakh et al., 2022), QAMPARI (Amouyal et al., 2022), ELI5 (Fan et al., 2019) | Fluency: MAUVE (Pillutla et al., 2021), Correctness: precision, recall. Citation quality: citation recall, citation precision (Gao et al., 2023) |\n|  | CiteBench (Funkquist et al., 2023) | AbuRa\u2019ed et al. (2020), Chen et al. (2021a), Lu et al. (2020), Xing et al. (2020) | Quantitative: ROUGE (Lin, 2004), BertScore (Zhang et al., 2020), Qualitative: citation intent labeling (Cohan et al., 2019), CORWA tagging (Li et al., 2022) |\n| Related Work Generation | None | AAN (Radev et al., 2013), SciSummNet (Yasunaga et al., 2019), Delve (Akujuobi and Zhang, 2017), S2ORC (Lo et al., 2020), CORWA (Li et al., 2022) | ROUGE (Lin, 2004), BLEU (Papineni et al., 2002), Human evaluation: fluency, readability, coherence, relevance, informativeness |\n| Drafting and Writing | SciGen (Moosavi et al., 2021) | SciGen (Moosavi et al., 2021) | BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), MoverScore (Zhao et al., 2019), BertScore (Zhang et al., 2020), BLEURT (Sellam et al., 2020), Human evaluation: recall, precision, correctness, hallucination |\n|  | SciXGen (Chen et al., 2021b) | SciXGen (Chen et al., 2021b) | BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), MoverScore (Zhao et al., 2019), Human evaluation: fluency, faithfulness, entailment and overall |", "caption": "Table 4. Evaluation Methods for automated paper writing, which includes three subtasks: citation text generation, related work generation, and drafting and writing. For the related work generation, there is no universally recognized benchmark.", "description": "Table 4 presents evaluation methods used in automated scientific paper writing. It focuses on three key subtasks: citation text generation, related work generation, and drafting & writing.  The table details the specific datasets, evaluation metrics (both quantitative and qualitative), and benchmarks used for each subtask to assess the quality and effectiveness of automated writing systems. Notably, it highlights the absence of a universally accepted benchmark for evaluating related work generation.", "section": "4 LLMs for Scientific Paper Writing"}, {"content": "| Dataset Name | PR | MR | Additional Task | S | C | D | H |\n|---|---|---|---|---|---|---|---| \n| MOPRD (Lin et al., 2023b) | \u2713 | \u2713 | Editorial decision prediction, Scientometric analysis | \u2713 | \u2713 | \u2713 | - |\n| NLPEER (Dycke et al., 2023) | \u2713 | \u2713 | Score prediction, Guided skimming, Pragmatic labeling | \u2713 | \u2713 | - | - |\n| MReD (Shen et al., 2022) | - | \u2713 | Structured text summarization | \u2713 | - | - | \u2713 |\n| PEERSUM (Li et al., 2023a) | - | \u2713 | Opinion synthesis | \u2713 | \u2713 | - | - |\n| ORSUM (Zeng et al., 2024) | - | \u2713 | Opinion summarization, Factual consistency analysis | \u2713 | \u2713 | - | \u2713 |\n| ASAP-Review (Yuan et al., 2022) | \u2713 | - | Aspect-level analysis, Acceptance prediction | \u2713 | - | - | - |\n| REVIEWER2 (Gao et al., 2024) | \u2713 | - | Coverage & specificity enhancement | \u2713 | - | \u2713 | - |\n| PeerRead (Kang et al., 2018) | \u2713 | - | Acceptance prediction, Score prediction | \u2713 | - | - | - |\n| ReviewCritique (Du et al., 2024) | \u2713 | - | Deficiency identification | \u2713 | - | \u2713 | \u2713 |", "caption": "Table 5. Peer Review Datasets and Evaluation Metrics. The Evaluation Metrics columns use the following abbreviations: PR (Peer Review), MR (Meta-review), S (Semantic Similarity), C (Coherence & Relevance), D (Diversity & Specificity), and H (Human Evaluation). Columns S, C, D, and H represent the evaluation metrics used in the study.", "description": "Table 5 presents a summary of peer review datasets and their evaluation metrics.  It lists several datasets used to benchmark and evaluate the effectiveness of Large Language Models (LLMs) in automated peer review and LLM-assisted workflows.  For each dataset, it shows whether the dataset evaluates peer reviews (PR), meta-reviews (MR), or both.  The table further details which evaluation metrics are used for each dataset.  These metrics include: Semantic Similarity (S), measuring how similar the LLM-generated reviews are to human-written reviews; Coherence and Relevance (C), assessing the logical flow and relevance of the reviews; Diversity and Specificity (D), evaluating the range and depth of feedback in the reviews; and Human Evaluation (H), representing human judgments of review quality.  The table thus provides a comprehensive overview of the resources and methods used for evaluating the performance of LLM-based peer review systems.", "section": "5 LLMs for Peer Reviewing"}]