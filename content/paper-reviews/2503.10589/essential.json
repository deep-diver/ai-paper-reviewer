{"importance": "This paper is important for researchers because it **addresses a critical gap in video generation by enabling the creation of longer, coherent, multi-shot scenes.** LCT offers a practical way to extend existing single-shot models, paving the way for more complex and realistic video content creation. The emergent capabilities demonstrated by LCT, such as compositional generation and shot extension, **open new avenues for interactive video editing and personalized content creation.**", "summary": "LCT: Fine-tunes single-shot video diffusion models for coherent multi-shot video generation without extra parameters!", "takeaways": ["Long Context Tuning (LCT) expands the context window of pre-trained single-shot video diffusion models to learn scene-level consistency directly from data.", "LCT enables both joint and auto-regressive shot generation without additional parameters by expanding full attention mechanisms from individual shots to encompass all shots within a scene.", "Models with bidirectional attention after LCT can further be fine-tuned with context-causal attention, facilitating auto-regressive generation with efficient KV-cache."], "tldr": "Recent advances in video generation can produce realistic single-shot videos, but real-world videos require multi-shot scenes with consistency across shots. There is a gap between current single-shot video capabilities and real video content production demands. Bridging this gap necessitates evolving from single-shot synthesis to scene-level generation, where a scene is a series of coherent single-shot videos. This requires consistency in visual appearance and temporal dynamics to ensure a coherent narration flow. Current solutions struggle with both visual and temporal coherence across shots. \n\nTo solve the problem, the paper introduces **Long Context Tuning (LCT), a training paradigm that expands the context window of pre-trained single-shot video diffusion models to learn scene-level consistency directly from data.** LCT expands full attention mechanisms to encompass all shots within a scene, incorporating interleaved 3D position embedding and an asynchronous noise strategy. This enables both joint and auto-regressive shot generation without additional parameters. The method demonstrated single-shot models after LCT can produce coherent multi-shot scenes and exhibit compositional generation and interactive shot extension.", "affiliation": "The Chinese University of Hong Kong", "categories": {"main_category": "Computer Vision", "sub_category": "Video Understanding"}, "podcast_path": "2503.10589/podcast.wav"}