[{"figure_path": "https://arxiv.org/html/2503.13288/x1.png", "caption": "Figure 1: Comparisons of different decoding paradigms. (a) is auto-regressive decoding, which has high efficiency but lacks global awareness. (b) represents search-based methods, which requires huge search space with extensive time cost. (c) is the foresight sampling strategy. It leverages the simulated future steps to estimate the step value, which can strike a balanced inference-time exploration and exploitation.", "description": "This figure compares three different decoding paradigms for large language models (LLMs): auto-regressive decoding, search-based methods, and foresight sampling. Auto-regressive decoding is efficient but lacks global awareness, meaning it only considers preceding steps when generating the next one. Search-based methods explore the vast search space of possible steps but are computationally expensive.  Foresight sampling strikes a balance by using simulated future steps to estimate the optimal step, improving both efficiency and global awareness.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2503.13288/x2.png", "caption": "Figure 2: The overall framework of \u03d5italic-\u03d5\\phiitalic_\u03d5-Decoding. We visualize the decoding process at the timestamp t\ud835\udc61titalic_t. For simplicity, we set step beam size M\ud835\udc40Mitalic_M as 2, the number of rollouts N\ud835\udc41Nitalic_N as 3, and the number of clusters K\ud835\udc3eKitalic_K as 2.", "description": "Figure 2 illustrates the \u03d5-Decoding framework, focusing on the decoding process at a single timestamp (t).  It simplifies the visualization by using a step beam size (M) of 2, 3 rollouts (N), and 2 clusters (K). The figure shows how \u03d5-Decoding uses foresight sampling, in-width pruning, in-depth pruning and cluster alignment to estimate and select optimal steps during inference.  It details the steps involved in: step rollout, foresight calculation, advantage and alignment estimation, and final step sampling. The process combines absolute and relative step value estimations to select the best steps for continuation.", "section": "2 Methodology"}, {"figure_path": "https://arxiv.org/html/2503.13288/x3.png", "caption": "Figure 3: Inference-time scaling law on LLaMA3.1-8B-Instruct. The horizontal axis denotes the inference-time computational cost, while the vertical axis represents the average performances on 6 benchmarks.", "description": "This figure demonstrates the relationship between inference-time computational cost and model performance across six reasoning benchmarks using the LLaMA3.1-8B-Instruct large language model.  The x-axis represents the computational cost (in FLOPS), while the y-axis shows the average performance across the six benchmarks.  The graph illustrates how different decoding methods scale with increasing computational resources, showing their trade-off between efficiency and accuracy.", "section": "3. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.13288/x4.png", "caption": "Figure 4: Analysis on the accuracy of step value estimation. The bar in light blue represents the accuracy of the step values, while the bar in dark blue denotes the averaged task performances.", "description": "Figure 4 illustrates the correlation between the accuracy of step value estimation and the overall task performance for four different decoding methods: Autoregressive, Tree-of-Thought, Predictive Decoding, and the proposed \u03c6-Decoding.  The light blue bars represent the accuracy of the step value estimations, indicating how well the estimated values reflect the true rewards associated with each step. The dark blue bars show the average task performance achieved by each method.  The figure demonstrates that more accurate step value estimation generally leads to better overall task performance, highlighting the importance of precise step value estimation in effective decoding strategies.", "section": "4 Analysis"}, {"figure_path": "https://arxiv.org/html/2503.13288/x5.png", "caption": "Figure 5: Visualization of step-wise effects with alleviated overthinking. The first row displays the results for each independent benchmark using the LLaMA backbone, whereas the second row reflects the results with the Mistral backbone.", "description": "This figure visualizes how the proposed dynamic pruning strategy in $\\\\$ -Decoding$ effectively manages computational resources across different reasoning steps. The x-axis represents the step number in the reasoning process, and the y-axis indicates the proportion of computational cost (or the number of steps) allocated to each step. The first row shows the results when using LLaMA as the base language model, while the second row presents the findings when using Mistral.  The bars in different colors correspond to different benchmarks. We can observe that the pruning strategy allocates more resources to the initial steps of the reasoning process, which are more critical for reaching the correct solution, and gradually reduces resources as the reasoning process progresses. This effectively prevents overthinking by avoiding unnecessary computation in later, less critical steps.", "section": "4 Analysis"}, {"figure_path": "https://arxiv.org/html/2503.13288/x6.png", "caption": "Figure 6: Overall pipeline of \u03d5italic-\u03d5\\phiitalic_\u03d5-Decoding.", "description": "The figure illustrates the overall pipeline of the \u03d5-Decoding algorithm.  It starts with an input question, then proceeds through the steps of: step rollout (generating candidate next steps), in-width pruning (filtering less promising candidates), step foresight (simulating the outcomes of choosing each candidate step), step value estimation (calculating how good each candidate step is using foresight information), sampling (selecting the best step based on the estimations), and finally, in-depth pruning (potentially stopping early if the reasoning process is sufficiently advanced). The output is a complete step sequence.", "section": "2 Methodology"}]