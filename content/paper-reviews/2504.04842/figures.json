[{"figure_path": "https://arxiv.org/html/2504.04842/extracted/6341439/figs/fig0_1_0.png", "caption": "Figure 1. Given a portrait image, voice and text, FantasyTalking can generate animated portraits with rich expressions, natural body movements, and identity features. In addition, FantasyTalking can control the motion intensity of animated portraits. Please refer to our supplementary materials for the video results.", "description": "Figure 1 showcases the capabilities of the FantasyTalking model.  Given a single portrait image as input, along with audio (voice) and text prompts, the system generates a short video clip of the portrait realistically animated to speak.  The animation includes not only natural lip synchronization but also expressive facial movements and coherent body motions. Importantly, the model preserves the identity of the person in the portrait.  A key feature highlighted is the model's ability to control the intensity of these motions, allowing users to fine-tune the realism and expressiveness of the generated video. To see the videos, please refer to the supplementary materials.", "section": "1 INTRODUCTION"}, {"figure_path": "https://arxiv.org/html/2504.04842/x1.png", "caption": "Figure 2. Overview of FantasyTalking.", "description": "Figure 2 provides a detailed architecture overview of the FantasyTalking framework.  It illustrates the process of generating a realistic talking portrait video from a single reference image, audio input, and optional text prompts. The diagram shows various components, including audio encoding via Wav2Vec, text encoding using UMT5, face and body motion encoding, the core Diffusion Transformer (DiT) blocks for video generation, and a final reconstruction stage using a 3D decoder.  Key components such as the cross-attention modules for audio and visual feature integration, the face encoder, and the motion network are highlighted. The figure demonstrates how these elements work together to generate a coherent and realistic talking head animation by aligning audio, visual, and text information. Different stages of information processing are shown and their interactions are clearly visualized.", "section": "3 METHOD"}, {"figure_path": "https://arxiv.org/html/2504.04842/x2.png", "caption": "Figure 3. Dual-Stage Audio-Visual Alignment.", "description": "This figure illustrates the dual-stage audio-visual alignment strategy used in the FantasyTalking model.  The first stage (a) shows clip-level training where long-range audio-visual correlations are learned, establishing global coherent motion across the entire scene.  The second stage (b) performs frame-level refinement by focusing specifically on lip movements through a lip-tracing mask, ensuring precise synchronization with audio signals.  The process includes reshaping audio and video tokens to facilitate more precise attention mechanisms at the frame level.", "section": "3.2 Dual-Stage Audio-Visual Alignment"}, {"figure_path": "https://arxiv.org/html/2504.04842/extracted/6341439/figs/compare_htdf_2.png", "caption": "Figure 4. Qualitative comparison on tame talking head dataset (HDTF).", "description": "Figure 4 presents a qualitative comparison of different methods for generating talking head videos on the HDTF (High-Definition Talking Face) dataset. It visually demonstrates the performance of various methods in terms of the realism, naturalness, and synchronization of generated lip movements and overall facial expressions with the input audio. The figure showcases generated video frames alongside reference images, allowing for a direct comparison of the generated output with the original input. The visual comparison helps to understand the strengths and weaknesses of each method in capturing subtle facial expressions, identity preservation, and overall quality of animation.", "section": "4.2 Results and Analysis"}, {"figure_path": "https://arxiv.org/html/2504.04842/extracted/6341439/figs/compare_wild_2.png", "caption": "Figure 5. Qualitative comparison on wild talking head dataset.", "description": "Figure 5 presents a qualitative comparison of various talking head generation methods on a challenging dataset featuring diverse and dynamic real-world scenarios. The figure visually demonstrates the performance differences across different techniques in generating realistic and natural-looking talking head videos.  It highlights the superior quality of FantasyTalking, showcasing its ability to create highly realistic facial expressions, accurate lip synchronization, and coherent body movements, even in complex and uncontrolled settings. Comparisons include examples of less successful outputs from other methods to underscore the advanced performance of the proposed approach.", "section": "4.2 Results and Analysis"}, {"figure_path": "https://arxiv.org/html/2504.04842/extracted/6341439/figs/compare_sonic_1.png", "caption": "Figure 6. Comparison of Motion Intensity Controller with Sonic.", "description": "This figure provides a qualitative comparison of the motion intensity control capabilities of the proposed FantasyTalking model and the Sonic model.  Different rows represent different levels of motion intensity (subtle, natural, and intense), demonstrating how each model responds to adjustments in motion intensity.  The goal is to illustrate the nuances in generated animation and show how precisely the motion intensity can be controlled by FantasyTalking.", "section": "4.2 Results and Analysis"}, {"figure_path": "https://arxiv.org/html/2504.04842/extracted/6341439/figs/compare_hallo3_3.png", "caption": "Figure 7. Comparison of Visualization Results with Hallo3.", "description": "This figure presents a qualitative comparison of video generation results between the proposed FantasyTalking method and the Hallo3 method.  It visually demonstrates the differences in generated video quality, specifically highlighting the superior realism, naturalness of movement, and background consistency achieved by FantasyTalking.  Several examples of talking head videos from both methods are shown, allowing for a direct side-by-side comparison of the results.", "section": "4 EXPERIMENTS"}, {"figure_path": "https://arxiv.org/html/2504.04842/extracted/6341439/figs/ablation_1_2.png", "caption": "Figure 8. Ablation on DAVA.", "description": "This ablation study analyzes the impact of the dual-stage audio-visual alignment (DAVA) strategy on the performance of the FantasyTalking model.  It compares results when using only clip-level alignment, only frame-level alignment, and both (the full DAVA method).  The figure visually demonstrates the differences in generated video quality, showcasing how the combined approach yields superior results in terms of audio-visual synchronization and overall naturalness.", "section": "Ablation Studies and Discussion"}, {"figure_path": "https://arxiv.org/html/2504.04842/extracted/6341439/figs/ablation_2_yaqi.png", "caption": "Figure 9. Ablation on Identity Preservation.", "description": "This ablation study demonstrates the importance of the identity preservation module in FantasyTalking.  The figure shows a comparison of talking head generation results with and without the identity preservation component. It visually showcases how the model's ability to maintain consistent identity characteristics is significantly degraded without the module, leading to distorted facial features and reduced overall realism. The results highlight the effectiveness of the proposed identity preservation method in generating realistic and consistent talking head videos.", "section": "5 ABLATION STUDIES AND DISCUSSION"}, {"figure_path": "https://arxiv.org/html/2504.04842/x3.png", "caption": "Figure 10. Ablation on Motion Intensity Modulation Network.", "description": "This figure visualizes the results of an ablation study on the Motion Intensity Modulation Network. It shows how varying the facial expression coefficient (\u03c9\u03b9) and body movement coefficient (\u03c9b) affects the generated video's quality and motion intensity. Two subfigures are presented: (a) shows the relationship between the coefficients and the Fr\u00e9chet Video Distance (FVD), a metric assessing video quality, demonstrating that extreme values of coefficients (very low or very high motion) reduce quality. (b) illustrates how changing the coefficients impacts the Subject Dynamics (SD), which measures the intensity of motion in the video, showing a strong positive correlation between higher coefficients and higher motion intensity.  This demonstrates the effectiveness of the modulation network in controlling both the realism and dynamic range of the generated talking head videos.", "section": "4 EXPERIMENTS"}]