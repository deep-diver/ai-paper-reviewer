[{"heading_title": "Learnability Gap", "details": {"summary": "The 'Learnability Gap,' as introduced in the paper, underscores a crucial limitation in directly transferring reasoning capabilities from large models to smaller ones. **Smaller models** often **struggle** to effectively learn from the complex reasoning traces (e.g., long CoT) generated by larger models, leading to suboptimal performance. Instead, they tend to benefit more from simpler, shorter reasoning chains that better align with their intrinsic learning capacity. This gap suggests that direct distillation, a common strategy, may not always be the most effective approach for imbuing smaller models with sophisticated reasoning skills. The underlying causes of this gap are multifaceted. Limited domain knowledge of smaller models, coupled with a tendency to be overwhelmed by complex reasoning steps contribute to their inability to effectively leverage information from longer CoT or larger teacher CoT data. The paper addresses this gap by proposing 'Mix Distillation', a strategy that blends various forms of reasoning traces (short and long CoT, reasoning from larger and smaller models) to balance complexity and facilitate effective knowledge transfer. **Mix distillation yields better results**."}}, {"heading_title": "Mix Distillation", "details": {"summary": "**Mix Distillation** is introduced to bridge the observed learnability gap in small models. The approach blends easier-to-learn data with more challenging data. **Mix Distillation** leverages the strengths of both. The insight is small models perform better on data matching their inherent distribution. They struggle with data exhibiting greater distribution shifts. The mixed long CoT and large teacher CoT data's token distribution may become closer to small models' inherent distribution, enabling them to learn more effectively from challenging datasets. Two versions are proposed: **Mix-Long** combines long and short CoT data. **Mix-Large** combines large teacher CoT with small teacher CoT. The experiment results show, this method helps to solve the observed learnability gap."}}, {"heading_title": "CoT Complexity", "details": {"summary": "Chain-of-Thought (CoT) complexity is a critical factor in determining the effectiveness of large language models (LLMs), especially when transferring reasoning capabilities to smaller models. It encapsulates several dimensions, including the **length of reasoning chains**, the **depth of intermediate steps**, and the **intrinsic difficulty of each step**. Longer CoT traces don't always guarantee better outcomes, particularly for smaller models with limited capacity. This suggests a learnability gap where intricate reasoning processes become overwhelming, hindering effective knowledge transfer. **Adapting CoT complexity** is therefore crucial, balancing detail with conciseness to align with the student model's intrinsic learning capacity. Strategies like Mix Distillation, which blends long and short CoT examples, can bridge this gap by exposing models to varied levels of reasoning complexity, enhancing overall performance. It is important to **optimize the CoT complexity** of the teacher model for the distillation process. "}}, {"heading_title": "Speaking Styles", "details": {"summary": "The section on speaking styles reveals a fascinating aspect of language model behavior post-fine-tuning. By analyzing token distribution shifts, the research pinpoints that long CoT and large teacher CoT primarily influence a student model's expressive and stylistic elements. This goes beyond mere factual knowledge transfer; **models adapt their communication style, adopting patterns from the training data**. The identification of tokens like \"wait,\" \"But,\" and \"Let\" as being significantly impacted highlights the nuances of this stylistic adaptation. These words are not directly related to mathematical reasoning itself, yet their alteration suggests a deeper integration of the teacher model's communication nuances. The research emphasizes how fine-tuning with different CoT lengths not only affects the accuracy of the solutions but also subtly reshapes the way a model expresses itself. This offers further insights into the nature of knowledge distillation, where a model learns to mimic the tone and presentation of the teacher model. It would be interesting to study whether these changes are superficial or lead to a more robust understanding of the task and improved generalization in different contexts. Ultimately, this observation underscores the importance of carefully selecting training data to avoid unintended stylistic biases."}}, {"heading_title": "Domain Matters", "details": {"summary": "The influence of domain expertise on model performance is paramount; a model's grasp of domain-specific nuances critically shapes its problem-solving capabilities. Models trained on a diverse range of tasks may show proficiency across different areas but often lack the depth of understanding seen in models specifically honed for a single domain. **This domain-specific understanding often enables more accurate and efficient reasoning.** The disparity in performance arises from the concentrated exposure to domain-relevant data, which enables a model to learn intricate patterns, relationships, and subtleties that would otherwise be diluted in a more generalized training scheme. **Adaptation to specific domains is crucial for achieving state-of-the-art performance** in complex reasoning tasks. The ability to effectively leverage pre-existing knowledge and adapt it to new situations is a hallmark of intelligent systems. "}}]