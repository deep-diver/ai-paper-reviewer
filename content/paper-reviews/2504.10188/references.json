{"references": [{"fullname_first_author": "Jonathan Ho", "paper_title": "Denoising diffusion probabilistic models", "publication_date": "2020-01-01", "reason": "This paper introduces Denoising Diffusion Probabilistic Models (DDPMs), a foundational generative modeling technique upon which the current paper builds."}, {"fullname_first_author": "William Peebles", "paper_title": "Scalable diffusion models with transformers", "publication_date": "2023-01-01", "reason": "This paper introduces Diffusion Transformers (DiT), which serve as a backbone architecture for the generative models examined in the current study."}, {"fullname_first_author": "Robin Rombach", "paper_title": "High-resolution image synthesis with latent diffusion models", "publication_date": "2022-01-01", "reason": "This paper introduces Latent Diffusion Models (LDMs), a approach to perform diffusion in the latent space of an image, which is leveraged to improve training efficiency."}, {"fullname_first_author": "Maxime Oquab", "paper_title": "DINOv2: Learning robust visual features without supervision", "publication_date": "2023-04-07", "reason": "This paper presents DINOv2, a self-supervised learning method that learns robust visual features and is used in the current work to provide high quality representations for warmup."}, {"fullname_first_author": "Sihyun Yu", "paper_title": "Representation Alignment for Generation: Training Diffusion Transformers is Easier than You Think", "publication_date": "2024-01-01", "reason": "This paper explores Representation Alignment for Generation (REPA) which is used as one of the baselines for performance comparision."}]}