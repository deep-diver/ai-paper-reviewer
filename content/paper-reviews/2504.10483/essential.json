{"importance": "This research overcomes the challenge of end-to-end tuning for VAEs with latent diffusion transformers, offering faster training, better image generation, and improved latent space structures. It can potentially accelerate the development and application of generative models.", "summary": "REPA-E unlocks end-to-end tuning for VAE-latent diffusion transformers, achieving faster training and enhanced image generation.", "takeaways": ["REPA-E enables end-to-end training of VAEs with latent diffusion models.", "REPA-E significantly accelerates diffusion model training.", "End-to-end tuning with REPA-E improves VAE latent space structure."], "tldr": "Traditional wisdom dictates that end-to-end training is optimal, but latent diffusion models (LDMs) often keep the variational auto-encoder (VAE) fixed due to latent space collapse when directly updating the VAE with the diffusion loss. Existing methods also have limitations, such as training inefficiency. This paper identifies that while direct diffusion-loss is ineffective.\n\nTo tackle these problems, this work introduces REPA-E, a novel training recipe that unlocks end-to-end training via representation-alignment (REPA) loss, enabling joint tuning of both VAE and diffusion model. REPA-E speeds up training by over 17x-45x and enhances VAE's latent space structure, leading to state-of-the-art image generation performance with an FID of 1.26. **REPA-E represents a significant advancement in latent diffusion models**.", "affiliation": "Australian National University", "categories": {"main_category": "Computer Vision", "sub_category": "Image Generation"}, "podcast_path": "2504.10483/podcast.wav"}