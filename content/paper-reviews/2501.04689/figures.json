[{"figure_path": "https://arxiv.org/html/2501.04689/x2.png", "caption": "Figure 1: We present SPAR3D, a state-of-the-art 3D reconstructor that reconstructs high-quality 3D meshes from single-view images. SPAR3D enjoys a fast reconstruction speed at 0.7 seconds and supports interactive user edits.", "description": "This figure showcases SPAR3D's capabilities by presenting six example images. Each example shows the input image, the generated point cloud, and the resulting 3D mesh. This highlights SPAR3D's ability to create high-quality 3D meshes from single images quickly (0.7 seconds) and allows for interactive user edits. The images demonstrate a variety of objects, illustrating the model's versatility and robustness.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2501.04689/x3.png", "caption": "Figure 2: SPAR3D Overview. Conditioned on the input image, SPAR3D first leverages a point diffusion model to generate a sparse point cloud. The triplane transformer then uses the sampled point cloud and image features to produce high-resolution triplane features. The triplane features are then queried to reconstruct the geometry, texture, and illumination of the object in the image.", "description": "SPAR3D is a two-stage method.  First, a point diffusion model processes the input image to generate a sparse point cloud. Second, a triplane transformer uses this point cloud and image features to create high-resolution triplane features. These features are then used to reconstruct the 3D object's geometry, texture, and illumination.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2501.04689/x4.png", "caption": "Figure 3: Our Differentiable Renderer. We estimate geometry, albedo, lighting, and normal maps from the triplane and metallic/roughness values from the image. We rasterize and interpolate these values as input to our shader (omitted here for simplicity). Our shader uses the Disney BRDF\u00a0[3] and performs Monte Carlo integration. We further perform visibility testing to improve shadow modeling. Finally, we compare the rendered image with the GT image and minimize the rendering loss.", "description": "This figure details the differentiable renderer used in the SPAR3D model.  The process begins by estimating geometry, albedo, lighting, and normal maps using the triplane features and metallic/roughness values derived from the input image. These estimations are then rasterized and interpolated before being fed into a custom shader (based on Disney's BRDF). The shader uses Monte Carlo integration for improved accuracy and includes visibility testing to enhance shadow modeling. Finally, the rendered image is compared against a ground truth image to calculate a rendering loss that is minimized during training, ensuring the model's output aligns with reality.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2501.04689/x5.png", "caption": "Figure 4: Shadow Modeling. We perform visibility testing in screen-space by marching along sampled rays. If any point along the ray has a ray depth which is farther away than the depth map, we consider the entire ray as shadowed.", "description": "This figure illustrates the shadow modeling technique used in the SPAR3D model.  A ray is cast from the camera through each pixel. The depth of each point along the ray is compared to the depth stored in the depth map, which represents the distances to the closest visible surface. If the ray intersects a point whose depth is greater than the depth in the depth map, it implies that the point is occluded, and thus shadowed. This visibility test is performed in screen space, making it computationally efficient.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2501.04689/x6.png", "caption": "Figure 5: Qualitative Comparison. We compare SPAR3D to other state-of-the-art methods visually. SPAR3D not only aligns better with the visible surfaces from images, but also generates higher-quality geometries and textures for the occluded surfaces.", "description": "Figure 5 presents a visual comparison of 3D reconstruction results from SPAR3D and several state-of-the-art methods.  Each row shows the input image followed by the 3D mesh reconstruction from different methods. The comparison highlights that SPAR3D not only accurately reconstructs the visible parts of objects, as shown in the input images, but also produces superior quality geometry and textures, especially for the portions of the objects that are not directly visible in the image.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2501.04689/x7.png", "caption": "Figure 6: Generalization Results. We show qualitative results of SPAR3D on in-the-wild images from 2D generative models (top 2 rows) and ImageNet (bottom 2 rows). The reconstructed meshes exhibit accurate geometric structures with great textures, demonstrating a strong generalization performance of SPAR3D.", "description": "Figure 6 presents a qualitative evaluation of SPAR3D's generalization capabilities.  The top two rows showcase 3D reconstructions from images generated by 2D generative models, illustrating how well SPAR3D handles diverse and complex input styles beyond the controlled datasets used for training.  The bottom two rows demonstrate reconstructions from ImageNet images, further expanding the range of tested image types. In all cases, the generated 3D meshes accurately reflect the objects' geometric details and textures, even for occluded parts not directly visible in the 2D input. This successful reconstruction across a wide variety of image sources highlights SPAR3D's robustness and strong generalization performance.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2501.04689/x8.png", "caption": "Figure 7: Editing Results. We show qualitative examples of interactive editing with SPAR3D. On the left two examples, we add a handle to the mug and a tail to the elephant doll by duplicating existing points. On the right two examples, we move or delete points to fix imperfections and to improve local details on the mesh. All the edits are performed in Blender within a minute.", "description": "This figure showcases the interactive editing capabilities of SPAR3D.  The left side demonstrates adding features (a handle to a mug, a tail to an elephant) by duplicating existing points in the sparse point cloud. The right side shows how imperfections are fixed and fine details are improved by manipulating points (moving or deleting them). The entire editing process, done in Blender, takes under a minute.", "section": "3.3 Interactive Editing"}, {"figure_path": "https://arxiv.org/html/2501.04689/x11.png", "caption": "Figure 8: Generated Mesh with Conflicting Cues. Under conflicting cues from images and point clouds, our model reconstructs the visible surface based on the image, while generating the backside surface based on the point cloud.", "description": "This figure demonstrates the robustness of the SPAR3D model when presented with conflicting information from the input image and the point cloud.  The input consists of an image of a squirrel and a point cloud of a horse.  The resulting 3D mesh generated by SPAR3D prioritizes the image data for the visible surface, accurately representing the squirrel. However, the back surface, which is not visible in the image, is generated using information from the horse point cloud. This highlights the model's ability to integrate cues selectively from multiple sources to create a coherent and accurate 3D reconstruction, even when those cues are contradictory.  The resulting mesh shows a squirrel from the front, but the back side appears more horse-like.", "section": "4.5. Analysis"}, {"figure_path": "https://arxiv.org/html/2501.04689/x12.png", "caption": "Figure 9: Decomposition and Relighting Results. We show decomposed albedo and relighting results of SPAR3D in comparison with SF3D. The albedo estimated by SPAR3D has less baked-in lighting compared with SF3D and results in better relighting outcomes.", "description": "Figure 9 presents a comparison of albedo and relighting results between SPAR3D and SF3D.  The comparison highlights that SPAR3D produces albedo estimations with less baked-in lighting than SF3D. This leads to more realistic and improved relighting outcomes when using SPAR3D's albedo estimations.", "section": "3. Method"}]