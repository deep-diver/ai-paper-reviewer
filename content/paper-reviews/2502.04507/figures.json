[{"figure_path": "https://arxiv.org/html/2502.04507/x1.png", "caption": "Figure 1: (a) Generating a 5s 720P clip in Hunyuan involves processing 115K tokens, making attention the dominant cost. (b) Attention latency comparison: existing methods fail to translate FLOP reduction into wall-clock speedup; STA\u00a0is hardware-efficient and achieves proportional speedup with sparsity.", "description": "Figure 1 is a two-part figure demonstrating the efficiency of Sliding Tile Attention (STA) in video generation. Part (a) shows the computational breakdown of generating a 5-second, 720p video using the HunyuanVideo model. It highlights that attention operations consume the bulk (around 86%) of the total inference time, processing 115,000 tokens.  Part (b) compares the attention latency of STA against existing methods (FlashAttention-2 and FlashAttention-3). It shows that unlike other methods, STA translates its theoretical FLOP reduction into a proportional wall-clock speedup, which demonstrates its hardware efficiency and effectiveness.", "section": "Abstract"}, {"figure_path": "https://arxiv.org/html/2502.04507/x5.png", "caption": "Figure 2: Visualization of attention locality. The green point means the query point and the magma-colored regions indicate areas of high attention values in response to the query. Instead of attending to the entire image, the query\u2019s attention forms a concentrated local hotspot.", "description": "This figure visualizes the locality of attention in a pretrained video diffusion model.  Each image shows a single query point (green dot) and its corresponding attention weights (magma color scale). The intensity of the color represents the attention score; brighter colors mean higher attention weights.  Instead of attending to all tokens, the query's attention is concentrated in a small spatial-temporal region (hotspot) near the query point.  This demonstrates the highly localized nature of attention, suggesting that only a small portion of the video needs to be processed for each query.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2502.04507/extracted/6182573/media/Fig6/f6-sta.png", "caption": "Figure 3: Left: Fraction of attention scores within a (12, 24, 24) local window across diffusion steps and 10 different prompts. Most heads show high recall, indicating a local attention pattern.\nRight: Despite the different recall across heads, the standard deviation across prompts remains low.", "description": "This figure visualizes the locality of attention in a pretrained video diffusion model. The left panel shows the fraction of total attention scores that fall within a 3D window of size (12, 24, 24) across different diffusion steps and 10 different prompts.  The high recall values observed across most attention heads demonstrate a strong tendency for attention to focus on localized spatial-temporal regions rather than attending to global context. The right panel complements this by showing that despite variations in recall across different heads, the standard deviation of recall scores across the 10 prompts remains low. This implies that the local attention pattern is robust and consistent across different inputs.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2502.04507/x6.png", "caption": "Figure 4: The attention map of NATTEN, Tiled NATTEN, and STA. We plot with an image size 24\u00d7\\times\u00d724 and a 12\u00d7\\times\u00d712 local window. The tile size is set to 4\u00d7\\times\u00d74. (a) NATTEN creates many mixed blocks that are very inefficient for Flash Attention computation. (b) Tiled NATTEN increases the number of dense blocks, but the mixed blocks persist. (c) STA\u00a0completely eliminates the mixed block, making the computation extremely friendly for GPU. Note that we mainly show STA\u2019s application in 3D scenarios for video generation in this paper, but for better illustration, we present the 2D scenario in this plot.", "description": "This figure compares the attention maps of three different attention mechanisms: NATTEN, Tiled NATTEN, and STA.  All are visualized for a 24x24 image with a 12x12 local window and 4x4 tiles.  NATTEN shows many inefficient \"mixed blocks\" where both attended and un-attended values exist, requiring more computation. Tiled NATTEN improves on this slightly by increasing the number of efficient \"dense blocks,\" but mixed blocks remain.  STA, however, entirely eliminates mixed blocks, resulting in a much more efficient computation pattern suitable for GPUs. Although the paper primarily focuses on the 3D application of STA for video generation, this figure uses a simplified 2D example for easier comprehension.", "section": "Methods"}, {"figure_path": "https://arxiv.org/html/2502.04507/extracted/6182573/media/Fig5/human_eval.png", "caption": "Figure 5: 2D Sliding Tile Attention\u00a0with tile size (2, 2) and window size (6, 6). After attending to all the key tiles, each query tile will generate nine 4x4 dense blocks in the attention map. We showcase 2D STA for better illustration. 3D STA can be inferred similarly.", "description": "This figure illustrates the SLIDING TILE ATTENTION (STA) mechanism in two dimensions.  It shows how tiles of query tokens attend to tiles of key tokens within a defined window.  Each query tile interacts with a set of key tiles to form a set of 4x4 dense blocks, avoiding the sparse and irregular attention patterns found in traditional sliding window approaches.  The use of tiles ensures efficient memory access and computation.  The 2D example in the figure serves as a simplified representation to clarify the concept.  The 3D version of STA functions similarly.", "section": "3. Methods"}, {"figure_path": "https://arxiv.org/html/2502.04507/extracted/6182573/media/appendix/reorder.png", "caption": "Figure 6: Qualitative example of 720P 5-second videos. While fine-tuning introduces minor shifts in the output distribution of STA-t-2.43x, the model still preserves high video generation quality. Videos generated by \u0394\u0394\\Deltaroman_\u0394-DiT are generally less sharp than those generated by the original HunyuanVideo and \u00a0STA.", "description": "Figure 6 presents a qualitative comparison of 5-second, 720p videos generated by four different methods: the original HunyuanVideo model, STA without fine-tuning (STA-tf-1.36x), STA with fine-tuning (STA-t-2.43x), and A-DiT. The image showcases that while fine-tuning STA (STA-t-2.43x) leads to subtle changes in the video's visual style, its overall quality remains high. In contrast, videos generated by A-DiT appear noticeably less sharp than those produced by HunyuanVideo and STA.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.04507/extracted/6182573/media/Fig6/f6-swa.png", "caption": "Figure 7: Human evaluation on 200 prompts from the MovieGen Bench\u00a0(Polyak et\u00a0al., 2024). STA\u00a0achieves a 1.36\u00d7 end-to-end speedup while maintaining performance comparable to the original HunyuanVideo. Additionally, STA\u00a0consistently outperforms \u0394\u0394\\Deltaroman_\u0394-DiT across different inference budgets.", "description": "This figure displays the results of a human evaluation comparing the video generation quality and speed of different models.  Specifically, it shows that the proposed SLIDING TILE ATTENTION (STA) method achieves a 1.36x speedup in end-to-end video generation compared to the original HunyuanVideo model while maintaining comparable video quality. Importantly, STA consistently outperforms the A-DiT model across varying inference budgets, demonstrating its superior efficiency and quality in video generation.", "section": "4.2 Human Evaluations"}, {"figure_path": "https://arxiv.org/html/2502.04507/x7.png", "caption": "Figure 8: Left: Conventional zigzag flattening strategy. Right: STA\u2019 sequence flattening strategy. The plot is given assuming a (9, 9) image with (3, 3) tile size.", "description": "This figure compares the conventional zigzag flattening method used in processing image data with the SLIDING TILE ATTENTION (STA) method proposed in the paper.  The zigzag method flattens a 2D image into a 1D sequence in a non-contiguous manner, which can lead to inefficiencies in processing. In contrast, the STA method groups tokens into tiles (in this example, 3x3 tiles within a larger 9x9 image) before flattening. This tile-based approach maintains spatial locality and improves the efficiency of attention mechanisms by processing contiguous data.", "section": "3. Methods"}, {"figure_path": "https://arxiv.org/html/2502.04507/x8.png", "caption": "Figure 9: 2D Sliding Window Attention visualization.", "description": "This figure visualizes how the sliding window attention mechanism works in two dimensions.  It shows how a query token's attention is focused on a localized window of key tokens. The window slides across the input, with each position showing the attention weights between the center query token and the surrounding key tokens within the window.", "section": "Methods"}, {"figure_path": "https://arxiv.org/html/2502.04507/x9.png", "caption": "Figure 10: Qualitative comparisons. While fine-tuning introduces minor shifts in the output distribution of STA-t-2.43x, the model still preserves high video generation quality. Videos generated by \u0394\u0394\\Deltaroman_\u0394-DiT are generally less sharp than those generated by the original HunyuanVideo and \u00a0STA.", "description": "This figure displays a qualitative comparison of video generation results from four different methods: the original HunyuanVideo model, a version using STA without fine-tuning (STA-tf-1.36x), a version using STA with fine-tuning (STA-t-2.43x), and a method called A-DiT.  Two example prompts are used to generate videos, and the resulting videos are shown for each method. The caption highlights that while fine-tuning STA may slightly alter the video distribution, the overall quality remains high. In contrast, videos generated by A-DiT appear less sharp compared to those generated by HunyuanVideo and STA.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.04507/x10.png", "caption": "Figure 11: Qualitative comparisons. While fine-tuning introduces minor shifts in the output distribution of STA-t-2.43x, the model still preserves high video generation quality. Videos generated by \u0394\u0394\\Deltaroman_\u0394-DiT are generally less sharp than those generated by the original HunyuanVideo and \u00a0STA.", "description": "Figure 11 presents a qualitative comparison of video generation results from four different models: the original HunyuanVideo model, STA with training-free mask search (STA-tf-1.36x), STA with finetuning (STA-t-2.43x), and A-DiT.  Two example prompts were used to generate videos. The figure showcases that while fine-tuning STA leads to minor changes in the video distribution (STA-t-2.43x), the overall video quality remains high.  In contrast, videos produced by A-DiT exhibit noticeably lower sharpness compared to those generated by HunyuanVideo and STA.", "section": "4. Experiments"}]