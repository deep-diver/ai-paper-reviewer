[{"figure_path": "https://arxiv.org/html/2504.05782/x1.png", "caption": "Figure 1: Overview of MDK12-Bench. It comprises 140K instances and spans 6 disciplines in K-12 education. The knowledge system of our bench is structured into six fine-grained levels: discipline, grade, curriculum, topic, meta-knowledge, and key knowledge points, where the three rings showcase the first three levels. Examples illustrate the representative grades (elementary, middle, and high schools), difficulty levels (easy, medium, and high), questions and answers, and key knowledge points of each discipline. The diverse question forms (single- and multiple-choice, open-ended question, fill-in-blank, true-or-false) and detailed answer explanations are also demonstrated.", "description": "MDK12-Bench is a multi-disciplinary benchmark for evaluating reasoning in multimodal large language models (MLLMs).  It contains 140,000 real-world K-12 exam questions across six subjects (math, physics, chemistry, biology, geography, and information science).  The questions span various difficulty levels (easy, medium, hard) and grade levels (elementary, middle, high school).  The figure displays the structure of the benchmark's knowledge system, which is organized into six levels (discipline, grade, curriculum, topic, meta-knowledge, and key knowledge points). It also shows example questions and answers from different subjects and difficulty levels, highlighting the diverse question formats included (multiple choice, open-ended, fill-in-the-blank, true/false).", "section": "3. MKD12-Benchmark"}, {"figure_path": "https://arxiv.org/html/2504.05782/x2.png", "caption": "Figure 2: Data curation and statistics of our MDK12-Bench. (a) The data curation pipeline consists of four stages: data collection, screening, parsing, and processing. The knowledge in our benchmark is structured into six interconnected levels: Level 1 - discipline, Level 2 - grade, Level 3 - curriculum, Level 4 - topics, Level 5 - meta-knowledge, and Level 6 - key knowledge point. Statistics of knowledge coverage of our bench is illustrated in terms of the number of instance occurrences at (b) discipline and grade levels; (c) high-school curriculum level; and (d) elementary- and middle-school curriculum level. Examples of curriculum-level knowledge points are also demonstrated.", "description": "Figure 2 illustrates the creation and statistical properties of the MDK12-Bench dataset.  Panel (a) details the four-stage data curation process: data collection from various K-12 exams, data screening to filter out low-quality entries, data parsing to organize the information into a structured format, and data processing for consistency and translation. The figure also highlights the six hierarchical levels used to structure knowledge within the benchmark: discipline, grade, curriculum, topic, meta-knowledge, and key knowledge point. Panels (b), (c), and (d) present the distribution of data across these levels: (b) shows instance counts by discipline and grade, (c) displays high school curriculum instance counts, and (d) shows instance counts for elementary and middle school curricula.  Examples of curriculum-level knowledge points are also included for clarity.", "section": "3. MKD12-Benchmark"}, {"figure_path": "https://arxiv.org/html/2504.05782/x3.png", "caption": "Figure 3: The proposed dynamic MLLMs evaluation pipeline. It includes an image and a text bootstrapping module to mitigate data contamination and a two-stage answer evaluation module comparing the model answers with ground truth.", "description": "This figure illustrates the dynamic evaluation pipeline for multimodal large language models (MLLMs).  It addresses the problem of data contamination by using image and text bootstrapping techniques to create variations of the original questions. The image bootstrapping module applies spatial, color, and style transformations to the images, while the text bootstrapping module uses word substitution, sentence paraphrasing, and question type permutation to modify the text. A two-stage answer evaluation module then compares the model's answers to the ground truth, using GPT as an intermediary to parse the model's response and determine the final score. This process helps to provide a more robust and fair evaluation of MLLM reasoning capabilities.", "section": "4. Dynamic MLLM Evaluation"}, {"figure_path": "https://arxiv.org/html/2504.05782/x4.png", "caption": "Figure 4: Knowledge points (Level 5 - Meta Knowledge) ranked by mean accuracy of Gemini2-thinking on MDK12-Mini dataset.", "description": "This figure presents a bar chart visualization showing the performance of the Gemini2-thinking model on the MDK12-Mini dataset.  It specifically focuses on Level 5 Meta Knowledge points, ranking them by the model's average accuracy.  The chart allows for a detailed analysis of model strengths and weaknesses across various high-level knowledge areas within the dataset, revealing where the model excels and where it struggles.", "section": "4. Dynamic MLLM Evaluation"}, {"figure_path": "https://arxiv.org/html/2504.05782/x5.png", "caption": "Figure 5: Breakdown of accuracy on MDK12-Mini across different exam years.", "description": "This figure displays the breakdown of accuracy across different disciplines and difficulty levels of the MDK12-Mini benchmark over various exam years (from 2016 to 2025).  It provides a visual representation of model performance trends over time, showing how accuracy fluctuates across different years and question types. This allows for analysis of model performance consistency and potential biases related to year-specific patterns in the exam data.", "section": "5.2. Results on MDK12-mini"}, {"figure_path": "https://arxiv.org/html/2504.05782/x6.png", "caption": "Figure 6: Accuracy of MLLMs on full-set of MDK12-Bench. We demonstrate the results across six disciplines (mathematics, physics, chemistry, biology, geography, and information science) and three grade levels (elementary, middle school, and high school).", "description": "Figure 6 presents a comprehensive evaluation of various Multimodal Large Language Models (MLLMs) on the complete MDK12-Bench dataset.  It displays the accuracy of each MLLM across six different academic disciplines: mathematics, physics, chemistry, biology, geography, and information science.  The results are further broken down by grade level, showing performance at the elementary, middle school, and high school levels. This allows for a nuanced understanding of each model's strengths and weaknesses across different subjects and levels of educational complexity.", "section": "5.2. Results on MDK12-mini"}, {"figure_path": "https://arxiv.org/html/2504.05782/x7.png", "caption": "Figure 7: Test logic of using subsets and the fullset data of MDK12-Bench progressively.", "description": "This figure illustrates the progressive testing strategy employed in evaluating models using the MDK12-Bench benchmark.  It starts by evaluating models on smaller subsets (mini-subsets) of the data.  These subsets allow for efficient initial evaluation, particularly for computationally intensive models.  Based on the performance on these subsets, specific knowledge points or question types where the model struggles can be identified. Then, a targeted subset is created focusing on these areas of weakness from the full dataset. This allows for a deeper, more focused evaluation. Finally, a complete evaluation is performed on the full MDK12-Bench dataset. This approach ensures a more efficient and insightful evaluation process by initially focusing on areas needing improvement, before performing a full evaluation.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2504.05782/x8.png", "caption": "Figure 8: Accuracy of InternVL2.5-8B on the sampled subset using different combinations of dynamic bootstrapping strategies.", "description": "This figure illustrates the performance of the InternVL2.5-8B model on a subset of the MDK12-Mini benchmark.  It shows how accuracy changes when using various combinations of dynamic image and text bootstrapping techniques. Each bar represents the average accuracy across 'easy', 'medium', and 'hard' difficulty levels for a specific combination of bootstrapping methods.  The x-axis categorizes the combinations, showing the model's robustness against different levels of data augmentation. The y-axis shows the accuracy percentage.", "section": "5.4. Dynamic Evaluation Results"}]