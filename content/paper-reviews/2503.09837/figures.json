[{"figure_path": "https://arxiv.org/html/2503.09837/x1.png", "caption": "Figure 1: Comparison of image augmentation understanding between humans and Vision Language Models (CLIP/SigLIP). While humans can recognize and describe image transformations like rotation, brightness adjustment, and contrast changes, Vision Language Models show significant limitations in comprehending these basic image manipulations.", "description": "This figure compares the ability of humans and vision-language models (VLMs), specifically CLIP and SigLIP, to understand and describe common image transformations.  It visually demonstrates that while humans easily recognize and articulate changes such as rotation, brightness adjustments, and contrast alterations, the VLMs exhibit significant limitations in comprehending these basic image manipulations. The radar chart displays the accuracy of each model for various image transformations, highlighting the substantial performance gap between human visual understanding and the capabilities of current VLMs.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.09837/x2.png", "caption": "Figure 2: Distribution of individual augmentations applied to the Flickr8k dataset. The augmentations span across multiple transformation types including geometric (rotations, flips), color adjustments (brightness, contrast, saturation), clarity modifications (blur, sharpness), and various image processing effects.", "description": "This figure shows the distribution of various image augmentations applied to the Flickr8k dataset for a research study on vision-language models.  The augmentations are categorized into six groups: geometric transformations (rotations and flips), color adjustments (brightness, contrast, saturation, and hue), clarity modifications (blur and sharpness), geometric distortions (perspective and affine transformations), resolution and size modifications (cropping and stretching), and image processing effects (noise, solarization, posterization, and equalization). The bar chart visually represents the frequency of each individual augmentation used in the dataset, providing insights into the diversity and balance of the augmentation strategies employed.", "section": "3. Dataset & Augmentation Methodology"}, {"figure_path": "https://arxiv.org/html/2503.09837/x3.png", "caption": "Figure 3: Distribution of augmentations applied to the dataset. The augmentations are grouped into six primary categories: Geometric (rotations and flips), Color (brightness, contrast, saturation, and hue adjustments), Clarity (blur and sharpness), Distortion (perspective and affine transformations), Size (cropping and stretching), and Processing (noise, solarization, posterization, and other effects).", "description": "This figure shows a pie chart visualizing the distribution of different types of image augmentations applied to the Flickr8k dataset. The augmentations are categorized into six main groups: Geometric (including rotations and flips), Color (brightness, contrast, saturation, and hue adjustments), Clarity (blur and sharpness), Distortion (perspective and affine transformations), Size (cropping and stretching), and Processing (noise, solarization, posterization, and other effects).  The chart presents the percentage of each category within the total number of augmentations, providing a clear overview of the dataset's composition in terms of augmentation types.", "section": "3. Dataset & Augmentation Methodology"}, {"figure_path": "https://arxiv.org/html/2503.09837/x4.png", "caption": "Figure 4: Accuracy comparison of model performance on augmented prompt recognition. Higher values indicate better understanding of the relationship between textual descriptions of transformations and their visual manifestations.", "description": "This figure displays a bar chart comparing the accuracy of different vision-language models (CLIP and SigLIP with various sizes) in recognizing image transformations from their textual descriptions.  Higher bars represent better accuracy, indicating a stronger understanding of how textual descriptions of transformations (e.g., 'rotated 90 degrees', 'increased brightness') correspond to the actual visual changes in the images.  The chart shows the performance for each model on a variety of individual augmentations. This helps assess the models' capacity to link textual descriptions to visual alterations.", "section": "4.1. Understanding Augmented Descriptions"}, {"figure_path": "https://arxiv.org/html/2503.09837/x5.png", "caption": "Figure 5: Comparison of model performance on augmentations grouped according to their properties.", "description": "This radar chart visualizes the performance of different vision-language models on various image augmentations categorized by their properties (Geometric, Color, Clarity, Distortion, Size, Processing). Each axis represents a category of augmentations, and the distance from the center indicates the model's accuracy on that category.  This allows for a comparison of model performance across different augmentation types and provides insights into the strengths and weaknesses of each model in handling specific image manipulations.", "section": "4. Evaluation of Vision Language Models"}, {"figure_path": "https://arxiv.org/html/2503.09837/x6.png", "caption": "Figure 6: Mean difference between similarity of augmented image with actual prompt and augmented image with augmented prompt", "description": "Figure 6 illustrates the average difference in similarity scores between two scenarios: (1) an augmented image compared to its original caption and (2) the same augmented image compared to a caption that includes a description of the applied augmentation.  It shows how well the models can distinguish between augmented images paired with original versus augmented captions.", "section": "4.2 Matching Augmented Images with Descriptions"}, {"figure_path": "https://arxiv.org/html/2503.09837/x7.png", "caption": "Figure 7: Per Augmentation Accuracy Experiment 2", "description": "This figure presents a detailed comparison of model performance across various image augmentations in Experiment 2.  It shows the accuracy achieved by different Vision-Language Models (VLMs) for each specific augmentation type. This allows for a granular analysis of which augmentations are more challenging for the models to understand and accurately classify.", "section": "4.2.3 Per-Augmentation Analysis"}, {"figure_path": "https://arxiv.org/html/2503.09837/x8.png", "caption": "Figure 8: Per Augmentation Accuracy Experiment 2", "description": "Figure 8 presents a detailed comparison of the performance of different Vision Language Models (VLMs) on various image augmentation tasks. It illustrates the accuracy of each model in identifying specific augmentations (like rotations, flips, brightness changes etc.).  The augmentations are categorized into six groups (Geometric, Color, Clarity, Distortion, Size, and Processing), which helps to reveal the strengths and weaknesses of each model in terms of their understanding of different types of image manipulations.", "section": "4.2. Per-Augmentation Analysis"}, {"figure_path": "https://arxiv.org/html/2503.09837/x9.png", "caption": "Figure 9: Top-1 Accuracy per Augmentation type for all models", "description": "This bar chart visualizes the performance of different Vision Language Models (VLMs) in correctly identifying various image augmentations.  Each bar represents an augmentation type (e.g., rotation, brightness change, blur), and the bar's height indicates the Top-1 accuracy\u2014the percentage of times the model correctly classified that specific augmentation.  Different colored bars represent different VLMs allowing for a comparison of their performance on each augmentation type.  The figure highlights the challenges VLMs face in accurately understanding and classifying various image transformations.", "section": "4.3 Classifying Image Transformations"}]