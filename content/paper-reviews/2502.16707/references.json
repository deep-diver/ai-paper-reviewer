{"references": [{"fullname_first_author": "Brohan, A.", "paper_title": "Rt-2: Vision-language-action models transfer web knowledge to robotic control", "publication_date": "2023-07-31", "reason": "This paper is important as it introduces RT-2, a vision-language-action model that transfers web knowledge to robotic control, a key concept relevant to the current paper's focus on leveraging VLMs for robotic manipulation."}, {"fullname_first_author": "Driess, D.", "paper_title": "PaLM-E: An Embodied Multimodal Language Model", "publication_date": "2023-03-06", "reason": "This paper is important because it presents PaLM-E, an embodied multimodal language model which combines vision and language for robotic tasks, a crucial baseline relevant to the reflective planning approach proposed in the paper."}, {"fullname_first_author": "Liu, H.", "paper_title": "Visual Instruction Tuning", "publication_date": "2023-04-17", "reason": "This paper is important as it introduces visual instruction tuning, which is the core concept in developing the VLM used in the submitted paper."}, {"fullname_first_author": "Rombach, R.", "paper_title": "High-Resolution Image Synthesis with Latent Diffusion Models", "publication_date": "2021-12-27", "reason": "This paper is important since it introduces latent diffusion models, which the paper leverages to imagine future world states for reflective planning."}, {"fullname_first_author": "Brooks, T.", "paper_title": "InstructPix2Pix: Learning to Follow Image Editing Instructions", "publication_date": "2022-11-17", "reason": "This paper is important because the diffusion dynamics model used to imagine future outcomes is initialized with InstructPix2Pix, providing a foundation for predicting visual changes based on instructions."}]}