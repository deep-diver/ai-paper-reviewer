[{"figure_path": "https://arxiv.org/html/2502.05173/x1.png", "caption": "Figure 1: VideoRoPE outperforms RoPE variants on benchmarks.", "description": "This radar chart displays the performance of VideoRoPE and other RoPE variants across multiple video benchmarks: Long Video Bench, MLVU, VideoMME, V-NIAH, and VideoHalluciner.  Each benchmark assesses different aspects of video understanding capabilities. VideoRoPE demonstrates superior performance compared to other methods, showcasing its effectiveness in various video tasks. The chart visually highlights VideoRoPE's consistent improvements across these diverse benchmarks.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2502.05173/x2.png", "caption": "Figure 2: Left: To demonstrate the importance of frequential allocation, based on VIAH (a) we present a more challenging V-NIAH-D task (b) that similar images are inserted as distractors.\nRight: Compared to M-RoPE, our VideoRoPE is more robust in retrieval and is less affected by distractors.", "description": "Figure 2 demonstrates the effect of frequency allocation on video retrieval performance.  The left side shows two examples, (a) V-NIAH (Visual Needle-in-a-Haystack) and (b) V-NIAH-D (Visual Needle-in-a-Haystack with Distractors).  V-NIAH-D is a more challenging variant that introduces visually similar distractor images around the relevant \u2018needle\u2019 image.  The right side presents a comparison of retrieval accuracy between M-ROPE and VideoRoPE on both tasks. This comparison highlights VideoRoPE's improved robustness to distractors, suggesting the effectiveness of its frequency allocation strategy.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2502.05173/x3.png", "caption": "Figure 3: Attention-based frequential allocation analysis.\nMiddle: M-RoPE\u2019s temporal dimension (t\ud835\udc61titalic_t) is limited to local information, resulting in a diagonal layout.\nBottom: VideoRoPE effectively retrieves the needle using the temporal dimension.\nThe x and y coordinates represent the video frame number, e.g., 50 for 50 frames.\nFor more details see Appendix E.", "description": "Figure 3 visualizes the attention weights of different RoPE methods for a video retrieval task. The top row shows the attention weights for M-ROPE, highlighting how its temporal dimension focuses on short-term relationships, resulting in a diagonal pattern. This limitation hinders the model's ability to capture long-range temporal dependencies, which are crucial for accurately identifying the target 'needle' in a long video sequence.  The bottom row displays the attention weights for VideoRoPE, demonstrating its improved ability to capture long-range temporal dependencies. The VideoRoPE's temporal dimension effectively focuses on the relevant temporal segments, allowing it to successfully retrieve the 'needle'. The x and y axes represent the spatial coordinates (horizontal and vertical frame indices) in the video, and the color intensity of each cell represents the magnitude of the attention weight.", "section": "3. Analysis"}, {"figure_path": "https://arxiv.org/html/2502.05173/x4.png", "caption": "(a) Temporal Frequency Allocation in M-RoPE", "description": "This figure compares the temporal frequency allocation strategies of M-ROPE and VideoRoPE.  M-ROPE allocates higher frequencies (shorter monotonic intervals) to the temporal dimension, while VideoRoPE uses lower frequencies (wider monotonic intervals). This difference is visually represented using graphs that show how quickly the frequency changes across different dimensions. The y-axis shows the index of the dimension, and the x-axis shows the token index (or time).  The difference in frequency allocation significantly impacts the robustness of the models to periodic oscillations and distractors, as discussed in the paper.", "section": "3. Analysis"}, {"figure_path": "https://arxiv.org/html/2502.05173/x5.png", "caption": "(b) Temporal Frequency Allocation in VideoRoPE (ours)", "description": "This figure compares the frequency allocation strategies between M-ROPE and VideoRoPE for modeling temporal dependencies.  M-ROPE uses higher frequencies for the temporal dimension resulting in pronounced oscillations and a diagonal layout in the attention patterns. In contrast, VideoRoPE allocates lower frequencies to the temporal dimension, resulting in wider, more monotonic intervals,  mitigating the periodic oscillations and resulting in improved robustness against distractors in long video retrieval. The x-axis represents the token index, indicating the position within the sequence, while the y-axis represents the dimension index, showing the different frequency bands used for temporal encoding.", "section": "4. VideoRoPE"}, {"figure_path": "https://arxiv.org/html/2502.05173/x6.png", "caption": "Figure 4: (a) M-RoPE (Wang et\u00a0al., 2024b) models temporal dependencies using the first 16 rotary angles, which exhibit higher frequencies and more pronounced oscillations. (b) In contrast, VideoRoPE models temporal dependencies using the last 16 rotary angles, characterized by significantly wider, monotonic intervals. Our frequency allocation effectively mitigates the misleading influence of distractors in V-NIAH-D. For a more detailed analysis, please refer to Appendix F.", "description": "Figure 4 illustrates the difference in temporal frequency allocation between M-ROPE and VideoRoPE.  M-ROPE allocates higher frequencies (and thus, more pronounced oscillations) to the early dimensions, making it susceptible to interference from periodic distractors. In contrast, VideoRoPE uses lower frequencies for the temporal dimension, resulting in wider, monotonic intervals that are more robust against periodic distractions in the V-NIAH-D task.  This is because the higher frequency of the temporal dimension in M-ROPE makes it more sensitive to noise and periodic patterns, whereas the lower frequency in VideoRoPE makes it more resilient to noise.", "section": "4. VideoRoPE"}, {"figure_path": "https://arxiv.org/html/2502.05173/x7.png", "caption": "Figure 5: The position embeddings of adjacent text tokens for Vanilla RoPE (top row), the corresponding visual tokens in adjacent frames for M-RoPE (middle row) and our VideoRoPE (bottom row) with interleaved spatial and temporal last design.", "description": "This figure compares the positional embeddings of adjacent text and visual tokens for three different methods: Vanilla RoPE, M-ROPE, and VideoRoPE.  Vanilla RoPE, designed for 1D sequential data, flattens the video frames into a 1D sequence. M-ROPE uses a 3D structure but divides the dimensions into distinct groups for temporal and spatial features.  VideoRoPE, in contrast, employs a 3D structure with an interleaved spatial and temporal layout, aiming to better represent the spatio-temporal relationships in video data. The visualization helps show how each method handles the positional encoding, highlighting differences in how they capture spatial and temporal context.", "section": "3. Analysis"}, {"figure_path": "https://arxiv.org/html/2502.05173/x8.png", "caption": "(a) 3D visualization for Vanilla RoPE.", "description": "This 3D visualization illustrates the relative positional encoding scheme of Vanilla RoPE.  It shows how the vanilla RoPE, designed for 1D sequential data, handles higher dimensional data by flattening it into a single dimension.  The plot lacks the explicit representation of spatiotemporal relationships inherent in 3D data. This visualization helps to illustrate the limitations of applying a 1D positional encoding method to video data, which possesses an inherent 3D structure (temporal, height, width).", "section": "3. Analysis"}, {"figure_path": "https://arxiv.org/html/2502.05173/x9.png", "caption": "(b) 3D visualization for M-RoPE.", "description": "This 3D visualization shows how M-RoPE (Multi-dimensional Rotary Position Embedding) allocates dimensions for representing temporal, horizontal, and vertical information in video data.  It highlights that M-RoPE, while employing a 3D structure, introduces inconsistencies in the index growth pattern for visual tokens across frames. Some indices remain constant, leading to an imbalance in representing spatial and temporal relationships.", "section": "3. Analysis"}, {"figure_path": "https://arxiv.org/html/2502.05173/x10.png", "caption": "(c) 3D visualization for VideoRoPE.", "description": "This 3D visualization showcases VideoRoPE's unique approach to positional encoding in video data.  Unlike previous methods that flatten video frames into a 1D sequence or exhibit inconsistencies in index growth across dimensions, VideoRoPE maintains a consistent index growth pattern while simultaneously incorporating spatial modeling. The visualization clearly illustrates the consistent progression of indices across the temporal (t), horizontal (x), and vertical (y) axes, highlighting the balance achieved between spatial and temporal information.", "section": "3. Analysis"}, {"figure_path": "https://arxiv.org/html/2502.05173/x11.png", "caption": "Figure 6: The 3D visualization for different position embedding. (a) The vanilla 1D RoPE\u00a0(Su et\u00a0al., 2024) does not incorporate spatial modeling.\n(b) M-RoPE\u00a0(Wang et\u00a0al., 2024b), while have the 3D structure, introduces a discrepancy in index growth for visual tokens across frames, with some indices remaining constant.\n(c) In contrast, our VideoRoPE achieves the desired balance, maintaining the consistent index growth pattern of vanilla RoPE while simultaneously incorporating spatial modeling.", "description": "Figure 6 visualizes the positional encoding of three different methods: Vanilla RoPE, M-ROPE, and VideoRoPE.  Vanilla RoPE, being a 1D method, lacks spatial information, resulting in a linear progression along the time axis (t).  M-ROPE attempts 3D modeling, but introduces inconsistencies in the index growth for visual tokens across frames; some indices remain static, breaking the consistent progression. In contrast, VideoRoPE maintains a consistent index growth across all dimensions (t, x, y) while effectively incorporating spatial relationships within its 3D structure. This balanced approach is visualized as a smooth, consistent progression in the figure.", "section": "3. Analysis"}, {"figure_path": "https://arxiv.org/html/2502.05173/x12.png", "caption": "Figure 7: Visualization of the retrieval results for V-NIAH and V-NIAH-D. The color gradient from green to red represents the progression of needle retrieval performance, from perfect to zero.", "description": "Figure 7 visualizes the performance of different RoPE methods on the V-NIAH and V-NIAH-D datasets.  The plots show how well each method can locate the 'needle' (target frame) within the 'haystack' (video sequence), particularly in the presence of distractors (V-NIAH-D). The color gradient shifting from green to red signifies the performance, ranging from perfect retrieval (green) to complete failure (red).  The x-axis shows the length of the haystack, demonstrating the models' ability to perform long-range retrieval. The y-axis represents the number of frames examined. The four subplots show the results for Vanilla RoPE, TAD-ROPE, M-ROPE, and VideoRoPE, respectively.  The figure highlights how VideoRoPE significantly outperforms the others in accurately locating the target frame even with increasing haystack length and distractors.", "section": "5.3. Results on Long Video Retrieval"}]