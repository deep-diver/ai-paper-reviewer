[{"figure_path": "https://arxiv.org/html/2504.10462/x1.png", "caption": "Figure 1: \n(A) Data scaling curve for Modular Multimodal Large Language Model (MLLM) and Sail, our Single Transformer-based MLLM. As pretraining data increases, the single transformer Sail shows a sharper performance gain, demonstrating its superior data scalability.\n(B) Comparison to existing Single Transformer-based MLLMs: our Sail pushes the performance boundaries on both vision tasks and vision-language tasks.", "description": "Figure 1 presents a comparative analysis of the scalability and performance of the SAIL model against other Multimodal Large Language Models (MLLMs).  Panel (A) displays data scaling curves for both modular MLLMs (representing the current state-of-the-art) and the single-transformer SAIL model.  It shows how performance improves as the amount of pre-training data increases. Notably, the SAIL model exhibits a steeper performance increase, highlighting its superior data scalability.  Panel (B) focuses on a comparison of SAIL's performance with existing single-transformer MLLMs on both vision-centric tasks and vision-language tasks.  SAIL demonstrates significantly improved results, pushing the boundaries of what single-transformer models can achieve.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2504.10462/x2.png", "caption": "Figure 2: Model architecture and micro-designs for Sail.\n(A) Model Architecture: Sail is a unified transformer that processes both images and texts without extra module designs. (B) Mixed Attention Mechanism: we adopt bidirectional attention for image patches from the same image and causal attention for text tokens. Examples for a multimodal sequence and a text sequence are provided. Colored squares represent \u201callow to attend\u201d and white squares indicate \u201cprevent from attending\u201d.\n(C) Multimodal RoPE: an illustration of the multimodal rotary position embedding for Sail, with examples for a multimodal sequence and a text sequence.", "description": "Figure 2 illustrates the architecture of SAIL, a unified transformer model for vision and language.  Panel (A) shows the overall architecture: input text and images are preprocessed and fed into the transformer without additional modules. Panel (B) details the mixed attention mechanism, using bidirectional attention within image patches and causal attention for text tokens.  Color-coded squares highlight the attention flow. Panel (C) explains the multimodal rotary position embedding (M-ROPE) used to handle positional information for both image and text data.", "section": "3 SAIL: Training a Single Transformer for Vision and Language"}, {"figure_path": "https://arxiv.org/html/2504.10462/x3.png", "caption": "Figure 3: \nModel scaling of Sail. Left: As the model size increases, the training language modeling loss gradually decreases. Right: As the model size increases, performance on downstream VLM tasks progressively improves.", "description": "This figure demonstrates the effects of model scaling on the performance of the SAIL model. The left panel shows that as the model size increases (from 0.5B to 7B parameters), the training loss for language modeling decreases, indicating better model learning.  The right panel shows that increased model size leads to improved performance on downstream vision-language model (VLM) tasks. This confirms that larger models can capture more complex relationships between visual and textual data, leading to enhanced performance. ", "section": "3 SAIL: Training a Single Transformer for Vision and Language"}, {"figure_path": "https://arxiv.org/html/2504.10462/x4.png", "caption": "Figure 4: \nImage Attention Score Allocation: The figure shows the proportion of image attention scores across different transformer layers for Single Transformer-based MLLM and modular MLLM when predicting tokens. Single Transformer-based MLLM generally allocates higher attention weights to image tokens compared to modular MLLM.", "description": "This figure compares the allocation of attention weights to image tokens across different transformer layers in Single Transformer-based and modular Multimodal Large Language Models (MLLMs).  The x-axis represents the layer index, and the y-axis represents the percentage of attention weights assigned to image tokens.  The graph shows that Single Transformer-based MLLMs consistently allocate a significantly higher proportion of attention to image tokens than modular MLLMs, indicating a more vision-centric processing approach.", "section": "4.3.2 Information Flow Pattern"}, {"figure_path": "https://arxiv.org/html/2504.10462/x5.png", "caption": "Figure 5: Comparison of Sail and LLaVA1.5 on MMVP examples. Sail demonstrates better performance in perceiving minor regions and objects, as well as more accurately distinguishing object states.", "description": "Figure 5 presents a comparison between the performance of SAIL and LLaVA1.5 on the MMVP benchmark.  The figure showcases four example image-based questions where both models answer differently, highlighting the strengths of SAIL.  SAIL outperforms LLaVA1.5 by accurately identifying smaller details (such as patterns on Easter eggs or the direction birds are flapping), and by more reliably differentiating between similar states (e.g., raised vs. lowered elephant trunk). This demonstrates SAIL's enhanced ability to perceive subtle differences and object states compared to LLaVA1.5.", "section": "4.3. Properties of Single Transformer"}, {"figure_path": "https://arxiv.org/html/2504.10462/x6.png", "caption": "Figure 6: \nImage attention score allocation for Sail and its modular MLLM counterpart.\nWe compared the attention score allocation distribution for shallow layers, medium layers, and deep layers between these two models.\nThe Single Transformer-based MLLM model significantly allocates a higher proportion of attention score to image tokens during prediction than the modular MLLM.", "description": "This figure compares the attention mechanisms of a single-transformer based multi-modal large language model (MLLM) called SAIL, and a modular MLLM.  It shows the percentage of attention weights allocated to image tokens at different layers of the transformer networks (shallow, medium, and deep). The key finding is that SAIL consistently allocates a substantially higher proportion of its attention to image tokens than the modular MLLM across all layers. This suggests that SAIL's architecture facilitates a more direct and vision-centric processing of visual information during prediction.", "section": "4.3.2 Information Flow Pattern"}, {"figure_path": "https://arxiv.org/html/2504.10462/x7.png", "caption": "Figure 7: \nVisualization of Sail\u2019s attention distribution across image regions during token prediction.\nIn early transformer layers, attention primarily focuses on the salient regions of the image. As the model progresses to deeper layers, attention shifts to areas more relevant to the predicted tokens.", "description": "This figure visualizes how SAIL's attention mechanism focuses on different parts of an image during the process of predicting tokens.  In the initial layers of the transformer, attention is concentrated primarily on the most visually prominent areas of the image. As processing progresses through the deeper layers of the transformer, the attention shifts towards image regions that are semantically more relevant to the specific token being predicted, demonstrating a refinement of focus from overall saliency to contextual relevance.", "section": "3. SAIL: Training a Single Transformer for Vision and Language"}]