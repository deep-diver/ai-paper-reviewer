[{"figure_path": "https://arxiv.org/html/2501.01904/x1.png", "caption": "Figure 1: The radar chart compares the performance of top-tier MLLMs across four challenging multimodal benchmarks. Our model, Vigor-72B (fine-tuned from Qwen2-VL-72B-Instruct), demonstrates leading performance.", "description": "Figure 1 presents a radar chart comparing the performance of several leading multimodal large language models (MLLMs) across four challenging benchmarks.  The benchmarks assess performance on complex reasoning tasks requiring integration of multiple modalities (e.g., text and images).  The chart visually displays each model's relative strengths and weaknesses across these benchmarks.  The authors highlight that their model, Virgo-72B, achieves the highest overall performance, indicating superiority in multimodal reasoning capabilities. Virgo-72B is a fine-tuned version of the Qwen2-VL-72B-Instruct model.", "section": "Abstract"}, {"figure_path": "https://arxiv.org/html/2501.01904/x2.png", "caption": "Figure 2: The relationship between the average thought length of each benchmark and the corresponding performance of both Virgo and Qwen2-VL-72B-Instruct. The \u201caverage thought length\u201d is represented by the line, while \u201cperformance\u201d is indicated by the bar. The bars in light color represent Vigor\u2019s performance, while the bars in dark color represent Qwen2-VL-72B-Instruct\u2019s performance. We observe that benchmarks with longer thought lengths generally correspond to greater performance improvements.", "description": "Figure 2 illustrates the correlation between the average length of reasoning processes (thought length) in different benchmarks and the performance of two models: Virgo and Qwen2-VL-72B-Instruct.  The graph uses lines to represent the average thought length for each benchmark (MMMU, MathVerse, MathVision, and OlympiadBench), and bars to show the corresponding performance scores of both models on those benchmarks.  Lighter bars indicate Virgo's performance, and darker bars represent Qwen2-VL-72B-Instruct's performance. The figure visually demonstrates a positive correlation: longer average thought lengths generally lead to better performance for both models, indicating that more extensive reasoning improves accuracy in complex problem-solving tasks.", "section": "3.2 Main Results"}, {"figure_path": "https://arxiv.org/html/2501.01904/x3.png", "caption": "Figure 3: The domain distribution of textual long thought instructions.", "description": "This figure shows a breakdown of the textual long-form thought instruction data used to train the Virgo model.  It displays the proportion of instructions belonging to different domains: math, code, science, and puzzle. The visualization helps illustrate the relative quantity of data from each domain, indicating the focus on mathematical problems within the overall training dataset.", "section": "3.3 Further Analysis"}]