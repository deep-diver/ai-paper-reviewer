[{"figure_path": "https://arxiv.org/html/2412.07769/extracted/6059664/Figures/llavamed_spider.png", "caption": "Figure 1: Model Performance Comparison on BiMed-MBench: These comparisons are made across different categories, including CT, MRI, CXR, Histology, Gross, and their Arabic counterparts (CT_ar, MRI_ar, CXR_ar, Histology_ar, Gross_ar). The models compared are LLaVA-pp, LLaVA-Med, BiMediX2, Dragonfly-Med, MiniGPT-Med, and BiomedGPT. Each axis represents the performance score in a specific category, allowing for a visual comparison of how each model performs in bilingual medical contexts.", "description": "This radar chart compares the performance of several large multimodal models (LMMs) on the BiMed-MBench, a bilingual (Arabic-English) medical benchmark.  The models evaluated include LLaVA-pp, LLaVA-Med, BiMediX2, Dragonfly-Med, MiniGPT-Med, and BiomedGPT. Performance is assessed across different medical image categories: Computed Tomography (CT), Magnetic Resonance Imaging (MRI), Chest X-Ray (CXR), Histology, and Gross pathology, in both English and Arabic.  Each axis of the chart represents a specific category, and the model's score on that category determines its distance from the center. This visualization allows for easy comparison of model performance across different modalities and languages.", "section": "Technical Report"}, {"figure_path": "https://arxiv.org/html/2412.07769/x1.png", "caption": "Figure 2: BiMediX2: Overall Architecture Our model is designed for medical image analysis and bilingual multi-turn conversations. Medical images are processed through a Vision Encoder and aligned with a Projector, while the text inputs are tokenized using the default tokenizer. The resulting tokens are then passed into the language model (Meta Llama 3.1) to generate responses in the prompted language. We only train the language model using LoRA adapters, while the projector is finetuned for medical image-text alignment. A robust data generation framework translates an English data corpus into Arabic using GPT-4o, with verification by a medical expert to ensure accurate and contextually appropriate translations. This approach supports effective training and benchmarking in a bilingual context.", "description": "BiMediX2 processes medical images through a Vision Encoder and aligns them with text using a Projector. Text is tokenized and input to Llama 3.1, which generates responses in the prompted language. The language model is trained using LoRA adapters, and the projector is fine-tuned for medical image-text alignment. An English data corpus is translated to Arabic using GPT-40 and verified by medical experts, facilitating bilingual training and benchmarking.", "section": "BIMEDIX2"}, {"figure_path": "https://arxiv.org/html/2412.07769/x2.png", "caption": "Figure 3: State of the art comparison of models in Clinical LLM Benchmarks", "description": "This figure presents a comparison of the state-of-the-art models in Clinical Large Language Model (LLM) benchmarks. Each bar represents a different model and its height represents the average performance across various clinical datasets (Cli-KG, C-Bio, C-Med, Med-Gen, Pro-Med, Ana, MedMCQA, MedQA, USMLE, PubmedQA). BiMediX2 models (4B, 8B, and 70B) are compared against existing LLM models (BioMedGPT, LLaVA-Med, Dragonfly-Med, GPT 3.5 & 4, Meditron, Llama3-Med42, OpenBioLLM, Llama 3.1). Overall, the results show that larger models generally exhibit higher average performance and the proposed models demonstrate competitive performance against existing models on this benchmark.", "section": "EXPERIMENTS"}, {"figure_path": "https://arxiv.org/html/2412.07769/x3.png", "caption": "Figure 4: Performance comparison on UPHILL OpenQA (Kaur et\u00a0al. (2023)), assessing the model\u2019s ability to address false medical claims at different presupposition levels.", "description": "This figure compares the performance of several large language models (LLMs), including BiMediX2, on the UPHILL OpenQA benchmark.  The UPHILL benchmark tests the factual accuracy of LLMs in handling health-related queries, specifically focusing on the models' ability to correctly refute false medical claims. The x-axis shows the model names, and the y-axis represents the overall factual accuracy percentage achieved by each model on the benchmark.", "section": "4 RESULTS"}, {"figure_path": "https://arxiv.org/html/2412.07769/x9.png", "caption": "Figure 5: Qualitative Examples of our BiMediX2 for Medical Image Understanding in a Conversational Context.", "description": "Figure 5 showcases BiMediX2's capabilities in analyzing medical images within a conversational context. The top part presents a conversation about a sagittal CT scan of the lumbar spine, where BiMediX2 identifies the scan type, describes the anatomy, and pinpoints a fracture in the L4 vertebra, explaining potential causes. The bottom section features a color Doppler ultrasound of the left ovary. Here, the model explains the technique, identifies the organ, and notes a potential left ovarian cyst with a solid component, highlighting the need for further evaluation. These examples illustrate BiMediX2's ability to interpret complex medical images and engage in informative medical conversations.", "section": "4 RESULTS"}]