{"references": [{"fullname_first_author": "Brooks", "paper_title": "Video generation models as world simulators", "publication_date": "2024", "reason": "This paper is cited as having revolutionized video generation, enabling the creation of realistic and compelling videos."}, {"fullname_first_author": "Kong", "paper_title": "Hunyuanvideo: A systematic framework for large video generative models", "publication_date": "2025", "reason": "This paper introduces a state-of-the-art text-to-video diffusion model that produces high-resolution and temporally coherent videos."}, {"fullname_first_author": "Yang", "paper_title": "Cogvideox: Text-to-video diffusion models with an expert transformer", "publication_date": "2024", "reason": "This paper presents a model that uses a 3D full attention mechanism and expert transformers to improve motion consistency and semantic alignment."}, {"fullname_first_author": "HaCohen", "paper_title": "LTX-video: Realtime video latent diffusion", "publication_date": "2024", "reason": "This paper introduces a real-time latent text-to-video diffusion model that generates high-quality, temporally consistent videos efficiently."}, {"fullname_first_author": "Zheng", "paper_title": "Open-sora: Democratizing efficient video production for all", "publication_date": "2024", "reason": "This paper presents an efficient text-to-video generation model that uses a decomposed spatial-temporal attention mechanism to balance computational efficiency and video quality."}]}