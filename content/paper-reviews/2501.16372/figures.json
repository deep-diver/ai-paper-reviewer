[{"figure_path": "https://arxiv.org/html/2501.16372/x1.png", "caption": "Figure 1: Vanilla LoRA Adapter and two different modes of the elastic adapter. Mode A allows only the LoRA rank to be elastic, while Mode B also enables the input or output channels to be elastic.", "description": "This figure illustrates the architecture of a vanilla LoRA adapter alongside two variations called elastic adapters.  The vanilla LoRA adapter shows the standard low-rank decomposition of a weight matrix (W) into two smaller matrices (L1 and L2), which are trained while the original matrix remains frozen. In contrast, the elastic adapter offers two modes of operation. Mode A maintains the fixed dimensions of the input and output channels, but allows the rank (r) of the low-rank matrices (L1 and L2) to be dynamically adjusted during training, effectively providing a range of possible rank values. Mode B expands on this by additionally enabling the adjustment of the input and/or output channels' dimensions, giving the model even more flexibility to adapt to different scenarios. This enhances efficiency by only training the smaller adapter matrices.", "section": "Elastic LORA Adapters and Their Applications"}, {"figure_path": "https://arxiv.org/html/2501.16372/x2.png", "caption": "Figure 2: Elastic adapters guide the removal of elements in the frozen model weights, resulting in smaller, high-performing models. This process exemplifies the application of Mode B as depicted in Figure 1.", "description": "This figure illustrates how elastic adapters, specifically Mode B, facilitate model compression.  Mode B allows the adapters to dynamically adjust the input and output channels, as well as the rank.  By integrating with the frozen weights of the base model (shown as the 'W' matrix), the elastic adapters guide the selection of a smaller, more efficient subset of weights. This selective pruning results in a reduced model size with minimal performance degradation, as indicated by the smaller, high-performing model. The original frozen weights are shown with the elastic adapter's output, emphasizing that the adapter is acting as a filter and only utilizing a portion of the original model's parameters.", "section": "Efficient Neural Architecture Search with the Guidance of Low-Rank Adapters"}, {"figure_path": "https://arxiv.org/html/2501.16372/x3.png", "caption": "Figure 3: Elastic low-Rank adapters for fine-tuning sparse efficient models. This style exemplifies the application of Mode A as depicted in Figure 1.", "description": "Figure 3 illustrates how elastic low-rank adapters are used in the fine-tuning of sparse models.  It shows that instead of adapting all weights in a dense layer, only a smaller subset of weights (the ones indicated by the non-zero values in the sparse weights matrix) are modified. The elastic adapters (L1 and L2) are applied only to these non-zero elements. This method, which is an example of Mode A from Figure 1, allows for efficient fine-tuning and reduces the number of parameters that need to be updated. The figure visually shows the integration of sparse weights with the elastic adapters, improving the efficiency of the fine-tuning process.", "section": "Restricting the Elasticity to the Adapter Rank and Exploiting Model Sparsity and Low Numerical Precision"}, {"figure_path": "https://arxiv.org/html/2501.16372/extracted/6149735/figures/search_progression.png", "caption": "Figure 4: Search progression to discover Pareto-optimal low-rank adapter configurations. The horizontal line represents the zero-shot accuracy of the midpoint heuristic sub-adapter.", "description": "This figure visualizes the search process for finding the best low-rank adapter configurations using a multi-objective optimization algorithm (likely NSGA-II).  The plot shows the trade-off between the number of multiply-accumulate (MAC) operations (a measure of computational cost) and the validation accuracy. Each point represents a different configuration tested during the search. The horizontal line indicates the accuracy achieved by a simpler heuristic method that evaluates a midpoint in the search space, providing a baseline for comparison against configurations found through the complete search process. The figure demonstrates how the search algorithm explores the design space and finds configurations that balance performance and efficiency.", "section": "Efficient Neural Architecture Search with the Guidance of Low-Rank Adapters"}]