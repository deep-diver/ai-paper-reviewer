[{"figure_path": "https://arxiv.org/html/2412.21079/x2.png", "caption": "Figure 1: Given two images in the wild, Edicho generates consistent editing versions of them in a zero-shot manner.\nOur approach achieves precise consistency for editing parts (left), objects (middle), and the entire images (right) by leveraging explicit correspondence.", "description": "This figure demonstrates the capabilities of the Edicho model.  Given two input images (unedited versions), Edicho performs consistent zero-shot editing across both images. The results show the model's ability to precisely edit different image components: small parts (left column), larger objects (middle column), and entire scene edits (right column). The consistency is achieved through the use of explicit correspondence between the images, ensuring that the edits are harmoniously applied in each image.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2412.21079/x3.png", "caption": "Figure 2: \nComparisons of the implicit and our explicit correspondence prediction for the images in the wild.\nThe implicit correspondence from cross-image attention calculation is less accurate and unstable with the change of denoising steps and network layers.", "description": "Figure 2 shows a comparison of implicit versus explicit correspondence prediction methods for images from diverse, real-world sources.  The left column shows the input images. The middle column demonstrates the results using an explicit correspondence prediction method. The rightmost columns show results using implicit methods.  The implicit method is less accurate and its results are less consistent as the denoising step and network layer change. This highlights the advantage of the explicit method, which produces more stable and accurate correspondence.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2412.21079/x4.png", "caption": "Figure 3: Framework of\u00a0Edicho.\nTo achieve consistent editing, we first predict the explicit correspondence with extractors for the input images.\nThe pre-computed correspondence is injected into the pre-trained diffusion models and guide the denoising in the two levels of (a) attention features and (b) noisy latents in CFG.", "description": "Edicho's framework for consistent image editing involves two main steps. First, explicit correspondence between input images is predicted using extractors.  Then, this pre-computed correspondence is integrated into pre-trained diffusion models to guide the denoising process at two levels: (a) attention features and (b) noisy latents within the Classifier-free Guidance (CFG) method. This dual-level approach ensures consistency across multiple images.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2412.21079/x5.png", "caption": "Figure 4: \nQualitative comparisons on local editing with Adobe Firefly (AF)\u00a0[45], Anydoor (AD)\u00a0[12], and Paint-by-Example (PBE)\u00a0[58].\nThe inpainted areas of the inputs are highlighted in red.", "description": "Figure 4 presents a qualitative comparison of local image editing results using four different methods: the proposed approach, Adobe Firefly, Anydoor, and Paint-by-Example.  The figure showcases several examples where a small area of the input image has been edited.  The inpainted areas are highlighted in red to clearly distinguish the modified portions from the original image. This comparison highlights the differences in the quality and consistency of the inpainting performed by each method when editing various types of images, including faces, animals, and shoes.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.21079/x6.png", "caption": "Figure 5: \nQualitative comparisons on global editing with MasaCtrl (MC)\u00a0[7], StyleAligned (SA)\u00a0[18], and Cross-Image-Attention (CIA)\u00a0[1].", "description": "Figure 5 presents a qualitative comparison of global image editing results using four different methods: the proposed method and three existing methods, MasaCtrl (MC) [7], StyleAligned (SA) [18], and Cross-Image-Attention (CIA) [1].  Each row shows the input image and the results generated by each method for a specific editing task, using the same text prompt for all methods. This allows for a visual comparison of the consistency and quality of the edits across different approaches. The figure demonstrates how the proposed method achieves more consistent and natural-looking results than the existing baselines.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.21079/x7.png", "caption": "Figure 6: Ablation studies on the (a) correspondence-guided attention manipulation (Corr-Attention) and (b) correspondence-guided CFG (Corr-CFG).", "description": "This figure shows the ablation study results for the proposed method, Edicho.  It compares the performance of Edicho with and without two key components: (a) correspondence-guided attention manipulation (Corr-Attention) and (b) correspondence-guided classifier-free guidance (Corr-CFG).  The images visually demonstrate the impact of removing each component on the quality and consistency of the image editing results. By comparing the edited images with and without these components, we can assess their individual contributions to the overall performance of the Edicho method.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2412.21079/x8.png", "caption": "Figure 7: With outputs from our consistent editing method (upper) and the customization\u00a0[48] techniques,\ncustomized generation (lower) could be achieved by injecting the edited concepts into the generative model.", "description": "This figure demonstrates the application of consistent image editing to enable customized image generation.  The top row shows the results of consistent editing applied to a set of images.  These edited images are then used as input for a customization technique (DreamBooth [48]), which fine-tunes a generative model to learn the new concepts represented in the edited images. The bottom row showcases the customized generations produced by the fine-tuned model, highlighting how consistent editing improves the quality and consistency of the newly generated images.", "section": "4.4 Additional Applications and Results"}, {"figure_path": "https://arxiv.org/html/2412.21079/x9.png", "caption": "Figure 8: We adopt the neural regressor Dust3R\u00a0[55] for 3D reconstruction based on the edits by matching the 2D points in a 3D space.", "description": "This figure demonstrates the application of the Edicho method to 3D reconstruction.  The Edicho method, having performed consistent image editing, utilizes the neural regressor Dust3R to generate 3D models. This is achieved by matching corresponding 2D points across multiple edited images, mapping them into a common 3D space to create a 3D representation of the edited scene.  The figure visually displays this process, showing input images, the consistent edits, and the resulting 3D reconstructions.", "section": "4.4 Additional Applications and Results"}, {"figure_path": "https://arxiv.org/html/2412.21079/x10.png", "caption": "Figure 9: Diverse results of consistent image inpainting (a) and translation (b) by the proposed method.\nEditing results for an image set of three images are demonstrated in (c).", "description": "Figure 9 showcases the versatility of the proposed consistent image editing method.  Panel (a) presents examples of consistent inpainting, where missing parts of images are filled in a way that maintains consistency across multiple images.  Panel (b) demonstrates consistent image translation, where the style or content of images is transformed consistently across multiple images. Finally, panel (c) illustrates how the method produces consistent edits across an entire set of three images.", "section": "Additional Applications and Results"}, {"figure_path": "https://arxiv.org/html/2412.21079/x11.png", "caption": "Figure S1: Additional ablations on the correspondence-guided attention (upper) and CFG (lower).", "description": "This figure shows the results of ablation studies conducted to evaluate the effectiveness of the correspondence-guided attention and correspondence-guided classifier-free guidance (CFG) mechanisms proposed in the paper. The upper part displays the impact of modifying the correspondence-guided attention mechanism by warping attention outputs instead of queries.  The lower part shows the results of removing the correspondence-guided CFG, in which only the unconditional noisy latents are guided instead of both the conditional and unconditional latents.  The results demonstrate that warping attention queries is essential for maintaining high-quality and consistent edits, while using correspondence guidance in both conditional and unconditional branches is crucial for preserving the generative priors and obtaining coherent edits.", "section": "C. Additional Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2412.21079/x12.png", "caption": "Figure S2: Additional correspondence prediction comparisons.\nThe numbers behind \u201cImplicit\u201d respectively indicate the network layer and denoising step for correspondence prediction.", "description": "Figure S2 compares different methods for predicting image correspondences. It shows three examples of image pairs, each processed with an explicit correspondence method and two implicit methods. The implicit methods vary in the network layer and denoising step used for correspondence prediction, highlighting how these choices impact the accuracy of the predictions.", "section": "Appendix D. Additional Correspondence Comparisons"}, {"figure_path": "https://arxiv.org/html/2412.21079/x13.png", "caption": "Figure S3: \nAdditional correspondence prediction results with attention visualization. Regions with the highest attention weights are outlined with dashed circles.", "description": "Figure S3 shows a comparison of explicit and implicit correspondence prediction methods using attention visualization.  It presents three examples, each displaying an input image alongside visualizations of explicit correspondence, and implicit correspondence at different network layers and denoising steps. Dashed circles highlight areas with the highest attention weights in the implicit methods. This visualization demonstrates the difference in accuracy and consistency between the explicit and implicit approaches in predicting correspondences.", "section": "Appendix D. Additional Correspondence Comparisons"}, {"figure_path": "https://arxiv.org/html/2412.21079/x14.png", "caption": "Figure S4: User study results of consistent local editing (left) and global editing (right).", "description": "This figure presents the results of user studies comparing the performance of Edicho against other methods in both local and global image editing tasks.  Participants rated the results based on criteria such as consistency of edits across multiple images, overall quality of the generated images, and how well the edits adhered to the given instructions. The bar charts illustrate the percentage of participants who preferred Edicho for each task, showing a significant preference for Edicho over competing methods.", "section": "E. User Studies"}]