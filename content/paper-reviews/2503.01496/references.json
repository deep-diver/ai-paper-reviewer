{"references": [{"fullname_first_author": "Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-01-01", "reason": "This paper introduced the Transformer architecture, which is the foundation for most modern LLMs."}, {"fullname_first_author": "Katharopoulos", "paper_title": "Transformers are rnns: Fast autoregressive transformers with linear attention", "publication_date": "2020-01-01", "reason": "This paper introduced linear attention mechanisms, which are used to reduce the computational complexity of Transformers."}, {"fullname_first_author": "Hu", "paper_title": "Lora: Low-rank adaptation of large language models", "publication_date": "2021-01-01", "reason": "This paper introduces LoRA, a parameter-efficient fine-tuning technique used in this paper to restore performance of the linearized models."}, {"fullname_first_author": "Touvron", "paper_title": "Llama: Open and efficient foundation language models", "publication_date": "2023-01-01", "reason": "This paper presents the LLaMA model, which is a popular LLM architecture used as a base model in this paper."}, {"fullname_first_author": "Yang", "paper_title": "Gated linear attention transformers with hardware-efficient training", "publication_date": "2023-01-01", "reason": "This paper introduces Gated Linear Attention (GLA), a linear attention variant incorporating gating mechanisms, which is used as the basis of the Liger architecture."}]}