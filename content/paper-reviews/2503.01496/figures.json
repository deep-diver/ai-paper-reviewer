[{"figure_path": "https://arxiv.org/html/2503.01496/x1.png", "caption": "Figure 1: Liger Performance and Efficiency. Our proposed Liger recovers nearly 93% performance of Llama-3.2-1B and outperforms pretrained gated recurrent models at only 0.02% of the pre-training tokens cost.", "description": "This figure showcases the performance and efficiency gains achieved by Liger, a novel linearization technique for large language models (LLMs).  The left-hand side displays the performance across various benchmarks (PIQA, MMLU, ARC-e, ARC-C, HellaSwag, Winograd Schema Challenge) comparing Liger's performance to Llama-3.2-1B, a standard Transformer-based model, and other pretrained gated recurrent models. The key observation is that Liger, despite using only 0.02% of the training tokens of Llama-3.2-1B, nearly matches the performance of the original model and significantly outperforms existing gated recurrent models. The right-hand side visually reinforces this finding by showing the performance comparison in terms of training tokens used, highlighting Liger's exceptional efficiency.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.01496/x2.png", "caption": "Figure 2: Overall Framework of Liger. We linearize the Transformer-based large language model (LLM) architecture into a gated linear recurrent model by 1) Replacing Softmax Attention with a Gated Recurrent Memory module, and 2) Employing LoRA to fine-tune the Liger architecture while frozen most original weight parameters. The Liger architecture enables efficient chunk-wise parallel training also enjoying cheap linear recurrent inference.", "description": "This figure illustrates Liger's framework, which transforms a Transformer-based large language model (LLM) into a gated linear recurrent structure.  It highlights two key steps: (1) replacing the standard softmax attention mechanism with a gated recurrent memory module, which allows for more efficient processing of sequential information and (2) employing Low-Rank Adaptation (LoRA) to fine-tune the resulting Liger architecture, keeping most of the original weights frozen to leverage pre-trained knowledge. The LoRA fine-tuning is lightweight, making the process efficient. This transformation enables efficient chunk-wise parallel training during the training phase and cost-effective linear recurrent inference during the inference phase.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2503.01496/x3.png", "caption": "Figure 3: Hybrid Architecture of Liger. Liger adopts intra-hybrid Liger Attention and inter-hybrid model architecture by stacking a layer of standard attention Transformer blocks every a few (e.g. 7) layers of Liger Transformer blocks.", "description": "Figure 3 illustrates the hybrid architecture of the Liger model.  Liger integrates two types of attention mechanisms:  Liger Attention (a novel hybrid mechanism combining gated recurrent modeling and sliding window softmax attention) and standard Transformer attention. The architecture alternates between layers of Liger Transformer blocks, which use the efficient Liger Attention, and standard Transformer blocks, which use traditional softmax attention. This hybrid approach leverages the strengths of both mechanisms \u2013 the efficiency of Liger Attention for long sequences and the accuracy of standard Transformer attention \u2013 to improve overall model performance.  The frequency of standard Transformer block insertion (e.g., every 7 Liger layers) is a hyperparameter that can be tuned. This alternating design is called an 'inter-hybrid' architecture because it combines different layer types. The use of Liger Attention within each Liger layer constitutes an 'intra-hybrid' architecture.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2503.01496/x4.png", "caption": "Figure 4: Decoding Latency Time and GPU Memory Usage of Each 8B Models. We variate the decoding length from 1K to 32K with fixed batch size of 16 on single A800 80GB GPU to evaluate the models\u2019 efficiency. Liger enjoys linear-time inference with constant GPU memory usage.", "description": "This figure showcases a comparison of decoding latency and GPU memory usage among three 8B parameter models: Llama-3-8B (without FlashAttention-2), Llama-3-8B (with FlashAttention-2), and Liger-8B.  The x-axis represents the decoding sequence length, ranging from 1K to 32K tokens, while the y-axis displays both decoding latency (in seconds) and GPU memory consumption (in GB). A fixed batch size of 16 was used on a single NVIDIA A800 80GB GPU for all experiments. The results highlight that Llama-3-8B, especially with FlashAttention-2, experiences a significant increase in latency and memory usage as the decoding length increases, ultimately resulting in an out-of-memory (OOM) error for the 32K sequence length.  In contrast, the Liger-8B model exhibits constant GPU memory usage and linear decoding time, demonstrating its superior efficiency for long sequences.", "section": "4. Experiments"}]