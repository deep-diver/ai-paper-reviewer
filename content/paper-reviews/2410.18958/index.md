---
title: "Stable Consistency Tuning: Understanding and Improving Consistency Models"
summary: "Stable Consistency Tuning (SCT) significantly boosts consistency model training, achieving state-of-the-art results by reducing variance and improving sampling efficiency."
categories: ["AI Generated"]
tags: [" 24-10-24", " 24-10-25"]
showSummary: true
date: 2024-10-24
draft: false
---

### TL;DR


{{< lead >}}

This research paper focuses on consistency models, a faster alternative to diffusion models for generating images.  The authors found that current training methods for these models suffer from high variance and discretization errors, leading to instability and suboptimal results.  To address this, they propose a new method called Stable Consistency Tuning (SCT). SCT incorporates variance-reduced learning and a smoother training schedule, significantly improving training stability and convergence speed.  They also provide a novel theoretical framework by modeling the denoising process as a Markov Decision Process (MDP), which helps explain the limitations of existing training methods. Experiments demonstrate that SCT achieves state-of-the-art performance on benchmark datasets like CIFAR-10 and ImageNet-64, surpassing previous consistency models and even some diffusion models in terms of both speed and image quality.

{{< /lead >}}


{{< button href="https://arxiv.org/abs/2410.18958" target="_self" >}}
{{< icon "link" >}} &nbsp; read the paper on arXiv
{{< /button >}}
<br><br>
{{< button href="https://huggingface.co/papers/2410.18958" target="_self" >}}
{{< icon "hf-logo" >}} &nbsp; on Hugging Face
{{< /button >}}

#### Why does it matter?
This paper is crucial for researchers in generative modeling due to its novel framework for understanding and improving consistency models.  It introduces Stable Consistency Tuning (SCT), a technique that significantly improves the training stability and speed of these models, leading to state-of-the-art results. The MDP-based analysis provides a new theoretical perspective, while the practical improvements offered by SCT are directly applicable to ongoing research. This work opens doors for further investigation into variance reduction, multistep sampling strategies, and the application of consistency models to more complex domains.
#### Key Takeaways

{{< alert "star" >}}
{{< typeit speed=10 lifeLike=true >}} Stable Consistency Tuning (SCT) improves the speed and stability of consistency model training. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=1000 lifeLike=true >}} SCT achieves state-of-the-art results on CIFAR-10 and ImageNet-64 benchmarks. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=2000 lifeLike=true >}} The paper proposes a novel theoretical framework for understanding consistency models using Markov Decision Processes (MDPs). {{< /typeit >}}
{{< /alert >}}

------
#### Visual Insights



![](figures/figures_2_0.png)

>  The figure illustrates Stable Consistency Tuning (SCT) as a unifying framework for understanding different training strategies of consistency models, including consistency distillation (CD) and consistency training (CT), by modeling the denoising process as a Markov Decision Process (MDP).
> <details>
> <summary>read the caption</summary>
> Figure 1: Stable consistency tuning (SCT) with variance reduced training target. SCT provides a unifying perspective to understand different training strategies of consistency models.
> </details>





![](charts/charts_8_0.png)

>  The chart compares the FID scores of ECT and SCT models over training iterations for both 1-step and 2-step sampling, demonstrating SCT's faster convergence and superior performance.
> <details>
> <summary>read the caption</summary>
> Figure 3: FID vs Training iterations. SCT has faster convergence speed and better performance upper bound than ECT.
> </details>





{{< table-caption >}}
<table id='2' style='font-size:16px'><tr><td>Fu-Yun Wang</td><td>Zhengyang Geng</td><td>Hongsheng Li</td></tr><tr><td>MMLab, CUHK</td><td>Carnegie Mellon University</td><td>MMLab, CUHK</td></tr><tr><td>Hong Kong SAR</td><td>Pittsburgh, USA</td><td>Hong Kong SAR</td></tr><tr><td>fywang@link . cuhk 路 edu . hk</td><td>zhengyanggeng@gmail 路 com</td><td>hsli@ee 路 cuhk 路 edu. hk</td></tr></table>{{< /table-caption >}}

>  Table 2 compares the quality of image samples generated by various methods on the CIFAR-10 dataset, showing the number of forward function evaluations (NFE) and Frechet Inception Distance (FID) scores.
> <details>
> <summary>read the caption</summary>
> Table 2: Comparing the quality of samples on CIFAR-10.
> </details>



### More visual insights

<details>
<summary>More on figures
</summary>


![](figures/figures_5_0.png)

>  The figure illustrates the one-step and multistep (phased) inference techniques of consistency models, highlighting the difference in ODE solving and bootstrapping processes for each.
> <details>
> <summary>read the caption</summary>
> Figure 2: Phasing the ODE path along the time axis for consistency training. We visualize both training and inference techniques in discrete form for easier understanding.
> </details>



![](figures/figures_18_0.png)

>  The figure illustrates Stable Consistency Tuning (SCT) and how it unifies different training strategies of consistency models by using variance-reduced training target.
> <details>
> <summary>read the caption</summary>
> Figure 1: Stable consistency tuning (SCT) with variance reduced training target. SCT provides a unifying perspective to understand different training strategies of consistency models.
> </details>



![](figures/figures_19_0.png)

>  Figure 7 shows 1-step samples generated by the Stable Consistency Tuning (SCT) model trained on the CIFAR-10 dataset, with each row representing a different class.
> <details>
> <summary>read the caption</summary>
> Figure 7: 1-step samples from class-conditional SCT trained on CIFAR-10. Each row corresponds to a different class.
> </details>



![](figures/figures_20_0.png)

>  The figure illustrates stable consistency tuning (SCT), a new training framework unifying different training strategies for consistency models, by comparing consistency distillation (CD), consistency training (CT), and SCT.
> <details>
> <summary>read the caption</summary>
> Figure 1: Stable consistency tuning (SCT) with variance reduced training target. SCT provides a unifying perspective to understand different training strategies of consistency models.
> </details>



![](figures/figures_21_0.png)

>  The figure illustrates Stable Consistency Tuning (SCT) which provides a unifying perspective to understand different training strategies of consistency models by modeling the denoising process of the diffusion model as a Markov Decision Process (MDP).
> <details>
> <summary>read the caption</summary>
> Figure 1: Stable consistency tuning (SCT) with variance reduced training target. SCT provides a unifying perspective to understand different training strategies of consistency models.
> </details>



![](figures/figures_22_0.png)

>  The figure illustrates Stable Consistency Tuning (SCT) and its variance-reduced training target, unifying different training strategies of consistency models.
> <details>
> <summary>read the caption</summary>
> Figure 1: Stable consistency tuning (SCT) with variance reduced training target. SCT provides a unifying perspective to understand different training strategies of consistency models.
> </details>



![](figures/figures_23_0.png)

>  The figure illustrates Stable Consistency Tuning (SCT) which provides a unifying perspective to understand different training strategies of consistency models by modeling the denoising process of the diffusion model as a Markov Decision Process (MDP) and framing consistency model training as the value estimation through Temporal Difference (TD) Learning.
> <details>
> <summary>read the caption</summary>
> Figure 1: Stable consistency tuning (SCT) with variance reduced training target. SCT provides a unifying perspective to understand different training strategies of consistency models.
> </details>



![](figures/figures_24_0.png)

>  Figure 13 shows 1-step samples generated from a class-conditional Stable Consistency Tuning (SCT) model trained on the ImageNet-64 dataset, achieving a Fr茅chet Inception Distance (FID) score of 2.23.
> <details>
> <summary>read the caption</summary>
> Figure 13: 1-step samples from class-conditional SCT trained on ImageNet-64 (FID 2.23). Each row corresponds to a different class.
> </details>



![](figures/figures_25_0.png)

>  Figure 13 presents 1-step samples generated from class-conditional Stable Consistency Tuning (SCT) model trained on ImageNet-64 dataset, achieving a Fr茅chet Inception Distance (FID) score of 2.23.
> <details>
> <summary>read the caption</summary>
> Figure 13: 1-step samples from class-conditional SCT trained on ImageNet-64 (FID 2.23). Each row corresponds to a different class.
> </details>



![](figures/figures_26_0.png)

>  Figure 13 shows 1-step samples generated from a class-conditional Stable Consistency Tuning (SCT) model trained on the ImageNet-64 dataset, achieving a Fr茅chet Inception Distance (FID) score of 2.23.
> <details>
> <summary>read the caption</summary>
> Figure 13: 1-step samples from class-conditional SCT trained on ImageNet-64 (FID 2.23). Each row corresponds to a different class.
> </details>



</details>



<details>
<summary>More on charts
</summary>


![](charts/charts_9_0.png " Figure 4: The effectiveness of variance reduced training target.")

>  The chart compares the 1-step and 2-step FID scores for different training variance reduction methods, showing that using a stable target (all) significantly improves model performance compared to using no stable target or only a batch-based stable target.
> <details>
> <summary>read the caption</summary>
> Figure 4: The effectiveness of variance reduced training target.
> </details>


![](charts/charts_9_1.png " Figure 5: The effectiveness of edge-skipping multi-step sampling.")

>  The chart displays the FID score comparison for different 畏 values in the edge-skipping multi-step sampling method during the training process.
> <details>
> <summary>read the caption</summary>
> Figure 5: The effectiveness of edge-skipping multi-step sampling.
> </details>


![](charts/charts_9_2.png " Figure 6: The effectiveness of classifier-free guidance on consistency models.")

>  The chart displays the impact of classifier-free guidance (CFG) strength on the FID scores for both 1-step and 2-step sampling from consistency models.
> <details>
> <summary>read the caption</summary>
> Figure 6: The effectiveness of classifier-free guidance on consistency models.
> </details>


</details>



### Full paper

{{< gallery >}}
<img src="paper_images/1.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/2.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/3.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/4.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/5.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/6.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/7.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/8.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/9.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/10.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/11.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/12.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/13.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/14.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/15.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/16.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/17.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/18.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/19.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/20.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/21.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/22.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/23.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/24.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/25.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/26.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
{{< /gallery >}}