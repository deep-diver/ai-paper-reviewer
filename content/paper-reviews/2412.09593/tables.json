[{"content": "| Method | Mean \u2193 | Median \u2193 | 3\u00b0 \u2191 | 5\u00b0 \u2191 | 7.5\u00b0 \u2191 | 11.25\u00b0 \u2191 | 22.5\u00b0 \u2191 | 30\u00b0 \u2191 |\n|---|---|---|---|---|---|---|---|---|\n| RGB \u2194 X [57] | 14.847 | 13.704 | 11.676 | 23.073 | 35.196 | 49.829 | 75.777 | 86.348 |\n| DSINE [2] | 9.161 | 7.457 | 23.565 | 41.751 | 57.596 | 72.003 | 90.294 | 95.297 |\n| GeoWizard [16] | 8.455 | 6.926 | 22.245 | 40.993 | 58.457 | 74.916 | 93.315 | 97.162 |\n| Marigold [25] | 8.652 | 7.078 | 25.219 | 42.289 | 58.062 | 72.873 | 92.326 | 96.742 |\n| StableNormal [53] | 8.034 | 6.568 | 21.393 | 43.917 | 63.740 | 78.568 | 93.671 | 96.785 |\n| **Ours** | **6.413** | **4.897** | **38.656** | **56.780** | **70.938** | **82.853** | **95.412** | **98.063** |", "caption": "Table 1: Quantitative comparison on surface normal estimation. We report mean and median angular errors, as well as accuracies within different angular thresholds from 3\u2062\u00b03\u00b03\\degree3 \u00b0 to 30\u2062\u00b030\u00b030\\degree30 \u00b0.", "description": "This table presents a quantitative comparison of different methods for surface normal estimation.  The metrics used are mean and median angular errors, which measure the average and middle values of the errors in angle between the estimated surface normal and the ground truth normal.  In addition, the table shows the accuracy of the estimation for different angular thresholds (3\u00b0, 5\u00b0, 7.5\u00b0, 11.25\u00b0, 22.5\u00b0, and 30\u00b0). A higher accuracy percentage at a given threshold means the estimated normal is closer to the ground truth within that angular tolerance.  This allows for a comprehensive evaluation of the performance across various levels of precision.", "section": "4. Experiments"}, {"content": "| Method | Albedo PSNR \u2191 | Albedo RMSE \u2193 | Roughness PSNR \u2191 | Roughness RMSE \u2193 | Metallic PSNR \u2191 | Metallic RMSE \u2193 | Relighting PSNR \u2191 | Relighting SSIM \u2191 | Relighting LPIPS \u2193 | Latency |\n|---|---|---|---|---|---|---|---|---|---|---|\n| RGB \u2194 X [57] | 16.26 | 0.176 | 19.21 | 0.134 | 16.65 | 0.199 | 20.78 | 0.8927 | 0.0781 | 15s |\n| Yi. et al [54] | 21.10 | 0.106 | 16.88 | 0.180 | 20.30 | 0.144 | 26.47 | 0.9316 | 0.0691 | 5s |\n| IntrinsicAnything [8] | 23.88 | 0.078 | 17.25 | 0.172 | 22.00 | 0.134 | 27.98 | 0.9474 | 0.0490 | 2min |\n| DiLightNet [56] | - | - | - | - | - | - | 22.68 | 0.8751 | 0.0981 | 30s |\n| IC-Light [60] | - | - | - | - | - | - | 20.29 | 0.9027 | 0.0638 | 1min |\n| **Ours** | **26.62** | **0.054** | **23.44** | **0.085** | **26.23** | **0.109** | **30.12** | **0.9601** | **0.0371** | **5s** |", "caption": "Table 2: Quantitative comparison on PBR materials estimation and single-image relighting.", "description": "This table presents a quantitative comparison of different methods for estimating physically-based rendering (PBR) material properties (albedo, roughness, metallic) and performing single-image relighting.  It compares the performance of several state-of-the-art techniques, including the proposed method, using metrics like Peak Signal-to-Noise Ratio (PSNR) and Root Mean Squared Error (RMSE) for material estimation, and PSNR, Structural Similarity Index (SSIM), Learned Perceptual Image Patch Similarity (LPIPS), and average processing time for relighting.  Lower RMSE values and higher PSNR, SSIM values indicate better performance.", "section": "4. Experiments"}, {"content": "| Method | PSNR \u2191 | SSIM \u2191 | LPIPS \u2193 |\n|---|---|---|---| \n| Concatenation | 19.32 | 0.8597 | 0.0909 |\n| Reference Attention | 19.87 | 0.8691 | 0.0829 |\n| Concatenation + Reference Attention | **20.01** | **0.8718** | **0.0815** |", "caption": "Table 3: Effects of condition strategies in multi-light diffusion.", "description": "This table presents a comparison of different conditioning strategies used in the multi-light diffusion model. It shows the impact of using concatenation, reference attention, and a combination of both on the performance of the model in terms of PSNR, SSIM, and LPIPS scores. This helps in understanding which strategy is most effective for generating high-quality multi-light images for subsequent G-buffer prediction.", "section": "3.1 Multi-light Diffusion"}, {"content": "| Number of Light Images | Albedo PSNR \u2191 | Albedo RMSE \u2193 | Roughness PSNR \u2191 | Roughness RMSE \u2193 | Metallic PSNR \u2191 | Metallic RMSE \u2193 | Normal MAE \u2193 | Normal 5\u00b0 \u2191 | Normal 7.5\u00b0 \u2191 | Normal 11.25\u00b0 \u2191 | Normal 22.5\u00b0 \u2191 |\n|---|---|---|---|---|---|---|---|---|---|---|---| \n| 0 | 22.22 | 0.082 | 20.99 | 0.104 | 18.56 | 0.136 | 7.563 | 45.846 | 61.425 | 76.948 | 95.488 |\n| 3 | 23.72 | 0.068 | 23.89 | 0.075 | 20.66 | 0.106 | 4.763 | 68.344 | 80.896 | 89.959 | 97.928 |\n| 6 | 23.82 | 0.068 | 24.19 | 0.072 | 20.64 | 0.106 | 4.275 | 72.777 | 83.997 | 91.730 | 98.312 |\n| 9 | 23.90 | 0.067 | 24.36 | 0.069 | 20.74 | 0.105 | 4.059 | 74.720 | 85.092 | 92.330 | 98.431 |", "caption": "Table 4: Effect of the number of multi-light images on the performance of the large G-buffer model.", "description": "This table shows how the performance of the large G-buffer model (which predicts surface normals and material properties) changes depending on the number of multi-light images used as input.  It presents quantitative results, such as Peak Signal-to-Noise Ratio (PSNR) and Root Mean Squared Error (RMSE) for albedo, roughness, metallic values, and Mean Angular Error (MAE) for surface normals, along with accuracy within specific angular thresholds (5\u00b0, 7.5\u00b0, 11.25\u00b0, 22.5\u00b0).  The different numbers of input images allow for assessing the impact of additional lighting information on prediction accuracy.", "section": "4. Experiments"}, {"content": "|                       | Albedo PSNR \u2191 | Albedo RMSE \u2193 | Roughness PSNR \u2191 | Roughness RMSE \u2193 | Metallic PSNR \u2191 | Metallic RMSE \u2193 | Normal MAE \u2193 | Normal 5\u00b0 \u2191 | Normal 7.5\u00b0 \u2191 | Normal 11.25\u00b0 \u2191 | Normal 22.5\u00b0 \u2191 |\n|-----------------------|-----------------|-----------------|------------------|-------------------|-----------------|-----------------|-----------------|----------------|-----------------|-------------------|-------------------|\n| w/o augmentation      | 21.69            | 0.087            | 20.46             | 0.110            | 16.61            | 0.179            | 7.080            | 52.235          | 67.032          | 80.115            | 94.802             |\n| w/ augmentation       | **22.36**        | **0.081**        | **21.39**         | **0.099**        | **18.81**        | **0.135**        | **6.342**        | **55.893**      | **70.326**      | **82.848**        | **96.230**         |", "caption": "Table 5: Effect of augmentation strategy on the large G-buffer model.", "description": "This table presents a quantitative analysis of the impact of using data augmentation strategies during the training of the large G-buffer model.  It compares the performance of the model trained with augmentations against a model trained without augmentations, evaluating the results across various metrics for Albedo, Roughness, Metallic, and Normal estimations.  These metrics likely include things like Peak Signal-to-Noise Ratio (PSNR) and Root Mean Square Error (RMSE) to assess the quality and accuracy of the model's predictions.  The comparison helps determine the effectiveness of data augmentation in improving the overall robustness and accuracy of the model's predictions.", "section": "3.2 Large G-Buffer Model"}]