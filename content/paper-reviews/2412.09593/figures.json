[{"figure_path": "https://arxiv.org/html/2412.09593/x1.png", "caption": "Figure 1: Neural LightRig takes an image as input and generates multi-light images to assist the estimation of high-quality normal and PBR materials, which can be used to render realistic relit images under various environment lighting.", "description": "This figure demonstrates the Neural LightRig pipeline.  An input image is fed into the system. The system then generates multiple images of the same object, each illuminated from a different direction (multi-light images). This process helps to overcome the inherent ambiguity in estimating surface normal and physically-based rendering (PBR) materials from a single image. These estimated materials are high-quality and can be used to create photorealistic renderings of the object under various lighting conditions, showcasing the system's ability to improve both the accuracy and realism of material estimation compared to only using a single image.", "section": "Abstract"}, {"figure_path": "https://arxiv.org/html/2412.09593/x2.png", "caption": "Figure 2: Framework Overview. Multi-light diffusion generates multi-light images from an input image. These images with corresponding lighting orientations are then used to predict surface normals and PBR materials with a regression U-Net.", "description": "This figure illustrates the Neural LightRig framework.  It begins with a single input image. This image is fed into a multi-light diffusion model, which generates multiple consistent images of the object, each illuminated from a different light source direction.  These directions are explicitly tracked. This set of multi-light images, along with their corresponding lighting orientations, is then input to a large G-buffer model (a U-Net). This model simultaneously predicts the surface normal and physically-based rendering (PBR) material properties (albedo, roughness, and metallic) of the object from the varied lighting conditions. This allows for more accurate and robust estimation compared to methods using only a single image.", "section": "3. Approach"}, {"figure_path": "https://arxiv.org/html/2412.09593/x3.png", "caption": "Figure 3: Hybrid condition in multi-light diffusion. Input images are incorporated via concatenation with noise latents and enhanced through reference attention, where queries in the denoise stream attend to keys and values from both streams.", "description": "This figure illustrates the hybrid conditioning strategy used in the multi-light diffusion model.  The input image isn't simply added to the noise; it's cleverly integrated using two methods: concatenation and reference attention. Concatenation merges the input image's features directly with the noise latents, providing a straightforward way to incorporate image context.  Simultaneously, reference attention enhances this integration.  This mechanism allows the network to focus on relevant aspects of the input image while generating the multi-light versions.  Specifically, the 'queries' within the noise-reduction process of the diffusion model attend to both the 'keys' and 'values' derived from both the noise and the input image, creating a refined, context-aware representation before generating the final image.", "section": "3.1. Multi-Light Diffusion"}, {"figure_path": "https://arxiv.org/html/2412.09593/extracted/6065430/Figures/normal/normal_compare_main-HQ.jpeg", "caption": "Figure 4: Visualization of multi-light setup in LightProp. Camera and point lights are positioned on a sphere around the object. \u03b8,\u03c6\ud835\udf03\ud835\udf11\\theta,\\varphiitalic_\u03b8 , italic_\u03c6 are spherical coordinates to determine each light\u2019s orientation relative to the object.", "description": "Figure 4 illustrates the LightProp dataset's multi-light setup.  A camera and multiple point light sources are arranged on a sphere surrounding a 3D object. The position of each light source is defined by its spherical coordinates (\u03b8, \u03c6), representing the polar and azimuthal angles respectively.  These coordinates determine the direction of the light relative to the object, enabling the creation of diverse lighting conditions for each image in the dataset.", "section": "3.3 LightProp Dataset"}, {"figure_path": "https://arxiv.org/html/2412.09593/extracted/6065430/Figures/relit/relit_compare_main-HQ.jpeg", "caption": "Figure 5: Qualitative comparison on surface normal estimation. Ground truth normals (G.T.) are provided for input images rendered from available 3D objects (the last two rows) and are omitted for in-the-wild images (the first two rows).", "description": "This figure showcases a qualitative comparison of surface normal estimation results from different methods, including DSINE, GeoWizard, Marigold, StableNormal, and the proposed method.  The first two rows display results for images from real-world scenes ('in-the-wild'), while the last two rows show results using images synthetically rendered from 3D models.  The ground truth (G.T.) surface normals are provided for the 3D-rendered images, allowing for a direct comparison. This visualization helps to illustrate the relative strengths and weaknesses of each method in accurately estimating surface normals in both controlled and uncontrolled image settings.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.09593/extracted/6065430/Figures/pbr/pbr_compare_main-HQ.jpeg", "caption": "Figure 6: Qualitative comparison on single-image relighting.", "description": "This figure displays a qualitative comparison of single-image relighting results produced by various methods, including the proposed Neural LightRig model.  Each row shows results for different objects; the input image is presented alongside environment maps, demonstrating lighting conditions. Then, results from multiple existing methods and the proposed approach are shown, enabling visual comparison of the methods' abilities to accurately predict the relighting given changes to the lighting condition. The ground truth relit image is also included for objective comparison.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.09593/extracted/6065430/Figures/ablation/ablate_diffusion-HQ.jpeg", "caption": "Figure 7: Qualitative comparison on PBR material estimation. Ground truth materials (G.T.) are provided for input images rendered from available 3D objects (the right column) and are omitted for in-the-wild images (the left column).", "description": "This figure shows a qualitative comparison of PBR material estimation results from different methods. The left column showcases results from real-world images, while the right column uses images rendered from 3D models, allowing for ground truth comparison (G.T.). Each row displays the input image, followed by the results from various methods: RGB-X, Yi et al., IntrinsicAnything, and the proposed method. The results demonstrate the methods' accuracy in estimating albedo, roughness, and metallic properties of the objects.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.09593/extracted/6065430/Figures/ablation/ablate_ref-HQ.jpeg", "caption": "Figure 8: Visualization of different conditioning strategies in multi-light diffusion. Concat stands for concatenation. RA stands for reference attention.", "description": "This figure compares three different methods for incorporating the input image into the multi-light diffusion model: concatenation (Concat), reference attention (RA), and a combination of both.  Each method generates nine multi-light images from a single input image, showing variations in lighting.  The goal is to assess how effectively each approach leverages the input image to produce consistent and realistic multi-light images, ultimately improving the accuracy of downstream G-buffer estimation. Visual comparisons are shown for two examples: a vase and chess pieces, highlighting differences in color tones, texture, and overall fidelity.", "section": "3.1. Multi-light Diffusion"}, {"figure_path": "https://arxiv.org/html/2412.09593/extracted/6065430/Figures/ablation/ablate_aug-HQ.jpeg", "caption": "Figure 9: Visualization of using different numbers of multi-light images. We evaluate the G-Buffer prediction model with different numbers of novel-light images (00, 3333, 6666, and 9999) as conditions.", "description": "This figure visualizes the impact of using varying numbers of multi-light images as input to the G-Buffer prediction model.  The experiment compares four scenarios: using no additional multi-light images (0), three additional images (3), six additional images (6), and nine additional images (9).  Each scenario's output of albedo, roughness, metallic properties, and surface normals is shown, allowing for a direct comparison of the model's performance and accuracy as the number of multi-light images increases.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.09593/extracted/6065430/Figures/failcase/failcase-crop-L.jpeg", "caption": "Figure 10: Visualization of the augmentation strategy.", "description": "This figure shows the effects of the data augmentation strategy used in training the Neural LightRig model. It visually compares the results of using the augmentation strategy to the results without it. The results with augmentation show improved consistency and robustness in the generated images, indicating its efficacy in enhancing the model's performance.", "section": "3.2 Large G-Buffer Model"}, {"figure_path": "https://arxiv.org/html/2412.09593/extracted/6065430/Figures/results/results_supp_1-L.jpeg", "caption": "Figure 11: Failure case.", "description": "This figure showcases a failure case of the Neural LightRig model, highlighting its limitations. The model struggles to accurately estimate the albedo (surface color) when the input image contains extreme highlights or shadows, leading to inaccurate predictions. This emphasizes the challenges in robustly handling varied lighting conditions during inverse rendering tasks.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.09593/extracted/6065430/Figures/results/results_supp_2-L.jpeg", "caption": "Figure 12: More results of our method.", "description": "This figure displays additional examples of the Neural LightRig method's performance.  For a variety of objects, it shows the input image, the generated multi-light images (multiple images of the object under varying lighting conditions), and the estimated surface normal, albedo, roughness, and metallic maps.  These results demonstrate the model's ability to accurately reconstruct the object's geometry and material properties from a single image, even under challenging lighting conditions.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.09593/extracted/6065430/Figures/various_relighting/various_relighting_supp_1-L.jpeg", "caption": "Figure 13: More results of our method.", "description": "This figure displays additional results generated by the Neural LightRig model.  It showcases the model's ability to accurately estimate surface normals, albedo, roughness, and metallic properties from a single input image. For various objects, the figure presents the input image, the generated multi-light images, and the estimated surface properties.  The results demonstrate the effectiveness of the proposed method in handling diverse objects and lighting conditions.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2412.09593/extracted/6065430/Figures/various_relighting/various_relighting_supp_2-L.jpeg", "caption": "Figure 14: More single-image relighting results of our method.", "description": "This figure displays additional examples of single-image relighting results achieved using the Neural LightRig method.  It showcases the model's ability to generate realistic relit images of various objects under different lighting conditions. Each row presents an input image followed by the relit images generated by the model, highlighting the diversity of lighting scenarios and the method's capacity to maintain object details and visual fidelity across different lighting conditions. This figure supports the paper's claim on the effectiveness of the method for single-image relighting.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.09593/extracted/6065430/Figures/normal/normal_compare_supp_eval-L.jpeg", "caption": "Figure 15: More single-image relighting results of our method.", "description": "This figure shows additional examples of single-image relighting results produced by the Neural LightRig method.  It demonstrates the model's ability to realistically relight various objects under diverse lighting conditions, showcasing the quality and consistency of the generated images.", "section": "4. Qualitative Evaluation"}, {"figure_path": "https://arxiv.org/html/2412.09593/extracted/6065430/Figures/pbr/pbr_compare_supp_1-L.jpeg", "caption": "Figure 16: More comparisons on surface normal estimation.", "description": "This figure provides an extended qualitative comparison of surface normal estimation results. It showcases the performance of various methods (DSINE, GeoWizard, Marigold, StableNormal, and the proposed method) on diverse objects.  Each row presents a different object, with the input image on the left followed by surface normal maps generated by each method and the ground truth normal map (G.T.) on the far right.  Visual differences between the generated normal maps highlight the strengths and weaknesses of the different approaches in accurately capturing surface details and geometric features.", "section": "4.2. Qualitative Evaluation"}, {"figure_path": "https://arxiv.org/html/2412.09593/extracted/6065430/Figures/pbr/pbr_compare_supp_2-L.jpeg", "caption": "Figure 17: More comparisons on PBR material estimation.", "description": "This figure provides additional qualitative comparisons of PBR material estimation results.  It shows multiple example objects, each with the input image, outputs from several competing methods (Yi et al., IntrinsicAnything), and the output from the proposed Neural LightRig method, alongside the ground truth.  The comparison allows for visual assessment of the accuracy of albedo, roughness, and metallic property estimation for different methods across various object types.", "section": "4.2. Qualitative Evaluation"}, {"figure_path": "https://arxiv.org/html/2412.09593/extracted/6065430/Figures/relit/relit_compare_supp-L.jpeg", "caption": "Figure 18: More comparisons on PBR material estimation.", "description": "This figure provides additional qualitative comparisons of PBR material estimation results.  It shows input images alongside the albedo, roughness, and metallic maps generated by different methods (Yi et al., IntrinsicAnything, and the proposed method, Neural LightRig). The ground truth (GT) is also included for comparison.  The goal is to visually demonstrate the accuracy and effectiveness of the proposed method in recovering fine material details, particularly in challenging conditions, compared to other state-of-the-art techniques.", "section": "4.2. Qualitative Evaluation"}]