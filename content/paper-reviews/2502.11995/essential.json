{"importance": "This study underscores the need for nuanced personalization strategies in LLMs, urging researchers to develop methods that respect cultural diversity and avoid reinforcing stereotypes. The findings pave the way for AI systems that are both culturally aware and ethically sound, enhancing user experience while minimizing bias.", "summary": "LLMs personalize based on user names, but this study reveals that cultural presumptions in LLM responses risk reinforcing stereotypes.", "takeaways": ["LLMs exhibit strong cultural identity assumptions based on names, leading to biased responses.", "East Asian and Russian names are particularly prone to cultural stereotyping in LLM outputs.", "Clothing and tradition-related queries in LLMs amplify cultural biases significantly."], "tldr": "Names carry deep cultural and personal identity, but when interacting with LLMs, names as a core indicator can lead to over-simplification of complex identities. Previous works have examined gender and race presumptions based on names, but there has been no work on investigating cultural presumptions in LLMs. Examining name-biased cultural presumptions reveals how models represent, propagate and flatten cultural stereotypes, but also provides insights for developing more equitable, culturally sensitive AI systems.\n\nTo address the issue, this study measures cultural presumptions in LLM responses to common suggestion-seeking queries, noting cultural identity associated with names. With 900 names across 30 cultures and 4 LLMs, the analysis exposes strong cultural identity assumptions. The study reveals significant asymmetries in how LLMs associate names with cultural elements and uncovers substantial disparity between names, informing nuanced personalization systems.", "affiliation": "University of Copenhagen", "categories": {"main_category": "AI Theory", "sub_category": "Fairness"}, "podcast_path": "2502.11995/podcast.wav"}