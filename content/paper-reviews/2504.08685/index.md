---
title: "Seaweed-7B: Cost-Effective Training of Video Generation Foundation Model"
summary: "Seaweed-7B: Cost-effective training of video generation model, achieving competitive results with only 665k GPU hours."
categories: ["AI Generated", "ü§ó Daily Papers"]
tags: ["Computer Vision", "Video Understanding", "üè¢ ByteDance",]
showSummary: true
date: 2025-04-11
draft: false
---

<br>

{{< keywordList >}}
{{< keyword icon="fingerprint" >}} 2504.08685 {{< /keyword >}}
{{< keyword icon="writer" >}} Team Seawead et el. {{< /keyword >}}
 
{{< keyword >}} ü§ó 2025-04-14 {{< /keyword >}}
 
{{< /keywordList >}}

{{< button href="https://arxiv.org/abs/2504.08685" target="_self" >}}
‚Üó arXiv
{{< /button >}}
{{< button href="https://huggingface.co/papers/2504.08685" target="_self" >}}
‚Üó Hugging Face
{{< /button >}}



<audio controls>
    <source src="https://ai-paper-reviewer.com/2504.08685/podcast.wav" type="audio/wav">
    Your browser does not support the audio element.
</audio>


### TL;DR


{{< lead >}}

Foundation models are essential in modern machine learning, yet training them requires massive computational resources. While language models have seen success with smaller models, video generation models still lag in efficient scaling. This paper addresses the challenge of training video generation models under resource constraints, where design choices are critical. Current models demand a massive GPU cost, impeding innovation. 



This research introduces a cost-efficient strategy for training a video generation model, Seaweed-7B, which uses a mid-sized architecture (7B parameters) and limited GPU hours. **This model matches or surpasses the performance of much larger models**, demonstrating strong generalization. Key innovations include the VAE designs for reconstruction quality and lessons for DiT training and cost-effective strategies. The model is highly competitive and applicable to downstream tasks.

{{< /lead >}}


#### Key Takeaways

{{< alert "star" >}}
{{< typeit speed=10 lifeLike=true >}} Seaweed-7B achieves performance comparable to larger models with substantially greater GPU resources. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=1000 lifeLike=true >}} Design choices are crucial for enhancing medium-sized diffusion models in resource-constrained settings. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=2000 lifeLike=true >}} Seaweed-7B can be effectively adapted across various downstream applications through lightweight fine-tuning or continued training. {{< /typeit >}}
{{< /alert >}}

#### Why does it matter?
The study is important for researchers because it demonstrates **how to effectively train video generation models with limited computational resources**. The introduced methods can lead to more efficient and accessible video creation, broadening the scope of research and applications in the field.

------
#### Visual Insights



![](https://arxiv.org/html/2504.08685/x1.png)

> üîº This figure showcases example short and detailed video captions generated by the authors' video captioning model.  The short captions offer concise, action-focused summaries of the video content, while the detailed captions provide much richer descriptions.  These detailed descriptions include visual details such as the specific objects and attributes present, a more thorough explanation of the scene, and environmental contextual information.
> <details>
> <summary>read the caption</summary>
> Figure 1: Short and detailed captions are generated by our video captioning model. The short captions provide action-centric summaries of the videos, while the detailed captions offer rich descriptions of the scenes, including attributes, objects, and environments.
> </details>





{{< table-caption >}}
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T1.11">
<tr class="ltx_tr" id="S3.T1.11.12">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S3.T1.11.12.1"><span class="ltx_text ltx_font_bold" id="S3.T1.11.12.1.1" style="font-size:90%;">Source</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S3.T1.11.12.2"><span class="ltx_text ltx_font_bold" id="S3.T1.11.12.2.1" style="font-size:90%;">Iteration</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.11.12.3"><span class="ltx_text ltx_font_bold" id="S3.T1.11.12.3.1" style="font-size:90%;">Resolution</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.2" rowspan="3"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.2.1" style="font-size:90%;">Image</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.3" rowspan="3"><span class="ltx_text" id="S3.T1.1.1.3.1" style="font-size:90%;">500K</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.1">
<span class="ltx_text" id="S3.T1.1.1.1.1" style="font-size:90%;">720</span><math alttext="\times" class="ltx_Math" display="inline" id="S3.T1.1.1.1.m1.1"><semantics id="S3.T1.1.1.1.m1.1a"><mo id="S3.T1.1.1.1.m1.1.1" mathsize="90%" xref="S3.T1.1.1.1.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S3.T1.1.1.1.m1.1b"><times id="S3.T1.1.1.1.m1.1.1.cmml" xref="S3.T1.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.1.1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.T1.1.1.1.m1.1d">√ó</annotation></semantics></math><span class="ltx_text" id="S3.T1.1.1.1.2" style="font-size:90%;">720</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.2">
<td class="ltx_td ltx_align_center" id="S3.T1.2.2.1">
<span class="ltx_text" id="S3.T1.2.2.1.1" style="font-size:90%;">480</span><math alttext="\times" class="ltx_Math" display="inline" id="S3.T1.2.2.1.m1.1"><semantics id="S3.T1.2.2.1.m1.1a"><mo id="S3.T1.2.2.1.m1.1.1" mathsize="90%" xref="S3.T1.2.2.1.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S3.T1.2.2.1.m1.1b"><times id="S3.T1.2.2.1.m1.1.1.cmml" xref="S3.T1.2.2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.2.2.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.T1.2.2.1.m1.1d">√ó</annotation></semantics></math><span class="ltx_text" id="S3.T1.2.2.1.2" style="font-size:90%;">480</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.3.3">
<td class="ltx_td ltx_align_center" id="S3.T1.3.3.1">
<span class="ltx_text" id="S3.T1.3.3.1.1" style="font-size:90%;">256</span><math alttext="\times" class="ltx_Math" display="inline" id="S3.T1.3.3.1.m1.1"><semantics id="S3.T1.3.3.1.m1.1a"><mo id="S3.T1.3.3.1.m1.1.1" mathsize="90%" xref="S3.T1.3.3.1.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S3.T1.3.3.1.m1.1b"><times id="S3.T1.3.3.1.m1.1.1.cmml" xref="S3.T1.3.3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.3.3.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.T1.3.3.1.m1.1d">√ó</annotation></semantics></math><span class="ltx_text" id="S3.T1.3.3.1.2" style="font-size:90%;">256</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.5.5">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S3.T1.5.5.3" rowspan="4"><span class="ltx_text ltx_font_bold" id="S3.T1.5.5.3.1" style="font-size:90%;">Video</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S3.T1.5.5.4" rowspan="4"><span class="ltx_text" id="S3.T1.5.5.4.1" style="font-size:90%;">800K</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.5.5.2">
<span class="ltx_text" id="S3.T1.5.5.2.1" style="font-size:90%;">17</span><math alttext="\times" class="ltx_Math" display="inline" id="S3.T1.4.4.1.m1.1"><semantics id="S3.T1.4.4.1.m1.1a"><mo id="S3.T1.4.4.1.m1.1.1" mathsize="90%" xref="S3.T1.4.4.1.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S3.T1.4.4.1.m1.1b"><times id="S3.T1.4.4.1.m1.1.1.cmml" xref="S3.T1.4.4.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.4.4.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.T1.4.4.1.m1.1d">√ó</annotation></semantics></math><span class="ltx_text" id="S3.T1.5.5.2.2" style="font-size:90%;">256</span><math alttext="\times" class="ltx_Math" display="inline" id="S3.T1.5.5.2.m2.1"><semantics id="S3.T1.5.5.2.m2.1a"><mo id="S3.T1.5.5.2.m2.1.1" mathsize="90%" xref="S3.T1.5.5.2.m2.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S3.T1.5.5.2.m2.1b"><times id="S3.T1.5.5.2.m2.1.1.cmml" xref="S3.T1.5.5.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.5.5.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.T1.5.5.2.m2.1d">√ó</annotation></semantics></math><span class="ltx_text" id="S3.T1.5.5.2.3" style="font-size:90%;">256</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.7.7">
<td class="ltx_td ltx_align_center" id="S3.T1.7.7.2">
<span class="ltx_text" id="S3.T1.7.7.2.1" style="font-size:90%;">9</span><math alttext="\times" class="ltx_Math" display="inline" id="S3.T1.6.6.1.m1.1"><semantics id="S3.T1.6.6.1.m1.1a"><mo id="S3.T1.6.6.1.m1.1.1" mathsize="90%" xref="S3.T1.6.6.1.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S3.T1.6.6.1.m1.1b"><times id="S3.T1.6.6.1.m1.1.1.cmml" xref="S3.T1.6.6.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.6.6.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.T1.6.6.1.m1.1d">√ó</annotation></semantics></math><span class="ltx_text" id="S3.T1.7.7.2.2" style="font-size:90%;">480</span><math alttext="\times" class="ltx_Math" display="inline" id="S3.T1.7.7.2.m2.1"><semantics id="S3.T1.7.7.2.m2.1a"><mo id="S3.T1.7.7.2.m2.1.1" mathsize="90%" xref="S3.T1.7.7.2.m2.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S3.T1.7.7.2.m2.1b"><times id="S3.T1.7.7.2.m2.1.1.cmml" xref="S3.T1.7.7.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.7.7.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.T1.7.7.2.m2.1d">√ó</annotation></semantics></math><span class="ltx_text" id="S3.T1.7.7.2.3" style="font-size:90%;">480</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.9.9">
<td class="ltx_td ltx_align_center" id="S3.T1.9.9.2">
<span class="ltx_text" id="S3.T1.9.9.2.1" style="font-size:90%;">33</span><math alttext="\times" class="ltx_Math" display="inline" id="S3.T1.8.8.1.m1.1"><semantics id="S3.T1.8.8.1.m1.1a"><mo id="S3.T1.8.8.1.m1.1.1" mathsize="90%" xref="S3.T1.8.8.1.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S3.T1.8.8.1.m1.1b"><times id="S3.T1.8.8.1.m1.1.1.cmml" xref="S3.T1.8.8.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.8.8.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.T1.8.8.1.m1.1d">√ó</annotation></semantics></math><span class="ltx_text" id="S3.T1.9.9.2.2" style="font-size:90%;">256</span><math alttext="\times" class="ltx_Math" display="inline" id="S3.T1.9.9.2.m2.1"><semantics id="S3.T1.9.9.2.m2.1a"><mo id="S3.T1.9.9.2.m2.1.1" mathsize="90%" xref="S3.T1.9.9.2.m2.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S3.T1.9.9.2.m2.1b"><times id="S3.T1.9.9.2.m2.1.1.cmml" xref="S3.T1.9.9.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.9.9.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.T1.9.9.2.m2.1d">√ó</annotation></semantics></math><span class="ltx_text" id="S3.T1.9.9.2.3" style="font-size:90%;">256</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.11.11">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.11.11.2">
<span class="ltx_text" id="S3.T1.11.11.2.1" style="font-size:90%;">113</span><math alttext="\times" class="ltx_Math" display="inline" id="S3.T1.10.10.1.m1.1"><semantics id="S3.T1.10.10.1.m1.1a"><mo id="S3.T1.10.10.1.m1.1.1" mathsize="90%" xref="S3.T1.10.10.1.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S3.T1.10.10.1.m1.1b"><times id="S3.T1.10.10.1.m1.1.1.cmml" xref="S3.T1.10.10.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.10.10.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.T1.10.10.1.m1.1d">√ó</annotation></semantics></math><span class="ltx_text" id="S3.T1.11.11.2.2" style="font-size:90%;">144</span><math alttext="\times" class="ltx_Math" display="inline" id="S3.T1.11.11.2.m2.1"><semantics id="S3.T1.11.11.2.m2.1a"><mo id="S3.T1.11.11.2.m2.1.1" mathsize="90%" xref="S3.T1.11.11.2.m2.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S3.T1.11.11.2.m2.1b"><times id="S3.T1.11.11.2.m2.1.1.cmml" xref="S3.T1.11.11.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.11.11.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.T1.11.11.2.m2.1d">√ó</annotation></semantics></math><span class="ltx_text" id="S3.T1.11.11.2.3" style="font-size:90%;">144</span>
</td>
</tr>
</table>{{< /table-caption >}}

> üîº This table details the training stages for the Variational Autoencoder (VAE) model used in the paper.  The VAE is trained using a mixed-resolution approach, meaning it is trained on images and videos with a variety of resolutions and durations. This helps the model to generalize better to different resolutions during inference. The table lists four stages, each specifying the image and video resolutions used along with the percentage of total training steps allocated to each stage. The resolutions indicated are target areas, not exact dimensions.  The images and videos are resized while maintaining aspect ratio.
> <details>
> <summary>read the caption</summary>
> Table 1: VAE training stages for images and videos. Both stages use mixed-resolution data.
> </details>





### In-depth insights


#### Cost-Effective 7B
**Cost-Effective 7B Parameter Models** mark a significant shift in AI development, emphasizing efficiency without sacrificing performance. These models, exemplified by Seaweed-7B, challenge the conventional wisdom that larger models inherently deliver superior results. The focus is on **optimizing architectural designs and training strategies** to achieve competitive or even superior performance compared to much larger models, while using fewer computational resources. This approach democratizes AI research and development, enabling smaller teams and organizations to contribute meaningfully. Key to their success is a **thoughtful balance between model size, data quality, and training techniques**, ensuring that the model generalizes well and can be adapted for various downstream tasks through fine-tuning or continued training. The emphasis on cost-effectiveness also makes AI more accessible for real-world applications where resources may be limited.

#### Scalable Curation
**Scalable curation** is crucial for efficiently managing and processing vast amounts of data in modern AI, especially in resource-constrained settings. It involves developing robust infrastructure and data processors to **scan and filter high-quality data effectively**. This includes temporal splitting, spatial cropping, quality filtering, multi-aspect data balancing, video deduplication, and video captioning. The goal is to create a training dataset that is both diverse and of high quality, leading to better model performance and generalization. Techniques like clustering visual and semantic features aid in identifying and removing duplicates, while downsampling head categories ensures a smooth distribution. The success of scalable curation hinges on balancing computational efficiency with data quality, enabling the creation of robust video generation models even with moderate resources. Overall scalable curation is an essential step towards producing high-quality video, while using minimal resources.

#### VAE is Critical
Variational Autoencoders (VAEs) emerge as a **critical component** in video generation. Acting as the bridge between pixel and latent space, VAEs dictate the realism and fidelity achievable. **High compression with quality** is essential; the compression ratio influences reconstruction, impacting downstream generation. **Downsampling ratios** affect convergence speed, smaller ratios accelerating the process. A well-trained VAE sets the upper limit for visual quality; fine textures should reconstruct accurately. VAE compression outperforms naive DiT patchification, offering better performance. **Resolution diversity** is another key to reconstruction performance. VAE optimization with visual and control parameters and architectural designs will make them highly usable for training and inference.

#### MM-ROPE Benefit
Multimodal Rotary Position Embedding (**MM-ROPE**) emerges as a pivotal technique for enhancing positional awareness within video generation models. By **incorporating temporal, width, and height components**, it transcends basic positional encoding, crucial for understanding video tokens' relationships. It considers both **absolute and relative position dependencies**, proving vital for attention mechanisms. Inspired by prior art, MM-ROPE facilitates positional information fusion between text and video by adding compatible 1D positional encoding for text tokens. This strategic design leads to a substantial **reduction in training loss** within dual-stream MMDiT structures, marking a significant step forward in achieving superior video generation outcomes.

#### DPO Improves RLHF
**Direct Preference Optimization (DPO)** offers a more stable and efficient alternative to traditional Reinforcement Learning from Human Feedback (RLHF). DPO streamlines the training process by directly optimizing the policy based on human preferences, bypassing the complexities of reward modeling inherent in RLHF. This approach often leads to **improved performance** in terms of alignment with human preferences and reduced training instability.  By sidestepping the reward modeling step, DPO **mitigates issues like reward hacking and mode collapse**, which are common challenges in RLHF. The simplified training objective allows for more effective exploration of the policy space, leading to **enhanced generalization** and better overall results. Consequently, DPO represents a significant advancement in training language models, providing a **more robust and reliable method** for aligning AI systems with human values, particularly in areas requiring nuanced or subjective feedback.


### More visual insights

<details>
<summary>More on figures
</summary>


![](https://arxiv.org/html/2504.08685/extracted/6349619/figures/video_data_pipeline.png)

> üîº This figure illustrates the high-throughput video data processing pipeline used in the Seaweed-7B model training.  The pipeline consists of several key stages: 1) Video Input: The process starts with raw video inputs from various sources. 2) Split: The raw videos are split into single-shot clips using a method that identifies shot boundaries. 3) Crop: Unwanted regions (e.g., black borders, watermarks, logos) are removed through spatial cropping based on frame-level object detection. 4) Filter: Multiple filters are applied to ensure high quality, such as visual quality assessment (aesthetics and clarity), spatial-temporal motion analysis (removing static clips and unwanted movements), safety screening (remove harmful content), and artifact detection. 5) Dedup: Duplicate videos and those belonging to the head categories are identified and removed to reduce redundancy and balance the data. 6) Pack: The preprocessed video clips are packaged into a dataset for training.  7) Dataset: The final dataset is saved in a database, along with associated metadata.  The pipeline is designed to manage video encoding and decoding, perform temporal segmentation and spatial cropping, and apply various quality filters, to mine high-quality clips efficiently. This pipeline allowed for processing over 500,000 hours of video data per day.
> <details>
> <summary>read the caption</summary>
> Figure 2: Video Data Processing Pipeline Overview.
> </details>



![](https://arxiv.org/html/2504.08685/x2.png)

> üîº This figure illustrates the architecture of the Variational Autoencoder (VAE) used in the Seaweed-7B video generation model.  It shows the process of encoding an input video (with dimensions (1+T) x H x W, where T represents the number of frames, H the height, and W the width) into a lower-dimensional latent space (T x H/dh x W/dw x C, where dh and dw are downsampling factors and C is the number of channels).  This compression is achieved using a 3D causal convolutional encoder. The decoder then reconstructs the video from this latent representation, using a 3D causal convolutional decoder to produce an output video with the same dimensions as the input.  The diagram highlights the key components of the VAE, including the encoder, latent space, and decoder, showing how the input video is compressed and then reconstructed.
> <details>
> <summary>read the caption</summary>
> Figure 3: Overview of VAE architecture.
> </details>



![](https://arxiv.org/html/2504.08685/extracted/6349619/figures/vae_visualization/original_cake.jpg)

> üîº This figure shows a comparison of the original video with its reconstruction after applying a variational autoencoder (VAE). The VAE is a crucial component of the Seaweed-7B video generation model, responsible for compressing the video data into a lower-dimensional latent space and then reconstructing it.  This figure helps to visualize the model's ability to reconstruct video data with high fidelity. The quality of the reconstructed video is key to the performance of the overall generation process.
> <details>
> <summary>read the caption</summary>
> (a) Original Video
> </details>



![](https://arxiv.org/html/2504.08685/extracted/6349619/figures/vae_visualization/v3_rec_cake.jpg)

> üîº The figure shows a visualization comparing the reconstruction quality of the 48x48x48 compression Seaweed VAE model.  This refers to the compression ratio achieved by the variational autoencoder (VAE) component of the model.  The image likely depicts the output of this VAE when processing a video frame, showing how well the model reconstructs the original video information from a compressed representation. The 48x refers to the overall downsampling factor applied during compression.  Higher numbers indicate stronger compression.
> <details>
> <summary>read the caption</summary>
> (b) 48√ó48\times48 √ó compression Seaweed VAE
> </details>



![](https://arxiv.org/html/2504.08685/extracted/6349619/figures/vae_visualization/v4_rec_cake.jpg)

> üîº The figure shows the result of compressing a video sequence using the 64x64x64 compression Seaweed VAE.  This refers to a variational autoencoder (VAE) model designed to compress video data efficiently while maintaining a good level of reconstruction quality. The '64x64x64' likely denotes the compression ratio or the reduction in spatial and temporal dimensions of the video.  The figure likely visualizes the reconstructed video after compression by this VAE, enabling a comparison of quality before and after the compression process. 
> <details>
> <summary>read the caption</summary>
> (c) 64√ó64\times64 √ó compression Seaweed VAE
> </details>



![](https://arxiv.org/html/2504.08685/extracted/6349619/figures/vae_visualization/original_skiing.jpg)

> üîº This figure compares the visual reconstruction quality of three different video versions: the original video and the output videos from two variants of the Seaweed VAE (Variational Autoencoder) model.  The Seaweed VAE is a crucial component of the overall video generation model, responsible for compressing the raw video data into a lower-dimensional latent representation which can be processed by the subsequent DiT (Diffusion Transformer) model. The two VAE variants differ in their compression ratios: one with a 48x compression ratio and the other with a 64x compression ratio. The figure displays a frame from each video at 25 frames per second (fps), and each video's resolution is 720x720 pixels.  This comparison helps to illustrate how compression ratio affects the reconstruction quality. By comparing the original to the outputs with the differing levels of compression, one can see how much information is lost or retained in the process.
> <details>
> <summary>read the caption</summary>
> Figure 4: VAE visualization comparison at 25 fps, with a resolution of 720√ó\times√ó720.
> </details>



![](https://arxiv.org/html/2504.08685/extracted/6349619/figures/vae_visualization/v3_skiing.jpg)

> üîº The figure shows a comparison of three video frames. (a) displays the original video frame.  The other frames (b and c) likely show the same frame reconstructed using different versions of a variational autoencoder (VAE) described in the paper, demonstrating the effect of different compression techniques on the quality of video reconstruction.  The differences between (a), (b) and (c) highlight the VAE's ability to compress and reconstruct video frames, with (b) possibly representing a lower compression ratio than (c).
> <details>
> <summary>read the caption</summary>
> (a) Original Video
> </details>



![](https://arxiv.org/html/2504.08685/extracted/6349619/figures/vae_visualization/v4_skiing.jpg)

> üîº This figure shows a visualization of the Seaweed VAE (Variational Autoencoder) with a compression ratio of 48x48x48. The VAE is a crucial component of the Seaweed-7B video generation model, responsible for compressing raw video data into a lower-dimensional latent space and then reconstructing the video from this compressed representation. A compression ratio of 48x48x48 indicates a significant reduction in data size.  The figure likely visualizes a sample video or frame that has been compressed and then reconstructed by the Seaweed VAE.  The visual quality of the reconstruction is a key indicator of the VAE's effectiveness.  A high-quality reconstruction, despite the high compression ratio, would show that the VAE is efficiently preserving important information during the compression process. The figure likely aims to demonstrate that the VAE achieves good compression without significant loss of visual fidelity.
> <details>
> <summary>read the caption</summary>
> (b) 48√ó48\times48 √ó Seaweed VAE
> </details>



![](https://arxiv.org/html/2504.08685/x3.png)

> üîº This figure visualizes the reconstruction quality of a variational autoencoder (VAE) model. Specifically, it showcases the results of a 64x64x64 Seaweed VAE, which compresses an input video into a 64x64x64 latent representation and then reconstructs it. This figure likely aims to demonstrate the VAE's ability to accurately represent the key features of a video with a relatively high compression ratio.  The figure likely shows a comparison of the original video with the reconstructed one to illustrate the efficacy of the VAE. This evaluation is crucial for demonstrating the effectiveness of the VAE as a component within the larger video generation model.
> <details>
> <summary>read the caption</summary>
> (c) 64√ó64\times64 √ó Seaweed VAE
> </details>



![](https://arxiv.org/html/2504.08685/x4.png)

> üîº This figure compares the visual quality of video reconstruction by three different methods: (a) the original video, (b) reconstruction using the 48x compression Seaweed VAE, and (c) reconstruction using the 64x compression Seaweed VAE.  All videos are shown at 24 frames per second (fps) and a resolution of 684x684 pixels. The comparison allows for a visual assessment of how different compression ratios impact the reconstruction's fidelity, detail, and overall visual quality.
> <details>
> <summary>read the caption</summary>
> Figure 5: VAE visualization comparison at 24 fps, with a resolution of 684√ó\times√ó684.
> </details>



![](https://arxiv.org/html/2504.08685/x5.png)

> üîº This figure shows a visualization of a Variational Autoencoder (VAE) with a compression ratio of 48x.  The VAE was trained for 30,000 steps. The visualization likely demonstrates the VAE's reconstruction capability, showing either input video frames and their corresponding compressed and then reconstructed representations, or the generated output video frames compared to the original video frames. The 48x compression ratio indicates that the VAE significantly reduces the size of the video data during encoding. This is crucial for efficient video generation, allowing the model to handle larger videos and reducing computational costs.
> <details>
> <summary>read the caption</summary>
> 48√ó48\times48 √ó VAE at 30k steps
> </details>



![](https://arxiv.org/html/2504.08685/x6.png)

> üîº This figure shows a visualization of the results of a variational autoencoder (VAE) model after 30,000 training steps.  The VAE uses a 64x64x64 compression ratio, meaning the input video frames are downsampled significantly before being encoded into a latent representation. This visualization likely demonstrates the quality of reconstruction after the model has learned to compress and decompress video frames efficiently.  The image shows a sample video frame from the reconstruction. This visualization is key in evaluating the performance and tradeoffs of different compression ratios in the VAE architecture, helping to determine the optimal balance between compression efficiency and reconstruction quality.
> <details>
> <summary>read the caption</summary>
> 64√ó64\times64 √ó VAE at 30k steps
> </details>



![](https://arxiv.org/html/2504.08685/x7.png)

> üîº This figure shows the results of using a 48x compression Seaweed Variational Autoencoder (VAE) trained for 45,000 steps.  The VAE is a crucial component in the video generation pipeline, responsible for compressing the input video frames into a lower-dimensional latent space.  This figure likely displays sample video frames to visually demonstrate the reconstruction quality of the VAE after training, showcasing how well the VAE can compress and then reconstruct video data. The '48x' likely refers to the compression ratio achieved by the model.
> <details>
> <summary>read the caption</summary>
> 48√ó48\times48 √ó VAE, 45k steps
> </details>



![](https://arxiv.org/html/2504.08685/x8.png)

> üîº This figure visualizes the reconstruction of a video by a Variational Autoencoder (VAE) model. Specifically, it shows the result of a 64x64x64 VAE model after 45,000 training steps.  The '64x64x64' refers to the compression ratio or dimensions of the latent space representation produced by the VAE.  The image likely demonstrates the model's ability to reconstruct the original video from its compressed latent representation, highlighting its reconstruction quality after a significant number of training iterations. 
> <details>
> <summary>read the caption</summary>
> 64√ó64\times64 √ó VAE at 45k steps
> </details>



![](https://arxiv.org/html/2504.08685/x9.png)

> üîº This figure visualizes the output of a 48x compression Variational Autoencoder (VAE) after 60,000 training steps.  The VAE is a crucial component in the Seaweed-7B video generation model, responsible for compressing raw video data into a lower-dimensional latent representation. This specific VAE uses a compression ratio of 48x, meaning that it reduces the dimensionality of the video data by a factor of 48. The image likely shows a sample video reconstruction from the latent space, demonstrating the quality of the VAE's compression and reconstruction capabilities after extensive training.
> <details>
> <summary>read the caption</summary>
> 48√ó48\times48 √ó VAE at 60k steps
> </details>



![](https://arxiv.org/html/2504.08685/x10.png)

> üîº This figure visualizes the results of a Variational Autoencoder (VAE) model trained for 60,000 steps with a 64x64x64 compression ratio.  It showcases the model's ability to reconstruct video frames after compression. The image likely depicts sample frames of a video showing how well the VAE reconstructs high-dimensional video data into a lower-dimensional latent space and back again.
> <details>
> <summary>read the caption</summary>
> 64√ó64\times64 √ó VAE at 60k steps
> </details>



![](https://arxiv.org/html/2504.08685/x11.png)

> üîº Figure 6 presents a comparison of video generation results using two different configurations of the Seaweed VAE model.  Both configurations used the prompt 'Zoom in, cat watching TV with a remote in hand, highly detailed.'  The left side shows results from a 48x Seaweed VAE, which has a lower compression ratio and uses DiT patchification with a patch size of (1,2,2).  The right side shows results from a 64x Seaweed VAE, which has a higher compression ratio and does *not* use DiT patchification, employing instead a patch size of (1,1,1).  The experiment highlights that, under the same computational resources, the 64x VAE achieves faster convergence.
> <details>
> <summary>read the caption</summary>
> Figure 6:  DiT generation results: keyframes from 73√ó192√ó3207319232073\times 192\times 32073 √ó 192 √ó 320 videos with the prompt 'Zoom in, cat watching TV with a remote in hand, highly detailed' are shown. Under the same compute, 64√ó64\times64 √ó Seaweed VAE converges faster with a higher compression ratio, without using a DiT patchify. Left: 48√ó48\times48 √ó Seaweed VAE (dt,dh,dw)=(4,16,16)subscriptùëëùë°subscriptùëë‚Ñésubscriptùëëùë§41616(d_{t},d_{h},d_{w})=(4,16,16)( italic_d start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_d start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT , italic_d start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT ) = ( 4 , 16 , 16 ) with DiT patch size (pt,ph,pw)=(1,2,2)subscriptùëùùë°subscriptùëù‚Ñésubscriptùëùùë§122(p_{t},p_{h},p_{w})=(1,2,2)( italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_p start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT , italic_p start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT ) = ( 1 , 2 , 2 ). Right: 64√ó64\times64 √ó Seaweed VAE (dt,dh,dw)=(4,32,32)subscriptùëëùë°subscriptùëë‚Ñésubscriptùëëùë§43232(d_{t},d_{h},d_{w})=(4,32,32)( italic_d start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_d start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT , italic_d start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT ) = ( 4 , 32 , 32 ) with DiT patch size (pt,ph,pw)=(1,1,1)subscriptùëùùë°subscriptùëù‚Ñésubscriptùëùùë§111(p_{t},p_{h},p_{w})=(1,1,1)( italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_p start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT , italic_p start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT ) = ( 1 , 1 , 1 ).
> </details>



![](https://arxiv.org/html/2504.08685/extracted/6349619/figures/dit_attn/scaling_laws.png)

> üîº Figure 7 presents a comparison of training a Variational Autoencoder (VAE) using two different approaches: one using only low-resolution (256x256) images and another using a mix of low-resolution and high-resolution (512x512) images.  The results show that while training with only low-resolution images yields faster initial convergence, performance plateaus and may even decline towards the end of training.  Conversely, using a mix of resolutions consistently improves the VAE's ability to reconstruct high-resolution images throughout the training process, as measured by rFID, LPIPS, and PSNR metrics. This demonstrates the effectiveness of the mixed-resolution training strategy for achieving better high-resolution reconstruction in VAEs.
> <details>
> <summary>read the caption</summary>
> Figure 7:  Validation metric curves on high-resolution image reconstruction (512√ó512512512512\times 512512 √ó 512) show the effectiveness of mix-resolution VAE training.
> </details>



![](https://arxiv.org/html/2504.08685/extracted/6349619/figures/dit_attn/new_attn_loss2.png)

> üîº This figure compares the training loss curves of two different diffusion transformer architectures: the dual-stream architecture and the hybrid-stream architecture.  Both architectures were trained with the same computational budget (measured in FLOPs, or floating point operations). The graph shows the training loss over time (steps) for each architecture. The accompanying table provides a direct numerical comparison of the final training loss achieved by each architecture under the identical FLOP constraint, highlighting the superior efficiency of the hybrid-stream design.
> <details>
> <summary>read the caption</summary>
> Figure 8: Loss comparison between the dual-stream and the hybrid-stream architectures. The table compares the two losses under the same training FLOPs.
> </details>



![](https://arxiv.org/html/2504.08685/x12.png)

> üîº Figure 9 illustrates three different attention mechanisms used in the Seaweed-7B video generation model.  (a) shows the 'space-full attention' architecture, which alternates between layers of full attention (considering all tokens in the sequence) and space-only attention (focusing only on spatial relationships within the video frames). (b) depicts the 'window attention' method where attention is restricted to smaller windows within the video sequence.  This is done to reduce computational cost for longer videos.  Each method has different trade-offs in terms of computation and performance. The figure visually represents these different attention patterns applied to video and text tokens during the processing.
> <details>
> <summary>read the caption</summary>
> Figure 9: Illustration of the space-full and window attention architecture.
> </details>



![](https://arxiv.org/html/2504.08685/extracted/6349619/figures/sft/turtle_keyframes.jpg)

> üîº This figure shows the loss comparison between two different attention mechanisms: full attention and space-full attention.  The x-axis represents the computational cost (measured in FLOPs), and the y-axis represents the loss. The space-full approach interleaves full attention and space-only attention layers, aiming to improve efficiency. The plot demonstrates how the loss changes as the computational budget increases for both methods, highlighting the trade-offs between computational cost and model performance in video generation tasks.
> <details>
> <summary>read the caption</summary>
> Figure 10: Loss comparison of full and space-full attention.
> </details>



![](https://arxiv.org/html/2504.08685/extracted/6349619/figures/sft/astronaut_00007.jpg)

> üîº This figure illustrates a comparison of training loss between three different attention mechanisms used in a video generation model: full attention, space-full attention (interleaving full and spatial attention), and windowed attention.  The x-axis represents the computational cost (FLOPs), while the y-axis shows the training loss.  The graph demonstrates how the loss varies with increasing computational cost for each attention method. This allows for an assessment of the trade-offs between computational efficiency and model performance for different attention strategies.  The figure helps in deciding which attention strategy is most suitable for video generation tasks, given resource constraints.
> <details>
> <summary>read the caption</summary>
> Figure 11: Loss comparison of full and window attention.
> </details>



![](https://arxiv.org/html/2504.08685/x13.png)

> üîº This figure showcases a comparison of training loss between two positional encoding methods: RoPE (Rotary Position Embedding) and MM-ROPE (Multimodal Rotary Position Embedding).  It demonstrates that MM-ROPE, which incorporates positional information for both text and video tokens, leads to lower training loss compared to the standard RoPE method.  This suggests that MM-ROPE is more effective in handling the diverse positional information present in the combined text and video data used for video generation.
> <details>
> <summary>read the caption</summary>
> Figure 12: Loss comparison between RoPE and MM-Rope.
> </details>



![](https://arxiv.org/html/2504.08685/extracted/6349619/figures/training_infrastructure/mlao.png)

> üîº This figure shows a comparison of video generation results before and after supervised fine-tuning (SFT). The prompt used was 'Turtle swimming in the ocean'. The top row displays the video generated before SFT, while the bottom row shows the improved video generated after SFT. The images illustrate how SFT enhances the visual quality of the generated video.  Specifically, one can see improvements in the visual details and clarity of the generated turtle and ocean after SFT.
> <details>
> <summary>read the caption</summary>
> Figure 13:  Top: Before SFT. Bottom: After SFT. Results for prompt 'Turtle swimming in the ocean'.
> </details>



![](https://arxiv.org/html/2504.08685/extracted/6349619/figures/training_infrastructure/runtime_balance.png)

> üîº This figure displays three video generation results using different training stages. The prompt is 'An astronaut running through an alley in Rio de Janeiro, 4k, high resolution'.  The left image shows the result before supervised fine-tuning (SFT), demonstrating a less refined and possibly less coherent video. The middle image displays the result after a successful SFT, showing improved visual quality and adherence to the prompt. The right image displays an overfit SFT result, where the model has become overly specialized to the training data, losing the ability to generate good results for unseen prompts. The comparison highlights how SFT improves video generation quality, but overfitting can decrease quality.
> <details>
> <summary>read the caption</summary>
> Figure 14:  Left: Before SFT. Middle: Good SFT. Right: Overfit SFT. Results for prompt: 'An astronaut running through an alley in Rio de Janeiro, 4k, high resolution'.
> </details>



![](https://arxiv.org/html/2504.08685/x14.png)

> üîº This figure showcases two examples of image-to-video generation using the Direct Preference Optimization (DPO) technique. The top row displays the results before applying DPO, while the bottom row presents the results after applying DPO.  A visual comparison reveals that DPO significantly improves the structural integrity and the quality of motion in the generated videos. The enhancements are particularly notable in the overall coherence and smoothness of the movements within the videos.
> <details>
> <summary>read the caption</summary>
> Figure 15: Two image-to-video examples before (top row) and after (bottom row) DPO. DPO significantly improves the structure and motion quality.
> </details>



![](https://arxiv.org/html/2504.08685/x15.png)

> üîº This figure illustrates the concept of Multi-level Activation Checkpointing (MLAC) and its advantages over traditional Activation Checkpointing (AC).  Panel (a) shows Vanilla AC, where intermediate activations are saved to the device's memory, which can lead to out-of-memory (OOM) errors, especially with large models.  Panel (b) shows MLAC, which addresses the OOM issue by offloading some module inputs to CPU or disk memory (zero-activation AC).  Finally, panel (c) details how MLAC minimizes recomputation costs by intelligently storing and retrieving activations from a hierarchy of storage (GPU, CPU, and disk), prioritizing the saving of computationally intensive activations to reduce the number of computations needed during backpropagation.
> <details>
> <summary>read the caption</summary>
> Figure 16: Multi-level activation checkpointing(MLAC). (a) Vanilla AC saves inputs on device could still encounter GPU OOM. (b) MLAC further supports offloading module inputs to achieve zero-activation AC, (c) and minimize recomputation overheads by saving compute-bound activations to multi-level storage space.
> </details>



![](https://arxiv.org/html/2504.08685/extracted/6349619/figures/vae_visualization/original.jpg)

> üîº Figure 17 illustrates the process of balancing computational load across multiple GPUs during model training. The top panel shows a lookup table that maps sequence lengths (seqlen) to their corresponding runtime on a single GPU. This table is used to estimate the runtime of each sample in a batch. The bottom-left panel displays the distribution of samples in a batch before load balancing; some GPUs have significantly more samples than others. The bottom-right panel shows the improved distribution of samples after applying the load-balancing technique.  The goal is to achieve a more even distribution of work across all GPUs for improved efficiency and reduced training time.
> <details>
> <summary>read the caption</summary>
> Figure 17: Balance samples within one batch across GPUs by runtime metric. Top: seqlen-to-runtime lookup table. Bottom left: One batch samples across GPUs before balance. Bottom left: One batch samples across GPUs after balance.
> </details>



![](https://arxiv.org/html/2504.08685/extracted/6349619/figures/vae_visualization/v3_formal.jpg)

> üîº This figure presents a detailed comparison of Seaweed-7B's performance on the image-to-video generation task against four top-performing models: Kling 1.6 HD, Sora, HunyuanVideo, and Wan 2.1.  The comparison is multifaceted, using human evaluations across multiple aspects of video quality.  These aspects include: Motion Quality (smoothness and naturalness of movement), Prompt Following (how well the generated video adheres to the input image prompt), Ref-image Consistency (how similar the generated video is to the input image), and Visual Quality (overall aesthetic appeal and fidelity). Each comparison is presented as a bar chart showing the percentage of times each model was preferred by human evaluators, with the remaining percentage indicating ties. This provides a comprehensive assessment of Seaweed-7B's strengths and weaknesses relative to the leading models, highlighting its competitive performance despite its smaller size and training resources.
> <details>
> <summary>read the caption</summary>
> Figure 18: Comparison of Seaweed-7B with the top ranking models: Kling 1.6, Sora, HunyuanVideo, and Wan-2.1. The task is image-to-video.
> </details>



![](https://arxiv.org/html/2504.08685/extracted/6349619/figures/vae_visualization/v4_formal.jpg)

> üîº This figure compares the performance of the Seaweed-7B model against Veo 2.0 and Wan-2.1 on text-to-video generation tasks.  It provides a detailed breakdown of each model's performance across several key metrics: Motion Quality, Prompt Following, Ref-image Consistency, Visual Quality, and an Overall score.  Each metric is presented as a bar graph showing the relative performance of each model. The visual comparison helps highlight the strengths and weaknesses of Seaweed-7B in relation to the leading models in text-to-video generation.
> <details>
> <summary>read the caption</summary>
> Figure 19: Comparison of the Seaweed-7B Model top ranking models: Veo 2.0 and Wan-2.1. The task is text-to-video.
> </details>



![](https://arxiv.org/html/2504.08685/extracted/6349619/figures/vae_visualization/hunyuan_4x8x8.jpg)

> üîº The figure shows a comparison of the original video with its reconstruction using the Seaweed VAE model at different compression ratios.  Specifically, it displays the original video frame alongside versions that have undergone compression with 48x and 64x compression ratios applied by the VAE, allowing visual assessment of the fidelity and quality of the compression.
> <details>
> <summary>read the caption</summary>
> (a) Original Video
> </details>



![](https://arxiv.org/html/2504.08685/extracted/6349619/figures/application/audio-cavp.png)

> üîº The figure shows a visualization of the 48x Seaweed VAE (Variational Autoencoder).  The 48x refers to the compression ratio achieved by this specific VAE model. The VAE takes an input video, compresses it into a lower-dimensional latent space, and then reconstructs the video from this compressed representation.  This particular figure likely showcases the reconstruction quality of the VAE, demonstrating its ability to effectively compress and reconstruct video data with minimal information loss.  The effectiveness of the compression is crucial, as this latent space representation forms the basis for the subsequent video generation process using a diffusion transformer.
> <details>
> <summary>read the caption</summary>
> (b) 48√ó48\times48 √ó Seaweed VAE
> </details>



</details>




<details>
<summary>More on tables
</summary>


{{< table-caption >}}
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T2.11">
<tr class="ltx_tr" id="S3.T2.11.12">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S3.T2.11.12.1"><span class="ltx_text ltx_font_bold" id="S3.T2.11.12.1.1">Training stage</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S3.T2.11.12.2"><span class="ltx_text ltx_font_bold" id="S3.T2.11.12.2.1">Image Resolution</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S3.T2.11.12.3"><span class="ltx_text ltx_font_bold" id="S3.T2.11.12.3.1">Video Resolution</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.11.12.4"><span class="ltx_text ltx_font_bold" id="S3.T2.11.12.4.1">Step Percentage</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.2.2.3">Stage 0: 256p</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.2.2.2">[256<math alttext="\times" class="ltx_Math" display="inline" id="S3.T2.1.1.1.m1.1"><semantics id="S3.T2.1.1.1.m1.1a"><mo id="S3.T2.1.1.1.m1.1.1" xref="S3.T2.1.1.1.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S3.T2.1.1.1.m1.1b"><times id="S3.T2.1.1.1.m1.1.1.cmml" xref="S3.T2.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.1.1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.T2.1.1.1.m1.1d">√ó</annotation></semantics></math>256, 512<math alttext="\times" class="ltx_Math" display="inline" id="S3.T2.2.2.2.m2.1"><semantics id="S3.T2.2.2.2.m2.1a"><mo id="S3.T2.2.2.2.m2.1.1" xref="S3.T2.2.2.2.m2.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S3.T2.2.2.2.m2.1b"><times id="S3.T2.2.2.2.m2.1.1.cmml" xref="S3.T2.2.2.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.2.2.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.T2.2.2.2.m2.1d">√ó</annotation></semantics></math>512]</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.2.2.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.2.5">37.5%</td>
</tr>
<tr class="ltx_tr" id="S3.T2.5.5">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.5.5.4">Stage 1: 256p</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.4.4.2">[256<math alttext="\times" class="ltx_Math" display="inline" id="S3.T2.3.3.1.m1.1"><semantics id="S3.T2.3.3.1.m1.1a"><mo id="S3.T2.3.3.1.m1.1.1" xref="S3.T2.3.3.1.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S3.T2.3.3.1.m1.1b"><times id="S3.T2.3.3.1.m1.1.1.cmml" xref="S3.T2.3.3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.3.3.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.T2.3.3.1.m1.1d">√ó</annotation></semantics></math>256, 512<math alttext="\times" class="ltx_Math" display="inline" id="S3.T2.4.4.2.m2.1"><semantics id="S3.T2.4.4.2.m2.1a"><mo id="S3.T2.4.4.2.m2.1.1" xref="S3.T2.4.4.2.m2.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S3.T2.4.4.2.m2.1b"><times id="S3.T2.4.4.2.m2.1.1.cmml" xref="S3.T2.4.4.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.4.4.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.T2.4.4.2.m2.1d">√ó</annotation></semantics></math>512]</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.5.5.3">[256<math alttext="\times" class="ltx_Math" display="inline" id="S3.T2.5.5.3.m1.1"><semantics id="S3.T2.5.5.3.m1.1a"><mo id="S3.T2.5.5.3.m1.1.1" xref="S3.T2.5.5.3.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S3.T2.5.5.3.m1.1b"><times id="S3.T2.5.5.3.m1.1.1.cmml" xref="S3.T2.5.5.3.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.5.5.3.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.T2.5.5.3.m1.1d">√ó</annotation></semantics></math>256]</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.5.5.5">25.0%</td>
</tr>
<tr class="ltx_tr" id="S3.T2.8.8">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.8.8.4">Stage 2: 480p</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.7.7.2">[640<math alttext="\times" class="ltx_Math" display="inline" id="S3.T2.6.6.1.m1.1"><semantics id="S3.T2.6.6.1.m1.1a"><mo id="S3.T2.6.6.1.m1.1.1" xref="S3.T2.6.6.1.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S3.T2.6.6.1.m1.1b"><times id="S3.T2.6.6.1.m1.1.1.cmml" xref="S3.T2.6.6.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.6.6.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.T2.6.6.1.m1.1d">√ó</annotation></semantics></math>480, 1280<math alttext="\times" class="ltx_Math" display="inline" id="S3.T2.7.7.2.m2.1"><semantics id="S3.T2.7.7.2.m2.1a"><mo id="S3.T2.7.7.2.m2.1.1" xref="S3.T2.7.7.2.m2.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S3.T2.7.7.2.m2.1b"><times id="S3.T2.7.7.2.m2.1.1.cmml" xref="S3.T2.7.7.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.7.7.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.T2.7.7.2.m2.1d">√ó</annotation></semantics></math>720]</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.8.8.3">[640<math alttext="\times" class="ltx_Math" display="inline" id="S3.T2.8.8.3.m1.1"><semantics id="S3.T2.8.8.3.m1.1a"><mo id="S3.T2.8.8.3.m1.1.1" xref="S3.T2.8.8.3.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S3.T2.8.8.3.m1.1b"><times id="S3.T2.8.8.3.m1.1.1.cmml" xref="S3.T2.8.8.3.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.8.8.3.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.T2.8.8.3.m1.1d">√ó</annotation></semantics></math>480]</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.8.8.5">25.0%</td>
</tr>
<tr class="ltx_tr" id="S3.T2.11.11">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S3.T2.11.11.4">Stage 3: 720p</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S3.T2.10.10.2">[1280<math alttext="\times" class="ltx_Math" display="inline" id="S3.T2.9.9.1.m1.1"><semantics id="S3.T2.9.9.1.m1.1a"><mo id="S3.T2.9.9.1.m1.1.1" xref="S3.T2.9.9.1.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S3.T2.9.9.1.m1.1b"><times id="S3.T2.9.9.1.m1.1.1.cmml" xref="S3.T2.9.9.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.9.9.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.T2.9.9.1.m1.1d">√ó</annotation></semantics></math>720, 1920<math alttext="\times" class="ltx_Math" display="inline" id="S3.T2.10.10.2.m2.1"><semantics id="S3.T2.10.10.2.m2.1a"><mo id="S3.T2.10.10.2.m2.1.1" xref="S3.T2.10.10.2.m2.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S3.T2.10.10.2.m2.1b"><times id="S3.T2.10.10.2.m2.1.1.cmml" xref="S3.T2.10.10.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.10.10.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.T2.10.10.2.m2.1d">√ó</annotation></semantics></math>1024]</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S3.T2.11.11.3">[1280<math alttext="\times" class="ltx_Math" display="inline" id="S3.T2.11.11.3.m1.1"><semantics id="S3.T2.11.11.3.m1.1a"><mo id="S3.T2.11.11.3.m1.1.1" xref="S3.T2.11.11.3.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S3.T2.11.11.3.m1.1b"><times id="S3.T2.11.11.3.m1.1.1.cmml" xref="S3.T2.11.11.3.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.11.11.3.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.T2.11.11.3.m1.1d">√ó</annotation></semantics></math>720]</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T2.11.11.5">12.5%</td>
</tr>
</table>{{< /table-caption >}}
> üîº This table summarizes the multi-stage training process for the Seaweed-7B video generation model.  Training is divided into four stages, each focusing on a specific resolution range (256p, 480p, 720p). The table shows the image and video resolutions targeted in each stage, indicating the target area rather than exact dimensions (images and videos are resized to match the target aspect ratio).  The 'Step Percentage' column indicates the proportion of the total training steps dedicated to each stage.  This multi-stage approach allows for efficient use of computational resources by starting with lower resolutions and gradually increasing the complexity.
> <details>
> <summary>read the caption</summary>
> Table 2: Summary of the pre-training stages. Step Percentage is the proportion of total training steps allocated to each stage. The image and video resolution (e.g., 256√ó256) refers to the target area, not the exact dimension.
> </details>

{{< table-caption >}}
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T3.2">
<tr class="ltx_tr" id="S4.T3.2.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T3.2.1.1"><span class="ltx_text ltx_font_bold" id="S4.T3.2.1.1.1">Name</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.2.1.2"><span class="ltx_text ltx_font_bold" id="S4.T3.2.1.2.1">ELO</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.2.1.3"><span class="ltx_text ltx_font_bold" id="S4.T3.2.1.3.1">Win Ratio</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T3.2.1.4"><span class="ltx_text ltx_font_bold" id="S4.T3.2.1.4.1">Model Size</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.2.2.1">Kling 1.6 HD</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.2.2">1,065</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.2.3">61%</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T3.2.2.4">-</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.3">
<td class="ltx_td ltx_align_left" id="S4.T3.2.3.1"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T3.2.3.1.1">Seaweed-7B</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.3.2">1,047</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.3.3">58%</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.2.3.4">7B</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.4">
<td class="ltx_td ltx_align_left" id="S4.T3.2.4.1">Wan 2.1</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.4.2">1,015</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.4.3">53%</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.2.4.4">14B</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.5">
<td class="ltx_td ltx_align_left" id="S4.T3.2.5.1">Luma Ray 2</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.5.2">1,003</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.5.3">51%</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.2.5.4">-</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.6">
<td class="ltx_td ltx_align_left" id="S4.T3.2.6.1">Runway Gen-3 Alpha</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.6.2">1,000</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.6.3">53%</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.2.6.4">-</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.7">
<td class="ltx_td ltx_align_left" id="S4.T3.2.7.1">Veo 2.0</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.7.2">992</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.7.3">50%</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.2.7.4">-</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.8">
<td class="ltx_td ltx_align_left" id="S4.T3.2.8.1">HunyuanVideo</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.8.2">944</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.8.3">43%</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.2.8.4">13B</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.9">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T3.2.9.1">Sora</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.2.9.2">903</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.2.9.3">36%</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T3.2.9.4">-</td>
</tr>
</table>{{< /table-caption >}}
> üîº This table presents the results of an Elo rating system comparison for image-to-video generation models.  The Elo rating system ranks models based on head-to-head comparisons of video generation quality as judged by human evaluators.  Each model's Elo score reflects its overall performance relative to others in the comparison, with higher scores indicating better quality. The table lists the model name, its Elo score, win ratio (percentage of wins in head-to-head comparisons), model type, and the number of parameters (size) of each model. This allows for a quantitative comparison of the relative performance of different video generation models, considering both model size and generated video quality.
> <details>
> <summary>read the caption</summary>
> Table 3: Elo comparison for the image-to-video task.
> </details>

{{< table-caption >}}
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T4.2">
<tr class="ltx_tr" id="S4.T4.2.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T4.2.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.2.1.1.1">Model</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T4.2.1.2" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.2.1.2.1">Parameters</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T4.2.1.3" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.2.1.3.1">I2V Win Ratio</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T4.2.1.4" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.2.1.4.1">NFEs</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T4.2.1.5" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.2.1.5.1">Time (s)</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.2.2.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">Wan 2.1</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T4.2.2.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">14B</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T4.2.2.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">53%</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T4.2.2.4" style="padding-top:1.5pt;padding-bottom:1.5pt;">100</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T4.2.2.5" style="padding-top:1.5pt;padding-bottom:1.5pt;">1837.9</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.3">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T4.2.3.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">Seaweed</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T4.2.3.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">7B</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T4.2.3.3" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.2.3.3.1">58%</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T4.2.3.4" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.2.3.4.1">12</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T4.2.3.5" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.2.3.5.1">29.6</span></td>
</tr>
</table>{{< /table-caption >}}
> üîº This table presents a comparison of the computational efficiency between the Seaweed-7B model and the Wan 2.1 model.  The comparison focuses on the inference time required for video generation on a single NVIDIA H100 GPU.  Metrics such as the number of parameters, the compression ratio used during VAE encoding, the number of neural function evaluations (NFEs) needed for inference, and the total inference time are included to provide a comprehensive view of the efficiency differences between these models.
> <details>
> <summary>read the caption</summary>
> Table 4: Computational efficiency comparison between ours and Wan 2.1 measured on a single H100 GPU.
> </details>

{{< table-caption >}}
<table class="ltx_tabular ltx_align_middle" id="S4.T5.11.11">
<tr class="ltx_tr" id="S4.T5.11.11.12">
<td class="ltx_td ltx_border_tt" id="S4.T5.11.11.12.1"></td>
<td class="ltx_td ltx_border_tt" colspan="4" id="S4.T5.11.11.12.2"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="4" id="S4.T5.11.11.12.3">UCF-101</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T5.11.11.12.4">MCL-JCV</td>
</tr>
<tr class="ltx_tr" id="S4.T5.8.8.8">
<td class="ltx_td" id="S4.T5.8.8.8.9"></td>
<td class="ltx_td ltx_align_center" id="S4.T5.8.8.8.10">Params (M)</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.1.1.1"><math alttext="(d_{t},d_{h},d_{w})" class="ltx_Math" display="inline" id="S4.T5.1.1.1.1.m1.3"><semantics id="S4.T5.1.1.1.1.m1.3a"><mrow id="S4.T5.1.1.1.1.m1.3.3.3" xref="S4.T5.1.1.1.1.m1.3.3.4.cmml"><mo id="S4.T5.1.1.1.1.m1.3.3.3.4" stretchy="false" xref="S4.T5.1.1.1.1.m1.3.3.4.cmml">(</mo><msub id="S4.T5.1.1.1.1.m1.1.1.1.1" xref="S4.T5.1.1.1.1.m1.1.1.1.1.cmml"><mi id="S4.T5.1.1.1.1.m1.1.1.1.1.2" xref="S4.T5.1.1.1.1.m1.1.1.1.1.2.cmml">d</mi><mi id="S4.T5.1.1.1.1.m1.1.1.1.1.3" xref="S4.T5.1.1.1.1.m1.1.1.1.1.3.cmml">t</mi></msub><mo id="S4.T5.1.1.1.1.m1.3.3.3.5" xref="S4.T5.1.1.1.1.m1.3.3.4.cmml">,</mo><msub id="S4.T5.1.1.1.1.m1.2.2.2.2" xref="S4.T5.1.1.1.1.m1.2.2.2.2.cmml"><mi id="S4.T5.1.1.1.1.m1.2.2.2.2.2" xref="S4.T5.1.1.1.1.m1.2.2.2.2.2.cmml">d</mi><mi id="S4.T5.1.1.1.1.m1.2.2.2.2.3" xref="S4.T5.1.1.1.1.m1.2.2.2.2.3.cmml">h</mi></msub><mo id="S4.T5.1.1.1.1.m1.3.3.3.6" xref="S4.T5.1.1.1.1.m1.3.3.4.cmml">,</mo><msub id="S4.T5.1.1.1.1.m1.3.3.3.3" xref="S4.T5.1.1.1.1.m1.3.3.3.3.cmml"><mi id="S4.T5.1.1.1.1.m1.3.3.3.3.2" xref="S4.T5.1.1.1.1.m1.3.3.3.3.2.cmml">d</mi><mi id="S4.T5.1.1.1.1.m1.3.3.3.3.3" xref="S4.T5.1.1.1.1.m1.3.3.3.3.3.cmml">w</mi></msub><mo id="S4.T5.1.1.1.1.m1.3.3.3.7" stretchy="false" xref="S4.T5.1.1.1.1.m1.3.3.4.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.1.1.1.1.m1.3b"><vector id="S4.T5.1.1.1.1.m1.3.3.4.cmml" xref="S4.T5.1.1.1.1.m1.3.3.3"><apply id="S4.T5.1.1.1.1.m1.1.1.1.1.cmml" xref="S4.T5.1.1.1.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S4.T5.1.1.1.1.m1.1.1.1.1.1.cmml" xref="S4.T5.1.1.1.1.m1.1.1.1.1">subscript</csymbol><ci id="S4.T5.1.1.1.1.m1.1.1.1.1.2.cmml" xref="S4.T5.1.1.1.1.m1.1.1.1.1.2">ùëë</ci><ci id="S4.T5.1.1.1.1.m1.1.1.1.1.3.cmml" xref="S4.T5.1.1.1.1.m1.1.1.1.1.3">ùë°</ci></apply><apply id="S4.T5.1.1.1.1.m1.2.2.2.2.cmml" xref="S4.T5.1.1.1.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S4.T5.1.1.1.1.m1.2.2.2.2.1.cmml" xref="S4.T5.1.1.1.1.m1.2.2.2.2">subscript</csymbol><ci id="S4.T5.1.1.1.1.m1.2.2.2.2.2.cmml" xref="S4.T5.1.1.1.1.m1.2.2.2.2.2">ùëë</ci><ci id="S4.T5.1.1.1.1.m1.2.2.2.2.3.cmml" xref="S4.T5.1.1.1.1.m1.2.2.2.2.3">‚Ñé</ci></apply><apply id="S4.T5.1.1.1.1.m1.3.3.3.3.cmml" xref="S4.T5.1.1.1.1.m1.3.3.3.3"><csymbol cd="ambiguous" id="S4.T5.1.1.1.1.m1.3.3.3.3.1.cmml" xref="S4.T5.1.1.1.1.m1.3.3.3.3">subscript</csymbol><ci id="S4.T5.1.1.1.1.m1.3.3.3.3.2.cmml" xref="S4.T5.1.1.1.1.m1.3.3.3.3.2">ùëë</ci><ci id="S4.T5.1.1.1.1.m1.3.3.3.3.3.cmml" xref="S4.T5.1.1.1.1.m1.3.3.3.3.3">ùë§</ci></apply></vector></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.1.1.1.1.m1.3c">(d_{t},d_{h},d_{w})</annotation><annotation encoding="application/x-llamapun" id="S4.T5.1.1.1.1.m1.3d">( italic_d start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_d start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT , italic_d start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.2.2.2"><math alttext="C" class="ltx_Math" display="inline" id="S4.T5.2.2.2.2.m1.1"><semantics id="S4.T5.2.2.2.2.m1.1a"><mi id="S4.T5.2.2.2.2.m1.1.1" xref="S4.T5.2.2.2.2.m1.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S4.T5.2.2.2.2.m1.1b"><ci id="S4.T5.2.2.2.2.m1.1.1.cmml" xref="S4.T5.2.2.2.2.m1.1.1">ùê∂</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.2.2.2.2.m1.1c">C</annotation><annotation encoding="application/x-llamapun" id="S4.T5.2.2.2.2.m1.1d">italic_C</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T5.3.3.3.3"><math alttext="r" class="ltx_Math" display="inline" id="S4.T5.3.3.3.3.m1.1"><semantics id="S4.T5.3.3.3.3.m1.1a"><mi id="S4.T5.3.3.3.3.m1.1.1" xref="S4.T5.3.3.3.3.m1.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S4.T5.3.3.3.3.m1.1b"><ci id="S4.T5.3.3.3.3.m1.1.1.cmml" xref="S4.T5.3.3.3.3.m1.1.1">ùëü</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.3.3.3.3.m1.1c">r</annotation><annotation encoding="application/x-llamapun" id="S4.T5.3.3.3.3.m1.1d">italic_r</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.4.4.4.4">rFVD <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T5.4.4.4.4.m1.1"><semantics id="S4.T5.4.4.4.4.m1.1a"><mo id="S4.T5.4.4.4.4.m1.1.1" stretchy="false" xref="S4.T5.4.4.4.4.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S4.T5.4.4.4.4.m1.1b"><ci id="S4.T5.4.4.4.4.m1.1.1.cmml" xref="S4.T5.4.4.4.4.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.4.4.4.4.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T5.4.4.4.4.m1.1d">‚Üì</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.5.5.5.5">LPIPS <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T5.5.5.5.5.m1.1"><semantics id="S4.T5.5.5.5.5.m1.1a"><mo id="S4.T5.5.5.5.5.m1.1.1" stretchy="false" xref="S4.T5.5.5.5.5.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S4.T5.5.5.5.5.m1.1b"><ci id="S4.T5.5.5.5.5.m1.1.1.cmml" xref="S4.T5.5.5.5.5.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.5.5.5.5.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T5.5.5.5.5.m1.1d">‚Üì</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.6.6.6.6">PSNR <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T5.6.6.6.6.m1.1"><semantics id="S4.T5.6.6.6.6.m1.1a"><mo id="S4.T5.6.6.6.6.m1.1.1" stretchy="false" xref="S4.T5.6.6.6.6.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S4.T5.6.6.6.6.m1.1b"><ci id="S4.T5.6.6.6.6.m1.1.1.cmml" xref="S4.T5.6.6.6.6.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.6.6.6.6.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T5.6.6.6.6.m1.1d">‚Üë</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.7.7.7.7">SSIM <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T5.7.7.7.7.m1.1"><semantics id="S4.T5.7.7.7.7.m1.1a"><mo id="S4.T5.7.7.7.7.m1.1.1" stretchy="false" xref="S4.T5.7.7.7.7.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S4.T5.7.7.7.7.m1.1b"><ci id="S4.T5.7.7.7.7.m1.1.1.cmml" xref="S4.T5.7.7.7.7.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.7.7.7.7.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T5.7.7.7.7.m1.1d">‚Üë</annotation></semantics></math>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T5.8.8.8.8">LPIPS <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T5.8.8.8.8.m1.1"><semantics id="S4.T5.8.8.8.8.m1.1a"><mo id="S4.T5.8.8.8.8.m1.1.1" stretchy="false" xref="S4.T5.8.8.8.8.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S4.T5.8.8.8.8.m1.1b"><ci id="S4.T5.8.8.8.8.m1.1.1.cmml" xref="S4.T5.8.8.8.8.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.8.8.8.8.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T5.8.8.8.8.m1.1d">‚Üì</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr" id="S4.T5.11.11.13">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T5.11.11.13.1">Open-Sora v1.2 <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08685v1#bib.bib99" title=""><span class="ltx_text" style="font-size:90%;">99</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.11.11.13.2">393.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.11.11.13.3">(4, 8, 8)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.11.11.13.4">4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.11.11.13.5">1:192</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.11.11.13.6">47.04</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.11.11.13.7">0.1661</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.11.11.13.8">27.70</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.11.11.13.9">0.8893</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T5.11.11.13.10">0.2687</td>
</tr>
<tr class="ltx_tr" id="S4.T5.11.11.14">
<td class="ltx_td ltx_align_left" id="S4.T5.11.11.14.1">LTX-Video <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08685v1#bib.bib28" title=""><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T5.11.11.14.2">935.0</td>
<td class="ltx_td ltx_align_center" id="S4.T5.11.11.14.3">(8, 32, 32)</td>
<td class="ltx_td ltx_align_center" id="S4.T5.11.11.14.4">128</td>
<td class="ltx_td ltx_align_center" id="S4.T5.11.11.14.5">1:192</td>
<td class="ltx_td ltx_align_center" id="S4.T5.11.11.14.6">45.08</td>
<td class="ltx_td ltx_align_center" id="S4.T5.11.11.14.7">0.1257</td>
<td class="ltx_td ltx_align_center" id="S4.T5.11.11.14.8">29.30</td>
<td class="ltx_td ltx_align_center" id="S4.T5.11.11.14.9">0.8591</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T5.11.11.14.10">0.2486</td>
</tr>
<tr class="ltx_tr" id="S4.T5.9.9.9">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T5.9.9.9.1">Cosmos (<math alttext="48\times" class="ltx_math_unparsed" display="inline" id="S4.T5.9.9.9.1.m1.1"><semantics id="S4.T5.9.9.9.1.m1.1a"><mrow id="S4.T5.9.9.9.1.m1.1b"><mn id="S4.T5.9.9.9.1.m1.1.1">48</mn><mo id="S4.T5.9.9.9.1.m1.1.2" lspace="0.222em">√ó</mo></mrow><annotation encoding="application/x-tex" id="S4.T5.9.9.9.1.m1.1c">48\times</annotation><annotation encoding="application/x-llamapun" id="S4.T5.9.9.9.1.m1.1d">48 √ó</annotation></semantics></math>) <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08685v1#bib.bib3" title=""><span class="ltx_text" style="font-size:90%;">3</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.9.9.9.2">90.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.9.9.9.3">(4, 8, 8)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.9.9.9.4">16</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.9.9.9.5">1:48</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.9.9.9.6">13.02</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.9.9.9.7">0.0847</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.9.9.9.8">32.34</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.9.9.9.9">0.9484</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T5.9.9.9.10">0.1851</td>
</tr>
<tr class="ltx_tr" id="S4.T5.11.11.15">
<td class="ltx_td ltx_align_left" id="S4.T5.11.11.15.1">SVD <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08685v1#bib.bib5" title=""><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T5.11.11.15.2">97.7</td>
<td class="ltx_td ltx_align_center" id="S4.T5.11.11.15.3">(1, 8, 8)</td>
<td class="ltx_td ltx_align_center" id="S4.T5.11.11.15.4">4</td>
<td class="ltx_td ltx_align_center" id="S4.T5.11.11.15.5">1:48</td>
<td class="ltx_td ltx_align_center" id="S4.T5.11.11.15.6">11.10</td>
<td class="ltx_td ltx_align_center" id="S4.T5.11.11.15.7">0.0751</td>
<td class="ltx_td ltx_align_center" id="S4.T5.11.11.15.8">30.81</td>
<td class="ltx_td ltx_align_center" id="S4.T5.11.11.15.9">0.9356</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T5.11.11.15.10">0.1137</td>
</tr>
<tr class="ltx_tr" id="S4.T5.11.11.16">
<td class="ltx_td ltx_align_left" id="S4.T5.11.11.16.1">Wan-VAE <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08685v1#bib.bib77" title=""><span class="ltx_text" style="font-size:90%;">77</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T5.11.11.16.2">126.9</td>
<td class="ltx_td ltx_align_center" id="S4.T5.11.11.16.3">(4, 8, 8)</td>
<td class="ltx_td ltx_align_center" id="S4.T5.11.11.16.4">16</td>
<td class="ltx_td ltx_align_center" id="S4.T5.11.11.16.5">1:48</td>
<td class="ltx_td ltx_align_center" id="S4.T5.11.11.16.6">2.08</td>
<td class="ltx_td ltx_align_center" id="S4.T5.11.11.16.7">0.0463</td>
<td class="ltx_td ltx_align_center" id="S4.T5.11.11.16.8">34.00</td>
<td class="ltx_td ltx_align_center" id="S4.T5.11.11.16.9">0.9603</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T5.11.11.16.10">0.1034</td>
</tr>
<tr class="ltx_tr" id="S4.T5.11.11.17">
<td class="ltx_td ltx_align_left" id="S4.T5.11.11.17.1">CV-VAE (SD3) <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08685v1#bib.bib97" title=""><span class="ltx_text" style="font-size:90%;">97</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T5.11.11.17.2">181.9</td>
<td class="ltx_td ltx_align_center" id="S4.T5.11.11.17.3">(4, 8, 8)</td>
<td class="ltx_td ltx_align_center" id="S4.T5.11.11.17.4">16</td>
<td class="ltx_td ltx_align_center" id="S4.T5.11.11.17.5">1:48</td>
<td class="ltx_td ltx_align_center" id="S4.T5.11.11.17.6">6.50</td>
<td class="ltx_td ltx_align_center" id="S4.T5.11.11.17.7">0.0589</td>
<td class="ltx_td ltx_align_center" id="S4.T5.11.11.17.8">33.21</td>
<td class="ltx_td ltx_align_center" id="S4.T5.11.11.17.9">0.9612</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T5.11.11.17.10">0.1437</td>
</tr>
<tr class="ltx_tr" id="S4.T5.11.11.18">
<td class="ltx_td ltx_align_left" id="S4.T5.11.11.18.1">CogVideoX <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08685v1#bib.bib91" title=""><span class="ltx_text" style="font-size:90%;">91</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T5.11.11.18.2">215.6</td>
<td class="ltx_td ltx_align_center" id="S4.T5.11.11.18.3">(4, 8, 8)</td>
<td class="ltx_td ltx_align_center" id="S4.T5.11.11.18.4">16</td>
<td class="ltx_td ltx_align_center" id="S4.T5.11.11.18.5">1:48</td>
<td class="ltx_td ltx_align_center" id="S4.T5.11.11.18.6">6.06</td>
<td class="ltx_td ltx_align_center" id="S4.T5.11.11.18.7">0.0623</td>
<td class="ltx_td ltx_align_center" id="S4.T5.11.11.18.8">34.30</td>
<td class="ltx_td ltx_align_center" id="S4.T5.11.11.18.9">0.9650</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T5.11.11.18.10">0.1378</td>
</tr>
<tr class="ltx_tr" id="S4.T5.11.11.19">
<td class="ltx_td ltx_align_left" id="S4.T5.11.11.19.1">HunyuanVideo <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08685v1#bib.bib41" title=""><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T5.11.11.19.2">246.5</td>
<td class="ltx_td ltx_align_center" id="S4.T5.11.11.19.3">(4, 8, 8)</td>
<td class="ltx_td ltx_align_center" id="S4.T5.11.11.19.4">16</td>
<td class="ltx_td ltx_align_center" id="S4.T5.11.11.19.5">1:48</td>
<td class="ltx_td ltx_align_center" id="S4.T5.11.11.19.6"><span class="ltx_text ltx_font_bold" id="S4.T5.11.11.19.6.1">1.79</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.11.11.19.7">0.0456</td>
<td class="ltx_td ltx_align_center" id="S4.T5.11.11.19.8">35.15</td>
<td class="ltx_td ltx_align_center" id="S4.T5.11.11.19.9">0.9713</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T5.11.11.19.10">0.1102</td>
</tr>
<tr class="ltx_tr" id="S4.T5.10.10.10">
<td class="ltx_td ltx_align_left" id="S4.T5.10.10.10.1">our Seaweed VAE (<math alttext="48\times" class="ltx_math_unparsed" display="inline" id="S4.T5.10.10.10.1.m1.1"><semantics id="S4.T5.10.10.10.1.m1.1a"><mrow id="S4.T5.10.10.10.1.m1.1b"><mn id="S4.T5.10.10.10.1.m1.1.1">48</mn><mo id="S4.T5.10.10.10.1.m1.1.2" lspace="0.222em">√ó</mo></mrow><annotation encoding="application/x-tex" id="S4.T5.10.10.10.1.m1.1c">48\times</annotation><annotation encoding="application/x-llamapun" id="S4.T5.10.10.10.1.m1.1d">48 √ó</annotation></semantics></math>)</td>
<td class="ltx_td ltx_align_center" id="S4.T5.10.10.10.2">250.6</td>
<td class="ltx_td ltx_align_center" id="S4.T5.10.10.10.3">(4, 8, 8)</td>
<td class="ltx_td ltx_align_center" id="S4.T5.10.10.10.4">16</td>
<td class="ltx_td ltx_align_center" id="S4.T5.10.10.10.5">1:48</td>
<td class="ltx_td ltx_align_center" id="S4.T5.10.10.10.6">1.85</td>
<td class="ltx_td ltx_align_center" id="S4.T5.10.10.10.7">0.0517</td>
<td class="ltx_td ltx_align_center" id="S4.T5.10.10.10.8">33.83</td>
<td class="ltx_td ltx_align_center" id="S4.T5.10.10.10.9">0.9643</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T5.10.10.10.10">0.1477</td>
</tr>
<tr class="ltx_tr" id="S4.T5.11.11.20">
<td class="ltx_td ltx_align_left" id="S4.T5.11.11.20.1">WFVAE <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08685v1#bib.bib43" title=""><span class="ltx_text" style="font-size:90%;">43</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T5.11.11.20.2">317.1</td>
<td class="ltx_td ltx_align_center" id="S4.T5.11.11.20.3">(4, 8, 8)</td>
<td class="ltx_td ltx_align_center" id="S4.T5.11.11.20.4">16</td>
<td class="ltx_td ltx_align_center" id="S4.T5.11.11.20.5">1:48</td>
<td class="ltx_td ltx_align_center" id="S4.T5.11.11.20.6">3.15</td>
<td class="ltx_td ltx_align_center" id="S4.T5.11.11.20.7">0.0643</td>
<td class="ltx_td ltx_align_center" id="S4.T5.11.11.20.8">34.13</td>
<td class="ltx_td ltx_align_center" id="S4.T5.11.11.20.9">0.9687</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T5.11.11.20.10">0.1572</td>
</tr>
<tr class="ltx_tr" id="S4.T5.11.11.11">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T5.11.11.11.1">Our Seaweed VAE (<math alttext="64\times" class="ltx_math_unparsed" display="inline" id="S4.T5.11.11.11.1.m1.1"><semantics id="S4.T5.11.11.11.1.m1.1a"><mrow id="S4.T5.11.11.11.1.m1.1b"><mn id="S4.T5.11.11.11.1.m1.1.1">64</mn><mo id="S4.T5.11.11.11.1.m1.1.2" lspace="0.222em">√ó</mo></mrow><annotation encoding="application/x-tex" id="S4.T5.11.11.11.1.m1.1c">64\times</annotation><annotation encoding="application/x-llamapun" id="S4.T5.11.11.11.1.m1.1d">64 √ó</annotation></semantics></math>)</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T5.11.11.11.2">552.8</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T5.11.11.11.3">(4, 16, 16)</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T5.11.11.11.4">48</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T5.11.11.11.5">1:64</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T5.11.11.11.6">2.43</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T5.11.11.11.7"><span class="ltx_text ltx_font_bold" id="S4.T5.11.11.11.7.1">0.0391</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T5.11.11.11.8"><span class="ltx_text ltx_font_bold" id="S4.T5.11.11.11.8.1">35.23</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T5.11.11.11.9"><span class="ltx_text ltx_font_bold" id="S4.T5.11.11.11.9.1">0.9717</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" id="S4.T5.11.11.11.10"><span class="ltx_text ltx_font_bold" id="S4.T5.11.11.11.10.1">0.0945</span></td>
</tr>
</table>{{< /table-caption >}}
> üîº This table presents a quantitative comparison of the performance of different Variational Autoencoder (VAE) models on two video datasets: UCF-101 and MCL-JCV.  UCF-101 contains short video clips (17 frames) resized to 256x256 pixels, while MCL-JCV consists of longer, higher-resolution videos ([117-149] frames) with a resolution of 720x1080 pixels. The table compares several state-of-the-art VAEs and the Seaweed VAE (both 48x and 64x versions) across multiple metrics, including reconstruction quality (LPIPS, rFVD, PSNR, SSIM). These metrics evaluate how well the VAEs reconstruct the original videos from their compressed latent representations, providing insights into the effectiveness of different VAE architectures and compression strategies. The parameters (model size), compression ratio (r), and other crucial aspects of the VAEs are also included for better context.
> <details>
> <summary>read the caption</summary>
> Table 5: Reconstruction results for VAE on UCF-101 (17√ó\times√ó256√ó\times√ó256) and MCL-JCV ([117-149] √ó\times√ó720√ó\times√ó1080)
> </details>

</details>




### Full paper

{{< gallery >}}
<img src="https://ai-paper-reviewer.com/2504.08685/1.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.08685/2.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.08685/3.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.08685/4.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.08685/5.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.08685/6.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.08685/7.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.08685/8.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.08685/9.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.08685/10.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.08685/11.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.08685/12.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.08685/13.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.08685/14.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.08685/15.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.08685/16.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.08685/17.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.08685/18.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.08685/19.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.08685/20.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
{{< /gallery >}}