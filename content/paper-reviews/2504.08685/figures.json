[{"figure_path": "https://arxiv.org/html/2504.08685/x1.png", "caption": "Figure 1: Short and detailed captions are generated by our video captioning model. The short captions provide action-centric summaries of the videos, while the detailed captions offer rich descriptions of the scenes, including attributes, objects, and environments.", "description": "This figure showcases example short and detailed video captions generated by the authors' video captioning model.  The short captions offer concise, action-focused summaries of the video content, while the detailed captions provide much richer descriptions.  These detailed descriptions include visual details such as the specific objects and attributes present, a more thorough explanation of the scene, and environmental contextual information.", "section": "2 Data"}, {"figure_path": "https://arxiv.org/html/2504.08685/extracted/6349619/figures/video_data_pipeline.png", "caption": "Figure 2: Video Data Processing Pipeline Overview.", "description": "This figure illustrates the high-throughput video data processing pipeline used in the Seaweed-7B model training.  The pipeline consists of several key stages: 1) Video Input: The process starts with raw video inputs from various sources. 2) Split: The raw videos are split into single-shot clips using a method that identifies shot boundaries. 3) Crop: Unwanted regions (e.g., black borders, watermarks, logos) are removed through spatial cropping based on frame-level object detection. 4) Filter: Multiple filters are applied to ensure high quality, such as visual quality assessment (aesthetics and clarity), spatial-temporal motion analysis (removing static clips and unwanted movements), safety screening (remove harmful content), and artifact detection. 5) Dedup: Duplicate videos and those belonging to the head categories are identified and removed to reduce redundancy and balance the data. 6) Pack: The preprocessed video clips are packaged into a dataset for training.  7) Dataset: The final dataset is saved in a database, along with associated metadata.  The pipeline is designed to manage video encoding and decoding, perform temporal segmentation and spatial cropping, and apply various quality filters, to mine high-quality clips efficiently. This pipeline allowed for processing over 500,000 hours of video data per day.", "section": "2 Data"}, {"figure_path": "https://arxiv.org/html/2504.08685/x2.png", "caption": "Figure 3: Overview of VAE architecture.", "description": "This figure illustrates the architecture of the Variational Autoencoder (VAE) used in the Seaweed-7B video generation model.  It shows the process of encoding an input video (with dimensions (1+T) x H x W, where T represents the number of frames, H the height, and W the width) into a lower-dimensional latent space (T x H/dh x W/dw x C, where dh and dw are downsampling factors and C is the number of channels).  This compression is achieved using a 3D causal convolutional encoder. The decoder then reconstructs the video from this latent representation, using a 3D causal convolutional decoder to produce an output video with the same dimensions as the input.  The diagram highlights the key components of the VAE, including the encoder, latent space, and decoder, showing how the input video is compressed and then reconstructed.", "section": "3.1 Variational Autoencoder"}, {"figure_path": "https://arxiv.org/html/2504.08685/extracted/6349619/figures/vae_visualization/original_cake.jpg", "caption": "(a) Original Video", "description": "This figure shows a comparison of the original video with its reconstruction after applying a variational autoencoder (VAE). The VAE is a crucial component of the Seaweed-7B video generation model, responsible for compressing the video data into a lower-dimensional latent space and then reconstructing it.  This figure helps to visualize the model's ability to reconstruct video data with high fidelity. The quality of the reconstructed video is key to the performance of the overall generation process.", "section": "3.1 Variational Autoencoder"}, {"figure_path": "https://arxiv.org/html/2504.08685/extracted/6349619/figures/vae_visualization/v3_rec_cake.jpg", "caption": "(b) 48\u00d748\\times48 \u00d7 compression Seaweed VAE", "description": "The figure shows a visualization comparing the reconstruction quality of the 48x48x48 compression Seaweed VAE model.  This refers to the compression ratio achieved by the variational autoencoder (VAE) component of the model.  The image likely depicts the output of this VAE when processing a video frame, showing how well the model reconstructs the original video information from a compressed representation. The 48x refers to the overall downsampling factor applied during compression.  Higher numbers indicate stronger compression.", "section": "3.1 Variational Autoencoder"}, {"figure_path": "https://arxiv.org/html/2504.08685/extracted/6349619/figures/vae_visualization/v4_rec_cake.jpg", "caption": "(c) 64\u00d764\\times64 \u00d7 compression Seaweed VAE", "description": "The figure shows the result of compressing a video sequence using the 64x64x64 compression Seaweed VAE.  This refers to a variational autoencoder (VAE) model designed to compress video data efficiently while maintaining a good level of reconstruction quality. The \"64x64x64\" likely denotes the compression ratio or the reduction in spatial and temporal dimensions of the video.  The figure likely visualizes the reconstructed video after compression by this VAE, enabling a comparison of quality before and after the compression process. ", "section": "3.1 Variational Autoencoder"}, {"figure_path": "https://arxiv.org/html/2504.08685/extracted/6349619/figures/vae_visualization/original_skiing.jpg", "caption": "Figure 4: VAE visualization comparison at 25 fps, with a resolution of 720\u00d7\\times\u00d7720.", "description": "This figure compares the visual reconstruction quality of three different video versions: the original video and the output videos from two variants of the Seaweed VAE (Variational Autoencoder) model.  The Seaweed VAE is a crucial component of the overall video generation model, responsible for compressing the raw video data into a lower-dimensional latent representation which can be processed by the subsequent DiT (Diffusion Transformer) model. The two VAE variants differ in their compression ratios: one with a 48x compression ratio and the other with a 64x compression ratio. The figure displays a frame from each video at 25 frames per second (fps), and each video's resolution is 720x720 pixels.  This comparison helps to illustrate how compression ratio affects the reconstruction quality. By comparing the original to the outputs with the differing levels of compression, one can see how much information is lost or retained in the process.", "section": "3.1 Variational Autoencoder"}, {"figure_path": "https://arxiv.org/html/2504.08685/extracted/6349619/figures/vae_visualization/v3_skiing.jpg", "caption": "(a) Original Video", "description": "The figure shows a comparison of three video frames. (a) displays the original video frame.  The other frames (b and c) likely show the same frame reconstructed using different versions of a variational autoencoder (VAE) described in the paper, demonstrating the effect of different compression techniques on the quality of video reconstruction.  The differences between (a), (b) and (c) highlight the VAE's ability to compress and reconstruct video frames, with (b) possibly representing a lower compression ratio than (c).", "section": "3.1 Variational Autoencoder"}, {"figure_path": "https://arxiv.org/html/2504.08685/extracted/6349619/figures/vae_visualization/v4_skiing.jpg", "caption": "(b) 48\u00d748\\times48 \u00d7 Seaweed VAE", "description": "This figure shows a visualization of the Seaweed VAE (Variational Autoencoder) with a compression ratio of 48x48x48. The VAE is a crucial component of the Seaweed-7B video generation model, responsible for compressing raw video data into a lower-dimensional latent space and then reconstructing the video from this compressed representation. A compression ratio of 48x48x48 indicates a significant reduction in data size.  The figure likely visualizes a sample video or frame that has been compressed and then reconstructed by the Seaweed VAE.  The visual quality of the reconstruction is a key indicator of the VAE's effectiveness.  A high-quality reconstruction, despite the high compression ratio, would show that the VAE is efficiently preserving important information during the compression process. The figure likely aims to demonstrate that the VAE achieves good compression without significant loss of visual fidelity.", "section": "3.1 Variational Autoencoder"}, {"figure_path": "https://arxiv.org/html/2504.08685/x3.png", "caption": "(c) 64\u00d764\\times64 \u00d7 Seaweed VAE", "description": "This figure visualizes the reconstruction quality of a variational autoencoder (VAE) model. Specifically, it showcases the results of a 64x64x64 Seaweed VAE, which compresses an input video into a 64x64x64 latent representation and then reconstructs it. This figure likely aims to demonstrate the VAE's ability to accurately represent the key features of a video with a relatively high compression ratio.  The figure likely shows a comparison of the original video with the reconstructed one to illustrate the efficacy of the VAE. This evaluation is crucial for demonstrating the effectiveness of the VAE as a component within the larger video generation model.", "section": "3.1 Variational Autoencoder"}, {"figure_path": "https://arxiv.org/html/2504.08685/x4.png", "caption": "Figure 5: VAE visualization comparison at 24 fps, with a resolution of 684\u00d7\\times\u00d7684.", "description": "This figure compares the visual quality of video reconstruction by three different methods: (a) the original video, (b) reconstruction using the 48x compression Seaweed VAE, and (c) reconstruction using the 64x compression Seaweed VAE.  All videos are shown at 24 frames per second (fps) and a resolution of 684x684 pixels. The comparison allows for a visual assessment of how different compression ratios impact the reconstruction's fidelity, detail, and overall visual quality.", "section": "3.1 Variational Autoencoder"}, {"figure_path": "https://arxiv.org/html/2504.08685/x5.png", "caption": "48\u00d748\\times48 \u00d7 VAE at 30k steps", "description": "This figure shows a visualization of a Variational Autoencoder (VAE) with a compression ratio of 48x.  The VAE was trained for 30,000 steps. The visualization likely demonstrates the VAE's reconstruction capability, showing either input video frames and their corresponding compressed and then reconstructed representations, or the generated output video frames compared to the original video frames. The 48x compression ratio indicates that the VAE significantly reduces the size of the video data during encoding. This is crucial for efficient video generation, allowing the model to handle larger videos and reducing computational costs.", "section": "3.1 Variational Autoencoder"}, {"figure_path": "https://arxiv.org/html/2504.08685/x6.png", "caption": "64\u00d764\\times64 \u00d7 VAE at 30k steps", "description": "This figure shows a visualization of the results of a variational autoencoder (VAE) model after 30,000 training steps.  The VAE uses a 64x64x64 compression ratio, meaning the input video frames are downsampled significantly before being encoded into a latent representation. This visualization likely demonstrates the quality of reconstruction after the model has learned to compress and decompress video frames efficiently.  The image shows a sample video frame from the reconstruction. This visualization is key in evaluating the performance and tradeoffs of different compression ratios in the VAE architecture, helping to determine the optimal balance between compression efficiency and reconstruction quality.", "section": "3.1 Variational Autoencoder"}, {"figure_path": "https://arxiv.org/html/2504.08685/x7.png", "caption": "48\u00d748\\times48 \u00d7 VAE, 45k steps", "description": "This figure shows the results of using a 48x compression Seaweed Variational Autoencoder (VAE) trained for 45,000 steps.  The VAE is a crucial component in the video generation pipeline, responsible for compressing the input video frames into a lower-dimensional latent space.  This figure likely displays sample video frames to visually demonstrate the reconstruction quality of the VAE after training, showcasing how well the VAE can compress and then reconstruct video data. The '48x' likely refers to the compression ratio achieved by the model.", "section": "3.1 Variational Autoencoder"}, {"figure_path": "https://arxiv.org/html/2504.08685/x8.png", "caption": "64\u00d764\\times64 \u00d7 VAE at 45k steps", "description": "This figure visualizes the reconstruction of a video by a Variational Autoencoder (VAE) model. Specifically, it shows the result of a 64x64x64 VAE model after 45,000 training steps.  The \"64x64x64\" refers to the compression ratio or dimensions of the latent space representation produced by the VAE.  The image likely demonstrates the model's ability to reconstruct the original video from its compressed latent representation, highlighting its reconstruction quality after a significant number of training iterations. ", "section": "3.1 Variational Autoencoder"}, {"figure_path": "https://arxiv.org/html/2504.08685/x9.png", "caption": "48\u00d748\\times48 \u00d7 VAE at 60k steps", "description": "This figure visualizes the output of a 48x compression Variational Autoencoder (VAE) after 60,000 training steps.  The VAE is a crucial component in the Seaweed-7B video generation model, responsible for compressing raw video data into a lower-dimensional latent representation. This specific VAE uses a compression ratio of 48x, meaning that it reduces the dimensionality of the video data by a factor of 48. The image likely shows a sample video reconstruction from the latent space, demonstrating the quality of the VAE's compression and reconstruction capabilities after extensive training.", "section": "3.1 Variational Autoencoder"}, {"figure_path": "https://arxiv.org/html/2504.08685/x10.png", "caption": "64\u00d764\\times64 \u00d7 VAE at 60k steps", "description": "This figure visualizes the results of a Variational Autoencoder (VAE) model trained for 60,000 steps with a 64x64x64 compression ratio.  It showcases the model's ability to reconstruct video frames after compression. The image likely depicts sample frames of a video showing how well the VAE reconstructs high-dimensional video data into a lower-dimensional latent space and back again.", "section": "3.1 Variational Autoencoder"}, {"figure_path": "https://arxiv.org/html/2504.08685/x11.png", "caption": "Figure 6: \nDiT generation results: keyframes from 73\u00d7192\u00d73207319232073\\times 192\\times 32073 \u00d7 192 \u00d7 320 videos with the prompt \"Zoom in, cat watching TV with a remote in hand, highly detailed\" are shown. Under the same compute, 64\u00d764\\times64 \u00d7 Seaweed VAE converges faster with a higher compression ratio, without using a DiT patchify.\nLeft: 48\u00d748\\times48 \u00d7 Seaweed VAE (dt,dh,dw)=(4,16,16)subscript\ud835\udc51\ud835\udc61subscript\ud835\udc51\u210esubscript\ud835\udc51\ud835\udc6441616(d_{t},d_{h},d_{w})=(4,16,16)( italic_d start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_d start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT , italic_d start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT ) = ( 4 , 16 , 16 ) with DiT patch size (pt,ph,pw)=(1,2,2)subscript\ud835\udc5d\ud835\udc61subscript\ud835\udc5d\u210esubscript\ud835\udc5d\ud835\udc64122(p_{t},p_{h},p_{w})=(1,2,2)( italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_p start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT , italic_p start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT ) = ( 1 , 2 , 2 ).\nRight: 64\u00d764\\times64 \u00d7 Seaweed VAE (dt,dh,dw)=(4,32,32)subscript\ud835\udc51\ud835\udc61subscript\ud835\udc51\u210esubscript\ud835\udc51\ud835\udc6443232(d_{t},d_{h},d_{w})=(4,32,32)( italic_d start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_d start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT , italic_d start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT ) = ( 4 , 32 , 32 ) with DiT patch size (pt,ph,pw)=(1,1,1)subscript\ud835\udc5d\ud835\udc61subscript\ud835\udc5d\u210esubscript\ud835\udc5d\ud835\udc64111(p_{t},p_{h},p_{w})=(1,1,1)( italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_p start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT , italic_p start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT ) = ( 1 , 1 , 1 ).", "description": "Figure 6 presents a comparison of video generation results using two different configurations of the Seaweed VAE model.  Both configurations used the prompt \"Zoom in, cat watching TV with a remote in hand, highly detailed.\"  The left side shows results from a 48x Seaweed VAE, which has a lower compression ratio and uses DiT patchification with a patch size of (1,2,2).  The right side shows results from a 64x Seaweed VAE, which has a higher compression ratio and does *not* use DiT patchification, employing instead a patch size of (1,1,1).  The experiment highlights that, under the same computational resources, the 64x VAE achieves faster convergence.", "section": "3 Design and Discussions"}, {"figure_path": "https://arxiv.org/html/2504.08685/extracted/6349619/figures/dit_attn/scaling_laws.png", "caption": "Figure 7: \nValidation metric curves on high-resolution image reconstruction (512\u00d7512512512512\\times 512512 \u00d7 512) show the effectiveness of mix-resolution VAE training.", "description": "Figure 7 presents a comparison of training a Variational Autoencoder (VAE) using two different approaches: one using only low-resolution (256x256) images and another using a mix of low-resolution and high-resolution (512x512) images.  The results show that while training with only low-resolution images yields faster initial convergence, performance plateaus and may even decline towards the end of training.  Conversely, using a mix of resolutions consistently improves the VAE's ability to reconstruct high-resolution images throughout the training process, as measured by rFID, LPIPS, and PSNR metrics. This demonstrates the effectiveness of the mixed-resolution training strategy for achieving better high-resolution reconstruction in VAEs.", "section": "3.1 Variational Autoencoder"}, {"figure_path": "https://arxiv.org/html/2504.08685/extracted/6349619/figures/dit_attn/new_attn_loss2.png", "caption": "Figure 8: Loss comparison between the dual-stream and the hybrid-stream architectures. The table compares the two losses under the same training FLOPs.", "description": "This figure compares the training loss curves of two different diffusion transformer architectures: the dual-stream architecture and the hybrid-stream architecture.  Both architectures were trained with the same computational budget (measured in FLOPs, or floating point operations). The graph shows the training loss over time (steps) for each architecture. The accompanying table provides a direct numerical comparison of the final training loss achieved by each architecture under the identical FLOP constraint, highlighting the superior efficiency of the hybrid-stream design.", "section": "3.2 Diffusion Transformer Model"}, {"figure_path": "https://arxiv.org/html/2504.08685/x12.png", "caption": "Figure 9: Illustration of the space-full and window attention architecture.", "description": "Figure 9 illustrates three different attention mechanisms used in the Seaweed-7B video generation model.  (a) shows the 'space-full attention' architecture, which alternates between layers of full attention (considering all tokens in the sequence) and space-only attention (focusing only on spatial relationships within the video frames). (b) depicts the 'window attention' method where attention is restricted to smaller windows within the video sequence.  This is done to reduce computational cost for longer videos.  Each method has different trade-offs in terms of computation and performance. The figure visually represents these different attention patterns applied to video and text tokens during the processing.", "section": "3.2 Diffusion Transformer Model"}, {"figure_path": "https://arxiv.org/html/2504.08685/extracted/6349619/figures/sft/turtle_keyframes.jpg", "caption": "Figure 10: Loss comparison of full and space-full attention.", "description": "This figure shows the loss comparison between two different attention mechanisms: full attention and space-full attention.  The x-axis represents the computational cost (measured in FLOPs), and the y-axis represents the loss. The space-full approach interleaves full attention and space-only attention layers, aiming to improve efficiency. The plot demonstrates how the loss changes as the computational budget increases for both methods, highlighting the trade-offs between computational cost and model performance in video generation tasks.", "section": "3.2 Diffusion Transformer Model"}, {"figure_path": "https://arxiv.org/html/2504.08685/extracted/6349619/figures/sft/astronaut_00007.jpg", "caption": "Figure 11: Loss comparison of full and window attention.", "description": "This figure illustrates a comparison of training loss between three different attention mechanisms used in a video generation model: full attention, space-full attention (interleaving full and spatial attention), and windowed attention.  The x-axis represents the computational cost (FLOPs), while the y-axis shows the training loss.  The graph demonstrates how the loss varies with increasing computational cost for each attention method. This allows for an assessment of the trade-offs between computational efficiency and model performance for different attention strategies.  The figure helps in deciding which attention strategy is most suitable for video generation tasks, given resource constraints.", "section": "3.2 Diffusion Transformer Model"}, {"figure_path": "https://arxiv.org/html/2504.08685/x13.png", "caption": "Figure 12: Loss comparison between RoPE and MM-Rope.", "description": "This figure showcases a comparison of training loss between two positional encoding methods: RoPE (Rotary Position Embedding) and MM-ROPE (Multimodal Rotary Position Embedding).  It demonstrates that MM-ROPE, which incorporates positional information for both text and video tokens, leads to lower training loss compared to the standard RoPE method.  This suggests that MM-ROPE is more effective in handling the diverse positional information present in the combined text and video data used for video generation.", "section": "3.3 Training Stages"}, {"figure_path": "https://arxiv.org/html/2504.08685/extracted/6349619/figures/training_infrastructure/mlao.png", "caption": "Figure 13: \nTop: Before SFT.\nBottom: After SFT.\nResults for prompt \"Turtle swimming in the ocean\".", "description": "This figure shows a comparison of video generation results before and after supervised fine-tuning (SFT). The prompt used was \"Turtle swimming in the ocean\". The top row displays the video generated before SFT, while the bottom row shows the improved video generated after SFT. The images illustrate how SFT enhances the visual quality of the generated video.  Specifically, one can see improvements in the visual details and clarity of the generated turtle and ocean after SFT.", "section": "3.3 Training Stages"}, {"figure_path": "https://arxiv.org/html/2504.08685/extracted/6349619/figures/training_infrastructure/runtime_balance.png", "caption": "Figure 14: \nLeft: Before SFT.\nMiddle: Good SFT.\nRight: Overfit SFT.\nResults for prompt: \"An astronaut running through an alley in Rio de Janeiro, 4k, high resolution\".", "description": "This figure displays three video generation results using different training stages. The prompt is \"An astronaut running through an alley in Rio de Janeiro, 4k, high resolution\".  The left image shows the result before supervised fine-tuning (SFT), demonstrating a less refined and possibly less coherent video. The middle image displays the result after a successful SFT, showing improved visual quality and adherence to the prompt. The right image displays an overfit SFT result, where the model has become overly specialized to the training data, losing the ability to generate good results for unseen prompts. The comparison highlights how SFT improves video generation quality, but overfitting can decrease quality.", "section": "3.3 Training Stages"}, {"figure_path": "https://arxiv.org/html/2504.08685/x14.png", "caption": "Figure 15: Two image-to-video examples before (top row) and after (bottom row) DPO. DPO significantly improves the structure and motion quality.", "description": "This figure showcases two examples of image-to-video generation using the Direct Preference Optimization (DPO) technique. The top row displays the results before applying DPO, while the bottom row presents the results after applying DPO.  A visual comparison reveals that DPO significantly improves the structural integrity and the quality of motion in the generated videos. The enhancements are particularly notable in the overall coherence and smoothness of the movements within the videos.", "section": "3.2 Diffusion Transformer Model"}, {"figure_path": "https://arxiv.org/html/2504.08685/x15.png", "caption": "Figure 16: Multi-level activation checkpointing(MLAC). (a) Vanilla AC saves inputs on device could still encounter GPU OOM. (b) MLAC further supports offloading module inputs to achieve zero-activation AC, (c) and minimize recomputation overheads by saving compute-bound activations to multi-level storage space.", "description": "This figure illustrates the concept of Multi-level Activation Checkpointing (MLAC) and its advantages over traditional Activation Checkpointing (AC).  Panel (a) shows Vanilla AC, where intermediate activations are saved to the device's memory, which can lead to out-of-memory (OOM) errors, especially with large models.  Panel (b) shows MLAC, which addresses the OOM issue by offloading some module inputs to CPU or disk memory (zero-activation AC).  Finally, panel (c) details how MLAC minimizes recomputation costs by intelligently storing and retrieving activations from a hierarchy of storage (GPU, CPU, and disk), prioritizing the saving of computationally intensive activations to reduce the number of computations needed during backpropagation.", "section": "3.4 Infrastructure"}, {"figure_path": "https://arxiv.org/html/2504.08685/extracted/6349619/figures/vae_visualization/original.jpg", "caption": "Figure 17: Balance samples within one batch across GPUs by runtime metric.\nTop: seqlen-to-runtime lookup table.\nBottom left: One batch samples across GPUs before balance.\nBottom left: One batch samples across GPUs after balance.", "description": "Figure 17 illustrates the process of balancing computational load across multiple GPUs during model training. The top panel shows a lookup table that maps sequence lengths (seqlen) to their corresponding runtime on a single GPU. This table is used to estimate the runtime of each sample in a batch. The bottom-left panel displays the distribution of samples in a batch before load balancing; some GPUs have significantly more samples than others. The bottom-right panel shows the improved distribution of samples after applying the load-balancing technique.  The goal is to achieve a more even distribution of work across all GPUs for improved efficiency and reduced training time.", "section": "3.4 Infrastructure"}, {"figure_path": "https://arxiv.org/html/2504.08685/extracted/6349619/figures/vae_visualization/v3_formal.jpg", "caption": "Figure 18: Comparison of Seaweed-7B with the top ranking models: Kling 1.6, Sora, HunyuanVideo, and Wan-2.1. The task is image-to-video.", "description": "This figure presents a detailed comparison of Seaweed-7B's performance on the image-to-video generation task against four top-performing models: Kling 1.6 HD, Sora, HunyuanVideo, and Wan 2.1.  The comparison is multifaceted, using human evaluations across multiple aspects of video quality.  These aspects include: Motion Quality (smoothness and naturalness of movement), Prompt Following (how well the generated video adheres to the input image prompt), Ref-image Consistency (how similar the generated video is to the input image), and Visual Quality (overall aesthetic appeal and fidelity). Each comparison is presented as a bar chart showing the percentage of times each model was preferred by human evaluators, with the remaining percentage indicating ties. This provides a comprehensive assessment of Seaweed-7B's strengths and weaknesses relative to the leading models, highlighting its competitive performance despite its smaller size and training resources.", "section": "4.1 Quantitative Analysis of Video Generation"}, {"figure_path": "https://arxiv.org/html/2504.08685/extracted/6349619/figures/vae_visualization/v4_formal.jpg", "caption": "Figure 19: Comparison of the Seaweed-7B Model top ranking models: Veo 2.0 and Wan-2.1. The task is text-to-video.", "description": "This figure compares the performance of the Seaweed-7B model against Veo 2.0 and Wan-2.1 on text-to-video generation tasks.  It provides a detailed breakdown of each model's performance across several key metrics: Motion Quality, Prompt Following, Ref-image Consistency, Visual Quality, and an Overall score.  Each metric is presented as a bar graph showing the relative performance of each model. The visual comparison helps highlight the strengths and weaknesses of Seaweed-7B in relation to the leading models in text-to-video generation.", "section": "4.1 Quantitative Analysis of Video Generation"}, {"figure_path": "https://arxiv.org/html/2504.08685/extracted/6349619/figures/vae_visualization/hunyuan_4x8x8.jpg", "caption": "(a) Original Video", "description": "The figure shows a comparison of the original video with its reconstruction using the Seaweed VAE model at different compression ratios.  Specifically, it displays the original video frame alongside versions that have undergone compression with 48x and 64x compression ratios applied by the VAE, allowing visual assessment of the fidelity and quality of the compression.", "section": "3.1 Variational Autoencoder"}, {"figure_path": "https://arxiv.org/html/2504.08685/extracted/6349619/figures/application/audio-cavp.png", "caption": "(b) 48\u00d748\\times48 \u00d7 Seaweed VAE", "description": "The figure shows a visualization of the 48x Seaweed VAE (Variational Autoencoder).  The 48x refers to the compression ratio achieved by this specific VAE model. The VAE takes an input video, compresses it into a lower-dimensional latent space, and then reconstructs the video from this compressed representation.  This particular figure likely showcases the reconstruction quality of the VAE, demonstrating its ability to effectively compress and reconstruct video data with minimal information loss.  The effectiveness of the compression is crucial, as this latent space representation forms the basis for the subsequent video generation process using a diffusion transformer.", "section": "3.1 Variational Autoencoder"}]