[{"figure_path": "https://arxiv.org/html/2501.09038/x1.png", "caption": "Figure 1: Sample scenarios from the Physics-IQ dataset for testing physical understanding in generative video models. Models are shown the beginning of a video (single frame for image2video models; 3 seconds for video2video models) and need to predict how the video continues over the next 5 seconds, which requires understanding different physical properties: Solid Mechanics, Fluid Dynamics, Optics, Thermodynamics, and Magnetism. See here for an animated version of this figure.", "description": "Figure 1 presents example scenarios from the Physics-IQ benchmark dataset, designed to evaluate the physical understanding capabilities of video generative models.  Each scenario tests a specific aspect of physics (Solid Mechanics, Fluid Dynamics, Optics, Thermodynamics, and Magnetism).  The models receive either a single frame (for image-to-video models) or a 3-second video clip (for video-to-video models) as input, and are then tasked with predicting the next 5 seconds of the video. Successful prediction necessitates an understanding of the relevant physical principles involved in each scene.", "section": "Physics-IQ benchmark"}, {"figure_path": "https://arxiv.org/html/2501.09038/x2.png", "caption": "Figure 2: Overview of the Physics-IQ evaluation protocol. A video generative model produces a 5 second continuation of the conditioning frame(s), optionally including a textual description of the conditioning frames for models that accept text input. They are compared against the ground truth test frames using four metrics that quantify different properties of physical understanding. The metrics are defined and explained in the methods section. Code to run the evaluation is available at Physics-IQ-benchmark.", "description": "This figure illustrates the Physics-IQ evaluation process.  A video generation model is given a short video clip (conditioning frames) as input, optionally along with a text description if the model supports it. The model then predicts a 5-second continuation of the video. This prediction is compared to the actual video (ground truth test frames) using four metrics: Spatial IoU, Spatiotemporal IoU, Weighted spatial IoU, and MSE. These metrics assess different aspects of physical understanding, such as the location, timing, extent, and precision of the predicted actions. The results from these metrics help determine how well the model understands the physics of the scene.  The code for running this evaluation is publicly available on the Physics-IQ-benchmark.", "section": "Evaluation protocol"}, {"figure_path": "https://arxiv.org/html/2501.09038/x3.png", "caption": "Figure 3: A qualitative overview of recent synthetic datasets related to physical understanding (19, 20, 17, 38, 18, 36, 37, 39). These datasets are great for the purposes they were designed for, but not ideal for evaluating models trained on real-world videos due to the distribution shift.", "description": "Figure 3 showcases several existing datasets used for evaluating physical reasoning in AI models.  These datasets, including CRAFT, IntPhys, Physion, ESPRIT, Physion++, CoPhy, CLEVERER, and PhyWorld, all use synthetic data (computer-generated) rather than real-world video data. While effective for their intended purposes, using them to evaluate models trained on real videos is problematic because the characteristics of the simulated environments differ significantly from the real-world data distribution, potentially leading to inaccurate assessment of a model's capabilities.", "section": "Models"}, {"figure_path": "https://arxiv.org/html/2501.09038/x4.png", "caption": "Figure 4: How well do current video generative models understand physical principles? Left.\u00a0The Physics-IQ score is an aggregated measure across four individual metrics, normalized such that pairs of real videos that differ only by physical randomness score 100%. All evaluated models show a large gap, with the best model scoring 24.1%, indicating that physical understanding is severely limited. Right.\u00a0In addition, the mean rank of models across all four metrics is shown here; the Spearman correlation between aggregated results on the left and mean rank on the right is high (-\u2062.87,p<.005-.87p.005\\text{-}.87,\\emph{p}<.005- .87 , p < .005), thus aggregating to a single Physics-IQ score largely preserves model rankings.", "description": "Figure 4 assesses the physical understanding of eight different video generative models using the Physics-IQ benchmark. The left panel displays the Physics-IQ scores, which aggregate four individual metrics (Spatial IoU, Spatiotemporal IoU, Weighted spatial IoU, and MSE) measuring different aspects of physical understanding. The scores are normalized so that pairs of real videos differing only by random physical variations score 100%. The results reveal a significant gap between model performance and this upper bound, with the best-performing model reaching only 24.1%, highlighting the models' limited physical understanding. The right panel complements this by showing the mean rank of models across the four metrics. The high Spearman correlation (-0.87, p < 0.005) between the aggregated Physics-IQ scores and the mean ranks confirms that aggregating the four metrics into a single Physics-IQ score effectively reflects model performance.", "section": "Results"}, {"figure_path": "https://arxiv.org/html/2501.09038/x5.png", "caption": "Figure 5: Relationship between visual realism and physical understanding. Left. A multimodal large language model (Gemini 1.5 Pro) is asked to identify the generated video among the real and the generated video for each scenario (MLLM score) in a two-alternative forced choice paradigm. Chance rate is 50%; lower scores indicate that the model finds it harder to tell apart generated from real videos (= better realism). Sora-generated videos are hardest to distinguish from real videos for the model, whereas Lumiere (multiframe) is easiest. Right. Do models that produce \u2018realistic-looking\u2019 videos (as assessed by the MLLM score) also score better in terms of physical understanding (as assessed via the Physics-IQ score)? This scatterplot with linear fit and 95% confidence interval as a shaded blue area shows that this is not the case: Visual realism is uncorrelated with physical understanding (Pearson\u2019s r = - 0.46, p=.247 not significant). Note that the y axis on this plot is inverted for easier interpretation (up & right are best).", "description": "This figure investigates the relationship between visual realism and physical understanding in video generation models. The left panel shows the results of a two-alternative forced-choice experiment where a large language model (MLLM) distinguished between real and generated videos. Lower scores indicate higher visual realism, with Sora achieving the lowest score (best realism) and Lumiere (multiframe) the highest. The right panel displays a scatter plot examining the correlation between visual realism (MLLM score) and physical understanding (Physics-IQ score). The plot reveals a lack of correlation, suggesting that high visual realism does not necessarily imply strong physical understanding (Pearson's r = -0.46, p = .247).", "section": "Results"}, {"figure_path": "https://arxiv.org/html/2501.09038/x6.png", "caption": "Figure 6: Performance comparison of video generative models across different physical categories (columns) and metrics (rows). For the top three metrics, higher is better; for the last metric lower values are best. Throughout, physical variance (i.e., the performance that is achievable by real videos differing only by physical randomness) is indicated by a dashed line. Across metrics and categories, models show a considerable lack in physical understanding. More lenient metrics like \ud835\uddb2\ud835\uddc9\ud835\uddba\ud835\uddcd\ud835\uddc2\ud835\uddba\ud835\uddc5\u2062-\u2062\ud835\udda8\ud835\uddc8\ud835\uddb4\ud835\uddb2\ud835\uddc9\ud835\uddba\ud835\uddcd\ud835\uddc2\ud835\uddba\ud835\uddc5-\ud835\udda8\ud835\uddc8\ud835\uddb4\\mathsf{Spatial}\\text{-}\\mathsf{IoU}sansserif_Spatial - sansserif_IoU (top row) that only assess where an action occurred lead to higher scores than more strict metrics that also take into account e.g.\u00a0when or how much action should be taking place.", "description": "Figure 6 presents a detailed comparison of eight different video generative models' performance across various physical phenomena and evaluation metrics.  The models are assessed on their ability to predict the continuation of short video clips depicting events governed by different physical principles (solid mechanics, fluid dynamics, optics, thermodynamics, and magnetism). Four distinct metrics are used to evaluate the models: Spatial IoU (measuring the correctness of the location of actions), Spatiotemporal IoU (assessing both location and timing accuracy), Weighted Spatial IoU (considering both location and the amount of action), and MSE (measuring the pixel-level difference between the generated and real videos). Each metric's results are displayed for each physical category and model, allowing for a comprehensive comparison of model performance.  A dashed line indicates the 'physical variance', representing the performance limit imposed by inherent variability in real-world physical events.  The figure demonstrates that while some models perform reasonably well on less stringent metrics like Spatial IoU, they struggle with more demanding metrics that require understanding not only *where* but also *when* and *how much* action takes place, highlighting a general lack of robust physical understanding in current video generative models.", "section": "Results"}, {"figure_path": "https://arxiv.org/html/2501.09038/x7.png", "caption": "Figure 7: We here visualize success and failure scenarios within the fluid dynamics and solid mechanics categories for the two best models, VideoPoet and Runway Gen 3, according to our metrics. Both models are able to generate physics plausible frames for scenarios such as smearing paint on glass (VideoPoet) and pouring red liquid on a rubber duck (Runway Gen 3). At the same time, the models fail to simulate a ball falling into a crate or cutting a tangerine with a knife. See here for an animated version.", "description": "Figure 7 shows example successes and failures of two top-performing video generation models (VideoPoet and Runway Gen 3) on tasks requiring an understanding of fluid dynamics and solid mechanics.  The figure demonstrates that while the models can accurately simulate some simple physics-based scenarios (e.g., smearing paint or pouring liquid), they struggle with more complex scenarios that involve interactions and precise movements (e.g., a ball falling into a crate or a knife cutting a tangerine). The examples highlight the limitations of current generative video models in realistically representing physical interactions.", "section": "Results"}, {"figure_path": "https://arxiv.org/html/2501.09038/x8.png", "caption": "Figure 8: Illustration of recording setup (top) and perspectives (bottom).", "description": "The figure shows the setup used to record videos for the Physics-IQ dataset.  The top panel displays three Sony Alpha a6400 cameras mounted on tripods, positioned to capture three different perspectives (left, center, right) of the same scene. The bottom panel shows example images captured from each of these three perspectives, demonstrating the slightly varied viewpoints obtained.", "section": "Physics-IQ benchmark"}, {"figure_path": "https://arxiv.org/html/2501.09038/extracted/6132070/figures/combined_setup_views.png", "caption": "Figure 9: The switch frames (here: center view only) of all scenarios in the Physics-IQ benchmark. A switch frame is the last conditioning frame before a model is asked to predict 5 seconds of future frames.", "description": "Figure 9 shows the \"switch frames\" from the Physics-IQ benchmark dataset.  Each image represents the final frame of a 3-second conditioning video shown to a generative video model. The model is then tasked with predicting the next 5 seconds of video based on this single frame.  The figure provides a visual overview of all 66 scenarios included in the benchmark, allowing for a visual inspection of the diversity and complexity of physical phenomena represented.", "section": "Physics-IQ benchmark"}, {"figure_path": "https://arxiv.org/html/2501.09038/extracted/6132070/figures/all_scenarios_overview.png", "caption": "Figure 10: Since mean squared error (MSE) values can be hard to interpret, this figure shows the effect of a distortion applied to the scene, serving as a rough intuition for the effect of a MSE at different noise levels.", "description": "Figure 10 demonstrates how different levels of mean squared error (MSE) affect image quality.  MSE is a metric used to evaluate how close a generated image is to the ground truth image.  The figure shows a series of images, all starting from a clear, original image of a puppy in a field.  As the MSE value increases (MSE = 0.001, MSE = 0.005, MSE = 0.01, MSE = 0.02, MSE = 0.04), progressive distortions are added to the image.  This visually demonstrates how higher MSE values correspond to greater noise and a blurring effect, ultimately resulting in a less clear and more distorted image.  The figure provides a visual aid to help readers better understand what different MSE scores represent in terms of the visual quality of images and videos.", "section": "Metrics for physical understanding"}]