[{"figure_path": "https://arxiv.org/html/2503.21088/x1.png", "caption": "Figure 1: Visualizing Unlearning via Model Merging.\nThe vanilla model (top) initially assigns high probabilities to forget set (member) and low probabilities to holdout data (nonmember).\nWe then merge two individually unlearned models:\none exhibiting over-forgetting (middle left) and the other under-forgetting (middle right).\nModel merging aims to achieve balanced forgetting (bottom), effectively reducing the model\u2019s confidence in predicting sensitive member data while preserving its performance on nonmember data.", "description": "This figure illustrates the model merging technique used for unlearning sensitive information.  The top panel shows a vanilla language model, where high probabilities are assigned to data points in the forget set (data to be removed) and low probabilities are given to the holdout data (data to be retained). The middle-left panel displays an \"over-forgetting\" model, which removes too much information, while the middle-right panel shows an \"under-forgetting\" model that doesn't remove enough. The bottom panel depicts the final \"balanced-forgetting\" model obtained by merging the over-forgetting and under-forgetting models. This merging process reduces the model's confidence in predicting sensitive data from the forget set while maintaining its performance on the non-sensitive holdout data.", "section": "3 Methodology"}, {"figure_path": "https://arxiv.org/html/2503.21088/x2.png", "caption": "Figure 2: Performance Curves: Regurgitation and Knowledge Scores During Training.", "description": "This figure displays the performance of the model during training, focusing on two key metrics: Regurgitation Score and Knowledge Score.  The x-axis represents the training epochs, indicating the progression of training over time.  The y-axis shows the scores for both metrics. Separate lines are provided for different tasks (Task 1, Task 2, and Task 3) and whether the data used is the forget set or the retain set.  The curves illustrate how the model's ability to recall (regurgitate) sensitive information from the forget set and its ability to maintain general knowledge from the retain set changes over the course of training. This allows for an analysis of the effectiveness of the training process in achieving the dual goal of removing sensitive information while preserving useful information.", "section": "4 Experiments"}, {"figure_path": "https://arxiv.org/html/2503.21088/x3.png", "caption": "Figure 3: Training Loss Curves of NPO and NPO+GDR+KLR.", "description": "This figure shows the training loss curves for two different model training approaches: one using only Negative Preference Optimization (NPO) and the other using NPO in combination with Gradient Descent on Retain Set (GDR) and Kullback-Leibler Divergence Minimization on Retain Set (KLR).  The graph illustrates how the loss changes over training epochs for each method.  By comparing the loss curves, we can observe the effectiveness of adding GDR and KLR to NPO for improving training performance and achieving balanced unlearning.", "section": "3 Methodology"}, {"figure_path": "https://arxiv.org/html/2503.21088/x4.png", "caption": "Figure 4: Angle (\u03b8\ud835\udf03\\thetaitalic_\u03b8) between Parameter Change Vectors: \u0394\u2062P0165\u0394superscriptsubscript\ud835\udc430165\\Delta P_{0}^{165}roman_\u0394 italic_P start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 165 end_POSTSUPERSCRIPT, \u0394\u2062P165f\u2062i\u2062n\u2062a\u2062l\u0394superscriptsubscript\ud835\udc43165\ud835\udc53\ud835\udc56\ud835\udc5b\ud835\udc4e\ud835\udc59\\Delta P_{165}^{final}roman_\u0394 italic_P start_POSTSUBSCRIPT 165 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_f italic_i italic_n italic_a italic_l end_POSTSUPERSCRIPT, \u0394\u2062P0f\u2062i\u2062n\u2062a\u2062l\u0394superscriptsubscript\ud835\udc430\ud835\udc53\ud835\udc56\ud835\udc5b\ud835\udc4e\ud835\udc59\\Delta P_{0}^{final}roman_\u0394 italic_P start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_f italic_i italic_n italic_a italic_l end_POSTSUPERSCRIPT.", "description": "Figure 4 illustrates the angle (\u03b8) between parameter change vectors at different stages of model training.  Specifically, it shows the angles between the parameter changes from step 0 to 165 (\u0394P\u2080\u00b9\u2076\u2075), from step 165 to the final step (\u0394P\u2081\u2086\u2085final), and from step 0 to the final step (\u0394P\u2080final). This visualization helps analyze the optimization dynamics during training and understand how the model's focus shifts between forgetting and retaining knowledge.", "section": "5 Analysis"}, {"figure_path": "https://arxiv.org/html/2503.21088/x5.png", "caption": "Figure 5: Performance for different density choices", "description": "This figure demonstrates the impact of different density choices during the model merging phase on overall performance.  The x-axis likely represents different density thresholds used for trimming less important parameters during merging. The y-axis shows various performance metrics, such as aggregate score, task aggregate, MIA AUC score, and MMLU score.  The graph visualizes how varying the density affects the balance between retaining important information and removing sensitive data, ultimately revealing the optimal density setting for achieving the best results in the unlearning task.", "section": "5.2 Why Merge works?"}]