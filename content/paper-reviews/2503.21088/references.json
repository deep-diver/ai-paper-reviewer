{"references": [{"fullname_first_author": "Yadav", "paper_title": "TIES-merging: Resolving interference when merging models", "publication_date": "2023-MM-DD", "reason": "This paper is important because the method proposed by the paper is a critical component in the model merging phase, as the paper mentions: \"After training, we apply TIES-Merging (Yadav et al., 2023) to combine the LoRA adapters of the two models.\""}, {"fullname_first_author": "Zhang", "paper_title": "Negative Preference Optimization", "publication_date": "2024-MM-DD", "reason": "This paper is important because the method proposed in the paper is a critical component in the training phase, as the paper mentions: \"Three components are included in the optimization process: Negative Preference Optimization (NPO) (Zhang et al., 2024a) on forget set, alongside Gradient Descent on Retain Set (GDR) and Kullback-Leibler Divergence Minimization on Retain Set (KLR).\""}, {"fullname_first_author": "Hu", "paper_title": "Lora: Low-rank adaptation of large language models", "publication_date": "2021-MM-DD", "reason": "This paper is important because the method proposed in the paper is a critical component of the system's training phase, as the paper mentions: \"We train two models with identical objectives but different hyperparameters via Low-Rank Adaptation (LoRA) (Hu et al., 2021).\""}, {"fullname_first_author": "Chen", "paper_title": "Large knowledge model: Perspectives and challenges", "publication_date": "2024-MM-DD", "reason": "This paper is important as it provides a general overview of Large Knowledge Models, offering context for the task of unlearning sensitive content and is cited early in the introduction section of the paper."}, {"fullname_first_author": "Ramakrishna", "paper_title": "Lume: LLM unlearning with multitask evaluations", "publication_date": "2025-MM-DD", "reason": "This paper is important as it provides details about task description, as the paper mentions: \"For details about task description, please refer to the official paper (Ramakrishna et al., 2025).\""}]}