{"references": [{"fullname_first_author": "Bram Wallace", "paper_title": "Diffusion model alignment using direct preference optimization", "publication_date": "2024-01-01", "reason": "This paper is essential as it introduces Diffusion-DPO, which the current paper builds upon by addressing limitations in preference data."}, {"fullname_first_author": "Yuval Kirstain", "paper_title": "Pick-a-pic: An open dataset of user preferences for text-to-image generation", "publication_date": "2024-01-01", "reason": "This paper defines the benchmark dataset used for training and evaluation, making it crucial for the experimental setup and demonstrating the generalizability of the proposed method."}, {"fullname_first_author": "Rafael Rafailov", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "publication_date": "2024-01-01", "reason": "This paper presents the original Direct Preference Optimization (DPO) method, which forms the basis for the Adaptive-DPO approach proposed in the current work."}, {"fullname_first_author": "Xiaoshi Wu", "paper_title": "Better aligning text-to-image models with human preference", "publication_date": "2023-03-14", "reason": "This paper focuses on aligning text-to-image models with human preferences and sets the stage for further research in this area, providing context for the present paper's contributions."}, {"fullname_first_author": "Mohammad Gheshlaghi Azar", "paper_title": "A general theoretical paradigm to understand learning from human preferences", "publication_date": "2024-01-01", "reason": "This paper is important because it establishes a framework for understanding learning from human preferences, providing a theoretical foundation for the current paper's investigation into preference data."}]}